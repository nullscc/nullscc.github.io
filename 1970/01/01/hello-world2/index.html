
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Hello World | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="llama.cppInference of LLaMA model in pure C&#x2F;C++ Hot topics Added support for Falcon models: https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2717  A new file format has been introduced: GGUF Last revi">
<meta property="og:type" content="article">
<meta property="og:title" content="Hello World">
<meta property="og:url" content="http://example.com/1970/01/01/hello-world2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="llama.cppInference of LLaMA model in pure C&#x2F;C++ Hot topics Added support for Falcon models: https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2717  A new file format has been introduced: GGUF Last revi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png">
<meta property="article:published_time" content="1970-01-01T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T08:05:31.185Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-hello-world2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/1970/01/01/hello-world2/" class="article-date">
  <time datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hello World
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h1 id="llama-cpp"><a href="#llama-cpp" class="headerlink" title="llama.cpp"></a>llama.cpp</h1><p>Inference of <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">LLaMA</a> model in pure C&#x2F;C++</p>
<h3 id="Hot-topics"><a href="#Hot-topics" class="headerlink" title="Hot topics"></a>Hot topics</h3><ul>
<li><p>Added support for Falcon models: <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/2717">https://github.com/ggerganov/llama.cpp/pull/2717</a></p>
</li>
<li><p>A new file format has been introduced: <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/pull/2398">GGUF</a></p>
<p>Last revision compatible with the old format: <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa">dadbed9</a></p>
<h3 id="Current-master-should-be-considered-in-Beta-expect-some-issues-for-a-few-days"><a href="#Current-master-should-be-considered-in-Beta-expect-some-issues-for-a-few-days" class="headerlink" title="Current master should be considered in Beta - expect some issues for a few days!"></a>Current <code>master</code> should be considered in Beta - expect some issues for a few days!</h3><h3 id="Be-prepared-to-re-convert-and-or-re-quantize-your-GGUF-models-while-this-notice-is-up"><a href="#Be-prepared-to-re-convert-and-or-re-quantize-your-GGUF-models-while-this-notice-is-up" class="headerlink" title="Be prepared to re-convert and &#x2F; or re-quantize your GGUF models while this notice is up!"></a>Be prepared to re-convert and &#x2F; or re-quantize your GGUF models while this notice is up!</h3><h3 id="Issues-with-non-GGUF-models-will-be-considered-with-low-priority"><a href="#Issues-with-non-GGUF-models-will-be-considered-with-low-priority" class="headerlink" title="Issues with non-GGUF models will be considered with low priority!"></a>Issues with non-GGUF models will be considered with low priority!</h3></li>
</ul>
<hr>
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#description">Description</a>
    </li>
    <li>
      <a href="#usage">Usage</a>
      <ul>
        <li><a href="#get-the-code">Get the Code</a></li>
        <li><a href="#build">Build</a></li>
        <li><a href="#blas-build">BLAS Build</a></li>
        <li><a href="#prepare-data--run">Prepare Data & Run</a></li>
        <li><a href="#memorydisk-requirements">Memory/Disk Requirements</a></li>
        <li><a href="#quantization">Quantization</a></li>
        <li><a href="#interactive-mode">Interactive mode</a></li>
        <li><a href="#constrained-output-with-grammars">Constrained output with grammars</a></li>
        <li><a href="#instruction-mode-with-alpaca">Instruction mode with Alpaca</a></li>
        <li><a href="#using-openllama">Using OpenLLaMA</a></li>
        <li><a href="#using-gpt4all">Using GPT4All</a></li>
        <li><a href="#using-pygmalion-7b--metharme-7b">Using Pygmalion 7B & Metharme 7B</a></li>
        <li><a href="#obtaining-the-facebook-llama-original-model-and-stanford-alpaca-model-data">Obtaining the Facebook LLaMA original model and Stanford Alpaca model data</a></li>
        <li><a href="#verifying-the-model-files">Verifying the model files</a></li>
        <li><a href="#seminal-papers-and-background-on-the-models">Seminal papers and background on the models</a></li>
        <li><a href="#perplexity-measuring-model-quality">Perplexity (measuring model quality)</a></li>
        <li><a href="#android">Android</a></li>
        <li><a href="#docker">Docker</a></li>
      </ul>
    </li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#coding-guidelines">Coding guidelines</a></li>
    <li><a href="#docs">Docs</a></li>
  </ol>
</details>

<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>The main goal of <code>llama.cpp</code> is to run the LLaMA model using 4-bit integer quantization on a MacBook</p>
<ul>
<li>Plain C&#x2F;C++ implementation without dependencies</li>
<li>Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks</li>
<li>AVX, AVX2 and AVX512 support for x86 architectures</li>
<li>Mixed F16 &#x2F; F32 precision</li>
<li>2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit integer quantization support</li>
<li>CUDA, Metal and OpenCL GPU backend support</li>
</ul>
<p>The original implementation of <code>llama.cpp</code> was <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">hacked in an evening</a>.<br>Since then, the project has improved significantly thanks to many contributions. This project is mainly for educational purposes and serves<br>as the main playground for developing new features for the <a target="_blank" rel="noopener" href="https://github.com/ggerganov/ggml">ggml</a> library.</p>
<p><strong>Supported platforms:</strong></p>
<ul>
<li><input checked="" disabled="" type="checkbox"> Mac OS</li>
<li><input checked="" disabled="" type="checkbox"> Linux</li>
<li><input checked="" disabled="" type="checkbox"> Windows (via CMake)</li>
<li><input checked="" disabled="" type="checkbox"> Docker</li>
</ul>
<p><strong>Supported models:</strong></p>
<ul>
<li><input checked="" disabled="" type="checkbox"> LLaMA 🦙</li>
<li><input checked="" disabled="" type="checkbox"> LLaMA 2 🦙🦙</li>
<li><input checked="" disabled="" type="checkbox"> Falcon</li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca">Alpaca</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp#using-gpt4all">GPT4All</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese LLaMA &#x2F; Alpaca</a> and <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese LLaMA-2 &#x2F; Alpaca-2</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/bofenghuang/vigogne">Vigogne (French)</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/discussions/643#discussioncomment-5533894">Vicuna</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2023/04/03/koala/">Koala</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/OpenBuddy/OpenBuddy">OpenBuddy 🐶 (Multilingual)</a></li>
<li><input checked="" disabled="" type="checkbox"> <a href="#using-pygmalion-7b--metharme-7b">Pygmalion 7B &#x2F; Metharme 7B</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://github.com/nlpxucan/WizardLM">WizardLM</a></li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://huggingface.co/baichuan-inc/baichuan-7B">Baichuan-7B</a> and its derivations (such as <a target="_blank" rel="noopener" href="https://huggingface.co/hiyouga/baichuan-7b-sft">baichuan-7b-sft</a>)</li>
<li><input checked="" disabled="" type="checkbox"> <a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/Aquila-7B">Aquila-7B</a> &#x2F; <a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/AquilaChat-7B">AquilaChat-7B</a></li>
</ul>
<p><strong>Bindings:</strong></p>
<ul>
<li>Python: <a target="_blank" rel="noopener" href="https://github.com/abetlen/llama-cpp-python">abetlen&#x2F;llama-cpp-python</a></li>
<li>Go: <a target="_blank" rel="noopener" href="https://github.com/go-skynet/go-llama.cpp">go-skynet&#x2F;go-llama.cpp</a></li>
<li>Node.js: <a target="_blank" rel="noopener" href="https://github.com/hlhr202/llama-node">hlhr202&#x2F;llama-node</a></li>
<li>Ruby: <a target="_blank" rel="noopener" href="https://github.com/yoshoku/llama_cpp.rb">yoshoku&#x2F;llama_cpp.rb</a></li>
<li>Rust: <a target="_blank" rel="noopener" href="https://github.com/mdrokz/rust-llama.cpp">mdrokz&#x2F;rust-llama.cpp</a></li>
<li>C#&#x2F;.NET: <a target="_blank" rel="noopener" href="https://github.com/SciSharp/LLamaSharp">SciSharp&#x2F;LLamaSharp</a></li>
<li>Scala 3: <a target="_blank" rel="noopener" href="https://github.com/donderom/llm4s">donderom&#x2F;llm4s</a></li>
<li>Clojure: <a target="_blank" rel="noopener" href="https://github.com/phronmophobic/llama.clj">phronmophobic&#x2F;llama.clj</a></li>
</ul>
<p><strong>UI:</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nat/openplayground">nat&#x2F;openplayground</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/oobabooga/text-generation-webui">oobabooga&#x2F;text-generation-webui</a></li>
</ul>
<hr>
<p>Here is a typical run using LLaMA v2 13B on M2 Ultra:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">$ make -j &amp;&amp; ./main -m models/llama-13b-v2/ggml-model-q4_0.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:\nStep 1:&quot;</span> -n <span class="number">400</span> -e</span><br><span class="line">I llama.cpp build info:</span><br><span class="line">I UNAME_S:  Darwin</span><br><span class="line">I UNAME_P:  arm</span><br><span class="line">I UNAME_M:  arm64</span><br><span class="line">I CFLAGS:   -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE</span><br><span class="line">I CXXFLAGS: -I. -I./common -O3 -std=c++<span class="number">11</span> -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS</span><br><span class="line">I LDFLAGS:   -framework Accelerate</span><br><span class="line">I CC:       Apple clang version <span class="number">14.0</span><span class="number">.3</span> (clang-<span class="number">1403.0</span><span class="number">.22</span><span class="number">.14</span><span class="number">.1</span>)</span><br><span class="line">I CXX:      Apple clang version <span class="number">14.0</span><span class="number">.3</span> (clang-<span class="number">1403.0</span><span class="number">.22</span><span class="number">.14</span><span class="number">.1</span>)</span><br><span class="line"></span><br><span class="line">make: Nothing to be done <span class="keyword">for</span> `<span class="keyword">default</span><span class="string">&#x27;.</span></span><br><span class="line"><span class="string">main: build = 1041 (cf658ad)</span></span><br><span class="line"><span class="string">main: seed  = 1692823051</span></span><br><span class="line"><span class="string">llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from models/llama-13b-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))</span></span><br><span class="line"><span class="string">llama_model_loader: - type  f32:   81 tensors</span></span><br><span class="line"><span class="string">llama_model_loader: - type q4_0:  281 tensors</span></span><br><span class="line"><span class="string">llama_model_loader: - type q6_K:    1 tensors</span></span><br><span class="line"><span class="string">llm_load_print_meta: format         = GGUF V1 (latest)</span></span><br><span class="line"><span class="string">llm_load_print_meta: arch           = llama</span></span><br><span class="line"><span class="string">llm_load_print_meta: vocab type     = SPM</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_vocab        = 32000</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_merges       = 0</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_ctx_train    = 4096</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_ctx          = 512</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_embd         = 5120</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_head         = 40</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_head_kv      = 40</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_layer        = 40</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_rot          = 128</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_gqa          = 1</span></span><br><span class="line"><span class="string">llm_load_print_meta: f_norm_eps     = 1.0e-05</span></span><br><span class="line"><span class="string">llm_load_print_meta: f_norm_rms_eps = 1.0e-05</span></span><br><span class="line"><span class="string">llm_load_print_meta: n_ff           = 13824</span></span><br><span class="line"><span class="string">llm_load_print_meta: freq_base      = 10000.0</span></span><br><span class="line"><span class="string">llm_load_print_meta: freq_scale     = 1</span></span><br><span class="line"><span class="string">llm_load_print_meta: model type     = 13B</span></span><br><span class="line"><span class="string">llm_load_print_meta: model ftype    = mostly Q4_0</span></span><br><span class="line"><span class="string">llm_load_print_meta: model size     = 13.02 B</span></span><br><span class="line"><span class="string">llm_load_print_meta: general.name   = LLaMA v2</span></span><br><span class="line"><span class="string">llm_load_print_meta: BOS token = 1 &#x27;</span>&lt;s&gt;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">llm_load_print_meta: EOS token = 2 &#x27;</span>&lt;/s&gt;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">llm_load_print_meta: UNK token = 0 &#x27;</span>&lt;unk&gt;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">llm_load_print_meta: LF token  = 13 &#x27;</span>&lt;<span class="number">0x0A</span>&gt;<span class="string">&#x27;</span></span><br><span class="line"><span class="string">llm_load_tensors: ggml ctx size =    0.11 MB</span></span><br><span class="line"><span class="string">llm_load_tensors: mem required  = 7024.01 MB (+  400.00 MB per state)</span></span><br><span class="line"><span class="string">...................................................................................................</span></span><br><span class="line"><span class="string">llama_new_context_with_model: kv self size  =  400.00 MB</span></span><br><span class="line"><span class="string">llama_new_context_with_model: compute buffer total size =   75.41 MB</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |</span></span><br><span class="line"><span class="string">sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000</span></span><br><span class="line"><span class="string">generate: n_ctx = 512, n_batch = 512, n_predict = 400, n_keep = 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> Building a website can be done in 10 simple steps:</span></span><br><span class="line"><span class="string">Step 1: Find the right website platform.</span></span><br><span class="line"><span class="string">Step 2: Choose your domain name and hosting plan.</span></span><br><span class="line"><span class="string">Step 3: Design your website layout.</span></span><br><span class="line"><span class="string">Step 4: Write your website content and add images.</span></span><br><span class="line"><span class="string">Step 5: Install security features to protect your site from hackers or spammers</span></span><br><span class="line"><span class="string">Step 6: Test your website on multiple browsers, mobile devices, operating systems etc…</span></span><br><span class="line"><span class="string">Step 7: Test it again with people who are not related to you personally – friends or family members will work just fine!</span></span><br><span class="line"><span class="string">Step 8: Start marketing and promoting the website via social media channels or paid ads</span></span><br><span class="line"><span class="string">Step 9: Analyze how many visitors have come to your site so far, what type of people visit more often than others (e.g., men vs women) etc…</span></span><br><span class="line"><span class="string">Step 10: Continue to improve upon all aspects mentioned above by following trends in web design and staying up-to-date on new technologies that can enhance user experience even further!</span></span><br><span class="line"><span class="string">How does a Website Work?</span></span><br><span class="line"><span class="string">A website works by having pages, which are made of HTML code. This code tells your computer how to display the content on each page you visit – whether it’s an image or text file (like PDFs). In order for someone else’s browser not only be able but also want those same results when accessing any given URL; some additional steps need taken by way of programming scripts that will add functionality such as making links clickable!</span></span><br><span class="line"><span class="string">The most common type is called static HTML pages because they remain unchanged over time unless modified manually (either through editing files directly or using an interface such as WordPress). They are usually served up via HTTP protocols – this means anyone can access them without having any special privileges like being part of a group who is allowed into restricted areas online; however, there may still exist some limitations depending upon where one lives geographically speaking.</span></span><br><span class="line"><span class="string">How to</span></span><br><span class="line"><span class="string">llama_print_timings:        load time =   576.45 ms</span></span><br><span class="line"><span class="string">llama_print_timings:      sample time =   283.10 ms /   400 runs   (    0.71 ms per token,  1412.91 tokens per second)</span></span><br><span class="line"><span class="string">llama_print_timings: prompt eval time =   599.83 ms /    19 tokens (   31.57 ms per token,    31.68 tokens per second)</span></span><br><span class="line"><span class="string">llama_print_timings:        eval time = 24513.59 ms /   399 runs   (   61.44 ms per token,    16.28 tokens per second)</span></span><br><span class="line"><span class="string">llama_print_timings:       total time = 25431.49 ms</span></span><br></pre></td></tr></table></figure>

<p>And here is another demo of running both LLaMA-7B and <a target="_blank" rel="noopener" href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a> on a single M1 Pro MacBook:</p>
<p><a target="_blank" rel="noopener" href="https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4">https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4</a></p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>Here are the steps for the LLaMA-7B model.</p>
<h3 id="Get-the-Code"><a href="#Get-the-Code" class="headerlink" title="Get the Code"></a>Get the Code</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br></pre></td></tr></table></figure>

<h3 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h3><p>In order to build llama.cpp you have three different options.</p>
<ul>
<li><p>Using <code>make</code>:</p>
<ul>
<li><p>On Linux or MacOS:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure>
</li>
<li><p>On Windows:</p>
<ol>
<li>Download the latest fortran version of <a target="_blank" rel="noopener" href="https://github.com/skeeto/w64devkit/releases">w64devkit</a>.</li>
<li>Extract <code>w64devkit</code> on your pc.</li>
<li>Run <code>w64devkit.exe</code>.</li>
<li>Use the <code>cd</code> command to reach the <code>llama.cpp</code> folder.</li>
<li>From here you can run: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
</li>
<li><p>Using <code>CMake</code>:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure>
</li>
<li><p>Using <code>Zig</code> (version 0.11 or later):</p>
<p>  Building for optimization levels and CPU features can be accomplished using standard build arguments, for example AVX2, FMA, F16C,<br>  it’s also possible to cross compile for other operating systems and architectures:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build -Doptimize=ReleaseFast -Dtarget=x86_64-windows-gnu -Dcpu=x86_64+avx2+fma+f16c</span><br></pre></td></tr></table></figure>

<p>  The <code>zig targets</code> command will give you valid options to use.</p>
</li>
<li><p>Using <code>gmake</code> (FreeBSD):</p>
<ol>
<li><p>Install and activate <a target="_blank" rel="noopener" href="https://wiki.freebsd.org/Graphics">DRM in FreeBSD</a></p>
</li>
<li><p>Add your user to <strong>video</strong> group</p>
</li>
<li><p>Install compilation dependencies.</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo pkg install gmake automake autoconf pkgconf llvm15 clinfo clover \</span><br><span class="line">    opencl clblast openblas</span><br><span class="line"></span><br><span class="line">    gmake CC=/usr/local/bin/clang15 CXX=/usr/local/bin/clang++15 -j4</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>Notes:</strong> With this packages you can build llama.cpp with OPENBLAS and<br>CLBLAST support for use OpenCL GPU acceleration in FreeBSD. Please read<br>the instructions for use and activate this options in this document below.</p>
</li>
</ul>
<h3 id="Metal-Build"><a href="#Metal-Build" class="headerlink" title="Metal Build"></a>Metal Build</h3><p>Using Metal allows the computation to be executed on the GPU for Apple devices:</p>
<ul>
<li><p>Using <code>make</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LLAMA_METAL=1 make</span><br></pre></td></tr></table></figure>
</li>
<li><p>Using <code>CMake</code>:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build-metal</span><br><span class="line"><span class="built_in">cd</span> build-metal</span><br><span class="line">cmake -DLLAMA_METAL=ON ..</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure></li>
</ul>
<p>When built with Metal support, you can enable GPU inference with the <code>--gpu-layers|-ngl</code> command-line argument.<br>Any value larger than 0 will offload the computation to the GPU. For example:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./main -m ./models/7B/ggml-model-q4_0.gguf -n 128 -ngl 1</span><br></pre></td></tr></table></figure>

<h3 id="MPI-Build"><a href="#MPI-Build" class="headerlink" title="MPI Build"></a>MPI Build</h3><p>MPI lets you distribute the computation over a cluster of machines. Because of the serial nature of LLM prediction, this won’t yield any end-to-end speed-ups, but it will let you run larger models than would otherwise fit into RAM on a single machine.</p>
<p>First you will need MPI libraries installed on your system. The two most popular (only?) options are <a target="_blank" rel="noopener" href="https://www.mpich.org/">MPICH</a> and <a target="_blank" rel="noopener" href="https://www.open-mpi.org/">OpenMPI</a>. Either can be installed with a package manager (<code>apt</code>, Homebrew, MacPorts, etc).</p>
<p>Next you will need to build the project with <code>LLAMA_MPI</code> set to true on all machines; if you’re building with <code>make</code>, you will also need to specify an MPI-capable compiler (when building with CMake, this is configured automatically):</p>
<ul>
<li><p>Using <code>make</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make CC=mpicc CXX=mpicxx LLAMA_MPI=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>Using <code>CMake</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -S . -B build -DLLAMA_MPI=ON</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Once the programs are built, download&#x2F;convert the weights on all of the machines in your cluster. The paths to the weights and programs should be identical on all machines.</p>
<p>Next, ensure password-less SSH access to each machine from the primary host, and create a <code>hostfile</code> with a list of the hostnames and their relative “weights” (slots). If you want to use localhost for computation, use its local subnet IP address rather than the loopback address or “localhost”.</p>
<p>Here is an example hostfile:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.1:2</span><br><span class="line">malvolio.local:1</span><br></pre></td></tr></table></figure>

<p>The above will distribute the computation across 2 processes on the first host and 1 process on the second host. Each process will use roughly an equal amount of RAM. Try to keep these numbers small, as inter-process (intra-host) communication is expensive.</p>
<p>Finally, you’re ready to run a computation using <code>mpirun</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpirun -hostfile hostfile -n 3 ./main -m ./models/7B/ggml-model-q4_0.gguf -n 128</span><br></pre></td></tr></table></figure>

<h3 id="BLAS-Build"><a href="#BLAS-Build" class="headerlink" title="BLAS Build"></a>BLAS Build</h3><p>Building the program with BLAS support may lead to some performance improvements in prompt processing using batch sizes higher than 32 (the default is 512). BLAS doesn’t affect the normal generation performance. There are currently three different implementations of it:</p>
<ul>
<li><h4 id="Accelerate-Framework"><a href="#Accelerate-Framework" class="headerlink" title="Accelerate Framework:"></a>Accelerate Framework:</h4><p>This is only available on Mac PCs and it’s enabled by default. You can just build using the normal instructions.</p>
</li>
<li><h4 id="OpenBLAS"><a href="#OpenBLAS" class="headerlink" title="OpenBLAS:"></a>OpenBLAS:</h4><p>This provides BLAS acceleration using only the CPU. Make sure to have OpenBLAS installed on your machine.</p>
<ul>
<li><p>Using <code>make</code>:</p>
<ul>
<li><p>On Linux:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make LLAMA_OPENBLAS=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>On Windows:</p>
<ol>
<li><p>Download the latest fortran version of <a target="_blank" rel="noopener" href="https://github.com/skeeto/w64devkit/releases">w64devkit</a>.</p>
</li>
<li><p>Download the latest version of <a target="_blank" rel="noopener" href="https://github.com/xianyi/OpenBLAS/releases">OpenBLAS for Windows</a>.</p>
</li>
<li><p>Extract <code>w64devkit</code> on your pc.</p>
</li>
<li><p>From the OpenBLAS zip that you just downloaded copy <code>libopenblas.a</code>, located inside the <code>lib</code> folder, inside <code>w64devkit\x86_64-w64-mingw32\lib</code>.</p>
</li>
<li><p>From the same OpenBLAS zip copy the content of the <code>include</code> folder inside <code>w64devkit\x86_64-w64-mingw32\include</code>.</p>
</li>
<li><p>Run <code>w64devkit.exe</code>.</p>
</li>
<li><p>Use the <code>cd</code> command to reach the <code>llama.cpp</code> folder.</p>
</li>
<li><p>From here you can run:</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make LLAMA_OPENBLAS=1</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
</li>
<li><p>Using <code>CMake</code> on Linux:</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><h4 id="BLIS"><a href="#BLIS" class="headerlink" title="BLIS"></a>BLIS</h4><p>Check <a href="docs/BLIS.md">BLIS.md</a> for more information.</p>
</li>
<li><h4 id="Intel-MKL"><a href="#Intel-MKL" class="headerlink" title="Intel MKL"></a>Intel MKL</h4><p>By default, <code>LLAMA_BLAS_VENDOR</code> is set to <code>Generic</code>, so if you already sourced intel environment script and assign <code>-DLLAMA_BLAS=ON</code> in cmake, the mkl version of Blas will automatically been selected. You may also specify it by:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure>
</li>
<li><h4 id="cuBLAS"><a href="#cuBLAS" class="headerlink" title="cuBLAS"></a>cuBLAS</h4><p>This provides BLAS acceleration using the CUDA cores of your Nvidia GPU. Make sure to have the CUDA toolkit installed. You can download it from your Linux distro’s package manager or from here: <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a>.</p>
<ul>
<li><p>Using <code>make</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make LLAMA_CUBLAS=1</span><br></pre></td></tr></table></figure></li>
<li><p>Using <code>CMake</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DLLAMA_CUBLAS=ON</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure></li>
</ul>
<p>The environment variable <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars"><code>CUDA_VISIBLE_DEVICES</code></a> can be used to specify which GPU(s) will be used. The following compilation options are also available to tweak performance:</p>
</li>
</ul>
<!---
  | LLAMA_CUDA_CUBLAS       | Boolean                |   false | Use cuBLAS instead of custom CUDA kernels for prompt processing. Faster for all quantization formats except for q4_0 and q8_0, especially for k-quants. Increases VRAM usage (700 MiB for 7b, 970 MiB for 13b, 1430 MiB for 33b). |
--->
<table>
<thead>
<tr>
<th>Option</th>
<th>Legal values</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>LLAMA_CUDA_FORCE_DMMV</td>
<td>Boolean</td>
<td>false</td>
<td>Force the use of dequantization + matrix vector multiplication kernels instead of using kernels that do matrix vector multiplication on quantized data. By default the decision is made based on compute capability (MMVQ for 6.1&#x2F;Pascal&#x2F;GTX 1000 or higher). Does not affect k-quants.</td>
</tr>
<tr>
<td>LLAMA_CUDA_DMMV_X</td>
<td>Positive integer &gt;&#x3D; 32</td>
<td>32</td>
<td>Number of values in x direction processed by the CUDA dequantization + matrix vector multiplication kernel per iteration. Increasing this value can improve performance on fast GPUs. Power of 2 heavily recommended. Does not affect k-quants.</td>
</tr>
<tr>
<td>LLAMA_CUDA_MMV_Y</td>
<td>Positive integer</td>
<td>1</td>
<td>Block size in y direction for the CUDA mul mat vec kernels. Increasing this value can improve performance on fast GPUs. Power of 2 recommended.</td>
</tr>
<tr>
<td>LLAMA_CUDA_F16</td>
<td>Boolean</td>
<td>false</td>
<td>If enabled, use half-precision floating point arithmetic for the CUDA dequantization + mul mat vec kernels and for the q4_1 and q5_1 matrix matrix multiplication kernels. Can improve performance on relatively recent GPUs.</td>
</tr>
<tr>
<td>LLAMA_CUDA_KQUANTS_ITER</td>
<td>1 or 2</td>
<td>2</td>
<td>Number of values processed per iteration and per CUDA thread for Q2_K and Q6_K quantization formats. Setting this value to 1 can improve performance for slow GPUs.</td>
</tr>
</tbody></table>
<ul>
<li><h4 id="CLBlast"><a href="#CLBlast" class="headerlink" title="CLBlast"></a>CLBlast</h4><p>OpenCL acceleration is provided by the matrix multiplication kernels from the <a target="_blank" rel="noopener" href="https://github.com/CNugteren/CLBlast">CLBlast</a> project and custom kernels for ggml that can generate tokens on the GPU.</p>
<p>You will need the <a target="_blank" rel="noopener" href="https://github.com/KhronosGroup/OpenCL-SDK">OpenCL SDK</a>.</p>
<ul>
<li><p>For Ubuntu or Debian, the packages <code>opencl-headers</code>, <code>ocl-icd</code> may be needed.</p>
</li>
<li><details>
  <summary>Installing the OpenCL SDK from source</summary>

  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recurse-submodules https://github.com/KhronosGroup/OpenCL-SDK.git</span><br><span class="line"><span class="built_in">mkdir</span> OpenCL-SDK/build</span><br><span class="line"><span class="built_in">cd</span> OpenCL-SDK/build</span><br><span class="line">cmake .. -DBUILD_DOCS=OFF \</span><br><span class="line">  -DBUILD_EXAMPLES=OFF \</span><br><span class="line">  -DBUILD_TESTING=OFF \</span><br><span class="line">  -DOPENCL_SDK_BUILD_SAMPLES=OFF \</span><br><span class="line">  -DOPENCL_SDK_TEST_SAMPLES=OFF</span><br><span class="line">cmake --build . --config Release</span><br><span class="line">cmake --install . --prefix /some/path</span><br></pre></td></tr></table></figure>
</details></li>
</ul>
<p>Installing CLBlast: it may be found in your operating system’s packages.</p>
<ul>
<li><details>
<summary>If not, then installing from source:</summary>

  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/CNugteren/CLBlast.git</span><br><span class="line"><span class="built_in">mkdir</span> CLBlast/build</span><br><span class="line"><span class="built_in">cd</span> CLBlast/build</span><br><span class="line">cmake .. -DBUILD_SHARED_LIBS=OFF -DTUNERS=OFF</span><br><span class="line">cmake --build . --config Release</span><br><span class="line">cmake --install . --prefix /some/path</span><br></pre></td></tr></table></figure>

  Where <code>/some/path</code> is where the built library will be installed (default is <code>/usr/local</code>).</details></li>
</ul>
<p>Building:</p>
<ul>
<li>Build with make:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make LLAMA_CLBLAST=1</span><br></pre></td></tr></table></figure></li>
<li>CMake:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DLLAMA_CLBLAST=ON -DCLBlast_dir=/some/path</span><br><span class="line">cmake --build . --config Release</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Running:</p>
<p>The CLBlast build supports <code>--gpu-layers|-ngl</code> like the CUDA version does.</p>
<p>To select the correct platform (driver) and device (GPU), you can use the environment variables <code>GGML_OPENCL_PLATFORM</code> and <code>GGML_OPENCL_DEVICE</code>.<br>The selection can be a number (starting from 0) or a text string to search:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGML_OPENCL_PLATFORM=1 ./main ...</span><br><span class="line">GGML_OPENCL_DEVICE=2 ./main ...</span><br><span class="line">GGML_OPENCL_PLATFORM=Intel ./main ...</span><br><span class="line">GGML_OPENCL_PLATFORM=AMD GGML_OPENCL_DEVICE=1 ./main ...</span><br></pre></td></tr></table></figure>

<p>The default behavior is to find the first GPU device, but when it is an integrated GPU on a laptop, for instance, the selectors are useful.<br>Using the variables it is possible to select a CPU-based driver as well, if so desired.</p>
<p>You can get a list of platforms and devices from the <code>clinfo -l</code> command, etc.</p>
</li>
</ul>
<h3 id="Prepare-Data-Run"><a href="#Prepare-Data-Run" class="headerlink" title="Prepare Data &amp; Run"></a>Prepare Data &amp; Run</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># obtain the original LLaMA model weights and place them in ./models</span></span><br><span class="line"><span class="built_in">ls</span> ./models</span><br><span class="line">65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model</span><br><span class="line">  <span class="comment"># [Optional] for models using BPE tokenizers</span></span><br><span class="line">  <span class="built_in">ls</span> ./models</span><br><span class="line">  65B 30B 13B 7B vocab.json</span><br><span class="line"></span><br><span class="line"><span class="comment"># install Python dependencies</span></span><br><span class="line">python3 -m pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert the 7B model to ggml FP16 format</span></span><br><span class="line">python3 convert.py models/7B/</span><br><span class="line"></span><br><span class="line">  <span class="comment"># [Optional] for models using BPE tokenizers</span></span><br><span class="line">  python convert.py models/7B/ --vocabtype bpe</span><br><span class="line"></span><br><span class="line"><span class="comment"># quantize the model to 4-bits (using q4_0 method)</span></span><br><span class="line">./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0</span><br><span class="line"></span><br><span class="line"><span class="comment"># run the inference</span></span><br><span class="line">./main -m ./models/7B/ggml-model-q4_0.gguf -n 128</span><br></pre></td></tr></table></figure>

<p>When running the larger models, make sure you have enough disk space to store all the intermediate files.</p>
<h3 id="Memory-Disk-Requirements"><a href="#Memory-Disk-Requirements" class="headerlink" title="Memory&#x2F;Disk Requirements"></a>Memory&#x2F;Disk Requirements</h3><p>As the models are currently fully loaded into memory, you will need adequate disk space to save them and sufficient RAM to load them. At the moment, memory and disk requirements are the same.</p>
<table>
<thead>
<tr>
<th align="right">Model</th>
<th align="right">Original size</th>
<th align="right">Quantized size (4-bit)</th>
</tr>
</thead>
<tbody><tr>
<td align="right">7B</td>
<td align="right">13 GB</td>
<td align="right">3.9 GB</td>
</tr>
<tr>
<td align="right">13B</td>
<td align="right">24 GB</td>
<td align="right">7.8 GB</td>
</tr>
<tr>
<td align="right">30B</td>
<td align="right">60 GB</td>
<td align="right">19.5 GB</td>
</tr>
<tr>
<td align="right">65B</td>
<td align="right">120 GB</td>
<td align="right">38.5 GB</td>
</tr>
</tbody></table>
<h3 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a>Quantization</h3><p>Several quantization methods are supported. They differ in the resulting model disk size and inference speed.</p>
<p><em>(outdated)</em></p>
<table>
<thead>
<tr>
<th align="right">Model</th>
<th>Measure</th>
<th align="right">F16</th>
<th align="right">Q4_0</th>
<th align="right">Q4_1</th>
<th align="right">Q5_0</th>
<th align="right">Q5_1</th>
<th align="right">Q8_0</th>
</tr>
</thead>
<tbody><tr>
<td align="right">7B</td>
<td>perplexity</td>
<td align="right">5.9066</td>
<td align="right">6.1565</td>
<td align="right">6.0912</td>
<td align="right">5.9862</td>
<td align="right">5.9481</td>
<td align="right">5.9070</td>
</tr>
<tr>
<td align="right">7B</td>
<td>file size</td>
<td align="right">13.0G</td>
<td align="right">3.5G</td>
<td align="right">3.9G</td>
<td align="right">4.3G</td>
<td align="right">4.7G</td>
<td align="right">6.7G</td>
</tr>
<tr>
<td align="right">7B</td>
<td>ms&#x2F;tok @ 4th</td>
<td align="right">127</td>
<td align="right">55</td>
<td align="right">54</td>
<td align="right">76</td>
<td align="right">83</td>
<td align="right">72</td>
</tr>
<tr>
<td align="right">7B</td>
<td>ms&#x2F;tok @ 8th</td>
<td align="right">122</td>
<td align="right">43</td>
<td align="right">45</td>
<td align="right">52</td>
<td align="right">56</td>
<td align="right">67</td>
</tr>
<tr>
<td align="right">7B</td>
<td>bits&#x2F;weight</td>
<td align="right">16.0</td>
<td align="right">4.5</td>
<td align="right">5.0</td>
<td align="right">5.5</td>
<td align="right">6.0</td>
<td align="right">8.5</td>
</tr>
<tr>
<td align="right">13B</td>
<td>perplexity</td>
<td align="right">5.2543</td>
<td align="right">5.3860</td>
<td align="right">5.3608</td>
<td align="right">5.2856</td>
<td align="right">5.2706</td>
<td align="right">5.2548</td>
</tr>
<tr>
<td align="right">13B</td>
<td>file size</td>
<td align="right">25.0G</td>
<td align="right">6.8G</td>
<td align="right">7.6G</td>
<td align="right">8.3G</td>
<td align="right">9.1G</td>
<td align="right">13G</td>
</tr>
<tr>
<td align="right">13B</td>
<td>ms&#x2F;tok @ 4th</td>
<td align="right">-</td>
<td align="right">103</td>
<td align="right">105</td>
<td align="right">148</td>
<td align="right">160</td>
<td align="right">131</td>
</tr>
<tr>
<td align="right">13B</td>
<td>ms&#x2F;tok @ 8th</td>
<td align="right">-</td>
<td align="right">73</td>
<td align="right">82</td>
<td align="right">98</td>
<td align="right">105</td>
<td align="right">128</td>
</tr>
<tr>
<td align="right">13B</td>
<td>bits&#x2F;weight</td>
<td align="right">16.0</td>
<td align="right">4.5</td>
<td align="right">5.0</td>
<td align="right">5.5</td>
<td align="right">6.0</td>
<td align="right">8.5</td>
</tr>
</tbody></table>
<h3 id="Perplexity-measuring-model-quality"><a href="#Perplexity-measuring-model-quality" class="headerlink" title="Perplexity (measuring model quality)"></a>Perplexity (measuring model quality)</h3><p>You can use the <code>perplexity</code> example to measure perplexity over a given prompt (lower perplexity is better).<br>For more information, see <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a>.</p>
<p>The perplexity measurements in table above are done against the <code>wikitext2</code> test dataset (<a target="_blank" rel="noopener" href="https://paperswithcode.com/dataset/wikitext-2">https://paperswithcode.com/dataset/wikitext-2</a>), with context length of 512.<br>The time per token is measured on a MacBook M1 Pro 32GB RAM using 4 and 8 threads.</p>
<h3 id="Interactive-mode"><a href="#Interactive-mode" class="headerlink" title="Interactive mode"></a>Interactive mode</h3><p>If you want a more ChatGPT-like experience, you can run in interactive mode by passing <code>-i</code> as a parameter.<br>In this mode, you can always interrupt generation by pressing Ctrl+C and entering one or more lines of text, which will be converted into tokens and appended to the current context. You can also specify a <em>reverse prompt</em> with the parameter <code>-r &quot;reverse prompt string&quot;</code>. This will result in user input being prompted whenever the exact tokens of the reverse prompt string are encountered in the generation. A typical use is to use a prompt that makes LLaMa emulate a chat between multiple users, say Alice and Bob, and pass <code>-r &quot;Alice:&quot;</code>.</p>
<p>Here is an example of a few-shot interaction, invoked with the command</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default arguments using a 7B model</span></span><br><span class="line">./examples/chat.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># advanced chat with a 13B model</span></span><br><span class="line">./examples/chat-13B.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># custom arguments using a 13B model</span></span><br><span class="line">./main -m ./models/13B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r <span class="string">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure>

<p>Note the use of <code>--color</code> to distinguish between user input and generated text. Other parameters are explained in more detail in the <a href="examples/main/README.md">README</a> for the <code>main</code> example program.</p>
<p><img src="https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png" alt="image"></p>
<h3 id="Persistent-Interaction"><a href="#Persistent-Interaction" class="headerlink" title="Persistent Interaction"></a>Persistent Interaction</h3><p>The prompt, user inputs, and model generations can be saved and resumed across calls to <code>./main</code> by leveraging <code>--prompt-cache</code> and <code>--prompt-cache-all</code>. The <code>./examples/chat-persistent.sh</code> script demonstrates this with support for long-running, resumable chat sessions. To use this example, you must provide a file to cache the initial chat prompt and a directory to save the chat session, and may optionally provide the same variables as <code>chat-13B.sh</code>. The same prompt cache can be reused for new chat sessions. Note that both prompt cache and chat directory are tied to the initial prompt (<code>PROMPT_TEMPLATE</code>) and the model file.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start a new chat</span></span><br><span class="line">PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resume that chat</span></span><br><span class="line">PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start a different chat with the same prompt/model</span></span><br><span class="line">PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/another ./examples/chat-persistent.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Different prompt cache for different prompt/model</span></span><br><span class="line">PROMPT_TEMPLATE=./prompts/chat-with-bob.txt PROMPT_CACHE_FILE=bob.prompt.bin \</span><br><span class="line">    CHAT_SAVE_DIR=./chat/bob ./examples/chat-persistent.sh</span><br></pre></td></tr></table></figure>

<h3 id="Constrained-output-with-grammars"><a href="#Constrained-output-with-grammars" class="headerlink" title="Constrained output with grammars"></a>Constrained output with grammars</h3><p><code>llama.cpp</code> supports grammars to constrain model output. For example, you can force the model to output JSON only:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./main -m ./models/13B/ggml-model-q4_0.gguf -n 256 --grammar-file grammars/json.gbnf -p <span class="string">&#x27;Request: schedule a call at 8pm; Command:&#x27;</span></span><br></pre></td></tr></table></figure>

<p>The <code>grammars/</code> folder contains a handful of sample grammars. To write your own, check out the <a href="./grammars/README.md">GBNF Guide</a>.</p>
<h3 id="Instruction-mode-with-Alpaca"><a href="#Instruction-mode-with-Alpaca" class="headerlink" title="Instruction mode with Alpaca"></a>Instruction mode with Alpaca</h3><ol>
<li>First, download the <code>ggml</code> Alpaca model into the <code>./models</code> folder</li>
<li>Run the <code>main</code> tool like this:</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./examples/alpaca.sh</span><br></pre></td></tr></table></figure>

<p>Sample run:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">== Running in interactive mode. ==</span><br><span class="line"> - Press Ctrl+C to interject at any time.</span><br><span class="line"> - Press Return to return control to LLaMa.</span><br><span class="line"> - If you want to submit another line, end your input in &#x27;\&#x27;.</span><br><span class="line"></span><br><span class="line"> Below is an instruction that describes a task. Write a response that appropriately completes the request.</span><br><span class="line"></span><br><span class="line">&gt; How many letters are there in the English alphabet?</span><br><span class="line">There 26 letters in the English Alphabet</span><br><span class="line">&gt; What is the most common way of transportation in Amsterdam?</span><br><span class="line">The majority (54%) are using public transit. This includes buses, trams and metros with over 100 lines throughout the city which make it very accessible for tourists to navigate around town as well as locals who commute by tram or metro on a daily basis</span><br><span class="line">&gt; List 5 words that start with &quot;ca&quot;.</span><br><span class="line">cadaver, cauliflower, cabbage (vegetable), catalpa (tree) and Cailleach.</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Using-OpenLLaMA"><a href="#Using-OpenLLaMA" class="headerlink" title="Using OpenLLaMA"></a>Using <a target="_blank" rel="noopener" href="https://github.com/openlm-research/open_llama">OpenLLaMA</a></h3><p>OpenLLaMA is an openly licensed reproduction of Meta’s original LLaMA model. It uses the same architecture and is a drop-in replacement for the original LLaMA weights.</p>
<ul>
<li>Download the <a target="_blank" rel="noopener" href="https://huggingface.co/openlm-research/open_llama_3b">3B</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/openlm-research/open_llama_7b">7B</a>, or <a target="_blank" rel="noopener" href="https://huggingface.co/openlm-research/open_llama_13b">13B</a> model from Hugging Face.</li>
<li>Convert the model to ggml FP16 format using <code>python convert.py &lt;path to OpenLLaMA directory&gt;</code></li>
</ul>
<h3 id="Using-GPT4All"><a href="#Using-GPT4All" class="headerlink" title="Using GPT4All"></a>Using <a target="_blank" rel="noopener" href="https://github.com/nomic-ai/gpt4all">GPT4All</a></h3><p><em>Note: these instructions are likely obsoleted by the GGUF update</em></p>
<ul>
<li>Obtain the <code>tokenizer.model</code> file from LLaMA model and put it to <code>models</code></li>
<li>Obtain the <code>added_tokens.json</code> file from Alpaca model and put it to <code>models</code></li>
<li>Obtain the <code>gpt4all-lora-quantized.bin</code> file from GPT4All model and put it to <code>models/gpt4all-7B</code></li>
<li>It is distributed in the old <code>ggml</code> format which is now obsoleted</li>
<li>You have to convert it to the new format using <code>convert.py</code>:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 convert.py models/gpt4all-7B/gpt4all-lora-quantized.bin</span><br></pre></td></tr></table></figure>

<ul>
<li><p>You can now use the newly generated <code>models/gpt4all-7B/ggml-model-q4_0.bin</code> model in exactly the same way as all other models</p>
</li>
<li><p>The newer GPT4All-J model is not yet supported!</p>
</li>
</ul>
<h3 id="Using-Pygmalion-7B-Metharme-7B"><a href="#Using-Pygmalion-7B-Metharme-7B" class="headerlink" title="Using Pygmalion 7B &amp; Metharme 7B"></a>Using Pygmalion 7B &amp; Metharme 7B</h3><ul>
<li>Obtain the <a href="#obtaining-the-facebook-llama-original-model-and-stanford-alpaca-model-data">LLaMA weights</a></li>
<li>Obtain the <a target="_blank" rel="noopener" href="https://huggingface.co/PygmalionAI/pygmalion-7b/">Pygmalion 7B</a> or <a target="_blank" rel="noopener" href="https://huggingface.co/PygmalionAI/metharme-7b">Metharme 7B</a> XOR encoded weights</li>
<li>Convert the LLaMA model with <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">the latest HF convert script</a></li>
<li>Merge the XOR files with the converted LLaMA weights by running the <a target="_blank" rel="noopener" href="https://huggingface.co/PygmalionAI/pygmalion-7b/blob/main/xor_codec.py">xor_codec</a> script</li>
<li>Convert to <code>ggml</code> format using the <code>convert.py</code> script in this repo:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 convert.py pygmalion-7b/ --outtype q4_1</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The Pygmalion 7B &amp; Metharme 7B weights are saved in <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> precision. If you wish to convert to <code>ggml</code> without quantizating, please specify the <code>--outtype</code> as <code>f32</code> instead of <code>f16</code>.</p>
</blockquote>
</li>
</ul>
<h3 id="Obtaining-the-Facebook-LLaMA-original-model-and-Stanford-Alpaca-model-data"><a href="#Obtaining-the-Facebook-LLaMA-original-model-and-Stanford-Alpaca-model-data" class="headerlink" title="Obtaining the Facebook LLaMA original model and Stanford Alpaca model data"></a>Obtaining the Facebook LLaMA original model and Stanford Alpaca model data</h3><ul>
<li><strong>Under no circumstances should IPFS, magnet links, or any other links to model downloads be shared anywhere in this repository, including in issues, discussions, or pull requests. They will be immediately deleted.</strong></li>
<li>The LLaMA models are officially distributed by Facebook and will <strong>never</strong> be provided through this repository.</li>
<li>Refer to <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/pull/73/files">Facebook’s LLaMA repository</a> if you need to request access to the model data.</li>
</ul>
<h3 id="Obtaining-and-using-the-Facebook-LLaMA-2-model"><a href="#Obtaining-and-using-the-Facebook-LLaMA-2-model" class="headerlink" title="Obtaining and using the Facebook LLaMA 2 model"></a>Obtaining and using the Facebook LLaMA 2 model</h3><ul>
<li>Refer to <a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">Facebook’s LLaMA download page</a> if you want to access the model data.</li>
<li>Alternatively, if you want to save time and space, you can download already converted and quantized models from <a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke">TheBloke</a>, including:<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-7B-GGML">LLaMA 2 7B base</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-13B-GGML">LLaMA 2 13B base</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-70B-GGML">LLaMA 2 70B base</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-7B-chat-GGML">LLaMA 2 7B chat</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML">LLaMA 2 13B chat</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TheBloke/Llama-2-70B-chat-GGML">LLaMA 2 70B chat</a></li>
</ul>
</li>
<li>Specify <code>-eps 1e-5</code> for best generation quality</li>
<li>Specify <code>-gqa 8</code> for 70B models to work</li>
</ul>
<h3 id="Verifying-the-model-files"><a href="#Verifying-the-model-files" class="headerlink" title="Verifying the model files"></a>Verifying the model files</h3><p>Please verify the <a href="SHA256SUMS">sha256 checksums</a> of all downloaded model files to confirm that you have the correct model data files before creating an issue relating to your model files.</p>
<ul>
<li>The following python script will verify if you have all possible latest files in your self-installed <code>./models</code> subdirectory:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run the verification script</span></span><br><span class="line">./scripts/verify-checksum-models.py</span><br></pre></td></tr></table></figure>

<ul>
<li>On linux or macOS it is also possible to run the following commands to verify if you have all possible latest files in your self-installed <code>./models</code> subdirectory:<ul>
<li>On Linux: <code>sha256sum --ignore-missing -c SHA256SUMS</code></li>
<li>on macOS: <code>shasum -a 256 --ignore-missing -c SHA256SUMS</code></li>
</ul>
</li>
</ul>
<h3 id="Seminal-papers-and-background-on-the-models"><a href="#Seminal-papers-and-background-on-the-models" class="headerlink" title="Seminal papers and background on the models"></a>Seminal papers and background on the models</h3><p>If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:</p>
<ul>
<li>LLaMA:<ul>
<li><a target="_blank" rel="noopener" href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Introducing LLaMA: A foundational, 65-billion-parameter large language model</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
</ul>
</li>
<li>GPT-3<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
</ul>
</li>
<li>GPT-3.5 &#x2F; InstructGPT &#x2F; ChatGPT:<ul>
<li><a target="_blank" rel="noopener" href="https://openai.com/research/instruction-following">Aligning language models to follow instructions</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
</ul>
</li>
</ul>
<h4 id="How-to-run"><a href="#How-to-run" class="headerlink" title="How to run"></a>How to run</h4><ol>
<li>Download&#x2F;extract: <a target="_blank" rel="noopener" href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research">https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research</a></li>
<li>Run <code>./perplexity -m models/7B/ggml-model-q4_0.gguf -f wiki.test.raw</code></li>
<li>Output:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity : calculating perplexity over 655 chunks</span><br><span class="line">24.43 seconds per pass - ETA 4.45 hours</span><br><span class="line">[1]4.5970,[2]5.1807,[3]6.0382,...</span><br></pre></td></tr></table></figure>
And after 4.45 hours, you will have the final perplexity.</li>
</ol>
<h3 id="Android"><a href="#Android" class="headerlink" title="Android"></a>Android</h3><h4 id="Building-the-Project-using-Android-NDK"><a href="#Building-the-Project-using-Android-NDK" class="headerlink" title="Building the Project using Android NDK"></a>Building the Project using Android NDK</h4><p>You can easily run <code>llama.cpp</code> on Android device with <a target="_blank" rel="noopener" href="https://termux.dev/">termux</a>.</p>
<p>First, install the essential packages for termux:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pkg install clang wget git cmake</span><br></pre></td></tr></table></figure>
<p>Second, obtain the <a target="_blank" rel="noopener" href="https://developer.android.com/ndk">Android NDK</a> and then build with CMake:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build-android</span><br><span class="line">$ cd build-android</span><br><span class="line">$ export NDK=&lt;your_ndk_directory&gt;</span><br><span class="line">$ cmake -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_C_FLAGS=-march=armv8.4a+dotprod ..</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure>
<p>Install <a target="_blank" rel="noopener" href="https://termux.dev/">termux</a> on your device and run <code>termux-setup-storage</code> to get access to your SD card.<br>Finally, copy the <code>llama</code> binary and the model files to your device storage. Here is a demo of an interactive session running on Pixel 5 phone:</p>
<p><a target="_blank" rel="noopener" href="https://user-images.githubusercontent.com/271616/225014776-1d567049-ad71-4ef2-b050-55b0b3b9274c.mp4">https://user-images.githubusercontent.com/271616/225014776-1d567049-ad71-4ef2-b050-55b0b3b9274c.mp4</a></p>
<h4 id="Building-the-Project-using-Termux-F-Droid"><a href="#Building-the-Project-using-Termux-F-Droid" class="headerlink" title="Building the Project using Termux (F-Droid)"></a>Building the Project using Termux (F-Droid)</h4><p>Termux from F-Droid offers an alternative route to execute the project on an Android device. This method empowers you to construct the project right from within the terminal, negating the requirement for a rooted device or SD Card.</p>
<p>Outlined below are the directives for installing the project using OpenBLAS and CLBlast. This combination is specifically designed to deliver peak performance on recent devices that feature a GPU.</p>
<p>If you opt to utilize OpenBLAS, you’ll need to install the corresponding package.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install libopenblas</span><br></pre></td></tr></table></figure>

<p>Subsequently, if you decide to incorporate CLBlast, you’ll first need to install the requisite OpenCL packages:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install ocl-icd opencl-headers opencl-clhpp clinfo</span><br></pre></td></tr></table></figure>

<p>In order to compile CLBlast, you’ll need to first clone the respective Git repository, which can be found at this URL: <a target="_blank" rel="noopener" href="https://github.com/CNugteren/CLBlast">https://github.com/CNugteren/CLBlast</a>. Alongside this, clone this repository into your home directory. Once this is done, navigate to the CLBlast folder and execute the commands detailed below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">cp libclblast.so* $PREFIX/lib</span><br><span class="line">cp ./include/clblast.h ../llama.cpp</span><br></pre></td></tr></table></figure>

<p>Following the previous steps, navigate to the LlamaCpp directory. To compile it with OpenBLAS and CLBlast, execute the command provided below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /data/data/com.termux/files/usr/include/openblas/cblas.h .</span><br><span class="line">cp /data/data/com.termux/files/usr/include/openblas/openblas_config.h .</span><br><span class="line">make LLAMA_CLBLAST=1 //(sometimes you need to run this command twice)</span><br></pre></td></tr></table></figure>

<p>Upon completion of the aforementioned steps, you will have successfully compiled the project. To run it using CLBlast, a slight adjustment is required: a command must be issued to direct the operations towards your device’s physical GPU, rather than the virtual one. The necessary command is detailed below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GGML_OPENCL_PLATFORM=0</span><br><span class="line">GGML_OPENCL_DEVICE=0</span><br><span class="line">export LD_LIBRARY_PATH=/vendor/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<p>(Note: some Android devices, like the Zenfone 8, need the following command instead - “export LD_LIBRARY_PATH&#x3D;&#x2F;system&#x2F;vendor&#x2F;lib64:$LD_LIBRARY_PATH”. Source: <a target="_blank" rel="noopener" href="https://www.reddit.com/r/termux/comments/kc3ynp/opencl_working_in_termux_more_in_comments/">https://www.reddit.com/r/termux/comments/kc3ynp/opencl_working_in_termux_more_in_comments/</a> )</p>
<p>For easy and swift re-execution, consider documenting this final part in a .sh script file. This will enable you to rerun the process with minimal hassle.</p>
<p>Place your desired model into the <code>~/llama.cpp/models/</code> directory and execute the <code>./main (...)</code> script.</p>
<h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><h4 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h4><ul>
<li>Docker must be installed and running on your system.</li>
<li>Create a folder to store big models &amp; intermediate files (ex. &#x2F;llama&#x2F;models)</li>
</ul>
<h4 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h4><p>We have two Docker images available for this project:</p>
<ol>
<li><code>ghcr.io/ggerganov/llama.cpp:full</code>: This image includes both the main executable file and the tools to convert LLaMA models into ggml and convert into 4-bit quantization.</li>
<li><code>ghcr.io/ggerganov/llama.cpp:light</code>: This image only includes the main executable file.</li>
</ol>
<h4 id="Usage-1"><a href="#Usage-1" class="headerlink" title="Usage"></a>Usage</h4><p>The easiest way to download the models, convert them to ggml and optimize them is with the –all-in-one command which includes the full docker image.</p>
<p>Replace <code>/path/to/models</code> below with the actual path where you downloaded the models.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one <span class="string">&quot;/models/&quot;</span> 7B</span><br></pre></td></tr></table></figure>

<p>On completion, you are ready to play!</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:full --run -m /models/7B/ggml-model-q4_0.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:&quot;</span> -n 512</span><br></pre></td></tr></table></figure>

<p>or with a light image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v /path/to/models:/models ghcr.io/ggerganov/llama.cpp:light -m /models/7B/ggml-model-q4_0.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:&quot;</span> -n 512</span><br></pre></td></tr></table></figure>

<h3 id="Docker-With-CUDA"><a href="#Docker-With-CUDA" class="headerlink" title="Docker With CUDA"></a>Docker With CUDA</h3><p>Assuming one has the <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a> properly installed on Linux, or is using a GPU enabled cloud, <code>cuBLAS</code> should be accessible inside the container.</p>
<h4 id="Building-Locally"><a href="#Building-Locally" class="headerlink" title="Building Locally"></a>Building Locally</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t <span class="built_in">local</span>/llama.cpp:full-cuda -f .devops/full-cuda.Dockerfile .</span><br><span class="line">docker build -t <span class="built_in">local</span>/llama.cpp:light-cuda -f .devops/main-cuda.Dockerfile .</span><br></pre></td></tr></table></figure>

<p>You may want to pass in some different <code>ARGS</code>, depending on the CUDA environment supported by your container host, as well as the GPU architecture.</p>
<p>The defaults are:</p>
<ul>
<li><code>CUDA_VERSION</code> set to <code>11.7.1</code></li>
<li><code>CUDA_DOCKER_ARCH</code> set to <code>all</code></li>
</ul>
<p>The resulting images, are essentially the same as the non-CUDA images:</p>
<ol>
<li><code>local/llama.cpp:full-cuda</code>: This image includes both the main executable file and the tools to convert LLaMA models into ggml and convert into 4-bit quantization.</li>
<li><code>local/llama.cpp:light-cuda</code>: This image only includes the main executable file.</li>
</ol>
<h4 id="Usage-2"><a href="#Usage-2" class="headerlink" title="Usage"></a>Usage</h4><p>After building locally, Usage is similar to the non-CUDA examples, but you’ll need to add the <code>--gpus</code> flag. You will also want to use the <code>--n-gpu-layers</code> flag.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --gpus all -v /path/to/models:/models <span class="built_in">local</span>/llama.cpp:full-cuda --run -m /models/7B/ggml-model-q4_0.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:&quot;</span> -n 512 --n-gpu-layers 1</span><br><span class="line">docker run --gpus all -v /path/to/models:/models <span class="built_in">local</span>/llama.cpp:light-cuda -m /models/7B/ggml-model-q4_0.gguf -p <span class="string">&quot;Building a website can be done in 10 simple steps:&quot;</span> -n 512 --n-gpu-layers 1</span><br></pre></td></tr></table></figure>

<h3 id="Contributing"><a href="#Contributing" class="headerlink" title="Contributing"></a>Contributing</h3><ul>
<li>Contributors can open PRs</li>
<li>Collaborators can push to branches in the <code>llama.cpp</code> repo and merge PRs into the <code>master</code> branch</li>
<li>Collaborators will be invited based on contributions</li>
<li>Any help with managing issues and PRs is very appreciated!</li>
<li>Make sure to read this: <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/discussions/205">Inference at the edge</a></li>
<li>A bit of backstory for those who are interested: <a target="_blank" rel="noopener" href="https://changelog.com/podcast/532">Changelog podcast</a></li>
</ul>
<h3 id="Coding-guidelines"><a href="#Coding-guidelines" class="headerlink" title="Coding guidelines"></a>Coding guidelines</h3><ul>
<li>Avoid adding third-party dependencies, extra files, extra headers, etc.</li>
<li>Always consider cross-compatibility with other operating systems and architectures</li>
<li>Avoid fancy looking modern STL constructs, use basic <code>for</code> loops, avoid templates, keep it simple</li>
<li>There are no strict rules for the code style, but try to follow the patterns in the code (indentation, spaces, etc.). Vertical alignment makes things more readable and easier to batch edit</li>
<li>Clean-up any trailing whitespaces, use 4 spaces for indentation, brackets on the same line, <code>void * ptr</code>, <code>int &amp; a</code></li>
<li>See <a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/issues?q=is:issue+is:open+label:%22good+first+issue%22">good first issues</a> for tasks suitable for first contributions</li>
</ul>
<h3 id="Docs"><a href="#Docs" class="headerlink" title="Docs"></a>Docs</h3><ul>
<li><a href="./examples/main/README.md">main</a></li>
<li><a href="./examples/server/README.md">server</a></li>
<li><a href="./examples/embd-input/README.md">embd-input</a></li>
<li><a href="./examples/jeopardy/README.md">jeopardy</a></li>
<li><a href="./docs/BLIS.md">BLIS</a></li>
<li><a href="./docs/token_generation_performance_tips.md">Performance troubleshooting</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&-Tricks">GGML tips &amp; tricks</a></li>
<li><a href="./grammars/README.md">GBNF grammars</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/1970/01/01/hello-world2/" data-id="cllt61jnw00025m88gqppedb0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/1970/01/01/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
