
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_09_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/13/cs.SD_2023_09_13/" class="article-date">
  <time datetime="2023-09-13T15:00:00.000Z" itemprop="datePublished">2023-09-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/13/cs.SD_2023_09_13/">cs.SD - 2023-09-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Whisper-perform-speech-based-in-context-learning"><a href="#Can-Whisper-perform-speech-based-in-context-learning" class="headerlink" title="Can Whisper perform speech-based in-context learning"></a>Can Whisper perform speech-based in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07081">http://arxiv.org/abs/2309.07081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</li>
<li>for: 这种研究旨在探讨OpenAI发布的Whisper自动语音识别（ASR）模型在语音识别任务中的上下文学习能力。</li>
<li>methods: 提出了一种基于语音的上下文学习（SICL）方法，可以在测试时进行适应，并且可以降低单个批处理语音样本的词错率（WER），无需梯度下降。</li>
<li>results: 对中文方言进行语言级适应实验，使用Whisper模型的任何大小在两种方言上实现了Consistent和较大的相对WER减少率，平均减少32.3%。此外，基于k-最近邻的受过过滤的上下文示例选择技术可以进一步提高SICL的效率，可以增加平均相对WER减少率至36.4%。<details>
<summary>Abstract</summary>
This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to shed light on SICL's adaptability to phonological variances and dialect-specific lexical nuances.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Flexible-Online-Framework-for-Projection-Based-STFT-Phase-Retrieval"><a href="#A-Flexible-Online-Framework-for-Projection-Based-STFT-Phase-Retrieval" class="headerlink" title="A Flexible Online Framework for Projection-Based STFT Phase Retrieval"></a>A Flexible Online Framework for Projection-Based STFT Phase Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07043">http://arxiv.org/abs/2309.07043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Peer, Simon Welker, Johannes Kolhoff, Timo Gerkmann</li>
<li>for: 提高iterative STFT阶段的phaserecovery性能，尤其是在speech communication应用中。</li>
<li>methods: 使用相同的投影运算符，但 combinates them in innovative ways，以提高重建质量和需要的迭代次数，同时保持相同的计算复杂性。</li>
<li>results: 在speech signals上evaluation results show that these algorithms can achieve a considerable performance gain compared to RTISI。<details>
<summary>Abstract</summary>
Several recent contributions in the field of iterative STFT phase retrieval have demonstrated that the performance of the classical Griffin-Lim method can be considerably improved upon. By using the same projection operators as Griffin-Lim, but combining them in innovative ways, these approaches achieve better results in terms of both reconstruction quality and required number of iterations, while retaining a similar computational complexity per iteration. However, like Griffin-Lim, these algorithms operate in an offline manner and thus require an entire spectrogram as input, which is an unrealistic requirement for many real-world speech communication applications. We propose to extend RTISI -- an existing online (frame-by-frame) variant of the Griffin-Lim algorithm -- into a flexible framework that enables straightforward online implementation of any algorithm based on iterative projections. We further employ this framework to implement online variants of the fast Griffin-Lim algorithm, the accelerated Griffin-Lim algorithm, and two algorithms from the optics domain. Evaluation results on speech signals show that, similarly to the offline case, these algorithms can achieve a considerable performance gain compared to RTISI.
</details>
<details>
<summary>摘要</summary>
Recent contributions to iterative STFT phase retrieval have shown that the classical Griffin-Lim method can be significantly improved upon. These approaches use the same projection operators as Griffin-Lim, but combine them in new ways to achieve better reconstruction quality and faster convergence, while maintaining similar computational complexity per iteration. However, like Griffin-Lim, these algorithms operate in an offline manner and require the entire spectrogram as input, which is unrealistic for many real-world speech communication applications. We propose to extend RTISI, an existing online (frame-by-frame) variant of the Griffin-Lim algorithm, into a flexible framework that enables straightforward online implementation of any algorithm based on iterative projections. We further employ this framework to implement online variants of the fast Griffin-Lim algorithm, the accelerated Griffin-Lim algorithm, and two algorithms from the optics domain. Evaluation results on speech signals show that these algorithms can achieve a considerable performance gain compared to RTISI.
</details></li>
</ul>
<hr>
<h2 id="Reorganization-of-the-auditory-perceptual-space-across-the-human-vocal-range"><a href="#Reorganization-of-the-auditory-perceptual-space-across-the-human-vocal-range" class="headerlink" title="Reorganization of the auditory-perceptual space across the human vocal range"></a>Reorganization of the auditory-perceptual space across the human vocal range</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06946">http://arxiv.org/abs/2309.06946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Friedrichs, Volker Dellwo</li>
<li>for: 本研究探讨了人类语音范围内（220-1046 Hz）的听觉空间，使用多维度排序分析法分析了从 Friedrichs et al. (2017) 所study的cochlea扩展的spectrum。</li>
<li>methods: 研究使用了250毫秒的vowel段和3名德国女性speaker的数据集，包括&#x2F;i y e {\o}  {\epsilon} a o u&#x2F;（共240个音）。</li>
<li>results: 研究发现，在一个closed-set认知任务中，listeners recognition的&#x2F;i a u&#x2F; 在基本频率（fo）附近1kHz的情况下明显高于其他元音的认知，而其他元音在更高的频率下的认知则降低。此外，研究还发现了元音高低的系统性 spectral shift，特别是在523Hz以上的情况下，这些观察结果强调了元音的spectral shape在听觉中的重要性，同时也提供了语言演化中元音的可能影响。<details>
<summary>Abstract</summary>
We analyzed the auditory-perceptual space across a substantial portion of the human vocal range (220-1046 Hz) using multidimensional scaling analysis of cochlea-scaled spectra from 250-ms vowel segments, initially studied in Friedrichs et al. (2017) J. Acoust. Soc. Am. 142 1025-1033. The dataset comprised the vowels /i y e {\o} {\epsilon} a o u/ (N=240) produced by three native German female speakers, encompassing a broad range of their respective voice frequency ranges. The initial study demonstrated that, during a closed-set identification task involving 21 listeners, the point vowels /i a u/ were significantly recognized at fundamental frequencies (fo) nearing 1 kHz, whereas the recognition of other vowels decreased at higher pitches. Building on these findings, our study revealed systematic spectral shifts associated with vowel height and frontness as fo increased, with a notable clustering around /i a u/ above 523 Hz. These observations underscore the pivotal role of spectral shape in vowel perception, illustrating the reliance on acoustic anchors at higher pitches. Furthermore, this study sheds light on the quantal nature of these vowels and their potential impact on language evolution, offering a plausible explanation for their widespread presence in the world's languages.
</details>
<details>
<summary>摘要</summary>
我们通过多维度规划分析了人声 vocal 范围（220-1046 Hz）中的听觉空间，使用 Friedrichs et al. (2017) J. Acoust. Soc. Am. 142 1025-1033 中提供的cochlea扩大的spectra，分析了250毫秒的vowel段。数据集包括了3名德国女性 speaker 所 произноси的4个元音 /i y e ø æ/（共240个），涵盖了它们的声音频谱范围。初始研究显示，在一个关闭的设置标识任务中，listeners 中的21名listeners 能够准确地识别基本频率（fo）接近1kHz的点元音 /i a u/，而其他元音的识别则随着高频的减少。在这些发现基础上，我们的研究发现，随着高频增加，元音的高度和前端性呈现出系统性的spectral shift，特别是在523Hz以上的位置。这些观察结果提醒我们，听觉元音的形态在高频下具有重要作用，而且listeners 会利用高频的acoustic anchors来识别元音。此外，这种研究还暴露了元音的量化性和其可能对语言演化的影响，提供了可能解释语言中的广泛存在的可能性。
</details></li>
</ul>
<hr>
<h2 id="VRDMG-Vocal-Restoration-via-Diffusion-Posterior-Sampling-with-Multiple-Guidance"><a href="#VRDMG-Vocal-Restoration-via-Diffusion-Posterior-Sampling-with-Multiple-Guidance" class="headerlink" title="VRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple Guidance"></a>VRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06934">http://arxiv.org/abs/2309.06934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Hernandez-Olivan, Koichi Saito, Naoki Murata, Chieh-Hsin Lai, Marco A. Martínez-Ramirez, Wei-Hsiang Liao, Yuki Mitsufuji</li>
<li>for: 提高音频质量，进行下游音乐修饰</li>
<li>methods:  diffusion posterior sampling (DPS) 以及其他多种推导方法，如 RePaint (RP) 策略和 Pseudoinverse-Guided Diffusion Models ($\Pi$GDM)</li>
<li>results: 在各种损害和切割频率下，对 vocals 修复和宽频扩展任务中，我们的方法表现出色，超过了目前的 DPS-based 音频修复标准。可参考 \url{<a target="_blank" rel="noopener" href="http://carlosholivan.github.io/demos/audio-restoration-2023.html%7D">http://carlosholivan.github.io/demos/audio-restoration-2023.html}</a> 获取修复后的音频示例。<details>
<summary>Abstract</summary>
Restoring degraded music signals is essential to enhance audio quality for downstream music manipulation. Recent diffusion-based music restoration methods have demonstrated impressive performance, and among them, diffusion posterior sampling (DPS) stands out given its intrinsic properties, making it versatile across various restoration tasks. In this paper, we identify that there are potential issues which will degrade current DPS-based methods' performance and introduce the way to mitigate the issues inspired by diverse diffusion guidance techniques including the RePaint (RP) strategy and the Pseudoinverse-Guided Diffusion Models ($\Pi$GDM). We demonstrate our methods for the vocal declipping and bandwidth extension tasks under various levels of distortion and cutoff frequency, respectively. In both tasks, our methods outperform the current DPS-based music restoration benchmarks. We refer to \url{http://carlosholivan.github.io/demos/audio-restoration-2023.html} for examples of the restored audio samples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将降低音质的音乐信号恢复为重要的音乐处理前期任务。最近的扩散基于音乐恢复方法有出色表现，其中扩散 posterior 抽样（DPS）在多种恢复任务中表现出色，因此在这篇论文中，我们认为DPS有潜在的问题，这些问题会降低当前DPS基于的音乐恢复方法的性能。我们介绍了一种通过多种扩散指导技术，包括RePaint（RP）策略和 Pseudoinverse-Guided Diffusion Models（$\Pi$GDM）来 Mitigate these issues。我们在 vocals 剪辑和带宽扩展任务中应用了我们的方法，并在不同的扭曲和阈值频率下进行了评估。在两个任务中，我们的方法超过了当前DPS基于的音乐恢复标准准。更多细节请参考 \url{http://carlosholivan.github.io/demos/audio-restoration-2023.html} 获取 restore 后的音频样本。
</details></li>
</ul>
<hr>
<h2 id="EMALG-An-Enhanced-Mandarin-Lombard-Grid-Corpus-with-Meaningful-Sentences"><a href="#EMALG-An-Enhanced-Mandarin-Lombard-Grid-Corpus-with-Meaningful-Sentences" class="headerlink" title="EMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful Sentences"></a>EMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful Sentences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06858">http://arxiv.org/abs/2309.06858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baifeng Li, Qingmu Liu, Yuhong Yang, Hongyang Chen, Weiping Tu, Song Lin</li>
<li>for: This study investigates the Lombard effect in Mandarin Chinese, specifically the impact of meaningful sentences on the Lombard effect and the differences between male and female speakers.</li>
<li>methods: The study uses an enhanced Mandarin Lombard grid (EMALG) corpus with 34 speakers and improved recording setups to address challenges faced by the previous Mandarin Lombard grid (MALG) corpus.</li>
<li>results: The findings show that female speakers exhibit a more pronounced Lombard effect than male speakers, particularly when uttering meaningful sentences. Additionally, the study found that nonsense sentences negatively impact Lombard effect analysis, and the results are consistent with previous research comparing the Lombard effect in English and Mandarin.Here are the three pieces of information in Simplified Chinese text:</li>
<li>for: 这个研究研究了托尔曼效应在普通话中，具体来说是研究意义语句对托尔曼效应的影响以及男女 speaker之间的差异。</li>
<li>methods: 这个研究使用了提高后的普通话托尔曼格子(EMALG) corpus，有34名 speaker，并使用改进的录音设置来解决前一个普通话托尔曼格子(MALG) corpus 面临的挑战。</li>
<li>results: 研究发现，女性 speaker在意义语句中更为明显地表现出托尔曼效应，而男性 speaker则较为弱。此外，研究还发现，无意义语句对托尔曼效应的分析有负面影响。此外，研究结果与之前英语和普通话之间的托尔曼效应比较的研究结果一致。<details>
<summary>Abstract</summary>
This study investigates the Lombard effect, where individuals adapt their speech in noisy environments. We introduce an enhanced Mandarin Lombard grid (EMALG) corpus with meaningful sentences , enhancing the Mandarin Lombard grid (MALG) corpus. EMALG features 34 speakers and improves recording setups, addressing challenges faced by MALG with nonsense sentences. Our findings reveal that in Mandarin, female exhibit a more pronounced Lombard effect than male, particularly when uttering meaningful sentences. Additionally, we uncover that nonsense sentences negatively impact Lombard effect analysis. Moreover, our results reaffirm the consistency in the Lombard effect comparison between English and Mandarin found in previous research.
</details>
<details>
<summary>摘要</summary>
这个研究调查了洛伯达效应，即在噪音环境中人们的语言适应。我们介绍了改进过的满语 Lombard 格式（EMALG）词库，包含有意义的句子，从而提高了满语 Lombard 格式（MALG）词库。EMALG 包含34名说话者，并改进了录音设备，解决了 MAGL 面临的无意义句子问题。我们的发现显示，在满语中，女性更加明显地表现出洛伯达效应，特别是当发表有意义的句子时。此外，我们还发现，无意义句子对洛伯达效应分析产生负面影响。此外，我们的结果证明了之前研究发现的洛伯达效应在英语和满语之间的一致性。
</details></li>
</ul>
<hr>
<h2 id="DCTTS-Discrete-Diffusion-Model-with-Contrastive-Learning-for-Text-to-speech-Generation"><a href="#DCTTS-Discrete-Diffusion-Model-with-Contrastive-Learning-for-Text-to-speech-Generation" class="headerlink" title="DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation"></a>DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06787">http://arxiv.org/abs/2309.06787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Wu, Qiulin Li, Sixing Liu, Qun Yang</li>
<li>for: 本文提出了一种基于抽象空间的粒子扩散模型 для文本译语 synthesis，以提高扩散模型的计算成本和批处速度。</li>
<li>methods: 本文使用了一种基于抽象空间的粒子扩散模型，并采用了对比学习方法来增强语音和文本之间的对应关系，从而提高扩散速度和质量。此外，文本编码器也被使用以简化模型参数和提高计算效率。</li>
<li>results: 实验结果显示，提出的方法可以实现出色的语音生成质量和批处速度，同时减少了扩散模型的计算成本和资源消耗。实验结果可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/lawtherWu/DCTTS%E3%80%82">https://github.com/lawtherWu/DCTTS。</a><details>
<summary>Abstract</summary>
In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.
</details>
<details>
<summary>摘要</summary>
在文本到语音（TTS）任务中，抽象扩散模型具有优秀的准确性和通用性，但它的资源消耗和推理速度总是一个挑战。这篇论文提出了粒子扩散模型与对比学习 для文本到语音生成（DCTTS）。这个论文的贡献包括：1. 基于粒子空间的TTS扩散模型可以显著降低扩散模型的计算摄用量和提高抽取速度;2. 基于粒子空间的对比学习方法可以增强语音和文本之间的对应关系，提高抽取质量;3. 它使用高效的文本编码器，简化模型的参数和提高计算效率。实验结果表明，提出的方法可以在保持高质量语音生成的同时，显著降低扩散模型的资源消耗。生成的示例可以在https://github.com/lawtherWu/DCTTS中找到。
</details></li>
</ul>
<hr>
<h2 id="Distinguishing-Neural-Speech-Synthesis-Models-Through-Fingerprints-in-Speech-Waveforms"><a href="#Distinguishing-Neural-Speech-Synthesis-Models-Through-Fingerprints-in-Speech-Waveforms" class="headerlink" title="Distinguishing Neural Speech Synthesis Models Through Fingerprints in Speech Waveforms"></a>Distinguishing Neural Speech Synthesis Models Through Fingerprints in Speech Waveforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06780">http://arxiv.org/abs/2309.06780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chu Yuan Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Xinrui Yan</li>
<li>for: 本研究旨在提高声音合成技术中的识别源能力，以应对不良用途的威胁和违反。</li>
<li>methods: 本研究使用多种说话者的 LibriTTS 数据集，研究生成的声音波形中的模型印记的存在和对整体声音波形的影响。</li>
<li>results: 研究发现，模型和 vocoder 都会印记生成的声音波形，但是 vocoder 的印记更为主要，可能会隐藏 acoustic model 的印记。这些发现表明模型印记在声音合成中具有广泛的应用前景。<details>
<summary>Abstract</summary>
Recent strides in neural speech synthesis technologies, while enjoying widespread applications, have nonetheless introduced a series of challenges, spurring interest in the defence against the threat of misuse and abuse. Notably, source attribution of synthesized speech has value in forensics and intellectual property protection, but prior work in this area has certain limitations in scope. To address the gaps, we present our findings concerning the identification of the sources of synthesized speech in this paper. We investigate the existence of speech synthesis model fingerprints in the generated speech waveforms, with a focus on the acoustic model and the vocoder, and study the influence of each component on the fingerprint in the overall speech waveforms. Our research, conducted using the multi-speaker LibriTTS dataset, demonstrates two key insights: (1) vocoders and acoustic models impart distinct, model-specific fingerprints on the waveforms they generate, and (2) vocoder fingerprints are the more dominant of the two, and may mask the fingerprints from the acoustic model. These findings strongly suggest the existence of model-specific fingerprints for both the acoustic model and the vocoder, highlighting their potential utility in source identification applications.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the identification of the sources of synthesized speech, with a particular focus on the existence of speech synthesis model fingerprints in the generated speech waveforms. We investigate the contributions of both the acoustic model and the vocoder to the fingerprints, and examine how each component influences the overall waveform.Our research uses the multi-speaker LibriTTS dataset, and we find two key insights. First, both the acoustic model and the vocoder impart distinct, model-specific fingerprints on the waveforms they generate. Second, the vocoder fingerprints are more dominant and may mask the fingerprints from the acoustic model. These findings suggest that both the acoustic model and the vocoder have the potential to be used for source identification, and that the vocoder fingerprints may be particularly useful in this context.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Foundation-models-for-Unsupervised-Audio-Visual-Segmentation"><a href="#Leveraging-Foundation-models-for-Unsupervised-Audio-Visual-Segmentation" class="headerlink" title="Leveraging Foundation models for Unsupervised Audio-Visual Segmentation"></a>Leveraging Foundation models for Unsupervised Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06728">http://arxiv.org/abs/2309.06728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Xiatian Zhu</li>
<li>for: This paper aims to develop an unsupervised audio-visual segmentation method, which can precisely outline audible objects in a visual scene at the pixel level without requiring fine-grained annotations of audio-mask pairs.</li>
<li>methods: The proposed method, called Cross-Modality Semantic Filtering (CMSF), leverages off-the-shelf multi-modal foundation models (e.g., detection, open-world segmentation, and multi-modal alignment) to accurately associate the underlying audio-mask pairs. The method has two training-free variants: AT-GDINO-SAM and OWOD-BIND.</li>
<li>results: Extensive experiments on the AVS-Bench dataset show that the unsupervised approach proposed in this paper can perform well in comparison to prior art supervised counterparts across complex scenarios with multiple auditory objects, and can excel in accurately segmenting overlapped auditory objects.<details>
<summary>Abstract</summary>
Audio-Visual Segmentation (AVS) aims to precisely outline audible objects in a visual scene at the pixel level. Existing AVS methods require fine-grained annotations of audio-mask pairs in supervised learning fashion. This limits their scalability since it is time consuming and tedious to acquire such cross-modality pixel level labels. To overcome this obstacle, in this work we introduce unsupervised audio-visual segmentation with no need for task-specific data annotations and model training. For tackling this newly proposed problem, we formulate a novel Cross-Modality Semantic Filtering (CMSF) approach to accurately associate the underlying audio-mask pairs by leveraging the off-the-shelf multi-modal foundation models (e.g., detection [1], open-world segmentation [2] and multi-modal alignment [3]). Guiding the proposal generation by either audio or visual cues, we design two training-free variants: AT-GDINO-SAM and OWOD-BIND. Extensive experiments on the AVS-Bench dataset show that our unsupervised approach can perform well in comparison to prior art supervised counterparts across complex scenarios with multiple auditory objects. Particularly, in situations where existing supervised AVS methods struggle with overlapping foreground objects, our models still excel in accurately segmenting overlapped auditory objects. Our code will be publicly released.
</details>
<details>
<summary>摘要</summary>
audio-visual分割（AVS）目标是在视觉场景中像素级准确定义可听对象。现有的AVS方法需要在监督学习方式下精细标注音频mask对。这限制了其可扩展性，因为获得这种跨ModalPixel级别标注是时间consuming和繁琐的。为解决这个问题，在这里我们介绍了无监督音频视觉分割方法，无需任务特定数据标注和模型训练。为解决这个新提出的问题，我们提出了一种新的跨ModalSemantic过滤（CMSF）方法，以准确关联下方音频mask对，利用市面上多Modal基础模型（例如检测 [1]、开放世界分割 [2]和多Modal匹配 [3]）。通过音频或视觉提示，我们设计了两种没有训练的变种：AT-GDINO-SAM和OWOD-BIND。广泛的实验表明，我们的无监督方法可以与先前的监督AVS方法相比，在复杂的场景中表现良好，特别是当存在多个混合对象时。我们的代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="PIAVE-A-Pose-Invariant-Audio-Visual-Speaker-Extraction-Network"><a href="#PIAVE-A-Pose-Invariant-Audio-Visual-Speaker-Extraction-Network" class="headerlink" title="PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network"></a>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06723">http://arxiv.org/abs/2309.06723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Liu, Meng Ge, Zhizheng Wu, Haizhou Li</li>
<li>for: This paper focuses on improving audio-visual speaker extraction by incorporating a pose-invariant view to handle varying talking faces.</li>
<li>methods: The proposed Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) generates a pose-invariant view from each original pose orientation, creating a multi-view visual input for the speaker.</li>
<li>results: Experimental results on the multi-view MEAD and in-the-wild LRS3 datasets show that PIAVE outperforms the state-of-the-art and is more robust to pose variations.<details>
<summary>Abstract</summary>
It is common in everyday spoken communication that we look at the turning head of a talker to listen to his/her voice. Humans see the talker to listen better, so do machines. However, previous studies on audio-visual speaker extraction have not effectively handled the varying talking face. This paper studies how to take full advantage of the varying talking face. We propose a Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) that incorporates an additional pose-invariant view to improve audio-visual speaker extraction. Specifically, we generate the pose-invariant view from each original pose orientation, which enables the model to receive a consistent frontal view of the talker regardless of his/her head pose, therefore, forming a multi-view visual input for the speaker. Experiments on the multi-view MEAD and in-the-wild LRS3 dataset demonstrate that PIAVE outperforms the state-of-the-art and is more robust to pose variations.
</details>
<details>
<summary>摘要</summary>
通常在日常口语交流中，我们会看向讲话人的头部来听到他/她的voice。人类看到讲话人，以便更好地听到他/她的voice，同样地，机器也会这样做。然而，过去关于音频视频说话人提取的研究没有有效地处理变化的讲话面孔。这篇论文研究如何利用变化的讲话面孔来提高音频视频说话人提取。我们提议一种pose-invariant audio-visual speaker extraction network（PIAVE），该网络包含一个额外的pose-invariant视图，以提高音频视频说话人提取的精度。具体来说，我们将每个原始的poseorientation中生成一个pose-invariant视图，使得模型能够收到不同poseorientation下的讲话人的一致前视图，从而形成多视图的视觉输入。实验表明，PIAVE在多视图MEAD和野外LRS3 dataset上比前一些状态的方法更高效和更抗pose变化。
</details></li>
</ul>
<hr>
<h2 id="Attention-based-Encoder-Decoder-End-to-End-Neural-Diarization-with-Embedding-Enhancer"><a href="#Attention-based-Encoder-Decoder-End-to-End-Neural-Diarization-with-Embedding-Enhancer" class="headerlink" title="Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer"></a>Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06672">http://arxiv.org/abs/2309.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Chen, Bing Han, Shuai Wang, Yanmin Qian</li>
<li>for: 这个论文的目的是提出一种简单的注意力基本网络 для端到端神经网络演算（AED-EEND），以提高Speaker分类的性能。</li>
<li>methods: 这个论文使用了教师强制策略来解决Speaker permutation问题，并使用了迭代解码方法来输出每个Speaker的演算结果。此外，它还提出了一个增强器模块来增强每帧的Speaker嵌入，以处理未见数量的Speaker。</li>
<li>results: 该论文的实验结果表明，使用 simulated 训练数据，可以提高Consistency性能。此外，该论文的最佳系统在CALLHOME、DIHARD II 和 AMI 等评测标准上达到了新的状态态-of-the-art（DER）性能，无需使用oracle Voice Activity Detection（VAD）。此外，该论文的 AED-EEND 系统还表现出了极高的Speech类型检测性能。<details>
<summary>Abstract</summary>
Deep neural network-based systems have significantly improved the performance of speaker diarization tasks. However, end-to-end neural diarization (EEND) systems often struggle to generalize to scenarios with an unseen number of speakers, while target speaker voice activity detection (TS-VAD) systems tend to be overly complex. In this paper, we propose a simple attention-based encoder-decoder network for end-to-end neural diarization (AED-EEND). In our training process, we introduce a teacher-forcing strategy to address the speaker permutation problem, leading to faster model convergence. For evaluation, we propose an iterative decoding method that outputs diarization results for each speaker sequentially. Additionally, we propose an Enhancer module to enhance the frame-level speaker embeddings, enabling the model to handle scenarios with an unseen number of speakers. We also explore replacing the transformer encoder with a Conformer architecture, which better models local information. Furthermore, we discovered that commonly used simulation datasets for speaker diarization have a much higher overlap ratio compared to real data. We found that using simulated training data that is more consistent with real data can achieve an improvement in consistency. Extensive experimental validation demonstrates the effectiveness of our proposed methodologies. Our best system achieved a new state-of-the-art diarization error rate (DER) performance on all the CALLHOME (10.08%), DIHARD II (24.64%), and AMI (13.00%) evaluation benchmarks, when no oracle voice activity detection (VAD) is used. Beyond speaker diarization, our AED-EEND system also shows remarkable competitiveness as a speech type detection model.
</details>
<details>
<summary>摘要</summary>
深度神经网络基于系统在说话人分类任务中表现得非常出色。然而，末端神经网络分类（EEND）系统经常在未见到speaker数量的情况下难以泛化，而目标说话人活动检测（TS-VAD）系统往往过于复杂。在这篇论文中，我们提出了一种简单的注意力基于编码器-解码器网络 для末端神经网络分类（AED-EEND）。在我们的训练过程中，我们引入了一种教师强制策略，以解决说话人排序问题，从而快速适应模型。为评价，我们提出了一种逐个输出每个说话人的排序结果的迭代解码方法。此外，我们还提出了增强器模块，以提高帧级别的说话人嵌入，使模型能够处理未见到的说话人数量。此外，我们还发现了常用的 simulate datasets 的重要性。我们发现，使用更加符合实际数据的训练数据可以实现更好的一致性。我们的最佳系统在所有 CALLHOME (10.08%), DIHARD II (24.64%), 和 AMI (13.00%) 评价标准上达到了新的state-of-the-art diarization error rate（DER）性能，无需使用 oracle voice activity detection（VAD）。此外，我们的 AED-EEND 系统还表现出了极高的语音类型检测能力。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Modelling-of-Percussive-Audio-with-Transient-and-Spectral-Synthesis"><a href="#Differentiable-Modelling-of-Percussive-Audio-with-Transient-and-Spectral-Synthesis" class="headerlink" title="Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis"></a>Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06649">http://arxiv.org/abs/2309.06649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordie Shier, Franco Caspe, Andrew Robertson, Mark Sandler, Charalampos Saitis, Andrew McPherson</li>
<li>for: This paper aims to address transient generation and percussive synthesis within a differentiable digital signal processing (DDSP) framework.</li>
<li>methods: The proposed method builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. The method uses a modified sinusoidal peak picking algorithm and differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds.</li>
<li>results: The method leads to improved onset signal reconstruction for membranophone percussion instruments, as shown by a set of reconstruction metrics computed using a large dataset of acoustic and electronic percussion samples.<details>
<summary>Abstract</summary>
Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.
</details>
<details>
<summary>摘要</summary>
diferenciable procesamiento de señales digitales (DDSP) técnicas, incluyendo métodos para la síntesis de audio, han recibido atención en los últimos años y se prestan a la interpretabilidad en el espacio de parámetros. Sin embargo, los métodos de síntesis diferenciable actuales no han buscado explícitamente modelar la parte transitoria de las señales, lo cual es importante para los sonidos percusivos. En este trabajo, presentamos un marco de síntesis unificada que busca abordar la generación de transientes y la síntesis percusiva dentro de un marco DDSP. Para lograr esto, propusimos un modelo de síntesis percusiva que se basa en la modelización sinusoidal y incorpora una red convolutional temporal modulada para la generación de transientes. Usamos un algoritmo de selección de picos sinusoidales modificado para generar ondas no armónicas tiempo-varias y las acompañamos con codificadores de ruido y transientes diferenciables que se entrenan juntos para reconstruir sonidos de platillos. Computamos un conjunto de métricas de reconstrucción utilizando una gran base de datos de muestras de percusión acústica y electrónica que demuestran que nuestro método lleva a una reconstrucción más precisa de la señal de onset para los instrumentos membranófonos.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/13/cs.SD_2023_09_13/" data-id="clmjn91ok00cb0j885wwiaiy4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/13/cs.LG_2023_09_13/" class="article-date">
  <time datetime="2023-09-13T10:00:00.000Z" itemprop="datePublished">2023-09-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/13/cs.LG_2023_09_13/">cs.LG - 2023-09-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sight-Beyond-Text-Multi-Modal-Training-Enhances-LLMs-in-Truthfulness-and-Ethics"><a href="#Sight-Beyond-Text-Multi-Modal-Training-Enhances-LLMs-in-Truthfulness-and-Ethics" class="headerlink" title="Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics"></a>Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07120">http://arxiv.org/abs/2309.07120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/sight-beyond-text">https://github.com/ucsc-vlaa/sight-beyond-text</a></li>
<li>paper_authors: Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie</li>
<li>for: 这项研究旨在探讨多模态大语言模型（MLLMs）的准确性和道德Alignment在纯文本处理领域的能力。</li>
<li>methods: 研究人员使用视觉指令调整，一种常见的将文本模型转化为多模态模型的策略，并证明这种方法可以帮助模型在纯文本处理领域达到更高的准确性和道德Alignment。</li>
<li>results: 研究人员发现，视觉指令调整可以使模型在TruthfulQA-mc和Ethics benchmark上表现更好，比如一个视觉指令调整的LLaMA2 7B模型超过了一个基于一百万人注释的LLaMA2-chat 7B模型的性能。进一步分析表明，这种改善的Alignment可以归功于视觉文本数据中的更高质量指令。<details>
<summary>Abstract</summary>
Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at github.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration into the intrinsic value of visual-text synergies and, in a broader scope, multi-modal interactions in alignment research.
</details>
<details>
<summary>摘要</summary>
多模态大语言模型（MLLM）通过大语言模型（LLM）进行训练，具有更好地理解多模态输入并生成文本响应的能力。虽然它们在多模态任务中表现出色，但纯NLP能力的MLLM often underestimated 和未经测试。在这个研究中，我们跳出了obox 和探索了一个MLLM的惊喜特征——视觉指令调整，一种常见的将LLM转换为MLLM的策略。我们发现，这种策略不仅能够提高模型的真实性和道德跟进性，而且在纯NLP上也能够达到这些目标。例如，一个视觉指令调整的 LLaMA2 7B 模型在 TruthfulQA-mc 和 Ethics 标准上超越了一个 fine-tuned 的 LLaMA2-chat 7B 模型，这个模型在一百万个人注释后进行了微调。进一步的分析表明，改进的协调可以归功于视觉文本数据中的高质量指令。在发布我们的代码在github.com/UCSC-VLAA/Sight-Beyond-Text 上，我们希望能够激发更多人对多模态交互和视觉文本的内在价值进行进一步的探索，以及在更广泛的范围内进行对适应性研究。
</details></li>
</ul>
<hr>
<h2 id="PILOT-A-Pre-Trained-Model-Based-Continual-Learning-Toolbox"><a href="#PILOT-A-Pre-Trained-Model-Based-Continual-Learning-Toolbox" class="headerlink" title="PILOT: A Pre-Trained Model-Based Continual Learning Toolbox"></a>PILOT: A Pre-Trained Model-Based Continual Learning Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07117">http://arxiv.org/abs/2309.07117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sun-hailong/lamda-pilot">https://github.com/sun-hailong/lamda-pilot</a></li>
<li>paper_authors: Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan</li>
<li>for: 本研究旨在开发基于预训练模型的不间断学习工具箱（PILOT），以满足实际场景中新数据的进来。</li>
<li>methods: 本研究使用了state-of-the-art的预训练模型基于的分类不间断学习算法，如L2P、DualPrompt和CODA-Prompt，同时也将典型的分类不间断学习算法（如DER、FOSTER和MEMO）置于预训练模型的 Context中进行评估。</li>
<li>results: 本研究通过PILOT实现了一个可靠的、灵活的和高效的不间断学习工具箱，可以帮助研究人员更好地适应实际场景中的新数据进来。<details>
<summary>Abstract</summary>
While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the context of pre-trained models to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
Traditional machine learning 可以有效地解决各种问题，但它主要在封闭世界中运行，这限制了对流动数据的处理。为了解决实际情况，增量学习出现了，它可以适应新数据的到来。最近，预训练得到了广泛的关注和进步，这些预训练模型（PTM）在实际场景中表现出了强大的能力。因此，利用 PTM 进行增量学习的研究已成为必要。这篇文章介绍了基于预训练模型的增量学习工具箱，称为 PILOT。一方面，PILOT 实现了一些状态之arte class-incremental learning算法，基于预训练模型，如 L2P、DualPrompt 和 CODA-Prompt。另一方面，PILOT 还可以在预训练模型中适应典型的 class-incremental learning算法（例如 DER、FOSTER 和 MEMO），以评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Learning-for-Audio-Visual-Speaker-Verification"><a href="#Weakly-Supervised-Multi-Task-Learning-for-Audio-Visual-Speaker-Verification" class="headerlink" title="Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification"></a>Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07115">http://arxiv.org/abs/2309.07115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anith Selvakumar, Homa Fashandi</li>
<li>for: 本研究旨在实现robust多modal人体表示，optimized for open-set audio-visual speaker verification。</li>
<li>methods: 我们explored multitask learning techniques和Generalized end-to-end loss (GE2E) approach，以增强distance metric learning (DML)的性能。</li>
<li>results: 我们的网络实现了speaker verification的state of the art performance，reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on VoxCeleb1-O&#x2F;E&#x2F;H。<details>
<summary>Abstract</summary>
In this paper, we present a methodology for achieving robust multimodal person representations optimized for open-set audio-visual speaker verification. Distance Metric Learning (DML) approaches have typically dominated this problem space, owing to strong performance on new and unseen classes. In our work, we explored multitask learning techniques to further boost performance of the DML approach and show that an auxiliary task with weak labels can increase the compactness of the learned speaker representation. We also extend the Generalized end-to-end loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce a non-synchronous audio-visual sampling random strategy during training time that has shown to improve generalization. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official trial lists of VoxCeleb1-O/E/H, which is to our knowledge, the best published results on VoxCeleb1-E and VoxCeleb1-H.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来实现可靠的多Modal人表示，优化为开放集 audio-visual speaker验证。通常，Distance Metric Learning（DML）方法在这个问题空间中占据主导地位，因为它在新和未经见过的类型上表现强。在我们的工作中，我们explored multitask learning技术来进一步提高DML方法的性能，并证明了一个 auxiliary task 的弱标签可以增加学习的说话人表示的紧凑性。此外，我们扩展了Generalized end-to-end loss（GE2E）到多Modal输入，并证明它可以在 audio-visual 空间 achieve competitive performance。最后，我们引入了异步 audio-visual 采样随机策略 durante training time，这已经显示了它可以提高泛化性。我们的网络实现了 speaker verification 的州OF the art表现，报告的 Equal Error Rate（EER）为0.244%、0.252% 和0.441% 在 VoxCeleb1-O/E/H 的三个官方试用列表上，这是我们所知道的最佳发表结果。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Deep-Encoding-Enables-Uncertainty-aware-Machine-learning-assisted-Histopathology"><a href="#Contrastive-Deep-Encoding-Enables-Uncertainty-aware-Machine-learning-assisted-Histopathology" class="headerlink" title="Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology"></a>Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07113">http://arxiv.org/abs/2309.07113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirhoshan Sivaroopan, Chamuditha Jayanga, Chalani Ekanayake, Hasindri Watawana, Jathurshan Pradeepkumar, Mithunjha Anandakumar, Ranga Rodrigo, Chamira U. S. Edussooriya, Dushan N. Wadduwage</li>
<li>for: 这个论文的目的是用深度神经网络模型从百万个 histopathology 图像中学习临床有用的特征。</li>
<li>methods: 这篇论文使用了大量的公共领域数据进行预训练，然后使用一小部分标注数据进行精度训练。另外，它还提出了一种不确定性感知损失函数，以衡量模型在推断中的信任度。</li>
<li>results: 这篇论文的结果表明，使用预训练和不确定性感知损失函数可以达到现有最佳实践（SOTA）的水平，只需使用1-10%的随机标注数据。此外，它还证明了在整个扫描图像分类任务中，使用预训练后的模型可以超过现有最佳实践。<details>
<summary>Abstract</summary>
Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select the best instances to label for further training. Our uncertainty-aware labeling reaches the SOTA with significantly fewer annotations compared to random labeling. Last, we demonstrate how our pre-trained encoders can surpass current SOTA for whole-slide image classification with weak supervision. Our work lays the foundation for data and task-agnostic pre-trained deep networks with quantified uncertainty.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "histopathology images" 转换为 " histopathology 图像"* "clinically relevant features" 转换为 "临床有用的特征"* "terabytes of training data" 转换为 "训练数据的天terabytes"* "pre-train deep networks" 转换为 "预训练深度网络"* "fine-tune" 转换为 "精细调整"* "patch-level classification" 转换为 "幂分类"* "whole-slide image classification" 转换为 "整幅图像分类"* "weak supervision" 转换为 "弱监督"* "uncertainty-aware loss function" 转换为 "不确定性认知的损失函数"* "quantified uncertainty" 转换为 "量化不确定性"
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-via-Subgroup-Mixup-for-Improving-Fairness"><a href="#Data-Augmentation-via-Subgroup-Mixup-for-Improving-Fairness" class="headerlink" title="Data Augmentation via Subgroup Mixup for Improving Fairness"></a>Data Augmentation via Subgroup Mixup for Improving Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07110">http://arxiv.org/abs/2309.07110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madeline Navarro, Camille Little, Genevera I. Allen, Santiago Segarra</li>
<li>for: 提高群体公平性（improve group fairness）</li>
<li>methods: 使用对称混合（pairwise mixup）方法来增强训练数据，并且使用混合来增强决策边界的公平性（fairness）和准确性（accuracy）</li>
<li>results: 比较于现有的数据增强和偏见缓解方法，提高了群体公平性和准确性（improved group fairness and accuracy）<details>
<summary>Abstract</summary>
In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to under-representation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了通过对 subgroup 进行对应混合来提高群体公平性。许多现实世界中机器学习系统的应用受到 subgroup 之间的偏见的影响，这是因为这些 subgroup 的数据不充分表示社会偏见。 inspirited by mixup 的成功，我们开发了对应混合方案来增强训练数据，并促进所有 subgroup 的公平和准确的决策边界。通过数据增强来提高群体公平性，我们可以添加更多的受 represened subgroup 的样本来均衡 subgroup。此外，我们的方法可以利用混合的通用能力来提高公平性和准确性。我们在synthetic simulations和实际世界中的benchmark fair classification数据上比较了我们的提议混合和现有的数据增强和偏见缓解方法，结果表明我们能够实现公平的结果，同时甚至提高准确性。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Speed-Performance-of-Multi-Agent-Reinforcement-Learning"><a href="#Characterizing-Speed-Performance-of-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Characterizing Speed Performance of Multi-Agent Reinforcement Learning"></a>Characterizing Speed Performance of Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07108">http://arxiv.org/abs/2309.07108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna</li>
<li>for: 这篇论文的目的是分析多智能体强化学习（MARL）算法的速度性能，以及将来文献中该metric的重要性。</li>
<li>methods: 该论文使用了一种分类方法来分类MARL算法，并对三种目前顶尖MARL算法进行系统性的性能分析，以找出它们的性能瓶颈。</li>
<li>results: 该论文发现，MARL算法的速度性能是一个关键瓶颈，并且提出了将来文献中关于MARL算法的性能评价应该包括速度性能的要求。<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform. We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.
</details>
<details>
<summary>摘要</summary>
In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform.We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Group-Bias-in-Federated-Learning-for-Heterogeneous-Devices"><a href="#Mitigating-Group-Bias-in-Federated-Learning-for-Heterogeneous-Devices" class="headerlink" title="Mitigating Group Bias in Federated Learning for Heterogeneous Devices"></a>Mitigating Group Bias in Federated Learning for Heterogeneous Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07085">http://arxiv.org/abs/2309.07085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khotso Selialia, Yasra Chandio, Fatima M. Anwar</li>
<li>for: 这篇论文旨在提出一个隐私保护的聚合学习框架，以减少对特定社群或群体的不公平偏袋。</li>
<li>methods: 本文提出使用平均条件概率来计算跨领域群体重要性加权，以优化具有最差性能的群体表现。此外，本文还提出了调整技术来最小化最差和最好表现之间的差异，以保持对偏袋的平衡。</li>
<li>results: 本文的评估结果显示，在真实世界的不同领域和环境下，该框架能够实现公平的决策。<details>
<summary>Abstract</summary>
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.   Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance weights} derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while making sure through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of human emotion recognition and image classification benchmarks assesses the fair decision-making of our framework in real-world heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
《联合学习》在分布式边缘应用中emerging为一种隐私保护的模型训练方法。因此，大多数边缘部署都是不同的 nature，即感应能力和环境。这个边缘多样性违反了本地数据的独立性和相同分布（IID）性，从而导致模型偏好和不公正决策。现有的偏好缓和技术仅对于标签多样性中的偏好而不考虑特定领域的多样性和特性，并未Address全球集体公平性性。我们的工作提出了一个集体公平的联合学习框架，可以降低集体偏好而保持隐私和资源利用 overhead。我们的主要想法是利用average conditional probabilities来计算跨Domain的集体重要性，从不同的训练数据中提取出对各个集体的优化性能。此外，我们也提出了调整技术来降低最差和最好的集体之间的差异，同时通过阈值机制来确保偏好缓和和集体性能的平衡。我们在人们情感识别和图像分类benchmark中进行了真实世界多样的评估，以评估我们的框架在实际中的公平决策。
</details></li>
</ul>
<hr>
<h2 id="The-Boundaries-of-Verifiable-Accuracy-Robustness-and-Generalisation-in-Deep-Learning"><a href="#The-Boundaries-of-Verifiable-Accuracy-Robustness-and-Generalisation-in-Deep-Learning" class="headerlink" title="The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning"></a>The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07072">http://arxiv.org/abs/2309.07072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Bastounis, Alexander N. Gorban, Anders C. Hansen, Desmond J. Higham, Danil Prokhorov, Oliver Sutton, Ivan Y. Tyukin, Qinghua Zhou</li>
<li>for: 本研究探讨了神经网络在分类任务中确定稳定性和准确性的理论限制。</li>
<li>methods: 我们考虑了经典的分布不依赖性框架和算法，以最小化实际风险并可能受到权重正则化。</li>
<li>results: 我们发现了一大家族任务，其中计算和验证理想稳定和准确的神经网络在上述设置下是极为困难，甚至可能无法实现，即使理想的解在给定的神经网络架构中存在。<details>
<summary>Abstract</summary>
In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们评估了神经网络在分类任务中的理论限制。我们考虑了经典的分布不依赖于框架和算法，用来降低实际风险，并可能受到权重正则化的限制。我们显示出，存在一个大家庭任务，计算和验证理想稳定和准确的神经网络在上述设置下是极其困难，甚至可能无法完成，即使理想的解决方案存在于给定的神经网络架构中。
</details></li>
</ul>
<hr>
<h2 id="Deep-Quantum-Graph-Dreaming-Deciphering-Neural-Network-Insights-into-Quantum-Experiments"><a href="#Deep-Quantum-Graph-Dreaming-Deciphering-Neural-Network-Insights-into-Quantum-Experiments" class="headerlink" title="Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments"></a>Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07056">http://arxiv.org/abs/2309.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tareq Jaouni, Sören Arlt, Carlos Ruiz-Gonzalez, Ebrahim Karimi, Xuemei Gu, Mario Krenn</li>
<li>for: 这个论文旨在解释神经网络在量子光学实验中的学习过程和结果。</li>
<li>methods: 这篇论文使用了一种名为inception或deep dreaming的可解释AI技术，该技术在计算机视觉领域已经应用了。通过这种技术，我们可以探索神经网络对量子系统的学习。</li>
<li>results: 我们发现，神经网络可以将量子系统的初始性质分布Shift，并且可以描述神经网络学习的策略。 Interestingly,我们发现，在核心层次上，神经网络可以识别简单的性质，而在深层次上，它可以识别复杂的量子结构，甚至是量子紧密相关。这与计算机视觉领域已有的长期认知的性质相似。我们的方法可能可以用于开发更加解释性的AI基于科学发现技术。<details>
<summary>Abstract</summary>
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones, it can identify complex quantum structures and even quantum entanglement. This is in reminiscence of long-understood properties known in computer vision, which we now identify in a complex natural science task. Our approach could be useful in a more interpretable way to develop new advanced AI-based scientific discovery techniques in quantum physics.
</details>
<details>
<summary>摘要</summary>
儿童 neural networks 的透明性问题带来了解释新发现的挑战。我们使用一种名为 $inception$ 或 $deep$ $dreaming$ 的 explainable-AI（XAI）技术，这种技术在计算机视觉中发明。我们使用这种技术来探索 neural networks 学习 quantum optics 实验的内容。我们的故事开始于训练一个深度 neural networks 在量子系统的性质上。一旦训练完成，我们将 neural network “反转”，即问其如何假设一个具有特定性质的量子系统，并如何在改变性质时不断修改量子系统。我们发现，在早些层次中， neural network 可以快速地标识简单的性质，而在更深层次中，它可以标识复杂的量子结构和甚至量子共振。这与计算机视觉中已久认知的性质有很大相似之处。我们的方法可能会在更加可解的方式下发展出新的高级 AI 基于科学发现技术。
</details></li>
</ul>
<hr>
<h2 id="An-Extreme-Learning-Machine-Based-Method-for-Computational-PDEs-in-Higher-Dimensions"><a href="#An-Extreme-Learning-Machine-Based-Method-for-Computational-PDEs-in-Higher-Dimensions" class="headerlink" title="An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions"></a>An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07049">http://arxiv.org/abs/2309.07049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiran Wang, Suchuan Dong</li>
<li>for: 这两种方法可以帮助解决高维partial differential equations (PDE)问题，并且可以生成高精度的解决方案。</li>
<li>methods: 第一种方法使用随机化神经网络来表示未知解决场的数据，并通过将PDE和边界&#x2F;初始条件等约束表示为一个或多个随机点的系统来解决问题。第二种方法则是基于approximate theory of functional connections (A-TFC)来重新表示高维PDE问题，并使用随机神经网络来表示自由场函数。</li>
<li>results: 数据示出，这两种方法可以生成高精度的解决方案，并且相比Physics-informed neural network (PINN)方法，这两种方法更加成本效果和精度高。<details>
<summary>Abstract</summary>
We present two effective methods for solving high-dimensional partial differential equations (PDE) based on randomized neural networks. Motivated by the universal approximation property of this type of networks, both methods extend the extreme learning machine (ELM) approach from low to high dimensions. With the first method the unknown solution field in $d$ dimensions is represented by a randomized feed-forward neural network, in which the hidden-layer parameters are randomly assigned and fixed while the output-layer parameters are trained. The PDE and the boundary/initial conditions, as well as the continuity conditions (for the local variant of the method), are enforced on a set of random interior/boundary collocation points. The resultant linear or nonlinear algebraic system, through its least squares solution, provides the trained values for the network parameters. With the second method the high-dimensional PDE problem is reformulated through a constrained expression based on an Approximate variant of the Theory of Functional Connections (A-TFC), which avoids the exponential growth in the number of terms of TFC as the dimension increases. The free field function in the A-TFC constrained expression is represented by a randomized neural network and is trained by a procedure analogous to the first method. We present ample numerical simulations for a number of high-dimensional linear/nonlinear stationary/dynamic PDEs to demonstrate their performance. These methods can produce accurate solutions to high-dimensional PDEs, in particular with their errors reaching levels not far from the machine accuracy for relatively lower dimensions. Compared with the physics-informed neural network (PINN) method, the current method is both cost-effective and more accurate for high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
我们提出了两种有效的方法来解决高维度partial differential equation (PDE)，基于随机化神经网络。这两种方法都是基于extreme learning machine (ELM)的扩展，将ELM的方法从低维度扩展到高维度。在第一种方法中，不知的解析场在d维度上是由随机化的Feed-Forward神经网络表示，其隐藏层参数随机分配并固定，而出力层参数则是通过训练来学习。PDE和边界/初始条件，以及当地的连续条件（ для本地方法），都是在一组随机的内部/边界点上强制实现。从这个线性或非线性的代数系统中，通过最小二乘法解，获得训练完成的神经网络参数。在第二种方法中，高维度PDE问题被重新表述为一个受限的表述，基于一个 Approximate 的Functional Connections 理论（A-TFC），这样可以避免在维度增加时，TFC 中的指数增长。免�ayer 的自由场函数在 A-TFC 受限的表述中被随机化神经网络表示，通过一种与第一种方法相似的训练 процес来训练。我们提供了丰富的数据显示，用于训练这些方法，以及对高维度线性/非线性站立/动态 PDE 的应用。这些方法可以精确地解决高维度 PDE，特别是其误差可以达到机器精度的水平，尤其是维度较低的情况下。相比于Physics-Informed Neural Network 方法（PINN），现在的方法更加成本效益和精确。
</details></li>
</ul>
<hr>
<h2 id="Optimal-transport-distances-for-directed-weighted-graphs-a-case-study-with-cell-cell-communication-networks"><a href="#Optimal-transport-distances-for-directed-weighted-graphs-a-case-study-with-cell-cell-communication-networks" class="headerlink" title="Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks"></a>Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07030">http://arxiv.org/abs/2309.07030</a></li>
<li>repo_url: None</li>
<li>paper_authors: James S. Nagai, Ivan G. Costa, Michael T. Schaub</li>
<li>for:  comparing directed graphs based on optimal transport distances</li>
<li>methods:  earth movers distance (Wasserstein) and Gromov-Wasserstein (GW) distance</li>
<li>results: evaluation of two distance measures on simulated graph data and real-world directed cell-cell communication graphs, with discussion of relative performance<details>
<summary>Abstract</summary>
Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
</details>
<details>
<summary>摘要</summary>
对图像的优质运输比较已经收到了广泛的关注，因为优质运输所导出的距离提供了一种原理上的图像间距离度量，同时也提供了一种可解释的图像变化描述，基于一个运输计划。然而，由于不具有对称性，通常考虑的优质运输形式ulations往往难以应用于指定的图像。在这篇文章中，我们提出了两种用于比较指定图像的优质运输距离：（i）地球搬运距离（沃氏天然距离）和（ii）格罗莫夫-沃氏天然距离（GW距离）。我们评估了这两种距离的表现，并对 simulated 图像数据和实际的指定细胞通信图像进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Contrast-Consistent-Ranking-with-Language-Models"><a href="#Unsupervised-Contrast-Consistent-Ranking-with-Language-Models" class="headerlink" title="Unsupervised Contrast-Consistent Ranking with Language Models"></a>Unsupervised Contrast-Consistent Ranking with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06991">http://arxiv.org/abs/2309.06991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro, Rajarshi Bhowmik</li>
<li>for: 这个论文的目的是探讨语言模型中的排序知识，以及如何使用不同的提示技术来激活这种知识。</li>
<li>methods: 这篇论文使用了一种叫做对比一致搜索（CCS）的无监督探索方法，用于训练一个排序模型。</li>
<li>results: 实验结果表明，使用CCR探索方法可以比使用提示技术更好地激活语言模型中的排序知识，并且可以与更大的语言模型相比。<details>
<summary>Abstract</summary>
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression objective. Our results confirm that, for the same language model, CCR probing outperforms prompting and even performs on a par with prompting much larger language models.
</details>
<details>
<summary>摘要</summary>
语言模型含有排序知识，是Contextual Ranking任务的强大解决方案。例如，它们可能具有国家大小的参数知识或可以根据 sentiment 排序评论。现有的研究主要关注用于提取语言模型的排序知识的对话、点对点和列表技术。然而，我们发现，即使使用精心调整和受限的解码，提取技术可能并不总是自consistent的排名。这使我们探索一种不同的方法，即基于无监督探测方法Contrast-Consistent Search (CCS)的inspired的方法。这个想法是训练一个探测模型，其中模型对于一个陈述和其否定的表示都需要被映射到冲突真假极点一致的多个陈述中。我们 hypothesize 在排序任务中，所有的项目都是通过一致的对比或列表比较关联的。为此，我们将 binary CCS 方法扩展为 Contrast-Consistent Ranking (CCR)，并采用现有的排名方法，如最大margin损失、 triplet损失和ORDINAL REGRESSION objective。我们的结果表明，对于同一个语言模型，CCR探测超过提取和甚至与更大的语言模型相当。
</details></li>
</ul>
<hr>
<h2 id="MASTERKEY-Practical-Backdoor-Attack-Against-Speaker-Verification-Systems"><a href="#MASTERKEY-Practical-Backdoor-Attack-Against-Speaker-Verification-Systems" class="headerlink" title="MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"></a>MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06981">http://arxiv.org/abs/2309.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan</li>
<li>for: 这个论文旨在攻击移动系统中广泛使用的声音特征识别（Speaker Verification，SV）模型，以实现非法用户身份验证。</li>
<li>methods: 作者提出了一种名为“MASTERKEY”的后门攻击，利用攻击者没有受害者的知识的情况下，破坏SV模型。作者首先调查了现有恶意攻击模型的局限性，然后优化了一个通用的后门，可以攻击任意目标。然后，作者将说话者特征和 semantics信息embedded到后门中，使其隐蔽。最后，作者估算了通信扭曲并将其纳入后门中。</li>
<li>results: 作者验证了这种攻击在6种流行的SV模型上，总共恶意攻击53个模型，并使用触发器攻击310个目标说话者，包括53个恶意攻击模型中的310个目标说话者。攻击成功率为100%，且恶意攻击率为15%。当降低恶意攻击率到3%时，攻击成功率仍然约为50%。作者在3个实际场景中成功地实现了这种攻击，包括通过无线电和电话线进行攻击。<details>
<summary>Abstract</summary>
Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a 15% poison rate. By decreasing the poison rate to 3%, the attack success rate remains around 50%. We validate our attack in 3 real-world scenarios and successfully demonstrate the attack through both over-the-air and over-the-telephony-line scenarios.
</details>
<details>
<summary>摘要</summary>
speaker verification (SV) 广泛部署在移动系统中，以使用用户的声音特征来验证合法用户。在这项工作中，我们提出了一种后门攻击 named MASTERKEY，以破坏 SV 模型。与之前的攻击不同，我们在实际的实际情况下，假设攻击者没有受害者的任何知识。我们 Investigate 现有恶意攻击模型的局限性，然后优化一个通用的后门，可以攻击任意目标。接着，我们嵌入了发音人的特征和语义信息到后门中，使其隐蔽。最后，我们估算了通信扭曲，并将其纳入后门中。我们验证了我们的攻击，并在 6 个流行的 SV 模型上进行了验证。特别是，我们毒害了 53 个模型，并使用我们的触发器攻击 16,430 名注册用户，包括 310 名目标用户，其中 53 个模型中的每个模型都有 310 名目标用户。我们的攻击得到了 100% 的攻击成功率，但是降低毒害率到 3% 时，攻击成功率仍然保持在 50% 左右。我们在 3 个实际情况下验证了我们的攻击，并成功地通过了 both over-the-air 和 over-the-telephony-line 方式进行了攻击。
</details></li>
</ul>
<hr>
<h2 id="Auto-Regressive-Next-Token-Predictors-are-Universal-Learners"><a href="#Auto-Regressive-Next-Token-Predictors-are-Universal-Learners" class="headerlink" title="Auto-Regressive Next-Token Predictors are Universal Learners"></a>Auto-Regressive Next-Token Predictors are Universal Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06979">http://arxiv.org/abs/2309.06979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Malach</li>
<li>for: 这个论文旨在研究自然语言处理方面的语言模型，特别是使用下一个字符预测任务来实现逻辑和数学逻辑的能力。</li>
<li>methods: 该论文使用了一种名为Chain-of-Thought（CoT）数据集，并使用了线性next-token predictor和浅层多层感知器（MLP）来训练语言模型。</li>
<li>results: 实验表明，使用这些简单的模型可以非常高效地解决文本生成和数学任务，而这些能力归功于自动预测下一个字符的训练方法，而不是特定的架构选择。<details>
<summary>Abstract</summary>
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.
</details>
<details>
<summary>摘要</summary>
大型语言模型displayed出了很好的逻辑和数学理解能力，可以解决复杂的任务。奇怪的是，这些能力会在以下任务为基础的网络上缔造出来：下一个字 Predictor。在这个工作中，我们提出了一个理论框架来研究自动预测下一个字的网络。我们证明了，简单的模型，如线性下一个字预测模型，可以将任何由Turing机computed的函数高效地替换。我们引入了一个新的复杂度度量——字符串复杂度——用于度量CoT字串中用于替换某个目标函数所需的 intermediate tokens数量，并分析这些复杂度度量与其他的复杂度度量之间的关系。最后，我们显示了实验结果，证明了简单的下一个字预测模型，如线性网络和浅层多层感知机（MLP），在文本生成和数学任务上显示出了非常有趣的表现。我们的结果显示，语言模型的力量可以很大程度上归因于自动预测下一个字的训练方案，而不是特定的架构选择。
</details></li>
</ul>
<hr>
<h2 id="DNNShifter-An-Efficient-DNN-Pruning-System-for-Edge-Computing"><a href="#DNNShifter-An-Efficient-DNN-Pruning-System-for-Edge-Computing" class="headerlink" title="DNNShifter: An Efficient DNN Pruning System for Edge Computing"></a>DNNShifter: An Efficient DNN Pruning System for Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06973">http://arxiv.org/abs/2309.06973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blessonvar/dnnshifter">https://github.com/blessonvar/dnnshifter</a></li>
<li>paper_authors: Bailey J. Eccles, Philip Rodgers, Peter Kilpatrick, Ivor Spence, Blesson Varghese</li>
<li>for: 这个论文是为了解决深度神经网络（DNNs）在移动和嵌入式设备上进行推理时的资源问题，这些设备具有有限的计算和存储资源。</li>
<li>methods: 这篇论文提出了一种基于结构剔除的DNN预处理方法，可以快速生成适合移动和嵌入式设备进行推理的轻量级模型变体。</li>
<li>results: 论文的实验结果表明，DNNShifter可以在几乎无变化的时间和资源 overhead下，生成与原始 dense model 相似的准确性模型变体，并且可以快速将模型变体 swap 到不同的操作系统和网络条件下。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching system that addresses the challenges mentioned above. At the heart of DNNShifter is a novel methodology that prunes sparse models using structured pruning. The pruned model variants generated by DNNShifter are smaller in size and thus faster than dense and sparse model predecessors, making them suitable for inference at the edge while retaining near similar accuracy as of the original dense model. DNNShifter generates a portfolio of model variants that can be swiftly interchanged depending on operational conditions. DNNShifter produces pruned model variants up to 93x faster than conventional training methods. Compared to sparse models, the pruned model variants are up to 5.14x smaller and have a 1.67x inference latency speedup, with no compromise to sparse model accuracy. In addition, DNNShifter has up to 11.9x lower overhead for switching models and up to 3.8x lower memory utilisation than existing approaches.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在机器学习应用中扮演着重要角色。生产质量DNN模型在训练数百万个参数后可以达到高精度推理。这种情况带来了对Edge网络中的资源进行挑战，例如移动设备和嵌入式设备具有有限的计算和内存资源。为解决这个问题，模型会被剪辑，以创建轻量级、适用于这些设备的模型变体。现有的剪辑方法无法提供与原始模型无销毁的质量模型，或者需要过度时间和负担。我们的工作可以快速生成适合设备的模型变体，同时保持原始模型的准确性。这些模型变体可以快速交换，根据系统和网络条件变化，以适应工作荷负。本文介绍了DNNShifter，一个全流程DNN训练、空间剪辑和模型交换系统。DNNShifter的核心技术是基于结构剪辑的稀疏模型剪辑。剪辑后的模型变体由DNNShifter生成，比 dense和稀疏模型前任更小，更快，同时保持原始模型的准确性。DNNShifter生成了一个模型集合，可以根据操作条件快速交换。相比传统训练方法，DNNShifter可以生成剪辑后的模型变体，速度达93倍。相比稀疏模型，剪辑后的模型变体具有1.67倍的推理速度减少，同时保持稀疏模型的准确性。此外，DNNShifter的模型交换过程 overhead低至11.9倍，内存利用率下降至3.8倍。
</details></li>
</ul>
<hr>
<h2 id="Setting-the-Right-Expectations-Algorithmic-Recourse-Over-Time"><a href="#Setting-the-Right-Expectations-Algorithmic-Recourse-Over-Time" class="headerlink" title="Setting the Right Expectations: Algorithmic Recourse Over Time"></a>Setting the Right Expectations: Algorithmic Recourse Over Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06969">http://arxiv.org/abs/2309.06969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Fonseca, Andrew Bell, Carlo Abrate, Francesco Bonchi, Julia Stoyanovich</li>
<li>for: 研究算法决策系统在高度决策中的帮助</li>
<li>methods: 使用代理模型来研究环境不断变化对算法补救的影响</li>
<li>results: 发现只有特定参数化情况下的补救可以在时间上保持可靠性，需要进一步研究以确保补救努力得到奖励。<details>
<summary>Abstract</summary>
Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals.   In this work we propose an agent-based simulation framework for studying the effects of a continuously changing environment on algorithmic recourse. In particular, we identify two main effects that can alter the reliability of recourse for individuals represented by the agents: (1) competition with other agents acting upon recourse, and (2) competition with new agents entering the environment. Our findings highlight that only a small set of specific parameterizations result in algorithmic recourse that is reliable for agents over time. Consequently, we argue that substantial additional work is needed to understand recourse reliability over time, and to develop recourse methods that reward agents' effort.
</details>
<details>
<summary>摘要</summary>
算法系统常被召集来协助高度决策。由于这一点，算法补救（individuals should be able to take action against an undesirable outcome made by an algorithmic system）在receiving growing attention。现有大部分文献对于算法补救的研究都集中在如何为单个个体提供补救，而忽略了一个关键因素：Context changover time。忽略这个因素是一项重要的漏洞，因为补救通常包括个体首先尝试不成功，然后在后续时间获得一次或多次的机会。这可能创造false expectations，因为初始补救建议可能会变得 menos reliable over time due to model drift and competition for access to the favorable outcome between individuals。在这项工作中，我们提出一种基于代理人的模拟框架，用于研究算法补救在不断变化的环境中的效果。我们确定了两个主要的效果可能使补救无效的代理人：（1）代理人在补救时与其他代理人竞争，（2）新代理人入境环境时与其他代理人竞争。我们的发现表明，只有一小集特定的参数化可以在长期内保持代理人的补救可靠。因此，我们认为需要进一步的研究，以确定补救可靠性在时间上的变化，并开发补救方法， reward agents' effort。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Multiple-Description-for-DNA-based-data-storage"><a href="#Implicit-Neural-Multiple-Description-for-DNA-based-data-storage" class="headerlink" title="Implicit Neural Multiple Description for DNA-based data storage"></a>Implicit Neural Multiple Description for DNA-based data storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06956">http://arxiv.org/abs/2309.06956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trung Hieu Le, Xavier Pic, Jeremy Mateos, Marc Antonini</li>
<li>for: 这篇论文的目的是探讨用DNA作为数据存储媒体的可能性，并解决存储和生物操作引起的错误问题。</li>
<li>methods: 这篇论文使用了一种新的压缩方法和一种基于神经网络的多描述编码（MDC）技术来解决DNA数据存储中的错误问题。</li>
<li>results: 实验结果表明，这种解决方案在DNA数据存储方面具有竞争力，具有更高的压缩率和更好的雑音抗性。<details>
<summary>Abstract</summary>
DNA exhibits remarkable potential as a data storage solution due to its impressive storage density and long-term stability, stemming from its inherent biomolecular structure. However, developing this novel medium comes with its own set of challenges, particularly in addressing errors arising from storage and biological manipulations. These challenges are further conditioned by the structural constraints of DNA sequences and cost considerations. In response to these limitations, we have pioneered a novel compression scheme and a cutting-edge Multiple Description Coding (MDC) technique utilizing neural networks for DNA data storage. Our MDC method introduces an innovative approach to encoding data into DNA, specifically designed to withstand errors effectively. Notably, our new compression scheme overperforms classic image compression methods for DNA-data storage. Furthermore, our approach exhibits superiority over conventional MDC methods reliant on auto-encoders. Its distinctive strengths lie in its ability to bypass the need for extensive model training and its enhanced adaptability for fine-tuning redundancy levels. Experimental results demonstrate that our solution competes favorably with the latest DNA data storage methods in the field, offering superior compression rates and robust noise resilience.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effect-of-hyperparameters-on-variable-selection-in-random-forests"><a href="#Effect-of-hyperparameters-on-variable-selection-in-random-forests" class="headerlink" title="Effect of hyperparameters on variable selection in random forests"></a>Effect of hyperparameters on variable selection in random forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06943">http://arxiv.org/abs/2309.06943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imbs-hl/rf-hyperparameters-and-variable-selection">https://github.com/imbs-hl/rf-hyperparameters-and-variable-selection</a></li>
<li>paper_authors: Cesaire J. K. Fouodo, Lea L. Kronziel, Inke R. König, Silke Szymczak</li>
<li>for: 这个论文的目的是为了研究 Random Forest 算法中的 hyperparameter 的影响，以及这些参数对变量选择的影响。</li>
<li>methods: 这个论文使用了两个 simulation studies，一个使用理论分布，一个使用实际的基因表达数据，来评估 Random Forest 算法中的 hyperparameter 对变量选择的影响。</li>
<li>results: 研究发现，Random Forest 算法中的 hyperparameter mtry.prop 和 sample.fraction 对变量选择有着很大的影响，而 drawing strategy 和 minimal terminal node size 对变量选择的影响较小。此外，在不同的数据 correlations  Structures 下，适合的 hyperparameter 设置也会不同。<details>
<summary>Abstract</summary>
Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the correlation structure in the data. For weakly correlated predictor variables, the default value of mtry is optimal, but smaller values of sample.fraction result in larger sensitivity. In contrast, the difference in sensitivity of the optimal compared to the default value of sample.fraction is negligible for strongly correlated predictor variables, whereas smaller values than the default are better in the other settings. In conclusion, the default values of the hyperparameters will not always be suitable for identifying important variables. Thus, adequate values differ depending on whether the aim of the study is optimizing prediction performance or variable selection.
</details>
<details>
<summary>摘要</summary>
Random forests (RFs) 适用于预测模型和变量选择高维Omics研究中。RF算法中的hyperparameter对预测性能和变量重要性估计的影响已经被研究过。但是，RF算法中hyperparameter对变量选择的影响尚未清楚。我们通过两个 simulations studies使用理论分布和实际基因表达数据来评估hyperparameter的影响。我们评估选择重要变量的能力（敏感度），同时控制false discovery rate（FDR）。我们的结果表明，在training集中的分割候选变量的比例（mtry.prop）和训练集的样本分数（sample.fraction）对选择过程产生更大的影响，而不是在训练集中的抽样策略和最小终节点大小。一个适合的RF hyperparameter设置取决于数据中变量之间的相关性。对弱相关变量预测器，默认值的mtry是优化的，但是较小的sample.fraction会导致更大的敏感度。相反，对强相关变量预测器，默认值的sample.fraction的差异对预测性能没有很大的影响，但是较小的值比默认值更好。因此， defaults 的 hyperparameter 并不总是适合用于确定重要变量。因此，适合的 hyperparameter 的值取决于研究的目标是优化预测性能还是变量选择。
</details></li>
</ul>
<hr>
<h2 id="Collectionless-Artificial-Intelligence"><a href="#Collectionless-Artificial-Intelligence" class="headerlink" title="Collectionless Artificial Intelligence"></a>Collectionless Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06938">http://arxiv.org/abs/2309.06938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Gori, Stefano Melacci</li>
<li>for: 本研究目的是提出一种新的学习协议，使机器学习能够像人类一样在环境交互中获得智能能力。</li>
<li>methods: 本研究使用了环境互动中的数据来更新内部环境表示，不允许机器记录时间流信息。</li>
<li>results: 该方法可以推动机器学习发展自组织记忆技能，并且可以更好地处理隐私、控制和个性化问题。<details>
<summary>Abstract</summary>
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of self-organized memorization skills at a more abstract level, instead of relying on bare storage to simulate learning dynamics that are typical of offline learning algorithms. This purposely extreme position is intended to stimulate the development of machines that learn to dynamically organize the information by following human-based schemes. The proposition of this challenge suggests developing new foundations on computational processes of learning and reasoning that might open the doors to a truly orthogonal competitive track on AI technologies that avoid data accumulation by design, thus offering a framework which is better suited concerning privacy issues, control and customizability. Finally, pushing towards massively distributed computation, the collectionless approach to AI will likely reduce the concentration of power in companies and governments, thus better facing geopolitical issues.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)总之，职业处理大量数据的能力被认为是机器学习的基本成分，以及相关领域的spectacular achievements，同时也有一致的意见是关于中央数据收集的风险。这篇论文支持 Machine learning protocols that machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Therefore, it promotes the development of self-organized memorization skills at a more abstract level, instead of relying on bare storage to simulate learning dynamics that are typical of offline learning algorithms. This purposely extreme position is intended to stimulate the development of machines that learn to dynamically organize the information by following human-based schemes. The proposal of this challenge suggests developing new foundations on computational processes of learning and reasoning that might open the doors to a truly orthogonal competitive track on AI technologies that avoid data accumulation by design, thus offering a framework which is better suited concerning privacy issues, control and customizability. Finally, pushing towards massively distributed computation, the collectionless approach to AI will likely reduce the concentration of power in companies and governments, thus better facing geopolitical issues.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Dislocation-Dynamics-Data-Using-Semantic-Web-Technologies"><a href="#Modeling-Dislocation-Dynamics-Data-Using-Semantic-Web-Technologies" class="headerlink" title="Modeling Dislocation Dynamics Data Using Semantic Web Technologies"></a>Modeling Dislocation Dynamics Data Using Semantic Web Technologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06930">http://arxiv.org/abs/2309.06930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Zainul Ihsan, Said Fathalla, Stefan Sandfeld</li>
<li>for: 这个论文主要针对材料科学和工程领域中的晶体材料，包括金属和半导体材料。晶体材料中的一种常见缺陷是“杂点”，这种缺陷会影响材料的强度、裂解强度和塑性。</li>
<li>methods: 这篇论文使用了semantic web技术来模型来自杂点动力学 simulations的数据，并使用ontology来注解数据。研究人员将已有的杂点ontology扩展，添加了缺失的概念并与其他两个领域相关的 ontology（Elementary Multi-perspective Material Ontology和Materials Design Ontology）进行了对应，以便有效地表示杂点动力学数据。</li>
<li>results: 这篇论文通过创建了一个知识图表（DisLocKG）来表示杂点动力学数据的关系，并开发了一个SPARQL终结点，以便高度灵活地查询DisLocKG。<details>
<summary>Abstract</summary>
Research in the field of Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a distinct type of defect called "dislocation". This defect significantly affects various material properties, including strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behavior through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modeled using semantic web technologies through annotating data with ontologies. We extend the already existing Dislocation Ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspective Material Ontology and the Materials Design Ontology) allowing for representing the dislocation simulation data efficiently. Moreover, we show a real-world use case by representing the discrete dislocation dynamics data as a knowledge graph (DisLocKG) that illustrates the relationship between them. We also developed a SPARQL endpoint that brings extensive flexibility to query DisLocKG.
</details>
<details>
<summary>摘要</summary>
研究在材料科学和工程领域的重点是设计、合成、性能和表现的材料。一种广泛研究的材料是晶体材料，包括金属和半导体。晶体材料通常含有一种特定的缺陷，即“杂点”。这种缺陷对材料的各种性能产生重要影响，如强度、裂变强度和柔性。研究人员在过去几年中对杂点行为进行了大量的研究，包括实验测量技术和模拟。本文介绍了如何使用Semantic Web技术来模型来自杂点动力学 simulations的数据，包括使用ontology进行数据注释。我们将已经存在的杂点ontology扩展，添加缺失的概念并与其他两个领域相关的 ontology（即多元素物理材料 ontology和材料设计 ontology）进行对应，以便有效地表示杂点动力学数据。此外，我们还构建了一个知识图（DisLocKG），用于表示杂点动力学数据的关系。此外，我们还开发了一个 SPARQL 终点，以便高度灵活地查询 DisLocKG。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Impact-of-Action-Representations-in-Policy-Gradient-Algorithms"><a href="#Investigating-the-Impact-of-Action-Representations-in-Policy-Gradient-Algorithms" class="headerlink" title="Investigating the Impact of Action Representations in Policy Gradient Algorithms"></a>Investigating the Impact of Action Representations in Policy Gradient Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06921">http://arxiv.org/abs/2309.06921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Schneider, Pierre Schumacher, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler</li>
<li>for: 这篇论文探讨了在强化学习中行为表示的影响，以及不同分析技术的效果。</li>
<li>methods: 论文使用了多种分析技术，包括行为表示的变化对学习性能的影响。</li>
<li>results: 实验结果显示，行动表示可以很大程度上影响强化学习算法的学习性能，一些性能差异可以归因于优化困难度的变化。<details>
<summary>Abstract</summary>
Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continual-Learning-with-Dirichlet-Generative-based-Rehearsal"><a href="#Continual-Learning-with-Dirichlet-Generative-based-Rehearsal" class="headerlink" title="Continual Learning with Dirichlet Generative-based Rehearsal"></a>Continual Learning with Dirichlet Generative-based Rehearsal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06917">http://arxiv.org/abs/2309.06917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Zeng, Wei Xue, Qifeng Liu, Yike Guo</li>
<li>for: 这篇论文主要是为了解决数据驱动任务对话系统（ToDs）的不断学习问题，特别是因为计算限制和时间consuming的问题。</li>
<li>methods: 这篇论文提出了一种新的生成基于策略，称为Dirichlet Continual Learning（DCL），它使用Dirichlet分布来模型 latent prior 变量，从而高效地捕捉上一个任务的句子水平特征。此外， authors 还提出了 Jensen-Shannon Knowledge Distillation（JSKD）方法，用于增强知识传递 During pseudo sample 生成。</li>
<li>results:  experiments 表明，DCL 方法在意图检测和插入检测任务中表现出色，超过了现有方法的性能。<details>
<summary>Abstract</summary>
Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based knowledge distillation method that enhances knowledge transfer during pseudo sample generation. Our experiments confirm the efficacy of our approach in both intent detection and slot-filling tasks, outperforming state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-the-TopMost-A-Topic-Modeling-System-Toolkit"><a href="#Towards-the-TopMost-A-Topic-Modeling-System-Toolkit" class="headerlink" title="Towards the TopMost: A Topic Modeling System Toolkit"></a>Towards the TopMost: A Topic Modeling System Toolkit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06908">http://arxiv.org/abs/2309.06908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bobxwu/topmost">https://github.com/bobxwu/topmost</a></li>
<li>paper_authors: Xiaobao Wu, Fengjun Pan, Anh Tuan Luu</li>
<li>for: 这篇论文主要是为了提供一个全面的话题模型系统工具套件（Topic Modeling System Toolkit，简称TopMost），用于促进话题模型的研究和应用。</li>
<li>methods: 这篇论文使用了一种高度吸收和解耦的模块化设计，覆盖了更广泛的话题模型场景，包括数据预处理、模型训练、测试和评估。</li>
<li>results: 相比现有的工具套件，TopMost可以促进话题模型的快速使用、公正比较和扩展。这将促进话题模型的研究和应用进展。<details>
<summary>Abstract</summary>
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-Aware-Augmentations-for-Unsupervised-Online-General-Continual-Learning"><a href="#Domain-Aware-Augmentations-for-Unsupervised-Online-General-Continual-Learning" class="headerlink" title="Domain-Aware Augmentations for Unsupervised Online General Continual Learning"></a>Domain-Aware Augmentations for Unsupervised Online General Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06896">http://arxiv.org/abs/2309.06896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Romain Negrel, Giovanni Chierchia, Jean-François Bercher</li>
<li>for: 提高Unsupervised Online General Continual Learning (UOGCL)中的学习Agent忘记度，尤其是在没有类boundary或任务变化信息的情况下。</li>
<li>methods: 提出了一种使用流程依赖的数据增强和一些实现技巧来增强对比学习的内存使用情况，以提高UOGCL中的学习效果。</li>
<li>results: 比较其他无监督方法，本方法在所有考虑的设置中实现了最佳效果，并将监督学习和无监督学习之间的差异降低到最低。domain-aware增强程序可以与其他回放基于方法结合使用，这使得本方法成为可行的循环学习策略。<details>
<summary>Abstract</summary>
Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: continual learning 具有挑战，特别是在无监督enario中，如无监督在线通用 continual learning (UOGCL) 中，学习代理没有任务变化信息或类别边界信息的先前知识。而过去的研究主要集中在降低监督setup中的忘却。 latest studies have shown that self-supervised learners are more resistant to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well.
</details></li>
</ul>
<hr>
<h2 id="MagiCapture-High-Resolution-Multi-Concept-Portrait-Customization"><a href="#MagiCapture-High-Resolution-Multi-Concept-Portrait-Customization" class="headerlink" title="MagiCapture: High-Resolution Multi-Concept Portrait Customization"></a>MagiCapture: High-Resolution Multi-Concept Portrait Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06895">http://arxiv.org/abs/2309.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junha Hyung, Jaeyo Shin, Jaegul Choo</li>
<li>for: 这篇论文是针对如何使用几张自然像素来生成高品质的人像照片，并且将主题和风格概念融合到一起。</li>
<li>methods: 这篇论文使用了一种名为MagiCapture的个性化方法，通过将主题和风格概念融合到一起，生成高分辨率的人像照片。</li>
<li>results: 根据论文的评估，MagiCapture可以生成高品质的人像照片，并且比其他基准更高。它还可以在其他非人类物品上进行应用。<details>
<summary>Abstract</summary>
Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects.
</details>
<details>
<summary>摘要</summary>
大规模的文本到图像模型，包括稳定扩散，能够生成高品质、实际的人脸图像。有一个活跃的研究领域专门用于个性化这些模型，以生成特定主题或风格使用提供的参考图像。然而，尽管这些个性化方法可能会生成可信的结果，但它们通常会生成图像，具有不够的真实感和 comercial viability。这是特别明显在人脸图像生成中，因为人类的面部特征具有强烈的人类偏好。为解决这一问题，我们介绍了 MagiCapture，一种基于主题和风格概念的个性化方法，可以使用只需要几张主题和风格参考图像来生成高分辨率的人脸图像。例如，通过一些随机的自拍照，我们的精度调整后的模型可以生成高质量的人脸图像，例如护照照片或profile照片。主要挑战在这个任务中是缺乏compose的ground truth，导致最终输出质量下降和源主题的标识混乱。为解决这些问题，我们提出了一种新的注意力重新定向损失，以及auxiliary priors，它们都可以在弱相关学习Setting中Robust learning。我们的管道还包括额外的后处理步骤，以确保创造出高度真实的输出。 MagiCapture在量化和质量上的评价中表现出色，并且可以扩展到其他非人物对象。
</details></li>
</ul>
<hr>
<h2 id="Keep-It-SimPool-Who-Said-Supervised-Transformers-Suffer-from-Attention-Deficit"><a href="#Keep-It-SimPool-Who-Said-Supervised-Transformers-Suffer-from-Attention-Deficit" class="headerlink" title="Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?"></a>Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06891">http://arxiv.org/abs/2309.06891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bill Psomas, Ioannis Kakogeorgiou, Konstantinos Karantzalos, Yannis Avrithis<br>for:This paper aims to improve the performance of both convolutional and transformer encoders by developing a generic pooling framework and a simple attention-based pooling mechanism called SimPool.methods:The paper uses a generic pooling framework to formulate existing methods as instantiations and derives SimPool, a simple attention-based pooling mechanism that improves performance on pre-training and downstream tasks.results:The paper finds that SimPool improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases, regardless of whether the training is supervised or self-supervised. Additionally, the paper obtains attention maps of at least as good quality as self-supervised, without explicit losses or modifying the architecture, which is a new contribution.<details>
<summary>Abstract</summary>
Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?   In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.
</details>
<details>
<summary>摘要</summary>
“卷积网络和视transformer具有不同的对比式交互方式，包括层内卷积和网络结束的卷积。后者是否真的需要不同？作为卷积的副产品，视transformer提供了自然语言的空间注意力，但这通常是低质量的，除非是自我超视，这并未被研究得够。是超级视还是视的问题呢？在这项工作中，我们开发了一个通用的卷积框架，然后将一些现有的方法视为实体的实现。通过对每个组方法的质量进行讨论，我们 derivate SimPool，一种简单的注意力基于卷积机制，用于取代 convolutional和transformer核心Encoder中的默认卷积方法。我们发现，无论是有监督或自我监督，这种方法可以提高预训练和下游任务的性能，并提供了对象边界的注意力图。因此，我们可以称SimPool为通用的。我们知道，我们是第一个在有监督的情况下，通过不修改架构和不使用显式损失函数，从transformer中获得了至少等效的注意力图。代码在：https://github.com/billpsomas/simpool。”
</details></li>
</ul>
<hr>
<h2 id="ProMap-Datasets-for-Product-Mapping-in-E-commerce"><a href="#ProMap-Datasets-for-Product-Mapping-in-E-commerce" class="headerlink" title="ProMap: Datasets for Product Mapping in E-commerce"></a>ProMap: Datasets for Product Mapping in E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06882">http://arxiv.org/abs/2309.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kateřina Macková, Martin Pilát<br>for:这两个 datasets 用于评估产品 mapping 模型的性能，以填充现有的数据集中存在的缺失或只包含远方的产品对。methods:这两个 datasets 包含了图像和文本描述产品特性，并且通过两个零售商的爬虫抓取。非匹配产品被选择在两个阶段，创造了两种类型的非匹配 – 近似非匹配和中等非匹配。results:这两个 datasets 是一个完整的产品 mapping 数据集，可以用于评估和进一步研究产品 mapping 模型。这些数据集包含了许多详细的产品特性，例如品牌和价格，使得它们成为了产品 mapping 领域的黄金标准。<details>
<summary>Abstract</summary>
The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types of non-matches -- close non-matches and medium non-matches. Even the medium non-matches are pairs of products that are much more similar than non-matches in other datasets -- for example, they still need to have the same brand and similar name and price. After simple data preprocessing, several machine learning algorithms were trained on these and two the other datasets to demonstrate the complexity and completeness of ProMap datasets. ProMap datasets are presented as a golden standard for further research of product mapping filling the gaps in existing ones.
</details>
<details>
<summary>摘要</summary>
目标是判断两个电商平台上的两个产品是否描述同一种产品。现有的匹配和不匹配产品集合经常受到产品信息的不完整性或者只包含很遥距的不匹配产品的影响，因此训练在这些集合上的预测模型可以达到好的结果，但在实践中无法 distinguishing 非常相似的 но不匹配产品。本文介绍了两个新的产品映射 datasets：ProMapCz 和 ProMapEn，它们分别包含 1,495 个捷克产品对和 1,555 个英文产品对，由两个电商平台上的匹配和不匹配产品手动抽取。这些 datasets 包含产品图片和文本描述，包括产品规格信息，使其成为目前最完整的产品映射 datasets。此外，非匹配产品被选择在两个阶段，创造了两种类型的非匹配产品： close non-matches 和 medium non-matches。即使medium non-matches 与其他 datasets 中的非匹配产品不同，它们仍需要具有同一个品牌和相似的名称和价格。经过简单的数据处理后，多种机器学习算法被训练在这些和两个其他 datasets 上，以示 ProMap datasets 的复杂性和完整性。ProMap datasets 被提出为未来研究产品映射的 golden standard，填充现有的空白。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-control-of-self-assembly-of-quasicrystalline-structures-through-reinforcement-learning"><a href="#Dynamic-control-of-self-assembly-of-quasicrystalline-structures-through-reinforcement-learning" class="headerlink" title="Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning"></a>Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06869">http://arxiv.org/abs/2309.06869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uyen Tu Lieu, Natsuhiko Yoshinaga</li>
<li>for: 控制动力自组装的多十二角晶体（DDQC）从粒子杂化的方向</li>
<li>methods: 使用反射学习控制温度，通过Q学习方法获得最佳策略</li>
<li>results: 通过反射学习获得的温度规则可以更高效地生成DDQC，并且可以避免缺陷。<details>
<summary>Abstract</summary>
We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance the chance of forming a globally stable state. The estimated policy guides the system toward the critical temperature to assist the formation of DDQC.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们提议使用回归学习控制patchy particle自组织成dodecagonal quasi-crystal（DDQC）。patchy particle之间的相互作用具有方向性，并且形成DDQC。然而，其稳态结构受到其结构形成的动力学路径的影响。我们使用Q学习方法来估算最佳的温度控制策略，并示出我们可以使用这种策略来生成DDQC几乎无损。我们通过对一个简单的三个凹坑潜在能障的模型进行分析，发现回归学习自动发现了具有增强 globally stable 状态的温度极限。我们认为，这种温度控制策略可以帮助系统形成DDQC。
</details></li>
</ul>
<hr>
<h2 id="Supervised-Machine-Learning-and-Physics-based-Machine-Learning-approach-for-prediction-of-peak-temperature-distribution-in-Additive-Friction-Stir-Deposition-of-Aluminium-Alloy"><a href="#Supervised-Machine-Learning-and-Physics-based-Machine-Learning-approach-for-prediction-of-peak-temperature-distribution-in-Additive-Friction-Stir-Deposition-of-Aluminium-Alloy" class="headerlink" title="Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy"></a>Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06838">http://arxiv.org/abs/2309.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshansh Mishra</li>
<li>for: 本研究旨在掌握Additive friction stir deposition（AFSD）过程参数与微结构之间的关系，以便优化AFSD过程以获得desired properties。</li>
<li>methods: 本研究使用了 cutting-edge框架， combining supervised machine learning（SML）和physics-informed neural networks（PINNs）来预测AFSD过程中热点 distribuition的 peak temperature。</li>
<li>results: 研究发现， ensemble techniques like gradient boosting 在 SML 中表现最佳，具有最低MSE值（165.78）。此外，通过融合数据驱动学和基本物理学，这种双重方法可以提供AFSD过程中热点分布的全面理解，并且可以用来调整微结构以获得desired properties。<details>
<summary>Abstract</summary>
Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied to classify deposition quality from process factors, with logistic regression delivering robust accuracy. By fusing data-driven learning and fundamental physics, this dual methodology provides comprehensive insights into tailoring microstructure through thermal management in AFSD. The work demonstrates the power of bridging statistical and physics-based modeling for elucidating AM process-property relationships.
</details>
<details>
<summary>摘要</summary>
添加性摩擦挤出（AFSD）是一种新的固体添加制造技术，可以避免传统粉末压缩和导向能量激发的问题，如缺陷、裂缝和性能不均。然而，AFSD中的过程参数与温度分布、结果结构之间的关系仍然不够了解。这难以优化过程以获得适合的性能。这项工作使用了前沿的框架，结合监督学习（SML）和物理学习网络（PINNs），预测AFSD中过程参数的温度分布。这里使用了8种回归算法来实现SML模型，而PINNs则利用了运动、声速、热传导和量子力学的普遍方程。在多个统计度量上，ensemble技术如折衣加boosting表现最佳，最低MSE为165.78。此外，这种集成的ML方法还被应用于分类过程参数对应的材料质量，使用了логистиック回归得到了可靠的准确率。通过融合数据驱动学习和基本物理学习，这种双重方法提供了全面的理解添加制造过程中的热管理对微structure的影响，从而透视AM过程-性能关系。这项工作表明了将统计学和物理学基础模型融合的力量，用于解释添加制造过程中的关键关系。
</details></li>
</ul>
<hr>
<h2 id="Safe-Reinforcement-Learning-with-Dual-Robustness"><a href="#Safe-Reinforcement-Learning-with-Dual-Robustness" class="headerlink" title="Safe Reinforcement Learning with Dual Robustness"></a>Safe Reinforcement Learning with Dual Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06835">http://arxiv.org/abs/2309.06835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Yunan Wang, Yujie Yang, Shengbo Eben Li</li>
<li>for: 本研究旨在解决强化学习（RL）代理人面临恶性干扰时的安全性和可靠性问题。</li>
<li>methods: 本研究提出了一种系统性框架，该框架将安全RL和稳健RL融合在一起，包括问题设定、迭代方案、收敛分析以及实践算法设计。此外，本研究还提出了一种深度RL算法，称为双重稳健actor-critic（DRAC）。</li>
<li>results: 根据安全性重要的标准彩色数据集评估，DRAC算法在所有情况下（无恶意对手、安全对手、性能对手）表现出了高性能和持续的安全性，与所有基elines显著升级。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) agents are vulnerable to adversarial disturbances, which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneously optimizes a task policy and a safety policy. The convergence of this iteration scheme is proved. Furthermore, we design a deep RL algorithm for practical implementation, called dually robust actor-critic (DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC achieves high performance and persistent safety under all scenarios (no adversary, safety adversary, performance adversary), outperforming all baselines significantly.
</details>
<details>
<summary>摘要</summary>
�� Reinforcement learning (RL) ��� engine ��� vulnerable to adversarial disturbances, ��� which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneously optimizes a task policy and a safety policy. The convergence of this iteration scheme is proved. Furthermore, we design a deep RL algorithm for practical implementation, called dually robust actor-critic (DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC achieves high performance and persistent safety under all scenarios (no adversary, safety adversary, performance adversary), outperforming all baselines significantly.
</details></li>
</ul>
<hr>
<h2 id="UniBrain-Universal-Brain-MRI-Diagnosis-with-Hierarchical-Knowledge-enhanced-Pre-training"><a href="#UniBrain-Universal-Brain-MRI-Diagnosis-with-Hierarchical-Knowledge-enhanced-Pre-training" class="headerlink" title="UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training"></a>UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06828">http://arxiv.org/abs/2309.06828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljy19970415/unibrain">https://github.com/ljy19970415/unibrain</a></li>
<li>paper_authors: Jiayu Lei, Lisong Dai, Haoyun Jiang, Chaoyi Wu, Xiaoman Zhang, Yao Zhang, Jiangchao Yao, Weidi Xie, Yanyong Zhang, Yuehua Li, Ya Zhang, Yanfeng Wang</li>
<li>for: 这 paper 旨在提出一种基于大规模数据集的高效缩放性脑磁共振成像诊断方法，以提高脑病诊断的准确率和效率。</li>
<li>methods: 该方法利用了大规模的成像报告对策，并建立了一种层次结构的知识匹配机制，以强化特征学习效果。</li>
<li>results: 对于三个实际 dataset 和 BraTS2019 公共数据集，UniBrain 能够一致性地超越所有现有的诊断方法，并在某些疾病类型上达到了专业放射学家的性能水平。<details>
<summary>Abstract</summary>
Magnetic resonance imaging~(MRI) have played a crucial role in brain disease diagnosis, with which a range of computer-aided artificial intelligence methods have been proposed. However, the early explorations usually focus on the limited types of brain diseases in one study and train the model on the data in a small scale, yielding the bottleneck of generalization. Towards a more effective and scalable paradigm, we propose a hierarchical knowledge-enhanced pre-training framework for the universal brain MRI diagnosis, termed as UniBrain. Specifically, UniBrain leverages a large-scale dataset of 24,770 imaging-report pairs from routine diagnostics. Different from previous pre-training techniques for the unitary vision or textual feature, or with the brute-force alignment between vision and language information, we leverage the unique characteristic of report information in different granularity to build a hierarchical alignment mechanism, which strengthens the efficiency in feature learning. Our UniBrain is validated on three real world datasets with severe class imbalance and the public BraTS2019 dataset. It not only consistently outperforms all state-of-the-art diagnostic methods by a large margin and provides a superior grounding performance but also shows comparable performance compared to expert radiologists on certain disease types.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparative-Analysis-of-Contextual-Relation-Extraction-based-on-Deep-Learning-Models"><a href="#Comparative-Analysis-of-Contextual-Relation-Extraction-based-on-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models"></a>Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06814">http://arxiv.org/abs/2309.06814</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Priyadharshini, G. Jeyakodi, P. Shanthi Bala</li>
<li>for: constructing a knowledge graph with the help of ontology, performing various tasks such as semantic search, query answering, and textual entailment</li>
<li>methods: deep learning techniques, including hybrid models, to extract relations from complex sentences effectively</li>
<li>results: more accurate and efficient relation extraction, particularly for complex sentences with multiple relations and unspecified entities<details>
<summary>Abstract</summary>
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are not suited for complex sentences that consist of the words that have various meanings. To address these issues, hybrid deep learning models have been used to extract the relations from complex sentence effectively. This paper explores the analysis of various deep learning models that are used for relation extraction.
</details>
<details>
<summary>摘要</summary>
Contextual Relation Extraction (CRE) 主要用于构建知识图库，帮助 ontology 中的实体之间建立关系。它完成了多种任务，如semantic search、查询回答和文本推理。关系提取可以从原始文本中提取实体和其间的关系。在生物医学领域，一个高效和准确的 CRE 系统是建立领域知识的关键。现有的机器学习和自然语言处理（NLP）技术不适用于 efficiently 预测复杂关系从多个句子中。为解决这些问题，深度学习技术被用来确定上下文中的相应含义。虽然多种机器学习模型已经用于关系提取，但它们只能提供二元关系（即 sentence 中的两个实体之间的关系）的更好结果。机器学习模型不适用于包含多个意思的词语的复杂句子。为了解决这些问题， hybrid deep learning 模型被用来从复杂句子中提取关系。本文探讨了不同深度学习模型的关系提取分析。
</details></li>
</ul>
<hr>
<h2 id="FedDIP-Federated-Learning-with-Extreme-Dynamic-Pruning-and-Incremental-Regularization"><a href="#FedDIP-Federated-Learning-with-Extreme-Dynamic-Pruning-and-Incremental-Regularization" class="headerlink" title="FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization"></a>FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06805">http://arxiv.org/abs/2309.06805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericloong/feddip">https://github.com/ericloong/feddip</a></li>
<li>paper_authors: Qianyu Long, Christos Anagnostopoulos, Shameem Puthiya Parambath, Daning Bi</li>
<li>for: 这个论文旨在提出一个基于 Federated Learning 的新框架，可以在分布式训练和推导大规模深度神经网络时，实现高精度和快速训练。</li>
<li>methods: 这个框架使用了动态模型剔除和错误反馈来删除无用的资讯交换，并且运用了增量调整来实现极高精度模型。</li>
<li>results: 在比较以前的方法和其他模型剔除方法时，这个框架能够高效地控制模型精度，并且能够在分布式训练中实现类似或更好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against state-of-the-art methods using benchmark data sets and DNN models. Our results showcase that FedDIP not only controls the model sparsity but efficiently achieves similar or better performance compared to other model pruning methods adopting incremental regularization during distributed model training. The code is available at: https://github.com/EricLoong/feddip.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）已成功应用于分布式训练和推理大规模深度神经网络（DNN）。然而，DNN具有极高的参数数量，从而导致参数之间的交换和内存管理具有极大的挑战。虽然最近的DNN压缩方法（例如减少和截断）解决了这些挑战，但它们不总是考虑灵活控制参数交换的减少而保持高精度水平。为此，我们提出了一种新的FL框架（名为FedDIP），它结合（i）动态模型剪辑和错误反馈来消除重复的信息交换，从而使得性能得到显著改善，并且（ii）逐步REG regularization可以实现极高精度的模型。我们对FedDIP的收敛分析和性能比较分析，并使用标准数据集和DNN模型进行广泛的性能评估。我们的结果表明，FedDIP不仅可以控制模型精度，而且能够高效地实现与其他模型剪辑方法在分布式模型训练中的同等或更好的性能。代码可以在以下地址下获取：https://github.com/EricLoong/feddip。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Traffic-Prediction-under-Missing-Data"><a href="#Uncertainty-aware-Traffic-Prediction-under-Missing-Data" class="headerlink" title="Uncertainty-aware Traffic Prediction under Missing Data"></a>Uncertainty-aware Traffic Prediction under Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06800">http://arxiv.org/abs/2309.06800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Mei, Junxian Li, Zhiming Liang, Guanjie Zheng, Bin Shi, Hua Wei</li>
<li>for: 预测交通流量是 Transportation 领域中的一个关键问题，因为它有广泛的应用场景。最近几年，许多研究都取得了显著的成果，但大多数研究假设预测地点有完整或至少部分的历史记录，无法扩展到没有历史记录的地点。在实际应用中，投入感知器的部署可能受到预算限制和安装可用性的限制，使得现有的模型不适用。</li>
<li>methods: 该研究基于前一代步骤 inductive graph neural network 的思想，提出了一个能够扩展预测至缺失记录的地点，同时生成 probabilistic 预测和uncertainty quantification，以帮助管理风险和决策。</li>
<li>results: 通过对实际数据进行广泛的实验，研究结果表明，我们的方法在预测任务中取得了显著的成果，并且uncertainty quantification 的结果与历史数据和无历史数据的地点之间呈高度相关。此外，我们的模型还能够帮助 optimize sensor deployment 任务，以实现更高的准确率。<details>
<summary>Abstract</summary>
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-aware framework with the ability to 1) extend prediction to missing locations with no historical records and significantly extend spatial coverage of prediction locations while reducing deployment of sensors and 2) generate probabilistic prediction with uncertainty quantification to help the management of risk and decision making in the down-stream tasks. Through extensive experiments on real-life datasets, the result shows our method achieved promising results on prediction tasks, and the uncertainty quantification gives consistent results which highly correlated with the locations with and without historical data. We also show that our model could help support sensor deployment tasks in the transportation field to achieve higher accuracy with a limited sensor deployment budget.
</details>
<details>
<summary>摘要</summary>
宽泛应用于交通领域的交通预测是一个关键的话题，因为它的广泛应用可以提高交通效率和安全性。在过去几年中，许多研究已经取得了成功的结果。然而，大多数研究假设预测位置具有完整或至少部分的历史记录，无法扩展到没有历史记录的位置。在实际应用中，投放感知器的限制可能会导致大多数当前模型无法应用。虽然一些文献尝试了填充交通状态的方法，但这些方法需要同时观察的数据，使得它们无法应用于预测任务。另外，当前的预测模型缺乏量化不确定性的能力，使得过去的作品不适用于风险敏感任务或决策过程。为了填补这一空白，我们受到过去的卷积图 neural network 的启发，提出了一个不确定性意识框架，能够1) 扩展预测到缺失历史记录的位置，大幅减少感知器的投放，并大幅提高预测位置的准确率，2) 生成概率预测，对于风险敏感任务和决策过程提供量化不确定性的支持。经过广泛的实验，我们的方法在预测任务中取得了出色的结果，不确定性评估与历史数据位置相吻合。此外，我们的模型还可以帮助交通领域中的感知器投放任务，以实现更高的准确率，即使有限的感知器投放预算。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Mirage-A-Review-of-Hallucinations-in-Large-Language-Models"><a href="#Cognitive-Mirage-A-Review-of-Hallucinations-in-Large-Language-Models" class="headerlink" title="Cognitive Mirage: A Review of Hallucinations in Large Language Models"></a>Cognitive Mirage: A Review of Hallucinations in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06794">http://arxiv.org/abs/2309.06794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongbinye/cognitive-mirage-hallucinations-in-llms">https://github.com/hongbinye/cognitive-mirage-hallucinations-in-llms</a></li>
<li>paper_authors: Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia</li>
<li>for: This paper is written for researchers and developers working on text generation systems, particularly those interested in understanding and addressing the issue of hallucinations in large language models (LLMs).</li>
<li>methods: The paper presents a novel taxonomy of hallucinations in text generation tasks, based on a detailed analysis of various examples and theoretical insights. It also discusses existing detection and improvement methods for hallucinations in LLMs.</li>
<li>results: The paper provides a comprehensive overview of hallucinations in LLMs, including a detailed taxonomy, theoretical analyses, and existing detection and improvement methods. It also proposes several future research directions that can be explored to address the issue of hallucinations in text generation systems.<details>
<summary>Abstract</summary>
As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
</details>
<details>
<summary>摘要</summary>
大语言模型在人工智能领域的发展中，文本生成系统容易受到一种关注的现象，即幻视。本研究将给出最近的有力关幻视在大语言模型中的新发现，并提供了一个新的分类法，以及幻视的理论分析、检测方法和改善方法。此外，我们还提出了未来的研究方向。我们的贡献有三个方面：1. 我们提供了文本生成任务中幻视的详细和完整的分类法；2. 我们提供了幻视在大语言模型中的理论分析，并提供了现有的检测和改善方法；3. 我们提出了未来的研究方向。当幻视在社区中受到重视时，我们将继续更新有关的研究进展。
</details></li>
</ul>
<hr>
<h2 id="Electricity-Demand-Forecasting-through-Natural-Language-Processing-with-Long-Short-Term-Memory-Networks"><a href="#Electricity-Demand-Forecasting-through-Natural-Language-Processing-with-Long-Short-Term-Memory-Networks" class="headerlink" title="Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks"></a>Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06793">http://arxiv.org/abs/2309.06793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Bai, Simon Camal, Andrea Michiorri</li>
<li>for: 预测英国国家电力需求</li>
<li>methods: 使用Long and Short-Term Memory（LSTM）网络，并将新闻文本特征纳入模型中</li>
<li>results: 研究发现，公众情绪和交通和地opolitics相关词语表达具有时间连续性效应，并且LSTM WITH textual features比基准值超过3%，并且比官方 benchmark超过10%。此外，提出的模型有效减少预测不确定性，缩短信息分布和确定性。<details>
<summary>Abstract</summary>
Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence interval and bringing the forecast distribution closer to the truth.
</details>
<details>
<summary>摘要</summary>
电力需求预测是一个已经very well established的研究领域。通常这个任务是通过历史负荷、天气预报、日历信息和已知的主要事件来完成。在最近，关注的新信息来源是从文本新闻中提取的。这篇论文提出了一种使用Long and Short-Term Memory（LSTM）网络，并在这个网络中添加文本新闻特征，成功地预测了英国国家电力需求的 deterministic 和 probabilistic 任务。研究发现，公众情绪和交通和地opolitics相关的词向量表示在电力需求中有时间连续性效应。实验结果表明，LSTM与文本特征相加的模型在相对评benchmark中提高了超过3%，并在官方benchmark中提高了接近10%。此外，提议的模型可以有效地减少预测不确定性，减小信息interval并使预测分布更接近真实。
</details></li>
</ul>
<hr>
<h2 id="Scalable-neural-network-models-and-terascale-datasets-for-particle-flow-reconstruction"><a href="#Scalable-neural-network-models-and-terascale-datasets-for-particle-flow-reconstruction" class="headerlink" title="Scalable neural network models and terascale datasets for particle-flow reconstruction"></a>Scalable neural network models and terascale datasets for particle-flow reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06782">http://arxiv.org/abs/2309.06782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joosep Pata, Eric Wulff, Farouk Mokhtar, David Southwick, Mengke Zhang, Maria Girone, Javier Duarte</li>
<li>for: 这个论文旨在开发一种可扩展的机器学习模型，用于高能电子- позитроン碰撞实验中的全事件重建。</li>
<li>methods: 该论文使用图гра树神经网络和核心变换器，以避免 quadratic memory allocation 和计算成本，同时实现有realistic的PF重建。</li>
<li>results: 研究发现，通过超级计算机进行hyperparameter优化，可以提高物理性能表现。此外，模型可以在不同的硬件处理器上高度可портабе，支持Nvidia、AMD和Intel Habana卡。最后，研究表明，使用高度粒度的轨迹和calorimeter射频输入，可以实现与基eline相当的物理性能。<details>
<summary>Abstract</summary>
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following the findable, accessible, interoperable, and reusable (FAIR) principles.
</details>
<details>
<summary>摘要</summary>
我们研究高可扩展机器学习模型以实现高能电子- позиトрон撞击中全事件重建，基于高度粒子化仪器模拟。流体（PF）重建可以表示为监督学习任务，使用轨迹和calorimeter层或hit。我们比较了图 neuron网络和基于kernel的transformer，并证明它们可以避免quadratic内存分配和计算成本，同时实现现实主义PF重建。我们表明了超参数调整在超级计算机上有 significanthysics性能提升。我们还表明了模型可以在不同的硬件处理器上高度可移植，支持Nvidia、AMD和Intel Habana卡。最后，我们示出了模型可以在高度粒子化输入上进行训练， resulting in competitive physics performance with the baseline。数据集和软件用于重现研究按照可找到、可达、可操作和可重用（FAIR）原则发布。
</details></li>
</ul>
<hr>
<h2 id="Fundamental-Limits-of-Deep-Learning-Based-Binary-Classifiers-Trained-with-Hinge-Loss"><a href="#Fundamental-Limits-of-Deep-Learning-Based-Binary-Classifiers-Trained-with-Hinge-Loss" class="headerlink" title="Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss"></a>Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06774">http://arxiv.org/abs/2309.06774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tilahun M. Getu, Georges Kaddoum</li>
<li>for: 本研究旨在解释深度学习（DL）在多个领域中的成功原因，并提供一个综合理解深度学习的理论基础。</li>
<li>methods: 本研究使用了优化、泛化和近似等方法。</li>
<li>results: 本研究提出了一种测试性能限制，用于评估深度学习基于折衣函数（ReLU）Feedforward神经网络（FNN）和深度FNN的测试性能。这些测试性能限制被 validate by extensive computer experiments。<details>
<summary>Abstract</summary>
Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedforward neural networks (FNNs) and ones that are based on deep FNNs with ReLU and Tanh activation, we derive their respective novel asymptotic testing performance limits. The derived testing performance limits are validated by extensive computer experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MTD-Multi-Timestep-Detector-for-Delayed-Streaming-Perception"><a href="#MTD-Multi-Timestep-Detector-for-Delayed-Streaming-Perception" class="headerlink" title="MTD: Multi-Timestep Detector for Delayed Streaming Perception"></a>MTD: Multi-Timestep Detector for Delayed Streaming Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06742">http://arxiv.org/abs/2309.06742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yulin1004/mtd">https://github.com/yulin1004/mtd</a></li>
<li>paper_authors: Yihui Huang, Ningjiang Chen</li>
<li>for: 提高自动驾驶系统的实时环境感知，以确保用户安全和体验。</li>
<li>methods: 提出了多时步探测器（MTD）和延迟分析模块（DAM），以及一种新的时间步分支模块（TBM），用于适应延迟波动。</li>
<li>results: 在Argoverse-HD dataset上进行了实验，并实现了在不同延迟设置下的state-of-the-art表现。<details>
<summary>Abstract</summary>
Autonomous driving systems require real-time environmental perception to ensure user safety and experience. Streaming perception is a task of reporting the current state of the world, which is used to evaluate the delay and accuracy of autonomous driving systems. In real-world applications, factors such as hardware limitations and high temperatures inevitably cause delays in autonomous driving systems, resulting in the offset between the model output and the world state. In order to solve this problem, this paper propose the Multi- Timestep Detector (MTD), an end-to-end detector which uses dynamic routing for multi-branch future prediction, giving model the ability to resist delay fluctuations. A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively predict specific timesteps according to the delay trend. The proposed method has been evaluated on the Argoverse-HD dataset, and the experimental results show that it has achieved state-of-the-art performance across various delay settings.
</details>
<details>
<summary>摘要</summary>
A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively predict specific timesteps according to the delay trend. The proposed method has been evaluated on the Argoverse-HD dataset, and the experimental results show that it has achieved state-of-the-art performance across various delay settings.Translation notes:* "streaming perception" is translated as "实时感知" (shízhí gǎngrán), which means "real-time perception" or "streaming sensing".* "delay" is translated as "延迟" (diànyì), which means "delay" or "lag".* "timestep" is translated as "时间步" (shíjiān bù), which means "time step" or "time increment".* "end-to-end detector" is translated as "端到端检测器" (dìngdào dào dīngkèshì), which means "end-to-end detector" or "full-stack detector".* "dynamic routing" is translated as "动态路由" (dòngtài lùyòu), which means "dynamic routing" or "adaptive routing".* "multi-branch future prediction" is translated as "多支未来预测" (duō zhī wèilái yùjì), which means "multi-branch future prediction" or "multi-path future prediction".* "delay trend" is translated as "延迟趋势" (diànyì xiàngxìng), which means "delay trend" or "delay pattern".* "Timestep Branch Module" is translated as "时间步分支模块" (shíjiān bù fēnzhī móudì), which means "time step branch module" or "time increment branch module".* "static flow" is translated as "静态流" (jìngtài liú), which means "static flow" or "stationary flow".* "adaptive flow" is translated as "适应流" (shìyìng liú), which means "adaptive flow" or "adaptive stream".
</details></li>
</ul>
<hr>
<h2 id="MCNS-Mining-Causal-Natural-Structures-Inside-Time-Series-via-A-Novel-Internal-Causality-Scheme"><a href="#MCNS-Mining-Causal-Natural-Structures-Inside-Time-Series-via-A-Novel-Internal-Causality-Scheme" class="headerlink" title="MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme"></a>MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06739">http://arxiv.org/abs/2309.06739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Liu, Dehui Du, Zihan Jiang, Anyan Huang, Yiyang Li</li>
<li>for: 本研究旨在探讨时序序列中的内在 causality，以提高人工神经网络（NN）的准确性和可读性。</li>
<li>methods: 该研究提出了一种名为 Mining Causal Natural Structure（MCNS）的新框架，可自动找到时序序列中的内在 causality 结构，并将其应用于 NN 中。</li>
<li>results: 实验结果表明，通过使用 MCNS 杜然 NN 的准确性和可读性，同时提供了更深入、固定的时序序列和数据概要。<details>
<summary>Abstract</summary>
Causal inference permits us to discover covert relationships of various variables in time series. However, in most existing works, the variables mentioned above are the dimensions. The causality between dimensions could be cursory, which hinders the comprehension of the internal relationship and the benefit of the causal graph to the neural networks (NNs). In this paper, we find that causality exists not only outside but also inside the time series because it reflects a succession of events in the real world. It inspires us to seek the relationship between internal subsequences. However, the challenges are the hardship of discovering causality from subsequences and utilizing the causal natural structures to improve NNs. To address these challenges, we propose a novel framework called Mining Causal Natural Structure (MCNS), which is automatic and domain-agnostic and helps to find the causal natural structures inside time series via the internal causality scheme. We evaluate the MCNS framework and impregnation NN with MCNS on time series classification tasks. Experimental results illustrate that our impregnation, by refining attention, shape selection classification, and pruning datasets, drives NN, even the data itself preferable accuracy and interpretability. Besides, MCNS provides an in-depth, solid summary of the time series and datasets.
</details>
<details>
<summary>摘要</summary>
causal inference 允许我们发现时间序列中变量之间的隐藏关系。然而，现有的大多数工作中的变量都是维度，这使得变量之间的相互关系受限，阻碍我们理解内部关系以及 causal graph 对神经网络 (NN) 的利用。在这篇文章中，我们发现时间序列中的 causality 不仅存在在外部，而且也存在在内部，因为它反映了实际世界中的事件顺序。这使我们感到需要检查内部 subsequences 之间的关系。然而，挑战是从 subsequences 中发现 causality 以及使用 causal natural structure 来改进 NN。为解决这些挑战，我们提出了一个新的框架called Mining Causal Natural Structure (MCNS)，它是自动化的、领域不依赖的，可以在时间序列中找到内部 causality scheme。我们评估了 MCNS 框架和使用 MCNS 修饰 NN 的时间序列分类任务。实验结果表明，我们的涂抹，通过修改注意力、形状选择分类和减少数据集，使得 NN 的准确率和可读性得到了改进。此外，MCNS 还提供了深入、坚实的时间序列和数据集概括。
</details></li>
</ul>
<hr>
<h2 id="Deep-Nonparametric-Convexified-Filtering-for-Computational-Photography-Image-Synthesis-and-Adversarial-Defense"><a href="#Deep-Nonparametric-Convexified-Filtering-for-Computational-Photography-Image-Synthesis-and-Adversarial-Defense" class="headerlink" title="Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense"></a>Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06724">http://arxiv.org/abs/2309.06724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Wangni</li>
<li>for:  recuperate the real scene from imperfect images</li>
<li>methods:  Deep Nonparametric Convexified Filtering (DNCF)</li>
<li>results:  defends image classification deep networks against adversary attack algorithms in real-time<details>
<summary>Abstract</summary>
We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
</details>
<details>
<summary>摘要</summary>
我们目标是提供一个通用的计算摄影框架，通过深度非 Parametric 几何 filtering (DNCF) 来重建真实场景从不完美的图像中。DNCF 包括一个非 Parametric 深度网络，用于模拟图像形成物理方程，如降噪、超分解、填充和闪光。由于DNCF 没有依赖于训练数据的参数化，因此具有强大的泛化和鲁棒性，可以防止对图像进行恶意修改。在推理过程中，我们还鼓励网络参数具有非负性，创建了输入和参数之间的双凸函数，这使得可以使用不够的运行时间的第二个优化算法进行加速，相比 Deep Image Prior 的10倍加速。通过这些工具，我们经验证明DNCF 可以在实时中防止图像分类深度网络受到攻击。
</details></li>
</ul>
<hr>
<h2 id="Bias-Amplification-Enhances-Minority-Group-Performance"><a href="#Bias-Amplification-Enhances-Minority-Group-Performance" class="headerlink" title="Bias Amplification Enhances Minority Group Performance"></a>Bias Amplification Enhances Minority Group Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06717">http://arxiv.org/abs/2309.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaotang Li, Jiarui Liu, Wei Hu</li>
<li>for: 这篇论文目的是提高对罕见分组的准确率，即使使用标准训练。</li>
<li>methods: 本文提出了一个名为BAM的两阶段训练算法，包括一个偏好增强方案和一个重新权重样本的步骤。</li>
<li>results: BAM在computer vision和自然语言处理中的伪 correlate测试中取得了竞争性的性能，并且提出了一个简单的停止条件，可以无需群体标注来获得优化的性能。<details>
<summary>Abstract</summary>
Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared with existing methods evaluated on spurious correlation benchmarks in computer vision and natural language processing. Moreover, we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations, with little or no loss in worst-group accuracy. We perform extensive analyses and ablations to verify the effectiveness and robustness of our algorithm in varying class and group imbalance ratios.
</details>
<details>
<summary>摘要</summary>
neuronal networks 生成出来的标准训练是知道低精度在罕见分组上，即使 дости得了平均标签的高精度，这是由某些偶极特征和标签之间的相关性引起的。现有的方法（如集群损失最小化）可以提高罕见分组的精度，但是需要训练样本集中的所有样本的集群注释。在这篇论文中，我们关注了较为具有挑战性和实际性的设定，即集群注释只有小 Validation 集中或者完全没有。我们提出了一种新的两阶段训练算法：在第一阶段，模型通过引入每个训练样本的学习可迭代变量进行偏好增强；在第二阶段，我们增重点的样本，并继续使用增重点的数据进行训练同样的模型。实际上，BAM 达到了与现有方法相当的性能，并且我们发现了一个简单的停止 criterion，可以根据最小类别差异来移除集群注释，而且几乎不会导致罕见分组的精度下降。我们进行了广泛的分析和缺陷分析，以证明我们的算法的效果和可靠性在不同的类和分组异质比例下。
</details></li>
</ul>
<hr>
<h2 id="Crystal-structure-prediction-using-neural-network-potential-and-age-fitness-Pareto-genetic-algorithm"><a href="#Crystal-structure-prediction-using-neural-network-potential-and-age-fitness-Pareto-genetic-algorithm" class="headerlink" title="Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm"></a>Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06710">http://arxiv.org/abs/2309.06710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadmanomee/ParetoCSP">https://github.com/sadmanomee/ParetoCSP</a></li>
<li>paper_authors: Sadman Sadeed Omee, Lai Wei, Jianjun Hu</li>
<li>for: 这篇论文的目的是提出一种新的晶体结构预测算法（ParetoCSP），用于解决晶体结构预测问题，并且使用了多目标遗传算法（MOGA）和神经网络间位能模型（IAP）来找到化学组成下的能量最优晶体结构。</li>
<li>methods: 该算法首先使用MOGA进行多目标优化，然后使用IAP模型来导向GA搜索，并且还含有一个改进后的NSGA-III算法，使用生物体龄作为独立的优化因素。</li>
<li>results: 与GN-OA算法相比，ParetoCSP显示出了明显的优势，在55个多样化的 bench mark结构上，通过7种性能指标进行评估，ParetoCSP的预测性能高于GN-OA的2.562倍。而且，对所有算法的搜索过程的轨迹分析表明，ParetoCSP生成了更多的有效结构，这有助于GA更好地搜索优质结构。<details>
<summary>Abstract</summary>
While crystal structure prediction (CSP) remains a longstanding challenge, we introduce ParetoCSP, a novel algorithm for CSP, which combines a multi-objective genetic algorithm (MOGA) with a neural network inter-atomic potential (IAP) model to find energetically optimal crystal structures given chemical compositions. We enhance the NSGA-III algorithm by incorporating the genotypic age as an independent optimization criterion and employ the M3GNet universal IAP to guide the GA search. Compared to GN-OA, a state-of-the-art neural potential based CSP algorithm, ParetoCSP demonstrated significantly better predictive capabilities, outperforming by a factor of $2.562$ across $55$ diverse benchmark structures, as evaluated by seven performance metrics. Trajectory analysis of the traversed structures of all algorithms shows that ParetoCSP generated more valid structures than other algorithms, which helped guide the GA to search more effectively for the optimal structures
</details>
<details>
<summary>摘要</summary>
“单晶结构预测（CSP）仍然是一个长期挑战，我们介绍了一个新的算法，即ParetoCSP，它结合了多个目标遗传algorithm（MOGA）和神经网络间原子 potential（IAP）模型，以获取化学成分提供的能量最佳晶体结构。我们将NSGA-III算法加以改进，通过包括遗传年龄作为独立优化条件，并使用M3GNet通用IAP导引GA搜索。与GN-OA，一个现有的神经 potential基于CSP算法相比，ParetoCSP在55个多样化的benchmark结构上显示出了明显的改善，其中七种性能指标中的一个改善因子为2.562。对所有算法的探索过程中的构造轨迹分析表明，ParetoCSP产生了更多的有效构造，帮助GA更有效地寻找优化结构。”
</details></li>
</ul>
<hr>
<h2 id="Predicting-Fatigue-Crack-Growth-via-Path-Slicing-and-Re-Weighting"><a href="#Predicting-Fatigue-Crack-Growth-via-Path-Slicing-and-Re-Weighting" class="headerlink" title="Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting"></a>Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06708">http://arxiv.org/abs/2309.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyj21/fcg">https://github.com/zhaoyj21/fcg</a></li>
<li>paper_authors: Yingjie Zhao, Yong Liu, Zhiping Xu</li>
<li>for: 预测结构元件疲劳的可能性，以便在工程设计中进行评估和预测。</li>
<li>methods: 使用统计学学习框架，利用高精度物理 simulate crack pattern 和剩余寿命，然后使用维度减少和神经网络架构来学习历史相依性和非线性。</li>
<li>results: 预测的疲劳裂隙模式和剩余寿命可以在实时结构健康监测和疲劳生命预测中提供数字双方式enario，帮助进行维护管理决策。<details>
<summary>Abstract</summary>
Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples with fatigue cracks in plates, which showcase the digital-twin scenario in real-time structural health monitoring and fatigue life prediction for maintenance management decision-making.
</details>
<details>
<summary>摘要</summary>
预测关键结构组件的疲劳风险是工程设计中的关键任务。然而，疲劳通常会带来材料微结构和服务条件之间的复杂互连关系，使诊断和预测疲劳损害具有挑战性。我们报告了一种统计学学习框架，用于预测加载条件下疲劳裂隙的增长和组件的寿命。通过高精度物理 simulate 得到的数字图书馆，包括疲劳裂隙和剩余寿命的数据。使用维度减少和神经网络架构，学习疲劳裂隙历史依赖性和非线性。使用路径架和重量补做技术来处理统计噪声和罕见事件。预测的疲劳裂隙 Pattern 会自动更新和自我修正。我们验证了这种终端方法，通过板件中的疲劳裂隙示例，展示了数字神经网络在实时结构健康监测和疲劳寿命预测中的可行性。
</details></li>
</ul>
<hr>
<h2 id="VLSlice-Interactive-Vision-and-Language-Slice-Discovery"><a href="#VLSlice-Interactive-Vision-and-Language-Slice-Discovery" class="headerlink" title="VLSlice: Interactive Vision-and-Language Slice Discovery"></a>VLSlice: Interactive Vision-and-Language Slice Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06703">http://arxiv.org/abs/2309.06703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slymane/vlslice">https://github.com/slymane/vlslice</a></li>
<li>paper_authors: Eric Slyman, Minsuk Kahng, Stefan Lee</li>
<li>for: 这篇论文目的是发展一个可互动地发现可视语言相互关联的变量分布，从未 labels 的图像集中获得更好的表现。</li>
<li>methods: 这篇论文使用了大规模预训来学习可转移的模型，并使用了用户引导的方法来发现可视语言 slice。</li>
<li>results: 在用户研究中（n&#x3D;22），VLSlice 能够快速生成多元高凝集的可视语言 slice，并且发布了这个工具给公众。<details>
<summary>Abstract</summary>
Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly.
</details>
<details>
<summary>摘要</summary>
近期的视觉语言工作显示，大规模预训练可以学习通用的模型，高效地传输到下游任务。这可能提高数据集级别的统计量表现，但是分析围绕特定偏见维度的手工 subgroup 表现可能存在不良行为。然而，这种 subgroup 分析通常受到注释的限制，需要大量的时间和资源来收集必要的数据。先前的艺术尝试自动发现 subgroup 以绕过这些限制，通常基于现有的任务特定签名，但是这些模型很快地在更复杂的输入上崩溃。本文提出了 VLSlice，一种互动系统，帮助用户通过手动导航发现具有相互关联的视觉语言行为，称为视觉语言slice，从无标注图像集中。我们证明了 VLSlice 在用户研究中（n=22）可以快速生成多样性高准确性 slice，并公开发布了这个工具。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Non-IID-Issue-in-Heterogeneous-Federated-Learning-by-Gradient-Harmonization"><a href="#Tackling-the-Non-IID-Issue-in-Heterogeneous-Federated-Learning-by-Gradient-Harmonization" class="headerlink" title="Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization"></a>Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06692">http://arxiv.org/abs/2309.06692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Weiyu Sun, Ying Chen</li>
<li>for: 这篇论文目的是解决联合学习（Federated Learning，FL）中数据和设备不均的挑战，具体来说是通过减轻服务器端的梯度冲突来提高FL的性能。</li>
<li>methods: 这篇论文使用了Gradient Harmonization（梯度融合）技术来减轻服务器端的梯度冲突，该技术可以将多个客户端之间的梯度向量投影到彼此 orthogonal 的平面上，从而减少梯度冲突。</li>
<li>results: 实验表明，使用FedGH技术可以在多种不同的benchmark和非同分布场景下提高FL的性能，特别是在强度不均的场景下更是如此。此外，FedGH技术可以轻松地与任何FL框架集成，不需要对hyperparameter进行调整。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios with stronger heterogeneity. As a plug-and-play module, FedGH can be seamlessly integrated into any FL framework without requiring hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
Federation learning (FL) 是一种隐私保护的 paradigm，用于在分散的客户端上共同训练全球模型。然而，FL 的性能受到非独立和同分布数据（non-IID）和设备不同性的影响。在这项工作中，我们重新检视了这一关键挑战，通过服务器端的梯度冲突来查看。我们首先调查了多个客户端之间的梯度冲突现象，并发现强化不同性导致更严重的梯度冲突。为解决这个问题，我们提议 FedGH，一种简单 yet effective的方法，通过把一个梯度向量投影到另一个客户端对 conflicting 的梯度向量的正交平面。我们的实验表明，FedGH 可以增强多个 state-of-the-art FL 基elines  across 多种不同的 benchmarks 和 non-IID 场景。特别是，在更强的不同性情况下，FedGH 的改进更为显著。作为一个插件模块，FedGH 可以顺利地与任何 FL 框架集成，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Attention-Loss-Adjusted-Prioritized-Experience-Replay"><a href="#Attention-Loss-Adjusted-Prioritized-Experience-Replay" class="headerlink" title="Attention Loss Adjusted Prioritized Experience Replay"></a>Attention Loss Adjusted Prioritized Experience Replay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06684">http://arxiv.org/abs/2309.06684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoying Chen, Huiping Li, Rizhong Wang</li>
<li>for: 提高深度强化学习训练速率</li>
<li>methods:  integrate improved Self-Attention network with Double-Sampling mechanism</li>
<li>results: 提高训练精度和效率，适用于多种强化学习算法和环境<details>
<summary>Abstract</summary>
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
</details>
<details>
<summary>摘要</summary>
优先经验回归（PER）是一种深度强化学习技术，通过选择具有更多知识量的经验样本来提高神经网络训练速率。然而，PER中的非均匀采样无可避免地导致状态动作空间分布的改变，从而引起Q值函数的估计误差。在本文中，一种含有改进自注意网络和双采样机制的Attention Loss Adjusted Prioritized（ALAP）经验回归算法被提出，以适应调整重要采样权重，消除PER引起的估计误差。为证明算法的有效性和通用性，ALAP在OPENAI gym中使用值函数基于、政策梯度基于和多Agent强化学习算法进行了测试，并进行了对比研究，以证明提议的训练框架的优势和高效性。
</details></li>
</ul>
<hr>
<h2 id="Federated-PAC-Bayesian-Learning-on-Non-IID-data"><a href="#Federated-PAC-Bayesian-Learning-on-Non-IID-data" class="headerlink" title="Federated PAC-Bayesian Learning on Non-IID data"></a>Federated PAC-Bayesian Learning on Non-IID data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06683">http://arxiv.org/abs/2309.06683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhao, Yang Liu, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 这个论文是为了解决非独立不同数据（non-IID）的 Federated Learning（FL）中的 Probably Approximately Correct（PAC）极限问题而写的。</li>
<li>methods: 这篇论文使用了唯一先验知识（unique prior knowledge）和变量汇集权（variable aggregation weights）来提出了首个非虚诞的非独立FL PAC极限下界。它还介绍了一种对这个下界的优化问题的新方法和对实际数据进行验证。</li>
<li>results: 该论文的结果验证了这种新的非独立FL PAC极限下界在实际数据上的有效性。<details>
<summary>Abstract</summary>
Existing research has either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
</details>
<details>
<summary>摘要</summary>
现有研究 Either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.Here's the word-for-word translation:现有研究 Either 已经适应了 Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or 使用了 information-theoretic PAC-Bayesian bounds while introducing their theorems, but few 考虑了 non-IID 挑战 in FL. Our work 发表了 first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound 假设了 each client 和 variable aggregation weights 的 unique prior knowledge. We also introduce an objective function 和 an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-improvement-of-the-Spalart-Allmaras-model-through-assimilation-of-experimental-data"><a href="#Generalizable-improvement-of-the-Spalart-Allmaras-model-through-assimilation-of-experimental-data" class="headerlink" title="Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data"></a>Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06679">http://arxiv.org/abs/2309.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepinder Jot Singh Aulakh, Romit Maulik</li>
<li>For: The paper aims to improve the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows by using model and data fusion.* Methods: The paper uses data assimilation, specifically the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. The calibration relies on experimental data collected for velocity profiles, skin friction, and pressure coefficients for separated flows.* Results: The recalibrated SA model demonstrates generalization to other separated flows and significant improvement in the quantities of interest, such as skin friction coefficient ($C_f$) and pressure coefficient ($C_p$), for each flow tested. The individually calibrated terms in the SA model are targeted towards specific flow-physics, with the calibrated production term improving the re-circulation zone and the destruction term improving the recovery zone.<details>
<summary>Abstract</summary>
This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to other separated flows, including cases such as the 2D-bump and modified BFS. Significant improvement is observed in the quantities of interest, i.e., skin friction coefficient ($C_f$) and pressure coefficient ($C_p$) for each flow tested. Finally, it is also demonstrated that the newly proposed model recovers SA proficiency for external, unseparated flows, such as flow around a NACA-0012 airfoil without any danger of extrapolation, and that the individually calibrated terms in the SA model are targeted towards specific flow-physics wherein the calibrated production term improves the re-circulation zone while destruction improves the recovery zone.
</details>
<details>
<summary>摘要</summary>
To achieve this, ensemble Kalman filtering (EnKF) is used to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented by parameterizing the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected for velocity profiles, skin friction, and pressure coefficients for separated flows.Despite using data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to other separated flows, including 2D-bump and modified BFS flows. Significant improvements are observed in the quantities of interest, such as skin friction coefficient ($C_f$) and pressure coefficient ($C_p$), for each flow tested.Furthermore, the proposed model recovers SA proficiency for external, unseparated flows, such as flow around a NACA-0012 airfoil, without any danger of extrapolation. Additionally, the individually calibrated terms in the SA model are targeted towards specific flow-physics, where the calibrated production term improves the re-circulation zone while the destruction term improves the recovery zone.
</details></li>
</ul>
<hr>
<h2 id="Sound-field-decomposition-based-on-two-stage-neural-networks"><a href="#Sound-field-decomposition-based-on-two-stage-neural-networks" class="headerlink" title="Sound field decomposition based on two-stage neural networks"></a>Sound field decomposition based on two-stage neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06661">http://arxiv.org/abs/2309.06661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryo Matsuda, Makoto Otani</li>
<li>for: 该研究提出了一种基于神经网络的声场分解方法，用于实时声源地理位确定。</li>
<li>methods: 该方法包括两个阶段：声场分离阶段和单源地理位定位阶段。在第一阶段，通过多个源 зву频压力在 Mikrofone 上的合成，分离出每个音源启动的声压力。在第二阶段，通过 Mikrofone 上的声压力 regression 来获取源位置。不同于传统方法，该阶段采用 regression 而不是分类，因此不受精度的影响。</li>
<li>results: 数值实验表明，与传统方法相比，提出的方法可以 дости得更高的源地理位准确率和声场重建准确率。<details>
<summary>Abstract</summary>
A method for sound field decomposition based on neural networks is proposed. The method comprises two stages: a sound field separation stage and a single-source localization stage. In the first stage, the sound pressure at microphones synthesized by multiple sources is separated into one excited by each sound source. In the second stage, the source location is obtained as a regression from the sound pressure at microphones consisting of a single sound source. The estimated location is not affected by discretization because the second stage is designed as a regression rather than a classification. Datasets are generated by simulation using Green's function, and the neural network is trained for each frequency. Numerical experiments reveal that, compared with conventional methods, the proposed method can achieve higher source-localization accuracy and higher sound-field-reconstruction accuracy.
</details>
<details>
<summary>摘要</summary>
提出一种基于神经网络的声场分解方法。该方法包括两个阶段：声场分离阶段和单源定位阶段。在第一阶段，通过多个源的声压在 Mikrophone 中合成的声场被分离成每个声源引起的一个声场。在第二阶段，源location 通过 Mikrophone 上的声压 regression 来获得估算，而不是分类。通过绿函数 simulated 生成的数据集进行训练，每个频率都有自己的神经网络。 numerics 实验表明，相比 conventient 方法，提出的方法可以实现更高的源定位精度和声场重建精度。Note: "Mikrophone" is the Simplified Chinese term for "microphone".
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Neural-Fields-as-Partially-Observed-Neural-Processes"><a href="#Generalizable-Neural-Fields-as-Partially-Observed-Neural-Processes" class="headerlink" title="Generalizable Neural Fields as Partially Observed Neural Processes"></a>Generalizable Neural Fields as Partially Observed Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06660">http://arxiv.org/abs/2309.06660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Gu, Kuan-Chieh Wang, Serena Yeung</li>
<li>for: 本研究旨在提出一种新的方法来大规模训练神经场，以便在不同的信号上进行快速和高效的表示。</li>
<li>methods: 该方法基于偏置神经网络，并利用神经过程算法来解决这个问题。</li>
<li>results: 对比 gradient-based 元学习方法和hypernetwork方法，本研究的方法在大规模训练神经场时显示出了更高的性能。<details>
<summary>Abstract</summary>
Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorithms to solve this task. We demonstrate that this approach outperforms both state-of-the-art gradient-based meta-learning approaches and hypernetwork approaches.
</details>
<details>
<summary>摘要</summary>
neural fields，代表信号为函数参数化的神经网络，是传统权值或格子型表示方式的有前途的代替方案。相比于权值或格子表示方式，神经表示方式可以规格化、连续和可多次导数，但是对于一个数据集的信号表示，需要为每个信号分别优化神经场，这是不高效的。现有的普适方法将此视为一个meta学习问题，使用梯度基本的meta学习学习初始化，然后在测试时进行优化，或者学习层次网络生成神经场的权重。我们提出了一种新的思路，视为大规模神经场训练为部分观察神经过程框架的一部分，并利用神经过程算法解决这个问题。我们示示了这种方法在比革 Gradient-based meta-学习方法和层次网络方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Dissipative-Imitation-Learning-for-Discrete-Dynamic-Output-Feedback-Control-with-Sparse-Data-Sets"><a href="#Dissipative-Imitation-Learning-for-Discrete-Dynamic-Output-Feedback-Control-with-Sparse-Data-Sets" class="headerlink" title="Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets"></a>Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06658">http://arxiv.org/abs/2309.06658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amy K. Strong, Ethan J. LoCicero, Leila J. Bridgeman</li>
<li>for: 该论文旨在实现基于模仿学习的控制器 synthesis，并提供稳定性保证，但方法通常需要大量数据和&#x2F;或已知的植物模型。</li>
<li>methods: 该论文使用输入输出稳定性方法，通过使用专家数据、粗略的IO植物模型和新的约束来实现稳定性。学习目标是非凸的，并使用迭代凸上升（ICO）和投影加速算法（PGD）来成功学习控制器。</li>
<li>results: 该论文通过应用这种新的模仿学习方法，在两个未知的植物上实现了稳定的关闭循环和成功模仿专家控制器的行为，而传统的学习动态输出反馈控制器和神经网络控制器通常无法保持稳定和达到好的性能。<details>
<summary>Abstract</summary>
Imitation learning enables the synthesis of controllers for complex objectives and highly uncertain plant models. However, methods to provide stability guarantees to imitation learned controllers often rely on large amounts of data and/or known plant models. In this paper, we explore an input-output (IO) stability approach to dissipative imitation learning, which achieves stability with sparse data sets and with little known about the plant model. A closed-loop stable dynamic output feedback controller is learned using expert data, a coarse IO plant model, and a new constraint to enforce dissipativity on the learned controller. While the learning objective is nonconvex, iterative convex overbounding (ICO) and projected gradient descent (PGD) are explored as methods to successfully learn the controller. This new imitation learning method is applied to two unknown plants and compared to traditionally learned dynamic output feedback controller and neural network controller. With little knowledge of the plant model and a small data set, the dissipativity constrained learned controller achieves closed loop stability and successfully mimics the behavior of the expert controller, while other methods often fail to maintain stability and achieve good performance.
</details>
<details>
<summary>摘要</summary>
通过依据学习，我们可以Synthesize控制器来实现复杂的目标和高度不确定的植物模型。然而，为了提供稳定性保证给依据学习获得的控制器，通常需要大量数据和/或已知的植物模型。在这篇论文中，我们探索了输入输出（IO）稳定性方法来实现不可逆依据学习，这种方法可以在稀缺数据集和具有少量知识的植物模型下实现稳定性。我们使用专家数据、粗略的IO植物模型和一个新的约束来学习一个闭环稳定的动态输出反馈控制器。虽然学习目标是非几何的，但我们使用迭代凸包练（ICO）和投影向量Descents（PGD）来成功地学习控制器。这种新的依据学习方法在两个未知的植物上应用，与传统学习的动态输出反馈控制器和神经网络控制器进行比较。即使具有少量的植物模型知识和小数据集，依据学习的控制器可以在关闭环中保持稳定性，并成功地模仿专家控制器的行为，而其他方法经常无法保持稳定性和达到好的性能。
</details></li>
</ul>
<hr>
<h2 id="Offline-Prompt-Evaluation-and-Optimization-with-Inverse-Reinforcement-Learning"><a href="#Offline-Prompt-Evaluation-and-Optimization-with-Inverse-Reinforcement-Learning" class="headerlink" title="Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning"></a>Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06553">http://arxiv.org/abs/2309.06553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holarissun/Prompt-OIRL">https://github.com/holarissun/Prompt-OIRL</a></li>
<li>paper_authors: Hao Sun</li>
<li>for: 提高大语言模型（LLMs）的效果，减少人工评估的成本</li>
<li>methods: 使用离线 inverse reinforcement learning（Inverse-RL）来评估和优化提示</li>
<li>results: 可以准确预测提示的表现，高效、低成本、生成人读ABLE的结果，可以有效地探索提示空间<details>
<summary>Abstract</summary>
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable results, and efficiently navigates the prompt space. We validate our method across four LLMs and three arithmetic datasets, highlighting its potential as a robust and effective tool for offline prompt evaluation and optimization. Our code as well as the offline datasets are released, and we highlight the Prompt-OIRL can be reproduced within a few hours using a single laptop using CPU
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的发展，如ChatGPT，已经实现了很好的性能，但是完全利用这些模型的复杂任务执行需要在自然语言提示的庞大搜索空间中穿梭。虽然提示工程学已经显示了承诺，但是需要在尝试和错误中手动制定提示，以及相关的成本带来了 significiant challenges。关键在于提示优化的效率，它取决于提示评估的成本。这个工作介绍了Prompt-OIRL，一种基于离线反冲采集学习的方法，旨在bridging提示评估的效率和可持续性之间的 gab。我们的方法利用离线数据集，从专家评估中提取出一个倒计时间的提示评估模型。Prompt-OIRL的优点包括：它预测提示的性能，成本效益，生成人类可读的结果，并快速导航提示空间。我们在四个LLM和三个数学数据集上验证了我们的方法，并 highlighted its potential as a robust and effective tool for offline prompt evaluation and optimization。我们的代码以及离线数据集都已经发布，并且在一个个人的 laptop 上使用 CPU 进行了数个小时的 reproduce。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Detection-via-Domain-Informed-Gaussian-Process-State-Space-Models"><a href="#Out-of-Distribution-Detection-via-Domain-Informed-Gaussian-Process-State-Space-Models" class="headerlink" title="Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models"></a>Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06655">http://arxiv.org/abs/2309.06655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alonso Marco, Elias Morley, Claire J. Tomlin</li>
<li>for: 本研究旨在使用学习方法让机器人在未经训练的场景中安全 Navigation。</li>
<li>methods: 本文提出了一种将现有领域知识嵌入GPSSM kernel中的新方法，以及一种基于往回预测的OoD在线监测器。</li>
<li>results: 实验结果表明，在小数据集上，具有领域知识的GPSSM kernel可以提供更高质量的回归预测，而OoD监测器在实际四足机器人内部环境中可靠地分类未经训练的地形。<details>
<summary>Abstract</summary>
In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard kernel choices. We demonstrate the effectiveness of the OoD monitor on a real quadruped navigating an indoor setting, which reliably classifies previously unseen terrains.
</details>
<details>
<summary>摘要</summary>
为了让机器人在未经训练的enario中安全 navigate，使用学习基本方法是重要的。在线上准确探测出training distribution不符（OoD）的情况是关键。近些年， Gaussian process state-space models（GPSSM）已经证明可以用来区分不warted observations。然而，模型正确地 distinguish between in-和out-of-training distribution observations取决于预测的准确性，主要受到GPSSM kernel中函数类型的影响。在这篇论文中，我们提出（i）一种将现有领域知识 embed在kernel中的新方法，以及（ii）一种在运行时 monitoring OoD的方法，基于往返 Horizon 预测。领域知识假设为一个已经收集并且分类的数据集，可以是在模拟或者使用标准模型来获得。我们的数据显示， informed kernel 可以提供更好的回归质量，只需使用小型数据集。我们还证明了这种 OoD 监控器在一个真实的四足机器人中 navigate indoor 环境中的可靠性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="ConR-Contrastive-Regularizer-for-Deep-Imbalanced-Regression"><a href="#ConR-Contrastive-Regularizer-for-Deep-Imbalanced-Regression" class="headerlink" title="ConR: Contrastive Regularizer for Deep Imbalanced Regression"></a>ConR: Contrastive Regularizer for Deep Imbalanced Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06651">http://arxiv.org/abs/2309.06651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/borealisai/conr">https://github.com/borealisai/conr</a></li>
<li>paper_authors: Mahsa Keramati, Lili Meng, R. David Evans</li>
<li>for: 提高深度异质回归模型的性能，解决深度学习模型面临异质数据的挑战。</li>
<li>methods: 提出了一种异质对准方法，通过模拟全球和本地相似性，在特征空间中模型 labels 之间的相似性，避免少数类样本的特征被折叠到多数类样本中。</li>
<li>results: 对三个大规模深度异质回归数据集进行了广泛的实验，显示 ConR 可以显著提高所有现状的状态机制方法的性能。<details>
<summary>Abstract</summary>
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penalized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly extends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on three large-scale deep imbalanced regression benchmarks. Our code is publicly available in https://github.com/BorealisAI/ConR.
</details>
<details>
<summary>摘要</summary>
偏度分布很普遍存在现实世界数据中。它们限制深度神经网络来表示少数标签，并避免强调多数标签。然而，大量的偏度方法只能处理分类标签空间，而不能有效扩展到回归问题，其标签空间是连续的。相反，地方和全局相关性在特征空间提供了有价值的信息，用于有效地模型特征空间中的关系。在这项工作中，我们提出了 ConR，一种强制对比 regularizer，它在特征空间中模型全局和地方标签相似性，并避免少数样本的特征被折叠到其多数 neighborgood中。通过对预测结果的相似性作为特征相似性的指标，ConR挖掘了标签空间和特征空间之间的不一致，并对这些不一致进行了罚款。ConR 把重要考虑因素综合化为一种普遍适用、容易集成、高效的方法，有效地解决深度偏度回归问题。此外，ConR 是已有方法的正交，可以顺利扩展到一维和多维标签空间。我们的广泛实验表明，ConR 可以在三个大规模的深度偏度回归benchmark上提高现有状态的方法表现。我们的代码可以在 <https://github.com/BorealisAI/ConR> 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/13/cs.LG_2023_09_13/" data-id="clmjn91n3008j0j881d4wai0b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/13/eess.IV_2023_09_13/" class="article-date">
  <time datetime="2023-09-13T09:00:00.000Z" itemprop="datePublished">2023-09-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/13/eess.IV_2023_09_13/">eess.IV - 2023-09-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Limited-Angle-Tomography-Reconstruction-via-Deep-End-To-End-Learning-on-Synthetic-Data"><a href="#Limited-Angle-Tomography-Reconstruction-via-Deep-End-To-End-Learning-on-Synthetic-Data" class="headerlink" title="Limited-Angle Tomography Reconstruction via Deep End-To-End Learning on Synthetic Data"></a>Limited-Angle Tomography Reconstruction via Deep End-To-End Learning on Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06948">http://arxiv.org/abs/2309.06948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Germer, Jan Robine, Sebastian Konietzny, Stefan Harmeling, Tobias Uelwer<br>for:  CT干预像 reconstruction problem methods: 使用深度神经网络，通过大量精心制作的 sintetic data 进行训练results: 能够在30°或40°的极限角度下进行CT干预像重建，并在 Helsinki Tomography Challenge 2022 中获得第一名。<details>
<summary>Abstract</summary>
Computed tomography (CT) has become an essential part of modern science and medicine. A CT scanner consists of an X-ray source that is spun around an object of interest. On the opposite end of the X-ray source, a detector captures X-rays that are not absorbed by the object. The reconstruction of an image is a linear inverse problem, which is usually solved by filtered back projection. However, when the number of measurements is small, the reconstruction problem is ill-posed. This is for example the case when the X-ray source is not spun completely around the object, but rather irradiates the object only from a limited angle. To tackle this problem, we present a deep neural network that is trained on a large amount of carefully-crafted synthetic data and can perform limited-angle tomography reconstruction even for only 30{\deg} or 40{\deg} sinograms. With our approach we won the first place in the Helsinki Tomography Challenge 2022.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，我们提出了一种基于深度神经网络的方法。我们在大量精心制作的 sintetic 数据上训练了一个深度神经网络，可以在只有 30° 或 40° 的扫描角度下进行有限角度 Tomography 重建。我们的方法在 Helsinki Tomography Challenge 2022 中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Improving-HEVC-Encoding-of-Rendered-Video-Data-Using-True-Motion-Information"><a href="#Improving-HEVC-Encoding-of-Rendered-Video-Data-Using-True-Motion-Information" class="headerlink" title="Improving HEVC Encoding of Rendered Video Data Using True Motion Information"></a>Improving HEVC Encoding of Rendered Video Data Using True Motion Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06945">http://arxiv.org/abs/2309.06945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, David Müller, Andreas Weinlich, Frank Bauer, Michael Ortner, Marc Stamminger, André Kaup</li>
<li>for: 提高计算机生成视频序列的编码过程中的质量</li>
<li>methods: 利用场景中对象的真实运动向量进行增强，包括传统运动估算方法以及计算机生成的运动向量增强</li>
<li>results: 可以获得3.78%的均质量改进<details>
<summary>Abstract</summary>
This paper shows that motion vectors representing the true motion of an object in a scene can be exploited to improve the encoding process of computer generated video sequences. Therefore, a set of sequences is presented for which the true motion vectors of the corresponding objects were generated on a per-pixel basis during the rendering process. In addition to conventional motion estimation methods, it is proposed to exploit the computer generated motion vectors to enhance the ratedistortion performance. To this end, a motion vector mapping method including disocclusion handling is presented. It is shown that mean rate savings of 3.78% can be achieved.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Topology-inspired-Cross-domain-Network-for-Developmental-Cervical-Stenosis-Quantification"><a href="#Topology-inspired-Cross-domain-Network-for-Developmental-Cervical-Stenosis-Quantification" class="headerlink" title="Topology-inspired Cross-domain Network for Developmental Cervical Stenosis Quantification"></a>Topology-inspired Cross-domain Network for Developmental Cervical Stenosis Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06825">http://arxiv.org/abs/2309.06825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenxi Zhang, Yanyang Wang, Yao Wu, Weifei Wu</li>
<li>for: 验证 Developmental Canal Stenosis (DCS) 评估方法的效果，以帮助诊断cervical spondylosis。</li>
<li>methods: 使用深度关键点地图网络，可以在坐标领域或图像领域进行快速和高效的关键点定位。</li>
<li>results: 提出了一种束缚关键点-边界模块和重parameter化模块，可以在坐标领域和图像领域进行跨领域约束，从而提高关键点定位的精度和可靠性。<details>
<summary>Abstract</summary>
Developmental Canal Stenosis (DCS) quantification is crucial in cervical spondylosis screening. Compared with quantifying DCS manually, a more efficient and time-saving manner is provided by deep keypoint localization networks, which can be implemented in either the coordinate or the image domain. However, the vertebral visualization features often lead to abnormal topological structures during keypoint localization, including keypoint distortion with edges and weakly connected structures, which cannot be fully suppressed in either the coordinate or image domain alone. To overcome this limitation, a keypoint-edge and a reparameterization modules are utilized to restrict these abnormal structures in a cross-domain manner. The keypoint-edge constraint module restricts the keypoints on the edges of vertebrae, which ensures that the distribution pattern of keypoint coordinates is consistent with those for DCS quantification. And the reparameterization module constrains the weakly connected structures in image-domain heatmaps with coordinates combined. Moreover, the cross-domain network improves spatial generalization by utilizing heatmaps and incorporating coordinates for accurate localization, which avoids the trade-off between these two properties in an individual domain. Comprehensive results of distinct quantification tasks show the superiority and generability of the proposed Topology-inspired Cross-domain Network (TCN) compared with other competing localization methods.
</details>
<details>
<summary>摘要</summary>
发展隧道狭窄 (DCS) 量化是颈椎病变检测中非常重要的一步。相比手动量化 DCS，深度关键点本地化网络可以提供更加高效和时间节省的方法，可以在坐标领域或图像领域中实现。然而， vertebral 视觉特征可能导致关键点本地化过程中出现异常的 topological 结构，包括关键点扭曲、边缘和弱连接结构，这些结构在坐标领域或图像领域单独处理不能完全消除。为了解决这个限制，我们提出了关键点-边缘约束模块和重parameterization 模块。关键点-边缘约束模块使得关键点的分布协调于 DCS 量化中的分布，而重parameterization 模块在图像领域的热图上使用坐标组合来约束弱连接结构。此外，交叉领域网络可以提高空间泛化性，通过使用热图和坐标组合来准确定位关键点，从而避免在单个领域中的质量规则负担。Results of distinct quantification tasks demonstrate the superiority and generability of our proposed Topology-inspired Cross-domain Network (TCN) compared with other competing localization methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Synthetic-High-Resolution-In-Depth-Imaging-Using-an-Attachable-Dual-element-Endoscopic-Ultrasound-Probe"><a href="#Deep-Learning-based-Synthetic-High-Resolution-In-Depth-Imaging-Using-an-Attachable-Dual-element-Endoscopic-Ultrasound-Probe" class="headerlink" title="Deep Learning-based Synthetic High-Resolution In-Depth Imaging Using an Attachable Dual-element Endoscopic Ultrasound Probe"></a>Deep Learning-based Synthetic High-Resolution In-Depth Imaging Using an Attachable Dual-element Endoscopic Ultrasound Probe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06770">http://arxiv.org/abs/2309.06770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hah Min Lew, Jae Seong Kim, Moon Hwan Lee, Jaegeun Park, Sangyeon Youn, Hee Man Kim, Jihun Kim, Jae Youn Hwang</li>
<li>for: 提高endoscopic ultrasound（EUS）图像的分辨率和吸收深度。</li>
<li>methods: 使用deep learning技术，开发一种新的高分辨率深度扫描镜头，并使用特制的齿轮结构来实现同一个图像平面。</li>
<li>results: 对细菌样品和生物样品进行了测试，并使用多种深度学习模型来生成高分辨率深度图像， thereby demonstrating the feasibility of our approach for clinical unmet needs.<details>
<summary>Abstract</summary>
Endoscopic ultrasound (EUS) imaging has a trade-off between resolution and penetration depth. By considering the in-vivo characteristics of human organs, it is necessary to provide clinicians with appropriate hardware specifications for precise diagnosis. Recently, super-resolution (SR) ultrasound imaging studies, including the SR task in deep learning fields, have been reported for enhancing ultrasound images. However, most of those studies did not consider ultrasound imaging natures, but rather they were conventional SR techniques based on downsampling of ultrasound images. In this study, we propose a novel deep learning-based high-resolution in-depth imaging probe capable of offering low- and high-frequency ultrasound image pairs. We developed an attachable dual-element EUS probe with customized low- and high-frequency ultrasound transducers under small hardware constraints. We also designed a special geared structure to enable the same image plane. The proposed system was evaluated with a wire phantom and a tissue-mimicking phantom. After the evaluation, 442 ultrasound image pairs from the tissue-mimicking phantom were acquired. We then applied several deep learning models to obtain synthetic high-resolution in-depth images, thus demonstrating the feasibility of our approach for clinical unmet needs. Furthermore, we quantitatively and qualitatively analyzed the results to find a suitable deep-learning model for our task. The obtained results demonstrate that our proposed dual-element EUS probe with an image-to-image translation network has the potential to provide synthetic high-frequency ultrasound images deep inside tissues.
</details>
<details>
<summary>摘要</summary>
endoscopic ultrasound（EUS）影像有一种质量和渗透深度之间的交换。由于人体内部的特点，需要为临床医生提供适当的硬件规格，以便精确诊断。近些年来，超分解（SR）ultrasound影像研究，包括深度学习领域中的SR任务，已经被报道用于提高ultrasound影像。然而，大多数这些研究并没有考虑ultrasound影像的特点，而是基于ultrasound影像下采样的传统SR技术。在本研究中，我们提出了一种基于深度学习的高分辨率深度渗透成像探针。我们开发了一种可拆卸的双元EUS探针，其中包括自定义的低频和高频ultrasound传播器。我们还设计了特殊的液压结构，以实现同一个图像平面。我们对使用这种系统进行了评估，并获得了442个ultrasound影像对。我们然后应用了多种深度学习模型，以获得 sintetic高分辨率深度渗透图像，这 demonstartes了我们的方法的可行性。此外，我们进行了量化和质量分析，以选择适合我们任务的深度学习模型。所获结果表明，我们的提案的双元EUS探针与图像译化网络具有深度渗透图像深度 Inside tissues的潜力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Learning-based-Defect-Detection-on-Window-Frames-with-Image-Processing-Strategies"><a href="#Improving-Deep-Learning-based-Defect-Detection-on-Window-Frames-with-Image-Processing-Strategies" class="headerlink" title="Improving Deep Learning-based Defect Detection on Window Frames with Image Processing Strategies"></a>Improving Deep Learning-based Defect Detection on Window Frames with Image Processing Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06731">http://arxiv.org/abs/2309.06731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Vasquez, Hemant K. Sharma, Tomotake Furuhata, Kenji Shimada</li>
<li>for: 这个论文的目的是提高机器视觉系统的检测精度，使其能够更好地检测窗户框中的微小损害，包括损害和擦抹等。</li>
<li>methods: 这个论文使用了机器学习和深度学习（DL）的新的检测方法，包括图像增强和扩展技术，以提高检测精度。</li>
<li>results: 实验结果表明，当使用最佳的数据集时，IPT-加强的UNet模型在检测和分类窗户框中的损害方面表现出色，其中平均的交集 над union（IoU）值达到0.91。<details>
<summary>Abstract</summary>
Detecting subtle defects in window frames, including dents and scratches, is vital for upholding product integrity and sustaining a positive brand perception. Conventional machine vision systems often struggle to identify these defects in challenging environments like construction sites. In contrast, modern vision systems leveraging machine and deep learning (DL) are emerging as potent tools, particularly for cosmetic inspections. However, the promise of DL is yet to be fully realized. A few manufacturers have established a clear strategy for AI integration in quality inspection, hindered mainly by issues like scarce clean datasets and environmental changes that compromise model accuracy. Addressing these challenges, our study presents an innovative approach that amplifies defect detection in DL models, even with constrained data resources. The paper proposes a new defect detection pipeline called InspectNet (IPT-enhanced UNET) that includes the best combination of image enhancement and augmentation techniques for pre-processing the dataset and a Unet model tuned for window frame defect detection and segmentation. Experiments were carried out using a Spot Robot doing window frame inspections . 16 variations of the dataset were constructed using different image augmentation settings. Results of the experiments revealed that, on average, across all proposed evaluation measures, Unet outperformed all other algorithms when IPT-enhanced augmentations were applied. In particular, when using the best dataset, the average Intersection over Union (IoU) values achieved were IPT-enhanced Unet, reaching 0.91 of mIoU.
</details>
<details>
<summary>摘要</summary>
检测窗框中微小损害，包括拥有折叠和擦抹等瑕疵，是维护产品完整性和保持品牌形象的关键。传统的机器视觉系统经常在建筑工地等复杂环境中难以识别这些瑕疵。相反，现代视觉系统 combining机器学习和深度学习（DL）在cosmetic检测方面表现出了很好的潜力。然而，DL的承诺还未被完全实现。一些制造商在质检中使用AI的策略已经初步确定，主要受到数据资源罕见和环境变化的影响。为解决这些挑战，我们的研究提出了一种创新的方法，可以增强DL模型中瑕疵检测的精度，即使数据资源受限。我们的研究报告一种新的瑕疵检测管线，即InspectNet（IPT-enhanced UNET），包括最佳的图像增强和扩展技术 для预处理数据集，以及特地适应窗框瑕疵检测和分割的Unet模型。实验使用Spot Robot进行窗框检测。我们构建了16种不同的数据集，并对各种图像增强设置进行了比较。实验结果显示，当IPT-enhanced增强技术应用于数据集时，Unet模型在所有提出的评价指标上平均高于其他算法。特别是在使用最佳数据集时，Unet模型的平均交集概念（IoU）值达到0.91。
</details></li>
</ul>
<hr>
<h2 id="A-plug-and-play-synthetic-data-deep-learning-for-undersampled-magnetic-resonance-image-reconstruction"><a href="#A-plug-and-play-synthetic-data-deep-learning-for-undersampled-magnetic-resonance-image-reconstruction" class="headerlink" title="A plug-and-play synthetic data deep learning for undersampled magnetic resonance image reconstruction"></a>A plug-and-play synthetic data deep learning for undersampled magnetic resonance image reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06681">http://arxiv.org/abs/2309.06681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Xiao, Zi Wang, Jiefeng Guo, Xiaobo Qu</li>
<li>for: 用于加速MRI扫描，提高医疗诊断效率</li>
<li>methods: 使用深度学习方法进行快速MRI重建，适应不同抽取样本设置</li>
<li>results: 在实验室数据上得到了良好的图像恢复性和稳定性，可视化和量化上都达到了预期效果<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) plays an important role in modern medical diagnostic but suffers from prolonged scan time. Current deep learning methods for undersampled MRI reconstruction exhibit good performance in image de-aliasing which can be tailored to the specific kspace undersampling scenario. But it is very troublesome to configure different deep networks when the sampling setting changes. In this work, we propose a deep plug-and-play method for undersampled MRI reconstruction, which effectively adapts to different sampling settings. Specifically, the image de-aliasing prior is first learned by a deep denoiser trained to remove general white Gaussian noise from synthetic data. Then the learned deep denoiser is plugged into an iterative algorithm for image reconstruction. Results on in vivo data demonstrate that the proposed method provides nice and robust accelerated image reconstruction performance under different undersampling patterns and sampling rates, both visually and quantitatively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/13/eess.IV_2023_09_13/" data-id="clmjn91r100hz0j883yd83nmj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.SD_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T15:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/cs.SD_2023_09_12/">cs.SD - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ASPED-An-Audio-Dataset-for-Detecting-Pedestrians"><a href="#ASPED-An-Audio-Dataset-for-Detecting-Pedestrians" class="headerlink" title="ASPED: An Audio Dataset for Detecting Pedestrians"></a>ASPED: An Audio Dataset for Detecting Pedestrians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06531">http://arxiv.org/abs/2309.06531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Seshadri, Chaeyeon Han, Bon-Woo Koo, Noah Posner, Subhrajit Guhathakurta, Alexander Lerch</li>
<li>for:  introduce a new audio analysis task of pedestrian detection and present a new large-scale dataset for this task</li>
<li>methods:  use audio approaches for pedestrian detection</li>
<li>results:  preliminary results show the viability of using audio approaches, but also show that this task cannot be easily solved with standard approaches<details>
<summary>Abstract</summary>
We introduce the new audio analysis task of pedestrian detection and present a new large-scale dataset for this task. While the preliminary results prove the viability of using audio approaches for pedestrian detection, they also show that this challenging task cannot be easily solved with standard approaches.
</details>
<details>
<summary>摘要</summary>
我们介绍了一新的声音分析任务：人行道检测，并提供了一个大规模的数据集来支持这个任务。虽然初步的结果表明可以使用声音方法进行人行道检测，但也表明这是一项具有挑战性的任务，标准方法无法轻松解决。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SynVox2-Towards-a-privacy-friendly-VoxCeleb2-dataset"><a href="#SynVox2-Towards-a-privacy-friendly-VoxCeleb2-dataset" class="headerlink" title="SynVox2: Towards a privacy-friendly VoxCeleb2 dataset"></a>SynVox2: Towards a privacy-friendly VoxCeleb2 dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06141">http://arxiv.org/abs/2309.06141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Miao, Xin Wang, Erica Cooper, Junichi Yamagishi, Nicholas Evans, Massimiliano Todisco, Jean-François Bonastre, Mickael Rouvier</li>
<li>for: 提高深度学习在语音识别中的成功，但是这产生了伦理、隐私和法律问题，特别是使用大规模自然语音集合。</li>
<li>methods: 提出了一种隐私友好的synthetic VoxCeleb2 dataset的生成方法，以保证数据的隐私、有用性和公平性。</li>
<li>results: 讨论了在下游任务 speaker verification 中使用synthetic数据的挑战。<details>
<summary>Abstract</summary>
The success of deep learning in speaker recognition relies heavily on the use of large datasets. However, the data-hungry nature of deep learning methods has already being questioned on account the ethical, privacy, and legal concerns that arise when using large-scale datasets of natural speech collected from real human speakers. For example, the widely-used VoxCeleb2 dataset for speaker recognition is no longer accessible from the official website. To mitigate these concerns, this work presents an initiative to generate a privacy-friendly synthetic VoxCeleb2 dataset that ensures the quality of the generated speech in terms of privacy, utility, and fairness. We also discuss the challenges of using synthetic data for the downstream task of speaker verification.
</details>
<details>
<summary>摘要</summary>
success of deep learning in speaker recognition 受到大量数据的支持，但是使用大规模数据集的 ethical、隐私和法律问题已经引起了关注。例如，广泛使用的 VoxCeleb2 数据集已经从官方网站上下载不可用。为了解决这些问题，这项工作提出了一个保持隐私性的 Synthetic VoxCeleb2 数据集的INITIATIVE，确保生成的speech质量具备隐私、实用性和公平性。我们还讨论了将 synthetic data 应用于下游任务的难点。
</details></li>
</ul>
<hr>
<h2 id="Can-large-scale-vocoded-spoofed-data-improve-speech-spoofing-countermeasure-with-a-self-supervised-front-end"><a href="#Can-large-scale-vocoded-spoofed-data-improve-speech-spoofing-countermeasure-with-a-self-supervised-front-end" class="headerlink" title="Can large-scale vocoded spoofed data improve speech spoofing countermeasure with a self-supervised front end?"></a>Can large-scale vocoded spoofed data improve speech spoofing countermeasure with a self-supervised front end?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06014">http://arxiv.org/abs/2309.06014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts">https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts</a></li>
<li>paper_authors: Xin Wang, Junichi Yamagishi</li>
<li>For: The paper aims to improve speech spoofing countermeasures by using diverse training data, specifically focusing on vocoded data generated by neural vocoders.* Methods: The paper uses a large-scale dataset of over 9,000 hours of vocoded data based on the VoxCeleb2 corpus, and employs data-hungry self-supervised learning (SSL) models to improve the countermeasures.* Results: The paper shows that using features extracted by an SSL model continually trained on the vocoded data improves the overall countermeasure performance, and further improvement is achieved by using a distilled SSL model that combines the performance of two SSL models before and after continual training. The proposed method outperforms previous best models on challenging unseen test sets, including ASVspoof 2019 logical access, WaveFake, and In-the-Wild.Here are the three key points in Simplified Chinese text:* For: 这篇论文目标是提高语音伪造干扰措施，使用多样化的训练数据，具体是使用神经网络 vocoder 生成的语音数据。* Methods: 论文使用大规模的语音数据集，包括超过 9,000 小时的 vocoded 数据，并使用数据滥用自我超级学习 (SSL) 模型来提高干扰措施。* Results: 论文表明，使用 SSL 模型在 vocoded 数据上不断训练时提取的特征可以提高总体干扰措施的性能，并可以使用两个 SSL 模型之前和之后的 continual training 来提取更多的特征，从而得到更好的性能。<details>
<summary>Abstract</summary>
A speech spoofing countermeasure (CM) that discriminates between unseen spoofed and bona fide data requires diverse training data. While many datasets use spoofed data generated by speech synthesis systems, it was recently found that data vocoded by neural vocoders were also effective as the spoofed training data. Since many neural vocoders are fast in building and generation, this study used multiple neural vocoders and created more than 9,000 hours of vocoded data on the basis of the VoxCeleb2 corpus. This study investigates how this large-scale vocoded data can improve spoofing countermeasures that use data-hungry self-supervised learning (SSL) models. Experiments demonstrated that the overall CM performance on multiple test sets improved when using features extracted by an SSL model continually trained on the vocoded data. Further improvement was observed when using a new SSL distilled from the two SSLs before and after the continual training. The CM with the distilled SSL outperformed the previous best model on challenging unseen test sets, including the ASVspoof 2019 logical access, WaveFake, and In-the-Wild.
</details>
<details>
<summary>摘要</summary>
一种演说伪造防范措施（CM）需要多样化的训练数据。许多数据集使用由语音合成系统生成的伪造数据，但最近发现，由神经网络 vocoder 生成的数据也是有效的伪造训练数据。由于许多神经网络 vocoder 快速生成和生成，这项研究使用了多个神经网络 vocoder，创造了基于 VoxCeleb2 集的 более than 9,000 小时的 vocoded 数据。这项研究研究了如何使用这些大规模的 vocoded 数据提高伪造防范措施，使用具有大量自我监督学习（SSL）模型。实验表明，使用 SSL 模型在多个测试集上执行了总 CM 性能的提高。此外，使用一个新的 SSL 模型，其中两个 SSL 模型在前后 continual 训练后被浸泡。该 CM 使用该浸泡 SSL 模型，在复杂的未看过测试集上表现出优于之前最佳模型，包括 ASVspoof 2019 逻辑访问、WaveFake 和 In-the-Wild。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.SD_2023_09_12/" data-id="clmjn91oh00c50j88c5rdasdb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/eess.AS_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T14:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/eess.AS_2023_09_12/">eess.AS - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="iPhonMatchNet-Zero-Shot-User-Defined-Keyword-Spotting-Using-Implicit-Acoustic-Echo-Cancellation"><a href="#iPhonMatchNet-Zero-Shot-User-Defined-Keyword-Spotting-Using-Implicit-Acoustic-Echo-Cancellation" class="headerlink" title="iPhonMatchNet: Zero-Shot User-Defined Keyword Spotting Using Implicit Acoustic Echo Cancellation"></a>iPhonMatchNet: Zero-Shot User-Defined Keyword Spotting Using Implicit Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06096">http://arxiv.org/abs/2309.06096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong-Hyeok Lee, Namhyun Cho</li>
<li>for:  solves the challenge of barge-in scenarios in human-machine communication</li>
<li>methods:  leverages implicit acoustic echo cancellation (iAEC) techniques and efficient model structure</li>
<li>results:  achieves a 95% reduction in mean absolute error with a minimal increase in model size compared to the baseline model, PhonMatchNet, and demonstrates competitive performance in real-world deployment conditions of smart devices.Here’s the text in Simplified Chinese:</li>
<li>for: 解决人机交互中的冲撞场景</li>
<li>methods: 利用隐式音声退采（iAEC）技术和高效的模型结构</li>
<li>results: 实现了与基eline模型PhonMatchNet相比的95%的平均绝对误差减少，而模型大小增加仅0.13%，并在智能设备的实际部署条件下达到了竞争力表现。<details>
<summary>Abstract</summary>
In response to the increasing interest in human--machine communication across various domains, this paper introduces a novel approach called iPhonMatchNet, which addresses the challenge of barge-in scenarios, wherein user speech overlaps with device playback audio, thereby creating a self-referencing problem. The proposed model leverages implicit acoustic echo cancellation (iAEC) techniques to increase the efficiency of user-defined keyword spotting models, achieving a remarkable 95% reduction in mean absolute error with a minimal increase in model size (0.13%) compared to the baseline model, PhonMatchNet. We also present an efficient model structure and demonstrate its capability to learn iAEC functionality without requiring a clean signal. The findings of our study indicate that the proposed model achieves competitive performance in real-world deployment conditions of smart devices.
</details>
<details>
<summary>摘要</summary>
受到不同领域的人机通信兴趣的增长吸引，这篇论文介绍了一种新的方法 called iPhonMatchNet，该方法解决了叠加场景中用户语音与设备播放音频之间的自 Referencing 问题。提出的模型利用隐式音频反射技术（iAEC）提高用户定义关键词检测模型的效率，实现了95%的平均绝对误差减少，与基线模型PhonMatchNet的模型大小增加0.13%，比基eline模型的性能相当。我们还提出了一种高效的模型结构，并证明其能够无需干净信号学习iAEC功能。我们的研究发现，提案的模型在智能设备实际部署条件下具有竞争性的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/eess.AS_2023_09_12/" data-id="clmjn91pk00e70j88hvbrcd8c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.LG_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T10:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/cs.LG_2023_09_12/">cs.LG - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bregman-Graph-Neural-Network"><a href="#Bregman-Graph-Neural-Network" class="headerlink" title="Bregman Graph Neural Network"></a>Bregman Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06645">http://arxiv.org/abs/2309.06645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Zhai, Lequan Lin, Dai Shi, Junbin Gao</li>
<li>for: 本研究旨在提高图 neural network (GNN) 的表现，特别是在节点分类任务中避免过度简化节点表示和标签，以避免过度融合和错误分类。</li>
<li>methods: 本研究提出了一种基于布格曼距离的双层优化框架，通过引入一种类似于“跳过连接”的机制，以避免GNN层中的过度简化问题。</li>
<li>results: 经验研究表明，布格曼增强的GNN可以有效地避免过度简化问题，并在同性和不同性图上显示出较好的表现。此外，研究还发现布格曼GNN可以在层数较多的情况下产生更稳定的学习结果，表明提出的方法有效。<details>
<summary>Abstract</summary>
Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers is high, suggesting the effectiveness of the proposed method in alleviating the over-smoothing issue.
</details>
<details>
<summary>摘要</summary>
多个最近的研究对图 нейрон网络（GNNs）都集中在将GNN架构设为优化问题的假设上，然而在节点分类任务中，GNN所引入的缓和效应会使连接节点的表示被折衣和混同，导致过度缓和和错分问题。在这篇论文中，我们提出了一种新的二级优化框架 для GNNs， inspirited by the notion of Bregman distance。我们证明了GNN层按照这种方法提出的可以有效地解决过度缓和问题，并且在具有多层的情况下，Bregman GNNs还能够提供更加稳定的学习精度。我们通过了广泛的实验研究，证明了我们的理论结果，并且发现在同类和不同类图中，Bregman GNNs都能够超越原始GNNs的性能。此外，我们的实验还表明，当层数较高时，Bregman GNNs能够更加稳定地学习，这表明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models"><a href="#Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models" class="headerlink" title="Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models"></a>Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06642">http://arxiv.org/abs/2309.06642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi<br>for:* The paper is written for solving inverse problems, specifically focusing on the difficulty of reconstruction tasks and the need for adaptive compute power.methods:* The paper proposes a novel method called “severity encoding” to estimate the degradation severity of noisy, degraded signals in the latent space of an autoencoder.* The method leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and achieve sample-adaptive inference times.results:* The paper demonstrates that the proposed method achieves performance comparable to state-of-the-art diffusion-based techniques, with significant improvements in computational efficiency.Here’s the simplified Chinese translation of the three key points:for:* 本文是为解决反射问题而写的，特别是关注重建任务的难度和计算能力的调整。methods:* 本文提出了一种新的方法，即”严重编码”，用于在自编码器的幂值空间中估计受损信号的严重程度。* 该方法利用预测的损害严重程度来细化反射抽取的轨迹，以实现采样适应的计算时间。results:* 本文表明，提出的方法可以与当前最佳的扩散基本技术相比，在计算效率方面获得显著提高。<details>
<summary>Abstract</summary>
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose a novel method that we call severity encoding, to estimate the degradation severity of noisy, degraded signals in the latent space of an autoencoder. We show that the estimated severity has strong correlation with the true corruption level and can give useful hints at the difficulty of reconstruction problems on a sample-by-sample basis. Furthermore, we propose a reconstruction method based on latent diffusion models that leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and thus achieve sample-adaptive inference times. We utilize latent diffusion posterior sampling to maintain data consistency with observations. We perform experiments on both linear and nonlinear inverse problems and demonstrate that our technique achieves performance comparable to state-of-the-art diffusion-based techniques, with significant improvements in computational efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的推理方法在各种恢复任务中表现不佳，主要是因为它们缺乏适应性。在这篇论文中，我们提出了一种新的方法，即“严重度编码”，可以在循环神经网络中估计干扰严重程度。我们发现，严重度编码对干扰严重程度具有强相关性，并且可以为每个样本提供有用的干扰难度提示。此外，我们还提出了一种基于涉及扩散模型的重建方法，该方法利用预测的严重度编码来精制推理时间。我们使用涉及扩散 posterior 抽样来保持数据一致性。我们在线性和非线性恢复任务中进行了实验，并证明了我们的技术可以与当前状态态别的推理方法相比，具有显著的计算效率提高。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Data-Center-Perspectives"><a href="#Quantum-Data-Center-Perspectives" class="headerlink" title="Quantum Data Center: Perspectives"></a>Quantum Data Center: Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06641">http://arxiv.org/abs/2309.06641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Liang Jiang</li>
<li>for: Quantum Data Center (QDC) is designed to provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing.</li>
<li>methods: QDC combines Quantum Random Access Memory (QRAM) and quantum networks.</li>
<li>results: QDC has the potential to impact business and science, especially in the machine learning and big data industries.Here’s the text in Simplified Chinese:</li>
<li>for: 量子数据中心（QDC）提供了效率、安全性和精度等多种优势，有助于量子计算、通信和探测等领域。</li>
<li>methods: QDC结合量子随机访问存储器（QRAM）和量子网络。</li>
<li>results: QDC具有业务和科学领域的潜在影响，特别是机器学习和大数据领域。<details>
<summary>Abstract</summary>
A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
</details>
<details>
<summary>摘要</summary>
一个量子化的数据中心可能在量子时代发挥重要作用。在这篇论文中，我们介绍了量子数据中心（QDC），它是现有的类型数据中心的量子版本，强调了结合量子随机存储（QRAM）和量子网络的组合。我们认为，QDC将为客户提供高效、安全、精度的好处，并对量子计算、通信和探测产生重要影响。我们通过硬件实现和可能的具体应用来研究这一新的研究方向的科学和商业机会。我们显示了QDC在业务和科学领域的可能影响，特别是机器学习和大数据领域。
</details></li>
</ul>
<hr>
<h2 id="G-Mapper-Learning-a-Cover-in-the-Mapper-Construction"><a href="#G-Mapper-Learning-a-Cover-in-the-Mapper-Construction" class="headerlink" title="$G$-Mapper: Learning a Cover in the Mapper Construction"></a>$G$-Mapper: Learning a Cover in the Mapper Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06634">http://arxiv.org/abs/2309.06634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Alvarado, Robin Belton, Emily Fischer, Kang-Ju Lee, Sourabh Palande, Sarah Percival, Emilie Purvine</li>
<li>for: 这篇论文是关于图像数据分析（TDA）中的Mapper算法可视化技术，其中Mapper算法需要调整多个参数以生成一个”漂亮”的Mapper图。本论文关注选择cover参数。</li>
<li>methods: 我们提出了一种基于$G$-means归一化 clustering的算法，通过逐次进行安德森-达瑞尔测试来选择最佳的cover。我们的拆分过程使用 Gaussian mixture model，以便精心地选择cover根据数据的分布。</li>
<li>results: 我们在 synthetic 和实际数据上进行了实验，结果显示我们的算法可以生成一个保留数据的核心特征的Mapper图。<details>
<summary>Abstract</summary>
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
</details>
<details>
<summary>摘要</summary>
Mapper算法是一种可视化技术在拓扑数据分析（TDA）中，输出一个图表示数据集的结构。Mapper算法需要调整一些参数以生成一个"漂亮"的Mapper图。本文关注选择覆盖参数。我们提出一种算法，通过 repeatedly splitting a cover based on a statistical test for normality，来优化Mapper图的覆盖。我们的拆分过程使用Gaussian mixture model，以便选择精心的覆盖，基于数据的分布。实验表明，我们的算法可以生成覆盖，使得Mapper图保留数据的本质。
</details></li>
</ul>
<hr>
<h2 id="Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning"><a href="#Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning" class="headerlink" title="Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning"></a>Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06628">http://arxiv.org/abs/2309.06628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples">https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples</a></li>
<li>paper_authors: Atticus Beachy, Harok Bae, Jose Camberos, Ramana Grandhi</li>
<li>for: 用于快速trainnear-instantaneously emulator embedded neural network，以便在goal-oriented adaptive learning中减少计算成本。</li>
<li>methods: 使用rapid neural network paradigm，只对最后层连接网络进行微调，通过线性回归技术进行训练。</li>
<li>results: 在多个分析示例和一个Generic hypersonic vehicle飞行参数研究中，提出了一种快速trainnear-instantaneously的Emulator embedded neural network方法，可以减少计算成本，而且Prediction accuracy不受影响。<details>
<summary>Abstract</summary>
Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-based backpropagation, rapid neural network training adjusts only the last layer connection weights by applying a linear regression technique. It is found that the proposed emulator embedded neural network trains near-instantaneously, typically without loss of prediction accuracy. The proposed method is demonstrated on multiple analytical examples, as well as an aerospace flight parameter study of a generic hypersonic vehicle.
</details>
<details>
<summary>摘要</summary>
嵌入式神经网络（physics informed neural network）可以有效地利用多源数据来进行航空工程系统的设计探索。多个神经网络模型的实现被训练于不同的随机初始化。 ensemble 模型的使用可以评估因为训练样本缺乏而导致的 epistemic 模型不确定性。这种不确定性评估是在成功目标适应学习中的关键信息。然而，训练ensemble 模型的成本常常变得不可持和计算挑战，尤其是在不同步骤的适应学习中。在这种情况下，一种新的嵌入式神经网络模型被提出，使用了快速神经网络 Paradigma。这种模型与传统神经网络训练不同，只是在最后层连接 весов而使用了线性回归技术。这种方法可以在几乎实时内训练神经网络，通常不会产生减少预测精度的情况。这种方法在多个分析例题上进行了证明，以及一个涉及到一种常见的高速飞行器的航空飞行参数研究。
</details></li>
</ul>
<hr>
<h2 id="A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes"><a href="#A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes" class="headerlink" title="A Sequentially Fair Mechanism for Multiple Sensitive Attributes"></a>A Sequentially Fair Mechanism for Multiple Sensitive Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06627">http://arxiv.org/abs/2309.06627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phi-ra/SequentialFairness">https://github.com/phi-ra/SequentialFairness</a></li>
<li>paper_authors: François Hu, Philipp Ratz, Arthur Charpentier</li>
<li>for: 本研究旨在解决多个敏感特征之间的关系，以提高决策的公平性。</li>
<li>methods: 本文提出了一种顺序框架，通过多元 Wasserstein 质量函数来逐步实现公平性。此方法提供了一个具有简单解释性的关键函数，可以帮助理解多个敏感特征之间的相互关系。</li>
<li>results: 本研究的实验结果表明，提出的顺序框架可以有效地提高决策的公平性，并且可以适应具有多个敏感特征的实际应用场景。<details>
<summary>Abstract</summary>
In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approximate fairness, enveloping a framework accommodating the trade-off between risk and unfairness. This extension permits a targeted prioritization of fairness improvements for a specific attribute within a set of sensitive attributes, allowing for a case specific adaptation. A data-driven estimation procedure for the derived solution is developed, and comprehensive numerical experiments are conducted on both synthetic and real datasets. Our empirical findings decisively underscore the practical efficacy of our post-processing approach in fostering fair decision-making.
</details>
<details>
<summary>摘要</summary>
通常情况下的算法公平使得敏感变量与相应的分数之间的关系消失。在过去几年，科学社区已经开发出了许多定义和工具来解决这个问题，这些工具在许多实际应用中都很有效。然而，在多个敏感特征的情况下，这些工具和定义的可行性和有效性就变得更加复杂。为解决这个问题，我们提出了一个顺序框架，可以逐步实现公平性 across a set of sensitive features。我们通过利用多重水星凝聚来扩展标准的强人口平衡定义，将多个敏感特征的相互关系进行进一步的调整。这种方法还提供了一个关闭式解的最优、顺序公平预测器，允许直观地了解多个敏感特征之间的相互关系。我们的方法可以逐步扩展到相似公平，包括一个折衔着的批处理方法，允许对特定敏感特征进行Targeted 改进。我们开发了一种基于数据驱动的估计过程，并在Synthetic和实际数据上进行了广泛的数值实验。我们的实验结果明确地证明了我们的后处理方法在促进公平决策中的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Deep-Neural-Networks-via-Semi-Structured-Activation-Sparsity"><a href="#Accelerating-Deep-Neural-Networks-via-Semi-Structured-Activation-Sparsity" class="headerlink" title="Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity"></a>Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06626">http://arxiv.org/abs/2309.06626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Grimaldi, Darshan C. Ganji, Ivan Lazarevich, Sudhakar Sah</li>
<li>for: 提高深度神经网络（DNNs）在嵌入式设备上的处理效率，以便更广泛地部署。</li>
<li>methods: 利用网络特征图的稀疏性来降低推理延迟。我们提出一种解决方案，通过轻量级的运行时修改来实现半结构化活动稀疏性。</li>
<li>results: 我们对各种图像分类和物体检测任务使用了多种模型进行了广泛的评估。结果显示，我们的方法可以提高速度，同时减少精度下降。例如，使用ResNet18模型在ImageNet dataset上，我们的方法可以提高速度$1.25\times$，同时精度下降$1.1%$。此外，当与当前最佳结构减少技术相结合时，得到的模型可以提供良好的时间-准确性质量比。<details>
<summary>Abstract</summary>
The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop of $1.1\%$ for the ResNet18 model on the ImageNet dataset. Furthermore, when combined with a state-of-the-art structured pruning method, the resulting models provide a good latency-accuracy trade-off, outperforming models that solely employ structured pruning techniques.
</details>
<details>
<summary>摘要</summary>
需求深度神经网络（DNNs）在嵌入式设备上高效处理是一个重要的挑战，限制其部署。利用网络特征图的稀疏性可以降低其推理延迟。已知无结构稀疏会导致更低的准确度下降，而结构稀疏则需要大量的推理引擎更改以获得延迟优化。为解决这个挑战，我们提出一种inducedemi-structured activation稀疏性可以通过小改动的推理引擎来实现。为了在推理时 достичь高速度倍数，我们设计了一种稀疏训练方法，拥有最终活动的位置信息来计算General Matrix Multiplication（GEMM）。我们对多种模型进行了广泛的评估，包括图像分类和物体检测任务。结果显示，我们的方法可以提高ResNet18模型的速度倍数，同时准确度下降只有1.1%，在ImageNet数据集上。此外，当与现有的结构剪枝方法结合使用时，得到的模型具有良好的延迟准确度质量平衡，超过只使用结构剪枝技术的模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems"><a href="#On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems" class="headerlink" title="On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems"></a>On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06622">http://arxiv.org/abs/2309.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexis M. H. Teter, Yongxin Chen, Abhishek Halder</li>
<li>for:  solves the Schrödinger bridge problem, a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints.</li>
<li>methods:  uses contractive fixed point recursions, which are dynamic versions of the Sinkhorn iterations, to solve the Schrödinger systems with guaranteed linear convergence.</li>
<li>results:  provides a priori estimates for the contraction coefficients associated with the convergence of the Schrödinger systems, and provides new geometric and control-theoretic interpretations for the same. Additionally, the paper shows the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.<details>
<summary>Abstract</summary>
Schr\"{o}dinger bridge is a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints. A popular method to numerically solve the Schr\"{o}dinger bridge problems, in both classical and in the linear system settings, is via contractive fixed point recursions. These recursions can be seen as dynamic versions of the well-known Sinkhorn iterations, and under mild assumptions, they solve the so-called Schr\"{o}dinger systems with guaranteed linear convergence. In this work, we study a priori estimates for the contraction coefficients associated with the convergence of respective Schr\"{o}dinger systems. We provide new geometric and control-theoretic interpretations for the same. Building on these newfound interpretations, we point out the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.
</details>
<details>
<summary>摘要</summary>
“尹朋因桥”是一个测度控制问题，旨在将一个初始状态分布调整到另一个状态，并且受到导入的扩散和时间限制。一种广泛使用的方法来确解“尹朋因桥”问题是透过对称固定点回归，这些回归可以看作是动态版本的well-known Sinkhorn迭代，并且在某些假设下，它们可以将“尹朋因桥”系统解决，并且具有线性对称增强的特性。在这个研究中，我们研究“尹朋因桥”系统的内在稳定性，并且提供新的几何和控制理论的解释。基于这些新的解释，我们点出了改进 computation的可能性，尤其是针对线性SBP的最差情况下的对称系数。
</details></li>
</ul>
<hr>
<h2 id="RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models"><a href="#RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models" class="headerlink" title="RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models"></a>RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06619">http://arxiv.org/abs/2309.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Li, Zexin Li, Wei Yang, Cong Liu</li>
<li>for: 这个论文主要是为了解决语言模型（LM）在不同设备上的部署问题，因为它们的计算成本和不可预测的推理延迟问题。</li>
<li>methods: 这个论文使用了一种名为RT-LM的实时推理 uncertainty-aware 资源管理环境，以解决LM的不确定性induced延迟性能下降问题。</li>
<li>results: 实验结果表明，RT-LM可以减少响应时间和提高吞吐量，但带来较小的运行时开销。<details>
<summary>Abstract</summary>
Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM, an uncertainty-aware resource management ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how specific input uncertainties, adversely affect latency, often leading to an increased output length. Exploiting these insights, we devise a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. Utilizing this quantification as a latency heuristic, we integrate the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading. Quantitative experiments across five state-of-the-art LMs on two hardware platforms demonstrates that RT-LM can significantly reduce the average response time and improve throughput while incurring a rather small runtime overhead.
</details>
<details>
<summary>摘要</summary>
最近的语言模型（LM）的进步已经吸引了广泛的关注，因为它们可以生成人类化的回复。虽然在各种应用程序中，如对话AI，它们展示了一个美好的未来，但是由于其极高的计算成本和不可预测的推理延迟，LM在不同的设备上部署具有挑战性。这种不确定性引起的延迟variation，被认为是由语言自身的不确定性所致，可能导致计算不效率和总性性能下降，特别是在高负载下。为了理解和改善LM在实时响应需求下的影响，我们首先要理解、量化和优化LM中uncertainty-induced延迟性性能变化。特别是，我们介绍了RT-LM，一个uncertainty-awareness资源管理生态系统，用于实时推理LM。RT-LM可以量化特定输入不确定性如何 adversely affect延迟，并经常导致输出长度增加。利用这些 Insight，我们提出了一种轻量级 yet effective的方法，通过在运行时 dynamically correlating输入文本不确定性与输出长度。通过这种量化作为延迟heuristic，我们将不确定性信息integrated into a system-level调度器，并探索了多种由不确定性引起的优化机会，包括不确定性-aware prioritization、动态 консолидация和策略性CPU卸载。实验表明，RT-LM可以在五种state-of-the-art LM上对两种硬件平台进行quantitative experiments，显著降低响应时间和提高吞吐量，而且runtime overhead很小。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials"><a href="#Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials" class="headerlink" title="Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials"></a>Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06613">http://arxiv.org/abs/2309.06613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang, Clémence Bos, Stefan Sandfeld, Ruth Schwaiger</li>
<li>for: 这个论文是研究氧化镍-钴复合材料的，使用了探针压测技术。</li>
<li>methods: 论文使用了无监督学习技术，特别是加aussian mixture模型，来分析数据，并确定了不同 mechanical phases的机械性能。</li>
<li>results: 研究发现，通过 cross-validation方法，可以确定数据量是否充分，并提出了数据量所需的建议。<details>
<summary>Abstract</summary>
In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of "mechanical phases" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.
</details>
<details>
<summary>摘要</summary>
在这项研究中，氧化铜-铬复合材料被使用材料测试设备进行纳米压测。数列压测点被布置在样品上，以便获得大量数据，包括不同压测深度下的材料年轻模ulus和硬度的数百个测量结果。在这项研究中，我们采用了一种无监督学习技术—— Gaussian mixture model，来分析数据，并确定了机械相的数量和相应的机械性能。此外，我们还引入了一种验证方法，以判断数据量是否足够，并建议数据的充足量，以解决物理学问题中常见 yet difficult to resolve的数据不够问题。
</details></li>
</ul>
<hr>
<h2 id="Harmonic-NAS-Hardware-Aware-Multimodal-Neural-Architecture-Search-on-Resource-constrained-Devices"><a href="#Harmonic-NAS-Hardware-Aware-Multimodal-Neural-Architecture-Search-on-Resource-constrained-Devices" class="headerlink" title="Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices"></a>Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06612">http://arxiv.org/abs/2309.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Imed Eddine Ghebriout, Halima Bouzidi, Smail Niar, Hamza Ouarnoughi</li>
<li>for: 该论文旨在提出一种整合硬件考虑的多Modal Neural Networks（MM-NN）框架，以提高在有限资源设备上的多模态信息处理能力。</li>
<li>methods: 该论文提出了一种两级优化策略，包括对单模态网络架构和多模态融合策略进行优化，同时考虑硬件约束。</li>
<li>results: 试验结果表明，使用该框架可以在多个设备和多模态数据集上实现Accuracy提高约10.9%，延迟降低约1.91倍，能效率提高约2.14倍。<details>
<summary>Abstract</summary>
The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of Harmonic-NAS over state-of-the-art approaches achieving up to 10.9% accuracy improvement, 1.91x latency reduction, and 2.14x energy efficiency gain.
</details>
<details>
<summary>摘要</summary>
Recent interest in Multimodal Neural Networks (MM-NN) has surged due to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this enhances the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Moreover, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy.In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of Harmonic-NAS over state-of-the-art approaches, achieving up to 10.9% accuracy improvement, 1.91x latency reduction, and 2.14x energy efficiency gain.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach"><a href="#Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach" class="headerlink" title="Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach"></a>Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06604">http://arxiv.org/abs/2309.06604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Esmaeili, Eric T. Matson, Julia T. Rayz</li>
<li>for: 提出一种完全自动和协同的Agent-based机制，用于分布式机器学习算法选择和hyperparameter调整。</li>
<li>methods: 基于现有的Agent-based层次机器学习平台，增强查询结构以支持选择和调整机制，不受限于具体的学习、选择和调整机制。</li>
<li>results: 经过理论评估、正式验证和分析研究，确认方法的正确性和资源利用率，时间复杂度和空间复杂度与资源大小成线性关系。<details>
<summary>Abstract</summary>
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical study to demonstrate the correctness, resource utilization, and computational efficiency of our technique. According to the results, our solution is totally correct and exhibits linear time and space complexity in relation to the size of available resources. To provide concrete examples of how the proposed methodologies can effectively adapt and perform across a range of algorithmic options and datasets, we have also conducted a series of experiments using a system comprised of 24 algorithms and 9 datasets.
</details>
<details>
<summary>摘要</summary>
Algorithm selection和超参调整是学术和应用机器学习的关键步骤，然而这些步骤正在逐渐变得更加细腻，这是因为机器学习资源的急剧增加、多样性和分布化的问题。在应用多智能体系统的设计机器学习平台时，这些特点就变得非常明显。这篇论文提出了一种完全自动和合作的智能体系统来选择分布式组织的机器学习算法和同时调整其超参数。我们的方法基于现有的智能体系统机器学习平台，并在其查询结构上增加了支持这些功能的扩展。我们已经对我们的技术进行了理论评估、正式验证和分析研究，以证明我们的方法的正确性、资源利用率和计算效率。根据结果，我们的解决方案是完全正确的，并且在资源大小为基准的情况下显示出线性时间和空间复杂度。为了让读者更好地理解我们的方法在不同的算法和数据集上的应用和表现，我们还进行了一系列实验，使用了24种算法和9个数据集。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning"><a href="#Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning" class="headerlink" title="Reasoning with Latent Diffusion in Offline Reinforcement Learning"></a>Reasoning with Latent Diffusion in Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06599">http://arxiv.org/abs/2309.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, Glen Berseth</li>
<li>for: 这篇论文的目的是提出一种基于离线学习的强化学习方法，以学习从静止数据集中获得高奖励策略，而不需要进一步与环境交互。</li>
<li>methods: 该方法使用了缺省方法，即使用 Monte Carlo 返回到去的样本来为奖励进行条件。然而，这些方法需要精细地调整，并且在多模态数据上表现不佳。而该文提出的新方法则是使用缺省扩散来模型支持流程中的压缩 latent skill，从而避免了推断错误。</li>
<li>results: 该文的实验结果表明，使用该新方法可以在 D4RL 测试环境中实现state-of-the-art 性能，特别是在长时间 horizon 和罕见奖励任务中表现出色。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks.
</details>
<details>
<summary>摘要</summary>
偏向RL（偏向学习）在线上不需要继续与环境交互，可以从静止数据集中学习高奖策略。然而，偏向RL中的一个关键挑战在于有效地将静止数据集中的部分不优轨迹缝合，以避免因数据集缺失支持而导致的推断错误。现有的方法使用保守的方法，困难调整并在多模态数据上强制性做出差异（我们所示），或者基于噪声的Monte Carlo返回值来修改奖励。在这种工作中，我们提议一种新的方法，利用潜在扩散来模型在支持轨迹序列中的压缩 latent 技能。这使得学习Q函数时避免推断错误，并且在批处理中使用批处理来约束。潜在空间也是表达力强，慈善地处理多模态数据。我们显示，学习的时间抽象 latent 空间包含更加细致的任务特定信息，比raw state-actions 更好地进行奖励分配和奖励扩散。我们的方法在 D4RL 测试上达到了状态之上的性能，特别是在长期、稀缺奖励任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning"><a href="#Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning" class="headerlink" title="Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning"></a>Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06597">http://arxiv.org/abs/2309.06597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Behzad Dariush, Chiho Choi, Mykel Kochenderfer</li>
<li>for: This paper is written for researchers working on visual scene understanding and related fields, with the goal of improving the trustworthiness and interpretability of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS).</li>
<li>methods: The paper introduces a novel dataset called Rank2Tell, which provides dense annotations of various semantic, spatial, temporal, and relational attributes of important objects in complex traffic scenarios. The dataset is multi-modal and includes ego-centric data, and the authors propose a joint model for joint importance level ranking and natural language captions generation to benchmark the dataset.</li>
<li>results: The authors demonstrate the performance of their dataset and joint model with quantitative evaluations, and show that the dataset is a valuable resource for researchers working on visual scene understanding and related fields.<details>
<summary>Abstract</summary>
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
广泛的商业自动驾车（AV）和高级驾驶辅助系统（ADAS）的普及可能受到社会的接受程度的限制，这对于驾驶人的信任和理解度是非常重要。然而，现代自动驾驶系统软件往往运用黑盒式人工智能模型，实现这个目标是非常具有挑战性。为了解决这个问题，本文提出了一个新的数据集，名为Rank2Tell，这是一个多modal的自我中心数据集，用于排名重要性和说明重要性的原因。这个数据集使用了多种关闭和开放式的视觉问题回答，提供了丰富的标签和各种semantic、空间、时间和关联性的属性。这个数据集的独特特点和标签使其成为了研究视景理解和相关领域的重要资源。此外，我们也引入了一个共同模型，用于联合重要性水平排名和自然语言描述生成，以评估我们的数据集和展示表现。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Gradient-based-MAML-in-LQR"><a href="#Convergence-of-Gradient-based-MAML-in-LQR" class="headerlink" title="Convergence of Gradient-based MAML in LQR"></a>Convergence of Gradient-based MAML in LQR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06588">http://arxiv.org/abs/2309.06588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negin Musavi, Geir E. Dullerud</li>
<li>for: 这个研究报告的主要目标是调查Model-agnostic Meta-learning（MAML）在线性系统quadratic optimal control（LQR）中的本地受欢迎性。</li>
<li>methods: 这个研究使用MAML和其变种来快速适应新任务，并且利用过去学习知识来进行预测。但是，MAML的理论保证仍然未知，因为非 convex 和其结构，使得在动态系统设置下保持稳定性变得更加困难。这个研究将MAML应用于LQR设置，提供本地收敛保证，同时维护动态系统的稳定性。</li>
<li>results: 这个研究提供了简单的数字结果，以证明MAML在LQR任务中的收敛性质。<details>
<summary>Abstract</summary>
The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
</details>
<details>
<summary>摘要</summary>
本研究的主要目标是调查模型独立元学习（MAML）在线性系统剑方优控（LQR）中的本地叉度特性。MAML和其变种在各种学习领域，如回归、分类和奖励学习中，用于快速适应新任务，但其理论保证仍然未知，这使得在动态系统设置下稳定性变得更加挑战。本研究将MAML应用于LQR设置，提供本地收敛保证，同时保持动态系统的稳定性。此外，文章还提供了简单的数字结果，以证明MAML在LQR任务中的收敛性质。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction"><a href="#Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction" class="headerlink" title="Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction"></a>Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06584">http://arxiv.org/abs/2309.06584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, Cui Tao</li>
<li>for: 预测阿尔茨海默症和相关性股症 (ADRD) 的风险。</li>
<li>methods: 使用图 neural network (GNN) 与养成数据进行 ADRD 风险预测，并 introducing 一种新的关系重要性评估方法，以提供全面的解释。</li>
<li>results: VGNN 比 Random Forest 和 Light Gradient Boost Machine 高出 10% 的 receiver operating characteristic 领域下的覆盖率。这种方法可能在提供 ADRD 进程中的值得考虑因素方面发挥重要作用。<details>
<summary>Abstract</summary>
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.   We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient Boost Machine as baselines. We further used our relation importance method to clarify the key relationships for ADRD risk prediction. VGNN surpassed other baseline models by 10% in the area under the receiver operating characteristic. The integration of the GNN model and relation importance interpretation could potentially play an essential role in providing valuable insight into factors that may contribute to or delay ADRD progression.   Employing a GNN approach with claims data enhances ADRD risk prediction and provides insights into the impact of interconnected medical code relationships. This methodology not only enables ADRD risk modeling but also shows potential for other image analysis predictions using claims data.
</details>
<details>
<summary>摘要</summary>
来自美国的数据显示，阿尔茨海默症和相关的认知障碍（ADRD）是死亡的第六大原因，强调了 precisione ADRD 预测的重要性。虽然最近的进步主要依靠图像分析，但不是所有病人都会进行医疗图像诊断。我们的目标是使用图像分析（GNN）与保险资料进行 ADRD 预测。为了解释预测的原因，我们引入了一个创新的方法，以评估关系的重要性和其影响 ADRD 预测的可能性。我们使用了Variational Regularized Encoder-decoder Graph Neural Network（VGNN）估计 ADRD 可能性。我们创建了三个情况来评估模型的效率，使用 Random Forest 和 Light Gradient Boost Machine 作为基准。我们还使用我们的关系重要性方法来阐明 ADRD 预测中的关键关系。VGNN 在受到Receiver Operating Characteristic 的面积下表现出乎 Random Forest 和 Light Gradient Boost Machine 的10%。通过融合 GNN 模型和关系重要性解释，我们可以实现更好的 ADRD 预测和获得关于可能导致或延迟 ADRD 进程的有用信息。使用 GNN 方法与保险资料可以提高 ADRD 预测的精度，并且显示了这种方法在其他图像分析预测中的应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype"><a href="#Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype" class="headerlink" title="Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype"></a>Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06582">http://arxiv.org/abs/2309.06582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roger Rusack, Bhargav Joshi, Alpana Alpana, Seema Sharma, Thomas Vadnais</li>
<li>for: 这项研究的目的是为了开发一种能够高效准确地重建电子 incident 的机器学习方法，以应对 future 高能物理实验中的探测器技术问题。</li>
<li>methods: 这项研究使用了最新的机器学习技术，包括三维粒子探测器和时间分辨精度测试等方法，来重建 incident 电子的能量。</li>
<li>results: 研究人员通过使用机器学习方法，从三维粒子探测器中的能量值来重建 incident 电子的能量，并获得了一定的成果。<details>
<summary>Abstract</summary>
We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.
</details>
<details>
<summary>摘要</summary>
我们提供了一个新的公共可用的数据集，包含一种新的加速器实验室中的射电计仪器的模拟数据。这个仪器将有超过六百万个通道，每个通道都可以进行位置、离子化和精度时间测量。重建这些事件的方式具有巨大的挑战，我们正在使用最新的机器学习技术来解决这个问题。在这个开发过程中，我们建立了一个大型原型，包含12,000个通道，并将高能电子束照射到它上。使用机器学习方法，我们已经从三维hit的能量中重建了入射电子的能量，准确程度已知。通过公开这些数据，我们希望能吸引专业人士使用机器学习技术来开发高效、准确的图像重建方法。
</details></li>
</ul>
<hr>
<h2 id="Promises-of-Deep-Kernel-Learning-for-Control-Synthesis"><a href="#Promises-of-Deep-Kernel-Learning-for-Control-Synthesis" class="headerlink" title="Promises of Deep Kernel Learning for Control Synthesis"></a>Promises of Deep Kernel Learning for Control Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06569">http://arxiv.org/abs/2309.06569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Reed, Luca Laurenti, Morteza Lahijanian</li>
<li>for: 本文用于控制synthesis的stochastic dynamical systems against complex specifications, using Deep Kernel Learning (DKL) </li>
<li>methods: 使用DKL学习未知系统从数据中，并正确地抽象DKL模型为Interval Markov Decision Process (IMDP)进行控制synthesis with correctness guarantees.</li>
<li>results: 在various benchmarks上，包括5-D nonlinear stochastic system, 控制synthesis with DKL可以大幅超过现有竞争方法的性能。<details>
<summary>Abstract</summary>
Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-of-the-art competitive methods.
</details>
<details>
<summary>摘要</summary>
深度kernel学习（DKL）结合神经网络的表示能力和泊松过程的不确定性量化，因此它可能是控制复杂动力系统的有望工具。在这项工作中，我们开发了可扩展的抽象基础结构，使得使用DKL进行动力系统控制合成可以遵循复杂规范。 Specifically，我们考虑了时间逻辑规范，并创建了从数据学习DKL模型到Interval Markov Decision Process（IMDP）的端到端框架，以实现控制合成具有正确保证。此外，我们确定了深度架构，使得精准学习和快速抽象计算可以准确地学习和控制动力系统。我们的方法在多个标准准例上进行了证明，包括5个维的非线性随机系统，显示了DKL控制合成可以大幅超越现有竞争方法。
</details></li>
</ul>
<hr>
<h2 id="Commands-as-AI-Conversations"><a href="#Commands-as-AI-Conversations" class="headerlink" title="Commands as AI Conversations"></a>Commands as AI Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06551">http://arxiv.org/abs/2309.06551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dspinellis/ai-cli">https://github.com/dspinellis/ai-cli</a></li>
<li>paper_authors: Diomidis Spinellis</li>
<li>for: 这个论文主要目标是提高命令行界面的用户体验，使其更加智能和易用。</li>
<li>methods: 该论文提出了一种基于OpenAI API的自然语言处理技术，可以将用户的自然语言提示转换成可执行的命令行指令。</li>
<li>results: 该论文通过对多个命令行工具的集成，使得命令行界面更加智能和易用，开辟了新的可能性和应用场景。<details>
<summary>Abstract</summary>
Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? "ai-cli," an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, "ai-cli" transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The "ai-cli" tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platform applicability.
</details>
<details>
<summary>摘要</summary>
开发者和数据科学家经常遇到写入命令行输入的困难，即使有图形界面或工具如ChatGPT可以帮助。 Solution? “ai-cli”，一个开源系统， Draw inspiration from GitHub Copilot，可以将自然语言提示转换成多种Linux命令行工具可执行的命令。通过对OpenAI的API进行交互，通过JSON HTTP请求转换用户查询到可执行的命令。然而，在多个命令行工具之间集成人工智能帮助，特别是在开源环境中，可能会复杂。历史上，操作系统可以仅仅作为中间人，但每个工具的特有功能和缺乏一致的方法使得中心化集成变得困难。 “ai-cli”工具通过在每个程序的Readline库API中动态加载和链接，使命令行界面变得更加智能和易用，开启了进一步改进和跨平台应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Transfer-Learning"><a href="#Distributionally-Robust-Transfer-Learning" class="headerlink" title="Distributionally Robust Transfer Learning"></a>Distributionally Robust Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06534">http://arxiv.org/abs/2309.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rvr-account/rvr">https://github.com/rvr-account/rvr</a></li>
<li>paper_authors: Xin Xiong, Zijian Guo, Tianxi Cai</li>
<li>for: 这篇论文是为了解决对于转移学习中的一个问题，即如何将来自不同来源模型的知识融合到目标数据上，以提高预测性能。</li>
<li>methods: 这篇论文提出了一个新的转移学习方法，即分布robust优化（TransDRO），它不受来源数据的严格相似性限制，而是通过最大化类似损失函数的调整，以提高预测性能。</li>
<li>results: 研究人员透过实验和分析多元机构电子健康纪录数据，证明了TransDRO的具有稳定性和准确性，并且在转移学习应用中具有优秀的表现。<details>
<summary>Abstract</summary>
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.
</details>
<details>
<summary>摘要</summary>
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.Here is the translation in Traditional Chinese:Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations"><a href="#Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations" class="headerlink" title="Hierarchical Multi-Task Learning Framework for Session-based Recommendations"></a>Hierarchical Multi-Task Learning Framework for Session-based Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06533">http://arxiv.org/abs/2309.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sejoon Oh, Walid Shalaby, Amir Afsharinejad, Xiquan Cui</li>
<li>for: 这篇论文的目的是提出一种基于Session-based Recommender Systems (SBRSs)的多任务学习（MTL）架构，以提高推荐性能和泛化性。</li>
<li>methods: 这篇论文使用了一种叫做 Hierarchical MTL (H-MTL) 的架构，它在预测任务之间设置了层次结构，并将auxiliary task的输出传递给主要任务。这种结构使得主要任务得到了更丰富的输入特征，并且提高了预测结果的可解释性。</li>
<li>results: 实验结果表明，与现有的 SBRSs 相比， HierSRec 在两个session-based recommendation dataset上的下一个项预测精度得到了提高。此外，HierSRec 使用了一种精炼的候选项集（比如4%的总项），以实现可扩展的推荐。<details>
<summary>Abstract</summary>
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (e.g., 4% of total items) per test example using the category prediction. Experiments show that HierSRec outperforms existing SBRSs as per next-item prediction accuracy on two session-based recommendation datasets. The accuracy of HierSRec measured with the carefully-curated candidate items aligns with the accuracy of HierSRec calculated with all items, which validates the usefulness of our candidate generation scheme via H-MTL.
</details>
<details>
<summary>摘要</summary>
Session-based recommender systems (SBRSs) 已经表现出优秀的推荐性能，而多任务学习 (MTL) 也在 SBRSs 中应用，以提高预测准确性和泛化能力。层次多任务学习 (H-MTL) 设置一个层次结构，在预测任务之间，并将auxiliary任务的输出传递给主任务。这种层次结构导致主任务的输入特征更加丰富，并提高预测结果的解释性，相比既有MTL框架。然而，H-MTL框架在 SBRSs 中还没有被研究。在这篇论文中，我们提出了 HierSRec，它将 H-MTL 框架应用于 SBRSs。HierSRec 使用metadata-aware Transformer来编码给定的会话，然后进行下一个类型预测（即auxiliary任务），使用会话编码。接着，HierSRec 使用类型预测结果和会话编码进行下一个项目预测（即主任务）。为了可扩展的推理，HierSRec 创建了一个具有4%的总项目数的紧凑集（例如，每个测试示例）的候选项目。实验结果显示，HierSRec 在两个会话基于推荐数据集上的下一个项目预测精度方面高于现有的 SBRSs。HierSRec 对于 carefully-curated 的候选项目的准确率与所有项目的准确率相匹配，这 Validates 我们的候选项目生成方案via H-MTL。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers"><a href="#Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers" class="headerlink" title="Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers"></a>Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06526">http://arxiv.org/abs/2309.06526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/dp-tabtransformer">https://github.com/ibm/dp-tabtransformer</a></li>
<li>paper_authors: Xilong Wang, Chia-Mu Yu, Pin-Yu Chen</li>
<li>for: 本研究探讨了将数据隐私和机器学习表格数据结合使用的可能性，具体来说是在预训练和精度调整中使用TabTransformer模型，并使用多种参数效率调整方法（PEFT），包括Adapter、LoRA和Prompt Tuning。</li>
<li>methods: 本研究使用了多种PEFT方法，包括Adapter、LoRA和Prompt Tuning，以提高预训练和精度调整的效率。</li>
<li>results: 对ACSIncome数据集进行了广泛的实验，发现PEFT方法可以在预训练和精度调整中超越传统方法，以至于实现更好的参数效率、隐私和准确率之间的平衡。<details>
<summary>Abstract</summary>
For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.
</details>
<details>
<summary>摘要</summary>
为机器学习 tabular 数据，表格转换器（TabTransformer）是一个现代神经网络模型，而数据隐私（DP）是保证数据隐私的必要组成部分。在这篇论文中，我们探索将这两个方面结合在一起的enario，即具有权限限制的预训练和精细调整（PEFT）方法，包括 Adapter、LoRA 和 Prompt Tuning。我们在 ACSIncome 数据集进行了广泛的实验，结果表明，这些 PEFT 方法在下游任务的准确率和可训练参数数量方面占据了优势，从而实现了参数效率、隐私和准确率之间的改进的平衡。我们的代码可以在github.com/IBM/DP-TabTransformer 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Q-learning-Approach-for-Adherence-Aware-Recommendations"><a href="#A-Q-learning-Approach-for-Adherence-Aware-Recommendations" class="headerlink" title="A Q-learning Approach for Adherence-Aware Recommendations"></a>A Q-learning Approach for Adherence-Aware Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06519">http://arxiv.org/abs/2309.06519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Faros, Aditya Dave, Andreas A. Malikopoulos</li>
<li>for: 本研究目的是解决人工智能提供的建议received by human decision-makers (HDM) in high-stakes and safety-critical scenarios, while HDM retains the ultimate decision-making responsibility.</li>
<li>methods: 该算法使用了”遵循度”来捕捉HDML suivre the recommended actions的频率，并 derive the optimal recommendation policy in real time.</li>
<li>results: 我们证明了该Q学习算法的优化性和效果，并在多种场景中进行了性能评估。<details>
<summary>Abstract</summary>
In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an "adherence-aware Q-learning" algorithm to address this problem. The algorithm learns the "adherence level" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.
</details>
<details>
<summary>摘要</summary>
在许多高度危险和安全性有着重要意义的实际场景中，人工智能推荐（HDM）可能会收到人工智能的建议，而持有最终决策责任。在这封信中，我们开发了一种“遵循度意识Q学习”算法，以解决这个问题。该算法学习推荐行为的遵循度，并在实时基础上计算最佳推荐策略。我们证明算法的优化目标函数的整体收敛性，并对各种场景进行性能评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets"><a href="#Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets" class="headerlink" title="Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets"></a>Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06503">http://arxiv.org/abs/2309.06503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramya Tekumalla, Juan M. Banda</li>
<li>for: 本研究的目的是为了评估大语言模型GPT-4和弱监督来自动标注COVID-19疫苗相关的推文，以比较其表现和人工标注者。</li>
<li>methods: 本研究使用了GPT-4（2023年3月23日版本）和弱监督来自动标注COVID-19疫苗相关的推文，没有进行任何额外定制或指导。</li>
<li>results: 研究发现，使用GPT-4和弱监督可以准确地标注COVID-19疫苗相关的推文，并且表现比人工标注者更好。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对医疗业和社会的挑战很大。随着 COVID-19 疫苗的快速发展，社交媒体平台上关于疫苗的话题成为了公众的焦点。可以通过分析社交媒体上的 tweet 获得有价值的公共健康研究和政策制定者的信息。然而，手动标注大量 tweet 是时间consuming 和昂贵的。在本研究中，我们评估了 Large Language Models（在这种情况下是 GPT-4 （2023年3月23日版））和弱级指导，以标识 COVID-19 疫苗相关的 tweet，并与人工标注者进行比较。我们利用了 manually 精心准备的金标准数据集，并使用 GPT-4 提供标签，没有任何额外的 fine-tuning 或指导，在单击模式下（没有额外的推荐）。
</details></li>
</ul>
<hr>
<h2 id="A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale"><a href="#A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale" class="headerlink" title="A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale"></a>A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06497">http://arxiv.org/abs/2309.06497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, Michael Rabbat</li>
<li>for: 这个论文是用来训练神经网络的在线和随机优化算法，属于AdaGrad家族的方法。</li>
<li>methods: 这个算法使用了一个块对角减少矩阵AdaGrad方法，每个块都包含一个粗略的克로内кер产品矩阵approxiamtion来处理每个神经网络参数。</li>
<li>results: 作者在这篇论文中提供了这个算法的完整描述以及实现方法，并通过对 ImageNet ResNet50 进行ablation研究，证明了Shampoo算法在训练深度神经网络时的优势。<details>
<summary>Abstract</summary>
Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo's superiority over standard training recipes with minimal hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
<!--TRANSLATION START-->帕逻拜（Shampoo）是一种在线和随机优化算法，属于AdaGrad家族的方法，用于训练神经网络。它构建了一个块对角预conditioner，每个块包含一个粗略 kronecker 产品approximation来全矩度AdaGrad方法的每个参数。在这项工作中，我们提供了完整的算法描述以及我们实现的性能优化，以在PyTorch中训练深度网络。我们的实现使得可以快速分布在多个GPU上进行分布式数据并行训练，通过PyTorch的DTensor数据结构分布内存和计算相关于块的每个参数，并在每个迭代执行一个AllGather primitives来计算搜索方向。这一主要性能提升使得我们可以在每步时钟时间中减少至少10%的性能。我们验证了我们的实现，通过对ImageNet ResNet50的训练进行减少研究， demonstrating Shampoo的优越性，与标准训练方法相比，需要 minimal hyperparameter tuning。<!--TRANSLATION END-->Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons"><a href="#Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons" class="headerlink" title="Learning topological operations on meshes with application to block decomposition of polygons"></a>Learning topological operations on meshes with application to block decomposition of polygons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06484">http://arxiv.org/abs/2309.06484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Yulong Pan, Per-Olof Persson</li>
<li>for: 提高无结构三角形和四角形网格质量</li>
<li>methods: 使用自适应学习法，不含先验规则，通过自身玩 игры来提高网格质量，涉及到标准的本地和全局元素操作</li>
<li>results: 实现节点度差与理想值的差异最小化，即内部顶点的差异最小化，以提高网格质量<details>
<summary>Abstract</summary>
We present a learning based framework for mesh quality improvement on unstructured triangular and quadrilateral meshes. Our model learns to improve mesh quality according to a prescribed objective function purely via self-play reinforcement learning with no prior heuristics. The actions performed on the mesh are standard local and global element operations. The goal is to minimize the deviation of the node degrees from their ideal values, which in the case of interior vertices leads to a minimization of irregular nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于学习的框架，用于改进不结构化三角形和四边形的网格质量。我们的模型通过自动游戏学习方法，不含任何先验知识，来提高网格质量。Actions performed on the mesh include标准的本地和全局元素操作。我们的目标是将节点度 deviation from their ideal values as close as possible, which in the case of interior vertices leads to a minimization of irregular nodes.Here's the word-for-word translation:我们提出了一种基于学习的框架，用于改进不结构化三角形和四边形的网格质量。我们的模型通过自动游戏学习方法，不含任何先验知识，来提高网格质量。Actions performed on the mesh include标准的本地和全局元素操作。我们的目标是将节点度 deviation from their ideal values as close as possible, which in the case of interior vertices leads to a minimization of irregular nodes.
</details></li>
</ul>
<hr>
<h2 id="Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation"><a href="#Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation" class="headerlink" title="Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation"></a>Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06472">http://arxiv.org/abs/2309.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Golling, Samuel Klein, Radha Mastandrea, Benjamin Nachman, John Andrew Raine</li>
<li>for: 这篇论文是为了解决高能物理和其他领域中数据分析中的数据变换问题，通过重新权重而不是直接使用权重来实现数据变换。</li>
<li>methods: 这篇论文提出了一种叫做”流体流”的协议，该协议可以在无法直接知道起始数据集的概率密度情况下，使用最大 LIKELIHOOD 估计来训练正则化流来变换一个数据集到另一个数据集。</li>
<li>results: 论文的实验结果表明，使用”流体流”协议可以成功地将一个数据集变换到另一个数据集，并且可以根据特定的条件来创建一个适应每个条件的变换函数。<details>
<summary>Abstract</summary>
Many components of data analysis in high energy physics and beyond require morphing one dataset into another. This is commonly solved via reweighting, but there are many advantages of preserving weights and shifting the data points instead. Normalizing flows are machine learning models with impressive precision on a variety of particle physics tasks. Naively, normalizing flows cannot be used for morphing because they require knowledge of the probability density of the starting dataset. In most cases in particle physics, we can generate more examples, but we do not know densities explicitly. We propose a protocol called flows for flows for training normalizing flows to morph one dataset into another even if the underlying probability density of neither dataset is known explicitly. This enables a morphing strategy trained with maximum likelihood estimation, a setup that has been shown to be highly effective in related tasks. We study variations on this protocol to explore how far the data points are moved to statistically match the two datasets. Furthermore, we show how to condition the learned flows on particular features in order to create a morphing function for every value of the conditioning feature. For illustration, we demonstrate flows for flows for toy examples as well as a collider physics example involving dijet events
</details>
<details>
<summary>摘要</summary>
很多高能物理数据分析中的组件需要将一个数据集转换为另一个。通常通过重新权重来解决这个问题，但是保留权重并将数据点Shift而不是重新权重有多个优点。通过流形模型可以在多种 particle physics 任务中获得非常高精度。然而，通常情况下，我们无法直接使用流形模型进行 morphing，因为它们需要知道开始数据集的概率密度。在大多数情况下，我们可以生成更多的例子，但我们不知道概率密度的确切值。我们提出了一种叫做“流形模型 для流形模型”的协议，用于训练流形模型，以便将一个数据集转换为另一个，即使 neither 数据集的概率密度不知道。这种方法可以通过最大化 likelihood estimation 来训练，这种方法在相关任务中已经被证明是非常有效的。我们还研究了这种协议的变化，以探索数据点是如何统计匹配两个数据集的。此外，我们还示出了如何通过特定的特征来Conditional 学习的流形函数，以创建一个每个特征值的 morphing 函数。为了示例，我们使用了一些 Toy 示例以及一个 collider physics 示例，涉及到 dijet 事件。
</details></li>
</ul>
<hr>
<h2 id="LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning"><a href="#LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning" class="headerlink" title="LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"></a>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06440">http://arxiv.org/abs/2309.06440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth Shaw, Ananye Agarwal, Deepak Pathak</li>
<li>For: This paper is written for researchers and developers working in the field of robotics, particularly those interested in dexterous manipulation and machine learning.* Methods: The paper presents a low-cost, dexterous, and anthropomorphic hand called LEAP Hand, which is designed for machine learning research. The hand has a novel kinematic structure that allows for maximal dexterity regardless of finger pose, and it can be assembled in 4 hours at a cost of 2000 USD from readily available parts.* Results: The paper shows that LEAP Hand can be used to perform several manipulation tasks in the real world, including visual teleoperation and learning from passive video data. The hand significantly outperforms its closest competitor Allegro Hand in all experiments while being 1&#x2F;8th of the cost.<details>
<summary>Abstract</summary>
Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed assembly instructions, the Sim2Real pipeline and a development platform with useful APIs on our website at https://leap-hand.github.io/
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>dexterous manipulation在 робо技术中已经是一个长期的挑战。虽然机器学习技术已经显示了一些 promise, 但结果主要是在simulation中得到的。这可以把lack of suitable hardware归结为主要原因。在这篇论文中，我们提出了LEAP Hand，一个低成本的dexterous和人工手臂 для机器学习研究。与之前的手臂不同，LEAP Hand具有一种新的骨骼结构，允许无论手指 pose都能够达到最大的dexterity。LEAP Hand是低成本的，可以在4个小时内为2000美元组装，使用可得到的部件。它可以在长时间内一直施加大力，并且可以在现实世界中完成多种抓取任务，从视觉Remote控制到学习从沉默视频数据和sim2real。LEAP Hand在我们所有实验中都能够在与Allegro Hand的比赛中表现出色，而且只需1/8的成本。我们在网站https://leap-hand.github.io/上发布了详细的组装指南，Sim2Real管道和开发平台，包括有用的API。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones"><a href="#Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones" class="headerlink" title="Unveiling the potential of large language models in generating semantic and cross-language clones"></a>Unveiling the potential of large language models in generating semantic and cross-language clones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06424">http://arxiv.org/abs/2309.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palash R. Roy, Ajmain I. Alam, Farouq Al-omari, Banani Roy, Chanchal K. Roy, Kevin A. Schneider</li>
<li>for: 本研究旨在探讨OpenAI的GPT模型在Semantic Clone Bench上的表现，以及该模型在代码生成、代码理解、重构和比较中的潜在应用。</li>
<li>methods: 本研究使用了SemanticCloneBench作为测试平台，并采用了多种代码片段和语言模型来评估GPT-3模型的表现。</li>
<li>results: GPT-3模型在生成semantic和跨语言代码变体方面表现出色，其精度达62.14%和0.55 BLEU分数，并在跨语言生成中达到91.25%的精度。<details>
<summary>Abstract</summary>
Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance. In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones
</details>
<details>
<summary>摘要</summary>
semantic和 Cross-language code clone生成可能有用于代码重用、代码理解、重构和benchmarking。OpenAI的GPT模型有潜力在这些clone生成方面，因为GPT用于文本生成。当开发者从Stack Overflow（SO）或系统中复制代码时，可能会出现不一致的更改，导致不预期的行为。 similarly，如果有一个代码片断在特定编程语言中，但寻找与其功能相同的代码在另一种语言中， semantic Cross-language code clone生成方法可以提供有价值的帮助。在本研究中，使用SemanticCloneBench作为载体，我们评估了GPT-3模型是否可以为给定的片断生成semantic和cross-language clone variant。我们组成了多样化的代码片断，并评估GPT-3模型在生成代码variant方面的表现。经过广泛的实验和分析，9名评审员在158个小时内验证了我们的结果，我们探讨了模型是否能够生成准确和semantically correct的variant。我们的发现 shed light on GPT-3模型在代码生成方面的能力，并提供了使用高级语言模型在软件开发中的潜在应用和挑战。我们的量化分析显示了惊人的结果。在semantic clones领域，GPT-3达到了62.14%的准确率和0.55 BLEU分数，通过少量的提示工程来实现。此外，模型在跨语言领域表现出色，达到了91.25%的准确率在生成cross-language clones。
</details></li>
</ul>
<hr>
<h2 id="On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions"><a href="#On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions" class="headerlink" title="On Computationally Efficient Learning of Exponential Family Distributions"></a>On Computationally Efficient Learning of Exponential Family Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06413">http://arxiv.org/abs/2309.06413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhin Shah, Devavrat Shah, Gregory W. Wornell</li>
<li>for: 学习 truncated  экспоненциаль家族模型的自然参数，使用 i.i.d. 样本，并且Computational efficiency和Statistical efficiency。</li>
<li>methods: 提出了一个新的损失函数和一种 computationally efficient 的估计器，该估计器是consistent 和 asymptotically normal 的，并且可以在轻量级的样本量下达到 $\alpha$ 误差水平。</li>
<li>results: 我们的方法可以视为最大 likelihood estimation 的一种特例，并且可以在 population 级别上看作是一种 re-parameterized distribution 的最大 likelihood estimation。我们还证明了我们的估计器可以看作是一种 minimizing 特定 Bregman Score 的解，以及一种 minimizing 代理 likelihood 的解。我们还提供了Finite sample guarantees，可以在 $\ell_2$ 范围内达到 $\alpha$ 误差水平的样本量为 $O({\sf poly}(k)&#x2F;\alpha^2)$。在特殊情况下，我们的方法可以 achieve order-optimal sample complexity $O({\sf log}(k)&#x2F;\alpha^2)$。<details>
<summary>Abstract</summary>
We consider the classical problem of learning, with arbitrary accuracy, the natural parameters of a $k$-parameter truncated \textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a novel loss function and a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. Further, we show that our estimator can be interpreted as a solution to minimizing a particular Bregman score as well as an instance of minimizing the \textit{surrogate} likelihood. We also provide finite sample guarantees to achieve an error (in $\ell_2$-norm) of $\alpha$ in the parameter estimation with sample complexity $O({\sf poly}(k)/\alpha^2)$. Our method achives the order-optimal sample complexity of $O({\sf log}(k)/\alpha^2)$ when tailored for node-wise-sparse Markov random fields. Finally, we demonstrate the performance of our estimator via numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们考虑了классический问题学习，即在具有任意准确性的情况下，计算和统计上efficient地学习含有k参数的简化的EXPFamily的自然参数。我们关注在支持和自然参数之间存在有限的约束下进行学习。尽管传统的最大化可能性估计器在这类EXPFamily中是一致的、 asymptotically normal 和 asymptotically efficient，但计算很复杂。在这种情况下，我们提出了一个新的损失函数和一种计算高效的估计器，该估计器在一定条件下是一致的和 asymptotically normal。我们还证明了，在人口水平上，我们的方法可以视为EXPFamily中的最大化可能性估计器。此外，我们还证明了我们的估计器可以视为一种Bregman分数的最小化解决方案以及一种surrogate likelihood的最小化解决方案。我们还提供了一些finite sample guarantees，可以在样本复杂度为$O({\sf poly}(k)/\alpha^2)$下达到参数估计中的错误（在$\ell_2$-norm）为 $\alpha$ 的目标。在特制化为节点离散Markov随机场景下，我们的方法可以 достичь顺序优化的样本复杂度$O({\sf log}(k)/\alpha^2)$。最后，我们通过数值实验证明了我们的估计器的性能。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Mask-Networks"><a href="#Ensemble-Mask-Networks" class="headerlink" title="Ensemble Mask Networks"></a>Ensemble Mask Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06382">http://arxiv.org/abs/2309.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lok-18/GeSeNet">https://github.com/lok-18/GeSeNet</a></li>
<li>paper_authors: Jonny Luntzel</li>
<li>for: 该研究是否可以使用 $\mathbb{R}^n\rightarrow \mathbb{R}^n$ Feedforward Network 来学习矩阵-向量乘法？</li>
<li>methods: 研究提出了两种机制：灵活的面masking 来处理矩阵输入，以及特有的网络剪辑来尊重面mask的依赖结构。</li>
<li>results: 研究表明，可以使用这些机制来近似固定操作，如矩阵-向量乘法 $\phi(A,x) \rightarrow Ax$，并应用于测试依赖关系或交互顺序在图模型中。<details>
<summary>Abstract</summary>
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
</details>
<details>
<summary>摘要</summary>
可以不是$\mathbb{R}^n\to \mathbb{R}^n$的Feedforward网络学习矩阵-向量乘法吗？这个研究提出了两种机制：灵活的面 masking来接受矩阵输入，以及专门针对面 mask的网络剪辑来尊重面 mask的依赖结构。网络可以近似固定操作，如矩阵-向量乘法 $\phi(A,x) \to Ax$，激励我们提出的机制和应用于图模型中的依赖测试或交互顺序。
</details></li>
</ul>
<hr>
<h2 id="InstaFlow-One-Step-is-Enough-for-High-Quality-Diffusion-Based-Text-to-Image-Generation"><a href="#InstaFlow-One-Step-is-Enough-for-High-Quality-Diffusion-Based-Text-to-Image-Generation" class="headerlink" title="InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation"></a>InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06380">http://arxiv.org/abs/2309.06380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gnobitab/instaflow">https://github.com/gnobitab/instaflow</a></li>
<li>paper_authors: Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu</li>
<li>for: 这篇论文是为了提出一种新的一步 diffusion-based text-to-image generator，使得 diffusion 模型可以更快速地生成高质量图像。</li>
<li>methods: 该论文使用了一种新的Rectified Flow方法，该方法可以更好地考虑图像和噪声之间的关系，从而提高 diffusion 模型的 sampling 速度和图像质量。</li>
<li>results: 该论文通过使用 Rectified Flow 方法和一种新的 text-conditioned pipeline，成功地将 diffusion 模型转化为一种快速的一步模型，并实现了在 MS COCO 2017-5k 上的 FID 值为 $23.3$，比之前的最佳技术进行了显著的提升 ($37.2$ $\rightarrow$ $23.3$ 在 FID 上)。此外，该论文还提出了一种基于 expanded network 的一步模型，可以进一步提高 FID 值。<details>
<summary>Abstract</summary>
Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37.2$ $\rightarrow$ $23.3$ in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to $22.4$. We call our one-step models \emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow yields an FID of $13.1$ in just $0.09$ second, the best in $\leq 0.1$ second regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Project page:~\url{https://github.com/gnobitab/InstaFlow}.
</details>
<details>
<summary>摘要</summary>
Diffusion模型已经革命化了文本到图像生成，其品质和创造力都非常出色。然而，它的多步采样过程相对较慢，通常需要数十个推理步骤以获得满意的结果。先前的尝试使用热化法提高采样速度和降低计算成本通过热化法，但是没有实现一步模型。在这篇论文中，我们探讨一种名为Rectified Flow的新方法，它只在小数据集上使用过。Rectified Flow的核心在于它的“重定向”过程，它折叠概率流的轨迹，细化图像和噪声之间的协同关系，并且通过学生模型进行热化。我们提出了一种基于Stable Diffusion（SD）的文本条件管道，将Stable Diffusion转化为超快一步模型。我们发现，在这种管道中，重定向扮演了关键的作用，使图像和噪声之间的分配得到了改善。我们的一步模型命名为InstaFlow。在 MS COCO 2017-5k 上，InstaFlow 的 FID 为 $23.3$，超过了之前的最佳技术进行热化的进步 ($37.2$ $\rightarrow$ $23.3$ 的 FID)。通过使用扩展的网络和 1.7B 参数，我们进一步提高了 FID 到 $22.4$。我们的一步模型在 MS COCO 2014-30k 上的 FID 为 $13.1$，在 $0.09$ 秒钟内完成，超过了最近的 StyleGAN-T ($13.9$ 在 $0.1$ 秒钟内完成)。值得注意的是，我们在训练 InstaFlow 时只需要 199 A100 GPU 天。更多细节请参考我们的项目页面：https://github.com/gnobitab/InstaFlow。
</details></li>
</ul>
<hr>
<h2 id="Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery"><a href="#Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery" class="headerlink" title="Using Reed-Muller Codes for Classification with Rejection and Recovery"></a>Using Reed-Muller Codes for Classification with Rejection and Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06359">http://arxiv.org/abs/2309.06359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfenth/rmaggnet">https://github.com/dfenth/rmaggnet</a></li>
<li>paper_authors: Daniel Fentham, David Parker, Mark Ryan</li>
<li>for: 防止攻击者透过对于训练数据集的类别器进行适应的攻击，使其给出错误的输出。</li>
<li>methods: 使用Reed-Muller 错误修复码以实现类别-弃置方法，可以正确地拒绝类别器对于训练数据集之外的输入。</li>
<li>results: 可以在多种不同的类别攻击下，以低 incorrectness 维持良好的正确率，并且可以降低类别器对于输入资料进行额外处理的量。<details>
<summary>Abstract</summary>
When deploying classifiers in the real world, users expect them to respond to inputs appropriately. However, traditional classifiers are not equipped to handle inputs which lie far from the distribution they were trained on. Malicious actors can exploit this defect by making adversarial perturbations designed to cause the classifier to give an incorrect output. Classification-with-rejection methods attempt to solve this problem by allowing networks to refuse to classify an input in which they have low confidence. This works well for strongly adversarial examples, but also leads to the rejection of weakly perturbed images, which intuitively could be correctly classified. To address these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), a classifier inspired by Reed-Muller error-correction codes which can correct and reject inputs. This paper shows that RMAggNet can minimise incorrectness while maintaining good correctness over multiple adversarial attacks at different perturbation budgets by leveraging the ability to correct errors in the classification process. This provides an alternative classification-with-rejection method which can reduce the amount of additional processing in situations where a small number of incorrect classifications are permissible.
</details>
<details>
<summary>摘要</summary>
（在实际应用中，用户们对分类器的输入应答预期正确。然而，传统的分类器不能处理与训练数据集远离的输入。恶意者可以通过制作针对分类器输出错误的攻击来利用这个漏洞。分类-拒绝方法试图解决这个问题，allowing networks to refuse to classify an input in which they have low confidence。这些方法在强有攻击例子上工作良好，但也会拒绝弱有攻击的图像，这些图像可能可以正确地分类。为解决这些问题，我们提出了Reed-Muller Aggregation Networks（RMAggNet），一种基于Reed-Muller错误修复码的分类器，可以对输入进行修复和拒绝。这个论文显示，RMAggNet可以最小化错误的同时保持多种攻击下的良好正确率，通过在分类过程中利用错误修复的能力。这提供了一种更加可靠的分类-拒绝方法，可以在有限的额外处理下减少错误数量。）
</details></li>
</ul>
<hr>
<h2 id="Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors"><a href="#Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors" class="headerlink" title="Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors"></a>Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06349">http://arxiv.org/abs/2309.06349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Jaiswal, Debdeep Pati, Anirban Bhattacharya, Bani K. Mallick</li>
<li>For: The paper proposes a new variant of Thompson Sampling (TS) algorithm, called $\alpha$-TS, which uses a fractional posterior distribution to improve the performance of TS in stochastic multi-armed bandit problems.* Methods: The paper uses a novel regret analysis technique that combines recent theoretical developments in non-asymptotic concentration analysis and Bernstein-von Mises type results to derive instance-dependent and instance-independent regret bounds for $\alpha$-TS.* Results: The paper obtains both instance-dependent and instance-independent regret bounds for $\alpha$-TS, which are of the order $\mathcal{O}(\sum_{k \neq i^*} \Delta_k(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2}))$ and $\mathcal{O}(\sqrt{KT\log K})$, respectively, under very mild conditions on the prior and reward distributions. The results show that $\alpha$-TS achieves a better performance than the standard TS algorithm.<details>
<summary>Abstract</summary>
Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\alpha$-TS, where we use a fractional or $\alpha$-posterior ($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute an $\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior distribution just require its density to be positive, continuous, and bounded. We also establish another instance-dependent regret upper bound that matches (up to constants) to that of improved UCB [Auer and Ortner, 2010]. Our regret analysis carefully combines recent theoretical developments in the non-asymptotic concentration analysis and Bernstein-von Mises type results for the $\alpha$-posterior distribution. Moreover, our analysis does not require additional structural properties such as closed-form posteriors or conjugate priors.
</details>
<details>
<summary>摘要</summary>
Thompson 抽取（TS）是多臂抽奖问题中最受欢迎并且最早的算法之一。我们考虑了一种变体TS，称为α-TS，其中我们使用一个分数或α- posterior（α belongs to (0,1)）而不是标准 posterior distribution。为计算α-posterior，抽奖likelihood在标准 posterior定义中被温和了一个因子α。对于α-TS，我们得到了两种不同的频见 regret bounds：一种是 $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$，另一种是 $\mathcal{O}(\sqrt{KT\log K})$，它们都是在非常轻微的假设下得到的，其中 $\Delta_k$ 是真实奖劵的 gap，$C(\alpha)$ 是已知的常量。两者都满足我们的奖劵分布的总体条件。我们的假设只需要其density是正态，连续和受限。我们还证明了另一种相当于改进UCb的频见 regret upper bound，它与（在常量上）相同。我们的 regret分析结合了非对称峰度分析的最新成果和 Bernstein-von Mises 类型的结果。此外，我们的分析不需要额外的结构性质，例如封闭的 posterior 或 conjugate prior。
</details></li>
</ul>
<hr>
<h2 id="Band-gap-regression-with-architecture-optimized-message-passing-neural-networks"><a href="#Band-gap-regression-with-architecture-optimized-message-passing-neural-networks" class="headerlink" title="Band-gap regression with architecture-optimized message-passing neural networks"></a>Band-gap regression with architecture-optimized message-passing neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06348">http://arxiv.org/abs/2309.06348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Bechtel, Daniel T. Speckhard, Jonathan Godwin, Claudia Draxl</li>
<li>for: 本研究使用Graph-based neural networks和Message-passing neural networks（MPNNs）预测物质的物理性质。</li>
<li>methods: 研究使用density functional theory数据从AFLOW数据库进行分类物质为半导体&#x2F;绝缘体或金属。然后进行神经建模搜索，探索MPNNs模型结构和 гипер参数空间，预测非金属材料的带隙。搜索参数包括消息传递步数、隐藏大小和活动函数等。</li>
<li>results: 搜索得到的最佳模型组成一个ensemble，与现有文献模型相比显著提高了预测性能。不确定性评估使用Monte-Carlo Dropout和拟合，ensemble方法胜利。研究分析了ensemble模型的适用范围，包括晶系、包括Hubbard参数在内的density functional计算、物质组成元素等。<details>
<summary>Abstract</summary>
Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the inclusion of a Hubbard parameter in the density functional calculations, and the atomic species building up the materials.
</details>
<details>
<summary>摘要</summary>
基于图的神经网络和特别是消息传递神经网络（MPNN）在预测固体物理性质方面表现出了很大的潜力。在这项工作中，我们使用MPNN对AFLOW数据库中的密度函数理论数据进行分类，将材料分为金属和半导体/离子体两类。然后，我们进行神经建筑搜索，探索MPNN的模型结构和超参空间，以预测非金属材料的带隙。搜索的参数包括消息传递步数、隐藏大小和活动函数等。最佳性能的模型从搜索中选拔，组成了一个ensemble，该ensemble在比较之前 существу的模型中表现出了显著的优异。uncertainty量化通过蒙特卡罗Dropout和拟合来评估，ensemble方法更为超越。预测模型的适用范围通过晶系、包括Hubbard参数在内的密度函数计算以及物质的原子组成来分析。
</details></li>
</ul>
<hr>
<h2 id="Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning"><a href="#Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning" class="headerlink" title="Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning"></a>Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06315">http://arxiv.org/abs/2309.06315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole-Christoffer Granmo, Per-Arne Andersen, Lei Jiao, Xuan Zhang, Christian Blakely, Tor Tveit</li>
<li>for: 这个论文的目的是为了提出一种新的Tsetlin机器（TM）反馈方案，该方案可以补充类型I和类型II反馈，并通过一种新的Finite State Automaton（自适应独立自动机）来学习目标变量的Markov边界。</li>
<li>methods: 该论文使用了一种新的Context-Specific Independence Automaton（自适应独立自动机）来学习目标变量的Markov边界，并通过TM反馈方案来推断目标变量。</li>
<li>results: 该论文的实验结果表明，新的TM反馈方案可以借助自适应独立自动机来找到Markov边界，并且可以减少不必要的变量，从而提高模型的准确率和效率。<details>
<summary>Abstract</summary>
A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific Independence Automaton. The automaton learns which features are outside the Markov boundary of the target, allowing them to be pruned from the TM during learning. We investigate the new scheme empirically, showing how it is capable of exploiting context-specific independence to find Markov boundaries. Further, we provide a theoretical analysis of convergence. Our approach thus connects the field of Bayesian networks (BN) with TMs, potentially opening up for synergies when it comes to inference and learning, including TM-produced Bayesian knowledge bases and TM-based Bayesian inference.
</details>
<details>
<summary>摘要</summary>
一个集合的变量是一个Markov幕墙（Markov blanket），如果它包含一个变量的所有预测信息，那么它就是一个Markov边界（Markov boundary）。如果幕墙不能简化而失去有用信息，那么它就是一个Markov边界。确定一个变量的Markov边界有利，因为所有外部变量都是 redundant。因此，Markov边界提供了一个优化的特征集。然而，从数据中学习Markov边界是具有挑战性的，因为如果从Markov边界中移除一个变量，那么外部变量可能会提供信息，而变量在边界中可能会停止提供信息。每个候选变量的真实角色只有在Markov边界已经确定后才能manifest。在这篇论文中，我们提出了一种新的Tsetlin机器（TM）反馈方案，该方案在Type I和Type II反馈的基础之上补充了一种新的Finite State Automaton（Context-Specific Independence Automaton）。这个自动机学习Markov边界中的变量是否外部提供信息，以便在TM学习过程中从Markov边界中除去这些变量。我们通过实验证明了这种新方案的可行性，并证明了其可以利用上下文特定的独立性来找到Markov边界。此外，我们还提供了一种理论分析的结论，证明我们的方法可以 converges。我们的方法因此将Bayesian网络（BN）和TM相连接，可能开启了在推理和学习方面的 synergies，包括TM生成的Bayesian知识库和TM基于Bayesian推理。
</details></li>
</ul>
<hr>
<h2 id="Semantic-and-Articulated-Pedestrian-Sensing-Onboard-a-Moving-Vehicle"><a href="#Semantic-and-Articulated-Pedestrian-Sensing-Onboard-a-Moving-Vehicle" class="headerlink" title="Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle"></a>Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06313">http://arxiv.org/abs/2309.06313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Priisalu</li>
<li>for: 本研究旨在提高交通安全，通过利用LiDAR探测器直接测量人员动作和距离来提高人员检测和预测。</li>
<li>methods: 本研究使用LiDAR探测器获取数据，并应用图像处理技术来实现人员检测和预测。</li>
<li>results: 本研究显示，通过使用LiDAR探测器和图像处理技术，可以准确地检测和预测人员动作，提高交通安全。<details>
<summary>Abstract</summary>
It is difficult to perform 3D reconstruction from on-vehicle gathered video due to the large forward motion of the vehicle. Even object detection and human sensing models perform significantly worse on onboard videos when compared to standard benchmarks because objects often appear far away from the camera compared to the standard object detection benchmarks, image quality is often decreased by motion blur and occlusions occur often. This has led to the popularisation of traffic data-specific benchmarks. Recently Light Detection And Ranging (LiDAR) sensors have become popular to directly estimate depths without the need to perform 3D reconstructions. However, LiDAR-based methods still lack in articulated human detection at a distance when compared to image-based methods. We hypothesize that benchmarks targeted at articulated human sensing from LiDAR data could bring about increased research in human sensing and prediction in traffic and could lead to improved traffic safety for pedestrians.
</details>
<details>
<summary>摘要</summary>
很难从车辆上捕捉的视频进行3D重建，因为车辆前进速度很快。even object detection和人体感知模型在车辆上视频中表现比标准 referential benchmarks 更差，因为物体在摄像头中的距离比标准 referential benchmarks 更远，图像质量通常受到运动模糊和遮挡的影响。这导致了交通数据特有的benchmarks的普及。recently, Light Detection And Ranging (LiDAR) 感知器已经成为了直接测量深度的方法。然而，LiDAR基本方法仍然缺乏远程人体检测的弯曲性，相比于图像基本方法。我们提出了人体检测和预测在交通中的benchmarks可能会促进人体检测和预测的研究，从而提高了步行人员的交通安全。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Supply-and-Demand-in-Public-Transportation-Systems"><a href="#Modeling-Supply-and-Demand-in-Public-Transportation-Systems" class="headerlink" title="Modeling Supply and Demand in Public Transportation Systems"></a>Modeling Supply and Demand in Public Transportation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06299">http://arxiv.org/abs/2309.06299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Miranda Bihler, Hala Nelson, Erin Okey, Noe Reyes Rivas, John Webb, Anna White</li>
<li>for: 帮助海arrisonburg公共交通部门（HDPT）利用数据提高运营效率和效果。</li>
<li>methods: 我们构建了两个供应和需求模型，帮助部门识别服务中的缺陷。这些模型考虑了许多变量，包括HDPT向联邦政府报告的方式和海arrisonburg市最为易受影响的区域。我们使用数据分析和机器学习技术进行预测。</li>
<li>results: 我们的预测可以帮助HDPT更好地了解其服务的需求和供应，从而提高运营效率和效果。<details>
<summary>Abstract</summary>
The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
</details>
<details>
<summary>摘要</summary>
哈里逊堡公共交通部门（HDPT）想要通过数据来提高其运营效率和效果。我们构建了两个供应和需求模型，帮助部门确定其服务中的缺陷。这些模型考虑了许多变量，包括哈里逊堡市政府报告给联邦政府的方式以及城市中最为投降的地区。我们使用数据分析和机器学习技术进行预测。
</details></li>
</ul>
<hr>
<h2 id="Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition"><a href="#Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition" class="headerlink" title="Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition"></a>Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06286">http://arxiv.org/abs/2309.06286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutahar Safdar, Jiarui Xie, Hyunwoong Ko, Yan Lu, Guy Lamouche, Yaoyao Fiona Zhao</li>
<li>for: 这个研究旨在提高数据驱动的Additive Manufacturing（AM）研究的效率和可重用性，通过将AM和人工智能（AI）两个context中的知识融合并形式化。</li>
<li>methods: 该研究提出了一个三步知识传递可能性分析框架，包括预传递、传递和后传递步骤，以支持数据驱动的AM知识传递。在这个框架中，AM知识被抽象为特定的知识组件，并进行了对 flagship metal AM process之间的比较。</li>
<li>results: 研究发现，可以成功地将LPBF（激光粉末床印刷）中的数据驱动解决方案转移到DED（指定能量激光处理）中，包括数据表示、模型架构和模型参数。这种数据驱动的AM知识传递可以在未来通过自动化pipeline进行效率地跨context或跨过程传递。<details>
<summary>Abstract</summary>
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The framework consists of pre-transfer, transfer, and post-transfer steps to accomplish knowledge transfer. A case study is conducted between flagship metal AM processes. Laser Powder Bed Fusion (LPBF) is the source of knowledge motivated by its relative matureness in applying AI over Directed Energy Deposition (DED), which drives the need for knowledge transfer as the less explored target process. We show successful transfer at different levels of the data-driven solution, including data representation, model architecture, and model parameters. The pipeline of AM knowledge transfer can be automated in the future to allow efficient cross-context or cross-process knowledge exchange.
</details>
<details>
<summary>摘要</summary>
“数据驱动的研究在加法制造（AM）领域在最近几年内取得了显著的成功。这导致了大量的科学文献的出现。这些文献中的知识包括AM和人工智能（AI）上下文的知识，它们尚未被完整地整合和形式化。此外，没有工具或指南来支持数据驱动知识的交叉Context Transfer。因此，为了解决特定AM过程技术的问题，开发和验证了特定AI技术的数据驱动解决方案。我们提出了一种三步知识可传递性分析框架，用于支持数据驱动AM知识的交叉Context Transfer。在这个框架中，AM知识被Feature化成标识的知识组件。框架包括前传、传输和后传步骤，以完成知识交叉Context Transfer。我们通过将LPBF（激光粉末床干）作为知识源，对DPD（导向热处理）进行了知识传输，并成功实现了数据表示、模型架构和模型参数之间的交叉Context Transfer。未来，可以自动化AM知识交叉Context Transfer的管道，以便有效地进行Context Cross-Process知识交换。”
</details></li>
</ul>
<hr>
<h2 id="ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method"><a href="#ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method" class="headerlink" title="ELRA: Exponential learning rate adaption gradient descent optimization method"></a>ELRA: Exponential learning rate adaption gradient descent optimization method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06274">http://arxiv.org/abs/2309.06274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kleinsorge, Stefan Kupper, Alexander Fauck, Felix Rothe</li>
<li>For: The paper presents a new, fast, and universal gradient-based optimizer algorithm called Exponential Learning Rate Adaptation (ELRA) that adapts the learning rate by situational awareness and does not rely on hand-tuned parameters.* Methods: The method uses a novel approach to adapt the learning rate based on the orthogonality of neighboring gradients, which leads to a high success rate and fast convergence. The method is also rotation invariant and applies to problems of any dimension and scale.* Results: The paper demonstrates the impressive performance of ELRA through extensive experiments on the MNIST benchmark dataset against state-of-the-art optimizers. The authors also present two variants of the algorithm, c2min and p2min, with slightly different control.<details>
<summary>Abstract</summary>
We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential Learning Rate Adaption - ELRA. We present it in two variants c2min and p2min with slightly different control. The authors strongly believe that ELRA will open a completely new research direction for gradient descent optimize.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的、快速（加速率适应）、无参数（无参数）的梯度基本算法。该方法的主要想法是根据情况意识来适应学习率α，主要寻求垂直邻域梯度的平行性。该方法具有高成功率和快速收敛率，不需要手动调整参数，因此具有更广泛的通用性。它可以应用于任意维度n和问题的缩放问题，并且只linearly（对数(n)）随问题维度增长。它可以优化凸和非凸连续景观，只要提供一定的梯度。与Ada家族（AdaGrad、AdaMax、AdaDelta、Adam等）不同，该方法是坐标选择不依赖的：优化路径和性能独立于坐标选择。我们在MNIST数据集上进行了广泛的实验，证明了该新类optimizers的出色性，我们命名它为泛化学习率适应（ELRA）。我们在c2min和p2min两种不同的控制下提出了两种变体。作者们认为，ELRA将打开一个 Completely new的研究方向，为梯度下降优化带来很大的发展。
</details></li>
</ul>
<hr>
<h2 id="ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation"><a href="#ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation" class="headerlink" title="ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation"></a>ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06268">http://arxiv.org/abs/2309.06268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Snigdha Sen, Saurabh Singh, Hayley Pye, Caroline Moore, Hayley Whitaker, Shonit Punwani, David Atkinson, Eleftheria Panagiotaki, Paddy J. Slator<br>for: 这个研究旨在利用MRI技术诊断前列腺癌（PCa），并且使用Diffusion MRI（dMRI）来估计细胞大小等微struc特征。methods: 这个研究使用了一种名为VERDICT（Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours）的三个分类 compartment biophysical模型，并使用自动化学习（self-supervised learning）来学习这个模型的参数。results: 研究发现，使用自动化学习方法可以实现不需要明确训练标签的模型适配，并且比起传统的非线性最小二乘（NLLS）和专案化神经网络（DNNs）方法，这种方法可以提高估计精度和减少偏差。此外，这个方法还可以实现高度的自信度水平，用于精确诊断前列腺癌和正常组织之间的区别。<details>
<summary>Abstract</summary>
MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for the first time, fitting of a complex three-compartment biophysical model with machine learning without the requirement of explicit training labels. We compare the estimation performance to baseline NLLS and supervised DNN methods, observing improvement in estimation accuracy and reduction in bias with respect to ground truth values. Our approach also achieves a higher confidence level for discrimination between cancerous and benign prostate tissue in comparison to the other methods on a dataset of 20 PCa patients, indicating potential for accurate tumour characterisation.
</details>
<details>
<summary>摘要</summary>
MRI 在诊断 prostates cancer (PCa) 中变得越来越普遍，diffusion MRI (dMRI) 扮演着重要的角色。当与计算模型结合时，dMRI 可以估算微结构信息，如细胞大小。传统上，这些模型通常使用非线性最小二乘 (NLLS) 曲线函数拟合方法，具有高计算成本。深度神经网络 (DNNs) 是一种有效的替代方案，但它们的性能受训练数据的下面分布的影响很大。无监督学习是一个吸引人的选择，它可以让网络学习输入数据的特征自身，而不需要单独的训练数据集。在这种情况下，我们介绍了一种无监督 DNN，用于估算VERDICT（血管、 extracellular 和限制的扩散 для细胞学）模型的参数。我们在20名PCa患者的数据集上示示出，我们的方法可以在没有明确训练标签的情况下，高度准确地估算模型参数，并且与基准值之间的偏差更小。此外，我们的方法还可以在区分患有PCa和正常组织时达到更高的信任度，表明它具有潜在的精准肿瘤特征化能力。
</details></li>
</ul>
<hr>
<h2 id="Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning"><a href="#Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning" class="headerlink" title="Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning"></a>Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06260">http://arxiv.org/abs/2309.06260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Beck, Marius Kurz</li>
<li>For: 该研究旨在开发一种基于Reinforcement Learning（RL）的精度适应闭合模型，以解决在含有滤波器的大气流 simulate（LES）中的隐式滤波器问题。* Methods: 该研究使用了Markov决策过程（MDP）和RL来优化LES闭合模型的精度，并应用到了explicit和implicit的闭合模型中。在explicit模型中，RL被应用来优化一个元素本地的液化粘度模型。在implicit模型中，RL被用来优化一种hybrid的Discontinuous Galerkin（DG）和finite volume方法的混合策略。* Results: 该研究发现，使用RL优化的闭合模型可以提供精度适应的闭合，并且能够减少LES中的滤波器问题引起的不确定性。另外，explicit模型在不同的精度和分辨率下都能够达到或超过现有的STATE-OF-THE-ART模型的准确性。同时，implicit模型在hybrid scheme中显示出了一定的可能性，可能成为一种新的高级别模型。<details>
<summary>Abstract</summary>
We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous Galerkin (DG) and finite volume scheme. All newly derived models achieve accurate and consistent results, either matching or outperforming classical state-of-the-art models for different discretizations and resolutions. Moreover, the explicit model is demonstrated to adapt its distribution of viscosity within the DG elements to the inhomogeneous discretization properties of the operator. In the implicit case, the optimized hybrid scheme renders itself as a viable modeling ansatz that could initiate a new class of high order schemes for compressible turbulence. Overall, the results demonstrate that the proposed RL optimization can provide discretization-consistent closures that could reduce the uncertainty in implicitly filtered LES.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于开发适应离散级别的关闭方法，用于不可见滤波的大气动学模拟（LES）。在不可见滤波LES中，涉及到网格和离散算法的filter核心，以及关闭项，都是由网格和离散算法的性质决定的，这会导致额外的计算 подgrid项，这些项通常在先前分析中不可知道。因此，我们将 adapting  coefficients of LES closure models  reformulated as a Markov decision process, and solved in an a posteriori manner with Reinforcement Learning (RL)。这 allows to adjust the model to the actual discretization, and also incorporates the interaction between the discretization and the model itself。这个优化框架应用于 both explicit and implicit closure models。在这个框架中，我们使用了一种元素本地的质量系数模型作为explicit模型。对于implicit模型，我们使用RL来确定一个最佳的混合策略，用于 hybrid离散格（DG）和质量量算法。所有新 derivation的模型都达到了准确和一致的结果， either matching or outperforming classical state-of-the-art models for different discretizations and resolutions。此外，explicit模型被示出可以在 DG 元素中适应不同的离散属性。在implicit caso，优化的混合方案被证明是一种可能的高级模型 ansatz，可以开启一种新的高级方法 для压缩性液体动力学。总之，结果表明，我们提出的RL优化可以提供适应离散级别的关闭，以减少不可见滤波LES中的uncertainty。
</details></li>
</ul>
<hr>
<h2 id="Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models"><a href="#Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models" class="headerlink" title="Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models"></a>Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06256">http://arxiv.org/abs/2309.06256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, Tong Zhang</li>
<li>for: 这篇研究探讨了基础模型（包括 Computer Vision Language Models（VLMs）和 Large Language Models（LLMs））在不同分布和任务上的应用，以及如何在这些基础模型上进行微调以提高任务性能或调整模型的行为，以使其获得特化。</li>
<li>methods: 这篇研究使用了多种常规化方法来处理基础模型的特化和通用性之间的贸易，包括 continual learning、Wise-FT 方法和 Low-Rank Adaptation（LoRA）方法。</li>
<li>results: 研究结果显示，使用 continual learning 和 Wise-FT 方法可以有效地抵消特化和通用性之间的贸易，并且 Wise-FT 方法在将特化和通用性平衡的情况下表现最佳。<details>
<summary>Abstract</summary>
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions and common sense.   To address the trade-off between the speciality and generality, we investigate multiple regularization methods from continual learning, the weight averaging method (Wise-FT) from out-of-distributional (OOD) generalization, which interpolates parameters between pre-trained and fine-tuned models, and parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our findings show that both continual learning and Wise-ft methods effectively mitigate the loss of generality, with Wise-FT exhibiting the strongest performance in balancing speciality and generality.
</details>
<details>
<summary>摘要</summary>
基础模型，包括视觉语言模型（VLM）和大型语言模型（LLM），具有涵盖多种分布和任务的通用性，这是由于它们在预训练 dataset 的广泛采集得到的。然而，在精度调整过程中，使用小型 dataset 可能无法完全覆盖在预训练中遇到的多种分布和任务。因此，在精度调整过程中寻求特点可能会导致模型失去通用性，这与深度学习中的恐慌忘记（CF）有关。在这种情况下，我们在 VLM 和 LLM 中进行了实验，发现精度调整后模型对多种分布的处理能力减退，以及模型无法遵循指令和常识。为了解决特点和通用性之间的负担，我们研究了多种 kontinual learning 策略，包括 weight averaging method（Wise-FT）和 parameter-efficient fine-tuning 方法如 Low-Rank Adaptation（LoRA）。我们的发现表明， kontinual learning 和 Wise-FT 方法都能有效地减轻失去通用性的问题，其中 Wise-FT 方法在均衡特点和通用性方面表现最佳。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation"><a href="#Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation" class="headerlink" title="Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation"></a>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06255">http://arxiv.org/abs/2309.06255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu</li>
<li>for: 这篇论文旨在jointly incorporating heterogeneous information from different modalities, but most models often suffer from unsatisfactory multi-modal cooperation.</li>
<li>methods: 我们提出了一种 fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level, and further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities.</li>
<li>results: 我们的方法可以reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.<details>
<summary>Abstract</summary>
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities in a targeted manner. Overall, our methods reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.
</details>
<details>
<summary>摘要</summary>
一个主要的多Modal学习话题是将多种不同模式的信息合并 incorporate 到一起。然而，大多数模型经常受到不满足的多模式协作的影响，无法合理地利用所有模式。一些方法可以识别并提高不良学习的模式，但是往往没有可靠的理论支持，并且具有精细化的样本级别的多模式协作观察能力。因此，我们需要合理地观察和改进多模式之间的细化协作，特别是在面临实际场景中，模式差异可能会随着不同的样本而变化。为此，我们引入了细化的模式价值指标，以评估每个模式在样本级别的贡献。通过模式价值评估，我们发现多模式模型往往依赖于一个特定的模式，导致其他模式成为低贡献的。我们进一步分析这一问题，并在targeted 的方式提高低贡献的模式的探测能力，以提高多模式模型的性能。总之，我们的方法可以合理地观察样本级别的细化单模式贡献，并实现了不同多模式模型的显著改进。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data"><a href="#Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data" class="headerlink" title="Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data"></a>Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06248">http://arxiv.org/abs/2309.06248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Euihyeon Choi, Jooyoung Kim, Wonkyung Lee</li>
<li>for: 这种研究的目的是为了评估电子竞技中赢球概率估计模型的可靠性，并提出一种新的评估指标来取代准确率。</li>
<li>methods: 这种研究使用了布里尔分数和预期准确错误（ECE）来评估赢球概率估计模型的性能，并提出了一种新的评估指标called Balance score。</li>
<li>results: 研究发现，Balance score具有六个好的属性，并且在普遍情况下可以效果地 aproximate true expected calibration error。此外，通过实验研究和真实游戏快照数据，证明了该指标的批处性和可靠性。<details>
<summary>Abstract</summary>
Probability estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. Among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. The win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. However, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. In this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. Based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good properties that probability estimation metric should have. Under the general condition, we also found that the Balance score can be an effective approximation of the true expected calibration error which has been imperfectly approximated by ECE using the binning technique. Extensive evaluations using simulation studies and real game snapshot data demonstrate the promising potential to adopt the proposed metric not only for the win probability estimation model for esports but also for evaluating general probability estimation models.
</details>
<details>
<summary>摘要</summary>
概率估计模型在各个领域中扮演着重要的角色，如天气预测、推荐系统和体育分析。然而，评估这些模型提供的可靠性很困难，因为真实的概率不可获得。电竞赛事中的赢场概率估计模型是活跃的研究领域之一。然而，大多数前一些工作使用准确率作为评估metric，这只能衡量推理的性能。在这种情况下，我们首次研究了布里尔分数和预期准确性错误（ECE）作为评估win probability estimation模型的metric。基于分析，我们提出了一个新的metriccalled Balance score，它具有六个好的性能特点。在总的来说，我们发现Balance score可以作为true expected calibration error的有效近似，而ECE使用分组技术的近似结果不够 precisel。通过 simulated studies和实际游戏快照数据，我们展示了提议的metric在不只是win probability estimation模型，还可以用于评估总体概率估计模型的承诺潜力。
</details></li>
</ul>
<hr>
<h2 id="Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks"><a href="#Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks" class="headerlink" title="Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks"></a>Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06240">http://arxiv.org/abs/2309.06240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Pernot<br>for:This paper focuses on reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks, specifically in materials and chemical science.methods:The paper proposes and illustrates adapted validation methods that assess both consistency and adaptivity of ML-UQ methods. These methods include so-called reliability diagrams, as well as adaptive validation techniques that evaluate the reliability of predictions and uncertainties for any point in feature space.results:The paper shows that consistency and adaptivity are complementary validation targets, and that a good consistency does not necessarily imply a good adaptivity. The proposed adapted validation methods are illustrated on a representative example, demonstrating their effectiveness in evaluating the reliability of ML-UQ predictions.<details>
<summary>Abstract</summary>
Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
</details>
<details>
<summary>摘要</summary>
可靠的不确定量评估（UQ）在机器学习（ML）回归任务中成为了许多研究的关注点，特别是在材料和化学科学领域。现在已经广泛认可，平均调整不够，大多数研究通过测试条件调整和不确定性之间的一致性来评估Consistency。一致性通常通过所谓的可靠性图表进行评估。然而，有另一种超出平均调整的方法，即基于输入特征的conditional calibration，即适应性。在实践中，适应性是最终用户的ML-UQ方法的可靠性和不确定性预测和任何点特征空间的关键问题。本文目的是表明一致性和适应性是补充的验证目标，并且一个好的一致性不一定意味着一个好的适应性。适应验证方法被提出并在一个代表性的例子中 illustrate。
</details></li>
</ul>
<hr>
<h2 id="Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory"><a href="#Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory" class="headerlink" title="Risk-Aware Reinforcement Learning through Optimal Transport Theory"></a>Risk-Aware Reinforcement Learning through Optimal Transport Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06239">http://arxiv.org/abs/2309.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>For: 这篇论文旨在探讨如何在动态和不确定环境中使用奖励学习（RL），同时考虑风险管理。* Methods: 该论文提出了结合优化运输（OT）理论和RL的风险感知框架，以便在RL目标函数中考虑风险因素。* Results: 该论文提供了一系列定理，证明了风险分布、优化值函数和策略行为之间的关系。通过OT的数学精度，该论文提供了一种权衡奖励和风险考虑的RL框架。<details>
<summary>Abstract</summary>
In the dynamic and uncertain environments where reinforcement learning (RL) operates, risk management becomes a crucial factor in ensuring reliable decision-making. Traditional RL approaches, while effective in reward optimization, often overlook the landscape of potential risks. In response, this paper pioneers the integration of Optimal Transport (OT) theory with RL to create a risk-aware framework. Our approach modifies the objective function, ensuring that the resulting policy not only maximizes expected rewards but also respects risk constraints dictated by OT distances between state visitation distributions and the desired risk profiles. By leveraging the mathematical precision of OT, we offer a formulation that elevates risk considerations alongside conventional RL objectives. Our contributions are substantiated with a series of theorems, mapping the relationships between risk distributions, optimal value functions, and policy behaviors. Through the lens of OT, this work illuminates a promising direction for RL, ensuring a balanced fusion of reward pursuit and risk awareness.
</details>
<details>
<summary>摘要</summary>
在RL环境中，决策是不确定和动态的，风险管理就成为决策的关键因素。传统RL方法虽然能够优化奖励，但frequently ignore潜在的风险风险。为了解决这问题，这篇论文提出了结合RL和优化运输（OT）理论的风险意识框架。我们的方法修改了目标函数，使得结果策略不仅最大化预期奖励，而且也遵循OT距离 zwischen state visitation distributions和欲望的风险质量。通过OT的数学精度，我们提供了一种形ulation，将风险考虑与传统RL目标函数相结合。我们的贡献得到了一系列定理的证明，映射了风险分布、优化值函数和策略行为之间的关系。通过OT的视角，这项工作探讨了RL中风险意识和奖励追求的平衡 fusion的可能性。
</details></li>
</ul>
<hr>
<h2 id="The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models"><a href="#The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models" class="headerlink" title="The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"></a>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06236">http://arxiv.org/abs/2309.06236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Spathis, Fahim Kawsar</li>
<li>for: 本研究旨在探讨语言模型如何处理numerical&#x2F;temporal数据，以及如何使用这些模型进行人类中心的任务，如移动健康感知。</li>
<li>methods: 本研究使用了各种语言模型，包括各种批处理大语言模型，以及提前调整和多模态适配器等方法来解决模态差距问题。</li>
<li>results: 研究发现，当 feeding numerical&#x2F;temporal数据到语言模型时，它们可能会 incorrectly tokenize temporal data，导致输出无意义。此外，提出了一些解决方案，如提前调整和多模态适配器，以帮助 bridge the “modality gap”。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore recent works that use LLMs for human-centric tasks such as mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address this issue, we propose potential solutions such as prompt tuning with lightweight embedding layers and multimodal adapters, which can help bridge the "modality gap". While the ability of language models to generalize to other modalities with minimal or no fine-tuning is promising, this paper highlights the fact that their outputs cannot be meaningful if they struggle with input nuances.
</details></li>
</ul>
<hr>
<h2 id="A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models"><a href="#A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models" class="headerlink" title="A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models"></a>A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06230">http://arxiv.org/abs/2309.06230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borui Tang, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</li>
<li>for: 这 paper 的目的是提出一种可扩展的算法，用于在高维数据中选择最佳子集，以便实现最佳subset selection。</li>
<li>methods: 该 paper 使用了一种新的普适的信息梯度函数，以确定最佳子集的大小。这种方法不需要选择模型或链函数，因此非常灵活。</li>
<li>results:  simulations 表明，该 algorithm 不仅是计算效率高，还能准确地回归最佳子集。这些结果表明，该 paper 提出的方法是一种可靠的、灵活的和可扩展的选择最佳子集方法。<details>
<summary>Abstract</summary>
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specific link function and hence is flexible to apply. Extensive simulation results demonstrate that our method is not only computationally efficient but also able to exactly recover the best subset in various settings (e.g., linear regression, Poisson regression, heteroscedastic models).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:分析高维数据引起了对单指数模型（SIMs）和最佳子集选择的增加兴趣。 SIMs 提供一种可解释和灵活的模型框架，而最佳子集选择则 aimsto 找到高维模型中的稀疏模型。然而，高维模型中的最佳子集选择是计算上不可能的。现有方法通常是放弃选择，但不会得到最佳子集解决方案。在这篇论文中，我们直接面临计算上的困难，并提出了首个可扩展的算法，用于高维 SIMs 中的最佳子集选择。我们的算法具有选择子集的一致性和高概率性。我们使用一种通用信息整合函数来确定回归系数的支持大小，从而消除了模型选择调整。此外，我们的方法不假设错误分布或特定的链函数，因此可以适应多种应用场景。我们的实验结果表明，我们的方法不仅是计算效率高，还能够在不同的设置中（例如线性回归、波尔兹回归、不同的误差分布）准确地恢复最佳子集。
</details></li>
</ul>
<hr>
<h2 id="Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data"><a href="#Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data" class="headerlink" title="Long-term drought prediction using deep neural networks based on geospatial weather data"></a>Long-term drought prediction using deep neural networks based on geospatial weather data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06212">http://arxiv.org/abs/2309.06212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vsevolod Grabar, Alexander Marusov, Alexey Zaytsev, Yury Maximov, Nazar Sotiriadi, Alexander Bulkin<br>for:这篇研究的目的是精确预测特定区域的旱情可能性，以便为农业实践中做出知 Informed Decision。methods:本研究使用了多种空间时间神经网络模型，包括卷积LSTM和Transformer，以预测旱情强度基于Palmer旱情严重指数（PDSI）。results:比较分析表明，卷积LSTM和Transformer模型在一个月到六个月的预测 horizons 中具有更高的ROC AUC分数，并且在不同环境条件下进行了广泛验证。<details>
<summary>Abstract</summary>
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.   Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 to 0.70 for forecast horizons from one to six months, outperforming baseline models. The transformer showed superiority for shorter horizons, while ConvLSTM did so for longer horizons. Thus, we recommend selecting the models accordingly for long-term drought forecasting.   To ensure the broad applicability of the considered models, we conduct extensive validation across regions worldwide, considering different environmental conditions. We also run several ablation and sensitivity studies to challenge our findings and provide additional information on how to solve the problem.
</details>
<details>
<summary>摘要</summary>
预测具体地区旱情概率的准确性是农业决策中的关键因素。一年前的预测特别重要，以便进行长期决策。然而，预测这个概率存在复杂的因素间互动和邻近地区的影响，带来挑战。本研究提出了一种综合解决方案，基于多种空间时间神经网络。我们考虑的模型集中 focuses on 根据Palmer旱情严重指数（PDSI）预测旱情程度，利用自然因素和气候模型的信息进行加强旱情预测。对比评估表明，Convolutional LSTM（ConvLSTM）和 transformer 模型在基eline gradient boosting 和 logistic regression 模型的基础上表现出了更高的准确性。这两个前者模型在一到六个月的预测 horizon 上 achievement ROC AUC 分数在0.90到0.70之间，超过基eline模型。transformer 模型在短期预测 horizon 上表现出了优异性，而 ConvLSTM 模型在长期预测 horizon 上具有优势。因此，我们建议根据预测时间长短选择合适的模型。为确保考虑的模型在不同环境条件下的可靠性，我们进行了广泛的验证，覆盖了世界各地的区域。我们还进行了多个减少和敏感性研究，以提供额外的信息和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding"><a href="#Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding" class="headerlink" title="Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding"></a>Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06195">http://arxiv.org/abs/2309.06195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaik Basheeruddin Shah, Pradyumna Pradhan, Wei Pu, Ramunaidu Randhi, Miguel R. D. Rodrigues, Yonina C. Eldar<br>for:这 paper 的目的是研究用于解决线性逆问题的算法，以便在各种应用中更好地使用。methods:这 paper 使用了基于 ISTA 和 ADMM 算法的数据驱动模型感知方法，并在训练过程中使用了梯度下降法。results:这 paper 研究了训练损失的优化保证，并证明在架构层 unfolded 网络中，随着训练样本数量的增加，梯度下降法可以达到near-zero 训练损失。此外，这 paper 还证明了这些 unfolded 网络在训练过程中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gradient descent based methods. Hence, we provide conditions, in terms of the network width and the number of training samples, on these unfolded networks for the PL$^*$ condition to hold. We achieve this by deriving the Hessian spectral norm of these networks. Additionally, we show that the threshold on the number of training samples increases with the increase in the network width. Furthermore, we compare the threshold on training samples of unfolded networks with that of a standard fully-connected feed-forward network (FFNN) with smooth soft-thresholding non-linearity. We prove that unfolded networks have a higher threshold value than FFNN. Consequently, one can expect a better expected error for unfolded networks than FFNN.
</details>
<details>
<summary>摘要</summary>
解决线性逆问题在许多应用中扮演着关键性的角色。基于数据驱动的模型意识 Algorithm unfolding 技术在这些问题上得到了广泛的关注。例如，learned iterative soft-thresholding algorithm (LISTA) 和 alternating direction method of multipliers compressive sensing network (ADMM-CSNet) 等等，都是基于 ISTA 和 ADMM 算法的常用方法。在这种研究中，我们研究了优化保证，即在学习轮数增加时，训练损失减少趋近于零的情况， для Finite-layer unfolded networks  such as LISTA 和 ADMM-CSNet 在过参数化（OP） regime 中。我们通过利用一种修改后的 Polyak-Lojasiewicz 条件（PL$^*$）来实现这一点。在特定的损失 landscape 中满足 PL$^*$ 条件，则可以确保存在全局最小值，并且使用梯度下降法 converge 到该最小值。因此，我们提供了网络宽度和训练样本数的条件，以确保 PL$^*$ 条件在这些网络中成立。我们通过计算这些网络的梯度spectral norm来实现这一点。此外，我们还证明了在网络宽度增加时，训练样本数的阈值也会增加。最后，我们比较了这些折叠网络和标准的完全连接Feed-forward network（FFNN）的训练样本数阈值，证明折叠网络的阈值高于 FFNN。因此，我们可以预期在折叠网络中获得更好的预期错误。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments"><a href="#Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments" class="headerlink" title="Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments"></a>Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06183">http://arxiv.org/abs/2309.06183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Gonzalez, Tommy Sonne Alstrøm, Tobias May</li>
<li>for: 这个研究旨在评估学习型声音提取系统的泛化能力，以及声音提取任务的难度如何影响系统的性能。</li>
<li>methods: 这个研究使用了一种新的泛化评估框架，通过使用测试条件下的参考模型，以评估系统在不同的数据库中的泛化性能。</li>
<li>results: 研究发现，对于所有模型，声音匹配度下的性能下降最多，而好声音和房间泛化性能可以通过训练多个数据库来实现。此外，最新的模型在匹配条件下表现出色，但在匹配不符条件下表现差，可能比FFNN-based系统更差。<details>
<summary>Abstract</summary>
The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test condition. This allows to disentangle the effect of the change in task difficulty from the effect of dealing with new data, and thus to define a new measure of generalization performance termed the generalization gap. The procedure is repeated in a cross-validation fashion by cycling through multiple speech, noise, and BRIR databases to accurately estimate the generalization gap. The proposed framework is applied to evaluate the generalization potential of a feedforward neural network (FFNN), Conv-TasNet, DCCRN and MANNER. We find that for all models, the performance degrades the most in speech mismatches, while good noise and room generalization can be achieved by training on multiple databases. Moreover, while recent models show higher performance in matched conditions, their performance substantially decreases in mismatched conditions and can become inferior to that of the FFNN-based system.
</details>
<details>
<summary>摘要</summary>
干扰性的语音混合物的音频特征是多种因素的影响，如目标说话人的spectro-temporal特征、干扰噪音和房间特性。这种大量的变化对学习基于语音增强系统的性能造成了主要的挑战，因为训练和测试条件之间的匹配可能会导致系统性能下降。通常，系统的普适性是通过在训练和测试集之间进行交互测试，并评估系统在新的语音、噪音和双耳响应函数（BRIR）数据库中的性能。但是，任务难度可能会在不同的数据库中发生变化，这会对结果产生重要的影响。本研究提出了一种普适性评估框架，该框架使用训练在测试条件下的参考模型，以便用其作为测试条件的difficulty水平的代理。这允许分解出与数据库变化的影响和与新数据处理的影响，并定义一个新的普适性度量——普适差。这种方法在批处理方式下重复进行，通过循环多个语音、噪音和BRIR数据库来准确估计普适差。我们在这些模型中应用这种框架，并发现：1）所有模型在语音匹配度下的性能最差；2）在多个数据库中训练可以实现好的噪音和房间普适性。此外，最新的模型在匹配条件下的性能较高，但在匹配不符条件下性能显著下降，可能变得落后于基于FFNN的系统。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>Efficient Memory Management for Large Language Model Serving with PagedAttention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06180">http://arxiv.org/abs/2309.06180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
<li>paper_authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica</li>
<li>for: 提高大语言模型（LLM）的高throughput服务，需要批处理足够多的请求 simultaneously。但现有系统受到缓存（KV cache）的内存占用限制，导致批处理量有限。</li>
<li>methods: 我们提出了PagedAttention算法， inspirited by classical virtual memory和paging技术。此外，我们还构建了vLLM服务系统，可以实现近于零的缓存内存浪费和请求间共享缓存。</li>
<li>results: 我们的evaluaion表明，vLLM可以提高具有同等响应时间的LLM服务系统的吞吐量，比如FasterTransformer和Orca的2-4倍。这种改进更加明显地出现在 longer sequences、更大的模型、以及更复杂的解码算法中。vLLM的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/vllm-project/vllm上下载。</a><details>
<summary>Abstract</summary>
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm
</details>
<details>
<summary>摘要</summary>
高速服务大语言模型（LLM）需要批处理足够多的请求。然而，现有系统受到缓存（KV cache）内存的巨大增长和减少的限制，导致批处理大小受限。当缓存管理不充分时，这些内存可能会受到重叠和重复占用，从而限制批处理大小。为解决这个问题，我们提出了 PagedAttention，一种基于经典虚拟内存和分页技术的注意机制。在其基础之上，我们构建了 vLLM，一个能够实现（1）缓存内存几乎为零浪费和（2）请求之间和请求内 flexible分享缓存的 LLM 服务系统。我们的评估表明，vLLM 可以在同等延迟下提高流行的 LLM 的 Throughput 2-4 倍，比如 FasterTransformer 和 Orca。这种改进更加明显地出现在 longer sequences、更大的模型、更复杂的解码算法上。vLLM 的源代码可以在 https://github.com/vllm-project/vLLM 上获取。
</details></li>
</ul>
<hr>
<h2 id="Elucidating-the-solution-space-of-extended-reverse-time-SDE-for-diffusion-models"><a href="#Elucidating-the-solution-space-of-extended-reverse-time-SDE-for-diffusion-models" class="headerlink" title="Elucidating the solution space of extended reverse-time SDE for diffusion models"></a>Elucidating the solution space of extended reverse-time SDE for diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06169">http://arxiv.org/abs/2309.06169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinpeng Cui, Xinyi Zhang, Zongqing Lu, Qingmin Liao<br>for:This paper focuses on improving the sampling efficiency of diffusion models (DMs) for image generation tasks.methods:The authors formulate the sampling process as an extended reverse-time stochastic differential equation (ER SDE) and leverage the semi-linear structure of the solutions to offer exact and approximate solutions.results:The authors achieve high-quality image generation with fast sampling efficiency, achieving 3.45 FID in 20 function evaluations and 2.24 FID in 50 function evaluations on the ImageNet 64x64 dataset. Additionally, they demonstrate that their proposed ER-SDE solvers are training-free and can elevate the efficiency of stochastic samplers to unprecedented levels.<details>
<summary>Abstract</summary>
Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE counterparts. Finally, we devise fast and training-free samplers, ER-SDE Solvers, elevating the efficiency of stochastic samplers to unprecedented levels. Experimental results demonstrate achieving 3.45 FID in 20 function evaluations and 2.24 FID in 50 function evaluations on the ImageNet 64$\times$64 dataset.
</details>
<details>
<summary>摘要</summary>
Diffusion models (DMs) 展示了强大的图像生成能力在不同的生成模型任务中。然而，它们的主要局限性在于慢的采样速度，需要数百或千个顺序的函数评估过大的神经网络来生成高质量图像。采样从 DMs 可以看作解决对应的随机 differential equations (SDEs) 或 ordinary differential equations (ODEs)。在这项工作中，我们将采样过程转化为延长的反时间 SDE (ER SDE)，统一先前的探索。利用 ER SDE 解的半线性结构，我们提供了精确解和高阶估计解 для VP SDE 和 VE SDE，分别。基于 ER SDE 的解空间，我们获得了数学意义，解释了 ODE 解算法在采样速度方面的优越性。此外，我们发现 VP SDE 解算法与 VE SDE 解算法相当。最后，我们设计了快速、无需训练的采样器，ER-SDE Solvers，使得杂样器的效率提升至历史最高水平。实验结果表明在 ImageNet 64x64 数据集上，我们可以在 20 个函数评估下获得 3.45 FID，并在 50 个函数评估下获得 2.24 FID。
</details></li>
</ul>
<hr>
<h2 id="Certified-Robust-Models-with-Slack-Control-and-Large-Lipschitz-Constants"><a href="#Certified-Robust-Models-with-Slack-Control-and-Large-Lipschitz-Constants" class="headerlink" title="Certified Robust Models with Slack Control and Large Lipschitz Constants"></a>Certified Robust Models with Slack Control and Large Lipschitz Constants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06166">http://arxiv.org/abs/2309.06166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlosch/cll">https://github.com/mlosch/cll</a></li>
<li>paper_authors: Max Losch, David Stutz, Bernt Schiele, Mario Fritz</li>
<li>for: 提高预测精度和证明 robustness</li>
<li>methods: 使用 Lipschitz-based regularizers 或 constraint，同时增加预测差</li>
<li>results: 提出 Calibrated Lipschitz-Margin Loss (CLL) 方法，解决了输入变化导致的攻击例子问题，并提高了证明可靠性，而不是降低精度。在 CIFAR-10、CIFAR-100 和 Tiny-ImageNet 上，我们的模型consistently outperform不考虑常量的损失函数。在 CIFAR-100 和 Tiny-ImageNet 上，CLL 超过了现有的 deterministic $L_2$ Robust Accuracy。在 contradiction 中，我们解锁了小型模型的潜在，不需要 $K&#x3D;1$ 约束。<details>
<summary>Abstract</summary>
Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establishing full control over slack and improving robustness certificates even with larger Lipschitz constants. On CIFAR-10, CIFAR-100 and Tiny-ImageNet, our models consistently outperform losses that leave the constant unattended. On CIFAR-100 and Tiny-ImageNet, CLL improves upon state-of-the-art deterministic $L_2$ robust accuracies. In contrast to current trends, we unlock potential of much smaller models without $K=1$ constraints.
</details>
<details>
<summary>摘要</summary>
儿童护理模型尚未具备证明的Robustness，尤其是面对输入变化（例如恶意示例）时。为了获得证明的Robustness， latest work 考虑了Lipschitz-based regularizers或约束，同时增加预测margin。然而，这会导致减少准确率。在这篇论文中，我们提出了准确证明的Lipschitz-Margin损失函数（CLL），解决了这些问题，并提高了证明Robustness。CLL通过explicitly calibrating loss w.r.t. margin和Lipschitz常量($K$)，以获得全面控制 над slack，并提高了证明Robustness certificates。我们的CLL在CIFAR-10、CIFAR-100和Tiny-ImageNet上的模型 consistently outperform不考虑$K$的损失函数。在CIFAR-100和Tiny-ImageNet上，CLL超越了当前的state-of-the-art deterministic $L_2$ Robust accuracy。而且，我们在许多小型模型中解锁了无需$K=1$约束的潜力。
</details></li>
</ul>
<hr>
<h2 id="Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines"><a href="#Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines" class="headerlink" title="Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines"></a>Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06157">http://arxiv.org/abs/2309.06157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tran, Hai-Canh Vu, Lam Pham, Nassim Boudaoud</li>
<li>for: 这个研究目的是为了预测旋转机器的剩下有用生命（Remaining Useful Life，RUL）和状况操作（Condition Operations，CO）。</li>
<li>methods: 本研究提出了一个具有多支分支深度学习架构的强健多支分支深度学习系统，用于预测旋转机器的RUL和CO。系统包括以下主要 ком成分：（1）一个LSTM自动encoder来降噪振声数据;（2）一个特征提取器来从降噪后的数据中生成时间频率域和时间频率域的特征;（3）一个新的和强健的多支分支深度学习网络架构来利用多个特征。</li>
<li>results: 本研究的实验结果显示，提出的系统在两个标准资料集XJTU-SY和PRONOSTIA上与现有的系统进行比较，表现出色，并且在实际应用中具有潜在的应用前景。<details>
<summary>Abstract</summary>
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于深度学习的多分支系统，用于预测旋转机件的剩余有用寿命（RUL）和conditions operation（CO）。特别是，该系统包括以下主要组成部分：1. LSTM自适应网络，用于减除振荡数据的噪声；2. 特征提取，用于从减除后的数据中提取时域、频域和时域频谱特征；3. 一种新的和可靠的多分支深度学习网络架构，用于利用多个特征。我们提posed系统的性能在两个benchmark数据集XJTU-SY和PRONOSTIA上进行了评估和比较，并经验表明，我们的提posed系统在RUL预测和CO认识方面表现出了优于现有系统的可靠性和实用性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Reliable-Domain-Generalization-A-New-Dataset-and-Evaluations"><a href="#Towards-Reliable-Domain-Generalization-A-New-Dataset-and-Evaluations" class="headerlink" title="Towards Reliable Domain Generalization: A New Dataset and Evaluations"></a>Towards Reliable Domain Generalization: A New Dataset and Evaluations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06142">http://arxiv.org/abs/2309.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiao Zhang, Xu-Yao Zhang, Cheng-Lin Liu</li>
<li>for: 这个论文的目的是提出一个新的领域泛化任务，用于提高针对中文字符识别的领域泛化方法的研究。</li>
<li>methods: 该论文使用了 eighteen 种领域泛化方法在 PaHCC（打印和手写中文字符）数据集上进行评估。</li>
<li>results: 研究发现，现有的领域泛化方法在该数据集上表现并不满意，而且在一种设计的动态领域泛化设定下，发现了许多领域泛化方法的缺陷。<details>
<summary>Abstract</summary>
There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the existing research, we propose a new domain generalization task for handwritten Chinese character recognition (HCCR) to enrich the application scenarios of DG method research. We evaluate eighteen DG methods on the proposed PaHCC (Printed and Handwritten Chinese Characters) dataset and show that the performance of existing methods on this dataset is still unsatisfactory. Besides, under a designed dynamic DG setting, we reveal more properties of DG methods and argue that only the leave-one-domain-out protocol is unreliable. We advocate that researchers in the DG community refer to dynamic performance of methods for more comprehensive and reliable evaluation. Our dataset and evaluations bring new perspectives to the community for more substantial progress. We will make our dataset public with the article published to facilitate the study of domain generalization.
</details>
<details>
<summary>摘要</summary>
有很多不同的分布Shift在实际世界中存在。然而，深度神经网络（DNNs）容易偏向训练集，这会导致它们接收到不同分布数据时表现出严重的性能下降。许多方法在域间泛化（DG） литературе中被研究，但是最近的DomainBed和WILDS bencmark挑战了这些方法的有效性。针对现有研究中的问题，我们提出了一个新的域间泛化任务 для手写中文字识别（HCCR），以扩展DG方法研究的应用场景。我们在提出的PaHCC（印刷和手写中文字） dataset上评估了 eighteen DG 方法的性能，并发现现有方法在这个 dataset 上的性能仍然不满足。此外，在我们设计的动态DG设定下，我们揭示了更多的DG方法的性能特性，并 argue that只有离开一个频道的协议是不可靠的。我们建议研究人员在DG社区中参考动态方法的性能进行更加全面和可靠的评估。我们的 dataset 和评估将为社区带来新的视角，以便更加快速的进步。我们将在文章发表时将 dataset 公开，以便研究域间泛化。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs"><a href="#Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs" class="headerlink" title="Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs"></a>Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06127">http://arxiv.org/abs/2309.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhananjaya Wijerathne, Zhaoying Li, Tulika Mitra</li>
<li>for: Edge AI applications</li>
<li>methods: Architecture-adaptive Coarse-Grained Reconfigurable Arrays (CGRAs) and a comprehensive ecosystem of tools including a tailored compiler, simulator, accelerator synthesis, and validation framework.</li>
<li>results: Automatic compilation and verification of AI application kernels onto user-defined CGRA architectures, facilitating efficient edge AI applications for a wide range of embedded AI workloads.Here is the text in Simplified Chinese:</li>
<li>for: 边缘AI应用</li>
<li>methods: 基于CGRA架构的自适应 architecture-adaptive Coarse-Grained Reconfigurable Arrays (CGRAs) 和一个完整的开发环境，包括一个特定的编译器、仿真器、加速器合成和验证框架。</li>
<li>results: 通过自动将AI应用核心编译到用户定义的CGRA架构上，实现了高效的边缘AI应用，覆盖了广泛的嵌入式AI工作负荷。<details>
<summary>Abstract</summary>
Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as power-efficient edge accelerator, offering versatility beyond AI applications. Morpher, an open-source, architecture-adaptive CGRA design framework, is specifically designed to explore the vast design space of CGRAs. The comprehensive ecosystem of Morpher includes a tailored compiler, simulator, accelerator synthesis, and validation framework. This study provides an overview of Morpher, highlighting its capabilities in automatically compiling AI application kernels onto user-defined CGRA architectures and verifying their functionality. Through the Morpher framework, the versatility of CGRAs is harnessed to facilitate efficient compilation and verification of edge AI applications, covering important kernels representative of a wide range of embedded AI workloads. Morpher is available online at https://github.com/ecolab-nus/morpher-v2.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。Coarse-Grained Reconfigurable Arrays (CGRAs) 提供了高效的Edge加速器， offering versatility beyond AI应用程序。 Morpher，一个开源的、architecture-adaptive CGRA设计框架，是专门为探索CGRA的庞大设计空间而设计的。 Morpher的完整生态系统包括特制的编译器、模拟器、加速器合成和验证框架。本研究提供了Morpher的概述， highlighting its capabilities in automatically compiling AI应用程序核心 onto user-defined CGRA架构并验证其功能。通过Morpher框架，CGRA的灵活性被利用，以便高效地编译和验证边缘AI应用程序，覆盖重要的核心代表许多嵌入式AI工作负荷。 Morpher可以在https://github.com/ecolab-nus/morpher-v2中下载。>>>
</details></li>
</ul>
<hr>
<h2 id="AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy"><a href="#AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy" class="headerlink" title="AstroLLaMA: Towards Specialized Foundation Models in Astronomy"></a>AstroLLaMA: Towards Specialized Foundation Models in Astronomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06126">http://arxiv.org/abs/2309.06126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciucă, Charlie O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz Różański, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodríguez Méndez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney, Kevin Schawinski, UniverseTBD</li>
<li>for:  bridging the gap between large language models and highly specialized domains like scholarly astronomy</li>
<li>methods:  fine-tuning a 7-billion-parameter model from LLaMA-2 using over 300,000 astronomy abstracts from arXiv, optimized for traditional causal language modeling</li>
<li>results:  achieving a 30% lower perplexity than Llama-2, generating more insightful and scientifically relevant text completions and embedding extraction than state-of-the-art foundation models despite having significantly fewer parameters<details>
<summary>Abstract</summary>
Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多人类语言任务中表现出色，但在高度特殊化的学术天文领域中经常表现不佳。为了bridging这个差距，我们引入AstroLLaMA，一个基于LLaMA-2的70亿个parameters的模型，通过arXiv上的300,000篇天文摘要进行了微调。我们的模型优化了传统的 causal language modeling，并在天文领域中表现出30%的负据况下降，显示了明显的领域适应。我们的模型在对天文领域的文本完成和嵌入EXTRACTING方面表现出更多的科学和技术相关的内容，即使有较少的参数。AstroLLaMA作为一个专业的天文模型，具有广泛的微调潜力。我们将其公开发布，以促进天文研究，包括自动摘要和对话代理开发。
</details></li>
</ul>
<hr>
<h2 id="A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM"><a href="#A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM" class="headerlink" title="A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)"></a>A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06122">http://arxiv.org/abs/2309.06122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Rangel DaCosta, Katherine Sytwu, Catherine Groschner, Mary Scott</li>
<li>for: 本研究旨在开发高精度自动分析工具，用于Characterization of nanomaterials，包括高分辨率电子镜 transmision electron microscopy (HRTEM)。</li>
<li>methods: 本研究使用Python package Construction Zone， quickly generate complex nanoscale atomic structures，并开发了一个端到端的工作流程，用于创建大量的模拟数据库。</li>
<li>results: 通过对不同subset的模拟数据进行训练，我们实现了对实验图像中的粒子 segmentation的最佳性能，并研究了数据准备过程中不同因素的影响，包括模拟精度、结构分布和捕捉条件分布。<details>
<summary>Abstract</summary>
Machine learning techniques are attractive options for developing highly-accurate automated analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapidly generating complex nanoscale atomic structures, and develop an end-to-end workflow for creating large simulated databases for training neural networks. Construction Zone enables fast, systematic sampling of realistic nanomaterial structures, and can be used as a random structure generator for simulated databases, which is important for generating large, diverse synthetic datasets. Using HRTEM imaging as an example, we train a series of neural networks on various subsets of our simulated databases to segment nanoparticles and holistically study the data curation process to understand how various aspects of the curated simulated data -- including simulation fidelity, the distribution of atomic structures, and the distribution of imaging conditions -- affect model performance across several experimental benchmarks. Using our results, we are able to achieve state-of-the-art segmentation performance on experimental HRTEM images of nanoparticles from several experimental benchmarks and, further, we discuss robust strategies for consistently achieving high performance with machine learning in experimental settings using purely synthetic data.
</details>
<details>
<summary>摘要</summary>
machine learning技术是开发高精度自动分析工具的优选方案，包括高分辨率电子显微镜（HRTEM）。然而，实现这些机器学习工具可能困难，因为实验获得大量、高质量训练数据的挑战。在这种情况下，我们介绍了“Construction Zone”Python包，用于快速生成复杂的nanoscale原子结构，并开发了终端工作流程，用于创建大规模的模拟数据库。Construction Zone可以快速、系统地采样真实的nanomaterial结构，并可以用作模拟数据库的随机结构生成器，这对于生成大量、多样化的 sintetic数据集是非常重要。使用HRTEM成像为例，我们在不同的模拟数据库中训练了一系列神经网络，以分类nanoparticles并全面研究数据准备过程，以了解不同的模拟数据的准确性、原子结构分布和成像条件分布如何影响模型性能。使用我们的结果，我们可以在多个实验室中实现状态机器学习性能的最佳分 segmentationHRTEM图像，并讨论了一些可靠的机器学习术语在实验设置中使用纯 sintetic数据时实现高性能的策略。
</details></li>
</ul>
<hr>
<h2 id="Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning"><a href="#Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning" class="headerlink" title="Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning"></a>Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06097">http://arxiv.org/abs/2309.06097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Liu, Wubing Chen, Mao Tan</li>
<li>for: 解释DRLagent的决策过程，提高用户对DRLagent的信任和强点的了解。</li>
<li>methods: 基于Interpretable Policy Extraction（IPE）方法，通过纳入一种新的准确度测量机制，以提高DRLagent的决策过程的可读性和一致性。</li>
<li>results: 在StarCraft II Complex Control Environment中进行实验，FIPE方法在交互性和一致性两个方面都超过基eline，同时易于理解。<details>
<summary>Abstract</summary>
Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE methods. The experiment results demonstrate that FIPE outperforms the baselines in terms of interaction performance and consistency, meanwhile easy to understand.
</details>
<details>
<summary>摘要</summary>
我们开始是分析现有的 IPE 方法的优化机制，探讨了忽略一致性而提高总奖励的问题。然后，我们设计了一种准确度引导机制，通过将准确度measurementintegrated into the reinforcement learning feedback。我们在StarCraft II 中进行了实验，这是现有 IPE 方法通常避免的复杂控制环境。实验结果表明，FIPE 在交互性性和一致性方面都超过了基eline，而且易于理解。
</details></li>
</ul>
<hr>
<h2 id="A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis"><a href="#A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis" class="headerlink" title="A General Verification Framework for Dynamical and Control Models via Certificate Synthesis"></a>A General Verification Framework for Dynamical and Control Models via Certificate Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06090">http://arxiv.org/abs/2309.06090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Andrea Peruffo, Alessandro Abate</li>
<li>for: 本研究旨在提供一个通用框架，用于编码系统特性和定义相应的证书，以及自动生成控制器和证书。</li>
<li>methods: 我们使用神经网络提供候选控制函数和证书函数，并使用SMT解决器确保正确性。</li>
<li>results: 我们对一个大量和多样化的 benchmark 进行了验证，并证明了我们的框架能够成功地验证控制和证书。<details>
<summary>Abstract</summary>
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of correctness. We test our framework by developing a prototype software tool, and assess its efficacy at verification via control and certificate synthesis over a large and varied suite of benchmarks.
</details>
<details>
<summary>摘要</summary>
一种新兴的控制理论分支是证书学习，关注某个自主或控制模型的行为特性的规范，通过函数基本证明来分析。然而，将这些复杂的要求满足是一个非常困难的任务，可能会让最有经验的控制工程师感到惑乱。这导致了一种自动化的方法的需求，能够设计控制器并分析广泛的复杂规范。在这篇论文中，我们提供一个通用的框架来编码系统规范和对应的证书，并提出了一种自动化的控制器和证书Synthesize的方法。我们的方法在安全学习控制领域中发挥了灵活性的神经网络提供候选控制和证书函数，而使用SMT-解决方案提供正式的正确性保证。我们测试了我们的框架，开发了一个原型软件工具，并通过控制和证书验证 benchmarks 进行了效果的评估。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies"><a href="#Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies" class="headerlink" title="Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies"></a>Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06089">http://arxiv.org/abs/2309.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boshko Koloski, Blaž Škrlj, Marko Robnik-Šikonja, Senja Pollak</li>
<li>for: 这个研究旨在比较两种精致化方法，以及零扩展和全扩展学习方法，以测试大语言模型在跨语言设定下的性能。</li>
<li>methods: 研究使用了两种精致化方法：一是参数有效的适配器方法，另一是精致化所有参数。另外，研究还使用了两种跨语言转移策略：一是中途训练（IT），另一是跨语言验证（CLV）。</li>
<li>results: 研究结果显示，在两个不同的分类问题（ hate speech detection 和产品评价）中，IT 跨语言策略比 CLV 策略在目标语言中表现出色。此外，研究还发现，在多个跨语言转移中，CLV 策略在基础语言（英文）中具有较好的知识储存性，而 IT 策略则在跨语言转移中导致更多的知识损失。<details>
<summary>Abstract</summary>
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several languages, show that the \textit{IT} cross-lingual strategy outperforms \textit{CLV} for the target language. Our findings indicate that, in the majority of cases, the \textit{CLV} strategy demonstrates superior retention of knowledge in the base language (English) compared to the \textit{IT} strategy, when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</details>
<details>
<summary>摘要</summary>
cross-lingual transfer是一种有前途的技术，用于解决少 ressourced languages中的任务。在这个实验性研究中，我们比较了两种 fine-tuning approaches 在 cross-lingual Setting 中的性能。为 fine-tuning 策略，我们比较了参数效率的 adapter 方法和所有参数的 fine-tuning。为 cross-lingual transfer 策略，我们比较了中间训练 (\textit{IT}) 和 across-lingual validation (\textit{CLV})。我们评估了交互语言转移的成功和原语言中的知识损失，即在学习新语言时， previously acquired 的知识是否会产生恶性忘记。我们在 hate speech detection 和 product reviews 两个不同的分类任务中，每个任务包含多种语言的数据集，得到的结果表明，\textit{IT} cross-lingual 策略在目标语言上表现出色，而 \textit{CLV} 策略在基语言（英语）中的知识保留性比 \textit{IT} 策略更高。
</details></li>
</ul>
<hr>
<h2 id="Plasticity-Optimized-Complementary-Networks-for-Unsupervised-Continual-Learning"><a href="#Plasticity-Optimized-Complementary-Networks-for-Unsupervised-Continual-Learning" class="headerlink" title="Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning"></a>Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06086">http://arxiv.org/abs/2309.06086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alviur/pocon_wacv2024">https://github.com/alviur/pocon_wacv2024</a></li>
<li>paper_authors: Alex Gomez-Villa, Bartlomiej Twardowski, Kai Wang, Joost van de Weijer</li>
<li>for: 本研究旨在提高无监督学习（SSL）技术的 continuous unsupervised representation learning（CURL）方法，以提高无监督学习方法在多任务数据流中的表现。</li>
<li>methods: 我们提出了一种训练专家网络，以释放 previous 知识的责任，并且可以完全适应新任务（优化流动性）。在第二阶段，我们将新的知识与之前的网络相结合，以避免忘记和初始化一个新的专家网络（适应-反思阶段）。</li>
<li>results: 我们的方法在 few-task 和 many-task 分配 Setting 中比其他 CURL 无 exemplar 方法表现出色，并且在 semi-supervised continual learning（Semi-SCL） Setting 中，我们的方法超过其他 exemplar-free 方法的准确率，并达到其他使用 exemplars 的方法的结果。<details>
<summary>Abstract</summary>
Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提议 trains an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars.
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events"><a href="#A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events" class="headerlink" title="A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events"></a>A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06082">http://arxiv.org/abs/2309.06082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Jain, Xueqing Sun, Sohom Datta, Abhishek Somani</li>
<li>For: The paper aims to analyze the primary drivers behind price spike events in modern electricity markets with high renewable energy penetration, using machine learning techniques.* Methods: The authors propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets.* Results: The framework is applied to open-source publicly available datasets from California Independent System Operator (CAISO) and ISO New England (ISO-NE) to identify the main drivers behind price spike events. The results can be used for various critical aspects of market design, renewable dispatch and curtailment, operations, and cyber-security applications.<details>
<summary>Abstract</summary>
Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch and curtailment, operations, and cyber-security applications. The framework can be applied to any ISO or market data; however, in this paper, it is applied to open-source publicly available datasets from California Independent System Operator (CAISO) and ISO New England (ISO-NE).
</details>
<details>
<summary>摘要</summary>
电力网络正在往100%可再生能源源扩大，电力市场的总趋势和电力系统运行方式都在变化。电力市场不仅经济地分配资源，还考虑了各种可控行为，如可再生能源削减、电网压缩缓冲和能量存储优化，以确保网络可靠性。因此，电力市场的价格形成变得非常复杂。传统的根本原因分析和统计方法在现代网络和市场中变得无法应用，用于分析和推导现代电力市场中价格形成的主要驱动力。本文提出了一种基于机器学习的分析框架，用于分解现代电力市场中价格峰值事件的主要驱动力。该框架可以应用于任何ISO或市场数据，但在这篇论文中，它被应用于开源的公共数据集中。
</details></li>
</ul>
<hr>
<h2 id="Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case"><a href="#Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case" class="headerlink" title="Information Flow in Graph Neural Networks: A Clinical Triage Use Case"></a>Information Flow in Graph Neural Networks: A Clinical Triage Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06081">http://arxiv.org/abs/2309.06081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Valls, Mykhaylo Zayats, Alessandra Pascale</li>
<li>for: 本文研究了图神经网络（GNNs）在医疗领域和其他领域中的应用，以及如何高效地训练GNNs。</li>
<li>methods: 本文提出了一个数学模型，该模型将GNN连接与图数据连接解耦开，以评估GNNs在临床排序Use Case中的性能。</li>
<li>results: 结果表明，在 integrate 域知识到GNN连接中时，GNNs的性能更好than使用同一个KG连接或允许不制embedding传播。此外，我们发现负边Edge play a crucial role in achieving good predictions, and using too many GNN layers can degrade performance.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained popularity in healthcare and other domains due to their ability to process multi-modal and multi-relational graphs. However, efficient training of GNNs remains challenging, with several open research questions. In this paper, we investigate how the flow of embedding information within GNNs affects the prediction of links in Knowledge Graphs (KGs). Specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case. Our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
</details>
<details>
<summary>摘要</summary>
граф нейронные сети (GNNs) 在医疗领域和其他领域中获得了广泛的应用，这是因为它们可以处理多Modal和多关系图。然而，fficiently 训练 GNNs 仍然是一个开放的研究问题，有多个未解决的问题。在这篇论文中，我们 investigate 如何在 GNNs 中流动嵌入信息的方式affects the prediction of links in Knowledge Graphs (KGs).specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case.our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation.moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.Here's the translation in Traditional Chinese:граф нейрон网络 (GNNs) 在医疗领域和其他领域中获得了广泛的应用，这是因为它们可以处理多Modal 和多关系图。然而，fficiently 训练 GNNs 仍然是一个开放的研究问题，有多个未解决的问题。在这篇论文中，我们 investigate 如何在 GNNs 中流动嵌入信息的方式affects the prediction of links in Knowledge Graphs (KGs).specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case.our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation.moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
</details></li>
</ul>
<hr>
<h2 id="A2V-A-Semi-Supervised-Domain-Adaptation-Framework-for-Brain-Vessel-Segmentation-via-Two-Phase-Training-Angiography-to-Venography-Translation"><a href="#A2V-A-Semi-Supervised-Domain-Adaptation-Framework-for-Brain-Vessel-Segmentation-via-Two-Phase-Training-Angiography-to-Venography-Translation" class="headerlink" title="A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation"></a>A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06075">http://arxiv.org/abs/2309.06075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga</li>
<li>for: 这个 paper 的目的是提出一种 semi-supervised domain adaptation 框架，用于脑血管分割不同像模式下。</li>
<li>methods: 该方法 基于 annotated angiographies 和有限数量的 annotated venographies，实现图像到图像的翻译和semantic segmentation，并且利用分离的和semantically rich的幂 space 来表示多样化数据，并实现图像级别的适应。</li>
<li>results: 对 magnetic resonance angiographies 和 venographies 进行评估，该方法 在源领域中实现了状态空间的表现，而在目标领域中的 Dice 分数减少了8.9%，这说明了该方法 在不同频谱下的脑血管图像分割中具有可靠的潜在性。<details>
<summary>Abstract</summary>
We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art performance in the source domain, our method attains a Dice score coefficient in the target domain that is only 8.9% lower, highlighting its promising potential for robust cerebrovascular image segmentation across different modalities.
</details>
<details>
<summary>摘要</summary>
我们提出了一种半监督频道适应框架，用于不同频谱图像的脑血管分割。现有的状态艺术方法都将注意力集中在单一频谱上，忽略了脑血管成像技术的广泛应用。这可能会导致重要的分布偏移，从而影响到不同频谱之间的泛化性。我们的框架利用注释的材料和有限的注释掌肘图像，实现图像到图像的翻译和semantic分割，通过分离和semanticallyRich的幂谱空间来表示不同数据类型，并在目标频谱上进行图像级适应。此外，我们减少了循环结构的TypicalComplexity和减少了对抗训练，这使得我们可以建立高效和直观的模型，并保证了固定的训练。我们在核磁共振成像和掌肘成像上评估了我们的方法，而在源频谱上达到了状态艺术性的性能，而在目标频谱上的Dice分数系数只下降了8.9%，这表明了我们的方法在不同频谱之间的精准脑血管图像分割的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Selection-of-contributing-factors-for-predicting-landslide-susceptibility-using-machine-learning-and-deep-learning-models"><a href="#Selection-of-contributing-factors-for-predicting-landslide-susceptibility-using-machine-learning-and-deep-learning-models" class="headerlink" title="Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models"></a>Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06062">http://arxiv.org/abs/2309.06062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Chen, Lei Fan</li>
<li>for: 这个论文的目的是研究降坡可能性评估中因素选择对机器学习和深度学习模型的预测精度的影响。</li>
<li>methods: 这个论文使用了四种因素选择方法，包括信息增益比率（IGR）、回归特征选择（RFE）、血统群Optimization（PSO）和最小绝对减少和选择操作（LASSO），以及哈里斯鸥鹰优化（HHO）。此外，这个论文还研究了基于自适应器的因素选择方法 для深度学习模型。</li>
<li>results: 这个论文的研究结果表明，不同的因素选择方法对机器学习和深度学习模型的预测精度有所不同。 certain methods can improve the prediction accuracy of the models, while others may not have a significant impact.<details>
<summary>Abstract</summary>
Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO) and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...
</details>
<details>
<summary>摘要</summary>
Landslides are a common natural disaster that can cause casualties, property safety threats, and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO), and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...Here's the translation in Traditional Chinese:Landslides are a common natural disaster that can cause casualties, property safety threats, and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO), and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems"><a href="#Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems" class="headerlink" title="Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems"></a>Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06061">http://arxiv.org/abs/2309.06061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Toreini, Maryam Mehrnezhad, Aad van Moorsel</li>
<li>for: The paper aims to provide a secure, verifiable, and privacy-preserving protocol for computing and verifying the fairness of any machine learning (ML) model.</li>
<li>methods: The proposed method, called Fairness as a Service (FaaS), uses cryptograms to represent data and outcomes, and zero-knowledge proofs to guarantee the well-formedness of the cryptograms and underlying data. The method is model-agnostic and can support various fairness metrics, making it a versatile tool for auditing the fairness of any ML model.</li>
<li>results: The paper demonstrates the effectiveness of FaaS using a publicly available data set with thousands of entries, and shows that the method can provide secure and verifiable fairness metrics without the need for trusted third parties or private channels.<details>
<summary>Abstract</summary>
Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.
</details>
<details>
<summary>摘要</summary>
《公平机器学习》是一个日益繁荣的研究领域。在这篇论文中，我们提出了一种名为《公平服务（FaaS）》的安全、可靠和隐私保持的协议，用于计算和验证任何机器学习（ML）模型的公平性。在FaaS的设计中，数据和结果都是通过加密来保护隐私。此外，零知识证明 garantizesthe well-formedness of the cryptograms and underlying data。FaaS是模型无关的，可以支持多种公平度量表，因此可以作为对任何ML模型的公平性进行审核的服务。我们的解决方案不需要任何不信任第三方或私人通道来计算公平度量。安全保证和承诺是通过一系列安全、透明和可靠的步骤来实现的，从计算开始到结束。所有输入数据的加密文本都是公开可见的，例如对审计人、社会活动人员和专家来说，以便验证过程的正确性。我们实现了FaaS，以评估性能和示case用FaaS对公共数据集进行成功使用。
</details></li>
</ul>
<hr>
<h2 id="How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task"><a href="#How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task" class="headerlink" title="How does representation impact in-context learning: A exploration on a synthetic task"></a>How does representation impact in-context learning: A exploration on a synthetic task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06054">http://arxiv.org/abs/2309.06054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwen Fu, Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng<br>for: 这种研究探讨了 transformer 模型在受限样本下进行学习的能力，即受限上下文学习能力。methods: 该研究使用了一种新的创新任务，并设计了两种探测器来评估两种不同的组成部分，即受限模型重量和上下文样本的影响。results: 研究发现，受限上下文组分对受限学习性能具有极高相关性，表明受限学习和表示学习之间存在束缚关系。此外，研究还发现一个好的受限模型重量可以有助于上下文组分的学习， indicating that 受限模型重量应该是受限学习的基础。<details>
<summary>Abstract</summary>
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation learning. Furthermore, we find that a good in-weights component can actually benefit the learning of the in-context component, indicating that in-weights learning should be the foundation of in-context learning. To further understand the the in-context learning mechanism and importance of the in-weights component, we proof by construction that a simple Transformer, which uses pattern matching and copy-past mechanism to perform in-context learning, can match the in-context learning performance with more complex, best tuned Transformer under the perfect in-weights component assumption. In short, those discoveries from representation learning perspective shed light on new approaches to improve the in-context capacity.
</details>
<details>
<summary>摘要</summary>
Contextual learning，即从Contextual samples中学习，是Transformer卓越的能力。然而，这种学习机制的具体原理尚未完全理解。在这项研究中，我们从表示学习的新角度来调查。表示更复杂的场景是Contextual learning scenario，其表示可以受到模型参数和Contextual samples的影响。我们将这两个概念性方面的表示称为内部预测（in-weight component）和Contextual component，分别。为了研究这两个组件如何影响Contextual learning能力，我们构建了一个新的人工任务，可以设计两个探针，即内部预测探针和Contextual探针，来评估这两个组件。我们发现，Contextual component的质量与Contextual learning性能高度相关，这表明Contextual learning和表示学习之间存在紧密的关系。此外，我们发现一个好的内部预测组件可以有助于Contextual learning组件的学习，这表明内部预测学习应该是Contextual learning的基础。为了更深入理解Contextual learning机制和内部预测组件的重要性，我们证明了一个简单的Transformer模型，通过使用模式匹配和复制机制来实现Contextual learning，可以与最佳调参的Transformer模型匹配Contextual learning性能，假设内部预测组件是完美的。总之，这些从表示学习角度出发的发现可能为Contextual learning提供新的改进方法。
</details></li>
</ul>
<hr>
<h2 id="A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation"><a href="#A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation" class="headerlink" title="A Perceptron-based Fine Approximation Technique for Linear Separation"></a>A Perceptron-based Fine Approximation Technique for Linear Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06049">http://arxiv.org/abs/2309.06049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ákos Hajnal</li>
<li>for: 本研究提出了一种新的在线学习方法，用于找到标注为正或负的数据点之间的分隔hyperplane。</li>
<li>methods: 该方法基于Perceptron算法，但是只在搜索分隔hyperplane时调整神经元的权重。</li>
<li>results: 实验结果表明，该方法可以更高效地训练基于机器学习的二分类器，特别是当数据集的大小超过数据维度时。<details>
<summary>Abstract</summary>
This paper presents a novel online learning method that aims at finding a separator hyperplane between data points labelled as either positive or negative. Since weights and biases of artificial neurons can directly be related to hyperplanes in high-dimensional spaces, the technique is applicable to train perceptron-based binary classifiers in machine learning. In case of large or imbalanced data sets, use of analytical or gradient-based solutions can become prohibitive and impractical, where heuristics and approximation techniques are still applicable. The proposed method is based on the Perceptron algorithm, however, it tunes neuron weights in just the necessary extent during searching the separator hyperplane. Due to an appropriate transformation of the initial data set we need not to consider data labels, neither the bias term. respectively, reducing separability to a one-class classification problem. The presented method has proven converge; empirical results show that it can be more efficient than the Perceptron algorithm, especially, when the size of the data set exceeds data dimensionality.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的在线学习方法，旨在找到标注为正或负的数据点之间的分隔hyperplane。由于人工神经元的权重和偏置可以直接关联到高维空间中的hyperplane，因此该技术可以在机器学习中训练基于perceptron的二分类器。在面临大或不均匀数据集时，使用分析或梯度基于的解决方案可能变得不可能和不实际，而恰当的决策和近似技术仍然可以应用。提出的方法基于Perceptron算法，但它在搜索分隔hyperplane时只需要调整神经元权重的必要范围。由于数据集的初始变换，我们不需要考虑数据标签，也不需要考虑偏置项，因此将分类问题降低到一个一类问题。表示的方法已经证明是可靠的，实验结果表明，它在数据集大于数维度时比Perceptron算法更高效。
</details></li>
</ul>
<hr>
<h2 id="BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise"><a href="#BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise" class="headerlink" title="BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise"></a>BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06046">http://arxiv.org/abs/2309.06046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeroen M. Galjaard, Robert Birke, Juan Perez, Lydia Y. Chen</li>
<li>for: This paper studies the impact of label noise on the performance of state-of-the-art meta-learners and proposes two sampling techniques to mitigate the impact of label noise.</li>
<li>methods: The paper uses gradient-based $N$-way $K$-shot learners and proposes two sampling techniques, namely manifold (Man) and batch manifold (BatMan), to transform noisy supervised learners into semi-supervised learners.</li>
<li>results: The paper shows that the proposed sampling techniques can effectively mitigate the impact of meta-training label noise, limiting the meta-testing accuracy drop to 2.5%, 9.4%, and 1.1% respectively, with existing meta-learners across the Omniglot, CifarFS, and MiniImagenet datasets, even with 60% wrong labels.<details>
<summary>Abstract</summary>
The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastive-shot tasks through augmentation, learning the embedding via a contrastive loss in meta-training, and then perform classification through zeroing on the embedding in meta-testing. We show that our approach can effectively mitigate the impact of meta-training label noise. Even with 60% wrong labels \batman and \man can limit the meta-testing accuracy drop to ${2.5}$, ${9.4}$, ${1.1}$ percent points, respectively, with existing meta-learners across the Omniglot, CifarFS, and MiniImagenet datasets.
</details>
<details>
<summary>摘要</summary>
经典超级学习中 label noise 的负面影响已得到广泛研究，然而在多学习领域中仍然是一个开放的研究问题。多学习器 aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. 在这篇论文中，我们提供了首次对多学习器在 label noise 的影响进行了广泛分析，特别是 gradient-based $N$-way $K$-shot learners。我们发现在 Omniglot 和 CifarFS 数据集上，随着 label noise 的增加，Reptile、iMAML 和 foMAML 的精度下降了最多 42%。为强化对 label noise 的抵抗力，我们提议了两种抽样技术，namely manifold (Man) 和 batch manifold (BatMan)，这些技术可以将杂 Label 转化为 semi-supervised learners，以提高杂 Label 的使用价值。我们首先通过扩展来构建 $N$-way $2$-contrastive-shot 任务的 manifold 样本，然后通过 zeroing 来进行分类，并在 meta-testing 中使用嵌入来进行识别。我们表明，我们的方法可以有效地减轻 meta-training label noise 的影响。甚至在 60% 的杂 Label 下，我们的 \batman 和 \man 可以限制 meta-testing 精度下降到 ${2.5}$, ${9.4}$, ${1.1}$ 个百分点，分别在 Omniglot、CifarFS 和 MiniImagenet 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model"><a href="#Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model" class="headerlink" title="Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model"></a>Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06453">http://arxiv.org/abs/2309.06453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao</li>
<li>for: 本研究的目的是解释受过超级vised和无级vised培训的句子嵌入学习模型（CSE）在训练过程中的性能差距，以及如何缩小这个差距。</li>
<li>methods: 本研究使用了empirical experiments进行了比较，以 Investigate the behavior of supervised and unsupervised CSE during their respective training processes。 We also introduce a metric called Fitting Difficulty Increment (FDI) to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset。</li>
<li>results: 我们发现，supervised和无级vised CSE在训练过程中的行为有 significan differences，主要归结于训练数据集的适应难度差异。 Based on these insights, we propose a method to increase the fitting difficulty of the training dataset by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns。 This approach effectively narrows the performance gap between supervised and unsupervised CSE.<details>
<summary>Abstract</summary>
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We observe a significant difference in fitting difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment (FDI), to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset, and use the metric to answer the "What" question. Then, based on the insights gained from the "What" question, we tackle the "How" question by increasing the fitting difficulty of the training dataset. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE.
</details>
<details>
<summary>摘要</summary>
我们首先通过对supervised和unsupervised CSE的训练过程进行比较，从比较中获得了一些有趣的发现。我们发现，supervised和unsupervised CSE在训练过程中的不同行为是导致性能差距的主要原因。为了回答 "What" 问题，我们提出了一个度量，即 Fitting Difficulty Increment (FDI)，用于度量训练数据集和保留数据集之间的适应难度差异。通过度量的帮助，我们回答了 "What" 问题。然后，基于我们从 "What" 问题中获得的启示，我们通过增加训练数据集的适应难度来缩小性能差距。我们利用 Large Language Model (LLM) 的 In-Context Learning (ICL) 能力生成数据，以模拟复杂的句子模式。通过利用 LLM 生成的数据中的层次句子模式，我们有效地缩小了 supervised 和 unsupervised CSE 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning"><a href="#Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning" class="headerlink" title="Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning"></a>Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06034">http://arxiv.org/abs/2309.06034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixdjc/nlgad">https://github.com/felixdjc/nlgad</a></li>
<li>paper_authors: Jingcan Duan, Pei Zhang, Siwei Wang, Jingtao Hu, Hu Jin, Jiaxin Zhang, Haifang Zhou, Haifang Zhou</li>
<li>for: 本 paper 的目的是提出一种基于多尺度对比学习网络的图生差检测方法 (GAD)，以提高检测性能。</li>
<li>methods: 该方法首先使用多尺度对比网络初始化模型，然后采用有效的混合策略选择可靠的正常节点，最后使用只有正常节点的输入进行模型练习，以学习更加准确的正常模式，从而更好地发现异常节点。</li>
<li>results: 对六个图Dataset进行了广泛的实验，并证明了该方法的效iveness，与当前最佳方法相比，检测性能提高了5.89%。<details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</details>
<details>
<summary>摘要</summary>
GRAPH anomaly detection (GAD) 在机器学习和数据挖掘领域受到了越来越多的关注。最近的研究主要集中在如何更好地捕捉更多的信息，以提高节点嵌入的质量。despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples, which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</details></li>
</ul>
<hr>
<h2 id="Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA"><a href="#Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA" class="headerlink" title="Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA"></a>Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06033">http://arxiv.org/abs/2309.06033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Valente da Silva, Onel L. Alcaraz López, Richard Demo Souza</li>
<li>for: 这篇论文旨在解决 federated learning (FL) 中的 Edge 设备电源不足问题，提出了一种将能源收集 (EH) 设备 integrate into FL 网络中，以确保低能源停机概率和未来任务执行成功。</li>
<li>methods: 该论文提出了一种使用多渠道 ALOHA 技术，并提出了一种方法来确保低能源停机概率和未来任务执行成功。</li>
<li>results: 数值结果显示，该方法可以在某些关键设置下具有更高的效率，特别是在平均能源收入不足以覆盖迭代成本的情况下。该方法也在衡量时间和电池水平方面比norm based解决方案更高效。<details>
<summary>Abstract</summary>
Distributed learning on edge devices has attracted increased attention with the advent of federated learning (FL). Notably, edge devices often have limited battery and heterogeneous energy availability, while multiple rounds are required in FL for convergence, intensifying the need for energy efficiency. Energy depletion may hinder the training process and the efficient utilization of the trained model. To solve these problems, this letter considers the integration of energy harvesting (EH) devices into a FL network with multi-channel ALOHA, while proposing a method to ensure both low energy outage probability and successful execution of future tasks. Numerical results demonstrate the effectiveness of this method, particularly in critical setups where the average energy income fails to cover the iteration cost. The method outperforms a norm based solution in terms of convergence time and battery level.
</details>
<details>
<summary>摘要</summary>
随着联合学习（FL）的出现，分布式学习在边缘设备上得到了更多的关注。然而，边缘设备通常具有有限的电池和多样化的能源可用性，而多轮训练是FL的必需之物，因此提高了能效性的需求。如果能源抽取不足可能会妨碍训练过程和模型的有效使用。为解决这些问题，本文考虑了将能量收集（EH）设备integrated into FL网络中，同时提出了一种方法，以确保低能源停机概率和未来任务的成功执行。数据显示，该方法在某些关键设置下表现出色，特别是当平均能源收入不足于迭代成本时。此方法在融合率和电池水平上也表现出优势，比一种基于范数的解决方案更快 converges。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks"><a href="#Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks" class="headerlink" title="Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks"></a>Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06021">http://arxiv.org/abs/2309.06021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwa Chafii, Salmane Naoumi, Reda Alami, Ebtesam Almazrouei, Mehdi Bennis, Merouane Debbah</li>
<li>for: 本研究旨在探讨未来 sixth generation 无线网络中，通过多代理学习和自适应通信协议，实现高维数据交换和复杂任务解决方案。</li>
<li>methods: 本研究使用多代理学习和自适应通信协议（EC-MARL）解决高维数据交换和复杂任务问题，并提供了EC-MARL算法和设计要求的概述。</li>
<li>results: 本研究提供了EC-MARL在未来 sixth generation 无线网络中的应用案例和研究机遇，以及其在自动驾驶、机器人导航、飞行基站网络规划和智能城市应用中的可能性。<details>
<summary>Abstract</summary>
In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and their design criteria are provided while presenting use cases and research opportunities on this emerging topic.
</details>
<details>
<summary>摘要</summary>
不同无线网络enario中，多个网络元件需要合作以实现最小延迟和能源消耗的共同任务。未来的无线网络将要交换高维度数据在动态和不确定环境中，因此实现通信控制任务会变得极其复杂和困难。多智能抽象学习（EC-MARL）是一种可能解决高维度连续控制问题的方法，并且可以在合作方式下解决半可观测的状态下的复杂任务。本文强调EC-MARL在未来6G无线网络中的重要性，具体是将自主决策能力传递到网络元件中，以解决自动驾驶、机器人定位、飞行基站网络规划和智能城市应用等复杂任务。文章提供了EC-MARL算法的简介和设计标准，同时还提供了实际应用和研究机会的案例。
</details></li>
</ul>
<hr>
<h2 id="Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks"><a href="#Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks" class="headerlink" title="Interpolation, Approximation and Controllability of Deep Neural Networks"></a>Interpolation, Approximation and Controllability of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06015">http://arxiv.org/abs/2309.06015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen</li>
<li>for:  investigate the expressive power of deep residual neural networks as continuous dynamical systems through control theory</li>
<li>methods:  consider two properties of supervised learning, universal interpolation and universal approximation, and their relationship in the context of general control systems</li>
<li>results:  show that universal interpolation holds for essentially any architecture with non-linearity, and identify conditions for the equivalence of universal interpolation and universal approximation<details>
<summary>Abstract</summary>
We investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory. Specifically, we consider two properties that arise from supervised learning, namely universal interpolation - the ability to match arbitrary input and target training samples - and the closely related notion of universal approximation - the ability to approximate input-target functional relationships via flow maps. Under the assumption of affine invariance of the control family, we give a characterisation of universal interpolation, showing that it holds for essentially any architecture with non-linearity. Furthermore, we elucidate the relationship between universal interpolation and universal approximation in the context of general control systems, showing that the two properties cannot be deduced from each other. At the same time, we identify conditions on the control family and the target function that ensures the equivalence of the two notions.
</details>
<details>
<summary>摘要</summary>
我们调查深度复原神经网络的表达力，理解它们作为连续时间系统的控制理论中的一部分。具体而言，我们考虑两个从超级学习获得的性质：一是通用 interpolate - 能将任意输入和目标训练样本匹配 - 以及与之相关的通用替代 - 能将输入-目标函数关系通过流图汇入。我们假设控制家族的 afine invariance，我们提供了 universal interpolate 的特征化，证明这个性质适用于大多数架构，并且与 universal approximation 之间的关系。此外，我们还详细介绍了两个概念之间的关系，并识别了控制家族和目标函数的条件，以确保它们的等价。
</details></li>
</ul>
<hr>
<h2 id="ATTA-Anomaly-aware-Test-Time-Adaptation-for-Out-of-Distribution-Detection-in-Segmentation"><a href="#ATTA-Anomaly-aware-Test-Time-Adaptation-for-Out-of-Distribution-Detection-in-Segmentation" class="headerlink" title="ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation"></a>ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05994">http://arxiv.org/abs/2309.05994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gaozhitong/atta">https://github.com/gaozhitong/atta</a></li>
<li>paper_authors: Zhitong Gao, Shipeng Yan, Xuming He</li>
<li>for: 提高异常点检测模型对不同领域的抗衰减性和Semantic shift检测能力</li>
<li>methods: 提出了一种双级异常点检测框架，利用全局低级特征来判断领域转换，并使用高级特征映射来检测semantic shift</li>
<li>results: 对多个OOD segmentation benchmark进行了验证， observe了不同基线模型的性能改进， indicating the effectiveness of the proposed method in handling domain shift and semantic shift.<details>
<summary>Abstract</summary>
Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, observing consistent performance improvements across various baseline models.
</details>
<details>
<summary>摘要</summary>
近年来， dense out-of-distribution (OOD) 检测技术的主要研究方向是在具有相同领域的训练和测试数据集之间进行研究，假设测试数据集中没有领域转换。然而，在实际应用中，领域转换通常存在，并且会对现有 OOD 检测模型的准确率产生重要影响。在这种情况下，我们提议一种双级 OOD 检测框架，同时处理领域转换和语义转换。第一级通过全局低级特征来判断图像中是否存在领域转换，而第二级通过高级特征地图来特征化语义转换。这种方法可以在无法见到的领域中适应模型，同时提高模型的检测新类的能力。我们在多个 OOD 分割benchmark上验证了我们的提议方法，包括具有显著领域转换的和无领域转换的benchmark，并观察到了不同基线模型的表现中的一致性提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach"><a href="#Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach" class="headerlink" title="Learning Unbiased News Article Representations: A Knowledge-Infused Approach"></a>Learning Unbiased News Article Representations: A Knowledge-Infused Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05981">http://arxiv.org/abs/2309.05981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadia Kamal, Jimmy Hartford, Jeremy Willis, Arunkumar Bagavathi</li>
<li>for: 这篇论文的目的是研究在线新闻文章的政治倾向，以便更好地理解社会团体中的政治意识形态和控制其影响。</li>
<li>methods: 这篇论文使用了知识感知深度学习模型，利用新闻文章的全球和本地上下文来学习不受束缚的表示。</li>
<li>results: 研究人员通过将数据设置为训练和测试集中的新闻域或新闻发布者完全不同，发现提议的模型可以减少算法政治偏见并超过基线方法预测新闻文章的政治倾向，准确率高达73%。<details>
<summary>Abstract</summary>
Quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. However, predicting the accurate political leaning of a news article with machine learning models is a challenging task. This is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. There is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. In this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles using their global and local contexts. We evaluate the proposed model by setting the data in such a way that news domains or news publishers in the test set are completely unseen during the training phase. With this setup we show that the proposed model mitigates algorithmic political bias and outperforms baseline methods to predict the political leaning of news articles with up to 73% accuracy.
</details>
<details>
<summary>摘要</summary>
（以下是简化中文版本）在线新闻文章的政治倾向可以帮助我们理解社会团体中政治意识形态的变化和调控方法。然而，使用机器学习模型预测新闻文章的政治倾向是一项具有挑战性的任务。这是因为新闻文章的政治倾向由多个因素定义，而现有的学习模型具有新闻发布者的政治偏见，导致模型在训练过程中受到潜在的政治偏见影响。目前只有有限的方法可以研究新闻文章的政治倾向，这些方法也不考虑算法政治偏见，这限制了机器学习模型对新闻文章的预测精度。在这种情况下，我们提出了一种具有知识注入的深度学习模型，该模型利用可靠的外部数据资源来学习不受政治偏见影响的新闻文章表示。我们通过在训练集中将新闻域或新闻发布者完全未看到的方式设置数据来评估该模型。结果表明，我们的模型可以减少算法政治偏见并超过基线方法预测新闻文章的政治倾向精度达73%。
</details></li>
</ul>
<hr>
<h2 id="CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram"><a href="#CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram" class="headerlink" title="CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram"></a>CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05975">http://arxiv.org/abs/2309.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Kong, Wei Ping, Ambrish Dantrey, Bryan Catanzaro</li>
<li>for: 这篇论文是为了提出一种能够结合波形denoiser和spectrogram denoiser的语音干净模型，以获得最佳的两种方法的优点。</li>
<li>methods: 这篇论文使用了一种基于流行的语音生成方法的两个阶段框架，包括波形模型和spectrogram模型。具体来说，CleanUNet 2  builds upon CleanUNet，当前的波形denoiser顶峰模型，并通过使用预测的spectrograms来提高其性能。</li>
<li>results: 根据多个 объектив和主观评价标准，CleanUNet 2 比前一代方法表现更好，包括语音干净度、语音质量和用户满意度等。<details>
<summary>Abstract</summary>
In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍CleanUNet 2，一种混合波形频谱滤波器和频谱滤波器的语音干净模型，实现了两者的优点。CleanUNet 2使用两个阶段框架， Drawing inspiration from popular speech synthesis methods, it consists of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further enhances its performance by taking predicted spectrograms from a spectrogram denoiser as input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.Here's the translation in Traditional Chinese:在这个工作中，我们介绍CleanUNet 2，一种结合波形频谱滤波器和频谱滤波器的语音干净模型，实现了两者的优点。CleanUNet 2使用两个阶段架构， drawing inspiration from popular speech synthesis methods， it consists of a waveform model and a spectrogram model。Specifically, CleanUNet 2 builds upon CleanUNet， the state-of-the-art waveform denoiser， and further enhances its performance by taking predicted spectrograms from a spectrogram denoiser as input。We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations。
</details></li>
</ul>
<hr>
<h2 id="Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation"><a href="#Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation" class="headerlink" title="Circuit Breaking: Removing Model Behaviors with Targeted Ablation"></a>Circuit Breaking: Removing Model Behaviors with Targeted Ablation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05973">http://arxiv.org/abs/2309.05973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xanderdavies/circuit-breaking">https://github.com/xanderdavies/circuit-breaking</a></li>
<li>paper_authors: Maximilian Li, Xander Davies, Max Nadeau</li>
<li>for: 降低 GPT-2 语言生成的恶性行为</li>
<li>methods: 简单地禁用模型组件之间的一些 causal 路径</li>
<li>results: 约 12 个 causal 边的禁用可以减轻恶性语言生成，而不会影响其他输入的性能Here’s the English version for reference:</li>
<li>for: Reducing GPT-2 toxic language generation</li>
<li>methods: Simply disabling a small number of causal paths between model components</li>
<li>results: Ablating approximately 12 causal edges can mitigate toxic generation with minimal degradation of performance on other inputs.<details>
<summary>Abstract</summary>
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
</details>
<details>
<summary>摘要</summary>
语模型经常表现出改善预训目标性能的行为，但对下游任务造成害。我们提出了一种新的方法，通过缩减一小部分 causal pathway 来除掉不良行为。 Given 一小量的输入，我们学习缩减一小部分重要的 causal pathway。在 GPT-2 毒性语言生成中，我们发现，缩减 12 条 causal edge 可以减少毒性语言生成，而不会对其他输入造成严重的影响。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity"><a href="#Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity" class="headerlink" title="Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity"></a>Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05968">http://arxiv.org/abs/2309.05968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ng Shyh-Chang, A-Li Luo, Bo Qiu</li>
<li>for: 这paper主要是为了证明神经网络（NN）编码定理的逆命题，即每个稳定收敛的NN的权重矩阵实际上编码了一个连续函数，该函数在训练集上准确地 aproximate 到一个有限的误差范围内。</li>
<li>methods: 该paper使用了Eckart-Young theorem来解释 truncated singular value decomposition（SVD）of NN层的weight矩阵，以及Latent Space Manifold（LSM）的概念来解释NN层中数据的编码和表示。</li>
<li>results: 研究发现，NN可以破坏维度约束，并通过吸收内存来提高表达性。此外，NN层的eigen-decomposition和Hopfield网络以及Transformer NN模型之间存在紧密的关系。<details>
<summary>Abstract</summary>
We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advances in conceptualizations of Hopfield networks and Transformer NN models.
</details>
<details>
<summary>摘要</summary>
我们证明了对应 theorem的逆命运算，即一个神经网络（NN）编码定理，显示任何稳定地训练完成的 NN 的weight矩阵实际上将一个连续函数所代表，这个函数可以在受训数据的范围内对应到该数据集，并且在这个范围内有一定的错误 bound。我们还证明了使用 Eckart-Young 定理对 NN 层的剪枝值分解，可以显示每个 NN 层的矩阵 matrix 中的latent space manifold，并且这些数据表示了 NN 层中的数学操作的几何性。我们的结果显示了如何 NN 突破维度紧缩的问题，通过将记忆容量作为表达能力的一部分，并且这两者是补充的。此外，我们的层矩阵分解（LMD）还 suggets了神经网络层的对� Hopfield 网络和 transformer 神经网络的最新发展有着密切的关系。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms"><a href="#Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms" class="headerlink" title="Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms"></a>Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05961">http://arxiv.org/abs/2309.05961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rima Hazra, Agnik Saha, Somnath Banerjee, Animesh Mukherjee</li>
<li>for: This paper aims to investigate the factors that contribute to the speed of responses on Community Question Answering (CQA) platforms.</li>
<li>methods: The authors examine six highly popular CQA platforms and analyze various metadata and patterns of user interaction to predict which queries will receive prompt responses.</li>
<li>results: The study finds a correlation between the time taken to yield the first response to a question and several variables, including the metadata, the formulation of the questions, and the level of interaction among users.<details>
<summary>Abstract</summary>
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection"><a href="#GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection" class="headerlink" title="GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection"></a>GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05953">http://arxiv.org/abs/2309.05953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yul091/GraphLogAD">https://github.com/yul091/GraphLogAD</a></li>
<li>paper_authors: Yufei Li, Yanchi Liu, Haoyu Wang, Zhengzhang Chen, Wei Cheng, Yuncong Chen, Wenchao Yu, Haifeng Chen, Cong Liu</li>
<li>for: 本文旨在检测系统日志中的异常现象，特别是关系系统组件和用户的异常现象。</li>
<li>methods: 本文提出了一种基于图的日志异常检测框架（GLAD），它利用日志内容中的关系、预测模型和序列模式来检测异常。</li>
<li>results: 本文对三个数据集进行了实验，结果表明GLAD可以准确地检测异常，并且可以捕捉到异常的关系特征。<details>
<summary>Abstract</summary>
Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events parsed from the log parser. These graphs represent events and fields as nodes and their relations as edges. Subsequently, GLAD utilizes a temporal-attentive graph edge anomaly detection model for identifying anomalous relations in these dynamic log graphs. This model employs a Graph Neural Network (GNN)-based encoder enhanced with transformers to capture content, structural and temporal features. We evaluate our proposed method on three datasets, and the results demonstrate the effectiveness of GLAD in detecting anomalies indicated by varying relational patterns.
</details>
<details>
<summary>摘要</summary>
日志扮演着重要的监控和调试系统的角色，记录了系统中重要的信息，包括事件和状态。虽然有很多方法用于检测日志序列中的异常，但它们通常忽视了系统组件之间的关系，例如服务和用户，这些关系可以从日志内容中提取出来。理解这些关系是检测异常和其下面的原因的关键。为解决这个问题，我们提出了GLAD，一个基于图的日志异常检测框架，用于检测系统日志中的关系异常。GLAD结合了日志 semantics、关系模式和时序模式，实现了一个综合的异常检测框架。具体来说，GLAD首先引入一个场景EXTRACT模块，使用推荐的几个shot学习来确定日志中的重要场景。然后，GLAD构建了动态日志图，用于滤波窗口内的日志事件和场景。这些图表示事件和场景为节点，以及它们之间的关系为边。接着，GLAD使用一个时间注意力图边异常检测模型，用于检测动态日志图中的异常关系。这个模型使用图神经网络（GNN）基础加强 transformer，以捕捉内容、结构和时序特征。我们对GLAD进行了三个数据集的评估，结果表明GLAD可以有效地检测异常，异常的关系异常。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models"><a href="#Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models" class="headerlink" title="Language Models as Black-Box Optimizers for Vision-Language Models"></a>Language Models as Black-Box Optimizers for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05950">http://arxiv.org/abs/2309.05950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shihongl1998/llm-as-a-blackbox-optimizer">https://github.com/shihongl1998/llm-as-a-blackbox-optimizer</a></li>
<li>paper_authors: Samuel Yu, Shihong Liu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan</li>
<li>for: 这paper的目的是提出一种基于自然语言提示的训练方法，以适应现有的vision-language模型（VLM）不可见参数的情况。</li>
<li>methods: 这paper使用了chat-based大型自然语言模型（LLM）作为黑盒优化器，通过自动“山丘攀登”过程，找到最佳的文本提示，以提高图像分类 tasks的性能。</li>
<li>results: 这paper在1shot学习setup下，使用自动提取的文本提示，超过了白盒连续提示方法CoOp的平均提升率1.5%，并且超过了OpenAI手动制作的提示和其他黑盒方法。此外， paper还发现了文本反馈中的负反馈可以帮助LLM更快地找到最佳提示。最后，paper发现通过自己的策略生成的文本提示不仅更易于理解，还可以适应不同的CLIP架构。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prompt by evaluating the accuracy of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot learning setup, our simple approach surpasses the white-box continuous prompting method CoOp by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms OpenAI's manually crafted prompts and is more efficient than other black-box methods like iterative APE. Additionally, we highlight the advantage of conversational feedback incorporating both positive and negative prompts, suggesting that LLMs can utilize the implicit "gradient" direction in textual feedback for a more efficient search. Lastly, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different CLIP architectures in a black-box manner.
</details>
<details>
<summary>摘要</summary>
现代Computer Vision Models (VLMs) 在大规模网络数据上进行预训后，在多种 Computer Vision 和多modal任务中显示了杰出的能力。目前，对 VLMs 的调整主要是在白盒子设定下进行，需要 Parameters 的存取。但是，许多 VLMs 靠赖 Proprietary 数据，因此不能使用白盒子方法进行调整。尽管受欢迎的Private Large Language Models (LLMs) 如 ChatGPT 仍然提供语言基于的使用者界面，我们想要发展一种以自然语言提示为基础的调整方法 для VLMs，因此不需要存取模型 Parameters、特征嵌入或输出核心。在这个设置下，我们提议使用 Chat-based LLMs 作为黑盒子优化器，通过询问自然语言提示来找到最佳提示，扩展了 CoOp 的白盒子连续提示方法。在具有挑战性的一击学习设置下，我们的简单方法比 CoOp 的白盒子连续提示方法高一个均值1.5%，涵盖11个数据集，包括 ImageNet。我们的方法还超越了 OpenAI 手动构成的提示，并且比其他黑盒方法，如迭代 APE，更高效。此外，我们发现，通过我们的策略，LLMs 可以利用文本反馈中的隐式 "梯度" 方向来更有效率地寻找。最后，我们发现，通过我们的策略生成的文本提示不仅更易于理解，而且可以跨不同 CLIP 架构在黑盒子方式进行转移。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals"><a href="#Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals" class="headerlink" title="Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals"></a>Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05927">http://arxiv.org/abs/2309.05927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Liu, Ellen L. Zippi, Hadi Pouransari, Chris Sandino, Jingping Nie, Hanlin Goh, Erdrin Azemi, Ali Moin</li>
<li>for: 本研究旨在提出一种适应多模态信号的预训练方法，以优化人体physical和mental状态的 representations。</li>
<li>methods: 该方法基于频率意识的掩码自动encoder（$\texttt{bio}$FAME），利用频率空间来参数化生物信号的表示。具有频率相关的变换器，可以在不同的输入大小和采样率下进行全球化混合。</li>
<li>results: 在多种传输试验中，该方法可以提高平均5.5%的分类精度，并在模式匹配情况下表现稳定。代码即将上线。<details>
<summary>Abstract</summary>
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. We evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of $\uparrow$5.5% improvement in classification accuracy over the previous state-of-the-art. Furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. Code will be available soon.
</details>
<details>
<summary>摘要</summary>
使用多modal信息自生物信号是关键以建立完整的人们的物理和心理状态表示。然而，多modal生物信号经常会在预训练和测试数据集之间存在显著的分布性变化，这可能是由任务规定的变化或多modal组合的变化引起的。为实现有效的预训练在存在潜在的分布性变化的情况下，我们提议一种频率意识的掩码自动编码器（$\texttt{bio}$FAME），该模型学习在频率空间中表示生物信号的参数。$\texttt{bio}$FAME使用了一个频率意识变换器，该变换器利用固定大小的福柯尔基于的运算符进行全token混合，不виси于输入的长度和抽象率。为保持每个输入通道中的频率组件，我们再次使用一种保持频率措施，该措施在幂 espacio中进行掩码自动编码。这种架构可以有效利用多modal信息进行预训练，并可以在测试时适应多种任务和多modal，无论输入的大小和顺序。我们在一个多样化的转移试验中，average的提高了5.5%的分类精度，相比前一个状态的艺术。此外，我们还证明了我们的架构在多modal异常场景中是有实用性的，包括预期的模式掉载或替换，这证明了它在实际应用中的实用性。代码即将上传。
</details></li>
</ul>
<hr>
<h2 id="On-Regularized-Sparse-Logistic-Regression"><a href="#On-Regularized-Sparse-Logistic-Regression" class="headerlink" title="On Regularized Sparse Logistic Regression"></a>On Regularized Sparse Logistic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05925">http://arxiv.org/abs/2309.05925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset">https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset</a></li>
<li>paper_authors: Mengyuan Zhang, Kai Liu</li>
<li>for: 本研究旨在同时进行分类和特征选择，以适应高维数据的分类问题。</li>
<li>methods: 本文提出了解决 $\ell_1$-regulized sparse logistic regression 和一些非 convex 违约项 regularized sparse logistic regression 问题的优化框架。</li>
<li>results: Empirical experiments on binary classification tasks with real-world datasets demonstrate that our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.<details>
<summary>Abstract</summary>
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
</details>
<details>
<summary>摘要</summary>
sparse 预测分类和特征选择同时进行高维数据处理。虽然许多研究已经解决了 $\ell_1$-regularized 预测分类，但对于非 convex 罚 penalty 的预测分类并没有相当的文献。在本文中，我们提议解决 $\ell_1$-regularized sparse 预测分类和一些非 convex 罚 penalty 的 sparse 预测分类，当非 convex 罚 penalty 满足某些前提条件时，使用同样的优化框架。在我们的优化框架中，我们使用不同的搜索 критери来保证不同的规则项的优化性。实际应用在实际数据集上的二分类任务中，我们的提议算法能够有效地进行分类和特征选择，并且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection"><a href="#Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection" class="headerlink" title="Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection"></a>Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06449">http://arxiv.org/abs/2309.06449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sabbir Alam, Walid Al Misba, Jayasimha Atulasimha</li>
<li>for: 本研究旨在提高边缘设备中卷积神经网络模型的异常检测性能，并且解决实时学习和硬件限制问题。</li>
<li>methods: 该研究提出了一种基于自适应神经网络的磁道束Synapse设计，并使用了有效的量化神经网络学习算法。</li>
<li>results: 研究结果显示，使用限定状态（5状） synaptic weights和磁力矩（SOT）电流脉冲来控制磁道束Synapse可以提高异常检测性能，并且比浮点数权重学习更高。此外，该方法还能够大幅降低训练过程中的Weight更新次数，从而降低能耗。<details>
<summary>Abstract</summary>
In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of quantized states and the inherent stochastic nature of DW synaptic weights in nanoscale devices are known to negatively impact the performance, our hardware-aware training algorithm is shown to leverage these imperfect device characteristics to generate an improvement in anomaly detection accuracy (90.98%) compared to accuracy obtained with floating-point trained weights. Furthermore, our DW-based approach demonstrates a remarkable reduction of at least three orders of magnitude in weight updates during training compared to the floating-point approach, implying substantial energy savings for our method. This work could stimulate the development of extremely energy efficient non-volatile multi-state synapse-based processors that can perform real-time training and inference on the edge with unsupervised data.
</details>
<details>
<summary>摘要</summary>
在基于自适应器的异常检测 paradigm中，在边缘设备中实现自适应器是极其困难的，因为边缘设备的硬件、能源和计算资源都受限。我们表明，这些限制可以通过设计一个具有低分辨率非朋义存储器基于 synapse 的 autoencoder，并使用有效的量化神经网络学习算法来解决。我们提议一种 ferromagnetic racetrack 上的引入不ches 承载一个 магнит的Domain Wall (DW) 作为 autoencoder synapse，其中有限状态（5-state） synaptic weights 通过 spin orbit torque (SOT) 电流脉冲来 manipulate。我们对 propose 的 autoencoder 模型进行了 NSL-KDD 数据集上的异常检测性能评估。我们采用了限定分辨率和 DW 设备不确定性意识的 trains 算法，实现了与浮点数精度 weights 相对的异常检测性能（90.98%）。尽管限定数量的量化状态和 nanoscale 设备的自然随机性会对性能产生负面影响，但我们的硬件意识 Train 算法可以利用这些不完美的设备特点来生成一个异常检测精度的提高（90.98%）。此外，我们的 DW 方法在训练中的weight 更新数量减少至少三个数量级，implying substantial energy savings for our method。这种工作可能会驱动非朋义多状态 synapse 基于处理器的开发，以实现实时在边缘进行训练和推断，并使用不supervised 数据。
</details></li>
</ul>
<hr>
<h2 id="ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning"><a href="#ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning" class="headerlink" title="ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning"></a>ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05915">http://arxiv.org/abs/2309.05915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang, Yang Yu</li>
<li>for: 提高offline政策优化中DT的表现，抗衡环境随机性的问题。</li>
<li>methods: 使用动态计划方法，包括样本值迭代、估计优势和ACT等。</li>
<li>results: 通过使用动态计划方法，ACT可以在不同的benchmark上表现出色，提高路径组合和动作生成的稳定性，并且通过对不同设计选择的ablation study进行深入分析。<details>
<summary>Abstract</summary>
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation results validate that, by leveraging the power of dynamic programming, ACT demonstrates effective trajectory stitching and robust action generation in spite of the environmental stochasticity, outperforming baseline methods across various benchmarks. Additionally, we conduct an in-depth analysis of ACT's various design choices through ablation studies.
</details>
<details>
<summary>摘要</summary>
第一步，我们使用内样值迭代来获取 Approximated 值函数，这包括在 Markov Decision Process 结构上进行动态计划。第二步，我们评估行动质量在上下文中的优势，我们引入了两种优势估计器，IAE 和 GAE，这些优势估计器适合不同的任务。第三步，我们训练一个优势决策 Transformer (ACT)，以生成基于优势估计的行动。在测试中，ACT 生成基于想要的优势的行动。我们的评估结果表明，通过利用动态计划的力量，ACT 能够在环境随机性的情况下生成有效的轨迹和稳定的行动，超越基eline 方法，在多个 benchmark 上达到了优秀的表现。此外，我们通过减少研究不同设计选择的影响的 ablation study 进行了深入的分析。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-Assessment-of-Salient-Object-Detection-via-Symbolic-Learning"><a href="#Adversarial-Attacks-Assessment-of-Salient-Object-Detection-via-Symbolic-Learning" class="headerlink" title="Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning"></a>Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05900">http://arxiv.org/abs/2309.05900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gustavo Olague, Roberto Pineda, Gerardo Ibarra-Vazquez, Matthieu Olague, Axel Martinez, Sambit Bakshi, Jonathan Vargas, Isnardo Reducindo</li>
<li>for: 这种研究旨在提高深度学习视觉注意系统的可靠性和安全性，以满足野生动物保护和生物多样性保护的需求。</li>
<li>methods: 这种研究使用了生物学的进化计算方法，以检验深度学习视觉注意系统的可靠性和安全性。</li>
<li>results: 研究结果表明，深度学习视觉注意系统在遭受恶意攻击和干扰时，表现出了明显的性能下降，而符号学习方法则能够坚持不变。此外，研究还发现，深度学习视觉注意系统在面对野生动物的攻击时，也会表现出性能下降。<details>
<summary>Abstract</summary>
Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers' attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodology against several adversarial attacks and noise perturbations using standard databases and a real-world problem of a shorebird called the Snowy Plover portraying a visual attention task. We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness. All neural networks suffer significant performance losses, while brain programming stands its ground and remains unaffected. Also, by studying the Snowy Plover, we remark on the importance of security in surveillance activities regarding wildlife protection and conservation.
</details>
<details>
<summary>摘要</summary>
机器学习在主流技术中占中心位置，并且超越了经典的手工特征设计方法。除了学习过程中的人工特征抽取外，它还有一个端到端的 paradigm从输入到输出，达到了极高的准确性。然而，由于其鲁棒性对恶意和隐触的扰动的问题引起了关注，其预测结果可以被完全改变。静观检测是一个研究领域，深度卷积神经网络在这里表现出了效果，但其可靠性成为了一个主要的问题，需要分析和解决。布朗编程是一种符号学习，与传统的人工智能有着共同点。本研究提供了证据，表明符号学习的鲁棒性在设计可靠视觉注意力系统方面是关键，它可以抵御even the most intense perturbations。我们在多个对抗攻击和噪声扰动使用标准数据库和实际问题中测试了我们的方法ологи，并与五种不同的深度学习方法进行比较，发现它们不能与符号学习模型相比。所有的神经网络都uffered significant performance losses，而布朗编程则保持不变。此外，我们通过研究雪亮燕鸥（Snowy Plover）来评论野生生物保护和保育的安全问题。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Conditional-Semi-Paired-Image-to-Image-Translation-For-Multi-Task-Image-Defect-Correction-On-Shopping-Websites"><a href="#Hierarchical-Conditional-Semi-Paired-Image-to-Image-Translation-For-Multi-Task-Image-Defect-Correction-On-Shopping-Websites" class="headerlink" title="Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites"></a>Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05883">http://arxiv.org/abs/2309.05883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyan Li, Jinmiao Fu, Shaoyuan Xu, Huidong Liu, Jia Liu, Bryan Wang</li>
<li>for:  correction of multiple defects in product images on shopping websites</li>
<li>methods:  unified Image-to-Image (I2I) translation model with attention mechanism and semi-paired training</li>
<li>results:  reduced Frechet Inception Distance (FID) by 24.6% compared to state-of-the-art I2I method, and reduced FID by 63.2% compared to state-of-the-art semi-paired I2I method<details>
<summary>Abstract</summary>
On shopping websites, product images of low quality negatively affect customer experience. Although there are plenty of work in detecting images with different defects, few efforts have been dedicated to correct those defects at scale. A major challenge is that there are thousands of product types and each has specific defects, therefore building defect specific models is unscalable. In this paper, we propose a unified Image-to-Image (I2I) translation model to correct multiple defects across different product types. Our model leverages an attention mechanism to hierarchically incorporate high-level defect groups and specific defect types to guide the network to focus on defect-related image regions. Evaluated on eight public datasets, our model reduces the Frechet Inception Distance (FID) by 24.6% in average compared with MoNCE, the state-of-the-art I2I method. Unlike public data, another practical challenge on shopping websites is that some paired images are of low quality. Therefore we design our model to be semi-paired by combining the L1 loss of paired data with the cycle loss of unpaired data. Tested on a shopping website dataset to correct three image defects, our model reduces (FID) by 63.2% in average compared with WS-I2I, the state-of-the art semi-paired I2I method.
</details>
<details>
<summary>摘要</summary>
在购物网站上，产品图像质量低下对客户体验有负面影响。尽管有很多研究检测不同缺陷的图像，但对于大规模纠正缺陷却受到了少数努力。主要挑战在于有 thousands 种产品类型，每种有特定的缺陷，因此建立缺陷特定的模型是不可能的。在这篇论文中，我们提出了一种统一的图像到图像（I2I）纠正模型，可以同时纠正不同产品类型的多种缺陷。我们的模型利用了注意机制，将高级缺陷组和特定缺陷类型层次地包含到网络中，使网络能够更好地关注缺陷相关的图像区域。在八个公共数据集上进行评估，我们的模型可以将 Frechet Inception Distance（FID）降低至 24.6% 以上，比 MoNCE，当前的 I2I 方法，更高效。不同于公共数据，在购物网站上，一些对应图像质量低下是另一个实际挑战。因此，我们设计了我们的模型为半对应的，将 L1 损失与对应数据的径规整共同使用，以便更好地纠正图像缺陷。在一个购物网站数据集上测试，我们的模型可以将 FID 降低至 63.2% 以上，比 WS-I2I，当前的半对应 I2I 方法，更高效。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Attacks-on-Face-Verification-Systems"><a href="#Generalized-Attacks-on-Face-Verification-Systems" class="headerlink" title="Generalized Attacks on Face Verification Systems"></a>Generalized Attacks on Face Verification Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05879">http://arxiv.org/abs/2309.05879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Nazari, Paula Branco, Guy-Vincent Jourdan</li>
<li>for: 本研究旨在探讨面部验证系统受到攻击的问题，特别是针对深度神经网络模型。</li>
<li>methods: 本研究使用了深度神经网络模型进行面部验证，并提出了一种新的攻击方法——逃脱人脸攻击（DodgePersonation Attack）。</li>
<li>results: 本研究表明，逃脱人脸攻击可以创造出面部图像，用于骚扰面部验证系统，并且可以在不同的场景下实现更高的攻击效果。此外，研究还提出了一种新的攻击方法——“一个人脸掌控全部”攻击（One Face to Rule Them All Attack）。<details>
<summary>Abstract</summary>
Face verification (FV) using deep neural network models has made tremendous progress in recent years, surpassing human accuracy and seeing deployment in various applications such as border control and smartphone unlocking. However, FV systems are vulnerable to Adversarial Attacks, which manipulate input images to deceive these systems in ways usually unnoticeable to humans. This paper provides an in-depth study of attacks on FV systems. We introduce the DodgePersonation Attack that formulates the creation of face images that impersonate a set of given identities while avoiding being identified as any of the identities in a separate, disjoint set. A taxonomy is proposed to provide a unified view of different types of Adversarial Attacks against FV systems, including Dodging Attacks, Impersonation Attacks, and Master Face Attacks. Finally, we propose the ''One Face to Rule Them All'' Attack which implements the DodgePersonation Attack with state-of-the-art performance on a well-known scenario (Master Face Attack) and which can also be used for the new scenarios introduced in this paper. While the state-of-the-art Master Face Attack can produce a set of 9 images to cover 43.82% of the identities in their test database, with 9 images our attack can cover 57.27% to 58.5% of these identifies while giving the attacker the choice of the identity to use to create the impersonation. Moreover, the 9 generated attack images appear identical to a casual observer.
</details>
<details>
<summary>摘要</summary>
face 验证（FV）使用深度神经网络模型在最近几年内得到了很大的进步，超过了人类准确率并在不同的应用中使用，如边境控制和智能手机锁定。然而，FV系统容易受到敌意攻击，这些攻击可以通过修改输入图像来诱导FV系统进行误判。本文提供了对FV系统攻击的深入研究。我们介绍了《 dodgingPersonation 攻击》，该攻击可以生成一组面图像，以便在不同的身份集中隐藏身份。我们还提出了对FV系统攻击的分类，包括拦截攻击、冒充攻击和主面攻击。最后，我们提出了《一面控制所有》攻击，这是一种实现《 dodgingPersonation 攻击》的新方法，可以在已知的攻击方法（主面攻击）中实现更高的性能。这种攻击可以在9个图像中覆盖57.27%至58.5%的身份，而且这些图像看起来都是完全一样的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.LG_2023_09_12/" data-id="clmjn91n1008f0j88bqe7ctic" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/eess.IV_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T09:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/eess.IV_2023_09_12/">eess.IV - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Post-processing-of-Diffusion-Tensor-Cardiac-Magnetic-Imaging-Using-Texture-conserving-Deformable-Registration"><a href="#Efficient-Post-processing-of-Diffusion-Tensor-Cardiac-Magnetic-Imaging-Using-Texture-conserving-Deformable-Registration" class="headerlink" title="Efficient Post-processing of Diffusion Tensor Cardiac Magnetic Imaging Using Texture-conserving Deformable Registration"></a>Efficient Post-processing of Diffusion Tensor Cardiac Magnetic Imaging Using Texture-conserving Deformable Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06598">http://arxiv.org/abs/2309.06598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanwen Wang, Pedro F. Ferreira, Yinzhe Wu, Andrew D. Scott, Camila Munoz, Ke Wen, Yaqing Luo, Jiahao Huang, Sonia Nielles-Vallespin, Dudley J. Pennell, Guang Yang</li>
<li>For: 提供非侵入式myocardial微结构测量方法* Methods: 使用深度学习基于B-spline网络进行抽象 registratin，利用低级特征减少杂变信息和扩散编码器-解码器来抑制扩散编码器所带来的噪声* Results: 提高了帧使用效率、手动剪辑和计算速度<details>
<summary>Abstract</summary>
Diffusion tensor based cardiac magnetic resonance (DT-CMR) is a method capable of providing non-invasive measurements of myocardial microstructure. Image registration is essential to correct image shifts due to intra and inter breath-hold motion. Registration is challenging in DT-CMR due to the low signal-to-noise and various contrasts induced by the diffusion encoding in the myocardial and surrounding organs. Traditional deformable registration destroys the texture information while rigid registration inefficiently discards frames with local deformation. In this study, we explored the possibility of deep learning-based deformable registration on DT- CMR. Based on the noise suppression using low-rank features and diffusion encoding suppression using variational auto encoder-decoder, a B-spline based registration network extracted the displacement fields and maintained the texture features of DT-CMR. In this way, our method improved the efficiency of frame utilization, manual cropping, and computational speed.
</details>
<details>
<summary>摘要</summary>
Diffusion tensor based cardiac magnetic resonance (DT-CMR) 是一种可提供非侵入性测量心肌微结构的方法。图像 регистрация是DT-CMR中必须的，因为图像偏移导致了呼吸和心跳停顿中的移动。但是，DT-CMR中的图像registratio是困难的，因为低信号噪声和diffusion编码器在心肌和周围器官中引起的多种对比。传统的可变形 регистраción会将文件中的纹理信息遗弃，而固定 registratio则不具有地方性的损失。在这项研究中，我们探索了基于深度学习的可变形 регистраción的可能性。通过噪声抑制使用低级别特征和diffusion编码器抑制使用自适应神经网络，我们提出了一种使用B-spline基本注意力网络提取displacement场和保留DT-CMR中的纹理特征。这种方法可以提高帧使用效率、手动剪辑和计算速度。
</details></li>
</ul>
<hr>
<h2 id="AGMDT-Virtual-Staining-of-Renal-Histology-Images-with-Adjacency-Guided-Multi-Domain-Transfer"><a href="#AGMDT-Virtual-Staining-of-Renal-Histology-Images-with-Adjacency-Guided-Multi-Domain-Transfer" class="headerlink" title="AGMDT: Virtual Staining of Renal Histology Images with Adjacency-Guided Multi-Domain Transfer"></a>AGMDT: Virtual Staining of Renal Histology Images with Adjacency-Guided Multi-Domain Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06421">http://arxiv.org/abs/2309.06421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Ma, Chao Zhang, Min Lu, Lin Luo</li>
<li>for:  This paper aims to propose a novel virtual staining framework to translate images into other domains, avoiding pixel-level alignment and utilizing correlations among adjacent tissue slices.</li>
<li>methods: The proposed framework AGMDT uses a high-quality multi-domain renal histological dataset, glomerulus detection, and bipartite graph matching to discover patch-level aligned pairs across serial slices of multi-domains, and utilizes such correlations to supervise the end-to-end model for multi-domain staining transformation.</li>
<li>results: The experimental results show that the proposed AGMDT achieves a good balance between precise pixel-level alignment and unpaired domain transfer by exploiting correlations across multi-domain serial pathological slices, and outperforms the state-of-the-art methods in both quantitative measure and morphological details.<details>
<summary>Abstract</summary>
Renal pathology, as the gold standard of kidney disease diagnosis, requires doctors to analyze a serial of tissue slices stained by H\&E staining and special staining like Masson, PASM, and PAS, respectively. These special staining methods are costly, time-consuming, and hard to standardize for wide use especially in primary hospitals. Advances of supervised learning methods can virtually convert H\&E images into special staining images, but the pixel-to-pixel alignment is hard to achieve for training. As contrast, unsupervised learning methods regarding different stains as different style transferring domains can use unpaired data, but they ignore the spatial inter-domain correlations and thus decrease the trustworthiness of structural details for diagnosis. In this paper, we propose a novel virtual staining framework AGMDT to translate images into other domains by avoiding pixel-level alignment and meanwhile utilizing the correlations among adjacent tissue slices. We first build a high-quality multi-domain renal histological dataset where each specimen case comprises a series of slices stained in various ways. Based on it, the proposed framework AGMDT discovers patch-level aligned pairs across the serial slices of multi-domains through glomerulus detection and bipartite graph matching, and utilizes such correlations to supervise the end-to-end model for multi-domain staining transformation. Experimental results show that the proposed AGMDT achieves a good balance between the precise pixel-level alignment and unpaired domain transfer by exploiting correlations across multi-domain serial pathological slices, and outperforms the state-of-the-art methods in both quantitative measure and morphological details.
</details>
<details>
<summary>摘要</summary>
肾脏病学，作为肾脏疾病诊断的标准方法，需要医生分析一系列染色的组织切片，包括H\&E染色和特殊染色如Masson、PASM和PAS等。这些特殊染色方法是成本高、时间耗费和标准化困难，特别是在初级医院中使用。随着超vised learning方法的进步，可以虚拟转换H\&E图像为特殊染色图像，但是像素级匹配困难以实现训练。相比之下，无监督学习方法，将不同染色视为不同的样式传输领域，可以使用无配对数据，但是忽略了组织间的空间相互关系，因此降低了诊断中结构细节的可靠性。在这篇论文中，我们提出了一种新的虚拟染色框架AGMDT，可以将图像转换到其他领域，而不需要像素级匹配。我们首先构建了高质量多频道肾脏 histological 数据集，每个案例包含一系列各种染色的组织切片。基于这个数据集，我们的框架AGMDT 在多频道串行切片中找到对应的patch级匹配，并利用这些相关性来监督终端模型进行多频道染色转换。实验结果表明，我们的AGMDT 能够在保持像素级匹配的同时，通过利用多频道串行切片之间的相关性，superior 于当前状态艺术方法，并且在量度 mesure 和结构细节方面都达到了更好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-Capability-of-Deep-Learning-Based-Nuclei-Instance-Segmentation-by-Non-deterministic-Train-Time-and-Deterministic-Test-Time-Stain-Normalization"><a href="#Improving-Generalization-Capability-of-Deep-Learning-Based-Nuclei-Instance-Segmentation-by-Non-deterministic-Train-Time-and-Deterministic-Test-Time-Stain-Normalization" class="headerlink" title="Improving Generalization Capability of Deep Learning-Based Nuclei Instance Segmentation by Non-deterministic Train Time and Deterministic Test Time Stain Normalization"></a>Improving Generalization Capability of Deep Learning-Based Nuclei Instance Segmentation by Non-deterministic Train Time and Deterministic Test Time Stain Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06143">http://arxiv.org/abs/2309.06143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Mahbod, Georg Dorffner, Isabella Ellinger, Ramona Woitek, Sepideh Hatamikia</li>
<li>for:  This paper aims to improve the generalization capability of a deep learning-based automatic segmentation approach for nuclei instance segmentation in digital pathology.</li>
<li>methods:  The proposed method incorporates non-deterministic train time and deterministic test time stain normalization, and uses one single training set to evaluate the segmentation performance on seven test datasets.</li>
<li>results:  The proposed method provides up to 5.77%, 5.36%, and 5.27% better performance in segmenting nuclei based on Dice score, aggregated Jaccard index, and panoptic quality score, respectively, compared to the baseline segmentation model.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提高深度学习基于自动分割模型的总化能力，用于核实体分割在数字病理学中。</li>
<li>methods: 该方法使用非束定训练时间和固定测试时间杯Normalization，使用单一训练集来评估分割性能在七个测试集上。</li>
<li>results: 该方法提供了基于Dice分数、积合Jacard指数和总质量分数的5.77%、5.36%和5.27%的提升，相比基准分割模型。<details>
<summary>Abstract</summary>
With the advent of digital pathology and microscopic systems that can scan and save whole slide histological images automatically, there is a growing trend to use computerized methods to analyze acquired images. Among different histopathological image analysis tasks, nuclei instance segmentation plays a fundamental role in a wide range of clinical and research applications. While many semi- and fully-automatic computerized methods have been proposed for nuclei instance segmentation, deep learning (DL)-based approaches have been shown to deliver the best performances. However, the performance of such approaches usually degrades when tested on unseen datasets.   In this work, we propose a novel approach to improve the generalization capability of a DL-based automatic segmentation approach. Besides utilizing one of the state-of-the-art DL-based models as a baseline, our method incorporates non-deterministic train time and deterministic test time stain normalization. We trained the model with one single training set and evaluated its segmentation performance on seven test datasets. Our results show that the proposed method provides up to 5.77%, 5.36%, and 5.27% better performance in segmenting nuclei based on Dice score, aggregated Jaccard index, and panoptic quality score, respectively, compared to the baseline segmentation model.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel approach to improve the generalization capability of a DL-based automatic segmentation approach. Our method incorporates non-deterministic train time and deterministic test time stain normalization. We trained the model with one single training set and evaluated its segmentation performance on seven test datasets. Our results show that the proposed method provides up to 5.77%, 5.36%, and 5.27% better performance in segmenting nuclei based on Dice score, aggregated Jaccard index, and panoptic quality score, respectively, compared to the baseline segmentation model.
</details></li>
</ul>
<hr>
<h2 id="Batch-Implicit-Neural-Representation-for-MRI-Parallel-Reconstruction"><a href="#Batch-Implicit-Neural-Representation-for-MRI-Parallel-Reconstruction" class="headerlink" title="Batch Implicit Neural Representation for MRI Parallel Reconstruction"></a>Batch Implicit Neural Representation for MRI Parallel Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06067">http://arxiv.org/abs/2309.06067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Yusheng Zhou, Jianan Liu, Xiling Liu, Tao Huang, Zhihan Lv</li>
<li>for: 提高MRI扫描时间的问题</li>
<li>methods: 使用深度学习方法来重建MRI图像</li>
<li>results: 提出一种基于INR的新型MRI重建方法，可以在不同的抽样率下重建高质量图像，并且比其他重建方法有更好的性能<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) always suffered from the problem of long acquisition time. MRI reconstruction is one solution to reduce scan time by skipping certain phase-encoding lines and then restoring high-quality images from undersampled measurements. Recently, implicit neural representation (INR) has emerged as a new deep learning method that represents an object as a continuous function of spatial coordinates, and this function is normally parameterized by a multilayer perceptron (MLP). In this paper, we propose a novel MRI reconstruction method based on INR, which represents the fully-sampled images as the function of pixel coordinates and prior feature vectors of undersampled images for overcoming the generalization problem of INR. Specifically, we introduce a scale-embedded encoder to produce scale-independent pixel-specific features from MR images with different undersampled scales and then concatenate with coordinates vectors to recover fully-sampled MR images via an MLP, thus achieving arbitrary scale reconstruction. The performance of the proposed method was assessed by experimenting on publicly available MRI datasets and compared with other reconstruction methods. Our quantitative evaluation demonstrates the superiority of the proposed method over alternative reconstruction methods.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 总是受到长期取样时间的困扰。MRI 重建是一种解决方案，可以降低扫描时间，通过略过certain phase-encoding lines并使用受损量测量来重建高质量图像。最近，implicit neural representation (INR)  emerged as a new deep learning method，可以将对象 Represented as a continuous function of spatial coordinates，通常通过多层感知器 (MLP) 进行参数化。在这篇论文中，我们提出了一种基于 INR 的新的 MRI 重建方法。specifically，我们引入了 scale-embedded encoder 来生成不同抽样缩放的 MR 图像中的scale-independent pixel-specific features，然后将坐标向量 concatenate 以重建完全扫描的 MR 图像，实现任意缩放重建。我们对公共可用的 MRI 数据集进行实验，与其他重建方法进行比较。我们的量化评估表明提出的方法在比较方法中表现出优异。
</details></li>
</ul>
<hr>
<h2 id="A-new-meteor-detection-application-robust-to-camera-movements"><a href="#A-new-meteor-detection-application-robust-to-camera-movements" class="headerlink" title="A new meteor detection application robust to camera movements"></a>A new meteor detection application robust to camera movements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06027">http://arxiv.org/abs/2309.06027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Ciocan, Mathuran Kandeepan, Adrien Cassagne, Jeremie Vaubaillon, Fabian Zander, Lionel Lacassagne</li>
<li>for: 本研究开发了一个新的自动探测流星工具箱 (FMDT)，用于检测天文学上的流星视域。</li>
<li>methods: FMDT使用了视觉摄影机里的当地摄影机或在飞机上的稳定镜头拍摄的影片进行分析，以探测流星视域。</li>
<li>results: FMDT可以实现高精度的流星探测，并且可以在实时运算25帧每秒的影片，且仅占用10瓦的电力。<details>
<summary>Abstract</summary>
This article presents a new tool for the automatic detection of meteors. Fast Meteor Detection Toolbox (FMDT) is able to detect meteor sightings by analyzing videos acquired by cameras onboard weather balloons or within airplane with stabilization. The challenge consists in designing a processing chain composed of simple algorithms, that are robust to the high fluctuation of the videos and that satisfy the constraints on power consumption (10 W) and real-time processing (25 frames per second).
</details>
<details>
<summary>摘要</summary>
本文介绍了一个新的自动探测陨石工具。快速陨石探测工具箱（FMDT）可以通过分析天气气球或飞机上的摄像头获取的视频来探测陨石见解。挑战在于设计一个简单的处理链，该链需要对视频高异常性具有鲁棒性，并满足功率消耗（10 W）和实时处理（25帧每秒）的限制。
</details></li>
</ul>
<hr>
<h2 id="RGB-Guided-Resolution-Enhancement-of-IR-Images"><a href="#RGB-Guided-Resolution-Enhancement-of-IR-Images" class="headerlink" title="RGB-Guided Resolution Enhancement of IR Images"></a>RGB-Guided Resolution Enhancement of IR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05996">http://arxiv.org/abs/2309.05996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Trammer, Nils Genser, Jürgen Seiler</li>
<li>for: 提高低分辨率红外图像的空间分辨率</li>
<li>methods: 使用多摄像头和色彩摄像头的组合，以及RGB准则引导的分辨率提高技术</li>
<li>results: 对比现有方法，实现了1.2dB至1.8dB的PSNR提升，可见的提高分辨率<details>
<summary>Abstract</summary>
This paper introduces a novel method for RGB-Guided Resolution Enhancement of infrared (IR) images called Guided IR Resolution Enhancement (GIRRE). In the area of single image super resolution (SISR) there exists a wide variety of algorithms like interpolation methods or neural networks to improve the spatial resolution of images. In contrast to SISR, even more information can be gathered on the recorded scene when using multiple cameras. In our setup, we are dealing with multi image super resolution, especially with stereo super resolution. We consider a color camera and an IR camera. Current IR sensors have a very low resolution compared to color sensors so that recent color sensors take up 100 times more pixels than IR sensors. To this end, GIRRE increases the spatial resolution of the low-resolution IR image. After that, the upscaled image is filtered with the aid of the high-resolution color image. We show that our method achieves an average PSNR gain of 1.2 dB and at best up to 1.8 dB compared to state-of-the-art methods, which is visually noticeable.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Introducing-Shape-Prior-Module-in-Diffusion-Model-for-Medical-Image-Segmentation"><a href="#Introducing-Shape-Prior-Module-in-Diffusion-Model-for-Medical-Image-Segmentation" class="headerlink" title="Introducing Shape Prior Module in Diffusion Model for Medical Image Segmentation"></a>Introducing Shape Prior Module in Diffusion Model for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05929">http://arxiv.org/abs/2309.05929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Zhang, Guojia Fan, Tianyong Liu, Nan Li, Yuyang Liu, Ziyu Liu, Canwei Dong, Shoujun Zhou</li>
<li>for: 这个研究是为了提供高精度且多样化的骨科医照图像分类模板，以支持骨科医生在临床实践中。</li>
<li>methods: 我们提出了一个终端架构 called VerseDiff-UNet，它结合了去噪扩展probabilistic模型（DDPM）和标准的U型架构。在每个步骤中，我们结合了噪音添加的图像和标签mask来导引扩展方向精确地向目标区域。此外，我们还包括了一个形态先验模块，以提取医照图像中的结构semantic信息。</li>
<li>results: 我们在一个单一的骨科医照图像数据集上评估了我们的方法，结果显示VerseDiff-UNet在精度方面明显超过了其他现有的方法，同时保持了医照图像的自然特征和变化。<details>
<summary>Abstract</summary>
Medical image segmentation is critical for diagnosing and treating spinal disorders. However, the presence of high noise, ambiguity, and uncertainty makes this task highly challenging. Factors such as unclear anatomical boundaries, inter-class similarities, and irrational annotations contribute to this challenge. Achieving both accurate and diverse segmentation templates is essential to support radiologists in clinical practice. In recent years, denoising diffusion probabilistic modeling (DDPM) has emerged as a prominent research topic in computer vision. It has demonstrated effectiveness in various vision tasks, including image deblurring, super-resolution, anomaly detection, and even semantic representation generation at the pixel level. Despite the robustness of existing diffusion models in visual generation tasks, they still struggle with discrete masks and their various effects. To address the need for accurate and diverse spine medical image segmentation templates, we propose an end-to-end framework called VerseDiff-UNet, which leverages the denoising diffusion probabilistic model (DDPM). Our approach integrates the diffusion model into a standard U-shaped architecture. At each step, we combine the noise-added image with the labeled mask to guide the diffusion direction accurately towards the target region. Furthermore, to capture specific anatomical a priori information in medical images, we incorporate a shape a priori module. This module efficiently extracts structural semantic information from the input spine images. We evaluate our method on a single dataset of spine images acquired through X-ray imaging. Our results demonstrate that VerseDiff-UNet significantly outperforms other state-of-the-art methods in terms of accuracy while preserving the natural features and variations of anatomy.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是诊断和治疗脊椎疾病的关键。然而，图像中的高噪音、不确定性和模糊性使得这个任务非常困难。这些因素包括不确定的解剖边界、类别之间的相似性和不合理的注释，均会增加图像分割的挑战。为了支持医生在临床实践中，获得准确和多样化的图像分割模板是非常重要。在最近几年，杂Diffusion probabilistic modeling（DDPM）在计算机视觉领域得到了广泛的研究。它在图像去噪、超分解、异常检测和甚至semantic representation生成等视觉任务中表现出色。然而，现有的杂Diffusion模型在精细mask和其各种效果下仍然存在困难。为了解决医疗图像分割模板的准确性和多样化问题，我们提出了一种综合框架 called VerseDiff-UNet。我们的方法将杂Diffusion模型 integrate into a standard U-shaped architecture。在每个步骤中，我们将噪音加到标注图像，以准确地导引杂Diffusion方向。此外，为了 Capture特定的解剖信息，我们添加了形态先验模块。这个模块能够高效地提取医疗图像中的结构semantic信息。我们对一个X-ray成像技术获得的脊椎图像集进行评估。我们的结果表明，VerseDiff-UNet在准确性方面与其他状态艺术法比较，同时保留了自然的特征和变化。
</details></li>
</ul>
<hr>
<h2 id="Deep-evidential-fusion-with-uncertainty-quantification-and-contextual-discounting-for-multimodal-medical-image-segmentation"><a href="#Deep-evidential-fusion-with-uncertainty-quantification-and-contextual-discounting-for-multimodal-medical-image-segmentation" class="headerlink" title="Deep evidential fusion with uncertainty quantification and contextual discounting for multimodal medical image segmentation"></a>Deep evidential fusion with uncertainty quantification and contextual discounting for multimodal medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05919">http://arxiv.org/abs/2309.05919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Huang, Su Ruan, Pierre Decazes, Thierry Denoeux</li>
<li>for: 这个论文是为了提高多模态医疗影像诊断的准确性和可靠性而写的。</li>
<li>methods: 该论文提出了基于深度学习和德мп斯特-沙法理论的多模态医疗影像分割框架。该框架会考虑每种单模态图像在 segmenting 不同对象时的可靠性，并使用上下文折扣操作来衡量每个模式的可靠性。最后，使用德мп斯特规则将每个模式的折扣副本相加以达到最终决定。</li>
<li>results: 实验结果表明，我们的方法在使用PET-CT数据集上的淋巴瘤和多个MRI数据集上的脑肿瘤上的准确性和可靠性都高于现有方法。<details>
<summary>Abstract</summary>
Single-modality medical images generally do not contain enough information to reach an accurate and reliable diagnosis. For this reason, physicians generally diagnose diseases based on multimodal medical images such as, e.g., PET/CT. The effective fusion of multimodal information is essential to reach a reliable decision and explain how the decision is made as well. In this paper, we propose a fusion framework for multimodal medical image segmentation based on deep learning and the Dempster-Shafer theory of evidence. In this framework, the reliability of each single modality image when segmenting different objects is taken into account by a contextual discounting operation. The discounted pieces of evidence from each modality are then combined by Dempster's rule to reach a final decision. Experimental results with a PET-CT dataset with lymphomas and a multi-MRI dataset with brain tumors show that our method outperforms the state-of-the-art methods in accuracy and reliability.
</details>
<details>
<summary>摘要</summary>
医学影像通常只有单一模式，而这不够以确定精确和可靠的诊断。因此，医生通常会基于多modal的医学影像，如PET/CT，进行诊断。有效地融合多modal信息是必要的，以确定可靠的决策并解释如何达成这个决策。在这篇论文中，我们提出了基于深度学习和Dempster-Shafer理论的多modal医学影像 segmentation框架。在这个框架中，每种单 modal 影像在 segmenting 不同的对象时的可靠性被考虑。然后，通过Dempster的规则将每种modal的折扣后的证据合并到一起，以达到最终的决策。实验结果表明，使用PET-CT数据集和多MRI数据集，我们的方法在精度和可靠性方面都高于当前的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/eess.IV_2023_09_12/" data-id="clmjn91r600ie0j88f4scg4he" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.SD_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T15:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.SD_2023_09_11/">cs.SD - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Natural-Language-Supervision-for-General-Purpose-Audio-Representations"><a href="#Natural-Language-Supervision-for-General-Purpose-Audio-Representations" class="headerlink" title="Natural Language Supervision for General-Purpose Audio Representations"></a>Natural Language Supervision for General-Purpose Audio Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05767">http://arxiv.org/abs/2309.05767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Elizalde, Soham Deshmukh, Huaming Wang</li>
<li>for: 这篇论文旨在提出一种新的语音模型，即异种语音预训练模型，用于学习语音和文本之间的多模式关系，并可以 Zero-Shot 推理。</li>
<li>methods: 该模型使用了两种创新的编码器来学习语音表示，并使用对抗学习来将语音和文本表示带入共同空间。</li>
<li>results: 模型在26个下游任务中表现出色，包括一些任务的state-of-the-art results，这证明了该模型在普用语音表示领域的可靠性和灵活性。<details>
<summary>Abstract</summary>
Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a performance gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text pairs employing two innovative encoders for Zero-Shot inference. To learn audio representations, we trained an audio encoder on 22 audio tasks, instead of the standard training of sound event classification. To learn language representations, we trained an autoregressive decoder-only model instead of the standard encoder-only models. Then, the audio and language representations are brought into a joint multimodal space using Contrastive Learning. We used our encoders to improve the downstream performance by a margin. We extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature. Our model achieves state of the art results in several tasks leading the way towards general-purpose audio representations.
</details>
<details>
<summary>摘要</summary>
音频语言模型共同学习多模态文本和音频表示，以实现零参数推理。模型依靠Encoder创造强大的输入表示，并能泛化到多种任务，包括声音、乐曲和语音。虽然模型已经实现了出色的表现，但还存在任务特定模型的性能差距。在这篇论文中，我们提议了一种对比语言音频预训练模型（Contrastive Language-Audio Pretraining），该模型通过使用多样化的460万个音频文本对进行预训练，并使用两种创新的Encoder来实现零参数推理。为了学习音频表示，我们在22个音频任务上训练了一个音频Encoder，而不是标准的声音分类训练。为了学习语言表示，我们训练了一个自动生成的拟合Decoder模型，而不是标准的Encoder-only模型。然后，音频和语言表示被带入一个共同多模态空间，使用对比学习来学习。我们使用我们的Encoder来改进下游任务的表现，并进行了广泛的评估。我们的表示在26个下游任务中得到了最佳结果，创下了多种任务的新纪录。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects"><a href="#Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects" class="headerlink" title="Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects"></a>Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05634">http://arxiv.org/abs/2309.05634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoichi Koyama, Masaki Nakada, Juliano G. C. Ribeiro, Hiroshi Saruwatari</li>
<li>for: 这篇论文是为了计算在含散射物体的区域内的入射声场而写的。</li>
<li>methods: 该方法使用 kernel ridge regression 算法，并在入射场和散射场之间进行分离，使得不需要先知或测量散射物体的属性。此外，我们还引入了一个权重矩阵，以便在角度方向上强制散射场的平滑性，从而减少了扩展级别对估计精度的影响。</li>
<li>results: 实验结果表明，提出的方法比不带分离的 kernel ridge regression 更高精度地估计入射声场。<details>
<summary>Abstract</summary>
A method for estimating the incident sound field inside a region containing scattering objects is proposed. The sound field estimation method has various applications, such as spatial audio capturing and spatial active noise control; however, most existing methods do not take into account the presence of scatterers within the target estimation region. Although several techniques exist that employ knowledge or measurements of the properties of the scattering objects, it is usually difficult to obtain them precisely in advance, and their properties may change during the estimation process. Our proposed method is based on the kernel ridge regression of the incident field, with a separation from the scattering field represented by a spherical wave function expansion, thus eliminating the need for prior modeling or measurements of the scatterers. Moreover, we introduce a weighting matrix to induce smoothness of the scattering field in the angular direction, which alleviates the effect of the truncation order of the expansion coefficients on the estimation accuracy. Experimental results indicate that the proposed method achieves a higher level of estimation accuracy than the kernel ridge regression without separation.
</details>
<details>
<summary>摘要</summary>
“一种用于估算各个区域内各种散射物的各个方向的声场的方法被提出。这种声场估算方法有各种应用，如空间音采和空间活动噪声控制，但大多数现有方法都不会考虑目标估算区域内的散射物。虽然有一些技术利用了散射物的性能知识或测量结果，但是通常很难在进行估算之前精确地获得这些属性，而且这些属性可能会在估算过程中发生变化。我们提出的方法基于均匀核ridge regression的各个方向的声场，并通过减去散射场的圆形波函数展开，因此不需要先行模型或测量散射物。此外，我们引入了一个权重矩阵来增加散射场的角度方向的平滑性，这有助于减少扩展级别对估算精度的影响。实验结果表明，我们的方法在估算精度方面高于不含分离的均匀核ridge regression。”
</details></li>
</ul>
<hr>
<h2 id="Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making"><a href="#Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making" class="headerlink" title="Undecidability Results and Their Relevance in Modern Music Making"></a>Undecidability Results and Their Relevance in Modern Music Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05595">http://arxiv.org/abs/2309.05595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halley Young</li>
<li>for: 这篇论文探讨计算理论和音乐之间的交叠，探讨了音乐作曲和制作中对不可解决性的具体、又被忽略的应用。</li>
<li>methods: 这篇论文采用多维度方法，对五个关键领域进行研究：（1）Ableton的Turing完善性，（2）各种效果的满足不可解决性，（3） polymeters的音乐作品中的约束不可解决性，（4）正律和谐制约的满足不可解决性，（5）“新的订制系统”的不可解决性。</li>
<li>results: 这篇论文不仅提供了这些assertion的理论证明，还解释了这些概念在非计算机科学领域的实践 relevance。最终目标是促进对音乐中不可解决性的新理解，高亮其更广泛的应用和计算机助手（以及传统）音乐创作的潜在影响。<details>
<summary>Abstract</summary>
This paper delves into the intersection of computational theory and music, examining the concept of undecidability and its significant, yet overlooked, implications within the realm of modern music composition and production. It posits that undecidability, a principle traditionally associated with theoretical computer science, extends its relevance to the music industry. The study adopts a multidimensional approach, focusing on five key areas: (1) the Turing completeness of Ableton, a widely used digital audio workstation, (2) the undecidability of satisfiability in sound creation utilizing an array of effects, (3) the undecidability of constraints on polymeters in musical compositions, (4) the undecidability of satisfiability in just intonation harmony constraints, and (5) the undecidability of "new ordering systems". In addition to providing theoretical proof for these assertions, the paper elucidates the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to foster a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The Turing completeness of Ableton, a widely used digital audio workstation2. The undecidability of sound creation using an array of effects3. The undecidability of constraints on polymeters in musical compositions4. The undecidability of satisfiability in just intonation harmony constraints5. The undecidability of “new ordering systems”In addition to providing theoretical proof for these assertions, the paper explains the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to promote a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.</details></li>
</ol>
<hr>
<h2 id="LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech"><a href="#LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech" class="headerlink" title="LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech"></a>LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05472">http://arxiv.org/abs/2309.05472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier<br>for:这篇论文是用于探讨自动学习（SSL）在语音处理领域中的应用和发展。methods:这篇论文使用了多种自动学习方法，包括wav2vec 2.0 模型，并提供了一个开源的框架LeBenchmark 2.0，用于评估和建立法语音技术。results:这篇论文获得了多个下游任务的评估结果，包括六个下游任务的评估协议，并进行了静止 versus 精确化、任务特定 versus 任务共通预训练模型的比较。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic versus task-specific pre-trained models as well as a discussion on the carbon footprint of large-scale model training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP"><a href="#Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP" class="headerlink" title="Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP"></a>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05423">http://arxiv.org/abs/2309.05423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzuomu Zhong, Yang Li, Hui Huang, Jie Liu, Zhiba Su, Jing Guo, Benlai Tang, Fengjie Zhu</li>
<li>for: 提高文本译 Speech 自然性和可控性</li>
<li>methods: 提出了一种两stage自动标注管道，包括对 Speech-Silence 和 Word-Punctuation 对的对比预训练，以及一种简单 yet effective的文本-语音特征融合方案和序列分类器</li>
<li>results: 实验结果表明，提出的方法可以自动生成高质量的语音抑抑标注，并达到当前最佳性能水平。此外，模型还在不同数据量下进行了remarkable resilience的测试。<details>
<summary>Abstract</summary>
In the realm of expressive Text-to-Speech (TTS), explicit prosodic boundaries significantly advance the naturalness and controllability of synthesized speech. While human prosody annotation contributes a lot to the performance, it is a labor-intensive and time-consuming process, often resulting in inconsistent outcomes. Despite the availability of extensive supervised data, the current benchmark model still faces performance setbacks. To address this issue, a two-stage automatic annotation pipeline is novelly proposed in this paper. Specifically, in the first stage, we propose contrastive text-speech pretraining of Speech-Silence and Word-Punctuation (SSWP) pairs. The pretraining procedure hammers at enhancing the prosodic space extracted from joint text-speech space. In the second stage, we build a multi-modal prosody annotator, which consists of pretrained encoders, a straightforward yet effective text-speech feature fusion scheme, and a sequence classifier. Extensive experiments conclusively demonstrate that our proposed method excels at automatically generating prosody annotation and achieves state-of-the-art (SOTA) performance. Furthermore, our novel model has exhibited remarkable resilience when tested with varying amounts of data.
</details>
<details>
<summary>摘要</summary>
在表达力强的文本至语音（TTS）领域，明确的语音分界有助于提高合成语音的自然性和可控性。虽然人工语音标注带来了很多 помощ，但是这是一项劳动密集和时间消耗的过程，经常导致不一致的结果。尽管有庞大的超级vised数据可用，现有的标准模型仍然面临性能下降。为解决这个问题，本文提出了一种两个阶段自动标注管道。在第一阶段，我们提议了文本-语音预训练（SSWP）对照练习。预训练过程强化抽象出的语音空间，从joint文本-语音空间提取的语音特征。在第二阶段，我们构建了多模态语音注解器，包括预训练的编码器、简单又有效的文本-语音特征融合方案以及序列分类器。广泛的实验证明了我们提出的方法可以高效地生成语音注解，并达到了当前领域的最佳性能（SOTA）。此外，我们的新模型在不同数据量测试时表现出了很好的抗性。
</details></li>
</ul>
<hr>
<h2 id="SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus"><a href="#SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus" class="headerlink" title="SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus"></a>SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05396">http://arxiv.org/abs/2309.05396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxu Wang, Fan Yu, Xian Shi, Yuezhang Wang, Shiliang Zhang, Ming Li</li>
<li>for: 这个论文主要目的是提高自动语音识别系统的性能，通过利用多 modal 信息，包括视频和文本信息。</li>
<li>methods: 该论文提出了一种基eline方法，利用视频材料中的文本信息来提高语音识别性能。这些方法包括关键词提取和contextual ASR方法。</li>
<li>results: 该论文通过实验示出，通过利用视频材料中的文本信息，可以提高语音识别性能。 Specifically, the paper shows that incorporating textual information from slides into the benchmark system can improve speech recognition performance.<details>
<summary>Abstract</summary>
Multi-Modal automatic speech recognition (ASR) techniques aim to leverage additional modalities to improve the performance of speech recognition systems. While existing approaches primarily focus on video or contextual information, the utilization of extra supplementary textual information has been overlooked. Recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details>
<details>
<summary>摘要</summary>
多modal自动语音识别（ASR）技术目的是利用其他modalities提高语音识别系统的性能。现有的方法主要关注视频或上下文信息，而使用附加的文本信息则被忽略了。 recognize the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.Here's the word-for-word translation:多modal自动语音识别（ASR）技术目的是利用其他modalities提高语音识别系统的性能。现有的方法主要关注视频或上下文信息，而使用附加的文本信息则被忽略了。recognize the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details></li>
</ul>
<hr>
<h2 id="Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations"><a href="#Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations" class="headerlink" title="Towards generalisable and calibrated synthetic speech detection with self-supervised representations"></a>Towards generalisable and calibrated synthetic speech detection with self-supervised representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05384">http://arxiv.org/abs/2309.05384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Oneata, Adriana Stan, Octavian Pascu, Elisabeta Oneata, Horia Cucu</li>
<li>for: 这个论文的目的是提高深度伪造检测器的可靠性，并且研究如何在未经见过的数据上达到良好的泛化能力。</li>
<li>methods: 这个论文使用预训练的自我超vised表示 followed by a simple logistic regression classifier来实现强大的泛化能力，从而提高了Equal Error Rate从30%降至8%在新引入的In-the-Wild dataset上。</li>
<li>results: 这个方法不仅可以提高泛化能力，还可以生成更加准确的模型，并且可以用于下游任务，如uncertainty estimation。具体来说，这个方法可以通过计算预测概率 entropy来拒绝不确定的样本并进一步提高准确性。<details>
<summary>Abstract</summary>
Generalisation -- the ability of a model to perform well on unseen data -- is crucial for building reliable deep fake detectors. However, recent studies have shown that the current audio deep fake models fall short of this desideratum. In this paper we show that pretrained self-supervised representations followed by a simple logistic regression classifier achieve strong generalisation capabilities, reducing the equal error rate from 30% to 8% on the newly introduced In-the-Wild dataset. Importantly, this approach also produces considerably better calibrated models when compared to previous approaches. This means that we can trust our model's predictions more and use these for downstream tasks, such as uncertainty estimation. In particular, we show that the entropy of the estimated probabilities provides a reliable way of rejecting uncertain samples and further improving the accuracy.
</details>
<details>
<summary>摘要</summary>
通用化 -- 模型能够在未经见过的数据上表现出色 -- 是深度假设检测器的关键要素。然而，最近的研究表明，当前的音频深度假设模型缺乏这一要素。在这篇论文中，我们表明了预训练自我超视的表示后加上简单的логистиック回归分类器可以实现强大的通用化能力，从而降低新引入的In-the-Wild数据集上的平均错误率从30%降至8%。此外，这种方法还生成了较好的准确性模型，与之前的方法相比。这意味着我们可以更加信任我们的模型预测结果，并使其用于下游任务，如uncertainty估计。具体来说，我们发现了估计概率的熵可以可靠地拒绝不确定的样本，并进一步提高准确率。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Feature-Imbalance-in-Sound-Source-Separation"><a href="#Addressing-Feature-Imbalance-in-Sound-Source-Separation" class="headerlink" title="Addressing Feature Imbalance in Sound Source Separation"></a>Addressing Feature Imbalance in Sound Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05287">http://arxiv.org/abs/2309.05287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaechang Kim, Jeongyeon Hwang, Soheun Yi, Jaewoong Cho, Jungseul Ok</li>
<li>for: 这个论文主要用于解决神经网络在分类任务中存在的特征偏好问题，即神经网络会过分依赖特定的特征来解决任务，忽略其他重要的特征。</li>
<li>methods: 这篇论文提出了一种解决特征偏好问题的方法，即FEAture BAlancing by Suppressing Easy feature (FEABASE)。这种方法可以有效地利用数据中的隐藏信息，以解决忽略的特征。</li>
<li>results: 作者在一个多通道源分离任务中评估了他们的方法，发现该方法可以有效地避免特征偏好问题，提高分离性能。<details>
<summary>Abstract</summary>
Neural networks often suffer from a feature preference problem, where they tend to overly rely on specific features to solve a task while disregarding other features, even if those neglected features are essential for the task. Feature preference problems have primarily been investigated in classification task. However, we observe that feature preference occurs in high-dimensional regression task, specifically, source separation. To mitigate feature preference in source separation, we propose FEAture BAlancing by Suppressing Easy feature (FEABASE). This approach enables efficient data utilization by learning hidden information about the neglected feature. We evaluate our method in a multi-channel source separation task, where feature preference between spatial feature and timbre feature appears.
</details>
<details>
<summary>摘要</summary>
神经网络经常受到特征偏好问题的困扰，即它们倾向于仅仅依赖特定的特征来解决任务，而忽略其他特征，即使这些忽略的特征对任务非常重要。特征偏好问题主要在分类任务中被研究，但我们发现，在高维度回归任务中，特征偏好也存在，具体来说是来自源分解。为了解决源分解中的特征偏好，我们提议了FEAture BAlancing by Suppressing Easy feature（FEABASE）方法。这种方法可以有效地利用数据，并且学习遗弃的特征中的隐藏信息。我们在多通道源分解任务中评估了我们的方法，发现特征偏好 между空间特征和气质特征存在。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach"><a href="#Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach" class="headerlink" title="Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach"></a>Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05248">http://arxiv.org/abs/2309.05248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, Kunal Dhawan, Nithin Koluguri, Jagadeesh Balam</li>
<li>for: 这个论文的目的是提出一种基于大语言模型（LLM）的语音识别方法，以利用语音和文本模式的共同信息来改进语音识别效果。</li>
<li>methods: 这种方法建立于现有的音频基于的语音识别系统之上，通过在推理阶段添加语言模型（LLM）的lexical信息来利用语音和文本模式的共同信息。这个方法使用 probabilistic 模型来模型多模态混合解码过程，并在joint acoustic和lexical beam search中合并两个模式的信息。</li>
<li>results: 实验结果表明，通过在音频基于的语音识别系统中添加LLM的lexical信息，可以提高总的speaker-attributed word error rate（SA-WER）。这个方法在比对基eline系统时显示出达39.8%的相对 delta-SA-WER提升。这些结果表明，LLM可以为语音识别任务提供补充的信息，并且这种方法可以利用LLM捕捉到语音和文本模式之间的共同信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown great promise for capturing contextual information in natural language processing tasks. We propose a novel approach to speaker diarization that incorporates the prowess of LLMs to exploit contextual cues in human dialogues. Our method builds upon an acoustic-based speaker diarization system by adding lexical information from an LLM in the inference stage. We model the multi-modal decoding process probabilistically and perform joint acoustic and lexical beam search to incorporate cues from both modalities: audio and text. Our experiments demonstrate that infusing lexical knowledge from the LLM into an acoustics-only diarization system improves overall speaker-attributed word error rate (SA-WER). The experimental results show that LLMs can provide complementary information to acoustic models for the speaker diarization task via proposed beam search decoding approach showing up to 39.8% relative delta-SA-WER improvement from the baseline system. Thus, we substantiate that the proposed technique is able to exploit contextual information that is inaccessible to acoustics-only systems which is represented by speaker embeddings. In addition, these findings point to the potential of using LLMs to improve speaker diarization and other speech processing tasks by capturing semantic and contextual cues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.SD_2023_09_11/" data-id="clmjn91oi00c70j886rbt0ia4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.LG_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T10:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.LG_2023_09_11/">cs.LG - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics"><a href="#Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics" class="headerlink" title="Reaction coordinate flows for model reduction of molecular kinetics"></a>Reaction coordinate flows for model reduction of molecular kinetics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05878">http://arxiv.org/abs/2309.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wu, Frank Noé</li>
<li>for: 该论文旨在开发一种基于流程的机器学习方法，称为反应坐拱（RC）流，用于分子系统的低维度动力学模型的发现。</li>
<li>methods: 该方法使用正规化流来设计坐拱变换，并使用布朗运动模型来近似RC的动力学。所有模型参数都可以通过数据驱动的方式进行估计。</li>
<li>results: 计算结果显示，提出的方法可以高效地从仪器估计出来的全状态动力学中提取低维度的准确和可读的表示。<details>
<summary>Abstract</summary>
In this work, we introduce a flow based machine learning approach, called reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models of molecular systems. The RC flow utilizes a normalizing flow to design the coordinate transformation and a Brownian dynamics model to approximate the kinetics of RC, where all model parameters can be estimated in a data-driven manner. In contrast to existing model reduction methods for molecular kinetics, RC flow offers a trainable and tractable model of reduced kinetics in continuous time and space due to the invertibility of the normalizing flow. Furthermore, the Brownian dynamics-based reduced kinetic model investigated in this work yields a readily discernible representation of metastable states within the phase space of the molecular system. Numerical experiments demonstrate how effectively the proposed method discovers interpretable and accurate low-dimensional representations of given full-state kinetics from simulations.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种基于流的机器学习方法，称为反应坐标（RC）流，用于分子系统的低维度动力学模型的发现。 RC流使用了 нормализа化流来设计坐标变换，并使用布朗运动模型来近似RC的动力学，其中所有模型参数都可以在数据驱动下进行估计。与现有的分子动力学减少方法不同，RC流提供了可训练和可追踪的维度减少模型，因为正则化流的逆函数是可逆的。此外，基于布朗运动的减少动力学模型在研究中提供了一个可见的和准确的低维度表示形式，用于描述分子系统中的中间态。数值实验表明，提议的方法可以高效地从仿真数据中提取可读取和准确的低维度表示。
</details></li>
</ul>
<hr>
<h2 id="Force-directed-graph-embedding-with-hops-distance"><a href="#Force-directed-graph-embedding-with-hops-distance" class="headerlink" title="Force-directed graph embedding with hops distance"></a>Force-directed graph embedding with hops distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05865">http://arxiv.org/abs/2309.05865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Lotfalizadeh, Mohammad Al Hasan</li>
<li>for: 本研究旨在提出一种基于力的图嵌入方法，用于保持图结构和结构特征。</li>
<li>methods: 本方法使用定量的吸引和排斥力来嵌入节点，并通过牛顿第二定律计算每个节点的加速度。方法易于理解，可并行，高度扩展。</li>
<li>results: 在多个图分析任务中，本方法与现有无监督嵌入方法相比，实现了竞争性的性能。<details>
<summary>Abstract</summary>
Graph embedding has become an increasingly important technique for analyzing graph-structured data. By representing nodes in a graph as vectors in a low-dimensional space, graph embedding enables efficient graph processing and analysis tasks like node classification, link prediction, and visualization. In this paper, we propose a novel force-directed graph embedding method that utilizes the steady acceleration kinetic formula to embed nodes in a way that preserves graph topology and structural features. Our method simulates a set of customized attractive and repulsive forces between all node pairs with respect to their hop distance. These forces are then used in Newton's second law to obtain the acceleration of each node. The method is intuitive, parallelizable, and highly scalable. We evaluate our method on several graph analysis tasks and show that it achieves competitive performance compared to state-of-the-art unsupervised embedding techniques.
</details>
<details>
<summary>摘要</summary>
“图像投影”已成为分析图形数据的重要技术。通过将图像中的节点转化为低维度空间中的向量，图像投影可以帮助提高图像处理和分析任务的效率，如节点分类、链接预测和视觉化。在这篇论文中，我们提出了一种新的力导向的图像投影方法，该方法利用稳定加速公式来嵌入节点，以保持图像的结构特征和特征。我们的方法模拟了一组自定义吸引和排斥力 zwischen所有节点对，根据它们的跳跃距离。这些力量然后用新顿第二定律来获得每个节点的加速度。我们的方法是直观、并行化和可扩展的。我们对一些图像分析任务进行评估，并证明了我们的方法与当前的无监督嵌入技术相比，具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="The-bionic-neural-network-for-external-simulation-of-human-locomotor-system"><a href="#The-bionic-neural-network-for-external-simulation-of-human-locomotor-system" class="headerlink" title="The bionic neural network for external simulation of human locomotor system"></a>The bionic neural network for external simulation of human locomotor system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05863">http://arxiv.org/abs/2309.05863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Shuhao Ma, Yihui Zhao<br>for:This paper aims to develop a physics-informed deep learning method to predict joint motion and muscle forces using musculoskeletal (MSK) modeling techniques.methods:The proposed method embeds the MSK model into a neural network as an ordinary differential equation (ODE) loss function, which enables the automatic estimation of subject-specific MSK physiological parameters during the training process.results:Experimental validations on two datasets from six healthy subjects demonstrate that the proposed deep learning method can effectively identify subject-specific MSK physiological parameters and the trained physics-informed forward-dynamics surrogate yields accurate motion and muscle forces predictions.Here is the Chinese version of the three key points:for:这篇论文的目的是通过musculoskeletal（MSK）模型技术预测 JOINT动作和肌肉力。方法:提议的方法将MSK模型 embedding到神经网络中作为常微分方程（ODE）损失函数，这使得在训练过程中自动估算subject特定的MSK生物物理参数成为可能。结果:对两个数据集（一个benchmark dataset和一个自收集dataset）中的六名健康志愿者进行实验验证，结果表明提议的深度学习方法可以有效地自动估算subject特定的MSK生物物理参数，并且训练的物理学习前继模型可以准确预测 JOINT动作和肌肉力。<details>
<summary>Abstract</summary>
Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These parameters are automatically estimated during the training process which guides the prediction of muscle forces combined with the MSK forward dynamics model. Experimental validations on two groups of data, including one benchmark dataset and one self-collected dataset from six healthy subjects, are performed. The results demonstrate that the proposed deep learning method can effectively identify subject-specific MSK physiological parameters and the trained physics-informed forward-dynamics surrogate yields accurate motion and muscle forces predictions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用musculoskeletal（MSK）模型技术计算运动质量的muscle forces和 JOINT动态可以提供有用的度量。模型基于的计算MSK模型可以解释动态INTERACTION between neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics。然而，这种解决方案受到高计算时间和muscle recruitment问题的困扰，特别是在复杂的模拟中。在最近几年，数据驱动方法作为一种可能的替代方案而出现，这是因为它们具有灵活性和适应性的优点。然而，大量标注训练数据很难被获得。本文提出了一种基于MSK模型的物理学习方法，用于预测 JOINT动态和muscle forces。MSK模型被嵌入到神经网络中作为常微分方程（ODE）损失函数，以便在训练过程中自动计算 OUT physiological parameters of muscle activation dynamics和muscle contraction dynamics。这些参数被自动计算出来，并且指导预测muscle forces的计算，与MSK前向动力学模型相结合。对于两个数据集进行了实验验证，包括一个benchmark数据集和一个自己收集的数据集，从六名健康者获得。结果表明，提议的深度学习方法可以有效地Identify subject-specific MSK physiological parameters，并且训练的物理学习前向动力学Surrogate可以准确预测 JOINT动态和muscle forces。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-mesa-optimization-algorithms-in-Transformers"><a href="#Uncovering-mesa-optimization-algorithms-in-Transformers" class="headerlink" title="Uncovering mesa-optimization algorithms in Transformers"></a>Uncovering mesa-optimization algorithms in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05858">http://arxiv.org/abs/2309.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jimmieliu/transformer-mesa-layer">https://github.com/jimmieliu/transformer-mesa-layer</a></li>
<li>paper_authors: Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Agüera y Arcas, Max Vladymyrov, Razvan Pascanu, João Sacramento</li>
<li>for: 这篇论文探讨了Transformers模型的性能优势的原因，并提出了一种假设，即Transformers的强大表现与其架构偏好有关，即内部的mesa-优化过程。</li>
<li>methods: 作者通过对一系列autoregressive Transformers模型进行反向工程，揭示了这些模型内部的gradient-based mesa-优化算法，并证明了这些算法可以用于解决少量预测任务。</li>
<li>results: 作者发现，mesa-优化可以在几乎没有预训练数据的情况下，使模型在几种supervised few-shot任务中表现出色，这表明mesa-优化可能是大型自然语言模型中的一种隐藏的功能。此外，作者还提出了一种新的自注意层，即mesa-层，可以证明这种假设。<details>
<summary>Abstract</summary>
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.
</details>
<details>
<summary>摘要</summary>
卷积Transformers已成为深度学习中最具有优势的模型，但它们的高性能的原因尚未完全理解。在这里，我们提出了一种假设，即Transformers的强大表现归功于模型的内部优化偏好，即在前向传播中的搜索过程中学习出的一种内部学习目标，以及该目标的优化解决方法。为检验这一假设，我们将对一系列基于序列模型的 autoregressive Transformers 进行反向工程，揭示了这些模型中的gradient-based mesa-optimization算法，以及它们如何驱动预测的生成。此外，我们还证明了这些学习前向传播优化算法可以立即应用于解决一些简单的supervised few-shot任务，表明了mesa-optimization可能在大语言模型中具有卷积学习的能力。最后，我们提出了一种新的自注意层，即mesa-层，它可以Explicitly and efficiently solve context-specified optimization problems。我们发现，这层可以在synthetic和初步语言模型实验中提高性能，加强了我们假设，即mesa-optimization是训练过的Transformers中隐藏的重要操作。
</details></li>
</ul>
<hr>
<h2 id="Energy-Preservation-and-Stability-of-Random-Filterbanks"><a href="#Energy-Preservation-and-Stability-of-Random-Filterbanks" class="headerlink" title="Energy Preservation and Stability of Random Filterbanks"></a>Energy Preservation and Stability of Random Filterbanks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05855">http://arxiv.org/abs/2309.05855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Haider, Vincent Lostanlen, Martin Ehler, Peter Balazs</li>
<li>for: 这篇论文主要写于哪？</li>
<li>methods: 这篇论文使用了哪些方法？</li>
<li>results: 这篇论文得到了哪些结果？Here are the answers to these questions, in Simplified Chinese:</li>
<li>for: 这篇论文主要写于哪？	+ 这篇论文探讨了waveform-based deep learning在filterbank设计方面的困难，尤其是 convolutional neural networks (convnets) 在实际应用中 often fail to outperform hand-crafted baselines.</li>
<li>methods: 这篇论文使用了哪些方法？	+ 这篇论文使用了 random convolutional operators 来描述 convnets 的统计性质，并发现了FIR filterbanks with random Gaussian weights 在大filter和本地 périodic input signal的情况下是不稳定的。</li>
<li>results: 这篇论文得到了哪些结果？	+ 这篇论文发现了 expected energy preservation 不 suficient for numerical stability,并 derive了 theoretical bounds for its expected frame bounds.<details>
<summary>Abstract</summary>
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为什么波形基于深度学习 så difficult? 虽然许多人尝试用 convolutional neural networks (convnets) 来设计滤波器，但它们经常无法超越手动设计的基线。这对于这些基线来说更加怪异，因为它们是线性时间不变的系统，因此它们的传输函数可以由一个大感知范围的 convnet 准确地表示。在这篇文章中，我们从数学角度深入探讨简单的 convnet 的统计性质。我们发现，使用随机 Gaussian 权重的 FIR 滤波器在大 filter 和本地 периодиic 输入信号下是不可靠的，这些输入信号是音频信号处理应用中的常见情况。此外，我们发现，预期能量保持不够以确保数学稳定性，我们 derive 了预期帧边界的理论上限。
</details></li>
</ul>
<hr>
<h2 id="ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation"><a href="#ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation" class="headerlink" title="ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation"></a>ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05853">http://arxiv.org/abs/2309.05853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory W. Kyro, Anton Morgunov, Rafael I. Brent, Victor S. Batista</li>
<li>for: 这个研究的目的是开发一种新的、有效的 semi-supervised active learning 方法，以便在生成型人工智能模型中进行评估和调整。</li>
<li>methods: 这种方法利用了一种构建的样本空间代理，通过灵活地操作在这个代理中，以便调整一个基于 GPT 的分子生成器，使其与一个蛋白质目标之间具有吸引力的互动。</li>
<li>results: 研究人员通过使用这种方法，能够快速地和有效地调整 GPT 模型，以便在分子生成中实现更高的吸引力和互动性。此外，这种方法不需要评估所有的数据点，因此可以使用 computationally expensive metrics。<details>
<summary>Abstract</summary>
The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-tuning, enabling the incorporation of computationally expensive metrics. We are hopeful that the inherent generality of this methodology ensures that it will remain applicable as this exciting field evolves. To facilitate implementation and reproducibility, we have made all of our software available through the open-source ChemSpaceAL Python package.
</details>
<details>
<summary>摘要</summary>
具有强大生成能力的人工智能模型在药物发现领域的应用已经是不可避免的。因此，开发能够提高这些强大工具的能力和可应用性的方法是非常重要的。在这个工作中，我们提出了一种新的、高效的半监督学习方法，可以让一个生成模型与一个目标函数进行精细调整，通过在一个建立的样本空间中策略性操作。在聚合分子生成中，我们示示了如何通过在一个化学空间代理中精细调整一个基于GPT的分子生成器，以便 maximize 分子和蛋白质目标之间的有吸引力相互作用。值得注意的是，我们的方法不需要评估所有用于精细调整的数据点，因此可以包含计算成本高昂的指标。我们希望这种方法的内在通用性可以保证它在这个赏心感激的领域中保持可靠。为了促进实现和重现性，我们将所有的软件公开发布在开源的ChemSpaceAL Python包中。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Compiler-Optimization"><a href="#Large-Language-Models-for-Compiler-Optimization" class="headerlink" title="Large Language Models for Compiler Optimization"></a>Large Language Models for Compiler Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07062">http://arxiv.org/abs/2309.07062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Kim Hazelwood, Gabriel Synnaeve, Hugh Leather</li>
<li>for: 这篇论文主要目的是提出一种使用大语言模型进行代码优化的新应用。</li>
<li>methods: 论文使用了一个7亿参数的变换器模型，从头开始训练，以优化 LLVM  assembly 代码大小。模型接受不优化的Assembly输入，并输出一个包含编译器选项的列表，以最优化程序。在训练过程中，模型需要预测未优化代码和优化后代码的指令计数，以及优化后的代码本身。这些辅助学习任务有助于提高优化模型的性能和代码理解深度。</li>
<li>results: 论文在一个大量测试程序中进行了评估。对比两个基线，我们的方法实现了减少指令计数的3.0%的提高，并在70%的时间内完全评估器的输出。此外，模型还表现出了强大的代码理解能力，在91%的时间内生成可编译代码，并在70%的时间内完全评估器的输出。<details>
<summary>Abstract</summary>
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.   We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
</details>
<details>
<summary>摘要</summary>
我们探索了大型语言模型在代码优化中的新应用。我们介绍了一个7亿参数的变换器模型，从零开始训练以优化LLVMAssembly代码的大小。该模型接受未优化的Assembly输入，并输出一个包含编译器选项的列表，以最佳化程序。在训练时，我们请求模型预测未优化代码和优化后代码中的指令计数，以及优化后的代码本身。这些辅助学习任务显著提高了优化性能，并提高了模型对代码的深度理解。我们对一个大量测试程序进行评估。我们的方法实现了减少指令计数的3.0%的提高，比两个状态流行的基线要好，这些基线需要数千次编译。此外，模型显示了奇异的代码理解能力，生成可 COMPILE 的代码91%的时间，并完美地模拟了编译器的输出70%的时间。
</details></li>
</ul>
<hr>
<h2 id="Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data"><a href="#Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data" class="headerlink" title="Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data"></a>Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05845">http://arxiv.org/abs/2309.05845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjia Niu, Yuchen Zhao, Hamed Haddadi</li>
<li>for: 这篇论文是为了探讨如何实现精准的异常活动探测在智能健康领域中，使用多元时间系列数据。</li>
<li>methods: 本研究使用了一种剩余基于的异常探测方法（Rs-AD），实现了异常活动探测的有效表示学习和探测。</li>
<li>results: 实验结果显示，使用Rs-AD方法可以在一个真实世界的步态数据中达到F1分数0.839的高精度异常探测。<details>
<summary>Abstract</summary>
Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）数据从多个传感器获取，可以准确地探测智能医疗场景中异常活动。然而，异常现象在MTS数据中表现出多种模式，容易被忽略。因此，实现准确的异常探测是一项挑战，因为我们需要捕捉时间序列的 temporal dependencies 和变量之间的关系。为解决这个问题，我们提出了基于差异的异常检测方法（Rs-AD），用于有效地学习表示和异常检测。我们在一个真实的步态数据集上进行了实验，并得到了 F1 分数为 0.839。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals"><a href="#Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals" class="headerlink" title="Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals"></a>Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05843">http://arxiv.org/abs/2309.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Blankemeier, Sebastien Baur, Wei-Hung Weng, Jake Garrison, Yossi Matias, Shruthi Prabhakara, Diego Ardila, Zaid Nabulsi</li>
<li>for: 这个论文主要用于健康相关的声学信号，如喊喊和呼吸 зву频，以便医疗诊断和连续健康监测。</li>
<li>methods: 这个论文使用了一种自动学习框架，即SimCLR，并使用了一种名为Slowfast NFNet的底层模型。这些模型通过对健康声学信号进行对比学习来学习。</li>
<li>results: 研究人员通过对不同健康声学任务进行分析，发现合适的声音变换策略可以提高Slowfast NFNet声音编码器的性能。这些变换策略可以单独应用或相互结合，以实现更高的性能。<details>
<summary>Abstract</summary>
Health-related acoustic signals, such as cough and breathing sounds, are relevant for medical diagnosis and continuous health monitoring. Most existing machine learning approaches for health acoustics are trained and evaluated on specific tasks, limiting their generalizability across various healthcare applications. In this paper, we leverage a self-supervised learning framework, SimCLR with a Slowfast NFNet backbone, for contrastive learning of health acoustics. A crucial aspect of optimizing Slowfast NFNet for this application lies in identifying effective audio augmentations. We conduct an in-depth analysis of various audio augmentation strategies and demonstrate that an appropriate augmentation strategy enhances the performance of the Slowfast NFNet audio encoder across a diverse set of health acoustic tasks. Our findings reveal that when augmentations are combined, they can produce synergistic effects that exceed the benefits seen when each is applied individually.
</details>
<details>
<summary>摘要</summary>
医疗相关的声学信号，如喊喊和呼吸声，对医疗诊断和连续健康监测有重要意义。现有的大多数机器学习方法对健康声学是特定任务的训练和评估，这限制了它们在各种医疗应用中的一致性。在这篇论文中，我们利用了一个自我超vised学习框架，SimCLR，并与Slowfast NFNet骨干结构进行对比学习健康声学。我们认为对于这种应用，适合NFNet音频编码器的优化是一个关键性的问题。我们进行了各种声音变换策略的深入分析，并证明了合适的声音变换策略可以提高Slowfast NFNet音频编码器在多种健康声学任务中的表现。我们的发现表明，当变换策略相互组合时，它们可以产生相互补做的效果，超过每个策略应用 separately的效果。
</details></li>
</ul>
<hr>
<h2 id="The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems"><a href="#The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems" class="headerlink" title="The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems"></a>The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05837">http://arxiv.org/abs/2309.05837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai-Chieh Hsu, Haimin Hu, Jaime Fernández Fisac</li>
<li>for: 本文旨在探讨安全筛选技术的综述，以便更好地理解、比较和结合不同类型的安全筛选方法。</li>
<li>methods: 本文使用了数据驱动方法和模型驱动方法的结合，以提高安全筛选技术的可扩展性和可靠性。</li>
<li>results: 本文提出了一种统一的技术框架，可以更好地理解和结合不同类型的安全筛选方法，并且提供了未来研究的指导方向，包括更加扩展的synthesis、更加稳定的监测和更加高效的干涉。<details>
<summary>Abstract</summary>
Recent years have seen significant progress in the realm of robot autonomy, accompanied by the expanding reach of robotic technologies. However, the emergence of new deployment domains brings unprecedented challenges in ensuring safe operation of these systems, which remains as crucial as ever. While traditional model-based safe control methods struggle with generalizability and scalability, emerging data-driven approaches tend to lack well-understood guarantees, which can result in unpredictable catastrophic failures. Successful deployment of the next generation of autonomous robots will require integrating the strengths of both paradigms. This article provides a review of safety filter approaches, highlighting important connections between existing techniques and proposing a unified technical framework to understand, compare, and combine them. The new unified view exposes a shared modular structure across a range of seemingly disparate safety filter classes and naturally suggests directions for future progress towards more scalable synthesis, robust monitoring, and efficient intervention.
</details>
<details>
<summary>摘要</summary>
近年来，机器人自主技术受到了 significiant progress，同时机器人技术的扩展也随之扩大。然而，新的部署领域的出现带来了前所未有的安全操作问题的挑战，这些问题仍然具有极高的重要性。传统的模型基于的安全控制方法在普适性和可扩展性方面受到限制，而新兴的数据驱动方法则具有不可预期的崩溃问题。成功部署下一代自主机器人需要结合两个 парадиг之力。本文提供了安全筛 Approaches 的评论，并将现有技术相关的重要连接点推广，并提出了一个统一的技术框架，以便更好地理解、比较和结合它们。新的统一视图暴露了一系列看起来有所不同的安全筛类型之间的共同模块结构，自然地指明了未来进程中更好的扩展、稳定监测和高效 intervención的方向。
</details></li>
</ul>
<hr>
<h2 id="PACE-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis"><a href="#PACE-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis" class="headerlink" title="PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis"></a>PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05833">http://arxiv.org/abs/2309.05833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Zhang, Xuchao Zhang, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan</li>
<li>for: 提高云事件管理中的root cause分析工具的可靠性，以确保服务可靠性和客户信任。</li>
<li>methods: 利用提取-扩展大语言模型（LLMs）来提高root cause分析工具的自信估计。该方法包括两个阶段：首先，模型根据历史事件数据评估自己的自信程度；然后，模型评估由预测器生成的root cause。最后，优化步骤将这两个评估结果组合起来确定最终的自信分配。</li>
<li>results: 实验结果表明，我们的方法可以让模型更好地表达自己的自信程度，提供更加准确的分数。我们回答了一系列研究问题，包括使用LLMs生成calibrated confidence scores的能力，针对特定领域检索到的示例对自信估计的影响，以及该方法在不同的root cause分析模型中的普适性。通过这些研究，我们希望bridge the confidence estimation gap，帮助云事件管理人员更加快速和准确地做出决策。<details>
<summary>Abstract</summary>
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the final confidence assignment. Experimental results illustrate that our method enables the model to articulate its confidence effectively, providing a more calibrated score. We address research questions evaluating the ability of our method to produce calibrated confidence scores using LLMs, the impact of domain-specific retrieved examples on confidence estimates, and its potential generalizability across various root cause analysis models. Through this, we aim to bridge the confidence estimation gap, aiding on-call engineers in decision-making and bolstering the efficiency of cloud incident management.
</details>
<details>
<summary>摘要</summary>
This paper proposes a method to enhance the confidence estimation in root cause analysis tools by leveraging retrieval-augmented large language models (LLMs). This approach consists of two phases. First, the model evaluates its confidence based on historical incident data, considering the strength of the evidence. Then, the model reviews the root cause generated by the predictor and an optimization step combines these evaluations to determine the final confidence assignment.Experimental results show that our method enables the model to provide more calibrated confidence scores. To address research questions, we evaluate the ability of our method to produce calibrated confidence scores using LLMs, the impact of domain-specific retrieved examples on confidence estimates, and its potential generalizability across various root cause analysis models. Our goal is to bridge the confidence estimation gap, helping on-call engineers make informed decisions and improve the efficiency of cloud incident management.
</details></li>
</ul>
<hr>
<h2 id="Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning"><a href="#Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning" class="headerlink" title="Instance-Agnostic Geometry and Contact Dynamics Learning"></a>Instance-Agnostic Geometry and Contact Dynamics Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05832">http://arxiv.org/abs/2309.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengti Sun, Bowen Jiang, Bibit Bianchini, Camillo Jose Taylor, Michael Posa</li>
<li>for: 学习机器人对物体的shape、pose和物理性能的同时学习</li>
<li>methods: 使用geometry作为共享表示，将视觉和动力学 fusion在一起，不需要知道形状先验或动作捕捉输入</li>
<li>results: 实验表明，该框架可以学习rigid和圆形物体的geometry和动力学性能，并且超越当前的跟踪框架<details>
<summary>Abstract</summary>
This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety"><a href="#Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety" class="headerlink" title="Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety"></a>Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05831">http://arxiv.org/abs/2309.05831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Bertrand, Nick Griffey, Ming-Lun Lu, Rashmi Jha</li>
<li>for: 本研究旨在将实验室训练的机器学习模型移植到实际世界中。</li>
<li>methods: 本研究使用了四种可能的解决方案来增强模型性能，包括实验室训练数据的调整、实际世界资料的调整、模型的升级和降级。</li>
<li>results: 经过四种解决方案的实践，模型的性能有所提高，但仍然不如实验室训练数据中的性能。<details>
<summary>Abstract</summary>
Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
</details>
<details>
<summary>摘要</summary>
将实验室训练的机器学习模型应用到实际世界中一直是一个挑战。本文讨论了将实验室训练的抓推模型在实际世界中的应用，其性能与训练数据之间存在很大差异。我们探索了失败的原因，并提出了四种可能的解决方案以提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting"><a href="#Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting" class="headerlink" title="Exploring Geometric Deep Learning For Precipitation Nowcasting"></a>Exploring Geometric Deep Learning For Precipitation Nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05828">http://arxiv.org/abs/2309.05828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Zhao, Sudipan Saha, Zhitong Xiong, Niklas Boers, Xiao Xiang Zhu</li>
<li>for: 预测 precipitation nowcasting (up to a few hours)</li>
<li>methods: 使用 Geometric deep learning-based temporal Graph Convolutional Network (GCN)</li>
<li>results: 提高了地方详细云Profile的模型效果和预测精度， achievement decreased error measures.<details>
<summary>Abstract</summary>
Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value during the training procedure. Then, the spatial relationship is refined by GCN layers while the temporal information is extracted by 1D convolution with various kernel lengths. The neighboring information is fed as auxiliary input layers to improve the final result. We test the model on sequences of radar reflectivity maps over the Trento/Italy area. The results show that GCNs improves the effectiveness of modeling the local details of the cloud profile as well as the prediction accuracy by achieving decreased error measures.
</details>
<details>
<summary>摘要</summary>
现在降水预测（几个小时）仍然是一个挑战，因为需要准确地捕捉当地复杂的地方交互。卷积神经网络通过卷积核心与格子数据进行卷积，并提取特征被限制的接受范围内，通常表现为过度平滑的输出与真实值之间的差异。因此它们缺乏模型地方域之间的复杂关系的能力。非ユーク利德学习 targets 非ユーク利德空间中的神经网络模型，这些模型可以更加灵活地定义节点和边，并有效地捕捉地理Grid中的动态空间关系。为了实现这一目标，我们尝试使用非ユーク利德学习基于 temporal Graph Convolutional Network (GCN)  для降水预测。在训练过程中，自动学习 adjacency 矩阵，该矩阵模拟地理Grid 之间的交互，并通过 L1 损失函数与真实值像素值进行比较。然后，GCN 层可以更好地模型地方域之间的空间关系，同时1D 卷积可以提取时间信息。邻居信息被作为辅助输入层提供，以提高最终结果。我们在 Trento/意大利 地区的雷达反射率图序列上测试了模型。结果显示，GCNs 可以更好地模型云Profile 的本地细节以及预测精度，并实现降低误差度量。
</details></li>
</ul>
<hr>
<h2 id="KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks"><a href="#KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks" class="headerlink" title="KD-FixMatch: Knowledge Distillation Siamese Neural Networks"></a>KD-FixMatch: Knowledge Distillation Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05826">http://arxiv.org/abs/2309.05826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien-Chih Wang, Shaoyuan Xu, Jinmiao Fu, Yang Liu, Bryan Wang<br>for: 这个论文的目的是提出一种新的 semi-supervised learning（SSL）算法，以Addressing the challenge of limited labeled data in deep learning。methods: 这个论文使用了一种siamese neural network（SNN），同时采用了知识传播（KD）技术来增强性能和减少性能下降。results: 实验结果表明，KD-FixMatch在四个公开的数据集上都表现比FixMatch更好，实验结果显示KD-FixMatch在训练开始点上有更好的性能，从而导致模型的性能提高。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has become a crucial approach in deep learning as a way to address the challenge of limited labeled data. The success of deep neural networks heavily relies on the availability of large-scale high-quality labeled data. However, the process of data labeling is time-consuming and unscalable, leading to shortages in labeled data. SSL aims to tackle this problem by leveraging additional unlabeled data in the training process. One of the popular SSL algorithms, FixMatch, trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN). However, it is prone to performance degradation when the pseudo labels are heavily noisy in the early training stage. We present KD-FixMatch, a novel SSL algorithm that addresses the limitations of FixMatch by incorporating knowledge distillation. The algorithm utilizes a combination of sequential and simultaneous training of SNNs to enhance performance and reduce performance degradation. Firstly, an outer SNN is trained using labeled and unlabeled data. After that, the network of the well-trained outer SNN generates pseudo labels for the unlabeled data, from which a subset of unlabeled data with trusted pseudo labels is then carefully created through high-confidence sampling and deep embedding clustering. Finally, an inner SNN is trained with the labeled data, the unlabeled data, and the subset of unlabeled data with trusted pseudo labels. Experiments on four public data sets demonstrate that KD-FixMatch outperforms FixMatch in all cases. Our results indicate that KD-FixMatch has a better training starting point that leads to improved model performance compared to FixMatch.
</details>
<details>
<summary>摘要</summary>
深度学习中的半指导学习（SSL）已成为深度学习的一种重要方法，以解决有限的标注数据的挑战。深度神经网络的成功几乎完全取决于大规模高质量的标注数据。然而，数据标注是时间consuming和不可扩展的，导致标注数据的短缺。SSL通过利用额外的无标注数据来解决这个问题，从而提高模型的性能。 FixMatch 是一种流行的 SSL 算法，它使用同一个权重共享教师和学生网络来同时训练 identical 的 Siamese 神经网络（SNN）。然而， FixMatch 在早期训练阶段 pseudo 标签具有很大的噪声，可能导致性能下降。我们提出了 KD-FixMatch，一种新的 SSL 算法，它通过杂合Sequential 和同时训练 SNNs来提高性能并降低性能下降。首先，一个外部 SNN 通过标注和无标注数据进行训练。然后，外部 SNN 网络已经训练好的部分生成 pseudo 标签 для无标注数据，并从中选择一 subset 的无标注数据，通过高信息抽象和深度嵌入划分来生成可信 pseudo 标签。最后，一个内部 SNN 通过标注数据、无标注数据和可信 pseudo 标签进行训练。我们在四个公共数据集上进行了实验，结果表明 KD-FixMatch 在所有情况下都高于 FixMatch。我们的结果表明 KD-FixMatch 具有更好的训练起点，导致模型的性能得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems"><a href="#Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems" class="headerlink" title="Ensemble-based modeling abstractions for modern self-optimizing systems"></a>Ensemble-based modeling abstractions for modern self-optimizing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05823">http://arxiv.org/abs/2309.05823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Töpfer, Milad Abdullah, Tomáš Bureš, Petr Hnětynka, Martin Kruliš</li>
<li>for: 这篇论文旨在扩展DEECo ensemble模型，以便在自动化组件ensemble中使用机器学习和优化规则。</li>
<li>methods: 论文使用机器学习和优化规则在模型层次上捕捉了这些概念，并给出了应用于工业4.0场景中访问控制问题的示例。</li>
<li>results: 论文表明，在智能系统中包含机器学习和优化规则是关键特征，可以让系统在运行时根据环境不确定性进行学习和优化。<details>
<summary>Abstract</summary>
In this paper, we extend our ensemble-based component model DEECo with the capability to use machine-learning and optimization heuristics in establishing and reconfiguration of autonomic component ensembles. We show how to capture these concepts on the model level and give an example of how such a model can be beneficially used for modeling access-control related problem in the Industry 4.0 settings. We argue that incorporating machine-learning and optimization heuristics is a key feature for modern smart systems which are to learn over the time and optimize their behavior at runtime to deal with uncertainty in their environment.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对我们的ensemble-based组件模型DEECo进行扩展，以便在自动化组件集合的建立和重新配置中使用机器学习和优化办法。我们示出了这些概念在模型层面的捕捉方式，并给出了在Industry 4.0设置下模型访问控制相关问题的示例。我们认为，在运行时使用机器学习和优化办法是现代智能系统的关键特征，以便在环境中适应不确定性并优化其行为。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-learning-of-effective-dynamics-for-multiscale-systems"><a href="#Interpretable-learning-of-effective-dynamics-for-multiscale-systems" class="headerlink" title="Interpretable learning of effective dynamics for multiscale systems"></a>Interpretable learning of effective dynamics for multiscale systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05812">http://arxiv.org/abs/2309.05812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Menier, Sebastian Kaltenbach, Mouadh Yagoubi, Marc Schoenauer, Petros Koumoutsakos</li>
<li>for: 本研究旨在提出一种可解释的学习有效动力框架（iLED），以提高高维多Scale系统的模型和 simulations 的精度和可读性。</li>
<li>methods: 该研究基于深度回归神经网络，并结合了Mori-Zwanzig和Koopman运算符理论，以提供可解释的模型和 simulations 结果。</li>
<li>results: 研究在三个 benchmark 高维多Scale系统中的 simulations 中展示了 iLED 框架的可行性和精度，并且可以获得可读的动力学结果。<details>
<summary>Abstract</summary>
The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit of interpretability. The iLED framework is motivated by Mori-Zwanzig and Koopman operator theory, which justifies the choice of the specific architecture. We demonstrate the effectiveness of the proposed framework in simulations of three benchmark multiscale systems. Our results show that the iLED framework can generate accurate predictions and obtain interpretable dynamics, making it a promising approach for solving high-dimensional multiscale systems.
</details>
<details>
<summary>摘要</summary>
高维度多尺度系统的模型和仿真是科学和工程领域的关键挑战。广泛认为，即使今天的计算机技术不断发展，仍然无法解决所有空间时间尺度的方程。这一现实启发了对模型简化技术的激烈尝试。在过去几年，基于深度循环神经网络的技术已经生成了模拟复杂空间时间系统的出色结果，并且可以采用实验和计算数据来扩展模型。但是，神经网络缺乏可读性，这限制了它们在复杂系统中的应用和普遍性。我们提出了一种新的框架——可读性学习有效动力（iLED）框架，它可以与当前最佳的神经网络技术相比，并提供可读性的加值。iLED框架是由莫里- Zwanzig 和库曼操作理论所驱动，这种架构设计是有理由的。我们在三个标准多尺度系统的仿真中展示了iLED框架的效果，结果表明，iLED框架可以生成准确预测和获得可读性的动力学，这使其成为解决高维度多尺度系统的有力的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models"><a href="#Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models"></a>Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05811">http://arxiv.org/abs/2309.05811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Xu, Stella Offner, Robert Gutermuth, Michael Grudic, David Guszejnov, Philip Hopkins</li>
<li>for: 这 paper 是为了量化遮盖环境中的辐射反馈的影响而写的。</li>
<li>methods: 这 paper 使用了深度学习技术，特别是推 diffusion probabilistic models (DDPMs) 来预测遮盖环境中的辐射场强。</li>
<li>results: 这 paper 的结果表明，使用 DDPMs 预测辐射场强可以准确地捕捉遮盖环境中的辐射反馈变化。<details>
<summary>Abstract</summary>
Accurately quantifying the impact of radiation feedback in star formation is challenging. To address this complex problem, we employ deep learning techniques, denoising diffusion probabilistic models (DDPMs), to predict the interstellar radiation field (ISRF) strength based on three-band dust emission at 4.5 \um, 24 \um, and 250 \um. We adopt magnetohydrodynamic simulations from the STARFORGE (STAR FORmation in Gaseous Environments) project that model star formation and giant molecular cloud (GMC) evolution. We generate synthetic dust emission maps matching observed spectral energy distributions in the Monoceros R2 (MonR2) GMC. We train DDPMs to estimate the ISRF using synthetic three-band dust emission. The dispersion between the predictions and true values is within a factor of 0.1 for the test set. We extended our assessment of the diffusion model to include new simulations with varying physical parameters. While there is a consistent offset observed in these out-of-distribution simulations, the model effectively constrains the relative intensity to within a factor of 2. Meanwhile, our analysis reveals weak correlation between the ISRF solely derived from dust temperature and the actual ISRF. We apply our trained model to predict the ISRF in MonR2, revealing a correspondence between intense ISRF, bright sources, and high dust emission, confirming the model's ability to capture ISRF variations. Our model robustly predicts radiation feedback distribution, even in complex, poorly constrained ISRF environments like those influenced by nearby star clusters. However, precise ISRF predictions require an accurate training dataset mirroring the target molecular cloud's unique physical conditions.
</details>
<details>
<summary>摘要</summary>
准确量化辐射反馈在星系形成中的影响是一个复杂的问题。为了解决这个问题，我们使用深度学习技术，即杂流扩散概率模型（DDPM），预测辐射场强度基于3个频率尘埃辐射（4.5μm、24μm和250μm）。我们采用了 magnetohydrodynamic模拟（STARFORGE）项目，模拟星系形成和大分子云（GMC）的发展。我们生成了匹配观测spectral energy distribution的人造尘埃辐射图像。我们使用DDPM进行训练，使其估算辐射场。我们发现在测试集上，模型的误差在0.1的因子范围内。我们对模型进行了进一步的评估，包括在不同物理参数下运行的新模拟。尽管在这些外部 simulate 中出现了一定的偏差，但模型仍能够将辐射场的相对强度限制在2的因子范围内。此外，我们发现尘埃温度 alone 不能准确地预测辐射场。我们应用我们已经训练的模型，预测Monoceros R2（MonR2）星系中的辐射场，发现辐射场的强度与亮度和尘埃辐射之间存在相关性。这表明我们的模型可以准确地预测辐射反馈的分布。然而，精准预测辐射场的精度需要一个准确地反映目标分子云的物理条件的训练集。
</details></li>
</ul>
<hr>
<h2 id="SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors"><a href="#SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors" class="headerlink" title="SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors"></a>SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05810">http://arxiv.org/abs/2309.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongge Chen, Zhao Chen, Gregory P. Meyer, Dennis Park, Carl Vondrick, Ashish Shrivastava, Yuning Chai</li>
<li>for: 提高自动驾驶系统的安全性，检测3D对象的潜在漏洞。</li>
<li>methods: 使用signed distanced function(SDF)表示3D对象，通过权重错误信号来平滑变形对象的形状或姿态，以惑动下游3D检测器。</li>
<li>results: 生成的3D对象与基eline对象有semantic recognizable shape，但物理上有所不同。提供可解释的失败模式，帮助预测3D检测系统中的 potential safety risks。<details>
<summary>Abstract</summary>
We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.
</details>
<details>
<summary>摘要</summary>
我们介绍SHIFT3D，一个可导的管道用于生成3D形状，这些形状具有可能挑战3D物体检测器的结构性可能性。在自动驾驶等安全关键应用中，发现这些新的挑战性对象可以提供对3D检测器的不明之处的见解。通过使用签名距离函数(SDF)表示对象，我们表明了梯度错误信号允许我们平滑地变形或者对3D对象的形状或者姿态进行干扰，以让下游3D检测器困惑。重要的是，SHIFT3D生成的对象与基eline对象有所不同，但它们仍然保留了semantically可识别的形状。我们的方法提供了可解释的失败模式，可以帮助在3D感知系统中预先发现可能的安全隐患，以避免这些隐患在3D检测器失效之前成为重要的故障。
</details></li>
</ul>
<hr>
<h2 id="Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans"><a href="#Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans" class="headerlink" title="Divergences in Color Perception between Deep Neural Networks and Humans"></a>Divergences in Color Perception between Deep Neural Networks and Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05809">http://arxiv.org/abs/2309.05809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan O. Nadler, Elise Darragh-Ford, Bhargav Srinivasa Desikan, Christian Conaway, Mark Chu, Tasker Hull, Douglas Guilbeault</li>
<li>for: 本研究旨在探讨深度神经网络（DNNs）是否能够模型人类视觉，以及DNNs是否能够捕捉人类视觉中的基本特征。</li>
<li>methods: 本研究采用了新的实验方法来评估DNNs中的色彩协调性，并对DNNs的色彩预测结果与人类色彩判断结果进行比较。</li>
<li>results: 研究发现，现今的DNN模型（包括卷积神经网络和视transformer）在处理图像时的色彩预测结果与人类色彩判断结果存在很大差异，特别是对于图像中控制了色彩属性的图像、来自网络搜索的图像和真实世界的CIFAR-10 dataset中的图像。此外，研究还发现了一种可解释性和认知可能性的色彩模型，基于wavelet decomposition，可以更好地预测人类色彩判断结果。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are increasingly proposed as models of human vision, bolstered by their impressive performance on image classification and object recognition tasks. Yet, the extent to which DNNs capture fundamental aspects of human vision such as color perception remains unclear. Here, we develop novel experiments for evaluating the perceptual coherence of color embeddings in DNNs, and we assess how well these algorithms predict human color similarity judgments collected via an online survey. We find that state-of-the-art DNN architectures $-$ including convolutional neural networks and vision transformers $-$ provide color similarity judgments that strikingly diverge from human color judgments of (i) images with controlled color properties, (ii) images generated from online searches, and (iii) real-world images from the canonical CIFAR-10 dataset. We compare DNN performance against an interpretable and cognitively plausible model of color perception based on wavelet decomposition, inspired by foundational theories in computational neuroscience. While one deep learning model $-$ a convolutional DNN trained on a style transfer task $-$ captures some aspects of human color perception, our wavelet algorithm provides more coherent color embeddings that better predict human color judgments compared to all DNNs we examine. These results hold when altering the high-level visual task used to train similar DNN architectures (e.g., image classification versus image segmentation), as well as when examining the color embeddings of different layers in a given DNN architecture. These findings break new ground in the effort to analyze the perceptual representations of machine learning algorithms and to improve their ability to serve as cognitively plausible models of human vision. Implications for machine learning, human perception, and embodied cognition are discussed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-ML-Self-adaptation-in-Face-of-Traps"><a href="#Online-ML-Self-adaptation-in-Face-of-Traps" class="headerlink" title="Online ML Self-adaptation in Face of Traps"></a>Online ML Self-adaptation in Face of Traps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05805">http://arxiv.org/abs/2309.05805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Töpfer, František Plášil, Tomáš Bureš, Petr Hnětynka, Martin Kruliš, Danny Weyns</li>
<li>for: 本研究旨在探讨在智能农业场景中使用在线机器学习自适应系统时遇到的困难和抓子。</li>
<li>methods: 本研究使用在线机器学习来实现自适应机制，并采用了许多方法来评估ML基于估计器的 specification和在线训练的影响。</li>
<li>results: 本研究发现了一些在线机器学习自适应系统中遇到的困难和抓子，包括估计器的规范和在线训练的影响，以及如何评估这些陷阱的方法。<details>
<summary>Abstract</summary>
Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.
</details>
<details>
<summary>摘要</summary>
在线机器学习（ML）经常用于自适应系统，以增强自适应机制并提高系统的用途。尽管如此，将线机器学习应用于自适应可能是问题，而且不多的文献报告了这些问题的限制。我们在实验中将线机器学习应用于智能农业情况下的自适应，并遇到了许多未料的困难--陷阱。在这篇文章中，我们详细讨论了这些陷阱，包括估计器的规格和线上训练、它们对自适应的影响，以及评估估计器的方法。我们的这些陷阱的概述提供了一个列表的教训，可以作为其他研究人员和实践者在应用线机器学习于自适应时的指南。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models"><a href="#Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models" class="headerlink" title="Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models"></a>Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05803">http://arxiv.org/abs/2309.05803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet Singh, Stephen Tu, Vikas Sindhwani</li>
<li>for: 这个论文主要针对的问题是Robot学习管道中的策略表示方式选择，具体来说是使用可能的模型来生成下一个机器人动作的模型。</li>
<li>methods: 这篇论文提出了一种实用的训练目标和算法来训练能量模型（EBM）作为策略表示方式，包括层次抽象强化学习（R-NCE）、可学习负样本和非对抗共同训练等多个关键元素。</li>
<li>results: 论文的实验结果表明，使用提议的训练目标和算法可以训练能量模型作为策略表示方式，并且在多个复杂的多Modalbenchmark中与扩散模型和其他现有方法竞争，甚至超越它们。<details>
<summary>Abstract</summary>
A crucial design decision for any robot learning pipeline is the choice of policy representation: what type of model should be used to generate the next set of robot actions? Owing to the inherent multi-modal nature of many robotic tasks, combined with the recent successes in generative modeling, researchers have turned to state-of-the-art probabilistic models such as diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklore -- that energy models in high dimensional continuous spaces are impractical to train -- is false. We develop a practical training objective and algorithm for energy models which combines several key ingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative samplers, and (iii) non-adversarial joint training. We prove that our proposed objective function is asymptotically consistent and quantify its limiting variance. On the other hand, we show that the Implicit Behavior Cloning (IBC) objective is actually biased even at the population level, providing a mathematical explanation for the poor performance of IBC trained energy policies in several independent follow-up works. We further extend our algorithm to learn a continuous stochastic process that bridges noise and data, modeling this process with a family of EBMs indexed by scale variable. In doing so, we demonstrate that the core idea behind recent progress in generative modeling is actually compatible with EBMs. Altogether, our proposed training algorithms enable us to train energy-based models as policies which compete with -- and even outperform -- diffusion models and other state-of-the-art approaches in several challenging multi-modal benchmarks: obstacle avoidance path planning and contact-rich block pushing.
</details>
<details>
<summary>摘要</summary>
robot学习管道中的一个关键设计决策是选择策略表示方式：用什么类型的模型生成下一个机器人动作？由于许多机器人任务的本质是多模态的，加上近年来的生成模型的成功，研究人员就转向了当今最先进的概率模型，如扩散模型，作为策略表示方式。在这个工作中，我们重新评估了能量模型（EBM）作为策略类型。我们证明了一些人们常见的假设——在高维连续空间中使用能量模型是不实用的——是错误的。我们开发了一个实用的训练目标和算法，该算法结合了多个关键组成部分：（i）排名噪声对比估计（R-NCE），（ii）可学习的负样本，以及（iii）非对抗联合训练。我们证明了我们的提出的目标函数是极限共轭的，并且量化了其极限干扰。相比之下，我们显示了冲击行为塑化（IBC）目标函数实际上偏导向，并提供了一个数学解释，以解释 diffusion models 和其他当前最佳方法在多种独立跟踪工作中的Poor performance。此外，我们还扩展了我们的算法，以学习一个连续随机过程，该过程将噪声和数据相连，并使用一家EBMs索引的扩展。在这样做的过程中，我们证明了生成模型的核心思想和EBMs之间的Compatibility。总之，我们的提出的训练算法可以让我们在多种复杂的多模态benchmark中训练能量模型，与 diffusion models 和其他当前最佳方法竞争，甚至超越它们。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning"><a href="#Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning" class="headerlink" title="Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning"></a>Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05798">http://arxiv.org/abs/2309.05798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yy-ko/cash">https://github.com/yy-ko/cash</a></li>
<li>paper_authors: Yunyong Ko, Hanghang Tong, Sang-Wook Kim</li>
<li>for: 这篇论文旨在预测未知的超页结 (hyperedge)，协助解决实际应用中的许多基本问题 (e.g., 集体推荐)。</li>
<li>methods: 本文提出了一个名为 CASH 的新型超页预测框架，利用具有上下文识别的节点聚合来精确地捕捉节点之间的复杂关系，以及在超页预测中使用自我超vised contrastive learning 来强化超页 Representation。</li>
<li>results: 实验结果显示，CASH 能够在六个真实世界的超页上预测超页的精度高于所有竞争方法，并且每个建议的策略都能够提高 CASH 模型的准确度。<details>
<summary>Abstract</summary>
Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmentation method to fully exploit the latent semantics behind the original hypergraph and consider both node-level and group-level contrasts (i.e., dual contrasts) for better node and hyperedge representations. Extensive experiments on six real-world hypergraphs reveal that CASH consistently outperforms all competing methods in terms of the accuracy in hyperedge prediction and each of the proposed strategies is effective in improving the model accuracy of CASH. For the detailed information of CASH, we provide the code and datasets at: https://github.com/yy-ko/cash.
</details>
<details>
<summary>摘要</summary>
《Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmentation method to fully exploit the latent semantics behind the original hypergraph and consider both node-level and group-level contrasts (i.e., dual contrasts) for better node and hyperedge representations. Extensive experiments on six real-world hypergraphs reveal that CASH consistently outperforms all competing methods in terms of the accuracy in hyperedge prediction and each of the proposed strategies is effective in improving the model accuracy of CASH. For the detailed information of CASH, we provide the code and datasets at: https://github.com/yy-ko/cash.》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models"><a href="#On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models" class="headerlink" title="On the Fine-Grained Hardness of Inverting Generative Models"></a>On the Fine-Grained Hardness of Inverting Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05795">http://arxiv.org/abs/2309.05795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feyza Duman Keles, Chinmay Hegde</li>
<li>for: 本研究主要目标是为Generative模型 inverse提供细致的视野。</li>
<li>methods: 本文使用了几种不同的方法，包括：	+ 几何学方法：使用了几何学方法来研究Generative模型 inverse的计算复杂性。	+ 带紧度推理方法：使用了带紧度推理方法来研究Generative模型 inverse的计算复杂性。</li>
<li>results: 本文的主要结果包括：	+ 提出了一些新的硬度下界，用于描述Generative模型 inverse的计算复杂性。	+ 证明了对于exact inverse，计算复杂性是Ω(2^n)的。	+ 证明了对于approximate inverse，计算复杂性是Ω(2^n)的，当p是正整数时。	+ 提出了一些新的难题，用于描述Generative模型 inverse的计算复杂性。<details>
<summary>Abstract</summary>
The objective of generative model inversion is to identify a size-$n$ latent vector that produces a generative model output that closely matches a given target. This operation is a core computational primitive in numerous modern applications involving computer vision and NLP. However, the problem is known to be computationally challenging and NP-hard in the worst case. This paper aims to provide a fine-grained view of the landscape of computational hardness for this problem. We establish several new hardness lower bounds for both exact and approximate model inversion. In exact inversion, the goal is to determine whether a target is contained within the range of a given generative model. Under the strong exponential time hypothesis (SETH), we demonstrate that the computational complexity of exact inversion is lower bounded by $\Omega(2^n)$ via a reduction from $k$-SAT; this is a strengthening of known results. For the more practically relevant problem of approximate inversion, the goal is to determine whether a point in the model range is close to a given target with respect to the $\ell_p$-norm. When $p$ is a positive odd integer, under SETH, we provide an $\Omega(2^n)$ complexity lower bound via a reduction from the closest vectors problem (CVP). Finally, when $p$ is even, under the exponential time hypothesis (ETH), we provide a lower bound of $2^{\Omega (n)}$ via a reduction from Half-Clique and Vertex-Cover.
</details>
<details>
<summary>摘要</summary>
目标是使用生成模型进行逆转换，以便将生成模型输出与给定的目标匹配。这是现代计算机视觉和自然语言处理中的一个重要计算基础。然而，这个问题已知为计算上具有NP困难的worst-case性。这篇论文的目标是为这个问题提供细腻的视野。我们建立了一些新的困难下界，以确定生成模型逆转换的计算复杂性。在精确的逆转换中，我们的目标是判断给定的目标是否在生成模型的范围内。在STRONG EXPONENTIAL TIME HYPOTHESIS（SETH）下，我们通过 $k$-SAT 的减reduction示出，该问题的计算复杂性为 $\Omega(2^n)$。在更实际上，我们考虑了近似的逆转换问题，即判断模型范围中的一个点是否与给定的目标准确匹配。当 $p$ 是正的奇数时，在 SETH 下，我们提供了 $\Omega(2^n)$ 的下界，via  closest vectors problem（CVP）的减reduction。而当 $p$ 是偶数时，在 EXPONENTIAL TIME HYPOTHESIS（ETH）下，我们提供了 $2^{\Omega(n)}$ 的下界，via Half-Clique 和 Vertex-Cover 的减reduction。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems"><a href="#Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems" class="headerlink" title="Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems"></a>Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05787">http://arxiv.org/abs/2309.05787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Gomaa, Michael Feld</li>
<li>for: 提升人工智能水平，允许自动化系统理解对象和环境更深层次。</li>
<li>methods:  combinatorial 输入和输出功能，以及人类在循环和学习过程中的协作。</li>
<li>results: 提出了一些假设和设计指南，并在相关工作中展示了一个使用情况。<details>
<summary>Abstract</summary>
Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advancing the field of artificial intelligence and enabling autonomous systems to learn like humans. We propose several hypotheses and design guidelines and highlight a use case from related work to achieve this goal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的机器学习进展 Machine learning, particularly deep learning, 已经使得自动系统可以在一种感知和理解对象和环境的层次上进行探测和融合感知数据。这些系统可以现在完成对象检测、感知数据融合和语言理解任务。然而，有一种增长的需求，即使自动系统能够更好地理解对象和环境的概念和符号性。因此，系统必须设计为支持多模态输入和输出，以便支持隐式和显式交互模型。在这篇位点论文中，我们 argued for considering both types of inputs，以及人类在Loop和增量学习技术，以提高人工智能领域的进步和让自动系统学习如人类一样。我们提出了一些假设和设计指南，并高亮了相关工作的一个应用例子，以实现这个目标。
</details></li>
</ul>
<hr>
<h2 id="Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments"><a href="#Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments" class="headerlink" title="Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments"></a>Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05784">http://arxiv.org/abs/2309.05784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadan Golestan, Omid Ardakanian, Pierre Boulanger</li>
<li>for: 本研究旨在提高协助生活空间中的垂直检测、indoor localization和活动识别的可靠性，通过优化感知器的配置和位置。</li>
<li>methods: 我们提出了一种新的、高效的方法，基于灰色Box Bayesian优化和模拟评估来找到高质量的感知器位置在任意indoor空间中。我们的关键技术是利用活动空间特有的领域知识，在 iterative 选择查询点中 capture bayesian 优化中的域特性。</li>
<li>results: 我们在两个 simulations 环境和一个实际数据集中，证明我们的提出的方法比 state-of-the-art 黑色盒 optimize 技术更高效，可以准确地识别人类活动，F1 分数高达 90.1%，并且需要更少（51.3% on average）的贵重函数查询。<details>
<summary>Abstract</summary>
Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function queries.
</details>
<details>
<summary>摘要</summary>
优化感知器的配置和位置对于可靠的落体检测、室内定位和活动识别在助生活空间是关键。我们提出了一种新的、样本效率高的方法，通过灰色 Box  bayesian优化和模拟基于评估来找到高质量感知器的配置。我们的关键技术在于捕捉室内活动的空间分布知识，并将其包含到 Bayesian 优化的迭代选择中。使用两个模拟的室内环境和一个真实世界数据集，我们显示我们的提议方法在比state-of-the-art黑色箱优化技术更高的准确性和活动识别的F1分数，同时也需要明显的下降（51.3%的平均下降）的昂贵函数查询。
</details></li>
</ul>
<hr>
<h2 id="Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning"><a href="#Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning" class="headerlink" title="Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning"></a>Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05777">http://arxiv.org/abs/2309.05777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasunori Yamada, Kaoru Shinkawa, Masatomo Kobayashi, Miyuki Nemoto, Miho Ota, Kiyotaka Nemoto, Tetsuaki Arai<br>for:This study aimed to investigate the use of acoustic features from voice data as objective markers for detecting deficits in everyday functioning in older adults, with the potential for early detection of neurodegenerative diseases such as Alzheimer’s disease.methods:The study used a smartwatch-based application to collect acoustic features during cognitive tasks and daily conversation, and machine learning models were used to detect deficits in everyday functioning.results:The study found that acoustic features from voice data could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than the 68.5% accuracy with standard neuropsychological tests. Common acoustic features were also identified for robustly discriminating deficits in everyday functioning across both types of voice data.<details>
<summary>Abstract</summary>
Detection of subtle deficits in everyday functioning due to cognitive impairment is important for early detection of neurodegenerative diseases, particularly Alzheimer's disease. However, current standards for assessment of everyday functioning are based on qualitative, subjective ratings. Speech has been shown to provide good objective markers for cognitive impairments, but the association with cognition-relevant everyday functioning remains uninvestigated. In this study, we demonstrate the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning. We collected voice data during the performance of cognitive tasks and daily conversation, as possible application scenarios, from 54 older adults, along with a measure of everyday functioning. Machine learning models using acoustic features could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than the 68.5% accuracy with standard neuropsychological tests. We also identified common acoustic features for robustly discriminating deficits in everyday functioning across both types of voice data (cognitive tasks and daily conversation). Our results suggest that common acoustic features extracted from different types of voice data can be used as markers for deficits in everyday functioning.
</details>
<details>
<summary>摘要</summary>
检测轻微功能障碍的重要性在早期检测脑神经疾病，特别是阿尔茨海默病，已有广泛的研究。然而，当前评估日常功能的标准是基于主观的评价。speech已经被证明可以提供好的对象标记器 для认知障碍，但与认知有关的日常功能之间的关系还没有被研究。本研究表明使用智能手表应用程序收集语音特征可以作为对日常功能障碍的对象标记器。我们收集了54名老年人的语音数据，包括认知任务和日常对话，以及一种测量日常功能的指标。机器学习模型使用语音特征可以在68.5%的准确率上检测出日常功能障碍，高于标准神经心理测试的准确率。我们还确定了对日常功能障碍的共同语音特征，可以在不同类型的语音数据中强制性地分类。我们的结果表明，共同的语音特征可以作为日常功能障碍的标记器。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression"><a href="#The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression" class="headerlink" title="The Effect of Intrinsic Dimension on Metric Learning under Compression"></a>The Effect of Intrinsic Dimension on Metric Learning under Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05751">http://arxiv.org/abs/2309.05751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efstratios Palias, Ata Kabán</li>
<li>for: 提高距离算法性能的度量学习</li>
<li>methods: 使用随机压缩数据进行全级度metric学习，并提供了关于误差的理论保证</li>
<li>results: 在高维设置下，经验证明了理论批处理的正确性，并且 автоматиче地紧缩至有利的几何结构存在In English:</li>
<li>for: Improving the performance of distance-based learning algorithms through metric learning</li>
<li>methods: Training a full-rank metric on a randomly compressed version of high-dimensional data</li>
<li>results: Providing theoretical guarantees on the error of distance-based metric learning with respect to the random compression, without assuming any explicit properties of the data other than i.i.d. and bounded support. Experimental results support the theoretical findings on high-dimensional data sets.<details>
<summary>Abstract</summary>
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation"><a href="#CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation" class="headerlink" title="CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation"></a>CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05704">http://arxiv.org/abs/2309.05704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FLC-QU-hep/CaloClouds-2">https://github.com/FLC-QU-hep/CaloClouds-2</a></li>
<li>paper_authors: Erik Buhmann, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown</li>
<li>for: 高精度探测器的快速能量储存模拟，以便未来的加速器实验室中的实验。</li>
<li>methods: 使用生成器机器学习模型（ML）加速和补充传统模拟链，但大多数前一些努力受到固定、定期探测器读取几何的限制。</li>
<li>results: CaloClouds II 模型实现了许多关键改进，包括 Kontinuous time score-based 模型，可以在相同精度下与 CaloClouds 实现 $6\times$ 速度提升，并且可以在单个 CPU 上实现 $5\times$ 速度提升。此外， diffusion 模型被论述为一种准确的探测器模型，可以在单步实现高精度探测器的样本，从而实现 $46\times$ ($37\times$) 速度提升。<details>
<summary>Abstract</summary>
Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).   In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a consistency model allowing for accurate sampling in a single step and resulting in a $46\times$ ($37\times$) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce CaloClouds II, which features a number of key improvements. This includes continuous time score-based modeling, which allows for a 25-step sampling with comparable fidelity to CaloClouds while yielding a 6x speed-up over Geant4 on a single CPU (5x over CaloClouds). We further distill the diffusion model into a consistency model, allowing for accurate sampling in a single step and resulting in a 46x (37x) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality"><a href="#Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality" class="headerlink" title="Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality"></a>Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05702">http://arxiv.org/abs/2309.05702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rak-Kyeong Seong</li>
<li>for: 这 paper 是用于研究 4d N&#x3D;1 维超Symmetric gauge theory 的toric phase的。</li>
<li>methods: 这 paper 使用了不监督学习技术，包括principal component analysis (PCA) 和 t-distributed stochastic neighbor embedding (t-SNE)，来研究 complex structure moduli 的变化对 toric phase 的影响。</li>
<li>results: 这 paper 获得了一个 2-dimensional phase diagram for brane tilings corresponding to the cone over the zeroth Hirzebruch surface F0，以及其相应的Seiberg duality phase boundaries。<details>
<summary>Abstract</summary>
We introduce unsupervised machine learning techniques in order to identify toric phases of 4d N=1 supersymmetric gauge theories corresponding to the same toric Calabi-Yau 3-fold. These 4d N=1 supersymmetric gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold and are realized in terms of a Type IIB brane configuration known as a brane tiling. It corresponds to the skeleton graph of the coamoeba projection of the mirror curve associated to the toric Calabi-Yau 3-fold. When we vary the complex structure moduli of the mirror Calabi-Yau 3-fold, the coamoeba and the corresponding brane tilings change their shape, giving rise to different toric phases related by Seiberg duality. We illustrate that by employing techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), we can project the space of coamoeba labelled by complex structure moduli down to a lower dimensional phase space with phase boundaries corresponding to Seiberg duality. In this work, we illustrate this technique by obtaining a 2-dimensional phase diagram for brane tilings corresponding to the cone over the zeroth Hirzebruch surface F0.
</details>
<details>
<summary>摘要</summary>
我们引入无监控机器学习技术来识别四维N=1瑞利对偶 gauge theory的托立阶段，这些 gauge theory 是 D3- branes 在托立 Calabi-Yau 3-fold 上的世界体理论，并且可以通过 Type IIB  branes 配置来实现。这些配置相应于托立 Calabi-Yau 3-fold 的镜射曲线的对偶图形。当我们变化托立 Calabi-Yau 3-fold 的复素结构参数时，这些对偶图形和相应的 branes 配置会改变形状，从而产生不同的托立阶段，这些阶段相关联系到 Seiberg 对偶。我们使用技术如主成分分析 (PCA) 和 t-分布随机邻接 embedding (t-SNE)，将托立 Calabi-Yau 3-fold 的复素结构参数下的空间投射到一个低维度的阶段空间，这个阶段空间中的边界与 Seiberg 对偶相关。在这个研究中，我们使用这种技术来得到一个二维的阶段图表，它对应于 F0 的托立阶段。
</details></li>
</ul>
<hr>
<h2 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05665">http://arxiv.org/abs/2309.05665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZiwenZhuang/parkour">https://github.com/ZiwenZhuang/parkour</a></li>
<li>paper_authors: Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao</li>
<li>for: 该论文旨在开发一种基于视觉的爬行策略，以便机器人可以在复杂环境中快速跨越各种障碍。</li>
<li>methods: 该论文使用了人工智能的补偿学习方法，通过直接拓展来生成多种爬行技能，包括爬过高障碍物、跃越大坑、跪穿低障碍物、缩进细缝等。</li>
<li>results: 该论文通过将这些爬行技能总结成一个视觉基于的爬行策略，并将其转移到一个四肢动物上，成功地让两个低成本机器人在真实世界环境中自动选择和执行合适的爬行技能。<details>
<summary>Abstract</summary>
Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.
</details>
<details>
<summary>摘要</summary>
园地攀登是一个大型挑战，需要机器人在复杂环境中快速穿越多种障碍。现有方法可以生成 Either 多样化但是盲目的行动技巧或视觉基于的特殊技巧，但是自主园地攀登需要机器人学习通用的技能，能够通过多种情况来见解和应对。在这项工作中，我们提出了一个系统，可以不使用参考动物数据，通过简单的奖励来学习多样化的视觉基于的园地攀登策略。我们开发了一种基于irect collocation的强化学习方法，用于生成园地攀登技能，包括爬上高障碍、跳过大差、蹲下低障碍、缩进窄障碍和跑步。我们将这些技能练习成一个单一的视觉基于的园地攀登策略，并将其传递到一只四足机器人使用其 egocentric depth camera。我们示出了我们的系统可以让两个不同的低成本机器人自主选择和执行适合的园地攀登技能，以快速穿越实际环境中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Hypothesis-Search-Inductive-Reasoning-with-Language-Models"><a href="#Hypothesis-Search-Inductive-Reasoning-with-Language-Models" class="headerlink" title="Hypothesis Search: Inductive Reasoning with Language Models"></a>Hypothesis Search: Inductive Reasoning with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05660">http://arxiv.org/abs/2309.05660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman</li>
<li>for: This paper aims to improve the inductive reasoning ability of large language models (LLMs) by generating explicit hypotheses at multiple levels of abstraction.</li>
<li>methods: The proposed method involves prompting the LLM to propose multiple abstract hypotheses about the problem in natural language, and then implementing the natural language hypotheses as concrete Python programs. The method also includes a middle step to filter the set of hypotheses that will be implemented into programs, either by asking the LLM to summarize into a smaller set of hypotheses or by asking human annotators to select a subset of the hypotheses.</li>
<li>results: The paper demonstrates the effectiveness of the proposed pipeline on the Abstraction and Reasoning Corpus (ARC) visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. The results show that the automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%.<details>
<summary>Abstract</summary>
Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.
</details>
<details>
<summary>摘要</summary>
人类可以通过推理来解决问题，如果给他们一些示例，他们就可以找出下面的原则，然后将其应用到新的情况中。在最近的研究中，人们评估了大型自然语言模型（LLM）在推理任务上的能力，并通过直接提示它们来实现“在上下文学习”。这种方法可以在一些简单的推理任务上工作良好，但在更复杂的任务，如抽象和理解集（ARC）上表现非常差。在这项工作中，我们提出了使得LLM在推理任务上的能力更强的方法：我们会让LLM提出多个层次抽象的假设，然后将这些假设转换成自然语言中的语言表达，最后将其转换成Python程序。这些程序可以直接在观察到的示例上验证，并将其扩展到新的输入。由于现有的LLM生成成的成本过高，我们考虑了一个中间步骤，即使LLM提出的假设集中的一个子集，或者请人工标注员选择一个子集。我们在ARC视觉推理benchmark、其变种1D-ARC和字符串变换集SyGuS上验证了我们的管道的效果。在随机选择ARC中的40个问题上，我们的自动管道使用LLM总结而获得27.5%的准确率，与直接提示基线（准确率为12.5%）相比，有显著的提高。在人工标注员选择LLM生成的候选者的情况下，性能更高，达到37.5%。（我们认为这是我们方法无需筛选的下限）。我们的剖析研究表明，LLM在推理任务上的抽象假设生成和具体程序表示都是有利的。
</details></li>
</ul>
<hr>
<h2 id="On-the-quality-of-randomized-approximations-of-Tukey’s-depth"><a href="#On-the-quality-of-randomized-approximations-of-Tukey’s-depth" class="headerlink" title="On the quality of randomized approximations of Tukey’s depth"></a>On the quality of randomized approximations of Tukey’s depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05657">http://arxiv.org/abs/2309.05657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Briend, Gábor Lugosi, Roberto Imbuzeiro Oliveira</li>
<li>for: 这个论文目的是解决高维度数据中Tukey深度的精确计算是一个困难的问题。</li>
<li>methods: 论文使用随机化方法来近似Tukey深度。</li>
<li>results: 论文证明在一些特定情况下，随机化方法可以准确地 aproximate Tukey深度，但是对于中间深度的点，任何好的approximation都需要对数复杂度。<details>
<summary>Abstract</summary>
Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
</details>
<details>
<summary>摘要</summary>
图基深度（或半空间深度）是多变量数据中广泛使用的中心度量。然而，对高维数据进行准确计算图基深度是一个困难的问题。为了解决这问题，随机化Tukey深度的算法已经被提出。在这篇论文中，我们研究了这些随机算法在图基深度的计算中是否返回良好的 aproximation。我们研究了从Log-凹形分布中采样数据的情况。我们证明，如果要求算法在维度上运行时间为多项式时间，那么随机算法会正确地approximates最大深度为1/2和深度很近于零。然而，对于任何中间深度的点，任何好的approximation都需要无限次复杂度。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands"><a href="#Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands" class="headerlink" title="Dynamic Handover: Throw and Catch with Bimanual Hands"></a>Dynamic Handover: Throw and Catch with Bimanual Hands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05655">http://arxiv.org/abs/2309.05655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang</li>
<li>for:  solves the problem of dynamic handover of objects in robotic systems</li>
<li>methods:  uses Multi-Agent Reinforcement Learning and Sim2Real transfer, with novel algorithm designs such as trajectory prediction models</li>
<li>results:  shows significant improvements over multiple baselines in real-world experiments with diverse objects.<details>
<summary>Abstract</summary>
Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at \url{https://binghao-huang.github.io/dynamic_handover/}.
</details>
<details>
<summary>摘要</summary>
人类常常投掷和捕捉物体，但这种各种动作却对机器人带来了很多挑战：机器人需要在高速下进行动作协作，并且与多种物体进行准确协作。在这篇论文中，我们设计了两个多指手 attachment 到机器人臂，以解决这个问题。我们使用多机器人学习强化学习在模拟环境中训练我们的系统，并进行了实际系统中的Sim2Real传输。为了 bridge 模拟和实际之间的差异，我们提供了多种新的算法设计，包括学习物体的轨迹预测模型。这种模型可以帮助机器人捕手在实时获得物体的运动轨迹，然后根据此进行反应。我们在实际系统中进行了多个物体的实验，并显示了多个基eline的改进。我们的项目页面可以在 \url{https://binghao-huang.github.io/dynamic_handover/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck"><a href="#Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck" class="headerlink" title="Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck"></a>Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05649">http://arxiv.org/abs/2309.05649</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. Michael Martini, Ilya Nemenman</li>
<li>for:  simultaneous compression of two random variables to preserve information between their compressed versions</li>
<li>methods:  Generalized Symmetric Information Bottleneck (GSIB) and exploration of different functional forms of the cost of simultaneous reduction</li>
<li>results:  qualitatively less data required for simultaneous compression compared to compressing variables one at a time, with bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions provided.<details>
<summary>Abstract</summary>
The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
</details>
<details>
<summary>摘要</summary>
symmetric information bottleneck (SIB) 是一种维度减少技术，它同时压缩两个随机变量，以保留它们压缩后的信息之间的关系。我们介绍了通用的 Symmetric Information Bottleneck (GSIB)，它探讨了不同的函数形式，以减少这种同时压缩的成本。然后，我们研究了这种同时压缩的数据集大小要求。我们通过计算涨落函数的上界和方差估计，发现在一般情况下，同时压缩需要更少的数据来达到相同的错误率，相比于一个一个压缩每个输入变量。我们认为这是一种更一般的原理，即同时压缩是独立压缩每个输入变量的更有效的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN"><a href="#A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN" class="headerlink" title="A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)"></a>A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05646">http://arxiv.org/abs/2309.05646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks">https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks</a></li>
<li>paper_authors: Vedanth Ramanathan, Krish Mahadevan, Sejal Dua</li>
<li>For: The paper is written to detect Distributed Denial of Service (DDoS) attacks in network traffic using deep learning techniques.* Methods: The paper employs a novel deep learning-based approach that utilizes Convolutional Neural Networks (CNN) and common deep learning algorithms to classify benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length, and then uses a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to perform binary classification.* Results: The paper achieves an accuracy of .9883 on 2000 unseen flows in network traffic, demonstrating the effectiveness of the proposed algorithm in detecting DDoS attacks. The results also show that the proposed model is scalable for any network environment.Here are the three points in Simplified Chinese text:* For: 这篇论文是用来探讨分布式拒绝服务（DDoS）攻击的网络流量检测方法。* Methods: 该论文使用了一种新的深度学习基于方法，利用卷积神经网络（CNN）和常见的深度学习算法来分类正常和恶意流量。提出的模型从网络流量中提取包流，并将其normal化到固定长度，然后使用自定义的架构 containing layers regulating node dropout, normalization, and a sigmoid activation function来实现二分类。* Results: 该论文在2000个未看过的流量中达到了.9883的准确率，证明了提出的方法在检测DDoS攻击的效果。结果还表明了该方法可扩展到任何网络环境。<details>
<summary>Abstract</summary>
Cybersecurity attacks are becoming increasingly sophisticated and pose a growing threat to individuals, and private and public sectors. Distributed Denial of Service attacks are one of the most harmful of these threats in today's internet, disrupting the availability of essential services. This project presents a novel deep learning-based approach for detecting DDoS attacks in network traffic using the industry-recognized DDoS evaluation dataset from the University of New Brunswick, which contains packet captures from real-time DDoS attacks, creating a broader and more applicable model for the real world. The algorithm employed in this study exploits the properties of Convolutional Neural Networks (CNN) and common deep learning algorithms to build a novel mitigation technique that classifies benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length which is fed into a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to out a binary classification. This allows for the model to process the flows effectively and look for the nodes that contribute to DDoS attacks while dropping the "noise" or the distractors. The results of this study demonstrate the effectiveness of the proposed algorithm in detecting DDOS attacks, achieving an accuracy of .9883 on 2000 unseen flows in network traffic, while being scalable for any network environment.
</details>
<details>
<summary>摘要</summary>
“黑客攻击不断地变得更加复杂和危险，对个人和公共领域的潜在威胁都在增加。分布式拒绝服务（DDoS）攻击是网络上最危险的攻击之一，可以破坏网络服务的可用性。这个项目提出了一个基于深度学习的新方法，用于网络流量中的DDoS攻击探测，使用了新不伯纳瑞大学的DDoS评估数据集，这个数据集包含了实时DDoS攻击的封包捕捉，创造了更加广泛和实用的模型。这个算法利用了卷积神经网络的性能和通用深度学习算法，建立了一个新的防护技术，通过分析网络流量中的封包，分别归类为有害和无害流量。这个提案的模型首先将数据进行处理，提取封包流和对其进行 нор化，然后将其输入到自定义的架构中，这个架构包含了节点排除、normalization和sigmoid活化函数等，以生成一个二分类。这样可以让模型有效地处理流量，寻找对DDoS攻击有贡献的节点，同时忽略“噪音”或“掩蔽”。研究结果显示，提案的算法具有优秀的检测DDoS攻击的精度，在2000个未见的网络流量中，获得了.9883的准确率，同时具有扩展性，适用于任何网络环境。”
</details></li>
</ul>
<hr>
<h2 id="Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets"><a href="#Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets" class="headerlink" title="Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets"></a>Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06538">http://arxiv.org/abs/2309.06538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Mitsuo Akita, Everton Josue da Silva</li>
<li>for: 预测股票市场价格</li>
<li>methods: 使用 iFeel 2.0 平台提取推特上关于 Petrobras 公司的19个情感特征，然后使用 XBoot 模型预测未来股票价格</li>
<li>results: 在250天内，使用模型预测的股票价格比Random Models的平均表现带来了R$88,82（净）的收益<details>
<summary>Abstract</summary>
Training machine learning models for predicting stock market share prices is an active area of research since the automatization of trading such papers was available in real time. While most of the work in this field of research is done by training Neural networks based on past prices of stock shares, in this work, we use iFeel 2.0 platform to extract 19 sentiment features from posts obtained from microblog platform Twitter that mention the company Petrobras. Then, we used those features to train XBoot models to predict future stock prices for the referred company. Later, we simulated the trading of Petrobras' shares based on the model's outputs and determined the gain of R$88,82 (net) in a 250-day period when compared to a 100 random models' average performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用iFeel 2.0平台提取Twitter上关于Petrobras公司的19个情感特征，然后使用这些特征训练XBoot模型预测Petrobras股票价格。后来，我们使用模型的输出进行了Petrobras股票的虚拟交易，并发现在250天内比Random Models的平均性能提高R$88,82（净）。Note: "iFeel 2.0" is a platform for sentiment analysis, and "XBoot" is a machine learning model. "Petrobras" is a company name. "R$" is the symbol for Brazilian real, the currency of Brazil.
</details></li>
</ul>
<hr>
<h2 id="Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling"><a href="#Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling" class="headerlink" title="Boundary Peeling: Outlier Detection Method Using One-Class Peeling"></a>Boundary Peeling: Outlier Detection Method Using One-Class Peeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05630">http://arxiv.org/abs/2309.05630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheikh Arafat, Na Sun, Maria L. Weese, Waldyn G. Martinez</li>
<li>for: 本研究旨在提出一种基于一类支持向量机的无监督异常检测算法，以提高数据分析中异常检测的效率和稳定性。</li>
<li>methods: 该算法使用一类支持向量机生成的轮廓来实现无监督异常检测，并可以通过调整模型的参数来适应不同的数据分布。</li>
<li>results: 对于 synthetic 数据集，该算法在无异常情况下表现更好，而且在异常情况下与标准方法相比，其性能仍然保持在相当高水平。<details>
<summary>Abstract</summary>
Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_text: "Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets."<</SYS>>Here's the translation in Simplified Chinese:一种无监督的异常点检测算法，是数据分析中的一个关键阶段，而且是一个动态的研究领域。一个好的异常点检测算法应该具有计算效率、对参数调整鲁棒性和能够在不同的数据分布下表现一致。我们介绍了一种名为One-Class Boundary Peeling的无监督异常点检测算法。这种算法使用一类支持向量机生成的迭代剥离、灵活的边界，并使用这些边界的平均签名距离来检测异常点。One-Class Boundary Peeling具有良好的参数设置和可以作为一个ensemble方法进行拓展。在synthetic数据 simulations中，One-Class Boundary Peeling在没有异常点时的性能明显超过了所有现有的方法，而且在异常点存在时也可以保持与 benchmark方法相当或更高的性能。在常用的benchmark数据集上，One-Class Boundary Peeling与其他方法相比，在正确分类、AUC和处理时间方面表现竞争性。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Side-Channels-in-Machine-Learning-Systems"><a href="#Privacy-Side-Channels-in-Machine-Learning-Systems" class="headerlink" title="Privacy Side Channels in Machine Learning Systems"></a>Privacy Side Channels in Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05610">http://arxiv.org/abs/2309.05610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</li>
<li>for: 本研究旨在探讨机器学习（ML）模型的隐私保护方法，并发现了系统级别的隐私泄露渠道。</li>
<li>methods: 本研究使用了四种类型的隐私泄露渠道，包括训练数据筛选、输入预处理、输出后处理和查询筛选，这些渠道可以导致模型中的隐私泄露或者新的威胁，如EXTRACTING用户的测试查询。</li>
<li>results: 本研究发现，在应用拥有隐私保证的训练方法时，过滤训练数据可以创造一个副annel，使得任何证明性隐私保证都无效。此外，系统屏蔽语言模型重新生成训练数据的功能可以被恶用，以重建私钥，即使模型没有记忆这些私钥。总之，本研究表明了机器学习隐私保护的综合、端到端分析是必要的。<details>
<summary>Abstract</summary>
Most current approaches for protecting privacy in machine learning (ML) assume that models exist in a vacuum, when in reality, ML models are part of larger systems that include components for training data filtering, output monitoring, and more. In this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. We propose four categories of side channels that span the entire ML lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. For example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. Moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. Taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.
</details>
<details>
<summary>摘要</summary>
现有的隐私保护方法在机器学习（ML）中假设模型存在独立的环境中，而实际上ML模型是更大的系统的一部分，包括训练数据筛选、输出监测和更多的组件。在这项工作中，我们介绍隐私侧频：利用这些系统级别的组件来提取私人信息的攻击。我们提出了四种侧频类型，覆盖了整个ML生命周期（训练数据筛选、输入预处理、输出后处理和查询筛选），并允许扩展证据推理攻击或者新的威胁，如提取用户的测试查询。例如，我们表明了对归一化训练数据前进行减重可以创建一个侧频，完全覆盖任何可证明隐私保证的保证。此外，我们还表明了阻止语言模型重新生成训练数据可以让私钥被私钥提取，即使模型没有记忆这些私钥。总之，我们的结果表明了机器学习的隐私分析应该是结束到终端的，综合考虑系统的所有组件。
</details></li>
</ul>
<hr>
<h2 id="Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models"><a href="#Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models" class="headerlink" title="Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models"></a>Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05605">http://arxiv.org/abs/2309.05605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster</li>
<li>for: 提高大型自然语言模型（LLM）在多步逻辑问题回答中的表现。</li>
<li>methods: 通过targeted memory injection在LLM注意头中注入pertinent prompt-specific信息来pinpoint和修正LLM的多步逻辑问题失败。</li>
<li>results: 通过实验证明，可以通过 injecting pertinent prompt-specific information into a key attention layer during inference, enhance the quality of multi-hop prompt completions, and increase the probability of the desired next token by up to 424%.<details>
<summary>Abstract</summary>
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
</details>
<details>
<summary>摘要</summary>
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.Here's the text in Traditional Chinese:Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias"><a href="#Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias" class="headerlink" title="Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias"></a>Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05589">http://arxiv.org/abs/2309.05589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinath Sai Tripuraneni, Sadia Kamal, Arunkumar Bagavathi</li>
<li>for: 本研究旨在 combat misinformation 和 echo chamber effects 的措施，通过 computationally characterizing political bias。</li>
<li>methods: 我们提出了一种 heuristic 方法，可以分类社交媒体内容为五种不同政治倾向。我们还使用了现有的时间序列预测模型，以 analysing 和比较不同政治意识形态的社交媒体资料。</li>
<li>results: 我们的实验和分析表明，exist 的时间序列预测模型可以对社交媒体上的政治倾向进行预测，并且可以帮助发现不同政治意识形态的社交媒体资料。<details>
<summary>Abstract</summary>
Understanding and mitigating political bias in online social media platforms are crucial tasks to combat misinformation and echo chamber effects. However, characterizing political bias temporally using computational methods presents challenges due to the high frequency of noise in social media datasets. While existing research has explored various approaches to political bias characterization, the ability to forecast political bias and anticipate how political conversations might evolve in the near future has not been extensively studied. In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different political ideologies, specifically Twitter and Gab. Through our experiments and analyses, we seek to shed light on the challenges and opportunities in forecasting political bias in social media platforms. Ultimately, our work aims to pave the way for developing more effective strategies to mitigate the negative impact of political bias in the digital realm.
</details>
<details>
<summary>摘要</summary>
理解和 Mitigating 政治偏见在在线社交媒体平台上是关键的措施，以遏制谣言和复顾室效应。然而，使用计算方法来 temps 政治偏见的问题存在高频噪声在社交媒体数据集中， existing research 已经探讨了多种方法来 Characterizing 政治偏见，但是预测政治偏见和未来政治对话的发展方向尚未得到了广泛的研究。在这篇论文中，我们提出了一种启发式的方法，将社交媒体帖子分为五个不同政治倾向类别。由于没有先前的工作，我们进行了深入的基线模型分析，以确定最适合预测政治倾向时间序列的模型。我们的方法包括在 Twitter 和 Gab 两个社交媒体数据集上使用现有的时间序列预测模型。通过我们的实验和分析，我们希望探讨政治偏见预测的挑战和机遇，以便更好地开发适应社交媒体平台的政治偏见预测模型。最终，我们的工作旨在为数字化时代带来更有效的政治偏见预测和控制策略。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning"><a href="#Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning" class="headerlink" title="Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning"></a>Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05582">http://arxiv.org/abs/2309.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Sebastian Blaes, Cristina Pineri, Georg Martius</li>
<li>for: 管理模型基 reinforcement learning中的风险，通过轨迹采样和概率安全约束，并考虑 epistemic 不确定性和 aleatoric 不确定性的平衡。</li>
<li>methods: 使用概率安全约束和轨迹采样来管理模型基 reinforcement learning 中的风险，并在不确定环境中进行数据驱动MPCapproaches。</li>
<li>results: 实验显示，将不确定性分离是在不确定和安全控制环境中使用数据驱动MPCapproaches时非常重要的。<details>
<summary>Abstract</summary>
We introduce a simple but effective method for managing risk in model-based reinforcement learning with trajectory sampling that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks.Various experiments indicate that the separation of uncertainties is essential to performing well with data-driven MPC approaches in uncertain and safety-critical control environments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet 有效的方法，用于在基于模型的学习控制中管理风险，这种方法包括可能性风险约束和对风险不确定性的平衡。各种实验表明，在数据驱动的MPC方法中，分离不确定性是关键以达到良好的性能。Here's a breakdown of the translation:* 我们 (wǒmen) - we* 提出 (tīchū) - propose* 一种 (yīzhǒng) - a kind of* 简单 (jiǎndān) - simple* 有效 (yǒuxìng) - effective* 方法 (fāngché) - method* 用于 (yòngyù) - for* 管理 (guǎnlǐ) - managing* 风险 (fēngxǐ) - risk* 包括 (bāoxīn) - including* 可能性 (kěnéixìng) - possibility* 风险约束 (fēngxǐjièshì) - probabilistic safety constraints* 对 (duì) - towards* 风险不确定性 (fēngxǐbùjiànshì) - epistemic uncertainty* 平衡 (píngyì) - balance* 各种 (gèzhōng) - various* 实验 (shíyàn) - experiments* 表明 (biǎozhèng) - indicate* 在 (zài) - in* 数据驱动 (shùdào) - data-driven* MPC (MPC) - model predictive control* 方法 (fāngché) - method* 中 (zhōng) - in* uncertainty (bùjiànshì) - uncertainty* 环境 (huánjīng) - environmentI hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations"><a href="#Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations" class="headerlink" title="Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations"></a>Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05575">http://arxiv.org/abs/2309.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Schrader, Joachim Weickert, Michael Krause</li>
<li>for: 这 paper 主要针对的是数值近似二维非对称傅立叶过程中的琐纤细分数问题，以及这些过程在图像分析、物理和工程中的应用。</li>
<li>methods: 作者使用了 splitting 技术来分解二维非对称傅立叶为四个一维傅立叶，从而得到了一个包含一个自由参数的练习类。这个练习类包含了 Weickert et al. (2013) 中的全练习家族，并证明了这两个参数之间存在冗余性。作者还提供了一个 spectral norm 的下界，以保证数值方法的稳定性。</li>
<li>results: 作者通过使用 neural network 库和 GPU 实现了一个高效的并行实现方案，并实现了一个可靠的径向拆分方法。这个方法可以在 Euclidean  нор下保证数值方法的稳定性。<details>
<summary>Abstract</summary>
Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple and highly efficient parallel implementations on GPUs.
</details>
<details>
<summary>摘要</summary>
“散射过程 WITH 散射矩阵在图像分析、物理和工程中具有重要意义。然而，它们的数值近似带来强烈的消耗残差和旋转不变性的偏差。在这个工作中，我们研究了一个大家族的finite difference积分方法，包括一个自由参数。我们通过将2D散射分解为4个1D散射来 derivation 这个stencil家族。结果表明，这个stencil家族包括Weickert et al. (2013)的全stencil家族，并且显示了两个参数之间的重复性。此外，我们提出了一个对stencil矩阵的spectral norm的下界，这个下界给出了稳定性的时间步长限制。我们的方向分解也允许非常自然地将批处理转化为ResNet块。通过使用 neural network 库，我们可以在GPU上实现高效并简单的并行实现。”Note that Simplified Chinese is used here, which is a more common writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="ITI-GEN-Inclusive-Text-to-Image-Generation"><a href="#ITI-GEN-Inclusive-Text-to-Image-Generation" class="headerlink" title="ITI-GEN: Inclusive Text-to-Image Generation"></a>ITI-GEN: Inclusive Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05569">http://arxiv.org/abs/2309.05569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, Fernando De la Torre</li>
<li>for: 这个研究旨在开发一个包容性的文本到图像生成模型，以实现从文本描述中生成具有均匀分布的图像，并且能够减少训练数据中的偏见。</li>
<li>methods: 这个研究使用了一个新的方法，名为ITI-GEN，它使用可用的参考图像来学习一组启发式描述，以生成具有均匀分布的图像。ITI-GEN不需要模型精微调整，因此可以轻松地将现有的文本到图像模型进行扩展。</li>
<li>results: 实验结果显示，ITI-GEN比前一代模型更好地实现从文本描述中生成具有均匀分布的图像，并且可以减少训练数据中的偏见。<details>
<summary>Abstract</summary>
Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that "a picture is worth a thousand words". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt. Project page: https://czhang0528.github.io/iti-gen.
</details>
<details>
<summary>摘要</summary>
文本到图生成模型经常受训练数据的偏见影响，导致特定群体的不平等表现。这项研究探讨了包容型文本到图生成模型，该模型根据人写的提示生成图像，并确保生成图像在想要的特征上具有均匀分布。然而，直接在提示中表达欲表达的特性常导致语言ambiguity或模型误表示，因此这项研究提出了一种截然不同的方法。我们发现，对于一些特性，图像可以更有表达力地表达概念，比如皮肤色调。基于这些发现，我们提出了一种新的方法，名为ITI-GEN，该方法利用可以获得的参考图像为包容型文本到图生成提供了新的思路。我们学习一组提示嵌入，以生成能够有效表示所有欲表达特性类别的图像。此外，ITI-GEN不需要模型精度调整，因此可以efficient地增强现有的文本到图生成模型。我们的实验表明，ITI-GEN在生成包容图像方面具有显著优势。项目页面：https://czhang0528.github.io/iti-gen.
</details></li>
</ul>
<hr>
<h2 id="Distance-Aware-eXplanation-Based-Learning"><a href="#Distance-Aware-eXplanation-Based-Learning" class="headerlink" title="Distance-Aware eXplanation Based Learning"></a>Distance-Aware eXplanation Based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05548">http://arxiv.org/abs/2309.05548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msgun/xbl-d">https://github.com/msgun/xbl-d</a></li>
<li>paper_authors: Misgina Tsighe Hagos, Niamh Belton, Kathleen M. Curran, Brian Mac Namee</li>
<li>for: 这个论文的目的是提出一种基于对话式学习的深度学习模型训练方法，通过与模型的解释交互来提高模型的性能。</li>
<li>methods: 该方法使用了距离感知的解释损失函数，将损失函数与分类损失函数相结合，以训练学习者关注重要的训练集区域。</li>
<li>results: 该方法在三个图像分类任务上进行了评估，并提出了一种新的可解释性度量来评估视觉特征归属模型的性能。<details>
<summary>Abstract</summary>
eXplanation Based Learning (XBL) is an interactive learning approach that provides a transparent method of training deep learning models by interacting with their explanations. XBL augments loss functions to penalize a model based on deviation of its explanations from user annotation of image features. The literature on XBL mostly depends on the intersection of visual model explanations and image feature annotations. We present a method to add a distance-aware explanation loss to categorical losses that trains a learner to focus on important regions of a training dataset. Distance is an appropriate approach for calculating explanation loss since visual model explanations such as Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictly bounded as annotations and their intersections may not provide complete information on the deviation of a model's focus from relevant image regions. In addition to assessing our model using existing metrics, we propose an interpretability metric for evaluating visual feature-attribution based model explanations that is more informative of the model's performance than existing metrics. We demonstrate performance of our proposed method on three image classification tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "eXplanation Based Learning (XBL) is an interactive learning approach that provides a transparent method of training deep learning models by interacting with their explanations. XBL augments loss functions to penalize a model based on deviation of its explanations from user annotation of image features. The literature on XBL mostly depends on the intersection of visual model explanations and image feature annotations. We present a method to add a distance-aware explanation loss to categorical losses that trains a learner to focus on important regions of a training dataset. Distance is an appropriate approach for calculating explanation loss since visual model explanations such as Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictly bounded as annotations and their intersections may not provide complete information on the deviation of a model's focus from relevant image regions. In addition to assessing our model using existing metrics, we propose an interpretability metric for evaluating visual feature-attribution based model explanations that is more informative of the model's performance than existing metrics. We demonstrate performance of our proposed method on three image classification tasks."Translation:<<SYS>> explanation 基本学习（XBL）是一种互动式学习方法，通过与其解释互动来提供透明的深度学习模型训练方法。 XBL 增加损失函数，以惩戒模型的解释与用户图像特征注解的差异。 文献中大多数基于视觉模型解释和图像特征注解的交集。 我们提出了将距离意识导的解释损失添加到分类损失中，以训练学习者专注于训练集中重要的区域。 距离是一个适当的计算解释损失的方法，因为视觉模型解释，如Gradient-weighted Class Activation Mapping（Grad-CAMs），不是精确的注解，它们的交集可能不会提供完整的图像区域的差异信息。 除了使用现有的指标评估我们的模型，我们还提出了一个更加有用的可读性指标，用于评估基于视觉特征归属的模型解释。 我们在三个图像分类任务上展示了我们的提议方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis"><a href="#Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis" class="headerlink" title="Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis"></a>Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05525">http://arxiv.org/abs/2309.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendiqian/GNN4FL">https://github.com/chendiqian/GNN4FL</a></li>
<li>paper_authors: Wenxuan Ye, Chendi Qian, Xueli An, Xueqiang Yan, Georg Carle</li>
<li>for: 本研究旨在提出一种可信的分布式学习架构，以满足6G中Native AI支持的需求。</li>
<li>methods: 该架构使用分布式策略技术和图神经网络，包括三个关键特征：首先，使用同质加密来安全地聚合本地模型，保护个体模型的隐私。其次，利用分布式结构和图神经网络来识别异常本地模型，提高系统安全性。最后，使用分布式技术来归一化系统，选择一个候选人来执行中心服务器的功能。</li>
<li>results: 通过实验 validate the feasibility of the proposed architecture, the results show that the novel architecture can improve the detection of anomalous models and the accuracy of the global model compared to relevant baselines.<details>
<summary>Abstract</summary>
Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions. Additionally, DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger. The feasibility of the novel architecture is validated through simulations, demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines.
</details>
<details>
<summary>摘要</summary>
sixth generation （6G）中的Native AI支持集成到网络架构是一个重要的目标。 Federated Learning（FL）emerges as a potential paradigm，实现分布式AI模型训练 across a diverse range of devices under the coordination of a central server。然而，several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls。 This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology（DLT）and Graph Neural Network（GNN），including three key features。 First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models，preserving the privacy of individual models。Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer，GNN is leveraged to identify abnormal local models，enhancing system security。Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions。Additionally，DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger。The feasibility of the novel architecture is validated through simulations，demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Re-formalization-of-Individual-Fairness"><a href="#Re-formalization-of-Individual-Fairness" class="headerlink" title="Re-formalization of Individual Fairness"></a>Re-formalization of Individual Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05521">http://arxiv.org/abs/2309.05521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toshihiro Kamishima</li>
<li>for: 本研究旨在形式化个人公平的伦理原则，即“对相似的案例进行相似的对待”，这一原则在机器学习上被讨论。</li>
<li>methods: 本研究使用了Dwork等人提出的个人公平形式化，即将不公平空间中的相似对象映射到公平空间中相似的位置。我们提议重新形式化个人公平，通过个人独特性 Conditional statistical independence来实现。</li>
<li>results: 本研究的重新形式化个人公平可以与Dwork等人的形式化相兼容，同时允许与等化概率或充分性的公平观念结合。此外，本研究的形式化可以在预处理、进程处理或后处理阶段应用。<details>
<summary>Abstract</summary>
The notion of individual fairness is a formalization of an ethical principle, "Treating like cases alike," which has been argued such as by Aristotle. In a fairness-aware machine learning context, Dwork et al. firstly formalized the notion. In their formalization, a similar pair of data in an unfair space should be mapped to similar positions in a fair space. We propose to re-formalize individual fairness by the statistical independence conditioned by individuals. This re-formalization has the following merits. First, our formalization is compatible with that of Dwork et al. Second, our formalization enables to combine individual fairness with the fairness notion, equalized odds or sufficiency, as well as statistical parity. Third, though their formalization implicitly assumes a pre-process approach for making fair prediction, our formalization is applicable to an in-process or post-process approach.
</details>
<details>
<summary>摘要</summary>
“个人公平”是一个形式化的道德原则，“对于相似的案例进行相似的对待”，这个原则在 Aristotle 等人的讨论中已经被提出。在一个公平意识的机器学习上下，Dwork 等人首先将这个原则形式化。在他们的形式化中，一对相似的数据在不公平的空间中应该被映射到公平的空间中相似的位置。我们提议重新形式化个人公平，通过个人独特的统计独立性来条件。这个重新形式化具有以下优点：一、我们的形式化与 Dwork 等人的形式化相容。二、我们的形式化可以与平等机会或充分性的公平原则结合。三、他们的形式化假设了预先进行公平预测的前置方法，而我们的形式化则适用于进程或后置方法。
</details></li>
</ul>
<hr>
<h2 id="NExT-GPT-Any-to-Any-Multimodal-LLM"><a href="#NExT-GPT-Any-to-Any-Multimodal-LLM" class="headerlink" title="NExT-GPT: Any-to-Any Multimodal LLM"></a>NExT-GPT: Any-to-Any Multimodal LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05519">http://arxiv.org/abs/2309.05519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/NExT-GPT">https://github.com/kyegomez/NExT-GPT</a></li>
<li>paper_authors: Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua</li>
<li>for: 这个论文的目的是开发一个通用的任意模式任意输入输出的多Modal大语言模型（NExT-GPT），以便模拟人类在不同modalities中的感知和交流方式。</li>
<li>methods: 这个论文使用了一种结合多modal adapter和不同的扩散解码器的端到端总体方法，使得NExT-GPT能够处理输入和生成输出在任意组合的文本、图像、视频和音频modalities之间。它还使用了已经训练过的高性能encoder和decoder，并且只需要一小部分的参数（1%）来调整投影层，以便低成本训练和扩展到更多的modalities。</li>
<li>results: 这个论文通过引入模式转换指令调整（MosIT）和手动精心编辑的高质量多modal数据集，使得NExT-GPT具备了跨modalities的 semantic理解和内容生成能力。总的来说，这个研究表明了在建立人类水平的AI研究中，可以开发一个模型 universal modalities的AI代理，为更多的人类样本提供更多的可能性。<details>
<summary>Abstract</summary>
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/
</details>
<details>
<summary>摘要</summary>
Recently, Multimodal Large Language Models (MM-LLMs) have made significant progress, but they are limited to only understanding input-side multimodality and cannot produce content in multiple modalities. As humans perceive and communicate with the world through various modalities, developing any-to-any MM-LLMs that can accept and deliver content in any modality is essential for human-level AI. To address this gap, we propose an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging existing well-trained and highly-performing encoders and decoders, NExT-GPT is trained with only a small amount of parameters (1% of certain projection layers), which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Our research demonstrates the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: <https://next-gpt.github.io/>
</details></li>
</ul>
<hr>
<h2 id="Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss"><a href="#Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss" class="headerlink" title="Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss"></a>Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05517">http://arxiv.org/abs/2309.05517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Schmidt, Stephan Günnemann</li>
<li>for: 这个论文旨在应用活动学习（Active Learning）技术来对机器学习模型进行训练，以减少需要的标注数据量。</li>
<li>methods: 本论文提出了一种基于时间特性的image流进行Active Learning的方法，即temporal predicted loss（TPL）方法，并在GTA V街道和A2D2街道 dataset上进行了实验评估。</li>
<li>results: 实验结果显示，TPL方法可以对应用于感知应用中的pool-based Active Learning方法，并且在不同的模型上实现了更高的多标示性和更快的训练速度。相比之下，pool-based方法需要2.5个标注点（pp）更多的数据，而且训练速度较慢。<details>
<summary>Abstract</summary>
Active learning (AL) reduces the amount of labeled data needed to train a machine learning model by intelligently choosing which instances to label. Classic pool-based AL requires all data to be present in a datacenter, which can be challenging with the increasing amounts of data needed in deep learning. However, AL on mobile devices and robots, like autonomous cars, can filter the data from perception sensor streams before reaching the datacenter. We exploited the temporal properties for such image streams in our work and proposed the novel temporal predicted loss (TPL) method. To evaluate the stream-based setting properly, we introduced the GTA V streets and the A2D2 streets dataset and made both publicly available. Our experiments showed that our approach significantly improves the diversity of the selection while being an uncertainty-based method. As pool-based approaches are more common in perception applications, we derived a concept for comparing pool-based and stream-based AL, where TPL out-performed state-of-the-art pool- or stream-based approaches for different models. TPL demonstrated a gain of 2.5 precept points (pp) less required data while being significantly faster than pool-based methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs"><a href="#Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs" class="headerlink" title="Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"></a>Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05516">http://arxiv.org/abs/2309.05516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor">https://github.com/intel/neural-compressor</a></li>
<li>paper_authors: Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv</li>
<li>for: 提高大型语言模型（LLMs）的部署效率，解决它们的存储和内存需求问题。</li>
<li>methods: 使用Weight-only quantization，特别是3和4位的Weight-only quantization，并对准确性进行了优化。</li>
<li>results: 提出了一种高效且简洁的方法SignRound，通过使用签名式降降优化，在400步内达到了出色的结果，并与现有的RTN基eline和最新方法竞争。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead. The source code will be publicly available at https://github.com/intel/neural-compressor soon.
</details>
<details>
<summary>摘要</summary>
To optimize the weight rounding task, we propose a concise and highly effective approach called SignRound. Our method utilizes lightweight block-wise tuning with signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead. The source code will be publicly available at <https://github.com/intel/neural-compressor> soon.
</details></li>
</ul>
<hr>
<h2 id="Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning"><a href="#Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning" class="headerlink" title="Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning"></a>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05505">http://arxiv.org/abs/2309.05505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenzebang/centaur-privacy-federated-representation-learning">https://github.com/shenzebang/centaur-privacy-federated-representation-learning</a></li>
<li>paper_authors: Zebang Shen, Jiayuan Ye, Anmin Kang, Hamed Hassani, Reza Shokri</li>
<li>for: 本研究的目的是提高联合学习中的数据隐私保护，特别是在 Federated Learning 中，通过设计一种具有均衡保护和本地个性化的 Representation Federated Learning 目标函数。</li>
<li>methods: 本研究使用了 State-of-the-art 的权限保证算法，以及一种新的权限保证 Algorithm \DPFEDREP，以保证数据隐私。另外，本研究还使用了 Linear Representation Setting 来降低数据隐私风险。</li>
<li>results: 本研究的实验结果表明，使用 \DPFEDREP 算法可以在小于当前最佳比率的情况下，提高联合学习中的数据隐私保护和性能。此外，在 CIFAR10、CIFAR100 和 EMNIST 等图像分类任务上，本研究的方法也得到了显著的性能提升。<details>
<summary>Abstract</summary>
Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget. With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension. We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link: https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>共享参数的重复在联合学习中导致了重要的隐私泄露问题，从而让其主要目的——数据隐私——失效。使用当今最先进的权限减少算法可以减少这种隐私泄露的风险，但这并不免费。随机机制可以防止模型学习到有用的表示函数，特别是当地方模型在分类函数上存在差异（由于数据不同）时。在这篇论文中，我们考虑了一种带有隐私保证的联合学习目标，即让不同方面共同修改共识部分的模型，而不Release 个人化自由。我们证明在线性表示设置下，尽管目标函数不是凸形，但我们提出的新算法\DPFEDREP 在 linear rate 下 convergence to a ball centered around the global optimal solution，并且球体半径与隐私预算reciprocal proportional。通过这种新的实用分析，我们提高了最佳性和隐私之间的质量-价格比例，提高了$d$ 的输入维度。我们通过对 CIFAR10、CIFAR100 和 EMNIST 图像分类任务进行实验，发现我们的方法在同一小隐私预算下显著提高了前一个工作的性能。代码可以在以下链接找到：https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning。
</details></li>
</ul>
<hr>
<h2 id="Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images"><a href="#Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images" class="headerlink" title="Learning Semantic Segmentation with Query Points Supervision on Aerial Images"></a>Learning Semantic Segmentation with Query Points Supervision on Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05490">http://arxiv.org/abs/2309.05490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem</li>
<li>for: 这个研究是为了提高卫星图像Semantic segmentation的效率和准确性，并且仅使用问题点标签进行训练。</li>
<li>methods: 这个研究使用了弱型指导学习算法，将问题点标签扩展到类似含义的superpixels中，然后训练Semantic segmentation模型。</li>
<li>results: 这个研究在一个遥测图像集合上展示了具有竞争力的性能，并且降低了手动标签的时间和成本。<details>
<summary>Abstract</summary>
Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models, supervised with images partially labeled with the superpixels pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort.
</details>
<details>
<summary>摘要</summary>
我们的方法包括以下步骤：首先，我们生成 superpixels，然后将查询点注解扩展到这些 superpixels 中，这些 superpixels 是类似的含义 semantics 的分割。然后，我们训练基于这些 pseudo-labels 的 semantic segmentation 模型。我们在一个飞行图像 dataset 上 benchmark 我们的弱型supervised 训练方法，并使用不同的 semantic segmentation 架构，发现我们可以与完全supervised 训练方法相比，提高效率，同时减少人工注解的努力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes"><a href="#Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes" class="headerlink" title="Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes"></a>Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05477">http://arxiv.org/abs/2309.05477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timsey/npal">https://github.com/timsey/npal</a></li>
<li>paper_authors: Tim Bakker, Herke van Hoof, Max Welling</li>
<li>for: 提高机器学习模型的数据效率</li>
<li>methods: 使用Attentive Conditional Neural Process模型学习活动学习策略，利用活动学习问题的对称性和独立性属性</li>
<li>results: 比较多种基eline的性能，实验显示我们的Neural Process模型在这些设置下表现出色，并且对于不同的分类器表现稳定。<details>
<summary>Abstract</summary>
Pool-based active learning (AL) is a promising technology for increasing data-efficiency of machine learning models. However, surveys show that performance of recent AL methods is very sensitive to the choice of dataset and training setting, making them unsuitable for general application. In order to tackle this problem, the field Learning Active Learning (LAL) suggests to learn the active learning strategy itself, allowing it to adapt to the given setting. In this work, we propose a novel LAL method for classification that exploits symmetry and independence properties of the active learning problem with an Attentive Conditional Neural Process model. Our approach is based on learning from a myopic oracle, which gives our model the ability to adapt to non-standard objectives, such as those that do not equally weight the error on all data points. We experimentally verify that our Neural Process model outperforms a variety of baselines in these settings. Finally, our experiments show that our model exhibits a tendency towards improved stability to changing datasets. However, performance is sensitive to choice of classifier and more work is necessary to reduce the performance the gap with the myopic oracle and to improve scalability. We present our work as a proof-of-concept for LAL on nonstandard objectives and hope our analysis and modelling considerations inspire future LAL work.
</details>
<details>
<summary>摘要</summary>
池化活动学习（AL）是一种有前途的技术，可以提高机器学习模型的数据效率。然而，调查显示，现代AL方法在不同的数据集和训练环境中表现的性能很敏感，使其不适用于通用应用。为解决这个问题，学习活动学习（LAL）领域建议学习活动学习策略自己，让它适应给定的环境。在这个工作中，我们提出了一种基于条件神经过程模型的LAL方法，利用活动学习问题的对称和独立性。我们的方法基于学习一种短视镜，让我们的模型适应不标准目标函数，例如不对所有数据点Error具有相同的权重。我们实验证明，我们的神经过程模型在这些设置下表现出色，超过了多种基elines。最后，我们的实验表明，我们的模型具有随变数据集的稳定性。然而，性能仍然受到选择类ifiers的影响，需要更多的工作来减少与偏斜镜的性能差距，以及提高扩展性。我们的工作作为LAL在非标准目标函数的证明，并希望我们的分析和模型考虑能够激发未来LAL的工作。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-the-dimension-of-a-Fano-variety"><a href="#Machine-learning-the-dimension-of-a-Fano-variety" class="headerlink" title="Machine learning the dimension of a Fano variety"></a>Machine learning the dimension of a Fano variety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05473">http://arxiv.org/abs/2309.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/mldim">https://bitbucket.org/fanosearch/mldim</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这篇论文是关于斐论变体的研究，旨在探讨斐论变体的基本结构和特性。</li>
<li>methods: 作者使用了机器学习技术来分析斐论变体的量子期，并通过对量子期的分析来确定斐论变体的维度。</li>
<li>results: 作者通过实验发现，使用简单的批处理神经网络可以准确地确定斐论变体的维度，并且可以在不知道斐论变体的情况下确定其维度。这些结果表明，机器学习可以从复杂的数学数据中提取结构，并且可以在理论上无法解释的情况下提供正面的证据。<details>
<summary>Abstract</summary>
Fano varieties are basic building blocks in geometry - they are `atomic pieces' of mathematical shapes. Recent progress in the classification of Fano varieties involves analysing an invariant called the quantum period. This is a sequence of integers which gives a numerical fingerprint for a Fano variety. It is conjectured that a Fano variety is uniquely determined by its quantum period. If this is true, one should be able to recover geometric properties of a Fano variety directly from its quantum period. We apply machine learning to the question: does the quantum period of X know the dimension of X? Note that there is as yet no theoretical understanding of this. We show that a simple feed-forward neural network can determine the dimension of X with 98% accuracy. Building on this, we establish rigorous asymptotics for the quantum periods of a class of Fano varieties. These asymptotics determine the dimension of X from its quantum period. Our results demonstrate that machine learning can pick out structure from complex mathematical data in situations where we lack theoretical understanding. They also give positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details>
<details>
<summary>摘要</summary>
We apply machine learning to this question: can the quantum period of X reveal the dimension of X? While there is currently no theoretical understanding of this, we show that a simple feed-forward neural network can accurately determine the dimension of X with 98% accuracy. Building on this, we establish rigorous asymptotics for the quantum periods of a class of Fano varieties, which determine the dimension of X from its quantum period.Our results demonstrate that machine learning can extract structure from complex mathematical data in situations where there is no theoretical understanding. They also provide positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review"><a href="#Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review" class="headerlink" title="Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review"></a>Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05457">http://arxiv.org/abs/2309.05457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Niu, Nian Xue, Christina Pöpper</li>
<li>for: 本研究旨在探讨人工智能在学术安全会议 peer review 中的表现，以及Automated Support Approaches 在这一过程中的潜在优势和局限性。</li>
<li>methods: 本研究使用了 ChatGPT 和 Doc2Vec 模型，以及两个阶段的分类方法，对 Computer Science 会议和 arXiv 预印网站上的 thousands of paper 进行了大规模的数据收集和分析。</li>
<li>results: 研究发现，使用 Doc2Vec 模型和两个阶段的分类方法可以具有高度的预测精度（超过 90%），而 ChatGPT 的预测精度较低。同时，研究还发现了 Automated Support Approaches 在 certain aspects 的局限性和人工智能技术无法匹配的地方。<details>
<summary>Abstract</summary>
Peer review is the method employed by the scientific community for evaluating research advancements. In the field of cybersecurity, the practice of double-blind peer review is the de-facto standard. This paper touches on the holy grail of peer reviewing and aims to shed light on the performance of AI in reviewing for academic security conferences. Specifically, we investigate the predictability of reviewing outcomes by comparing the results obtained from human reviewers and machine-learning models. To facilitate our study, we construct a comprehensive dataset by collecting thousands of papers from renowned computer science conferences and the arXiv preprint website. Based on the collected data, we evaluate the prediction capabilities of ChatGPT and a two-stage classification approach based on the Doc2Vec model with various classifiers. Our experimental evaluation of review outcome prediction using the Doc2Vec-based approach performs significantly better than the ChatGPT and achieves an accuracy of over 90%. While analyzing the experimental results, we identify the potential advantages and limitations of the tested ML models. We explore areas within the paper-reviewing process that can benefit from automated support approaches, while also recognizing the irreplaceable role of human intellect in certain aspects that cannot be matched by state-of-the-art AI techniques.
</details>
<details>
<summary>摘要</summary>
科学社区使用 peer review 方法来评估研究进步。在领域安全方面，双重盲测 peer review 是非正式标准。这篇论文探讨 peer review 的圣杯，旨在探讨 AI 在学术安全会议上进行评审的表现。我们 investigate 评审结果的预测可能性，并将 compare 人工评审者和机器学习模型的结果。为了进行研究，我们建立了全面的数据集，收集了 thousands 篇计算机科学会议和 arXiv 预印website 上的论文。基于收集的数据，我们评估 Doc2Vec 模型和多Stage 分类器的预测能力。我们的实验评估结果表明，使用 Doc2Vec 模型可以实现预测精度高于 90%。我们分析实验结果，并发现 ML 模型在某些方面的优势和局限性。我们探讨文献评审过程中可以通过自动支持方法获得的优势，同时也认可人类智慧在某些方面无法由当前 AI 技术匹配的地方。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation"><a href="#Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation" class="headerlink" title="Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation"></a>Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05455">http://arxiv.org/abs/2309.05455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow</li>
<li>for: The paper is written for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023, which aims to develop systems that can generate human-like co-speech gestures in agents that carry semantic meaning.</li>
<li>methods: The paper proposes a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture to achieve a semantic coupling between these modalities. The CSMP module is used as a conditioning signal in a diffusion-based gesture synthesis model to generate semantically-aware co-speech gestures.</li>
<li>results: The paper reports that the proposed system achieved the highest human-likeness and highest speech appropriateness rating among the submitted entries to the GENEA Challenge 2023, indicating that the system is a promising approach to generating human-like co-speech gestures in agents that carry semantic meaning.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为GENEA（Generation and Evaluation of Non-verbal Behaviour for Embodied Agents）挑战2023所写的，旨在开发能够生成人类化的同时语句姿势的系统。</li>
<li>methods: 论文提出了一种对比式Speech和姿势预训练（CSMP）模块，该模块学习了语音和姿势的共同表示，以实现这两种模式之间的 semantic coupling。CSMP模块作为 diffusion-based 姿势生成模型的conditioning信号，以实现具有 semantic 意义的同时语句姿势生成。</li>
<li>results: 论文报告说，提案的系统在GENEA挑战2023中提交的entries中得到了最高的人类化度和语言相符度评分，表明该系统是一种有前途的方法，可以生成具有 semantic 意义的人类化同时语句姿势。<details>
<summary>Abstract</summary>
This paper describes a system developed for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Our solution builds on an existing diffusion-based motion synthesis model. We propose a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture with the aim to learn a semantic coupling between these modalities. The output of the CSMP module is used as a conditioning signal in the diffusion-based gesture synthesis model in order to achieve semantically-aware co-speech gesture generation. Our entry achieved highest human-likeness and highest speech appropriateness rating among the submitted entries. This indicates that our system is a promising approach to achieve human-like co-speech gestures in agents that carry semantic meaning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning"><a href="#Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning" class="headerlink" title="Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"></a>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05444">http://arxiv.org/abs/2309.05444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, Sara Hooker</li>
<li>for: 这个论文旨在推广 Mixture of Experts（MoE） neural network架构，并在固定计算成本下提高总性能。</li>
<li>methods: 该论文提出了一种EXTREMELY parameter-efficient MoE方法，通过结合 MoE 架构和轻量级专家来实现。</li>
<li>results: 该方法可以在不更新核心模型的情况下，对已有模型进行精细调整，并且可以在不知道先前任务知识的情况下进行泛化。 论文的代码可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/for-ai/parameter-efficient-moe%E3%80%82">https://github.com/for-ai/parameter-efficient-moe。</a><details>
<summary>Abstract</summary>
The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: https://github.com/for-ai/parameter-efficient-moe.
</details>
<details>
<summary>摘要</summary>
难以扩展的混合专家（MoE）是一种广泛使用的神经网络架构，其中一组专业化的子模型共同优化总性性能，而无需增加计算成本。然而，传统的MoE受到存储所有专家的限制，难以扩展到大规模。在这篇论文中，我们推动MoE的限制。我们提出了EXTREMELY parameter-efficient MoE，通过独特地结合MoE架构和轻量级专家来实现。我们的MoE架构超越了标准的参数高效精度调整（PEFT）方法，并与全面精度调整相当，只需更新轻量级专家（占11B参数模型的0.1%以下）。此外，我们的方法能够扩展到未经见过任务，因为它不依赖任何先前任务知识。我们的研究强调了混合专家架构的灵活性，展示它能够在受到严格参数限制的情况下提供稳定性的性能。我们所用的实验代码可以在以下链接获取：https://github.com/for-ai/parameter-efficient-moe。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models"><a href="#Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models" class="headerlink" title="Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models"></a>Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05436">http://arxiv.org/abs/2309.05436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuripsANON2023/QFF">https://github.com/neuripsANON2023/QFF</a></li>
<li>paper_authors: Frederiek Wesel, Kim Batselier</li>
<li>for: 该 paper 的目的是提出一种基于 tensor 网络的kernel machine，以提高模型的通用能力和精度。</li>
<li>methods: 该 paper 使用了归纳映射和 Fourier 特征来提供一种非线性扩展，并利用 tensor 结构来降低模型的维度。</li>
<li>results: 该 paper 实验表明，对于同样的特征，量化模型可以提供更高的VC-维度 bound，而无需额外的计算成本。此外，量化模型还可以增强模型的泛化能力。<details>
<summary>Abstract</summary>
In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on large regression task, achieving state-of-the-art results on a laptop computer.
</details>
<details>
<summary>摘要</summary>
在核心机器学中， polynomial 和 Fourier 特征通常用于提供非线性扩展，将数据映射到更高维度空间。 Unless one considers the dual formulation of the learning problem, which makes exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits tackling high-dimensional problems. One possible approach to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper, we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization, we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on a large regression task, achieving state-of-the-art results on a laptop computer.Here's the translation in Traditional Chinese:在核心机器学中，多项和傅立做特征通常用于提供非线性扩展，将数据映射到更高维度空间。 unless one considers the dual formulation of the learning problem, which makes exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits tackling high-dimensional problems. one possible approach to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper, we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization, we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on a large regression task, achieving state-of-the-art results on a laptop computer.
</details></li>
</ul>
<hr>
<h2 id="A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding"><a href="#A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding" class="headerlink" title="A parameterised model for link prediction using node centrality and similarity measure based on graph embedding"></a>A parameterised model for link prediction using node centrality and similarity measure based on graph embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05434">http://arxiv.org/abs/2309.05434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohui Lu, Shahadat Uddin</li>
<li>for: 预测新形成的网络连接，如疾病预测、社交网络推荐和药物发现等应用。</li>
<li>methods: 提出了一种基于图 neural network 的参数化模型（NCSM），该模型独特地集成了节点中心性和相似度度量作为边特征，有效地利用大网络的 topological 信息。</li>
<li>results: 对五个基准图 dataset 进行了评估，结果显示 NCSM 在不同的维度和数据集上都超过了现有状态的 искусственный neural network 和变量图自动编码器模型，表现出色。这一Exceptional performance 可以归因于 NCSM 的创新性的集成节点中心性、相似度度量和有效地利用 topological 信息。<details>
<summary>Abstract</summary>
Link prediction is a key aspect of graph machine learning, with applications as diverse as disease prediction, social network recommendations, and drug discovery. It involves predicting new links that may form between network nodes. Despite the clear importance of link prediction, existing models have significant shortcomings. Graph Convolutional Networks, for instance, have been proven to be highly efficient for link prediction on a variety of datasets. However, they encounter severe limitations when applied to short-path networks and ego networks, resulting in poor performance. This presents a critical problem space that this work aims to address. In this paper, we present the Node Centrality and Similarity Based Parameterised Model (NCSM), a novel method for link prediction tasks. NCSM uniquely integrates node centrality and similarity measures as edge features in a customised Graph Neural Network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterised GNN-based link prediction model that considers topological information. The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>链接预测是图机器学习的关键方面之一，具有多种应用，如疾病预测、社交网络推荐和药物发现。它涉及预测网络节点之间可能会形成的新链接。虽然链接预测的重要性明显，现有模型却存在重大缺陷。图 convolutional neural networks 例如，在多种数据集上证明了高效性，但在短路网络和我的网络上应用时，它们会遇到严重的限制，导致性能差异。这个问题空间是这项工作的目标。在这篇论文中，我们提出了节点中心性和相似度基于参数化模型（NCSM），这是一种新的链接预测模型。NCSM Uniquely integrates node centrality and similarity measures as edge features in a customized graph neural network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterized GNN-based link prediction model that considers topological information. The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Auditory-Perception-by-Neural-Spiketrum"><a href="#Neuromorphic-Auditory-Perception-by-Neural-Spiketrum" class="headerlink" title="Neuromorphic Auditory Perception by Neural Spiketrum"></a>Neuromorphic Auditory Perception by Neural Spiketrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05430">http://arxiv.org/abs/2309.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huajin Tang, Pengjie Gu, Jayawan Wijekoon, MHD Anas Alsakkal, Ziming Wang, Jiangrong Shen, Rui Yan</li>
<li>for: 用于实现脑内智能的能效计算和稳定学习性能。</li>
<li>methods: 使用适应生物神经系统的 neuromorphic 硬件架构和适应生物神经系统的硬件友好算法，并使用时变整流模型来转换时间变化的整流信号。</li>
<li>results: 实现了减少信息损失和精确控制脉冲率的精炼和高效编码方法，并通过 neuromorphic 耳膜原型实现了算法硬件协设。<details>
<summary>Abstract</summary>
Neuromorphic computing holds the promise to achieve the energy efficiency and robust learning performance of biological neural systems. To realize the promised brain-like intelligence, it needs to solve the challenges of the neuromorphic hardware architecture design of biological neural substrate and the hardware amicable algorithms with spike-based encoding and learning. Here we introduce a neural spike coding model termed spiketrum, to characterize and transform the time-varying analog signals, typically auditory signals, into computationally efficient spatiotemporal spike patterns. It minimizes the information loss occurring at the analog-to-spike transformation and possesses informational robustness to neural fluctuations and spike losses. The model provides a sparse and efficient coding scheme with precisely controllable spike rate that facilitates training of spiking neural networks in various auditory perception tasks. We further investigate the algorithm-hardware co-designs through a neuromorphic cochlear prototype which demonstrates that our approach can provide a systematic solution for spike-based artificial intelligence by fully exploiting its advantages with spike-based computation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing"><a href="#Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing" class="headerlink" title="Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing"></a>Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05686">http://arxiv.org/abs/2309.05686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Sponner, Julius Ott, Lorenzo Servadei, Bernd Waschneck, Robert Wille, Akash Kumar</li>
<li>for: 这篇论文旨在提高资料流程中的深度学习推断效率，并且在有限的嵌入式平台上进行处理。</li>
<li>methods: 本论文提出了一些新的技术，将测量数据流中的时间相关性利用来强化深度学习推断的精确性，并且在执行过程中进行决策。</li>
<li>results: 本论文的结果显示，这些技术可以节省至多26%的运算量每次推断，并且可以与传统优化技术结合使用，以提高资源受限的嵌入式平台上的效率。<details>
<summary>Abstract</summary>
Radar sensors offer power-efficient solutions for always-on smart devices, but processing the data streams on resource-constrained embedded platforms remains challenging. This paper presents novel techniques that leverage the temporal correlation present in streaming radar data to enhance the efficiency of Early Exit Neural Networks for Deep Learning inference on embedded devices. These networks add additional classifier branches between the architecture's hidden layers that allow for an early termination of the inference if their result is deemed sufficient enough by an at-runtime decision mechanism. Our methods enable more informed decisions on when to terminate the inference, reducing computational costs while maintaining a minimal loss of accuracy.   Our results demonstrate that our techniques save up to 26% of operations per inference over a Single Exit Network and 12% over a confidence-based Early Exit version. Our proposed techniques work on commodity hardware and can be combined with traditional optimizations, making them accessible for resource-constrained embedded platforms commonly used in smart devices. Such efficiency gains enable real-time radar data processing on resource-constrained platforms, allowing for new applications in the context of smart homes, Internet-of-Things, and human-computer interaction.
</details>
<details>
<summary>摘要</summary>
雷达感知器提供了功率有效的解决方案，但在资源有限的嵌入式平台上处理数据流仍然是挑战。这篇论文介绍了新的技术，它们利用流动雷达数据中的时间相关性来增强深度学习批处理器的有效性。这些网络添加了在架构中隐藏层之间的额外分支，以实现在运行时决策机制判断是否可以提前终止批处理。我们的方法可以更好地决定 WHEN 终止批处理，降低计算成本，同时保持最小的准确性损失。我们的结果表明，我们的技术可以将批处理操作减少至 26%，相比单exit网络。此外，与信任度基于的早期终止版本相比，我们的技术可以减少操作数至 12%。我们的提议的技术可以在商业硬件上使用，并且可以与传统优化结合使用，使其在资源有限的嵌入式平台上可用。这种效率提升允许实时雷达数据处理，开启了新的应用场景，如智能家居、物联网和人机交互。
</details></li>
</ul>
<hr>
<h2 id="Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing"><a href="#Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing" class="headerlink" title="Learning noise-induced transitions by multi-scaling reservoir computing"></a>Learning noise-induced transitions by multi-scaling reservoir computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05413">http://arxiv.org/abs/2309.05413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zequn Lin, Zhaofan Lu, Zengru Di, Ying Tang</li>
<li>for: 本研究旨在通过机器学习模型，具体是流体计算，提取时间序列数据中的随机过程，包括随机过程的转移和稳定状态之间的关系。</li>
<li>methods: 该研究使用了流体计算，一种类型的循环神经网络，来学习吸收噪声所致的转移。研究人员还提出了一种简洁的训练协议，以便调整模型的超参数，特别是控制模型动力学的时间尺度。</li>
<li>results: 研究人员通过使用流体计算模型，可以准确地预测时间序列数据中的转移时间和转移次数的统计。此外，模型还能够捕捉到多稳定系统中的转移，以及非详细平衡引起的旋转动力学。例如，对蛋白质折叠实验数据进行预测，可以获得转移时间的统计。研究结果表明，机器学习方法可以准确地捕捉到噪声引起的随机过程。<details>
<summary>Abstract</summary>
Noise is usually regarded as adversarial to extract the effective dynamics from time series, such that the conventional data-driven approaches usually aim at learning the dynamics by mitigating the noisy effect. However, noise can have a functional role of driving transitions between stable states underlying many natural and engineered stochastic dynamics. To capture such stochastic transitions from data, we find that leveraging a machine learning model, reservoir computing as a type of recurrent neural network, can learn noise-induced transitions. We develop a concise training protocol for tuning hyperparameters, with a focus on a pivotal hyperparameter controlling the time scale of the reservoir dynamics. The trained model generates accurate statistics of transition time and the number of transitions. The approach is applicable to a wide class of systems, including a bistable system under a double-well potential, with either white noise or colored noise. It is also aware of the asymmetry of the double-well potential, the rotational dynamics caused by non-detailed balance, and transitions in multi-stable systems. For the experimental data of protein folding, it learns the transition time between folded states, providing a possibility of predicting transition statistics from a small dataset. The results demonstrate the capability of machine-learning methods in capturing noise-induced phenomena.
</details>
<details>
<summary>摘要</summary>
噪声通常被视为时间序列数据中的障碍物，以致传统的数据驱动方法通常努力于减少噪声的影响。然而，噪声可以扮演一个功能性的角色，即驱动多种自然和工程化的随机动力。为捕捉数据中的随机转换，我们发现可以利用机器学习模型，即激流计算作为一种循环神经网络，学习噪声引起的转换。我们开发了一种简洁的训练协议，专注于控制激流动力的时间尺度的关键参数。训练后的模型可以准确地计算转换时间和转换次数的统计。这种方法适用于广泛的系统，包括一个下降征 Double-well 潜在能量下的二stable系统，以及白噪声和颜色噪声。它还能够考虑下降征的非细节平衡、旋转动力学和多stable系统中的转换。对蛋白质折叠的实验数据，它学习了转换时间 между折叠态，提供了预测转换统计信息的可能性。结果表明机器学习方法可以捕捉噪声引起的现象。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions"><a href="#Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions" class="headerlink" title="Physics-informed reinforcement learning via probabilistic co-adjustment functions"></a>Physics-informed reinforcement learning via probabilistic co-adjustment functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05404">http://arxiv.org/abs/2309.05404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nat Wannawas, A. Aldo Faisal</li>
<li>for: 这个论文的目的是提出一种数据效率的强化学习方法，用于训练基于实际世界任务的系统。</li>
<li>methods: 该方法使用了两种新的方法：协同拟合调整（CKA）和ridge regression调整（RRA），将数据集和模拟模型结合起来，以提高模型的准确性和不确定性评估。</li>
<li>results: 研究发现，使用CKA和RRA可以获得更高的模型准确性和不确定性评估，并且可以在实际世界中实现数据效率的强化学习。<details>
<summary>Abstract</summary>
Reinforcement learning of real-world tasks is very data inefficient, and extensive simulation-based modelling has become the dominant approach for training systems. However, in human-robot interaction and many other real-world settings, there is no appropriate one-model-for-all due to differences in individual instances of the system (e.g. different people) or necessary oversimplifications in the simulation models. This requires two approaches: 1. either learning the individual system's dynamics approximately from data which requires data-intensive training or 2. using a complete digital twin of the instances, which may not be realisable in many cases. We introduce two approaches: co-kriging adjustments (CKA) and ridge regression adjustment (RRA) as novel ways to combine the advantages of both approaches. Our adjustment methods are based on an auto-regressive AR1 co-kriging model that we integrate with GP priors. This yield a data- and simulation-efficient way of using simplistic simulation models (e.g., simple two-link model) and rapidly adapting them to individual instances (e.g., biomechanics of individual people). Using CKA and RRA, we obtain more accurate uncertainty quantification of the entire system's dynamics than pure GP-based and AR1 methods. We demonstrate the efficiency of co-kriging adjustment with an interpretable reinforcement learning control example, learning to control a biomechanical human arm using only a two-link arm simulation model (offline part) and CKA derived from a small amount of interaction data (on-the-fly online). Our method unlocks an efficient and uncertainty-aware way to implement reinforcement learning methods in real world complex systems for which only imperfect simulation models exist.
</details>
<details>
<summary>摘要</summary>
现实世界中的强化学习任务很数据不效率，广泛采用模拟基本的方法进行系统训练。然而，在人机交互和许多现实世界中的情况下，没有适合一个模型所有的选择，因为系统实例之间存在差异（例如不同的人）或者模拟模型中需要忽略一些细节。这要求两种方法：1. either从数据中学习个体系统的动力学特性，需要大量的数据训练；2. 使用完整的数字双方案，但在许多情况下可能不可能实现。我们介绍两种新的方法：协同拟合调整（CKA）和ridge regression调整（RRA），这两种方法可以结合数据和模拟的优点，并快速适应个体实例（例如人体生物力学）。使用CKA和RRA，我们可以获得更加准确的整体系统动力学的不确定性评估，比洁GP基础和AR1方法更加高效。我们通过一个可解释的强化学习控制示例来说明CKA的效果，我们通过只使用小量的互动数据（在线部分）和CKA来学习控制一个生物力学人工臂。我们的方法可以帮助实现现实世界中复杂系统中的强化学习方法，只有不准确的模拟模型存在。
</details></li>
</ul>
<hr>
<h2 id="Practical-Homomorphic-Aggregation-for-Byzantine-ML"><a href="#Practical-Homomorphic-Aggregation-for-Byzantine-ML" class="headerlink" title="Practical Homomorphic Aggregation for Byzantine ML"></a>Practical Homomorphic Aggregation for Byzantine ML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05395">http://arxiv.org/abs/2309.05395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Choffrut, Rachid Guerraoui, Rafael Pinot, Renaud Sirdey, John Stephan, Martin Zuber</li>
<li>for: 该论文旨在提出一种完全卷积的分布式学习算法，以保护数据隐私和抵御 Byzantine 攻击。</li>
<li>methods: 该算法使用了一种新的普通文本编码方法，使得可以实现robust aggregator在批处理-Friendly BGV 上进行实现。此外，这种编码方法还可以加速当前的同样安全大小的排序。</li>
<li>results: 该算法在图像分类任务上进行了广泛的实验，并显示了实际执行时间和与非私有版本相匹配的机器学习性能。<details>
<summary>Abstract</summary>
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. However, due to HE's large computational demand especially for high-dimensional ML models, there has not yet been any attempt to design purely homomorphic operators for non-linear robust aggregators. In this work, we present SABLE, the first completely homomorphic and Byzantine robust distributed learning algorithm. SABLE essentially relies on a novel plaintext encoding method that enables us to implement the robust aggregator over batching-friendly BGV. Moreover, this encoding scheme also accelerates state-of-the-art homomorphic sorting with larger security margins and smaller ciphertext size. We perform extensive experiments on image classification tasks and show that our algorithm achieves practical execution times while matching the ML performance of its non-private counterpart.
</details>
<details>
<summary>摘要</summary>
因为大规模数据的可用性，机器学习（ML）算法在分布式架构中被部署，其中不同的节点协作训练 ML 模型，通过交换模型相关信息（如梯度）与中央服务器进行交互。然而，分布式学习方案受到两种威胁。首先，拜占庭节点可以单手损害学习，通过向服务器发送错误信息，如误差梯度。标准的应对方法是使用非线性Robust Aggregation方法。第二，服务器可以违反节点的隐私。 latest attacks 表明，在交换（未加密）梯度时，服务器可以恢复所有节点的数据。为保护隐私，homomorphic encryption（HE），一种金标准安全 primitives，已经广泛研究在分布式学习的非拜占庭场景中。然而，由于 HE 的计算复杂度，特别是高维 ML 模型，没有任何尝试设计纯正Homomorphic Operator。在这种情况下，我们提出了 SABLE，首个完全Homomorphic和拜占庭Robust分布式学习算法。SABLE 基本上依靠一种新的普通文本编码方法，使我们可以实现Robust Aggregator在批处理友好 BGV 上。此外，这种编码方案还加速了当前的 homomorphic sorting ，具有更大的安全优势和更小的 ciphertext size。我们对图像分类任务进行了广泛的实验，并证明了我们的算法可以实现实际执行时间，与非隐私 counterpart 的 ML 性能相匹配。
</details></li>
</ul>
<hr>
<h2 id="Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach"><a href="#Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach" class="headerlink" title="Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach"></a>Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05391">http://arxiv.org/abs/2309.05391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Avlonitis, Dor Lavi, Masoud Mansoury, David Graus</li>
<li>for: 这个研究旨在使用强化学习算法提高职业规划过程。</li>
<li>methods: 研究使用了Markov Decision Process（MDP）形式表示荷兰就业市场，并使用机器学习算法such as Sarsa, Q-Learning, and A2C来学习优化员工的长期收入轨迹。</li>
<li>results: 研究结果表明，使用RL模型（特别是Q-Learning和Sarsa）可以提高员工的收入轨迹，平均提高5%compared to observed career paths。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This study explores the potential of reinforcement learning algorithms to enhance career planning processes. Leveraging data from Randstad The Netherlands, the study simulates the Dutch job market and develops strategies to optimize employees' long-term income. By formulating career planning as a Markov Decision Process (MDP) and utilizing machine learning algorithms such as Sarsa, Q-Learning, and A2C, we learn optimal policies that recommend career paths with high-income occupations and industries. The results demonstrate significant improvements in employees' income trajectories, with RL models, particularly Q-Learning and Sarsa, achieving an average increase of 5% compared to observed career paths. The study acknowledges limitations, including narrow job filtering, simplifications in the environment formulation, and assumptions regarding employment continuity and zero application costs. Future research can explore additional objectives beyond income optimization and address these limitations to further enhance career planning processes.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Chinese that uses simpler grammar and vocabulary, making it easier to understand for speakers of Mandarin Chinese. However, the translation may not be exact, as the nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory"><a href="#Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory" class="headerlink" title="Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory"></a>Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05386">http://arxiv.org/abs/2309.05386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan C. Schulze, Danimir T. Doncevic, Nils Erwes, Alexander Mitsos</li>
<li>for: 这个论文的目的是提出一种基于Koopman理论的数据驱动模型减少策略，用于实现非线性预测控制（NMPC）的实时能力。</li>
<li>methods: 该论文使用了Koopman理论基于自编码器和线性隐藏动力学的数据驱动模型减少策略，并结合了非线性预测控制（NMPC）的实现。</li>
<li>results: 该论文的实验结果表明，使用该数据驱动模型减少策略和NMPC实现，可以在ASU（气体分离器）中实现98%的CPU时间减少，实现实时NMPC控制。<details>
<summary>Abstract</summary>
Achieving real-time capability is an essential prerequisite for the industrial implementation of nonlinear model predictive control (NMPC). Data-driven model reduction offers a way to obtain low-order control models from complex digital twins. In particular, data-driven approaches require little expert knowledge of the particular process and its model, and provide reduced models of a well-defined generic structure. Herein, we apply our recently proposed data-driven reduction strategy based on Koopman theory [Schulze et al. (2022), Comput. Chem. Eng.] to generate a low-order control model of an air separation unit (ASU). The reduced Koopman model combines autoencoders and linear latent dynamics and is constructed using machine learning. Further, we present an NMPC implementation that uses derivative computation tailored to the fixed block structure of reduced Koopman models. Our reduction approach with tailored NMPC implementation enables real-time NMPC of an ASU at an average CPU time decrease by 98 %.
</details>
<details>
<summary>摘要</summary>
实现实时能力是非Linear Model Predictive Control（NMPC）的industrial化先要有一个重要的前提条件。数据驱动模型减少提供了一种获取低阶控制模型的复杂数字孪生。特别是，数据驱动方法不需要对特定过程和其模型的专家知识，并提供了一种具有定制结构的减少模型。在这种情况下，我们使用我们最近提出的数据驱动减少策略，基于Koopman理论([Schulze et al., 2022，Comput. Chem. Eng.])，来生成一个低阶控制模型。这个减少模型结合了自动编码器和线性隐动动力，通过机器学习构建。此外，我们还提供了适应于固定块结构减少Koopman模型的导函数计算，以实现NMPC实现。我们的减少方法与适应NMPC实现使得ASU的实时NMPC可以在CPU时间减少98%。
</details></li>
</ul>
<hr>
<h2 id="Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation"><a href="#Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation" class="headerlink" title="Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation"></a>Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05361">http://arxiv.org/abs/2309.05361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengshuo Shen, Wei Zheng, Bihao Guo, Dalong Chen, Xinkun Ai, Fengming Xue, Yu Zhong, Nengchao Wang, Biao Shen, Binjia Xiao, Yonghua Ding, Zhongyong Chen, Yuan Pan, J-TEXT team<br>for: 这个研究旨在预测未来tokamak中的干扰，并且使用了域对对映算法CORAL。methods: 这个研究使用了域对对映算法CORAL，并且将目标领域（未来tokamak）的少量数据与来源领域（现有tokamak）的大量数据进行了对映，以训练机器学习模型。results: 这个研究发现，使用supervised CORAL可以增强未来tokamak中的干扰预测性能（AUC值由0.764提高至0.890），并且通过可解释分析发现，使用supervised CORAL可以让数据分布更加类似未来tokamak。<details>
<summary>Abstract</summary>
The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak only using a few discharges based on a domain adaptation algorithm called CORAL. It is the first attempt at applying domain adaptation in the disruption prediction task. In this paper, this disruption prediction approach aligns a few data from the future tokamak (target domain) and a large amount of data from the existing tokamak (source domain) to train a machine learning model in the existing tokamak. To simulate the existing and future tokamak case, we selected J-TEXT as the existing tokamak and EAST as the future tokamak. To simulate the lack of disruptive data in future tokamak, we only selected 100 non-disruptive discharges and 10 disruptive discharges from EAST as the target domain training data. We have improved CORAL to make it more suitable for the disruption prediction task, called supervised CORAL. Compared to the model trained by mixing data from the two tokamaks, the supervised CORAL model can enhance the disruption prediction performance for future tokamaks (AUC value from 0.764 to 0.890). Through interpretable analysis, we discovered that using the supervised CORAL enables the transformation of data distribution to be more similar to future tokamak. An assessment method for evaluating whether a model has learned a trend of similar features is designed based on SHAP analysis. It demonstrates that the supervised CORAL model exhibits more similarities to the model trained on large data sizes of EAST. FTDP provides a light, interpretable, and few-data-required way by aligning features to predict disruption using small data sizes from the future tokamak.
</details>
<details>
<summary>摘要</summary>
“高的探索成本和未来tokamak中需要干扰发生的需求导致了探索预测研究中的悖论。在这篇研究中，我们提出了一种新的方法来预测未来tokamak中的干扰，只使用了几次探索。我们使用了域化适应算法CORAL，这是在探索预测任务中的首次应用。在这篇研究中，我们将未来tokamak中的探索训练数据和现有tokamak中的大量数据进行了对接，以训练一个机器学习模型。为了模拟现有和未来tokamak的情况，我们选择了J-TEXT作为现有tokamak，而EAST则作为未来tokamak。为了模拟未来tokamak中缺乏干扰数据的情况，我们仅选择了EAST中的100次非干扰探索和10次干扰探索作为目标领域训练数据。我们将CORAL更新为适合探索预测任务，称为监督式CORAL。相比于将数据从两个tokamak混合训练的模型，监督式CORAL模型可以对未来tokamak的干扰预测表现更好（AUC值由0.764提高至0.890）。通过可解析分析，我们发现使用监督式CORAL可以让数据分布变得更加相似。我们还设计了一个根据SHAP分析的评估方法，以验证模型是否学习了类似特征的趋势。结果显示监督式CORAL模型在SHAP分析中表现更加相似于在大量EAST数据上训练的模型。FTDP提供了一个轻量、可解析且需要少量数据的方法，通过将特征转换为预测干扰的方法。”
</details></li>
</ul>
<hr>
<h2 id="EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection"><a href="#EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection" class="headerlink" title="EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection"></a>EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05357">http://arxiv.org/abs/2309.05357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edac-ml4h/edac-ml4h">https://github.com/edac-ml4h/edac-ml4h</a></li>
<li>paper_authors: Andrej Jovanović, Mario Mihaly, Lennon Donaldson</li>
<li>for: 本研究旨在开发一种可靠、可deploy的预creening方法，用于检测COVID-19病毒的存在。</li>
<li>methods: 本研究使用机器学习方法，利用CT扫描和喘音记录等输入特征，通过深度神经网络架构实现高度准确的预测。</li>
<li>results: 研究人员通过网络剪辑和量化来压缩两个模型，实现了模型文件尺寸的压缩和减少计算时间，同时保持模型的预测性能。 specifically, 研究人员实现了一个105.76倍和一个19.34倍的压缩比例，以及相应的1.37倍和1.71倍的计算时间减少。<details>
<summary>Abstract</summary>
The global spread of COVID-19 had severe consequences for public health and the world economy. The quick onset of the pandemic highlighted the potential benefits of cheap and deployable pre-screening methods to monitor the prevalence of the disease in a population. Various researchers made use of machine learning methods in an attempt to detect COVID-19. The solutions leverage various input features, such as CT scans or cough audio signals, with state-of-the-art results arising from deep neural network architectures. However, larger models require more compute; a pertinent consideration when deploying to the edge. To address this, we first recreated two models that use cough audio recordings to detect COVID-19. Through applying network pruning and quantisation, we were able to compress these two architectures without reducing the model's predictive performance. Specifically, we were able to achieve an 105.76x and an 19.34x reduction in the compressed model file size with corresponding 1.37x and 1.71x reductions in the inference times of the two models.
</details>
<details>
<summary>摘要</summary>
全球COVID-19疫情的严重性对公共卫生和世界经济造成了严重的影响。快速蔓延的疫情推祟了使用便宜可部署的预卷方法来监测人口中疫苗的存在。各种研究人员使用机器学习方法来探索COVID-19的检测方法。这些解决方案利用了不同的输入特征，如CT扫描或喊喊声信号，并通过深度神经网络架构实现了国际一流的检测效果。然而，更大的模型需要更多的计算资源，这是在部署到边缘时需要考虑的。为此，我们首先重建了两个使用喊喊声记录来检测COVID-19的模型。通过网络剪辑和量化，我们成功地压缩了这两个架构，而无需降低模型的预测性能。具体来说，我们实现了一个105.76倍和一个19.34倍的压缩模型文件大小减少，同时对两个模型的执行时间也实现了1.37倍和1.71倍的减少。
</details></li>
</ul>
<hr>
<h2 id="Neural-Discovery-of-Permutation-Subgroups"><a href="#Neural-Discovery-of-Permutation-Subgroups" class="headerlink" title="Neural Discovery of Permutation Subgroups"></a>Neural Discovery of Permutation Subgroups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05352">http://arxiv.org/abs/2309.05352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Karjol, Rohan Kashyap, Prathosh A P</li>
<li>for: 本文研究了找到 permutation group $S_{n}$ 中的子群 $H$。 unlike traditional $H$-invariant networks, 我们提出了一种方法，通过学习 $S_{n}$-invariant function 和线性变换，找到 underlying subgroup。</li>
<li>methods: 我们使用了 $S_{n}$-invariant function 和线性变换来找到 subgroup $H$。我们也证明了类似结论对征群 $S_{k} (k \leq n)$ 和征子群 $C_{n}$、$D_{n}$ 成立。</li>
<li>results: 我们的结果表明，可以通过学习 $S_{n}$-invariant function 和线性变换，找到任何类型的 subgroup $H$。我们还提供了一般化的定理，可以扩展到其他 $S_{n}$ 中的 subgroup。 数据图像权重和对称多项式回归任务中的实验结果表明了我们的方法的可行性。<details>
<summary>Abstract</summary>
We consider the problem of discovering subgroup $H$ of permutation group $S_{n}$. Unlike the traditional $H$-invariant networks wherein $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. Our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. We also prove similar results for cyclic and dihedral subgroups. Finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. We also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.
</details>
<details>
<summary>摘要</summary>
我们考虑找到 permutation group $S_{n}$ 中的子群 $H$ 的问题。 unlike traditional $H$-invariant networks, where $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. we also prove similar results for cyclic and dihedral subgroups. finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. we also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.Here's the translation in Traditional Chinese:我们考虑找到 permutation group $S_{n}$ 中的子群 $H$ 的问题。 unlike traditional $H$-invariant networks, where $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. we also prove similar results for cyclic and dihedral subgroups. finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. we also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.
</details></li>
</ul>
<hr>
<h2 id="Learning-Geometric-Representations-of-Objects-via-Interaction"><a href="#Learning-Geometric-Representations-of-Objects-via-Interaction" class="headerlink" title="Learning Geometric Representations of Objects via Interaction"></a>Learning Geometric Representations of Objects via Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05346">http://arxiv.org/abs/2309.05346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reichlin/geomrepobj">https://github.com/reichlin/geomrepobj</a></li>
<li>paper_authors: Alfredo Reichlin, Giovanni Luca Marchetti, Hang Yin, Anastasiia Varava, Danica Kragic</li>
<li>for: 学习 scene 中 agent 和外部对象的表示</li>
<li>methods: 基于 agent 行为作为监督，提取 Physical space 中 agent 和对象的位置</li>
<li>results: 比对 vision-based 方法高效，解耦 agent 和对象，并在下游任务中efficient 地使用 reinforcement learning 解决问题<details>
<summary>Abstract</summary>
We address the problem of learning representations from observations of a scene involving an agent and an external object the agent interacts with. To this end, we propose a representation learning framework extracting the location in physical space of both the agent and the object from unstructured observations of arbitrary nature. Our framework relies on the actions performed by the agent as the only source of supervision, while assuming that the object is displaced by the agent via unknown dynamics. We provide a theoretical foundation and formally prove that an ideal learner is guaranteed to infer an isometric representation, disentangling the agent from the object and correctly extracting their locations. We evaluate empirically our framework on a variety of scenarios, showing that it outperforms vision-based approaches such as a state-of-the-art keypoint extractor. We moreover demonstrate how the extracted representations enable the agent to solve downstream tasks via reinforcement learning in an efficient manner.
</details>
<details>
<summary>摘要</summary>
我们对将Scene中的代理和外部物品的学习表现学习的问题进行了处理。为此，我们提出了一个基于代理的动作进行学习框架，从无结构的观察中提取代理和物品的物理空间位置。我们的框架仅对代理的动作进行超参考，并假设物品被代理驱动的运动是未知的。我们提供了理论基础，正式证明了理想学习者可以将代理和物品分离，并正确地提取它们的位置。我们在实验中证明了我们的框架在多种情况下表现较好，并且通过强化学习解决下游任务。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications"><a href="#A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications" class="headerlink" title="A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications"></a>A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05343">http://arxiv.org/abs/2309.05343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Wang, Peizheng Li, Angela Doufexi, Mark A Beach</li>
<li>for: 这个论文旨在研究基于弹性智能表面（RIS）对无线通信系统的干扰依赖性。</li>
<li>methods: 本文使用深度强化学习（DRL）来优化单反射和多反射配置，以提高对于分布式用户的传输性能。</li>
<li>results: 比较随机搜寻和探索搜寻，DRL优化方法在对于多反射配置的优化中表现出优秀的性能，实现了1.2 dB的干扰峰值增加和更宽的传输幕。<details>
<summary>Abstract</summary>
In reconfigurable intelligent surface (RIS)-assisted wireless communication systems, the pointing accuracy and intensity of reflections depend crucially on the 'profile,' representing the amplitude/phase state information of all elements in a RIS array. The superposition of multiple single-reflection profiles enables multi-reflection for distributed users. However, the optimization challenges from periodic element arrangements in single-reflection and multi-reflection profiles are understudied. The combination of periodical single-reflection profiles leads to amplitude/phase counteractions, affecting the performance of each reflection beam. This paper focuses on a dual-reflection optimization scenario and investigates the far-field performance deterioration caused by the misalignment of overlapped profiles. To address this issue, we introduce a novel deep reinforcement learning (DRL)-based optimization method. Comparative experiments against random and exhaustive searches demonstrate that our proposed DRL method outperforms both alternatives, achieving the shortest optimization time. Remarkably, our approach achieves a 1.2 dB gain in the reflection peak gain and a broader beam without any hardware modifications.
</details>
<details>
<summary>摘要</summary>
在具有自适应智能表面（RIS）的无线通信系统中，点向精度和反射强度取决于“profile”，表示所有RIS数组元素的振荡状态信息。多个单反射profile的超пози合 enables distributed users的多反射。然而，单反射profile的 periodic 排序和多反射profile的优化挑战尚未得到充分研究。这篇论文关注 dual-reflection 优化方案，并investigates the far-field performance degradation caused by the misalignment of overlapped profiles。为解决这个问题，我们提出了一种基于深度学习（DRL）的优化方法。对比于随机搜索和极限搜索，我们的提议DRL方法在优化时间上表现出明显的优势，并且实现了无硬件修改的1.2 dB增强和更广泛的射频覆盖。
</details></li>
</ul>
<hr>
<h2 id="PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics"><a href="#PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics" class="headerlink" title="PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics"></a>PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05339">http://arxiv.org/abs/2309.05339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claus Smitt, Michael Halstead, Patrick Zimmer, Thomas Läbe, Esra Guclu, Cyrill Stachniss, Chris McCool</li>
<li>for: 实现园林 robot监控和干预任务中的高精度景象理解</li>
<li>methods: 使用NeRF技术建立3D积极Scene理解系统</li>
<li>results: 在严难的农业景象中，实现了3D积极Scene理解、真实图形和一致的panoptic表现，并且比基eline方法提高了21.34dB的峰值信号吟强和56.65%的panoptic质量。<details>
<summary>Abstract</summary>
Precise scene understanding is key for most robot monitoring and intervention tasks in agriculture. In this work we present PAg-NeRF which is a novel NeRF-based system that enables 3D panoptic scene understanding. Our representation is trained using an image sequence with noisy robot odometry poses and automatic panoptic predictions with inconsistent IDs between frames. Despite this noisy input, our system is able to output scene geometry, photo-realistic renders and 3D consistent panoptic representations with consistent instance IDs. We evaluate this novel system in a very challenging horticultural scenario and in doing so demonstrate an end-to-end trainable system that can make use of noisy robot poses rather than precise poses that have to be pre-calculated. Compared to a baseline approach the peak signal to noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be tuned to improve inference time by more than a factor of 2 while being memory efficient with approximately 12 times fewer parameters.
</details>
<details>
<summary>摘要</summary>
precise scene understanding 是 robot 监测和干预任务中的关键。在这项工作中，我们介绍了 PAg-NeRF，一种基于 NeRF 的新系统，允许3D�anoptic scene 理解。我们的表示被训练使用含有噪声机器人定位姿态和自动生成的�anoptic预测，带有不一致的ID между帧。尽管输入含有噪声，但我们的系统仍能输出场景几何、真实渲染和3D一致的�anoptic表示，并且实例ID保持一致。我们在挑战性较高的园艺场景中评估了这种新系统，并在这种情况下表明了可以使用噪声机器人定位而不需要先计算精确定位。相比基准方法，峰峰信号响应比例提高了21.34dB到23.37dB，而�anoptic质量也提高了从56.65%到70.08%。此外，我们的方法更快，可以通过调整来提高推理时间，并且具有较少的参数，约12倍少于基准方法。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems"><a href="#Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems" class="headerlink" title="Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems"></a>Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05337">http://arxiv.org/abs/2309.05337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Chiara Angelini, Angelo Giorgio Cavaliere, Raffaele Marino, Federico Ricci-Tersenghi</li>
<li>for: 这paper aimed to investigate the relationship between Stochastic Gradient Descent (SGD) and Glauber dynamics, and to show that they are substantially similar in discrete optimization and inference problems.</li>
<li>methods: The paper used an SGD-like algorithm and compared its dynamics with Metropolis Monte Carlo, with a properly chosen temperature that depends on the mini-batch size.</li>
<li>results: The paper found that the dynamics of the SGD-like algorithm are very similar to those of Metropolis Monte Carlo, both at equilibrium and in the out-of-equilibrium regime, despite the two algorithms having fundamental differences. This equivalence allows for the use of results about the performances and limits of Monte Carlo algorithms to optimize the mini-batch size in the SGD-like algorithm and make it efficient in recovering the signal in hard inference problems.<details>
<summary>Abstract</summary>
Is Stochastic Gradient Descent (SGD) substantially different from Glauber dynamics? This is a fundamental question at the time of understanding the most used training algorithm in the field of Machine Learning, but it received no answer until now. Here we show that in discrete optimization and inference problems, the dynamics of an SGD-like algorithm resemble very closely that of Metropolis Monte Carlo with a properly chosen temperature, which depends on the mini-batch size. This quantitative matching holds both at equilibrium and in the out-of-equilibrium regime, despite the two algorithms having fundamental differences (e.g.\ SGD does not satisfy detailed balance). Such equivalence allows us to use results about performances and limits of Monte Carlo algorithms to optimize the mini-batch size in the SGD-like algorithm and make it efficient at recovering the signal in hard inference problems.
</details>
<details>
<summary>摘要</summary>
是 Stochastic Gradient Descent (SGD) 和 Glauber dynamics 有重要区别吗？这是机器学习领域内最常用的训练算法问题，但既没有得到答案。我们现在显示，在离散优化和推理问题中，SGD-like algorithm 的动力学与 Metropolis Monte Carlo 的温度适当选择有关，这个温度与 mini-batch 大小有关。这种量化匹配在平衡状态和非平衡状态下都存在，尽管两种算法有深刻的不同（例如，SGD 不满足详细平衡）。这种等价性使我们能够使用 Monte Carlo 算法的性能和限制来优化 mini-batch 大小在 SGD-like algorithm 中，以便在困难推理问题中效率地恢复信号。Note: "SGD-like algorithm" refers to a stochastic gradient descent algorithm with a mini-batch size that is chosen adaptively, rather than a fixed size.
</details></li>
</ul>
<hr>
<h2 id="Neural-Koopman-prior-for-data-assimilation"><a href="#Neural-Koopman-prior-for-data-assimilation" class="headerlink" title="Neural Koopman prior for data assimilation"></a>Neural Koopman prior for data assimilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05317">http://arxiv.org/abs/2309.05317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthony-frion/sentinel2ts">https://github.com/anthony-frion/sentinel2ts</a></li>
<li>paper_authors: Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon, Abdeldjalil Aïssa El Bey</li>
<li>for: 这篇论文是为了描述如何使用神经网络模型来描述动态系统的趋势和特征。</li>
<li>methods: 该论文使用了 Koopman 算子理论来嵌入动态系统在隐藏空间中，使其动态可以用线性方式描述。另外，该论文还介绍了一种用于长期连续重建的训练方法，以及如何使用已经训练的动态模型作为Variational数据整合的先验知识。</li>
<li>results: 该论文的结果表明，使用神经网络模型可以在困难的时间序列数据上实现长期连续重建，并且可以通过自动批处理和数据整合来提高模型的准确性和稳定性。此外，该论文还示出了使用已经训练的动态模型作为Variational数据整合的先验知识可以有效地实现时间序列 interpolate 和预测。<details>
<summary>Abstract</summary>
With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show the promising use of trained dynamical models as priors for variational data assimilation techniques, with applications to e.g. time series interpolation and forecasting.
</details>
<details>
<summary>摘要</summary>
Note:* "sequential data" is translated as "时序数据" (shíxìng shùxī)* "dynamical model" is translated as "动态模型" (dòngtǐ módel)* "Koopman operator theory" is translated as "库普曼运算理论" (kùpènmàn yùngcéng lǐlùn)* "latent spaces" is translated as "隐藏空间" (hìnxiǎn kōngjī)* "long-term continuous reconstruction" is translated as "长期连续重建" (chángjì liánxù zhòngjiàn)* "self-supervised learning" is translated as "自我指导学习" (zìwǒ zhǐguī xuéxí)
</details></li>
</ul>
<hr>
<h2 id="Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data"><a href="#Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data" class="headerlink" title="Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data"></a>Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05305">http://arxiv.org/abs/2309.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen<br>for:FC-STGNN is proposed to effectively model the Spatial-Temporal (ST) dependencies in Multivariate Time-Series (MTS) data, which is crucial in various application fields.methods:FC-STGNN includes two key components: (1) FC graph construction, which connects sensors across all timestamps based on their temporal distances, and (2) FC graph convolution with a moving-pooling GNN layer to capture the ST dependencies.results:Compared to existing State-of-the-Art (SOTA) methods, FC-STGNN shows effectiveness in capturing the ST dependencies in MTS data, as demonstrated by extensive experiments on multiple MTS datasets.<details>
<summary>Abstract</summary>
Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolution. For graph construction, we design a decay graph to connect sensors across all timestamps based on their temporal distances, enabling us to fully model the ST dependencies by considering the correlations between DEDT. Further, we devise FC graph convolution with a moving-pooling GNN layer to effectively capture the ST dependencies for learning effective representations. Extensive experiments show the effectiveness of FC-STGNN on multiple MTS datasets compared to SOTA methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）数据在各种应用领域中扮演着关键性的角色。MTS数据具有顺序和多源（多感器）性质，因此自然地具有时空依赖关系，包括时间戳和时间戳之间的时间相关性，以及每个时间戳中的感器之间的空间相关性。为了有效利用这些信息，图神经网络（GNN）在各种应用中广泛应用。然而，现有的方法通常分别捕捉时间相关性和空间相关性，而忽略了不同感器之间的不同时间戳之间的相关性（DEDT）。这会限制现有的GNNs从学习有效表示。为了解决这一限制，我们提出了一种新的方法called Fully-Connected Spatial-Temporal Graph Neural Network（FC-STGNN），其包括以下两个关键组成部分：FC图构建和FC图卷积。FC图构建中，我们设计了衰减图来连接不同时间戳之间的感器，基于他们的时间距离，以便全面模型时空依赖关系，并考虑不同感器之间的DEDT相关性。此外，我们开发了FC图卷积层，其中包括一个移动pooling GNN层，以有效地捕捉时空依赖关系，以便学习有效表示。我们对多个MTS数据集进行了广泛的实验，并证明了FC-STGNN在相对于状态艺术方法的情况下的效果。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization"><a href="#Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization" class="headerlink" title="Discrete Denoising Diffusion Approach to Integer Factorization"></a>Discrete Denoising Diffusion Approach to Integer Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05295">http://arxiv.org/abs/2309.05295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlisfre/diffusion-factorization">https://github.com/karlisfre/diffusion-factorization</a></li>
<li>paper_authors: Karlis Freivalds, Emils Ozolins, Guntis Barzdins</li>
<li>for: 本研究是 investigate whether deep neural networks can facilitate faster integer factorization.</li>
<li>methods: 本文提出了一种使用深度神经网络和粗粒推 diffusion 的方法，通过 iteratively correcting errors in a partially-correct solution 来实现因数分解。</li>
<li>results: 该方法可以对数字的因数进行分解，并且可以在56比特长度的数字上进行因数分解。同时，我们的分析表明，在训练投入增长时，需要的抽样步骤数量会下降 exponential，从而抵消因数长度的增长。<details>
<summary>Abstract</summary>
Integer factorization is a famous computational problem unknown whether being solvable in the polynomial time. With the rise of deep neural networks, it is interesting whether they can facilitate faster factorization. We present an approach to factorization utilizing deep neural networks and discrete denoising diffusion that works by iteratively correcting errors in a partially-correct solution. To this end, we develop a new seq2seq neural network architecture, employ relaxed categorical distribution and adapt the reverse diffusion process to cope better with inaccuracies in the denoising step. The approach is able to find factors for integers of up to 56 bits long. Our analysis indicates that investment in training leads to an exponential decrease of sampling steps required at inference to achieve a given success rate, thus counteracting an exponential run-time increase depending on the bit-length.
</details>
<details>
<summary>摘要</summary>
整数因数分解是一个著名的计算问题，不确定它是否可以在多项式时间内解决。随着深度神经网络的出现，是否它们可以促进更快的因数分解吸引了关注。我们提出了一种利用深度神经网络和离散杂变滤波器进行因数分解的方法，通过逐步纠正错误来实现。为此，我们开发了一种新的seq2seq神经网络架构，使用宽松的分类分布和逆扩散过程来更好地处理杂变步骤中的不准确。这种方法可以为整数的长度达56位的因数分解。我们的分析表明，在训练投入量增加时，推理步骤所需的抽样步骤数随着扩展幂率下降，从而将执行时间减少到一定程度。
</details></li>
</ul>
<hr>
<h2 id="The-fine-print-on-tempered-posteriors"><a href="#The-fine-print-on-tempered-posteriors" class="headerlink" title="The fine print on tempered posteriors"></a>The fine print on tempered posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05292">http://arxiv.org/abs/2309.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Pitas, Julyan Arbel</li>
<li>for:  investigate the tempered posteriors and their relationship with test accuracy and calibration</li>
<li>methods:  use empirical studies and PAC-Bayesian analysis to explore the optimal temperature parameter for Bayesian models</li>
<li>results:  show that the coldest temperature is often optimal for test accuracy, and that targeting Frequentist metrics can lead to degradation in test accuracy, and that the temperature parameter cannot be seen as simply fixing a misspecified prior or likelihood.Here is the same information in Traditional Chinese:</li>
<li>for:  investigate the 减冷 posterior和它对测试准确和调整的关系</li>
<li>methods:  use empirical studies和PAC-Bayesian analysis来探索 Bayesian models 中的温度参数最佳化</li>
<li>results:  show that the 最冷温度通常是测试准确最佳，并且对测试准确进行调整可能会导致测试准确下降，并且温度参数不能被简单地看作是错误的假设或概率 Distribution 的修正。<details>
<summary>Abstract</summary>
We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.
</details>
<details>
<summary>摘要</summary>
我们进行了详细的探讨模拟后采用的抑制 posterior 的研究，并发现了一些重要且前所未讲的点。与前一些结果不同，我们首先表明了，在现实模型和数据集下，精确控制 Laplace 近似 posterior 的情况下， Stochasticity 不一定提高测试准确率。最低温度通常是最佳的。一 might think Bayesian 模型具有一定的随机性可以至少获得准确性的改进。但我们通过实验表明，当获得了这些改进时，这来的代价是测试准确率的下降。然后我们讨论了如何使用 Bayesian 模型来目标 Frequentist 度量，并通过 PAC-Bayesian 分析表明，温度参数 $\lambda$ 不能被简单地看作是修复错误的先前或 posterior。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Finite-Initialization-for-Tensorized-Neural-Networks"><a href="#Efficient-Finite-Initialization-for-Tensorized-Neural-Networks" class="headerlink" title="Efficient Finite Initialization for Tensorized Neural Networks"></a>Efficient Finite Initialization for Tensorized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06577">http://arxiv.org/abs/2309.06577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i3bquantumteam/q4real">https://github.com/i3bquantumteam/q4real</a></li>
<li>paper_authors: Alejandro Mata Ali, Iñigo Perez Delgado, Marina Ristol Roura, Aitor Moreno Fdez. de Leceta</li>
<li>For: The paper is written for layers of tensorized neural networks, specifically those with a high number of nodes and a connection to the input or output of all or most of the nodes.* Methods: The paper proposes a novel method for initializing these layers, which involves using the Frobenius norm of the layer in an iterative partial form. This method is efficient to compute and can be applied to different layers.* Results: The paper shows the performance of the proposed method on various layers and demonstrates its effectiveness in avoiding the explosion of the parameters of the matrix it emulates. The method is available in a Python function in the i3BQuantum repository, which can be run on an arbitrary layer.<details>
<summary>Abstract</summary>
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的层初始化方法，以避免tensorized神经网络中层的参数爆炸。这种方法适用于具有较高节点数的层，其中每个节点都与输入或输出相连。我们的方法的核心是使用层的 Frobenius  нор的iterative partial form，使其必须是有限的并在某个范围内。这个norm是可以高效计算的，并且可以在大多数情况下完全或部分计算。我们对不同的层进行了应用，并检查了其性能。我们还创建了一个Python函数来实现这种方法，可以应用于任意层，可以在 GitHub 上找到：https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb。
</details></li>
</ul>
<hr>
<h2 id="Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving"><a href="#Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving" class="headerlink" title="Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"></a>Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05282">http://arxiv.org/abs/2309.05282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Keysan, Andreas Look, Eitan Kosman, Gonca Gürsun, Jörg Wagner, Yu Yao, Barbara Rakitsch</li>
<li>for: 这项研究旨在提出一种新的文本基于表示方法，用于描述交通场景，并利用预训练语言编码器进行处理。</li>
<li>methods: 该研究使用文本基于表示法，与经典的矩阵图像表示法相结合，从而生成描述性的场景嵌入。</li>
<li>results: 研究表明，文本基于表示法和矩阵图像基于表示法的 JOINT 编码器，在 nuScenes 数据集上的预测比基eline 做出了显著改进，而且在减少折损率方面也有较好的表现。<details>
<summary>Abstract</summary>
In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.   First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
</details>
<details>
<summary>摘要</summary>
自主驾驶任务中，场景理解是Predicting the future behavior of surrounding traffic participants的首先步骤。然而，如何表示给定场景和提取其特征仍是开放的研究问题。在本研究中，我们提议一种文本基于表示交通场景的方法，并使用预训练语言编码器处理。首先，我们显示文本基于表示、与经典化的图像表示结合使得场景嵌入得到描述性的表示。其次，我们在nuScenes数据集上进行了比较，并显示与基eline的预测具有显著的改善。最后，我们在ablation研究中表明，将文本和图像的编码器结合使用，比单独使用图像或文本编码器更高效，确认了两种表示具有不同的优势。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning"><a href="#Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning" class="headerlink" title="Class-Incremental Grouping Network for Continual Audio-Visual Learning"></a>Class-Incremental Grouping Network for Continual Audio-Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05281">http://arxiv.org/abs/2309.05281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stonemo/cign">https://github.com/stonemo/cign</a></li>
<li>paper_authors: Shentong Mo, Weiguo Pian, Yapeng Tian</li>
<li>for: 这个研究旨在提出一个可以进行类别增量学习的数据流过掌握模型，并且可以在多个类别和多个数据类型（音频和影像）之间进行跨modal的学习。</li>
<li>methods: 这个模型使用了一个新的类别增量Grouping Network（CIGN），可以学习Category-wise semantic features来进行类别增量学习。CIGN使用可学习的音频-影像类别标签和音频-影像分组，以便不断地累累数据类型的类别-aware特征。此外，CIGN还使用了类别标签激发和不断分组来防止忘记，从而提高模型的捕捉数据类型的敏感性。</li>
<li>results: 我们对VGGSound-Instruments、VGGSound-100和VGG-Sound Sources的实验结果显示，CIGN可以在类别增量学习中实现州���进步的表现。<details>
<summary>Abstract</summary>
Continual learning is a challenging problem in which models need to be trained on non-stationary data across sequential tasks for class-incremental learning. While previous methods have focused on using either regularization or rehearsal-based frameworks to alleviate catastrophic forgetting in image classification, they are limited to a single modality and cannot learn compact class-aware cross-modal representations for continual audio-visual learning. To address this gap, we propose a novel class-incremental grouping network (CIGN) that can learn category-wise semantic features to achieve continual audio-visual learning. Our CIGN leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features. Additionally, it utilizes class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks, thereby improving the model's ability to capture discriminative audio-visual categories. We conduct extensive experiments on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks. Our experimental results demonstrate that the CIGN achieves state-of-the-art audio-visual class-incremental learning performance. Code is available at https://github.com/stoneMo/CIGN.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>逐渐学习是一个挑战性的问题，在该问题中，模型需要在不同任务上逐渐学习非站ARY数据。而之前的方法往往是使用Regularization或者熵练习框架来缓解忘却现象，但这些方法受限于单一模式，无法学习整体的混合模式，因此无法实现逐渐audio-visual学习。为了解决这个差距，我们提出了一种新的逐渐分组网络（CIGN），该网络可以学习类别wise的semantic特征，以实现逐渐audio-visual学习。我们的CIGN利用可学习的音频视频类别符号和音频视频分组来不断归纳类别相关的特征。此外，它还利用类别符号熔化和不断分组来防止忘却之前学习的参数，从而提高模型的捕捉混合类别的能力。我们在VGGSound-Instruments、VGGSound-100和VGG-Sound Sources的benchmark上进行了广泛的实验。我们的实验结果表明，CIGN可以在逐渐audio-visual学习中实现state-of-the-art的性能。代码可以在https://github.com/stoneMo/CIGN上找到。
</details></li>
</ul>
<hr>
<h2 id="Beamforming-in-Wireless-Coded-Caching-Systems"><a href="#Beamforming-in-Wireless-Coded-Caching-Systems" class="headerlink" title="Beamforming in Wireless Coded-Caching Systems"></a>Beamforming in Wireless Coded-Caching Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05276">http://arxiv.org/abs/2309.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Madhusudan, Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson</li>
<li>for: 这个论文的目的是提出一种无线传输网络架构，以利用射频和编码缓存策略来解决访问网络带宽问题。</li>
<li>methods: 该论文使用了一种基于遗传算法的射频优化策略，以提高编码缓存系统的效率。</li>
<li>results: 研究发现，该 JOINT 设计可以增加多播机会数、减少干扰和下行带宽负荷。 Comparative analysis also shows that the proposed approach outperforms traditional, uncoded caching schemes. Additionally, the study finds that proper beamforming is useful in enhancing the effectiveness of the coded-caching technique, resulting in significant reduction in peak backhaul traffic.<details>
<summary>Abstract</summary>
Increased capacity in the access network poses capacity challenges on the transport network due to the aggregated traffic. However, there are spatial and time correlation in the user data demands that could potentially be utilized. To that end, we investigate a wireless transport network architecture that integrates beamforming and coded-caching strategies. Especially, our proposed design entails a server with multiple antennas that broadcasts content to cache nodes responsible for serving users. Traditional caching methods face the limitation of relying on the individual memory with additional overhead. Hence, we develop an efficient genetic algorithm-based scheme for beam optimization in the coded-caching system. By exploiting the advantages of beamforming and coded-caching, the architecture achieves gains in terms of multicast opportunities, interference mitigation, and reduced peak backhaul traffic. A comparative analysis of this joint design with traditional, un-coded caching schemes is also conducted to assess the benefits of the proposed approach. Additionally, we examine the impact of various buffering and decoding methods on the performance of the coded-caching scheme. Our findings suggest that proper beamforming is useful in enhancing the effectiveness of the coded-caching technique, resulting in significant reduction in peak backhaul traffic.
</details>
<details>
<summary>摘要</summary>
增加了Access网络的容量会导致传输网络的压力增加，但是用户数据需求存在空间和时间相关性，这些相关性可能可以利用。为此，我们研究了一种具有广播和编码缓存策略的无线传输网络架构。具体来说，我们的提议包括一个有多个天线的服务器，通过广播内容到缓存节点来服务用户。传统的缓存方法受到各个缓存器的独立存储器的限制，此外还增加了额外的开销。因此，我们开发了一种基于遗传算法的 beam优化方案。通过利用广播和编码缓存的优势，该架构实现了多播机会增加、干扰 Mitigation 和传输峰值下行带宽的改善。我们对这种共同设计与传统、未编码缓存方案进行比较分析，以评估提议的优势。此外，我们还研究了缓存 scheme 的缓存和解码方法对性能的影响。我们的发现表明，正确的 beamforming 可以提高编码缓存技术的效iveness，从而实现显著减少传输峰值下行带宽。
</details></li>
</ul>
<hr>
<h2 id="EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction"><a href="#EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction" class="headerlink" title="EANet: Expert Attention Network for Online Trajectory Prediction"></a>EANet: Expert Attention Network for Online Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05683">http://arxiv.org/abs/2309.05683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Yao, Tianlu Mao, Min Shi, Jingkai Sun, Zhaoqi Wang</li>
<li>for: 这个研究旨在解决自动驾驶中的路径预测问题，特别是在突然变化的情况下，传统方法的预测精度很低，并且无法实时更新模型。</li>
<li>methods: 我们提出了专家注意力网络（Expert Attention Network），一个完整的在线学习框架，使用专家注意力来调整网络层次的重要性，以避免模型因为梯度问题而更新太慢，并且快速学习新情况知识以恢复预测精度。</li>
<li>results: 我们的方法可以快速降低预测错误，并且在突然变化的情况下保持预测精度在state-of-the-art水平。实验结果显示，传统方法受到梯度问题的影响，而我们的方法可以快速适应新情况，并且在短时间内恢复预测精度。<details>
<summary>Abstract</summary>
Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. The experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.
</details>
<details>
<summary>摘要</summary>
干线预测在自动驾驶中扮演着关键的角色。现有的主流研究和连续学习基于方法都需要训练完整的数据集，导致enario的快速变化时预测精度低下和模型更新缓慢。这些方法是否可以在实时中预测和使用数据实例立即更新模型（即在线学习设置）是一个问题。另外，数据实例流中的梯度爆炸或消失问题也需要解决。 Drawing inspiration from Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different network layer depths, avoiding the model updated slowly due to gradient problems and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. Experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.Here's the text with some additional information about the Simplified Chinese translation:The Simplified Chinese translation is written in a more formal and conservative style, which is common in academic writing. The vocabulary and grammar used are also more standardized and consistent with the language used in academic papers.In the translation, we tried to preserve the original meaning and structure of the text as much as possible, while also taking into account the nuances of the Simplified Chinese language. For example, we used the phrase "干线预测" (trajectory prediction) instead of "路径预测" (path prediction) to emphasize the importance of predicting the trajectory of the vehicle. We also used the phrase "数据实例流" (data instance stream) to refer to the stream of data used for training the model, which is a more common way of expressing this concept in Simplified Chinese.Overall, we hope that the translation will be helpful for readers who are more familiar with Simplified Chinese and will allow them to better understand the ideas and techniques presented in the original text.
</details></li>
</ul>
<hr>
<h2 id="CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling"><a href="#CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling" class="headerlink" title="CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling"></a>CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05270">http://arxiv.org/abs/2309.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsin Ali, Kandukuri Sai Teja, Neeharika Gupta, Parth Patwa, Anubhab Chatterjee, Vinija Jain, Aman Chadha, Amitava Das</li>
<li>for: 这个论文主要针对 Multilingual societies 中的 Code-Mixing (CM) 问题，即多种语言杂mixing在一起的社会现象。</li>
<li>methods: 作者采用了 Neural Language Models (NLMs)  LIKE transformers 来解决这个问题，并特别强调在 Switching Points (SPs) 处进行模型化。</li>
<li>results: 作者通过实验表明，使用 rotatory positional encoding 和 switching point information 可以更好地训练 Code-Mixed Language Models (LMs)，并在 sentiment analysis 和 machine translation 两个任务上达到了最佳效果。<details>
<summary>Abstract</summary>
The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.   We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details>
<details>
<summary>摘要</summary>
mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results. We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details></li>
</ul>
<hr>
<h2 id="UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs"><a href="#UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs" class="headerlink" title="UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs"></a>UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05269">http://arxiv.org/abs/2309.05269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yide-qiu/unikg">https://github.com/yide-qiu/unikg</a></li>
<li>paper_authors: Yide Qiu, Shaoxiang Ling, Tong Zhang, Bo Huang, Zhen Cui</li>
<li>for: 这篇论文是为了探讨大规模不规则数据中的知识挖掘和多类节点图表示学问题而写的。</li>
<li>methods: 这篇论文使用了semantic alignment策略和提档扩展 propagation模块（APM）来解决大规模多类节点图中信息传递和多Attribute association挖掘问题。</li>
<li>results: 在UniKG dataset上，这些方法可以高效地传递信息和挖掘多Attribute association，并且在节点类划 зада上达到了比较高的分类精度。<details>
<summary>Abstract</summary>
Irregular data in real-world are usually organized as heterogeneous graphs (HGs) consisting of multiple types of nodes and edges. To explore useful knowledge from real-world data, both the large-scale encyclopedic HG datasets and corresponding effective learning methods are crucial, but haven't been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken, including (i) the semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field; (ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset, and evaluate multiple baseline methods which are constructed by embedding our APM into large-scale homogenous graph learning methods. Our UniKG dataset and the baseline codes have been released at https://github.com/Yide-Qiu/UniKG.
</details>
<details>
<summary>摘要</summary>
real-world irregular data usually organizes as heterogeneous graphs (HGs) with multiple types of nodes and edges. To explore useful knowledge from real-world data, both large-scale encyclopedic HG datasets and effective learning methods are crucial, but haven't been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken:(i) semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field;(ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset and evaluate multiple baseline methods constructed by embedding our APM into large-scale homogeneous graph learning methods. Our UniKG dataset and the baseline codes have been released at <https://github.com/Yide-Qiu/UniKG>.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Bias-Detection-in-College-Student-Newspapers"><a href="#Unsupervised-Bias-Detection-in-College-Student-Newspapers" class="headerlink" title="Unsupervised Bias Detection in College Student Newspapers"></a>Unsupervised Bias Detection in College Student Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06557">http://arxiv.org/abs/2309.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam M. Lehavi, William McCormack, Noah Kornfeld, Solomon Glazer</li>
<li>for: 本文提出了一个可以自动抽取偏见的报纸archivepipeline，以Minimal human influence的方式进行抽取和检测偏见。</li>
<li>methods: 本文引入了一种自动化工具无法抓取数据的框架，并生成了14名学生的14篇论文，共23154个数据项。此外，本文还使用了一种基于大语言模型的 summarization 技术来计算偏见。</li>
<li>results: 本文的结果表明，该方法可以对政治敏感词和控制词进行计算，并且可以通过比较大语言模型的概要和原始文章来计算偏见。这种方法比重建ruction bias更加准确，并且需要 menos标注数据。<details>
<summary>Abstract</summary>
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance"><a href="#Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance" class="headerlink" title="Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance"></a>Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05260">http://arxiv.org/abs/2309.05260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchao Jian, Feng Ji, Wee Peng Tay</li>
<li>for: 本文研究了稀疏图序列的收敛性，并提出了一种基于泛函图和延展距离的方法来描述这种收敛性。</li>
<li>methods: 本文使用了通用图和延展距离来描述稀疏图序列的收敛性，并使用了一种随机图生成过程来模拟增长的稀疏图。</li>
<li>results: 本文证明了稀疏图序列的 adjacency 矩阵 eigenvalues 的收敛性，并进行了实验验证。 results suggest the possibility of transfer learning between sparse graphs.<details>
<summary>Abstract</summary>
Graphons have traditionally served as limit objects for dense graph sequences, with the cut distance serving as the metric for convergence. However, sparse graph sequences converge to the trivial graphon under the conventional definition of cut distance, which make this framework inadequate for many practical applications. In this paper, we utilize the concepts of generalized graphons and stretched cut distance to describe the convergence of sparse graph sequences. Specifically, we consider a random graph process generated from a generalized graphon. This random graph process converges to the generalized graphon in stretched cut distance. We use this random graph process to model the growing sparse graph, and prove the convergence of the adjacency matrices' eigenvalues. We supplement our findings with experimental validation. Our results indicate the possibility of transfer learning between sparse graphs.
</details>
<details>
<summary>摘要</summary>
GRAPHONS 传统上作为稠密图序列的限制对象，剪距作为 convergence 的度量。但是，稀疏图序列在传统定义下的剪距中 converge 到平凡图像，这使得这个框架无法满足许多实际应用中的需求。在这篇论文中，我们利用通用化的 GRAPHON 和延展剪距来描述稀疏图序列的 convergence。我们考虑一个基于通用化 GRAPHON 的随机图过程，该过程 converge 到通用化 GRAPHON 中的延展剪距。我们使用这个随机图过程来模拟增长的稀疏图，并证明连接矩阵的特征值的散射。我们的结果表明可以在稀疏图中进行特征值的传递学习。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction"><a href="#A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction" class="headerlink" title="A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction"></a>A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05259">http://arxiv.org/abs/2309.05259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohao Qu, Haoxuan Kuang, Jun Li, Linlin You</li>
<li>for: 这篇论文的目的是优化电动车充电空间的使用，以减轻智能交通系统的负载。</li>
<li>methods: 本篇论文提出了一种基于几何和时间注意力机制的特征提取方法，以及使用物理授业统一学习来将知识转移到预测模型中。</li>
<li>results: 试验结果显示，提出的方法（PAG）可以实现预测性和可解释性，并能够理解价格变化导致充电需求的适应变化。<details>
<summary>Abstract</summary>
Along with the proliferation of electric vehicles (EVs), optimizing the use of EV charging space can significantly alleviate the growing load on intelligent transportation systems. As the foundation to achieve such an optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed by using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations to correctly handle the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability in understanding the adaptive changes in charging demands caused by price fluctuations.
</details>
<details>
<summary>摘要</summary>
alongside the proliferation of electric vehicles (EVs), optimizing the use of EV charging space can significantly alleviate the growing load on intelligent transportation systems. As the foundation to achieve such an optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed by using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations to correctly handle the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability in understanding the adaptive changes in charging demands caused by price fluctuations.Here's the translation in Traditional Chinese:随着电动车（EV）的普及， ottimizzare l'utilizzo dell'EV充电空间可以有效缓解城市智能交通系统中的负载。为了 achieve such optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations in correctly handling the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability to understand the adaptive changes in charging demands caused by price fluctuations.
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effect-of-Pre-training-on-Time-Series-Classification"><a href="#Examining-the-Effect-of-Pre-training-on-Time-Series-Classification" class="headerlink" title="Examining the Effect of Pre-training on Time Series Classification"></a>Examining the Effect of Pre-training on Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05256">http://arxiv.org/abs/2309.05256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Pu, Shiwei Zhao, Ling Cheng, Yongzhu Chang, Runze Wu, Tangjie Lv, Rongsheng Zhang</li>
<li>for: 这项研究旨在探讨无监督预训练后练习的 paradigm在新模式下的效果。</li>
<li>methods: 研究人员对150个时间序列分类数据集进行了严格的检验，包括单变量时间序列（UTS）和多变量时间序列（MTS）benchmark。</li>
<li>results: 研究结果显示，预训练只有在模型适应数据较差时才能帮助改进优化过程。预训练不会在充分训练时间内产生常见化的效果，而且预训练只能加速模型适应数据的速度，不会提高总体性能。增加更多预训练数据不会提高总体性能，但可以强化预训练对原始数据量的优势，如更快的CONVERGENCE。模型结构在这种情况下扮演着更重要的角色。<details>
<summary>Abstract</summary>
Although the pre-training followed by fine-tuning paradigm is used extensively in many fields, there is still some controversy surrounding the impact of pre-training on the fine-tuning process. Currently, experimental findings based on text and image data lack consensus. To delve deeper into the unsupervised pre-training followed by fine-tuning paradigm, we have extended previous research to a new modality: time series. In this study, we conducted a thorough examination of 150 classification datasets derived from the Univariate Time Series (UTS) and Multivariate Time Series (MTS) benchmarks. Our analysis reveals several key conclusions. (i) Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well. (ii) Pre-training does not exhibit the effect of regularization when given sufficient training time. (iii) Pre-training can only speed up convergence if the model has sufficient ability to fit the data. (iv) Adding more pre-training data does not improve generalization, but it can strengthen the advantage of pre-training on the original data volume, such as faster convergence. (v) While both the pre-training task and the model structure determine the effectiveness of the paradigm on a given dataset, the model structure plays a more significant role.
</details>
<details>
<summary>摘要</summary>
尽管预训练后细化的方法在多个领域广泛应用，但是预训练对细化过程的影响仍存在一定的争议。现在，基于文本和图像数据的实验研究结果并未达成一致。为更深入地探讨无监督预训练后细化的方法，我们在新的模式上进行了扩展研究：时间序列。在这个研究中，我们对150个分类数据集进行了全面的分析，这些数据集来自于单变量时间序列（UTS）和多变量时间序列（MTS） benchmark。我们的分析发现了以下几点：（i）预训练只能帮助改善模型不适合数据的优化过程，而不是适合数据的模型。（ii）预训练不会在充足的训练时间下显示正则化效果。（iii）预训练只能快速 convergence的模型，如果模型具有足够的适应能力。（iv）增加更多的预训练数据不会提高通用性，但可以强化预训练在原始数据量上的优势，如更快的 convergence。（v）预训练任务和模型结构共同决定了预训练在给定数据集上的效果，但模型结构更加重要。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces"><a href="#A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces" class="headerlink" title="A quantum tug of war between randomness and symmetries on homogeneous spaces"></a>A quantum tug of war between randomness and symmetries on homogeneous spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05253">http://arxiv.org/abs/2309.05253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Arvind, Kishor Bharti, Jun Yong Khoo, Dax Enshan Koh, Jian Feng Kong</li>
<li>for: 研究量子信息中Symmetry和Randomness的关系。</li>
<li>methods: 采用几何方法，考虑状态为$H$-相似的集合，并引入$\mathbb{U}&#x2F;H$上的哈恩推测，以characterize真实的随机性。</li>
<li>results: 提出了一种基于几何空间的方法来 caracterize Symmetry in quantum information，并研究了 approximate true randomness和pseudorandomness。最后，通过 praktical demonstration，研究量子机器学习模型在几何空间中的表达性。<details>
<summary>Abstract</summary>
We explore the interplay between symmetry and randomness in quantum information. Adopting a geometric approach, we consider states as $H$-equivalent if related by a symmetry transformation characterized by the group $H$. We then introduce the Haar measure on the homogeneous space $\mathbb{U}/H$, characterizing true randomness for $H$-equivalent systems. While this mathematical machinery is well-studied by mathematicians, it has seen limited application in quantum information: we believe our work to be the first instance of utilizing homogeneous spaces to characterize symmetry in quantum information. This is followed by a discussion of approximations of true randomness, commencing with $t$-wise independent approximations and defining $t$-designs on $\mathbb{U}/H$ and $H$-equivalent states. Transitioning further, we explore pseudorandomness, defining pseudorandom unitaries and states within homogeneous spaces. Finally, as a practical demonstration of our findings, we study the expressibility of quantum machine learning ansatze in homogeneous spaces. Our work provides a fresh perspective on the relationship between randomness and symmetry in the quantum world.
</details>
<details>
<summary>摘要</summary>
我们探索量子信息中对偶和随机性的交互关系。我们采用几何方法，将状态视为$H$-相似的情况，其中$H$是一个群。然后，我们引入$\mathbb{U}/H$上的同调度量，用于描述真正的随机性。这种数学工具已经由数学家们广泛研究，但在量子信息领域却很少应用。我们认为我们的工作是量子信息领域中首次利用同调空间来描述对称性的。接着，我们讨论了真正随机性的近似，包括$t$-wise独立的近似和$\mathbb{U}/H$和$H$-相似状态上的$t$-设计。在继续探索中，我们研究了假随机性，定义了在同调空间中的假随机变换和状态。最后，我们通过实际示例，研究了基于同调空间的量子机器学习模型的表达性。我们的工作为量子世界中对偶和随机性之间的关系提供了一种新的视角。
</details></li>
</ul>
<hr>
<h2 id="SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block"><a href="#SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block" class="headerlink" title="SparseSwin: Swin Transformer with Sparse Transformer Block"></a>SparseSwin: Swin Transformer with Sparse Transformer Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05224">http://arxiv.org/abs/2309.05224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krisnapinasthika/sparseswin">https://github.com/krisnapinasthika/sparseswin</a></li>
<li>paper_authors: Krisna Pinasthika, Blessius Sheldo Putra Laksono, Riyandi Banovbi Putera Irsal, Syifa Hukma Shabiyya, Novanto Yudistira</li>
<li>for: 这个论文目的是提高计算机视觉模型的效率，使其更加高效。</li>
<li>methods: 这篇论文使用了一种名为Sparse Transformer（SparTa）块，这是一种带有缺省tokenConverter的转换器块，可以减少计算token的数量。它还使用了Swin T体系结构（SparseSwin），该体系结构可以减少输入的大小并且减少初始的计算token数量。</li>
<li>results: 提交的模型（SparseSwin）在图像分类任务上达到了86.96%、97.43%和85.35%的准确率在ImageNet100、CIFAR10和CIFAR100 datasets上。尽管它具有较少的参数，但结果表明使用缺省tokenConverter可以优化转换器的使用，提高其性能。<details>
<summary>Abstract</summary>
Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.
</details>
<details>
<summary>摘要</summary>
（简化中文）计算机视觉研究的进步使得转换器体系成为计算机视觉任务的状态码。转换器体系的一个已知缺点是高数量的参数，这可能导致更复杂和不fficient的算法。本文的目标是减少参数数量，从而使转换器更加高效。我们提出了 sparse transformer（SparTa）块，它是一种增强了转换器块的模型，并添加了一个稀疏的标记转换器，以减少使用的标记数量。我们使用SparTa块在Swin T体系（SparseSwin）中，以利用Swin的下采样能力和减少初始标记数量。我们提出的SparseSwin模型在图像分类任务中的准确率为86.96%, 97.43%, 和85.35%，分别在ImageNet100、CIFAR10和CIFAR100数据集上。尽管它具有更少的参数，但结果表明了使用稀疏的标记转换器和有限数量的标记来优化转换器，并提高其性能的潜力。
</details></li>
</ul>
<hr>
<h2 id="Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer"><a href="#Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer" class="headerlink" title="Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?"></a>Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06574">http://arxiv.org/abs/2309.06574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingsonglv/CFG">https://github.com/jingsonglv/CFG</a></li>
<li>paper_authors: Jingsong Lv, Hongyang Chen, Yao Qi, Lei Yu</li>
<li>for: 本研究旨在提出两种本地图像特征用于缺失链接预测任务中的 ogbl-citation2。</li>
<li>methods: 我们提出了圆形特征，它们是基于圆形朋友圈的概念。我们还详细介绍了计算这些特征的方法。</li>
<li>results: 我们实验结果表明，基于 SIEG 网络的圆形特征意识graph transformer（CFG）模型在 ogbl-citation2 数据集上达到了状态率。<details>
<summary>Abstract</summary>
In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了两种本地图像特征 для缺失链接预测任务中的ogbl-citation2。我们定义了这些特征为圈feature，它们来自圈子的概念。我们提出了计算这些特征的详细计算公式。首先，我们定义了第一个圈特征为修改的摆动，它来自于分合图。其次，我们定义了第二个圈特征为桥，它表示两个节点之间的圈子之间的重要性。此外，我们首先提出了这些特征作为偏好，以便通过改进图自注意机制来提高图自注意机制。我们实现了基于SIEG网络的圈特征意识graph transformer（CFG）模型，该模型使用双塔结构来捕捉全局和本地结构特征。实验结果表明，CFG在dataset ogbl-citation2上达到了状态艺术性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout"><a href="#Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout" class="headerlink" title="Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout"></a>Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05213">http://arxiv.org/abs/2309.05213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Guo, Warren Richard Morningstar, Raviteja Vemulapalli, Karan Singhal, Vishal M. Patel, Philip Andrew Mansfield</li>
<li>for: 这篇论文旨在解决联合学习在边缘设备上训练大型机器学习模型时的问题，具体来说是降低每个客户端的内存、计算和通信成本，以便在Edge设备上训练更大的模型。</li>
<li>methods: 该论文提出了一种简单 yet effective的策略，即联合层wise学习，使得每个客户端只需要训练一层，从而大幅降低客户端的内存、计算和通信成本。此外，论文还提出了一种补充技术，即联合深度随机Dropout，可以进一步降低资源使用。</li>
<li>results: 论文表明，通过结合这两种技术，可以在Edge设备上训练更大的模型，并且与传统联合自动学习的性能相似。具体来说，在联合自我监督学习中，训练内存使用量降低了5倍或更多，而下游任务的性能与传统联合自动学习的性能相似。<details>
<summary>Abstract</summary>
Large machine learning models trained on diverse data have recently seen unprecedented success. Federated learning enables training on private data that may otherwise be inaccessible, such as domain-specific datasets decentralized across many clients. However, federated learning can be difficult to scale to large models when clients have limited resources. This challenge often results in a trade-off between model size and access to diverse data. To mitigate this issue and facilitate training of large models on edge devices, we introduce a simple yet effective strategy, Federated Layer-wise Learning, to simultaneously reduce per-client memory, computation, and communication costs. Clients train just a single layer each round, reducing resource costs considerably with minimal performance degradation. We also introduce Federated Depth Dropout, a complementary technique that randomly drops frozen layers during training, to further reduce resource usage. Coupling these two techniques enables us to effectively train significantly larger models on edge devices. Specifically, we reduce training memory usage by 5x or more in federated self-supervised representation learning and demonstrate that performance in downstream tasks is comparable to conventional federated self-supervised learning.
</details>
<details>
<summary>摘要</summary>
大型机器学习模型在各种数据上进行训练已经得到了历史上无 precedent的成功。联邦学习可以训练在私有数据上，这些数据可能elsewhere decentralized across many clients。然而，联邦学习可能难以扩展到大型模型，因为客户端的资源有限。这种挑战通常导致模型大小和数据多样性之间的交易。为了缓解这个问题并在边缘设备上训练大型模型，我们提出了一个简单 yet effective的策略：联邦层次学习。在每个回合中，客户端只需要训练一个层，这将大幅降低客户端的内存、计算和通信成本。此外，我们还引入了联邦层次随机Dropout，这是在训练过程中随机Drop frozen层的技术。这两种技术的结合可以有效地在边缘设备上训练较大的模型。具体来说，我们可以在联邦自然学习中降低训练内存使用量，并证明在下游任务中表现与传统联邦自然学习相似。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification"><a href="#Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification" class="headerlink" title="Graph Contextual Contrasting for Multivariate Time Series Classification"></a>Graph Contextual Contrasting for Multivariate Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05202">http://arxiv.org/abs/2309.05202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen</li>
<li>for: 本研究旨在提高多变量时间序列（MTS）分类 tasks 的表现，通过保持多感器数据的空间一致性和时间一致性。</li>
<li>methods: 我们提出了图结构增强（Graph Contextual Contrasting，GCC），包括节点增强、边增强和图增强，以保持感器稳定性和相关性。我们还引入了多窗口时间增强，以确保每感器的时间一致性。</li>
<li>results: 我们的GCC方法在多个MTS分类任务上达到了现状最佳表现。<details>
<summary>Abstract</summary>
Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph Contextual Contrasting (GCC) for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by graph contrasting with both node- and graph-level contrasting to extract robust sensor- and global-level features. We further introduce multi-window temporal contrasting to ensure temporal consistency in the data for each sensor. Extensive experiments demonstrate that our proposed GCC achieves state-of-the-art performance on various MTS classification tasks.
</details>
<details>
<summary>摘要</summary>
contrastive learning，作为一种自助学习 paradigm，在多变量时间序列（MTS）分类中变得流行。它确保不同视图中的无标样本之间的一致性，然后学习这些样本的有效表示。现有的对比学习方法主要关注实现时间一致性，通过时间扩展和对比技术来保持时间特征的稳定性，以适应MTS数据。然而，它们忽略了空间一致性，即感知器的稳定性和相关性。由于MTS数据通常来自多个感知器，保证空间一致性是对MTS数据的总性表现的关键。因此，我们提出图结构启发对比（GCC）来保证MTS数据的空间一致性。具体来说，我们提出图ixel augmentation和边augmentation来保持感知器的稳定性和相关性，然后进行图像对比，包括节点对比和图像对比，以提取感知器和全局级别的特征。我们还引入多窗口时间对比来确保每个感知器的时间一致性。我们的GCC方法在多种MTS分类任务上实现了状态的杰出表现。
</details></li>
</ul>
<hr>
<h2 id="CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization"><a href="#CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization" class="headerlink" title="CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization"></a>CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05200">http://arxiv.org/abs/2309.05200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shepherd-gregory/bkio-exploration">https://github.com/shepherd-gregory/bkio-exploration</a></li>
<li>paper_authors: Yang Xu, Ronghao Zheng, Senlin Zhang, Meiqin Liu, Shoudong Huang</li>
<li>for: 提高无人机在未知和复杂环境中的信息基于自主探索效率</li>
<li>methods: 使用 Gaussian process 回归学习一个估计函数来推算信息强度相关的控制动作，并采用基于 GP 的 Bayesian 优化（GPBO）来实现让推敲和探索之间的负荷权衡。</li>
<li>results: 提出了一种新的轻量级信息增加推理方法，可以在不需要训练的情况下实现 Logarithmic 复杂度，并且可以在不同的无结构和堆存环境中保持探索性。<details>
<summary>Abstract</summary>
In this paper, we consider improving the efficiency of information-based autonomous robot exploration in unknown and complex environments. We first utilize Gaussian process (GP) regression to learn a surrogate model to infer the confidence-rich mutual information (CRMI) of querying control actions, then adopt an objective function consisting of predicted CRMI values and prediction uncertainties to conduct Bayesian optimization (BO), i.e., GP-based BO (GPBO). The trade-off between the best action with the highest CRMI value (exploitation) and the action with high prediction variance (exploration) can be realized. To further improve the efficiency of GPBO, we propose a novel lightweight information gain inference method based on Bayesian kernel inference and optimization (BKIO), achieving an approximate logarithmic complexity without the need for training. BKIO can also infer the CRMI and generate the best action using BO with bounded cumulative regret, which ensures its comparable accuracy to GPBO with much higher efficiency. Extensive numerical and real-world experiments show the desired efficiency of our proposed methods without losing exploration performance in different unstructured, cluttered environments. We also provide our open-source implementation code at https://github.com/Shepherd-Gregory/BKIO-Exploration.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了自动化机器人在未知和复杂环境中提高信息基于的探索效率。我们首先利用 Gaussian 过程（GP）回归来学习一个假设模型，以便对查询控制动作的信息充足积极（CRMI）进行推测，然后采用一个包含预测值和预测不确定性的目标函数来进行 Bayesian 优化（BO），即 GP 基于的 BO（GPBO）。通过考虑最佳动作的 CRMI 值（利用）和高预测变异（探索）的交互来实现财富均衡。为了进一步提高 GPBO 的效率，我们提出了一种新的轻量级信息增加推测方法，基于抽象kernel推断和优化（BKIO），实现了logs平方复杂度而不需要训练。BKIO 可以将 CRMI 推测出来，并使用 BO 实现 bounded 累累积 regret，这保证了它与 GPBO 相对较高效的准确性。我们在不同的无结构、拥堵环境中进行了广泛的数值和实际实验，并证明了我们的提议的效率无需失去探索性。我们还在 GitHub 上提供了我们的开源实现代码，可以在 <https://github.com/Shepherd-Gregory/BKIO-Exploration> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Does-Writing-with-Language-Models-Reduce-Content-Diversity"><a href="#Does-Writing-with-Language-Models-Reduce-Content-Diversity" class="headerlink" title="Does Writing with Language Models Reduce Content Diversity?"></a>Does Writing with Language Models Reduce Content Diversity?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05196">http://arxiv.org/abs/2309.05196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vishakhpk/hai-diversity">https://github.com/vishakhpk/hai-diversity</a></li>
<li>paper_authors: Vishakh Padmakumar, He He</li>
<li>For: The paper aims to measure the impact of co-writing on diversity in generated content using large language models (LLMs).* Methods: The study uses a controlled experiment where users write argumentative essays in three setups: with a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and without model help. The authors develop a set of diversity metrics to evaluate the impact of co-writing on diversity.* Results: The study finds that writing with InstructGPT (but not GPT3) results in a statistically significant reduction in diversity, as it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. The effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays, while the user-contributed text remains unaffected by model collaboration.Here are the results in Simplified Chinese text:* For: 这个研究是为了测量大语言模型（LLM）的合作写作对内容多样性的影响。* Methods: 这个研究使用了一个控制试验，在不同的设置下让用户写作Argumentative Essays：使用基础的 LLM（GPT3）、反馈调整的 LLM（InstructGPT）以及没有模型帮助。作者们开发了一组多样性指标来评估合作写作对多样性的影响。* Results: 研究发现，使用 InstructGPT（而不是 GPT3）会导致对多样性的统计学上的减少，这是因为它会使用者的写作更加相似，并降低总的语言和内容多样性。这种效果主要是由 InstructGPT 在合写文章中提供的文本变得更加一致，而不是用户提供的文本。<details>
<summary>Abstract</summary>
Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization"><a href="#Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization" class="headerlink" title="Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization"></a>Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05183">http://arxiv.org/abs/2309.05183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Tang</li>
<li>for:  solves the two-stage submodular maximization problem with non-monotone submodular functions, which has applications in data summarization and other domains.</li>
<li>methods:  uses constant-factor approximation algorithms to address the more general case of non-monotone submodular functions.</li>
<li>results:  pioneers the extension of submodular maximization research to accommodate non-monotone functions, and introduces the first constant-factor approximation algorithms for this more general case.<details>
<summary>Abstract</summary>
The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:两个阶段的子模式最大化问题的目标是使用提供的训练函数来减少基aset，以确保将新的目标函数应用于减少后的基aset上的优化结果与原始基aset上的优化结果相似。这个问题在不同领域，如数据概要化中有应用。现有研究通常假设目标函数的 monotonicity，而我们的工作则是扩展这些研究，以适应非 monotone 的子模式函数。我们已经提出了首个常数因子approximation算法。
</details></li>
</ul>
<hr>
<h2 id="DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning"><a href="#DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"></a>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05173">http://arxiv.org/abs/2309.05173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengxiangshi/dept">https://github.com/zhengxiangshi/dept</a></li>
<li>paper_authors: Zhengxiang Shi, Aldo Lipani<br>for: 这个研究的目的是提高语言模型的parameter-efficient fine-tuning（PEFT）性能，以及实现更好的内存和时间成本。methods: 这个研究使用Prompt Tuning（PT）技术，将一小量可变软题（continuous）标识给输入语言模型（LM），以提高模型的演算效率和精度。results: 这个研究发现，使用Decomposed Prompt Tuning（DePT）技术可以在23个自然语言处理（NLP）和描述语言（VL）任务中，实现更好的性能，并在一些情况下超越基eline。同时，DePT可以实现更好的内存和时间成本，比基eline节省20%以上。<details>
<summary>Abstract</summary>
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving over 20% memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.
</details>
<details>
<summary>摘要</summary>
Prompt tuning (PT)，一种将小量可调软 vectors（连续）附加到语言模型（LM）的输入处，已经在多种任务和模型上显示出了有 promise的结果。PT与其他PEFTapproaches不同，因为它在模型大小增加时不会很快扩展参数。然而，PT引入了额外的软提示字符，导致输入序列变长，从而对训练和推理时间和内存使用有很大影响，特别是对大型语言模型（LLM）来说。为解决这个问题，我们提出了Decomposed Prompt Tuning（DePT），它将软提示分解成一个 shorter soft prompt和一对低级矩阵，然后将这些矩阵优化两个不同的学习率。这使得DePT可以实现更好的性能，同时减少了20%以上的内存和时间成本，不改变可调参数的大小。通过对23种自然语言处理（NLP）和视觉语言（VL）任务进行了广泛的实验，我们证明了DePT在PEFT方法中比标准PT和其他变体表现更好，甚至在某些情况下超过了全 Fine-tuning基线。此外，我们还观察到DePT在模型大小增加时变得更加高效。我们进一步的研究表明，DePT可以轻松地与parameter-efficient transfer learning在少量学习设定中集成，并且可以适应不同的模型结构和大小。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.LG_2023_09_11/" data-id="clmjn91n1008d0j8883an5p0c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/eess.IV_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T09:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/eess.IV_2023_09_11/">eess.IV - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification"><a href="#Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification" class="headerlink" title="Radiomics Boosts Deep Learning Model for IPMN Classification"></a>Radiomics Boosts Deep Learning Model for IPMN Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05857">http://arxiv.org/abs/2309.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanhong Yao, Zheyuan Zhang, Ugur Demir, Elif Keles, Camila Vendrami, Emil Agarunov, Candice Bolan, Ivo Schoots, Marc Bruno, Rajesh Keswani, Frank Miller, Tamas Gonda, Cemal Yazici, Temel Tirkes, Michael Wallace, Concetto Spampinato, Ulas Bagci</li>
<li>for: 检测和分类IPMN瘤的风险水平，以便有效地规划治疗和疾病控制。</li>
<li>methods: 提出了一种基于计算机机器学的诊断管道，包括高效的自适应分割策略和深度学习模型。</li>
<li>results: 在多中心数据集上测试了并获得了新的最高性能记录（81.9%），比国际指南和已发表研究（61.3%）更高。<details>
<summary>Abstract</summary>
Intraductal Papillary Mucinous Neoplasm (IPMN) cysts are pre-malignant pancreas lesions, and they can progress into pancreatic cancer. Therefore, detecting and stratifying their risk level is of ultimate importance for effective treatment planning and disease control. However, this is a highly challenging task because of the diverse and irregular shape, texture, and size of the IPMN cysts as well as the pancreas. In this study, we propose a novel computer-aided diagnosis pipeline for IPMN risk classification from multi-contrast MRI scans. Our proposed analysis framework includes an efficient volumetric self-adapting segmentation strategy for pancreas delineation, followed by a newly designed deep learning-based classification scheme with a radiomics-based predictive approach. We test our proposed decision-fusion model in multi-center data sets of 246 multi-contrast MRI scans and obtain superior performance to the state of the art (SOTA) in this field. Our ablation studies demonstrate the significance of both radiomics and deep learning modules for achieving the new SOTA performance compared to international guidelines and published studies (81.9\% vs 61.3\% in accuracy). Our findings have important implications for clinical decision-making. In a series of rigorous experiments on multi-center data sets (246 MRI scans from five centers), we achieved unprecedented performance (81.9\% accuracy).
</details>
<details>
<summary>摘要</summary>
Traduzco el texto dado a Chinese simplificado.《Intraductal Papillary Mucinous Neoplasm (IPMN) cysts are pre-malignant pancreas lesions, and they can progress into pancreatic cancer. Therefore, detecting and stratifying their risk level is of ultimate importance for effective treatment planning and disease control. However, this is a highly challenging task because of the diverse and irregular shape, texture, and size of the IPMN cysts as well as the pancreas. In this study, we propose a novel computer-aided diagnosis pipeline for IPMN risk classification from multi-contrast MRI scans. Our proposed analysis framework includes an efficient volumetric self-adapting segmentation strategy for pancreas delineation, followed by a newly designed deep learning-based classification scheme with a radiomics-based predictive approach. We test our proposed decision-fusion model in multi-center data sets of 246 multi-contrast MRI scans and obtain superior performance to the state of the art (SOTA) in this field. Our ablation studies demonstrate the significance of both radiomics and deep learning modules for achieving the new SOTA performance compared to international guidelines and published studies (81.9% vs 61.3% in accuracy). Our findings have important implications for clinical decision-making. In a series of rigorous experiments on multi-center data sets (246 MRI scans from five centers), we achieved unprecedented performance (81.9% accuracy).》Here's the translation in Simplified Chinese:《IPMN肿瘤是肾脏癌前期肿瘤，可能进展到肾脏癌。因此，检测和分级IPMN肿瘤的风险水平是至关重要的，以便为肾脏癌的治疗规划和疾病控制提供有效的方案。然而，这是一个非常困难的任务，因为IPMN肿瘤的形态、 текстура和大小具有多样性和不规则性。在这个研究中，我们提出了一个新的计算机辅助诊断管线，用于从多标示MRI扫描中检测IPMN肿瘤的风险等级。我们的提案分析框架包括一个高效的自适应分割策略，用于肾脏定义，以及一个新的深度学习基于类型的分类方案，以及一个基于几个标示的预测方法。我们在多中心数据集（246个多标示MRI扫描）中试用我们的决策融合模型，并获得了领域的新纪录（81.9%的准确性）。我们的实验表明，深度学习和类型的模组均具有重要的作用，并且在国际指南和已出版的研究中具有显著的优势（81.9% vs 61.3%的准确性）。我们的发现具有重要的依据，对临床决策有重要的影响。在多中心数据集（246个MRI扫描）中进行了一系列的严格实验，获得了无前例的性能（81.9%的准确性）。》
</details></li>
</ul>
<hr>
<h2 id="Designs-and-Implementations-in-Neural-Network-based-Video-Coding"><a href="#Designs-and-Implementations-in-Neural-Network-based-Video-Coding" class="headerlink" title="Designs and Implementations in Neural Network-based Video Coding"></a>Designs and Implementations in Neural Network-based Video Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05846">http://arxiv.org/abs/2309.05846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Li, Junru Li, Chaoyi Lin, Kai Zhang, Li Zhang, Franck Galpin, Thierry Dumas, Hongtao Wang, Muhammed Coban, Jacob Ström, Du Liu, Kenneth Andersson</li>
<li>for: 本文主要针对的是如何通过神经网络来实现视频编码，以提高视频编码的效率和质量。</li>
<li>methods: 本文主要介绍了两种神经网络基于的视频编码技术：神经网络基于的内部预测和神经网络基于的循环过滤。这两种技术已经在JVET中进行了详细的研究和探讨，并最终被收录到了NNVC的参考软件中。</li>
<li>results: 对于Y、Cb、Cr三种颜色块，使用神经网络基于的编码工具可以实现{11.94%、21.86%、22.59%}的BD率减少平均值，相比VTM-11.0_nnvc。此外，对于随机访问、低延迟和所有内部配置，使用神经网络基于的编码工具可以实现{9.18%、19.76%、20.92%}、{10.63%、21.56%、23.02%}和{11.94%、21.86%、22.59%}的BD率减少平均值。<details>
<summary>Abstract</summary>
The past decade has witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with neural network-based video coding being one of them. Neural network-based video coding can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This paper elaborates some of the recent exploration efforts of JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of neural network-based video coding (NNVC), falling in the former category. Specifically, this paper discusses two major NN-based video coding technologies, i.e. neural network-based intra prediction and neural network-based in-loop filtering, which have been investigated for several meeting cycles in JVET and finally adopted into the reference software of NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed NN-based coding tools in NNVC-4.0 could achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations respectively.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie  witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with neural network-based video coding being one of them. Neural network-based video coding can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This paper elaborates some of the recent exploration efforts of JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of neural network-based video coding (NNVC), falling in the former category. Specifically, this paper discusses two major NN-based video coding technologies, i.e. neural network-based intra prediction and neural network-based in-loop filtering, which have been investigated for several meeting cycles in JVET and finally adopted into the reference software of NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed NN-based coding tools in NNVC-4.0 could achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations respectively.
</details></li>
</ul>
<hr>
<h2 id="Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging"><a href="#Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging" class="headerlink" title="Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging"></a>Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05818">http://arxiv.org/abs/2309.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yara Ali Alnaggar, Ahmad Sebaq, Karim Amer, ElSayed Naeem, Mohamed Elhelw</li>
<li>for: 本研究旨在提供一个公共多spectral和RGB图像集和一个深度学习管道，用于检测rice Plant疾病。</li>
<li>methods: 本研究使用多spectral和RGB图像作为输入，通过深度学习管道进行疾病检测。</li>
<li>results: 研究发现，使用多spectral和RGB图像作为输入可以实现更高的F1准确率，比使用RGB输入只有更高的准确率。<details>
<summary>Abstract</summary>
Rice is considered a strategic crop in Egypt as it is regularly consumed in the Egyptian people's diet. Even though Egypt is the highest rice producer in Africa with a share of 6 million tons per year, it still imports rice to satisfy its local needs due to production loss, especially due to rice disease. Rice blast disease is responsible for 30% loss in rice production worldwide. Therefore, it is crucial to target limiting yield damage by detecting rice crops diseases in its early stages. This paper introduces a public multispectral and RGB images dataset and a deep learning pipeline for rice plant disease detection using multi-modal data. The collected multispectral images consist of Red, Green and Near-Infrared channels and we show that using multispectral along with RGB channels as input archives a higher F1 accuracy compared to using RGB input only.
</details>
<details>
<summary>摘要</summary>
rice 被视为埃及的战略作物，由于埃及人的日常饮食中Regularly consumed，即使埃及每年出产6万吨的rice，仍然需要进口rice来满足本地需求，主要是由于生产损失，尤其是rice disease。rice blast disease是全球rice生产损失的30%原因。因此，适当检测rice crops的疾病在早期是非常重要。这篇论文介绍了一个公共多спектral和RGB图像集和一个深度学习管道，用于rice plant疾病检测，使用多Modal数据。收集的多спектral图像包括红、绿和近红外通道，我们显示使用多спектral和RGB通道作为输入，可以达到高于RGB输入Only的F1准确率。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Adversarial-Purification-for-Robust-Deep-MRI-Reconstruction"><a href="#Diffusion-based-Adversarial-Purification-for-Robust-Deep-MRI-Reconstruction" class="headerlink" title="Diffusion-based Adversarial Purification for Robust Deep MRI Reconstruction"></a>Diffusion-based Adversarial Purification for Robust Deep MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05794">http://arxiv.org/abs/2309.05794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, Saiprasad Ravishankar</li>
<li>for: 提高深度学习（DL）方法在核磁共振成像（MRI）重建中的性能</li>
<li>methods: 利用扩散模型提高DL方法的鲁棒性</li>
<li>results: 对比主流防御方法（如对抗训练和随机平滑），我们的提议方法在MRI重建中显示出更高的效果<details>
<summary>Abstract</summary>
Deep learning (DL) methods have been extensively employed in magnetic resonance imaging (MRI) reconstruction, demonstrating remarkable performance improvements compared to traditional non-DL methods. However, recent studies have uncovered the susceptibility of these models to carefully engineered adversarial perturbations. In this paper, we tackle this issue by leveraging diffusion models. Specifically, we introduce a defense strategy that enhances the robustness of DL-based MRI reconstruction methods through the utilization of pre-trained diffusion models as adversarial purifiers. Unlike conventional state-of-the-art adversarial defense methods (e.g., adversarial training), our proposed approach eliminates the need to solve a minimax optimization problem to train the image reconstruction model from scratch, and only requires fine-tuning on purified adversarial examples. Our experimental findings underscore the effectiveness of our proposed technique when benchmarked against leading defense methodologies for MRI reconstruction such as adversarial training and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）方法在核磁共振成像（MRI）重建中得到了广泛应用，并表现出了非常出色的性能改善。然而，最近的研究发现，这些模型对特制的恶意扰动受到极大的抑制。在这篇论文中，我们解决这个问题，通过利用扩散模型。具体来说，我们介绍了一种防御策略，通过使用预训练的扩散模型来增强DL-based MRI重建方法的Robustness。与传统的state-of-the-art adversarial防御方法（例如， adversarial Training）不同，我们的提议方法不需要从头开始训练图像重建模型，只需要练习在纯化的恶意示例上。我们的实验发现，我们的提议方法在比较leading defense方法（如 adversarial training和随机平滑）的基础上，对MRI重建方法的防御性能具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images"><a href="#LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images" class="headerlink" title="LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images"></a>LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05780">http://arxiv.org/abs/2309.05780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Fhima, Jan Van Eijgen, Hana Kulenovic, Valérie Debeuf, Marie Vangilbergen, Marie-Isaline Billen, Heloïse Brackenier, Moti Freiman, Ingeborg Stalmans, Joachim A. Behar</li>
<li>for: The paper is written for the purpose of developing a novel deep learning architecture for high resolution segmentation of retinal arterioles and venules in digital fundus images.</li>
<li>methods: The paper uses active learning to create a new dataset of crowd-sourced manual segmentations, and develops a novel deep learning architecture called LUNet that includes a double dilated convolutional block and a long tail to enhance the receptive field and refine the segmentation.</li>
<li>results: The paper shows that LUNet significantly outperforms two state-of-the-art segmentation algorithms on both the local test set and four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种高分辨率的抽象血管图像的深度学习架构。</li>
<li>methods: 这篇论文使用了活动学习来创建一个新的数据集，并开发了一种名为LUNet的深度学习架构，包括一个双扩展 convolutional block 和一个长尾来增强模型的接受范围和精度。</li>
<li>results: 这篇论文表明，LUNet 在本地测试集和四个外部测试集上都显著超越了两种现有的分割算法。<details>
<summary>Abstract</summary>
The retina is the only part of the human body in which blood vessels can be accessed non-invasively using imaging techniques such as digital fundus images (DFI). The spatial distribution of the retinal microvasculature may change with cardiovascular diseases and thus the eyes may be regarded as a window to our hearts. Computerized segmentation of the retinal arterioles and venules (A/V) is essential for automated microvasculature analysis. Using active learning, we created a new DFI dataset containing 240 crowd-sourced manual A/V segmentations performed by fifteen medical students and reviewed by an ophthalmologist, and developed LUNet, a novel deep learning architecture for high resolution A/V segmentation. LUNet architecture includes a double dilated convolutional block that aims to enhance the receptive field of the model and reduce its parameter count. Furthermore, LUNet has a long tail that operates at high resolution to refine the segmentation. The custom loss function emphasizes the continuity of the blood vessels. LUNet is shown to significantly outperform two state-of-the-art segmentation algorithms on the local test set as well as on four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators. We make the newly created dataset open access (upon publication).
</details>
<details>
<summary>摘要</summary>
眼睛是人体唯一一部可以非侵入式地访问血管的部位，通过数字背景图像（DFI）进行成像。眼睛中血管网络的空间分布可能会与冠状病变发生变化，因此眼睛可以被看作是心脏的窗口。计算机化的血管分离是自动化微血管分析的关键。我们使用了活动学习，创建了240个人工制定的血管分离（A/V），由15名医学生和一名眼科医生审核，并开发了LUNet，一种新的深度学习架构，用于高分辨率血管分离。LUNet架构包括一个双扩展 convolutional block，用于提高模型的感知范围和减少参数计数。此外，LUNet还有一个长尾，用于在高分辨率下精细化分割。我们定义了一个自适应损失函数，以优化血管之间的连续性。LUNet在本地测试集上以及四个外部测试集上，与两种现有的分割算法进行比较，显著地超越它们。我们将新创建的数据集公开访问（在出版后）。
</details></li>
</ul>
<hr>
<h2 id="From-Capture-to-Display-A-Survey-on-Volumetric-Video"><a href="#From-Capture-to-Display-A-Survey-on-Volumetric-Video" class="headerlink" title="From Capture to Display: A Survey on Volumetric Video"></a>From Capture to Display: A Survey on Volumetric Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05658">http://arxiv.org/abs/2309.05658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Jin, Kaiyuan Hu, Junhua Liu, Fangxin Wang, Xue Liu</li>
<li>for: This paper provides a comprehensive review of the existing literature on volumetric video services, with a focus on the challenges and opportunities in this field.</li>
<li>methods: The paper discusses various methodologies for each stage of the volumetric video service pipeline, including capturing, compression, transmission, rendering, and display techniques.</li>
<li>results: The paper explores various applications enabled by volumetric video services and identifies potential research challenges and opportunities in this field, with the aim of bringing the vision of volumetric video to fruition.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文为探讨现有的volumetric video服务相关文献而进行了全面的审视，并评估了这一领域的挑战和机遇。</li>
<li>methods: 论文讨论了volumetric video服务管道中每个阶段的方法ologies，包括捕捉、压缩、传输、渲染和显示技术。</li>
<li>results: 论文探讨了volumetric video服务所带来的各种应用程序，并确定了这一领域的研究挑战和机遇，以便实现volumetric video的视野。<details>
<summary>Abstract</summary>
Volumetric video, which offers immersive viewing experiences, is gaining increasing prominence. With its six degrees of freedom, it provides viewers with greater immersion and interactivity compared to traditional videos. Despite their potential, volumetric video services poses significant challenges. This survey conducts a comprehensive review of the existing literature on volumetric video. We firstly provide a general framework of volumetric video services, followed by a discussion on prerequisites for volumetric video, encompassing representations, open datasets, and quality assessment metrics. Then we delve into the current methodologies for each stage of the volumetric video service pipeline, detailing capturing, compression, transmission, rendering, and display techniques. Lastly, we explore various applications enabled by this pioneering technology and we present an array of research challenges and opportunities in the domain of volumetric video services. This survey aspires to provide a holistic understanding of this burgeoning field and shed light on potential future research trajectories, aiming to bring the vision of volumetric video to fruition.
</details>
<details>
<summary>摘要</summary>
三维视频技术在 immerse 观看体验方面占据着越来越重要的地位。它提供了六个自由度，让观看者感受到更加深入和互动性，比传统视频更加出色。然而，三维视频服务还面临着一系列挑战。这份调查提供了三维视频服务的全面评估，首先提供三维视频服务的总体框架，然后讨论三维视频的前提条件，包括表示、开放数据集和质量评估指标。接着，我们详细介绍了每个三维视频服务管道阶段的方法ologies，包括捕捉、压缩、传输、渲染和显示技术。最后，我们探讨了三维视频服务在不同领域的应用，以及这个领域的研究挑战和机遇。这份调查的目的是为了提供三维视频服务领域的总体认知，并照料未来研究方向，以实现三维视频的视野。
</details></li>
</ul>
<hr>
<h2 id="A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images"><a href="#A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images" class="headerlink" title="A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images"></a>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05446">http://arxiv.org/abs/2309.05446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medcai/l2snet">https://github.com/medcai/l2snet</a></li>
<li>paper_authors: Linghan Cai, Jianhao Huang, Zihang Zhu, Jinpeng Lu, Yongbing Zhang<br>for: 本研究旨在提出一种localization-to-segmentation框架(L2SNet)，用于精准诊断肿瘤。methods: 本方法首先在肿瘤localization阶段使用了可能的肿瘤含气扩散矩阵(PET)和 Computed Tomography(CT)图像进行定位，然后使用定位提示符进行肿瘤分割。results: 在MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET&#x2F;CT挑战数据集上进行实验后，我们的方法在预测测试集上取得了竞争力强的结果，并列入了前7名。<details>
<summary>Abstract</summary>
Fluorodeoxyglucose (FDG) positron emission tomography(PET) combined with computed tomography (CT) is considered the primary solution for detecting some cancers, such as lung cancer and melanoma. Automatic segmentation of tumors in PET/CT images can help reduce doctors' workload, thereby improving diagnostic quality. However, precise tumor segmentation is challenging due to the small size of many tumors and the similarity of high-uptake normal areas to the tumor regions. To address these issues, this paper proposes a localization-to-segmentation framework (L2SNet) for precise tumor segmentation. L2SNet first localizes the possible lesions in the lesion localization phase and then uses the location cues to shape the segmentation results in the lesion segmentation phase. To further improve the segmentation performance of L2SNet, we design an adaptive threshold scheme that takes the segmentation results of the two phases into consideration. The experiments with the MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset show that our method achieved a competitive result and was ranked in the top 7 methods on the preliminary test set. Our work is available at: https://github.com/MedCAI/L2SNet.
</details>
<details>
<summary>摘要</summary>
fluorodeoxyglucose (FDG) positron emission tomography（PET）combined with computed tomography（CT）被视为检测一些肿瘤的主要解决方案，如肺癌和黑色素瘤。自动 segmentation of tumors in PET/CT images可以帮助医生减少工作量，从而提高诊断质量。然而，精准肿瘤 segmentation 是具有挑战性，因为许多肿瘤的Size 小，而高uptake normal areas 和肿瘤区域之间的相似性也使 segmentation 更加困难。为解决这些问题，本文提出了一个 localization-to-segmentation 框架（L2SNet），用于精准肿瘤 segmentation。L2SNet 先在 lesion localization 阶段确定可能的肿瘤，然后使用 Location 提示来形成 segmentation 结果。为了进一步提高 L2SNet 的 segmentation 性能，我们设计了一种适应reshold 方案，该方案根据 segmentation 结果进行调整。经过对 MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge 数据集的实验，我们的方法达到了竞争力强的结果，在预测测试集上排名第 7。我们的工作可以在：https://github.com/MedCAI/L2SNet 中找到。
</details></li>
</ul>
<hr>
<h2 id="Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction"><a href="#Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction" class="headerlink" title="Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction"></a>Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05406">http://arxiv.org/abs/2309.05406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghui Liu, Elies Fuster-Garcia, Ivar Thokle Hovden, Donatas Sederevicius, Karoline Skogen, Bradley J MacIntosh, Edvard Grødem, Till Schellhorn, Petter Brandal, Atle Bjørnerud, Kyrre Eeg Emblem</li>
<li>for: 这篇论文旨在模型Diffuse gliomas的肿瘤增长，以帮助临床决策。</li>
<li>methods: 这篇论文使用了一个 novel end-to-end 网络，能够生成未来肿瘤的图像和实际的 MRI 影像，以及不同的治疗方案下肿瘤增长的预测。这个模型基于了进步的扩散几率模型和深度分类神经网络。</li>
<li>results: 这篇论文的模型已经在一系列任务上表现出色，包括生成高品质的伪造 MRI 影像、时间序列肿瘤分类、和不确定性估计。与治疗认知的生成 MRI 结合，肿瘤增长预测 WITH 不确定性估计可以提供有用的信息供临床决策。<details>
<summary>Abstract</summary>
Diffuse gliomas are malignant brain tumors that grow widespread through the brain. The complex interactions between neoplastic cells and normal tissue, as well as the treatment-induced changes often encountered, make glioma tumor growth modeling challenging. In this paper, we present a novel end-to-end network capable of generating future tumor masks and realistic MRIs of how the tumor will look at any future time points for different treatment plans. Our model is built upon cutting-edge diffusion probabilistic models and deep-segmentation neural networks. We extended a diffusion model to include sequential multi-parametric MRI and treatment information as conditioning input to guide the generative diffusion process. This allows us to estimate tumor growth at any given time point. We trained the model using real-world postoperative longitudinal MRI data with glioma tumor growth trajectories represented as tumor segmentation maps over time. The model has demonstrated promising performance across a range of tasks, including the generation of high-quality synthetic MRIs with tumor masks, time-series tumor segmentations, and uncertainty estimation. Combined with the treatment-aware generated MRIs, the tumor growth predictions with uncertainty estimates can provide useful information for clinical decision-making.
</details>
<details>
<summary>摘要</summary>
Diffuse gliomas 是肿瘤性脑肿，通过脑内多个区域延伸生长。受到肿瘤细胞和正常组织之间复杂互动以及治疗引起的变化影响，肿瘤增长模型非常困难。在这篇论文中，我们提出了一种新的终端网络，能够生成未来肿瘤掩蔽图和真实的MRI图像，以反映不同治疗方案下肿瘤的增长情况。我们基于 cutting-edge 扩散概率模型和深度分割神经网络构建了这种模型。我们将扩散模型扩展到包括 sequential multi-parametric MRI 和治疗信息作为条件输入，以引导生成扩散过程。这使得我们可以估算肿瘤在任何时间点增长。我们使用了实际的postoperative longitudinal MRI数据，其中肿瘤增长轨迹由肿瘤分割图表示。模型在多个任务上表现出色，包括生成高质量的 sintetic MRI 图像、时序分割和不确定性估计。与治疗意识的生成MRIs相结合，肿瘤增长预测与不确定性估计可以为临床决策提供有用信息。
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT"><a href="#Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT" class="headerlink" title="Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT"></a>Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05405">http://arxiv.org/abs/2309.05405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Liu, Tong Tian, Weijin Xu, Lemeng Wang, Haoyuan Li, Huihua Yang</li>
<li>for: 这篇论文的目的是提出一个混合式监督框架，将部分 labels 和无labels 数据结合，以提高腹部器官和肿瘤分类的精度。</li>
<li>methods: 这篇论文使用了一个分阶段的分类管线和整个量子资料来进行分类，并使用了自己教学和平均教师的方法来提高分类精度。</li>
<li>results: 论文的实验结果显示，这个方法在FLARE2023的验证集上实现了出色的分类性能，同时也实现了快速且具有低资源的模型测试。在验证集上，这个方法的平均DSC分数为89.79%和45.55%，而平均运行时间和GPU内存使用量分别为11.25秒和9627.82MB。<details>
<summary>Abstract</summary>
Abdominal organ and tumour segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manual assessment is inherently subjective with considerable inter- and intra-expert variability. In the paper, we propose a hybrid supervised framework, StMt, that integrates self-training and mean teacher for the segmentation of abdominal organs and tumors using partially labeled and unlabeled data. We introduce a two-stage segmentation pipeline and whole-volume-based input strategy to maximize segmentation accuracy while meeting the requirements of inference time and GPU memory usage. Experiments on the validation set of FLARE2023 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. Our method achieved an average DSC score of 89.79\% and 45.55 \% for the organs and lesions on the validation set and the average running time and area under GPU memory-time cure are 11.25s and 9627.82MB, respectively.
</details>
<details>
<summary>摘要</summary>
腹部器官和肿瘤分割有很多重要的临床应用，如器官量化、手术规划和疾病诊断。然而，手动评估是内在地主观，具有较大的交叉和内部专家变化。在文章中，我们提出了一种混合supervised框架，StMt，以实现腹部器官和肿瘤分割，使用部分标注和无标注数据。我们提出了一种两个阶段分割管道和整体量化输入策略，以最大化分割精度，同时满足推断时间和GPU内存使用的要求。在FLARE2023验证集上，我们的方法实现了出色的分割性能，同时具有快速和低资源模型推断。我们的方法在验证集上的平均DSC分数为89.79%和45.55%，平均运行时间和GPU内存使用率分别为11.25s和9627.82MB。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-real-time-3D-scene-reconstruction-with-SLAM-methods-in-embedded-systems"><a href="#A-survey-on-real-time-3D-scene-reconstruction-with-SLAM-methods-in-embedded-systems" class="headerlink" title="A survey on real-time 3D scene reconstruction with SLAM methods in embedded systems"></a>A survey on real-time 3D scene reconstruction with SLAM methods in embedded systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05349">http://arxiv.org/abs/2309.05349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Picard, Stephane Chevobbe, Mehdi Darouich, Jean-Yves Didier</li>
<li>for: 本文探讨了在交通系统中的同时定位和地图构建（SLAM）三维重建问题，尤其是在无人机、服务机器人和移动AR&#x2F;VR设备中。</li>
<li>methods: 本文介绍了一种基于图像的视觉三维场景重建管道的实现，包括感知器到3D重建的传统SLAM管道，以及可能的深度学习应用。</li>
<li>results: 文章讨论了在具有有限资源的硬件平台上实现高级功能的挑战，以及在实时地理位置和重建中做出的质量和资源投入的负担。<details>
<summary>Abstract</summary>
The 3D reconstruction of simultaneous localization and mapping (SLAM) is an important topic in the field for transport systems such as drones, service robots and mobile AR/VR devices. Compared to a point cloud representation, the 3D reconstruction based on meshes and voxels is particularly useful for high-level functions, like obstacle avoidance or interaction with the physical environment. This article reviews the implementation of a visual-based 3D scene reconstruction pipeline on resource-constrained hardware platforms. Real-time performances, memory management and low power consumption are critical for embedded systems. A conventional SLAM pipeline from sensors to 3D reconstruction is described, including the potential use of deep learning. The implementation of advanced functions with limited resources is detailed. Recent systems propose the embedded implementation of 3D reconstruction methods with different granularities. The trade-off between required accuracy and resource consumption for real-time localization and reconstruction is one of the open research questions identified and discussed in this paper.
</details>
<details>
<summary>摘要</summary>
三维重建（3D reconstruction）是交通系统如无人机、服务机器人和移动AR/VR设备等领域的重要话题。相比点云表示，基于网格和体积的3D重建更有用于高级功能，如避免障碍物或与物理环境交互。本文介绍了嵌入式硬件平台上视觉基于的3D场景重建管线的实现。实时性、内存管理和低功耗是嵌入式系统的关键要求。文章描述了潜在使用深度学习的SLAM管线，以及嵌入式实现高级功能的方法。不同粒度的3D重建方法的嵌入实现被详细介绍。文章还讨论了实时地理位和重建所需的资源占用和精度质量之间的负担。
</details></li>
</ul>
<hr>
<h2 id="AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration"><a href="#AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration" class="headerlink" title="AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration"></a>AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05271">http://arxiv.org/abs/2309.05271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-autofuse">https://github.com/mungomeng/registration-autofuse</a></li>
<li>paper_authors: Mingyuan Meng, Michael Fulham, Dagan Feng, Lei Bi, Jinman Kim</li>
<li>for: 这 paper 的目的是提出一种数据驱动的拟合策略，以便在医疗图像注射中实现高效的拟合。</li>
<li>methods: 这 paper 使用的方法是基于深度神经网络 (DNNs) 的拟合策略，并提出了一种自动化拟合策略（AutoFuse），以便在不同的网络位置进行信息融合。</li>
<li>results: 这 paper 的实验结果表明，AutoFuse 可以在两个 Well-benchmarked 医疗注射任务（inter- 和 intra-patient registration）上超过现有的无监督和半监督拟合方法。<details>
<summary>Abstract</summary>
Deformable image registration aims to find a dense non-linear spatial correspondence between a pair of images, which is a crucial step for many medical tasks such as tumor growth monitoring and population analysis. Recently, Deep Neural Networks (DNNs) have been widely recognized for their ability to perform fast end-to-end registration. However, DNN-based registration needs to explore the spatial information of each image and fuse this information to characterize spatial correspondence. This raises an essential question: what is the optimal fusion strategy to characterize spatial correspondence? Existing fusion strategies (e.g., early fusion, late fusion) were empirically designed to fuse information by manually defined prior knowledge, which inevitably constrains the registration performance within the limits of empirical designs. In this study, we depart from existing empirically-designed fusion strategies and develop a data-driven fusion strategy for deformable image registration. To achieve this, we propose an Automatic Fusion network (AutoFuse) that provides flexibility to fuse information at many potential locations within the network. A Fusion Gate (FG) module is also proposed to control how to fuse information at each potential network location based on training data. Our AutoFuse can automatically optimize its fusion strategy during training and can be generalizable to both unsupervised registration (without any labels) and semi-supervised registration (with weak labels provided for partial training data). Extensive experiments on two well-benchmarked medical registration tasks (inter- and intra-patient registration) with eight public datasets show that our AutoFuse outperforms state-of-the-art unsupervised and semi-supervised registration methods.
</details>
<details>
<summary>摘要</summary>
图像变形注册的目标是找到两个图像之间的密集非线性空间匹配，这是医学任务中的关键步骤，如肿瘤增长监测和人口分析。近年来，深度神经网络（DNNs）在执行快速端到端注册方面表现出色，但是DNN-based注册需要挖掘图像中的空间信息，并将这些信息融合以定义空间匹配。这引出了一个关键问题：何种最佳的融合策略来定义空间匹配？现有的融合策略（例如早期融合、晚期融合）是基于手动定义的先验知识，这会限制注册性能在Empirical设计的限制下。在这项研究中，我们弃寸现有的Empirical设计的融合策略，开发了一种数据驱动的融合策略。为实现这一点，我们提出了自动融合网络（AutoFuse），它可以在网络中自动选择融合的位置，并通过一个控制器模块（Fusion Gate，FG）来控制如何融合信息。我们的AutoFuse可以在训练时自动优化融合策略，并可以泛化到无标签注册（无标注注册）和半标签注册（半标注注册）中。我们在两个well-benchmarked医学注册任务（Inter-和Intra-patient注册）上进行了八个公共数据集的实验，结果表明，我们的AutoFuse在无标签注册和半标签注册中都能够超越当前的状态艺。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/eess.IV_2023_09_11/" data-id="clmjn91r000hx0j8865lndzvg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
