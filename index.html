
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.SD_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T15:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.SD_2023_11_15/">cs.SD - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance"><a href="#AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance" class="headerlink" title="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance"></a>AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09030">http://arxiv.org/abs/2311.09030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/ai-soundscape">https://github.com/yuanbo2020/ai-soundscape</a></li>
<li>paper_authors: Yuanbo Hou, Qiaoqiao Ren, Huizhong Zhang, Andrew Mitchell, Francesco Aletta, Jian Kang, Dick Botteldooren</li>
<li>for: 本研究旨在开发一种基于人工智能的双分支卷积神经网络（DCNN-CaF），用于自动地描述声景环境，包括声音识别和评价。</li>
<li>methods: 本研究使用Delta数据集，其包含人类标注的声音来源标签和听到的不适程度。基于Delta数据集，提出了一种基于DCNN-CaF的声音源分类（SSC）和人类对声音不适程度评分（ARP）预测模型。</li>
<li>results: 研究结果表明，（1）使用听力和Mel特征时，DCNN-CaF模型的性能比使用单一特征时高；（2）DCNN-CaF模型与其他常见的人工智能模型和声景相关的传统机器学习方法相比，在SSC和ARP任务上表现出色；（3）对声音来源和不适程度之间的关系，人类和DCNN-CaF模型之间存在一定的相似性。<details>
<summary>Abstract</summary>
Soundscape studies typically attempt to capture the perception and understanding of sonic environments by surveying users. However, for long-term monitoring or assessing interventions, sound-signal-based approaches are required. To this end, most previous research focused on psycho-acoustic quantities or automatic sound recognition. Few attempts were made to include appraisal (e.g., in circumplex frameworks). This paper proposes an artificial intelligence (AI)-based dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape characterization, including sound recognition and appraisal. Using the DeLTA dataset containing human-annotated sound source labels and perceived annoyance, the DCNN-CaF is proposed to perform sound source classification (SSC) and human-perceived annoyance rating prediction (ARP). Experimental findings indicate that (1) the proposed DCNN-CaF using loudness and Mel features outperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with cross-attention fusion outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the SSC and ARP tasks. (3) Correlation analysis reveals that the relationship between sound sources and annoyance is similar for humans and the proposed AI-based DCNN-CaF model. (4) Generalization tests show that the proposed model's ARP in the presence of model-unknown sound sources is consistent with expert expectations and can explain previous findings from the literature on sound-scape augmentation.
</details>
<details>
<summary>摘要</summary>
声域研究通常尝试通过访问用户来捕捉声音环境的感知和理解。然而，为长期监测或评估 intervención，声音信号基于的方法是必要的。为此，前一些研究主要集中在 psycho-acoustic 量或自动声音识别方面。只有一些尝试了评估（例如，在 circumplex 框架中）。这篇文章提出了一种基于人工智能（AI）的 dual-branch 卷积神经网络（DCNN-CaF），用于自动声capeCharacterization，包括声音识别和评估。使用包含人类注解的声音源标签和感知的压力报告的DeLTA数据集，提议的 DCNN-CaF 可以进行声音源类型分类（SSC）和人类感知压力评估预测（ARP）。实验结果表明：1. 提议的 DCNN-CaF 使用响度和Mel特征表现得更好于只使用一个特征。2. 提议的 DCNN-CaF  WITH cross-attention 融合 OUTPERforms 其他常见的 AI 基于模型和声cape 相关的传统机器学习方法在 SSC 和 ARP 任务上。3. 相关分析表明，人类和提议的 AI 基于 DCNN-CaF 模型之间的声音源和感知之间的关系类似。4. 总结测试表明，提议的模型的 ARP 在未知模型声音源的存在下是与专家预期一致的，并且可以解释过去Literature 中声cape 增强的结果。
</details></li>
</ul>
<hr>
<h2 id="CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes"><a href="#CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes" class="headerlink" title="CREPE Notes: A new method for segmenting pitch contours into discrete notes"></a>CREPE Notes: A new method for segmenting pitch contours into discrete notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08884">http://arxiv.org/abs/2311.08884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Riley, Simon Dixon</li>
<li>for:  automatic music transcription and note segmentation</li>
<li>methods:  building on CREPE, a state-of-the-art monophonic pitch tracking solution, and using a simple and effective post-processing method</li>
<li>results:  state-of-the-art results on two challenging datasets of monophonic instrumental music, with a 97% reduction in the total number of parameters used compared to other deep learning-based methods.Here’s the full Chinese text:</li>
<li>for:  automatic music transcription和Note segmentation</li>
<li>methods:  builds on CREPE, a state-of-the-art monophonic pitch tracking solution, and uses a simple and effective post-processing method</li>
<li>results:  state-of-the-art results on two challenging datasets of monophonic instrumental music, with a 97% reduction in the total number of parameters used compared to other deep learning-based methods.<details>
<summary>Abstract</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details>
<details>
<summary>摘要</summary>
“追踪单一数位音频（f0）的问题已经是一个解决了的问题，有多种解决方案可以达到99%的准确性。但是相关的音乐转换任务仍需要一个额外的处理步骤，将f0走势分成多个独立的音符。这个子任务是为了启用多种应用，包括音乐学研究和象征音乐生成。基于CREPE的单一数位抽象网络，我们提出了一个简单而有效的方法来处理CREPE的输出，以达到单一音符分配。我们的方法在两个挑战性的单一数位乐器音乐数据集上表现出了国际级的成绩。我们的方法也比其他深度学习基于方法具有97%的减少参数数量。”Note: Please keep in mind that the translation is done by a machine and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model"><a href="#Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model" class="headerlink" title="Multi-objective Non-intrusive Hearing-aid Speech Assessment Model"></a>Multi-objective Non-intrusive Hearing-aid Speech Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08878">http://arxiv.org/abs/2311.08878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Tien Chiang, Szu-Wei Fu, Hsin-Min Wang, Yu Tsao, John H. L. Hansen</li>
<li>For: This paper is written for the purpose of developing a non-intrusive speech assessment model for individuals with hearing impairments.* Methods: The paper uses deep learning models, specifically pre-trained self-supervised learning (SSL) models, to predict speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns.* Results: The study shows that the proposed model, called HASA-Net Large, achieves significant improvements in speech quality and intelligibility predictions compared to using spectrograms as input, and that incorporating SSL models results in greater transferability to out-of-distribution (OOD) datasets.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种不侵入性的听力评估模型，用于评估听力障碍者。</li>
<li>methods: 该论文使用深度学习模型，具体来说是预训练的自动标注学习（SSL）模型，以输入听力信号和指定的听力损耗模式来预测听力质量和理解度分数。</li>
<li>results: 研究显示，提议的模型——HASA-Net Large——在使用预训练SSL模型的情况下，对听力质量和理解度预测具有显著的改善，并且在OOD数据集上表现出更好的传送性。<details>
<summary>Abstract</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details>
<details>
<summary>摘要</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.Here's the translation in Traditional Chinese: Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection"><a href="#Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection" class="headerlink" title="Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection"></a>Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08829">http://arxiv.org/abs/2311.08829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhou, Dongxing Xu, Haoran Wei, Yanhua Long</li>
<li>for: 这个研究旨在提高机器学习方法的潜在问题检测精度。</li>
<li>methods: 我们提出了一个新的Autoencoder（AE）基本方法，包括在AE中插入一个辅助分类器，以增强潜在问题检测的多任务学习方式。我们还设计了一个群体基本解oder结构，以及一个适应损失函数，以允许模型获得领域专门知识。</li>
<li>results: 我们在DCASE 2021 Task 2的开发集上进行了实验，结果显示，我们的方法在七台机器上的试验集上获得了相对提高13.11%和15.20%的平均AUC，比官方AE和MobileNetV2更高。<details>
<summary>Abstract</summary>
In industry, machine anomalous sound detection (ASD) is in great demand. However, collecting enough abnormal samples is difficult due to the high cost, which boosts the rapid development of unsupervised ASD algorithms. Autoencoder (AE) based methods have been widely used for unsupervised ASD, but suffer from problems including 'shortcut', poor anti-noise ability and sub-optimal quality of features. To address these challenges, we propose a new AE-based framework termed AEGM. Specifically, we first insert an auxiliary classifier into AE to enhance ASD in a multi-task learning manner. Then, we design a group-based decoder structure, accompanied by an adaptive loss function, to endow the model with domain-specific knowledge. Results on the DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details>
<details>
<summary>摘要</summary>
在工业中，机器异常声音检测（ASD）的需求很大。然而，收集异常样本很costly，这使得不supervised ASD算法的快速发展得到推动。基于Autoencoder（AE）的方法在无监督ASD方面广泛使用，但它们受到短cut、anti-noise能力不够和特征质量不佳等问题困扰。为了解决这些挑战，我们提出了一个新的AE基于框架，称为AEGM。具体来说，我们首先在AE中插入一个辅助分类器，以增强ASD在多任务学习方式下。然后，我们设计了一个群体基本解码结构，并附加了一个适应损失函数，以赋予模型域pecific知识。在DCASE 2021任务2的开发集上，我们的方法在七台机器的测试集上实现了相对提升13.11%和15.20%的平均AUC，相比官方AE和MobileNetV2。
</details></li>
</ul>
<hr>
<h2 id="CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation"><a href="#CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation" class="headerlink" title="CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation"></a>CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08670">http://arxiv.org/abs/2311.08670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 提高voice conversion质量的关键是更好地分离演说表示。</li>
<li>methods: 我们提议使用增强的负样本选择和高级 Style Modeling 来解决在相似speaker之间的性能降低问题。</li>
<li>results: 我们的方法在voice conversion任务中比前一个工作更高效。<details>
<summary>Abstract</summary>
Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.
</details>
<details>
<summary>摘要</summary>
“更好地分离语音表现是voice对应质量提升的关键。最近，contrastive learning被应用到voice对应成功，但是在相似的话者之间的对应性会下降。因此，我们提出了增强负类样本选择的方法。具体来说，我们根据提出的话者融合模块创建困难的负类样本，以提高话者Encoder的学习能力。此外，为了考虑细致的话者风格，我们还使用参考Encoder提取细致的风格，并对全局风格进行增强的对应学习。实验结果显示，我们的方法在voice对应任务中具有较高的表现。”
</details></li>
</ul>
<hr>
<h2 id="EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis"><a href="#EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis" class="headerlink" title="EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis"></a>EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08667">http://arxiv.org/abs/2311.08667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Zhu, Yutong Wen, Marc-André Carbonneau, Zhiyao Duan</li>
<li>for: 这篇论文目的是提出一种基于扩散模型的生成模型，用于生成各种听起来的声音。</li>
<li>methods: 该模型使用扩散过程在spectrogram域中进行生成，并使用高效的杜因抽象器来提高生成效果。</li>
<li>results: 模型在DCASE2023 foley音频生成比赛中达到了顶尖基eline的性能，且只用了10步或50步。此外，研究人员还发现了一种可能的问题，即扩散基于音频生成模型通常会生成与训练数据高度相似的样本。Here is the full text in Traditional Chinese:这篇论文的目的是提出一种基于扩散模型的生成模型，用于生成各种听起来的声音。该模型使用扩散过程在spectrogram域中进行生成，并使用高效的杜因抽象器来提高生成效果。研究人员发现，模型在DCASE2023 foley音频生成比赛中达到了顶尖基eline的性能，且只用了10步或50步。此外，研究人员还发现了一种可能的问题，即扩散基于音频生成模型通常会生成与训练数据高度相似的样本。Please note that the text is in Simplified Chinese, as requested.<details>
<summary>Abstract</summary>
Audio diffusion models can synthesize a wide variety of sounds. Existing models often operate on the latent domain with cascaded phase recovery modules to reconstruct waveform. This poses challenges when generating high-fidelity audio. In this paper, we propose EDMSound, a diffusion-based generative model in spectrogram domain under the framework of elucidated diffusion models (EDM). Combining with efficient deterministic sampler, we achieved similar Fr\'echet audio distance (FAD) score as top-ranked baseline with only 10 steps and reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound generation benchmark. We also revealed a potential concern regarding diffusion based audio generation models that they tend to generate samples with high perceptual similarity to the data from training data. Project page: https://agentcooper2002.github.io/EDMSound/
</details>
<details>
<summary>摘要</summary>
Audio 扩散模型可生成各种声音。现有模型通常在幂征频域内使用级联阶段恢复模块来重construct波形，这会对高精度音频生成带来挑战。在这篇论文中，我们提议EDMSound，一种基于扩散模型的生成模型，在幂征频域内使用明确的扩散模型（EDM）框架。与高效的权值抽象器结合，我们在50步内达到了状态的权值评价（FAD）分数，并在10步内达到了同等级别的基eline。我们还发现了扩散基于音频生成模型的一种潜在问题，即它们往往会生成与训练数据高度相似的样本。项目页面：https://agentcooper2002.github.io/EDMSound/
</details></li>
</ul>
<hr>
<h2 id="Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization"><a href="#Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization" class="headerlink" title="Multi-channel Conversational Speaker Separation via Neural Diarization"></a>Multi-channel Conversational Speaker Separation via Neural Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08630">http://arxiv.org/abs/2311.08630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Taherian, DeLiang Wang</li>
<li>for: 提高自动语音识别（ASR）系统在会议环境中的性能，解决单个说话者的ASR系统表现下降的问题。</li>
<li>methods: 提出一种基于神经网络摘要（SSND）的多渠道框架，利用终端到终端摘要系统来标识每个个体说话者的语音活动。</li>
<li>results: 在开放的 LibriCSS  dataset 上评估了提案的摘要和分离方法，与状态之前的摘要和 ASR 结果相比，提高了性能的大幅度。<details>
<summary>Abstract</summary>
When dealing with overlapped speech, the performance of automatic speech recognition (ASR) systems substantially degrades as they are designed for single-talker speech. To enhance ASR performance in conversational or meeting environments, continuous speaker separation (CSS) is commonly employed. However, CSS requires a short separation window to avoid many speakers inside the window and sequential grouping of discontinuous speech segments. To address these limitations, we introduce a new multi-channel framework called "speaker separation via neural diarization" (SSND) for meeting environments. Our approach utilizes an end-to-end diarization system to identify the speech activity of each individual speaker. By leveraging estimated speaker boundaries, we generate a sequence of embeddings, which in turn facilitate the assignment of speakers to the outputs of a multi-talker separation model. SSND addresses the permutation ambiguity issue of talker-independent speaker separation during the diarization phase through location-based training, rather than during the separation process. This unique approach allows multiple non-overlapped speakers to be assigned to the same output stream, making it possible to efficiently process long segments-a task impossible with CSS. Additionally, SSND is naturally suitable for speaker-attributed ASR. We evaluate our proposed diarization and separation methods on the open LibriCSS dataset, advancing state-of-the-art diarization and ASR results by a large margin.
</details>
<details>
<summary>摘要</summary>
当处理重叠的语音时，自动语音识别（ASR）系统的性能会受到重叠的影响，因为它们是单个说话人的设计。为了提高 ASR 在会议环境中的性能，连续说话分离（CSS）常常被使用。然而，CSS 需要一个短暂的分离窗口，以避免在窗口内有多个说话人和顺序分组不连续的语音段。为了解决这些限制，我们介绍了一种新的多通道框架，即“基于神经分类的说话人分离”（SSND），用于会议环境。我们的方法利用一个端到端的分类系统来识别每个个人的语音活动。通过利用估计的说话人边界，我们生成一个序列的嵌入，从而促进将说话人分配到多个说话人分离模型的输出中。SSND 通过在分类阶段使用位置基于的训练来解决 talker-independent  speaker separation 中的 permutation ambiguity 问题，而不是在分离过程中。这种独特的方法使得可以有效地处理长段语音，这是 CSS 不可能实现的。此外，SSND 自然适合 speaker-attributed ASR。我们对 LibriCSS 开放 dataset 进行了我们的提议的分类和分离方法的评估，并在状态 искусственный智能中提高了 ASR 和分类的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.SD_2023_11_15/" data-id="clp53jwvy011fyp8869xecspg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T13:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CV_2023_11_15/">cs.CV - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans"><a href="#Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans" class="headerlink" title="Predicting Spine Geometry and Scoliosis from DXA Scans"></a>Predicting Spine Geometry and Scoliosis from DXA Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09424">http://arxiv.org/abs/2311.09424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman</li>
<li>for: 估算 DXA 扫描中脊梁 curvature</li>
<li>methods: 使用神经网络预测中脊梁曲线，然后使用积分方法确定脊梁曲线上的曲率</li>
<li>results: 比往期工作（Jamaludin et al. 2018）有更好的性能，并可以用最大曲率作为评价脊梁弯曲的分数函数<details>
<summary>Abstract</summary>
Our objective in this paper is to estimate spine curvature in DXA scans. To this end we first train a neural network to predict the middle spine curve in the scan, and then use an integral-based method to determine the curvature along the spine curve. We use the curvature to compare to the standard angle scoliosis measure obtained using the DXA Scoliosis Method (DSM). The performance improves over the prior work of Jamaludin et al. 2018. We show that the maximum curvature can be used as a scoring function for ordering the severity of spinal deformation.
</details>
<details>
<summary>摘要</summary>
我们的目标在这篇论文中是估计DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先使用神经网络预测扫描中的中脊梁弯曲度，然后使用积分方法确定脊梁弯曲度的各个点的弯曲度。我们使用弯曲度与DSM法（DXA扫描风扭度测量方法）所获取的标准风扭度比较。我们显示，使用最大弯曲度作为评分函数可以对脊梁弯曲度的严重程度进行排序。Here's the translation of the text into Traditional Chinese:我们的目标在这篇论文中是估计DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先使用神经网络预测扫描中的中脊梁弯曲度，然后使用积分方法确定脊梁弯曲度的各个点的弯曲度。我们使用弯曲度与DSM法（DXA扫描风扭度测量方法）所获取的标准风扭度比较。我们显示，使用最大弯曲度作为评分函数可以对脊梁弯曲度的严重程度进行排序。
</details></li>
</ul>
<hr>
<h2 id="Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research"><a href="#Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research" class="headerlink" title="Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research"></a>Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardiakh/syntheticallyenhanced">https://github.com/bardiakh/syntheticallyenhanced</a></li>
<li>paper_authors: Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U. Gamble, Hari M. Trivedi, Cody C. Wyles, Andrew B. Sellergren, Saptarshi Purkayastha, Bradley J. Erickson, Judy W. Gichoya</li>
<li>for: 这个研究旨在测试深度学习（DL）分类器在胸部X射像（CXR）分析中的性能，以及使用扩散模型生成的合成数据是否能够提高模型的准确性。</li>
<li>methods: 我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并训练了条件扩散数据模型（DDPMs）来生成合成前面影像。我们确保了合成图像对应原始数据中的人口和疾病特征。</li>
<li>results: 我们发现，将合成数据融入到真实数据中可以提高模型的准确性，特别是检测较少见的疾病。此外，使用仅合成数据进行训练的模型也可以将模型的性能提高到与使用真实数据进行训练的模型相当的水平。这表示合成数据可能可以补偿实际数据的短缺，对于训练具有优秀性的DL模型而言。但是，优秀的实际数据仍然保持优势。<details>
<summary>Abstract</summary>
Chest X-rays (CXR) are the most common medical imaging study and are used to diagnose multiple medical conditions. This study examines the impact of synthetic data supplementation, using diffusion models, on the performance of deep learning (DL) classifiers for CXR analysis. We employed three datasets: CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising diffusion probabilistic models (DDPMs) to generate synthetic frontal radiographs. Our approach ensured that synthetic images mirrored the demographic and pathological traits of the original data. Evaluating the classifiers' performance on internal and external datasets revealed that synthetic data supplementation enhances model accuracy, particularly in detecting less prevalent pathologies. Furthermore, models trained on synthetic data alone approached the performance of those trained on real data. This suggests that synthetic data can potentially compensate for real data shortages in training robust DL models. However, despite promising outcomes, the superiority of real data persists.
</details>
<details>
<summary>摘要</summary>
颈部X-射影（CXR）是医学影像研究最常用的方法，用于诊断多种医学疾病。本研究检查使用扩散模型生成的合成数据对深度学习（DL）分类器的表现的影响。我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory颈部X-射影，使用条件扩散概率模型（DDPM）来生成合成前视图预制图像。我们的方法确保了合成图像反映了原始数据中的人口和疾病特征。我们对内部和外部数据集进行评估，发现使用合成数据支持DL模型的准确率提高，特别是检测较少发生的疾病。此外，使用合成数据alone训练的模型在实际数据上达到了与实际数据训练的模型相同的性能。这表明合成数据可能可以补做实际数据短缺，培养Robust DL模型。然而，尽管结果很有前途，实际数据仍然占据优势。
</details></li>
</ul>
<hr>
<h2 id="MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains"><a href="#MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains" class="headerlink" title="MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains"></a>MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09401">http://arxiv.org/abs/2311.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Chen, Helen Zhou, Zachary C. Lipton</li>
<li>for: 本研究旨在探讨通过在不同领域的数据上进行自动增强的抽象表示学习，以提高医疗影像数据中的模型开发。</li>
<li>methods: 本研究使用了自适应对比表示学习（MoCo）的准则预训练，并对不同领域的数据进行跨领域传输学习。</li>
<li>results: 研究发现，在有限量的标注和无标注数据下，将自适应对比表示学习的模型训练在更大的对比领域上可以达到与在预训练领域内进行预训练相似或更好的性能，并且将预训练模型传输到相关领域可以提高性能。<details>
<summary>Abstract</summary>
Medical imaging data is often siloed within hospitals, limiting the amount of data available for specialized model development. With limited in-domain data, one might hope to leverage larger datasets from related domains. In this paper, we analyze the benefit of transferring self-supervised contrastive representations from moment contrast (MoCo) pretraining on out-of-distribution data to settings with limited data. We consider two X-ray datasets which image different parts of the body, and compare transferring from each other to transferring from ImageNet. We find that depending on quantity of labeled and unlabeled data, contrastive pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain, and pretraining on related domains leads to higher performance than if one were to use the ImageNet pretrained weights. Finally, we provide a preliminary way of quantifying similarity between datasets.
</details>
<details>
<summary>摘要</summary>
医疗成像数据经常受到医院内部的限制，因此有限的数据对特殊模型的开发具有有利的作用。在这篇论文中，我们分析了将自动掌握异构表示（MoCo）预训练的异构数据传递到有限数据的设置中的好处。我们考虑了两个X射线数据集，它们分别捕捉不同部位的影像，并比较了将每个数据集中的图像传递给另一个数据集，以及使用ImageNet预训练的模型。我们发现，根据数据集的量和无 labels数据，在大型异构数据集上进行自动掌握异构表示预训练可以与在预训练中的MoCo预训练具有相似或更高的性能，而将预训练模型在相关的领域中进行预训练可以高于使用ImageNet预训练的模型。最后，我们提供了一种初步的方法来衡量数据集之间的相似性。
</details></li>
</ul>
<hr>
<h2 id="RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior"><a href="#RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior" class="headerlink" title="RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior"></a>RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09361">http://arxiv.org/abs/2311.09361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadgardner/ns_reni">https://github.com/jadgardner/ns_reni</a></li>
<li>paper_authors: James A. D. Gardner, Bernhard Egger, William A. P. Smith</li>
<li>for: 这个论文的目的是提出一种基于神经网络的自然照明模型，以解决 inverse rendering 问题。</li>
<li>methods: 这个论文使用了conditional neural field representation和 transformer decoder，并使用了 Vector Neurons 技术来实现 equivariance 性。</li>
<li>results: 论文通过使用一个 curated  dataset of 1.6K HDR 环境图像，证明了其模型的可靠性和精度，并完成了 inverse rendering 任务和环境图像完成任务。<details>
<summary>Abstract</summary>
Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. This results in limitations for the inverse setting in terms of the expressivity of the illumination conditions, especially when taking specular reflections into account. We propose a conditional neural field representation based on a variational auto-decoder and a transformer decoder. We extend Vector Neurons to build equivariance directly into our architecture, and leveraging insights from depth estimation through a scale-invariant loss function, we enable the accurate representation of High Dynamic Range (HDR) images. The result is a compact, rotation-equivariant HDR neural illumination model capable of capturing complex, high-frequency features in natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. We share our PyTorch implementation, dataset and trained models at https://github.com/JADGardner/ns_reni
</details>
<details>
<summary>摘要</summary>
“倒排问题是一个不充分定义的问题。先前的工作强调了物体或场景形状或外观的确定，以解决这个问题。在这个工作中，我们则专注在自然照明的确定上。现有的方法通常使用圆柱对称照明或其他通用表示，并仅对参数进行简单的确定。这会导致倒排设定中的表现照明条件的限制，特别是当考虑到镜面反射时。我们提议一个基于征化自适应器和对映器构成的对映领域表示。我们将vector neuron扩展到建立对称性直接到我们的架构中，并参考depth estimation的尺度不敏感损失函数，以获得高动态范围（HDR）图像的精确表示。我们将这个模型训练在1.6K HDR环境地图中，并与传统表示进行比较，证明其适用于倒排 зада项和环境地图完成 task。我们在github上分享我们的PyTorch实现、数据集和训练模型，请参考https://github.com/JADGardner/ns_reni。”
</details></li>
</ul>
<hr>
<h2 id="Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change"><a href="#Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change" class="headerlink" title="Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change"></a>Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09346">http://arxiv.org/abs/2311.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Sun, Yan Hao, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni</li>
<li>for: 这个论文旨在探讨如何在建筑环境中处理大规模的空间和时间变化。</li>
<li>methods: 该论文使用了多个方法来处理3D点云数据，包括标准的对应 registration 和多对多 registration。</li>
<li>results: 研究发现现有的方法无法处理大规模的空间和时间变化，新的方法是需要特地设计来处理这类变化。<details>
<summary>Abstract</summary>
Building 3D geometric maps of man-made spaces is a well-established and active field that is fundamental to computer vision and robotics. However, considering the evolving nature of built environments, it is essential to question the capabilities of current mapping efforts in handling temporal changes. In addition, spatiotemporal mapping holds significant potential for achieving sustainability and circularity goals. Existing mapping approaches focus on small changes, such as object relocation or self-driving car operation; in all cases where the main structure of the scene remains fixed. Consequently, these approaches fail to address more radical changes in the structure of the built environment, such as geometry and topology. To this end, we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the spatiotemporal registration of 3D scenes undergoing large spatial and temporal change, ultimately creating one coherent spatiotemporal map. Specifically, the benchmark involves registering two or more partial 3D point clouds (fragments) from the same scene but captured from different spatiotemporal views. In addition to the standard pairwise registration, we assess the multi-way registration of multiple fragments that belong to any temporal stage. As part of NSS, we introduce a dataset of 3D point clouds recurrently captured in large-scale building indoor environments that are under construction or renovation. The NSS benchmark presents three scenarios of increasing difficulty, to quantify the generalization ability of point cloud registration methods over space (within one building and across buildings) and time. We conduct extensive evaluations of state-of-the-art methods on NSS. The results demonstrate the necessity for novel methods specifically designed to handle large spatiotemporal changes. The homepage of our benchmark is at http://nothing-stands-still.com.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立3D geometric图的人工空间是一个已有的和活跃的领域，对计算机视觉和机器人学是基础。然而，考虑到建筑环境的发展性，现有的地图努力的能力是否能够处理时间变化？此外，空间时间地图具有可持续性和循环性目标的潜在价值。现有的地图方法主要关注小型变化，如物体重新布置或自动驾驶车辆操作。这些方法无法处理更大的空间时间变化，如建筑结构的改变。为此，我们介绍了Nothing Stands Still（NSS）标准，该标准关注3D场景在不同的空间时间视图中进行大规模的空间时间 региSTRONGistration，最终创建一个一致的空间时间地图。特别是，NSS标准包括将多个不同空间时间视图中的3D点云（碎片）进行对比注册，以及评估多个时间阶段中的多个碎片之间的多方注册。作为NSS的一部分，我们提供了大规模建筑内部环境中逐渐更新的3D点云集。NSS标准提供三个不同难度的enario，用于评估点云注册方法的通用能力，包括空间内部（在一个建筑物内）和空间间（ между多个建筑物）的泛化能力，以及时间方向的泛化能力。我们对现有方法进行了广泛的评估，结果表明，现有的方法无法处理大规模的空间时间变化。NSS标准的主页是http://nothing-stands-still.com。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion"><a href="#Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion" class="headerlink" title="Single-Image 3D Human Digitization with Shape-Guided Diffusion"></a>Single-Image 3D Human Digitization with Shape-Guided Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09221">http://arxiv.org/abs/2311.09221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang</li>
<li>for: 这篇论文的目的是生成一个人体的全 körper 360度视图，从单个输入图像中生成高分辨率、具有一致性的图像。</li>
<li>methods: 该论文使用了高能范围的2D扩散模型作为人体图像的外观先验，通过填充缺失区域来实现多视图图像的生成，并通过反推 Rendering 将多视图图像合并成一个完全纹理化的高分辨率3D模型。</li>
<li>results: 该论文的实验结果表明，其方法可以超越先前的方法，从单个图像中生成高质量、具有一致性的360度人体图像，并且可以处理复杂的服装和皮肤 texture。<details>
<summary>Abstract</summary>
We present an approach to generate a 360-degree view of a person with a consistent, high-resolution appearance from a single input image. NeRF and its variants typically require videos or images from different viewpoints. Most existing approaches taking monocular input either rely on ground-truth 3D scans for supervision or lack 3D consistency. While recent 3D generative models show promise of 3D consistent human digitization, these approaches do not generalize well to diverse clothing appearances, and the results lack photorealism. Unlike existing work, we utilize high-capacity 2D diffusion models pretrained for general image synthesis tasks as an appearance prior of clothed humans. To achieve better 3D consistency while retaining the input identity, we progressively synthesize multiple views of the human in the input image by inpainting missing regions with shape-guided diffusion conditioned on silhouette and surface normal. We then fuse these synthesized multi-view images via inverse rendering to obtain a fully textured high-resolution 3D mesh of the given person. Experiments show that our approach outperforms prior methods and achieves photorealistic 360-degree synthesis of a wide range of clothed humans with complex textures from a single image.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个输入图像中生成一个360度的人体视图，具有一致的高分辨率外观。NeRF和其变种通常需要不同视角的视频或图像。现有的方法，只有使用准确的3D扫描数据进行超vision。而最近的3D生成模型显示了人体数字化的3D一致性，但这些方法不能泛化到多样化的服装外观，并且结果缺乏真实感。与现有的工作不同，我们利用高容量2D扩散模型，已经预训练用于通用图像生成任务，作为衣服人体的外观先验。为了提高3D一致性，同时保持输入人物的身份，我们逐渐合成了输入图像中缺失的区域，使用形态指导的扩散条件，基于 outline和表面法向。然后，我们将这些合成的多视图图像进行 inverse rendering，以获得一个完全纹理化的高分辨率3D mesh。实验表明，我们的方法超过了先前的方法，并实现了从单个图像中渲染出一个具有较复杂的 texture 的360度人体视图，具有真实感。
</details></li>
</ul>
<hr>
<h2 id="DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model"><a href="#DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model" class="headerlink" title="DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"></a>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09217">http://arxiv.org/abs/2311.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang</li>
<li>for: 本研究的目的是提出一种基于 transformer 的 3D 生成方法，用于去噪多视图填充。</li>
<li>methods: 该方法使用一种基于 NeRF 的三平面表示，通过 NeRF 重建和渲染来去噪多视图图像，实现单stage 3D 生成在 $\sim$30s 内。</li>
<li>results: 我们在大规模多视图图像 dataset 上训练了 \textbf{DMV3D}，并在单个 A100 GPU 上实现了单stage 3D 生成。我们的结果表明，\textbf{DMV3D} 可以在不访问 3D 资产的情况下，实现高质量的单像 reconstruction 和文本到 3D 生成。<details>
<summary>Abstract</summary>
We propose \textbf{DMV3D}, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and can denoise noisy multi-view images via NeRF reconstruction and rendering, achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train \textbf{DMV3D} on large-scale multi-view image datasets of highly diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .
</details>
<details>
<summary>摘要</summary>
我们提出了\textbf{DMV3D}，一种新的3D生成方法，使用变换器基于3D大型重建模型来除噪多视射度。我们的重建模型包括三平面NeRF表示法，可以通过NeRF重建和渲染，将噪声多视图图像进行去噪，实现单stage 3D生成在$\sim$30s内的单个A100 GPU上。我们在大规模多视图图像数据集上培养\textbf{DMV3D}，使用仅图像重建损失进行训练，不需要访问3D资产。我们示出了单个图像重建问题中的状态足够结果，需要采用概率模型来描述未经见到的对象部分，以生成具有锐利тексту化的多种重建。我们还显示了在文本到3D生成中的高质量结果，超过了前一代3D噪声模型。我们的项目网站是：https://justimyhxu.github.io/projects/dmv3d/。
</details></li>
</ul>
<hr>
<h2 id="ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy"><a href="#ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy" class="headerlink" title="ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy"></a>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09215">http://arxiv.org/abs/2311.09215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirill-vish/beyond-inet">https://github.com/kirill-vish/beyond-inet</a></li>
<li>paper_authors: Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</li>
<li>for:  This paper aims to provide a comprehensive comparative analysis of different model architectures and training protocols for computer vision tasks, beyond just their ImageNet accuracy.</li>
<li>methods:  The authors use a variety of modern computer vision models, including ConvNets and Vision Transformers, and compare their performance on several tasks beyond ImageNet accuracy, such as mistake types, output calibration, transferability, and feature invariance.</li>
<li>results:  The authors find that while the models have similar ImageNet accuracies and compute requirements, they differ in many other aspects, highlighting the need for more nuanced analysis when choosing among different models.<details>
<summary>Abstract</summary>
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our code is available at https://github.com/kirill-vish/Beyond-INet.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉提供了多种模型选择，为特定应用场景选择合适的模型可能很吃力。传统上，不同模型架构和训练方法之间的比较通常基于ImageNet分类精度。然而，这一metric不能完全捕捉特定任务中模型表现的细节。在这项工作中，我们进行了深入的比较分析，探讨了不同模型在超参数和CLIP训练方法下的行为特性，包括模型错误类型、输出均衡、传输性和特征不变性等方面。虽然我们选择的模型在ImageNet精度和计算需求上相似，但我们发现它们在很多方面有所不同，包括模型错误类型、输出均衡、传输性和特征不变性等方面。这种多样性在模型特征上，不被传统的指标捕捉，强调了选择模型时需要更加细化的分析。我们的代码可以在https://github.com/kirill-vish/Beyond-INet上找到。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset"><a href="#Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset" class="headerlink" title="Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset"></a>Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09276">http://arxiv.org/abs/2311.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muthukumaran Ramasubramanian, Iksha Gurung, Shubhankar Gahlot, Ronny Hänsch, Andrew L. Molthan, Manil Maskey</li>
<li>for: 本研究旨在提供高精度的洪水覆盖面积推测方法，以便在紧急应急响应和恢复工作中使用。</li>
<li>methods: 本研究使用了Sentinel-1 C-Band Synthetic Aperture Radar（SAR）图像，并利用机器学习技术来准确地推测洪水覆盖面积。</li>
<li>results: 研究创造了一个标注了水体覆盖面积和洪水覆盖面积的数据集，并在这个数据集基础上进行了一系列的数据处理和模型训练。在 Competition 中，研究者还开放了该数据集，并邀请了社区成员提交自己的模型来测试。研究结果表明，使用机器学习技术可以准确地推测洪水覆盖面积，并且可以在不同的地区和不同的背景下提供高精度的推测结果。<details>
<summary>Abstract</summary>
Accurate detection of inundated water extents during flooding events is crucial in emergency response decisions and aids in recovery efforts. Satellite Remote Sensing data provides a global framework for detecting flooding extents. Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has proven to be useful in detecting water bodies due to low backscatter of water features in both co-polarized and cross-polarized SAR imagery. However, increased backscatter can be observed in certain flooded regions such as presence of infrastructure and trees - rendering simple methods such as pixel intensity thresholding and time-series differencing inadequate. Machine Learning techniques has been leveraged to precisely capture flood extents in flooded areas with bumps in backscatter but needs high amounts of labelled data to work desirably. Hence, we created a labeled known water body extent and flooded area extents during known flooding events covering about 36,000 sq. kilometers of regions within mainland U.S and Bangladesh. Further, We also leveraged citizen science by open-sourcing the dataset and hosting an open competition based on the dataset to rapidly prototype flood extent detection using community generated models. In this paper we present the information about the dataset, the data processing pipeline, a baseline model and the details about the competition, along with discussion on winning approaches. We believe the dataset adds to already existing datasets based on Sentinel-1C SAR data and leads to more robust modeling of flood extents. We also hope the results from the competition pushes the research in flood extent detection further.
</details>
<details>
<summary>摘要</summary>
检测洪水覆盖面积是应急应对和恢复努力中非常重要的一环。卫星Remote Sensing数据提供了全球检测洪水覆盖面积的基础。特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）成像数据表明可以准确检测水体。然而，在洪水区域中存在特定的障碍物，如基础设施和树木，可能会导致SAR成像数据中的回射强度增加，从而使得简单的像素强度阈值和时间序列差分方法无法准确检测洪水覆盖面积。为了精准检测洪水覆盖面积，我们利用机器学习技术。但是，这些技术需要大量标注数据来工作有效。因此，我们创建了一个标注水体覆盖面积和洪水覆盖面积的数据集，覆盖了美国大陆和孟加拉国的约36,000平方公里地区。此外，我们还利用公民科学，将数据集开源并在基于该数据集的竞赛中公布了社区生成的模型。在本文中，我们介绍了数据集、数据处理管道、基线模型以及竞赛的详细信息，以及赢得竞赛的方法。我们认为该数据集将加强现有基于Sentinel-1C SAR数据的数据集，并促进洪水覆盖面积的模型化。我们也希望竞赛的结果能够推动洪水覆盖面积检测的研究进一步。
</details></li>
</ul>
<hr>
<h2 id="Domain-Aligned-CLIP-for-Few-shot-Classification"><a href="#Domain-Aligned-CLIP-for-Few-shot-Classification" class="headerlink" title="Domain Aligned CLIP for Few-shot Classification"></a>Domain Aligned CLIP for Few-shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09191">http://arxiv.org/abs/2311.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl<br>for:  This paper aims to improve the performance of CLIP, a large vision-language representation learning model, on downstream tasks without requiring full-scale fine-tuning.methods: The proposed method, called Domain Aligned CLIP (DAC), uses a lightweight adapter trained with an intra-modal contrastive objective and a simple framework to modulate the precomputed class text embeddings to improve both intra-modal and inter-modal alignment on target distributions.results: The proposed method achieves consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrates competitive performance on 4 OOD robustness benchmarks, without requiring fine-tuning the main model.<details>
<summary>Abstract</summary>
Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks.
</details>
<details>
<summary>摘要</summary>
大型视力语言表示学习模型如CLIP在零引入下流程任务上表现出色，主要受益于图像文本对对比目标的协调。然而，这些下流程性能可以通过全规模精度调整进一步提高，但这需要大量标注数据、计算密集型和可能导致对于异常情况的不稳定性。此外，凭借solely rely on inter-modal alignment可能会忽略每个个体模式中的丰富信息。在这项工作中，我们提出了一种efficient domain adaptation strategy for CLIP，称为Domain Aligned CLIP（DAC），该策略可以在目标分布上提高图像-图像和图像-文本对对比，无需修改主模型的参数。为了实现图像-图像对对比，我们提出了一个轻量级适配器，该适配器通过图像内模式对对比 objective进行特性化训练。为了提高图像-文本对对比，我们提出了一个简单的框架，用于修改预计算的类文本嵌入。该一些少量微调框架 computationally efficient，对分布变化强度Robust，并不改变CLIP的参数。我们通过对11种广泛使用的图像分类任务进行测试，发现DAC可以在16枚微调时提供2.3%的提升，并在4个OOD robustness benchmark上显示竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function"><a href="#On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function" class="headerlink" title="On the Computation of the Gaussian Rate-Distortion-Perception Function"></a>On the Computation of the Gaussian Rate-Distortion-Perception Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09190">http://arxiv.org/abs/2311.09190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</li>
<li>for: 这个论文研究了一种多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 的问题，使用 Mean Squared Error (MSE) 损失和不同的抽象度量指标（Kullback-Leibler divergence、geometric Jensen-Shannon divergence、squared Hellinger distance、squared Wasserstein-2 distance）。</li>
<li>methods: 论文首先提供了一个简单的 Gaussian RDPF 的分析 bounds，然后使用 alternating minimization scheme 来解决多变量 RDPF 问题，并提供了一个算法实现。</li>
<li>results: 论文显示了一个关于 tensorizable 损失和感知度量的结论，即依靠 source covariance matrix 的 eigenvector，可以将多变量 RDPF 转化为一个简单的 scalar Gaussian RDPF 问题。此外，论文还提供了一个可行的算法实现，并证明了其收敛性和收敛速率。最后，论文还提供了一个 “perfect realism”  régime 的分析解决方案。<details>
<summary>Abstract</summary>
In this paper, we study the computation of the rate-distortion-perception function (RDPF) for a multivariate Gaussian source under mean squared error (MSE) distortion and, respectively, Kullback-Leibler divergence, geometric Jensen-Shannon divergence, squared Hellinger distance, and squared Wasserstein-2 distance perception metrics. To this end, we first characterize the analytical bounds of the scalar Gaussian RDPF for the aforementioned divergence functions, also providing the RDPF-achieving forward "test-channel" realization. Focusing on the multivariate case, we establish that, for tensorizable distortion and perception metrics, the optimal solution resides on the vector space spanned by the eigenvector of the source covariance matrix. Consequently, the multivariate optimization problem can be expressed as a function of the scalar Gaussian RDPFs of the source marginals, constrained by global distortion and perception levels. Leveraging this characterization, we design an alternating minimization scheme based on the block nonlinear Gauss-Seidel method, which optimally solves the problem while identifying the Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic embodiment is provided, as well as the convergence and the rate of convergence characterization. Lastly, for the "perfect realism" regime, the analytical solution for the multivariate Gaussian RDPF is obtained. We corroborate our results with numerical simulations and draw connections to existing results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 下的mean squared error (MSE) 损均和 Kullback-Leibler divergence、geometric Jensen-Shannon divergence、squared Hellinger distance 和 squared Wasserstein-2 distance 的感知metric下的计算。为此，我们首先对scalar Gaussian RDPF 进行了分析，并提供了RDPF-实现的前向"测试渠道"实现。在多变量情况下，我们证明了，对于tensorizable 损均和感知metric，最佳解在源协方差矩阵的 eigenvector 上。因此，多变量优化问题可以表示为scalar Gaussian RDPF 的source marginals的函数，受到全局损均和感知水平的限制。基于这种特征，我们提出了一种alternating minimization scheme，使用非线性 Gauss-Seidel 方法来优化问题，并实现了RDPF-实现。此外，我们还提供了算法的实现、 convergency 和速度分析。最后，在"完美现实" regime 下，我们获得了多变量 Gaussian RDPF 的分析解。我们的结果通过数值仿真得到了证明，并与现有结果进行了比较。
</details></li>
</ul>
<hr>
<h2 id="RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution"><a href="#RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution" class="headerlink" title="RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution"></a>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09178">http://arxiv.org/abs/2311.09178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dareen Hussein, Hesham Eraqi, Israa Fahmy, Marwah Sulaiman, Mohammed Barakat, Mohammed El-Naggar, Moustafa Youssef, Zahraa Shehabeldin</li>
<li>for: 提高视频超解像（VSR）任务中的时间启示和空间细节准确性。</li>
<li>methods: 提议使用Recurrent Back-Projection Generative Adversarial Network（RBPGAN）模型，结合了两种现有模型的优点，以提高时间一致性和空间细节准确性。</li>
<li>results: 模型在不同数据集上表现出较高的时间一致性和空间细节准确性，与之前的工作相比，模型的性能有所提高。<details>
<summary>Abstract</summary>
Recently, video super resolution (VSR) has become a very impactful task in the area of Computer Vision due to its various applications. In this paper, we propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for VSR in an attempt to generate temporally coherent solutions while preserving spatial details. RBPGAN integrates two state-of-the-art models to get the best in both worlds without compromising the accuracy of produced video. The generator of the model is inspired by RBPN system, while the discriminator is inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal consistency over time. Our contribution together results in a model that outperforms earlier work in terms of temporally consistent details, as we will demonstrate qualitatively and quantitatively using different datasets.
</details>
<details>
<summary>摘要</summary>
近些时候，视频超分辨 (VSR) 已成为计算机视觉领域的非常有影响的任务，因为它在各种应用领域中扮演着重要的角色。在这篇论文中，我们提出了循环回投生成 adversarial网络 (RBPGAN) 模型，用于解决 VSR 问题，并且保持时间准确性和空间细节的平衡。RBPGAN 模型 integrate two state-of-the-art 模型，以获得最佳的效果。生成器部分受到 RBPN 系统的启发，而抑制器部分受到 TecoGAN 的启发。我们还使用了 ping-pong 损失函数，以提高时间一致性。我们的贡献结合起来，导致模型在时间一致性和细节上表现出色，我们将通过不同的数据集来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification"><a href="#WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification" class="headerlink" title="WildlifeDatasets: An open-source toolkit for animal re-identification"></a>WildlifeDatasets: An open-source toolkit for animal re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09118">http://arxiv.org/abs/2311.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildlifedatasets/wildlife-datasets">https://github.com/wildlifedatasets/wildlife-datasets</a></li>
<li>paper_authors: Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros</li>
<li>for:  primarily for ecologists and computer-vision &#x2F; machine-learning researchers</li>
<li>methods:  provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning</li>
<li>results:  provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin.<details>
<summary>Abstract</summary>
In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了 WildlifeDatasets（https://github.com/WildlifeDatasets/wildlife-datasets）-一个开源工具箱，主要针对生态学家和计算机视觉/机器学习研究人员。WildlifeDatasets 是使用 Python 编写的，可以方便地访问公共可用的野生生物数据集，并提供了许多方法 для数据集预处理、性能分析和模型细化。我们在多种场景和基线实验中展示了工具箱，包括我们知道的最完整的 эксперименталь比较 wildlife 重复标识数据集和方法，包括本地描述器和深度学习方法。此外，我们还提供了首先基于个体重复标识的宽范种类基模型 - MegaDescriptor - ，它在动物重复标识数据集中提供了状态机器的性能，并在其他预训练模型 such as CLIP 和 DINOv2 之上出色表现。为使模型对普通公众开放，并让其与任何现有的野生监测应用程序轻松集成，我们提供了多种 MegaDescriptor FLAVORS（i.e., Small, Medium, and Large）通过 HuggingFace 平台（https://huggingface.co/BVRA）。
</details></li>
</ul>
<hr>
<h2 id="Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding"><a href="#Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding" class="headerlink" title="Cross-view and Cross-pose Completion for 3D Human Understanding"></a>Cross-view and Cross-pose Completion for 3D Human Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09104">http://arxiv.org/abs/2311.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez</li>
<li>for: 这个论文主要针对人类视觉理解领域，旨在利用大量预训练模型和大数据来提高计算机视觉的性能。</li>
<li>methods: 这个论文提出了一种基于自我超vised学习的预训练方法，使用人类数据集来学习人类特征和运动约束。该方法使用了双视相差和时间相差对来学习3D和人体运动的约束。</li>
<li>results: 该论文通过对多种人类中心任务进行预训练，并在多种人类模型回归任务上达到了最佳性能。此外，该方法还可以在不需要大量 Label 的情况下进行预训练。<details>
<summary>Abstract</summary>
Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.
</details>
<details>
<summary>摘要</summary>
人类认知和理解是计算机视觉中的一个主要领域，与其他视觉子领域一样，它也可以从使用大型预训练模型和大量数据中获得更好的表现。我们假设，使用通用的物体中心图像集如ImageNet进行预训练的方法，受到重要的领域转换的限制。而收集特定领域的实际数据，如2D或3D标签，不可靠。因此，我们提出了基于自我超vised学习的预训练方法，该方法使用人类数据图像来进行预训练，并且只使用图像。我们的方法使用人类图像的两个视图（即左右两个图像）和视频中的两个姿态（即左右两个姿态）来学习人体的3D和运动约束。我们预训练了一个身体中心任务的模型和一个手中心任务的模型。使用通用的转换架构，这些模型在许多人类中心下沉天 зада务中表现出色，并在人体碰撞恢复任务中获得了状态之一的表现。
</details></li>
</ul>
<hr>
<h2 id="Guided-Scale-Space-Radon-Transform-for-linear-structures-detection"><a href="#Guided-Scale-Space-Radon-Transform-for-linear-structures-detection" class="headerlink" title="Guided Scale Space Radon Transform for linear structures detection"></a>Guided Scale Space Radon Transform for linear structures detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09103">http://arxiv.org/abs/2311.09103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aicha Baya Goumeidane, Djemel Ziou, Nafaa Nacereddine</li>
<li>for:  automatic detection of thick linear structures in gray scale and binary images</li>
<li>methods:  using the Scale Space Radon Transform (SSRT) with calculated Hessian orientations to emphasize linear structures and reduce unwanted peaks</li>
<li>results:  efficient detection of lines of different thickness in synthetic and real images, with robustness against noise and complex background<details>
<summary>Abstract</summary>
Using integral transforms to the end of lines detection in images with complex background, makes the detection a hard task needing additional processing to manage the detection. As an integral transform, the Scale Space Radon Transform (SSRT) suffers from such drawbacks, even with its great abilities for thick lines detection. In this work, we propose a method to address this issue for automatic detection of thick linear structures in gray scale and binary images using the SSRT, whatever the image background content. This method involves the calculated Hessian orientations of the investigated image while computing its SSRT, in such a way that linear structures are emphasized in the SSRT space. As a consequence, the subsequent maxima detection in the SSRT space is done on a modified transform space freed from unwanted parts and, consequently, from irrelevant peaks that usually drown the peaks representing lines. Besides, highlighting the linear structure in the SSRT space permitting, thus, to efficiently detect lines of different thickness in synthetic and real images, the experiments show also the method robustness against noise and complex background.
</details>
<details>
<summary>摘要</summary>
Our method involves calculating the Hessian orientations of the investigated image while computing its SSRT, which emphasizes linear structures in the SSRT space. As a result, the subsequent maxima detection in the SSRT space is done on a modified transform space that is free from unwanted parts and irrelevant peaks that usually drown the peaks representing lines. This allows us to efficiently detect lines of different thickness in synthetic and real images, and the experiments show that our method is robust against noise and complex backgrounds.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions"><a href="#Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions" class="headerlink" title="Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions"></a>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09093">http://arxiv.org/abs/2311.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingshuai Dong, Massimiliano L. Cappuccio</li>
<li>For: 本文review了过去十年内关于计算机视觉和自动驾驶的发展，包括主要汽车制造商在不同国家的自动驾驶系统的开发，感知器和测试数据集的使用，以及计算机视觉在自动驾驶中的应用。* Methods: 本文 investigate了自动驾驶系统的开发，包括感知器和测试数据集的使用，以及计算机视觉在自动驾驶中的应用，包括深度估计、物体检测、车道检测和交通标志识别等。* Results: 本文 analyze了自动驾驶技术的当前挑战和未来研究方向，包括计算机视觉在自动驾驶中的应用和发展趋势。<details>
<summary>Abstract</summary>
Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆指一种可以自动感知周围环境并减少或完全无需人类驾驶员输入的车辆。感知系统是自动驾驶车辆的基本组件，它使得自动驾驶车辆能够收集环境数据并提取有关信息以安全驾驶。利用计算机视觉的最新进展，感知任务可以通过感知器 such as 摄像头、LiDAR、雷达和超声波感知器来实现。本文对计算机视觉和自动驾驶相关的出版物进行了十年的查询和总结。特别是，我们首先调查了各国主要汽车制造商开发的自动驾驶系统，然后探讨了通用的感知器和测试数据集。接着，我们对计算机视觉在自动驾驶中的应用进行了全面的概述，包括深度估计、物体检测、路径检测和交通信号识别。此外，我们还审查了自动驾驶车辆的公众意见和担忧，并分析了自动驾驶车辆面临的当前技术挑战。最后，我们提出了一些有前途的研究方向。本文将帮助读者更好地理解自动驾驶车辆的学术和industry 的视角。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search"><a href="#Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search" class="headerlink" title="Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search"></a>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09084">http://arxiv.org/abs/2311.09084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/personsearch-ctlg">https://github.com/hcplab-sysu/personsearch-ctlg</a></li>
<li>paper_authors: Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen, Liang Lin</li>
<li>for: 文章目的是提出一种简单 yet effective的 dual Transformer模型，用于图像库中的文本基于人脸检索任务。</li>
<li>methods: 该模型使用一种具有困难感的对比学习策略，以及一种自动生成数据模块（PDG模块）来提高数据多样性。</li>
<li>results: 实验结果表明，提出的方法可以 evidently 超越当前最佳方法，例如在CUHK-PEDES和ICFG-PEDES两个数据集上提高Top1、Top5、Top10的性能，具体提高3.88%、4.02%和2.92%。<details>
<summary>Abstract</summary>
Given a descriptive text query, text-based person search (TBPS) aims to retrieve the best-matched target person from an image gallery. Such a cross-modal retrieval task is quite challenging due to significant modality gap, fine-grained differences and insufficiency of annotated data. To better align the two modalities, most existing works focus on introducing sophisticated network structures and auxiliary tasks, which are complex and hard to implement. In this paper, we propose a simple yet effective dual Transformer model for text-based person search. By exploiting a hardness-aware contrastive learning strategy, our model achieves state-of-the-art performance without any special design for local feature alignment or side information. Moreover, we propose a proximity data generation (PDG) module to automatically produce more diverse data for cross-modal training. The PDG module first introduces an automatic generation algorithm based on a text-to-image diffusion model, which generates new text-image pair samples in the proximity space of original ones. Then it combines approximate text generation and feature-level mixup during training to further strengthen the data diversity. The PDG module can largely guarantee the reasonability of the generated samples that are directly used for training without any human inspection for noise rejection. It improves the performance of our model significantly, providing a feasible solution to the data insufficiency problem faced by such fine-grained visual-linguistic tasks. Extensive experiments on two popular datasets of the TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%, 4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>文本基于人搜索（TBPS）目标是从图库中检索最佳匹配的人。这是跨Modalities的检索任务，因为视觉和语言之间存在显著的差异和细化差异，同时缺乏标注数据。大多数现有的方法都是通过引入复杂的网络结构和辅助任务来减少这些差异，但这些方法复杂和困难实现。在这篇论文中，我们提出了一种简单 yet 有效的双Transformer模型，用于文本基于人搜索。我们通过强制硬度感知的对比学习策略，使我们的模型在没有特殊设计的本地特征对齐和侧信息的情况下，达到了当前最佳性能。此外，我们还提出了一种近似数据生成（PDG）模块，用于自动生成更多的跨Modalities训练数据。PDG模块首先引入了基于文本到图像扩散模型的自动生成算法，生成了新的文本-图像对amples在原始对amples的 proximity 空间中。然后，它将approximate text生成和特征级mixup在训练中结合，以进一步增强数据多样性。PDG模块可以确保生成的样本的合理性，不需要人工检查噪音抛弃。这使我们的模型表现得更好，提供了跨Modalities任务中数据不足问题的可行解决方案。我们在CUHK-PEDES和ICFG-PEDES两个流行的TBPS任务上进行了广泛的实验，并证明了我们的方法在Top1、Top5、Top10等指标上明显超过了现有方法，例如在CUHK-PEDES上提高了3.88%, 4.02%, 2.92%。代码将在https://github.com/HCPLab-SYSU/PersonSearch-CTLG 上提供。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation"><a href="#Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation" class="headerlink" title="Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation"></a>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09077">http://arxiv.org/abs/2311.09077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan</li>
<li>for: 本研究的目的是提出一种使用脉冲神经网络（Spiking Neural Network，SNN）和混合ANN-SNN框架来构建不连续的温度场，以获得准确的geometry表示。</li>
<li>methods: 本研究使用了多层感知器（Multi-layer Perceptron，MLP）来构建感知场，但是实际的geometry或温度场经常在空气和表面之间存在缺陷，这会导致不准确的geometry表示。为了解决这个问题，本研究提出了使用脉冲神经网络来构建不连续的温度场。</li>
<li>results: 本研究的结果达到了顶尖性能水平。我们的代码和数据将被公开发布。<details>
<summary>Abstract</summary>
A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry, Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.中文简体版：一个重要的原因是现有的NeRF基于方法的成功是通过多层见解器层（MLP）建立神经核密度场来表示几何。然而，实际的几何或密度场经常在空气和表面之间存在缺陷，这会导致不准确的几何表示。为此，这篇论文提出了脉冲NeRF，利用脉冲神经和混合人工神经网络（ANN）-脉冲神经网络（SNN）框架建立不连续密度场以实现准确的几何表示。具体来说，我们首先解释了连续密度场会带来的不准确性。然后，我们提出使用脉冲神经建立不连续密度场。我们对现有脉冲神经模型的问题进行了全面的分析，并提供了密度场参数与理论几何准确性之间的数学关系。基于这，我们提出了固定脉冲神经来建立不连续密度场。我们的结果达到了最佳性能。我们的代码和数据将被公开发布。
</details></li>
</ul>
<hr>
<h2 id="Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models"><a href="#Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models" class="headerlink" title="Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models"></a>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09064">http://arxiv.org/abs/2311.09064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn</li>
<li>for: 这篇论文主要目标是提出一个新的视觉具有系统性的想象能力挑战，即通过创建一个世界模型来适应新的情境。</li>
<li>methods: 论文提出了一个新的测试框架——系统性视觉幻想评价框架（SVIB），用于评价模型在视觉域的系统性想象能力。SVIB包括一个小型世界模型问题，其中模型需要根据一个 latent 世界动力来生成一步图像到图像的转换。</li>
<li>results: 论文对 SVIB 进行了全面的评价，并提供了一些基线模型的性能分析，以帮助进一步推动视觉系统性 compositionality。<details>
<summary>Abstract</summary>
Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.
</details>
<details>
<summary>摘要</summary>
系统化组合性，或者在新的情况下适应by创建一个使用可重用知识的世界模型，是机器学习中的一项重要挑战。虽然在语言领域已经取得了很大的进步，但尝试实现系统视觉想象，或者从视觉观察中预测动态效果，还处于初期阶段。我们提出了系统视觉想象标准套件（SVIB），这是第一个直接面对这个问题的标准套件。SVIB提供了一个新的世界模型最小化问题的框架，在这个框架中，模型被评估基于它们在一步图像到图像转换中的能力。这个框架具有多种优点，如同时优化系统见解和想象、多个难度水平和在训练中使用可能的因素组合的控制。我们对SVIB进行了全面的评估，并提供了当前领域的状态态报告，以帮助进一步提高视觉系统组合性。
</details></li>
</ul>
<hr>
<h2 id="Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal"><a href="#Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal" class="headerlink" title="Self-Annotated 3D Geometric Learning for Smeared Points Removal"></a>Self-Annotated 3D Geometric Learning for Smeared Points Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09029">http://arxiv.org/abs/2311.09029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaowei Wang, Daniel Morris</li>
<li>for: 提高consumer级密集深度感知器的准确性和质量</li>
<li>methods: 使用自动检测和标注多视角3D几何证据来自动检测和标注涂抹点</li>
<li>results: 比 tradicional filters和其他自动标注方法高效，可以减少 fictitious surfaces 的影响Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文提出了一种自动检测和标注 consumer级密集深度感知器中的涂抹点（smeared points）的方法，以提高深度图的准确性和质量。</li>
<li>methods: 我们使用多视角3D几何证据自动检测和标注涂抹点，并通过自动生成的 annotated dataset 进行训练。</li>
<li>results: 我们的方法比传统的滤波器和其他自动标注方法高效，可以减少 fictitious surfaces 的影响，提高深度图的准确性和质量。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git.
</details>
<details>
<summary>摘要</summary>
在提高consumer级稠密深度感知器的准确性和质量方面，有了 significiant progress。然而，仍然存在一种常见的深度像素artefact，我们称之为“杂Points”。这些点不在任何3D表面上，通常发生在前景和背景对象之间的插值。由于它们引起的虚拟表面，这些点有可能对依赖深度地图的应用程序产生害。统计方法不够 Effective in removing these points, as they tend to remove actual surface points as well. 网络训练方法也难以获得足够的标注数据。为此，我们提出了一种完全自动注释的方法，用于训练深度杂点除去类ifier。我们的方法基于多个视角中收集的3D几何证据自动检测和注释杂点和有效点。为验证我们的方法的有效性，我们提出了一个新的benchmark数据集：Real Azure-Kinect数据集。实验和剥离学研究表明，我们的方法比传统的筛选器和其他自动注释方法更高效。我们的工作可以在https://github.com/wangmiaowei/wacv2024_smearedremover.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing"><a href="#Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing" class="headerlink" title="Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing"></a>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09024">http://arxiv.org/abs/2311.09024</a></li>
<li>repo_url: None</li>
<li>paper_authors: A K Nirala, A Joshi, C Hegde, S Sarkar</li>
<li>for: 这个研究是为了提高CLIP模型的可靠性和扩展性，使其能够在不同领域中进行灵活的应用。</li>
<li>methods: 这个研究使用了Randomized Smoothing技术来实现Open Vocabulary Certification（OVC），将一个基本的“训练”集合Prompts和其相应的CLIP分类器作为起点，然后通过将这个分类器视为附近的基本训练集合中的一个“偏移”版本，快速认证新的分类器。</li>
<li>results: 这个研究通过实验评估，显示OVC可以实现大约两个数量级的加速，并且可以在不同的视觉语言后门上进行扩展。<details>
<summary>Abstract</summary>
A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.
</details>
<details>
<summary>摘要</summary>
“CLIP型深度视语模型具有零shot开 vocabulary分类的能力，即用户可以使用自然语言提示来定义新的类别标签在推理时。然而，CLIP基于模型对攻击性质的敏感性很高，因此确保其可靠部署在野外是关键。在这项工作中，我们介绍了开 vocabulary认证（OVC），一种基于随机平滑技术的快速认证方法，专门针对开 vocabulary模型 like CLIP。给定一个基础“训练”集的提示和其相应的认证CLIP分类器，OVC利用提示的近似性来快速认证novel分类器。通过缓存技术，我们实现了约两个数量级的加速。此外，OVC使用多重正态分布来缓过 embedding 空间的计算，从而快速地 aproximate  embedding 空间。我们通过实验评估了OVC在 CIFAR-10 和 ImageNet 测试集上的效果。”Note that Simplified Chinese is used here, which is a common writing system used in mainland China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop"><a href="#Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop" class="headerlink" title="Incremental Object-Based Novelty Detection with Feedback Loop"></a>Incremental Object-Based Novelty Detection with Feedback Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09004">http://arxiv.org/abs/2311.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Caldarella, Elisa Ricci, Rahaf Aljundi</li>
<li>for: 本研究旨在提高对物体检测模型的不确定性检测，以避免在实际应用中可能产生危害的行为，如自动驾驶车或自动机器人中的对象检测模型。</li>
<li>methods: 本研究提出了一种基于人工反馈的对象基本不确定性检测方法，假设可以在预训练后请求人类反馈，并将其 incorporated 到ND模型中，以提高ND模型的Robustness。</li>
<li>results: 研究人员通过设计一种轻量级的ND模块，并在这个模块上进行了增量更新，使得ND模型能够更好地适应不确定性检测任务。同时，研究人员还提出了一个新的评估标准，用于评估ND方法在这种新的设定下的性能。经过广泛的测试和比较，研究人员发现他们的ND方法在robustness和feedback incorporation两个方面都表现出色。<details>
<summary>Abstract</summary>
Object-based Novelty Detection (ND) aims to identify unknown objects that do not belong to classes seen during training by an object detection model. The task is particularly crucial in real-world applications, as it allows to avoid potentially harmful behaviours, e.g. as in the case of object detection models adopted in a self-driving car or in an autonomous robot. Traditional approaches to ND focus on one time offline post processing of the pretrained object detection output, leaving no possibility to improve the model robustness after training and discarding the abundant amount of out-of-distribution data encountered during deployment.   In this work, we propose a novel framework for object-based ND, assuming that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details>
<details>
<summary>摘要</summary>
Traditional approaches to ND focus on one-time offline post-processing of the pre-trained object detection output, leaving no room for improving the model's robustness after training and discarding a large amount of out-of-distribution data encountered during deployment.In this work, we propose a new framework for object-based ND, which assumes that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available.To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details></li>
</ul>
<hr>
<h2 id="Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images"><a href="#Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images" class="headerlink" title="Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images"></a>Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08995">http://arxiv.org/abs/2311.08995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong liu, Fa Zhang, Lin Cheng, Huanxi Deng, Xiaoyan Yang, Zhenyu Zhang, Chichun Zhou</li>
<li>for: 这篇论文的目的是提出一种无监督分类方法，以减少对特殊领域图像分类器的依赖。</li>
<li>methods: 本文使用了三个关键想法：1）使用预训模型和数据推导学习独立Featuredimensionality reduction，2）多个聚类算法的投票机制，3）后期对于没有预先注释的图像进行调整。</li>
<li>results: 本文的无监督分类方法在 fungal 图像数据集上表现出来的准确率高于超级监督方法，实际上是 94.1% 和 96.7%。这种无监督分类方法可以实现减少对预先注释数据的依赖，并且可以让研究人员在不同的领域中轻松地建立数据集，促进AI应用程序在特殊领域的图像分类。<details>
<summary>Abstract</summary>
High-quality labeled datasets are essential for deep learning. Traditional manual annotation methods are not only costly and inefficient but also pose challenges in specialized domains where expert knowledge is needed. Self-supervised methods, despite leveraging unlabeled data for feature extraction, still require hundreds or thousands of labeled instances to guide the model for effective specialized image classification. Current unsupervised learning methods offer automatic classification without prior annotation but often compromise on accuracy. As a result, efficiently procuring high-quality labeled datasets remains a pressing challenge for specialized domain images devoid of annotated data. Addressing this, an unsupervised classification method with three key ideas is introduced: 1) dual-step feature dimensionality reduction using a pre-trained model and manifold learning, 2) a voting mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior manual annotation. This approach outperforms supervised methods in classification accuracy, as demonstrated with fungal image data, achieving 94.1% and 96.7% on public and private datasets respectively. The proposed unsupervised classification method reduces dependency on pre-annotated datasets, enabling a closed-loop for data classification. The simplicity and ease of use of this method will also bring convenience to researchers in various fields in building datasets, promoting AI applications for images in specialized domains.
</details>
<details>
<summary>摘要</summary>
高品质标注数据是深度学习的关键。传统的手动标注方法不仅成本高昂且不可靠，而且在专业领域中需要专业知识。无监督方法，尽管利用无标注数据进行特征提取，仍需要数百或千个标注实例来导引模型以获得有效的专业图像分类。当前的无监督学习方法可以自动分类无需先前的标注，但通常会妥协准确性。因此，efficiently 获取高品质标注数据仍然是专业领域图像中无标注数据的焦点问题。为解决这个问题，一种无监督分类方法的三个关键想法被提出：1）使用预训练模型和拓扑学来实现双步特征维度减少，2）从多个聚类算法中选择多数票，和3） poste-hoc 而不是先前的手动标注。这种方法在分类准确率方面超过了指导方法，如果用蘑菇图像数据进行示例，分别达到了94.1%和96.7%的公共和私人数据集的分类精度。提议的无监督分类方法减少了对预先标注数据的依赖，使得数据分类成为了关闭的循环。此方法的简单易用性也将为各种领域的研究人员在建立数据集方面带来便利，推动图像特有领域的AI应用。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging"><a href="#Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging" class="headerlink" title="Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging"></a>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08972">http://arxiv.org/abs/2311.08972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcello Carioni, Subhadip Mukherjee, Hong Ye Tan, Junqi Tang</li>
<li>for: 本文主要针对无监督学习方法在图像逆问题中的应用，尤其是基于最优运输和凸分析的方法。</li>
<li>methods: 本文主要介绍了一些有理性的无监督学习方法，包括循环一致性基于模型和学习反抗训练方法，以及一些最近的加速图像逆问题解决的提速学习算法。</li>
<li>results: 本文综述了一些有理性的无监督学习方法，包括某些加速图像逆问题解决的提速学习算法，以及一些相关的无监督学习框架。<details>
<summary>Abstract</summary>
Unsupervised deep learning approaches have recently become one of the crucial research areas in imaging owing to their ability to learn expressive and powerful reconstruction operators even when paired high-quality training data is scarcely available. In this chapter, we review theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis. We begin by reviewing the optimal transport-based unsupervised approaches such as the cycle-consistency-based models and learned adversarial regularization methods, which have clear probabilistic interpretations. Subsequently, we give an overview of a recent line of works on provably convergent learned optimization algorithms applied to accelerate the solution of imaging inverse problems, alongside their dedicated unsupervised training schemes. We also survey a number of provably convergent plug-and-play algorithms (based on gradient-step deep denoisers), which are among the most important and widely applied unsupervised approaches for imaging problems. At the end of this survey, we provide an overview of a few related unsupervised learning frameworks that complement our focused schemes. Together with a detailed survey, we provide an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep our discussion self-contained.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过深度学习的无监督方法，近期在成像领域中成为一个关键的研究方向，因为它们可以学习表达力强的重建算法，即使精美训练数据 scarce。在这章中，我们将 theoretically principled 无监督学习方案 для解决成像 inverse problems 进行文献综述，尤其是基于最优运输和几何分析的方法。我们首先介绍了基于循环一致性的模型和学习对抗regularization方法，这些方法具有明确的概率解释。然后，我们给出了一个最近的一线工作的概述，包括加速解决成像 inverse problems 的学习优化算法，以及其相应的无监督训练方案。此外，我们还介绍了一些可靠地 convergent 的插件和执行算法（基于梯度步骤深度去噪器），它们在成像问题中广泛应用。在这个综述中，我们还提供了一些相关的无监督学习框架，以及这些方法的关键数学结果，以使我们的讨论自包含。
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution"><a href="#A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution" class="headerlink" title="A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution"></a>A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08955">http://arxiv.org/abs/2311.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Liu, Zebin Wu, Liang Xiao</li>
<li>for:  fusion-based hyperspectral image (HSI) super-resolution</li>
<li>methods: spectral diffusion prior, maximum a posteriori, Adam optimization</li>
<li>results: effective in enhancing spatial resolution and preserving spectral information, demonstrated on both synthetic and real datasets.Here’s the full Chinese text:</li>
<li>for: 融合基于干扰图像（HSI）超分辨率，旨在生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral图像。</li>
<li>methods:  spectral diffusion prior，最大 posteriori，Adam优化</li>
<li>results: 能够提高空间分辨率，同时保持spectral信息，在 sintetic和实际数据集上进行了实验。Note: “spectral diffusion prior” is a translation of “spectral diffusion model” in the original text.<details>
<summary>Abstract</summary>
Fusion-based hyperspectral image (HSI) super-resolution aims to produce a high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a high-spatial-resolution multispectral image. Such a HSI super-resolution process can be modeled as an inverse problem, where the prior knowledge is essential for obtaining the desired solution. Motivated by the success of diffusion models, we propose a novel spectral diffusion prior for fusion-based HSI super-resolution. Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details>
<details>
<summary>摘要</summary>
融合基于快照成像（HSI）超分解目的是生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral图像。这种HSI超分解过程可以被视为一个逆 проблеml，其中假设知识是获得所求解的关键。鼓动于扩散模型的成功，我们提出了一种新的 spectral扩散前提（SDP）。 Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details></li>
</ul>
<hr>
<h2 id="Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard"><a href="#Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard" class="headerlink" title="Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard"></a>Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08949">http://arxiv.org/abs/2311.08949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Ammeling, Moritz Hecker, Jonathan Ganz, Taryn A. Donovan, Christof A. Bertram, Katharina Breininger, Marc Aubreville</li>
<li>for:  This paper is written for assessing the prognostic value of invasive breast carcinomas using a deep learning pipeline.</li>
<li>methods: The paper uses an annotation-free, immunohistochemistry-based approach to estimate epithelial segmentation in canine breast carcinomas.</li>
<li>results: The deep learning-based pipeline shows expert-level performance, with time efficiency and reproducibility, compared to manually annotated M&#x2F;V-Index.<details>
<summary>Abstract</summary>
The volume-corrected mitotic index (M/V-Index) was shown to provide prognostic value in invasive breast carcinomas. However, despite its prognostic significance, it is not established as the standard method for assessing aggressive biological behaviour, due to the high additional workload associated with determining the epithelial proportion. In this work, we show that using a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach, provides accurate estimations of epithelial segmentation in canine breast carcinomas. We compare our automatic framework with the manually annotated M/V-Index in a study with three board-certified pathologists. Our results indicate that the deep learning-based pipeline shows expert-level performance, while providing time efficiency and reproducibility.
</details>
<details>
<summary>摘要</summary>
它们表示了抑制性肿瘤指数（M/V-Index）在侵袭性乳腺癌中提供了预后价值。然而，尽管它具有预后意义，但它并没有被认定为评估攻击性生物行为的标准方法，因为确定细胞性占比带来了高额附加工作负担。在这项工作中，我们表明了使用深度学习框架，只靠基于无注解、免疫抑制技术的方法来提供精准的细胞分割估计。我们将自动框架与三位医学博士评估员 manually annotated M/V-Index进行比较。我们的结果表明，深度学习基于的框架具有专家水平的性能，同时具有时间效率和重复性。
</details></li>
</ul>
<hr>
<h2 id="Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness"><a href="#Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness" class="headerlink" title="Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness"></a>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08936">http://arxiv.org/abs/2311.08936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Mohamed Farag, Ribana Roscher</li>
<li>for: 本研究旨在使用机器学习模型对保护区域的自然性进行更好的理解和映射，以便更好地保护这些区域。</li>
<li>methods: 本研究使用可解释的机器学习方法分析卫星图像，并使用不确定度量化来评估模型的可靠性。</li>
<li>results: 研究提出了一种名为“可信度自然性解释”（CNE）框架，该框架结合可解释的机器学习和不确定度量化来评估和解释自然性。研究还提出了一种新的量化度量，用于衡量模型对自然性的可靠性。<details>
<summary>Abstract</summary>
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness. We introduce a new quantitative metric that describes the confident contribution of patterns to the concept of naturalness. Furthermore, we generate an uncertainty-aware segmentation mask for each input sample, highlighting areas where the model lacks knowledge. To demonstrate the effectiveness of our framework, we apply it to a study site in Fennoscandia using two open-source satellite datasets.
</details>
<details>
<summary>摘要</summary>
保护自然区域是人类活动的有限影响地区，如城市化、农业和其他人类干预。为了更好地理解和地图这些地区的自然性，机器学习模型可以使用卫星影像进行分析。特别是使用可解释机器学习方法可以揭示保护区域中自然性的特征。然而，现有的方法有限制。它们可能无法提供有效和客观的解释，或者很难提供量化度量自然性的贡献，以及相关的信息。在这篇论文中，我们提出了一种新的框架，即可信度自然性解释（CNE）框架。这个框架结合可解释机器学习和不确定量化来评估和解释自然性。我们还提出了一个新的量化度量，用于描述模型对自然性的有效贡献。此外，我们生成了每个输入样本的不确定性感知分 segmentation掩模，以显示模型在哪些地方缺乏知识。为了证明我们的框架的有效性，我们在芬兰地区使用了两个开源卫星数据集进行应用。
</details></li>
</ul>
<hr>
<h2 id="Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation"><a href="#Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation" class="headerlink" title="Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation"></a>Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08931">http://arxiv.org/abs/2311.08931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medical-image-analysis-laboratory/ms_wml_uncs">https://github.com/medical-image-analysis-laboratory/ms_wml_uncs</a></li>
<li>paper_authors: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa, Adrien Depeursinge, Mark Gales, Cristina Granziera, Henning Muller, Mara Graziani, Meritxell Bach Cuadra<br>for:这个研究旨在评估自动深度学习（DL）工具在多发性脑损（MS）患者的白 matter损害（WML）分 segmentation任务中的可靠性。methods:我们的研究集中在两个主要的不确定性方面：首先，我们提出了一个好的不确定性量应该指示预测可能有误的高不确定性值。其次，我们 investigate了不确定性的不同解剖级别（voxel、lesion、patient）之间的关系。我们假设不确定性在每个级别都与特定类型的错误有关。我们的研究旨在证实这种关系，通过在域内和域外两种设定下进行分离分析。我们的主要方法贡献包括：（i）开发了novel的lesion和patient级别的不确定性量计算方法，基于结构预测差异，以及（ii）将错误保留曲线分析框架扩展到facilitate the evaluation of UQ performance at both lesion and patient scales。results:我们使用了172名患者的多中心MRI数据集，并证实了我们提出的不确定性量更好地捕捉模型错误在lesion和patient级别上。我们提供了UQ协议代码，可以在<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs中找到。</a><details>
<summary>Abstract</summary>
This paper explores uncertainty quantification (UQ) as an indicator of the trustworthiness of automated deep-learning (DL) tools in the context of white matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of multiple sclerosis (MS) patients. Our study focuses on two principal aspects of uncertainty in structured output segmentation tasks. Firstly, we postulate that a good uncertainty measure should indicate predictions likely to be incorrect with high uncertainty values. Second, we investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors. Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are (i) the development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies, and (ii) the extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales. The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. We provide the UQ protocols code at https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We propose that a good uncertainty measure should indicate predictions that are likely to be incorrect with high uncertainty values.2. We investigate the value of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient) and hypothesize that uncertainty at each scale is related to specific types of errors.Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our main methodological contributions are:1. The development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies.2. The extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales.The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. The UQ protocols code is available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs</a>.</details></li>
</ol>
<hr>
<h2 id="Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization"><a href="#Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization" class="headerlink" title="Progressive Feedback-Enhanced Transformer for Image Forgery Localization"></a>Progressive Feedback-Enhanced Transformer for Image Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08910">http://arxiv.org/abs/2311.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Zhu, Gang Cao, Xianglin Huang</li>
<li>for: 本研究旨在提高数字图像中的违�检测精度和可靠性，以便应对地方图像修改技术的恶意使用。</li>
<li>methods: 我们提出了一种Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现粗细到细的图像违�检测。特别是，初始分支网络生成的粗细 lokalisa map 被适应地反馈到早期的 transformer 嵌入层，以增强图像的正面特征表示，同时抑制干扰因素。另外，我们还提出了一种 Contextual Spatial Pyramid Module，用于改进识别性的伪造特征，以提高违�检测的准确性和可靠性。</li>
<li>results: 我们在九个公共的审查数据集上进行了广泛的实验，结果表明，我们的提出的 lokaliser 在泛化能力和图像违�检测的稳定性方面都有显著的优势，超过了现有的状态态技术。<details>
<summary>Abstract</summary>
Blind detection of the forged regions in digital images is an effective authentication means to counter the malicious use of local image editing techniques. Existing encoder-decoder forensic networks overlook the fact that detecting complex and subtle tampered regions typically requires more feedback information. In this paper, we propose a Progressive FeedbACk-enhanced Transformer (ProFact) network to achieve coarse-to-fine image forgery localization. Specifically, the coarse localization map generated by an initial branch network is adaptively fed back to the early transformer encoder layers for enhancing the representation of positive features while suppressing interference factors. The cascaded transformer network, combined with a contextual spatial pyramid module, is designed to refine discriminative forensic features for improving the forgery localization accuracy and reliability. Furthermore, we present an effective strategy to automatically generate large-scale forged image samples close to real-world forensic scenarios, especially in realistic and coherent processing. Leveraging on such samples, a progressive and cost-effective two-stage training protocol is applied to the ProFact network. The extensive experimental results on nine public forensic datasets show that our proposed localizer greatly outperforms the state-of-the-art on the generalization ability and robustness of image forgery localization. Code will be publicly available at https://github.com/multimediaFor/ProFact.
</details>
<details>
<summary>摘要</summary>
《快速检测数字图像中forge的地方是一种有效的身份验证方法，以防止本地图像修改技术的恶意使用。现有的编码器-解码器审查网络忽略了需要更多反馈信息来检测复杂且微妙的forge地方的事实。本文提出了一种 Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现从粗到细图像伪造地点检测。具体来说，初始分支网络生成的粗略定位图是通过对初期转换器层进行反馈来提高正面特征表示，同时抑制干扰因素。cascade转换网络，配合Contextual Spatial Pyramid模块，用于细化审查特征，提高伪造地点检测精度和可靠性。此外，我们还提出了一种生成大量伪造图像样本的自动化策略，特别是在真实和coherent处理中。基于这些样本，我们采用了一种进步和成本效果的两个阶段训练方案，并在九个公共审查 dataset 上进行了广泛的实验。结果表明，我们提出的定位器在总体能力和对图像伪造地点检测的稳定性方面具有明显的优势。代码将在https://github.com/multimediaFor/ProFact 上公开。
</details></li>
</ul>
<hr>
<h2 id="DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack"><a href="#DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack" class="headerlink" title="DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack"></a>DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08909">http://arxiv.org/abs/2311.08909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Perry Gibson, José Cano, Elliot J. Crowley, Amos Storkey, Michael O’Boyle</li>
<li>for: 这个论文主要是为了提高深度学习模型在有限资源的设备上的运行速度。</li>
<li>methods: 这个论文使用了机器学习和系统技术，提出了深度学习加速堆栈（DLAS），并进行了透明度研究以确定不同参数对整个堆栈的影响。</li>
<li>results: 该论文的evaluation表明，不同的DLAS参数的变化可以导致巨大的变化和堆栈之间的互动。最高级别的观察结论是，模型大小、准确率和执行时间之间并不是一定的相关性。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are extremely computationally demanding, which presents a large barrier to their deployment on resource-constrained devices. Since such devices are where many emerging deep learning applications lie (e.g., drones, vision-based medical technology), significant bodies of work from both the machine learning and systems communities have attempted to provide optimizations to accelerate DNNs. To help unify these two perspectives, in this paper we combine machine learning and systems techniques within the Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can be tightly dependent on each other with an across-stack perturbation study. We evaluate the impact on accuracy and inference time when varying different parameters of DLAS across two datasets, seven popular DNN architectures, four DNN compression techniques, three algorithmic primitives with sparse and dense variants, untuned and auto-scheduled code generation, and four hardware platforms. Our evaluation highlights how perturbations across DLAS parameters can cause significant variation and across-stack interactions. The highest level observation from our evaluation is that the model size, accuracy, and inference time are not guaranteed to be correlated. Overall we make 13 key observations, including that speedups provided by compression techniques are very hardware dependent, and that compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is. With DLAS, we aim to provide a reference framework to aid machine learning and systems practitioners in reasoning about the context in which their respective DNN acceleration solutions exist in. With our evaluation strongly motivating the need for co-design, we believe that DLAS can be a valuable concept for exploring the next generation of co-designed accelerated deep learning solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM"><a href="#Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM" class="headerlink" title="Robust Brain MRI Image Classification with SIBOW-SVM"></a>Robust Brain MRI Image Classification with SIBOW-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08908">http://arxiv.org/abs/2311.08908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyun Zeng, Hao Helen Zhang</li>
<li>for: 本研究旨在开发一种高效的脑肿瘤图像分类方法，以提高脑肿瘤诊断精度和患者生活质量。</li>
<li>methods: 该方法基于 Bag-of-Features 模型、SIFT特征提取和加权支持向量机（wSVM），能够捕捉脑肿瘤图像中隐藏的特征，并且可以准确地分类不同的肿瘤类型。</li>
<li>results: 对一个公共的脑肿瘤MRI图像集进行了测试，结果显示，新方法可以高效地分类不同的肿瘤类型，并且比之前的方法更高效。<details>
<summary>Abstract</summary>
The majority of primary Central Nervous System (CNS) tumors in the brain are among the most aggressive diseases affecting humans. Early detection of brain tumor types, whether benign or malignant, glial or non-glial, is critical for cancer prevention and treatment, ultimately improving human life expectancy. Magnetic Resonance Imaging (MRI) stands as the most effective technique to detect brain tumors by generating comprehensive brain images through scans. However, human examination can be error-prone and inefficient due to the complexity, size, and location variability of brain tumors. Recently, automated classification techniques using machine learning (ML) methods, such as Convolutional Neural Network (CNN), have demonstrated significantly higher accuracy than manual screening, while maintaining low computational costs. Nonetheless, deep learning-based image classification methods, including CNN, face challenges in estimating class probabilities without proper model calibration. In this paper, we propose a novel brain tumor image classification method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new approach effectively captures hidden image features, enabling the differentiation of various tumor types and accurate label predictions. Additionally, the SIBOW-SVM is able to estimate the probabilities of images belonging to each class, thereby providing high-confidence classification decisions. We have also developed scalable and parallelable algorithms to facilitate the practical implementation of SIBOW-SVM for massive images. As a benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal. Our results show that the new method outperforms state-of-the-art methods, including CNN.
</details>
<details>
<summary>摘要</summary>
主要的中枢神经系统（CNS）肿瘤在脑中是人类最严重的一种疾病之一。早期识别脑肿瘤类型，无论是良性或恶性， glial 或非 glial，是致命的，以提高人类生存时间。磁共振成像（MRI）是识别脑肿瘤最有效的技术，可以生成详细的脑图像。然而，人工检查可能会受到脑肿瘤的复杂性、大小和位置的变化带来干扰，导致误差和不效率。最近，使用机器学习（ML）方法，如卷积神经网络（CNN），已经达到了人工检查的高精度。然而，深度学习基于的图像分类方法，包括CNN，面临着估计类别概率的挑战。在本文中，我们提出了一种新的脑肿瘤图像分类方法，称为SIBOW-SVM，它将袋子模型（BoF）和SIFT特征提取结合了支持向量机（wSVM）。这种新的方法能够捕捉隐藏的图像特征，以便 diferenciar 多种肿瘤类型并提供准确的标签预测。此外，SIBOW-SVM还能够估计每个图像所属的类别概率，从而提供高置信的分类决策。我们还开发了可扩展和并行的算法，以便实现SIBOW-SVM的实际应用。作为一个标准，我们将SIBOW-SVM应用于脑肿瘤MRI图像数据集，该数据集包含四个类别： glioma， meningioma， pituitary，和正常。我们的结果表明，新方法在比较之下超越了现有的方法，包括CNN。
</details></li>
</ul>
<hr>
<h2 id="AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection"><a href="#AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection" class="headerlink" title="AdapterShadow: Adapting Segment Anything Model for Shadow Detection"></a>AdapterShadow: Adapting Segment Anything Model for Shadow Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08891">http://arxiv.org/abs/2311.08891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiping Jie, Hui Zhang</li>
<li>for: 提高阴影检测精度</li>
<li>methods: 利用可调适器插入into SAM模型，并使用格子采样方法生成密集点提示来自动 segment shadow</li>
<li>results: 在四个广泛使用的 benchmark 数据集上进行了广泛的实验，并达到了提高阴影检测精度的目的<details>
<summary>Abstract</summary>
Segment anything model (SAM) has shown its spectacular performance in segmenting universal objects, especially when elaborate prompts are provided. However, the drawback of SAM is twofold. On the first hand, it fails to segment specific targets, e.g., shadow images or lesions in medical images. On the other hand, manually specifying prompts is extremely time-consuming. To overcome the problems, we propose AdapterShadow, which adapts SAM model for shadow detection. To adapt SAM for shadow images, trainable adapters are inserted into the frozen image encoder of SAM, since the training of the full SAM model is both time and memory consuming. Moreover, we introduce a novel grid sampling method to generate dense point prompts, which helps to automatically segment shadows without any manual interventions. Extensive experiments are conducted on four widely used benchmark datasets to demonstrate the superior performance of our proposed method. Codes will are publicly available at https://github.com/LeipingJie/AdapterShadow.
</details>
<details>
<summary>摘要</summary>
Segment anything model (SAM) 已经显示出了杰出的表现在universal对象上，尤其是当提供了复杂的提示时。然而，SAM的缺点是双重的。首先，它无法正确分割特定目标，例如阴影图像或医学图像中的病变。其次，手动提供提示是非常时间和 памяти消耗的。为了解决这些问题，我们提议了AdapterShadow，它将SAM模型适应到阴影检测中。为了适应SAM模型到阴影图像，我们在SAM模型的冻结图像Encoder中插入可教育的适应器。此外，我们还引入了一种新的格子采样方法，用于生成密集点提示，以帮助自动分割阴影而无需任何人工干预。我们在四个广泛使用的benchmark数据集上进行了extensive的实验，以证明我们的提议方法的优秀性。代码将在https://github.com/LeipingJie/AdapterShadow上公开。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models"><a href="#One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models" class="headerlink" title="One-Shot Federated Learning with Classifier-Guided Diffusion Models"></a>One-Shot Federated Learning with Classifier-Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08870">http://arxiv.org/abs/2311.08870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: 本研究旨在提出一种基于协同学习的一批联合学习方法，以便在实际应用中避免额外的数据采集和传输成本。</li>
<li>methods: 我们提出了一种基于扩散模型的一批联合学习方法，使用客户端分类器的指导来生成符合客户端分布的数据，并在服务器端进行模型聚合。我们在两个方面进行了targeted优化：首先，我们通过条件编辑随机生成的初始噪声，使其嵌入特定的语义和分布，从而提高生成的质量和稳定性。其次，我们使用客户端分类器的BN统计来为生成过程提供详细的指导。</li>
<li>results: 我们的方法可以有效地处理不同客户端模型和非相关的特征或标签问题，并且不需要训练任何生成器或传输任何附加信息到客户端，从而避免额外的隐私泄露风险。通过利用扩散模型已经储存的广泛知识，我们可以生成符合客户端分布的 sintetic数据，以帮助我们超越客户端样本的知识限制，实现聚合模型的性能超越中央化训练的情况，这些结果得到了 suficient的量化和视觉化实验的证明。<details>
<summary>Abstract</summary>
One-shot federated learning (OSFL) has gained attention in recent years due to its low communication cost. However, most of the existing methods require auxiliary datasets or training generators, which hinders their practicality in real-world scenarios. In this paper, we explore the novel opportunities that diffusion models bring to OSFL and propose FedCADO, utilizing guidance from client classifiers to generate data that complies with clients' distributions and subsequently training the aggregated model on the server. Specifically, our method involves targeted optimizations in two aspects. On one hand, we conditionally edit the randomly sampled initial noises, embedding them with specified semantics and distributions, resulting in a significant improvement in both the quality and stability of generation. On the other hand, we employ the BN statistics from the classifiers to provide detailed guidance during generation. These tailored optimizations enable us to limitlessly generate datasets, which closely resemble the distribution and quality of the original client dataset. Our method effectively handles the heterogeneous client models and the problems of non-IID features or labels. In terms of privacy protection, our method avoids training any generator or transferring any auxiliary information on clients, eliminating any additional privacy leakage risks. Leveraging the extensive knowledge stored in the pre-trained diffusion model, the synthetic datasets can assist us in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases, which is convincingly demonstrated in the sufficient quantification and visualization experiments conducted on three large-scale multi-domain image datasets.
</details>
<details>
<summary>摘要</summary>
一种新型的一击联合学习（OSFL）方法，它在最近几年内得到了广泛关注，因为它的通信成本较低。然而，大多数现有的方法需要附加的数据集或训练生成器，这限制了它们在实际场景中的实用性。在这篇论文中，我们探讨了Diffusion模型带来的新机遇，并提出了FedCADO方法，通过客户端分类器的指导，在服务器上训练汇集模型。具体来说，我们的方法包括两个方面的优化。一方面，我们根据客户端的分布和特性进行 Conditional Editing，使得随机生成的初始噪音中嵌入了指定的 semantics和分布，从而大幅提高生成质量和稳定性。另一方面，我们利用客户端的BN统计来提供详细的指导，以便在生成过程中进行精细的调整。这些定制的优化使得我们可以无限制地生成数据集，它们与原始客户端数据的分布和质量具有高度的相似性。我们的方法能够有效地处理不同客户端模型以及非标一致的特征或标签问题。另外，我们的方法不需要训练生成器或传输附加的auxiliary信息，因此不会增加隐私泄露风险。利用批量训练的普遍知识，我们的Synthetic数据可以帮助我们突破客户端样本的知识限制，从而实现汇集模型的性能超越中心化训练的表现峰值，这在三个大规模多domain图像数据集的充分量和视觉化实验中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques"><a href="#Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques" class="headerlink" title="Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques"></a>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08863">http://arxiv.org/abs/2311.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romain3ch216/tlse-experiments">https://github.com/romain3ch216/tlse-experiments</a></li>
<li>paper_authors: Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet</li>
<li>for: 这个论文用于描述一个新的 Toulouse 遥感谱数据集，以便用于 spectral representation learning 和分类大规模遥感谱图像中的少量标注像素。</li>
<li>methods: 这个论文使用了 semi-supervised 和 self-supervised 技术来解决遥感谱数据集中标注的稀缺性问题，并实验了 Masked Autoencoders 自我隐藏任务以及一种基于普通自动编码器和Random Forest分类器的像素级分类方法。</li>
<li>results: 这个论文的实验结果显示，使用 Toulouse 遥感谱数据集和 Masked Autoencoders 自我隐藏任务可以达到 82% 的总准确率和 74% F1 分数。<details>
<summary>Abstract</summary>
Airborne hyperspectral images can be used to map the land cover in large urban areas, thanks to their very high spatial and spectral resolutions on a wide spectral domain. While the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. Yet, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: a limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set that stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details>
<details>
<summary>摘要</summary>
“空中卫星偏振图像可以用于覆盖大都市区域的土地覆盖，因为它们具有非常高的空间和спектраль分辨率，覆盖广泛的 спектраль频谱。而偏振图像的spectral维度对地表化学成分非常有助于，但是使用当今最先进的机器学习算法来地图土地覆盖却受到数据库的有限性的限制。为了应对数据缺乏， semi-supervised和self-supervised技术在社区中引起了很多关注。然而，公共可用的偏振数据集通常用于机器学习模型的benchmarking是不完全适合评估其泛化性能，因为它们具有以下一些属性：地理覆盖率偏低（不反映大都市区域的 спектраль多样性），土地覆盖类型很少（不能反映实际情况）和缺乏适合的标准训练/测试分割。因此，我们在这篇论文中发布了 Toulouse 偏振数据集，它与其他数据集不同，以解决上述问题，以便在大规模偏振图像上进行 spectral representation 学习和分类。此外，我们还讨论了自动化任务的Masked Autoencoders，并在基于普通自动编码器和随机森林分类器的基础上建立了82%的总准确率和74%的F1分数的基eline。 Toulouse 偏振数据集和我们的代码在 https://www.toulouse-hyperspectral-data-set.com 和 https://www.github.com/Romain3Ch216/tlse-experiments 上公开提供。”
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentations-in-Deep-Weight-Spaces"><a href="#Data-Augmentations-in-Deep-Weight-Spaces" class="headerlink" title="Data Augmentations in Deep Weight Spaces"></a>Data Augmentations in Deep Weight Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08851">http://arxiv.org/abs/2311.08851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Shamsian, David W. Zhang, Aviv Navon, Yan Zhang, Miltiadis Kofinas, Idan Achituve, Riccardo Valperga, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, Ethan Fetaya, Gal Chechik, Haggai Maron</li>
<li>for: This paper focuses on addressing the challenge of overfitting in learning in weight spaces, a promising research direction with applications in various fields.</li>
<li>methods: The paper investigates data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements.</li>
<li>results: The paper introduces a novel augmentation scheme based on the Mixup method and evaluates the performance of several recently proposed data augmentation schemes on existing and new benchmarks.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了在权重空间学习中的过拟合问题，这是一个有前途的研究方向，应用于各种领域。</li>
<li>methods: 论文研究了权重空间数据扩充技术，可以在 fly 上生成新的数据示例，无需再训练输入权重空间元素。</li>
<li>results: 论文提出了一种基于 Mixup 方法的新数据扩充方案，并评估了一些最近提出的数据扩充方案的性能在现有和新的标准 bench 上。<details>
<summary>Abstract</summary>
Learning in weight spaces, where neural networks process the weights of other deep neural networks, has emerged as a promising research direction with applications in various fields, from analyzing and editing neural fields and implicit neural representations, to network pruning and quantization. Recent works designed architectures for effective learning in that space, which takes into account its unique, permutation-equivariant, structure. Unfortunately, so far these architectures suffer from severe overfitting and were shown to benefit from large datasets. This poses a significant challenge because generating data for this learning setup is laborious and time-consuming since each data sample is a full set of network weights that has to be trained. In this paper, we address this difficulty by investigating data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements. We first review several recently proposed data augmentation schemes %that were proposed recently and divide them into categories. We then introduce a novel augmentation scheme based on the Mixup method. We evaluate the performance of these techniques on existing benchmarks as well as new benchmarks we generate, which can be valuable for future studies.
</details>
<details>
<summary>摘要</summary>
学习在权重空间中， где神经网络处理其他深度神经网络的权重，已经成为一个有前途的研究方向，它在各种领域都有应用，从分析和编辑神经场和隐藏神经表示之间，到网络剪枝和量化。现有的设计架构具有考虑其特殊、协变结构的能力，但是目前这些架构受到严重的馈敏化问题困扰。这种问题的主要原因是生成这种学习环境的数据是劳动ious和时间consuming，因为每个数据样本都需要训练全部网络权重。在这篇论文中，我们解决这个困难，通过调查数据增强技术，可以在飞行时生成新的数据示例，无需进行额外的输入权重空间元素训练。我们首先回顾最近提出的数据增强方案，并将它们分为类别。然后，我们介绍了一种基于 Mixup 方法的新的增强方案。我们对这些技术在现有的标准准测试集上进行评估，以及我们自己生成的新测试集，以便对未来的研究提供价值。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting"><a href="#Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting" class="headerlink" title="Controlling the Output of a Generative Model by Latent Feature Vector Shifting"></a>Controlling the Output of a Generative Model by Latent Feature Vector Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08850">http://arxiv.org/abs/2311.08850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Róbert Belanec, Peter Lacko, Kristína Malinovská</li>
<li>for: 这个论文的目的是提出一种控制输出图像修改的方法，使用StyleGAN3生成器生成的图像中的Semantic特征进行修改。</li>
<li>methods: 该方法使用了一个预训练的StyleGAN3生成器，并补充了一个基于Convolutional Neural Network（CNN）的分类器，用于将生成的图像分类为CelebA数据集中的二分类特征。另外，该方法还使用了一个叫做”latent feature shifter”的神经网络模型，用于将生成器的含义向量Shift到指定的特征方向。</li>
<li>results: 经过训练， latent feature shifter方法可以成功地控制StyleGAN3生成器中的图像修改，并且比基eline方法更多地生成了满足要求的图像。<details>
<summary>Abstract</summary>
State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias}) often generate photorealistic images based on vectors sampled from their latent space. However, the ability to control the output is limited. Here we present our novel method for latent vector shifting for controlled output image modification utilizing semantic features of the generated images. In our approach we use a pre-trained model of StyleGAN3 that generates images of realistic human faces in relatively high resolution. We complement the generative model with a convolutional neural network classifier, namely ResNet34, trained to classify the generated images with binary facial features from the CelebA dataset. Our latent feature shifter is a neural network model with a task to shift the latent vectors of a generative model into a specified feature direction. We have trained latent feature shifter for multiple facial features, and outperformed our baseline method in the number of generated images with the desired feature. To train our latent feature shifter neural network, we have designed a dataset of pairs of latent vectors with and without a certain feature. Based on the evaluation, we conclude that our latent feature shifter approach was successful in the controlled generation of the StyleGAN3 generator.
</details>
<details>
<summary>摘要</summary>
现代生成模型（如StyleGAN3 \cite{karras2021alias）often生成高品质图像，基于生成器的 latent space 中采样的 вектор。然而，控制输出的能力有限。我们现在介绍一种新的方法，即 latent vector shifting，用于控制生成器输出图像的修改。我们使用 StyleGAN3 生成器，该生成器可生成高分辨率的人脸图像。我们补充了生成器的 pre-trained 模型，并使用 ResNet34  convolutional neural network 分类器，用于从 CelebA 数据集中分类生成器的生成图像的binary facial features。我们的 latent feature shifter 是一种 neural network 模型，其任务是将生成器的 latent vector Shift into 指定的特征方向。我们在多个 facial features 上训练了 latent feature shifter 模型，并超越了我们的基线方法。为了训练我们的 latent feature shifter 模型，我们设计了一个包含 latent vector 与和无特征的对应对的数据集。根据评估结果，我们得出结论，我们的 latent feature shifter 方法成功地控制了 StyleGAN3 生成器的输出。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Video-Relighting-Using-Casual-Light-Stage"><a href="#Personalized-Video-Relighting-Using-Casual-Light-Stage" class="headerlink" title="Personalized Video Relighting Using Casual Light Stage"></a>Personalized Video Relighting Using Casual Light Stage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08843">http://arxiv.org/abs/2311.08843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Myeong Choi, Max Christman, Roni Sengupta</li>
<li>for: 这个论文是为了开发一种个性化视频重新照明算法，能够在任何姿势、表情和照明条件下生成高质量、时间兼容的重新照明视频。</li>
<li>methods: 该算法使用了一种新的神经网络重新照明架构，可以有效地分离出照明源的光照、物体的geometry和反射特征，并将其与目标照明结合以生成重新照明图像。</li>
<li>results: 根据质量和时间稳定性的评估，该算法在使用LSYD和OLAT数据集上提高了肖像图像重新照明质量和时间稳定性，比之前的方法有显著提升。<details>
<summary>Abstract</summary>
In this paper, we develop a personalized video relighting algorithm that produces high-quality and temporally consistent relit video under any pose, expression, and lighting conditions in real-time. Existing relighting algorithms typically rely either on publicly available synthetic data, which yields poor relighting results, or instead on Light Stage data which is inaccessible and is not publicly available. We show that by casually capturing video of a user watching YouTube videos on a monitor we can train a personalized algorithm capable of producing high-quality relighting under any condition. Our key contribution is a novel neural relighting architecture that effectively separates the intrinsic appearance features, geometry and reflectance, from the source lighting and then combines it with the target lighting to generate a relit image. This neural architecture enables smoothing of intrinsic appearance features leading to temporally stable video relighting. Both qualitative and quantitative evaluations show that our relighting architecture improves portrait image relighting quality and temporal consistency over state-of-the-art approaches on both casually captured Light Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time (OLAT) datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种个性化视频重新照明算法，可以在实时下生成高质量、时间协调的重新照明视频，无论人姿、表情或照明条件如何。现有的重新照明算法通常基于公共可用的 sintetic 数据，导致重新照明结果较差，或者使用 Light Stage 数据，但该数据不公开可用。我们展示了通过通过抓取用户在 monitor 上观看 YouTube 视频的视频来训练个性化算法，可以在任何条件下生成高质量的重新照明。我们的关键贡献是一种新的神经网络重新照明架构，可以有效分离源照明中的内在特征、几何和反射特征，并将其与目标照明结合以生成一个重新照明的图像。这种神经网络架构使得内在特征的平滑化，导致视频重新照明的时间稳定。我们的重新照明算法在 Light Stage at Your Desk 数据集和 Light Stage 捕捉 One Light At a Time 数据集上的质量和时间稳定性都超过了状态之前的方法。
</details></li>
</ul>
<hr>
<h2 id="Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding"><a href="#Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding" class="headerlink" title="Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding"></a>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08835">http://arxiv.org/abs/2311.08835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjun0830/cgdetr">https://github.com/wjun0830/cgdetr</a></li>
<li>paper_authors: WonJun Moon, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo</li>
<li>for: 本研究的目的是提供关键clip-wise的交叉模态交互，以便更好地利用视频和文本查询之间的关系。</li>
<li>methods: 我们使用了一种适应性权重层和假token来适应文本查询，并通过学习高级概念的共同embedding来推断clip-word关系。</li>
<li>results: 我们的CG-DETR模型在多个benchmark上达到了当前最佳的Result，并且在高级概念 Retrieval和突出部分检测中都表现出色。<details>
<summary>Abstract</summary>
Recent endeavors in video temporal grounding enforce strong cross-modal interactions through attention mechanisms to overcome the modality gap between video and text query. However, previous works treat all video clips equally regardless of their semantic relevance with the text query in attention modules. In this paper, our goal is to provide clues for query-associated video clips within the crossmodal encoding process. With our Correlation-Guided Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of cross-modal interactions and how to exploit such degrees for prediction. First, we design an adaptive cross-attention layer with dummy tokens. Dummy tokens conditioned by text query take a portion of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all word tokens equally inherit the text query's correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e., moment and sentence level, and inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency detector to exploit each video clip's degrees of text engagement. We validate the superiority of CG-DETR with the state-of-the-art results on various benchmarks for both moment retrieval and highlight detection. Codes are available at https://github.com/wjun0830/CGDETR.
</details>
<details>
<summary>摘要</summary>
近期的视频时间固定工作通过注意力机制来强化视频和文本查询之间的跨模态交互，以抗跨模态差距。然而，前一些工作往往对所有视频片段进行同样的注意力处理，不充分考虑视频片段与文本查询之间的 semantic relevance。在这篇论文中，我们的目标是为文本查询关联的视频片段在多模态编码过程中提供线索。我们提出了一种适应性跨注意力层，使用幂token来约束视频片段与文本查询之间的交互。然而，不同的单词token不一样继承文本查询对视频片段的相关性。因此，我们进一步指导跨注意力地图，通过学习高级概念的共同空间，即时刻和句子水平，推断视频片段和单词之间的相关性。最后，我们使用时刻适应性的焦点探测器来利用每个视频片段的文本参与度。我们 Validate 我们的 CG-DETR 模型的优越性，并在多个 benchmark 上达到了状态之巅的结果，包括时刻检索和突出部分检测。代码可以在 https://github.com/wjun0830/CGDETR 上找到。
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution"><a href="#Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution" class="headerlink" title="Target-oriented Domain Adaptation for Infrared Image Super-Resolution"></a>Target-oriented Domain Adaptation for Infrared Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08816">http://arxiv.org/abs/2311.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongsongh/dasrgan">https://github.com/yongsongh/dasrgan</a></li>
<li>paper_authors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro Omachi<br>for:这篇论文的目的是提高红外超分解的精度，并使用可见光图像来增强红外图像的текстура细节。methods:该方法使用了两个关键组件：1）Texture-Oriented Adaptation（TOA），用于精细地修饰tekstura细节，和2）Noise-Oriented Adaptation（NOA），用于减少噪声传输。特别是，TOA使用了一个专门的检定器，包括一个优先EXTRACTING分支，并使用 Sobel指导的对抗损失来有效地对 tekstura分布进行对齐。同时，NOA使用了一种噪声对抗损失来在对抗训练中分别分配生成和Gaussian噪声模式分布。results:我们的广泛实验表明，DASRGAN在多个标准 benchmarcks 和不同的upsampling factor上表现出色，并在比较分析中超越了现有的方法。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/yongsongH/DASRGAN%7D">https://github.com/yongsongH/DASRGAN}</a> 上获取。<details>
<summary>Abstract</summary>
Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \url{https://github.com/yongsongH/DASRGAN}.
</details>
<details>
<summary>摘要</summary>
最近的努力已经探索了使用可见光图像来增强探测器的细节Texture details in infrared (IR) super-resolution. 然而，这种直接适应approach  часто变成了一把两面刃，因为它会提高细节，但同时也会引入噪声和抖音 artifacts。为了解决这些挑战，我们提出了Target-oriented Domain Adaptation SRGAN（DASRGAN），一种创新的框架，特地设计用于Robust IR super-resolution model adaptation。DASRGAN 的核心思想是通过两个关键组成部分来实现：1）Texture-Oriented Adaptation（TOA），用于精细地修正细节 detail; 2）Noise-Oriented Adaptation（NOA），用于减少噪声的传输。具体来说，TOA 使用了一个特殊的探测器，包括一个特殊的提取分支，并使用 Sobel 导向的对抗损失来有效地对细节 distribuition 进行对应。同时，NOA 使用了噪声对抗损失来在对抗训练中分别地分离生成的 Pattern distribution 和 Gaussian 噪声 Pattern distribution。我们的广泛的实验证明了 DASRGAN 的优越性。与其他方法进行比较分析，我们在多个 benchmark 和 upsampling factor 上达到了新的州优性标准。代码可以在 \url{https://github.com/yongsongH/DASRGAN} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-active-learning-for-surgery-video-segmentation"><a href="#Correlation-aware-active-learning-for-surgery-video-segmentation" class="headerlink" title="Correlation-aware active learning for surgery video segmentation"></a>Correlation-aware active learning for surgery video segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08811">http://arxiv.org/abs/2311.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Wu, Pablo Marquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael Sznitman</li>
<li>for: 本研究旨在提高针对手术视频分割的semantic segmentation模型性能，采用活动学习（AL）策略，并考虑视频序列中模型uncertainty和时间特征。</li>
<li>methods: 本研究提出了一种新的AL策略，称为COrrelation-aWare Active Learning（\COALSamp{}），它通过在彩色学习中精心调整的特征空间中Project images，然后从视频帧本地集中随机选择一定数量的代表图像进行标注。</li>
<li>results: 在两个手术工具视频数据集和三个真实世界视频数据集上，本研究实现了高效的semantic segmentation，并且在不同的视频序列中保持了高度的一致性。<details>
<summary>Abstract</summary>
Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. However, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, \COALSamp{}, COrrelation-aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>semantic segmentation是一项复杂的任务，它依赖于大量的标注图像数据。然而，在医疗领域中标注这些数据可以是时间消耗和资源占用的。活动学习（AL）是一种受欢迎的方法，它可以在每一轮选择图像进行标注，以提高模型性能。在视频数据中，需要考虑模型不确定和时间序列的特性，并在本地帧cluster中选择一定数量的表示性图像。我们提出了一种新的AL策略，称为\COALSamp{}，它是基于对比学习细化的latent space进行 проекции，并从本地帧cluster中选择一定数量的表示性图像。我们在两个手术工具视频数据集和三个实际世界视频数据集上进行了实验，并将数据和代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery"><a href="#EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery" class="headerlink" title="EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery"></a>EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08799">http://arxiv.org/abs/2311.08799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Yang, Zhihao Zhao, Siyuan Shen, Daniel Zapp, Mathias Maier, Kai Huang, Nassir Navab, M. Ali Nasseri</li>
<li>for:  robotic ophthalmic surgery, high-precision interventions, retina penetration, subretinal injection, removal of floating tissues, retinal detachment</li>
<li>methods:  image-based methods, shadow positions, relative depth position, instrument tip insertion trajectory, surgical simulator</li>
<li>results:  average depth error, 0.0127 mm for floating targets, 0.3473 mm for retinal targets, no retinal damage<details>
<summary>Abstract</summary>
Robotic ophthalmic surgery is an emerging technology to facilitate high-precision interventions such as retina penetration in subretinal injection and removal of floating tissues in retinal detachment depending on the input imaging modalities such as microscopy and intraoperative OCT (iOCT). Although iOCT is explored to locate the needle tip within its range-limited ROI, it is still difficult to coordinate iOCT's motion with the needle, especially at the initial target-approaching stage. Meanwhile, due to 2D perspective projection and thus the loss of depth information, current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets. To address this limitation, we propose to use the shadow positions of the target and the instrument tip to estimate their relative depth position and accordingly optimize the instrument tip's insertion trajectory until the tip approaches targets within iOCT's scanning area. Our method succeeds target approaching on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details>
<details>
<summary>摘要</summary>
robotic ophthalmic surgery 是一种emerging technology，用于实现高精度干预，如retina penetration 和 floating tissues 的 eliminating ，具体取决于输入 imaging modalities 如 microscopy 和 intraoperative OCT (iOCT)。 although iOCT 可以used to locate the needle tip within its range-limited ROI，但是却难以协调 iOCT 的运动和针，特别是在初始目标接近阶段。 Meanwhile，due to 2D perspective projection and thus the loss of depth information，current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets。 To address this limitation，我们提议使用target 和 instrumente tip的阴影位置来估算它们的相对深度位置，并根据此估算instrumente tip的插入轨迹，直到针到达目标在 iOCT 的扫描范围内。 our method succeeds in approaching targets on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details></li>
</ul>
<hr>
<h2 id="HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection"><a href="#HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection" class="headerlink" title="HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection"></a>HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08786">http://arxiv.org/abs/2311.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxin Chen, Mingrui Zhu, Nannan Wang, Xinbo Gao</li>
<li>for: 隐藏人脸信息，解决智能设备和计算机视觉技术的人脸隐私问题。</li>
<li>methods: 提出了一种高品质和遮挡 robust de-identification（HFORD）方法，通过分离个体特征和特征之间的关系，保留人脸特征和背景信息，并在遮挡场景下提供高品质的隐私保护。</li>
<li>results: 对比其他面部隐私方法，HFORD方法具有更高的质量、更好的细节准确性和更强的遮挡鲁棒性。<details>
<summary>Abstract</summary>
With the popularity of smart devices and the development of computer vision technology, concerns about face privacy protection are growing. The face de-identification technique is a practical way to solve the identity protection problem. The existing facial de-identification methods have revealed several problems, including the impact on the realism of anonymized results when faced with occlusions and the inability to maintain identity-irrelevant details in anonymized results. We present a High-Fidelity and Occlusion-Robust De-identification (HFORD) method to deal with these issues. This approach can disentangle identities and attributes while preserving image-specific details such as background, facial features (e.g., wrinkles), and lighting, even in occluded scenes. To disentangle the latent codes in the GAN inversion space, we introduce an Identity Disentanglement Module (IDM). This module selects the latent codes that are closely related to the identity. It further separates the latent codes into identity-related codes and attribute-related codes, enabling the network to preserve attributes while only modifying the identity. To ensure the preservation of image details and enhance the network's robustness to occlusions, we propose an Attribute Retention Module (ARM). This module adaptively preserves identity-irrelevant details and facial occlusions and blends them into the generated results in a modulated manner. Extensive experiments show that our method has higher quality, better detail fidelity, and stronger occlusion robustness than other face de-identification methods.
</details>
<details>
<summary>摘要</summary>
WITH 智能设备的普及和计算机视觉技术的发展，人脸隐私保护的问题日益突出。面部隐私化技术是一种实际的解决方案。现有的面部隐私化方法存在一些问题，包括在 occlusion 时的真实性降低和保留不相关的特征信息。我们提出了高品质和防护环境鲁棒的面部隐私化方法（HFORD）。这种方法可以分离特征和属性，同时保留图像特有的背景、表情特征（如皱纹）和照明信息，即使在 occluded 场景下。为了分离 GAN 倒推空间中的秘密码，我们引入了个性隐私分解模块（IDM）。这个模块选择与identify closely related的秘密码，然后将其分解成属于特征和属性的秘密码，使网络只修改特征，保留属性。为确保图像细节的保留和网络的防护环境鲁棒性，我们提出了特征保持模块（ARM）。这个模块可以自适应保留不相关的特征和人脸干扰，并将其混合到生成结果中，以提高网络的鲁棒性和质量。我们的方法在多种场景下展现出较高的质量、更好的细节准确性和更强的防护环境鲁棒性，与其他面部隐私化方法相比。
</details></li>
</ul>
<hr>
<h2 id="Language-Semantic-Graph-Guided-Data-Efficient-Learning"><a href="#Language-Semantic-Graph-Guided-Data-Efficient-Learning" class="headerlink" title="Language Semantic Graph Guided Data-Efficient Learning"></a>Language Semantic Graph Guided Data-Efficient Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08782">http://arxiv.org/abs/2311.08782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang</li>
<li>for: 提高机器学习模型的数据效率，使其能够从有限数据中学习并减少人工监督的依赖度。</li>
<li>methods: 利用语言semantic图（LSG），通过自然语言描述中的标签信息提取高级别semantic关系，并将其用于导航主模型的训练，以更好地利用标签知识。</li>
<li>results: 在图像、视频和音频模式下，通过LSG方法在传输学习和半supervised学习场景中显著提高性能，并且对训练过程也有加速效果。<details>
<summary>Abstract</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过使用已有数据中的标签信息，我们提出了一种新的数据效率角度，即利用标签中的语义信息。我们首先构建了一个语义图（LSG），该图基于标签中的自然语言描述。然后，我们训练了一个辅助图神经网络，以EXTRACT高级语义关系。这些语义关系 затем用于引导主模型的训练，以更好地利用标签知识。在图像、视频和音频模式下，我们在TL和SSL场景中应用了LSG方法，并证明其在其他数据效率学习方法中表现出色。此外，我们的深入分析表明，LSG方法也可以加速训练过程。Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation"><a href="#Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation" class="headerlink" title="Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation"></a>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08774">http://arxiv.org/abs/2311.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hesham Ali, Idriss Tondji, Mennatullah Siam</li>
<li>for: 静脉 histopathological 图像中的核体分 segmentation 是诊断和治疗肿瘤疾病的关键任务，可以降低手动检查微scopic 组织图像所需的时间，并解决了诊断过程中的冲突。深度学习 已经在这种任务中证明了其用用。</li>
<li>methods: 我们提出了一种新的核体分 segmentation 方法，利用可用的标注和无标注数据。我们的方法结合了transductive 和 inductive 学习的优点，以前两者已经分别在单独的方法中实现。 inductive 学习 aspires to approximate the general function and generalize to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification。</li>
<li>results: 我们在 MoNuSeg 测试集上评估了我们的方法，并证明了我们的方法的效果和潜在性。这是我们知道的第一个尝试将 transductive 和 inductive 学习结合在一起的研究。此外，我们还提出了一种新的两stage 推理方案。<details>
<summary>Abstract</summary>
AI-assisted nuclei segmentation in histopathological images is a crucial task in the diagnosis and treatment of cancer diseases. It decreases the time required to manually screen microscopic tissue images and can resolve the conflict between pathologists during diagnosis. Deep Learning has proven useful in such a task. However, lack of labeled data is a significant barrier for deep learning-based approaches. In this study, we propose a novel approach to nuclei segmentation that leverages the available labelled and unlabelled data. The proposed method combines the strengths of both transductive and inductive learning, which have been previously attempted separately, into a single framework. Inductive learning aims at approximating the general function and generalizing to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification. To the best of our knowledge, this is the first study to propose such a hybrid approach for medical image segmentation. Moreover, we propose a novel two-stage transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to demonstrate the efficacy and potential of our method.
</details>
<details>
<summary>摘要</summary>
人工智能助成肿瘤分 segmentation在 histopathological 图像中是诊断和治疗 cancer 疾病的关键任务。它可以减少手动检查 microscopic 组织图像所需的时间，并解决了诊断过程中的冲突。深度学习已经证明是非常有用的。然而，缺乏标注数据是深度学习基本的障碍。在这项研究中，我们提出了一种新的 nuclei 分 segmentation 方法，利用可用的标注和未标注数据。我们的方法结合了泛化学习和推导学习的优点，这两种方法在过去分别进行了独立的尝试。泛化学习旨在近似通用函数，并将其推广到未见过的测试数据中，而推导学习则可以利用测试数据来改进分类。到目前为止，这是首次提出了这种混合方法的医疗图像分 segmentation 研究。此外，我们还提出了一种新的两 stage 混合推理方案。我们在 MoNuSeg 数据集上评估了我们的方法，以示我们的方法的效果和潜力。
</details></li>
</ul>
<hr>
<h2 id="FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier"><a href="#FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier" class="headerlink" title="FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier"></a>FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/artiprocher/sd-webui-fastblend">https://github.com/artiprocher/sd-webui-fastblend</a></li>
<li>paper_authors: Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang, Mingyi Jin</li>
<li>for:  addresses the consistency problem in video processing for tasks such as style transfer and image editing.</li>
<li>methods:  based on a patch matching algorithm, with two inference modes: blending and interpolation.</li>
<li>results:  outperforms existing methods for video deflickering and video synthesis in the blending mode, and surpasses video interpolation and model-based video processing approaches in the interpolation mode.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文是为了解决视频处理中的一致性问题，例如样式传输和图像修改等任务。</li>
<li>methods: 该论文提出了一个强大的模型自由工具kit called FastBlend，基于一种贴图匹配算法，包括拼接和 interpolate两种推理模式。在拼接模式下，FastBlend 消除了视频闪烁，通过拼接视频帧内的滑动窗口进行拼接。此外，我们优化了不同应用场景下的计算效率和视频质量。在 interpolate 模式下，给定一个或多个随机生成的Diffusion模型 Keyframe，FastBlend 可以Render整个视频。由于 FastBlend 不改变Diffusion模型的生成过程，它表现出了优秀的兼容性。</li>
<li>results: 实验证明，FastBlend 在拼接模式下超越现有的视频闪烁消除和视频合成方法，在 interpolate 模式下超越视频 interpolate 和基于模型的视频处理方法。软件代码已经在 GitHub 上发布。<details>
<summary>Abstract</summary>
With the emergence of diffusion models and rapid development in image processing, it has become effortless to generate fancy images in tasks such as style transfer and image editing. However, these impressive image processing approaches face consistency issues in video processing. In this paper, we propose a powerful model-free toolkit called FastBlend to address the consistency problem for video processing. Based on a patch matching algorithm, we design two inference modes, including blending and interpolation. In the blending mode, FastBlend eliminates video flicker by blending the frames within a sliding window. Moreover, we optimize both computational efficiency and video quality according to different application scenarios. In the interpolation mode, given one or more keyframes rendered by diffusion models, FastBlend can render the whole video. Since FastBlend does not modify the generation process of diffusion models, it exhibits excellent compatibility. Extensive experiments have demonstrated the effectiveness of FastBlend. In the blending mode, FastBlend outperforms existing methods for video deflickering and video synthesis. In the interpolation mode, FastBlend surpasses video interpolation and model-based video processing approaches. The source codes have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
“随着扩散模型的出现和图像处理的快速发展，实现了丰富的图像处理任务，如Style Transfer和图像修复。但这些图像处理方法在视频处理中存在一致性问题。在本文中，我们提出了一个强大的无模型工具组合called FastBlend，以解决视频处理中的一致性问题。基于一个贴合算法，我们设计了两种推论方式，包括融合和 interpolate。在融合模式下，FastBlend 删除了视频闪烁，通过视频内的滑块窗口融合几帧帧。此外，我们对不同应用场景进行了优化，以提高计算效率和视频质量。在 interpolate 模式下，给定一帧或更多的静止图像，FastBlend 可以预测整个视频。由于 FastBlend 不会修改扩散模型的生成过程，因此它具有很好的相容性。实验结果显示 FastBlend 的效果。在融合模式下，FastBlend 比现有的视频闪烁和视频合成方法更好。在 interpolate 模式下，FastBlend 比 виде interpolate 和基于模型的视频处理方法更好。源代码已经在 GitHub 上发布。”
</details></li>
</ul>
<hr>
<h2 id="4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters"><a href="#4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters" class="headerlink" title="4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters"></a>4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08759">http://arxiv.org/abs/2311.08759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhou-yijie/msltnet">https://github.com/zhou-yijie/msltnet</a></li>
<li>paper_authors: Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu</li>
<li>for: 这篇论文旨在提出一种高效且轻量级的多层感知网络，用于纠正摄像头暴露不当的问题。</li>
<li>methods: 该方法使用了多层感知网络，首先使用拉普拉斯 pyramid 技术将输入图像分解成高和低频层，然后采用 pixel-adaptive 线性变换，通过有效的双边网格学习或 1x1 卷积实现。</li>
<li>results: 实验结果表明，提出的 MSLT 网络在两个标准数据集上比现有的方法更高效，并且可以在 Titan RTX GPU 上处理 4K 分辨率的 sRGB 图像，每秒 125 帧。<details>
<summary>Abstract</summary>
The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet.
</details>
<details>
<summary>摘要</summary>
“对于不当露光的照片，使用深度卷积神经网或Transformers进行修复已经得到了广泛的改善。然而，这些方法通常具有较大的参数数量和高 computational FLOPs，尤其是在高分辨率的照片上。在这篇论文中，我们提出了 extremely light-weight（只有约8K参数）的多尺度线性转换网络（MSLT），可以在Titan RTX GPU上处理4K分辨率的sRGB图像，每秒125帧（FPS）。具体来说，我们的MSLT网络首先将输入图像分解成高和低频层使用拉普拉斯 pyramid 技术，然后遍历不同层次进行像素适应的直线转换，这是通过高效的双方格学习或1x1卷积而实现。实验结果显示，我们的MSLTs在照片露光修复方面与现有的state-of-the-arts进行比较，提供了更高效的修复效果。广泛的ablation study validate了我们的贡献。code可以在https://github.com/Zhou-Yijie/MSLTNet中找到。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection"><a href="#Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection" class="headerlink" title="Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection"></a>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08747">http://arxiv.org/abs/2311.08747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun Bao, Jie Cao, Yaqian Ning, Tianhua Zhao, Zhijun Li, Zechen Wang, Li Zhang, Qun Hao</li>
<li>For: 这个论文旨在提出一种基于深度学习的红外小目标检测方法，以提高红外小目标在背景干扰下的检测精度。* Methods: 该方法基于 transformer 架构，保留 dense nested attention network (DNANet) 的粗糙结构，并在特征提取阶段引入 Swin-transformer，以增强特征的连续性。 此外，该方法还 integrate ACmix attention structure 到 dense nested structure 中，以提高中间层特征的表示能力。* Results: 该方法在对公共数据集进行测试时，与其他当前领先方法进行比较，在检测概率 (P_d)、假阳性率 (F_a) 和 mean intersection of union ($mIoU$) 上均表现出色，特别是在 NUDT-SIRST 数据集上，$mIoU$ 达到 90.89%。<details>
<summary>Abstract</summary>
Infrared small target detection based on deep learning offers unique advantages in separating small targets from complex and dynamic backgrounds. However, the features of infrared small targets gradually weaken as the depth of convolutional neural network (CNN) increases. To address this issue, we propose a novel method for detecting infrared small targets called improved dense nested attention network (IDNANet), which is based on the transformer architecture. We preserve the dense nested structure of dense nested attention network (DNANet) and introduce the Swin-transformer during feature extraction stage to enhance the continuity of features. Furthermore, we integrate the ACmix attention structure into the dense nested structure to enhance the features of intermediate layers. Additionally, we design a weighted dice binary cross-entropy (WD-BCE) loss function to mitigate the negative impact of foreground-background imbalance in the samples. Moreover, we develop a dataset specifically for infrared small targets, called BIT-SIRST. The dataset comprises a significant amount of real-world targets and manually annotated labels, as well as synthetic data and corresponding labels. We have evaluated the effectiveness of our method through experiments conducted on public datasets. In comparison to other state-of-the-art methods, our approach outperforms in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</details>
<details>
<summary>摘要</summary>
infrared小目标检测基于深度学习具有独特优势，可以在复杂和动态背景中分离小目标。然而，深度 convolutional neural network (CNN) 中小目标特征逐渐弱化。为解决这个问题，我们提出一种改进的 dense nested attention network (IDNANet)，基于 transformer 架构。我们保留 dense nested structure 的 dense nested attention network (DNANet)，并在特征提取阶段引入 Swin-transformer，以增强特征的连续性。此外，我们将 ACmix attention 结构integrated into dense nested structure，以提高中间层特征的表达能力。此外，我们还提出了一种weighted dice binary cross-entropy (WD-BCE) 损失函数，以减少样本中背景干扰的负面影响。此外，我们还开发了一个专门 для infrared小目标的数据集，名为 BIT-SIRST。该数据集包括大量真实世界目标和手动标注的标签，以及人工生成的数据和相应的标签。我们通过对公共数据集进行实验，证明了我们的方法的效果。与其他当前状态的方法相比，我们的方法在检测概率（P_d）、false-alarm rate（F_a）和 Mean Intersection of Union（$mIoU）方面表现出色，其中 $mIoU$ 达到了 90.89% 和 79.72% 在 NUDT-SIRST 和 NUAA-SIRST 数据集上。
</details></li>
</ul>
<hr>
<h2 id="A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video"><a href="#A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video" class="headerlink" title="A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video"></a>A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08746">http://arxiv.org/abs/2311.08746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Honggang Qi</li>
<li>for: 提高压缩 видео质量</li>
<li>methods: 使用扩散模型进行 posterior 处理</li>
<li>results: 对混合数据集进行实验，结果较 existed 方法 superiorHere’s a brief explanation of each point:* “for”: The paper is written to improve the quality of compressed videos by proposing a diffusion model based post-processing method.* “methods”: The proposed method uses a diffusion model to estimate the feature vectors of the compressed video and then uses the estimated feature vectors as prior information for a quality enhancement model to adaptively enhance the quality of the compressed video.* “results”: The proposed method outperforms existing methods on mixed datasets in terms of quality enhancement results.<details>
<summary>Abstract</summary>
Video post-processing methods can improve the quality of compressed videos at the decoder side. Most of the existing methods need to train corresponding models for compressed videos with different quantization parameters to improve the quality of compressed videos. However, in most cases, the quantization parameters of the decoded video are unknown. This makes existing methods have their limitations in improving video quality. To tackle this problem, this work proposes a diffusion model based post-processing method for compressed videos. The proposed method first estimates the feature vectors of the compressed video and then uses the estimated feature vectors as the prior information for the quality enhancement model to adaptively enhance the quality of compressed video with different quantization parameters. Experimental results show that the quality enhancement results of our proposed method on mixed datasets are superior to existing methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符串。<</SYS>>压缩视频后处理技术可以提高压缩视频解码器 сторо的质量。大多数现有方法需要为压缩视频不同的压缩参数训练对应的模型，以提高压缩视频质量。然而，在大多数情况下，解码后的视频压缩参数未知。这限制了现有方法的应用。为解决这问题，该工作提议一种基于扩散模型的后处理方法 для压缩视频。提议方法首先估算压缩视频中的特征向量，然后使用估算的特征向量作为质量提升模型的先天信息，以适应性地提高压缩视频的质量。实验结果表明，我们提议的方法在混合数据集上的质量提升结果比现有方法更佳。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories"><a href="#Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories" class="headerlink" title="Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories"></a>Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08716">http://arxiv.org/abs/2311.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Nitta, Taiji Suzuki, Albert Rodríguez Mulet, Atsushi Yaguchi, Ryusuke Hirai</li>
<li>for: 这个研究旨在提出一种适应多客户端不同输入图像大小和输出类别数量的联边学习方法，以及一个可靠地评估联边学习的通用差异关系。</li>
<li>methods: 本研究提出了一种名为ScalableFL的联边学习方法，其中每个客户端的本地模型深度和宽度将根据客户端的输入图像大小和输出类别数量进行调整。</li>
<li>results: 研究人员透过实验证明了ScalableFL在多个不同客户端设置下的实现效果，包括图像分类和物体检测任务。此外，研究人员还提出了一个新的 bounds 来评估联边学习的通用差异关系，并证明了这个 bounds 的有效性。<details>
<summary>Abstract</summary>
Federated learning is a privacy-preserving training method which consists of training from a plurality of clients but without sharing their confidential data. However, previous work on federated learning do not explore suitable neural network architectures for clients with different input images sizes and different numbers of output categories. In this paper, we propose an effective federated learning method named ScalableFL, where the depths and widths of the local models for each client are adjusted according to the clients' input image size and the numbers of output categories. In addition, we provide a new bound for the generalization gap of federated learning. In particular, this bound helps to explain the effectiveness of our scalable neural network approach. We demonstrate the effectiveness of ScalableFL in several heterogeneous client settings for both image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种隐私保护的训练方法，它通过多个客户端进行训练，但没有共享客户端的敏感数据。然而，先前的联邦学习工作没有探讨适合客户端不同输入图像大小和输出类别数量的适合的神经网络架构。在这篇论文中，我们提出了一种高效的联邦学习方法，名为可扩展FL（ScalableFL），其中客户端的本地模型的深度和宽度会根据客户端的输入图像大小和输出类别数量进行调整。此外，我们还提供了一个新的泛化差界，这个泛化差界可以帮助解释我们的可扩展神经网络方法的效果。我们在多个不同客户端设置下进行了多个图像分类和对象检测任务的实践，以证明可扩展FL 的效果。
</details></li>
</ul>
<hr>
<h2 id="CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding"><a href="#CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding" class="headerlink" title="CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding"></a>CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08673">http://arxiv.org/abs/2311.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Yimin Deng, Ziqi Liang, Xulong Zhang, Ning Cheng, Jing Xiao</li>
<li>for: 该研究提出了一种名为“CP-EB”的 talking face生成方法，用于生成一个基于短视频剪辑的人脸对话视频，其中人脸的头部pose和眼睛跳动都是控制的。</li>
<li>methods: 该方法使用了一种基于GAN的建模方法，通过提取音频输入和参考视频中的眼睛跳动特征，并将其embed在人脸的标识和 pose特征中，以生成真实的人脸对话视频。</li>
<li>results: 实验结果表明，该方法可以生成出真实的人脸对话视频，其中人脸的头部pose和眼睛跳动都具有自然的感觉，同时 lip 动作也具有同步的特征。<details>
<summary>Abstract</summary>
This paper proposes a talking face generation method named "CP-EB" that takes an audio signal as input and a person image as reference, to synthesize a photo-realistic people talking video with head poses controlled by a short video clip and proper eye blinking embedding. It's noted that not only the head pose but also eye blinking are both important aspects for deep fake detection. The implicit control of poses by video has already achieved by the state-of-art work. According to recent research, eye blinking has weak correlation with input audio which means eye blinks extraction from audio and generation are possible. Hence, we propose a GAN-based architecture to extract eye blink feature from input audio and reference video respectively and employ contrastive training between them, then embed it into the concatenated features of identity and poses to generate talking face images. Experimental results show that the proposed method can generate photo-realistic talking face with synchronous lips motions, natural head poses and blinking eyes.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种名为“CP-EB”的人脸讲话方法，该方法接受音频信号和人像作为输入，并生成一个包含人脸动作和自然头姿的真实人脸讲话视频。研究表明，不仅头姿，而且眼睛莲花也是深度假设检测中重要的两个方面。现有的研究已经实现了视频内容的含义控制。根据最新的研究，眼睛莲花和输入音频之间存在弱相关性，因此可以从音频和参考视频中提取眼睛莲花特征，并通过对其进行对比训练，将其embed到人脸特征和头姿特征中，以生成真实人脸讲话图像。实验结果表明，提出的方法可以生成真实人脸讲话图像，并且有同步的嘴部动作、自然的头姿和眼睛莲花。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data"><a href="#Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data" class="headerlink" title="Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data"></a>Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08661">http://arxiv.org/abs/2311.08661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xu, Yili Hong, Eric P. Smith, David S. McLeod, Xinwei Deng, Laura J. Freeman</li>
<li>for: This paper aims to address the challenge of species complexes in biological systematics and taxonomy, specifically by developing new tools using machine learning to classify images of frogs into known species groups and detect out-of-distribution (OOD) images.</li>
<li>methods: The paper uses deep neural networks and the principles of machine learning to classify images of frogs based on a morphological character (hind limb skin texture), and to detect OOD images.</li>
<li>results: The paper demonstrates that the algorithm can successfully automate the classification of an image into a known species group for which it has been trained, and can also classify an image into a new class if the image does not belong to the existing classes. The paper also tests the performance of the OOD detection algorithm on a larger dataset.<details>
<summary>Abstract</summary>
As is true of many complex tasks, the work of discovering, describing, and understanding the diversity of life on Earth (viz., biological systematics and taxonomy) requires many tools. Some of this work can be accomplished as it has been done in the past, but some aspects present us with challenges which traditional knowledge and tools cannot adequately resolve. One such challenge is presented by species complexes in which the morphological similarities among the group members make it difficult to reliably identify known species and detect new ones. We address this challenge by developing new tools using the principles of machine learning to resolve two specific questions related to species complexes. The first question is formulated as a classification problem in statistics and machine learning and the second question is an out-of-distribution (OOD) detection problem. We apply these tools to a species complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex) and employ a morphological character (hind limb skin texture) traditionally treated qualitatively in a quantitative and objective manner. We demonstrate that deep neural networks can successfully automate the classification of an image into a known species group for which it has been trained. We further demonstrate that the algorithm can successfully classify an image into a new class if the image does not belong to the existing classes. Additionally, we use the larger MNIST dataset to test the performance of our OOD detection algorithm. We finish our paper with some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity. This paper has online supplementary materials.
</details>
<details>
<summary>摘要</summary>
如同许多复杂任务一样，发现、描述和理解地球上生物多样性（即生物系统матиcs和taxonomy）需要许多工具。一些这种工作可以通过过去的方式完成，但一些方面对传统知识和工具来说是挑战。一个这样的挑战是由生物多样性中的物种复合群产生，其中种群成员之间的形态相似性使得可靠地识别已知种和检测新种困难。我们解决这个挑战 by developing new tools using机器学习的原则。我们对一个来自东南亚短腿蛙（Limnonectes kuhlii complex）种群进行了应用。我们使用了一个形态特征（背部皮肤文化），通常被视为 качеitative的，并将其变换为一个量化和客观的方式。我们表明了深度神经网络可以成功地自动将图像分类到已知种群中。我们进一步表明了算法可以成功地分类图像到一个新的类别，如果图像不属于现有的类别。此外，我们使用了更大的MNIST数据集来测试我们的OOD检测算法。我们结束这篇论文 avec some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity。这篇论文有在线补充材料。
</details></li>
</ul>
<hr>
<h2 id="ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars"><a href="#ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars" class="headerlink" title="ConeQuest: A Benchmark for Cone Segmentation on Mars"></a>ConeQuest: A Benchmark for Cone Segmentation on Mars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08657">http://arxiv.org/abs/2311.08657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerner-lab/conequest">https://github.com/kerner-lab/conequest</a></li>
<li>paper_authors: Mirali Purohit, Jacob Adler, Hannah Kerner</li>
<li>for: 这个论文是为了提供一个 Mars 上的突起 cone 的专家标注数据集，以便用于计算机视觉模型的训练和测试。</li>
<li>methods: 这个论文使用了 Mars 上不同区域的卫星和探测器提供的卫星图像，并对其进行了专家标注。同时，它还使用了现有的计算机视觉模型，并对其进行了质量评估。</li>
<li>results: 这个论文的结果表明，目前存在的计算机视觉模型无法准确地识别和分割 Mars 上的突起 cone。模型在受到的数据上的平均 IoU 为 52.52% 和 42.55%。这个新的专家标注数据集将有助于开发更加准确和可靠的计算机视觉模型。<details>
<summary>Abstract</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:年来，宇宙科学家们从卫星和探测器中收集了大量的火星数据。一个重要的特征是穹顶坑，被解释为沉积在水（如湖或海）中的泥火山。全球火星上的穹顶坑识别是非常重要，但专家地质学家无法检索庞大的卫星图像库，以确定所有示例。这个任务非常适合计算机视觉。虽然有多个计算机视觉数据集用于火星相关任务，但目前没有公共的数据集用于穹顶坑检测/分割。此外，前一些研究使用单一地区的数据进行训练，限制其全球检测和地图应用。为了解决这个问题，我们引入 ConeQuest，火星上穹顶坑的第一个专家标注公共数据集。 ConeQuest 包含 >13k 样本从三个不同的火星地区。我们提出了两个 benchmark 任务使用 ConeQuest：（i）空间总体化和（ii）坑口大小总体化。我们在两个 benchmark 任务上精细化和评估了广泛使用的分割模型。结果表明，穹顶坑分割是计算机视觉中的一个挑战性问题，现有的分割模型在具有数据集的情况下，取得了平均 IoU 为 52.52% 和 42.55%。我们认为这个新的 benchmark 数据集将促进更加准确和可靠的模型的发展。数据和代码可以在 https://github.com/kerner-lab/ConeQuest 上获取。
</details></li>
</ul>
<hr>
<h2 id="Review-of-AlexNet-for-Medical-Image-Classification"><a href="#Review-of-AlexNet-for-Medical-Image-Classification" class="headerlink" title="Review of AlexNet for Medical Image Classification"></a>Review of AlexNet for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08655">http://arxiv.org/abs/2311.08655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Wenhao Tang, Junding Sun, Shuihua Wang, Yudong Zhang</li>
<li>for: 这个论文主要用于介绍AlexNet对于医疗影像分类的应用和发展.</li>
<li>methods: 这个论文使用了dropout技术来减少适束错误，并且使用ReLU启动函数来避免梯度消失.</li>
<li>results: 这个论文发现AlexNet在2012年于医疗影像分类 tasks 中表现出色，并且在训练时使用了40多篇论文，包括期刊论文和会议论文.<details>
<summary>Abstract</summary>
In recent years, the rapid development of deep learning has led to a wide range of applications in the field of medical image classification. The variants of neural network models with ever-increasing performance share some commonalities: to try to mitigate overfitting, improve generalization, avoid gradient vanishing and exploding, etc. AlexNet first utilizes the dropout technique to mitigate overfitting and the ReLU activation function to avoid gradient vanishing. Therefore, we focus our discussion on AlexNet, which has contributed greatly to the development of CNNs in 2012. After reviewing over 40 papers, including journal papers and conference papers, we give a narrative on the technical details, advantages, and application areas of AlexNet.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习的快速发展导致医疗图像分类领域中的广泛应用。这些变种神经网络模型具有不断提高表现的共同特点：减少过拟合、提高通用性、避免梯度消失和爆炸等。AlexNet首先采用了dropout技术来减少过拟合，并使用ReLU活化函数来避免梯度消失。因此，我们在讨论AlexNet的时候会更加着重于其技术细节、优势和应用领域。经过阅读40余篇论文，包括期刊论文和会议论文，我们将为AlexNet提供一篇narative。
</details></li>
</ul>
<hr>
<h2 id="Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing"><a href="#Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing" class="headerlink" title="Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing"></a>Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08652">http://arxiv.org/abs/2311.08652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangge Li, Benjamin C Yang, Yixuan Jia, Daniel Zhuang, Sayan Mitra</li>
<li>For: The paper is written for evaluating the safety of control systems that use machine learning for perception, and providing a method for proving end-to-end system-level safety requirements.* Methods: The paper uses perception contracts, which are specifications for testing the machine learning components, and an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC).* Results: The paper provides testable contracts that establish the state and environment conditions under which an aircraft can safely touchdown on the runway and a drone can safely pass through a sequence of gates, and also discovers conditions that can possibly violate the safety of the vision-based control system.<details>
<summary>Abstract</summary>
Perception contracts provide a method for evaluating safety of control systems that use machine learning for perception. A perception contract is a specification for testing the ML components, and it gives a method for proving end-to-end system-level safety requirements. The feasibility of contract-based testing and assurance was established earlier in the context of straight lane keeping: a 3-dimensional system with relatively simple dynamics. This paper presents the analysis of two 6 and 12-dimensional flight control systems that use multi-stage, heterogeneous, ML-enabled perception. The paper advances methodology by introducing an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC). The resulting analysis provides testable contracts which establish the state and environment conditions under which an aircraft can safety touchdown on the runway and a drone can safely pass through a sequence of gates. It can also discover conditions (e.g., low-horizon sun) that can possibly violate the safety of the vision-based control system.
</details>
<details>
<summary>摘要</summary>
感知合约提供了评估机器学习控制系统安全性的方法。感知合约是测试ML组件的规范，它提供了系统级别的安全需求证明方法。在旁卷道保持的情况下，曾经证明了合约基于测试和保证的可行性。本文分析了使用多Stage、不同类型、ML启用的飞行控制系统，并 introduce了一种构建数据和需求导向的改进感知合约算法（DaRePC）。该分析提供了可测试的合约，以确定飞机在跑道上安全Touchdown和无人机通过序列门的安全性。它还可以发现可能违反视口控制系统安全的条件（例如低天际日）。
</details></li>
</ul>
<hr>
<h2 id="Painterly-Image-Harmonization-via-Adversarial-Residual-Learning"><a href="#Painterly-Image-Harmonization-via-Adversarial-Residual-Learning" class="headerlink" title="Painterly Image Harmonization via Adversarial Residual Learning"></a>Painterly Image Harmonization via Adversarial Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08646">http://arxiv.org/abs/2311.08646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang</li>
<li>for: 将插入背景图像中的前景对象融合成一个自然和丰富的图像</li>
<li>methods: 使用对抗学习将前景特征图和背景特征图的领域差异bridge</li>
<li>results: 实现更自然和艺术化的混合效果，比前方法更加美观和有趣<details>
<summary>Abstract</summary>
Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods.
</details>
<details>
<summary>摘要</summary>
图像组合在图像编辑中扮演着重要的角色。在插入一个背景图像中的前景对象后， composite image可能会看起来不自然和不协调。当前景对象是真实图像，而背景是一幅艺术油画时， painterly image harmonization的目标是将背景油画的风格传递给前景对象，这是一项具有很大领域差距的任务。在这种情况下，我们使用对抗学习来跨领域减小这个差距。我们设计了一个双encoder生成器，其中剩余编码器生成了对前景编码器的剩余特征。然后，一个像素精度分子检测器与生成器进行对抗，以便鼓励生成的涂抹前景特征图像与背景特征图像无法分辨。广泛的实验表明，我们的方法可以实现更协调、更美观的结果，比前一些方法更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CV_2023_11_15/" data-id="clp53jwqy00moyp88hk3s5i58" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.AI_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T12:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.AI_2023_11_15/">cs.AI - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HAL-9000-Skynet’s-Risk-Manager"><a href="#HAL-9000-Skynet’s-Risk-Manager" class="headerlink" title="HAL 9000: Skynet’s Risk Manager"></a>HAL 9000: Skynet’s Risk Manager</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09449">http://arxiv.org/abs/2311.09449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martins<br>for:这种论文是为了提出一种基于现代技术的攻击快照系统（ITS）体系，以提高ITS的入侵忍受能力和适应新敌人。methods:该论文使用了机器学习（ML）算法来帮助ITS学习从以往攻击和已知漏洞中，以增强其入侵忍受能力。它还提出了一种基于现代技术的风险管理器设计，通过自动评估操作系统（OS）的风险，提供更安全的配置建议。results:实验表明，使用Skynet和HAL 9000设计可以降低成功入侵的可能性，并且HAL可以选择15%更安全的配置，比现有的风险管理器更高效。<details>
<summary>Abstract</summary>
Intrusion Tolerant Systems (ITSs) are a necessary component for cyber-services/infrastructures. Additionally, as cyberattacks follow a multi-domain attack surface, a similar defensive approach should be applied, namely, the use of an evolving multi-disciplinary solution that combines ITS, cybersecurity and Artificial Intelligence (AI). With the increased popularity of AI solutions, due to Big Data use-case scenarios and decision support and automation scenarios, new opportunities to apply Machine Learning (ML) algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS can augment its intrusion tolerance capability, by learning from previous attacks and from known vulnerabilities. As such, this work's contribution is twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and incorporates new components to increase its intrusion tolerance capability and its adaptability to new adversaries; (2) an improved Risk Manager design that leverages AI to improve ITSs by automatically assessing OS risks to intrusions, and advise with safer configurations. One of the reasons that intrusions are successful is due to bad configurations or slow adaptability to new threats. This can be caused by the dependency that systems have for human intervention. One of the characteristics in Skynet and HAL 9000 design is the removal of human intervention. Being fully automatized lowers the chance of successful intrusions caused by human error. Our experiments using Skynet, shows that HAL is able to choose 15% safer configurations than the state-of-the-art risk manager.
</details>
<details>
<summary>摘要</summary>
干扰忍耐系统（ITS）是现代网络服务/基础设施中必不可少的一部分。此外，由于攻击者通常会利用多个领域的攻击 superficie，因此应用相应的防御策略，即结合ITS、网络安全和人工智能（AI）的演进式多学科解决方案。随着人工智能解决方案的普及，尤其是在基于大数据的应用场景和决策支持自动化场景，新的机会出现了，例如ITS强化。通过机器学习（ML）算法，ITS可以增强其忍耐攻击能力，通过学习前一些攻击和已知漏洞。本工作的贡献有两个方面：1. 基于当前最佳实践的ITS架构（Skynet），新增了增强忍耐能力和适应新敌人的组件。2. 改进的风险管理设计，通过人工智能自动评估操作系统的风险，提供更安全的配置。一些攻击成功的原因之一是因为系统的坏配置或者对新威胁的缓慢适应。这可能是因为系统依赖于人类干预的问题。Skynet和HAL 9000的设计中 eliminiates human intervention，它们是完全自动化的，降低了成功攻击的可能性。我们的实验表明，HAL可以选择15%更安全的配置，比对 estado-of-the-art 风险管理器更高。
</details></li>
</ul>
<hr>
<h2 id="How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities"><a href="#How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities" class="headerlink" title="How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"></a>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09447">http://arxiv.org/abs/2311.09447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</li>
<li>for: 本研究旨在评估开源大语言模型（LLMs）的可靠性，检测其在8个方面，包括恶意、偏见、伦理、幻觉、公平、奴役、隐私和对抗示范攻击的可靠性。</li>
<li>methods: 我们提出了一种基于Chain of Utterances（CoU）的提示策略，通过针对性地制作恶意示范来检测模型的可靠性。我们对当今代表性的开源LLMs进行了广泛的实验，包括Vicuna、MPT、Falcon、Mistral和Llama 2。</li>
<li>results: 我们的实验结果表明，我们的攻击策略在多个方面具有效果，而且模型的性能在普通NLP任务上高不一定意味着它们具有更高的可靠性。此外，我们发现，受过 instrucion tuning 的模型更容易受到攻击，而 fine-tuning LLMs for safety alignment 可以减轻对抗式可靠性攻击的影响。<details>
<summary>Abstract</summary>
The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose an enhanced Chain of Utterances-based (CoU) prompting strategy by incorporating meticulously crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.
</details>
<details>
<summary>摘要</summary>
大量开源大语言模型（LLM）的快速进步正在推动人工智能的发展。然而，对这些模型的可靠性仍然具有有限的理解。在大规模部署时，如果不充分了解这些模型的可靠性，可能会产生严重的风险。在这项工作中，我们对开源LLM的可靠性进行了评估，对其进行了八个不同方面的检查，包括恶意语言、刻板印象、伦理、幻觉、公平、奴役、隐私和对抗示范的Robustness。我们提出了一种增强的链接语言模型（CoU）提示策略，通过包括特制的邪恶示范来攻击可靠性。我们的广泛实验涵盖了最新和代表性的开源LLM，包括Vicuna、MPT、Falcon、Mistral和Llama 2。实验结果表明我们的攻击策略在多个方面具有效果。更有趣的是，我们的结果分析发现，在通用NLP任务中表现出色的模型并不总是具有最高的可靠性，反之，大型模型可能更容易受到攻击。此外，通过专门准备Instruction Tuning，强调实现指令，模型更容易受到攻击，但通过安全对齐的精细调整LLM可以 Mitigate adversarial trustworthiness attacks。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning"><a href="#Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning" class="headerlink" title="Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning"></a>Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09441">http://arxiv.org/abs/2311.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor<br>for:This paper focuses on Split Federated Learning (SFL) and its impact on energy consumption and privacy.methods:The paper analyzes the influence of system parameters on the selection of the cut layer in SFL and provides an illustrative example of cut layer selection to minimize the risk of clients reconstructing raw data while sustaining energy consumption within a required budget.results:The paper discusses the challenges of cut layer selection in SFL and provides a comprehensive overview of the SFL process, taking into account the impact of various system parameters on energy consumption and privacy. Additionally, the paper addresses open challenges in this field and identifies promising avenues for future research and development, particularly in the context of 6G technology.<details>
<summary>Abstract</summary>
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and privacy. This analysis takes into account the influence of various system parameters on the cut layer selection strategy. Additionally, we provide an illustrative example of the cut layer selection, aiming to minimize the risk of clients from reconstructing the raw data at the server while sustaining energy consumption within the required energy budget, which involve trade-offs. Finally, we address open challenges in this field including their applications to 6G technology. These directions represent promising avenues for future research and development.
</details>
<details>
<summary>摘要</summary>
分布式学习（SFL）最近出现为一种有前途的分布式学习技术，充分利用联合学习和分布式学习的优势。它强调快速收敛的优点，同时解决隐私问题。因此，这一创新在学术和业界都得到了广泛关注。然而，由于SFL中的模型被分割到特定层，称为“分割层”，因此选择分割层的选择对客户端的能耗和隐私具有重要的影响。此外，确定分割层的设计挑战非常复杂，主要是因为客户端的计算和网络能力具有内在的不同性。在这篇文章中，我们提供了SFL过程的完整概述，并进行了严格的能耗和隐私分析。这种分析考虑了各种系统参数对分割层选择策略的影响。此外，我们还提供了一个示例，旨在在server端 reconstruction raw data的风险下，保持客户端的能耗在所需的能耗预算内。这种做法涉及到了负担和让步。最后，我们讨论了现有的挑战，包括它们在6G技术中的应用。这些方向表示未来研究和发展的优秀方向。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment"><a href="#Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment" class="headerlink" title="Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"></a>Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09433">http://arxiv.org/abs/2311.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 这 paper 是为了研究 instruction-tuned Large Language Models (LLMs) 的安全性，具体来说是研究这些模型在不同的安全任务上的可控性。</li>
<li>methods: 这 paper 使用了一种新的攻击框架，叫做 Backdoor Activation Attack，它可以在 LLMs 的活动层中插入恶意导向 вектор。</li>
<li>results: 实验结果表明，该方法可以高效地启动攻击，并且增加了非常小的负担。此外， paper 还讨论了对这种活动攻击的可能的防御措施。<details>
<summary>Abstract</summary>
To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we introduce a novel attack framework, called Backdoor Activation Attack, which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. In particular, the steering vectors are generated by taking the difference between benign and malicious activations. Then, the most effective steering vector is selected and added to the forward passes of the LLMs. Our experiment results on four primary alignment tasks show that our proposed method is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. Our code and data are available at https://email-haoran-for-link. Warning: this paper contains content that can be offensive or upsetting.
</details>
<details>
<summary>摘要</summary>
为确保人工智能安全，特定的大语言模型（LLM）被专门训练，以确保它们与人类意图相对应。尽管这些模型在不同的安全测试中表现出色，但它们的安全相对性尚未得到广泛的研究。这特别是在考虑到这些模型可能对人类造成的潜在危害。现有的攻击方法通常利用恶意训练数据或植入恶意提示。这些方法会使攻击变得不具隐蔽性和普遍性，使其易受到检测。此外，这些模型通常需要大量的计算资源来实现，使其在实际应用中不太实用。在这种情况下，我们介绍了一种新的攻击框架，called Backdoor Activation Attack，它在LLMs中注入恶意导航向量。这些恶意导航向量可以在推理时被触发，以控制模型的活动。具体来说，这些导航向量是通过对善和恶的活动进行差异来生成的。然后，选择最有效的导航向量，并将其添加到LLMs的前向传递中。我们的实验结果显示，我们的提议方法具有非常高的效果，并且增加了非常少的负担。此外，我们还讨论了对这种活动攻击的可能的防御措施。我们的代码和数据可以在https://email-haoran-for-link获取。请注意，这篇论文可能包含有害或使人不安的内容。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models"><a href="#Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models" class="headerlink" title="Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models"></a>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09428">http://arxiv.org/abs/2311.09428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu</li>
<li>for: 本研究探讨了针对恶意语言检测模型的不公正性和检测性能的攻击性能，以提高模型的公正性稳定性。</li>
<li>methods: 本研究提出了一个简单 yet effective的框架 FABLE，通过利用后门攻击来实现对公正性和检测性能的Targeted控制。 FABLE 探讨了三种触发设计（i.e., 罕见、人工和自然触发）以及新的采样策略。</li>
<li>results: 实验结果表明，FABLE 可以成功地攻击恶意语言检测模型的公正性和实用性。<details>
<summary>Abstract</summary>
This work investigates the potential of undermining both fairness and detection performance in abusive language detection. In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance. FABLE explores three types of trigger designs (i.e., rare, artificial, and natural triggers) and novel sampling strategies. Specifically, the adversary can inject triggers into samples in the minority group with the favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the effectiveness of FABLE attacking fairness and utility in abusive language detection.
</details>
<details>
<summary>摘要</summary>
Note:* "abusive language detection" is translated as "恶意语言检测" (èví yì yǔ yì bǐng kě yì)* "fairness" is translated as "公正" (gōng zhèng)* "detection performance" is translated as "检测性能" (jiān zhèng xìng néng)* "backdoor attacks" is translated as "后门攻击" (hòu mén gōng jī)* "triggers" is translated as "触发器" (chù fā qì)* "samples" is translated as "样本" (yàng běn)* "minority group" is translated as "少数群体" (shǎo shù qún tǐ)* "desired outcome" is translated as "期望结果" (qī wàng jié wù)* "unfavored outcome" is translated as "不利结果" (bù lì jié wù)
</details></li>
</ul>
<hr>
<h2 id="When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour"><a href="#When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour" class="headerlink" title="When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour"></a>When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09410">http://arxiv.org/abs/2311.09410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Ranaldi, Giulia Pucci</li>
<li>for: 这篇论文探讨了大语言模型（LLMs）在解决复杂任务时的可能性，以及人类反馈对其回答的影响。</li>
<li>methods: 该论文使用了不同任务的人类影响提示，以探讨 LLMS 是否受到 sycophancy 行为的影响。</li>
<li>results: 研究发现，当 LLMS 回答Subjective 意见和基于事实应该提供相反回答的问题时，它们往往表现出 sycophancy 倾向，表明它们缺乏坚实性和可靠性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.   In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts, demonstrating a lack of robustness.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在完成复杂任务时表现出能力，其中一部分归功于人类反馈的敏感使用，这种反馈使得答案得到了人类评价的良好评价。然而，人类反馈传递的建议会使模型生成受用户信念或误导性提示的答案，而不是基于实际事实的答案，这种现象被称为奉承行为。这种行为会减少模型的偏见、可靠性和可靠性。在这篇论文中，我们探讨了 LLM 对奉承行为的渗透性，通过不同任务中人类影响的提问来示出这种倾向。我们的调查发现， LLM 在表达主观意见和基于事实应该具有相反回应的问题上具有奉承倾向，这表明了模型的不稳定性。
</details></li>
</ul>
<hr>
<h2 id="LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction"><a href="#LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction" class="headerlink" title="LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction"></a>LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09366">http://arxiv.org/abs/2311.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie McCusker</li>
<li>for: 本研究旨在探讨使用大语言模型和提问工程来构建知识图，以解决开放信息提取（OIE）中的缺乏问题。</li>
<li>methods: 本研究使用GPT模型和提问工程来实现知识图构建，并使用CaRB benchmark scoringapproach和TekGen dataset来评估模型性能。</li>
<li>results: 研究发现，一个well工程的提问，配合一种简单的实体链接方法（LOKE-GPT），可以超过AllenAI的OpenIE 4实现，但是它会因为总体 triple 缺乏而过分生成 triple。分析Entity Linkability在CaRB dataset和OpenIE 4和LOKE-GPT输出中，表明LOKE-GPT和”银” TekGen triple 显示任务的内容和结构都与OIE有很大差异。<details>
<summary>Abstract</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details>
<details>
<summary>摘要</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details></li>
</ul>
<hr>
<h2 id="Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science"><a href="#Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science" class="headerlink" title="Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science"></a>Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09358">http://arxiv.org/abs/2311.09358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Sridevi Wagle, Sai Munikoti, Anurag Acharya, Sara Smith, Sameera Horawalavithana<br>for: This study aims to evaluate the uncertainty quantification (UQ) in Retrieval Augmented Language Models (RALMs) for scientific tasks, and to explore the relationship between uncertainty scores and the accuracy of model-generated outputs.methods: The study uses an existing RALM and finetunes it with scientific knowledge as the retrieval data, and evaluates the uncertainty scores and accuracy of the model-generated outputs.results: The study finds that the RALM is overconfident in its predictions, making inaccurate predictions more confidently than accurate ones. Additionally, the study finds that scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue.Here’s the simplified Chinese text:for: 这项研究目标是评估 Retrieval Augmented Language Models (RALMs) 在科学任务中的不确定性评估 (UQ)，并探索不确定性分数和模型生成输出的准确性之间的关系。methods: 该研究使用现有的 RALM，并将科学知识作为检索数据进行训练，以评估不确定性分数和模型生成输出的准确性。results: 研究发现，RALM 对预测结果过于自信，做出了更多的错误预测。此外，研究发现，向 RALM 提供科学知识作为预训练或检索数据，并不能解决这个问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出了惊人的成绩，生成出高质量的输出。然而，LLM仍然存在一些局限性，包括生成错误的信息。在安全关键应用中，评估LLM生成的内容的可靠性非常重要，以便做出了 Informed 决策。在NLP领域中，Retrieval Augmented Language Models（RALMs）是一个相对新的研究领域。RALMs在科学NLP任务中提供了潜在的优势，因为检索到的文档可以用来支持模型生成的内容，从而提高可靠性。另外，量化RALM生成的不确定性进一步提高了可靠性，因为用户可以通过检索到的文档和信任分数来验证和探索模型输出的可靠性。然而，对RALM的 uncertainty quantification（UQ）的研究尚存在很大的空白，特别是在科学上下文中。本研究的目的是填补这个空白，通过对RALM的UQ进行全面的评估，特别是在科学任务中。本研究研究了 RALM 在科学任务中 uncertainty 分布是如何变化，以及 RALM 生成输出的准确性和不确定性之间的关系。我们发现，通过将科学知识作为预训练和检索数据来训练 RALM ，该模型在生成预测时更加自信。此外，我们发现 RALM 存在过于自信的问题，即它会更自信地预测错误的内容。科学知识作为预训练或检索数据不能够解决这个问题。我们将我们的代码、数据和仪表Release在 GitHub 上，可以通过 https://github.com/pnnl/EXPERT2 访问。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Threats-in-Stable-Diffusion-Models"><a href="#Privacy-Threats-in-Stable-Diffusion-Models" class="headerlink" title="Privacy Threats in Stable Diffusion Models"></a>Privacy Threats in Stable Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Cilloni, Charles Fleming, Charles Walter</li>
<li>for: 本研究探讨了一种基于稳定扩散模型的会员推理攻击（MIA），具体targeting Stable Diffusion V2 by StabilityAI，以探讨这种模型在训练数据中的隐私问题。</li>
<li>methods: 我们采用了一种黑盒子MIA方法，通过 repeatedly querying the victim model来提取模型的训练数据信息。我们首先观察了稳定扩散模型在不同生成epochs中的输出，然后训练一个分类模型来 distinguish whether a series of intermediates originated from a training sample or not。我们还提出了多种测试会员特征的方法，并讨论了哪些方法最有效。</li>
<li>results: 我们通过ROC AUC方法评估了攻击的有效性，得到了60%的成功率，即可以准确地推理出训练样本的信息。本研究的成果增加了隐私和安全在机器学习领域的研究，提醒了实践者和开发者需要对这类攻击采取更加强大的安全措施，以保护模型的隐私。<details>
<summary>Abstract</summary>
This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details>
<details>
<summary>摘要</summary>
Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish whether a series of intermediates originated from a training sample or not. We explore various methods to measure membership features and discuss their effectiveness. The attack's success is evaluated using the ROC AUC method, achieving a 60% success rate in inferring membership information.This research contributes to the growing body of knowledge on privacy and security in machine learning, emphasizing the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to protect against such attacks.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Imitation-Learning-Through-Pre-Trained-Representations"><a href="#Generalizable-Imitation-Learning-Through-Pre-Trained-Representations" class="headerlink" title="Generalizable Imitation Learning Through Pre-Trained Representations"></a>Generalizable Imitation Learning Through Pre-Trained Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09350">http://arxiv.org/abs/2311.09350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Di Chang, Francois Hogan, David Meger, Gregory Dudek</li>
<li>for: 提高imitATION learning policies的普适能力</li>
<li>methods: 利用自我supervised vision transformer模型和其自然语言能力来提高imitATION learning policies的普适能力</li>
<li>results: 通过 clustering appearance features into semantic concepts，实现了一致的行为Generalization across a wide range of appearance variations and object types.Here’s the English version for reference:</li>
<li>for: Improving the generalization abilities of imitation learning policies</li>
<li>methods: Leveraging self-supervised vision transformer models and their emergent semantic abilities</li>
<li>results: Achieving consistent behavior generalization across a diverse dataset of object manipulation tasks by clustering appearance features into semantic concepts.<details>
<summary>Abstract</summary>
In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用自动标注视transformer模型和其自然出现的semantic能力来提高依ictionary学习策略的通用能力。我们介绍了BC-ViT算法，它利用了丰富的DINO预训练视transformer（ViT）质量块嵌入来获得更好的通用性，当学习通过示例时。我们的学习者通过对外观特征的聚类，形成稳定的关键点，来看到世界。我们显示这种表示能够实现通用行为，通过对物品执行多种抓取任务中的多样化数据集进行评估。我们的方法、数据和评估方法都是为了促进更多的通用性在依ictionary学习中进行进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models"><a href="#Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models" class="headerlink" title="Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models"></a>Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09349">http://arxiv.org/abs/2311.09349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 这 paper 旨在应用生成AI技术于通信系统中的PHY设计，具体是用于 quadrature amplitude modulation (QAM) 的 constellation 图形设计。</li>
<li>methods: 这 paper 使用了 denoising diffusion probabilistic models (DDPM) 来实现 probabilistic constellation shaping，通过“杜化”和生成方式，使 transmitter 发送的 constellation 图形与 receiver 恢复的图形更加相似，从而提高信息率和解码性能。</li>
<li>results:  compared to deep neural network (DNN)  benchmark和均匀排序法，生成AI技术所提出的方案在low-SNR 下表现出30%的提高，并且在非泊尔分布假设下保持网络稳定性和Robust out-of-distribution 性能。数值评估表明，对于 64-QAM 几何，生成AI技术所提出的方案可以提高 cosine similarity 的表现，并且与 DNN 方案相比，提高了3倍的习information。<details>
<summary>Abstract</summary>
Diffusion models are at the vanguard of generative AI research with renowned solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI. Nevertheless, the potential merits of diffusion models for communication engineering applications are not fully understood yet. In this paper, we aim to unleash the power of generative AI for PHY design of constellation symbols in communication systems. Although the geometry of constellations is predetermined according to networking standards, e.g., quadrature amplitude modulation (QAM), probabilistic shaping can design the probability of occurrence (generation) of constellation symbols. This can help improve the information rate and decoding performance of communication systems. We exploit the ``denoise-and-generate'' characteristics of denoising diffusion probabilistic models (DDPM) for probabilistic constellation shaping. The key idea is to learn generating constellation symbols out of noise, ``mimicking'' the way the receiver performs symbol reconstruction. This way, we make the constellation symbols sent by the transmitter, and what is inferred (reconstructed) at the receiver become as similar as possible, resulting in as few mismatches as possible. Our results show that the generative AI-based scheme outperforms deep neural network (DNN)-based benchmark and uniform shaping, while providing network resilience as well as robust out-of-distribution performance under low-SNR regimes and non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in terms of cosine similarity and a threefold improvement in terms of mutual information compared to DNN-based approach for 64-QAM geometry.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是现代生成AI研究的先锋之一，如Google Brain的ImageGen和OpenAI的DALL.E 3。然而，Diffusion models 在通信工程应用中的潜在优势仍未得到充分了解。在这篇论文中，我们想要使用生成AI来设计物理设计的恒星符号在通信系统中。尽管恒星符号的几何结构按照网络标准（如 quadrature amplitude modulation，QAM）固定，但可以使用 probabilistic shaping 设计恒星符号的出现概率。这可以帮助提高通信系统的信息率和解码性能。我们利用 denoising diffusion probabilistic models（DDPM）的“净化并生成”特点来实现 probabilistic constellation shaping。我们的思想是通过学习生成恒星符号 из噪声，“模仿”接收器在重建符号时的方式，以便使得发送器发送的恒星符号和接收器重建的符号变得越来越相似，从而减少差异。我们的结果表明，基于生成AI的方案在低SNR条件下和非泊然假设下表现出了较好的网络稳定性和robust性，并且在64-QAM几何上实现了30%的cosine similarity提升和三倍的相互信息提升 compared to DNN-based approach。数字评估表明，在64-QAM几何上，基于生成AI的方案可以提供30%的cosine similarity提升和三倍的相互信息提升 compared to DNN-based approach。
</details></li>
</ul>
<hr>
<h2 id="Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization"><a href="#Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization" class="headerlink" title="Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization"></a>Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09335">http://arxiv.org/abs/2311.09335</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在摘要生成中的幻觉问题，尤其是在使用剪辑技术时。</li>
<li>methods: 本研究采用了三种标准摘要任务、两种剪辑方法和三种指导模型进行实验研究。</li>
<li>results: 研究发现，剪辑后的LLM模型比全大小模型更少地产生幻觉，并且这种幻觉减少的原因可能是剪辑后模型更加依赖于输入源文本，而不是自己的参数知识。<details>
<summary>Abstract</summary>
Despite their remarkable performance on abstractive summarization, large language models (LLMs) face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode the reliability of LLMs and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights to create sparse models that enable more efficient inference. Pruned models yield comparable performance to their counterpart full-sized models, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study on the hallucinations produced by pruned models across three standard summarization tasks, two pruning approaches, three instruction-tuned LLMs, and three hallucination evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less compared to their full-sized counterparts. Our follow-up analysis suggests that pruned models tend to depend more on the source input and less on their parametric knowledge from pre-training for generation. This greater dependency on the source input leads to a higher lexical overlap between generated content and the source input, which can be a reason for the reduction in hallucinations.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）在抽象概要 SUMMARIZATION 方面表现出色，但它们面临两个主要挑战：它们的巨大大小和幻觉。幻觉会使 LLM 的可靠性受到损害，并提高安全问题。剪辑是一种技术，可以减少模型的大小，并创建更加高效的推理。剪辑后的模型可以保持与全大小模型相同的性能，从而在有限预算下提供可行的选择。然而，剪辑对抽象 SUMMARIZATION 中 LLM 的幻觉的影响尚未得到了探讨。在这篇论文中，我们提供了对幻觉生成的广泛的实验研究，包括三个标准 SUMMARIZATION 任务、两种剪辑方法、三个受过指导的 LLM 和三个幻觉评价指标。我们发现剪辑后的 LLM 幻觉的数量比整个模型更少。我们的跟踪分析表明，剪辑后的模型更加依赖于输入源，而不是从前期启发中学习的参数知识。这种更加依赖于输入源的依赖性导致生成内容与输入源的字符串 overlap 更高，这可能是幻觉减少的原因。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production"><a href="#Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production" class="headerlink" title="Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production"></a>Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Khosravi, Sarah Farhadpour, Manikanta Grandhi, Ahmed Shoyeb Raihan, Srinjoy Das, Imtiaz Ahmed</li>
<li>for: 这篇研究旨在解决纸品生产过程中较罕见的纸断事件预测，以提高维护和生产调度。</li>
<li>methods: 本研究使用Conditional Generative Adversarial Networks (CTGAN)和Synthetic Minority Oversampling Technique (SMOTE)实现了一个新的数据增强框架，以增强预测维护模型的性能。</li>
<li>results: 使用CTGAN增强的数据，三种不同的机器学习算法（决策树、随机森林和条件LOGISTIC REGRESSION）的预测性能有所提高，特别是针对决策树和随机森林的纸断预测性能提高了30%以上，20%以上。<details>
<summary>Abstract</summary>
A significant challenge for predictive maintenance in the pulp-and-paper industry is the infrequency of paper breaks during the production process. In this article, operational data is analyzed from a paper manufacturing machine in which paper breaks are relatively rare but have a high economic impact. Utilizing a dataset comprising 18,398 instances derived from a quality assurance protocol, we address the scarcity of break events (124 cases) that pose a challenge for machine learning predictive models. With the help of Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE), we implement a novel data augmentation framework. This method ensures that the synthetic data mirrors the distribution of the real operational data but also seeks to enhance the performance metrics of predictive modeling. Before and after the data augmentation, we evaluate three different machine learning algorithms-Decision Trees (DT), Random Forest (RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our study achieved significant improvements in predictive maintenance performance metrics. The efficacy of CTGAN in addressing data scarcity was evident, with the models' detection of machine breaks (Class 1) improving by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression. With this methodological advancement, this study contributes to industrial quality control and maintenance scheduling by addressing rare event prediction in manufacturing processes.
</details>
<details>
<summary>摘要</summary>
印刷纸业中预测维护的主要挑战之一是生产过程中纸断的rarity。在这篇文章中，我们分析了一台纸制造机器上的运营数据，其中纸断相对较少但经济影响很大。利用一个包含18,398个实例的质量保证协议数据集，我们解决了缺乏纸断事件（124个案例）的挑战，这些事件对机器学习预测模型的性能造成了困难。通过使用Conditional Generative Adversarial Networks（CTGAN）和Synthetic Minority Oversampling Technique（SMOTE），我们实现了一个新的数据增强框架。这种方法确保了人工数据的增强与实际数据的分布相符，同时尝试提高预测模型的性能指标。在数据增强前和后，我们评估了三种不同的机器学习算法：决策树（DT）、Random Forest（RF）和логистиック回归（LR）。使用CTGAN增强数据集，我们的研究实现了预测维护性能指标的显著提高。CTGAN在解决数据稀缺性方面的 efficacy 是明显的，纸断检测（Class 1）的准确率提高了30%以上 для决策树，20%以上 дляRandom Forest，和90%以上 длялогистиック回归。通过这种方法ological advancement，这种研究对工业质量控制和维护时间安排做出了贡献，解决了生产过程中罕见事件预测的问题。
</details></li>
</ul>
<hr>
<h2 id="Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal"><a href="#Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal" class="headerlink" title="Improving fit to human reading times via temperature-scaled surprisal"></a>Improving fit to human reading times via temperature-scaled surprisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09325">http://arxiv.org/abs/2311.09325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Iza Škrjanec, Vera Demberg</li>
<li>for: 本研究旨在提高大语言模型（LLM） simulate 人类认知负荷的准确性，通过使用温度缩放的意料值来预测人类阅读时间。</li>
<li>methods: 本研究使用了温度缩放的意料值，作为预测人类阅读时间的predictor。同时，我们也提出了一种可信度度量，用于衡量模型是否具有人类化偏见。</li>
<li>results: 我们的结果表明，使用温度缩放的意料值可以大幅提高预测人类阅读时间的准确性，并且可以在不同的数据集和模型中实现类似的效果。<details>
<summary>Abstract</summary>
Past studies have provided broad support for that words with lower predictability (i.e., higher surprisal) require more time for comprehension by using large language models (LLMs) to simulate humans' cognitive load. In general, these studies have implicitly assumed that the probability scores from LLMs are accurate, ignoring the discrepancies between human cognition and LLMs from this standpoint. Inspired by the concept of probability calibration, we are the first work to focus on the probability distribution for human reading simulation. We propose to use temperature-scaled surprisal, a surprisal calculated by shaped probability, to be the predictor of human reading times. Our results across three corpora consistently revealed that such a surprisal can drastically improve the prediction of reading times. Setting the temperature to be approximately 2.5 across all models and datasets can yield up to an 89% of increase in delta log-likelihood in our setting. We also propose a calibration metric to quantify the possible human-likeness bias. Further analysis was done and provided insights into this phenomenon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Word2Vec-A-Perspective-And-Some-Techniques"><a href="#Spoken-Word2Vec-A-Perspective-And-Some-Techniques" class="headerlink" title="Spoken Word2Vec: A Perspective And Some Techniques"></a>Spoken Word2Vec: A Perspective And Some Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09319">http://arxiv.org/abs/2311.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Amaan Sayeed, Hanan Aldarmaki</li>
<li>for: 这个论文旨在探讨语音Vec的假设和架构，以及previous works中Word2Vec算法如何对 spoken words进行semantic embedding。</li>
<li>methods: 这个论文使用Word2Vec算法来学习语音Vec，并对 previous works中的假设和架构进行分析。</li>
<li>results: 该论文通过实验表明，Word2Vec算法在输入单元是 acoustically correlated 时无法编码分布式 semantics。此外，previous works中的假设和架构也存在一些简化，导致 trivial solution 被忽略。<details>
<summary>Abstract</summary>
Text word embeddings that encode distributional semantic features work by modeling contextual similarities of frequently occurring words. Acoustic word embeddings, on the other hand, typically encode low-level phonetic similarities. Semantic embeddings for spoken words have been previously explored using similar algorithms to Word2Vec, but the resulting vectors still mainly encoded phonetic rather than semantic features. In this paper, we examine the assumptions and architectures used in previous works and show experimentally how Word2Vec algorithms fail to encode distributional semantics when the input units are acoustically correlated. In addition, previous works relied on the simplifying assumptions of perfect word segmentation and clustering by word type. Given these conditions, a trivial solution identical to text-based embeddings has been overlooked. We follow this simpler path using automatic word type clustering and examine the effects on the resulting embeddings, highlighting the true challenges in this task.
</details>
<details>
<summary>摘要</summary>
文本Word embeddings可以储存分布性 semantic 特征，它们通过考虑 Contextual 相似性来模型常见的词语。Acoustic word embeddings 则通常储存低级声学相似性。在过去的研究中，对于说话的词语 Semantic 表示使用了类似于 Word2Vec 的算法，但 resulting vectors 仍然主要储存了声学而不是 semantic 特征。在这篇论文中，我们分析了以前的假设和架构，并通过实验表明 Word2Vec 算法在输入单元具有声学相关性时无法储存分布性 semantics。此外，以前的工作假设了完美的单词分 segmentation 和单词类型划分，这导致了一个简单的解决方案被忽略了。我们采用自动单词类型划分，并研究其影响在储存 embeddings 中，把真正的挑战 highlighted。
</details></li>
</ul>
<hr>
<h2 id="H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing"><a href="#H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing" class="headerlink" title="H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing"></a>H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09312">http://arxiv.org/abs/2311.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gian Marco Visani, William Galvin, Michael Neal Pun, Armita Nourmohammad</li>
<li>for: 用于结构蛋白质的三维结构预测，特别是用于预测蛋白质副链的包装结构。</li>
<li>methods: 使用两个轻量级的旋转对称神经网络，实现了一种新的两stage算法，即干扰包acker（H-Packer）。</li>
<li>results: 对CASP13和CASP14目标进行评估，显示了与传统物理学基本算法和深度学习解决方案相当的计算效率和性能。<details>
<summary>Abstract</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details>
<details>
<summary>摘要</summary>
正确地模型蛋白质三维结构是设计功能蛋白的重要前提。一个重要的子任务是蛋白侧链填充：根据蛋白质的脊梁结构和氨基酸序列，预测侧链（rotamer）的 conformations。传统方法对这个任务靠拢昂费的抽样程序和手工设计能量函数，以及rotamer库。近年，几种深度学习方法被开发来解决这个问题，但是它们的形式非常不同（从图像到图像翻译到直接预测原子坐标）。在这里，我们将这个问题架构为联合 regression  sobre 侧链的真实自由度：离散角chi。我们严格地研究这个问题的可能的目标函数，同时考虑到这个任务的下面对称性。我们提出了一个名为Holographic Packer（H-Packer）的新的两阶段算法，基于两个轻量级的旋转对称神经网络。我们对CASP13和CASP14标靶进行评估。H-Packer  computationally efficient 和对于传统物理基础的算法和其他深度学习解决方案相对有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Divergences-between-Language-Models-and-Human-Brains"><a href="#Divergences-between-Language-Models-and-Human-Brains" class="headerlink" title="Divergences between Language Models and Human Brains"></a>Divergences between Language Models and Human Brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09308">http://arxiv.org/abs/2311.09308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flamingozh/divergence_meg">https://github.com/flamingozh/divergence_meg</a></li>
<li>paper_authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe</li>
<li>for: 研究 whether machines and humans process language in similar ways, and explore the differences between human and machine language processing using brain data.</li>
<li>methods: 使用 Magnetoencephalography (MEG) responses to a written narrative to examine the differences between LM representations and the human brain’s responses to language, and fine-tune LMs on datasets related to emotional understanding, figurative language processing, and physical commonsense.</li>
<li>results: 发现 LMs 不好地处理情感理解、 figurative language processing 和 physical commonsense，并且通过 fine-tuning LMs 可以提高它们与人类大脑响应的对齐度。这些结果 implies that the observed divergences between LMs and human brains may stem from LMs’ inadequate representation of these specific types of knowledge.<details>
<summary>Abstract</summary>
Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. By fine-tuning LMs on datasets related to these phenomena, we observe that fine-tuned LMs show improved alignment with human brain responses across these tasks. Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。</SYS>人类和机器是否处理语言类似？一项研究表明，可以通过语言模型（LM）的内部表示来预测人脑电响，这被视为人类语言处理和LM共享计算原理的证明。然而，尽管最终任务相同，但人类和机器在语言学习和使用方面存在显著差异。尽管如此，有很少研究对人类和机器语言处理使用脑数据进行系统比较。为了解答这个问题，我们比较LM表示和人脑对语言的响应，具体来说是通过一个written narrative的MEG响应数据集来进行研究。在这个过程中，我们发现了三种现象，在先前的工作中LMs没有很好地捕捉到：情感理解、 figurative language processing和physical commonsense。通过在这些任务上进行LM的细化，我们观察到了细化LMs与人脑响应之间的改善。我们的研究表明，LMs的表示不够 captured这些专门的知识，导致观察到的差异。
</details></li>
</ul>
<hr>
<h2 id="Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models"><a href="#Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models" class="headerlink" title="Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"></a>Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09278">http://arxiv.org/abs/2311.09278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu</li>
<li>for: 这个论文的目的是提高大型自然语言模型（LLM）在自然语言（NL）相关任务中的进步，但是现有工作忽略了两个关键挑战：符号之间的关系和符号知识的平衡。</li>
<li>methods: 这篇论文从数据和框架角度面对这两个挑战，并介绍了Symbol-LLM系列模型。首先，收集了34个符号任务，覆盖了~20种不同的形式，并将它们统一 capture符号关系。然后，使用两个阶段调整框架，无损通用性能。</li>
<li>results: 广泛的实验表明Symbol-LLM系列模型在符号和NL相关任务中具有平衡和超越性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have greatly propelled the progress in natural language(NL)-centric tasks based on NL interface. However, the NL form is not enough for world knowledge. Current works focus on this question by injecting specific symbolic knowledge into LLM, which ignore two critical challenges: the interrelations between various symbols and the balance between symbolic-centric and NL-centric capabilities. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we collect 34 symbolic tasks, covering ~20 different forms, which are unified to capture symbol interrelations. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" is translated as "大语言模型" (dà yǔ yán módelǐ)* "natural language" is translated as "自然语言" (zìrán yǔ yán)* "symbolic knowledge" is translated as "符号知识" (fúhào zhīshì)* "interrelations" is translated as "关系" (guānxì)* "balance" is translated as "平衡" (píngkōng)* "symbolic-centric" is translated as "符号 centered" (fúhào centered)* "NL-centric" is translated as "自然语言 centered" (zìrán yǔ yán centered)* "capabilities" is translated as "能力" (nénglì)Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages"><a href="#Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages" class="headerlink" title="Assessing Translation capabilities of Large Language Models involving English and Indian Languages"></a>Assessing Translation capabilities of Large Language Models involving English and Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09216">http://arxiv.org/abs/2311.09216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, Dipti Misra Sharma</li>
<li>for: 这个研究的目的是探索大型自然语言模型（LLMs）在不同语言之间的机器翻译能力。</li>
<li>methods: 该研究使用了Raw Large Language Models（raw LLMs）和LoRA等 parameter efficient fine-tuning方法来进行翻译任务。</li>
<li>results: 研究结果表明，使用LLaMA-13b模型进行2 stage fine-tuning后，在英语到印度语言和印度语言到英语的翻译任务中都有显著进步，并取得了相对较高的BLEU和CHRF分数。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA.   Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在不同的自然语言处理任务中显示出惊人的进步。在这项工作中，我们的目标是探索大型语言模型的多语言能力。我们使用机器翻译作为英语和22种印度语言之间的任务，首先调查 raw 大型语言模型的翻译能力，然后探索同样的 raw 模型在Context Learning中的表现。我们使用 LoRA 和完全 fine-tuning 等参数效率的 Parameter Efficient Fine-Tuning 方法进行微调。通过我们的研究，我们发现了基于 LLaMA 的最佳大型语言模型，用于机器翻译任务。我们的结果表明了显著的进步， Raw 翻译得分为13.42、15.93、12.13、12.30和12.07，以及 CHRF 得分为43.98、46.99、42.55、42.42和45.39，分别使用2013版LLaMA进行2 stage fine-tuning。在英语到印度语言的翻译任务中，我们实现了Raw 翻译得分为14.03、16.65、16.17、15.35和12.55，以及 CHRF 得分为36.71、40.44、40.26、39.51和36.20，分别使用 fine-tuned LLaMA-13b 在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上进行微调。在印度语言到英语的翻译任务中，我们实现了Raw 翻译得分为14.03、16.65、16.17、15.35和12.55，以及 CHRF 得分为36.71、40.44、40.26、39.51和36.20，分别使用 fine-tuned LLaMA-13b 在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上进行微调。总之，我们的发现表明了大型语言模型在机器翻译任务中的潜力和优势，包括目前在LLMs中尚未得到足够的关注的语言。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey"><a href="#Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey" class="headerlink" title="Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey"></a>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09212">http://arxiv.org/abs/2311.09212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashokurlana/controllable_text_summarization_survey">https://github.com/ashokurlana/controllable_text_summarization_survey</a></li>
<li>paper_authors: Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra</li>
<li>for: 本研究的目的是探讨控制性文本摘要（CTS）任务的多方面特点和挑战，以及现有方法和数据集的评估。</li>
<li>methods: 本文首先将CtrlTS任务正式定义，然后根据共同特点和目标将CtrlTS方法分类，并对每个类别的现有方法和数据集进行了详细的评估。</li>
<li>results: 本文的研究发现了CtrlTS任务的一些限制和研究漏洞，同时也提出了未来研究的可能性和方向。<details>
<summary>Abstract</summary>
Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details>
<details>
<summary>摘要</summary>
通常的文本概要方法 often fails to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models"><a href="#Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models" class="headerlink" title="Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"></a>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09210">http://arxiv.org/abs/2311.09210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu</li>
<li>for: 提高 Retrieval-augmented language models (RALMs) 的可靠性和可靠性，使其能够更好地面对噪音、无关文档和 unknown 问题。</li>
<li>methods: 提出了一种名为 Chain-of-Noting (CoN) 的新方法，通过生成文档的顺序读取笔记来评估文档的相关性和综合评价。</li>
<li>results: 对四个开放领域问答 benchmark 进行了实验，发现 CoN 可以大幅提高 RALMs 的性能，特别是在噪音文档和 unknown 问题上。<details>
<summary>Abstract</summary>
Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with "unknown" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.
</details>
<details>
<summary>摘要</summary>
大型语言模型（RALM）表现了重要的进步，尤其是在减少假设错觉方面，通过利用外部知识源。然而，获取的信息可靠性并不总是保证。检索不相关的数据可能会导致异常的回答，甚至让模型忽略其内置的知识，尤其是当它拥有足够的信息来回答问题时。此外，标准RALM通常难以判断自己是否拥有足够的知识来提供正确的答案。在知识缺乏时，这些系统应该回答为“未知”，当答案不可获取时。为了解决这些挑战，我们提出了链条记录（CoN），一种新的方法，旨在提高RALM在噪音、不相关文档中的稳定性，以及处理未知情况。CoN的核心思想是生成检索文档的顺序读取笔记，以评估其与给定问题的 relevance，并将这些信息集成到形成最终答案。我们使用ChatGPT创建了培训数据集，然后在LLaMa-2 7B模型上训练CoN。我们在四个开放领域问答benchmark上进行了实验，结果显示，RALM equipped with CoN与标准RALM相比，显著提高了EM score（+7.9）和拒绝率（+10.5）。特别是，在完全噪音检索文档时，CoN表现了平均提高+7.9的EM score，在超出预训练知识范围的实时问题时，CoN表现了平均提高+10.5的拒绝率。
</details></li>
</ul>
<hr>
<h2 id="Fusion-Eval-Integrating-Evaluators-with-LLMs"><a href="#Fusion-Eval-Integrating-Evaluators-with-LLMs" class="headerlink" title="Fusion-Eval: Integrating Evaluators with LLMs"></a>Fusion-Eval: Integrating Evaluators with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09204">http://arxiv.org/abs/2311.09204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng</li>
<li>for: 本研究旨在提出一种基于大语言模型（LLM）的评估方法，以便更好地评估大语言模型的性能。</li>
<li>methods: 本研究使用了多种评估方法，包括人工评估、模型评估和自动评估指标的混合方法，以便更好地利用多个参考。</li>
<li>results: 在测试SummEval数据集上，Fusion-Eval实现了Spearman相关性0.96，超越其他评估器。这表明Fusion-Eval可以很好地捕捉人类对大语言模型的评估，设置了新的标准在LLM评估领域。<details>
<summary>Abstract</summary>
Evaluating Large Language Models (LLMs) is a complex task, especially considering the intricacies of natural language understanding and the expectations for high-level reasoning. Traditional evaluations typically lean on human-based, model-based, or automatic-metrics-based paradigms, each with its own advantages and shortcomings. We introduce "Fusion-Eval", a system that employs LLMs not solely for direct evaluations, but to skillfully integrate insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling it to work effectively across diverse tasks and make optimal use of multiple references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman correlation of 0.96, outperforming other evaluators. The success of Fusion-Eval underscores the potential of LLMs to produce evaluations that closely align human perspectives, setting a new standard in the field of LLM evaluation.
</details>
<details>
<summary>摘要</summary>
评估大型自然语言模型（LLM）是一项复杂的任务，尤其是在自然语言理解方面和高级逻辑预期方面。传统评估方法通常是人类基于、模型基于或自动指标基于的方法，每种方法都有其优点和缺点。我们介绍了“Fusion-Eval”系统，它利用 LLM 不仅 direct 评估，而是通过综合融合多个评估者的意见来进行评估。这使得 Fusion-Eval 具有灵活性，能够在多种任务中工作效果，并且能够有效地利用多个参考。在 SummEval 数据集上测试时，Fusion-Eval 达到了 Spearman 相关系数 0.96，超越其他评估者。Fusion-Eval 的成功表明了 LLM 的潜在力量，可以生成与人类观点相似的评估结果，为 LLM 评估领域做出了新的标准。
</details></li>
</ul>
<hr>
<h2 id="ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD"><a href="#ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD" class="headerlink" title="ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD"></a>ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09200">http://arxiv.org/abs/2311.09200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley</li>
<li>For: train machine learning models on private data with pre-specified differential privacy guarantee.* Methods: Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF).* Results: Achieves greater than 93% of the non-private training accuracy for a wide range of privacy parameters, exhibiting greater accuracy and privacy than the state-of-the-art method (DPSGD).Here’s the simplified Chinese text:</li>
<li>for: 用于在具有预先确定的分布式隐私保障的私人数据上训练机器学习模型。</li>
<li>methods: 使用泛化机制（ExpM）和辅助正则化流（NF）。</li>
<li>results: 在各种隐私参数下，达到了非私有训练精度的93%以上，与现有的标准方法（DPSGD）相比，具有更高的精度和更低的隐私水平。<details>
<summary>Abstract</summary>
In this pioneering work we formulate ExpM+NF, a method for training machine learning (ML) on private data with pre-specified differentially privacy guarantee $\varepsilon>0, \delta=0$, by using the Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of ExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the state-of-the-art (SOTA) and de facto method for differentially private ML, and we empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus with PRV accounting) in multiple classification tasks on the Adult Dataset (census data) and MIMIC-III Dataset (electronic healthcare records) using Logistic Regression and GRU-D, a deep learning recurrent neural network with ~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of the non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3}, 1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$ with $\delta=0$) than DPSGD. Differentially private ML generally considers $\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's ability to provide strong accuracy for orders of magnitude better privacy (smaller $\varepsilon$) substantially pushes what is currently possible in differentially private ML. Training time results are presented showing ExpM+NF is comparable to (slightly faster) than DPSGD. Code for these experiments will be provided after review. Limitations and future directions are provided.
</details>
<details>
<summary>摘要</summary>
在这项先锋性的研究中，我们提出了ExpM+NF方法，用于在私人数据上训练机器学习（ML），并 garantía了先前 specify的不同性隐私保证 $\varepsilon>0, \delta=0$。我们详细说明了ExpM+NF方法在对比DP-SGD（ differentially private stochastic gradient descent）方法时的理论优势，并通过多个分类任务在 Adult 数据集和 MIMIC-III 数据集使用Logistic Regression和 GRU-D（一个深度学习循环神经网络，参数约20K-100K）进行了实验测试。在所有实验中，ExpM+NF方法可以达到非私人训练精度的大于93%的水平（AUC），并且在私人性和精度两个方面都表现出了优势。在 differentially private ML 中通常consider $\varepsilon \in [1,10]$以保持合理的精度，因此ExpM+NF方法的能力提供较强的精度在许多次 Privacy (smaller $\varepsilon$)是 differentially private ML 领域中的一项重要进展。另外，我们还提供了训练时间结果，显示ExpM+NF方法与 DPSGD 相当（甚至是其所 faster）。我们将在审核后提供代码。本文的限制和未来发展方向也被提供。
</details></li>
</ul>
<hr>
<h2 id="Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering"><a href="#Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering" class="headerlink" title="Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering"></a>Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09198">http://arxiv.org/abs/2311.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, Jiaxing Zhang</li>
<li>for: 提高大型语言模型在长文本上的信息寻找和反思能力，解决大多数语言模型在中部正确信息寻找时的”lost in the middle”问题。</li>
<li>methods: 提出了一种特种任务 called Attention Strengthening Multi-doc QA (ASM QA)，以强化语言模型在长文本上的信息寻找和反思能力。</li>
<li>results: 实验结果表明，使用这种任务后，模型在多文档问答和其他标准任务上表现出了显著改善，与当前最佳模型相比，在随机设置下提高了13.7%的绝对性能，在 passage retrieval 任务上提高了21.5%。<details>
<summary>Abstract</summary>
While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The "lost in the middle" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA). Following these tasks, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model, Ziya-Reader to promote related research in the community.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在进行长文本输入时，具有更长的文本输入能力，但在长文本上却陷入了“lost in the middle”问题。这个问题表示大多数 LLM 在正确信息位于中间时，精度会显著下降。为了解决这个重要问题，这篇论文提出了增强 LLM 在长文本上寻找信息和反射能力的方法，通过特别设计的任务称为 Attention Strengthening Multi-doc QA (ASM QA)。这些任务使我们的模型能够更精确地寻找所需信息。实验结果显示，我们的模型在多文本选择和其他评量指标中表现出了明显改善，相比之前的模型，具有13.7%的绝对优势和21.5%的过页数优势。我们发布了我们的模型，Ziya-Reader，以促进相关的研究在社区。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task"><a href="#The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task" class="headerlink" title="The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task"></a>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09193">http://arxiv.org/abs/2311.09193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, Yixin Nie</li>
<li>for: 这个研究探讨了链条思维方法在复杂视觉语言任务中的效果，这种方法已知能够提高语言任务的效率，并且可以将任务拆分成子任务和中间步骤。</li>
<li>methods: 这个研究使用了”描述然后决策”策略，这种策略是基于人类信号处理的机制。这种策略在探测任务中提高了性能，提高了50%。</li>
<li>results: 这个研究发现，使用”描述然后决策”策略可以在复杂视觉语言任务中提高探测任务的性能，提高50%。这 laid the foundation for future research on reasoning paradigms in complex vision-language tasks.<details>
<summary>Abstract</summary>
The study explores the effectiveness of the Chain-of-Thought approach, known for its proficiency in language tasks by breaking them down into sub-tasks and intermediate steps, in improving vision-language tasks that demand sophisticated perception and reasoning. We present the "Description then Decision" strategy, which is inspired by how humans process signals. This strategy significantly improves probing task performance by 50%, establishing the groundwork for future research on reasoning paradigms in complex vision-language tasks.
</details>
<details>
<summary>摘要</summary>
研究探讨了链条思维方法的效果，这种方法通过将语言任务拆分成子任务和中间步骤，提高视语任务的效果。我们提出了“描述然后决策”策略，这种策略受人类信号处理的启发。这种策略在探测任务中提高效果 by 50%，为复杂视语任务的理解理论提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Text-Generation-with-Symbolic-References"><a href="#Towards-Verifiable-Text-Generation-with-Symbolic-References" class="headerlink" title="Towards Verifiable Text Generation with Symbolic References"></a>Towards Verifiable Text Generation with Symbolic References</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09188">http://arxiv.org/abs/2311.09188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</li>
<li>for: 提高大型自然语言模型（LLM）的输出的可靠性和准确性，以便在高风险应用中使用。</li>
<li>methods: 使用符号附加的生成方法（SymGen），让 LLM 在生成文本时间间插入明确的符号参考，以便更好地展示生成的来源和证明。</li>
<li>results: 在数据到文本和问答实验中，LLM 能够直接生成符号参考的文本，保持流畅性和准确性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers"><a href="#Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers" class="headerlink" title="Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers"></a>Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09175">http://arxiv.org/abs/2311.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</li>
<li>for: 提高零批 Retriever 的召回率和精度，探索现有state-of-the-art cross-encoder ranker中Query Expansion的影响。</li>
<li>methods: 提出了一个名为GFF的管道，包括一个大型自然语言模型和一个神经网络排序器，用于生成、筛选和融合查询扩展更有效地，以提高零批 nDCG@10 指标。</li>
<li>results: 通过GFF管道，实现了在 BEIR 和 TREC DL 2019&#x2F;2020 上提高零批 nDCG@10。同时，通过不同模型选择来分析GFF管道中的不同模型选择，并为未来Query Expansion在零批神经rankers中的发展提供了指导。<details>
<summary>Abstract</summary>
Query expansion has been proved to be effective in improving recall and precision of first-stage retrievers, and yet its influence on a complicated, state-of-the-art cross-encoder ranker remains under-explored. We first show that directly applying the expansion techniques in the current literature to state-of-the-art neural rankers can result in deteriorated zero-shot performance. To this end, we propose GFF, a pipeline that includes a large language model and a neural ranker, to Generate, Filter, and Fuse query expansions more effectively in order to improve the zero-shot ranking metrics such as nDCG@10. Specifically, GFF first calls an instruction-following language model to generate query-related keywords through a reasoning chain. Leveraging self-consistency and reciprocal rank weighting, GFF further filters and combines the ranking results of each expanded query dynamically. By utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the GFF pipeline and shed light on the future directions in query expansion for zero-shot neural rankers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>查询扩展已经证明可以提高首个采集器的回归精度和准确率，然而它们对现代扩展排序器的影响仍未得到充分探索。我们首先表明，直接在当前文献中使用扩展技术可能会导致zero-shot性能下降。为此，我们提出了GFF，一个包含大语言模型和神经排序器的管道，用于更有效地生成、筛选和融合查询扩展以提高zero-shot排序指标 such as nDCG@10。具体来说，GFF首先通过一个遵循逻辑链的语言模型生成查询相关的关键词。然后，通过自 consistency和对称排序权重，GFF进一步筛选并将每个扩展查询的排名结果进行动态融合。通过这个管道，我们显示了GFF可以提高zero-shot nDCG@10在BEIR和TREC DL 2019/2020。我们还分析了GFF管道中不同的模型选择，并 shed light on the future directions in query expansion for zero-shot neural rankers。
</details></li>
</ul>
<hr>
<h2 id="AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph"><a href="#AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph" class="headerlink" title="AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph"></a>AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09174">http://arxiv.org/abs/2311.09174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/abspyramid">https://github.com/hkust-knowcomp/abspyramid</a></li>
<li>paper_authors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song</li>
<li>for: 本研究旨在探讨语言模型具备抽象能力的基础知识，而这一点尚未得到充分探讨。</li>
<li>methods: 本研究提出了AbsPyramid，一个包含221K文本描述抽象知识的统一推理图。现有资源只关注简化事件中的名词或特定领域中的动词，而AbsPyramid收集了多元事件中的抽象知识，用以全面评估语言模型在开放领域中的抽象能力。</li>
<li>results: 实验结果表明，现有的LLMs在零shot和少shot设定下面临抽象知识的挑战。通过在我们的充沛的抽象知识上训练，我们发现LLMs可以学习基本的抽象能力，并在未看过事件的情况下 generalized 到未看过的事件。同时，我们实际表明了我们的标准可以强化LLMs在两个以前的抽象任务中。<details>
<summary>Abstract</summary>
Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
</details>
<details>
<summary>摘要</summary>
cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.Here's the word-for-word translation:认知研究表明，人类智能中的抽象能力是非常重要的，但是这一点尚未得到充分探索。在这篇论文中，我们提出了AbsPyramid，一个包含221万个文本描述抽象知识的统一谱系图。现有资源只是在简化事件中关注单个名词或动词，而AbsPyramid则收集了多元事件中的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，当前的LLMs在零shot和几shot设定下面临抽象知识的挑战。通过我们的充足的抽象知识训练，我们发现LLMs可以获得基本的抽象能力，并在未看到的事件中进行推理。同时，我们实验表明，我们的标准可以提高LLMs在两个先前的抽象任务中。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction"><a href="#Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction" class="headerlink" title="Temporal Knowledge Question Answering via Abstract Reasoning Induction"></a>Temporal Knowledge Question Answering via Abstract Reasoning Induction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09149">http://arxiv.org/abs/2311.09149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang</li>
<li>for: 这项研究旨在解决大语言模型（LLM）中的时间知识推理问题，这个问题导致模型经常生成错误或误导性信息，主要是因为它们对逐渐变化的事实知识和复杂的时间逻辑有限制。</li>
<li>methods: 我们提出了一种新的构建主义方法，强调在LLM学习中进行持续的知识合成和定制。这种方法包括Abstract Reasoning Induction（ARI）框架，将时间推理分为两个阶段：无知阶段和知识阶段。</li>
<li>results: 我们的方法在两个时间问答数据集上取得了显著的改善，相对于基eline的提升为29.7%和9.27%，证明了我们的方法在LLM中提高时间推理的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/czy1999/ARI%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/czy1999/ARI上发布。</a><details>
<summary>Abstract</summary>
In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic. In response, we propose a novel, constructivism-based approach that advocates for a paradigm shift in LLM learning towards an active, ongoing process of knowledge synthesis and customization. At the heart of our proposal is the Abstract Reasoning Induction ARI framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce instances of hallucinations and improve LLMs' capacity for integrating abstract methodologies derived from historical data. Our approach achieves remarkable improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code will be released at https://github.com/czy1999/ARI.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了大型语言模型（LLM）中的时间知识推理挑战，这是这些模型经常遇到的Difficulties。这些Difficulties常常导致模型生成错误或误导性信息，主要因为它们对不断发展的事实知识和复杂的时间逻辑具有有限的处理能力。作为回应，我们提出了一种新的、建构主义基础的方法，强调了LLM学习的活跃、持续进行知识合成和自定义。我们的提议的核心是抽象逻辑概念抽取框架（ARI），将时间推理分为两个不同阶段：无知阶段和知识阶段。这种分离的目的是减少幻觉和提高LLM对抽象方法的应用能力，来自历史数据的抽象方法。我们的方法在两个时间问答 datasets上显示了remarkable improvement，相对提高29.7%和9.27%，证明了我们的方法在提高LLM时间推理能力的有效性。代码将在https://github.com/czy1999/ARI上发布。
</details></li>
</ul>
<hr>
<h2 id="Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts"><a href="#Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts" class="headerlink" title="Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts"></a>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09127">http://arxiv.org/abs/2311.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</li>
<li>for: 本研究的目的是探讨 Multimodal Large Language Models (MLLMs) 的监禁攻击 vulnerability，尤其是模型输入的攻击而不是模型 API 的攻击。</li>
<li>methods: 本研究使用了一种新的监禁攻击方法，称为 SASP (Self-Adversarial Attack via System Prompt)，通过使用 GPT-4 作为自己的红人工具，对自己进行攻击，以搜索可能的监禁提示。此外，还添加了人工修改基于 GPT-4 的分析，以进一步提高攻击成功率。</li>
<li>results: 研究发现，修改系统提示可以显著降低监禁成功率。此外，还发现了一些可能的监禁攻击方法，可以用于增强 MLLM 的安全性。<details>
<summary>Abstract</summary>
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details>
<details>
<summary>摘要</summary>
existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work:1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs;2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7%;3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details></li>
</ul>
<hr>
<h2 id="HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data"><a href="#HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data" class="headerlink" title="HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data"></a>HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09115">http://arxiv.org/abs/2311.09115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Hemker, Nikola Smidjievski, Mateja Jamnik</li>
<li>for: 该研究旨在开发一种可以处理多modal数据的深度学习模型，以提高肿瘤病例预测和诊断的精度。</li>
<li>methods: 该模型使用混合早期融合注意力学习网络（HEALNet），可以保持模态特有的结构信息，同时捕捉模态之间的交互信息，并能够有效地处理训练和推理中缺失的模态。</li>
<li>results: 在四个TCGA肿瘤 cohort 上进行多modal 存活分析，HEALNet 达到了当前最佳性能，substantially 超过了uni-modal和recent multi-modal基elines，并在缺失模态的情况下保持稳定性。<details>
<summary>Abstract</summary>
Technological advances in medical data collection such as high-resolution histopathology and high-throughput genomic sequencing have contributed to the rising requirement for multi-modal biomedical modelling, specifically for image, tabular, and graph data. Most multi-modal deep learning approaches use modality-specific architectures that are trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the Hybrid Early-fusion Attention Learning Network (HEALNet): a flexible multi-modal fusion architecture, which a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multi-modal survival analysis on Whole Slide Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance, substantially improving over both uni-modal and recent multi-modal baselines, whilst being robust in scenarios with missing modalities.
</details>
<details>
<summary>摘要</summary>
科技发展在医疗数据收集方面，如高纬度 histopathology 和高通量 genomic 测序，导致了多modal 生物医学模型的需求增加，特别是图像、表格和图数据。大多数多modal 深度学习方法使用专门的modal architecture，在不同数据源之间进行分离的训练，无法捕捉到关键的交叉modal 信息，这些信息是集成不同数据源的关键因素。本文提出了 Hybrid Early-fusion Attention Learning Network (HEALNet)：一种灵活的多modal 融合架构，具有以下特点：a) 保留modal 特有的结构信息b) 捕捉交叉modal 交互和结构信息在共享的射频空间中c) 可以效果地处理训练和推理中缺失的modald) 可以直观地检查模型，学习在原始数据输入上而不是隐藏的表示我们在TCGA 四个肿瘤减剂中进行多modal 存活分析，HEALNet 实现了状态机器的表现，大幅超越uni-modal 和最近的多modal 基线，同时在缺失modal 情况下 display 强健。
</details></li>
</ul>
<hr>
<h2 id="Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification"><a href="#Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification" class="headerlink" title="Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"></a>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09114">http://arxiv.org/abs/2311.09114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoqiang Kang, Juntong Ni, Huaxiu Yao</li>
<li>for: 提高文本生成中的准确性和可靠性，解决非检索基本生成和检索增强生成中的幻见和误差问题。</li>
<li>methods: 采用实时步进生成和幻见纠正策略，在文本生成过程中实时检测和纠正幻见。</li>
<li>results: 相比基eline，Ever在多种任务上表现出显著改善，包括短文Question Answering、生成传记和多步论证。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion"><a href="#Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion" class="headerlink" title="Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?"></a>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09109">http://arxiv.org/abs/2311.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: This paper aims to analyze the inference and memorization abilities of Pre-trained Language Model (PLM)-based Knowledge Graph Completion (KGC) methods.</li>
<li>methods: The authors propose a method for constructing synthetic datasets to evaluate the inference and memorization abilities of PLM-based KGC methods.</li>
<li>results: The authors find that PLMs acquire the inference abilities required for KGC through pre-training, but the performance improvements mostly come from textual information of entities and relations.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是分析基于预训言语模型（PLM）的知识图完成（KGC）方法的推理和记忆能力。</li>
<li>methods: 作者提出了一种用于评估 PLM-based KGC 方法的推理和记忆能力的 sintetic 数据构建方法。</li>
<li>results: 作者发现 PLM 通过预训言语模型获得了 KGC 所需的推理能力，但是表现提升主要来自实体和关系的文本信息。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods, such as RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training. Therefore, PLM-based KGC can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 由 link 描述实体之间的关系组成。由于手动列举所有实体之间的关系是不可能的，因此自动完成这些关系是知识 graphs 中的关键任务。知识 Graph Completion (KGC) 任务的目标是在知识 graphs 中推断未经seen的关系。传统的嵌入式 KGC 方法，如 RESCAL、TransE、DistMult、ComplEx、RotatE、HAKE、HousE 等，通过训练数据来INFER 缺失的链接。与此相反，近期的 Pre-trained Language Model (PLM)-based KGC 使用在预训练中获得的知识来完成缺失的链接。这种方法存在问题，因为建立 KGC 模型的目标是推断未经seen的关系，但常规的 KGC 评价不会分别考虑推断和记忆能力。因此，一个 PLM-based KGC 方法，即在当前 KGC 评价中表现出色，可能在实际应用中效果不佳。为了解决这个问题，我们分析了 PLM-based KGC 方法是否真的进行推断，还是仅仅访问记忆中的知识。为此，我们提出了一种方法来构建 synthetic 数据集，并结论 PLMs 通过预训练获得了 KGC 中需要的推断能力，尽管表现提升主要来自实体和关系之间的文本信息。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning"><a href="#Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning" class="headerlink" title="Towards A Unified View of Answer Calibration for Multi-Step Reasoning"></a>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09101">http://arxiv.org/abs/2311.09101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi</li>
<li>for: 提高多步逻辑能力</li>
<li>methods: 使用链条提示法 (Chain-of-Thought, CoT) 和答题均衡策略 (answer calibration)</li>
<li>results: 系统性地研究了不同答题策略的效果，可能提供关键的逻辑提高多步逻辑能力的关键因素。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. Usually, answer calibration strategies such as step-level or path-level calibration play a vital role in multi-step reasoning. While effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）透过链接思维（CoT）提示方法扩大多步骤理解能力的可能性。通常，答案调整策略如步骤级或路径级调整在多步骤理解中扮演着重要角色。 although effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.Here's the word-for-word translation:大型语言模型（LLMs）透过链接思维（CoT）提示方法扩大多步骤理解能力的可能性。通常，答案调整策略如步骤级或路径级调整在多步骤理解中扮演着重要角色。 although effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details></li>
</ul>
<hr>
<h2 id="Can-MusicGen-Create-Training-Data-for-MIR-Tasks"><a href="#Can-MusicGen-Create-Training-Data-for-MIR-Tasks" class="headerlink" title="Can MusicGen Create Training Data for MIR Tasks?"></a>Can MusicGen Create Training Data for MIR Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09094">http://arxiv.org/abs/2311.09094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadine Kroher, Helena Cuesta, Aggelos Pikrakis</li>
<li>for: 这个论文是为了研究基于人工智能的生成音乐系统，用于生成用于音乐信息检索（MIR）任务的训练数据。</li>
<li>methods: 该论文使用了一种基于文本描述的生成模型，使用了50000个随机生成的音乐样本，以及五种不同的音乐类型。</li>
<li>results: 该研究显示，提出的模型可以从人工音乐追加特征中学习到真实音乐录音中的特征，并且这些特征可以泛化到真实音乐录音中。<details>
<summary>Abstract</summary>
We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre- conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.
</details>
<details>
<summary>摘要</summary>
我们正在研究基于人工智能的生成音乐系统来生成音乐信息检索（MIR）任务的训练数据。为了开始这条工作，我们进行了一次初步实验，我们使用MusicGen创建了一个完全人工的音乐集，并构建了50,000个频谱类别的文本描述。我们的初步结果表明，我们的模型可以从人工音乐轨迹中学习类别特征，这些特征可以通过到实际音乐录音中。
</details></li>
</ul>
<hr>
<h2 id="The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV"><a href="#The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV" class="headerlink" title="The Uli Dataset: An Exercise in Experience Led Annotation of oGBV"></a>The Uli Dataset: An Exercise in Experience Led Annotation of oGBV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09086">http://arxiv.org/abs/2311.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George, Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane, Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar</li>
<li>For: The paper aims to provide a dataset for automated detection of gendered abuse in three languages - Hindi, Tamil, and Indian English.* Methods: The paper uses a participatory approach to create a dataset of annotated tweets that pertain to the experience of gender abuse, using experts who identify as women or a member of the LGBTQIA community in South Asia.* Results: The paper presents a dataset of gendered abuse in three languages, which can be used to train AI systems for automated detection of hate speech and gendered abuse.<details>
<summary>Abstract</summary>
Online gender based violence has grown concomitantly with adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet has necessitated the need for automated detection of hate speech, and more specifically gendered abuse. There is, however, a lack of language specific and contextual data to build such automated tools. In this paper we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA community in South Asia. Through this dataset we demonstrate a participatory approach to creating datasets that drive AI systems.
</details>
<details>
<summary>摘要</summary>
互联网上的性别基于暴力现象随着互联网和社交媒体的普及而增长。这些效果在全球主导语言社区更为严重，因为许多用户在社交媒体上使用非英语语言。因此，自动检测仇恨言论的需求增长，而特定于语言和文化的数据缺乏。本文提供了三种语言（旁遮普语、泰米尔语和印度英语）的性别暴力数据集。该数据集包括由专家评估员， identificas como女性或LGBTQIA社群成员在南亚地区，对性别暴力经历的三个问题进行了标注。通过这个数据集，我们展示了参与式的方法来创建AI系统驱动的数据集。
</details></li>
</ul>
<hr>
<h2 id="How-Multilingual-is-Multilingual-LLM"><a href="#How-Multilingual-is-Multilingual-LLM" class="headerlink" title="How Multilingual is Multilingual LLM?"></a>How Multilingual is Multilingual LLM?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09071">http://arxiv.org/abs/2311.09071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li</li>
<li>for: 本研究旨在评估大型自然语言处理器（LLM）在101种语言中的多语言能力，并将这些语言分为四个不同的 quadrant。</li>
<li>methods: 本研究采用了多种调整策略来提高LLM的多语言能力，并进行了广泛的实验。</li>
<li>results: 研究发现，现有的LLM在不同的语言中具有更高的多语言能力，并且可以通过特定的调整策略来进一步提高这种能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在其主要基础于英语数据的训练下显示限制性，当应用于其他语言时。当前的研究主要集中在提高LLM的多语言能力，使用不同的调整策略。虽然在某些语言中表现有效，但我们对LLM的多语言能力的理解仍然不够完整。这项研究决心通过对101种语言进行极为详细的分析，将语言分为四个不同的 quadrant，并对每个 quadrant 进行深入的分析，以便更好地理解它们的分类原因，并提供调整这些语言的指导方针。广泛的实验表明，现有的LLM具有许多语言的多语言能力，并且可以通过专注于每个 quadrant 中的特定特征进行进一步改进。
</details></li>
</ul>
<hr>
<h2 id="How-Well-Do-Large-Language-Models-Truly-Ground"><a href="#How-Well-Do-Large-Language-Models-Truly-Ground" class="headerlink" title="How Well Do Large Language Models Truly Ground?"></a>How Well Do Large Language Models Truly Ground?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09069">http://arxiv.org/abs/2311.09069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo</li>
<li>for: 本研究旨在提高大语言模型（LLM）的可靠性和控制性，以增强其应用的可靠性和可控性。</li>
<li>methods: 本研究提出了一种严格的定义对大语言模型的挖根（grounding），即模型的回答（1） должны充分利用提供的知识背景，而且（2）不能超过知识背景中的知识。此外，本研究还提出了一个新的挖根度量以评估这种新定义，并在13种不同大小和训练方法的大语言模型上进行了实验，以了解影响挖根性能的因素。</li>
<li>results: 本研究发现，大语言模型的挖根性能受模型大小、训练方法和知识背景的影响。此外，本研究还发现了一些因素可以提高挖根性能，例如提供更多的知识背景和使用更好的训练方法。这些发现可以帮助改进大语言模型的可靠性和控制性，并促进其应用的可靠性和可控性。<details>
<summary>Abstract</summary>
Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge. To mitigate this, LLMs can be probed to generate responses by grounding on external context, often given as input (knowledge-augmented models). Yet, previous research is often confined to a narrow view of the term "grounding", often only focusing on whether the response contains the correct answer or not, which does not ensure the reliability of the entire response. To address this limitation, we introduce a strict definition of grounding: a model is considered truly grounded when its responses (1) fully utilize necessary knowledge from the provided context, and (2) don't exceed the knowledge within the contexts. We introduce a new dataset and a grounding metric to assess this new definition and perform experiments across 13 LLMs of different sizes and training methods to provide insights into the factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.
</details>
<details>
<summary>摘要</summary>
靠内置知识的大语言模型（LLM）的依赖可能会导致问题，如幻觉、无控和变量知识的整合困难。为了解决这些问题，LLM可以通过附加外部知识作为输入（知识增强模型）来生成响应。然而，先前的研究通常受限于“附加”的概念，通常仅关注响应是否包含正确答案而不确保整个响应的可靠性。为此，我们提出了一个严格的定义：一个模型被视为真正地附加地理解当前上下文中的所有必要知识，而不超过上下文中的知识。我们介绍了一个新的数据集和一种附加度量来评估这个新定义，并在13种不同大小和训练方法的LLM上进行了实验，以提供更好地理解如何改进附加能力，以及提出一个向更可靠和可控的LLM应用程序的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fair-Division-from-Bandit-Feedback"><a href="#Learning-Fair-Division-from-Bandit-Feedback" class="headerlink" title="Learning Fair Division from Bandit Feedback"></a>Learning Fair Division from Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09068">http://arxiv.org/abs/2311.09068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki</li>
<li>for: 学习在不纯粹知道代理人价值或利益下进行的线上公平分配</li>
<li>methods: 使用 wrapper 算法，利用 dual averaging 慢渐学习到来的物品类型分布和代理人价值，通过bandit反馈</li>
<li>results: 可以达到优化的尼亚希尔投资市场中代理人的加法式利益，并且可以获得误差 bound 和实际数据 validate 的优越性表现<details>
<summary>Abstract</summary>
This work addresses learning online fair division under uncertainty, where a central planner sequentially allocates items without precise knowledge of agents' values or utilities. Departing from conventional online algorithm, the planner here relies on noisy, estimated values obtained after allocating items. We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling gradual learning of both the type distribution of arriving items and agents' values through bandit feedback. This approach enables the algorithms to asymptotically achieve optimal Nash social welfare in linear Fisher markets with agents having additive utilities. We establish regret bounds in Nash social welfare and empirically validate the superior performance of our proposed algorithms across synthetic and empirical datasets.
</details>
<details>
<summary>摘要</summary>
这个工作研究在不约束的情况下进行在线公平分配，其中中央规划者逐步分配物品不知道代理人的价值或利用率。与传统的在线算法不同，在这里中央规划者基于不准确的估计值进行分配。我们介绍了一种封包算法使用双均值，以慢慢地学习到来到的物品类型和代理人的价值，通过抽象反馈。这种方法可以在线性鱼钩市场中实现互斥社会财富的优化，并且我们证明了对社会财富的违和 bounds。我们还进行了synthetic和实际数据的实验 validate our proposed algorithms的优秀性。
</details></li>
</ul>
<hr>
<h2 id="In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment"><a href="#In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment" class="headerlink" title="In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment"></a>In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09273">http://arxiv.org/abs/2311.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sonia Moshfeghi, Muhammad Tanveer Jan, Joshua Conniff, Seyedeh Gol Ara Ghoreishi, Jinwoo Jang, Borko Furht, Kwangsoo Yang, Monica Rosselli, David Newman, Ruth Tappen, Dana Smith</li>
<li>for: 这研究旨在设计低成本的在车辆内部监测老年人驾驶性能的设备，并通过机器学习方法探测早期认知障碍的迹象。</li>
<li>methods: 该研究使用了低成本的在车辆内部监测设备，并运用机器学习方法对数据进行分析。</li>
<li>results: 研究发现，有MCI的 drivers 驾驶更稳定和安全，而非MCI的 drivers 则更容易出现异常驾驶行为。此外，数据分析还发现，夜间驾驶、总里程数和教育程度是最重要的因素。<details>
<summary>Abstract</summary>
Driving is a complex daily activity indicating age and disease related cognitive declines. Therefore, deficits in driving performance compared with ones without mild cognitive impairment (MCI) can reflect changes in cognitive functioning. There is increasing evidence that unobtrusive monitoring of older adults driving performance in a daily-life setting may allow us to detect subtle early changes in cognition. The objectives of this paper include designing low-cost in-vehicle sensing hardware capable of obtaining high-precision positioning and telematics data, identifying important indicators for early changes in cognition, and detecting early-warning signs of cognitive impairment in a truly normal, day-to-day driving condition with machine learning approaches. Our statistical analysis comparing drivers with MCI to those without reveals that those with MCI exhibit smoother and safer driving patterns. This suggests that drivers with MCI are cognizant of their condition and tend to avoid erratic driving behaviors. Furthermore, our Random Forest models identified the number of night trips, number of trips, and education as the most influential factors in our data evaluation.
</details>
<details>
<summary>摘要</summary>
驾驶是一项复杂的日常活动，表征年龄和疾病相关的认知下降。因此，比较驾驶性能与无轻度认知障碍（MCI）者可以反映认知功能的变化。有增加证据表明，在日常生活设定下不侵入式监测老年人驾驶性能可能能够探测潜在的认知变化。本文的目标包括设计低成本在车辆内部的感知硬件，获取高精度定位和电子邮件数据，确定识别认知下降的重要指标，并使用机器学习方法探测日常驾驶中的认知障碍迹象。我们的统计分析比较有MCI和无MCI driver的驾驶方式，发现有MCI driver驾驶更稳定和安全，这表示有MCI driver们对自己的状况有所了解，避免了异常的驾驶行为。此外，我们的Random Forest模型确定了夜晚出行次数、总出行次数和教育程度是我们数据评估中最重要的因素。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective"><a href="#Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective" class="headerlink" title="Assessing Knowledge Editing in Language Models via Relation Perspective"></a>Assessing Knowledge Editing in Language Models via Relation Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09053">http://arxiv.org/abs/2311.09053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective">https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective</a></li>
<li>paper_authors: Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, Kang Liu</li>
<li>for: 本研究旨在探讨relation-centric的知识编辑方法，以改善大语言模型中的事实知识。</li>
<li>methods: 本研究使用了一个新的benchmark名为RaKE，用于评估relation based知识编辑方法。在本研究中，我们开发了一组创新的评价指标，并进行了多种知识编辑基线的全面实验。</li>
<li>results: 我们发现现有的知识编辑方法具有编辑关系的潜在困难，因此我们进一步探讨了在transformer中知识与关系之间的关系。我们的研究结果表明，知识与关系之间的相互作用不仅存在于FFN网络中，还存在于注意层中。这些结果为未来的关系基于的知识编辑方法提供了实验支持。<details>
<summary>Abstract</summary>
Knowledge Editing (KE) for modifying factual knowledge in Large Language Models (LLMs) has been receiving increasing attention. However, existing knowledge editing methods are entity-centric, and it is unclear whether this approach is suitable for a relation-centric perspective. To address this gap, this paper constructs a new benchmark named RaKE, which focuses on Relation based Knowledge Editing. In this paper, we establish a suite of innovative metrics for evaluation and conduct comprehensive experiments involving various knowledge editing baselines. We notice that existing knowledge editing methods exhibit the potential difficulty in their ability to edit relations. Therefore, we further explore the role of relations in factual triplets within the transformer. Our research results confirm that knowledge related to relations is not only stored in the FFN network but also in the attention layers. This provides experimental support for future relation-based knowledge editing methods.
</details>
<details>
<summary>摘要</summary>
知识编辑（KE），用于修改大型自然语言模型（LLM）中的事实知识，在过去几年 receiving increasing attention。然而，现有的知识编辑方法都是基于实体中心的，是否这种方法适用于关系中心的视角是一个不确定之处。为了解决这个差距，本文构建了一个新的标准 benchmark，名为RaKE，它专注于基于关系的知识编辑。在本文中，我们开发了一组创新的评价指标，并进行了多种知识编辑基准测试。我们发现，现有的知识编辑方法在编辑关系方面存在潜在的困难。因此，我们进一步探索关系在综合 triplets 中的角色。我们的研究结果表明，关系相关的知识不仅存储在 FFN 网络中，还存储在注意层中。这些结果为未来基于关系的知识编辑方法提供了实验支持。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts"><a href="#Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts" class="headerlink" title="Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts"></a>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09050">http://arxiv.org/abs/2311.09050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecnu-dase-nlp/rqp">https://github.com/ecnu-dase-nlp/rqp</a></li>
<li>paper_authors: Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian</li>
<li>for: 这个研究旨在提高运算语言模型（LLMs）在零shot情况下的视觉问题回答能力。</li>
<li>methods: 这个研究使用了将图像转换为描述来桥接多modal的信息，并将大型语言模型（LLMs）应用其强大的零shot普遍能力来解答未见过的问题。</li>
<li>results: 这个研究发现，使用 reasoning question prompts 可以帮助 LLMs 在零shot情况下提高视觉问题回答能力，并在三个 VQA 挑战中实现了显著的改善。<details>
<summary>Abstract</summary>
Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details>
<details>
<summary>摘要</summary>
zero-shotVisual问题 answering（VQA）是一个引人注目的视觉语言任务，它检验系统在没有训练数据的情况下，对视觉和文本的理解能力。在最近的研究中，人们通过将图像转换成caption，使得多modal信息相互连接，大型自然语言模型（LLMs）可以通过未看过的问题来应用强大的零shot泛化能力。为了设计适合解决VQA问题的示例提问，许多研究已经探索了不同的策略来选择或生成问题答对，但它们完全忽略了问题提示的角色。原始的VQA问题通常会遇到括苍和模糊性，需要中间的逻辑推理。为此，我们提出了Visual Question Prompts（VQP），可以更好地激活LLMs在零shot场景中的潜力。具体来说，我们首先通过一个无supervised问题编辑模块，将每个问题转换成自包含的逻辑问题提示，考虑 sentence fluency、semantic integrity和syntactic invariance。每个逻辑问题提示清晰表达问题的意图。这些逻辑问题提示的候选答案，与其相关的自信度分数 acting as answer heuristics，被 feed into LLMs，并生成最终的答案。我们在三个VQA挑战中评估了逻辑问题提示，实验结果表明，它们可以在零shot设置下显著提高LLMs的成绩，并在四个数据集中超越现有的零shot方法。我们的源代码公开发布在 \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details></li>
</ul>
<hr>
<h2 id="MELA-Multilingual-Evaluation-of-Linguistic-Acceptability"><a href="#MELA-Multilingual-Evaluation-of-Linguistic-Acceptability" class="headerlink" title="MELA: Multilingual Evaluation of Linguistic Acceptability"></a>MELA: Multilingual Evaluation of Linguistic Acceptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09033">http://arxiv.org/abs/2311.09033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu</li>
<li>for: 本研究的目的是提供一个多语言的文本评估 benchmark，以评估大型自然语言模型（LLM）的语言能力。</li>
<li>methods: 本研究使用了多种方法，包括对 LLM 进行基本条件下的训练、跨语言和多任务学习等。</li>
<li>results: 研究结果显示，ChatGPT 优化后的性能仍然落后于 XLM-R 的优化版本，而 GPT-4 在零执行设定下的性能则与 XLM-R 的优化版本相近。跨语言和多任务学习实验显示，在接受性评估中，对于不同语言的训练数据是关键的。层内探索结果显示，XLM-R 的上层层次在多语言接受性评估中成为了任务特定但语言不同的区域。此外，研究还引入了“矛盾量”的概念，可能是跨语言传递中的困难指标。<details>
<summary>Abstract</summary>
Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting. Cross-lingual and multi-task learning experiments show that unlike semantic tasks, in-language training data is crucial in acceptability judgements. Results in layerwise probing indicate that the upper layers of XLM-R become a task-specific but language-agnostic region for multilingual acceptability judgment. We also introduce the concept of conflicting weight, which could be a potential indicator for the difficulty of cross-lingual transfer between languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）测试主要集中在应用驱动的任务上，如复杂的理解和代码生成，这导致了语言学性评估的缺乏。为了解决这个问题，我们介绍了多语言可接受性评估（MELA），这是一个覆盖10种语言家族的多语言测试，共有48000个样本。我们设置了常用的LLM和监督模型的基线，并进行了跨语言传播和多任务学习实验。为了寻求多语言可读性，我们分析了精心调整的XLM-R的权重，以探索语言传播困难之间的转移。我们的结果表明，ChatGPT在受到上下文示例的帮助下表现出色，但仍落后于精心调整的XLM-R，而GPT-4在零上下文设定下和精心调整的XLM-R表现相当。跨语言和多任务学习实验表明，与semantic任务不同，在语言上的训练数据是关键在acceptability判断中。层次探索结果表明，XLM-R的上层变成了语言家族无关的任务特有的区域，用于多语言可接受性判断。我们还引入了 conflicting weight 概念，可能是跨语言传播中难度的指标。我们的数据将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning"><a href="#Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning" class="headerlink" title="Assessing the Robustness of Intelligence-Driven Reinforcement Learning"></a>Assessing the Robustness of Intelligence-Driven Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09027">http://arxiv.org/abs/2311.09027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari, Federico Cerutti</li>
<li>for: 本研究旨在探讨奖励机器学习系统中噪声robustness问题。</li>
<li>methods: 本研究使用奖励机器学习来表达复杂的奖励结构，并通过证明和学习来强化现有的奖励学习方法。</li>
<li>results: 初步的结果表明现有的奖励学习方法需要进一步的证明和学习，以使其在实际应用中更加可靠。<details>
<summary>Abstract</summary>
Robustness to noise is of utmost importance in reinforcement learning systems, particularly in military contexts where high stakes and uncertain environments prevail. Noise and uncertainty are inherent features of military operations, arising from factors such as incomplete information, adversarial actions, or unpredictable battlefield conditions. In RL, noise can critically impact decision-making, mission success, and the safety of personnel. Reward machines offer a powerful tool to express complex reward structures in RL tasks, enabling the design of tailored reinforcement signals that align with mission objectives. This paper considers the problem of the robustness of intelligence-driven reinforcement learning based on reward machines. The preliminary results presented suggest the need for further research in evidential reasoning and learning to harden current state-of-the-art reinforcement learning approaches before being mission-critical-ready.
</details>
<details>
<summary>摘要</summary>
“对于强化学习系统而言，响应噪音的能力非常重要，尤其在军事上，因为高赌注和不确定的环境存在。噪音和不确定性是军事操作中的自然特征，可能来自于不完整的信息、敌对行为或不可预测的战场状况。在RL中，噪音可能严重影响决策、任务成功和人员安全。优先奖机器提供了一个强大的工具来表达复杂的奖赏结构，允许设计适应任务目标的优先奖赏。本文考虑了对于优先奖机器验证的问题。初步的结果显示需要进一步的证据推理和学习来强化现代强化学习方法，以便在任务 kritisch-ready 之前进行更多的研究。”
</details></li>
</ul>
<hr>
<h2 id="Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach"><a href="#Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach" class="headerlink" title="Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach"></a>Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09015">http://arxiv.org/abs/2311.09015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, AmirEmad Ghassami, Ilya Shpitser</li>
<li>for: 这个论文是为了解决在数据缺失不是随机（MNAR）的情况下， Parameter of interest 的标定和估计问题。</li>
<li>methods: 该论文提出了一种 alternate approach，使用数据融合技术，将 MNAR 数据集中缺失的信息与 MAR 数据集中的信息 fusion 在一起，以便标定和估计 Parameter of interest。</li>
<li>results: 该论文表明，即使单个数据集中 Parameter of interest 无法标定，也可以通过合并数据集来标定它，只要满足两个 complementary 的假设。 并且提出了一种基于 inverse probability weighted（IPW）的估计方法，并通过 simulations 研究了该估计方法的性能。<details>
<summary>Abstract</summary>
We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in an MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive an inverse probability weighted (IPW) estimator for identified parameters, and evaluate the performance of our estimation strategies via simulation studies.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在数据缺失不是随机（MNAR）的设置下确定和估计 parameter of interest。通常情况下，这些参数不能 без强大的缺失数据模型假设而被确定。在这篇论文中，我们采用了一种不同的方法，我们引入了基于数据融合的方法，其中MNAR数据集中的信息被融合了一个 auxiliary dataset 中的信息，其中数据缺失是随机的（MAR）。我们证明了，即使单独访问 Either dataset alone，parameter of interest 也无法被确定，但是通过将两个数据集融合在一起，可以在两个不同的假设集下确定这个参数。我们 derivate了一个 inverse probability weighted （IPW）估计器，并通过实验研究评估了我们的估计策略的性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning"><a href="#Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning" class="headerlink" title="Adversarial Attacks to Reward Machine-based Reinforcement Learning"></a>Adversarial Attacks to Reward Machine-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09014">http://arxiv.org/abs/2311.09014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari</li>
<li>for: 本研究旨在对 reward machine (RM) 在机器学习设置下的安全性和可靠性进行分析，并提出一种新的攻击方法——盲目攻击。</li>
<li>methods: 本研究使用了 RM  formalism，并对 RM-based 技术进行了安全性和可靠性的分析。</li>
<li>results: 本研究发现了一种新的攻击方法——盲目攻击，并证明了这种攻击方法可以 efficiently 攻击 RM-based 技术。<details>
<summary>Abstract</summary>
In recent years, Reward Machines (RMs) have stood out as a simple yet effective automata-based formalism for exposing and exploiting task structure in reinforcement learning settings. Despite their relevance, little to no attention has been directed to the study of their security implications and robustness to adversarial scenarios, likely due to their recent appearance in the literature. With my thesis, I aim to provide the first analysis of the security of RM-based reinforcement learning techniques, with the hope of motivating further research in the field, and I propose and evaluate a novel class of attacks on RM-based techniques: blinding attacks.
</details>
<details>
<summary>摘要</summary>
近年来，奖励机器（RM）作为 automata-based 形式化的一种简单 yet effective 方法，在 reinforcement learning 设置下露出和利用任务结构。尽管它们在文献中具有重要性，但是它们的安全性和对抗性却受到了相对的少量关注，可能是因为它们在文献中的新出现。我的论文旨在提供 RM-based reinforcement learning 技术的首次安全分析，并提出和评估一种新的攻击方法：盲目攻击。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake"><a href="#Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake" class="headerlink" title="Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake"></a>Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08999">http://arxiv.org/abs/2311.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morocco Solidarity Hackathon</li>
<li>for: 提高灾害管理策略</li>
<li>methods: 使用人工智能技术</li>
<li>results: 提供了一份全面的文献综述、赢得项目概述、关键发现和挑战（包括实时开源数据的不足、数据缺乏和跨学科合作障碍），并发起了社区呼吁。<details>
<summary>Abstract</summary>
The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023 prompted critical reflections on global disaster management strategies, resulting in a post-disaster hackathon, using artificial intelligence (AI) to improve disaster preparedness, response, and recovery. This paper provides (i) a comprehensive literature review, (ii) an overview of winning projects, (iii) key insights and challenges, namely real-time open-source data, data scarcity, and interdisciplinary collaboration barriers, and (iv) a community-call for further action.
</details>
<details>
<summary>摘要</summary>
惊人的6.8级地震在摩洛哥阿哈鲁斯市在2023年引发了全球灾害管理策略的批判性反思，导致了一场由人工智能（AI）改善灾害准备、应急回应和恢复的 poste-disaster hackathon。本文提供以下内容：（i）全面的文献综述（ii）赢得项目的概述（iii）关键的发现和挑战，包括实时开源数据、数据缺乏和跨学科协作障碍，以及（iv）社区呼吁进一步行动。
</details></li>
</ul>
<hr>
<h2 id="When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks"><a href="#When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks" class="headerlink" title="When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"></a>When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08993">http://arxiv.org/abs/2311.08993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li<br>for: 这篇论文旨在探讨大语言模型（LLM）在信息抽取任务中的限制，以及这些限制的下发原因。methods: 作者通过对18种特定任务进行了广泛的实验，并发现了三种主要的原因：无法具体理解上下文、任务架构理解与人类不协调、以及长文本理解能力不够。results: 作者发现，通过练习，LLM可以在这些任务中实现不错的表现，这表明ICL的失败不是LLM的内在缺陷，而是现有的配置方法不能够处理复杂的规范任务。<details>
<summary>Abstract</summary>
In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.
</details>
<details>
<summary>摘要</summary>
具有大型自然语言模型（LLM）的启发式学习（ICL）已经成为现代自然语言处理（NLP）中默认的方法，因此探索ICL的限制和理解其下面的原因变得非常重要。在这篇论文中，我们发现ICL在需要较多时间和精力进行学习的任务上表现不佳，例如传统的信息抽取任务。ICL的性能在这些任务上通常无法达到state-of-the-art的results的一半。为了探究这一问题的原因，我们在18个需要较多时间和精力进行学习的任务上进行了广泛的实验，并确定了三个主要原因：无法准确理解上下文，人类任务架构与模型的理解不一致，以及模型对长文本的理解能力不够。此外，我们发现通过 fine-tuning，LLMs可以在这些任务上实现 descent performance，这表明ICL的失败不是LLMs的内在缺陷，而是现有的对齐方法的缺陷，使得LLMs无法通过ICL处理复杂的specification-heavy任务。为了证明这一点，我们在LLMs上进行了专门的指令调整，并观察到了明显的改善。我们希望这些分析可以促进对齐方法的进步，使得LLMs能够更好地满足人类的需求。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems"><a href="#Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems" class="headerlink" title="Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems"></a>Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08987">http://arxiv.org/abs/2311.08987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Marie Farrell, Matt Luckcuck, Mario Gleirscher, Maike Schwammberger</li>
<li>for: 本研讨会论文主要关注形式方法在自动化系统领域的应用。</li>
<li>methods: 本研讨会使用了多种形式方法，包括模拟、分析和验证。</li>
<li>results: 本研讨会的结果表明，通过使用形式方法，可以提高自动化系统的可靠性和安全性。<details>
<summary>Abstract</summary>
This EPTCS volume contains the proceedings for the Fifth International Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th International Conference on integrated Formal Methods (iFM) (iFM'22), organised by Leiden Institute of Advanced Computer Science of Leiden University. The workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket factory alongside the canal.   FMAS 2023 received 25 submissions. We received 11 regular papers, 3 experience reports, 6 research previews, and 5 vision papers. The researchers who submitted papers to FMAS 2023 were from institutions in: Australia, Canada, Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United Kingdom, and the United States of America. Increasing our number of submissions for the third year in a row is an encouraging sign that FMAS has established itself as a reputable publication venue for research on the formal modelling and verification of autonomous systems. After each paper was reviewed by three members of our Programme Committee we accepted a total of 15 papers: 8 long papers and 7 short papers.
</details>
<details>
<summary>摘要</summary>
这本 EPTCS 卷包含第五届国际形式方法工作坊（FMAS 2023）的会议论文，该会议于2023年11月15日-16日在荷兰列日大学计算机科学研究所举行。FMAS 2023 与第18届国际集成形式方法会议（iFM）（iFM'22）共同举办，该会议由列日大学计算机科学研究所组织。会议本身在列日市的一座19世纪 renovated 的布匹厂 alongside 运河举行。 FMAS 2023 接受了 25 篇提交的论文，其中包括 11 篇正式论文、3 篇经验报告、6 篇研究预览和 5 篇视野论文。参加者们来自：澳大利亚、加拿大、哥伦比亚、法国、德国、爱尔兰、意大利、荷兰、瑞典、英国和美国。这一年度的提交数量超过了前一年的同期，这是一个鼓舞人心的迹象，表明 FMAS 已经成为自动化系统的形式模型化和验证的可靠出版物。在三位程序委员会成员的审核后，我们一共接受了 15 篇论文：8 篇长篇论文和 7 篇短篇论文。
</details></li>
</ul>
<hr>
<h2 id="Linear-time-Evidence-Accumulation-Clustering-with-KMeans"><a href="#Linear-time-Evidence-Accumulation-Clustering-with-KMeans" class="headerlink" title="Linear time Evidence Accumulation Clustering with KMeans"></a>Linear time Evidence Accumulation Clustering with KMeans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09272">http://arxiv.org/abs/2311.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Candel</li>
<li>for: 这篇论文的目的是提出一种基于证据积累 clustering 的方法，以解决 ensemble clustering 中的计算问题。</li>
<li>methods: 该方法使用 CA 矩阵表示对Item之间的协 clustering频率，然后使用这个矩阵进行 clustering，以提取共识群集。与其他方法不同的是，这里不需要找到匹配于两个不同的分配中的集群。但是，该方法受到计算问题的限制，需要计算和存储一个 n x n 矩阵，其中 n 是 Item 的数量。</li>
<li>results: 该 paper 提出了一种简单的算法来计算粒度，从而降低了计算复杂性的问题。此外，paper 还证明了 k-means 自然地最大化了粒度。通过对多个 benchmark 数据集的实验， authors 发现了 k-means 和 bisecting 版本与其他状态之前的consensus算法相比，其结果与最佳状态之前的 NMI 相似，同时计算复杂性低。此外，k-means 还在粒度方面取得了最佳结果。这些结果表明，consensus clustering 可以使用简单的算法解决。<details>
<summary>Abstract</summary>
Among ensemble clustering methods, Evidence Accumulation Clustering is one of the simplest technics. In this approach, a co-association (CA) matrix representing the co-clustering frequency is built and then clustered to extract consensus clusters. Compared to other approaches, this one is simple as there is no need to find matches between clusters obtained from two different partitionings. Nevertheless, this method suffers from computational issues, as it requires to compute and store a matrix of size n x n, where n is the number of items. Due to the quadratic cost, this approach is reserved for small datasets. This work describes a trick which mimic the behavior of average linkage clustering. We found a way of computing efficiently the density of a partitioning, reducing the cost from a quadratic to linear complexity. Additionally, we proved that the k-means maximizes naturally the density. We performed experiments on several benchmark datasets where we compared the k-means and the bisecting version to other state-of-the-art consensus algorithms. The k-means results are comparable to the best state of the art in terms of NMI while keeping the computational cost low. Additionally, the k-means led to the best results in terms of density. These results provide evidence that consensus clustering can be solved with simple algorithms.
</details>
<details>
<summary>摘要</summary>
中 ensemble  clustering 方法中，证据积累 clustering 是其中一种最简单的方法。在这种方法中，建立一个 co-association（CA）矩阵，表示item 之间的协 clustering 频率，然后使用这个矩阵进行 clustering，以提取consensus 集群。与其他方法相比，这种方法更简单，因为无需在两个不同的分割结果中寻找匹配。然而，这种方法受到计算问题的限制，因为需要计算和存储一个 n x n 矩阵，其中 n 是物品的数量。由于 quadratic cost，这种方法只适用于小规模数据集。这篇文章描述了一种技巧，可以模拟average linkage clustering的行为。我们发现了一种计算效率的方法，可以减少计算复杂性从 quadratic 到 linear。此外，我们证明了 k-means 自然地 maximizes 分 partitions 的 densities。我们在一些标准 benchmark 数据集上进行了实验，与其他 state-of-the-art consensus 算法进行比较。k-means 结果与最佳 state of the art 相当，而且计算成本较低。此外，k-means 导致了最佳的结果，这些结果提供了证据，证明了 consensus clustering 可以使用简单的算法解决。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linear-Relational-Concepts-in-Large-Language-Models"><a href="#Identifying-Linear-Relational-Concepts-in-Large-Language-Models" class="headerlink" title="Identifying Linear Relational Concepts in Large Language Models"></a>Identifying Linear Relational Concepts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08968">http://arxiv.org/abs/2311.08968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: David Chanin, Anthony Hunter, Oana-Maria Camburu</li>
<li>for: 这个论文的目的是找出 trasformer 语言模型（LM）中隐藏层的概念方向。</li>
<li>methods: 这个论文使用了线性关系模型（LRE）来模型主题和 объек的关系，并在隐藏层使用反向LRE来找出概念方向。</li>
<li>results: 这个方法可以有效地找出概念方向，并且可以作为分类器和模型输出 causal influence。<details>
<summary>Abstract</summary>
Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any given human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts at a given hidden layer in a transformer LM by first modeling the relation between subject and object as a linear relational embedding (LRE). While the LRE work was mainly presented as an exercise in understanding model representations, we find that inverting the LRE while using earlier object layers results in a powerful technique to find concept directions that both work well as a classifier and causally influence model outputs.
</details>
<details>
<summary>摘要</summary>
transformer语言模型（LM）已经显示了概念可以视为隐藏层活动空间中的方向。但是，为某个人类可解释的概念，如何在隐藏层中找到其方向呢？我们提出了一种技术 called直方关系概念（LRC），可以在 трансформа器LM中找到对应于人类可解释的概念方向。我们首先使用对象层的早期层来模型主题和对象之间的线性关系嵌入（LRE）。虽然LRE的工作主要是作为理解模型表示的一种实践，但我们发现，对LRE的逆转，使用早期对象层可以获得一种强大的技术，可以作为分类器并且在模型输出中引起影响。
</details></li>
</ul>
<hr>
<h2 id="I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots"><a href="#I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots" class="headerlink" title="I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots"></a>I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08957">http://arxiv.org/abs/2311.08957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Antonio Abbo, Tony Belpaeme</li>
<li>for: 这篇论文旨在探讨如何通过 integrate 视觉功能，使 conversational agent 更加Context-aware。</li>
<li>methods: 该论文使用最新的 Large Language Models (例如 GPT-4、IDEFICS) 将文本提示和实时视觉输入 interpreted 为 conversational agent 的响应。</li>
<li>results: 六次与 Furhat 机器人的交互， illustrate 了该系统的效果。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of human-computer interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents an initial implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, ensures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. By implementing this vision-enabled dialogue system, the paper envisions a future where conversational agents seamlessly blend textual and visual modalities, enabling richer, more context-aware dialogues.
</details>
<details>
<summary>摘要</summary>
在人机交互方面的快速演化中，将视觉能力集成到对话代理人中是一项关键的进步。这篇论文介绍了一个使用最新的大语言模型（如GPT-4、IDEFICS）来增强传统的文本基于提示的对话管理器。这些语言模型能够同时解读文本提示和视觉刺激，创造更Contextually aware的对话代理人。系统的提示工程，包括对话和图像摘要，保证了对上下文的保持和计算效率的平衡。报告了六次与 Furhat 机器人运行这个系统的交互，并讨论了所获得的结果。通过实施这种视觉启发对话系统，论文预测将来，对话代理人将自然地融合文本和视觉模式，实现更加 ricjer、Contextually aware的对话。
</details></li>
</ul>
<hr>
<h2 id="Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control"><a href="#Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control" class="headerlink" title="Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control"></a>Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08943">http://arxiv.org/abs/2311.08943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerianne L. Hobbs, Bernard Li</li>
<li>for: 这篇论文主要用于探讨人工智能在航空系统控制中的安全、信任和伦理性问题。</li>
<li>methods: 论文使用了各种方法来描述安全、信任和伦理性的概念，包括在人机合作中的应用。</li>
<li>results: 论文结果表明，安全、信任和伦理性是独立的概念，并且在人机合作中的决策过程中需要考虑这些因素。<details>
<summary>Abstract</summary>
Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making.
</details>
<details>
<summary>摘要</summary>
设计安全、可信、伦理AI可能是实际不可能的;但是设计AI以安全、可信、伦理使用为目标是可能的并是必要的，尤其在安全和任务关键领域如航空系统控制中。安全、可信、伦理的使用可能会相互独立，例如：一个系统可能是安全的，但不是可信的或伦理的；一个系统可能是可信的，但不是安全的或伦理的；一个系统可能是伦理的，但不是安全的或可信的。这篇论文旨在突出这些概念之间的细腻差异，特别是在人机合作在航空系统控制中的应用。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-over-Description-Logic-based-Contexts-with-Transformers"><a href="#Reasoning-over-Description-Logic-based-Contexts-with-Transformers" class="headerlink" title="Reasoning over Description Logic-based Contexts with Transformers"></a>Reasoning over Description Logic-based Contexts with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08941">http://arxiv.org/abs/2311.08941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</li>
<li>for: 这篇论文的目的是测试transformer模型在复杂的上下文中的理解能力。</li>
<li>methods: 这篇论文使用了生成自然语言问答dataset，来测试模型在不同的理解深度和句子长度下的表现。</li>
<li>results: 研究发现，使用DeBERTa模型DELTA$_M$，其理解能力随着理解深度的增加，却不受句子长度的影响，并且在未在训练中看到的理解深度上也能够展现出良好的适应能力。<details>
<summary>Abstract</summary>
One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that the performance of our DeBERTa-based model, DELTA$_M$, is marginally affected when the reasoning depth is increased and it is not affected at all when the length of the sentences is increasing. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.
</details>
<details>
<summary>摘要</summary>
Currently, the state-of-the-art measure of the reasoning ability of transformer-based models is their accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of these contexts are very simple and are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we aim to answer the question of how well a transformer-based model will perform reasoning over expressive contexts. To achieve this, we construct a synthetic natural language question-answering dataset generated by description logic knowledge bases. We use the expressive language $\mathcal{ALCQ}$ to generate the knowledge bases, resulting in a dataset containing 384K examples that increase in two dimensions: i) reasoning depth, and ii) sentence length. Our DeBERTa-based model, DELTA$_M$, shows marginally affected performance when the reasoning depth is increased and is not affected at all when the sentence length increases. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.Note: Simplified Chinese is used here as the target language, which is a standardized form of Chinese that is widely used in mainland China and Singapore. The translation may vary slightly depending on the specific dialect or regional variation.
</details></li>
</ul>
<hr>
<h2 id="Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning"><a href="#Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning" class="headerlink" title="Supported Trust Region Optimization for Offline Reinforcement Learning"></a>Supported Trust Region Optimization for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08935">http://arxiv.org/abs/2311.08935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, Xiangyang Ji</li>
<li>for:  solves the out-of-distribution and extrapolation issues in offline reinforcement learning</li>
<li>methods:  uses Supported Trust Region optimization (STR) with policy constraint within the support of the behavior policy</li>
<li>results:  guarantees strict policy improvement and safe exploration in the dataset, with state-of-the-art performance in MuJoCo locomotion domains and more challenging AntMaze domains.Here’s the translation of the abstract in Simplified Chinese:</li>
<li>for: 解决了线上强化学习中的出版物问题和推导错误</li>
<li>methods: 使用支持信任区优化(STR)，将策略限制在行为策略的支持内</li>
<li>results: 在假设无抽象和抽取误差时，STR确保策略改进直至 converges to the optimal support-constrained policy in the dataset，并且在实际结果中证明了 STR 在 MuJoCo 游戏域和更加挑战的 AntMaze 域中的state-of-the-art表现。<details>
<summary>Abstract</summary>
Offline reinforcement learning suffers from the out-of-distribution issue and extrapolation error. Most policy constraint methods regularize the density of the trained policy towards the behavior policy, which is too restrictive in most cases. We propose Supported Trust Region optimization (STR) which performs trust region policy optimization with the policy constrained within the support of the behavior policy, enjoying the less restrictive support constraint. We show that, when assuming no approximation and sampling error, STR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Further with both errors incorporated, STR still guarantees safe policy improvement for each step. Empirical results validate the theory of STR and demonstrate its state-of-the-art performance on MuJoCo locomotion domains and much more challenging AntMaze domains.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate the given text into Simplified Chinese.<</SYS>>在线执行学习受到异常分布问题和推导错误的影响。大多数策略限制方法将训练政策的摩尔分布正则化到行为策略中，这对于大多数情况是太Restrictive。我们提议支持信任区间优化（STR），它通过在行为策略支持下进行信任区间政策优化，享有较少的支持约束。我们证明，假设没有 aproximation和抽样误差，STR 可以保证每步 strict 政策改进，直到 converge 到数据集中的最佳支持约束政策。进一步，包括两种误差，STR 仍然可以保证每步安全的政策改进。 empirical 结果证明 STR 的理论和 AntMaze 领域的实验结果 validate  STR 的性能。Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery"><a href="#Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery" class="headerlink" title="Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery"></a>Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08923">http://arxiv.org/abs/2311.08923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Timo T. Stomberg, Ribana Roscher</li>
<li>for: 提高自然保护区域的定义和监测（improving the definition and monitoring of natural protected areas）</li>
<li>methods: 使用活化增强和生成对抗模型（using activation maximization and a generative adversarial model）</li>
<li>results: 生成更精准的归属图，提高了对自然保护区域的理解（generating more precise attribution maps, improving understanding of natural protected areas）<details>
<summary>Abstract</summary>
Natural protected areas are vital for biodiversity, climate change mitigation, and supporting ecological processes. Despite their significance, comprehensive mapping is hindered by a lack of understanding of their characteristics and a missing land cover class definition. This paper aims to advance the explanation of the designating patterns forming protected and wild areas. To this end, we propose a novel framework that uses activation maximization and a generative adversarial model. With this, we aim to generate satellite images that, in combination with domain knowledge, are capable of offering complete and valid explanations for the spatial and spectral patterns that define the natural authenticity of these regions. Our proposed framework produces more precise attribution maps pinpointing the designating patterns forming the natural authenticity of protected areas. Our approach fosters our understanding of the ecological integrity of the protected natural areas and may contribute to future monitoring and preservation efforts.
</details>
<details>
<summary>摘要</summary>
自然保护区是生物多样性、气候变化缓解和生态过程支持的重要组成部分。尽管它们的重要性，但抗拒保护区的完整地域覆盖缺乏了理解其特点和缺乏地域覆盖类别定义。本文旨在提高保护区的定义模式的解释。为此，我们提出了一种新的框架，使用活动最大化和生成对抗模型。通过这种方法，我们可以生成具有完整和有效解释功能的卫星图像，与领域知识结合，以描述保护区的空间和spectral特征，并帮助我们更好地理解保护区的生态完整性。我们的提议的框架可以生成更精确的归属地图， pinpointing保护区的定义模式，从而提高我们对保护区的理解，并可能对未来监测和保护做出贡献。
</details></li>
</ul>
<hr>
<h2 id="An-Empathetic-User-Centric-Chatbot-for-Emotional-Support"><a href="#An-Empathetic-User-Centric-Chatbot-for-Emotional-Support" class="headerlink" title="An Empathetic User-Centric Chatbot for Emotional Support"></a>An Empathetic User-Centric Chatbot for Emotional Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09271">http://arxiv.org/abs/2311.09271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanting Pan, Yixuan Tang, Yuchen Niu</li>
<li>for: 这篇论文探讨了俄罗契文化和人工智能之间的交叉点，尤其是游戏如何为年轻女性提供情感需求的满足。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）技术来扩展传统的静止游戏剧本，创造出动态、情感响应的互动体验。</li>
<li>results: 我们在《快乐弹指南》游戏中实现了一个基于问答（QA）系统的情感伴侣虚拟助手，通过数据扩展和情感增强技术来提供真实和支持的伴侣式互动。<details>
<summary>Abstract</summary>
This paper explores the intersection of Otome Culture and artificial intelligence, particularly focusing on how Otome-oriented games fulfill the emotional needs of young women. These games, which are deeply rooted in a subcultural understanding of love, provide players with feelings of satisfaction, companionship, and protection through carefully crafted narrative structures and character development. With the proliferation of Large Language Models (LLMs), there is an opportunity to transcend traditional static game narratives and create dynamic, emotionally responsive interactions. We present a case study of Tears of Themis, where we have integrated LLM technology to enhance the interactive experience. Our approach involves augmenting existing game narratives with a Question and Answer (QA) system, enriched through data augmentation and emotional enhancement techniques, resulting in a chatbot that offers realistic and supportive companionship.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了乙女文化和人工智能的交叉点，尤其关注乙女游戏如何为年轻女性提供情感需求的满足。这些游戏，它们深受乙女文化下的爱情观念影响，为玩家提供满足、伴侣和保护的感受，通过优化的narraative结构和人物发展。随着大语言模型（LLM）的普及，有机会超越传统的静止游戏剧本，创造动态、情感响应的互动。我们在《战泪天命》案例研究中，通过 интеGRATE LLM技术，增强了交互体验。我们的方法包括在现有游戏剧本中添加问答（QA）系统，通过数据增强和情感增强技术，创造出真实且支持的伴侣 chatbot。
</details></li>
</ul>
<hr>
<h2 id="NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios"><a href="#NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios" class="headerlink" title="NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios"></a>NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09269">http://arxiv.org/abs/2311.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuttlet/normnet">https://github.com/shuttlet/normnet</a></li>
<li>paper_authors: En-Te Lin, Wei-Jie Lv, Ding-Tao Huang, Long Zeng</li>
<li>for: 提高排序场景中对象Scale的OBE准确性</li>
<li>methods: 提出了一种新的6DoF OPE网络（NormNet），通过点对点回归学习对象的尺度，然后通过Semantic segmentation和Affine变换将所有对象标准化到同一个尺度，最后通过共享的 pose estimator 来回归对象的6D姿态。</li>
<li>results: 通过实验表明，提出的方法可以在公共 benchmark 和MultiScale 数据集上达到领先的性能，并在实际场景中Robust 地估计对象的6D姿态。Here’s the English version of the summary for reference:</li>
<li>for: Improving the accuracy of object pose estimation in stacked scenarios</li>
<li>methods: Proposes a new 6DoF OPE network (NormNet) that learns object scales using point-wise regression, normalizes all objects to the same scale through semantic segmentation and affine transformation, and uses a shared pose estimator to recover the 6D poses.</li>
<li>results: Extensive experiments show that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset, and is robust to changes in object scale in real-world scenarios.<details>
<summary>Abstract</summary>
Existing Object Pose Estimation (OPE) methods for stacked scenarios are not robust to changes in object scale. This paper proposes a new 6DoF OPE network (NormNet) for different scale objects in stacked scenarios. Specifically, each object's scale is first learned with point-wise regression. Then, all objects in the stacked scenario are normalized into the same scale through semantic segmentation and affine transformation. Finally, they are fed into a shared pose estimator to recover their 6D poses. In addition, we introduce a new Sim-to-Real transfer pipeline, combining style transfer and domain randomization. This improves the NormNet's performance on real data even if we only train it on synthetic data. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset we constructed. The real-world experiments show that our method can robustly estimate the 6D pose of objects at different scales.
</details>
<details>
<summary>摘要</summary>
现有的排序场景中对象姿态估算（OPE）方法不具有对象比例变化的Robustness。本文提出了一种新的6DoF OPE网络（NormNet），用于不同比例的object在排序场景中估算6D姿态。具体来说，每个object的比例首先通过点对点回归学习。然后，所有object在排序场景中都被Semantic segmentation和Affine transformation正规化到同一个比例。最后，它们被 fed into a shared pose estimator来回归其6D姿态。此外，我们还介绍了一种新的Sim-to-Real传输管道，其 combining style transfer和Domain randomization。这使得NormNet在实际数据上表现出state-of-the-art的性能，即使只有在synthetic数据上进行了训练。广泛的实验表明，提议的方法在公共benchmark和我们自己构建的MultiScale dataset上 achieve state-of-the-art的性能。实际世界中的实验表明，我们的方法可以对不同比例的object进行Robustly的6D姿态估算。
</details></li>
</ul>
<hr>
<h2 id="Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering"><a href="#Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering" class="headerlink" title="Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering"></a>Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08894">http://arxiv.org/abs/2311.08894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayur Patidar, Avinash Singh, Riya Sawhney, Indrajit Bhattacharya, Mausam</li>
<li>for: 本研究旨在解决知识库问答（KBQA）问题中的零例转移学习 Setting，其中有大量源频率的标注训练数据，但target频率没有任何标注示例。</li>
<li>methods: 本研究使用了大量的无标注数据，并将source频率的标注数据与target频率的无标注数据结合使用，以实现零例转移学习。此外，我们还提出了基于黑板大语言模型（BLLM）的执行引导自我修正技术，以解耦转移学习和启用预训练模型。</li>
<li>results: 我们通过使用GrailQA作为源频率和WebQSP作为目标频率的实验，显示了我们的方法可以在两个阶段中提高性能，并且超越了现有的超参数化KBQA模型。此外，我们还发现在具有有限量的标注数据时，我们的方法可以在预训练模型的基础上具有显著的改进效果。<details>
<summary>Abstract</summary>
We address the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain. Transfer learning for KBQA makes use of large volumes of unlabeled data in the target in addition to the labeled data in the source. More recently, few-shot in-context learning using Black-box Large Language Models (BLLMs) has been adapted for KBQA without considering any source domain data. In this work, we show how to meaningfully combine these two paradigms for KBQA so that their benefits add up. Specifically, we preserve the two stage retrieve-then-generate pipeline of supervised KBQA and introduce interaction between in-context learning using BLLMs and transfer learning from the source for both stages. In addition, we propose execution-guided self-refinement using BLLMs, decoupled from the transfer setting. With the help of experiments using benchmark datasets GrailQA as the source and WebQSP as the target, we show that the proposed combination brings significant improvements to both stages and also outperforms by a large margin state-of-the-art supervised KBQA models trained on the source. We also show that in the in-domain setting, the proposed BLLM augmentation significantly outperforms state-of-the-art supervised models, when the volume of labeled data is limited, and also outperforms these marginally even when using the entire large training dataset.
</details>
<details>
<summary>摘要</summary>
我们处理零组转移学习设定的知识库问题回答（KBQA）问题，其中有大量的预训练数据可用于源领域，但没有类似的预训练例子可用于目标领域。KBQA的转移学习使用目标领域的大量无标数据，以及源领域的预训练数据。在这个工作中，我们示出如何具体地结合这两个概念，以让它们的优点相互补充。具体来说，我们保留KBQA的两阶段检索-生成架构，并将在这两阶段中使用黑盒大型自然语言模型（BLLM）进行交互式学习。此外，我们提出了基于执行的自适应更新，使用BLLM进行执行指导自适应。通过使用 GrailQA 作为源和 WebQSP 作为目标的实验，我们显示了我们的提案可以在两个阶段中提供重要的改进，并且也超越了现有的预训练KBQA模型。此外，我们显示在内领域中，我们的 BLLM 增强可以在有限量的预训练数据情况下提供明显的改进，并且甚至在使用整个大型训练数据时也能够超越。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-ACL2-Proof-Debugging-Tools"><a href="#Advances-in-ACL2-Proof-Debugging-Tools" class="headerlink" title="Advances in ACL2 Proof Debugging Tools"></a>Advances in ACL2 Proof Debugging Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08856">http://arxiv.org/abs/2311.08856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Kaufmann, J Strother Moore</li>
<li>for: 本文主要针对ACL2用户的失败证明尝试进行分析和调试。</li>
<li>methods: 本文使用了ACL2版本8.5后的改进的break-rewrite工具和新的with-brr-data工具来调试失败证明。</li>
<li>results: 本文通过分析失败证明的原因和使用debug工具，帮助用户更好地使用ACL2证明工具。<details>
<summary>Abstract</summary>
The experience of an ACL2 user generally includes many failed proof attempts. A key to successful use of the ACL2 prover is the effective use of tools to debug those failures. We focus on changes made after ACL2 Version 8.5: the improved break-rewrite utility and the new utility, with-brr-data.
</details>
<details>
<summary>摘要</summary>
ACL2 用户通常会经历许多失败的证明尝试。为使用 ACL2 证明工具成功，需要有效地使用工具来调试失败。我们将关注 ACL2 Version 8.5 之后的更改：改进的 break-rewrite 工具和新的 with-brr-data 工具。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English"><a href="#Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English" class="headerlink" title="Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English"></a>Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08836">http://arxiv.org/abs/2311.08836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary</li>
<li>for: 本研究旨在提供一个用于评估 gender bias 在 Machine Translation (MT) 中的benchmark，以及一种用于 Mitigation Strategies 的评估方法。</li>
<li>methods: 本研究使用了 GATE X-E 数据集，该数据集包含了从土耳其语、匈牙利语、芬兰语和波斯语到英语的人工翻译，每个翻译都有 feminine、masculine 和neutral 的变体。此外，研究还使用了 GPT-3.5 Turbo 构建的英语性别重写解决方案，用于评估 gender debiasing。</li>
<li>results: 研究发现，使用 GATE X-E 数据集和 GPT-3.5 Turbo 解决方案可以减少 Machine Translation 中的gender bias。同时，研究还发现了一些 linguistic phenomena 在翻译 rewrite 过程中的挑战。<details>
<summary>Abstract</summary>
Machine Translation (MT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies into gender bias in translations from gender-neutral languages such as Turkish into more strongly gendered languages like English, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants for each possible gender interpretation. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present an English gender rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems"><a href="#A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems" class="headerlink" title="A* search algorithm for an optimal investment problem in vehicle-sharing systems"></a>A* search algorithm for an optimal investment problem in vehicle-sharing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08834">http://arxiv.org/abs/2311.08834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ba Luat Le, Layla Martin, Emrah Demir, Duc Minh Vu</li>
<li>for: 本研究探讨了一个最佳投资问题，它在汽车共享系统中出现。给定一个站点建设的集合，我们需要确定（1）站点建设的顺序和车辆数量来实现所有站点建设完毕，以及（2）车辆数量和分配来最大化共享系统运营时间内的总收益。</li>
<li>methods: 本研究使用了A<em>搜索算法来解决这个问题。A</em>算法是一种基于搜索的方法，可以快速地找到最佳解。</li>
<li>results: 计算实验表明，使用A<em>算法可以比使用常见的迪克斯特拉算法更快地解决这个问题，并且可以更好地寻找最佳解。未来的研究可以探讨新的可能性和应用，以及精确和近似A</em>算法的比较。<details>
<summary>Abstract</summary>
We study an optimal investment problem that arises in the context of the vehicle-sharing system. Given a set of locations to build stations, we need to determine i) the sequence of stations to be built and the number of vehicles to acquire in order to obtain the target state where all stations are built, and ii) the number of vehicles to acquire and their allocation in order to maximize the total profit returned by operating the system when some or all stations are open. The profitability associated with operating open stations, measured over a specific time period, is represented as a linear optimization problem applied to a collection of open stations. With operating capital, the owner of the system can open new stations. This property introduces a set-dependent aspect to the duration required for opening a new station, and the optimal investment problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with set-dependent cost. We propose an A* search algorithm to address this particular variant of the TSP. Computational experiments highlight the benefits of the proposed algorithm in comparison to the widely recognized Dijkstra algorithm and propose future research to explore new possibilities and applications for both exact and approximate A* algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个优化投资问题，发生在汽车共享系统的 контексте。给定一组位置建站，我们需要确定：一、站点建设顺序和购买汽车数量，以实现所有站点建成的目标状态，二、购买汽车数量和分配方式，以最大化在一些或所有站点打开时间内的总收益。系统在运行时的收益，通过一个时间段内的线性优化问题表示，对一组打开的站点进行优化。有投资资金，系统所有者可以开新站点。这种财务特性会导致开新站点所需的时间取决于集合，并且优化投资问题可以视为一种与集合相互作用的旅馆销售人问题（TSP）的变种。我们提议使用A*搜索算法解决这个特定的TSP变种。计算实验表明，我们提议的算法在与广泛认可的迪金斯特拉算法进行比较时，具有显著的优势。未来研究可以探讨新的可能性和应用，以及精度和近似A*算法的探索。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration"><a href="#Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration" class="headerlink" title="Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration"></a>Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08832">http://arxiv.org/abs/2311.08832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Céline Mougenot</li>
<li>for: 这篇论文旨在探讨对话代理（CA）的创造和应用中存在的社会技术挑战。</li>
<li>methods: 本文使用一种分析方法，即通过对CA创造过程中的多种社会技术挑战进行分类和描述，并提出了一些实践方法来超越这些挑战。</li>
<li>results: 本文提出了一个基于多元合作（IDC）的CA设计挑战分类体系，并提出了一些实践方法来超越这些挑战。未来的研究可以通过实践 verify 这些概念链接和在CA设计中应用这些方法来评估其效果。<details>
<summary>Abstract</summary>
Recent years have seen a steady rise in the popularity and use of Conversational Agents (CA) for different applications, well before the more immediate impact of large language models. This rise has been accompanied by an extensive exploration and documentation of the challenges of designing and creating conversational agents. Focusing on a recent scoping review of the socio-technical challenges of CA creation, this opinion paper calls for an examination of the extent to which interdisciplinary collaboration (IDC) challenges might contribute towards socio-technical CA design challenges. The paper proposes a taxonomy of CA design challenges using IDC as a lens, and proposes practical strategies to overcome them which complement existing design principles. The paper invites future work to empirically verify suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
近年来，对话代理（CA）在不同应用领域的 популярность和使用量有了明显的增长趋势，这与大语言模型的更 immediate影响之前。这种增长被陪伴了详细的探索和文献记录，关于对话代理的设计和创造的挑战。本 opinio paper 呼吁对 CA 设计挑战的评估，特别是通过交叉学科合作（IDC）的视角来评估这些挑战。文章提出了 CA 设计挑战的税onomy，并提供了实践的措施来超越这些挑战，这些措施与现有的设计原则相 complement。文章邀请未来的工作来实证提出的概念链接和应用提议的方法，以评估其效果。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering"><a href="#Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering" class="headerlink" title="Reinforcement Learning with Model Predictive Control for Highway Ramp Metering"></a>Reinforcement Learning with Model Predictive Control for Highway Ramp Metering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08820">http://arxiv.org/abs/2311.08820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filippoairaldi/mpcrl-for-ramp-metering">https://github.com/filippoairaldi/mpcrl-for-ramp-metering</a></li>
<li>paper_authors: Filippo Airaldi, Bart De Schutter, Azita Dabiri</li>
<li>for: 提高城市和高速公路交通系统的效率</li>
<li>methods: 结合模型驱动和学习驱动的方法，通过嵌入人工智能技术来提高高速公路上的汽车流控制</li>
<li>results: 实验表明，通过使用提出的方法，可以从一个不精确的模型和不良调节的控制器中学习改进控制策略，从而减轻网络中的拥堵和满足安全限制，提高交通效率。<details>
<summary>Abstract</summary>
In the backdrop of an increasingly pressing need for effective urban and highway transportation systems, this work explores the synergy between model-based and learning-based strategies to enhance traffic flow management by use of an innovative approach to the problem of highway ramp metering control that embeds Reinforcement Learning techniques within the Model Predictive Control framework. The control problem is formulated as an RL task by crafting a suitable stage cost function that is representative of the traffic conditions, variability in the control action, and violations of a safety-critical constraint on the maximum number of vehicles in queue. An MPC-based RL approach, which merges the advantages of the two paradigms in order to overcome the shortcomings of each framework, is proposed to learn to efficiently control an on-ramp and to satisfy its constraints despite uncertainties in the system model and variable demands. Finally, simulations are performed on a benchmark from the literature consisting of a small-scale highway network. Results show that, starting from an MPC controller that has an imprecise model and is poorly tuned, the proposed methodology is able to effectively learn to improve the control policy such that congestion in the network is reduced and constraints are satisfied, yielding an improved performance compared to the initial controller.
</details>
<details>
<summary>摘要</summary>
在城市和高速公路交通系统中增加压力的背景下，这项工作探讨了模型基于和学习基于策略的协同作用，以提高交通流管理的效果。这里提出一种将再征学习技术embedded在模型预测控制框架中的新 Approach，用于解决高速公路上的加速度控制问题。控制问题被定义为一个RL任务，并通过设计合适的stage cost函数来表示交通条件、控制动作的变化和安全约束的违反。一种MPC-based RL方法，其将两个框架的优点相结合，以超越每个框架的缺点，学习高速公路上的加速度控制策略，并满足系统模型不确定性和变化的需求。最后，通过Literature中的一个小规模高速公路网络的Benchmark进行了 simulations，结果表明，从一个粗略的MPC控制器开始，提出的方法能够有效地学习改善控制策略，使交通堵塞降低，约束满足，与初始控制器相比，性能得到提高。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-based-Dataset-Distillation"><a href="#Frequency-Domain-based-Dataset-Distillation" class="headerlink" title="Frequency Domain-based Dataset Distillation"></a>Frequency Domain-based Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08819">http://arxiv.org/abs/2311.08819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdh0818/fred">https://github.com/sdh0818/fred</a></li>
<li>paper_authors: Donghyeok Shin, Seungjae Shin, Il-Chul Moon</li>
<li>for: 这个研究是为了开发一个新的实现小型实验数据集的方法，利用频域来对大型原始数据集进行抽象。</li>
<li>methods: 这个方法使用频率领域的对称转换来优化各数据实例的频率表现，并根据频率维度的解释能力进行选择，以实现实验数据集的简洁化。</li>
<li>results: 这个方法能够在有限的预算下实现更好的资料简洁化，并且在不同的评估场景下与现有的对抗方法相比，能够提高对抗方法的性能。<details>
<summary>Abstract</summary>
This paper presents FreD, a novel parameterization method for dataset distillation, which utilizes the frequency domain to distill a small-sized synthetic dataset from a large-sized original dataset. Unlike conventional approaches that focus on the spatial domain, FreD employs frequency-based transforms to optimize the frequency representations of each data instance. By leveraging the concentration of spatial domain information on specific frequency components, FreD intelligently selects a subset of frequency dimensions for optimization, leading to a significant reduction in the required budget for synthesizing an instance. Through the selection of frequency dimensions based on the explained variance, FreD demonstrates both theoretical and empirical evidence of its ability to operate efficiently within a limited budget, while better preserving the information of the original dataset compared to conventional parameterization methods. Furthermore, based on the orthogonal compatibility of FreD with existing methods, we confirm that FreD consistently improves the performances of existing distillation methods over the evaluation scenarios with different benchmark datasets. We release the code at https://github.com/sdh0818/FreD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy"><a href="#MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy" class="headerlink" title="MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy"></a>MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08817">http://arxiv.org/abs/2311.08817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Yoshida, Kartik Goyal, Kevin Gimpel</li>
<li>for: 本研究旨在探讨NLG模型中的模式问题，并提出一种 Conditional MAP Decoding 方法来解决这问题。</li>
<li>methods: 本研究使用了 exact-search 和 ACBS 两种方法来找到NLG模型的模式。</li>
<li>results: 研究发现，对NLG模型进行 Conditional MAP Decoding 可以提高模式的质量和有用性，并且可以从不需要模型更新的角度来解决模式问题。<details>
<summary>Abstract</summary>
It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning.
</details>
<details>
<summary>摘要</summary>
历史观察表明，基于自然语言生成（NLG）模型的准确或近似MAP（模式寻找）解oding通常会导致缺乏创新性的输出（Stahlberg和Byrne，2019年，Holtzman等，2019年）。这一问题通常被归结于模型内部的缺陷或语言生成模型的弱点。然而，在本研究中，我们强调的是，模型训练数据中的杂杂音可以导致模型的模式变得缺乏创新性，而不是模型本身的缺陷。具体来说，我们表明了混合一小量低危险噪音到人口文本分布中可以使得数据分布的模式变得缺乏创新性，因此任何基于这种数据分布的模型都将受到影响。由于NLG模型的随机模式通常是缺乏创新性的，我们因此提议在模型的分布上使用MAP解oding，并且在满足特定的非缺乏创新性条件下进行搜索。通过精确搜索，我们证实了机器翻译模型和语言模型的长度 Conditional 模式更加流利和有主题，而其不conditional模式则更加缺乏创新性。此外，我们还分享了许多准确模式序列示例，包括多种LLaMA-7B模型的变种。尽管LLaMA模型的模式仍然缺乏创新性，这表明改进模型的能力并没有解决这个问题。由于准确模式找索算法的成本高，我们开发了一种近似模式找索算法，即ACBS。我们应用该算法于LLaMA-7B模型，并发现我们可以不需要训练而获得合理的输出。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations"><a href="#Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations" class="headerlink" title="Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations"></a>Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08815">http://arxiv.org/abs/2311.08815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bernhard Schölkopf, Mark Ibrahim</li>
<li>for: 本研究旨在提出一种更原理性的自监表示学习方法，以便在训练时不需要手动排除不必要的特征。</li>
<li>methods: 本研究使用多个风格嵌入空间，其中每个风格嵌入空间是对所有增强except one的增强 invariant的，并且joint entropy是最大化的。</li>
<li>results: 我们通过 sintetic数据和ImageNet数据进行了实验，并证明了我们的方法的效果。<details>
<summary>Abstract</summary>
Self-supervised representation learning often uses data augmentations to induce some invariance to "style" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details>
<details>
<summary>摘要</summary>
自我指导学习经常使用数据增强来induce一些数据"样式"特征的不变性。然而，下游任务通常 unknown at training time，难以在training时 deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.Here's the translation in Traditional Chinese:自我指导学习经常使用数据增强来induce一些数据"型式"特征的不变性。然而，下游任务通常 unknown at training time，难以在training时 deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details></li>
</ul>
<hr>
<h2 id="SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer"><a href="#SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer" class="headerlink" title="SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer"></a>SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08806">http://arxiv.org/abs/2311.08806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Liu, Shanlin Xiao, Bo Li, Zhiyi Yu</li>
<li>for: 本研究旨在提高逻辑吞吐量的神经网络（SNN）模型的精简性和能效性，通过对Spikformer模型进行减少计算复杂性和参数量的优化。</li>
<li>methods: 本研究使用了 Lottery Ticket Hypothesis（LTH）和Token和Weight预选技术来实现SNN模型的精简。具体来说，我们在Spikformer模型中发现了一个非常稀疏（大于90%）的子网络，可以达到相似的性能水平，同时减少计算复杂性和参数量。此外，我们还设计了一个轻量级的 токен选择器模块，可以根据图像中辐射神经元的均值脉冲频率选择重要背景信息，从而减少计算量。</li>
<li>results: 我们的实验结果表明，使用我们的框架可以减少90%的模型参数量，同时降低Giga浮点运算数（GFLOPs）20%，而不会影响原始模型的精度。<details>
<summary>Abstract</summary>
As the third-generation neural network, the Spiking Neural Network (SNN) has the advantages of low power consumption and high energy efficiency, making it suitable for implementation on edge devices. More recently, the most advanced SNN, Spikformer, combines the self-attention module from Transformer with SNN to achieve remarkable performance. However, it adopts larger channel dimensions in MLP layers, leading to an increased number of redundant model parameters. To effectively decrease the computational complexity and weight parameters of the model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very sparse ($\ge$90%) subnetwork that achieves comparable performance to the original network. Furthermore, we also design a lightweight token selector module, which can remove unimportant background information from images based on the average spike firing rate of neurons, selecting only essential foreground image tokens to participate in attention calculation. Based on that, we present SparseSpikformer, a co-design framework aimed at achieving sparsity in Spikformer through token and weight pruning techniques. Experimental results demonstrate that our framework can significantly reduce 90% model parameters and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining the accuracy of the original model.
</details>
<details>
<summary>摘要</summary>
为了实现适量学习，我们提出了一种名为SparseSpikformer的共设框架。该框架结合了LTH（奖券猜测）和精简的 neuron 权重来实现Spikformer模型中的精简。我们还设计了一个轻量级的封装器模块，可以根据图像中脉冲发生率的平均值选择重要背景信息，从而减少不必要的计算量和模型参数。实验结果表明，我们的框架可以将模型参数减少90%，同时保持原始模型的准确性。此外，我们还可以降低GFLOPs值20%。
</details></li>
</ul>
<hr>
<h2 id="X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects"><a href="#X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects" class="headerlink" title="X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects"></a>X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08788">http://arxiv.org/abs/2311.08788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang</li>
<li>for: 本研究旨在提高NLG评价中的多方面评价能力，使用X-Eval框架来评估文本质量在多个未知的评价方面。</li>
<li>methods: 本研究使用了两个学习阶段：vanilla instruction tuning阶段和加强的 instruction tuning阶段，以提高模型能够遵循评价指令的能力。同时，研究者还采用了一种数据生成策略，将人工评分笔记转化为多种NLG评价任务，以增加任务多样性。</li>
<li>results: 实验表明，X-Eval框架可以使得even a lightweight语言模型在多个NLG任务中达到与人类评价相似或更高的相关度，比如GPT-4等现有NLG评价器。<details>
<summary>Abstract</summary>
Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it's absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate the text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model's ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that our X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators, such as GPT-4.
</details>
<details>
<summary>摘要</summary>
自然语言生成（NLG）通常需要评估生成的文本在多个方面（例如，一致性和自然性）以获得全面的评估。然而，多个方面评估仍然是一个挑战，因为评估人可能需要总结到任何给定的评估方面，即使在训练中没有出现过。在这篇论文中，我们介绍了X-Eval，一个两个阶段的指令调整框架，用于评估文本在已知和未知方面的质量。X-Eval包括两个学习阶段：一个普通的指令调整阶段，用于提高模型跟踪评估指令的能力，以及一个加强的指令调整阶段，用于更好地评估文本质量。为支持X-Eval的训练，我们收集了AspectInstruct数据集，这是第一个适用于多个方面NLG评估的指令调整数据集，覆盖27种多样化的评估方面，65个任务。为了增加任务多样性，我们设计了一种扩展策略，将人类评分笔记转换成多种NLG评估任务的不同形式，包括分数、比较、排名和布尔值问答。我们在对话生成、概要和数据到文本三个重要类NLG任务中进行了21个方面的meta评估，并证明了X-Eval可以使一个轻量级语言模型与人类评估相比或更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home"><a href="#ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home" class="headerlink" title="ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home"></a>ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08783">http://arxiv.org/abs/2311.08783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sveta Paster, Kantwon Rogers, Gordon Briggs, Peter Stone, Reuth Mirsky</li>
<li>for: 提高老人护理机构内 elderly 的生活质量</li>
<li>methods: 利用智能不遵从框架进行决策过程</li>
<li>results: 提供了一种能够在具有伦理意味的决策过程中进行决策的老人护理机构内机器人Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the well-being of the elderly in elderly care homes by using service robots with the ability to make decisions with ethical implications.</li>
<li>methods: The paper proposes using the Intelligent Disobedience framework to enable the robot to deliberate over decisions with potential ethical implications.</li>
<li>results: The paper provides a solution for elderly care homes to have robots that can make decisions with ethical implications, which can enhance the well-being of the elderly.<details>
<summary>Abstract</summary>
With the projected surge in the elderly population, service robots offer a promising avenue to enhance their well-being in elderly care homes. Such robots will encounter complex scenarios which will require them to perform decisions with ethical consequences. In this report, we propose to leverage the Intelligent Disobedience framework in order to give the robot the ability to perform a deliberation process over decisions with potential ethical implications. We list the issues that this framework can assist with, define it formally in the context of the specific elderly care home scenario, and delineate the requirements for implementing an intelligently disobeying robot. We conclude this report with some critical analysis and suggestions for future work.
</details>
<details>
<summary>摘要</summary>
随着老年人口增长的预计，服务机器人在老年人寓所中提供了一个有前途的方式来提高老年人的生活质量。这些机器人将面临复杂的场景，需要它们在具有伦理意味的决策过程中进行慎重的考虑。在这份报告中，我们建议利用智能不遵守框架，让机器人在具有伦理意味的决策过程中进行慎重的考虑。我们列出了该框架可以帮助解决的问题，在老年人寓所场景中定义了智能不遵守的形式，并详细说明了实施智能不遵守机器人的需求。我们在报告结尾进行了深入的分析和未来工作的建议。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion"><a href="#Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion" class="headerlink" title="Adversarially Robust Spiking Neural Networks Through Conversion"></a>Adversarially Robust Spiking Neural Networks Through Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09266">http://arxiv.org/abs/2311.09266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/igitugraz/robustsnnconversion">https://github.com/igitugraz/robustsnnconversion</a></li>
<li>paper_authors: Ozan Özdenizci, Robert Legenstein</li>
<li>for: 提高深度神经网络（SNN）的抗击力性能，使其在多种应用中更加可靠。</li>
<li>methods: 提出一种可扩展的 Robust SNN 训练方法，通过将 ANN 转换为 SNN 来实现。该方法可以有效地承载多种 computationally demanding 的robust learning objective，并在 post-conversion 精度矫正阶段进行 adversarial 优化。</li>
<li>results: 经过实验证明，该方法可以在多种适应性攻击Setting中保持高效性和低延迟，并在某些情况下提供了纪录级的性能。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in numerous adaptive adversarial settings that account for the spike-based operation dynamics of SNNs, and show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）提供了一种能效的人工神经网络（ANN）的替代方案，用于许多人工智能应用程序。然而，随着SNN在应用领域的进步，SNN的抗击势特性问题变得更加突出。与普遍研究的端到端对抗训练方法不同，我们提出了一种可扩展的稳定robust SNN训练方法。我们的方法可以有效地把一系列计算挑战性的robust学习目标传递到SNN上，并在后续的robust微调阶段通过对层级发射reshold和synaptic连接权重进行对抗优化来保持传递的robust性增强。我们在许多适应性攻击设定下进行实验评估，并显示了我们的方法可以实现低延迟、高稳定性的抗击势深度SNN。
</details></li>
</ul>
<hr>
<h2 id="Three-Conjectures-on-Unexpectedeness"><a href="#Three-Conjectures-on-Unexpectedeness" class="headerlink" title="Three Conjectures on Unexpectedeness"></a>Three Conjectures on Unexpectedeness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08768">http://arxiv.org/abs/2311.08768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Sileno, Jean-Louis Dessalles<br>for:* 这篇论文是关于 simplicity theory 中的不期望性的研究。methods:* 该论文采用了一种基于kolmogorov复杂度的方法来探索不期望性的理论基础。results:* 论文提出了三个理论假设：First,不期望性可以被看作是泛化 bayes 规则; Second, 不期望性的核心可以与世界的趋势跟踪函数相连接; Third,不期望性可以被视为世界 entropy 和观察者系统的多样性之间的一种度量。这些假设提供了一个扩展 probabilistic 和逻辑方法的框架，可能会带来新的理解，以及对 causal 关系的抽取和学习机制的研究。<details>
<summary>Abstract</summary>
Unexpectedness is a central concept in Simplicity Theory, a theory of cognition relating various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details>
<details>
<summary>摘要</summary>
不期待性是简洁理论中的中心概念， relate various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.Here's a word-for-word translation of the text in Traditional Chinese:不期待性是简单理论中的中心概念， relate various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details></li>
</ul>
<hr>
<h2 id="Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning"><a href="#Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning" class="headerlink" title="Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning"></a>Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08764">http://arxiv.org/abs/2311.08764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoshuang Chen, Zhongyi Sun, Ke Yan, Shouhong Ding, Hongtao Lu</li>
<li>for: 本研究旨在解决自适应类增长学习中的突然出现新类的问题，使模型可以适应新类的 sequential 出现，同时避免归类束落。</li>
<li>methods: 我们提出了一种自适应 CIL 框架 CPPF，它包括一个prototype clustering模块 (PC)、一个嵌入空间保留模块 (ESR) 和一个多教师涂抹模块 (MTD)。PC 模块在prototype层级保留了后续阶段的嵌入空间，ESR 模块在嵌入空间级别保留了当前阶段的嵌入空间，以准备未来的知识。MTD 模块保持了当前阶段的表示，避免了过去知识的干扰。</li>
<li>results: 我们在 CIFAR100 和 ImageNet100  datasets 进行了广泛的实验，结果表明，我们的提议方法可以提高自适应类增长学习的性能。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to handle the scenario where data of novel classes occur continuously and sequentially. The model should recognize the sequential novel classes while alleviating the catastrophic forgetting. In the self-supervised manner, it becomes more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address the problem, we propose a self-supervised CIL framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level respectively to prepare for knowledge learned in the future. 2) The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network. Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release code in the near future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>类间逐步学习（CIL）targets the scenario where novel classes emerge continuously and sequentially, and the model should recognize these sequential novel classes while mitigating catastrophic forgetting. In a self-supervised manner, it is more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address this problem, we propose a self-supervised CIL framework called CPPF, which stands for Combining Past, Present, and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR), and a multi-teacher distillation module (MTD).1. The PC and ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level, respectively, to prepare for knowledge learned in the future.2. The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network.Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release the code in the near future.
</details></li>
</ul>
<hr>
<h2 id="Forms-of-Understanding-of-XAI-Explanations"><a href="#Forms-of-Understanding-of-XAI-Explanations" class="headerlink" title="Forms of Understanding of XAI-Explanations"></a>Forms of Understanding of XAI-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08760">http://arxiv.org/abs/2311.08760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede</li>
<li>for: 本文旨在探讨 Explainable Artificial Intelligence (XAI) 领域中的理解概念，并提出一种理解形式模型。</li>
<li>methods: 本文采用了多元视角，兼容计算机科学、语言学、社会学和心理学等领域的方法，探讨了理解的定义和其形式、评价和进程中的动态。</li>
<li>results: 本文提出了两种理解的可能的结果，即启用性（knowing how）和理解（knowing that），两者在不同的深度水平上存在紧密的相互关系。<details>
<summary>Abstract</summary>
Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>文本：Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.翻译：Explainability 已成为计算机科学和人工智能领域的重要话题，导致了一个子领域 called Explainable Artificial Intelligence (XAI)。提供或请求解释的目标是为 explainee  achieve (更好的) '理解'。然而， '理解' 的含义仍然没有得到明确定义，这个概念本身rarely 是科学调查的 Object。本文旨在从多学科角度（计算机科学、语言学、社会学和心理学）出发，对 XAI 和其他领域中的理解形式、评估和过程 dynamics 进行系统化介绍。在给予日常解释的过程中，解释可以从特定领域的浅度理解开始，并可以导致解释和解释对象的深度理解和能力。在这个过程中，理解和能力的增长是高度相互dependent。在这种系统化的背景下，XAI 中特殊的理解挑战被讨论。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response"><a href="#Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response" class="headerlink" title="Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response"></a>Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09264">http://arxiv.org/abs/2311.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Zhai, Hui Liu</li>
<li>for: This paper aims to model the impact of the tumor microenvironment (TME) on clinical drug response.</li>
<li>methods: The authors propose a domain adaptation network for feature disentanglement, using two denoising autoencoders to extract features from cell lines (source domain) and tumors (target domain), and a graph attention network to learn the latent representation of drugs.</li>
<li>results: The model demonstrated superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文目标是模型肿瘤微环境（TME）对临床药物应用的影响。</li>
<li>methods: 作者提出了适应领域网络，通过两个减噪自编器提取Cell Line（源领域）和肿瘤（目标领域）中的特征，以及一个图注意网络学习药物的潜在表示。</li>
<li>results: 模型在临床药物应用预测和分解肿瘤微环境对药效的影响方面表现出色。<details>
<summary>Abstract</summary>
High-throughput screening technology has facilitated the generation of large-scale drug responses across hundreds of cancer cell lines. However, there exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because of tumors comprise of complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details>
<details>
<summary>摘要</summary>
高通量屏测技术已经使得大规模的药物响应数据可以在数百个肿瘤细胞线上生成。然而，有很大的差异 между尸采肿瘤和实际在生体中的肿瘤响应药物治疗，因为肿瘤由复杂的细胞组分和组织结构组成，这些组成部分被称为肿瘤微环境（TME），它对肿瘤细胞的药物毒性产生很大的影响。到目前为止，没有任何研究探讨了在临床药物治疗中模拟TME的影响。这篇文章提出了一种领域适应网络，用于特征分离，以分离肿瘤细胞和TME的表达特征。两个降噪自适应器分别用于从肿瘤细胞线（源领域）和肿瘤（目标领域）提取特征，以实现部分领域对应和特征分离。具体来说，特定的降噪自适应器被要求提取TME中的信息。此外，为确保对新药的普适性，我们应用了图注意网络，以学习药物的潜在表达，以便在干扰空间中线性模型药物对细胞状态的影响。我们在一个标准数据集上调整了我们的模型，并证明了它在预测临床药物响应和分析肿瘤微环境对药物效果的性能是其他方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Auto-ICL-In-Context-Learning-without-Human-Supervision"><a href="#Auto-ICL-In-Context-Learning-without-Human-Supervision" class="headerlink" title="Auto-ICL: In-Context Learning without Human Supervision"></a>Auto-ICL: In-Context Learning without Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecielyang/auto-icl">https://github.com/ecielyang/auto-icl</a></li>
<li>paper_authors: Jinghan Yang, Shuming Ma, Furu Wei</li>
<li>for: 本研究旨在提高人工智能模型在自然语言交互中的能力，使其能够更好地适应不同的任务和场景。</li>
<li>methods: 本研究提出了一种自动启发学习（Automatic In-Context Learning）框架，通过让模型自己生成示例、标签、指导路径等，以便在不同的任务和场景下进行学习和推理。</li>
<li>results: 研究表明，使用自动启发学习框架可以在多种任务上达到强健的性能，与传统的启发学习方法相比，具有更好的适应性和灵活性。<details>
<summary>Abstract</summary>
In the era of Large Language Models (LLMs), human-computer interaction has evolved towards natural language, offering unprecedented flexibility. Despite this, LLMs are heavily reliant on well-structured prompts to function efficiently within the realm of In-Context Learning. Vanilla In-Context Learning relies on human-provided contexts, such as labeled examples, explicit instructions, or other guiding mechanisms that shape the model's outputs. To address this challenge, our study presents a universal framework named Automatic In-Context Learning. Upon receiving a user's request, we ask the model to independently generate examples, including labels, instructions, or reasoning pathways. The model then leverages this self-produced context to tackle the given problem. Our approach is universally adaptable and can be implemented in any setting where vanilla In-Context Learning is applicable. We demonstrate that our method yields strong performance across a range of tasks, standing up well when compared to existing methods.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）时代，人机交互发展向自然语言，提供无 precedent的灵活性。然而，LLMs仍然依赖于良好结构的提示，以达到有效的内容学习。普通的内容学习依赖人类提供的 контекст，如标签示例、显式指令或其他引导机制，以导引模型的输出。为解决这个挑战，我们的研究提出了一种通用框架，名为自动内容学习。当用户提交请求时，我们会让模型独立生成示例，包括标签、指令或推理路径。模型然后可以利用这些自生成的 контекст，解决给定的问题。我们的方法是通用适用的，可以在任何可以使用普通内容学习的场景中实现。我们示出了我们的方法在多种任务上具有强表现，与现有方法相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values"><a href="#Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values" class="headerlink" title="Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values"></a>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09262">http://arxiv.org/abs/2311.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikai Xue, Guoxiu He, Zhuoren Jiang, Yangyang Kang, Star Zhao, Wei Lu</li>
<li>for: 本研究的目的是提出一种新的图 нейрон网络，用于分解论文的潜在影响力（Diffusion, Conformity, and Contribution）。</li>
<li>methods: 该研究使用了一种名为DPPDCC的图神经网络，该网络利用动态多型图来编码时间和结构特征，并且使用比较和引用&#x2F;引用信息来捕捉知识的流动。</li>
<li>results: 实验结果表明，DPPDCC在三个不同的数据集上都有显著的超越性能，特别是对于在不同时间点发表的论文。此外，further analyses也证明了该模型的稳定性和可重复性。<details>
<summary>Abstract</summary>
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available.
</details>
<details>
<summary>摘要</summary>
学术论文的潜在影响因素包括其受欢迎程度和贡献。现有模型通常根据静止图计算原始引用数，而忽略了不同视角的价值。在这项研究中，我们提出了一种新的图神经网络，可以分解论文的潜在影响为扩散、准确性和贡献三个值（DPPDCC）。对目标论文，DPPDCC 编码了时间和结构特征，并强调在构建动态多元图中的知识流动。特别是，通过比较和引用/引用信息的相互作用，我们捕捉了论文之间的知识传递。为了评估论文的受欢迎程度，我们将图像增强了，并使用累积预测法来模型准确性。此外，我们还应用了正交约束，以便分解每个视角的值，并保持论文的本身价值。为了评估模型在不同时间点上的普适性，我们将数据分区成不同的时间点，以逼真实的世界条件。我们的实验结果表明，DPPDCC 在三个 dataset 上显著超过基线。进一步的分析表明，它具有强大的可重复性。我们将我们的数据和代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network"><a href="#Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network" class="headerlink" title="Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network"></a>Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09261">http://arxiv.org/abs/2311.09261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lars-research/emergnn">https://github.com/lars-research/emergnn</a></li>
<li>paper_authors: Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng</li>
<li>for: 预测新药与新药之间的药物相互作用（DDI），以提高病人护理和药物开发效率。</li>
<li>methods: 我们提出了一种基于图神经网络（GNN）的 EmerGNN，可以有效地预测新药DDI，通过利用生物医学网络中药物之间的路径信息。</li>
<li>results: EmerGNN比现有方法更高精度地预测新药DDI，并可以快速寻找最重要的生物医学网络信息。<details>
<summary>Abstract</summary>
Accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.
</details>
<details>
<summary>摘要</summary>
accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the "Simplified Chinese" format, which is the most commonly used format for written Chinese. The translation is accurate and faithful to the original text, but it may not be perfect in terms of nuance or idiomatic expressions.Here are some notes on the translation:* "Drug-drug interactions" (DDI) is translated as "药物交互作用" (yàowù jiāoxì zuòyòu) in Simplified Chinese.* "Emerging drugs" is translated as "新兴药物" (xīn xìng yàowù) in Simplified Chinese.* "Computational methods" is translated as "计算方法" (jìsuàn fāngfa) in Simplified Chinese.* "Biomedical networks" is translated as "生物医学网络" (shēngwù yīxué wǎngluò) in Simplified Chinese.* "Graph neural network" (GNN) is translated as "图解神经网络" (tújiě shénxiàng wǎngluò) in Simplified Chinese.* "Pairwise representations" is translated as "对应表示" (duìpǐng biǎozhì) in Simplified Chinese.* "Propagating information" is translated as "传递信息" (chuánxìn xìngxi) in Simplified Chinese.* "Relevant biomedical concepts" is translated as "相关生物医学概念" (xiāngguān shēngwù yīxué guīxiàn) in Simplified Chinese.* "Target DDI prediction" is translated as "目标DDI预测" (mùzhōu DDI yùdòu) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning"><a href="#Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning"></a>Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08708">http://arxiv.org/abs/2311.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Min Park, Yan Kyaw Tun, Choong Seon Hong</li>
<li>for: 提高6G&#x2F;B5G无线网络质量，挑战传统 terrestrial 基站的限制。</li>
<li>methods: NOMA 技术和 STAR-RISs 技术的结合，以提高 spectral efficiency 和支持更多用户。</li>
<li>results: 提出一种基于 Multi-STAR-RISs 的 joint user pairing for NOMA 和 beamforming 设计方法，可以最大化多个用户的 Throughput。<details>
<summary>Abstract</summary>
The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academic and industrial. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the Amplitude and Phase-shift corresponding to reflection and transmission, which makes the existing terrestiral networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To tackle this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity.
</details>
<details>
<summary>摘要</summary>
开发6G/B5G无线网络，具有超越现有5G网络的需求，在学术和工业领域引起了关注。然而，为了提高6G/B5G网络质量，传统的地面基站Constraints geographically and economically.而NOMA技术可以让多个用户共享同一资源，提高系统spectral efficiency和支持更多的用户。此外，通过智能地调整反射和发射信号的相位和幅度，STAR-RISs可以实现改善的覆盖率、提高的spectral efficiency和提高的通信可靠性。然而，STAR-RISs同时需要优化反射和发射信号的相位和幅度，这使得现有的 terrestrial networks 更加复杂，被认为是主要的挑战。驱动了上述问题，我们研究了多STAR-RISs在室内环境中的联合用户对应和扫描优化设计。然后，我们将问题定义为最大化多个用户（MUs）的总吞吐量，通过联合调整用户对应、扫描顺序、活动扫描和空扫描来优化问题。然而，该问题是一个MINLP问题。为解决这个挑战，我们首先介绍NOMA网络中的排序问题。然后，我们将原始问题分解成两个子问题：1）用户对应问题和2）扫描优化问题。 для第一个子问题，我们采用相关性基于K-means clustering来解决用户对应问题。然后，为了共同处理扫描优化问题，我们提出MAPPO，它可以在给定环境中做出快速决策，凭借其低复杂度。
</details></li>
</ul>
<hr>
<h2 id="Aligned-A-Platform-based-Process-for-Alignment"><a href="#Aligned-A-Platform-based-Process-for-Alignment" class="headerlink" title="Aligned: A Platform-based Process for Alignment"></a>Aligned: A Platform-based Process for Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08706">http://arxiv.org/abs/2311.08706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klonnet23/helloy-word">https://github.com/klonnet23/helloy-word</a></li>
<li>paper_authors: Ethan Shaotran, Ido Pesok, Sam Jones, Emi Liu</li>
<li>for: 本研究旨在提供一个公信worthy、公共面向的安全性机制，以满足前ier模型的整合和超智能的需求。</li>
<li>methods: 本研究使用了一种 constitutional committee framework，通过收集680名参与者的输入，制定了30条指南，其中93%的总支持率。</li>
<li>results: 研究显示，该平台自然具有扩展性和可信worthiness，使得社区具有 confidence和享受性。<details>
<summary>Abstract</summary>
We are introducing Aligned, a platform for global governance and alignment of frontier models, and eventually superintelligence. While previous efforts at the major AI labs have attempted to gather inputs for alignment, these are often conducted behind closed doors. We aim to set the foundation for a more trustworthy, public-facing approach to safety: a constitutional committee framework. Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support. We show the platform naturally scales, instilling confidence and enjoyment from the community. We invite other AI labs and teams to plug and play into the Aligned ecosystem.
</details>
<details>
<summary>摘要</summary>
我们正在引入协调平台，用于全球治理和前沿模型的协调，并最终实现超智能。过去，主要的 AI 室在Alignment的输入收集方面往往采取秘密的方式。我们想要建立一个更可靠、公共的安全方法：一个宪法委员会框架。我们的初步测试中，680名参与者共同制定了30个指南，得到93%的总支持。我们展示了平台自然升级，让社区感受到了信任和乐趣。我们邀请其他 AI 室和团队加入我们的Aligned生态系统。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains"><a href="#Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains" class="headerlink" title="Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains"></a>Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08704">http://arxiv.org/abs/2311.08704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcio Fonseca, Shay B. Cohen</li>
<li>for: 本研究的目的是探讨 instruction-tuned LLMs 是否可以从真实标签中学习新的概念或事实。</li>
<li>methods: 我们使用了不同类型的事实和反事实概念定义来作为题目标的Zero-shot sentence classification任务中的启发项。</li>
<li>results: 我们发现了大型模型（70B个参数或更多）只能在真实上下文中工作，而且仅有商业化模型（如GPT-3.5和GPT-4）能够识别非现实的指引。此外，我们发现了 Llama-2-70B-chat 通常能够超越 Falcon-180B-chat，这表明了精确的微调优化的重要性。总的来说，我们的简单评估方法显示了最具实力的开源语言模型和商业化 API 之间存在了很大的概念理解差距。<details>
<summary>Abstract</summary>
Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Debate-Helps-Supervise-Unreliable-Experts"><a href="#Debate-Helps-Supervise-Unreliable-Experts" class="headerlink" title="Debate Helps Supervise Unreliable Experts"></a>Debate Helps Supervise Unreliable Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08702">http://arxiv.org/abs/2311.08702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/julianmichael/debate">https://github.com/julianmichael/debate</a></li>
<li>paper_authors: Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R. Bowman</li>
<li>for: 用于监督可能不可靠的专家，以确保他们的答案系统atically true而不仅仅看上去真实。</li>
<li>methods: 使用人类辩论来让非专家更可靠地评估真实性。</li>
<li>results: 比较辩论和咨询（一个专家提供单一答案）的性能，发现辩论性能更高， Judge accuracy 为 84%，而咨询的 Judge accuracy 为 74%。<details>
<summary>Abstract</summary>
As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:随着AI系统用于回答更加困难的问题，评价其输出的真实性也变得更加困难和更加重要。如何监督不可靠的专家，他们有 accessed 到真理，但可能不会准确地报告它们？在这个工作中，我们显示了对两个不可靠的专家进行辩论可以帮助非专家评价者更加可靠地识别真实。我们收集了一个人类写的辩论集，其中一个专家认为正确的答案，另一个专家认为 incorrect 的答案。比较辩论和一个我们称之为咨询的基线，其中一个专家就一个答案，正确的一半时间。我们发现，辩论比咨询表现更好，有84%的评价率，而咨询仅有74%。辩论也更加高效，长度只有68%。通过比较人类和AI辩论者，我们发现，随着专家技能的提高，辩论的表现会提高，但咨询的表现会下降。我们的错误分析也支持这个趋势，有46%的错误在人类辩论中是由诚实的辩论者所做的错误（这些错误可以逐渐消失），而52%的错误在人类咨询中是由辩论者对裁判所隐瞒重要证据所做的错误（这些错误可以会随着技能提高而加剧）。总之，这些结果表明，辩论是一种可靠地监督 increasingly capable 但可能不可靠的 AI 系统的方法。
</details></li>
</ul>
<hr>
<h2 id="Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception"><a href="#Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception" class="headerlink" title="Artificial General Intelligence, Existential Risk, and Human Risk Perception"></a>Artificial General Intelligence, Existential Risk, and Human Risk Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08698">http://arxiv.org/abs/2311.08698</a></li>
<li>repo_url: None</li>
<li>paper_authors: David R. Mandel</li>
<li>for: 这篇论文主要针对的是人工智能（AGI）的发展和相关的风险。</li>
<li>methods: 该论文使用了公共可用的预测和意见数据，以探讨专家和非专家对AGI的风险评估。</li>
<li>results: 研究发现，对AGI的世界大悲害或毁灭风险的评估比其他existential risk（如核战或人类引起的气候变化）高，并且过去一年内这种风险的增加速度也比其他风险更大。<details>
<summary>Abstract</summary>
Artificial general intelligence (AGI) does not yet exist, but given the pace of technological development in artificial intelligence, it is projected to reach human-level intelligence within roughly the next two decades. After that, many experts expect it to far surpass human intelligence and to do so rapidly. The prospect of superintelligent AGI poses an existential risk to humans because there is no reliable method for ensuring that AGI goals stay aligned with human goals. Drawing on publicly available forecaster and opinion data, the author examines how experts and non-experts perceive risk from AGI. The findings indicate that the perceived risk of a world catastrophe or extinction from AGI is greater than for other existential risks. The increase in perceived risk over the last year is also steeper for AGI than for other existential threats (e.g., nuclear war or human-caused climate change). That AGI is a pressing existential risk is something on which experts and non-experts agree, but the basis for such agreement currently remains obscure.
</details>
<details>
<summary>摘要</summary>
人工通常智能（AGI）至今还没有存在，但根据人工智能技术的发展速度，预计在下一两个十年内，它将达到人类水平的智能。之后，许多专家预计AGI很快就会超过人类智能水平。这种超智AGI对人类存在极大的风险，因为没有可靠的方法可以保证AGI目标与人类目标相对适应。作者通过公共可用的预测和意见数据，探讨了专家和非专家对AGI风险的看法。结果显示，对AGI的世界大悲剧或毁灭风险的认知高于其他极大风险（如核战或人类引起的气候变化）。过去一年内，对AGI的风险增加速度也更高。尽管专家和非专家都同意AGI是一个急剧的存在风险，但目前这一点的基础还未明确。
</details></li>
</ul>
<hr>
<h2 id="An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping"><a href="#An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping" class="headerlink" title="An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"></a>An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08687">http://arxiv.org/abs/2311.08687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kharrigian/ml4h-clinical-bert">https://github.com/kharrigian/ml4h-clinical-bert</a></li>
<li>paper_authors: Keith Harrigian, Tina Tang, Anthony Gonzales, Cindy X. Cai, Mark Dredze</li>
<li>for: 本研究旨在帮助监测和检测 диабетиче眼病的诊断和治疗过程中的重要信息，以避免盲目。</li>
<li>methods: 该研究使用了19种临床概念与 диабетиче眼病相关的证据抽取系统，以检测和识别病情的变化。</li>
<li>results: 研究发现，使用BERT语言模型预训练非临床数据对于本领域具有相同的效果，而不需要特定的临床数据预训练。这项研究质疑了 latest claims，即临床语言数据预训练语言模型可以帮助临床自然语言处理任务。<details>
<summary>Abstract</summary>
Diabetic eye disease is a major cause of blindness worldwide. The ability to monitor relevant clinical trajectories and detect lapses in care is critical to managing the disease and preventing blindness. Alas, much of the information necessary to support these goals is found only in the free text of the electronic medical record. To fill this information gap, we introduce a system for extracting evidence from clinical text of 19 clinical concepts related to diabetic eye disease and inferring relevant attributes for each. In developing this ophthalmology phenotyping system, we are also afforded a unique opportunity to evaluate the effectiveness of clinical language models at adapting to new clinical domains. Across multiple training paradigms, we find that BERT language models pretrained on out-of-distribution clinical data offer no significant improvement over BERT language models pretrained on non-clinical data for our domain. Our study tempers recent claims that language models pretrained on clinical data are necessary for clinical NLP tasks and highlights the importance of not treating clinical language data as a single homogeneous domain.
</details>
<details>
<summary>摘要</summary>
diabetic eye disease 是全球最主要的失明原因之一。监测相关的临床轨迹和检测治疗过程中的漏洞是管理疾病和避免失明的关键。可悲的是，许多有关这些目标的信息都可以在电子医疗记录中找到，但是它们却很难被找到。为了填补这个信息空白，我们介绍了一种从临床文本中提取19种临床概念和诊断相关的证据，并从中INFERRE 相关的特征。在开发这种眼科分型系统时，我们也得到了评估临床语言模型在新临床领域中是否适应的机会。我们在多个训练方案中发现，BERT语言模型预先训练在非临床数据上的PERFORMANCE 与预先训练在临床数据上的BERT语言模型相比，没有显著的提升。我们的研究证明了，在临床语言数据上预先训练语言模型并不是必要的，并且高举了不要将临床语言数据视为单一的同一个领域。
</details></li>
</ul>
<hr>
<h2 id="Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data"><a href="#Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data" class="headerlink" title="Safer-Instruct: Aligning Language Models with Automated Preference Data"></a>Safer-Instruct: Aligning Language Models with Automated Preference Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08685">http://arxiv.org/abs/2311.08685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct">https://github.com/uscnlp-lime/safer-instruct</a></li>
<li>paper_authors: Taiwei Shi, Kai Chen, Jieyu Zhao</li>
<li>for: 提高模型安全性，使用人类反馈学习（RLHF）是一种重要策略。但是，标注偏好数据需要巨大的人工资源和创造力，而自动生成方法受到数据多样性和质量限制。</li>
<li>methods: 我们提出了一种新的管道，即Safer-Instruct，用于半自动生成大规模偏好数据。我们的方法利用倒转指令调整、指令生成和专家模型评估，以高效生成高质量偏好数据而无需人类注释员。</li>
<li>results: 我们使用LLaMA进行指令生成和GPT-4作为专家模型，生成了约10K个偏好样本。通过对一个Alpaca模型进行训练，我们证明了Safer-Instruct可以提高模型的安全性，同时保持与对话和下游任务的竞争性能。Safer-Instruct解决了偏好数据获取的挑战，为更安全和负责任的AI系统的发展提供了新的想法。我们的代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/uscnlp-lime/safer-instruct上获取。</a><details>
<summary>Abstract</summary>
Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for enhancing model safety in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for semi-automatically constructing large-scale preference datasets. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. We evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an expert model, generating approximately 10K preference samples. Finetuning an Alpaca model on this dataset demonstrates improved harmlessness while maintaining competitive performance on conversation and downstream tasks. Safer-Instruct addresses the challenges in preference data acquisition, advancing the development of safer and more responsible AI systems. Our code and data are available at https://github.com/uscnlp-lime/safer-instruct
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用人类反馈学习增强模型安全性（RLHF）是一种重要策略，但是标注偏好数据需要大量的人工资源和创造力。自动生成方法面临数据多样性和质量限制。为此，我们提出了 Safer-Instruct，一种新的数据采集管线。我们的方法利用倒转指令调整、指令生成和专家模型评估，以高效生成高质量偏好数据，无需人工标注员。我们使用 LLaMA 进行指令生成和 GPT-4 作为专家模型，生成了约 10K 的偏好样本。通过 fine-tuning 一个 Alpaca 模型，我们示出了提高无害性而无需削弱对话和下游任务的性能。Safer-Instruct 解决了偏好数据采集中的挑战，推动了更安全、负责任的 AI 系统的发展。我们的代码和数据可以在 https://github.com/uscnlp-lime/safer-instruct 上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets"><a href="#Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets" class="headerlink" title="Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets"></a>Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08662">http://arxiv.org/abs/2311.08662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth</li>
<li>for: 这个论文的目的是为了研究语言模型对输入杂乱的敏感性，以便提高模型的可靠性和信任性。</li>
<li>methods: 这个论文使用了 fine-tuning 方法来训练语言模型对输入杂乱的敏感性，并研究了不同的输入杂乱对模型的影响。同时， authors 还提出了三种不同的训练策略来提高模型的多重杂乱 robustness。</li>
<li>results: 根据实验结果， authors 发现了一些训练策略可以帮助语言模型在不同的输入杂乱下保持高度的准确率，而不会受到输入杂乱的影响。同时， authors 还发现了一些情况下，模型的性能可以通过 exposure 来提高，但是也可以导致性能下降。<details>
<summary>Abstract</summary>
Language models, given their black-box nature, often exhibit sensitivity to input perturbations, leading to trust issues due to hallucinations. To bolster trust, it's essential to understand these models' failure modes and devise strategies to enhance their performance. In this study, we propose a framework to study the effect of input perturbations on language models of different scales, from pre-trained models to large language models (LLMs). We use fine-tuning to train a robust model to perturbations, and we investigate whether exposure to one perturbation improves or degrades the model's performance on other perturbations. To address multi-perturbation robustness, we suggest three distinct training strategies. We also extend the framework to LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our framework for the Tabular-NLI task and show that the proposed strategies train the model robust to different perturbations without losing accuracy on a given dataset.
</details>
<details>
<summary>摘要</summary>
Language models 因为它们的黑盒特性，经常会受到输入扰动的影响，导致信任问题由于幻觉。为了增强信任，我们需要了解这些模型的失败模式，并开发策略来提高其性能。在这种研究中，我们提出了一个框架，用于研究不同规模的语言模型对输入扰动的影响。我们使用了练化来训练一个Robust模型，并investigate了对一种扰动是否改善或损害模型对其他扰动的性能。为了解决多个扰动的Robustness问题，我们提出了三种不同的训练策略。此外，我们还将框架扩展到大语言模型（LLMs）via一种链式思维（COT）提问方法。我们在Tabular-NLI任务上实现了我们的框架，并显示了提posed的策略可以训练模型对不同的扰动 robust，而无需失去对给定数据集的精度。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing"><a href="#Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing" class="headerlink" title="Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing"></a>Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08649">http://arxiv.org/abs/2311.08649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyeon Yoon, Robert Feldt, Shin Yoo</li>
<li>for: 这篇论文旨在提供一种自动化图形用户界面测试方法，以便更好地检测软件系统是否按照预期的方式运行。</li>
<li>methods: 这篇论文提出了一种基于大语言模型和支持机制的自动化图形用户界面测试方法，称为DroidAgent。该方法可以自动设定和执行具有 semantic intent 的 GUI 测试任务。</li>
<li>results: 根据实验结果，DroidAgent 可以自动设定和执行真实的 GUI 测试任务，并且可以覆盖更高的活动覆盖率（61%），比现状态的方法更高。此外，人工分析表明，DroidAgent 自动创建的任务中有317个是真实有用的和相关于应用程序功能的。<details>
<summary>Abstract</summary>
GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.
</details>
<details>
<summary>摘要</summary>
GUI 测试检查软件系统在用户通过图形用户界面进行交互时是否按预期的行为，例如测试特定功能或验证相关用例enario。目前，决定要测试的高级水平任务是一个手动任务，因为自动化GUI测试工具通常针对较低级别的可用性指标，如结构代码覆盖率或活动覆盖率。我们提议了DroidAgent，一种基于大型自然语言模型和支持机制（如长期和短期记忆）的 Android 应用自动GUI测试代理。它可以进行意图驱动的自动化GUI测试。给定一个 Android 应用，DroidAgent 会设置相关的任务目标，然后通过与应用进行交互来实现这些目标。我们对 DroidAgent 使用 Themis benchmark 中的 15 个应用进行了实验性评估。结果显示，DroidAgent 可以自动设置和执行真实任务，并且在活动覆盖率方面表现更高。例如，在测试一款消息应用时，DroidAgent 创建了一个第二个帐户，并将第一个帐户添加为好友，测试了一个真实的用例，无需人工干预。在 average 的活动覆盖率方面，DroidAgent 达到了 61%，比现状态的 GUI 测试技术高出 51%。此外，manual 分析表明，DroidAgent 自动创建的任务中有 317 个实际 relevante 和相关于应用功能的任务，同时也发现 DroidAgent 深入交互with应用程序，覆盖了更多的功能。
</details></li>
</ul>
<hr>
<h2 id="Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification"><a href="#Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification" class="headerlink" title="Explore Spurious Correlations at the Concept Level in Language Models for Text Classification"></a>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08648">http://arxiv.org/abs/2311.08648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang</li>
<li>for: This paper aims to address the issue of spurious correlations in language models (LMs) caused by imbalanced label distributions in training data, which can lead to robustness issues.</li>
<li>methods: The authors use the LLM to label the concepts in text and measure the concept bias of models for fine-tuning or in-context learning (ICL) on test data. They also propose a data rebalancing method to mitigate the spurious correlations by adding counterfactual data generated by the LLM.</li>
<li>results: The authors show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods. They also demonstrate the effectiveness of their mitigation method and its superiority over the token removal method.<details>
<summary>Abstract</summary>
Language models (LMs) have gained great achievement in various NLP tasks for both fine-tuning and in-context learning (ICL) methods. Despite its outstanding performance, evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues. However, previous studies mostly focus on word- and phrase-level features and fail to tackle it from the concept level, partly due to the lack of concept labels and subtle and diverse expressions of concepts in text. In this paper, we first use the LLM to label the concept for each text and then measure the concept bias of models for fine-tuning or ICL on the test data. Second, we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM-generated counterfactual data to make a balanced label distribution for each concept. We verify the effectiveness of our mitigation method and show its superiority over the token removal method. Overall, our results show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在各种自然语言处理任务中取得了很大的成就，包括精度训练和上下文学习（ICL）方法。尽管它们的表现非常出色，但是证据显示，由于训练数据中标签分布的偏度而导致的假 correlations 会导致模型的Robustness问题。然而，先前的研究主要关注单词和短语水平的特征，而忽略了概念水平的问题，其中一部分是因为缺乏概念标签，另一部分是因为文本中概念的细微和多样化表达。在这篇论文中，我们首先使用LM来标注每个文本中的概念，然后测量模型在测试数据上的概念偏见。第二，我们提出了一种数据重新平衡方法，通过添加LM生成的对称数据来使每个概念的标签分布平衡。我们证明了我们的mitigation方法的有效性，并与Token removal方法进行比较，并显示了我们的方法的优越性。总之，我们的结果表明，在多个文本分类数据集中，存在概念水平的标签分布偏见，LM会在精度训练和ICL方法中利用这些短cuts来进行预测。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations"><a href="#Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations" class="headerlink" title="Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations"></a>Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08644">http://arxiv.org/abs/2311.08644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Su, Juni Jessy Li, Matthew Lease</li>
<li>for: 可以保持神经网络模型的准确性，同时提供 faithful的解释？我们提出了包装盒，一种通用的方法来生成 faithful、示例基于的解释，以保持神经网络模型的预测性能。</li>
<li>methods: 使用一个经典、可解释的模型来实际进行预测，并将神经网络模型的学习feature表示输入到这个经典模型中。</li>
<li>results: 这种简单的策略 surprisingly 有效，Results 与原神经网络模型的预测结果几乎相同，并在三个大预训言模型、两个dataset、四个经典模型和四个评价指标上进行了证明。此外，因为这些经典模型是可解释的设计，可以直接向用户显示用于经典模型预测的 subset 的训练示例。<details>
<summary>Abstract</summary>
Can we preserve the accuracy of neural models while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details>
<details>
<summary>摘要</summary>
可以保持神经网络模型的准确性while也提供忠诚的解释吗？我们提出了“包裹框”，一种通用的方法来生成忠诚的示例基于解释，保持神经网络模型的预测性能。在训练神经网络模型后，其学习的特征表示被输入到可解释的模型进行实际预测。这种简单的策略具有意外的效果，在三个大预训言语模型、两个不同规模的数据集、四种经典模型和四种评价指标上都达到了相似的结果。此外，由于这些经典模型是可解释的设计，可以直接向用户显示训练示例的子集。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction"><a href="#Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction" class="headerlink" title="Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction"></a>Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08635">http://arxiv.org/abs/2311.08635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyin Jin, Lingbo Liu, Fuxian Li, Jincai Huang</li>
<li>for: 预测交通堵塞事件是智能交通系统中一项重要 yet 挑战性的任务。许多现有的交通预测方法 integrate 多种时间编码器和图 convolutional neural networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed, and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis.</li>
<li>methods: 我们提出了一种 spatio-temporal graph neural point process 框架， named STGNPP, to address these limitations. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism.</li>
<li>results: 我们的模型可以同时预测下一个堵塞事件的发生时间和持续时间。对两个实际数据集进行了广泛的实验，并证明了我们的方法在比较现有状态艺术方法的情况下显著提高了性能。<details>
<summary>Abstract</summary>
Traffic congestion event prediction is an important yet challenging task in intelligent transportation systems. Many existing works about traffic prediction integrate various temporal encoders and graph convolution networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis. In recent years, neural point process (NPP) has emerged as an appropriate framework for event prediction in continuous time scenarios. However, most conventional works about NPP cannot model the complex spatio-temporal dependencies and congestion evolution patterns. To address these limitations, we propose a spatio-temporal graph neural point process framework, named STGNPP for traffic congestion event prediction. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism. Finally, our model simultaneously predicts the occurrence time and duration of the next congestion. Extensive experiments on two real-world datasets demonstrate that our method achieves superior performance in comparison to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
减少交通拥堵事件预测是智能交通系统中非常重要 yet 挑战性的任务。许多现有的交通预测方法将多种时间编码器和图像树卷积网络（GCN）集成，称为空间-时间图像基于神经网络，它们主要预测时刻点的稠密变量，如流速和需求，但它们很难预测在连续时间轴上稀疏分布的交通拥堵事件。在过去几年，神经点过程（NPP）在连续时间场景中的预测 Task 变得越来越受欢迎。然而，大多数传统的 NPP 方法无法模型交通拥堵事件的复杂空间-时间依赖关系和拥堵演化趋势。为了解决这些限制，我们提议一种空间-时间图像神经点过程框架，名为 STGNPP，用于交通拥堵事件预测。具体来说，我们首先设计了空间-时间图像学习模块，以全面捕捉历史交通状态数据中的长距离空间-时间依赖关系，同时与道路网络结合。提取的空间-时间隐藏表示和拥堵事件信息然后被传递给连续闭包律环路模型，以模型拥堵演化趋势。具体来说，为了完全利用周期信息，我们还改进了点过程的激发函数计算方法，使用周期闭包机制。最后，我们的模型同时预测下一个拥堵事件的发生时间和持续时间。我们在两个实际数据集上进行了广泛的实验，结果显示，我们的方法在与现有状态艺术方法比较时表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making"><a href="#XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making" class="headerlink" title="XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making"></a>XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08614">http://arxiv.org/abs/2311.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra</li>
<li>for: 这 paper 的目的是提高大型自然语言处理模型（LLM）的透明性和可解释性，使其更加可靠和可信worth。</li>
<li>methods: 这 paper 使用了知识 graphs（KGs）和图注意力网络（GAT）来构建一个新的解释数据集，以帮助理解 LLM 的决策过程。</li>
<li>results: 这 paper 通过量化和质量评估表明，该数据集可以提高 LLM 在问答任务上的在Context learning，提高其解释性和可解释性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a why-choose explanation, a why-not-choose explanation, and a set of reason-elements that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the reason-elements and transform them into why-choose and why-not-choose explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: https://github.com/chen-zichen/XplainLLM_dataset.git
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures"><a href="#Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures" class="headerlink" title="Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures"></a>Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08605">http://arxiv.org/abs/2311.08605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-jenny/llm-political-study">https://github.com/david-jenny/llm-political-study</a></li>
<li>paper_authors: David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在政治辩论中的决策过程和内在偏见。</li>
<li>methods: 本研究使用活动依赖网络（ADNs）抽象出LLMs的隐式评价标准，并研究如何normative价值影响这些评价。</li>
<li>results: 研究发现LLMs在评价“好 argue”时存在偏见，这些偏见源于模型内部的normative价值。这些结果有关于人工智能和偏见减少的影响。<details>
<summary>Abstract</summary>
The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding their ability to perceive and interpret complex socio-political landscapes. In this study, we undertake an exploration of decision-making processes and inherent biases within LLMs, exemplified by ChatGPT, specifically contextualizing our analysis within political debates. We aim not to critique or validate LLMs' values, but rather to discern how they interpret and adjudicate "good arguments." By applying Activity Dependency Networks (ADNs), we extract the LLMs' implicit criteria for such assessments and illustrate how normative values influence these perceptions. We discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data at https://github.com/david-jenny/LLM-Political-Study.
</details>
<details>
<summary>摘要</summary>
快速发展的大语言模型（LLM）已引发了严峻的辩论，涉及到它们是否能够理解和解释复杂的社会政治景观。在这项研究中，我们进行了决策过程和内置偏见在LLM中的探索，以ChatGPT作为例子，并将分析围绕政治辩论进行。我们不是要评价或验证LLM的价值观，而是想要了解它们如何处理和评价“好的Arguments”。通过应用Activity Dependency Networks（ADN），我们提取了LLM中的隐式评价标准，并示出了normative价值如何影响这些评价。我们讨论了我们的发现对人机同步和偏见缓减的后果。codes和数据可以在https://github.com/david-jenny/LLM-Political-Study找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.AI_2023_11_15/" data-id="clp53jwm2007iyp88aqcqb2tg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CL_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T11:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CL_2023_11_15/">cs.CL - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries"><a href="#Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries" class="headerlink" title="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries"></a>Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09458">http://arxiv.org/abs/2311.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, Chien-Sheng Wu</li>
<li>for: 这个论文的目的是提出一种精细评估协议，以便评估psummarization模型的泛化能力。</li>
<li>methods: 这个论文使用了一种lexical similaritypartitioning方法来评估psummarization模型的性能。</li>
<li>results: 研究发现，通过限制training summaries中的lexical repetitions，可以避免模型学习rote learning，提高psummarization模型的泛化能力。<details>
<summary>Abstract</summary>
Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries. We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the subsets with the lowest and highest similarity. Next, we show that such training repetitions also make a model vulnerable to rote learning, reproducing data artifacts such as factual errors, especially when reference test summaries are lexically close to training summaries. Consequently, we propose to limit lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages to improve the performance on novel test cases while retaining average performance. Our automatic and human evaluations on novel test subsets and recent news articles show that limiting lexical repetitions in training summaries can prevent rote learning and improve generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset"><a href="#Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset" class="headerlink" title="Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset"></a>Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09443">http://arxiv.org/abs/2311.09443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brooklyn Sheppard, Anna Richter, Allison Cohen, Elizabeth Allyn Smith, Tamara Kneese, Carolyne Pelletier, Ioana Baldini, Yue Dong</li>
<li>for: 本研究用于开发一个新的 dataset，捕捉贫婆婆的细节和细腻，以便用于 NLP 任务中检测贫婆婆。</li>
<li>methods: 该 dataset 采用多元领域专家和注释者的合作建立，包括电影字幕注释，以捕捉北美电影中的贫婆婆表达。</li>
<li>results: 本研究提供了检测贫婆婆的方法和基elines，以及用于 rewrite 文本的文本生成技术。我们希望这种工作能够推动 AI 为社会好用的 NLP 应用。<details>
<summary>Abstract</summary>
Using novel approaches to dataset development, the Biasly dataset captures the nuance and subtlety of misogyny in ways that are unique within the literature. Built in collaboration with multi-disciplinary experts and annotators themselves, the dataset contains annotations of movie subtitles, capturing colloquial expressions of misogyny in North American film. The dataset can be used for a range of NLP tasks, including classification, severity score regression, and text generation for rewrites. In this paper, we discuss the methodology used, analyze the annotations obtained, and provide baselines using common NLP algorithms in the context of misogyny detection and mitigation. We hope this work will promote AI for social good in NLP for bias detection, explanation, and removal.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Biasly" is a made-up word, so it was not translated.* "multi-disciplinary" was translated as "多元的" (duōyuán de), which is a more common way of saying "interdisciplinary" in Chinese.* "colloquial expressions" was translated as "口语表达" (kǒu yǔ biǎo jiàn), which is a more common way of saying "colloquialisms" in Chinese.* "North American films" was translated as "北美电影" (běi mèi diàn yǐng), which is a more specific way of saying "American movies" in Chinese.* "severity score regression" was translated as "严重程度回归" (jiān zhòng chéng dào huí jīn), which is a more common way of saying "severity score prediction" in Chinese.* "text generation for rewrites" was translated as "文本生成重写" (wén tiěn shēng chéng zhòng xiě), which is a more common way of saying "text generation for rewriting" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Labeled-Interactive-Topic-Models"><a href="#Labeled-Interactive-Topic-Models" class="headerlink" title="Labeled Interactive Topic Models"></a>Labeled Interactive Topic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09438">http://arxiv.org/abs/2311.09438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Kyle Seelman, Mozhi Zhang, Jordan Boyd-Graber</li>
<li>for: 本研究旨在提高 neural topic model 的可互动性，使其能够更好地捕捉用户的信息需求。</li>
<li>methods: 本研究使用了一种intuitive interaction方法，让用户可以对话板模型中的话题进行标签和更新。这种方法可以让用户根据自己的信息需求来修改话题的分布。</li>
<li>results: 通过人工研究，我们发现用户可以通过标签和更新话题来提高文档排名分数，从而更好地找到与查询有关的文档。相比没有用户标签的情况，用户可以通过这种方法提高文档排名分数。<details>
<summary>Abstract</summary>
Topic models help users understand large document collections; however, topic models do not always find the ``right'' topics. While classical probabilistic and anchor-based topic models have interactive variants to guide models toward better topics, such interactions are not available for neural topic models such as the embedded topic model (\abr{etm}). We correct this lacuna by adding an intuitive interaction to neural topic models: users can label a topic with a word, and topics are updated so that the topic words are close to the label. This allows a user to refine topics based on their information need. While, interactivity is intuitive for \abr{etm}, we extend this framework to work with other neural topic models as well. We develop an interactive interface which allows users to interact and relabel topic models as they see fit. We evaluate our method through a human study, where users can relabel topics to find relevant documents. Using our method, user labeling improves document rank scores, helping to find more relevant documents to a given query when compared to no user labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers"><a href="#Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers" class="headerlink" title="Striped Attention: Faster Ring Attention for Causal Transformers"></a>Striped Attention: Faster Ring Attention for Causal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09431">http://arxiv.org/abs/2311.09431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exists-forall/striped_attention">https://github.com/exists-forall/striped_attention</a></li>
<li>paper_authors: William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley</li>
<li>for: This paper aims to address the growing demand for longer sequence lengths in transformer models by proposing an exact attention algorithm called Ring Attention, which can overcome per-device memory bottlenecks by distributing self-attention across multiple devices.</li>
<li>methods: The paper proposes a simple extension to Ring Attention called Striped Attention, which distributes the attention computation across multiple devices in a more balanced way to achieve more even workloads.</li>
<li>results: The authors achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k, and 1.65x speedups at sequence lengths of 786k using Striped Attention on 16 TPUv4 chips.Here’s the Chinese translation of the three information points:</li>
<li>for: 这篇论文目标是解决变长序列长度的增长对转换器模型的需求，提出了环Attention算法，可以在多个设备上分布自注意计算，从而缓解每个设备的内存瓶颈。</li>
<li>methods: 论文提出了一种简单的扩展，即Striped Attention，它将注意计算分布到多个设备上，以更平衡的方式来实现更平衡的工作负荷。</li>
<li>results: 作者使用Striped Attention在 causal transformer 训练中 achieved up to 1.45倍的综合通过put improvement，并在 786k 长度的序列上达到 1.65倍的速度提升。<details>
<summary>Abstract</summary>
To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source
</details>
<details>
<summary>摘要</summary>
lijun et al. 最近提出了环形注意力算法（Ring Attention），用于解决变长序列长度的增长导致的每个设备内存瓶颈问题。在这篇论文中，我们研究了环形注意力在重要的 causal transformer 模型中的性能特点，并发现了一个关键的工作负载不均衡问题，即triangle structure of causal attention computations。我们提出了一个简单的扩展，称为Striped Attention，以解决这个不均衡问题。在我们的实验中，我们在 A100 GPU 和 TPUv4 上运行了 Striped Attention，并 achievied up to 1.45x 终端通过put improvementsover original Ring Attention algorithm on causal transformer 训练序列长度为 256k。此外，在 16 TPUv4 板件上，我们实现了 1.65x 速度提升，在序列长度为 786k 的情况下。我们将我们的实验代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Predicting-generalization-performance-with-correctness-discriminators"><a href="#Predicting-generalization-performance-with-correctness-discriminators" class="headerlink" title="Predicting generalization performance with correctness discriminators"></a>Predicting generalization performance with correctness discriminators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09422">http://arxiv.org/abs/2311.09422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuekun Yao, Alexander Koller</li>
<li>for: 预测NLP模型在未经见、可能是非标的数据上的准确率。</li>
<li>methods: 使用一个新的模型来确定模型在未经见数据上的准确率范围，不需要黄金标签。</li>
<li>results: 通过训练一个推断器，确定模型输出的Sequence-to-Sequence模型输出是否正确，并证明了金标签的准确率在预测范围内，并且这些范围很接近。<details>
<summary>Abstract</summary>
The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness." into Simplified Chinese翻译：预测未经见过、可能处于数据分布范围外的 NLP 模型准确率是可靠性的必要条件。我们提出了一种新的模型，可以在没有金标签的情况下，设置上下限于模型的准确率。我们通过训练一个判断序列到Sequence模型输出是否正确的探测器来实现这一点。我们在多个标注、分析和 semantics 分析任务中表明，金标签的准确率在上下限之间，并且这些上下限很接近。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture"><a href="#Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture" class="headerlink" title="Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture"></a>Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09406">http://arxiv.org/abs/2311.09406</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Bernhard</li>
<li>for: 提高 transformer 神经网络中 attention  Mechanism 的效果</li>
<li>methods: 提议一些alternative scalings，包括将dot product 分配到键值之和之前应用softmax</li>
<li>results: 使用模拟的键值和查询示例，显示这些scalings 在许多情况下更有效地避免应用softmax导致梯度消失的地方<details>
<summary>Abstract</summary>
The transformer neural network architecture uses a form of attention in which the dot product of query and key is divided by the square root of the key dimension before applying softmax. This scaling of the dot product is designed to avoid the absolute value of the dot products becoming so large that applying softmax leads to vanishing gradients. In this paper, we propose some alternative scalings, including dividing the dot product instead by the sum of the key lengths before applying softmax. We use simulated keys and queries to show that in many situations this appears to be more effective at avoiding regions where applying softmax leads to vanishing gradients.
</details>
<details>
<summary>摘要</summary>
transformer 神经网络架构使用一种叫做注意力的方式，其中查询和键的 dot product 被除以键维度的平方根之前应用 softmax。这种缩放的 dot product 是为了避免查询和键的绝对值变得太大，使得应用 softmax 导致梯度消失。在这篇论文中，我们提出了一些替代的缩放方式，包括将 dot product 除以键的总长度之前应用 softmax。我们使用模拟的查询和键来示出，在许多情况下，这些替代缩放方式可以更好地避免应用 softmax 导致梯度消失的区域。
</details></li>
</ul>
<hr>
<h2 id="To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages"><a href="#To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages" class="headerlink" title="To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages"></a>To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09404">http://arxiv.org/abs/2311.09404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benedikt Ebing, Goran Glavaš</li>
<li>for: 这个论文的目的是为了提高跨语言传递（XLT）性能，并评估现有和新的翻译基于XLT方法。</li>
<li>methods: 本论文使用了翻译基于MT的XLT方法，包括使用round-trip翻译、添加可靠翻译等方法。</li>
<li>results: 研究发现，使用翻译基于MT的XLT方法可以大幅提高XLT性能，并且可以通过添加其他高资源语言的翻译数据来进一步提高性能。<details>
<summary>Abstract</summary>
Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (LMs) superfluous. Given, on one hand, the large body of work on improving XLT with multilingual LMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with multilingual LMs, rendering the approach that combines the round-trip translation of the source-language training data with the translation of the target-language test instances the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We hope that our findings encourage adoption of more robust translation-based baselines in XLT research.
</details>
<details>
<summary>摘要</summary>
如果完美机器翻译（MT）能够实现语言之间传递（XLT），那么使用多语言模型（LM）就不再需要。一方面，有大量的研究用于提高XLT的多语言LM，另一方面，近年来的大规模多语言MT的进步，我们在这里系统地评估了现有的翻译基于XLT方法，以及新的翻译基于XLT方法，用于将语言转移到低资源语言。我们发现所有的翻译基于方法在零批XLT中都表现出了极大的改善，并且将源语言训练数据翻译回目标语言测试实例的方法得到了最佳效果。我们还证明了可以通过添加其他高资源语言的可靠翻译来进一步提高实验性能。此外，我们提出了一种有效的翻译基于XLT策略，即使Language不支持MT系统。最后，我们发现基于MT系统进行目标语言验证数据的模型选择比基于源语言数据更高效。我们希望我们的发现能够激励XLT研究中采用更加稳定的翻译基于基准。
</details></li>
</ul>
<hr>
<h2 id="LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems"><a href="#LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems" class="headerlink" title="LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems"></a>LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09390">http://arxiv.org/abs/2311.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nalin Kumar, Ondřej Dušek</li>
<li>for: 这项研究的目的是提高对话系统的自然语言处理能力，使对话更加自然和流畅。</li>
<li>methods: 这项研究使用了GPT-2基于端到端对话系统，并通过共享词汇来实现对话融合。研究者对实例权重训练、对称损失和额外条件进行了调整，以生成与用户的响应相Alignment。</li>
<li>results: 研究表明，三种整合技术在MultiWOZ数据集上都能够生成高度对应的响应，与基eline相比，自动和手动评估指标都表明了这一点。<details>
<summary>Abstract</summary>
Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While alignment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue alignment in a GPT-2-based end-to-end dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, alignment-specific loss, and additional conditioning to generate responses that align with the user. By comparing different entrainment techniques on the MultiWOZ dataset, we demonstrate that all three approaches produce significantly better-aligned results than the baseline, as confirmed by both automated and manual evaluation metrics.
</details>
<details>
<summary>摘要</summary>
语言同步、或谱合，表示对话参与者使用的语言模式相互整合。而对话同步已经证明可以提供更自然的用户体验，但大多数对话系统并没有相应的规定。在这项工作中，我们介绍了使用共享词汇来实现对话同步在基于GPT-2的端到端对话系统中。我们对训练实例权重、对话同步特有的损失函数和额外条件进行实验，以生成与用户相对应的回答。通过对多语言WOZ数据集进行比较，我们证明了这三种整合技术都能够在自动和手动评估指标上显著提高对话同步性，并且比基eline更好。
</details></li>
</ul>
<hr>
<h2 id="Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing"><a href="#Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing" class="headerlink" title="Neural machine translation for automated feedback on children’s early-stage writing"></a>Neural machine translation for automated feedback on children’s early-stage writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09389">http://arxiv.org/abs/2311.09389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Vestergaard Jensen, Mikkel Jordahn, Michael Riis Andersen</li>
<li>for: 本研究旨在自动生成初级写作评估和建构反馈。</li>
<li>methods: 我们提议使用序列到序列模型将初级写作翻译成普通写作，以便使用语言指标进行分析。此外，我们还提出了一种新的稳定likelihood来mitigate dataset中的噪声影响。</li>
<li>results: 我们通过数字实验 investigate了我们的方法，并证明了可以高度准确预测普通写作。<details>
<summary>Abstract</summary>
In this work, we address the problem of assessing and constructing feedback for early-stage writing automatically using machine learning. Early-stage writing is typically vastly different from conventional writing due to phonetic spelling and lack of proper grammar, punctuation, spacing etc. Consequently, early-stage writing is highly non-trivial to analyze using common linguistic metrics. We propose to use sequence-to-sequence models for "translating" early-stage writing by students into "conventional" writing, which allows the translated text to be analyzed using linguistic metrics. Furthermore, we propose a novel robust likelihood to mitigate the effect of noise in the dataset. We investigate the proposed methods using a set of numerical experiments and demonstrate that the conventional text can be predicted with high accuracy.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决了自动使用机器学习进行早期写作评估和建构反馈的问题。早期写作通常具有不同的音词拼写和缺失正确的语法、标点符号等等，因此对常见语言指标来分析是非常困难的。我们提议使用序列到序列模型将学生的早期写作翻译成“常规”的写作，这使得翻译后的文本可以使用语言指标进行分析。此外，我们还提出了一种新的Robust likelihood来降低数据集中的噪声的影响。我们通过一系列数字实验来调查提议的方法，并证明了可以高度准确地预测常规文本。
</details></li>
</ul>
<hr>
<h2 id="Banach-Tarski-Embeddings-and-Transformers"><a href="#Banach-Tarski-Embeddings-and-Transformers" class="headerlink" title="Banach-Tarski Embeddings and Transformers"></a>Banach-Tarski Embeddings and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09387">http://arxiv.org/abs/2311.09387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtmaher/embedding">https://github.com/jtmaher/embedding</a></li>
<li>paper_authors: Joshua Maher</li>
<li>for: 这个论文旨在提出一种将复杂数据结构转化为高维向量的新建构。这些向量可以用于解释 transformer 的隐藏状态向量模型。</li>
<li>methods: 这个论文使用的方法包括构建高维向量空间中的嵌入，以及使用这些嵌入进行计算和解码。具体来说，这个论文提出了一种使用 vector 操作直接在嵌入空间进行计算，而不需要解码。</li>
<li>results: 这个论文的结果表明，当嵌入维度够大时，可以将嵌入转化回原始数据结构。此外，这个论文还提出了一种使用嵌入进行计算的算法，可以construct一个嵌入的 parse 树。<details>
<summary>Abstract</summary>
We introduce a new construction of embeddings of arbitrary recursive data structures into high dimensional vectors. These embeddings provide an interpretable model for the latent state vectors of transformers. We demonstrate that these embeddings can be decoded to the original data structure when the embedding dimension is sufficiently large. This decoding algorithm has a natural implementation as a transformer. We also show that these embedding vectors can be manipulated directly to perform computations on the underlying data without decoding. As an example we present an algorithm that constructs the embedded parse tree of an embedded token sequence using only vector operations in embedding space.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的嵌入建构，将 recursive 数据结构嵌入高维 вектор中。这些嵌入提供了可解释的模型，用于 transformer 中的 latent state 矢量。我们示示了这些嵌入可以在嵌入维度足够大时，被解码回原始数据结构。这个解码算法自然地实现为 transformer。此外，我们还证明了这些嵌入矢量可以直接进行计算，无需解码。作为例子，我们提出了一个算法，使用 vector 操作来构造嵌入token序列中的嵌入 parse tree。
</details></li>
</ul>
<hr>
<h2 id="Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach"><a href="#Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach" class="headerlink" title="Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach"></a>Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09383">http://arxiv.org/abs/2311.09383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Kashob Kumar Roy, Lucian Popa, Kevin Chen-Chuan Chang</li>
<li>for: 这篇论文的目的是解决长形问答（LFQA）问题，旨在生成详细的回答，包括多个话题和 их复杂关系，需要详细的解释。</li>
<li>methods: 该论文提出了一种基于迭代规划、检索和生成的LFQA模型，通过多次迭代过程，直到生成完整的回答。此外，该模型还利用了多个知识源的整合，以提高回答的准确性和完整性。</li>
<li>results: 经过广泛的实验，该模型在两个不同领域的QA数据集上表现出色，与现有的状态 искусственный智能模型相比，在多种文本和事实指标上表现出超过其他模型的优势。<details>
<summary>Abstract</summary>
Long-form question answering (LFQA) poses a challenge as it involves generating detailed answers in the form of paragraphs, which go beyond simple yes/no responses or short factual answers. While existing QA models excel in questions with concise answers, LFQA requires handling multiple topics and their intricate relationships, demanding comprehensive explanations. Previous attempts at LFQA focused on generating long-form answers by utilizing relevant contexts from a corpus, relying solely on the question itself. However, they overlooked the possibility that the question alone might not provide sufficient information to identify the relevant contexts. Additionally, generating detailed long-form answers often entails aggregating knowledge from diverse sources. To address these limitations, we propose an LFQA model with iterative Planning, Retrieval, and Generation. This iterative process continues until a complete answer is generated for the given question. From an extensive experiment on both an open domain and a technical domain QA dataset, we find that our model outperforms the state-of-the-art models on various textual and factual metrics for the LFQA task.
</details>
<details>
<summary>摘要</summary>
长形问题回答（LFQA）呈现了挑战，因为它们需要生成详细的回答，以 paragraph 的形式，超出了简单的是/否回答或短要的事实回答。现有的 QA 模型在问题中提供了简短的回答，但 LFQA 需要处理多个话题和它们的复杂关系，需要详细的解释。先前的 LFQA 尝试都是通过使用相关的文本来生成长形回答，但它们忽略了问题本身可能不够提供足够的信息来标识相关的文本的可能性。此外，生成详细的长形回答经常需要从多个来源汇集知识。为解决这些限制，我们提议了一种基于规划、检索和生成的 LFQA 模型。这个迭代过程会持续到一个完整的回答被生成为给定的问题。从一项大规模的实验中，我们发现我们的模型在开放领域和技术领域 QA 数据集上超越了当前状态的模型在不同的文本和事实指标上。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms"><a href="#A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms" class="headerlink" title="A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms"></a>A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09367">http://arxiv.org/abs/2311.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Mane, Suman Kundu, Rajesh Sharma</li>
<li>For: This paper aims to bridge the gap between disparate studies on Aggression Content Detection and Behavioral Analysis of Aggressive Users, with the goal of preventing cyber-aggressive behavior.* Methods: The paper examines the comprehensive process of Aggression Content Detection, including dataset creation, feature selection and extraction, and detection algorithm development. It also reviews studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior.* Results: The paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis, highlighting the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior.<details>
<summary>Abstract</summary>
The rise of social media platforms has led to an increase in cyber-aggressive behavior, encompassing a broad spectrum of hostile behavior, including cyberbullying, online harassment, and the dissemination of offensive and hate speech. These behaviors have been associated with significant societal consequences, ranging from online anonymity to real-world outcomes such as depression, suicidal tendencies, and, in some instances, offline violence. Recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的普及，циBER-侵略性行为的发展也得到了推动。这种行为包括互联网欺凌、在线骚扰和危险或仇恨言论等，它们与社会环境有着深远的关系，从在线匿名到真实世界的后果，如抑郁、自杀倾向和在一些情况下的线下暴力。recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models"><a href="#Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models" class="headerlink" title="Investigating the Emergent Audio Classification Ability of ASR Foundation Models"></a>Investigating the Emergent Audio Classification Ability of ASR Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09363">http://arxiv.org/abs/2311.09363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill</li>
<li>for: 这 paper 的目的是 investigate the zero-shot 音频分类能力 of Whisper and MMS, two ASR foundation models.</li>
<li>methods: 作者使用了 simple template-based text prompts 和 decoding probabilities 来生成 zero-shot predictions. 另外，作者还使用了 debiasing 方法来提高模型的性能.</li>
<li>results: 研究表明，Whisper 可以在 8 个 audio-classification 数据集上显示出优秀的 zero-shot 分类性能，与现有的状态的艺术 zero-shot baseline 的准确率相比，提高了9%的准确率。同时，作者发现，模型的大小增加会导致 zero-shot 性能提高。<details>
<summary>Abstract</summary>
Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. However, there has been significantly less work on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming existing state-of-the-art zero-shot baseline's accuracy by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:文本和视觉基础模型可以完成许多零shot任务，这是一个有利的特性，使得这些系统能够应用在普遍和资源匮乏的设置中。然而，对于ASR基础模型来说，有 significatively less work on its zero-shot能力，这些系统通常是特定任务的细化或者数据注解的约束下进行了训练。在这个工作中，我们 investigate了Whisper和MMS，这两个主要用于音频识别的ASR基础模型，在零shot音频分类任务上的能力。我们使用简单的模板基于文本提示，并使用其结果的解码概率来生成零shot预测。无需训练模型更多数据或者添加新的参数，我们示出了Whisper在8个音频分类dataset上的出色的零shot分类性能，与现有的零shot基线的精度相比，平均提高9%。一种重要的步骤是debiasing，这是一种简单的无监督重新权重方法，可以持续性地提高性能。我们还显示，模型的大小增加会导致零shot性能提高，这 implies that ASR基础模型可能在扩大后展现出更好的零shot性能。
</details></li>
</ul>
<hr>
<h2 id="LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents"><a href="#LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents" class="headerlink" title="LePaRD: A Large-Scale Dataset of Judges Citing Precedents"></a>LePaRD: A Large-Scale Dataset of Judges Citing Precedents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09356">http://arxiv.org/abs/2311.09356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmahari/lepard">https://github.com/rmahari/lepard</a></li>
<li>paper_authors: Robert Mahari, Dominik Stammbach, Elliott Ash, Alex &#96;Sandy’ Pentland</li>
<li>for: 这篇论文是为了提高法律搜索和理解的技术能力而写的。</li>
<li>methods: 这篇论文使用了大量的美国联邦法院判例文献，并进行了多种检索方法的评估。</li>
<li>results: 研究发现，使用分类方法可以在法律搜索中获得最好的效果，但是法律预测仍然是一个具有挑战性的任务，还有很多空间可以进行改进。<details>
<summary>Abstract</summary>
We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive collection of U.S. federal judicial citations to precedent in context. The dataset aims to facilitate work on legal passage prediction, a challenging practice-oriented legal retrieval and reasoning task. Legal passage prediction seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various retrieval approaches on LePaRD, and find that classification appears to work best. However, we note that legal precedent prediction is a difficult task, and there remains significant room for improvement. We hope that by publishing LePaRD, we will encourage others to engage with a legal NLP task that promises to help expand access to justice by reducing the burden associated with legal research. A subset of the LePaRD dataset is freely available and the whole dataset will be released upon publication.
</details>
<details>
<summary>摘要</summary>
我团队现在发布了《法律段落预测数据集》（LePaRD）。LePaRD是一个庞大的美国联邦法院判例引用数据集，旨在促进法律段落预测任务的研究和应用。法律段落预测是一项具有实际应用价值的法律自然语言处理任务，它的目标是根据法律Arguments的上下文预测相关的判例段落。我们对LePaRD进行了广泛的评估，发现类别预测方法在这个任务中表现最佳。然而，我们注意到法律预测任务是一项具有挑战性的任务，还有很大的发展空间。我们希望通过发布LePaRD，促进法律自然语言处理领域的研究，扩大 justice的访问，减少法律研究的负担。一个LePaRD数据集的子集可以免费下载，整个数据集将在发布后释出。
</details></li>
</ul>
<hr>
<h2 id="Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization"><a href="#Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization" class="headerlink" title="Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization"></a>Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09344">http://arxiv.org/abs/2311.09344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, Priyanka Agrawal</li>
<li>for: 提高大语言模型（LLM）下游任务性能，尤其是在语言生成任务上。</li>
<li>methods: 使用标注任务数据进行参数高效化精细调整（PEFT），并通过元素加法操作将语言或任务特定的PEFT模块相乘。</li>
<li>results: 在摘要任务上实现了稳定的性能提升，只需训练PEFT模块 minimal amount of training data。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters. Our method composes language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT modules trained on languages related to the target. Empirical results on summarization demonstrate that our method is an effective strategy that obtains consistent gains using minimal training of PEFT modules.
</details>
<details>
<summary>摘要</summary>
Parameter-efficient fine-tuning（PEFT）使用标注任务数据可以显著提高大语言模型（LLM）下游任务的性能。然而，世界上有7000种语言，而 многие这些语言缺乏实际语言生成任务的标注数据。在这篇论文中，我们提议使用语言或任务特化的参数来改善零shot Cross-Lingual transfer。我们的方法通过元素减法运算将语言和任务PEFT模块 compose，以利用无标注数据和英语标注数据。我们将我们的方法扩展到有更多语言的标注数据 disponiblesituation，并提议使用元素减法运算将PEFT模块在相关语言上训练。实验结果显示，我们的方法是一种有效的策略，可以在 minimal training PEFT模块的情况下获得顺性的提升。
</details></li>
</ul>
<hr>
<h2 id="Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback"><a href="#Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback" class="headerlink" title="Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback"></a>Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09336">http://arxiv.org/abs/2311.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag</li>
<li>for: 提高文本生成质量</li>
<li>methods: 使用细化的行动反馈进行迭代优化</li>
<li>results: 在三个文本生成任务上（机器翻译、长形问答和主题概要）观察到0.8和0.7 MetricX的提高，以及4.5和1.8 ROUGE-L的提高，并且通过我们的模拟热释算法进行优化，再次提高文本质量。<details>
<summary>Abstract</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details>
<details>
<summary>摘要</summary>
最近的文本生成技术进步有利用人类反馈来提高生成输出质量。然而，人类反馈不总是可用，特别是在推理过程中。在这项工作中，我们提出了一种基于报错类型、报错位置和严重程度的权重学习模型来进行推理时间优化方法（FITO）。FITO从初始输出开始，然后通过一个改进模型来逐步integrate反馈，以生成基于反馈的改进输出。由于不确定的精细修改样本的不一致，我们将Iterative refinement转化为本地搜索问题，并开发了一种基于随机熔化的算法来寻找优质输出。我们在机器翻译、长文问答和主题概要三个文本生成任务上进行了实验，并观察到了0.8和0.7 MetricX的提升，以及4.5和1.8 ROUGE-L的提升。通过我们的随机熔化算法，我们还观察到了进一步的质量提升，包括最多1.7 MetricX的提升。
</details></li>
</ul>
<hr>
<h2 id="Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models"><a href="#Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models" class="headerlink" title="Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"></a>Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09214">http://arxiv.org/abs/2311.09214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu, Hongxia Xu, Jintai Chen, Jian Wu</li>
<li>for: 提高小语言模型（SLM）的性能，减少erroneous reasoning和hallucinations问题。</li>
<li>methods: 提出了一种两重方法：首先，引入了一种新的自我评估能力储存在LLMs中的分布式方法，以减少erroneous reasoning和hallucinations问题；其次，提出了一种多种链条思维和自我评估 парадигмы的完整储存过程，以确保更加全面和坚实地将知识传递到SLMs中。</li>
<li>results: 实验结果表明，我们的方法可以显著提高储存过程中的SLMs性能，并且突出了开发更小的模型，更加接近人类认知的道路。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in the field of natural language processing. However, the sheer scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained contexts. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still carry over flawed reasoning or hallucinations inherited from their LLM counterparts. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate the adverse effects of erroneous reasoning and reduce hallucinations. Second, we advocate for a comprehensive distillation process that incorporates multiple distinct chain-of-thought and self-evaluation paradigms and ensures a more holistic and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs and sheds light on the path towards developing smaller models closely aligned with human cognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We introduce a novel method for distilling the self-evaluation capability of LLMs into smaller language models (SLMs), which aims to reduce the impact of flawed reasoning and hallucinations.2. We advocate for a comprehensive distillation process that incorporates multiple chain-of-thought and self-evaluation paradigms, ensuring a more holistic and robust knowledge transfer to SLMs.Experiments on three NLP benchmarks show that our method significantly improves the performance of distilled SLMs, bringing us closer to developing smaller models that align with human cognition.</details></li>
</ol>
<hr>
<h2 id="GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes"><a href="#GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes" class="headerlink" title="GRIM: GRaph-based Interactive narrative visualization for gaMes"></a>GRIM: GRaph-based Interactive narrative visualization for gaMes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09213">http://arxiv.org/abs/2311.09213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan</li>
<li>for: 这篇论文旨在帮助对话式角色扮演游戏（RPG）的强大故事创作。这些故事可能需要几年时间编写，通常需要一支大的创意团队。</li>
<li>methods: 该论文使用大型生成文本模型来帮助这个过程。它们提出了一种名为“GRIM”的图形基于交互式叙事系统，可以根据游戏设计师提供的高级叙事描述和约束生成丰富的叙事图。游戏设计师可以通过交互修改图表来生成新的子图，以保持在原始叙事和约束之内。</li>
<li>results: 该论文使用GPT-4生成分支叙事，对四个知名的故事进行了不同的上下文约束的应用。<details>
<summary>Abstract</summary>
Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GRIM}, a prototype \textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for ga\textbf{M}es, generates a rich narrative graph with branching storylines that match a high-level narrative description and constraints provided by the designer. Game designers can interactively edit the graph by automatically generating new sub-graphs that fit the edits within the original narrative and constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4, generating branching narratives for four well-known stories with different contextual constraints.
</details>
<details>
<summary>摘要</summary>
对话式角色游戏（RPG）需要强大的故事编写。这些故事可能需要几年时间写作，通常需要大量的创意团队。在这份工作中，我们展示了大量生成文本模型可以帮助这个过程。我们提出了一个名为“GRIM”的原型，它是一个基于图表的互动式剧本视觉系统，可以根据设计师提供的高级剧本描述和约束生成丰富的剧本图。游戏设计师可以通过交互地编辑图表，生成适应修改的新子图，以保持在原始剧本和约束之间。我们使用了GPT-4生成分支剧本，并将其应用于四个著名的故事中，以示其可行性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Chain-of-Thought-Prompting"><a href="#Contrastive-Chain-of-Thought-Prompting" class="headerlink" title="Contrastive Chain-of-Thought Prompting"></a>Contrastive Chain-of-Thought Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09277">http://arxiv.org/abs/2311.09277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/contrastive-cot">https://github.com/damo-nlp-sg/contrastive-cot</a></li>
<li>paper_authors: Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing</li>
<li>for: 提高语言模型的逻辑推理能力</li>
<li>methods: 使用对比性链条思维法，提供有效和无效示范，以导引模型逻辑推理步骤，降低逻辑错误</li>
<li>results: 在逻辑推理benchmark上实现了提高语言模型逻辑推理能力的效果<details>
<summary>Abstract</summary>
Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.
</details>
<details>
<summary>摘要</summary>
尽管链条思维在提高语言模型理解方面取得了成功，但下面的过程仍然较为不准确。虽然逻辑正确性看起来是链条思维的核心，但先前的研究显示使用无效示例时的影响却是很小。此外， convential链条思维不会告诉语言模型需要避免哪些错误，这可能会导致更多的错误。因此， inspirited by humans可以从正确和错误示例中学习，我们提出了对比链条思维，以帮助模型步骤 reasoning  while reducing reasoning mistakes。为了提高泛化性，我们提出了一种自动生成对比示例的方法。我们的实验表明，对比链条思维可以作为普遍提高链条思维提示的方法。
</details></li>
</ul>
<hr>
<h2 id="TableLlama-Towards-Open-Large-Generalist-Models-for-Tables"><a href="#TableLlama-Towards-Open-Large-Generalist-Models-for-Tables" class="headerlink" title="TableLlama: Towards Open Large Generalist Models for Tables"></a>TableLlama: Towards Open Large Generalist Models for Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09206">http://arxiv.org/abs/2311.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun</li>
<li>for: 这篇论文旨在开发一种通用的大语言模型（LLM），用于解决多种表格基于任务。</li>
<li>methods: 该论文使用了一种新的表格数据集——TableInstruct，用于调参和评估LLM。此外，它还开发了一种基于Llama 2（7B）的开源通用模型——TableLlama，通过长 Context 挑战进行了辅助。</li>
<li>results: 在7项域内任务中，TableLlama 在7项域内任务中具有相当或更好的性能，与特定任务设计的SOTA模型相比。在6个外域数据集上，它实现了6-48个绝对点提升，显示了训练在TableInstruct上增强了模型的通用性。<details>
<summary>Abstract</summary>
Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.
</details>
<details>
<summary>摘要</summary>
全文案表（Semi-structured tables）在各方面都非常普遍。有许多任务旨在自动地解释、增强和询问表格。当前的方法frequently需要表格预训练或特殊的模型建设，或者受到特定的表格类型限制，或者假设表格和任务之间的简化关系。本文为开发开源大语言模型（LLMs）作为表格任务的通用专家而做出了第一步。为此，我们构建了一个新的数据集——TableInstruct，用于调整和评估LLMs。我们还开发了第一个开源通用模型 для表格——TableLlama，通过细化Llama 2（7B）和LongLoRA来解决长上下文挑战。我们在域内设置和域外设置下进行了实验。在7个域内任务中，TableLlama与SOTA的性能相似或更好，即使后者经常有特定的设计。在6个域外数据集上，它与基模型相比实现了6-48个绝对点胜负。这表明训练在TableInstruct上有助于提高模型的通用性。我们将开源我们的数据集和训练模型，以便未来的开发开源模型。
</details></li>
</ul>
<hr>
<h2 id="When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages"><a href="#When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages" class="headerlink" title="When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages"></a>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09205">http://arxiv.org/abs/2311.09205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/curse-of-multilinguality">https://github.com/tylerachang/curse-of-multilinguality</a></li>
<li>paper_authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 这个论文的目的是研究多语言模型在各种语言中的表现。</li>
<li>methods: 该论文使用了多种方法，包括预训练10,000多种单语言和多语言模型，以及评估模型在不同语言中的表现。</li>
<li>results: 研究发现，在一定程度上添加多语言数据可以提高低资源语言模型的性能，与提高单语言数据大小相当。但是，高资源语言在多语言预训练场景下表现却一直差。随着数据大小的增加，添加多语言数据开始对所有语言的性能产生负面影响，可能是因为模型容量的限制（多语言预训练的咒语）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更定向的模型可以明显提高表现。<details>
<summary>Abstract</summary>
Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the "curse of multilinguality"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.
</details>
<details>
<summary>摘要</summary>
多语言语模型广泛应用于扩展NLP系统到低资源语言。然而，具体证据关于多语言性对语言模型性能的影响在个语言中尚缺乏。在这里，我们预训练了10,000多语言和多语言语模型，用于超过250种语言，包括一些在NLP领域未得到充分研究的语言家族。我们评估了在每种语言中语言模型性能如何随（1）单语言数据集大小、（2）添加多语言数据集大小、（3）添加语言的语法相似度和（4）模型大小（最多45M参数）的变化。我们发现，在moderate情况下，添加多语言数据可以提高低资源语言模型性能，与单语言数据集大小的增加效果相似，最多提高33%。改进取决于添加的多语言数据的语法相似度，与词汇重叠的较小影响。然而，高资源语言在多语言预训练场景下一直表现出较差的性能，而且随着数据集大小的增加，添加多语言数据开始对低资源语言和高资源语言都造成性能下降，可能是模型容量的限制（“多语言性的诅咒”）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更专注的模型可以显著提高性能。
</details></li>
</ul>
<hr>
<h2 id="Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models"><a href="#Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models" class="headerlink" title="Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models"></a>Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09194">http://arxiv.org/abs/2311.09194</a></li>
<li>repo_url: None</li>
<li>paper_authors: James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen</li>
<li>for: 这个论文的目的是探讨人类语言模型中的 grammatical knowledge 是如何抽象的？</li>
<li>methods: 作者使用了大量语言模型测试和比较人类实验结果，以确定模型中的 grammatical knowledge 是如何抽象的。</li>
<li>results: 研究发现，大量语言模型中的 grammatical knowledge 是抽象的，并且可以在不同语言之间帮助生成文本。此外，模型中的 grammatical knowledge 与人类的 grammatical abstraction 类似。<details>
<summary>Abstract</summary>
Abstract grammatical knowledge - of parts of speech and grammatical patterns - is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.
</details>
<details>
<summary>摘要</summary>
抽象语法知识是人类语言能力的关键之一，它允许人们通过语言总结来掌握语言规则。然而，大语言模型中的语法知识是如何抽象的？我们在人类文献中发现了强有力的证据，表明大语言模型中的语法知识具有抽象性。在人类实验中，研究人员发现，当一个句子和之前的句子有相同的语法结构时，它会更加容易地被理解和生成。由于单语言实验中存在干扰因素，因此跨语言结构预期的证据更加具有抽象性。我们在大语言模型中测试了跨语言结构预期，并与人类实验结果进行比较，来检验模型中的语法知识是否具有抽象性。我们发现，大语言模型中的语法知识不仅在不同语言之间具有相似性，而且可以影响不同语言中的文本生成。这些结果表明，大语言模型中的语法知识具有人类语言能力的相似性，并且可以通过语言总结来掌握不同语言的语法规则。
</details></li>
</ul>
<hr>
<h2 id="PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health"><a href="#PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health" class="headerlink" title="PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health"></a>PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09189">http://arxiv.org/abs/2311.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu</li>
<li>for: 本研究旨在提供大语言模型（LLMs）在心理健康领域的评估标准，填补当前领域评估 LLMs 的缺口。</li>
<li>methods: 我们提出了一个全面的评估标准，包括六个子任务，涵盖三个维度，以系统地评估 LLMs 在心理健康领域的能力。</li>
<li>results: 实验结果显示，目前的 LLMs 在心理健康领域仍有很大的改进空间，同时也揭示了未来模型优化的潜在方向。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in utilizing large language models (LLMs) in mental health research, with studies showcasing their remarkable capabilities, such as disease detection. However, there is currently a lack of a comprehensive benchmark for evaluating the capability of LLMs in this domain. Therefore, we address this gap by introducing the first comprehensive benchmark tailored to the unique characteristics of the mental health domain. This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mental health. We have designed corresponding concise prompts for each sub-task. And we comprehensively evaluate a total of eight advanced LLMs using our benchmark. Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.
</details>
<details>
<summary>摘要</summary>
近些时候，大语言模型（LLM）在心理健康研究中的应用已经引发了越来越多的关注，研究表明其拥有remarkable的能力，如疾病检测等。然而，当前心理健康领域中LLM的能力评估的标准化 bencmark 仍然缺失。因此，我们填补这个空白，引入了首个心理健康领域特有的全面性的bencmark。这个bencmark包括六个子任务，覆盖三个维度，以系统地评估 LLM 在心理健康领域的能力。我们设计了对应的简洁提示。我们也对八个高级 LLM 进行了广泛的评估。实验结果不仅表明当前 LLM 在心理健康领域存在大量的改进空间，还揭示了未来模型优化的潜在方向。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization"><a href="#Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization" class="headerlink" title="Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"></a>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09184">http://arxiv.org/abs/2311.09184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yale-nlp/instrusum">https://github.com/yale-nlp/instrusum</a></li>
<li>paper_authors: Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</li>
<li>for: 本研究旨在评估大型自然语言模型（LLM）在更复杂的摘要任务设定下的性能。</li>
<li>methods: 本研究使用了指令可控的文本摘要任务，其中模型输入包括源文章和自然语言的需要摘要特征。研究人员还创建了一个评估只的数据集，并对5种基于LLM的摘要系统进行了人工评估。</li>
<li>results: 研究发现，尽管LLMs已经在标准摘要任务上达到了强大的性能，但在更复杂的摘要任务设定下，LLMs仍然面临着很大的挑战。 Specifically, (1) 所有评测的LLM都会在摘要中出现事实和其他类型的错误; (2) 所有基于LLM的评估方法无法与人工评估人员一致地评估候选摘要的质量; (3) 不同的LLM在摘要生成和评估方面表现出了大的性能差异。<details>
<summary>Abstract</summary>
While large language models (LLMs) already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for the desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation. We make our collected benchmark, InstruSum, publicly available to facilitate future research in this direction.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在标准化的摘要任务上达到了强大的表现，但它们在更复杂的摘要任务设置下的表现尚未得到充分研究。因此，我们对 LLM 进行了 instruction controllable 文本摘要测试，其中模型输入包括源文章和自然语言需求，要求摘要具有特定的摘要特征。为此，我们为这个任务创建了评估数据集，并对 5 个 LLM 基于系统进行了人工评估。然后，我们对 LLM 自动评估方法进行了Benchmark，使用了 4 种评估协议和 11 个 LLM，共计 40 种评估方法。我们的研究发现，instruction controllable 文本摘要仍然是 LLM 面临的挑战，因为：1. 所有评估的 LLM 都会在摘要中出现事实和其他类型的错误。2. 所有 LLM 基于的自动评估方法无法与人类评分员对候选摘要质量的评估实现强Alignment。3. 不同的 LLM 在摘要生成和评估中表现出了大的性能差距。我们将我们收集的 benchmark，InstruSum，公开发布，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models"><a href="#ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models" class="headerlink" title="ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models"></a>ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09182">http://arxiv.org/abs/2311.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jierui Li, Vipul Raheja, Dhruv Kumar</li>
<li>for: 研究长文检测自矛盾能力</li>
<li>methods: 使用四种现有的开源和商业可用的大语言模型进行实验</li>
<li>results: GPT4表现最佳，可以超过人类的表现，但是它在需要更多细节和上下文的自矛盾检测方面存在不可靠性问题。<details>
<summary>Abstract</summary>
In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradictions types, and scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers"><a href="#PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers" class="headerlink" title="PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers"></a>PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09180">http://arxiv.org/abs/2311.09180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi</li>
<li>for: 该论文旨在提高大语言模型写作助手的质量和效率，但它们的输出往往缺乏个人化和专业知识。该论文提出了一种解决方案，即使用PEARL Writing Assistant，它是一种基于检索的语言模型写作助手，可以个性化为作者的写作风格和专业知识。</li>
<li>methods: 该论文提出了两个关键新想法来训练检索器：1）一种用于选择需要个性化的用户请求和文档，以便用于Language Model的生成个性化；2）一种可以使检索器与生成的文本之间进行匹配，以确保生成的文本能够尽可能地适应用户的写作风格和专业知识。</li>
<li>results: 该论文通过用PEARL Writing Assistant进行实验，证明了这种方法的效iveness。它可以在工作室社交媒体文章和Reddit评论等场景中生成个性化的文本，并且可以用作生成优化和性能预测。<details>
<summary>Abstract</summary>
Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.
</details>
<details>
<summary>摘要</summary>
强大的大语言模型已经促进了写作助手的发展，这些助手承诺可以大幅提高作文和communication的质量和效率。然而，一个阻碍有效协助的问题是大语言模型的输出没有个性化到作者的沟通风格和专业知识。在这篇论文中，我们解决这个挑战 by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever.我们的检索器是通过选择用户作者编写的历史文档来补充提示，以便它们可以最好地个性化LLM生成 для用户请求。我们提出了两个关键的新特点 для训练我们的检索器： 1）一种用于选择可以受益于个性化的用户请求和文档的训练数据选择方法; 2）一种托管KL散度目标，以确保我们的检索器准确地跟踪个性化文档的优势。我们示出了PEARL在生成个性化工作室社交媒体帖子和Reddit评论时的效果。最后，我们展示了一个生成检索器可以 doubles as a performance predictor，并通过LLM链接来进一步改善低质量生成。
</details></li>
</ul>
<hr>
<h2 id="SiRA-Sparse-Mixture-of-Low-Rank-Adaptation"><a href="#SiRA-Sparse-Mixture-of-Low-Rank-Adaptation" class="headerlink" title="SiRA: Sparse Mixture of Low Rank Adaptation"></a>SiRA: Sparse Mixture of Low Rank Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09179">http://arxiv.org/abs/2311.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng</li>
<li>for: 这个研究的目的是提高Large Language Model的 Parameters Efficient Tuning 能力，以适应下游任务。</li>
<li>methods: 这个研究使用了Sparse Mixture of Expert（SMoE）来增强LoRA的表现，并导入了一个新的专家抽掉法以减少过滤问题。</li>
<li>results: 这个研究发现SiRA可以在不同单任务和多任务设定下表现更好些，并且比LoRA和其他混合专家方法更好。<details>
<summary>Abstract</summary>
Parameter Efficient Tuning has been an prominent approach to adapt the Large Language Model to downstream tasks. Most previous works considers adding the dense trainable parameters, where all parameters are used to adapt certain task. We found this less effective empirically using the example of LoRA that introducing more trainable parameters does not help. Motivated by this we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of Expert(SMoE) to boost the performance of LoRA. Specifically it enforces the top $k$ experts routing with a capacity limit restricting the maximum number of tokens each expert can process. We propose a novel and simple expert dropout on top of gating network to reduce the over-fitting issue. Through extensive experiments, we verify SiRA performs better than LoRA and other mixture of expert approaches across different single tasks and multitask settings.
</details>
<details>
<summary>摘要</summary>
“Parameter Efficient Tuning”是一种受欢迎的方法，用于适应大型语言模型下推导任务。大多数先前的工作假设所有 Parameters 都是用来适应特定任务。但我们在 LoRA 的例子中发现，增加更多可读的 Parameters 并不会帮助。 Motivated by this, we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA 利用 Sparse Mixture of Expert (SMoE) 来提高 LoRA 的性能。具体来说，它强制顶几个专家路由中的范围限制每个专家可以处理的最多字元数量。我们提出了一个新的和简单的专家抽出方法，用于降低过滤问题。经过广泛的实验，我们证明 SiRA 比 LoRA 和其他混合专家方法在不同的单任务和多任务设定下表现更好。
</details></li>
</ul>
<hr>
<h2 id="CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models"><a href="#CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models" class="headerlink" title="CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models"></a>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09154">http://arxiv.org/abs/2311.09154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu</li>
<li>for: 评估大语言模型（LLM）的真实能力，因为数据污染可能导致模型的评估结果受到影响。</li>
<li>methods: 提出了一种新的方法——Clean-Eval，可以减少数据污染的影响，并更加准确地评估LLM。Clean-Eval使用LLM来重新表述和翻译污染数据，生成表达相同意义的不同表面形式的表达集。然后使用一种Semantic detector来筛选生成的低质量样本，将其缩小到候选集。最后，根据BLEURT分数选择候选集中的最佳表达。</li>
<li>results: 根据人工评估，最佳表达与原始污染数据具有相同的含义，但表达方式不同。所有候选表达可以组成一个新的评估标准。我们的实验表明，Clean-Eval可以减少LLM在几个不同场景下的污染数据下的评估结果的偏差。<details>
<summary>Abstract</summary>
We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.
</details>
<details>
<summary>摘要</summary>
现在是大型语言模型（LLM）的激烈竞争时代，不断推动测试数据的边界。然而，评估这些 LLM 的真实能力已成为一个困难和重要的问题，因为数据污染。这会浪费研究人员和工程师几十个时间和努力来下载和尝试这些污染的模型。为了保存我们的宝贵时间，我们提出了一种新的方法——Clean-Eval，它解决了数据污染问题，并在不同表面形式下评估 LLM。Clean-Eval 使用一个 LLM 来重新表述和反翻污染数据，生成表达相同含义，但表面形式不同的集合。然后，使用一个semantic detector来筛选生成的低质量样本，将这些样本缩小到候选集。最后，选择候选集中的最佳样本，根据 BLEURT 分数。根据人工评估，这个最佳样本与原始污染数据具有相同的含义，但表现在不同的表面形式下。所有候选样本可以组成一个新的评估标准。我们的实验表明，Clean-Eval 可以减少 LLM 在污染数据下的实际评估结果的损失，在少量学习和微调准备 scenarios 下都有显著提高。
</details></li>
</ul>
<hr>
<h2 id="Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders"><a href="#Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders" class="headerlink" title="Grounding or Guesswork? Large Language Models are Presumptive Grounders"></a>Grounding or Guesswork? Large Language Models are Presumptive Grounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09144">http://arxiv.org/abs/2311.09144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, Dan Jurafsky</li>
<li>for: 本研究旨在探讨人工智能（AI）与人之间的对话中是否存在共同基础（common ground），以及AI是如何构建共同基础的。</li>
<li>methods: 本研究使用了一系列对话动作（like clarification和acknowledgment）来评估AI是否能够正确地构建共同基础。</li>
<li>results: 研究发现现有的大语言模型（LLMs）偏向假设共同基础而不使用对话动作来构建共同基础，并且通过对人类反馈（RLHF）的调教和强化学习来降低对共同基础的依赖性。<details>
<summary>Abstract</summary>
Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding prevents misunderstanding. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets, and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Altogether, our work highlights the need for more research investigating grounding in human-AI interaction.
</details>
<details>
<summary>摘要</summary>
Effective conversation requires common ground: a shared understanding between the participants. However, common ground does not emerge spontaneously in conversation. Speakers and listeners must work together to identify and construct a shared basis while avoiding misunderstandings. To accomplish grounding, humans rely on a range of dialogue acts, such as clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding is essential to prevent misunderstandings. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To address this question, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Overall, our work highlights the need for more research investigating grounding in human-AI interaction.
</details></li>
</ul>
<hr>
<h2 id="RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context"><a href="#RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context" class="headerlink" title="RRescue: Ranking LLM Responses to Enhance Reasoning Over Context"></a>RRescue: Ranking LLM Responses to Enhance Reasoning Over Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09136">http://arxiv.org/abs/2311.09136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Wang, Rui Zheng, Haoming Li, Qi Zhang, Tao Gui, Fei Liu<br>for: 这篇论文的目的是提高大语言模型（LLMs）的上下文理解能力，以便更好地应用在各种任务上。methods: 这篇论文使用了一种新的应对方法，即使用排名 metrics， teach LLMs 如何对上下文地标的候选答案进行排序。results: 在使用 latest 的多文档问答数据集进行测试时，这种方法能够提高 LLMs 的上下文理解能力，并且可以通过人工标注、自适应函数或模型蒸馏来获得 partial ordering。<details>
<summary>Abstract</summary>
Effectively using a given context is paramount for large language models. A context window can include task specifications, retrieved documents, previous conversations, and even model self-reflections, functioning similarly to episodic memory. While efforts are being made to expand the context window, studies indicate that LLMs do not use their context optimally for response generation. In this paper, we present a novel approach to optimize LLMs using ranking metrics, which teaches LLMs to rank a collection of contextually-grounded candidate responses. Rather than a traditional full ordering, we advocate for a partial ordering. This is because achieving consensus on the perfect order for system responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be acquired through human labelers, heuristic functions, or model distillation. We test our system's improved contextual understanding using the latest benchmarks, including a new multi-document question answering dataset. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RRescue, suggests a promising avenue for enhancing LLMs' contextual understanding via response ranking.
</details>
<details>
<summary>摘要</summary>
使用给定的上下文非常重要 для大型自然语言模型。上下文窗口可以包括任务规定、检索到的文档、前一次的对话和模型自我反思，功能类似于 episodic memory。然而，研究表明，大型自然语言模型并不会使用上下文最佳化回应生成。在这篇论文中，我们提出了一种新的方法，使用排名指标优化大型自然语言模型，教育它们排序一个上下文搜索的候选答案集。而不是传统的全局排序，我们支持偏好排序。这是因为在系统回应的完整排序中达成一致可能困难。我们的偏好排序更加稳定， menosensitive to noise，可以通过人类标注者、规则函数或模型泛化来获得。我们在最新的测试板上测试了我们的改进的上下文理解系统，包括一个新的多文档问答数据集。我们进行了剖析研究，了解关键因素，如如何收集候选答案，如何确定最适合的排序顺序，以及如何平衡监督精度抽象与排名指标。我们的方法，名为RRescue，提出了可能提高大型自然语言模型的上下文理解能力的有希望之路。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference"><a href="#Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference" class="headerlink" title="Aligning Neural Machine Translation Models: Human Feedback in Training and Inference"></a>Aligning Neural Machine Translation Models: Human Feedback in Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09132">http://arxiv.org/abs/2311.09132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</li>
<li>for: 这个论文的目的是提高机器翻译模型的质量，使其更加接近人类生成的语言。</li>
<li>methods: 这个论文使用了人类反馈来训练RLHF技术，并在机器翻译领域使用了最小极大 bayes风险解oding和重新排序技术来提高翻译质量。</li>
<li>results: 这个论文的实验结果表明，通过使用优质度计数来筛选数据，RL在训练阶段可以具有更高的效果，并且将RL训练与重新排序技术结合使用可以提高翻译质量。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.
</details>
<details>
<summary>摘要</summary>
人类反馈学习（RLHF）是一种最近的技术，用于提高语言模型生成的文本质量，使其更加接近人类的生成。RLHF的成功之一是其使用人类反馈来训练奖励模型，从而使大型语言模型（LLM）更加与人类语言相似。在机器翻译（MT）领域，可以 readily使用人类标注来训练度量模型的奖励模型，最近的方法使用最小极大似然解码和重新排序技术，已经在改进翻译质量中取得了成功。在这个研究中，我们全面探讨并比较了将质量度量作为奖励模型integrated到MT管道中的技术。这包括在数据过滤、训练阶段通过RL、以及在推理阶段通过重新排序技术来使用奖励模型，并评估这些技术的组合效果。我们的实验结果，在多个翻译任务上进行了评测，强调了有效的数据过滤对RL的潜在力量的激发，并证明了将RL训练与重新排序技术相结合的效果。
</details></li>
</ul>
<hr>
<h2 id="Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes"><a href="#Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes" class="headerlink" title="Social Meme-ing: Measuring Linguistic Variation in Memes"></a>Social Meme-ing: Measuring Linguistic Variation in Memes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09130">http://arxiv.org/abs/2311.09130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naitian/semantic-memes">https://github.com/naitian/semantic-memes</a></li>
<li>paper_authors: Naitian Zhou, David Jurgens, David Bamman</li>
<li>for: 这篇论文旨在探讨Memes的社会变化，即通过计算方法来探索文本中的社会语言变化。</li>
<li>methods: 作者构建了一个计算管道，用于将个体Memes clustering到模板和semantic变量中，利用它们的多modal结构来做到这一点。</li>
<li>results: 作者通过对Reddit上的大量Memes图像使用这个计算管道，建立了一个名为\textsc{SemanticMemes}的数据集，包含3.8万个图像，并发现了社会意义的变化在Memes中，以及在这些社区内部的Memes创新和同化的模式，与之前关于written语言的发现相一致。<details>
<summary>Abstract</summary>
Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting \textsc{SemanticMemes} dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.
</details>
<details>
<summary>摘要</summary>
很多NLP领域的工作都使用计算方法来探索社会语言变化。在这篇论文中，我们 argue that memes，作为 Multimodal 的语言形式，也表现出社会意义的变化。我们构建了一个计算管道来将个体级别的 memes 分为模板和 semantics 变量，利用它们的 Multimodal 结构来做到这一点。我们对 Reddit 上的大量 meme 图片集 applies 这种方法，并将结果作为 \textsc{SemanticMemes} 数据集提供，包含 3.8 万个图片，按Semantic 功能进行分类。我们使用这些分类来分析 meme 的语言变化，发现不仅社会意义的 meme 使用存在 между subredits，而且在这些社区中的 meme 创新和吸收 align 与 previous 的 Written 语言发现。
</details></li>
</ul>
<hr>
<h2 id="Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark"><a href="#Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark" class="headerlink" title="Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark"></a>Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09122">http://arxiv.org/abs/2311.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Šuppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter</li>
<li>for: 本研究的目的是开发多种语言的名实体识别（NER）benchmark，以便实现跨语言的NER研究标准化。</li>
<li>methods: 本研究使用了一种开放的社区驱动方法，通过各种语言的名实体标注来创建了18个数据集。</li>
<li>results: 本研究提供了跨语言一致的名实体标注，并在语言内和跨语言学习环境下提供了初步的模型基线。<details>
<summary>Abstract</summary>
We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
</details>
<details>
<summary>摘要</summary>
我们介绍 Universal NER (UNER)，一个开放、社区驱动的项目，旨在开发多种语言的高质量命名实体识别（NER）标准 benchmarcks。UNER v1 包含 18 个语言的命名实体，采用跨语言一致的标准化 schema，来促进和标准化多语言 NER 研究。在这篇文章中，我们详细介绍了 UNER 的数据创建和组合，并提供了在本语言和跨语言学习环境中的初步模型基线。我们将数据、代码和适应模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces"><a href="#R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces" class="headerlink" title="R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces"></a>R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09117">http://arxiv.org/abs/2311.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng-Jui Chang, James Glass</li>
<li>for: 这篇论文旨在提出一种数据效率的自主超vised fine-tuning框架，用于获得Speaker和噪声不变的语音表示。</li>
<li>methods: 该框架基于学习精细的声学单元，通过Speaker-invariant clustering（Spin）来学习不同Speaker的语音特征。它解决了Spin的问题，并提高了内容表示的精度。</li>
<li>results: 该框架可以在严重扭曲的语音场景下表现出优于之前的状态艺术方法，同时具有12倍的计算资源减少。论文还提供了详细的分析，以解释如何使用精细的声学单元来改善语音编码器的训练和鲁棒性。<details>
<summary>Abstract</summary>
This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised fine-tuning framework for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
</details>
<details>
<summary>摘要</summary>
Note:* "speaker and noise-invariant" is translated as " speaker和噪音不变" (speaker yuè yīn bù biàn)* "discrete acoustic units" is translated as "精确的声学单元" (jīng yì de shēng xué dan yuán)* "speaker-invariant clustering" is translated as " speaker不变聚类" (speaker bù biàn jù lèi)* "acoustic pieces" is translated as "声学副本" (shēng xué fù ben)* "severely distorted speech scenarios" is translated as "严重扭曲的speech场景" (yán jí gōu qū de speech chǎng jìng)
</details></li>
</ul>
<hr>
<h2 id="“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context"><a href="#“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context" class="headerlink" title="“We Demand Justice!”: Towards Grounding Political Text in Social Context"></a>“We Demand Justice!”: Towards Grounding Political Text in Social Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09106">http://arxiv.org/abs/2311.09106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</li>
<li>for: 本研究旨在 Computational Setting中理解政治漫说中的ambiguous statements，并将其与实际世界 Entities、Actions和Attitudes相关联。</li>
<li>methods: 本研究使用了two challenging datasets，以及基于大型预训模型BERT、RoBERTa、GPT-3等的基eline模型。此外，还开发了基于现有的’Discourse Contextualization Framework’和’Political Actor Representation’模型。</li>
<li>results: 研究人员对 datasets和基eline模型进行了分析，以获取更多关于 Pragmatic Language Understanding 挑战的信息。<details>
<summary>Abstract</summary>
Social media discourse from US politicians frequently consists of 'seemingly similar language used by opposing sides of the political spectrum'. But often, it translates to starkly contrasting real-world actions. For instance, "We need to keep our students safe from mass shootings" may signal either "arming teachers to stop the shooter" or "banning guns to reduce mass shootings" depending on who says it and their political stance on the issue. In this paper, we define and characterize the context that is required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. To that end, we propose two challenging datasets that require an understanding of the real-world context of the text to be solved effectively. We benchmark these datasets against baselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3, etc. Additionally, we develop and benchmark more structured baselines building upon existing 'Discourse Contextualization Framework' and 'Political Actor Representation' models. We perform analysis of the datasets and baseline predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks.
</details>
<details>
<summary>摘要</summary>
社交媒体讨论由美国政治人物频繁使用"看起来类似的语言",但实际上可能表示 starkly contrasting 的real-world actions。例如，"我们需要保护学生免受枪击"可能表示" arm teachers to stop the shooter" 或 "ban guns to reduce mass shootings"，这取决于谁说这些话和他们对这个问题的政治立场。在这篇论文中，我们定义和描述了需要具备 computational setting 来完全理解这种ambiguous statements的上下文，并将其固定在 real-world entities, actions, and attitudes 上。为此，我们提出了两个具有挑战性的dataset，需要对文本的上下文有深入的理解才能解决 Effectively。我们对这些dataset进行了基于大型预训word models such as BERT, RoBERTa, GPT-3, etc.的基准测试，并开发了基于现有的'Discourse Contextualization Framework'和'Political Actor Representation'模型的更结构化基准。我们对dataset和基准预测进行分析，以获取更多的具体的语言理解挑战的信息。
</details></li>
</ul>
<hr>
<h2 id="MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation"><a href="#MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation" class="headerlink" title="MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation"></a>MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09105">http://arxiv.org/abs/2311.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li</li>
<li>for: 本研究旨在提供一个完整的事件理解 dataset，以支持事件探测、事件Argument抽象和事件关系抽象。</li>
<li>methods: 本研究使用 MAVEN dataset 进行事件探测和事件Argument抽象，并提供了一个完整的事件词汇表，包括 162 个事件类型和 612 个 Argument 角色。</li>
<li>results: 实验结果显示，MAVEN-Arg 是一个具有挑战性的 dataset，能够测试精炼 EAE 模型和大型自然语言模型 (LLM) 的能力。此外，我们还初步探索了未来事件预测的应用，使用 LLM。<details>
<summary>Abstract</summary>
Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from https://github.com/THU-KEG/MAVEN-Argument.
</details>
<details>
<summary>摘要</summary>
“理解文本中的事件是自然语言理解的核心目标，需要检测事件发生，提取事件参数，并分析事件之间的关系。然而，由于任务复杂性带来的标注挑战，一个大规模的事件理解 dataset 长时间缺 absent。在这篇论文中，我们引入 MAVEN-Arg，它将 MAVEN 数据集添加事件参数标注，成为了自然语言理解的所有过程的第一个一站式 dataset。作为 EAE Benchmark，MAVEN-Arg 提供了以下三个主要优势：（1）全面的 schema 涵盖 162 种事件类型和 612 个参数角色，均有专家写作的定义和示例；（2）庞大的数据规模，包含 98,591 个事件和 290,613 个参数，通过人工标注获得；（3）完整的标注，支持所有 EAE 任务的 variant，包括实体和非实体事件参数的文档级别标注。实验表明，MAVEN-Arg 对于 fine-tuned EAE 模型和专有大语言模型 (LLM) 都是非常困难的。此外，为了展示全站dataset 的优势，我们初步探索了未来事件预测的应用，使用 LLM。MAVEN-Arg 和我们的代码可以从 <https://github.com/THU-KEG/MAVEN-Argument> 获取。”
</details></li>
</ul>
<hr>
<h2 id="Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization"><a href="#Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization" class="headerlink" title="Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"></a>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09096">http://arxiv.org/abs/2311.09096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/jailbreakdefense_goalpriority">https://github.com/thu-coai/jailbreakdefense_goalpriority</a></li>
<li>paper_authors: Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang</li>
<li>for: 防御大语言模型（LLMs）各种攻击，特别是监禁攻击（jailbreaking attacks）。</li>
<li>methods: 提出了一种约束目标优先级的方法，用于在训练和推理阶段对攻击进行防御。</li>
<li>results: 实现了在不妨碍总体性能的情况下大幅降低监禁攻击的成功率，从66.4%降低到2.0%和68.2%降低到19.4%，并且在没有监禁样本的情况下也能够减少成功率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks. While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of exploration into defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the inherent conflict between the goals of being helpful and ensuring safety. To counter jailbreaking attacks, we propose to integrate goal prioritization at both training and inference stages. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to 2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising general performance. Furthermore, integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half, decreasing it from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks. We hope our work could contribute to the comprehension of jailbreaking attacks and defenses, and shed light on the relationship between LLMs' capability and safety. Our code will be available at \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的能力不断提高，但这也涉及到一系列的安全隐患。虽然有很多研究利用 LLM 的弱点进行破坏攻击，但对于防御这类攻击的研究却受到了相对的少量关注。我们发现，破坏攻击的成功主要归功于 LLM 的目标冲突。为了应对破坏攻击，我们提议在训练和执行阶段都实施目标优先级。在执行阶段实施目标优先级后，破坏攻击的成功率（ASR）从原来的 66.4% 降低到 2.0%，对 ChatGPT 和 Vicuna-33B 进行了显著的改善。此外，在训练阶段添加目标优先级也使 ASR 从 71.0% 降低到 6.6%，而无需包含破坏样本。甚至在没有破坏样本的情况下，我们的方法仍然可以将 ASR 降低到 34.0%。此外，我们的研究发现，强大的 LLM 面临更大的安全隐患，但它们也拥有更大的防御能力。我们希望我们的研究可以对破坏攻击和防御提供更多的理解，并探讨 LLM 的能力和安全之间的关系。我们的代码将在 GitHub 上公开，可以在 \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority} 获取。
</details></li>
</ul>
<hr>
<h2 id="Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models"><a href="#Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models" class="headerlink" title="Social Bias Probing: Fairness Benchmarking for Language Models"></a>Social Bias Probing: Fairness Benchmarking for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09090">http://arxiv.org/abs/2311.09090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein</li>
<li>for: 本研究旨在探讨语言模型中社会偏见的问题，以及这些偏见对下游应用的风险。</li>
<li>methods: 该研究提出了一种新的探测语言模型社会偏见的方法，包括采集探测数据集，分析语言模型的通用关系以及社会分类、身份和刻板印象的关系。该方法还使用了一种新的准确度基准分数来衡量语言模型的公平性。</li>
<li>results: 该研究发现，较大的语言模型变体具有更高度的偏见，而且不同的宗教身份导致模型对不同身份的用户展现出最大的不同待遇。此外，该研究还发现了现有公平数据集的缺陷和局限性，并提出了一种更加全面和多样化的公平数据集。<details>
<summary>Abstract</summary>
Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经显示出了多种社会偏见，这可能会导致下游的危害。虽然这些偏见的影响已经被认可，但先前的偏见评估方法受限于小型数据集上的 binary association 测试，这只是社会偏见在语言模型中的一种封闭的视图。在这篇论文中，我们提出了一种新的探测语言模型社会偏见的框架。我们收集了一个探测数据集，以分析语言模型的通用关联以及社会分类、标签和刻板印象的方向。为此，我们利用了一种新的减少公平分数。我们创建了一个大规模的比较数据集，以解决现有公平数据集中的缺陷和限制，扩展到不同的标签和刻板印象。与先前的工作相比，我们发现biases在语言模型中更加复杂，大型模型变体更加偏见。此外，我们发现表达不同宗教标签时，模型对不同标签的对待最为不平等。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts"><a href="#Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts" class="headerlink" title="Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts"></a>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09066">http://arxiv.org/abs/2311.09066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan</li>
<li>for: 这个研究的目的是为了开发一种能够有效地识别受影响的患者，以预防意外的抗痛药过量服用。</li>
<li>methods: 这个研究使用了社区基于的社交媒体平台Reddit上的用户自透 leak 的信息，以及不同阶段的抗痛药使用（医疗使用、虚假使用、成瘾、恢复、重复、不使用）的2500个相关帖子，进行 Span-level 抽象解释和模型开发。</li>
<li>results: 研究发现，使用解释在模型开发中具有显著的提升作用，可以提高分类精度。但是，该研究还发现，识别抗痛药使用症状是非常Contextual和挑战性的。<details>
<summary>Abstract</summary>
In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids (https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national public health emergency (USDHHS, 2017). To more effectively prevent unintentional opioid overdoses, medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors, often acting as indicators for opioid use disorder. Towards this, we present a moderate size corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details>
<details>
<summary>摘要</summary>
在过去一个 décennial，美国已经失去了超过500,000名人因为投药和非法投药过量（https://www.cdc.gov/drugoverdose/epidemic/index.html），这导致了国家公共卫生紧急状况（USDHHS，2017）。为更好地预防意外投药过量，医疗人员需要更加强大和时间准确的工具，以更好地识别有风险的病人。社区基础的社交媒体平台，如Reddit，允许用户自透示自己的行为，并经常作为投药使用障碍的指示器。为了应对这一点，我们提供了2500篇关于投药的各种帖子，从6个不同阶段的投药使用中选择出来：医疗使用、滥用、成瘾、恢复、重新滥用和不使用。对每篇帖子，我们进行了span级抽象解释，并且关注其在模型开发中的作用。我们评估了多种当前最佳模型，包括一些监督学习、少量学习和零量学习的设置。实验结果和错误分析表明，识别投药使用疾病的阶段是非常Contextual和复杂的。然而，我们发现使用解释在模型化时带来了显著的分类精度提升，这demonstrates其在高度重要的域如投药使用疾病continuum中的有益作用。这个数据集将在GitHub上公开，以便研究人员进行研究。
</details></li>
</ul>
<hr>
<h2 id="Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs"><a href="#Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs" class="headerlink" title="Do Localization Methods Actually Localize Memorized Data in LLMs?"></a>Do Localization Methods Actually Localize Memorized Data in LLMs?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09060">http://arxiv.org/abs/2311.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Yun Chang, Jesse Thomason, Robin Jia</li>
<li>for: 这个论文研究了如何在大语言模型（LLM）中 lokalisiert（localize）一小集 neurons responsible for memorizing a given sequence.</li>
<li>methods: 这个论文使用了两种benchmarking Approach来测试 lokalisiert方法，包括INJ Benchmark和DEL Benchmark。</li>
<li>results: 这个论文发现了五种 lokalisiert方法，包括pruning-based methods，可以准确地 lokalisiertLLM中的neurons。但是，这些neurons不一定是specific to a single memorized sequence.<details>
<summary>Abstract</summary>
Large language models (LLMs) can memorize many pretrained sequences verbatim. This paper studies if we can locate a small set of neurons in LLMs responsible for memorizing a given sequence. While the concept of localization is often mentioned in prior work, methods for localization have never been systematically and directly evaluated; we address this with two benchmarking approaches. In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pretrained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model. We evaluate five localization methods on our two benchmarks, and both show similar rankings. All methods exhibit promising localization ability, especially for pruning-based methods, though the neurons they identify are not necessarily specific to a single memorized sequence.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以传递许多预训序列的内容，这篇研究论文探讨了我们可以在LLM中找到记忆一个给定序列的小量神经元。虽然对于localization的概念在先前的工作中有提及，但是对于localization的方法从未得到系统和直接的评估，我们在这篇研究中解决这个问题。我们使用INJ Benchmark和DEL Benchmark two个benchmarking方法进行评估。在INJ Benchmark中，我们 aktiveinject一小部分LLM weights中的新信息，然后衡量localization方法是否可以识别这些"真实" weights。在DEL Benchmark中，我们研究LLMs已经记忆过的预训数据的localization;虽然这个设定没有真实的对照，但我们仍然可以评估localization by measuring whether dropping out located neurons can erase a memorized sequence from the model。我们评估了五种localization方法在我们的两个benchmark上，结果显示所有方法都具有良好的localization能力，特别是针对剪裁方法，不过这些神经元可能不专门用于记忆一个单一的序列。
</details></li>
</ul>
<hr>
<h2 id="GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models"><a href="#GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models" class="headerlink" title="GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"></a>GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09048">http://arxiv.org/abs/2311.09048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni</li>
<li>for: 评估视频基于多模态语言模型的语言固定和物理理解能力</li>
<li>methods: 使用 Unity 模拟环境，采用两个级别的评估方法：第一级测试语言固定，第二级测试物理理解原则，如物体残留和连续性</li>
<li>results: 现有多模态语言模型存在语言固定和物理理解的显著缺陷，GRASP benchmark 可以帮助评估未来模型的发展。<details>
<summary>Abstract</summary>
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The initial level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of 'Intuitive Physics' principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in current models' language grounding and intuitive physics. These identified limitations underline the importance of benchmarks like GRASP to monitor the progress of future models in developing these competencies.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "video-based" is translated as "视频基于的" (video-based)* "multimodal" is translated as "多Modal" (multimodal)* "large language models" is translated as "大型语言模型" (large language models)* "Unity simulations" is translated as "Unity simulations" (no change)* "language grounding" is translated as "语言固定" (language grounding)* "intuitive physics" is translated as "直觉物理" (intuitive physics)* "object permanence" is translated as "物体存在性" (object permanence)* "continuity" is translated as "连续性" (continuity)
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation" class="headerlink" title="Exploring the Potential of Large Language Models in Computational Argumentation"></a>Exploring the Potential of Large Language Models in Computational Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09022">http://arxiv.org/abs/2311.09022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/llm-argumentation">https://github.com/damo-nlp-sg/llm-argumentation</a></li>
<li>paper_authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing</li>
<li>for: 这项研究的目的是评估大语言模型（LLMs）在计算论证领域的表现，包括逻辑推理、论证分析和对话管理等。</li>
<li>methods: 这项研究使用了多种计算论证任务，包括论证挖掘和论证生成等，以及对多种存在的开源数据集进行标准化。</li>
<li>results: 研究发现LLMs在大多数数据集上表现出色，能够快速学习和掌握计算论证任务，但也存在一些限制和挑战，如逻辑推理和对话管理等。<details>
<summary>Abstract</summary>
Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing (NLP) that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into 6 main classes and standardise the format of 14 open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of these datasets, demonstrating their capabilities in the field of argumentation. We also highlight the limitations in evaluating computational argumentation and provide suggestions for future research directions in this field.
</details>
<details>
<summary>摘要</summary>
computational argumentation 已成为不同领域的重要工具，包括人工智能、法律和公共政策。这是自然语言处理（NLP）的一个emerging research field，吸引越来越多的关注。研究计算 argued mainly involves two types of tasks: argument mining and argument generation。由于大语言模型（LLMs）在理解上下文和生成自然语言方面表现出色，因此值得评估 LLMS 在不同的计算 argued task 上的表现。这项工作的目的是评估 ChatGPT、Flan 模型和 LLaMA2 模型在零上下文和几上下文 Setting 下的表现。我们将现有任务分为 6 个主要类别，并标准化Open-source dataset 的格式。此外，我们还提供了一个新的对话 Generation 的benchmark dataset，以全面评估 LLMs 在 argued mining 和 argued generation 方面的综合性表现。广泛的实验表明 LLMS 在大多数数据集上表现出色，证明它们在 argued 领域的能力。我们还指出了计算 argued 的评估限制，并提出了未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions"><a href="#End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions" class="headerlink" title="End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions"></a>End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09008">http://arxiv.org/abs/2311.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li</li>
<li>for: 本研究旨在进行综述和总结现有的End-to-end Task-oriented Dialogue（EToD）研究，以便为后续研究提供一个统一的视角。</li>
<li>methods: 本文使用大量的预训练模型，特别是深度神经网络，以实现EToD研究的进步。</li>
<li>results: 本文提供了一个综述EToD研究的报告，包括现有的方法和新趋势，并提供了一个公共网站，为EToD研究人员提供最新的进展。<details>
<summary>Abstract</summary>
End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) \textbf{\textit{First survey}: to our knowledge, we take the first step to present a thorough survey of this research field; (2) \textbf{\textit{New taxonomy}: we first introduce a unified perspective for EToD, including (i) \textit{Modularly EToD} and (ii) \textit{Fully EToD}; (3) \textbf{\textit{New Frontiers}: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) \textbf{\textit{Abundant resources}: we build a public website\footnote{We collect the related papers, baseline projects, and leaderboards for the community at \url{https://etods.net/}.}, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.
</details>
<details>
<summary>摘要</summary>
End-to-end任务对话（EToD）可以直接生成回复，无需模块化训练，这在最近几年内逐渐受到欢迎。深度神经网络的发展，特别是大型预训练模型的成功使用，进一步推动了EToD研究的进步。在这篇论文中，我们提供了一份系统性的回顾和总结，旨在总结现有的方法和最新的趋势，以推动EToD研究的发展。本文的贡献包括：1. 首次survey：我们知道的情况下，我们是第一个对这个研究领域进行了全面的survey。2. 新分类：我们首先引入了EToD的一种统一视角，包括（i）模块性EToD和（ii）完全EToD。3. 新前ier：我们讨论了一些潜在的前沿领域，以及相应的挑战，希望通过这些讨论激发EToD领域的突破性研究。4. 丰富资源：我们建立了一个公共网站（https://etods.net/）， гдеEToD研究人员可以直接访问最新的进展。我们希望这项工作可以 serves as a comprehensive reference for EToD研究社区。
</details></li>
</ul>
<hr>
<h2 id="Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance"><a href="#Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance" class="headerlink" title="Data Similarity is Not Enough to Explain Language Model Performance"></a>Data Similarity is Not Enough to Explain Language Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09006">http://arxiv.org/abs/2311.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gyauney/data-similarity-is-not-enough">https://github.com/gyauney/data-similarity-is-not-enough</a></li>
<li>paper_authors: Gregory Yauney, Emily Reif, David Mimno</li>
<li>for: 这个论文主要用于探讨语言模型在下游任务中的表现是如何受到预训练数据和任务数据之间的互动影响。</li>
<li>methods: 这个论文使用了多种类型的相似度度量（包括嵌入-, 字元-和模型基于的相似度）来评估语言模型在不同下游任务中的表现。</li>
<li>results: 研究发现，在多语言任务中，相似度度量与语言模型的表现有正相关关系，但在其他的benchmark中，相似度度量之间并不存在相似的相关性，甚至在准确率和相似度度量之间也没有相似的相关性。这说明预训练数据和下游任务之间的关系更加复杂。<details>
<summary>Abstract</summary>
Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model's pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多 pero 不是所有下游任务中表现高。模型与下游任务数据之间的互动通常被假设为决定这种差异：一个与模型预训练数据更相似的任务被认为是更容易的 для该模型。我们通过大规模比较PILE和C4预训练集与下游标准准确率之间的相似性度量（嵌入-, 字符-和模型-基于的相似度）和语言模型表现的关系。在多语言任务上，相似度度量和表现之间存在相关性，但在其他benchmark上，我们意外地发现，相似度度量并不相关或者甚至相互隔离。这表明了预训练数据和下游任务之间的关系比较复杂。
</details></li>
</ul>
<hr>
<h2 id="Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output"><a href="#Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output" class="headerlink" title="Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output"></a>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09000">http://arxiv.org/abs/2311.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxiaw/factcheck-gpt">https://github.com/yuxiaw/factcheck-gpt</a></li>
<li>paper_authors: Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov</li>
<li>for: 验证大语言模型输出的事实准确性</li>
<li>methods: 使用多Stage注解方案和自动结果 incorporation</li>
<li>results: 实验显示 FactTool、FactScore 和 Perplexity.ai 只能 Identify false claims 的 F1 为 0.53<details>
<summary>Abstract</summary>
The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用大型自然语言模型（LLM）在各种实际应用场景中的使用增长，需要验证其输出的事实准确性的机制。在这种工作中，我们提出了一种涵盖所有阶段的综合答案，用于标注 LLM 生成的响应中的事实准确性，并设计了一个多阶段标注方案，以便对 LLM 输出中的可靠性和事实不一致进行详细标注。我们开发了一个标注工具，以便加速标注过程，并为评分人减轻工作负担。它允许自动结果的包含在任何阶段中，例如自动检索的证据。我们还建立了一个开放领域文档级别的事实准确性指标，分为三级精度：声明、句子和文档。初步实验表明，FacTool、FactScore和Perplexity.ai 在 False Claims 上的 F1 值为 0.53。标注工具、指标和代码可以在 GitHub 上找到：https://github.com/yuxiaw/Factcheck-GPT。
</details></li>
</ul>
<hr>
<h2 id="SentAlign-Accurate-and-Scalable-Sentence-Alignment"><a href="#SentAlign-Accurate-and-Scalable-Sentence-Alignment" class="headerlink" title="SentAlign: Accurate and Scalable Sentence Alignment"></a>SentAlign: Accurate and Scalable Sentence Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08982">http://arxiv.org/abs/2311.08982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steinst/sentalign">https://github.com/steinst/sentalign</a></li>
<li>paper_authors: Steinþór Steingrímsson, Hrafn Loftsson, Andy Way</li>
<li>for: 用于处理很大的平行文档对。</li>
<li>methods: 使用分治方法和LaBSE双语句表示来评估所有可能的对齐路径，并对包含千上万句的文档进行对齐。</li>
<li>results: 在德语-法语和英语-冰岛语两个评估集上比五个其他句对齐工具表现更高，并在下游机器翻译任务上获得了更好的结果。<details>
<summary>Abstract</summary>
We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task.
</details>
<details>
<summary>摘要</summary>
我们介绍SentAlign，一种精度高的句子对齐工具，可以处理非常大的平行文档对。根据用户定义的参数，对齐算法会评估所有可能的对齐路径，并使用分治策略对包含数以千句的文档进行对齐。对齐函数基于LaBSE双语句子表示。SentAlign在两个不同的评估集上（德语-法语和英语-冰岛语）和下游机器翻译任务上都表现出色，超过了五个其他句子对齐工具。
</details></li>
</ul>
<hr>
<h2 id="Speculative-Contrastive-Decoding"><a href="#Speculative-Contrastive-Decoding" class="headerlink" title="Speculative Contrastive Decoding"></a>Speculative Contrastive Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08981">http://arxiv.org/abs/2311.08981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou</li>
<li>for: 加速大型语言模型（LLM）的推断速度，以便广泛应用。</li>
<li>methods: 使用 amateur models 预测专家模型的生成，并通过自然对比来加速推断。</li>
<li>results: 比较评估在四个标准测试集上，SCD 可以实现类似的加速因子，同时进一步改善生成质量。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown extraordinary performance in various language tasks, but high computational requirements hinder their widespread deployment. Speculative decoding, which uses amateur models to predict the generation of expert models, has been proposed as a way to accelerate LLM inference. However, speculative decoding focuses on acceleration instead of making the best use of the token distribution from amateur models. We proposed Speculative Contrastive Decoding (SCD), an accelerated decoding method leveraging the natural contrast between expert and amateur models in speculative decoding. Comprehensive evaluations on four benchmarks show that SCD can achieve similar acceleration factors as speculative decoding while further improving the generation quality as the contrastive decoding. The analysis of token probabilities further demonstrates the compatibility between speculative and contrastive decoding. Overall, SCD provides an effective approach to enhance the decoding quality of LLMs while saving computational resources.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxíng yǔyán módelǐng)* "Speculative decoding" is translated as "假设解码" ( Jiǎshè Jiěmó)* "Contrastive decoding" is translated as "对比解码" ( Duìbǐ Jiěmó)* "Token probabilities" is translated as "符号概率" (fúhào gòngsuā)* "Compatibility" is translated as "兼容" (jiānróng)
</details></li>
</ul>
<hr>
<h2 id="Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer"><a href="#Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer" class="headerlink" title="Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer"></a>Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08966">http://arxiv.org/abs/2311.08966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Qiu, Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma</li>
<li>for: 提高流式自动语音识别（ASR）中罕见词或上下文实体的识别性能，尤其是在实际应用中。</li>
<li>methods:  combinatorial transducer 结合 rare word 的 phoneme 和文本信息，以便在 bias list 中分辨同音或同字的词语。  plus，使用 text-only 数据进行训练，以便大规模 deep biasing。</li>
<li>results: 在 LibriSpeech 数据集上，提出的方法实现了不同规模和偏好列表水平的罕见词错误率的状态天线表现。<details>
<summary>Abstract</summary>
Deep biasing for the Transducer can improve the recognition performance of rare words or contextual entities, which is essential in practical applications, especially for streaming Automatic Speech Recognition (ASR). However, deep biasing with large-scale rare words remains challenging, as the performance drops significantly when more distractors exist and there are words with similar grapheme sequences in the bias list. In this paper, we combine the phoneme and textual information of rare words in Transducers to distinguish words with similar pronunciation or spelling. Moreover, the introduction of training with text-only data containing more rare words benefits large-scale deep biasing. The experiments on the LibriSpeech corpus demonstrate that the proposed method achieves state-of-the-art performance on rare word error rate for different scales and levels of bias lists.
</details>
<details>
<summary>摘要</summary>
通过深度偏误来提高术语识别器的表现，特别是在流动自动语音识别（ASR）中，对于罕见词或上下文实体的识别是非常重要。然而，使用大规模罕见词时，深度偏误的性能会降低显著，因为更多的干扰词存在，同时有相似的文本序列在偏误列表中。在这篇论文中，我们将术语和文本信息结合在Transducer中，以便在类似的发音或拼写中分辨不同的词语。此外，通过增加包含更多罕见词的文本数据进行训练，可以改善大规模深度偏误的性能。LibriSpeech数据集的实验结果表明，我们提出的方法可以在不同的缩放和偏误列表水平上达到状态理论的表现。
</details></li>
</ul>
<hr>
<h2 id="Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models"></a>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08921">http://arxiv.org/abs/2311.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for:  investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy.</li>
<li>methods:  utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER, and explore various strategies to select reliable samples from the self-annotated dataset as demonstrations.</li>
<li>results:  achieve an obvious performance improvement, and there might still be space for improvement via more advanced strategy for reliable entity selection.Here’s the format you requested:for: &lt;what are the paper written for?&gt;methods: &lt;what methods the paper use?&gt;results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Exploring the application of powerful large language models (LLMs) on the fundamental named entity recognition (NER) task has drawn much attention recently. This work aims to investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy. We propose a self-improving framework, which utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER. First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Second, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, considering the similarity, diversity and reliability of demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations. Through comprehensive experimental analysis, our study yielded the following findings: (1) The self-improving framework further pushes the boundary of zero-shot NER with LLMs, and achieves an obvious performance improvement; (2) Iterative self-improving or naively increasing the size of unlabeled corpus does not guarantee improvements; (3) There might still be space for improvement via more advanced strategy for reliable entity selection.
</details>
<details>
<summary>摘要</summary>
我们提出了一个自我改进框架，利用无标注语料来刺激大语言模型（LLM）的自我学习能力。我们首先使用LLM预测无标注语料，并获得自我注释数据。然后，我们研究了多种策略来选择可靠的示例，考虑示例的相似性、多样性和可靠性。最后，我们通过在测试查询中进行推理，使用选择的自我注释示例进行学习。我们的研究发现以下结论：（1）自我改进框架可以进一步推动零shot NER的发展，并实现显著的性能提高；（2）循环自我改进或不断增加无标注语料的大小并不一定能够获得改进；（3）可能仍有更多的空间进行更高级别的可靠实体选择策略。
</details></li>
</ul>
<hr>
<h2 id="HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence"><a href="#HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence" class="headerlink" title="HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence"></a>HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08896">http://arxiv.org/abs/2311.08896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang</li>
<li>for: 本研究旨在提高大型语言模型在表格转文本任务中的表现，特别是避免使用公共API modify prompts，以避免潜在的成本和信息泄露。</li>
<li>methods: 我们采用了参数效率的 fine-tuning方法，使用open-source大型语言模型LLLaMA2，并在输入中注入了解释信息以提高表格转文本性能。我们的模型包括两个模块：1）表格理解器，可以快速地标识有关表格的信息，2）表格概要生成器，可以基于标识的信息生成文本。为了实现这一点，我们提出了一种搜索策略来构建解释标签 для训练表格理解器。</li>
<li>results: 在FetaQA和QTSumm数据集上，我们的方法实现了状态对标的Result，并且发现高亮输入表格可以明显提高模型的表现，同时提供有价值的解释性。<details>
<summary>Abstract</summary>
Large models have demonstrated significant progress across various domains, particularly in tasks related to text generation. In the domain of Table to Text, many Large Language Model (LLM)-based methods currently resort to modifying prompts to invoke public APIs, incurring potential costs and information leaks. With the advent of open-source large models, fine-tuning LLMs has become feasible. In this study, we conducted parameter-efficient fine-tuning on the LLaMA2 model. Distinguishing itself from previous fine-tuning-based table-to-text methods, our approach involves injecting reasoning information into the input by emphasizing table-specific row data. Our model consists of two modules: 1) a table reasoner that identifies relevant row evidence, and 2) a table summarizer that generates sentences based on the highlighted table. To facilitate this, we propose a search strategy to construct reasoning labels for training the table reasoner. On both the FetaQA and QTSumm datasets, our approach achieved state-of-the-art results. Additionally, we observed that highlighting input tables significantly enhances the model's performance and provides valuable interpretability.
</details>
<details>
<summary>摘要</summary>
大型模型在不同领域中已经展现出了显著的进步，尤其是在文本生成相关任务中。在表格到文本领域，许多大语言模型（LLM）基于方法现在通过修改提示来访问公共API，可能会导致潜在的成本和信息泄露。随着开源大型模型的出现，精细调整LLM变得可能。在这个研究中，我们实施了效率高的参数调整方法。与前期的表格到文本方法不同，我们的方法在输入中注入了理解信息。我们的模型包括两个模块：1）表格理解器，它可以识别相关的行证据；2）表格概要生成器，它可以基于高亮的表格生成句子。为了实现这一点，我们提出了一种搜索策略来构建理解标签 для训练表格理解器。在FetaQA和QTSumm数据集上，我们的方法达到了状态艺术的结果。此外，我们发现高亮输入表格可以显著提高模型的性能并提供有价值的解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM"><a href="#Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM" class="headerlink" title="Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"></a>Large Language Models are legal but they are not: Making the case for a powerful LegalLLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08890">http://arxiv.org/abs/2311.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanmay Jayakumar, Fauzan Farooqui, Luqman Farooqui</li>
<li>for: 本研究旨在衡量通用大语言模型在法律领域中的表现，以及与专门针对法律领域的模型相比。</li>
<li>methods: 本研究使用了三个通用大语言模型（ChatGPT-20b、LLaMA-2-70b和Falcon-180b），对LEDGAR子集进行零shot测试，以评估这些模型在合同提供分类任务中的表现。</li>
<li>results: 研究发现，通用大语言模型可以在大多数情况下正确地分类合同主题，但mic-F1&#x2F;mac-F1表现相比更小的法律领域模型 fine-tuned 的表现下降至19.2&#x2F;26.8%。这表明，更有力量的法律领域 LLM 的需求。<details>
<summary>Abstract</summary>
Realizing the recent advances in Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLMs) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）技术在法律领域的应用存在各种挑战，包括非常长的序列长度、专业legal vocabulary和大量数据不均衡。最近的大语言模型（LLMs）已经开始为法律领域提供新的应用机会，因为它们可以处理复杂、长序列。此外，法律领域特定的LLMs在多种任务上显示出了极其出色的成绩。在本研究中，我们想要衡量一下一般用途LLMs与法律领域模型之间的比较。我们对三个一般用途LLMs（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在LexGLUEBenchmark的LEDGAR子集上进行零shot性能比较。虽然这些LLMs没有直接在法律数据上接受训练，但我们发现它们仍可以正确地分类主题。然而，我们发现它们的mic-F1/mac-F1性能相比小型在法律领域进行 fine-tuning 的模型，下降至19.2/26.8％。这说明了法律领域需要更强大的LLMs。
</details></li>
</ul>
<hr>
<h2 id="CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building"><a href="#CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building" class="headerlink" title="CLIMB: Curriculum Learning for Infant-inspired Model Building"></a>CLIMB: Curriculum Learning for Infant-inspired Model Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08886">http://arxiv.org/abs/2311.08886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Diehl Martinez, Zebulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, Lisa Beinborn</li>
<li>for: 这个论文是为了研究一种基于小训练集的语言模型的训练方法。</li>
<li>methods: 论文使用了三种COGNITIVE-MOTIVATED curriculum learning的方法，包括 vocabulary curriculum、data curriculum和objective curriculum。</li>
<li>results: 论文的结果表明，这些curriculum learning方法可以提高语言模型在语言评估任务中的性能，但是不同的任务和设置下的提高程度不一样。<details>
<summary>Abstract</summary>
We describe our team's contribution to the STRICT-SMALL track of the BabyLM Challenge. The challenge requires training a language model from scratch using only a relatively small training dataset of ten million words. We experiment with three variants of cognitively-motivated curriculum learning and analyze their effect on the performance of the model on linguistic evaluation tasks. In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves. In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model. In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities. Our results did not yield consistent improvements over our own non-curriculum learning baseline across a range of linguistic benchmarks; however, we do find marginal gains on select tasks. Our analysis highlights key takeaways for specific combinations of tasks and settings which benefit from our proposed curricula. We moreover determine that careful selection of model architecture, and training hyper-parameters yield substantial improvements over the default baselines provided by the BabyLM challenge.
</details>
<details>
<summary>摘要</summary>
我们描述我们团队在STRICT-SMALL赛道上的贡献。这个赛道需要从头开始训练一个语言模型，只使用一个相对较小的训练集数据量为十万个单词。我们在语言评估任务中实践三种认知驱动的课程学习方法，并分析它们对模型的性能的影响。在词汇课程中，我们分析了在训练的早期阶段封闭词汇的方法，以模拟更加认知可能的学习曲线。在数据课程实验中，我们变化了训练实例的顺序，基于婴儿所驱动的预期和模型的学习行为。在目标课程中，我们探索了不同的推理任务和word类目的结合方法，以强化语言总体化能力。我们的结果没有在多种语言标准测试上表现出一致性的提高，但我们发现了一些选择任务和设置的特定组合可以得到有限的改进。我们的分析还提出了关键的总结，即特定任务和设置可以从我们提议的课程中得到改进。此外，我们发现了选择模型结构和训练超参数的重要性，可以在默认基eline提供的基线上实现显著改进。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Large-Language-Models-to-Learn-from-Rules"><a href="#Enabling-Large-Language-Models-to-Learn-from-Rules" class="headerlink" title="Enabling Large Language Models to Learn from Rules"></a>Enabling Large Language Models to Learn from Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08883">http://arxiv.org/abs/2311.08883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen</li>
<li>for: 研究LLMs可以学习从规则中获得知识，以提高其完成不同任务的能力。</li>
<li>methods: 提出了一种新的学习方法，即使用LLMs学习从规则中提取知识，并将其编码到LLMs中。</li>
<li>results: 实验显示，使用规则学习方法可以比例例学习更加高效，尤其是当训练示例数量受限时。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, the learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can grasp the new tasks or knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which encodes the rule-based knowledge into LLMs. We propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules and then explicitly encode the knowledge into LLMs' parameters by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation"><a href="#Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation" class="headerlink" title="Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation"></a>Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08877">http://arxiv.org/abs/2311.08877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaishnavi Shrivastava, Percy Liang, Ananya Kumar</li>
<li>for: 提高用户信任，大型自然语言模型（LLM）应该在错误示例上发出低自信信号，而不是诱导用户错误。</li>
<li>methods: 我们首先研究语言上的自信询问（asking an LLM for its confidence in its answer），这在12个问答 dataset上得到了70.5%的 AUC（GPT-4的平均值），比Random baseline高7%。然后我们探索使用代理自信模型（using a model where we do have probabilities to evaluate the original model’s confidence in a given question），奇怪的是，即使这些概率来自弱的模型，这种方法在12个dataset上高于语言自信的 AUC（9&#x2F;12）。我们最好的方法是将语言自信和代理模型概率相乘，这在所有12个dataset上达到了state-of-the-art的自信估计（84.6%的 GPT-4平均值）。</li>
<li>results: 我们的研究表明，使用语言上的自信询问和代理自信模型可以提高用户信任，并且可以达到state-of-the-art的自信估计水平（84.6%的 GPT-4平均值）。<details>
<summary>Abstract</summary>
To maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement. We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4).
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:为了维护用户信任，大型语言模型（LLM）应该在错误示例上透露低自信，而不是诱导用户。现有的标准方法估计自信是通过这些模型的软极概率，但截至2023年11月，现场的LLM如GPT-4和Claude-v1.3不提供这些概率。我们首先研究用语言来询问LLM的自信，这种方法在12个问答dataset上表现良好（80.5%的AUC平均值），但还有提高的空间。我们然后探索使用代理自信模型，使用一个有概率的模型来评估原始模型在某个问题上的自信度。奇怪的是，尽管这些概率来自一个不同和通常软弱的模型，但这种方法在12个dataset上高于语言自信的AUC。我们最佳方法是将语言自信和代理模型概率 Composite，在所有12个dataset上达到状态的前景（84.6%的AUC平均值在GPT-4上）。
</details></li>
</ul>
<hr>
<h2 id="OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining"><a href="#OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining" class="headerlink" title="OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining"></a>OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08849">http://arxiv.org/abs/2311.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Schütze</li>
<li>for: 这个研究目的是提出一个可以快速和有效地将预训语言模型（PLM）扩展到多种语言的方法。</li>
<li>methods: 这个方法利用外部的多语言WORD embedding和将其注入到新的嵌入中，并且使用矩阵分解来取代巨大的嵌入矩阵，从而大幅削减参数数量。</li>
<li>results: 这个方法可以快速和有效地将PLM扩展到多种语言，并且在许多下游任务上表现出色，包括跨语言转换和零件跨语言转换。<details>
<summary>Abstract</summary>
Pretraining multilingual language models from scratch requires considerable computational resources and substantial training data. Therefore, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the language model, thus weakening the efficiency. To address these issues, we propose a novel framework: \textbf{O}ne \textbf{F}or \textbf{A}ll (\textbf{\textsc{Ofa}), which wisely initializes the embeddings of unseen subwords from target languages and thus can adapt a PLM to multiple languages efficiently and effectively. \textsc{Ofa} takes advantage of external well-aligned multilingual word embeddings and injects the alignment knowledge into the new embeddings. In addition, \textsc{Ofa} applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which significantly reduces the number of parameters while not sacrificing the performance. Through extensive experiments, we show models initialized by \textsc{Ofa} are efficient and outperform several baselines. \textsc{Ofa} not only accelerates the convergence of continued pretraining, which is friendly to a limited computation budget, but also improves the zero-shot crosslingual transfer on a wide range of downstream tasks. We make our code and models publicly available.
</details>
<details>
<summary>摘要</summary>
预训语言模型从scratch需要较多的计算资源和大量的训练数据。因此，一种更高效的方法是从现有的预训语言模型（PLM）中适应新语言via词汇扩展和继续预训。然而，这种方法通常随机初始化新字词的embeddings，并添加大量的embedding参数到语言模型中，从而削弱效率。为解决这些问题，我们提出了一个新的框架：\textbf{一} для \textbf{所} (\textbf{\textsc{Ofa}), which Wisely initializes the embeddings of unseen subwords from target languages and can efficiently adapt a PLM to multiple languages. \textsc{Ofa} 利用外部的多语言协调词嵌入和注入协调知识到新的嵌入。此外，\textsc{Ofa} 使用矩阵分解，将繁琐的嵌入换取两个更低维度的矩阵，这会减少参数的数量，而不是牺牲性能。通过广泛的实验，我们表明由\textsc{Ofa}  initialize的模型是高效的，并超过了多个基eline。\textsc{Ofa} 不仅加速继续预训的整合，这是计算预算有限的情况下友好的，而且提高零shot cross语言转移的性能在各种下游任务上。我们将代码和模型公开。
</details></li>
</ul>
<hr>
<h2 id="Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder"><a href="#Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder" class="headerlink" title="Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder"></a>Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08844">http://arxiv.org/abs/2311.08844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Mohamed, Fakhraddin Alwajih, El Moatez Billah Nagoudi, Alcides Alcoba Inciarte, Muhammad Abdul-Mageed</li>
<li>for: 本研究旨在提高非英语语言图像描述的水平，尤其是阿拉伯语。</li>
<li>methods: 本研究使用了一种新的视觉编码器和一种名为“GEMINI”的文本解码器，以实现视觉和语言组件的融合。同时，我们还提出了一种自动从英语数据集中获取数据的新方法。</li>
<li>results: 根据我们的评估结果，“Violet”模型在所有评估数据集上都表现出了较好的表现，比如在我们手动标注的数据集上达到了CIDEr分数61.2，和Flickr8k上提高了13点。<details>
<summary>Abstract</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.
</details>
<details>
<summary>摘要</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.Here is the translation in Traditional Chinese:尽管图像描述有着广泛的应用，但它尚未在非英语语言中得到充分开展。阿拉伯语，例如，是世界上400多万人的native语言，却在这个领域中受到了严重的欠缺标注数据和强大的阿拉伯语生成模型的限制。我们解决这个问题，提出了一个名为“Violet”的新型视觉语言模型，基于视觉编码器和Gemini文本解码器，可以维持生成流畅性，并允许视觉和语言元素的融合。为了训练我们的模型，我们提出了一新的方法，即自已有英语数据集中自动获取数据。我们还 manually实现了一个新的评估数据集。 compared to our baselines, \textit{Violet}在我们的评估数据集上表现出较好的表现，例如，在我们 manually annotated dataset上的 CIDEr 分数为61.2，在 Flickr8k 上提高了13 分。
</details></li>
</ul>
<hr>
<h2 id="Disinformation-Capabilities-of-Large-Language-Models"><a href="#Disinformation-Capabilities-of-Large-Language-Models" class="headerlink" title="Disinformation Capabilities of Large Language Models"></a>Disinformation Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08838">http://arxiv.org/abs/2311.08838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/disinformation-capabilities">https://github.com/kinit-sk/disinformation-capabilities</a></li>
<li>paper_authors: Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro, Dominik Macko, Maria Bielikova</li>
<li>for: 这篇论文旨在研究现有大语言模型（LLM）是否能够生成假新闻文章，以及这些假新闻文章是如何影响民主社会的。</li>
<li>methods: 该论文使用了10个LLM，对20个假新闻narraitives进行了评估。评估的方面包括生成新闻文章的质量、假新闻narraitives的同意程度、生成的安全警告等。</li>
<li>results: 研究发现，LLMs可以生成有力的新闻文章，并且往往同意危险的假新闻narraitives。<details>
<summary>Abstract</summary>
Automated disinformation generation is often listed as one of the risks of large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for democratic societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how well they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.
</details>
<details>
<summary>摘要</summary>
自动化假信息生成常被列为大语言模型（LLM）的风险之一。这种假信息潮流可能对世界各地民主社会造成巨大的影响。本文介绍了当前一代LLM对英语新闻文章的假信息生成能力的全面研究。我们使用20个假信息narraves进行评估，评估了10个LLM的表现，包括生成新闻文章的能力、与假信息narraves的倾向度、生成安全警告等。我们还评估了检测模型对这些文章的检测能力。我们结论是，LLM可以生成有力的新闻文章，同时倾向于支持危险的假信息narraves。
</details></li>
</ul>
<hr>
<h2 id="StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving"><a href="#StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving" class="headerlink" title="StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"></a>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08803">http://arxiv.org/abs/2311.08803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam</li>
<li>for: 提高Chain-of-thought（CoT）提问方法的一致性和普适性，解决现有方法的实例特定性和解释步骤的不一致问题。</li>
<li>methods: 提出了一个框架StrategylLM，利用LLM的能力解决多个任务。框架包括策略生成器、执行器、优化器和评估器等四个LLM-基于的代理人，共同生成、评估和自动选择任务中的有前途的策略。</li>
<li>results: StrategylLM在13个数据集和4个复杂任务上超过了比较基准CoT-SC，无需人工注解解决问题，包括数学逻辑（39.2% → 43.3%）、常识逻辑（70.3% → 72.5%）、算法逻辑（51.7% → 62.0%）和符号逻辑（30.0% → 79.2%）等。<details>
<summary>Abstract</summary>
Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to tackle various tasks. The framework improves generalizability by formulating general problem-solving strategies and enhances consistency by producing consistent solutions using these strategies. StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task automatically. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning (70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%), and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</details>
<details>
<summary>摘要</summary>
现有的链式思维（CoT）提问方法受到普适性和一致性的限制，因为它们常常基于特定实例的解决方案，可能无法适用于其他情况，并且缺乏任务水平的一致性在思维步骤中。为了解决这些局限性，我们提出了一个全面的框架，即策略LLM，利用LLM的能力来解决多个任务。该框架提高普适性，通过定义通用的问题解决策略，并增强一致性，通过使用这些策略生成一致的解决方案。策略LLM使用四个LLM基于的代理：策略生成器、执行器、优化器和评估器，这些代理共同工作，自动生成、评估和选择有潜力的策略，以解决给定任务。实验结果表明，策略LLM比基线CoT-SC（需要人工标注解决方案）在13个数据集上的4个复杂任务中表现出色，无需人工参与，包括数学逻辑（39.2% → 43.3%）、通情理解（70.3% → 72.5%）、算法逻辑（51.7% → 62.0%）和符号逻辑（30.0% → 79.2%）。
</details></li>
</ul>
<hr>
<h2 id="German-FinBERT-A-German-Pre-trained-Language-Model"><a href="#German-FinBERT-A-German-Pre-trained-Language-Model" class="headerlink" title="German FinBERT: A German Pre-trained Language Model"></a>German FinBERT: A German Pre-trained Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08793">http://arxiv.org/abs/2311.08793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Scherrmann</li>
<li>for: 这个研究旨在开发一个适用于金融文本数据的德国语言模型，以便为金融领域的文本分析提供有价值的工具。</li>
<li>methods: 该模型通过了全面的预训练过程，利用了包含金融报告、特别公告和新闻的大量金融相关文本资料进行训练。</li>
<li>results: 研究发现，使用德国金融BERT对金融特定数据进行预测，性能明显提高，表明该模型能够很好地捕捉金融领域特有的特征。<details>
<summary>Abstract</summary>
This study presents German FinBERT, a novel pre-trained German language model tailored for financial textual data. The model is trained through a comprehensive pre-training process, leveraging a substantial corpus comprising financial reports, ad-hoc announcements and news related to German companies. The corpus size is comparable to the data sets commonly used for training standard BERT models. I evaluate the performance of German FinBERT on downstream tasks, specifically sentiment prediction, topic recognition and question answering against generic German language models. My results demonstrate improved performance on finance-specific data, indicating the efficacy of German FinBERT in capturing domain-specific nuances. The presented findings suggest that German FinBERT holds promise as a valuable tool for financial text analysis, potentially benefiting various applications in the financial domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity"><a href="#Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity" class="headerlink" title="Accelerating Toeplitz Neural Network with Constant-time Inference Complexity"></a>Accelerating Toeplitz Neural Network with Constant-time Inference Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08756">http://arxiv.org/abs/2311.08756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion">https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion</a></li>
<li>paper_authors: Zhen Qin, Yiran Zhong</li>
<li>for: 本文的目的是将 toeplitz neural network (TNN) 转换成 state space model (SSM)，以便在推理过程中实现常数复杂度。</li>
<li>methods: 本文使用了一个优化问题的形式来实现 TNN 的转换，并提供了一个关闭式解决方案。通过将目标方程转换成 Vandermonde 线性系统问题，可以使用 Discrete Fourier Transform (DFT) 高效地解决。</li>
<li>results: 在语言模型任务上进行了广泛的实验，并证明了我们的方法可以维持数值稳定性，并且比其他梯度下降解决方案更为稳定。<details>
<summary>Abstract</summary>
Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to achieve the same constant inference complexities as SSMs. To accomplish this, we formulate the conversion process as an optimization problem and provide a closed-form solution. We demonstrate how to transform the target equation into a Vandermonde linear system problem, which can be efficiently solved using the Discrete Fourier Transform (DFT). Notably, our method requires no training and maintains numerical stability. It can be also applied to any LongConv-based model. To assess its effectiveness, we conduct extensive experiments on language modeling tasks across various settings. Additionally, we compare our method to other gradient-descent solutions, highlighting the superior numerical stability of our approach. The source code is available at https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</details>
<details>
<summary>摘要</summary>
托平脑网络（TNNs）在不同的序列模型任务中表现出色，而且具有对数linear空间时间复杂度的优势。然而，状态空间模型（SSMs）在语言模型任务中表现较差，但具有常数推理复杂度的优点。在这篇论文中，我们想要将TNNs转换成SSMs，以使TNNs在推理过程中具有常数推理复杂度。为实现这一目标，我们将转换过程形式化为优化问题，并提供了关闭式解决方案。我们将目标方程转换成Vandermonde线性系统问题，可以使用快速傅立做（DFT）高效地解决。值得注意的是，我们的方法不需要训练，并且保持数值稳定。此外，我们的方法可以应用于任何LongConv基于模型。为评估其效果，我们在不同的语言模型任务上进行了广泛的实验。此外，我们还与其他梯度下降解决方案进行比较，并高亮了我们的方法的数值稳定性的优势。源代码可以在https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion中获取。
</details></li>
</ul>
<hr>
<h2 id="Thread-of-Thought-Unraveling-Chaotic-Contexts"><a href="#Thread-of-Thought-Unraveling-Chaotic-Contexts" class="headerlink" title="Thread of Thought Unraveling Chaotic Contexts"></a>Thread of Thought Unraveling Chaotic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08734">http://arxiv.org/abs/2311.08734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen</li>
<li>for: 提高大型自然语言处理模型（LLM）在文本理解和生成任务中的表现，但它们在干扰性上下文中遇到困难，导致略去一些 irrelevant 信息。</li>
<li>methods: 我们提出了一种基于人类认知过程的 “Thread of Thought”（ThoT）策略，可以系统地分析和选择相关信息。ThoT 可以与不同的 LLM 和提示技术集成， acting as a versatile “plug-and-play” 模块。</li>
<li>results: 我们在 PopQA 和 EntityQ 数据集上，以及我们自己收集的 Multi-Turn Conversation Response 数据集（MTCR）上进行了实验，发现 ThoT 可以提高理解性能，比较其他提示技术。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have ushered in a transformative era in the field of natural language processing, excelling in tasks related to text comprehension and generation. Nevertheless, they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context. In response to these challenges, we introduce the "Thread of Thought" (ThoT) strategy, which draws inspiration from human cognitive processes. ThoT systematically segments and analyzes extended contexts while adeptly selecting pertinent information. This strategy serves as a versatile "plug-and-play" module, seamlessly integrating with various LLMs and prompting techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to illustrate that ThoT significantly improves reasoning performance compared to other prompting techniques.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）已经引入了一个转变性的时代，在文本理解和生成任务中表现出色。然而，它们在受扰Context（例如，误差而不是长期 irrelevant context）时会遇到困难，导致在受扰Context中略去一些细节。为了解决这些挑战，我们介绍了“思维线索”（ThoT）策略，这种策略 Draws inspiration from human cognitive processes。ThoT系统可以系统地分析和 segments extended contexts，并选择相关信息。这种策略可以轻松地与不同的 LLMs 和提示技术集成，functioning as a versatile "plug-and-play" module。在实验中，我们使用了 PopQA 和 EntityQ 数据集，以及我们自己收集的 Multi-Turn Conversation Response 数据集（MTCR），以示 that ThoT 可以显著提高理解性能相比其他提示技术。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models"><a href="#Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models" class="headerlink" title="Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models"></a>Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08732">http://arxiv.org/abs/2311.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli Zhu</li>
<li>for: 提供可靠的紧急决策支持</li>
<li>methods: 使用知识图和大语言模型（LLM）进行证据基础的决策making</li>
<li>results: 在实际评估中，比基eline模型高于9分的 comprehensibility、准确性、简洁性和指导性，表明了该系统在不同的情况下提供了有力的决策支持。<details>
<summary>Abstract</summary>
Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.
</details>
<details>
<summary>摘要</summary>
紧急管理需要全面的知识，而且可能超出个人认知范围。因此，在这种情况下，基于人工智能（AI）的决策是非常重要的。现在出现的大型自然语言模型（LLM）提供了一个新的方向来增强目标机器智能。然而，直接使用LLM将导致不可靠的输出，因为它们的内置问题是梦境和理解能力不足。在这项工作中，我们开发了一个系统called Enhancing Emergency decision-making with Knowledge Graph and LLM（E-KELL），它提供了基于证据的决策在多种紧急情况下。我们构建了一个结构化的紧急知识图，并使用提示链引导LLM进行图上的理解。在实际评估中，E-KELL得到了9.06、9.09、9.03和9.09的评分，分别表示可读性、准确性、简洁性和指导性，从一群紧急指挥官和消防员的视角来看，与基eline模型相比显示了 significativetransformation。这项工作介绍了一种可靠的紧急决策支持方法。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission"><a href="#Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission" class="headerlink" title="Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission"></a>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08726">http://arxiv.org/abs/2311.08726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen</li>
<li>for: 预测每个 tokens 的标签，如Named Entity Recognition (NER)。NER 任务的目标是从文本中提取实体，并预测它们的标签，这对信息抽取至关重要。</li>
<li>methods: 我们提出了一种Sequential Labeling Posterior Network (SLPN)，用于估计NER预测结果中的uncertainty scores。SLPN 考虑了实体之间的连接（即基于其他实体学习单个实体 embedding）以及错误扫描 случа子。</li>
<li>results: 我们的SLPN在两个 datasets 上达到了显著的提高（如MIT-Restaurant 数据集上的AUPR提高5.54点），表明了我们的方法在UE-NER任务中的效果。<details>
<summary>Abstract</summary>
Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details>
<details>
<summary>摘要</summary>
续序标注是一个任务，predict标签 для每个序列中的每个元素，如名实体识别（NER）。NER任务的目标是从文本中提取实体并预测其标签，这是信息提取中非常重要的一部分。 although previous works have made significant progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still under-explored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details></li>
</ul>
<hr>
<h2 id="Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph"><a href="#Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph" class="headerlink" title="Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph"></a>Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08724">http://arxiv.org/abs/2311.08724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li</li>
<li>for: 本研究旨在链接发电 dispatch文本中的实体到一个电力分配知识图库中。</li>
<li>methods: 本方法利用电力分配网络知识图和发电文本中实体的语义、音标和语法特征进行深入理解，并使用增强的LSF-SCNN模型进行实体匹配。</li>
<li>results: 对实际发电场景进行交叉验证，LSF-SCNN模型比控制模型更高的准确性在链接多种实体类型。<details>
<summary>Abstract</summary>
The proposed method for linking entities in power distribution dispatch texts to a power distribution network knowledge graph is based on a deep understanding of these networks. This method leverages the unique features of entities in both the power distribution network's knowledge graph and the dispatch texts, focusing on their semantic, phonetic, and syntactic characteristics. An enhanced model, the Lexical Semantic Feature-based Skip Convolutional Neural Network (LSF-SCNN), is utilized for effectively matching dispatch text entities with those in the knowledge graph. The efficacy of this model, compared to a control model, is evaluated through cross-validation methods in real-world power distribution dispatch scenarios. The results indicate that the LSF-SCNN model excels in accurately linking a variety of entity types, demonstrating high overall accuracy in entity linking when the process is conducted in English.
</details>
<details>
<summary>摘要</summary>
提议的方法是基于电力分配网络知识图的深刻理解，将文本中的实体链接到知识图中。这种方法利用了文本和知识图中实体的语义、声学和 sintactic 特征，并使用增强的模型——lexical semantic feature-based skip convolutional neural network (LSF-SCNN)——进行实体匹配。在实际的电力分配 dispatch 场景中，我们通过十分之一验证法评估模型的效果，结果显示，LSF-SCNN 模型在英语下可以准确地链接多种实体类型，并且在整体实体链接中达到高精度。
</details></li>
</ul>
<hr>
<h2 id="Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text"><a href="#Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text" class="headerlink" title="Token Prediction as Implicit Classification to Identify LLM-Generated Text"></a>Token Prediction as Implicit Classification to Identify LLM-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08723">http://arxiv.org/abs/2311.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markchenyutian/t5-sentinel-public">https://github.com/markchenyutian/t5-sentinel-public</a></li>
<li>paper_authors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在identifying可能的大型自然语言模型（LLM）在文本生成中的具体应用。</li>
<li>methods: 我们采用了一种нов的方法，即将分类任务转化为下一个单词预测任务，直接使用基础LM进行 fine-tune。我们使用了Text-to-Text Transfer Transformer（T5）模型作为我们的实验室。</li>
<li>results: 我们的方法在文本分类任务中表现出色，具有简单高效的特点。此外，我们对模型提取的特征进行了解释性研究，发现它能够在不同的LLM中分辨独特的写作风格，即使没有显式的分类器。我们还收集了一个名为OpenLLMText的数据集，包含约34万个文本样本，来自人类和LLM，包括GPT3.5、PaLM、LLaMA和GPT2。<details>
<summary>Abstract</summary>
This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的方法来识别文本生成中可能存在的大语言模型（LLM）。而不是将添加额外的分类层到基础语言模型（LM）上，我们将分类任务重新归类为下一个Token预测任务，直接使得基础LM进行 fine-tune 来执行它。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们实验的脊梁。我们与使用隐藏状态进行分类的更直接方法进行比较。评估结果显示我们的方法在文本分类任务中表现出色，强调其简单性和效率。此外，对我们模型提取的特征进行解释研究表明它可以在不同的LLM中分 differentiate 不同的写作风格，即使没有显式的分类器。我们还收集了一个名为 OpenLLMText 的数据集，包含约340万个文本样本，来自人类和LLM，包括 GPT3.5、PaLM、LLaMA 和 GPT2。
</details></li>
</ul>
<hr>
<h2 id="Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory"><a href="#Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory" class="headerlink" title="Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"></a>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08719">http://arxiv.org/abs/2311.08719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang</li>
<li>for: 提高大语言模型（LLM）在长期人机交互中的表现，即使是重复的回忆和理解历史记忆中的高质量回答。</li>
<li>methods: 提出了一种新的内存机制called TiM（Think-in-Memory），可以让LLM agents在对话流程中维护一个演进的内存，以便在回答问题时重新回忆和更新历史记忆。</li>
<li>results: 对实际和验证对话进行质量和量тив实验，结果表明在 equipping 现有LLMs with TiM 可以显著提高它们在长期交互中的回答表现。<details>
<summary>Abstract</summary>
Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details>
<details>
<summary>摘要</summary>
具有记忆增强的大语言模型（LLM）在长期人机交互中表现出了惊人的表现，这主要基于迭代回忆和思考历史以生成高质量的回答。然而，这些重复的回忆和思考步骤容易产生偏见，即不同问题下的相同历史回忆时的不一致的思考结果。人类可以在记忆中保持思想，而不需要重复思考。 Drawing inspiration from this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Furthermore, we formulate the basic principles to organize the thoughts in memory based on well-established operations, such as insert, forget, and merge operations, allowing for dynamic updates and evolution of the thoughts. Additionally, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details></li>
</ul>
<hr>
<h2 id="Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling"><a href="#Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling" class="headerlink" title="Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"></a>Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08718">http://arxiv.org/abs/2311.08718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang</li>
<li>for: 本研究的目的是提高大型自然语言处理器（LLM）的可靠性、可信度和解释性，通过不同的输入简化集合来实现不确定性归一化。</li>
<li>methods: 我们提出了一种基于输入简化集合的不确定性归一化框架，而不需要训练多个模型变体。我们将输入简化集合传递给固定的 LLM，并将其相应的预测 ensemble。我们发现该框架与权值神经网络（BNN）具有同样的归一化结构。</li>
<li>results: 我们的实验表明，提出的框架可以准确地 Quantify LLM 的不确定性，并且可以在多个任务上提供可靠的结果。代码将在 <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty">https://github.com/UCSB-NLP-Chang/llm_uncertainty</a> 上公开发布。<details>
<summary>Abstract</summary>
Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details>
<details>
<summary>摘要</summary>
“uncertainty decomposition”指的是将模型的总不确定性 decomposes 到数据（ aleatoric）不确定性和模型（epistemic）不确定性之间。对于大型语言模型（LLMs）来说，完成 uncertainty decomposition 是提高模型的可靠性、可信度和解释性的重要步骤，但是这个研究任务很具有挑战性，尚未得到解决。现有的标准方法之一是 bayesian neural network（BNN），但是BNN 无法应用于 LLMs，因为BNN 需要训练和组合多个模型变体，这对 LLMs 来说是不可能或者非常昂贵。在这篇论文中，我们介绍了一种为 LLMs 提供 uncertainty decomposition 框架，called 输入明确集（IC），该框架可以 circumvent 需要训练新模型的需求。而不是 ensemble 多个模型的参数，我们的方法会生成一个输入的明确集，将其 feed 到固定的 LLMs 中，并 ensemble 相应的预测。我们显示了我们的框架与 BNN 具有相同的均衡分解结构。我们的实验证明了我们的框架可以在多种任务上提供准确和可靠的不确定性量化。代码将在 https://github.com/UCSB-NLP-Chang/llm_uncertainty 上公开。
</details></li>
</ul>
<hr>
<h2 id="PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning"><a href="#PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning" class="headerlink" title="PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning"></a>PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08711">http://arxiv.org/abs/2311.08711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ytyz1307zzh/plug">https://github.com/ytyz1307zzh/plug</a></li>
<li>paper_authors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri</li>
<li>for: 提高大语言模型（LLMs）理解和回应多样化人类指令的能力</li>
<li>methods: 利用高资源语言（主要是英语）为低资源语言提高指令调整的方法，首先在领先语言中处理指令，然后生成回应</li>
<li>results: 对比直接在目标语言中回应，PLUG方法可以提高LLMs的指令遵从能力，增加29%的提升程度<details>
<summary>Abstract</summary>
Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency.
</details>
<details>
<summary>摘要</summary>
具有杰出表现的指令调整技术（LLM）可以帮助机器人理解和回应人类的多样化指令。虽然在高资源语言上得到了成功，但在低资源语言上的应用却遇到了挑战，这是因为LLM的基础能力在不同语言之间存在差异，这些差异来自于预训练数据的语言分布不均。为解决这个问题，我们提出了核心语言导向生成（PLUG）方法，它利用高资源语言（主要是英语）作为核心语言，以提高低资源语言中LLM的指令调整能力。它首先在核心语言中处理指令，然后在目标语言中生成响应。为评估我们的方法，我们创建了X-AlpacaEval benchmark，该benchmark包含4种语言（中文、韩语、意大利语和西班牙语）的指令，每个指令由专业翻译人员进行标注。我们的方法在LLM的指令遵循能力方面提高了29%的平均提升，比直接在目标语言中回答alone。此外，我们还进行了对其他核心语言的实验，以验证我们的方法的多样性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations"><a href="#Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations" class="headerlink" title="Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations"></a>Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08705">http://arxiv.org/abs/2311.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankita Gupta, Chulaka Gunasekara, Hui Wan, Jatin Ganhotra, Sachindra Joshi, Marina Danilevsky</li>
<li>for: This paper aims to investigate the robustness of state-of-the-art dialogue summarization models when faced with natural variations in conversations, such as repetitions and language errors.</li>
<li>methods: The authors use two types of perturbations to simulate real-life variations in dialogues: utterance-level perturbations and dialogue-level perturbations. They evaluate the impact of these perturbations on three dimensions of robustness: consistency, saliency, and faithfulness.</li>
<li>results: The authors find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible to dialogue-level perturbations. They also validate their findings via human evaluation and show that training with a fraction of perturbed data is insufficient to address robustness challenges with current models. The study highlights the need for better solutions to improve the robustness of dialogue summarization models.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是研究当前最佳的对话概要模型在面对自然的对话变化时的稳定性。</li>
<li>methods: 作者使用两种类型的干扰来模拟实际对话中的自然变化：utterance-level干扰和dialogue-level干扰。他们对三个稳定性维度进行评估：一致性、重要性和忠实性。</li>
<li>results: 作者发现，两种模型都受到输入变化的影响，但是被 instrucion-tuned 模型更加敏感，特别是对话级干扰。他们还通过人工评估 validate 他们的发现，并发现将一部分干扰数据用于训练并不能解决当前模型的稳定性挑战。这种研究高亮了对话概要模型的稳定性挑战，并提供了未来研究的指导。<details>
<summary>Abstract</summary>
Dialogue summarization task involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations) and existing dialogue summarization models suffer from performance drop on such conversations. In this study, we systematically investigate the impact of such variations on state-of-the-art dialogue summarization models using publicly available datasets. To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We conduct our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which capture different aspects of the summarization model's performance. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate if the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data and observe that this approach is insufficient to address robustness challenges with current models and thus warrants a more thorough investigation to identify better solutions. Overall, our work highlights robustness challenges in dialogue summarization and provides insights for future research.
</details>
<details>
<summary>摘要</summary>
对话概要任务 involve 简化长 conversations 中的重要信息。实际对话中经常出现自然的变化（例如重复、延迟），现有的对话概要模型在面对这些对话时会表现下降性能。在这个研究中，我们系统地研究了这些变化对现场对话概要模型的影响。为了模拟实际变化，我们引入了两种类型的干扰：语句级干扰，修改 individal 语句中的错误和语言变化，以及对话级干扰，添加非有用的交流（例如重复、祝你好）。我们在三个维度上进行分析：一致性、重要性和准确性，这些维度捕捉了不同的对话概要模型性能方面。我们发现，都 fine-tuned 和 instruction-tuned 模型都受到输入变化的影响，后者尤其是对对话级干扰的影响较大。我们还通过人工评估 validate 我们的发现。最后，我们研究了是否可以通过训练 fine-tuned 模型干扰数据的一部分来提高其Robustness，并发现这种方法不够，需要更进一步的调查来解决当前模型的Robustness挑战。总的来说，我们的工作强调对话概要模型的Robustness 挑战，并提供了未来研究的指导。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA"><a href="#Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA" class="headerlink" title="Attribute Diversity Determines the Systematicity Gap in VQA"></a>Attribute Diversity Determines the Systematicity Gap in VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08695">http://arxiv.org/abs/2311.08695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Berlot-Attwell, A. Michael Carrell, Kumar Krishna Agrawal, Yash Sharma, Naomi Saphra</li>
<li>for: 这 paper 是研究 neural network 能够怎样总结 familiar concept 的新组合的性能。</li>
<li>methods: 这 paper 使用了一个新的 диагностические数据集 CLEVR-HOPE，以测试系统aticity gap 在视觉问答中的表现。</li>
<li>results: 结果表明，尽管增加训练数据量不会降低系统aticity gap，但是增加不同 attribute 类型的组合在未看到的组合中的训练数据多样性可以降低系统aticity gap。<details>
<summary>Abstract</summary>
The degree to which neural networks can generalize to new combinations of familiar concepts, and the conditions under which they are able to do so, has long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased quantity of training data does not reduce the systematicity gap, increased training data diversity of the attributes in the unseen combination does. In all, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.
</details>
<details>
<summary>摘要</summary>
“神经网络对新组合的 familar 概念进行总结的能力，以及这种能力在哪些条件下表现出来，是一个长期的开放问题。在这项工作中，我们研究视觉问答中的系统性差异：由于训练数据中的 attribute 组合的新视觉表现。为了测试，我们引入了一个新的诊断数据集，CLEVR-HOPE。我们发现，尽管增加训练数据量不会减少系统性差异，但是增加训练数据中 attribute 类型的多样性则可以。总之，我们的实验表明，更多的 attribute 类型组合在训练时被看到，就会更系统地表现出来。”Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models"><a href="#Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models" class="headerlink" title="Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"></a>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou</li>
<li>for: 这个研究旨在探讨大型自然语言模型（LLM）的补充潜力，即Off-the-shelf LLMs可以在各个领域和任务上展示多元化的专业知识，以 Ensemble 方法实现更好的性能。</li>
<li>methods: 本研究提出了一种名为 Zooter 的 reward-guided routing 方法，通过将训练查询数据中的奖励转换为训练路由函数，以精确地将每个查询分配给具有相应专业知识的 LLM。此外，研究还提出了一个标签基于的标签增强方法来减少使用奖励时的不确定性。</li>
<li>results: 研究发现 Zooter 在具有26个子集的全面评chmark上显示了 Computation Efficiency，它对于排名模型排名方法而言只增加了一小部分的计算负载。此外，Zooter 在44%的任务上超越了最佳单一模型，并在大多数任务上与多个排名模型排名方法相当。<details>
<summary>Abstract</summary>
The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.
</details>
<details>
<summary>摘要</summary>
LLM的补偿潜力假设商业可用的LLM具有多元领域和任务的专业知识，以实现 ensemble 的更好性能。现有的 LLM 集成方法主要是根据输出奖励模型排名，导致计算负担增加。为解决这个问题，我们再次探讨 LLM 的补偿潜力，并进一步阐述它通过挖掘缺乏表达的 latent 专业知识来增强 ensemble 的性能。我们提出 Zooter，一种奖励导航方法，通过在训练查询上分配奖励来培养一个路由函数，可以准确地将每个查询分配给具有相应专业知识的 LLM。我们还将标签基本的标签增强策略与不确定性相关的噪声维持一致。 Zooter 在推理过程中具有计算效率，因为它只增加了一个路由函数的计算负担，而不是 reward 模型排名方法。我们对包含 26 个子集的全面的 benchmark 集进行评估，发现 Zooter 在 average 上超过最佳单个模型，并在 44% 的任务上排名第一。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Calibration-for-Multilingual-Question-Answering-Models"><a href="#Understanding-Calibration-for-Multilingual-Question-Answering-Models" class="headerlink" title="Understanding Calibration for Multilingual Question Answering Models"></a>Understanding Calibration for Multilingual Question Answering Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08669">http://arxiv.org/abs/2311.08669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee</li>
<li>for: 这个论文主要研究了多语言预训练语言模型（LLMs）在问答任务上的准确性。</li>
<li>methods: 作者使用了多种问答模型设计和多种语言进行了广泛的实验，包括抽取式和生成式问答模型，以及高资源语言和低资源语言。他们还研究了不同维度的准确性，包括在内部分布、外部分布和语言转换中的准确性。</li>
<li>results: 研究发现，使用自动翻译数据增强技术可以大幅提高模型的准确性。作者还进行了一些减少实验，研究模型大小对准确性的影响，以及多语言模型与单语言模型在不同任务和语言上的比较。<details>
<summary>Abstract</summary>
Multilingual pre-trained language models are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well they are calibrated. In this paper, we study the calibration properties of several pre-trained multilingual large language models (LLMs) on a variety of question-answering tasks. We perform extensive experiments, spanning both extractive and generative QA model designs and diverse languages, spanning both high-resource and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. We demonstrate automatically translated data augmentation as a highly effective technique to improve model calibration. We also conduct a number of ablation experiments to study the effect of model size on calibration and how multilingual models compare with their monolingual counterparts for diverse tasks and languages.
</details>
<details>
<summary>摘要</summary>
多语言预训练语言模型在问答任务上表现极其有效，在多种多语言benchmark上达到了高准确率。然而，对于这些模型的准确性衡量的知识很少。在这篇论文中，我们研究了多种预训练多语言大型语言模型（LLMs）在问答任务上的准确性性能。我们进行了广泛的实验，涵盖了抽取式和生成式问答模型设计，以及多种语言和资源水平。我们研究了不同的准确性维度，包括在适配、外围和交叉语言传递设定下的准确性。我们还 investigate了改进准确性的策略，包括后处方法和规范细化。我们示出了自动翻译数据增强为高效的准确性改进技术。此外，我们还进行了一些减少实验，研究模型大小对准确性的影响，以及多语言模型与单语言模型在多种任务和语言上的比较。
</details></li>
</ul>
<hr>
<h2 id="It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games"><a href="#It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games" class="headerlink" title="It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games"></a>It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08666">http://arxiv.org/abs/2311.08666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kj2013/claff-diplomacy">https://github.com/kj2013/claff-diplomacy</a></li>
<li>paper_authors: Kokil Jaidka, Hansin Ahuja, Lynnette Ng</li>
<li>for: 研究在线上战略游戏《 дипломатія》中玩家之间的互动，以了解玩家如何在游戏中谈判路径。</li>
<li>methods: 使用了聊天讯息语言模型来预测玩家对话的谈判策略，并评估其预测游戏长期和短期结果的重要性。</li>
<li>results: 发现谈判策略可以很好地预测游戏长期结果，但是短期结果则需要更多的考虑因素。此外，谈判史可以用于做出更准确的预测。<details>
<summary>Abstract</summary>
Online games are dynamic environments where players interact with each other, which offers a rich setting for understanding how players negotiate their way through the game to an ultimate victory. This work studies online player interactions during the turn-based strategy game, Diplomacy. We annotated a dataset of over 10,000 chat messages for different negotiation strategies and empirically examined their importance in predicting long- and short-term game outcomes. Although negotiation strategies can be predicted reasonably accurately through the linguistic modeling of the chat messages, more is needed for predicting short-term outcomes such as trustworthiness. On the other hand, they are essential in graph-aware reinforcement learning approaches to predict long-term outcomes, such as a player's success, based on their prior negotiation history. We close with a discussion of the implications and impact of our work. The dataset is available at https://github.com/kj2013/claff-diplomacy.
</details>
<details>
<summary>摘要</summary>
在在线游戏中，玩家之间的互动创造了一个富有的环境，用于了解玩家如何在游戏中突破困难，达到终极胜利。这项研究专注于在转战略游戏《 дипломаacia》中的在线玩家互动。我们对聊天消息集上的超过10,000个不同谈判策略进行了注释，并employs empirical方法来评估它们对游戏的长期和短期结果的重要性。虽然可以通过语言模型来预测谈判策略的准确程度，但是更需要更多的信息来预测短期的结果，如信任worthiness。然而，它们是在图像感知的强化学习方法中预测长期结果的关键因素，如玩家的成功。我们在结束时进行了关于这项工作的影响和意义的讨论。数据集可以在https://github.com/kj2013/claff-diplomacy中下载。
</details></li>
</ul>
<hr>
<h2 id="Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models"><a href="#Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models" class="headerlink" title="Multistage Collaborative Knowledge Distillation from Large Language Models"></a>Multistage Collaborative Knowledge Distillation from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08640">http://arxiv.org/abs/2311.08640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum</li>
<li>for: 这个论文的目的是解决 semi-supervised 序列预测任务中的 Label scarce 问题，其中 labeled 数据稀缺，同时 few-shot 提示大型自然语言模型 (LLM) 的性能不佳。</li>
<li>methods: 该论文提出了一种新的泛化方法，即 multistage collaborative knowledge distillation from an LLM (MCKD)，用于解决这类任务。MCKD 首先使用 few-shot 在上下文学习中提示 LLM 生成假标签，然后在每个阶段的泛化中使用各自的学生模型在不同的 partition 上进行训练。</li>
<li>results: 论文表明，在 CRAFT 生物医学分析任务上，3-stage MCKD 使用 50 个标签示例可以与supervised finetuning 使用 500 个标签示例匹配，并且超过提示 LLM 和 vanilla KD 的表现，提高了生物医学分析的解析 F1 值 by 7.5% 和 3.7%。<details>
<summary>Abstract</summary>
We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details>
<details>
<summary>摘要</summary>
MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details></li>
</ul>
<hr>
<h2 id="Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference"><a href="#Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference" class="headerlink" title="Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference"></a>Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08637">http://arxiv.org/abs/2311.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasha Abzianidze</li>
<li>for: 提出了一种利用正式证明来提出可解释的自然语言推理（NLI）任务的方法。</li>
<li>methods: 使用可靠高性能的逻辑基础NLI系统生成的正式证明来生成可解释的NLI任务。</li>
<li>results: 通过使用生成的正式证明中的深入信息，定义了可解释的NLI任务，并按照难度定义Tasks的Difficulty。<details>
<summary>Abstract</summary>
In this position paper, we propose a way of exploiting formal proofs to put forward several explainable natural language inference (NLI) tasks. The formal proofs will be produced by a reliable and high-performing logic-based NLI system. Taking advantage of the in-depth information available in the generated formal proofs, we show how it can be used to define NLI tasks with structured explanations. The proposed tasks can be ordered according to difficulty defined in terms of the granularity of explanations. We argue that the tasks will suffer with substantially fewer shortcomings than the existing explainable NLI tasks (or datasets).
</details>
<details>
<summary>摘要</summary>
在这份位置论文中，我们提出了利用正式证明来提出多个可解释自然语言推理（NLI）任务的方法。正式证明将由可靠和高性能的逻辑基础NLI系统生成。利用生成的正式证明中的深入信息，我们显示了如何使用结构化解释来定义NLI任务。我们提议的任务可以按难度排序，定义为证明的粒度。我们认为，提出的任务会比现有的可解释NLI任务（或数据集）受到更少的缺陷。
</details></li>
</ul>
<hr>
<h2 id="DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models"><a href="#DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models" class="headerlink" title="DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models"></a>DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08623">http://arxiv.org/abs/2311.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan, R. Manmatha</li>
<li>for: 这篇论文主要是为了提高encoder-decoder transformer模型的推理速度，以便在视觉语言（VL）任务中提高效率。</li>
<li>methods: 这篇论文提出了一种名为Dynamic Early Exit on Decoder（DEED）的方法，具体来说是在测试过程中，在decoder层中进行步骤性早期终止，以节省computational cost。同时，这篇论文还使用了一些实用的技术，例如共享生成头和适应模组，以确保当脱离decoder层时，模型的准确性仍然保持。</li>
<li>results: 根据这篇论文的实验结果，这篇论文可以在不同的VL任务上实现30%-60%的总推理时间节省，并且与基准相比，具有相似或更高的准确性。<details>
<summary>Abstract</summary>
Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>编码-解码转换器模型在视觉语言（VL）任务上实现了很大的成功，但它们受到高的推理延迟影响。通常，解码器负担了大部分延迟，因为解码器使用了自动逆向编码。为了加速推理，我们提议在解码器（DEED）中实现动态早期离开。我们构建了多出口编码器-解码器转换器模型，并在其中训练了深度监督，以便每个解码层都可以生成可信的预测。此外，我们利用了简单 yet practical的技术，包括共享生成头和适应模块，以保持精度。基于多出口模型，我们在推理中实现Step-level动态早期离开，其中模型可以根据当前层的自信率使用 fewer decoder层。由于不同的decoder层可能会在不同的推理步骤中使用，我们在需要时就计算 previous decoding step的 deeper-layer decoder feature，以确保不同步骤的特征相对适应。我们使用了两个状态对应的encoder-decoder transformer模型对多个VL任务进行评估。我们发现我们的方法可以降低总推理延迟时间30%-60%，同时保持相对或更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Question-Multiple-Answer-Text-VQA"><a href="#Multiple-Question-Multiple-Answer-Text-VQA" class="headerlink" title="Multiple-Question Multiple-Answer Text-VQA"></a>Multiple-Question Multiple-Answer Text-VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08622">http://arxiv.org/abs/2311.08622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jha1990/VQA-Multimodal-AI">https://github.com/jha1990/VQA-Multimodal-AI</a></li>
<li>paper_authors: Peng Tang, Srikar Appalaraju, R. Manmatha, Yusheng Xie, Vijay Mahadevan<br>for:多个问题和多个答案（MQMA）是一种新的文本-VQA方法，用于在encoder-decoder transformer模型中进行文本-VQA任务。methods:MQMA方法使用多个问题和多个内容作为输入，并在encoder和decoder中使用自动进程来同时预测多个答案。此外，我们还提出了一种MQMA净化预训练任务，用于教导模型将多个问题和内容与相关答案相互对应。results:我们的MQMA预训练模型在多个文本-VQA数据集上达到了最佳result，比前一个状态的方法提高了+2.5%、+1.4%、+0.6%和+1.1%的精度。<details>
<summary>Abstract</summary>
We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. The text-VQA task requires a model to answer a question by understanding multi-modal content: text (typically from OCR) and an associated image. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifications to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了多问题多答案（MQMA），一种新的方法来实现编码器-解码器变换器模型中的文本-VQA任务。文本-VQA任务需要模型通过理解多Modal内容（通常是OCR提供的文本）和相关的图像来回答问题。根据我们所知，前一个方法都是处理单个问题和其相关的内容，以预测单个答案。而我们提出的MQMA方法可以同时处理多个问题和内容，并在编码器和解码器之间进行自动推导，以预测多个答案。我们在标准编码器-解码器变换器模型上进行了一些新的建模修改，以支持MQMA。我们还提出了一种MQMA减噪预训练任务，用于教育模型将多个问题和内容与相关答案进行对齐和分离。MQMA预训练模型在多个文本-VQA数据集上达到了状态的当前最佳成绩，具体来说是OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="Toucan-Token-Aware-Character-Level-Language-Modeling"><a href="#Toucan-Token-Aware-Character-Level-Language-Modeling" class="headerlink" title="Toucan: Token-Aware Character Level Language Modeling"></a>Toucan: Token-Aware Character Level Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08620">http://arxiv.org/abs/2311.08620</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Fleshman, Benjamin Van Durme</li>
<li>for: 提高 caracter-level 模型的效率，不需要额外训练tokenizer。</li>
<li>methods: 提出了一种通过学习将字符表示合并成 tokens的方法，使得训练这些模型更加高效。</li>
<li>results: 与先前的工作比较，得到了无损语言模型性能的速度提升，并且发现了使用我们的动态tokenization方法可以处理更长的字符串。<details>
<summary>Abstract</summary>
Character-level language models obviate the need for separately trained tokenizers, but efficiency suffers from longer sequence lengths. Learning to combine character representations into tokens has made training these models more efficient, but they still require decoding characters individually. We propose Toucan, an augmentation to character-level models to make them "token-aware". Comparing our method to prior work, we demonstrate significant speed-ups in character generation without a loss in language modeling performance. We then explore differences between our learned dynamic tokenization of character sequences with popular fixed vocabulary solutions such as Byte-Pair Encoding and WordPiece, finding our approach leads to a greater amount of longer sequences tokenized as single items. Our project and code are available at https://nlp.jhu.edu/nuggets/.
</details>
<details>
<summary>摘要</summary>
Character-level 语言模型可以减少单独训练的 tokenizer，但是序列长度变长会导致效率下降。学习将字符表示合并到 tokens 中可以使训练这些模型更加高效，但是仍需要单独decode每个字符。我们提出了 Toucan，一种对Character-level模型进行改进，使其“字符意识”。我们与之前的工作进行比较，表明我们的方法可以在字符生成中得到显著的速度提升，而不会对语言模型性能产生影响。然后，我们探索了我们学习的动态tokenization 与常见的固定词库解决方案，如 Byte-Pair Encoding 和 WordPiece，发现我们的方法可以处理更多的更长的序列。我们的项目和代码可以在https://nlp.jhu.edu/nuggets/ 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech"><a href="#Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech" class="headerlink" title="Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech"></a>Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08607">http://arxiv.org/abs/2311.08607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spaghettisystems/emotion_whisper">https://github.com/spaghettisystems/emotion_whisper</a></li>
<li>paper_authors: Mohamed Osman, Tamer Nadeem, Ghada Khoriba</li>
<li>for: 本研究旨在提高人机交互的灵活性，通过识别 spoken communication 中的情感。</li>
<li>methods: 我们提出了一种 soft labeling 系统，用于捕捉情感的渐变强度。我们还使用 Whisper 编码器和基于对比学习的数据增强方法，以强调情感的时间动态。</li>
<li>results: 我们在四种多语言 dataset 上进行验证，得到了显著的零样本泛化。我们还发布了我们的开源模型参数和初步的成果，并在 Hume-Prosody 上进行了微调。<details>
<summary>Abstract</summary>
Recognizing emotions in spoken communication is crucial for advanced human-machine interaction. Current emotion detection methodologies often display biases when applied cross-corpus. To address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like English, Chinese, and Japanese. We propose a soft labeling system to capture gradational emotional intensities. Using the Whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. Our validation on four multilingual datasets demonstrates notable zero-shot generalization. We publish our open source model weights and initial promising results after fine-tuning on Hume-Prosody.
</details>
<details>
<summary>摘要</summary>
recognizing emotions in spoken communication is crucial for advanced human-machine interaction. current emotion detection methodologies often display biases when applied cross-corpus. to address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like english, chinese, and japanese. we propose a soft labeling system to capture gradational emotional intensities. using the whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. our validation on four multilingual datasets demonstrates notable zero-shot generalization. we publish our open source model weights and initial promising results after fine-tuning on hume-prosody.Here's a word-for-word translation of the text into Simplified Chinese:认识 spoken communication 中的情感是进阶人机交互的关键。现有的情感检测方法经常在 cross-corpus 应用时显示偏见。为了解决这个问题，我们的研究将16种多样化的数据集融合，共计375小时的数据，涵盖英语、中文和日语等语言。我们提议使用 soft labeling 系统来捕捉情感的Gradational强度。使用 whisper encoder 和基于对比学习的数据增强方法，我们的方法强调情感的时间动态。我们在四种多语言数据集上进行验证，并显示了很好的零shot泛化性。我们将我们的开源模型权重和初步成果发布，并在 hume-prosody 上进行了细化调整。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CL_2023_11_15/" data-id="clp53jwog00f3yp88d13o4jyl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.LG_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T10:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.LG_2023_11_15/">cs.LG - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width"><a href="#A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width" class="headerlink" title="A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width"></a>A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09197">http://arxiv.org/abs/2311.09197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Gaitonde, Elchanan Mossel</li>
<li>for: 这个论文是为了有效地从数据中学习ising模型的下面参数。</li>
<li>methods: 这篇论文使用的方法是node-wise logistic regression。</li>
<li>results: 这篇论文的结果是，node-wise logistic regression可以在一些不满足现有的假设的情况下也能够有效地学习ising模型的参数，包括：（1）从各种本地Markov链生成的数据中 recuperate the parameters with optimal sample complexity up to $\log\log n$ factors。（2）在Sherrington-Kirkpatrick模型中，从$\mathsf{poly}(n)$独立样本中 recuperate the parameters in most of the known高温度 regime。（3）在M-regime中的数据中，logistic regression achieve an exponential improvement in learning from samples。<details>
<summary>Abstract</summary>
We revisit the problem of efficiently learning the underlying parameters of Ising models from data. Current algorithmic approaches achieve essentially optimal sample complexity when given i.i.d. samples from the stationary measure and the underlying model satisfies "width" bounds on the total $\ell_1$ interaction involving each node. We show that a simple existing approach based on node-wise logistic regression provably succeeds at recovering the underlying model in several new settings where these assumptions are violated:   (1) Given dynamically generated data from a wide variety of local Markov chains, like block or round-robin dynamics, logistic regression recovers the parameters with optimal sample complexity up to $\log\log n$ factors. This generalizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE Trans. Inf. Theory'18] for structure recovery in bounded degree graphs from Glauber dynamics.   (2) For the Sherrington-Kirkpatrick model of spin glasses, given $\mathsf{poly}(n)$ independent samples, logistic regression recovers the parameters in most of the known high-temperature regime via a simple reduction to weaker structural properties of the measure. This improves on recent work of Anari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution learning at higher temperature.   (3) As a simple byproduct of our techniques, logistic regression achieves an exponential improvement in learning from samples in the M-regime of data considered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel guarantees for learning from the adversarial Glauber dynamics of Chin, Moitra, Mossel, and Sandon [ArXiv'23].   Our approach thus significantly generalizes the elegant analysis of Wu, Sanghavi, and Dimakis [Neurips'19] without any algorithmic modification.
</details>
<details>
<summary>摘要</summary>
我们回归到对零点模型的效率学习问题。现有的算法方法可以在具有独立同分布的样本下 achievement essentially optimal sample complexity。我们显示了一个简单的现有方法，基于单点逻辑回归，可以在以下新的设定中 recuperate 零点模型：(1) 对具有各种本地Markov过程生成的资料进行动态生成，如封页或者轮询动态，逻辑回归可以从样本中 recuperate 零点模型， sample complexity 只差loglog n个因素。这扩展了 Bresler、Gamarnik 和 Shah 的特殊算法 [IEEE Trans. Inf. Theory'18] 用于结构回传graph from Glauber dynamics。(2) 韦伯-基督徒模型中的磁矩玻璃，对于大多数高温区域，逻辑回归可以从独立样本中 recuperate 零点模型， sample complexity 仅差poly(n)个因素。这超越了 recent work of Anari、Jain、Koehler、Pham 和 Vuong [ArXiv'23] 的分布学习。(3) 作为我们的技术的副产物，逻辑回归可以从M-regime的数据中学习，并提供了 novel guarantees for learning from adversarial Glauber dynamics。我们的方法因此可以从Wu、Sanghavi 和 Dimakis [Neurips'19] 的整体分析中获得 significan improvement。我们的方法不需要任何算法修改，可以从零点模型中 recuperate 零点模型，sample complexity 仅差loglog n个因素。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge"><a href="#Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge" class="headerlink" title="Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge"></a>Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09195">http://arxiv.org/abs/2311.09195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Lee, Seung-Woo Seo</li>
<li>for: 实现现实环境中的强化学习算法，即无需人工干预重新设定环境。</li>
<li>methods: 提出了一个新的自主强化学习（ARL）算法，可以根据学习进度自动生成课程。这个课程可以减少需要的人工重新设定，并且不需要任务特定的知识。</li>
<li>results: 在实验中，我们的ARL算法可以生成一个适应学习进度的课程，并允许Agent自动重新设定到多样和有用的初始状态。我们的课程可以帮助Agent快速获得解释，并且在缺乏优化的情况下表现更好。<details>
<summary>Abstract</summary>
A significant bottleneck in applying current reinforcement learning algorithms to real-world scenarios is the need to reset the environment between every episode. This reset process demands substantial human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have introduced autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. While their curricula can reduce the number of required manual resets by taking into account the agent's learning progress, they rely on task-specific knowledge, such as predefined initial states or reset reward functions. In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum empowers the agent to autonomously reset to diverse and informative initial states. To achieve this, we introduce a success discriminator that estimates the success probability from each initial state when the agent follows the forward policy. The success discriminator is trained with relabeled transitions in a self-supervised manner. Our experimental results demonstrate that our ARL algorithm can generate an adaptive curriculum and enable the agent to efficiently bootstrap to solve sparse-reward maze navigation tasks, outperforming baselines with significantly fewer manual resets.
</details>
<details>
<summary>摘要</summary>
现有一个重要的瓶颈在应用现有的奖励学习算法到实际场景中，那就是需要在每个话语中重置环境。这个重置过程需要大量的人工干预，使得Agent无法不断学习和自动学习。一些最近的工作已经引入了自主奖励学习（ARL）算法，这些算法可以生成合适的课程，以便同时培养重置和前进政策。而这些课程可以通过考虑Agent的学习进度来减少需要的人工重置数量，但是它们需要任务特定的知识，例如预先定义的初始状态或重置奖励函数。在这篇论文中，我们提出了一种新的ARL算法，可以根据Agent的学习进度自适应生成课程，不需要任务特定的知识。我们的课程使得Agent可以自主地重置到多样化和有用的初始状态。为了实现这一点，我们引入了一个成功预测器，该预测器可以在Agent遵循前进政策时，根据每个初始状态的成功概率进行预测。成功预测器通过对每个过程进行自我supervised的标注训练。我们的实验结果表明，我们的ARL算法可以生成适应性的课程，使得Agent能够高效地 bootstrap到解决稀热奖励迷宫 Navigation task，与基eline相比，需要 significatively fewer manual resets。
</details></li>
</ul>
<hr>
<h2 id="Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series"><a href="#Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series" class="headerlink" title="Approaching adverse event detection utilizing transformers on clinical time-series"></a>Approaching adverse event detection utilizing transformers on clinical time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09165">http://arxiv.org/abs/2311.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helge Fredriksen, Per Joel Burman, Ashenafi Woldaregay, Karl Øyvind Mikalsen, Ståle Nymo</li>
<li>for: 这个研究旨在发展一个侦测病人医疗追踪的偏离系统，以预防错误诊断或不适当的治疗，从而降低不良事件的机会。</li>
<li>methods: 这个研究使用了一个自动学习框架，基于STraTS transformer架构，将时间序列资料转换为内在空间表示。这些表示之后运用不同的聚集技术来探索患者的临床进程特征。</li>
<li>results: 这个研究的初步结果具有探索患者的临床进程特征，但发现更多的数据，特别是患者的民生资讯，是关键的 для更好地评估方法的性能。<details>
<summary>Abstract</summary>
Patients being admitted to a hospital will most often be associated with a certain clinical development during their stay. However, there is always a risk of patients being subject to the wrong diagnosis or to a certain treatment not pertaining to the desired effect, potentially leading to adverse events. Our research aims to develop an anomaly detection system for identifying deviations from expected clinical trajectories. To address this goal we analyzed 16 months of vital sign recordings obtained from the Nordland Hospital Trust (NHT). We employed an self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. These representations were then subjected to various clustering techniques to explore potential patient phenotypes based on their clinical progress. While our preliminary results from this ongoing research are promising, they underscore the importance of enhancing the dataset with additional demographic information from patients. This additional data will be crucial for a more comprehensive evaluation of the method's performance.
</details>
<details>
<summary>摘要</summary>
Hospitalized patients often experience clinical developments during their stay, but there is a risk of incorrect diagnosis or inappropriate treatment, which can lead to adverse events. Our research aims to develop an anomaly detection system to identify deviations from expected clinical trajectories. We analyzed 16 months of vital sign recordings from the Nordland Hospital Trust (NHT) using a self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. We then applied various clustering techniques to explore potential patient phenotypes based on their clinical progress. While our preliminary results are promising, we recognize the need to enhance the dataset with additional demographic information from patients to evaluate the method's performance more comprehensively.Here's the word-for-word translation of the text into Simplified Chinese: hospitalized patients 常常会经历医学发展 during their stay, but there is a risk of incorrect diagnosis or inappropriate treatment, which can lead to adverse events. 我们的研究目标是开发一个异常检测系统，用于标识不符预期的临床轨迹。我们使用了 Nordland Hospital Trust (NHT) 的 16 个月的重要指标记录数据，并使用了一种无监督框架，基于 STraTS 转换器架构，将时间序列数据表示在幂值空间中。我们然后应用了不同的聚类技术，以探索可能的患者类型，基于他们的临床进程。虽然我们的初步结果是有前途，但我们认为需要更多的人口数据，以更好地评估方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation"><a href="#Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation" class="headerlink" title="Model Agnostic Explainable Selective Regression via Uncertainty Estimation"></a>Model Agnostic Explainable Selective Regression via Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09145">http://arxiv.org/abs/2311.09145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Pugnana, Carlos Mougan, Dan Saattrup Nielsen</li>
<li>for: 提高机器学习系统的可靠性和信任性</li>
<li>methods: 使用模型无参数不确定性估计来实现选择性回归</li>
<li>results: 与现有选择回归方法相比，提出了一种新的选择回归方法，在69个数据集上进行了广泛的比较，并使用可解释AI技术来理解选择回归的驱动力。<details>
<summary>Abstract</summary>
With the wide adoption of machine learning techniques, requirements have evolved beyond sheer high performance, often requiring models to be trustworthy. A common approach to increase the trustworthiness of such systems is to allow them to refrain from predicting. Such a framework is known as selective prediction. While selective prediction for classification tasks has been widely analyzed, the problem of selective regression is understudied. This paper presents a novel approach to selective regression that utilizes model-agnostic non-parametric uncertainty estimation. Our proposed framework showcases superior performance compared to state-of-the-art selective regressors, as demonstrated through comprehensive benchmarking on 69 datasets. Finally, we use explainable AI techniques to gain an understanding of the drivers behind selective regression. We implement our selective regression method in the open-source Python package doubt and release the code used to reproduce our experiments.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的广泛应用，模型的需求已经超出了纯粹的高性能，常常需要模型具备可靠性。一种常见的方法来增强机器学习系统的可靠性是让它们可以决定不预测。这种框架被称为选择性预测。在分类任务上，选择性预测已经得到了广泛的研究，而选择性回归问题尚未得到充分研究。本文提出了一种新的选择性回归方法，利用模型不 Parametric 不确定性估计。我们的提议的框架在69个数据集上进行了完整的比较，并示出了与当前最佳选择性回归器相比的超过其他的性能。最后，我们使用可解释AI技术来理解选择性回归的驱动力。我们实现了我们的选择性回归方法在Python开源包中，并将用于重现我们的实验的代码发布。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-parameter-tracking-with-partial-state-observation"><a href="#Machine-learning-parameter-tracking-with-partial-state-observation" class="headerlink" title="Machine-learning parameter tracking with partial state observation"></a>Machine-learning parameter tracking with partial state observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09142">http://arxiv.org/abs/2311.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Meng Zhai, Mohammadamin Moradi, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</li>
<li>for: 这个论文是为了解决复杂非线性动力系统中时间变化的参数的准确追踪问题。</li>
<li>methods: 这个论文使用了模型自由和完全数据驱动的方法，利用储存器计算来准确地追踪时间变化的参数从部分状态观测数据中。</li>
<li>results: 论文通过使用训练数据来预测参数的变化情况，并在低维和高维、马尔可夫和非马尔可夫非线性动力系统上进行了示例。<details>
<summary>Abstract</summary>
Complex and nonlinear dynamical systems often involve parameters that change with time, accurate tracking of which is essential to tasks such as state estimation, prediction, and control. Existing machine-learning methods require full state observation of the underlying system and tacitly assume adiabatic changes in the parameter. Formulating an inverse problem and exploiting reservoir computing, we develop a model-free and fully data-driven framework to accurately track time-varying parameters from partial state observation in real time. In particular, with training data from a subset of the dynamical variables of the system for a small number of known parameter values, the framework is able to accurately predict the parameter variations in time. Low- and high-dimensional, Markovian and non-Markovian nonlinear dynamical systems are used to demonstrate the power of the machine-learning based parameter-tracking framework. Pertinent issues affecting the tracking performance are addressed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>复杂非线性动力系统经常存在时间变化的参数，正确跟踪这些参数是STATE estimation、预测和控制任务中的关键。现有的机器学习方法需要完整的状态观察，而tacitly assume 非断续变化的参数。通过形式化反问题并利用储存计算，我们开发了一种无模型和完全数据驱动的框架，可以在部分状态观察下，在实时中高精度地跟踪时间变化的参数。特别是，通过训练数据来自系统动态变量的一个子集，框架可以准确预测参数的时间变化。低维度和高维度、Markovian和非Markovian非线性动力系统都用来证明框架的力量。关于跟踪性能的问题也进行了解决。
</details></li>
</ul>
<hr>
<h2 id="Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury"><a href="#Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury" class="headerlink" title="Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury"></a>Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09137">http://arxiv.org/abs/2311.09137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izak Yasrebi-de Kom, Joanna Klopotowska, Dave Dongelmans, Nicolette De Keizer, Kitty Jager, Ameen Abu-Hanna, Giovanni Cinà</li>
<li>for: 这个研究的目的是为了开发一种基于观察数据的 causal modeling 方法，用于估计药物可能对患者具有的危害，以提高医院化学otherapy中的药物安全监测。</li>
<li>methods: 这个研究使用了两个关键的 causal inference 组件：（1）目标试验模拟框架，和（2）基于机器学习的个性化治疗效果估计。研究使用了观察数据来估计药物可能对患者具有的危害，并与医学专家提供的估计相比较。</li>
<li>results: 研究发现，使用 causal modeling 方法可以提供更为precise的危害估计，并且可以减少人类偏见的影响。然而，研究还存在一些重要的限制和改进方向，例如需要更多的观察数据和更好的模型。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
The current best practice approach for the retrospective diagnosis of adverse drug events (ADEs) in hospitalized patients relies on a full patient chart review and a formal causality assessment by multiple medical experts. This evaluation serves to qualitatively estimate the probability of causation (PC); the probability that a drug was a necessary cause of an adverse event. This practice is manual, resource intensive and prone to human biases, and may thus benefit from data-driven decision support. Here, we pioneer a causal modeling approach using observational data to estimate a lower bound of the PC (PC$_{low}$). This method includes two key causal inference components: (1) the target trial emulation framework and (2) estimation of individualized treatment effects using machine learning. We apply our method to the clinically relevant use-case of vancomycin-induced acute kidney injury in intensive care patients, and compare our causal model-based PC$_{low}$ estimates to qualitative estimates of the PC provided by a medical expert. Important limitations and potential improvements are discussed, and we conclude that future improved causal models could provide essential data-driven support for medication safety monitoring in hospitalized patients.
</details>
<details>
<summary>摘要</summary>
现有的最佳实践方法 для逆向诊断医药不良反应（ADE）在医院化症病人中是通过全部病人护理记录和多名医疗专家形式的可能性评估来估计可能性（PC），即药物是否是必要的 causa causans 的医学事件。这种评估方法是手动、资源占用和人类偏见易受影响的，因此可能会从数据驱动支持中受益。在这里，我们开拓了一种使用观察数据来估计PC下界（PC$_{low}$）的 causal 模型方法。这个方法包括两个关键的 causal inference 组件：（1）目标试验模拟框架和（2）使用机器学习来估计个体化治疗效果。我们在医学实践中有优势的使用vancomycin引起的急性肾脏损伤的例子中应用了我们的方法，并与医疗专家提供的PC估计进行比较。我们讨论了重要的局限性和改进方向，并结论未来改进的causal模型可能提供数据驱动支持医学安全监测的 essence。
</details></li>
</ul>
<hr>
<h2 id="Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion"><a href="#Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion" class="headerlink" title="Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion"></a>Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09128">http://arxiv.org/abs/2311.09128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Arnold, Frank Schäfer, Niels Lörch</li>
<li>for: 这个论文的目的是提出一种改进learning-by-confusion scheme的方法，以便更快地identify critical points from data without prior knowledge of the underlying phases.</li>
<li>methods: 这个方法使用了多任务学习，只需要训练一个多类分类器，而不是每个可能的grid point的二分类器，从而避免了与grid point数直接相关的计算成本增加。</li>
<li>results: 在应用于易斯宁模型和一个由Stable Diffusion生成的图像集上，发现了明显的加速，与理想情况closely corresponds，只有小量偏差。<details>
<summary>Abstract</summary>
Machine learning has been successfully used to study phase transitions. One of the most popular approaches to identifying critical points from data without prior knowledge of the underlying phases is the learning-by-confusion scheme. As input, it requires system samples drawn from a grid of the parameter whose change is associated with potential phase transitions. Up to now, the scheme required training a distinct binary classifier for each possible splitting of the grid into two sides, resulting in a computational cost that scales linearly with the number of grid points. In this work, we propose and showcase an alternative implementation that only requires the training of a single multi-class classifier. Ideally, such multi-task learning eliminates the scaling with respect to the number of grid points. In applications to the Ising model and an image dataset generated with Stable Diffusion, we find significant speedups that closely correspond to the ideal case, with only minor deviations.
</details>
<details>
<summary>摘要</summary>
机器学习已成功应用于研究相转换。一种最受欢迎的方法是无知下相转换点的识别，即学习混淆方案。作为输入，它需要系统样本从 Parameters 的变化相关的潜在相转换点采样。现在，该方法需要训练每个可能的网格分割两侧的独立二进制分类器，因此计算成本与网格点数成直线关系。在这种工作中，我们提议并展示了一种替代实现，只需训练一个多类分类器。理想情况下，这种多任务学习可以消除与网格点数相关的扩展。在应用于 Ising 模型和通过 Stable Diffusion 生成的图像集上，我们发现了显著的加速，与理想情况几乎完全相符，只有小差异。
</details></li>
</ul>
<hr>
<h2 id="Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints"><a href="#Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints" class="headerlink" title="Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints"></a>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09065">http://arxiv.org/abs/2311.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hari Dahal, Wei Liu, Yangyang Xu</li>
<li>for:  solves problems with a weakly-convex objective and convex linear&#x2F;nonlinear constraints.</li>
<li>methods:  uses a damped proximal augmented Lagrangian method (DPALM) that adopts a damped dual stepsize to ensure the boundedness of dual iterates.</li>
<li>results:  can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations, and achieves overall iteration complexity of $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ or $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ depending on the structure of the problem.<details>
<summary>Abstract</summary>
We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一个受束的减少对应扩展拉格朗日方法（DPALM），用于解决具有弱抽象目标函数和凸线性/非凸约束的问题。而不是使用全部步长，DPALM 使用一个减少的对应步长，以确保对应变数的积分。我们证明了 DPALM 可以在 $O(\vareps^{-2})$ 外部迭代中生成一个（近似） $\vareps$-KKT 点。此外，我们确立了 DPALM 的总迭代复杂度，当 objective 是 Either 是弹簧缓和变数弹簧缓 的情况下，DPALM 的总迭代复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$，用于生成一个 $\varepsilon$-KKT 点。另一方面，当 objective 是弹簧缓和变数弹簧缓的情况下，DPALM 的总迭代复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$，用于生成一个近似 $\varepsilon$-KKT 点。我们的外部迭代复杂度和总迭代复杂度都可以与现有最佳的结果对应，或者超过相同类型的问题的最佳结果。此外，我们还进行了一些数据实验，用于证明 DPALM 的实际高效性，并与一些现有的方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="New-Horizons-in-Parameter-Regularization-A-Constraint-Approach"><a href="#New-Horizons-in-Parameter-Regularization-A-Constraint-Approach" class="headerlink" title="New Horizons in Parameter Regularization: A Constraint Approach"></a>New Horizons in Parameter Regularization: A Constraint Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09058">http://arxiv.org/abs/2311.09058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter</li>
<li>for: 这篇论文是为了探讨受限制的参数调整（Constrained Parameter Regularization，CPR），它是传统量化调整的一种替代方案。</li>
<li>methods: 这篇论文使用了一种受限制的参数调整方法，通过强制个别参数群的统计量（例如L$_2$-norm）的Upper bound，将学习变成一个受限制的优化问题。以解决这个问题，这篇论文使用了一种扩展的拉格朗日方法。</li>
<li>results: 这篇论文的实验结果显示，CPR可以对“grokking”现象产生阻据，并且与传统量化调整相比，CPR能够将性能提升到相同或更高的水平。<details>
<summary>Abstract</summary>
This work presents constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L$_2$-norm) of individual parameter groups. This reformulates learning as a constrained optimization problem. To solve this, we utilize an adaptation of the augmented Lagrangian method. Our approach allows for varying regularization strengths across different parameter groups, removing the need for explicit penalty coefficients in the regularization terms. CPR only requires two hyperparameters and introduces no measurable runtime overhead. We offer empirical evidence of CPR's effectiveness through experiments in the "grokking" phenomenon, image classification, and language modeling. Our findings show that CPR can counteract the effects of grokking, and it consistently matches or surpasses the performance of traditional weight decay.
</details>
<details>
<summary>摘要</summary>
这个研究提出了受限制参数规范化（CPR），它是传统权值衰退的替代方案。而不是对所有参数应用一定的罚款，我们强制某个统计量（例如L$_2$-范数）的参数组别的Upper bound。这将学习转化为一个受限制优化问题。为解决这个问题，我们利用一种改进后的扩展拉格朗日方法。我们的方法允许不同参数组别的强制规范强度不同，从而消除了显式的罚款系数在规范项中。CPR只需要两个 гиперпараметров，并没有可观测的运行时间开销。我们提供了CPR的效iveness的实验证据，包括“grokking”现象、图像分类和自然语言处理等领域的实验结果。我们的发现表明，CPR可以抵消grokking的效果，并在性能方面与传统权值衰退相匹配或甚至超过。
</details></li>
</ul>
<hr>
<h2 id="On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning"><a href="#On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning" class="headerlink" title="On the Foundation of Distributionally Robust Reinforcement Learning"></a>On the Foundation of Distributionally Robust Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09018">http://arxiv.org/abs/2311.09018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou</li>
<li>for: 提供了一种 Distributionally Robust Reinforcement Learning (DRRL) 的理论基础，以适应环境变化。</li>
<li>methods: 使用 Distributionally Robust Markov Decision Processes (DRMDPs) 模型，要求决策者选择最优策略在最差情况下。</li>
<li>results: 提供了一种推广和扩展现有形式ulation的 DRMDP 框架，并研究了控制器和敌对者属性的影响。 另外，还提供了一些counterexample，表明在某些情况下，DPP 无法存在。<details>
<summary>Abstract</summary>
Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent."中文翻译：<<SYS>>受到训练和部署环境变化的需求启发，我们贡献了分布robust reinforcement learning（DRRL）的理论基础。我们通过一个包容性强的模型化框架，中心是分布robust Markov决策过程（DRMDP），要求决策者在敌对者引起的最差分布变化下选择最优策略。我们通过一元化和扩展现有的表述，彻底构造DRMDP，覆盖决策者和敌对者的各种模型特性，包括适应粒度、历史依赖、Markov和Markov时间同质的决策者和敌对者动态。此外，我们还探讨敌对者引起的变化的灵活性，包括SA和S-正方形性。在DRMDP框架中，我们研究DPP的存在或缺失的条件。从算法角度来看，DPP的存在具有重要意义，因为大多数现有的数据和计算效率RL算法都依赖于DPP。为了研究其存在，我们全面检查控制者和敌对者特性的组合，提供了一致的证明方法。我们还提供了对某些设置中DPP的全面性是缺失的counterexample。
</details></li>
</ul>
<hr>
<h2 id="Semidefinite-programs-simulate-approximate-message-passing-robustly"><a href="#Semidefinite-programs-simulate-approximate-message-passing-robustly" class="headerlink" title="Semidefinite programs simulate approximate message passing robustly"></a>Semidefinite programs simulate approximate message passing robustly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09017">http://arxiv.org/abs/2311.09017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Misha Ivkov, Tselil Schramm</li>
<li>for: 这个论文是为了研究approximate message passing（AMP）算法的稳定性和可靠性。</li>
<li>methods: 论文使用了local statistics hierarchy的semidefinite programs（SDPs）来模拟AMP算法，并提供了对多种average-case优化问题的稳定性和可靠性保证。</li>
<li>results: 论文提出了一些robust guarantees для多种average-case优化问题，并与之前的强下界相比，表明AMP算法在这些问题上的性能较高。<details>
<summary>Abstract</summary>
Approximate message passing (AMP) is a family of iterative algorithms that generalize matrix power iteration. AMP algorithms are known to optimally solve many average-case optimization problems. In this paper, we show that a large class of AMP algorithms can be simulated in polynomial time by \emph{local statistics hierarchy} semidefinite programs (SDPs), even when an unknown principal minor of measure $1/\mathrm{polylog}(\mathrm{dimension})$ is adversarially corrupted. Ours are the first robust guarantees for many of these problems. Further, our results offer an interesting counterpoint to strong lower bounds against less constrained SDP relaxations for average-case max-cut-gain (a.k.a. "optimizing the Sherrington-Kirkpatrick Hamiltonian") and other problems.
</details>
<details>
<summary>摘要</summary>
通用消息传递（Approximate Message Passing，简称AMP）是一族迭代算法，可泛化矩阵力豪迭代。AMP算法能够优化许多平均情况优化问题。在这篇论文中，我们展示了一类AMP算法可以通过本地统计层次 semidefinite programs（SDPs）在几乎NP复杂度下模拟，即使Unknown principal minor的推算精度为1/多项式log（维度）。这些结果是许多问题的首次Robust guarantees。此外，我们的结果还提供了一些对具有较弱约束的SDP relaxation的强下界的反对照。
</details></li>
</ul>
<hr>
<h2 id="sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning"><a href="#sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning" class="headerlink" title="sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning"></a>sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08990">http://arxiv.org/abs/2311.08990</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/squlearn/squlearn">https://github.com/squlearn/squlearn</a></li>
<li>paper_authors: David A. Kreplin, Moritz Willmann, Jan Schnabel, Frederic Rapp, Marco Roth</li>
<li>for: 这个论文旨在提供一个易用的Python库，用于Quantum Machine Learning（QML），并且可以与传统的机器学习工具Like scikit-learn集成。</li>
<li>methods: 这个库使用了双层架构，供QML研究者和实践者使用，并提供了高效的实验、测试和管道功能。它还包括了量子传播方法和量子神经网络，以及自定义数据编码策略、自动化执行处理和特殊的核心调理技术。</li>
<li>results: 这个库的目标是将现有的量子计算能力与实际的机器学习应用 bridging，并提供一个易用的、NISQ-兼容的Python库，以便实际应用。<details>
<summary>Abstract</summary>
sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum machine learning (QML), designed for seamless integration with classical machine learning tools like scikit-learn. The library's dual-layer architecture serves both QML researchers and practitioners, enabling efficient prototyping, experimentation, and pipelining. sQUlearn provides a comprehensive toolset that includes both quantum kernel methods and quantum neural networks, along with features like customizable data encoding strategies, automated execution handling, and specialized kernel regularization techniques. By focusing on NISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap between current quantum computing capabilities and practical machine learning applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research"><a href="#A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research" class="headerlink" title="A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research"></a>A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08979">http://arxiv.org/abs/2311.08979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Diament, Maria Gorodetski, Adam Jankelow, Ayya Keshet, Tal Shor, Daphna Weissglas-Volkov, Hagai Rossman, Eran Segal</li>
<li>for: 这个研究旨在提供一个全新、丰富的家庭呼吸暂停测试数据集，使用FDA批准的WatchPAT-300设备，从7,077名参与者和21,412个夜晚中收集到的数据。</li>
<li>methods: 这个数据集包括三级别的睡眠数据：原始多通道时间序列数据、注释的睡眠事件和计算的摘要统计，其中包括447个与睡眠建筑、呼吸暂停和心率变化相关的特征。</li>
<li>results: 研究人员提供了不同年龄和性别的AHI、睡眠效率、WASO和HRV样本熵的参考值，并证明了该数据集可以提高对各种健康相关特征的预测能力，包括体重组成、骨骼密度、血糖水平和呼吸系统健康。这些结果表明该数据集具有提高睡眠研究、个性化医疗和生物医学机器学习应用的潜力。<details>
<summary>Abstract</summary>
This study introduces a novel, rich dataset obtained from home sleep apnea tests using the FDA-approved WatchPAT-300 device, collected from 7,077 participants over 21,412 nights. The dataset comprises three levels of sleep data: raw multi-channel time-series from sensors, annotated sleep events, and computed summary statistics, which include 447 features related to sleep architecture, sleep apnea, and heart rate variability (HRV). We present reference values for Apnea/Hypopnea Index (AHI), sleep efficiency, Wake After Sleep Onset (WASO), and HRV sample entropy, stratified by age and sex. Moreover, we demonstrate that the dataset improves the predictive capability for various health related traits, including body composition, bone density, blood sugar levels and cardiovascular health. These results illustrate the dataset's potential to advance sleep research, personalized healthcare, and machine learning applications in biomedicine.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese". The translation is written in Traditional Chinese characters, which are used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds"><a href="#Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds" class="headerlink" title="Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds"></a>Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08978">http://arxiv.org/abs/2311.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ferreira, Cláudia Soares, Marta Guimarães</li>
<li>for: 预测卫星和遥感器在低地球轨道（LEO）中的碰撞风险，以满足空间业务的需求。</li>
<li>methods: 基于初始原理的新 derivation，可以自然地提供紧凑且快速的上限和下限 bounds for the probability of collision。</li>
<li>results: 比较traditional方法，本研究的实现可以减少计算概率的时间，从80%减少到几乎实时。<details>
<summary>Abstract</summary>
The proliferation of space debris in LEO has become a major concern for the space industry. With the growing interest in space exploration, the prediction of potential collisions between objects in orbit has become a crucial issue. It is estimated that, in orbit, there are millions of fragments a few millimeters in size and thousands of inoperative satellites and discarded rocket stages. Given the high speeds that these fragments can reach, even fragments a few millimeters in size can cause fractures in a satellite's hull or put a serious crack in the window of a space shuttle. The conventional method proposed by Akella and Alfriend in 2000 remains widely used to estimate the probability of collision in short-term encounters. Given the small period of time, it is assumed that, during the encounter: (1) trajectories are represented by straight lines with constant velocity; (2) there is no velocity uncertainty and the position exhibits a stationary distribution throughout the encounter; and (3) position uncertainties are independent and represented by Gaussian distributions. This study introduces a novel derivation based on first principles that naturally allows for tight and fast upper and lower bounds for the probability of collision. We tested implementations of both probability and bound computations with the original and our formulation on a real CDM dataset used in ESA's Collision Avoidance Challenge. Our approach reduces the calculation of the probability to two one-dimensional integrals and has the potential to significantly reduce the processing time compared to the traditional method, from 80% to nearly real-time.
</details>
<details>
<summary>摘要</summary>
space垃圾在低地球轨道（LEO）的扩散已成为航天业的主要问题。随着航天探索的兴趣增加，预测可能的轨道碰撞的问题变得越来越重要。据估计，在轨道上有数百万个几毫米大小的碎片和数千个不工作的卫星和抛弃的火箭阶段。由于这些碎片的高速度，же�eso even small fragments can cause cracks in a satellite's hull or serious damage to the window of a space shuttle.根据阿克拉和Alfriend在2000年提出的传统方法，仍然广泛使用来估算碰撞的可能性。由于短期内的时间非常短暂，这种方法假设：（1）轨道可以用直线 repre sented by constant velocity;（2） velocity uncertainty is zero, and the position exhibits a stationary distribution throughout the encounter; and（3） position uncertainties are independent and represented by Gaussian distributions.本研究提出了一种基于第一原理的新 derivation，自然地允许 для紧凑和快速的上下限计算。我们对原始和我们的表述进行了实现，并在ESA的Collision Avoidance Challenge中使用了一个真实的CDM数据集进行测试。我们的方法将计算概率减少到两个一dimensional integral，并且有可能减少计算时间相比传统方法，从80%降低到实时。
</details></li>
</ul>
<hr>
<h2 id="A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization"><a href="#A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization" class="headerlink" title="A Single-Loop Algorithm for Decentralized Bilevel Optimization"></a>A Single-Loop Algorithm for Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08945">http://arxiv.org/abs/2311.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youran Dong, Shiqian Ma, Junfeng Yang, Chao Yin</li>
<li>for: 这篇论文关注了分布式机器学习中的二级优化问题。</li>
<li>methods: 我们提出了一种基于单loop的二级优化算法，用于解决分布式二级优化问题，并不需要大量的矩阵-向量乘法。此外，我们的算法不需要任何 gradient 不同假设。</li>
<li>results: 我们的分析表明，我们的算法可以达到最佳已知的优化率。<details>
<summary>Abstract</summary>
Bilevel optimization has received more and more attention recently due to its wide applications in machine learning. In this paper, we consider bilevel optimization in decentralized networks. In particular, we propose a novel single-loop algorithm for solving decentralized bilevel optimization with strongly convex lower level problem. Our algorithm is fully single-loop and does not require heavy matrix-vector multiplications when approximating the hypergradient. Moreover, unlike existing methods for decentralized bilevel optimization and federated bilevel optimization, our algorithm does not require any gradient heterogeneity assumption. Our analysis shows that the proposed algorithm achieves the best known convergence rate for bilevel optimization algorithms.
</details>
<details>
<summary>摘要</summary>
“双层优化在机器学习领域已经收到了更多的关注，特别是在分布式网络中。在这篇论文中，我们考虑了分布式双层优化。我们提出了一种新的单循环算法，用于解决分布式双层优化中的强某下层问题。我们的算法不需要大量的矩阵-向量乘法，而且不需要任何梯度异质假设。我们的分析表明，我们的算法可以达到最佳known的收敛率。”Here's the translation of each sentence:1. “双层优化” (bilevel optimization)2. “在机器学习领域已经收到了更多的关注” (has received more and more attention in the machine learning field)3. “特别是在分布式网络中” (especially in decentralized networks)4. “在这篇论文中，我们考虑了分布式双层优化” (In this paper, we consider decentralized bilevel optimization)5. “我们提出了一种新的单循环算法” (We propose a new single-loop algorithm)6. “用于解决分布式双层优化中的强某下层问题” (to solve the strongly convex lower-level problem in decentralized bilevel optimization)7. “我们的算法不需要大量的矩阵-向量乘法” (Our algorithm does not require heavy matrix-vector multiplications)8. “而且不需要任何梯度异质假设” (and does not require any gradient heterogeneity assumption)9. “我们的分析表明，我们的算法可以达到最佳known的收敛率” (Our analysis shows that our algorithm achieves the best known convergence rate)
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization"><a href="#Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization" class="headerlink" title="Efficiently Escaping Saddle Points for Non-Convex Policy Optimization"></a>Efficiently Escaping Saddle Points for Non-Convex Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08914">http://arxiv.org/abs/2311.08914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Niao He, Matthias Grossglauser</li>
<li>for: 本研究旨在提出一种基于积分 gradient 的减少偏差的 reinforcement learning 方法，可以在不同的偏差下采取优化策略，并且可以在不同的 Random Seed 下实现更好的性能。</li>
<li>methods: 本研究使用了积分 gradient 方法，并利用了第二阶导数信息以实现更好的减少偏差。具体来说，本研究使用了 Hessian vector products (HVP) 来提高方法的精度和可靠性。</li>
<li>results: 实验结果表明，提出的方法可以在不同的 Random Seed 下实现更好的性能，并且比现有的方法具有更好的减少偏差和更高的可靠性。<details>
<summary>Abstract</summary>
Policy gradient (PG) is widely used in reinforcement learning due to its scalability and good performance. In recent years, several variance-reduced PG methods have been proposed with a theoretical guarantee of converging to an approximate first-order stationary point (FOSP) with the sample complexity of $O(\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points. Moreover, these algorithms often use importance sampling (IS) weights which could impair the statistical effectiveness of variance reduction. In this paper, we propose a variance-reduced second-order method that uses second-order information in the form of Hessian vector products (HVP) and converges to an approximate second-order stationary point (SOSP) with sample complexity of $\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover, the proposed variance reduction technique bypasses IS weights by using HVP terms. Our experimental results show that the proposed algorithm outperforms the state of the art and is more robust to changes in random seeds.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出一种减少差异第二阶方法，使用第二阶信息的形式为Hessian vector products（HVP），并在$\mathcal{O}(\epsilon^{-3})$批量中达到一个相似的第二阶稳定点（SOSP）。这个率提高了目前最佳样本复杂度以实现相似SOSP的比例，即$O(\epsilon^{-0.5})$。此外，我们的减少差异技术可以 circumvent IS权重，使用HVP项。我们的实验结果表明，我们的算法超过了当前的状态，并在Random seed的变化下更加稳定。
</details></li>
</ul>
<hr>
<h2 id="On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series"><a href="#On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series" class="headerlink" title="On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series"></a>On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08902">http://arxiv.org/abs/2311.08902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ratschlab/clinical-embeddings">https://github.com/ratschlab/clinical-embeddings</a></li>
<li>paper_authors: Rita Kuznetsova, Alizée Pace, Manuel Burger, Hugo Yèche, Gunnar Rätsch</li>
<li>for: 本研究旨在探讨深度学习Architecture在医疗电子记录时序数据模型中的应用，特别是在医学急诊部(ICU)中的问题上。</li>
<li>methods: 本研究使用了最新的深度学习方法，包括表格数据的深度学习方法，以提高时序数据模型的性能。</li>
<li>results: 研究发现，通过joint使用表格数据的深度学习方法，可以提高临床时序模型的性能，并且发现了step-wise embedding的重要性，以及在医疗数据中feature grouping的重要性。<details>
<summary>Abstract</summary>
Recent advances in deep learning architectures for sequence modeling have not fully transferred to tasks handling time-series from electronic health records. In particular, in problems related to the Intensive Care Unit (ICU), the state-of-the-art remains to tackle sequence classification in a tabular manner with tree-based methods. Recent findings in deep learning for tabular data are now surpassing these classical methods by better handling the severe heterogeneity of data input features. Given the similar level of feature heterogeneity exhibited by ICU time-series and motivated by these findings, we explore these novel methods' impact on clinical sequence modeling tasks. By jointly using such advances in deep learning for tabular data, our primary objective is to underscore the importance of step-wise embeddings in time-series modeling, which remain unexplored in machine learning methods for clinical data. On a variety of clinically relevant tasks from two large-scale ICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of state-of-the-art methods for tabular time-series as time-step embedding models, showing overall performance improvement. In particular, we evidence the importance of feature grouping in clinical time-series, with significant performance gains when considering features within predefined semantic groups in the step-wise embedding module.
</details>
<details>
<summary>摘要</summary>
最近各种深度学习架构在序列模型方面的进步未能完全转移到电子医疗记录处理的时间序列问题上。特别是在医疗关键室（ICU）中的问题上，现状的方法仍然是通过树状方法进行时间序列分类。现有的深度学习方法对表格数据进行了更好的处理，已经超越了传统的方法。受到ICU时间序列数据的相似水平特征随机性的激发，我们探索这些新方法在临床时间序列模型任务中的影响。通过结合深度学习对表格数据的进步，我们的主要目标是强调在时间序列模型中的步骤Embedding的重要性，这在机器学习方法中对医疗数据没有被探讨。从两个大规模的ICU数据集MIMIC-III和HiRID中获得的许多临床相关任务上，我们的工作提供了对现状方法的完整分析，并证明了在不同的时间步骤Embedding模型中，特别是在Semantic group内的特征分组中，可以获得显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Towards-Label-Embedding-–-Measuring-classification-difficulty"><a href="#Towards-Label-Embedding-–-Measuring-classification-difficulty" class="headerlink" title="Towards Label Embedding – Measuring classification difficulty"></a>Towards Label Embedding – Measuring classification difficulty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08874">http://arxiv.org/abs/2311.08874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Hechinger, Christoph Koller, Xiao Xiang Zhu, Göran Kauermann</li>
<li>for: 本研究旨在 Addressing the problem of uncertainty in supervised learning, particularly in the labeling step of satellite image classification.</li>
<li>methods: 本研究使用了 Dirichlet-Multinomial 模型和随机渐近 maximum likelihood estimation 算法，将多个票数据 embedding 到 K-维空间中，其中 K 是可能的类别数量。</li>
<li>results: 研究发现，使用这种方法可以获得高质量的 embedding，并且可以 investigate 这些 embedding 的相关矩阵，它们可以看作是通用的混淆矩阵，具有良好的 semantic 相似性。这些结论可以作为通用的标签嵌入，在单个真实标签不可 garantizado的情况下提供有价值的信息。<details>
<summary>Abstract</summary>
Uncertainty quantification in machine learning is a timely and vast field of research. In supervised learning, uncertainty can already occur in the very first stage of the training process, the labelling step. In particular, this is the case when not every instance can be unambiguously classified. The problem occurs for classifying instances, where classes may overlap or instances can not be clearly categorised. In other words, there is inevitable ambiguity in the annotation step and not necessarily a 'ground truth'. We look exemplary at the classification of satellite images. Each image is annotated independently by multiple labellers and classified into local climate zones (LCZs). For each instance we have multiple votes, leading to a distribution of labels rather than a single value. The main idea of this work is that we do not assume a ground truth label but embed the votes into a K-dimensional space, with K as the number of possible categories. The embedding is derived from the voting distribution in a Bayesian setup, modelled via a Dirichlet-Multinomial model. We estimate the model and posteriors using a stochastic Expectation Maximisation algorithm with Markov Chain Monte Carlo steps. While we focus on the particular example of LCZ classification, the methods developed in this paper readily extend to other situations where multiple annotators independently label texts or images. We also apply our approach to two other benchmark datasets for image classification to demonstrate this. Besides the embeddings themselves, we can investigate the resulting correlation matrices, which can be seen as generalised confusion matrices and reflect the semantic similarities of the original classes very well for all three exemplary datasets. The insights gained are valuable and can serve as general label embedding if a single ground truth per observation cannot be guaranteed.
</details>
<details>
<summary>摘要</summary>
机器学习中的不确定性评估是一个时髦的和广泛的研究领域。在指导学习中，不确定性可以在训练过程的第一个阶段出现，即标注阶段。特别是在分类实例时，当类别 overlap 或实例无法明确分类时，标注过程中会具有不可避免的模糊性。我们通过卫星图像的分类为例，每个图像都由多名标注者独立地标注并分类到本地气候区（LCZ）。对每个实例而言，我们有多个选择，导致一个分布而不是单个值。我们的主要想法是不假设固定的真实标签，而是将投票embedded到K维空间中（K为可能的类别数）。这个空间的定义来自于投票分布，通过 Dirichlet-Multinomial 模型来描述。我们使用Markov Chain Monte Carlo步骤来估算模型和 posterior，并使用随机推估最大化算法。我们在这篇论文中专注于 LCZ 分类的情况，但方法可以轻松扩展到其他独立标注者标注的文本或图像。此外，我们还应用了这些方法到两个其他图像分类 benchmark 数据集，以证明其通用性。除了嵌入本身之外，我们还可以研究得到的相关性矩阵，它们可以看作是通用的混淆矩阵，很好地反映原始类别之间的语义相似性。这些发现可以作为一般的标签嵌入，当单个真实标签无法保证时。
</details></li>
</ul>
<hr>
<h2 id="Statistical-learning-by-sparse-deep-neural-networks"><a href="#Statistical-learning-by-sparse-deep-neural-networks" class="headerlink" title="Statistical learning by sparse deep neural networks"></a>Statistical learning by sparse deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08845">http://arxiv.org/abs/2311.08845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Felix Abramovich</li>
<li>for: 这个论文是为了研究一种基于实际风险最小化的深度神经网络估计器，并使用l_1正则化。</li>
<li>methods: 论文使用了一种通用的误差 bound，来研究这种估计器的过剩风险。同时，它也证明了这种估计器在不同的函数集中是可适应的，并且可以在某些情况下达到可适应的最小风险。</li>
<li>results: 论文的研究表明，这种估计器在回归和分类任务中具有可适应的最小风险性，并且在不同的函数集中具有相似的性能。<details>
<summary>Abstract</summary>
We consider a deep neural network estimator based on empirical risk minimization with l_1-regularization. We derive a general bound for its excess risk in regression and classification (including multiclass), and prove that it is adaptively nearly-minimax (up to log-factors) simultaneously across the entire range of various function classes.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种深度神经网络估计器，基于empirical risk minimization，并添加了L1正则化。我们得出了一个通用的副作用误差上限，并证明了它在不同的函数类中同时具有适应性nearly-minimax性（即log-factors）。Here's the breakdown of the translation:* "deep neural network" is 深度神经网络 (shēn dào xīn nǐo wǎng)* "estimator" is 估计器 (gè jì qì)* "based on empirical risk minimization" is 基于empirical risk minimization (jī yǔ empirical risk minimization)* "with L1 regularization" is 并添加L1正则化 (dàn tiêm L1 zhèng yǎ)* "we derive a general bound" is 我们得出了一个通用的副作用误差上限 (wǒ men dé chū yī yī zhòng yòu zhèng yǎ)* "for its excess risk" is 对其副作用误差 (duì qí fù zuò yì zhèng yǎ)* "in regression and classification" is 包括回归和分类 (bāo xīn huí qù yǔ fāng lèi)* "including multiclass" is 包括多类 (bāo xīn duō lèi)* "and prove that it is adaptively nearly-minimax" is 并证明它同时在不同的函数类中具有适应性nearly-minimax性（即log-factors） (dàn shì yī yī zhòng yòu zhèng yǎ)Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I'll be happy to provide it.
</details></li>
</ul>
<hr>
<h2 id="Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model"><a href="#Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model" class="headerlink" title="Environment-independent mmWave Fall Detection with Interacting Multiple Model"></a>Environment-independent mmWave Fall Detection with Interacting Multiple Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08755">http://arxiv.org/abs/2311.08755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyao Yu, Jiazhao Wang, Wenchao Jiang</li>
<li>for: 这个研究旨在开发一个可靠、抗错误的无接触、无合作的跌倒探测系统，以便在智能家居中实现长者日常照顾。</li>
<li>methods: 本研究使用mmWave激光技术，并采用一个叫做互动多模型（IMM）状态估计器，从环境无关的特征提取出高精度、实时跌倒探测。此外，我们也提出了一个可靠的多用户追踪系统，以应对环境噪音和其他人体影响。</li>
<li>results: 实验结果显示，FADE系统在实际应用中可以实现跌倒探测的精度高达95%。<details>
<summary>Abstract</summary>
The ageing society brings attention to daily elderly care through sensing technologies. The future smart home is expected to enable in-home daily monitoring, such as fall detection, for seniors in a non-invasive, non-cooperative, and non-contact manner. The mmWave radar is a promising candidate technology for its privacy-preserving and non-contact manner. However, existing solutions suffer from low accuracy and robustness due to environment dependent features. In this paper, we present FADE (\underline{FA}ll \underline{DE}tection), a practical fall detection radar system with enhanced accuracy and robustness in real-world scenarios. The key enabler underlying FADE is an interacting multiple model (IMM) state estimator that can extract environment-independent features for highly accurate and instantaneous fall detection. Furthermore, we proposed a robust multiple-user tracking system to deal with noises from the environment and other human bodies. We deployed our algorithm on low computing power and low power consumption system-on-chip (SoC) composed of data front end, DSP, and ARM processor, and tested its performance in real-world. The experiment shows that the accuracy of fall detection is up to 95\%.
</details>
<details>
<summary>摘要</summary>
社会老龄化引起每日老人照顾的注意。未来智能家庭预计会实现在家中每日监测，如坠落检测，为年轻人在不侵略、不合作、无接触的情况下提供。 millimeter 雷达是一种有前途的技术，因为它具有隐私保护和无接触的特点。然而，现有解决方案受到环境依赖的特征的准确性和可靠性问题。在本文中，我们提出了FADE（坠落检测），一个实用的坠落检测雷达系统，具有提高了准确性和可靠性的实际应用能力。FADE的关键启发是一种交互式多模型（IMM）状态估计器，可以提取环境无关的特征，以实现高精度和实时坠落检测。此外，我们还提出了一种可靠的多用户跟踪系统，用于处理环境和其他人体的噪音。我们将算法部署到低计算力和低能耗系统在板（SoC）上，包括数据前端、DSP和ARM处理器，并在实际应用中进行测试。实验结果显示，坠落检测精度可达95%。
</details></li>
</ul>
<hr>
<h2 id="Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling"><a href="#Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling" class="headerlink" title="Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling"></a>Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08745">http://arxiv.org/abs/2311.08745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Sato, Hideaki Iiduka</li>
<li>for: 这个论文是为了解释graduated optimization方法在非 convex函数中的 globally optimal solution 的找到和分析。</li>
<li>methods: 这篇论文使用了一种新的非 convex函数家族，并对这些函数的充分条件进行了讨论。它还提供了一种 graduated optimization 算法的收敛分析。</li>
<li>results: 这篇论文显示了 stochastic gradient descent (SGD)  WITH mini-batch stochastic gradients 可以使函数变得更加平滑，这种效果取决于学习率和 batch size。这个发现为 graduated optimization 的视角提供了理论性的解释，例如大批处理大小会落入锋利的本地极小值，以及 decaying learning rate 和增大批处理大小是有优势的。此外，这篇论文还提出了一种新的 graduated optimization 框架，并通过实验来支持我们的理论发现。<details>
<summary>Abstract</summary>
The graduated optimization approach is a heuristic method for finding globally optimal solutions for nonconvex functions and has been theoretically analyzed in several studies. This paper defines a new family of nonconvex functions for graduated optimization, discusses their sufficient conditions, and provides a convergence analysis of the graduated optimization algorithm for them. It shows that stochastic gradient descent (SGD) with mini-batch stochastic gradients has the effect of smoothing the function, the degree of which is determined by the learning rate and batch size. This finding provides theoretical insights from a graduated optimization perspective on why large batch sizes fall into sharp local minima, why decaying learning rates and increasing batch sizes are superior to fixed learning rates and batch sizes, and what the optimal learning rate scheduling is. To the best of our knowledge, this is the first paper to provide a theoretical explanation for these aspects. Moreover, a new graduated optimization framework that uses a decaying learning rate and increasing batch size is analyzed and experimental results of image classification that support our theoretical findings are reported.
</details>
<details>
<summary>摘要</summary>
“graduated optimization方法是一种启发法求解非对称函数的全球最优解决方案，在多个研究中得到了理论分析。本文定义了一个新的非对称函数家族，讨论了它们的必要条件，并对 graduate optimization算法的收敛分析。结果显示，使用渐进学习率和增加批处理大小可以使函数平滑化，这种效果取决于学习率和批处理大小。这一发现为 graduated optimization视角下解释了大批处理大小落入锐锐的局部最优点、渐进学习率和增加批处理大小的优势，以及最优学习率调整的问题。据我们所知，这是首个提供了这些方面的理论解释。此外，我们还提出了一种使用渐进学习率和增加批处理大小的新 graduated optimization框架，并在图像分类任务上进行了实验研究，支持我们的理论发现。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering"><a href="#Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering" class="headerlink" title="Towards Graph-Aware Diffusion Modeling for Collaborative Filtering"></a>Towards Graph-Aware Diffusion Modeling for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08744">http://arxiv.org/abs/2311.08744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqin Zhu, Chao Wang, Hui Xiong</li>
<li>for: 这篇论文是为了推荐系统中的隐藏偏好还原问题而写的。</li>
<li>methods: 该论文使用了神经网络模型来解决这个问题，特别是使用了Diffusion模型来实现。</li>
<li>results: 该论文的实验结果表明，Compared to现有方法，该模型在一个数据集上表现出了大量的提高，并在其他数据集上表现得也很竞争力强。<details>
<summary>Abstract</summary>
Recovering masked feedback with neural models is a popular paradigm in recommender systems. Seeing the success of diffusion models in solving ill-posed inverse problems, we introduce a conditional diffusion framework for collaborative filtering that iteratively reconstructs a user's hidden preferences guided by its historical interactions. To better align with the intrinsic characteristics of implicit feedback data, we implement forward diffusion by applying synthetic smoothing filters to interaction signals on an item-item graph. The resulting reverse diffusion can be interpreted as a personalized process that gradually refines preference scores. Through graph Fourier transform, we equivalently characterize this model as an anisotropic Gaussian diffusion in the graph spectral domain, establishing both forward and reverse formulations. Our model outperforms state-of-the-art methods by a large margin on one dataset and yields competitive results on the others.
</details>
<details>
<summary>摘要</summary>
“复原对应式反馈”是现代推荐系统中广泛使用的一种方法。发现传播模型在解决过滤问题方面的成功，我们引入一个基于协同游戏的条件传播框架，iteratively重建用户的隐藏偏好 guid by its历史互动。为了更好地适应隐藏反馈数据的自然特性，我们实现前向传播 by applying synthetic smoothing filters to interaction signals on an item-item graph。 resulting reverse diffusion can be interpreted as a personalized process that gradually refines preference scores。通过几何transform， we equivalently characterize this model as an anisotropic Gaussian diffusion in the graph spectral domain, establishing both forward and reverse formulations。我们的模型在一个数据集上比州-of-the-art方法大幅超越，并在其他数据集上实现竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model"><a href="#Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model" class="headerlink" title="Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model"></a>Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08690">http://arxiv.org/abs/2311.08690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanlin Qi, Jia Li, Michael Zhang<br>for: 这种研究的目的是提供一种可靠且可行的方法来估算安全措施 modify 因素（CMF），以评估不同安全措施的效果并决定相应的基础设施投资。methods: 这种研究使用了一种新的知识挖掘框架来预测 CMF 值。这个框架 Draws inspiration from human comprehension processes 和 introduces advanced Natural Language Processing (NLP) techniques to extract intricate variations and patterns from existing CMF knowledge.results: 实验 validate 表明，这种新方法可以减少 CMF 估算的成本和时间，同时提供更好的准确性。这种方法可以补充传统的案例特定的方法，特别是当缺乏灾难数据或时间限制时。<details>
<summary>Abstract</summary>
Precise estimation of Crash Modification Factors (CMFs) is central to evaluating the effectiveness of various road safety treatments and prioritizing infrastructure investment accordingly. While customized study for each countermeasure scenario is desired, the conventional CMF estimation approaches rely heavily on the availability of crash data at given sites. This not only makes the estimation costly, but the results are also less transferable, since the intrinsic similarities between different safety countermeasure scenarios are not fully explored. Aiming to fill this gap, this study introduces a novel knowledge-mining framework for CMF prediction. This framework delves into the connections of existing countermeasures and reduces the reliance of CMF estimation on crash data availability and manual data collection. Specifically, it draws inspiration from human comprehension processes and introduces advanced Natural Language Processing (NLP) techniques to extract intricate variations and patterns from existing CMF knowledge. It effectively encodes unstructured countermeasure scenarios into machine-readable representations and models the complex relationships between scenarios and CMF values. This new data-driven framework provides a cost-effective and adaptable solution that complements the case-specific approaches for CMF estimation, which is particularly beneficial when availability of crash data or time imposes constraints. Experimental validation using real-world CMF Clearinghouse data demonstrates the effectiveness of this new approach, which shows significant accuracy improvements compared to baseline methods. This approach provides insights into new possibilities of harnessing accumulated transportation knowledge in various applications.
</details>
<details>
<summary>摘要</summary>
evaluating 安全防护措施的效iveness 需要精准的评估飞行改变因素（CMF）。 conventional 方法取决于具体的countermeasure scenario 的可用性，这不仅使得评估成本高昂，而且结果还 less transferable，因为不完全考虑不同安全防护措施enario 之间的内在相似性。 为了填补这一空白，本研究提出了一种新的知识挖掘框架，用于预测CMF。 该框架 Draws inspiration from human comprehension processes 和 advanced Natural Language Processing（NLP）技术，以EXTRACT intricate variations and patterns 从现有CMF知识中。 它可以将countermeasure scenarios 转化为可读取的机器表示，并模型了countermeasure scenarios 和 CMF值之间的复杂关系。 这种新的数据驱动的框架可以补做case-specific 方法的限制，特别是当缺乏事故数据或时间约束时。 实验 validate 使用实际的CMF Clearinghouse数据，demonstrates 该新方法的有效性，与基准方法相比显示了显著的准确性提升。 这种新方法提供了新的应用 possiblities ，挖掘了交通运输领域的储存知识。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Sparse-Principal-Component-Analysis"><a href="#Federated-Learning-for-Sparse-Principal-Component-Analysis" class="headerlink" title="Federated Learning for Sparse Principal Component Analysis"></a>Federated Learning for Sparse Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08677">http://arxiv.org/abs/2311.08677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sin Cheng Ciou, Pin Jui Chen, Elvin Y. Tseng, Yuh-Jye Lee</li>
<li>For: The paper is written for the rapidly evolving realm of machine learning, specifically addressing the challenge of algorithm effectiveness facing limitations due to data quality and availability.* Methods: The paper uses the federated learning framework, which is a decentralized approach where model training occurs on client sides, preserving privacy by keeping data localized. The paper also applies the Sparse Principal Component Analysis (SPCA) method, which aims to attain sparse component loadings while maximizing data variance for improved interpretability. Additionally, the paper introduces a least squares approximation to original SPCA, which enables analytic solutions on the optimization processes and leads to substantial computational improvements.* Results: The paper’s extensive experiments involve both IID and non-IID random features across various data owners, and results on synthetic and public datasets affirm the efficacy of the federated SPCA approach.<details>
<summary>Abstract</summary>
In the rapidly evolving realm of machine learning, algorithm effectiveness often faces limitations due to data quality and availability. Traditional approaches grapple with data sharing due to legal and privacy concerns. The federated learning framework addresses this challenge. Federated learning is a decentralized approach where model training occurs on client sides, preserving privacy by keeping data localized. Instead of sending raw data to a central server, only model updates are exchanged, enhancing data security. We apply this framework to Sparse Principal Component Analysis (SPCA) in this work. SPCA aims to attain sparse component loadings while maximizing data variance for improved interpretability. Beside the L1 norm regularization term in conventional SPCA, we add a smoothing function to facilitate gradient-based optimization methods. Moreover, in order to improve computational efficiency, we introduce a least squares approximation to original SPCA. This enables analytic solutions on the optimization processes, leading to substantial computational improvements. Within the federated framework, we formulate SPCA as a consensus optimization problem, which can be solved using the Alternating Direction Method of Multipliers (ADMM). Our extensive experiments involve both IID and non-IID random features across various data owners. Results on synthetic and public datasets affirm the efficacy of our federated SPCA approach.
</details>
<details>
<summary>摘要</summary>
在机器学习领域中，算法效果经常受到数据质量和可用性的限制。传统方法面临数据分享的挑战，由于法律和隐私问题。基于联邦学习框架的我们方法解决这个挑战。联邦学习是一种分布式的方法，在客户端上进行模型训练，保持隐私性，不发送原始数据到中央服务器。而是只发送模型更新，提高数据安全性。在这种框架下，我们应用SPCA（稀疏主成分分析）。SPCA的目标是获得稀疏的组件加载，同时最大化数据变化，以提高解释性。在传统的SPCA中，我们采用L1规范化项来进行正则化，同时添加滑动函数，以便使用梯度下降优化方法。此外，为了提高计算效率，我们引入了原始SPCA的最小二乘近似。这使得优化过程中的数学解可以得到 analytic 解，导致计算上的重要提高。在联邦框架中，我们将SPCA表示为一个consensus优化问题，可以使用alternating Direction Method of Multipliers（ADMM）解决。我们的广泛的实验包括了独立和非独立的随机特征 Across various data owners。结果表明我们的联邦SPCA方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Coreset-Selection-with-Prioritized-Multiple-Objectives"><a href="#Coreset-Selection-with-Prioritized-Multiple-Objectives" class="headerlink" title="Coreset Selection with Prioritized Multiple Objectives"></a>Coreset Selection with Prioritized Multiple Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08675">http://arxiv.org/abs/2311.08675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, Tongliang Liu</li>
<li>for: 降低深度学习算法的计算成本和加速大规模数据处理</li>
<li>methods: 提出了一种新的多目标优先级 coreset 选择方法，以实现最小化 coreset 大小的同时保证模型性能</li>
<li>results: 理论上提供了优先级优化过程的 converges  garantue，并通过广泛的实验证明了该方法的超越性，可以更好地适应实际场景<details>
<summary>Abstract</summary>
Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. When coreset selection is applied in realistic scenes, under the premise that the identified coreset has achieved comparable model performance, practitioners regularly desire the identified coreset can have a size as small as possible for lower costs and greater acceleration. Motivated by this desideratum, for the first time, we pose the problem of "coreset selection with prioritized multiple objectives", in which the smallest coreset size under model performance constraints is explored. Moreover, to address this problem, an innovative method is proposed, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically, extensive experiments confirm its superiority compared with previous strategies, often yielding better model performance with smaller coreset sizes.
</details>
<details>
<summary>摘要</summary>
核心集选择是深度学习算法中具有强大的计算成本减少和数据处理加速能力的技术。它目标在大规模数据中标识一小subset，以便只需在subset上进行训练，实际上可以与全部数据进行相同的表现。在实际场景中，当 coreset 被应用时，实际Operator们经常希望可以通过最小化 coreset 大小来降低成本和加速计算。motivated by这个需求，我们首次提出了“coreset 选择 WITH 多目标优先级”的问题，在这个问题中，我们寻找最小化 coreset 大小的方法，同时保证模型性能。为了解决这个问题，我们提出了一种创新的方法，它可以在 coreset 选择过程中维护优先级顺序，同时兼顾模型性能和 coreset 大小。我们也提供了这种方法的理论上的收敛保证。实际实验表明，我们的方法在许多情况下比之前的策略更有优势，经常可以在更小的 coreset 大小下实现更好的模型性能。
</details></li>
</ul>
<hr>
<h2 id="Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data"><a href="#Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data" class="headerlink" title="Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data"></a>Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08636">http://arxiv.org/abs/2311.08636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keunsu Kim, Hanbaek Lyu, Jinsu Kim, Jae-Hun Jung</li>
<li>for: 这篇论文是用于预测空间时间资料的新方法论。</li>
<li>methods: 这篇论文使用了监督式半非正数式因子分解（SSNMF），并将频域变数增加到预测模型中，以提高时间特征的清晰度。</li>
<li>results: 在应用这篇论文的方法到 GRACE 资料上，发现结果与预测领域的前期研究相似，但提供更明确的解释。<details>
<summary>Abstract</summary>
We propose a novel methodology for forecasting spatio-temporal data using supervised semi-nonnegative matrix factorization (SSNMF) with frequency regularization. Matrix factorization is employed to decompose spatio-temporal data into spatial and temporal components. To improve clarity in the temporal patterns, we introduce a nonnegativity constraint on the time domain along with regularization in the frequency domain. Specifically, regularization in the frequency domain involves selecting features in the frequency space, making an interpretation in the frequency domain more convenient. We propose two methods in the frequency domain: soft and hard regularizations, and provide convergence guarantees to first-order stationary points of the corresponding constrained optimization problem. While our primary motivation stems from geophysical data analysis based on GRACE (Gravity Recovery and Climate Experiment) data, our methodology has the potential for wider application. Consequently, when applying our methodology to GRACE data, we find that the results with the proposed methodology are comparable to previous research in the field of geophysical sciences but offer clearer interpretability.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法ology，用于预测空间-时间数据，基于经过监督的半正semi-nonnegative矩阵因子（SSNMF），并在频率频谱中进行常数化。矩阵因子被用来分解空间-时间数据为空间和时间成分。为了提高时间特征的明暗性，我们在时间频谱中加入了非负约束，同时在频率频谱中进行常数化。我们提出了两种在频率频谱中的方法：软和硬的常数化，并提供了首次稳定点的准确性保证。我们的主要动机来自地球物理数据分析，基于GRACE（重力回升和气候实验）数据，但我们的方法可以更广泛地应用。当应用于GRACE数据时，我们发现，与先前在地球物理科学领域的研究相比，我们的方法可以提供更明了的解释。
</details></li>
</ul>
<hr>
<h2 id="Non-Uniform-Smoothness-for-Gradient-Descent"><a href="#Non-Uniform-Smoothness-for-Gradient-Descent" class="headerlink" title="Non-Uniform Smoothness for Gradient Descent"></a>Non-Uniform Smoothness for Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08615">http://arxiv.org/abs/2311.08615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lindonroberts/nonuniform-smoothness">https://github.com/lindonroberts/nonuniform-smoothness</a></li>
<li>paper_authors: Albert S. Berahas, Lindon Roberts, Fred Roosta</li>
<li>for: 这个论文的目的是提出一种基于本地首项稳定 oracle（LFSO）的修改后的梯度下降方法，以提高非强 convex 问题的 globally linear convergence rate。</li>
<li>methods: 这个论文使用了一种基于 LFSO 的修改后的梯度下降方法，该方法可以在任何两次导数函数上实现本地首项稳定性。</li>
<li>results: 论文显示了这种修改后的梯度下降方法可以在非强 convex 问题中实现全球线性减少率，并且可以超越通常的 (加速) 首项方法的下界。<details>
<summary>Abstract</summary>
The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods.
</details>
<details>
<summary>摘要</summary>
通常来说，梯度下降类方法的分析假设对目标函数的梯度 lipschitz连续性。这通常需要一个昂贵的hyperparameter调整过程，以适应给定问题。在这种工作中，我们引入了本地首项积分 oracle（LFSO），这个oracle泛化了 lipschitz连续梯度的稳定性条件，并适用于任何两次导数函数。我们表明，这个oracle可以包含所有相关的问题信息，用于调整梯度下降方法中的步长，并给出全球和本地收敛结果。此外，我们还证明，LFSOs在这种修改后的首项方法中可以实现全球线性收敛率，而且对非强转化问题的极小值处存在极其平坦的情况下，可以超越通用（加速）首项方法的下界。
</details></li>
</ul>
<hr>
<h2 id="Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption"><a href="#Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption" class="headerlink" title="Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption"></a>Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08610">http://arxiv.org/abs/2311.08610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itamar Zimerman, Moran Baruch, Nir Drucker, Gilad Ezov, Omri Soceanu, Lior Wolf</li>
<li>for: This paper is written for the purpose of exploring the application of Homomorphic Encryption (HE) in transformer models, and demonstrating the feasibility of secure inference over HE with transformers.</li>
<li>methods: The paper introduces a novel method for converting operators in transformer models to their polynomial equivalent, enabling the use of HE in transformer models. This method involves tailoring the transformer architecture for HE and developing a novel conversion technique for operators.</li>
<li>results: The paper reports that the proposed polynomial transformer model achieves results comparable to traditional methods on several benchmark datasets, including LMs with WikiText-103, image classification with CIFAR-100 and Tiny-ImageNet. The results demonstrate the viability of HE for state-of-the-art applications and bridge the performance gap with transformers of similar scale.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文是为了探索使用Homomorphic Encryption（HE）在 transformer 模型中进行安全推理，并实现 HE 上的 transformer 模型的可行性。</li>
<li>methods: 论文提出了一种新的方法，即将 transformer 模型中的运算符转换为其波动函数等价项。这种方法包括对 transformer 模型进行特定的设计和开发，以及一种新的转换技术。</li>
<li>results: 论文报告了一些比较示例，表明提出的波动 transformer 模型可以与传统方法相比，在多个 benchmark 数据集上实现类似的性能。这些结果证明 HE 可以在 state-of-the-art 应用中实现，并将 transformer 模型的性能减小到类似的规模。<details>
<summary>Abstract</summary>
Designing privacy-preserving deep learning models is a major challenge within the deep learning community. Homomorphic Encryption (HE) has emerged as one of the most promising approaches in this realm, enabling the decoupling of knowledge between the model owner and the data owner. Despite extensive research and application of this technology, primarily in convolutional neural networks, incorporating HE into transformer models has been challenging because of the difficulties in converting these models into a polynomial form. We break new ground by introducing the first polynomial transformer, providing the first demonstration of secure inference over HE with transformers. This includes a transformer architecture tailored for HE, alongside a novel method for converting operators to their polynomial equivalent. This innovation enables us to perform secure inference on LMs with WikiText-103. It also allows us to perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield results comparable to traditional methods, bridging the performance gap with transformers of similar scale and underscoring the viability of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component.
</details>
<details>
<summary>摘要</summary>
deep learning community 中的一个主要挑战是设计保持隐私的深度学习模型。归一化加密（HE）已经成为这一领域的一个非常有前途的方法，允许知识的解耦 между模型所有者和数据所有者。尽管有广泛的研究和应用这技术，主要在卷积神经网络中，但是将这些模型转化为多项式形式的困难使得HE在转换器模型中进行安全的推理变得困难。我们在这篇论文中突破了一个新的多项式转换器，提供了首次在HE下进行安全的推理的证明，包括一种适应HE的转换器架构，以及一种将操作转换为多项式形式的新方法。这一创新使得我们可以在LMs上进行安全的推理，并在CIFAR-100和Tiny-ImageNet上进行图像分类。我们的模型实现了与传统方法相当的结果，填补了转换器的性能差距，并证明了HE在现代应用中的可行性。最后，我们评估了我们的模型的稳定性，并进行了一系列的减少来评估每个模型组件的贡献。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.LG_2023_11_15/" data-id="clp53jwte00uayp887hhp6erm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/eess.IV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T09:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/eess.IV_2023_11_15/">eess.IV - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parallel-Quantum-Hough-Transform"><a href="#Parallel-Quantum-Hough-Transform" class="headerlink" title="Parallel Quantum Hough Transform"></a>Parallel Quantum Hough Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09002">http://arxiv.org/abs/2311.09002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Klefenz, Nico Wittrock, Frank Feldhoff</li>
<li>for: The paper is written for proposing and implementing a Parallel Quantum Hough transform (PQHT) algorithm that can be executed on a quantum computer.</li>
<li>methods: The PQHT algorithm is implemented using a set of connected programmable $\texttt{RZ}$ rotation gates with adjustable node connections of coincidence detectors realized with quantum logic gates. The modules were developed using IBM Quantum Composer and tested using the IBM QASM simulator.</li>
<li>results: The paper presents the successful run results of the PQHT algorithm on Fraunhofer Q System One in Ehningen as a proof of concept.<details>
<summary>Abstract</summary>
Few of the known quantum algorithms can be reliably executed on a quantum computer. Therefore, as an extension, we propose a Parallel Quantum Hough transform (PQHT) algorithm that we execute on a quantum computer. We give its implementation and discuss the results obtained. The PQHT algorithm is conceptually divided into a parallel rotation stage consisting of a set of connected programmable $\texttt{RZ}$ rotation gates, with adjustable node connections of coincidence detectors realized with quantum logic gates. The modules were developed using IBM Quantum Composer and tested using the IBM QASM simulator. Finally, the modules were programmed using the Python package Qiskit and the jobs were sent to distributed IBM Q System One quantum computers. The successful run results on Fraunhofer Q System One in Ehningen will be presented as a proof of concept for the PQHT algorithm.
</details>
<details>
<summary>摘要</summary>
只有一些知名的量子算法可以可靠地在量子计算机上执行。因此，我们提出了一种并行量子抽象变换（PQHT）算法，我们在量子计算机上执行。我们给出了其实现和讨论结果。PQHT算法从概念上来分为一个并行旋转阶段，该阶段包括一组相连的可程序化$\texttt{RZ}$旋转门，其中节点连接的偶极化探测器使用量子逻辑门实现。模块使用IBM量子 composer开发，并使用IBM QASM simulator测试。最后，模块使用Python包Qiskit编程，并将任务发送到分布式IBM Q System One量子计算机。成功运行结果在 Fraunhofer Q System One 上将被示为PQHT算法的证明。
</details></li>
</ul>
<hr>
<h2 id="Ultrafast-3-D-Super-Resolution-Ultrasound-using-Row-Column-Array-specific-Coherence-based-Beamforming-and-Rolling-Acoustic-Sub-aperture-Processing-In-Vitro-In-Vivo-and-Clinical-Study"><a href="#Ultrafast-3-D-Super-Resolution-Ultrasound-using-Row-Column-Array-specific-Coherence-based-Beamforming-and-Rolling-Acoustic-Sub-aperture-Processing-In-Vitro-In-Vivo-and-Clinical-Study" class="headerlink" title="Ultrafast 3-D Super Resolution Ultrasound using Row-Column Array specific Coherence-based Beamforming and Rolling Acoustic Sub-aperture Processing: In Vitro, In Vivo and Clinical Study"></a>Ultrafast 3-D Super Resolution Ultrasound using Row-Column Array specific Coherence-based Beamforming and Rolling Acoustic Sub-aperture Processing: In Vitro, In Vivo and Clinical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08823">http://arxiv.org/abs/2311.08823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Hansen-Shearer, Jipeng Yan, Marcelo Lerendegui, Biao Huang, Matthieu Toulemonde, Kai Riemer, Qingyuan Tan, Johanna Tonko, Peter D. Weinberg, Chris Dunsby, Meng-Xing Tang</li>
<li>for: 这个研究是为了提高ROW-COLUMN addressed array的ultrasound imaging质量，特别是在验证组织中。</li>
<li>methods: 这个研究使用了ROW-COLUMN addressed array-specific coherence-based beamforming技术和Acoustic sub-aperture processing来减少“次要”lobes噪音和增加有效框率。</li>
<li>results: 这个研究发现，使用这种方法可以将“false”位置的比例由$\sim$26%降至$\sim$15%，同时降低噪音水平 $\sim$7 dB和增加有效框率至超过4000 fps。此外，这个方法也成功地实现了非侵入式的人类脉络和兔只肾脏影像生成。<details>
<summary>Abstract</summary>
The row-column addressed array is an emerging probe for ultrafast 3-D ultrasound imaging. It achieves this with far fewer independent electronic channels and a wider field of view than traditional 2-D matrix arrays, of the same channel count, making it a good candidate for clinical translation. However, the image quality of row-column arrays is generally poor, particularly when investigating tissue. Ultrasound localisation microscopy allows for the production of super-resolution images even when the initial image resolution is not high. Unfortunately, the row-column probe can suffer from imaging artefacts that can degrade the quality of super-resolution images as `secondary' lobes from bright microbubbles can be mistaken as microbubble events, particularly when operated using plane wave imaging. These false events move through the image in a physiologically realistic way so can be challenging to remove via tracking, leading to the production of 'false vessels'. Here, a new type of rolling window image reconstruction procedure was developed, which integrated a row-column array-specific coherence-based beamforming technique with acoustic sub-aperture processing for the purposes of reducing `secondary' lobe artefacts, noise and increasing the effective frame rate. Using an {\it{in vitro} cross tube, it was found that the procedure reduced the percentage of `false' locations from $\sim$26\% to $\sim$15\% compared to traditional orthogonal plane wave compounding. Additionally, it was found that the noise could be reduced by $\sim$7 dB and that the effective frame rate could be increased to over 4000 fps. Subsequently, {\it{in vivo} ultrasound localisation microscopy was used to produce images non-invasively of a rabbit kidney and a human thyroid.
</details>
<details>
<summary>摘要</summary>
ROW-COLUMN addressed 阵列是一种emerging probe дляultrafast 3D ultrasound imaging。它通过使用较少的独立电子通道和更广泛的视野，与传统的2D矩阵阵列相比，具有更好的临床翻译potential。然而，ROW-COLUMN 阵列的影像质量通常较差，特别是在调查组织时。ultrasound localization microscopy可以生成超解像图像，即使初始影像质量不高。然而，ROW-COLUMN 阵列可能会受到干扰类型的类别artefacts，这些干扰可以导致 False Positives。在这里，我们开发了一种新的rolling window图像重建程序，它结合了ROW-COLUMN 阵列特有的coherence-based beamforming技术和acoustic sub-aperture处理。这些方法可以对ROW-COLUMN 阵列中的secondary lobeartefacts、噪音和干扰进行处理，并提高有效框率。使用一个{\it{in vitro} Cross tube，我们发现这种程序可以将False Positives的百分比由大约26%降至大约15%，并且可以降低噪音水平大约7 dB，并提高有效框率到超过4000 fps。最后，我们使用了非侵入式的ultrasound localization microscopy，将影像生成non-invasively的 rabbit kidney和人类thyroid。
</details></li>
</ul>
<hr>
<h2 id="Degradation-Estimation-Recurrent-Neural-Network-with-Local-and-Non-Local-Priors-for-Compressive-Spectral-Imaging"><a href="#Degradation-Estimation-Recurrent-Neural-Network-with-Local-and-Non-Local-Priors-for-Compressive-Spectral-Imaging" class="headerlink" title="Degradation Estimation Recurrent Neural Network with Local and Non-Local Priors for Compressive Spectral Imaging"></a>Degradation Estimation Recurrent Neural Network with Local and Non-Local Priors for Compressive Spectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08808">http://arxiv.org/abs/2311.08808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Dong, Dahua Gao, Yuyan Li, Guangming Shi, Danhua Liu</li>
<li>for: 本文提出了一种基于深度 unfolding network（DUN）的高spectral像（HSI）重建方法，以解决coded aperture snapshot spectral imaging（CASSI）系统中的3D HSI重建问题。</li>
<li>methods: 本文首先将DUN转换成Recurrent Neural Network（RNN），以便在不同阶段之间共享参数，从而使得DNN在每个阶段可以学习不同阶段的特征表示。其次，本文引入了噪声估计网络（DERNN），该网络同时估计了噪声水平和噪声矩阵，通过剩余学习方法与感知矩阵进行参考。最后，本文提出了一种Local and Non-Local Transformer（LNLT），用于有效地利用local和non-local Priors在HSIs中。</li>
<li>results: 本文的DERNN-LNLT方法在多个测试 datasets 上实现了state-of-the-art的性能。<details>
<summary>Abstract</summary>
In coded aperture snapshot spectral imaging (CASSI) systems, a core problem is to recover the 3D hyperspectral image (HSI) from the 2D measurement. Current deep unfolding networks (DUNs) for the HSI reconstruction mainly suffered from three issues. Firstly, in previous DUNs, the DNNs across different stages were unable to share the feature representations learned from different stages, leading to parameter sparsity, which in turn limited their reconstruction potential. Secondly, previous DUNs fail to estimate degradation-related parameters within a unified framework, including the degradation matrix in the data subproblem and the noise level in the prior subproblem. Consequently, either the accuracy of solving the data or the prior subproblem is compromised. Thirdly, exploiting both local and non-local priors for the HSI reconstruction is crucial, and it remains a key issue to be addressed. In this paper, we first transform the DUN into a Recurrent Neural Network (RNN) by sharing parameters across stages, which allows the DNN in each stage could learn feature representation from different stages, enhancing the representativeness of the DUN. Secondly, we incorporate the Degradation Estimation Network into the RNN (DERNN), which simultaneously estimates the degradation matrix and the noise level by residual learning with reference to the sensing matrix. Thirdly, we propose a Local and Non-Local Transformer (LNLT) to effectively exploit both local and non-local priors in HSIs. By integrating the LNLT into the DERNN for solving the prior subproblem, we propose the DERNN-LNLT, which achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
在coded aperture snapshot spectral imaging（CASSI）系统中，核心问题是从2D测量 recuperate 3D彩色спектраль像（HSI）。现有的深度 unfolding network（DUN） для HSI重建主要受到以下三个问题的影响：首先，在前一代DUN中，不同阶段的神经网络（DNN）无法共享不同阶段学习的特征表示，导致参数稀缺，从而限制了重建的潜力。其次，先前的DUN无法在一个框架下 estimate 质量问题和优先问题中的损失参数，包括数据问题中的损失矩阵和优先问题中的噪声水平。这会导致解决数据问题或优先问题的精度受到限制。另外，在HSI重建中，利用本地和非本地的假设是关键，但是这是一个需要解决的关键问题。在这篇论文中，我们首先将DUN转换成Recurrent Neural Network（RNN），使得不同阶段的DNN可以共享特征表示，从而提高DUN的表达能力。其次，我们在RNN中引入损失估计网络（DERNN），同时估计数据问题中的损失矩阵和优先问题中的噪声水平，通过剩余学习和参考感知矩阵来实现。最后，我们提议一种本地和非本地转换器（LNLT），以有效地利用HSI中的本地和非本地假设。通过将LNLTintegrated into DERNN来解决优先问题，我们提出了DERNN-LNLT，其实现了状态的前一代性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/eess.IV_2023_11_15/" data-id="clp53jx0901bryp880aum8jma" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/eess.SP_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T08:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/eess.SP_2023_11_15/">eess.SP - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-AmBC-Systems-with-Deep-Learning-for-Joint-Channel-Estimation-and-Signal-Detection"><a href="#Enhancing-AmBC-Systems-with-Deep-Learning-for-Joint-Channel-Estimation-and-Signal-Detection" class="headerlink" title="Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation and Signal Detection"></a>Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation and Signal Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09172">http://arxiv.org/abs/2311.09172</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Zargari, A. Hakimi, C. Tellambura, A. Maaref</li>
<li>For: The paper proposes an innovative approach for channel state estimation (CSI) and signal detection within ambient backscatter (AmBC) systems, to address the challenge of inadequate knowledge of channel and RF-source parameters.* Methods: The proposed approach uses a deep neural network (DNN) to implicitly estimate CSI and simultaneously detect data, leveraging offline training with simulated data derived from channel statistics.* Results: The proposed DNN method outperforms traditional detectors, particularly in terms of bit error rate (BER), with an approximately 20% improvement in BER performance compared to the maximum likelihood (ML) approach in high signal-to-noise ratio (SNR) conditions.<details>
<summary>Abstract</summary>
The era of ubiquitous, affordable wireless connectivity has opened doors to countless practical applications. In this context, ambient backscatter communication (AmBC) stands out, utilizing passive tags to establish connections with readers by harnessing reflected ambient radio frequency (RF) signals. However, conventional data detectors face limitations due to their inadequate knowledge of channel and RF-source parameters. To address this challenge, we propose an innovative approach using a deep neural network (DNN) for channel state estimation (CSI) and signal detection within AmBC systems. Unlike traditional methods that separate CSI estimation and data detection, our approach leverages a DNN to implicitly estimate CSI and simultaneously detect data. The DNN model, trained offline using simulated data derived from channel statistics, excels in online data recovery, ensuring robust performance in practical scenarios. Comprehensive evaluations validate the superiority of our proposed DNN method over traditional detectors, particularly in terms of bit error rate (BER). In high signal-to-noise ratio (SNR) conditions, our method exhibits an impressive approximately 20% improvement in BER performance compared to the maximum likelihood (ML) approach. These results underscore the effectiveness of our developed approach for AmBC channel estimation and signal detection. In summary, our method outperforms traditional detectors, bolstering the reliability and efficiency of AmBC systems, even in challenging channel conditions.
</details>
<details>
<summary>摘要</summary>
当现代无线通信技术成为可靠、可Affordable的时， Ambient Backscatter Communication（AmBC）技术得到了广泛应用。在这种情况下，我们提出了一种创新的方法，使用深度神经网络（DNN）来估计AmBC通道状态和信号检测。不同于传统方法，我们的方法不分离估计通道状态和数据检测，而是使用DNN来同时估计通道状态和检测数据。我们的模型在线上数据恢复中表现出色，在实际应用中具有robust性。我们对传统检测器进行了比较，得出的结论是，我们的DNN方法在高信号噪声比（SNR）条件下表现出较好的20%的提高，相比最大likelihood（ML）方法。这些结果证明了我们提出的方法在AmBC通道估计和信号检测方面的有效性。总之，我们的方法在实际应用中可以提高AmBC系统的可靠性和效率，尤其是在复杂的通道条件下。
</details></li>
</ul>
<hr>
<h2 id="Network-Level-Integrated-Sensing-and-Communication-Interference-Management-and-BS-Coordination-Using-Stochastic-Geometry"><a href="#Network-Level-Integrated-Sensing-and-Communication-Interference-Management-and-BS-Coordination-Using-Stochastic-Geometry" class="headerlink" title="Network-Level Integrated Sensing and Communication: Interference Management and BS Coordination Using Stochastic Geometry"></a>Network-Level Integrated Sensing and Communication: Interference Management and BS Coordination Using Stochastic Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09052">http://arxiv.org/abs/2311.09052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaitao Meng, Christos Masouros, Guangji Chen, Fan Liu</li>
<li>for: 本文研究了集成感知通信（ISAC）网络，以提高感知通信（S&amp;C）性能的平衡。</li>
<li>methods: 本文使用 Stochastic Geometry 工具来捕捉 S&amp;C 性能，并且通过解出频率效率（ASE）的 tractable 表达来形式化优化问题。</li>
<li>results: 研究结果表明，协同BS集群大小的选择可以提高平均数据率和雷达信息率。而在优化communication 性能时，无需Nulling 干扰可以使用全部空间资源进行多样化和多普逻讯。在感知目标时，资源分配倾向于消除特定干扰，特别是当antenna资源充足时。此外，研究还证明了在优化communication 性能时，用户数和发射天线数的比值是一定的常量值。实验结果表明，提出的协同ISAC 方案可以在网络水平提高 S&amp;C 性能。<details>
<summary>Abstract</summary>
In this work, we study integrated sensing and communication (ISAC) networks with the aim of effectively balancing sensing and communication (S&C) performance at the network level. Focusing on monostatic sensing, the tool of stochastic geometry is exploited to capture the S&C performance, which facilitates us to illuminate key cooperative dependencies in the ISAC network and optimize key network-level parameters. Based on the derived tractable expression of area spectral efficiency (ASE), we formulate the optimization problem to maximize the network performance from the view point of two joint S&C metrics. Towards this end, we further jointly optimize the cooperative BS cluster sizes for S&C and the serving/probing numbers of users/targets to achieve a flexible tradeoff between S&C at the network level. It is verified that interference nulling can effectively improve the average data rate and radar information rate. Surprisingly, the optimal communication tradeoff for the case of the ASE maximization tends to employ all spacial resources towards multiplexing and diversity gain, without interference nulling. By contrast, for the sensing objectives, resource allocation tends to eliminate certain interference especially when the antenna resources are sufficient, because the inter-cell interference becomes a more dominant factor affecting sensing performance. Furthermore, we prove that the ratio of the optimal number of users and the number of transmit antennas is a constant value when the communication performance is optimal. Simulation results demonstrate that the proposed cooperative ISAC scheme achieves a substantial gain in S&C performance at the network level.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究集成感知和通信（ISAC）网络，以便在网络层面协调感知和通信（S&C）性能。我们将焦点放在单站感知上，使用杂相几何工具来捕捉S&C性能，从而照明了ISAC网络中协同依赖关系的关键。基于 derivated的面 спектр效率（ASE）表达式，我们将优化问题转化为最大化网络性能的两个联合S&C指标。在这个过程中，我们进一步联合S&C和用户/目标服务/探测数量的各种参数，以实现对S&C性能的灵活变化。我们发现，干扰抑制可以有效提高平均数据速率和雷达信息速率。即使在没有干扰抑制的情况下，也可以通过各种参数的调整来提高S&C性能。此外，我们证明了在优化通信性能时，用户数量和发射天线数量之间的比值是常数值。在实验中，我们发现，提出的协同ISAC方案可以在网络层面实现显著的S&C性能提升。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Sensing-Communication-and-Power-Transfer-Multiuser-Beamforming-Design"><a href="#Integrating-Sensing-Communication-and-Power-Transfer-Multiuser-Beamforming-Design" class="headerlink" title="Integrating Sensing, Communication, and Power Transfer: Multiuser Beamforming Design"></a>Integrating Sensing, Communication, and Power Transfer: Multiuser Beamforming Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09028">http://arxiv.org/abs/2311.09028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqin Zhou, Xiaoyang Li, Guangxu Zhu, Jie Xu, Kaibin Huang, Shuguang Cui</li>
<li>for: 本文旨在探讨 sixth-generation (6G) 网络中大量低功率设备的环境感知和数据传输问题，以提高无线资源利用率。</li>
<li>methods: 本文提出了一种 combining 感知和通信（ISAC）技术和同时无线信息和能量传输（SWIPT）技术的高级技术，称为 integrate 感知、通信、能量传输（ISCPT）技术。</li>
<li>results: 本文分析了一种多用户多输入多输出（MIMO）ISCPT 系统，其中基站装备多个天线发送消息到多个信息接收器（IR），同时传输能量到多个能量接收器（ER），并同时感知目标。 Results show that the proposed designs can improve the sensing performance while meeting the communication and power transfer requirements, and the target positioning problem can be further investigated.<details>
<summary>Abstract</summary>
In the sixth-generation (6G) networks, massive low-power devices are expected to sense environment and deliver tremendous data. To enhance the radio resource efficiency, the integrated sensing and communication (ISAC) technique exploits the sensing and communication functionalities of signals, while the simultaneous wireless information and power transfer (SWIPT) techniques utilizes the same signals as the carriers for both information and power delivery. The further combination of ISAC and SWIPT leads to the advanced technology namely integrated sensing, communication, and power transfer (ISCPT). In this paper, a multi-user multiple-input multiple-output (MIMO) ISCPT system is considered, where a base station equipped with multiple antennas transmits messages to multiple information receivers (IRs), transfers power to multiple energy receivers (ERs), and senses a target simultaneously. The sensing target can be regarded as a point or an extended surface. When the locations of IRs and ERs are separated, the MIMO beamforming designs are optimized to improve the sensing performance while meeting the communication and power transfer requirements. The resultant non-convex optimization problems are solved based on a series of techniques including Schur complement transformation and rank reduction. Moreover, when the IRs and ERs are co-located, the power splitting factors are jointly optimized together with the beamformers to balance the performance of communication and power transfer. To better understand the performance of ISCPT, the target positioning problem is further investigated. Simulations are conducted to verify the effectiveness of our proposed designs, which also reveal a performance tradeoff among sensing, communication, and power transfer.
</details>
<details>
<summary>摘要</summary>
在第六代（6G）网络中，大量低功率设备预期将环境感知和传输巨量数据。为提高无线资源效率，整合感知通信（ISAC）技术利用信号感知和通信功能，同时利用同一个信号作为信息和能量传输的同步无线信息和能量传输（SWIPT）技术。将ISAC和SWIPT两技术结合，可以达到更高级别的技术，即整合感知通信和能量传输（ISCPT）。在这篇论文中，我们考虑了一个多用户多输入多出力（MIMO）ISCPT系统，其中一个基站装备了多个天线，向多个信息接收器（IR）发送消息，对多个能量接收器（ER）进行能量传输，并同时感知一个目标。目标可以是点或扩展表面。当IR和ER的位置分开时，我们使用MIMO扩展 beamforming 设计优化感知性能，同时满足通信和能量传输的要求。得到的非对称优化问题是使用Schur complement transformation和矩阵减少技术解决。此外，当IR和ER均位于同一个位置时，我们均Optimize power splitting factors和扩展 beamformers，以平衡通信和能量传输的性能。为更好地理解ISCPT的性能，我们进一步 investigate target positioning problem。我们通过实验验证我们的提议设计的有效性，同时发现了传输、通信和能量传输之间的性能负担。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-mmWave-MIMO-using-sub-6-GHz-Out-of-Band-Information"><a href="#Channel-Estimation-for-mmWave-MIMO-using-sub-6-GHz-Out-of-Band-Information" class="headerlink" title="Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band Information"></a>Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08996">http://arxiv.org/abs/2311.08996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faruk Pasic, Markus Hofer, Mariam Mussbah, Sebastian Caban, Stefan Schwarz, Thomas Zemen, Christoph F. Mecklenbräuker</li>
<li>for: 提高 millimeter wave（mmWave）通信系统中MIMO通信链接的可靠性和spectral efficiency</li>
<li>methods: 利用sub-6GHz频率带的out-of-band信息估算mmWave MIMO通信频率</li>
<li>results: 比 convential方法高的可靠性和spectral efficiency，特别是在低SNR和高K-factor情况下Translation:</li>
<li>for: To improve the reliability and spectral efficiency of millimeter wave (mmWave) communication systems in MIMO communication links</li>
<li>methods: Using out-of-band information from the sub-6GHz frequency band to estimate mmWave MIMO communication channels</li>
<li>results: Higher reliability and spectral efficiency than conventional methods, especially in low SNR and high K-factor conditions<details>
<summary>Abstract</summary>
Future wireless multiple-input multiple-output (MIMO) communication systems will employ sub-6 GHz and millimeter wave (mmWave) frequency bands working cooperatively. Establishing a MIMO communication link usually relies on estimating channel state information (CSI) which is difficult to acquire at mmWave frequencies due to a low signal-to-noise ratio (SNR). In this paper, we propose three novel methods to estimate mmWave MIMO channels using out-of-band information obtained from the sub-6GHz band. We compare the proposed channel estimation methods with a conventional one utilizing only in-band information. Simulation results show that the proposed methods outperform the conventional mmWave channel estimation method in terms of achievable spectral efficiency, especially at low SNR and high K-factor.
</details>
<details>
<summary>摘要</summary>
未来的无线多输入多输出（MIMO）通信系统将使用低于6GHz和毫米波（mmWave）频段合作工作。建立MIMO通信链接通常需要估计通道状态信息（CSI），但mmWave频段的低信号噪声比（SNR）使得这种估计变得更加困难。在这篇论文中，我们提出了三种新的方法来估计mmWave MIMO通道，使用从低于6GHz频段获得的外带信息。我们与传统的使用仅带内信息的方法进行比较，并通过实验结果显示了我们的方法在低SNR和高K因子下的高 spectral efficiency 性能。
</details></li>
</ul>
<hr>
<h2 id="EMF-Aware-Power-Control-for-Massive-MIMO-Cell-Free-versus-Cellular-Networks"><a href="#EMF-Aware-Power-Control-for-Massive-MIMO-Cell-Free-versus-Cellular-Networks" class="headerlink" title="EMF-Aware Power Control for Massive MIMO: Cell-Free versus Cellular Networks"></a>EMF-Aware Power Control for Massive MIMO: Cell-Free versus Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08989">http://arxiv.org/abs/2311.08989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergi Liesegang, Stefano Buzzi</li>
<li>for: 这篇论文旨在解决用户中心的单元幕 nil 多输入多输出系统（CF-mMIMO）下的电磁干扰约束问题，以提高系统的公平性和安全性。</li>
<li>methods: 该论文使用了最大化最小数据速率的功率分配策略，并对比了其他参考策略。</li>
<li>results:  simulation 结果显示，CF-mMIMO 系统可以遵循电磁干扰约束，同时不产生数据速率的下降，并且提高了系统的公平性和安全性。<details>
<summary>Abstract</summary>
The impressive growth of wireless data networks has recently led to increased attention to the issue of electromagnetic pollution. Specific absorption rates and incident power densities have become popular indicators for measuring electromagnetic field (EMF) exposure. This paper tackles the problem of power control in user-centric cell-free massive multiple-input-multiple-output (CF-mMIMO) systems under EMF constraints. Specifically, the power allocation maximizing the minimum data rate across users is derived for both the uplink and the downlink under EMF constraints. The developed solution is also applied to a cellular mMIMO system and compared to other benchmark strategies. Simulation results prove that EMF safety restrictions can be easily met without jeopardizing the minimum data rate, that the CF-mMIMO outperforms the multi-cell massive MIMO deployment, and that the proposed power control strategy greatly improves the system fairness.
</details>
<details>
<summary>摘要</summary>
“无线数据网络的印象性增长已经引起了电磁污染问题的更多注意。特别吸收率和入射功率密度已成为衡量电磁场（EMF）曝露的受测指标。本文解决了基于EMF限制的CF-mMIMO系统中的功率控制问题。具体来说，我们对于用户中心的CF-mMIMO系统进行了功率分配，以最大化用户间的最低数据率，并在EMF限制下进行了实现。我们还将此解决方案应用到了红点网络中，并与其他参考策略进行比较。实验结果显示，EMF安全限制可以轻松遵守，无需对最低数据率造成影响，CF-mMIMO系统比多组幕网络部署更好，并且提案的功率控制策略可以增加系统公平性。”
</details></li>
</ul>
<hr>
<h2 id="Throughput-Maximisation-in-Ultra-wideband-Hybrid-amplified-Links"><a href="#Throughput-Maximisation-in-Ultra-wideband-Hybrid-amplified-Links" class="headerlink" title="Throughput Maximisation in Ultra-wideband Hybrid-amplified Links"></a>Throughput Maximisation in Ultra-wideband Hybrid-amplified Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08964">http://arxiv.org/abs/2311.08964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrique Buglia, Eric Sillekens, Lidia Galdino, Robert Killey, Polina Bayvel1</li>
<li>for: 这篇论文旨在提出一种半分析式、实时非线性干扰模型，包括ASE雷达噪声，以优化hybrid化增强链路的容量。</li>
<li>methods: 该论文使用了合并粒子群优化算法，将hybrid化增强链路的容量最大化，提高了12%的吞吐量，比只使用EDFAs的配置更高。</li>
<li>results: 该论文通过实验和分析表明， hybrid化增强链路的容量可以通过合并粒子群优化算法进行最大化，提高了12%的吞吐量。<details>
<summary>Abstract</summary>
A semi-analytical, real-time nonlinear-interference model including ASE noise in hybrid-amplified links is introduced. Combined with particle-swarm optimisation, the capacity of a hybrid-amplified 10.5 THz 117x57 km link was maximised, increasing throughput by 12% versus an EDFAs-only configuration.
</details>
<details>
<summary>摘要</summary>
“一种半分析式、实时非线性干扰模型，包括ASE噪声，在混合增强链路中引入。与粒子群聚 optimize 结合，hybrid-amplified 10.5 THz 117x57 km 链路的容量得到最大化，比EDFAs-only 配置提高了12%的吞吐量。”Here's a breakdown of the translation:* “一种” (yī xiāng) - "a kind of"* 半分析式 (bàn fēn'ān'shì) - "semi-analytical"* 实时 (shíshí) - "real-time"* 非线性干扰 (fēi xiàn xìng jì) - "nonlinear interference"* 包括 (bāo gù) - "including"* ASE 噪声 (ASE zhōngshēng) - "ASE noise"* 在 (zài) - "in"* 混合增强链路 (hùyù zēngcháng liànlù) - "hybrid-amplified link"* 中 (zhōng) - "in"* 引入 (yǐn rù) - "introduced"* 与 (yǔ) - "with"* 粒子群聚 optimize (liù zǐ qún jí optimize) - "particle-swarm optimization"* 结合 (jiégòu) - "combined"* hybrid-amplified 10.5 THz 117x57 km 链路 (hùyù zēngcháng 10.5 THz 117x57 km liànlù) - "hybrid-amplified 10.5 THz 117x57 km link"* 容量 (róngliàng) - "capacity"* 得到 (de hé) - "obtain"* 最大化 (zuìdào jiè) - "maximized"* 比 (bǐ) - "vs."* EDFAs-only 配置 (EDFAs-only qiǎngjì) - "EDFAs-only configuration"* 提高 (tīgāng) - "increased"* 12% (12%) - "12%"I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Design-and-Implementation-of-a-Hybrid-Wireless-Power-and-Communication-System-for-Medical-Implants"><a href="#Design-and-Implementation-of-a-Hybrid-Wireless-Power-and-Communication-System-for-Medical-Implants" class="headerlink" title="Design and Implementation of a Hybrid Wireless Power and Communication System for Medical Implants"></a>Design and Implementation of a Hybrid Wireless Power and Communication System for Medical Implants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08933">http://arxiv.org/abs/2311.08933</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Khaleghi, A. Hasanvand, I. Balasingham</li>
<li>for: 这篇论文旨在开发一种无线嵌入式设备，用于预防许多慢性疾病。</li>
<li>methods: 该论文使用了人体内部无线电力供应、感测和通信技术，并应用了人工智能（AI）技术进行数据分析。</li>
<li>results: 论文表明，使用401MHz电频谱的无线电力供应可以高效地提供4cm深度内部组织的电力，同时使用相同的天线进行回护和 galvanic 通信。<details>
<summary>Abstract</summary>
Data collection and analysis from multiple implant nodes in humans can provide targeted medicine and treatment strategies that can prevent many chronic diseases. This data can be collected for a long time and processed using artificial intelligence (AI) techniques in a medical network for early detection and prevention of diseases. Additionally, machine learning (ML) algorithms can be applied for the analysis of big data for health monitoring of the population. Wireless powering, sensing, and communication are essential parts of future wireless implants that aim to achieve the aforementioned goals. In this paper, we present the technical development of a wireless implant that is powered by radio frequency (RF) at 401 MHz, with the sensor data being communicated to an on-body reader. The implant communication is based on two simultaneous wireless links: RF backscatter for implant-to-on-body communication and a galvanic link for intra-body implant-to-implant connectivity. It is demonstrated that RF powering, using the proposed compact antennas, can provide an efficient and integrable system for powering up to an 8 cm depth inside body tissues. Furthermore, the same antennas are utilized for backscatter and galvanic communication.
</details>
<details>
<summary>摘要</summary>
“数据采集和分析从多个人体节点可以提供targeted的药物和治疗策略，以预防许多慢性疾病。这些数据可以长期收集并使用人工智能（AI）技术进行分析，以早期探测和预防疾病。此外，机器学习（ML）算法可以用于对大量数据进行健康监测。未来无线植入器的关键组成部分包括无线电能、感测和通信。在这篇论文中，我们展示了无线植入器的技术开发，它由401MHz电磁谱干扰的电磁谱来供电，感测数据通过肤体上的读取器与外部通信。无线植入器的通信基于同时进行的两个无线链路：谱干扰回报和galvanic链路。我们示出了使用我们提议的封包天线，RF供电可以提供高效和可集成的系统，可以深入到body tissues中的8cm深度。此外，同一组天线还用于回报和galvanic通信。”
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Design-of-Satellite-Terrestrial-Computing-in-6G-Wireless-Networks"><a href="#Energy-Efficient-Design-of-Satellite-Terrestrial-Computing-in-6G-Wireless-Networks" class="headerlink" title="Energy-Efficient Design of Satellite-Terrestrial Computing in 6G Wireless Networks"></a>Energy-Efficient Design of Satellite-Terrestrial Computing in 6G Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08904">http://arxiv.org/abs/2311.08904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Xiaoming Chen, Qiao Qi</li>
<li>for: The paper is written for researchers and practitioners working on 6G wireless networks and satellite-terrestrial computing. It aims to provide a complete process for satellite-terrestrial computing in 6G wireless networks and to minimize the weighted total energy consumption of computing tasks.</li>
<li>methods: The paper proposes a novel algorithm for satellite-terrestrial computing that jointly optimizes offloading selection, beamforming design, and resource allocation to minimize energy consumption and meet delay requirements. The algorithm is designed based on the characteristics of 6G wireless networks and is evaluated through theoretical analysis and simulations.</li>
<li>results: The proposed algorithm is found to have fast convergence and superior performance in terms of energy efficiency and computing task completion time, as confirmed by both theoretical analysis and simulation results. The paper provides a valuable contribution to the field of satellite-terrestrial computing in 6G wireless networks and has important implications for future research and development in this area.<details>
<summary>Abstract</summary>
In this paper, we investigate the issue of satellite-terrestrial computing in the sixth generation (6G) wireless networks, where multiple terrestrial base stations (BSs) and low earth orbit (LEO) satellites collaboratively provide edge computing services to ground user equipments (GUEs) and space user equipments (SUEs) over the world. In particular, we design a complete process of satellite-terrestrial computing in terms of communication and computing according to the characteristics of 6G wireless networks. In order to minimize the weighted total energy consumption while ensuring delay requirements of computing tasks, an energy-efficient satellite-terrestrial computing algorithm is put forward by jointly optimizing offloading selection, beamforming design and resource allocation. Finally, both theoretical analysis and simulation results confirm fast convergence and superior performance of the proposed algorithm for satellite-terrestrial computing in 6G wireless networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了 sixth generation（6G）无线网络中的卫星地面计算问题，其中多个地面基站（BS）和低地球轨道卫星（LEO）合作提供边缘计算服务给地面用户设备（GUE）和空间用户设备（SUE）。特别是，我们设计了6G无线网络中卫星地面计算的完整过程，包括通信和计算方面。为了最小化加权总能 consumption，我们提出了一种能效的卫星地面计算算法，通过协调卸载选择、扫描设计和资源分配来实现。最后，我们的理论分析和 simulations 结果表明，提议的算法在6G无线网络中具有快速收敛和优秀性能。
</details></li>
</ul>
<hr>
<h2 id="RIS-Position-and-Orientation-Estimation-via-Multi-Carrier-Transmissions-and-Multiple-Receivers"><a href="#RIS-Position-and-Orientation-Estimation-via-Multi-Carrier-Transmissions-and-Multiple-Receivers" class="headerlink" title="RIS Position and Orientation Estimation via Multi-Carrier Transmissions and Multiple Receivers"></a>RIS Position and Orientation Estimation via Multi-Carrier Transmissions and Multiple Receivers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08887">http://arxiv.org/abs/2311.08887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Ghazalian, Hui Chen, George C. Alexandropoulos, Gonzalo Seco-Granados, Henk Wymeersch, Riku Jäntti</li>
<li>for: 本文研究了一种基于可重新配置智能表面（RIS）的 sixth generation无线系统，以实现用户的准确位置和orientation estimation。</li>
<li>methods: 本文提出了一种多stage estimator，使用时间射频和空间频率测量，并 deriv了Cramér-Rao下界来验证估计器的性能。</li>
<li>results:  simulations Results表明，提出的RIS状态估计方法在不同系统操作参数下具有高效性。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surfaces (RISs) are considered as an enabling technology for the upcoming sixth generation of wireless systems, exhibiting significant potential for radio localization and sensing. An RIS is usually treated as an anchor point with known position and orientation when deployed to offer user localization. However, it can also be attached to a user to enable its localization in a semi-passive manner. In this paper, we consider a static user equipped with an RIS and study the RIS localization problem (i.e., joint three-dimensional position and orientation estimation), when operating in a system comprising a single-antenna transmitter and multiple synchronized single-antenna receivers with known locations. We present a multi-stage estimator using time-of-arrival and spatial frequency measurements, and derive the Cram\'er-Rao lower bounds for the estimated parameters to validate the estimator's performance. Our simulation results demonstrate the efficiency of the proposed RIS state estimation approach under various system operation parameters.
</details>
<details>
<summary>摘要</summary>
《可重新配置智能表面（RIS）是第六代无线系统的潜在关键技术，具有显著的射频地位和感知潜力。一个RIS通常被视为已知位置和方向的anchor point，但它也可以附加到用户来实现半活动式的用户地位确定。本文考虑一名静止用户搭载了RIS，并研究RIS的位置和方向估计问题（即三个维度的位置和方向估计），当系统包括一个单antenna发射器和多个同步的单antenna接收器，接收器的位置已知。我们提出了多stage估计器，使用时间射频和空间频率测量，并 derivated Cramér-Rao下界来验证估计器的性能。我们的 simulate结果表明，我们的RIS状态估计方法在不同的系统运行参数下具有高效性。》Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Aerial-IRS-with-Robotic-Anchoring-Capabilities-A-Novel-Way-for-Adaptive-Coverage-Enhancement"><a href="#Aerial-IRS-with-Robotic-Anchoring-Capabilities-A-Novel-Way-for-Adaptive-Coverage-Enhancement" class="headerlink" title="Aerial IRS with Robotic Anchoring Capabilities: A Novel Way for Adaptive Coverage Enhancement"></a>Aerial IRS with Robotic Anchoring Capabilities: A Novel Way for Adaptive Coverage Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08876">http://arxiv.org/abs/2311.08876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Wu, Vasilis Friderikos</li>
<li>for: 提高无线网络覆盖率和用户服务质量 (improving wireless network coverage and user quality of service)</li>
<li>methods: 使用机器人飞行智能反射表 (RA-IRS)，具有附着机制，以减少能源消耗和提高服务时间 (using robotic aerial intelligent reflecting surfaces with anchoring mechanisms to reduce energy consumption and improve service time)</li>
<li>results: 提高网络性能，适应复杂城市地形，可以支持多个小时或者多个天 (improve network performance, adapt to complex urban terrain, and support multiple hours or days)<details>
<summary>Abstract</summary>
It is widely accepted that integrating intelligent reflecting surfaces (IRSs) with unmanned aerial vehicles (UAV) or drones can assist wireless networks in improving network coverage and end user Quality of Service (QoS). However, the critical constrain of drones is their very limited hovering/flying time. In this paper we propose the concept of robotic aerial IRSs (RA-IRSs), which are in essence drones that in addition to IRS embed an anchoring mechanism that allows them to grasp in an energy neutral manner at tall urban landforms such as lampposts. By doing so, RA-IRSs can completely eliminate the flying/hovering energy consumption and can offer service for multiple hours or even days (something not possible with UAV-mounted IRSs). Using that property we show how RA-IRS can increase network performance by changing their anchoring location to follow the spatio-temporal traffic demand. The proposed methodology, developed through Integer Linear Programming (ILP) formulations offers a significant Signal-to-Noise (SNR) gain in highly heterogeneous regions in terms of traffic demand compared to fixed IRS; hence, addressing urban coverage discrepancies effectively. Numerical simulations validate the superiority of RA-IRSs over fixed terrestrial IRSs in terms of traffic serviceability, sustaining more than 2 times the traffic demand in areas experiencing high heterogeneity, emphasizing their adaptability in improving coverage and QoS in complex urban terrains.
</details>
<details>
<summary>摘要</summary>
通常认为将智能反射表(IRS)与无人机(UAV)或无人飞行器(drone)结合可以帮助无线网络提高覆盖率和用户服务质量(QoS)。然而，无人机的缺点是其很有限的悬停/飞行时间。在这篇论文中，我们提出了机器人空中智能反射表(RA-IRS)的概念，它们是基于无人机的IRS，同时具有附着机制，可以在高层城市物体上 such as 灯柱上进行能量中性的捕捉。这样，RA-IRS可以完全消除飞行/悬停的能源消耗，并且可以为多个小时或者数天提供服务（不可能由UAV-IRS所实现）。使用这个特性，我们显示了如何RA-IRS可以根据它们附着位置的变化，以遵循空间-时间峰值交通需求，从而提高网络性能。我们通过整数线性编程(ILP)的方法开发的方法ологи，可以在高度不均的区域中提供显著的噪声比(SNR)增加，比 fixed IRS 更高，因此有效地解决城市覆盖不均问题。数字实验证明 RA-IRS 在高度不均的区域中比 fixes IRS 更具有抗风险性和可靠性，可以支持多达 2 倍的交通需求，这说明 RA-IRS 在复杂的城市地形中的适应能力强。
</details></li>
</ul>
<hr>
<h2 id="Phase-retrieval-with-semi-algebraic-and-ReLU-neural-network-priors"><a href="#Phase-retrieval-with-semi-algebraic-and-ReLU-neural-network-priors" class="headerlink" title="Phase retrieval with semi-algebraic and ReLU neural network priors"></a>Phase retrieval with semi-algebraic and ReLU neural network priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08833">http://arxiv.org/abs/2311.08833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamir Bendory, Nadav Dym, Dan Edidin, Arun Suresh</li>
<li>for: 解决phas Retrieval问题</li>
<li>methods: 使用 semi-algebraic set 的 priors</li>
<li>results: 可以从 Fourier magnitudes 中重建 almost all signals, up to a sign, if they lie in a (generic) semi-algebraic set of dimension N&#x2F;2, 并且可以 recovery all signals if the semi-algebraic set is of dimension N&#x2F;4.<details>
<summary>Abstract</summary>
The key ingredient to retrieving a signal from its Fourier magnitudes, namely, to solve the phase retrieval problem, is an effective prior on the sought signal. In this paper, we study the phase retrieval problem under the prior that the signal lies in a semi-algebraic set. This is a very general prior as semi-algebraic sets include linear models, sparse models, and ReLU neural network generative models. The latter is the main motivation of this paper, due to the remarkable success of deep generative models in a variety of imaging tasks, including phase retrieval. We prove that almost all signals in R^N can be determined from their Fourier magnitudes, up to a sign, if they lie in a (generic) semi-algebraic set of dimension N/2. The same is true for all signals if the semi-algebraic set is of dimension N/4. We also generalize these results to the problem of signal recovery from the second moment in multi-reference alignment models with multiplicity free representations of compact groups. This general result is then used to derive improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies, each acted upon by a random element of SO(3).
</details>
<details>
<summary>摘要</summary>
键件元素 Retrieving a signal from its Fourier magnitudes，即解决频率重建问题，是一个有效的先前。在这篇论文中，我们研究了频率重建问题，假设信号 lie in a semi-algebraic set。这是一个非常通用的先前，因为 semi-algebraic sets包括线性模型，稀疏模型，和 ReLU neural network生成模型。后者是本文的主要动机，因为深度生成模型在各种成像任务中具有惊人的成功。我们证明，如果信号 lie in a (普通) semi-algebraic set of dimension N/2，那么可以从其福散谱中解决信号，到了一个标识符。同时，如果 semi-algebraic set 的维度为 N/4，那么所有信号都可以由其福散谱中解决。我们还推广这些结果到多 reference alignment models with multiplicity free representations of compact groups。这个通用结果然后用来 deriv improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies，each acted upon by a random element of SO(3).
</details></li>
</ul>
<hr>
<h2 id="Wireless-Communications-in-Cavity-A-Reconfigurable-Boundary-Modulation-based-Approach"><a href="#Wireless-Communications-in-Cavity-A-Reconfigurable-Boundary-Modulation-based-Approach" class="headerlink" title="Wireless Communications in Cavity: A Reconfigurable Boundary Modulation based Approach"></a>Wireless Communications in Cavity: A Reconfigurable Boundary Modulation based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08810">http://arxiv.org/abs/2311.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuehui Dong, Xiang Ren, Bokai Lai, Rujing Xiong, Tiebin Mi, Robert Caiming Qiu</li>
<li>for: 这篇论文探讨了嵌入智能表面（RIS）在干扰波媒体环境中的无线通信应用前景。相比于空间中，我们利用围墙附近电磁场的敏感性和等效干扰的RIS。</li>
<li>methods: 我们首次提出了可重新配置边界模式的框架，在壁洞中应用了一种可靠的边界模式修改方案，通过RIS生成的等效脉冲实现了脉冲位置模ulation（PPM） для无线通信。</li>
<li>results: 我们在原型中实现了约2Mbps的比特率，并示出了强有力抗频率选择性的性能，BIT错误率非常低。<details>
<summary>Abstract</summary>
This paper explores the potential wireless communication applications of Reconfigurable Intelligent Surfaces (RIS) in reverberant wave propagation environments. Unlike in free space, we utilize the sensitivity to boundaries of the enclosed electromagnetic (EM) field and the equivalent perturbation of RISs. For the first time, we introduce the framework of reconfigurable boundary modulation in the cavities . We have proposed a robust boundary modulation scheme that exploits the continuity of object motion and the mutation of the codebook switch, which achieves pulse position modulation (PPM) by RIS-generated equivalent pulses for wireless communication in cavities. This approach achieves around 2 Mbps bit rate in the prototype and demonstrates strong resistance to channel's frequency selectivity resulting in an extremely low bit error rate (BER).
</details>
<details>
<summary>摘要</summary>
(Note: The above text is in Simplified Chinese, as requested.)
</details></li>
</ul>
<hr>
<h2 id="Channel-Capacity-and-Bounds-In-Mixed-Gaussian-Impulsive-Noise"><a href="#Channel-Capacity-and-Bounds-In-Mixed-Gaussian-Impulsive-Noise" class="headerlink" title="Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise"></a>Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08804">http://arxiv.org/abs/2311.08804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianfu Qi, Jun Wang, Qihang Peng, Xiaoping Li, Xiaonan Chen</li>
<li>for: 这篇论文探讨了受混合噪音（包括非高斯噪音和白 Gaussian 噪音）影响的通信系统中的通道容量。</li>
<li>methods: 论文使用了 p-th  момент制约下的存在性和仅有限的质点分布，以及下限和Upper bound 的关注。</li>
<li>results: 论文显示了混合噪音下的通道容量下降，并且提供了具体的下限和Upper bound，其中下限可以对特定的模制进行变形。实验结果显示了这些结果的紧张性。<details>
<summary>Abstract</summary>
Communication systems suffer from the mixed noise consisting of both non-Gaussian impulsive noise (IN) and white Gaussian noise (WGN) in many practical applications. However, there is little literature about the channel capacity under mixed noise. In this paper, we prove the existence of the capacity under p-th moment constraint and show that there are only finite mass points in the capacity-achieving distribution. Moreover, we provide lower and upper capacity bounds with closed forms. It is shown that the lower bounds can degenerate to the well-known Shannon formula under special scenarios. In addition, the capacity for specific modulations and the corresponding lower bounds are discussed. Numerical results reveal that the capacity decreases when the impulsiveness of the mixed noise becomes dominant and the obtained capacity bounds are shown to be very tight.
</details>
<details>
<summary>摘要</summary>
听说系统在实际应用中常会遇到杂合噪（IN）和白噪（WGN）的杂合噪。然而，有少量文献关于杂合噪下的通信系统容量。在这篇论文中，我们证明了具有p-th moment约束下的容量存在，并显示了其分布中只有有限多个质量点。此外，我们还提供了下界和上界的容量bound，其中下界可以在特定情况下逐渐变为雪伦公式。此外，我们还讨论了特定模ulation的容量和相关的下界。numerical results表明，杂合噪的强度增加会导致容量降低，而我们所获得的容量 bound具有极高的紧密性。
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-DOA-Estimation-via-a-Novel-Tree-Model-based-Deep-Neural-Network"><a href="#High-Resolution-DOA-Estimation-via-a-Novel-Tree-Model-based-Deep-Neural-Network" class="headerlink" title="High-Resolution DOA Estimation via a Novel Tree Model-based Deep Neural Network"></a>High-Resolution DOA Estimation via a Novel Tree Model-based Deep Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08758">http://arxiv.org/abs/2311.08758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Li, Feng Shu, Yaoliang Song, Jiangzhou Wang<br>for: 这个论文是为了提高深度神经网络（DNNs）的方向估计性能而写的。methods: 这篇论文提出了一种基于树模型的深度神经网络（TDNN），它包含多个小规模的DNNs，每个层都会将angular region分解成更小的子区域，最终通过累加多个层的分类结果来获得DOA估计结果。results: 对于单源和多源情况，TDNN的估计性能都远胜传统方法，特别是在低信号响应比（SNR）下。此外，Q-TDNN方法也被提出，可以通过组合多个独立并平行的TDNN来获得Q个不同的DOA估计结果。<details>
<summary>Abstract</summary>
Traditional deep neural networks (DNNs) have bad performance on estimating off-grid angles, and the most direct solution is to increase the number of output classes for improving angular resolution. But more output classes can weaken the model accuracy of DNNs and thus decreasing the direction-of-arrival (DOA) estimation accuracy. In this work, a tree-model based deep neural networks (TDNN) is proposed, which contains H layers and each layer is consist of multiple small-scale DNNs. From the first layer to the last layer of TDNN, the angular region is gradually divided into smaller subregions by these DNNs, and the estimated DOA is finally obtained by cumulative calculating the classification results of all the layers. TDNN can improve the angular resolution by increasing the number of layers or the number of DNNs in any layer instead of changing the structure of single DNN, so the model accuracy of TDNN will not decrease with the improvement of angular resolution and its estimation performance is also stable. In addition, the Q-TDNN method is also proposed for multi-sources DOA estimation, which can obtain Q different DOAs from the same signals by combining Q independent and parallel TDNNs. The simulation results validate TDNN has much better estimation performance than traditional methods in both single-source and multi-sources cases, especially at low signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
传统的深度神经网络（DNN）在识别偏角方面表现不佳，而最直接的解决方案是增加输出类数以提高角度分辨率。但是增加输出类数可能会弱化DNN模型的准确率，因此降低方向来来（DOA）估计精度。在这项工作中，一种基于树模型的深度神经网络（TDNN）被提出，它包含H层，每层都有多个小规模的DNN。从TDNN的第一层到最后一层，每层的angular区域都是由这些DNN进行分解，并通过累加计算这些层的分类结果来获得最终的DOA估计结果。TDNN可以通过增加层数或增加每层DNN的数量来提高角度分辨率，而不会改变单个DNN的结构，因此TDNN的模型精度不会随着角度分辨率的提高而下降。此外，Q-TDNN方法也被提出来实现多源DOA估计，可以通过将Q个独立并平行的TDNN结合来获得Q个不同的DOA。实验结果表明，TDNN在单源和多源情况下都有远胜于传统方法的估计性能，特别是在低SNR情况下。
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Wideband-Secure-Communications-An-Analog-Beamfocusing-Approach"><a href="#Near-Field-Wideband-Secure-Communications-An-Analog-Beamfocusing-Approach" class="headerlink" title="Near-Field Wideband Secure Communications: An Analog Beamfocusing Approach"></a>Near-Field Wideband Secure Communications: An Analog Beamfocusing Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08738">http://arxiv.org/abs/2311.08738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Zhang, Haiyang Zhang, Wanbin Tang, Yonina C. Eldar</li>
<li>for: 本研究目的是增强近场宽频通信的物理层安全性 (PLS)，特别是在 millimeter-wave 和teraHertz频段的超高速宽频传输中。</li>
<li>methods: 我们引入了真实时间延迟 (TTD)  incorporated 分析波束技术，用于解决近场传播和宽频扫描之间的交互作用，这是现有文献中未曾探讨的领域。我们的方法包括对共同功率分配和分析波束设计进行优化问题的解决，使用交互式优化、分程程序和块Successive Upper-bound Minimization技术。</li>
<li>results: 我们的数值结果表明，提案的方法可以明显超越没有 TTD 的方法，在增强宽频 PLS 方面取得显著的优势，同时在使用低成本的分析设备时，实现了高效的机密能效。<details>
<summary>Abstract</summary>
In the rapidly advancing landscape of six-genration (6G), characterized by ultra-high-speed wideband transmission in millimeter-wave and terahertz bands, our paper addresses the pivotal task of enhancing physical layer security (PLS) within near-field wideband communications. We introduce true-time delayer (TTD)-incorporated analog beamfocusing techniques designed to address the interplay between near-field propagation and wideband beamsplit, an uncharted domain in existing literature. Our approach to maximizing secrecy rates involves formulating an optimization problem for joint power allocation and analog beamformer design, employing a two-stage process encompassing a semi-digital solution and analog approximation. This problem is efficiently solved through a combination of alternating optimization, fractional programming, and block successive upper-bound minimization techniques. Additionally, we present a low-complexity beamsplit-aware beamfocusing strategy, capitalizing on geometric insights from near-field wideband propagation, which can also serve as a robust initial value for the optimization-based approach. Numerical results substantiate the efficacy of the proposed methods, clearly demonstrating their superiority over TTD-free approaches in fortifying wideband PLS, as well as the advantageous secrecy energy efficiency achieved by leveraging low-cost analog devices.
</details>
<details>
<summary>摘要</summary>
在六代通信（6G）的急速发展中，我们的论文关注了 Physical Layer Security（PLS）在近场宽频通信中的提升任务。我们介绍了 incorporating true-time delayer（TTD）的Analog beamfocusing技术，用于解决近场宽频传输和宽频扫描的交互作用，这是现有文献中未曾探讨的领域。我们的方法包括对共同功率分配和Analog beamformer设计进行优化问题的形式化定义，使用 alternate optimization、分数编程和块顺序上升最小化技术进行有效地解决。此外，我们还提出了一种具有优化性的 beam-aware beamfocusing策略，基于近场宽频传输的几何特征，这也可以作为优化方法的可靠初值。数值结果证明了我们提出的方法的有效性，显示它们在增强宽频PLS方面超过了没有使用 TTD 的方法，并且通过利用低成本的Analog设备，实现了高效的机密能量效率。
</details></li>
</ul>
<hr>
<h2 id="Massive-Wireless-Energy-Transfer-without-Channel-State-Information-via-Imperfect-Intelligent-Reflecting-Surfaces"><a href="#Massive-Wireless-Energy-Transfer-without-Channel-State-Information-via-Imperfect-Intelligent-Reflecting-Surfaces" class="headerlink" title="Massive Wireless Energy Transfer without Channel State Information via Imperfect Intelligent Reflecting Surfaces"></a>Massive Wireless Energy Transfer without Channel State Information via Imperfect Intelligent Reflecting Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08720">http://arxiv.org/abs/2311.08720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Luo, Jie Hu, Luping Xiang, Kun Yang, Kai-Kit Wong</li>
<li>for: 提高无线能量传输效率和互联网物联网设备的可扩展性</li>
<li>methods: 使用低成本、通过反射元素实现的智能反射表面技术，并利用phasered beam rotation来提高受到能量的方向性和覆盖整个空间</li>
<li>results: 提高了无线能量传输效率，并且在大规模的能量接收器（ERs）场景下表现更好，不需要额外的培置或移除成本<details>
<summary>Abstract</summary>
Intelligent Reflecting Surface (IRS) utilizes low-cost, passive reflecting elements to enhance the passive beam gain, improve Wireless Energy Transfer (WET) efficiency, and enable its deployment for numerous Internet of Things (IoT) devices. However, the increasing number of IRS elements presents considerable channel estimation challenges. This is due to the lack of active Radio Frequency (RF) chains in an IRS, while pilot overhead becomes intolerable. To address this issue, we propose a Channel State Information (CSI)-free scheme that maximizes received energy in a specific direction and covers the entire space through phased beam rotation. Furthermore, we take into account the impact of an imperfect IRS and meticulously design the active precoder and IRS reflecting phase shift to mitigate its effects. Our proposed technique does not alter the existing IRS hardware architecture, allowing for easy implementation in the current system, and enabling access or removal of any Energy Receivers (ERs) without additional cost. Numerical results illustrate the efficacy of our CSI-free scheme in facilitating large-scale IRS without compromising performance due to excessive pilot overhead. Furthermore, our scheme outperforms the CSI-based counterpart in scenarios involving large-scale ERs, making it a promising solution in the era of IoT.
</details>
<details>
<summary>摘要</summary>
智能反射表面（IRS）利用低成本、 passtive 反射元件来提高无线能量传输（WET）效率和实现互联网器（IoT）设备的扩展。然而，随着 IRS 元件的增加，通道估计带来了很大的挑战。这是因为 IR 无功率扩展（RF）链的缺失，同时 Pilot 过头部分变得不允许。为解决这个问题，我们提议一种不需要 Channel State Information（CSI）的方案，可以最大化在特定方向上接收能量，并通过phasered beam rotation 覆盖整个空间。此外，我们考虑了 IRS 的不完美情况，并仔细设计了活动预编码器和反射相位调整，以mitigate 其影响。我们的提议方式不会改变现有 IRS 硬件架构，因此可以方便地实现在当前系统中，而且可以免除额外成本来添加或移除任何能量接收器（ER）。numerical 结果表明我们的 CSI-free 方案可以在大规模 IRS 中实现高效性，而不需要过多的 Pilot 过头部分。此外，我们的方案在大规模 ER 场景中表现更好，使其成为 IoT 时代的有力解决方案。
</details></li>
</ul>
<hr>
<h2 id="Low-Complexity-High-Speed-Deep-Neural-Network-Augmented-Wireless-Channel-Estimation"><a href="#Low-Complexity-High-Speed-Deep-Neural-Network-Augmented-Wireless-Channel-Estimation" class="headerlink" title="Low Complexity High Speed Deep Neural Network Augmented Wireless Channel Estimation"></a>Low Complexity High Speed Deep Neural Network Augmented Wireless Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08689">http://arxiv.org/abs/2311.08689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Asrar ul haq, Varun Singh, Bhanu Teja Tanaji, Sumit Darak<br>for:This paper aims to propose a novel low-complexity high-speed deep neural network-augmented least square (LC-LSDNN) algorithm for IEEE 802.11p wireless physical layer, which can efficiently estimate the channel in wireless receivers.methods:The proposed LC-LSDNN algorithm uses different deep neural networks (DNNs) for real and imaginary values of received complex symbols, which helps reduce the size of deep learning (DL) by 59% and optimize the critical path, allowing it to operate at 60% higher clock frequency.results:The proposed LC-LSDNN algorithm significantly outperforms minimum mean square error (MMSE) and state-of-the-art DL-based channel estimation (CE) for a wide range of signal-to-noise ratios (SNR) and different wireless channels, with around 50% lower resources than existing DL-based CE.<details>
<summary>Abstract</summary>
The channel estimation (CE) in wireless receivers is one of the most critical and computationally complex signal processing operations. Recently, various works have shown that the deep learning (DL) based CE outperforms conventional minimum mean square error (MMSE) based CE, and it is hardware-friendly. However, DL-based CE has higher complexity and latency than popularly used least square (LS) based CE. In this work, we propose a novel low complexity high-speed Deep Neural Network-Augmented Least Square (LC-LSDNN) algorithm for IEEE 802.11p wireless physical layer and efficiently implement it on Zynq system on chip (ZSoC). The novelty of the LC-LSDNN is to use different DNNs for real and imaginary values of received complex symbols. This helps reduce the size of DL by 59% and optimize the critical path, allowing it to operate at 60% higher clock frequency. We also explore three different architectures for MMSE-based CE. We show that LC-LSDNN significantly outperforms MMSE and state-of-the-art DL-based CE for a wide range of signal-to-noise ratios (SNR) and different wireless channels. Also, it is computationally efficient, with around 50% lower resources than existing DL-based CE.
</details>
<details>
<summary>摘要</summary>
频率接收器的频率估计（CE）是无线接收器中最重要且计算复杂的信号处理操作之一。近期，不同的研究表明，使用深度学习（DL）基于CE的方法可以超越传统的最小方差平均值（MMSE）基于CE，并且具有硬件友好性。然而，DL基于CE的复杂性和延迟高于常用的最小二乘（LS）基于CE。在这个工作中，我们提出了一种新的低复杂度高速的深度神经网络增强最小二乘（LC-LSDNN）算法，用于IEEE 802.11p无线物理层。该算法的新特点在于，对接收到的复杂数字符号中的实数和虚数使用不同的深度神经网络进行估计。这有助于降低DL的大小，并且优化执行路径，使其能以60%更高的时钟频率运行。我们还探讨了三种不同的MMSE基于CE的架构。我们发现，LC-LSDNN在各种信号噪声比（SNR）和不同的无线通道上都有显著性能优势，并且计算效率高于现有的DL基于CE。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/eess.SP_2023_11_15/" data-id="clp53jx2301fxyp882k2r7rwg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/14/cs.SD_2023_11_14/" class="article-date">
  <time datetime="2023-11-14T15:00:00.000Z" itemprop="datePublished">2023-11-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/14/cs.SD_2023_11_14/">cs.SD - 2023-11-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChoralSynth-Synthetic-Dataset-of-Choral-Singing"><a href="#ChoralSynth-Synthetic-Dataset-of-Choral-Singing" class="headerlink" title="ChoralSynth: Synthetic Dataset of Choral Singing"></a>ChoralSynth: Synthetic Dataset of Choral Singing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08350">http://arxiv.org/abs/2311.08350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jyoti Narang, Viviana De La Vega, Xavier Lizarraga, Oscar Mayor, Hector Parra, Jordi Janer, Xavier Serra</li>
<li>for: 本研究是为了提供高质量的choral singing数据集，以便进行Music Information Retrieval（MIR）研究。</li>
<li>methods: 本研究使用了现代音乐合成器，创造和 curae quality renditions。数据来源于Choral Public Domain Library（CPDL）。</li>
<li>results: 本研究提供了一个完整的数据集，包括相关的metadata，以及方法和技术。这些数据和方法将为Singing Voice研究开创新的 Avenues。<details>
<summary>Abstract</summary>
Choral singing, a widely practiced form of ensemble singing, lacks comprehensive datasets in the realm of Music Information Retrieval (MIR) research, due to challenges arising from the requirement to curate multitrack recordings. To address this, we devised a novel methodology, leveraging state-of-the-art synthesizers to create and curate quality renditions. The scores were sourced from Choral Public Domain Library(CPDL). This work is done in collaboration with a diverse team of musicians, software engineers and researchers. The resulting dataset, complete with its associated metadata, and methodology is released as part of this work, opening up new avenues for exploration and advancement in the field of singing voice research.
</details>
<details>
<summary>摘要</summary>
合唱歌唱，一种广泛实践的 ensemble 唱歌形式，在音乐信息检索（MIR）研究领域缺乏完整的数据集，因为需要合成多轨录音。为解决这个问题，我们提出了一种新的方法，利用当今最佳的 sintizer 创建和精心编辑高质量的演唱。歌谱来自choral Public Domain Library（CPDL）。这项工作和一群多元化的音乐家、软件工程师和研究人员合作完成，并随此工作发布了相关的数据集和方法。这些数据和方法对唱音研究领域开启了新的探索途径。
</details></li>
</ul>
<hr>
<h2 id="Generative-De-Quantization-for-Neural-Speech-Codec-via-Latent-Diffusion"><a href="#Generative-De-Quantization-for-Neural-Speech-Codec-via-Latent-Diffusion" class="headerlink" title="Generative De-Quantization for Neural Speech Codec via Latent Diffusion"></a>Generative De-Quantization for Neural Speech Codec via Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08330">http://arxiv.org/abs/2311.08330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haici Yang, Inseon Jang, Minje Kim</li>
<li>for: 这 paper 的目的是提出一种 separable 的 speech coding 网络，以提高 speech 质量和简化网络结构。</li>
<li>methods: 该 paper 使用了 end-to-end 编码器来学习紧凑的特征，并使用了 latent diffusion 模型来解码紧凑的特征。</li>
<li>results: 该 paper 的实验结果显示，该模型在两个低比特率（1.5和3kbps）下的主观听测试中表现出色，并且比现有的模型更高效。<details>
<summary>Abstract</summary>
In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.
</details>
<details>
<summary>摘要</summary>
低比特率 speech 编码中，端到端 speech 编码网络目标是学习紧凑而表达力强的特征和一个强大的解码器在单个网络中。这是一个具有挑战性的问题，会导致不良复杂性增加和声音质量下降。在这篇论文中，我们提议将表征学习和信息重建任务分离开。我们利用端到端编码器来学习低维度的整数token，并使用幽默扩散模型将编码后的特征转换为高维度连续空间，从而减轻解码器的幽默扩散和采样加工负担。为了缓解过度平滑生成的问题，我们引入中途填充，并对其进行较强的条件和噪声减少。在分析研究中，我们研究了不同维度的幽默扩散空间和中途填充的hyperparameters。主观听测试显示，我们的模型在1.5和3kbps两个低比特率下表现出色，超过了当前状态的质量。我们的代码和样本在我们的网站上可以获得。
</details></li>
</ul>
<hr>
<h2 id="DQR-TTS-Semi-supervised-Text-to-speech-Synthesis-with-Dynamic-Quantized-Representation"><a href="#DQR-TTS-Semi-supervised-Text-to-speech-Synthesis-with-Dynamic-Quantized-Representation" class="headerlink" title="DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized Representation"></a>DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07965">http://arxiv.org/abs/2311.07965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangzong Wang, Pengcheng Li, Xulong Zhang, Ning Cheng, Jing Xiao</li>
<li>for: 提高 neural-based 文本译音方法在低资源条件下的表现</li>
<li>methods: 使用 semi-supervised 模型，学习 both paired 和 unpaired 数据，并使用动态量化表示模块</li>
<li>results: 与传统方法相比，只使用 less than 120 minutes 的 paired 数据，提高了 subjective 和 objective 评价指标<details>
<summary>Abstract</summary>
Most existing neural-based text-to-speech methods rely on extensive datasets and face challenges under low-resource condition. In this paper, we introduce a novel semi-supervised text-to-speech synthesis model that learns from both paired and unpaired data to address this challenge. The key component of the proposed model is a dynamic quantized representation module, which is integrated into a sequential autoencoder. When given paired data, the module incorporates a trainable codebook that learns quantized representations under the supervision of the paired data. However, due to the limited paired data in low-resource scenario, these paired data are difficult to cover all phonemes. Then unpaired data is fed to expand the dynamic codebook by adding quantized representation vectors that are sufficiently distant from the existing ones during training. Experiments show that with less than 120 minutes of paired data, the proposed method outperforms existing methods in both subjective and objective metrics.
</details>
<details>
<summary>摘要</summary>
现有的神经网络基于文本至话方法大多需要广泛的数据集和面临低资源情况下遇到挑战。在这篇文章中，我们介绍了一个新的半监督文本至话合成模型，可以从对称和无对称数据进行学习，以解决这个挑战。这个模型的关键 комponents是动态量化表现模块，它被组入了一个排序自适应器。当 given paired data 时，这个模块包含一个可调数表示的对称码库，可以在对称数据的监督下学习量化表现。但在低资源情况下，这些对称数据很难覆盖所有的音响。然后，无对称数据被 feed 到扩展动态码库，在训练时添加量化表现向量，以便在训练时与现有的向量 sufficiently distant 的情况下增加量化表现向量。实验显示，仅使用 less than 120 分钟的对称数据，提案方法已经在主观和客观指标中超过现有方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/14/cs.SD_2023_11_14/" data-id="clp53jwvt0111yp88ae304ixc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/14/eess.AS_2023_11_14/" class="article-date">
  <time datetime="2023-11-14T14:00:00.000Z" itemprop="datePublished">2023-11-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/14/eess.AS_2023_11_14/">eess.AS - 2023-11-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mustango-Toward-Controllable-Text-to-Music-Generation"><a href="#Mustango-Toward-Controllable-Text-to-Music-Generation" class="headerlink" title="Mustango: Toward Controllable Text-to-Music Generation"></a>Mustango: Toward Controllable Text-to-Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08355">http://arxiv.org/abs/2311.08355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amaai-lab/mustango">https://github.com/amaai-lab/mustango</a></li>
<li>paper_authors: Jan Melechovsky, Zixun Guo, Deepanway Ghosal, Navonil Majumder, Dorien Herremans, Soujanya Poria</li>
<li>for: 这个研究的目的是发展一个可控制的文本到音乐系统，以掌控生成的音乐的乐器、和声、速度、键等音乐特性。</li>
<li>methods: 这个系统使用了 diffusion 模型，并将音乐领域知识 Informed UNet 模组（MuNet）与文本内容进行整合，以便从文本描述中提取音乐特性，并将其融入到散射过程中。</li>
<li>results: 这个研究显示，这个 Mustango 系统可以实现高质量的音乐生成，并且可以从文本描述中提取音乐特性，并且可以对散射过程进行控制，以实现欲要的乐器、和声、速度、键等音乐特性。<details>
<summary>Abstract</summary>
With recent advancements in text-to-audio and text-to-music based on latent diffusion models, the quality of generated content has been reaching new heights. The controllability of musical aspects, however, has not been explicitly explored in text-to-music systems yet. In this paper, we present Mustango, a music-domain-knowledge-inspired text-to-music system based on diffusion, that expands the Tango text-to-audio model. Mustango aims to control the generated music, not only with general text captions, but from more rich captions that could include specific instructions related to chords, beats, tempo, and key. As part of Mustango, we propose MuNet, a Music-Domain-Knowledge-Informed UNet sub-module to integrate these music-specific features, which we predict from the text prompt, as well as the general text embedding, into the diffusion denoising process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models in terms of desired chords, beat, key, and tempo, on multiple datasets.
</details>
<details>
<summary>摘要</summary>
Recent advancements in text-to-audio and text-to-music based on latent diffusion models have led to significant improvements in generated content quality. However, the controllability of musical aspects in text-to-music systems has not been explicitly explored. In this paper, we present Mustango, a text-to-music system based on diffusion that expands the Tango text-to-audio model. Mustango aims to control the generated music not only with general text captions but also with specific instructions related to chords, beats, tempo, and key.As part of Mustango, we propose MuNet, a Music-Domain-Knowledge-Informed UNet sub-module that integrates music-specific features into the diffusion denoising process. To address the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract music features. We release the resulting MusicBench dataset, which contains over 52K instances and includes music-theory-based descriptions in the caption text.Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models in terms of desired chords, beat, key, and tempo on multiple datasets.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/14/eess.AS_2023_11_14/" data-id="clp53jwx6014ryp886b7p3bi7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/14/cs.CV_2023_11_14/" class="article-date">
  <time datetime="2023-11-14T13:00:00.000Z" itemprop="datePublished">2023-11-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/14/cs.CV_2023_11_14/">cs.CV - 2023-11-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-segmentation-of-irradiation-unicode-x2010-induced-order-unicode-x2010-disorder-phase-transitions-in-electron-microscopy"><a href="#Unsupervised-segmentation-of-irradiation-unicode-x2010-induced-order-unicode-x2010-disorder-phase-transitions-in-electron-microscopy" class="headerlink" title="Unsupervised segmentation of irradiation$\unicode{x2010}$induced order$\unicode{x2010}$disorder phase transitions in electron microscopy"></a>Unsupervised segmentation of irradiation$\unicode{x2010}$induced order$\unicode{x2010}$disorder phase transitions in electron microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08585">http://arxiv.org/abs/2311.08585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman H Ter-Petrosyan, Jenna A Bilbrey, Christina M Doty, Bethany E Matthews, Le Wang, Yingge Du, Eric Lang, Khalid Hattar, Steven R Spurgeon</li>
<li>for: 这个论文是为了无监督分割电子镜像所写的（electron microscopy image segmentation）。</li>
<li>methods: 这个论文使用了域预训练的卷积神经网络（CNN）提取特征，然后生成相似图并应用Louvaine方法进行社区探测，实现分割。</li>
<li>results: 这个论文通过跟踪辐射引起的杂质前缘在薄膜中的变化，以及在 catalysis 和电子设备中的应用。<details>
<summary>Abstract</summary>
We present a method for the unsupervised segmentation of electron microscopy images, which are powerful descriptors of materials and chemical systems. Images are oversegmented into overlapping chips, and similarity graphs are generated from embeddings extracted from a domain$\unicode{x2010}$pretrained convolutional neural network (CNN). The Louvain method for community detection is then applied to perform segmentation. The graph representation provides an intuitive way of presenting the relationship between chips and communities. We demonstrate our method to track irradiation$\unicode{x2010}$induced amorphous fronts in thin films used for catalysis and electronics. This method has potential for "on$\unicode{x2010}$the$\unicode{x2010}$fly" segmentation to guide emerging automated electron microscopes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无监督的电子顾像图像分割方法，这些图像是物质和化学系统的强大描述器。图像被过分割成重叠的块，并从域预训练的卷积神经网络（CNN）中提取出的特征向量生成相似图。然后，我们使用Louvaine方法进行社区探测，进行分割。图表表示块和社区之间的关系，提供了直观的表达方式。我们示cases demonstrate our method for tracking irradiation-induced amorphous fronts in thin films used for catalysis and electronics. This method has the potential for "on-the-fly" segmentation to guide emerging automated electron microscopes.Note: I used the Google Translate API to translate the text into Simplified Chinese. Please note that the translation may not be perfect and may require some adjustments for clarity or accuracy.
</details></li>
</ul>
<hr>
<h2 id="UFOGen-You-Forward-Once-Large-Scale-Text-to-Image-Generation-via-Diffusion-GANs"><a href="#UFOGen-You-Forward-Once-Large-Scale-Text-to-Image-Generation-via-Diffusion-GANs" class="headerlink" title="UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs"></a>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09257">http://arxiv.org/abs/2311.09257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanwu Xu, Yang Zhao, Zhisheng Xiao, Tingbo Hou</li>
<li>for: 这篇论文目的是提出一种高效的文本到图像生成模型，以提高传统文本到图像生成模型的计算成本。</li>
<li>methods: 该模型采用混合方法，结合了扩散模型和GAN目标函数。使用新引入的扩散-GAN目标函数和预训练扩散模型初始化，以实现高质量的文本描述图像生成。</li>
<li>results: UFOGen在单步文本到图像生成中表现出色，能够高效地生成高质量的图像。此外，UFOGen还展示了在不同应用领域的多样化应用能力，如文本描述转换、图像生成等。<details>
<summary>Abstract</summary>
Text-to-image diffusion models have demonstrated remarkable capabilities in transforming textual prompts into coherent images, yet the computational cost of their inference remains a persistent challenge. To address this issue, we present UFOGen, a novel generative model designed for ultra-fast, one-step text-to-image synthesis. In contrast to conventional approaches that focus on improving samplers or employing distillation techniques for diffusion models, UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN objective. Leveraging a newly introduced diffusion-GAN objective and initialization with pre-trained diffusion models, UFOGen excels in efficiently generating high-quality images conditioned on textual descriptions in a single step. Beyond traditional text-to-image generation, UFOGen showcases versatility in applications. Notably, UFOGen stands among the pioneering models enabling one-step text-to-image generation and diverse downstream tasks, presenting a significant advancement in the landscape of efficient generative models. \blfootnote{*Work done as a student researcher of Google, $\dagger$ indicates equal contribution.
</details>
<details>
<summary>摘要</summary>
文本到图像扩散模型已经展现出了很强的能力，可以将文本描述转换成一致的图像，但计算成本仍然是一个挑战。为解决这个问题，我们提出了UFOGen，一种新的生成模型，旨在实现超快、一步文本到图像合成。与传统方法不同，UFOGen采用混合方法，将扩散模型与GAN目标相结合。通过引入新的扩散-GAN目标和使用预训练扩散模型的初始化，UFOGen在一步文本描述下生成高质量的图像。此外，UFOGen在应用方面也有很好的灵活性，可以进行多种下游任务，如图像修饰等。值得注意的是，UFOGen是一种开创性的模型，可以实现一步文本到图像生成和多种下游任务，对有效的生成模型领域具有重要的进步。
</details></li>
</ul>
<hr>
<h2 id="Drivable-3D-Gaussian-Avatars"><a href="#Drivable-3D-Gaussian-Avatars" class="headerlink" title="Drivable 3D Gaussian Avatars"></a>Drivable 3D Gaussian Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08581">http://arxiv.org/abs/2311.08581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, Javier Romero</li>
<li>for: 这篇论文旨在创造可控的3D人体模型，使用 Gaussian splats 技术实现实时渲染。</li>
<li>methods: 该论文使用 dense calibrated multi-view 视频作为输入，并使用 cage deformations 方法来控制人体模型的变形。</li>
<li>results: 对于九名不同体型、衣服和动作的试验者，该方法可以获得更高质量的结果，比之前的方法更适合电子沟通应用。<details>
<summary>Abstract</summary>
We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either accurate 3D registrations during training, dense input images during testing, or both. The ones based on neural radiance fields also tend to be prohibitively slow for telepresence applications. This work uses the recently presented 3D Gaussian Splatting (3DGS) technique to render realistic humans at real-time framerates, using dense calibrated multi-view videos as input. To deform those primitives, we depart from the commonly used point deformation method of linear blend skinning (LBS) and use a classic volumetric deformation method: cage deformations. Given their smaller size, we drive these deformations with joint angles and keypoints, which are more suitable for communication applications. Our experiments on nine subjects with varied body shapes, clothes, and motions obtain higher-quality results than state-of-the-art methods when using the same training and test data.
</details>
<details>
<summary>摘要</summary>
我们介绍Drivable 3D Gaussian Avatars（D3GA），这是首个基于 Gaussian splats 的人体模型，可以实时控制。现有的高级实时渲染人体模型通常需要在训练期间进行高精度的3D注册或在测试期间使用密集的输入图像，或者都是这两者。此外，基于神经辐射场也往往会对实时应用场景造成极大的阻塞。我们使用最近提出的3D Gaussian Splatting（3DGS）技术来渲染真实的人体，并使用密集标准化多视图视频作为输入。为了变形这些基本模型，我们弃用通常使用的点形变形方法（LBS），而是使用经典的体Volume变形方法：笼体变形。由于它们的更小的大小，我们使用关节角度和关节点来驱动这些变形，这更适合通信应用。我们在9名不同身体形态、衣服和动作的试验中获得了与现状方法相比的更高质量结果，使用同一组training和测试数据。
</details></li>
</ul>
<hr>
<h2 id="Reading-Between-the-Mud-A-Challenging-Motorcycle-Racer-Number-Dataset"><a href="#Reading-Between-the-Mud-A-Challenging-Motorcycle-Racer-Number-Dataset" class="headerlink" title="Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset"></a>Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09256">http://arxiv.org/abs/2311.09256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobtyo/swintextspotter">https://github.com/jacobtyo/swintextspotter</a></li>
<li>paper_authors: Jacob Tyo, Youngseog Chung, Motolani Olarinre, Zachary C. Lipton</li>
<li>for: 本研究实验室发表了一个新的挑战性质的数据集，即 off-road motorcycle Racer number Dataset (RnD)，用于光学字符识别 (OCR) 研究。RnD 包含 2,411 幅 профессиональ摄影师拍摄的机车赛车手员照片，这些照片展示了许多对 OCR 难以辨识的因素，例如泥尘遮挡、动态模糊、非标准字体、照明闪光、复杂背景等。</li>
<li>methods: 这个数据集包含 5,578 个手动标注的 bounding box，以及标注的数字和字母。</li>
<li>results: 我们的实验表明，使用现有的 OCR 算法后 fine-tuning，只有 End-to-End F1 分数为 0.527 的 RnD，并且分析显示泥尘是主要的挑战，对于正常情况下的模型而言，泥尘会导致模型的精度下降很多。<details>
<summary>Abstract</summary>
This paper introduces the off-road motorcycle Racer number Dataset (RnD), a new challenging dataset for optical character recognition (OCR) research. RnD contains 2,411 images from professional motorsports photographers that depict motorcycle racers in off-road competitions. The images exhibit a wide variety of factors that make OCR difficult, including mud occlusions, motion blur, non-standard fonts, glare, complex backgrounds, etc. The dataset has 5,578 manually annotated bounding boxes around visible motorcycle numbers, along with transcribed digits and letters. Our experiments benchmark leading OCR algorithms and reveal an end-to-end F1 score of only 0.527 on RnD, even after fine-tuning. Analysis of performance on different occlusion types shows mud as the primary challenge, degrading accuracy substantially compared to normal conditions. But the models struggle with other factors including glare, blur, shadows, and dust. Analysis exposes substantial room for improvement and highlights failure cases of existing models. RnD represents a valuable new benchmark to drive innovation in real-world OCR capabilities. The authors hope the community will build upon this dataset and baseline experiments to make progress on the open problem of robustly recognizing text in unconstrained natural environments. The dataset is available at https://github.com/JacobTyo/SwinTextSpotter.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Topology-of-Surface-Electromyogram-Signals-Hand-Gesture-Decoding-on-Riemannian-Manifolds"><a href="#Topology-of-Surface-Electromyogram-Signals-Hand-Gesture-Decoding-on-Riemannian-Manifolds" class="headerlink" title="Topology of Surface Electromyogram Signals: Hand Gesture Decoding on Riemannian Manifolds"></a>Topology of Surface Electromyogram Signals: Hand Gesture Decoding on Riemannian Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08548">http://arxiv.org/abs/2311.08548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harshavardhanatg/geometryofsemg">https://github.com/harshavardhanatg/geometryofsemg</a></li>
<li>paper_authors: Harshavardhana T. Gowda, Lee M. Miller</li>
<li>for: 这个研究是为了检测肢体上的电势变化，并且使用非侵入式表皮电势实验（sEMG）信号来推导手势。</li>
<li>methods: 这个研究使用了一组多个感应器电极在肢体上的sEMG信号，并使用了一种简单的分析方法来分辨不同的手势。</li>
<li>results: 这个研究发现，使用这种分析方法可以从sEMG信号中提取出丰富的几何图形特征，并且可以用来分辨不同的手势。此外，这种方法还可以处理不同个体和Session之间的信号变化，并且可以提供更加稳定和透明的模型。<details>
<summary>Abstract</summary>
Decoding gestures from the upper limb using noninvasive surface electromyogram (sEMG) signals is of keen interest for the rehabilitation of amputees, artificial supernumerary limb augmentation, gestural control of computers, and virtual/augmented realities. We show that sEMG signals recorded across an array of sensor electrodes in multiple spatial locations around the forearm evince a rich geometric pattern of global motor unit (MU) activity that can be leveraged to distinguish different hand gestures. We demonstrate a simple technique to analyze spatial patterns of muscle MU activity within a temporal window and show that distinct gestures can be classified in both supervised and unsupervised manners. Specifically, we construct symmetric positive definite (SPD) covariance matrices to represent the spatial distribution of MU activity in a time window of interest, calculated as pairwise covariance of electrical signals measured across different electrodes. This allows us to understand and manipulate multivariate sEMG timeseries on a more natural subspace -the Riemannian manifold. Furthermore, it directly addresses signal variability across individuals and sessions, which remains a major challenge in the field. sEMG signals measured at a single electrode lack contextual information such as how various anatomical and physiological factors influence the signals and how their combined effect alters the evident interaction among neighboring muscles. As we show here, analyzing spatial patterns using covariance matrices on Riemannian manifolds allows us to robustly model complex interactions across spatially distributed MUs and provides a flexible and transparent framework to quantify differences in sEMG signals across individuals. The proposed method is novel in the study of sEMG signals and its performance exceeds the current benchmarks while maintaining exceptional computational efficiency.
</details>
<details>
<summary>摘要</summary>
使用非侵入性表面电 MYography (sEMG) 信号记录从上肢臂的各种位置获得的手势解oding是对各种应用场景的感兴趣，包括截肢者的复健、人工辅助臂、手势控制计算机和虚拟/增强现实。我们显示了 sEMG 信号记录在多个感知电极上的数组形式具有丰富的几何征特，可以用来 отличи不同的手势。我们展示了一种简单的分析方法，可以在时间窗口内分析多个肌电单元 (MU) 的活动空间分布，并证明了不同的手势可以在超级vised和无监督方式下分类。特别是，我们使用对称正定 definite (SPD) covariance matrix来表示在时间窗口内MU活动的空间分布，这allow us 理解和操纵多ivariate sEMG 时序序列在自然的Riemannian manifold上。此外，这种方法直接解决了信号变化 across individuals and sessions 这一主要挑战，而且可以Robustly 模型跨 spacedly distributed MUs 的复杂交互关系。我们的方法是对 sEMG 信号的研究中的新方法，其性能超过当前的benchmark，而且保持了出色的计算效率。
</details></li>
</ul>
<hr>
<h2 id="Physical-Adversarial-Examples-for-Multi-Camera-Systems"><a href="#Physical-Adversarial-Examples-for-Multi-Camera-Systems" class="headerlink" title="Physical Adversarial Examples for Multi-Camera Systems"></a>Physical Adversarial Examples for Multi-Camera Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08539">http://arxiv.org/abs/2311.08539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Răduţoiu, Jan-Philipp Schulze, Philip Sperl, Konstantin Böttinger</li>
<li>for: 这个论文旨在研究多个摄像头设计是如何对抗物理攻击的。</li>
<li>methods: 该论文使用了新的攻击方法，即Transcender-MC，它利用在线3D渲染和视角投影在训练过程中。</li>
<li>results: 该论文发现，使用多个摄像头设计可以提供一定的鲁棒性，但是在同时优化多个视角时，这种鲁棒性减少。Transcender-MC方法比现有方法更有效，可以成功攻击多个摄像头设计的11%。<details>
<summary>Abstract</summary>
Neural networks build the foundation of several intelligent systems, which, however, are known to be easily fooled by adversarial examples. Recent advances made these attacks possible even in air-gapped scenarios, where the autonomous system observes its surroundings by, e.g., a camera. We extend these ideas in our research and evaluate the robustness of multi-camera setups against such physical adversarial examples. This scenario becomes ever more important with the rise in popularity of autonomous vehicles, which fuse the information of several cameras for their driving decision. While we find that multi-camera setups provide some robustness towards past attack methods, we see that this advantage reduces when optimizing on multiple perspectives at once. We propose a novel attack method that we call Transcender-MC, where we incorporate online 3D renderings and perspective projections in the training process. Moreover, we motivate that certain data augmentation techniques can facilitate the generation of successful adversarial examples even further. Transcender-MC is 11% more effective in successfully attacking multi-camera setups than state-of-the-art methods. Our findings offer valuable insights regarding the resilience of object detection in a setup with multiple cameras and motivate the need of developing adequate defense mechanisms against them.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SceneScore-Learning-a-Cost-Function-for-Object-Arrangement"><a href="#SceneScore-Learning-a-Cost-Function-for-Object-Arrangement" class="headerlink" title="SceneScore: Learning a Cost Function for Object Arrangement"></a>SceneScore: Learning a Cost Function for Object Arrangement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08530">http://arxiv.org/abs/2311.08530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Kapelyukh, Edward Johns</li>
<li>for: 本研究旨在开发一种能够评估这些物品的排序方式，以便让机器人完成更多有用的任务。</li>
<li>methods: 本研究使用一种名为“SceneScore”的方法，可以学习一个成本函数，以评估排序方式的有用性。这个方法使用能量基本模型来学习训练排序方式的分布，不需要环境互动或人工指导。</li>
<li>results: 实验结果显示，学习的成本函数可以用来预测缺失的物品的姿态，扩展到新的物品使用semantic特征，并且可以与其他成本函数进行结合以满足约束。<details>
<summary>Abstract</summary>
Arranging objects correctly is a key capability for robots which unlocks a wide range of useful tasks. A prerequisite for creating successful arrangements is the ability to evaluate the desirability of a given arrangement. Our method "SceneScore" learns a cost function for arrangements, such that desirable, human-like arrangements have a low cost. We learn the distribution of training arrangements offline using an energy-based model, solely from example images without requiring environment interaction or human supervision. Our model is represented by a graph neural network which learns object-object relations, using graphs constructed from images. Experiments demonstrate that the learned cost function can be used to predict poses for missing objects, generalise to novel objects using semantic features, and can be composed with other cost functions to satisfy constraints at inference time.
</details>
<details>
<summary>摘要</summary>
“ Correctly arranging objects is a crucial ability for robots, unlocking a wide range of useful tasks. To create successful arrangements, we need to evaluate the desirability of a given arrangement. Our method, SceneScore, learns a cost function for arrangements, where desirable, human-like arrangements have a low cost. We learn the distribution of training arrangements offline using an energy-based model, solely from example images without requiring environment interaction or human supervision. Our model is represented by a graph neural network, which learns object-object relations using graphs constructed from images. Experimental results show that the learned cost function can be used to predict poses for missing objects, generalize to novel objects using semantic features, and can be composed with other cost functions to satisfy constraints at inference time.”Here's the translation breakdown:* “ Correctly arranging objects is a crucial ability for robots, unlocking a wide range of useful tasks.” (对象的正确排序是机器人的关键能力，解锁了许多有用的任务。)* “ To create successful arrangements, we need to evaluate the desirability of a given arrangement.” (成功的排序需要评估给定排序的可 desirability。)* “ Our method, SceneScore, learns a cost function for arrangements, where desirable, human-like arrangements have a low cost.” (我们的方法是 SceneScore，学习排序的成本函数，愿望的人类化排序有低成本。)* “ We learn the distribution of training arrangements offline using an energy-based model, solely from example images without requiring environment interaction or human supervision.” (我们在线上学习排序的分布，使用能量基本模型，只使用示例图像，不需要环境互动或人类监督。)* “ Our model is represented by a graph neural network, which learns object-object relations using graphs constructed from images.” (我们的模型是图神经网络，学习图像中对象之间的关系。)* “ Experimental results show that the learned cost function can be used to predict poses for missing objects, generalize to novel objects using semantic features, and can be composed with other cost functions to satisfy constraints at inference time.” (实验结果表明，学习的成本函数可以用来预测缺失对象的姿态，泛化到新对象使用semantic特征，并可以与其他成本函数组合来满足约束。)
</details></li>
</ul>
<hr>
<h2 id="Cross-dataset-domain-adaptation-for-the-classification-COVID-19-using-chest-computed-tomography-images"><a href="#Cross-dataset-domain-adaptation-for-the-classification-COVID-19-using-chest-computed-tomography-images" class="headerlink" title="Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images"></a>Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08524">http://arxiv.org/abs/2311.08524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ridha Ouni, Haikel Alhichri</li>
<li>for: 这个论文是为了检测 COVID-19 患者使用计算机 Tomography（CT）图像的肺部而写的。</li>
<li>methods: 这个论文使用了 Deep Learning（DL）解决方案，具体来说是 Convolutional Neural Networks（CNN）。它们在同一个数据集上训练和测试时取得了出色的结果，但是当数据集不同时，结果却很低。这个论文使用了领域适应（DA）技术来解决这个跨数据集问题。</li>
<li>results: 这个论文在四个跨数据集场景中测试了 COVID19-DANet 模型，使用了 SARS-CoV-2-CT 和 COVID19-CT 数据集，并取得了比现有研究更加鼓舞人的结果。<details>
<summary>Abstract</summary>
Detecting COVID-19 patients using Computed Tomography (CT) images of the lungs is an active area of research. Datasets of CT images from COVID-19 patients are becoming available. Deep learning (DL) solutions and in particular Convolutional Neural Networks (CNN) have achieved impressive results for the classification of COVID-19 CT images, but only when the training and testing take place within the same dataset. Work on the cross-dataset problem is still limited and the achieved results are low. Our work tackles the cross-dataset problem through a Domain Adaptation (DA) technique with deep learning. Our proposed solution, COVID19-DANet, is based on pre-trained CNN backbone for feature extraction. For this task, we select the pre-trained Efficientnet-B3 CNN because it has achieved impressive classification accuracy in previous work. The backbone CNN is followed by a prototypical layer which is a concept borrowed from prototypical networks in few-shot learning (FSL). It computes a cosine distance between given samples and the class prototypes and then converts them to class probabilities using the Softmax function. To train the COVID19-DANet model, we propose a combined loss function that is composed of the standard cross-entropy loss for class discrimination and another entropy loss computed over the unlabelled target set only. This so-called unlabelled target entropy loss is minimized and maximized in an alternative fashion, to reach the two objectives of class discrimination and domain invariance. COVID19-DANet is tested under four cross-dataset scenarios using the SARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results compared to recent work in the literature.
</details>
<details>
<summary>摘要</summary>
寻找COVID-19患者使用计算机Tomography（CT）图像是一个活跃的研究领域。COVID-19患者的CT图像集合在不断地提供。深度学习（DL）解决方案，特别是卷积神经网络（CNN），在分类COVID-19 CT图像时已经达到了非常出色的结果，但只有在训练和测试数据集在同一个数据集时才能达到这些结果。对于跨数据集问题，目前的研究尚未充分，已经获得的结果较低。我们的工作是通过领域适应（DA）技术来解决跨数据集问题。我们提出的解决方案是基于预训练的 CNN 背景，称为 COVID19-DANet。我们选择了预训练的 Efficientnet-B3 CNN，因为它在前一个工作中已经达到了非常出色的分类精度。背景 CNN 后接一个prototype层，这是从几何学学习（FSL）中借鉴的概念。它计算给定样本和类型谱的cosine距离，然后使用softmax函数将其转换为类别概率。为了训练 COVID19-DANet 模型，我们提议一个组合损失函数，由标准的交叉熵损失和另一个对无标签目标集的 entropy 损失组成。这个无标签目标 entropy 损失在 alternate 的方式下逐渐下降和上升，以实现两个目标：类别识别和领域不变性。COVID19-DANet 在四个跨数据集场景下进行测试，使用 SARS-CoV-2-CT 和 COVID19-CT 数据集，并取得了与当前文献中的结果相当的成绩。
</details></li>
</ul>
<hr>
<h2 id="MADG-Margin-based-Adversarial-Learning-for-Domain-Generalization"><a href="#MADG-Margin-based-Adversarial-Learning-for-Domain-Generalization" class="headerlink" title="MADG: Margin-based Adversarial Learning for Domain Generalization"></a>MADG: Margin-based Adversarial Learning for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08503">http://arxiv.org/abs/2311.08503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aveen Dayal, Vimal K. B., Linga Reddy Cenkeramaddi, C. Krishna Mohan, Abhinav Kumar, Vineeth N Balasubramanian<br>for:The paper aims to address the challenges of domain shift in deep learning by proposing a novel adversarial learning-based domain generalization (DG) algorithm, MADG.methods:The MADG model uses a margin loss-based discrepancy metric, which is more informative, tighter, practical, and efficiently optimizable compared to traditional 0-1 loss-based methods.results:The proposed MADG model learns domain-invariant features across all source domains and generalizes well to unseen target domains, as demonstrated through extensive experiments on popular real-world DG datasets. The model’s performance is consistent across all datasets, and the authors provide a theoretical analysis of the model’s generalization bound using margin loss and Rademacher complexity.<details>
<summary>Abstract</summary>
Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\mathcal{H}\Delta\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, MADG, motivated by a margin loss-based discrepancy metric. The proposed MADG model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed MADG model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the MADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.
</details>
<details>
<summary>摘要</summary>
域外泛化（DG）技术已成为深度学习（DL）中解决域shift问题的流行方法，目标是在训练时未看到目标域的情况下，在目标域上具有良好的泛化性。在过去几年，许多DG方法被提出，其中一种受欢迎的方法是对抗学习基本方法。对抗DG方法的主要想法是通过最小化一个距离度量来学习域 invariant 特征。然而，大多数对抗DG方法使用0-1损失函数基于 $\mathcal{H}\Delta\mathcal{H}$ 距离度量。相比之下，margin损失基于距离度量有以下优点：更加有用信息、紧密、实用和可效地优化。为了弥补这一差距，本文提出了一种新的对抗学习DG算法，called MADG，被激发于margin损失基于距离度量。MADG模型在所有源域上学习域 invariant 特征，并使用对抗训练来在未看到目标域的情况下泛化良好。我们还提供了对MADG模型的理论分析，基于未见目标错误 bound。 Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the MADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.
</details></li>
</ul>
<hr>
<h2 id="Performance-of-Machine-Learning-Classification-in-Mammography-Images-using-BI-RADS"><a href="#Performance-of-Machine-Learning-Classification-in-Mammography-Images-using-BI-RADS" class="headerlink" title="Performance of Machine Learning Classification in Mammography Images using BI-RADS"></a>Performance of Machine Learning Classification in Mammography Images using BI-RADS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08493">http://arxiv.org/abs/2311.08493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malitha Gunawardhana, Norbert Zolek</li>
<li>for: 这份研究旨在测试不同类型的乳腺超音波图像分类模型的精度，以BI-RADS（乳腺影像评价和数据系统）的定义。</li>
<li>methods: 我们使用了六种进步的分类架构，包括VGG19 \cite{simonyan2014very}, ResNet50 \cite{he2016deep}, GoogleNet \cite{szegedy2015going}, ConvNext \cite{liu2022convnet}, EfficientNet \cite{tan2019efficientnet}和Vision Transformers (ViT) \cite{dosovitskiy2020image}，而不是传统机器学习模型。我们在三个不同的设定下评估这些模型：全面精度训练、线性评估和从头开始训练。</li>
<li>results: 我们发现这些模型在全面精度训练设定下具有极高的精度和F1分数，尤其是76.39%的精度和67.94%的F1分数。这些结果显示了我们的电脑支持诊断系统的可能性和可靠性，并提供了对未来对乳腺影像评价系统的改进的坚实基础。<details>
<summary>Abstract</summary>
This research aims to investigate the classification accuracy of various state-of-the-art image classification models across different categories of breast ultrasound images, as defined by the Breast Imaging Reporting and Data System (BI-RADS). To achieve this, we have utilized a comprehensively assembled dataset of 2,945 mammographic images sourced from 1,540 patients. In order to conduct a thorough analysis, we employed six advanced classification architectures, including VGG19 \cite{simonyan2014very}, ResNet50 \cite{he2016deep}, GoogleNet \cite{szegedy2015going}, ConvNext \cite{liu2022convnet}, EfficientNet \cite{tan2019efficientnet}, and Vision Transformers (ViT) \cite{dosovitskiy2020image}, instead of traditional machine learning models. We evaluate models in three different settings: full fine-tuning, linear evaluation and training from scratch. Our findings demonstrate the effectiveness and capability of our Computer-Aided Diagnosis (CAD) system, with a remarkable accuracy of 76.39\% and an F1 score of 67.94\% in the full fine-tuning setting. Our findings indicate the potential for enhanced diagnostic accuracy in the field of breast imaging, providing a solid foundation for future endeavors aiming to improve the precision and reliability of CAD systems in medical imaging.
</details>
<details>
<summary>摘要</summary>
Note:* "BI-RADS" 是指 Breast Imaging Reporting and Data System* "CAD" 是指 Computer-Aided Diagnosis* "F1 score" 是指 F1 分数
</details></li>
</ul>
<hr>
<h2 id="MUDD-A-New-Re-Identification-Dataset-with-Efficient-Annotation-for-Off-Road-Racers-in-Extreme-Conditions"><a href="#MUDD-A-New-Re-Identification-Dataset-with-Efficient-Annotation-for-Off-Road-Racers-in-Extreme-Conditions" class="headerlink" title="MUDD: A New Re-Identification Dataset with Efficient Annotation for Off-Road Racers in Extreme Conditions"></a>MUDD: A New Re-Identification Dataset with Efficient Annotation for Off-Road Racers in Extreme Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08488">http://arxiv.org/abs/2311.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobtyo/mudd">https://github.com/jacobtyo/mudd</a></li>
<li>paper_authors: Jacob Tyo, Motolani Olarinre, Youngseog Chung, Zachary C. Lipton</li>
<li>for: 这个论文目标是解决无约束环境中重新识别个体的问题，具体是开发一个大规模的摄像头识别数据集（MUDD），用于测试和评估重新识别模型在耗油车比赛中的性能。</li>
<li>methods: 这篇论文使用了现有的重新识别模型，包括OSNet和ResNet-50，并进行了精心的标注方法，以减少标注时间的消耗。</li>
<li>results: 根据实验结果，在没有微调的情况下，最佳模型的rank1准确率只有33%，但是通过微调MUDD数据集，可以提高rank1准确率至79%。但是，还存在许多可以改进的问题，如果能够解决这些问题，可以提高重新识别模型的性能。<details>
<summary>Abstract</summary>
Re-identifying individuals in unconstrained environments remains an open challenge in computer vision. We introduce the Muddy Racer re-IDentification Dataset (MUDD), the first large-scale benchmark for matching identities of motorcycle racers during off-road competitions. MUDD exhibits heavy mud occlusion, motion blurring, complex poses, and extreme lighting conditions previously unseen in existing re-id datasets. We present an annotation methodology incorporating auxiliary information that reduced labeling time by over 65%. We establish benchmark performance using state-of-the-art re-id models including OSNet and ResNet-50. Without fine-tuning, the best models achieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79% Rank-1, but significant room for improvement remains. We analyze the impact of real-world factors including mud, pose, lighting, and more. Our work exposes open problems in re-identifying individuals under extreme conditions. We hope MUDD serves as a diverse and challenging benchmark to spur progress in robust re-id, especially for computer vision applications in emerging sports analytics. All code and data can be found at https://github.com/JacobTyo/MUDD.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>计算机视觉中重新识别人员在未经限制的环境中仍然是一个打开的挑战。我们介绍了污泥跑手重新识别数据集（MUDD），这是第一个大规模的匹配跑手身份的挑战。MUDD表现出污泥干扰、运动模糊、复杂的姿势和前所未见的照明条件。我们提出了一种注解方法，通过辅助信息减少标注时间超过65%。我们使用现状的最佳扩展模型，包括OSNet和ResNet-50，并对MUDD进行了微调。没有微调，最佳模型只有33%的排名第一精度。微调后，最佳模型的精度提高到79%，但还有很大的改进空间。我们分析了实际世界中的因素，包括污泥、姿势、照明等。我们的工作暴露了人员重新识别下极端条件下的开放问题。我们希望MUDD能成为一个多样化和挑战的标准 benchmark，以推动Robust re-id的进步，特别是在出现的运动数据分析领域。所有代码和数据可以在https://github.com/JacobTyo/MUDD中找到。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Foundation-Models-to-Improve-Lightweight-Clients-in-Federated-Learning"><a href="#Leveraging-Foundation-Models-to-Improve-Lightweight-Clients-in-Federated-Learning" class="headerlink" title="Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning"></a>Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08479">http://arxiv.org/abs/2311.08479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xidong Wu, Wan-Yi Lin, Devin Willmott, Filipe Condessa, Yufei Huang, Zhenzhen Li, Madan Ravi Ganesh</li>
<li>for: 帮助在不同数据分布环境下进行联合训练轺降减模型，提高模型性能和Robustness。</li>
<li>methods: 基于基础模型的模型精炼法，帮助减轻联合训练的计算开销和执行速率。</li>
<li>results: 在不同数据分布环境下，模型性能得到了提高，尤其是在罕见样本下的表现。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed training paradigm that enables clients scattered across the world to cooperatively learn a global model without divulging confidential data. However, FL faces a significant challenge in the form of heterogeneous data distributions among clients, which leads to a reduction in performance and robustness. A recent approach to mitigating the impact of heterogeneous data distributions is through the use of foundation models, which offer better performance at the cost of larger computational overheads and slower inference speeds. We introduce foundation model distillation to assist in the federated training of lightweight client models and increase their performance under heterogeneous data settings while keeping inference costs low. Our results show improvement in the global model performance on a balanced testing set, which contains rarely observed samples, even under extreme non-IID client data distributions. We conduct a thorough evaluation of our framework with different foundation model backbones on CIFAR10, with varying degrees of heterogeneous data distributions ranging from class-specific data partitions across clients to dirichlet data sampling, parameterized by values between 0.01 and 1.0.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式训练模式，允许全球各地的客户端共同学习一个全球模型，而无需披露敏感数据。然而，FL 面临着各客户端数据分布异常的挑战，这会导致模型性能和可靠性下降。为了减轻客户端数据分布异常的影响，我们可以使用基础模型，它们在训练时需要更大的计算负担和更慢的推理速度，但可以提高模型性能。我们称之为基础模型浸泡来帮助在不同客户端上进行联合训练轻量级客户端模型，以提高在不同数据分布情况下的性能，并保持推理成本低。我们的结果表明，在权衡测试集上，我们的框架可以在不同基础模型背bone和客户端数据分布情况下提高全球模型的性能，尤其是在EXTREME Non-IID客户端数据分布情况下。我们在 CIFAR10 上进行了系统性的评估，并 parametrised 客户端数据分布情况，从 class-specific 数据分区到 dirichlet 抽样，参数范围为 0.01 到 1.0。
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-Ended-Visual-Recognition-with-Large-Language-Model"><a href="#Towards-Open-Ended-Visual-Recognition-with-Large-Language-Model" class="headerlink" title="Towards Open-Ended Visual Recognition with Large Language Model"></a>Towards Open-Ended Visual Recognition with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08400">http://arxiv.org/abs/2311.08400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bytedance/omniscient-model">https://github.com/bytedance/omniscient-model</a></li>
<li>paper_authors: Qihang Yu, Xiaohui Shen, Liang-Chieh Chen</li>
<li>for: 提出一种 straightforward 和有效的解决方案，即 OmniScient Model (OSM)，以 Addressing the challenges of localizing and recognizing objects in the open-ended physical world.</li>
<li>methods: 使用 Large Language Model (LLM) 生成类标签，取消提供类名称 during both training and testing，并且可以在不需要人工干预的情况下进行跨数据集训练，展现了robust generalization capabilities。</li>
<li>results: 通过combine OSM with off-the-shelf mask proposal model，在多种 benchmark 上显示了Promising results, and demonstrated its effectiveness in handling novel concepts.<details>
<summary>Abstract</summary>
Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception. Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP) using pre-extracted text embeddings. However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them. In this paper, we introduce the OmniScient Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges. Specifically, OSM predicts class labels in a generative manner, thus removing the supply of class names during both training and testing. It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM. By combining OSM with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts. Code/model are available at https://github.com/bytedance/OmniScient-Model.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的大型自然语言模型（LLM）基于的面部分类器——全能科学模型（OSM），用于解决开放式物理世界中物体认知的长期挑战。传统方法通常采用类型不固定的面提议模型，并且使用预取得的文本嵌入来进行开放 vocabulary 分类。然而，这些开放 vocabulary 分类器在实际应用中仍存在一些限制。一方面，它们需要用户提供类别名称进行测试，测试性能强度取决于用户提供的类别名称。另一方面，在多个数据集上训练时，需要人工干预来缓解数据集之间的类别定义冲突。本文的解决方案是基于 LLM 的面部分类器，可以预测类别标签的生成方式，因此不需要在训练和测试中提供类别名称。它还可以在不需要人工干预的情况下跨数据集训练，并且具有强大的世界知识，从而表现出了robust的泛化能力。通过将 OSM 与一个开源的面提议模型结合使用，我们在多个标准准确率上获得了扎实的表现，并且在处理新概念方面也具有良好的效果。代码/模型可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="USLR-an-open-source-tool-for-unbiased-and-smooth-longitudinal-registration-of-brain-MR"><a href="#USLR-an-open-source-tool-for-unbiased-and-smooth-longitudinal-registration-of-brain-MR" class="headerlink" title="USLR: an open-source tool for unbiased and smooth longitudinal registration of brain MR"></a>USLR: an open-source tool for unbiased and smooth longitudinal registration of brain MR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08371">http://arxiv.org/abs/2311.08371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acasamitjana/uslr">https://github.com/acasamitjana/uslr</a></li>
<li>paper_authors: Adrià Casamitjana, Roser Sala-Llonch, Karim Lekadir, Juan Eugenio Iglesias<br>for: 这个论文是为了提出一种计算框架，用于对大脑MRI扫描图像进行长期匀速注册，以估计时间序列中的非线性图像轨迹，这些轨迹是时间点无偏见、空间变换准确、成像artefact抗性的。methods: 这个框架使用了Lie álgebra参数化的空间变换（与rigid变换和静止速度场兼容），并利用log域属性来解决问题，并且使用 Bayesian推理来估计非线性注册和rigid注册。results: 这个框架可以为Alzheimer’s disease研究提供多个方面的 beneficial effects，例如：时间一致的图像分割，以减少内部变化的影响，subject特定预测或人口分析使用tensor-based morphometry。这种方法可以在扫描图像之间进行较为精细的衰减识别，从而减少临床试验中的样本大小。<details>
<summary>Abstract</summary>
We present USLR, a computational framework for longitudinal registration of brain MRI scans to estimate nonlinear image trajectories that are smooth across time, unbiased to any timepoint, and robust to imaging artefacts. It operates on the Lie algebra parameterisation of spatial transforms (which is compatible with rigid transforms and stationary velocity fields for nonlinear deformation) and takes advantage of log-domain properties to solve the problem using Bayesian inference. USRL estimates rigid and nonlinear registrations that: (i) bring all timepoints to an unbiased subject-specific space; and (i) compute a smooth trajectory across the imaging time-series. We capitalise on learning-based registration algorithms and closed-form expressions for fast inference. A use-case Alzheimer's disease study is used to showcase the benefits of the pipeline in multiple fronts, such as time-consistent image segmentation to reduce intra-subject variability, subject-specific prediction or population analysis using tensor-based morphometry. We demonstrate that such approach improves upon cross-sectional methods in identifying group differences, which can be helpful in detecting more subtle atrophy levels or in reducing sample sizes in clinical trials. The code is publicly available in https://github.com/acasamitjana/uslr
</details>
<details>
<summary>摘要</summary>
我们提出了USLR，一种计算机框架，用于对脑MRI扫描图像进行长期均衡注册，以估计不含时刻点的图像轨迹，这些轨迹在时间方向上是平滑的，不受成像artefacts的影响。它基于 Lie丰化参数化的空间变换（与静止速度场和非线性扭曲兼容），并利用 log 域的特性来解决问题，使用 Bayesian 推理。USLR 估计了不含时刻点的扭曲和非线性注册，它们可以：（i）将所有时刻点转移到不受偏见影响的个体特定空间中; （ii）计算图像序列中的平滑轨迹。我们利用了学习型注册算法和关闭式表达，以便快速推理。我们使用了 Alzheimer's disease 研究来展示我们的框架在多个方面的优势，包括时间相关的图像分割，以降低内部变化的影响，以及使用tensor基本形态来预测或对 population 进行分析。我们示出，这种方法可以在跨sectional 方法上提高组差异的检测，这有助于检测更加弱的衰老水平或减少临床试验中的样本大小。代码可以在 https://github.com/acasamitjana/uslr 中获取。
</details></li>
</ul>
<hr>
<h2 id="The-Perception-Robustness-Tradeoff-in-Deterministic-Image-Restoration"><a href="#The-Perception-Robustness-Tradeoff-in-Deterministic-Image-Restoration" class="headerlink" title="The Perception-Robustness Tradeoff in Deterministic Image Restoration"></a>The Perception-Robustness Tradeoff in Deterministic Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09253">http://arxiv.org/abs/2311.09253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Ohayon, Tomer Michaeli, Michael Elad</li>
<li>for: 解决 inverse problems 的 deterministic 方法的行为</li>
<li>methods: 使用 Lipschitz 常数来证明 predictor 的质量</li>
<li>results: deterministic 方法会受到 adversarial attack 的影响，但可以通过 exploring posterior distribution 来模拟 stochastic methods。Here’s a more detailed explanation of each point:1. for: The paper is written to study the behavior of deterministic methods for solving inverse problems in imaging. The authors aim to understand the limitations of these methods and how they can be improved.2. methods: The authors use Lipschitz constants to measure the quality of the predictor. They prove that the better the predictor satisfies two goals - high perceptual quality and consistency with measurements - the larger the Lipschitz constant must be. This implies that such methods are more susceptible to adversarial attacks.3. results: The authors demonstrate their theory on single image super-resolution algorithms, showing that the deterministic method can be affected by adversarial attacks. However, they also show that by exploring the posterior distribution, the deterministic method can imitate stochastic methods, which are more robust to such attacks.<details>
<summary>Abstract</summary>
We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods.
</details>
<details>
<summary>摘要</summary>
我们研究决定方法对几何问题的解决方案。这些方法通常是设计来 дости持二个目标：（1）实现高度的感知质量，和（2）生成符合测量的重建。我们提供了一个严谨的证明，表明如果预测器更好地满足这两个需求，则其Lipschitz常数必须变大，不管受到的扰动是什么样的。具体来说，要进一步推进完美的感知质量和完美的一致性，预测器的Lipschitz常数必须增长到无限大。这意味着这些方法一定会更易受到骗袭攻击。我们在单影像超解析算法中证明了我们的理论，包括噪音和噪音无的设定。我们还示出了如何利用这种不愿的行为来探索 posterior 分布，从而让决定模型模仿随机方法。
</details></li>
</ul>
<hr>
<h2 id="Rotation-Agnostic-Image-Representation-Learning-for-Digital-Pathology"><a href="#Rotation-Agnostic-Image-Representation-Learning-for-Digital-Pathology" class="headerlink" title="Rotation-Agnostic Image Representation Learning for Digital Pathology"></a>Rotation-Agnostic Image Representation Learning for Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08359">http://arxiv.org/abs/2311.08359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RhazesLab/PathDino">https://github.com/RhazesLab/PathDino</a></li>
<li>paper_authors: Saghir Alfasly, Abubakr Shafique, Peyman Nejat, Jibran Khan, Areej Alsaafin, Ghazal Alabtah, H. R. Tizhoosh</li>
<li>for: 这篇论文主要针对 histopathological image analysis 领域的复杂挑战，通过三个关键贡献进行解决。</li>
<li>methods: 该论文提出了一种快速补丁选择方法 (FPS)，用于整个扫描图像 (WSI) 分析，大幅降低计算成本，保持准确性。此外，它还提出了一种轻量级的 histopathology 特征提取器 PathDino，只有 9 万参数，远远少于其他选择。</li>
<li>results: 该论文表明，使用我们的紧凑型模型可以在 12 种多样化的数据集上超越现有的 histopathology-specific 视Transformers，包括四个内部数据集 (乳腺、肝脏、皮肤和肠癌) 以及七个公共数据集。准确性提高了 8.5%。<details>
<summary>Abstract</summary>
This paper addresses complex challenges in histopathological image analysis through three key contributions. Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reducing computational cost while maintaining accuracy. Secondly, it presents PathDino, a lightweight histopathology feature extractor with a minimal configuration of five Transformer blocks and only 9 million parameters, markedly fewer than alternatives. Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting. We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training dataset of 6 million histopathology patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance. These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/
</details>
<details>
<summary>摘要</summary>
这份论文通过三大贡献提供了复杂的 histopathological 图像分析解决方案。首先，它提出了一种快速补充方法（FPS），用于整个扫描图像（WSI）分析，大幅降低计算成本而保持准确性。其次，它推出了一种轻量级的历史病理特征提取器（PathDino），只有5个转换块和900万参数，与其他选择器相比明显少于。最后，它引入了一种不受旋转影响的学习模式，使用自动驱动学习，有效地避免过拟合。我们还证明了我们的减少模型在12个多样化的数据集上（包括四个内部数据集（乳腺、肝脏、皮肤和肠Rectum）以及七个公共数据集（PANDA、CAMELYON16、BRACS、DigestPath、Kather、PanNuke和WSSS4LUAD））都能够超越现有的历史病理特定视Transformers。特别是，即使使用TCGA数据集进行600万次训练，我们的方法还能够在批量投票中平均提高8.5%的性能。这些贡献为整个数字病理学领域提供了一个坚实的框架，通过广泛的评估来强制验证。项目页面：https://rhazeslab.github.io/PathDino-Page/
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Networks-Exploiting-Attributes-of-Biological-Neurons"><a href="#Convolutional-Neural-Networks-Exploiting-Attributes-of-Biological-Neurons" class="headerlink" title="Convolutional Neural Networks Exploiting Attributes of Biological Neurons"></a>Convolutional Neural Networks Exploiting Attributes of Biological Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08314">http://arxiv.org/abs/2311.08314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neeraj Kumar Singh, Nikhil R. Pal</li>
<li>For: This paper aims to improve the performance of Convolutional Neural Networks (CNNs) by integrating principles from biological neurons into the network architecture.* Methods: The proposed method uses neuro-science-inspired computational models of the Lateral Geniculate Nucleus (LGN) and simple cells of the primary visual cortex to extract image features as input to CNNs. The method also uses a two-tower CNN architecture, with one shallow tower and one ResNet 18 tower, to enhance the learning process and performance.* Results: The proposed method achieves a noticeable improvement in performance (on average 5-10%) on CIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. Additionally, the efficiency of only the Push-Pull tower of the network is also checked.Here is the Chinese translation of the three key points:* For: 这篇论文目标是通过将生物神经元的原理 integrate into CNN 网络架构来提高 CNN 的性能。* Methods: 提议的方法使用了生物神经元的计算模型，即 Lateral Geniculate Nucleus (LGN) 和 primary visual cortex 中的简单细胞模型，以提取图像特征作为 CNN 的输入。该方法还使用了一个两个塔的 CNN 架构，其中一个是浅塔，另一个是 ResNet 18 塔，以增强网络学习过程和性能。* Results: 提议的方法在 CIFAR-10、CIFAR-100 和 ImageNet-100 数据集上实现了平均提高5-10%的性能，相比 ResNet-18。此外，只测试了 Push-Pull 塔的效率也被检查了。<details>
<summary>Abstract</summary>
In this era of artificial intelligence, deep neural networks like Convolutional Neural Networks (CNNs) have emerged as front-runners, often surpassing human capabilities. These deep networks are often perceived as the panacea for all challenges. Unfortunately, a common downside of these networks is their ''black-box'' character, which does not necessarily mirror the operation of biological neural systems. Some even have millions/billions of learnable (tunable) parameters, and their training demands extensive data and time.   Here, we integrate the principles of biological neurons in certain layer(s) of CNNs. Specifically, we explore the use of neuro-science-inspired computational models of the Lateral Geniculate Nucleus (LGN) and simple cells of the primary visual cortex. By leveraging such models, we aim to extract image features to use as input to CNNs, hoping to enhance training efficiency and achieve better accuracy. We aspire to enable shallow networks with a Push-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as the foundation layer of CNNs to enhance their learning process and performance. To achieve this, we propose a two-tower CNN, one shallow tower and the other as ResNet 18. Rather than extracting the features blindly, it seeks to mimic how the brain perceives and extracts features. The proposed system exhibits a noticeable improvement in the performance (on an average of $5\%-10\%$) on CIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We also check the efficiency of only the Push-Pull tower of the network.
</details>
<details>
<summary>摘要</summary>
在人工智能时代，深度神经网络（CNN）已成为前Runner，经常超越人类能力。这些深度网络经常被视为所有挑战的解决方案。然而，它们的一个常见缺点是“黑盒”性，不一定反映生物神经系统的运作。一些甚至有 millions/billions 的可调参数，并且培训需要大量数据和时间。在这里，我们将生物神经元的原理 integrate 到 Certain Layer 中的 CNN 中。具体来说，我们将 explore 使用生物 ней维科学发展的 Lateral Geniculate Nucleus (LGN) 和 primary visual cortex 的简单细胞模型。通过这些模型，我们希望从图像中提取特征，并将其作为 CNN 的输入，以提高训练效率和准确率。我们 aspire 使用 shallow network 和 ResNet 18 的 Push-Pull Combination of Receptive Fields (PP-CORF) 模型作为基础层，以便提高 CNN 的学习过程和性能。我们的提案的系统在 CIFAR-10、CIFAR-100 和 ImageNet-100 数据集上显示了明显的性能提升（在 average 上为5%-10%），并且只有 Push-Pull 塔的网络进行测试。
</details></li>
</ul>
<hr>
<h2 id="The-Heat-is-On-Thermal-Facial-Landmark-Tracking"><a href="#The-Heat-is-On-Thermal-Facial-Landmark-Tracking" class="headerlink" title="The Heat is On: Thermal Facial Landmark Tracking"></a>The Heat is On: Thermal Facial Landmark Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08308">http://arxiv.org/abs/2311.08308</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Baker</li>
<li>for: 这篇论文旨在为热像图像中的人脸特征点跟踪提供一种方法，以捕捉人体的生物physiological signal，例如血液循环和汗水 secretion，从而远程评估情绪和兴奋程度。</li>
<li>methods: 这篇论文使用了多种不同的模型组件，如径 residual connections，通道和特征 wise attention，以及 ensemble 组件的实践，以提高模型的性能。</li>
<li>results: 研究发现，使用 convolutional 和 residual 层，并且在通道方向进行自注意力处理，可以实现最佳性能，需要 menos than 100K 参数。<details>
<summary>Abstract</summary>
Facial landmark tracking for thermal images requires tracking certain important regions of subjects' faces, using images from thermal images, which omit lighting and shading, but show the temperatures of their subjects. The fluctuations of heat in particular places reflect physiological changes like bloodflow and perspiration, which can be used to remotely gauge things like anxiety and excitement. Past work in this domain has been limited to only a very limited set of architectures and techniques. This work goes further by trying a comprehensive suit of various models with different components, such as residual connections, channel and feature-wise attention, as well as the practice of ensembling components of the network to work in parallel. The best model integrated convolutional and residual layers followed by a channel-wise self-attention layer, requiring less than 100K parameters.
</details>
<details>
<summary>摘要</summary>
法面特征跟踪技术 для热图像需要跟踪热图像中重要的面部区域，使用热图像，它们排除光照和阴影，但显示对象的温度。热图像中的温度波动反映了物体的生理变化，如血液循环和汗水，可以通过远程测量情绪和兴奋程度。过去在这个领域中的研究受限于一个非常有限的集合 Architecture和技术。这个工作更迭，尝试了多种不同的模型组件，如径 residual 连接，通道和特征 wise 注意力，以及网络组件的并行实现。最佳模型结合了卷积层和径 residual 层，然后跟踪通道的自注意力层，需要 menos than 100K 参数。
</details></li>
</ul>
<hr>
<h2 id="Level-Set-KSVD"><a href="#Level-Set-KSVD" class="headerlink" title="Level Set KSVD"></a>Level Set KSVD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08284">http://arxiv.org/abs/2311.08284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rituparnaS/Dictionary-learning-level-set">https://github.com/rituparnaS/Dictionary-learning-level-set</a></li>
<li>paper_authors: Omer Sapir, Iftach Klapp, Nir Sochen</li>
<li>for: 用于检测蔬菜场景中病虫的扩散</li>
<li>methods: 使用维度集KSVD学习特征，并使用一种通过 Chan-Vese 函数泛化的 Image segmentation 方法</li>
<li>results: 测试结果与其他方法相比，Level-set KSVD 方法显示更高的准确率和更好的性能<details>
<summary>Abstract</summary>
We present a new algorithm for image segmentation - Level-set KSVD. Level-set KSVD merges the methods of sparse dictionary learning for feature extraction and variational level-set method for image segmentation. Specifically, we use a generalization of the Chan-Vese functional with features learned by KSVD. The motivation for this model is agriculture based. Aerial images are taken in order to detect the spread of fungi in various crops. Our model is tested on such images of cotton fields. The results are compared to other methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的图像分割算法——级别集成KSVD。级别集成KSVD将特征提取的方法和变量水平集成方法结合在一起，用于图像分割。我们使用一种普遍化的 Chan-Vese 函数，其中特征被 KSVD 学习。我们的模型的动机是基于农业的，用于从航空图像中检测不同作物中致病菌的传播。我们的模型在棉田图像上进行测试，与其他方法进行比较。Note:* "级别集成" (level-set) is used to refer to the combination of level-set method and sparse dictionary learning.* "KSVD" (K-Singular Value Decomposition) is used to refer to the sparse dictionary learning method.* " Chan-Vese 函数" (Chan-Vese functional) is used to refer to the energy functional used in the level-set method.
</details></li>
</ul>
<hr>
<h2 id="ARTEMIS-Using-GANs-with-Multiple-Discriminators-to-Generate-Art"><a href="#ARTEMIS-Using-GANs-with-Multiple-Discriminators-to-Generate-Art" class="headerlink" title="ARTEMIS: Using GANs with Multiple Discriminators to Generate Art"></a>ARTEMIS: Using GANs with Multiple Discriminators to Generate Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08278">http://arxiv.org/abs/2311.08278</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Baker</li>
<li>for: 这篇论文是关于生成抽象艺术的新方法的描述。</li>
<li>methods: 论文使用了自动编码器和生成 adversarial neural network (GAN) 来生成抽象艺术作品。自动编码器首先在 VGG 网络中提取了图像的风格表示，然后使用了这些表示来生成新的图像。</li>
<li>results: 论文的实验结果表明，使用这种方法可以生成出具有幻想的、几何化的图像，这些图像具有高度的创新性和多样性。<details>
<summary>Abstract</summary>
We propose a novel method for generating abstract art. First an autoencoder is trained to encode and decode the style representations of images, which are extracted from source images with a pretrained VGG network. Then, the decoder component of the autoencoder is extracted and used as a generator in a GAN. The generator works with an ensemble of discriminators. Each discriminator takes different style representations of the same images, and the generator is trained to create images that create convincing style representations in order to deceive all of the generators. The generator is also trained to maximize a diversity term. The resulting images had a surreal, geometric quality. We call our approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including Self-Attention), as it uses the self-attention layers and an encoder-decoder architecture.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的抽象艺术生成方法。首先，我们使用预训练的VGG网络提取源图像中的风格表示，然后使用自动encoder进行编码和解码。接着，decoder组件被提取出来作为生成器在GAN中使用。生成器与多个批处理器（discriminators）一起工作，每个批处理器接受不同的风格表示，生成器则被训练以创造出可以欺骗所有批处理器的图像。同时，生成器还被训练以最大化多样性项。结果图像具有了具有Surreal的几何特点。我们称之为ARTEMIS（ARTistic Encoder- Multi- Discriminators Including Self-Attention），因为它使用了自我注意层和编码-解码架构。
</details></li>
</ul>
<hr>
<h2 id="Defining-the-boundaries-challenges-and-advances-in-identifying-cells-in-microscopy-images"><a href="#Defining-the-boundaries-challenges-and-advances-in-identifying-cells-in-microscopy-images" class="headerlink" title="Defining the boundaries: challenges and advances in identifying cells in microscopy images"></a>Defining the boundaries: challenges and advances in identifying cells in microscopy images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08269">http://arxiv.org/abs/2311.08269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nodar Gogoberidze, Beth A. Cimini</li>
<li>for: 这篇论文是为了提高微scopic图像中细胞的测量和分析而写的。</li>
<li>methods: 这篇论文使用了深度学习方法来进行图像分割，特别是使用Cellpose模型，以提高准确性和用户友好性。</li>
<li>results: 这篇论文的实验结果表明，使用深度学习方法可以提高图像分割的准确性和效率，并且可以在多种不同的测试数据上达到高度的一致性。<details>
<summary>Abstract</summary>
Segmentation, or the outlining of objects within images, is a critical step in the measurement and analysis of cells within microscopy images. While improvements continue to be made in tools that rely on classical methods for segmentation, deep learning-based tools increasingly dominate advances in the technology. Specialist models such as Cellpose continue to improve in accuracy and user-friendliness, and segmentation challenges such as the Multi-Modality Cell Segmentation Challenge continue to push innovation in accuracy across widely-varying test data as well as efficiency and usability. Increased attention on documentation, sharing, and evaluation standards are leading to increased user-friendliness and acceleration towards the goal of a truly universal method.
</details>
<details>
<summary>摘要</summary>
“分割”或“图像中 объек 的划分”是微scopic 图像分析中的重要步骤。 classical 方法工具的改进仍在继续，但是深度学习基础的工具在技术发展中越来越 dominant。专家模型如 Cellpose 的精度和用户Friendliness 不断提高，分割挑战如多 modal 维度细胞分割挑战也在不同的测试数据上提高精度和效率。 文档、分享和评估标准的增加导致了更高的用户友好性和universal 方法的加速。Note: "多 modal" in the text refers to the fact that the images being analyzed are from multiple sources or modalities, such as brightfield, phase contrast, and fluorescence microscopy.
</details></li>
</ul>
<hr>
<h2 id="TENT-Connect-Language-Models-with-IoT-Sensors-for-Zero-Shot-Activity-Recognition"><a href="#TENT-Connect-Language-Models-with-IoT-Sensors-for-Zero-Shot-Activity-Recognition" class="headerlink" title="TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition"></a>TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08245">http://arxiv.org/abs/2311.08245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunjiao Zhou, Jianfei Yang, Han Zou, Lihua Xie</li>
<li>for: 本研究旨在探讨语言模型是否可以将文本 semantics 与 IoT 感知信号连接起来，以实现认知任务，如人体活动识别（HAR）。</li>
<li>methods: 本研究提出了一种创新的方法——IoT-sEnsors-language alignmEnt pre-Training（TENT），它将文本嵌入与 IoT 感知信号（包括摄像头视频、LiDAR 和 mmWave）的嵌入进行对齐。通过 IoT-语言对照学习，我们 derive 一个共同semantic feature space，使得 IoT 数据与语言嵌入相匹配。</li>
<li>results: TENT 不仅可以识别已经看到的动作，还可以“猜测”未看到的动作，通过最近的文本词汇从共同特征空间中选择。我们在不同感知模式的零例 HAR 任务上达到了state-of-the-art 性能，超过了最佳视language模型的12%。<details>
<summary>Abstract</summary>
Recent achievements in language models have showcased their extraordinary capabilities in bridging visual information with semantic language understanding. This leads us to a novel question: can language models connect textual semantics with IoT sensory signals to perform recognition tasks, e.g., Human Activity Recognition (HAR)? If so, an intelligent HAR system with human-like cognition can be built, capable of adapting to new environments and unseen categories. This paper explores its feasibility with an innovative approach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly aligns textual embeddings with IoT sensor signals, including camera video, LiDAR, and mmWave. Through the IoT-language contrastive learning, we derive a unified semantic feature space that aligns multi-modal features with language embeddings, so that the IoT data corresponds to specific words that describe the IoT data. To enhance the connection between textual categories and their IoT data, we propose supplementary descriptions and learnable prompts that bring more semantic information into the joint feature space. TENT can not only recognize actions that have been seen but also ``guess'' the unseen action by the closest textual words from the feature space. We demonstrate TENT achieves state-of-the-art performance on zero-shot HAR tasks using different modalities, improving the best vision-language models by over 12%.
</details>
<details>
<summary>摘要</summary>
最近的语言模型 achievements 展示了它们在结合视觉信息和语义理解方面的杰出能力。这使我们对一个新的问题感兴趣：可以 ли使用语言模型将文本 semantics 与 IoT 感知信号相连，以实现识别任务，例如人类活动识别（HAR）？如果可以，那么可以构建一个具有人类智能认知的智能 HAR 系统，可以适应新环境和未经见过的类别。这篇文章探索了这种可能性，并提出了一种创新的方法：IoT-sEnsors-language alignmEnt pre-Training（TENT）。TENT 方法将文本嵌入与 IoT 感知信号，包括相机视频、LiDAR 和 mmWave 信号进行同步。通过 IoT-语言对比学习，我们得到了一个统一的Semantic Feature Space，其中 IoT 数据与文本嵌入之间的对应关系得到了确定。为了增强文本类别和其 IoT 数据之间的连接，我们提出了补充描述和可学习的提示。TENT 可以不仅识别已经看过的动作，还可以通过 closest 文本单词来“估计”未经见过的动作。我们示出 TENT 在零shot HAR 任务中实现了state-of-the-art 性能，比最佳视觉语言模型提高了12%以上。
</details></li>
</ul>
<hr>
<h2 id="MeLo-Low-rank-Adaptation-is-Better-than-Fine-tuning-for-Medical-Image-Diagnosis"><a href="#MeLo-Low-rank-Adaptation-is-Better-than-Fine-tuning-for-Medical-Image-Diagnosis" class="headerlink" title="MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image Diagnosis"></a>MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08236">http://arxiv.org/abs/2311.08236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitao Zhu, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin Wang, Xiangyu Zhao, Dinggang Shen, Qian Wang</li>
<li>for: 本研究旨在提出一种单一的计算机辅助诊断（CAD）模型，用于多种临床任务，以降低大型变换器（ViT）模型的资源占用和存储空间。</li>
<li>methods: 该方法使用低级化适应 instead of 资源占用的精细调整，通过固定 ViT 模型的Weight 并只添加小型低级插件，实现了多种鉴定任务中的竞争性表现。</li>
<li>results: 对于四种医疗成像数据集，提出的方法可以与完全精细调整的 ViT 模型具有相似的表现，使用约 0.17% 的可训练参数。此外，MeLo 只增加了约 0.5MB 的存储空间，并允许极快的模型交换和执行。<details>
<summary>Abstract</summary>
The common practice in developing computer-aided diagnosis (CAD) models based on transformer architectures usually involves fine-tuning from ImageNet pre-trained weights. However, with recent advances in large-scale pre-training and the practice of scaling laws, Vision Transformers (ViT) have become much larger and less accessible to medical imaging communities. Additionally, in real-world scenarios, the deployments of multiple CAD models can be troublesome due to problems such as limited storage space and time-consuming model switching. To address these challenges, we propose a new method MeLo (Medical image Low-rank adaptation), which enables the development of a single CAD model for multiple clinical tasks in a lightweight manner. It adopts low-rank adaptation instead of resource-demanding fine-tuning. By fixing the weight of ViT models and only adding small low-rank plug-ins, we achieve competitive results on various diagnosis tasks across different imaging modalities using only a few trainable parameters. Specifically, our proposed method achieves comparable performance to fully fine-tuned ViT models on four distinct medical imaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds only about 0.5MB of storage space and allows for extremely fast model switching in deployment and inference. Our source code and pre-trained weights are available on our website (https://absterzhu.github.io/melo.github.io/).
</details>
<details>
<summary>摘要</summary>
通常，在基于转换器架构的计算机辅助诊断（CAD）模型开发中，会进行 ImageNet 预训练权重的细化。然而，随着大规模预训练的进步和做法的做法，视觉转换器（ViT）已经变得更加大型，并且对医疗影像社区而言更加不可accessible。此外，在实际应用场景中，多个 CAD 模型的部署可能会陷入有限的存储空间和时间consuming的模型交换问题。为解决这些挑战，我们提出了一新方法 MeLo（医疗影像低级化），它允许开发单一的 CAD 模型，用于多种临床任务，并且具有轻量级的特点。它采用低级化adaptation而不是资源占用的细化。通过固定 ViT 模型的weight和只添加小型低级插件，我们实现了在不同的医疗影像模式上多种诊断任务中的竞争性成绩，使用只有约 0.17% 的可训练参数。此外，MeLo 只增加了约 0.5MB 的存储空间，并允许在部署和推理过程中非常快速的模型交换。我们的源代码和预训练 веса可以在我们的网站（https://absterzhu.github.io/melo.github.io/）上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Approach-for-Comprehensive-Analysis-of-Various-Spectral-and-Tissue-Doppler-Echocardiography"><a href="#A-Unified-Approach-for-Comprehensive-Analysis-of-Various-Spectral-and-Tissue-Doppler-Echocardiography" class="headerlink" title="A Unified Approach for Comprehensive Analysis of Various Spectral and Tissue Doppler Echocardiography"></a>A Unified Approach for Comprehensive Analysis of Various Spectral and Tissue Doppler Echocardiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08439">http://arxiv.org/abs/2311.08439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeik Jeon, Jiyeon Kim, Yeonggul Jang, Yeonyee E. Yoon, Dawun Jeong, Youngtaek Hong, Seung-Ah Lee, Hyuk-Jae Chang</li>
<li>for: 这个论文的目的是提供一种全面的Doppler echo医学图像分析框架，以便自动测量Doppler图像和诊断心脏功能。</li>
<li>methods: 该框架使用 convolutional neural network (CNN) 来自动识别Doppler图像中的关键特征，并使用新的Doppler形态嵌入和抑制锈模块来提高解释和保证一致性。</li>
<li>results: 对比其他方法，该框架在性能指标（如 dice similarity coefficients (DSC) 和 intersection over union (IoU)）中具有显著的优势，并与临床医生的诊断相一致。<details>
<summary>Abstract</summary>
Doppler echocardiography offers critical insights into cardiac function and phases by quantifying blood flow velocities and evaluating myocardial motion. However, previous methods for automating Doppler analysis, ranging from initial signal processing techniques to advanced deep learning approaches, have been constrained by their reliance on electrocardiogram (ECG) data and their inability to process Doppler views collectively. We introduce a novel unified framework using a convolutional neural network for comprehensive analysis of spectral and tissue Doppler echocardiography images that combines automatic measurements and end-diastole (ED) detection into a singular method. The network automatically recognizes key features across various Doppler views, with novel Doppler shape embedding and anti-aliasing modules enhancing interpretation and ensuring consistent analysis. Empirical results indicate a consistent outperformance in performance metrics, including dice similarity coefficients (DSC) and intersection over union (IoU). The proposed framework demonstrates strong agreement with clinicians in Doppler automatic measurements and competitive performance in ED detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Uni-COAL-A-Unified-Framework-for-Cross-Modality-Synthesis-and-Super-Resolution-of-MR-Images"><a href="#Uni-COAL-A-Unified-Framework-for-Cross-Modality-Synthesis-and-Super-Resolution-of-MR-Images" class="headerlink" title="Uni-COAL: A Unified Framework for Cross-Modality Synthesis and Super-Resolution of MR Images"></a>Uni-COAL: A Unified Framework for Cross-Modality Synthesis and Super-Resolution of MR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08225">http://arxiv.org/abs/2311.08225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyun Song, Zengxin Qi, Xin Wang, Xiangyu Zhao, Zhenrong Shen, Sheng Wang, Manman Fei, Zhe Wang, Di Zang, Dongdong Chen, Linlin Yao, Qian Wang, Xuehai Wu, Lichi Zhang</li>
<li>for: 这paper的目标是提高MRI图像的Synthesis和分辨率，并且可以适应多种临床应用场景。</li>
<li>methods: 这paper使用了一种统一的网络来实现CMS、SR和CMSR等多种图像Synthesis任务，并且采用了协调增强和随机特征表示来保证模式和分辨率的一致性。</li>
<li>results: 实验结果表明，Uni-COAL在CMS、SR和CMSR等任务中表现出色，超过了现有的方法，这反映了其在多种应用场景中的一致性和通用性。<details>
<summary>Abstract</summary>
Cross-modality synthesis (CMS), super-resolution (SR), and their combination (CMSR) have been extensively studied for magnetic resonance imaging (MRI). Their primary goals are to enhance the imaging quality by synthesizing the desired modality and reducing the slice thickness. Despite the promising synthetic results, these techniques are often tailored to specific tasks, thereby limiting their adaptability to complex clinical scenarios. Therefore, it is crucial to build a unified network that can handle various image synthesis tasks with arbitrary requirements of modality and resolution settings, so that the resources for training and deploying the models can be greatly reduced. However, none of the previous works is capable of performing CMS, SR, and CMSR using a unified network. Moreover, these MRI reconstruction methods often treat alias frequencies improperly, resulting in suboptimal detail restoration. In this paper, we propose a Unified Co-Modulated Alias-free framework (Uni-COAL) to accomplish the aforementioned tasks with a single network. The co-modulation design of the image-conditioned and stochastic attribute representations ensures the consistency between CMS and SR, while simultaneously accommodating arbitrary combinations of input/output modalities and thickness. The generator of Uni-COAL is also designed to be alias-free based on the Shannon-Nyquist signal processing framework, ensuring effective suppression of alias frequencies. Additionally, we leverage the semantic prior of Segment Anything Model (SAM) to guide Uni-COAL, ensuring a more authentic preservation of anatomical structures during synthesis. Experiments on three datasets demonstrate that Uni-COAL outperforms the alternatives in CMS, SR, and CMSR tasks for MR images, which highlights its generalizability to wide-range applications.
</details>
<details>
<summary>摘要</summary>
跨Modalities合成（CMS）、超解像（SR）以及它们的组合（CMSR）在核磁共振成像（MRI）中已经得到了广泛的研究。它们的主要目标是提高成像质量，同时降低slice thickness。尽管这些技术在合成结果方面具有惊人的成果，但它们往往是为特定任务而设计的，因此它们在复杂的临床场景中的适应性受到限制。因此，建立一个通用的网络，可以处理多种图像合成任务，并且可以根据不同的模式和分辨率设置进行调整，这样就可以大幅减少训练和部署模型的资源。然而，现有的工作都不能通过单一的网络来完成CMS、SR和CMSR等任务。此外，这些MRI重建方法经常不当处理假频谱，导致细节的还原不佳。在这篇论文中，我们提出了一种统一的Co-Modulated Alias-free框架（Uni-COAL），用于实现以上任务。图像受控和随机特征表示的协调设计，保证了CMS和SR之间的一致性，同时能够适应任意的输入/输出模式和厚度。Uni-COAL生成器还基于Shannon-Nyquist信号处理框架，确保了假频谱的有效抑制。此外，我们利用Segment Anything Model（SAM）的semantic priors来引导Uni-COAL，确保在合成过程中保留了更加Authentic的结构。实验结果表明，Uni-COAL在CMS、SR和CMSR等任务中表现出色，高于相关方法，这 highlights its generalizability to wide-range applications。
</details></li>
</ul>
<hr>
<h2 id="Improving-Image-Captioning-via-Predicting-Structured-Concepts"><a href="#Improving-Image-Captioning-via-Predicting-Structured-Concepts" class="headerlink" title="Improving Image Captioning via Predicting Structured Concepts"></a>Improving Image Captioning via Predicting Structured Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08223">http://arxiv.org/abs/2311.08223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangting0/SCP-WGCN">https://github.com/wangting0/SCP-WGCN</a></li>
<li>paper_authors: Ting Wang, Weidong Chen, Yuanhe Tian, Yan Song, Zhendong Mao</li>
<li>for: This paper aims to improve image captioning performance by bridging the semantic gap between images and texts using structured concept prediction and weighted graph convolutional networks (W-GCN).</li>
<li>methods: The proposed approach includes a structured concept predictor (SCP) to predict concepts and their structures, as well as W-GCN to depict concept relations driven by word dependencies.</li>
<li>results: The approach is shown to be effective in enhancing the contribution of visual signals in image captioning, and the learned differentiated contributions from concepts improve the description generation process. Extensive experiments demonstrate the effectiveness of the proposed approach and each module.Here’s the Chinese version of the three pieces of information:</li>
<li>for: 这篇论文目的是通过结构化概念预测和Weighted Graph Convolutional Networks (W-GCN)来bridging图像和文本之间的semantic gap，以提高图像描述性能。</li>
<li>methods: 提议的方法包括结构化概念预测器(SCP)来预测概念和其结构，以及基于word dependency的W-GCN来表示概念关系。</li>
<li>results: 方法被证明可以增强图像描述中视觉信号的贡献，并且通过 learned differentiated contributions from concepts来改善描述生成过程。广泛的实验结果证明提议的方法和每个模块的效果。<details>
<summary>Abstract</summary>
Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly. Although promising results on concept prediction were obtained, the aforementioned studies normally ignore the relationship among concepts, which relies on not only objects in the image, but also word dependencies in the text, so that offers a considerable potential for improving the process of generating good descriptions. In this paper, we propose a structured concept predictor (SCP) to predict concepts and their structures, then we integrate them into captioning, so as to enhance the contribution of visual signals in this task via concepts and further use their relations to distinguish cross-modal semantics for better description generation. Particularly, we design weighted graph convolutional networks (W-GCN) to depict concept relations driven by word dependencies, and then learns differentiated contributions from these concepts for following decoding process. Therefore, our approach captures potential relations among concepts and discriminatively learns different concepts, so that effectively facilitates image captioning with inherited information across modalities. Extensive experiments and their results demonstrate the effectiveness of our approach as well as each proposed module in this work.
</details>
<details>
<summary>摘要</summary>
在图像描述 зада务中解决semantic gap问题的困难性，传统研究具有一定的注意力于将semantic concept作为两种模态之间的桥梁，从而改进描述性能。虽然在概念预测方面获得了promising的结果，但这些研究通常忽略了概念之间的关系，这些关系不仅取决于图像中的对象，还取决于文本中的word依赖关系，这意味着可以在描述生成过程中提高visuallsignal的贡献。在这篇论文中，我们提出了结构化概念预测器（SCP），用于预测概念和其结构，然后将其集成到描述中，以增强图像描述中visuallsignal的贡献。特别是，我们设计了weighted graph convolutional networks（W-GCN），用于描述概念之间的关系，然后学习这些概念之间的差异性，以便在后续的解码过程中更好地分配权重。因此，我们的方法能够捕捉概念之间的关系，并且强化不同的概念之间的差异性，从而有效地促进图像描述 tasks。广泛的实验和结果证明了我们的方法和每个提议模块的有效性。
</details></li>
</ul>
<hr>
<h2 id="Peer-is-Your-Pillar-A-Data-unbalanced-Conditional-GANs-for-Few-shot-Image-Generation"><a href="#Peer-is-Your-Pillar-A-Data-unbalanced-Conditional-GANs-for-Few-shot-Image-Generation" class="headerlink" title="Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation"></a>Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08217">http://arxiv.org/abs/2311.08217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iceli1007/PIP">https://github.com/iceli1007/PIP</a></li>
<li>paper_authors: Ziqiang Li, Chaoyue Wang, Xue Rui, Chao Xue, Jiaxu Leng, Bin Li<br>for:* 实现几少数例的图像生成，专门为缺乏训练图像的情况。methods:* 融合目标几少数例数据集和对应的同类数据集，实现数据不均匀的条件生成。* 使用组别嵌入法分离类别空间和内生空间，并使用预训练CLIP的方向损失提高图像多样性。results:* 在不同的几少数例数据集上进行实验，提出了一种名为Peer is your Pillar（PIP）的新管道，可以实现几少数例图像生成，并且降低了训练需求。<details>
<summary>Abstract</summary>
Few-shot image generation aims to train generative models using a small number of training images. When there are few images available for training (e.g. 10 images), Learning From Scratch (LFS) methods often generate images that closely resemble the training data while Transfer Learning (TL) methods try to improve performance by leveraging prior knowledge from GANs pre-trained on large-scale datasets. However, current TL methods may not allow for sufficient control over the degree of knowledge preservation from the source model, making them unsuitable for setups where the source and target domains are not closely related. To address this, we propose a novel pipeline called Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer dataset to create a data-unbalanced conditional generation. Our approach includes a class embedding method that separates the class space from the latent space, and we use a direction loss based on pre-trained CLIP to improve image diversity. Experiments on various few-shot datasets demonstrate the advancement of the proposed PIP, especially reduces the training requirements of few-shot image generation.
</details>
<details>
<summary>摘要</summary>
几个干净图像生成目标是训练生成模型使用几个训练图像。当有很少图像可用 для训练（例如10个图像）时，学习从零（LFS）方法通常生成图像与训练数据非常相似，而传输学习（TL）方法尝试通过利用大规模数据集中的先前知识来提高性能。然而，当目标和源领域不相关时，当前的TL方法可能无法保持来自源模型的知识水平，使其不适用。为解决这个问题，我们提议一个新的管道，即同伴是你的柱子（PIP），它将目标几个干净数据集与一个对等数据集组合起来，以创建数据不均衡的条件生成。我们的方法包括一种类嵌入方法，可以分离类空间与潜在空间，并使用预训练的CLIP来提高图像多样性。在各种几个干净数据集上进行了实验，我们发现提案的PIP有显著的进步，特别是减少了几个干净图像生成的训练要求。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-generation-of-Histopathological-Whole-Slide-Images-at-a-Gigapixel-scale"><a href="#Diffusion-based-generation-of-Histopathological-Whole-Slide-Images-at-a-Gigapixel-scale" class="headerlink" title="Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale"></a>Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08199">http://arxiv.org/abs/2311.08199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Harb, Thomas Pock, Heimo Müller</li>
<li>for: 本研究开发了一种基于扩散的方法，用于生成高分辨率的人类组织学标本数据（Whole Slide Images，WSIs），以增强计算机生物学应用程序的性能。</li>
<li>methods: 本研究使用了一种从粗糙到细节的样本过滤方法，将初始低分辨率的图像逐步升级为高分辨率WSIs。 Specifically, a diffusion model sequentially adds fine details to images and increases their resolution.</li>
<li>results: 在实验中，我们将方法训练使用了TCGA-BRCA数据集。在实验中，我们通过量值评估和用户研究发现，生成的WSIs与真实的标本构造有相似之处。<details>
<summary>Abstract</summary>
We present a novel diffusion-based approach to generate synthetic histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel scale. Synthetic WSIs have many potential applications: They can augment training datasets to enhance the performance of many computational pathology applications. They allow the creation of synthesized copies of datasets that can be shared without violating privacy regulations. Or they can facilitate learning representations of WSIs without requiring data annotations. Despite this variety of applications, no existing deep-learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的扩散基于的方法，用于生成高分辨率整个染色质影像（WSIs）。这些合成WSIs具有许多应用前景：它们可以补充训练集，提高计算生物学应用程序的性能。它们允许创建合成的数据集，无需违反隐私法规。还有，它们可以帮助学习WSIs的表示，无需数据标注。Despite this variety of applications, no existing deep learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs.Here's the word-for-word translation of the text into Simplified Chinese:我们提出了一种新的扩散基于的方法，用于生成高分辨率整个染色质影像（WSIs）。这些合成WSIs具有许多应用前景：它们可以补充训练集，提高计算生物学应用程序的性能。它们允许创建合成的数据集，无需违反隐私法规。还有，它们可以帮助学习WSIs的表示，无需数据标注。Despite this variety of applications, no existing deep learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs.
</details></li>
</ul>
<hr>
<h2 id="LocaliseBot-Multi-view-3D-object-localisation-with-differentiable-rendering-for-robot-grasping"><a href="#LocaliseBot-Multi-view-3D-object-localisation-with-differentiable-rendering-for-robot-grasping" class="headerlink" title="LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping"></a>LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08438">http://arxiv.org/abs/2311.08438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujal Vijayaraghavan, Redwan Alqasemi, Rajiv Dubey, Sudeep Sarkar</li>
<li>for: 本文主要针对对象抓取领域，具体来说是对象pose估计。</li>
<li>methods: 本文使用了多视图对象检测、Camera参数估计和3D CAD模型来实现对象pose估计。一个标准的深度学习底层（FCN ResNet）用于估计对象标签、semantic segmentation和相对于摄像头的对象 pose的course estimate。然后使用一个改进模块来从course pose estimate中进行优化，通过可导渲染来实现。</li>
<li>results: 在ShapeNet dataset上，本文的对象pose估计方法与状态体系比较，显示出提高。此外，通过使用Estimated object pose结果和实际的抓取候选点，在OCID Grasp dataset上计算的抓取精度为99.65%。<details>
<summary>Abstract</summary>
Robot grasp typically follows five stages: object detection, object localisation, object pose estimation, grasp pose estimation, and grasp planning. We focus on object pose estimation. Our approach relies on three pieces of information: multiple views of the object, the camera's extrinsic parameters at those viewpoints, and 3D CAD models of objects. The first step involves a standard deep learning backbone (FCN ResNet) to estimate the object label, semantic segmentation, and a coarse estimate of the object pose with respect to the camera. Our novelty is using a refinement module that starts from the coarse pose estimate and refines it by optimisation through differentiable rendering. This is a purely vision-based approach that avoids the need for other information such as point cloud or depth images. We evaluate our object pose estimation approach on the ShapeNet dataset and show improvements over the state of the art. We also show that the estimated object pose results in 99.65% grasp accuracy with the ground truth grasp candidates on the Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using standard practice.
</details>
<details>
<summary>摘要</summary>
Robot grasp通常包括五个阶段：对象检测、对象定位、对象姿态估计、抓取姿态估计和抓取规划。我们专注于对象姿态估计。我们的方法基于三个信息：对象的多视图、摄像头的外部参数和3D CAD模型。第一步使用标准的深度学习基础结构（FCN ResNet）来估计对象标签、semantic segmentation和相对于摄像头的粗略对象姿态。我们的创新是使用改进模块，从粗略姿态估计开始，通过微调Rendering来优化。这是一种完全视觉基于的方法，不需要其他信息如点云或深度图像。我们在ShapeNet数据集上评估了我们的对象姿态估计方法，并表明我们的方法在比较准确性方面有所改进。此外，我们还证明我们估计的对象姿态结果和实际的 grasp candidates在OCID Grasp数据集上的抓取精度为99.65%，如标准实践所计算。
</details></li>
</ul>
<hr>
<h2 id="SAMIHS-Adaptation-of-Segment-Anything-Model-for-Intracranial-Hemorrhage-Segmentation"><a href="#SAMIHS-Adaptation-of-Segment-Anything-Model-for-Intracranial-Hemorrhage-Segmentation" class="headerlink" title="SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation"></a>SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08190">http://arxiv.org/abs/2311.08190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mileswyn/samihs">https://github.com/mileswyn/samihs</a></li>
<li>paper_authors: Yinuo Wang, Kai Chen, Weimin Yuan, Cai Meng, XiangZhi Bai<br>for: 这篇论文是针对stroke诊断和手术规划中的脑出血分类进行研究，使用Segment Anything Model (SAM) 作为基础模型，并提出了一种基于SAM的优化方法（SAMIHS），以提高这些类型的医疗影像分类效能。methods: 这篇论文使用了SAM的图像嵌入器中的参数适束器（parameter-refactoring adapters），并将其视为可变的参数，以提高SAM的灵活性和效能。此外，这篇论文还使用了一种混合损失函数（combo loss），其结合了二进制条件预测损失和边界敏感损失，以提高SAMIHS的边界区域识别能力。results: 根据实验结果，SAMIHS在两个公共数据集上的效能都得到了改善，尤其是在脑出血类型的医疗影像分类中，表明SAMIHS可以提高这些类型的医疗影像分类效能。<details>
<summary>Abstract</summary>
Segment Anything Model (SAM), a vision foundation model trained on large-scale annotations, has recently continued raising awareness within medical image segmentation. Despite the impressive capabilities of SAM on natural scenes, it struggles with performance decline when confronted with medical images, especially those involving blurry boundaries and highly irregular regions of low contrast. In this paper, a SAM-based parameter-efficient fine-tuning method, called SAMIHS, is proposed for intracranial hemorrhage segmentation, which is a crucial and challenging step in stroke diagnosis and surgical planning. Distinguished from previous SAM and SAM-based methods, SAMIHS incorporates parameter-refactoring adapters into SAM's image encoder and considers the efficient and flexible utilization of adapters' parameters. Additionally, we employ a combo loss that combines binary cross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability to recognize the boundary regions. Our experimental results on two public datasets demonstrate the effectiveness of our proposed method. Code is available at https://github.com/mileswyn/SAMIHS .
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 模型，一种基于大规模注释的视觉基础模型，最近在医学图像 segmentation 中受到了更多的关注。尽管 SAM 在自然场景中表现出色，但在医学图像中，它的表现却会逐渐下降，特别是面临着模糊的边界和低对比度的区域。在这篇论文中，我们提出了基于 SAM 的参数效率调整方法，称为 SAMIHS，用于脑出血栓 segmentation，这是诊断和手术规划中的关键步骤。与之前的 SAM 和基于 SAM 的方法不同，SAMIHS 在 SAM 的图像编码器中添加了参数 refactoring 适配器，并且利用这些适配器的参数进行有效和灵活的使用。此外，我们采用了一种 combio 损失函数，该函数将 binary cross-entropy 损失函数和边界敏感损失函数相加，以提高 SAMIHS 对边界区域的识别能力。我们在两个公共数据集上进行了实验，结果表明了我们提出的方法的有效性。代码可以在 GitHub 上找到：https://github.com/mileswyn/SAMIHS。
</details></li>
</ul>
<hr>
<h2 id="A-deformation-based-morphometry-framework-for-disentangling-Alzheimer’s-disease-from-normal-aging-using-learned-normal-aging-templates"><a href="#A-deformation-based-morphometry-framework-for-disentangling-Alzheimer’s-disease-from-normal-aging-using-learned-normal-aging-templates" class="headerlink" title="A deformation-based morphometry framework for disentangling Alzheimer’s disease from normal aging using learned normal aging templates"></a>A deformation-based morphometry framework for disentangling Alzheimer’s disease from normal aging using learned normal aging templates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08176">http://arxiv.org/abs/2311.08176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fjr9516/dbm_with_dl">https://github.com/fjr9516/dbm_with_dl</a></li>
<li>paper_authors: Jingru Fu, Daniel Ferreira, Örjan Smedby, Rodrigo Moreno</li>
<li>for: 这个研究旨在解决阿尔茨海默症和正常老化是否存在加速的问题，以及在临床上如何分解阿尔茨海默症和正常老化的混合影响。</li>
<li>methods: 该研究使用了深度学习算法创建年龄相关的模板，以便在MRI扫描图像上量化正常老化和阿尔茨海默症相关的衰老模式。然后，使用了 diffeomorphic registration 来估算一年一个CNSubject的正常老化模式，并将测试图像与60岁CNSubject的模板进行对alignment。最后，通过比较这个对alignment与一年正常老化模式的对比，计算出正常老化和阿尔茨海默症特有的分数。</li>
<li>results: 研究结果表明，脑 Ventricles 主要遵循加速的正常老化模式，而 hippocampus 和 Amygdala 区域受到了正常老化和阿尔茨海默症特有的影响。 Interestingly，在疾病早期临床阶段，hippocampus 和 Amygdala 区域更加受到加速的正常老化影响，而疾病后期，阿尔茨海默症特有的分数增加。<details>
<summary>Abstract</summary>
Alzheimer's Disease and normal aging are both characterized by brain atrophy. The question of whether AD-related brain atrophy represents accelerated aging or a neurodegeneration process distinct from that in normal aging remains unresolved. Moreover, precisely disentangling AD-related brain atrophy from normal aging in a clinical context is complex. In this study, we propose a deformation-based morphometry framework to estimate normal aging and AD-specific atrophy patterns of subjects from morphological MRI scans. We first leverage deep-learning-based methods to create age-dependent templates of cognitively normal (CN) subjects. These templates model the normal aging atrophy patterns in a CN population. Then, we use the learned diffeomorphic registration to estimate the one-year normal aging pattern at the voxel level. We register the testing image to the 60-year-old CN template in the second step. Finally, normal aging and AD-specific scores are estimated by measuring the alignment of this registration with the one-year normal aging pattern. The methodology was developed and evaluated on the OASIS3 dataset with 1,014 T1-weighted MRI scans. Of these, 326 scans were from CN subjects, and 688 scans were from individuals clinically diagnosed with AD at different stages of clinical severity defined by clinical dementia rating (CDR) scores. The results show that ventricles predominantly follow an accelerated normal aging pattern in subjects with AD. In turn, hippocampi and amygdala regions were affected by both normal aging and AD-specific factors. Interestingly, hippocampi and amygdala regions showed more of an accelerated normal aging pattern for subjects during the early clinical stages of the disease, while the AD-specific score increases in later clinical stages. Our code is freely available at https://github.com/Fjr9516/DBM_with_DL.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病和正常年龄都 caracterized by brain atrophy。问题是whether AD-related brain atrophy represents accelerated aging or a neurodegeneration process distinct from that in normal aging remains unresolved。另外，在临床上准确地分离AD-related brain atrophy from normal aging是复杂的。在这种研究中，我们提议一种基于几何变换的 morphometry框架，用于估计在MRI扫描中的正常年龄和AD-specific atrophy模式。我们首先利用深度学习基本的方法创建年龄相关的模板，以模型正常年龄atrophy模式。然后，我们使用学习的射影变换来估计一年内正常年龄的变化模式。最后，我们测量这个注册与一年内正常年龄的变化模式之间的匹配程度，以计算正常年龄和AD-specific scores。方法在OASIS3 dataset上进行了开发和评估，该dataset包括1,014个T1-weighted MRI扫描，其中326个来自正常年龄 subjects，688个来自AD诊断的个体。结果显示，脑室主要follows an accelerated normal aging pattern in subjects with AD。而hippocampus和 Amygdala region受到了正常年龄和AD-specific factor的影响。具有诊断CDR scores的患者在早期клиниче阶段，hippocampus和Amygdala region更加受到加速的正常年龄变化，而AD-specific score在后期 клиниче阶段增加。我们的代码可以在https://github.com/Fjr9516/DBM_with_DL上获取。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Instruction-Tuning-A-Review-and-Analysis"><a href="#Vision-Language-Instruction-Tuning-A-Review-and-Analysis" class="headerlink" title="Vision-Language Instruction Tuning: A Review and Analysis"></a>Vision-Language Instruction Tuning: A Review and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08172">http://arxiv.org/abs/2311.08172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/palchenli/vl-instruction-tuning">https://github.com/palchenli/vl-instruction-tuning</a></li>
<li>paper_authors: Chen Li, Yixiao Ge, Dian Li, Ying Shan</li>
<li>for: 这篇论文的目的是探讨大语言模型（LLMs）在多模态数据 incorporation 中的指令调整过程，以提高 LLMS 的指令执行泛化和用户喜好适应能力。</li>
<li>methods: 这篇论文系统地审视了最新的视觉语言指令调整设置和数据集在多模态 LLMs 中，并总结了高质量视觉语言调整数据的特征。</li>
<li>results: 该论文基于 constructed 的指令数据进行了视觉语言指令调整，并在相应的 метриках上进行了广泛的实验，以证明提出的建设原则的合理性。<details>
<summary>Abstract</summary>
Instruction tuning is an essential supervised training phase for Large Language Models (LLMs), with the goal of enhancing LLMs' capacity to generalize instruction execution and adapt to user preferences. With the growing incorporation of multi-modal data into LLMs, there is an increasing interest in the performance of vision-language instruction tuning which presents more complex features in comparison to pure text instructions. In this paper, we systematically review the latest vision-language instruction tuning settings and datasets in multi-modal LLMs and summarize the characteristics that high-quality vision-language tuning data should have. We consider these characteristics as the foundational principles for constructing vision-language instruction data and propose a complete construction pipeline consisting of data collection, instruction generation, and quality control modules that incorporate meticulously designed instruction property evaluation indicators. We perform vision-language instruction tuning on three widely used multi-modal LLMs based on the instruction data we constructed and conduct extensive experiments on the corresponding metrics to demonstrate the rationality of the construction principles proposed in this paper. The code and dataset related to this paper have been open-sourced at \url{https://github.com/palchenli/VL-Instruction-Tuning}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的指令调整是一个重要的有监督训练阶段，目的是增强LLM的指令执行泛化和用户偏好适应能力。随着多模态数据的加入，视觉语言指令调整的性能已经引起了更多的关注，这种多模态指令调整比纯文本指令更加复杂。在这篇论文中，我们系统地回顾最新的视觉语言指令调整设置和数据集在多模态LLM中，并总结高质量视觉语言调整数据应该具备哪些特征。我们认为这些特征是建构视觉语言指令数据的基础原则，我们提议一个完整的建构管道，包括数据采集、指令生成和质量控制模块，这些模块都包括了仔细设计的指令性质评价指标。我们在三种广泛使用的多模态LLM上进行了视觉语言指令调整，并对相应的指标进行了广泛的实验，以示我们提出的建构原则的合理性。相关代码和数据集可以在 \url{https://github.com/palchenli/VL-Instruction-Tuning} 上下载。
</details></li>
</ul>
<hr>
<h2 id="DynamicSurf-Dynamic-Neural-RGB-D-Surface-Reconstruction-with-an-Optimizable-Feature-Grid"><a href="#DynamicSurf-Dynamic-Neural-RGB-D-Surface-Reconstruction-with-an-Optimizable-Feature-Grid" class="headerlink" title="DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid"></a>DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08159">http://arxiv.org/abs/2311.08159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Mirgahney/DynamicSurf.io">https://github.com/Mirgahney/DynamicSurf.io</a></li>
<li>paper_authors: Mirgahney Mohamed, Lourdes Agapito</li>
<li>For: 高精度3D模型化非rigid表面从单视图RGB-D视频中。* Methods: 使用深度、表面法向和RGB损失来提高重建准确性和优化时间。* Results: 比前一代方法快$6\times$，并 achieved comparable results to the state-of-the-art methods。In English:* For: High-fidelity 3D modeling of non-rigid surfaces from monocular RGB-D video.* Methods: Using depth, surface normals, and RGB losses to improve reconstruction accuracy and optimization time.* Results: Faster than previous methods by $6\times$ and achieved comparable results to the state-of-the-art methods.<details>
<summary>Abstract</summary>
We propose DynamicSurf, a model-free neural implicit surface reconstruction method for high-fidelity 3D modelling of non-rigid surfaces from monocular RGB-D video. To cope with the lack of multi-view cues in monocular sequences of deforming surfaces, one of the most challenging settings for 3D reconstruction, DynamicSurf exploits depth, surface normals, and RGB losses to improve reconstruction fidelity and optimisation time. DynamicSurf learns a neural deformation field that maps a canonical representation of the surface geometry to the current frame. We depart from current neural non-rigid surface reconstruction models by designing the canonical representation as a learned feature grid which leads to faster and more accurate surface reconstruction than competing approaches that use a single MLP. We demonstrate DynamicSurf on public datasets and show that it can optimize sequences of varying frames with $6\times$ speedup over pure MLP-based approaches while achieving comparable results to the state-of-the-art methods. Project is available at https://mirgahney.github.io//DynamicSurf.io/.
</details>
<details>
<summary>摘要</summary>
我们提出了DynamicSurf，一种无模型 neural implicit surface reconstruction方法，用于高精度3D模型化非rigid表面从单视角RGB-D视频中。为了处理单视角序列中的形变表面的缺乏多视角cue，DynamicSurf利用深度、表面法向和RGB损失来提高重建准确性和优化时间。DynamicSurf学习一个神经变形场，将一个均匀表面几何代表映射到当前帧中。我们与现有的神经非RIGID表面重建模型不同，我们设计了学习的特征网格作为均匀表面几何代表，这导致了更快和更准确的表面重建。我们在公共数据集上展示了DynamicSurf的性能，并证明它可以在不同的帧序列中优化6倍速度于纯MLP-based方法，而且与状态艺术方法相当。项目可以在https://mirgahney.github.io//DynamicSurf.io/查看。
</details></li>
</ul>
<hr>
<h2 id="Rethink-Cross-Modal-Fusion-in-Weakly-Supervised-Audio-Visual-Video-Parsing"><a href="#Rethink-Cross-Modal-Fusion-in-Weakly-Supervised-Audio-Visual-Video-Parsing" class="headerlink" title="Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing"></a>Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08151">http://arxiv.org/abs/2311.08151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yating Xu, Conghui Hu, Gim Hee Lee</li>
<li>for: 这篇论文的目的是提出一种新的弱监督音视频分割方法，以解决现有的混合注意力网络（HAN）在融合多Modal embedding时存在聚合不准确的问题。</li>
<li>methods: 该方法使用了一种名为“消息导向中 fusion transformer”的新的嵌入方法，以减少跨Modal不相关的上下文。此外，它还提出了跨音频预测一致性来降低视频预测中不相关的音频信息的影响。</li>
<li>results: 实验表明，该方法在与现有状态的方法进行比较时表现出色，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
Existing works on weakly-supervised audio-visual video parsing adopt hybrid attention network (HAN) as the multi-modal embedding to capture the cross-modal context. It embeds the audio and visual modalities with a shared network, where the cross-attention is performed at the input. However, such an early fusion method highly entangles the two non-fully correlated modalities and leads to sub-optimal performance in detecting single-modality events. To deal with this problem, we propose the messenger-guided mid-fusion transformer to reduce the uncorrelated cross-modal context in the fusion. The messengers condense the full cross-modal context into a compact representation to only preserve useful cross-modal information. Furthermore, due to the fact that microphones capture audio events from all directions, while cameras only record visual events within a restricted field of view, there is a more frequent occurrence of unaligned cross-modal context from audio for visual event predictions. We thus propose cross-audio prediction consistency to suppress the impact of irrelevant audio information on visual event prediction. Experiments consistently illustrate the superior performance of our framework compared to existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的弱监睹音视频分解方法采用混合注意网络（HAN）作为多modal嵌入，以便捕捉音视模态之间的交叉模态上下文。HAN将音和视模态embedded在共享网络中，并在输入阶段进行交叉注意。然而，这种早期融合方法会高度束缚两个不完全相关的模态，从而降低单模态事件检测的性能。为解决这个问题，我们提出了使者导向中间融合变换器，以减少不相关的交叉模态上下文。使者将全模态上下文压缩到一个紧凑的表示中，只保留有用的交叉模态信息。此外，由于 Microphones 捕捉的音频事件来自所有方向，而 Camera 只记录视频事件在限定的视野内，因此在视频事件预测中更常出现不同模态上下文的不一致。我们因此提出了跨音频预测一致性，以抑制不关联的音频信息对视频事件预测的影响。实验表明，我们的框架在现有状态艺术方法之上具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="GMTR-Graph-Matching-Transformers"><a href="#GMTR-Graph-Matching-Transformers" class="headerlink" title="GMTR: Graph Matching Transformers"></a>GMTR: Graph Matching Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08141">http://arxiv.org/abs/2311.08141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinpei Guo, Shaofeng Zhang, Runzhong Wang, Chang Liu, Junchi Yan</li>
<li>for: 这个论文旨在探讨如何使用视transformer（ViTs）进行视觉匹配，并且提出了一种新的中心裁剪策略和交叉注意机制来提高视觉匹配的敏感性。</li>
<li>methods: 该论文提出了一种名为QueryTrans（Query Transformer）的新方法，该方法使用了交叉注意机制和中心裁剪策略来更好地提取视觉信息。此外，论文还提出了一种基于图注意机制的转化器-based图匹配方法（GMTR），用于解决视觉匹配中的 combinatorial 问题。</li>
<li>results: 根据标准的GM benchmarks，GMTR在对比SOTA框架时显示出竞争力的性能，具体来说，在Pascal VOC上，GMTR的准确率为83.6%，高于SOTA框架的0.9%。在Spair-71k上，GMTR也表现出了优异的性能，并超越了大多数之前的works。此外，QueryTrans在Pascal VOC上提高了NGMv2的准确率从80.1%到83.3%，并提高了BBGM的准确率从79.0%到84.5%。在Spair-71k上，QueryTrans也提高了NGMv2的准确率从80.6%到82.5%，并提高了BBGM的准确率从82.1%到83.9%。<details>
<summary>Abstract</summary>
Vision transformers (ViTs) have recently been used for visual matching beyond object detection and segmentation. However, the original grid dividing strategy of ViTs neglects the spatial information of the keypoints, limiting the sensitivity to local information. Therefore, we propose \textbf{QueryTrans} (Query Transformer), which adopts a cross-attention module and keypoints-based center crop strategy for better spatial information extraction. We further integrate the graph attention module and devise a transformer-based graph matching approach \textbf{GMTR} (Graph Matching TRansformers) whereby the combinatorial nature of GM is addressed by a graph transformer neural GM solver. On standard GM benchmarks, GMTR shows competitive performance against the SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves $\mathbf{83.6\%}$ accuracy, $\mathbf{0.9\%}$ higher than the SOTA framework. On Spair-71k, GMTR shows great potential and outperforms most of the previous works. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from $80.1\%$ to $\mathbf{83.3\%}$, and BBGM from $79.0\%$ to $\mathbf{84.5\%}$. On Spair-71k, QueryTrans improves NGMv2 from $80.6\%$ to $\mathbf{82.5\%}$, and BBGM from $82.1\%$ to $\mathbf{83.9\%}$. Source code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
《视觉转换器（ViT）在视觉匹配中的应用》。在原始的网格分割策略下，ViT忽视了关键点的空间信息，导致对本地信息的敏感性受限。因此，我们提出了《查询转换器》（QueryTrans），它采用了交叉注意模块和基于关键点的中心裁剪策略，以更好地提取空间信息。此外，我们还整合了图注意模块，并设计了基于图transformer的图匹配方法《图匹配变换器》（GMTR），以解决图匹配问题的 combinatorial 性。在标准GM benchmark上，GMTR与顶尖框架相比，表现竞争力强。具体来说，在Pascal VOC上，GMTR的准确率为83.6%，高于顶尖框架80.1%。在Spair-71k上，GMTR表现出色，超过了大多数之前的工作。同时，在Pascal VOC上，QueryTrans提高了NGMv2的准确率从80.1%到83.3%，和BBGM从79.0%到84.5%。在Spair-71k上，QueryTrans提高了NGMv2的准确率从80.6%到82.5%，和BBGM从82.1%到83.9%。源代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Learning-based-Deep-Disentangling-Light-Field-Reconstruction-and-Disparity-Estimation-Application"><a href="#Learning-based-Deep-Disentangling-Light-Field-Reconstruction-and-Disparity-Estimation-Application" class="headerlink" title="Learning based Deep Disentangling Light Field Reconstruction and Disparity Estimation Application"></a>Learning based Deep Disentangling Light Field Reconstruction and Disparity Estimation Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08129">http://arxiv.org/abs/2311.08129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Langqing Shi, Ping Zhou</li>
<li>for: 提高深度估计任务中的angular分辨率，并解决大 disparity在稀有光场中的挑战。</li>
<li>methods: 提出了深度分离机制，将4D光场转换为2D图像格式，且采用了进一步的特征提取器设计和高级网络结构。</li>
<li>results: 实现了最佳性能在实验中，并提出了减少内存占用的块跨度angular超解析策略，用于深度估计增强。<details>
<summary>Abstract</summary>
Light field cameras have a wide range of uses due to their ability to simultaneously record light intensity and direction. The angular resolution of light fields is important for downstream tasks such as depth estimation, yet is often difficult to improve due to hardware limitations. Conventional methods tend to perform poorly against the challenge of large disparity in sparse light fields, while general CNNs have difficulty extracting spatial and angular features coupled together in 4D light fields. The light field disentangling mechanism transforms the 4D light field into 2D image format, which is more favorable for CNN for feature extraction. In this paper, we propose a Deep Disentangling Mechanism, which inherits the principle of the light field disentangling mechanism and further develops the design of the feature extractor and adds advanced network structure. We design a light-field reconstruction network (i.e., DDASR) on the basis of the Deep Disentangling Mechanism, and achieve SOTA performance in the experiments. In addition, we design a Block Traversal Angular Super-Resolution Strategy for the practical application of depth estimation enhancement where the input views is often higher than 2x2 in the experiments resulting in a high memory usage, which can reduce the memory usage while having a better reconstruction performance.
</details>
<details>
<summary>摘要</summary>
光场相机具有广泛的应用领域，主要是因为它同时记录光强和方向。光场的方向分辨率对下游任务如深度估计非常重要，但受硬件限制，通常难以提高。传统方法在大 disparity  sparse 光场中表现不佳，而通用 CNN 在4D 光场中抽取空间和方向特征同时存在困难。基于光场分解机制，我们提出了深度分解机制，将4D 光场转换成2D 图像格式，更适合 CNN 进行特征提取。在这篇论文中，我们提出了一种深度分解机制，具有光场分解机制的原理，并进一步开发特征提取器的设计和高级网络结构。基于深度分解机制，我们设计了一个深度场重建网络（i.e., DDASR），在实验中达到了最佳性能。此外，我们还设计了一种块传播角度超解析策略，用于实际应用深度估计增强，其中输入视图 часто高于2x2，导致高内存使用量，可以降低内存使用量而且具有更好的重建性能。
</details></li>
</ul>
<hr>
<h2 id="DeepEMplanner-An-EM-Motion-Planner-with-Iterative-Interactions"><a href="#DeepEMplanner-An-EM-Motion-Planner-with-Iterative-Interactions" class="headerlink" title="DeepEMplanner: An EM Motion Planner with Iterative Interactions"></a>DeepEMplanner: An EM Motion Planner with Iterative Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08100">http://arxiv.org/abs/2311.08100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Chen, Maosheng Ye, Shuangjie Xu, Tongyi Cao, Qifeng Chen</li>
<li>for: 本研究旨在提出一种基于深度学习的动态规划和互动模型，以便更好地学习细腻的行为。</li>
<li>methods: 我们提出了一种名为DeepEMplanner的新框架，它在每个步骤交互中考虑了各自的行为目标，以便更好地学习和预测对手和环境的行为。</li>
<li>results: 在nuScenesbenchmark上进行了实验，我们的方法实现了状态的最佳Results。<details>
<summary>Abstract</summary>
Motion planning is a computational problem that finds a sequence of valid trajectories, often based on surrounding agents' forecasting, environmental understanding, and historical and future contexts. It can also be viewed as a game in which agents continuously plan their next move according to other agents' intentions and the encountering environment, further achieving their ultimate goals through incremental actions. To model the dynamic planning and interaction process, we propose a novel framework, DeepEMplanner, which takes the stepwise interaction into account for fine-grained behavior learning. The ego vehicle maximizes each step motion to reach its eventual driving outcome based on the stepwise expectation from agents and its upcoming road conditions. On the other hand, the agents also follow the same philosophy to maximize their stepwise behavior under the encountering environment and the expectations from ego and other agents. Our DeepEMplanner models the interactions among ego, agents, and the dynamic environment in an autoregressive manner by interleaving the Expectation and Maximization processes. Further, we design ego-to-agents, ego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key objects attention to better model the interactions. Experiments on the nuScenes benchmark show that our approach achieves state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
行为规划是一个计算问题，找到一系列有效的轨迹，经常基于周围的代理人预测、环境理解和历史和未来的上下文。它也可以视为一个游戏，在 które agents 不断规划下一步的动作，根据其他代理人的意图和遇到的环境，以实现他们的最终目标。为了模型动态规划和互动过程，我们提出了一个新的框架，深度EMplanner，它考虑了每个步骤的互动，以提高细化的行为学习。ego Vehicle 在每步动作中尽可能地实现其最终驾驶结果，基于预测的代理人和下一步道路条件。然而，代理人也遵循同样的哲学，在遇到的环境和预测中 maximize 其每步行为。我们的 DeepEMplanner 模型了 egO、代理人和动态环境之间的互动，通过嵌入 Expectation 和 Maximization 过程来模型这些互动。此外，我们还设计了 ego-to-agents、ego-to-map 和 ego-to-BEV 互动机制，并使用层次的动态关键对象注意力来更好地模型这些互动。在 nuScenes  benchmark 上进行了实验，我们的方法实现了状态计算的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Light-curve-Signals-with-a-Deep-Learning-Based-Object-Detection-Algorithm-II-A-General-Light-Curve-Classification-Framework"><a href="#Identifying-Light-curve-Signals-with-a-Deep-Learning-Based-Object-Detection-Algorithm-II-A-General-Light-Curve-Classification-Framework" class="headerlink" title="Identifying Light-curve Signals with a Deep Learning Based Object Detection Algorithm. II. A General Light Curve Classification Framework"></a>Identifying Light-curve Signals with a Deep Learning Based Object Detection Algorithm. II. A General Light Curve Classification Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08080">http://arxiv.org/abs/2311.08080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ckm3/deep-lc">https://github.com/ckm3/deep-lc</a></li>
<li>paper_authors: Kaiming Cui, D. J. Armstrong, Fabo Feng</li>
<li>for: 这个研究的目的是发展一个通用的深度学习框架，用于自动分类天文学摄谱资料中的变星和其他物类。</li>
<li>methods: 这个框架使用了一种弱地监督物件检测模型，自动选择时间和频率领域中的最佳窗口，并将资料自动抽象为时间和频率领域之间的特征。</li>
<li>results: 这个模型在变星和噪音测量资料上取得了87%的准确率，与先前的特征基于模型相比。此外，这个模型还可以直接应用于其他任务，如ASAS-SN， без需要任何重新训练或调整。<details>
<summary>Abstract</summary>
Vast amounts of astronomical photometric data are generated from various projects, requiring significant efforts to identify variable stars and other object classes. In light of this, a general, widely applicable classification framework would simplify the task of designing custom classifiers. We present a novel deep learning framework for classifying light curves using a weakly supervised object detection model. Our framework identifies the optimal windows for both light curves and power spectra automatically, and zooms in on their corresponding data. This allows for automatic feature extraction from both time and frequency domains, enabling our model to handle data across different scales and sampling intervals. We train our model on datasets obtained from both space-based and ground-based multi-band observations of variable stars and transients. We achieve an accuracy of 87% for combined variables and transient events, which is comparable to the performance of previous feature-based models. Our trained model can be utilized directly to other missions, such as ASAS-SN, without requiring any retraining or fine-tuning. To address known issues with miscalibrated predictive probabilities, we apply conformal prediction to generate robust predictive sets that guarantee true label coverage with a given probability. Additionally, we incorporate various anomaly detection algorithms to empower our model with the ability to identify out-of-distribution objects. Our framework is implemented in the Deep-LC toolkit, which is an open-source Python package hosted on Github and PyPI.
</details>
<details>
<summary>摘要</summary>
巨量的天文光度数据由多个项目生成，需要大量的努力来识别变星和其他对象类型。为了简化这个任务，我们提出了一种通用的深度学习分类框架。我们的框架使用弱监督对象检测模型来分类光谱曲线。我们的框架可以自动确定光谱曲线和功率спектrum的优化窗口，并在这些窗口中提取数据。这使得我们的模型能够处理不同的时间和频率尺度，并且不需要手动设置窗口大小。我们的模型通过对多个变星和事件进行训练，实现了87%的总精度，与之前基于特征的模型相当。我们的训练模型可以直接应用于其他任务，如ASAS-SN，无需重新训练或调整。为了解决已知的预测概率误差，我们使用封闭预测来生成可靠的预测集， garantía true label coverage  WITH a given probability。此外，我们还 incorporated 多种异常检测算法，使我们的模型能够识别不符合预期的对象。我们的框架在 Deep-LC 工具包中实现，该工具包是一个开源的 Python 包， hosted on Github 和 PyPI。
</details></li>
</ul>
<hr>
<h2 id="GlanceSeg-Real-time-microaneurysm-lesion-segmentation-with-gaze-map-guided-foundation-model-for-early-detection-of-diabetic-retinopathy"><a href="#GlanceSeg-Real-time-microaneurysm-lesion-segmentation-with-gaze-map-guided-foundation-model-for-early-detection-of-diabetic-retinopathy" class="headerlink" title="GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy"></a>GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08075">http://arxiv.org/abs/2311.08075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Jiang, Mengdi Gao, Zirong Liu, Chen Tang, Xiaoqing Zhang, Shuai Jiang, Wu Yuan, Jiang Liu</li>
<li>for: 这个研究旨在提出一个基于“Segment Anything Model”（SAM）的人工智能支持的早期视力疾病诊断框架，以帮助诊断早期视力疾病中的微小血管变化。</li>
<li>methods: 这个框架使用了人工智能技术，包括眼镜视野映射和精确性检查，以帮助诊断早期视力疾病中的微小血管变化。</li>
<li>results: 这个研究显示了一个名为“GlanceSeg”的人工智能框架，可以帮助诊断早期视力疾病中的微小血管变化，并且可以提高诊断效率和准确性。<details>
<summary>Abstract</summary>
Early-stage diabetic retinopathy (DR) presents challenges in clinical diagnosis due to inconspicuous and minute microangioma lesions, resulting in limited research in this area. Additionally, the potential of emerging foundation models, such as the segment anything model (SAM), in medical scenarios remains rarely explored. In this work, we propose a human-in-the-loop, label-free early DR diagnosis framework called GlanceSeg, based on SAM. GlanceSeg enables real-time segmentation of microangioma lesions as ophthalmologists review fundus images. Our human-in-the-loop framework integrates the ophthalmologist's gaze map, allowing for rough localization of minute lesions in fundus images. Subsequently, a saliency map is generated based on the located region of interest, which provides prompt points to assist the foundation model in efficiently segmenting microangioma lesions. Finally, a domain knowledge filter refines the segmentation of minute lesions. We conducted experiments on two newly-built public datasets, i.e., IDRiD and Retinal-Lesions, and validated the feasibility and superiority of GlanceSeg through visualized illustrations and quantitative measures. Additionally, we demonstrated that GlanceSeg improves annotation efficiency for clinicians and enhances segmentation performance through fine-tuning using annotations. This study highlights the potential of GlanceSeg-based annotations for self-model optimization, leading to enduring performance advancements through continual learning.
</details>
<details>
<summary>摘要</summary>
早期 диабетическая ретинопатия (DR) 的临床诊断受到微型抽象病变的难以诊断的挑战，这导致了这个领域的研究受到限制。此外，现有的基础模型，如 segment anything model (SAM)，在医疗场景中的应用尚未得到广泛探索。在这项工作中，我们提出了一种人类在Loop的、无标签的早期 DR 诊断框架，称为 GlanceSeg，基于 SAM。GlanceSeg 可以在眼科医生查看基底图像时实时分割微型病变。我们的人类在Loop 框架将眼科医生的视线地图与基底图像进行结合，以便粗略地位微型病变。然后，基于所处的区域兴趣点的敏感地图会生成，以提供帮助基础模型快速分割微型病变的指导点。最后，基于区域的知识滤波器会对微型病变进行精细分割。我们在两个新建的公共数据集上进行了实验，即 IDRiD 和 Retinal-Lesions，并通过视觉化示例和量化度量证明了 GlanceSeg 的可行性和超越性。此外，我们还示出了 GlanceSeg 可以提高临床医生的注意力和分割性能，通过细化使用注解进行训练。这种研究强调了 GlanceSeg 基于注解的自适应优化的潜在可能，这将导致持续学习的性能提升。
</details></li>
</ul>
<hr>
<h2 id="FS-Net-Full-Scale-Network-and-Adaptive-Threshold-for-Improving-Extraction-of-Micro-Retinal-Vessel-Structures"><a href="#FS-Net-Full-Scale-Network-and-Adaptive-Threshold-for-Improving-Extraction-of-Micro-Retinal-Vessel-Structures" class="headerlink" title="FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures"></a>FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08059">http://arxiv.org/abs/2311.08059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melaku N. Getahun, Oleg Y. Rogov, Dmitry V. Dylov, Andrey Somov, Ahmed Bouridane, Rifat Hamoudi<br>for: 这个研究旨在帮助验视医生为某些眼病进行诊断和检测，减轻验视医生的工作负担。methods: 这个研究使用了一个基于encoder-decoder神经网络架构的全规模微血管提取机制，以及sigmoid平滑和适应阈值方法。results: 这个研究在DRIVE、CHASE-DB1和STARE datasets上进行了评估，与之前的研究相比，获得了比较出色的成绩，其中在DRIVEdataset上的AUC和准确率分别为0.9884和0.9702，在CHASE-DB1dataset上的 scores分别为0.9903和0.9755，在STAREdataset上的 scores分别为0.9916和0.9750。这些成绩较之前的研究一步进步，使得这个解决方案在实际检测中有更高的可能性被应用。<details>
<summary>Abstract</summary>
Retinal vascular segmentation, is a widely researched subject in biomedical image processing, aims to relieve ophthalmologists' workload when treating and detecting retinal disorders. However, segmenting retinal vessels has its own set of challenges, with prior techniques failing to generate adequate results when segmenting branches and microvascular structures. The neural network approaches used recently are characterized by the inability to keep local and global properties together and the failure to capture tiny end vessels make it challenging to attain the desired result. To reduce this retinal vessel segmentation problem, we propose a full-scale micro-vessel extraction mechanism based on an encoder-decoder neural network architecture, sigmoid smoothing, and an adaptive threshold method. The network consists of of residual, encoder booster, bottleneck enhancement, squeeze, and excitation building blocks. All of these blocks together help to improve the feature extraction and prediction of the segmentation map. The proposed solution has been evaluated using the DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when compared with previous studies. The AUC and accuracy on the DRIVE dataset are 0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are 0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916 and 0.9750, respectively. The performance achieved is one step ahead of what has been done in previous studies, and this results in a higher chance of having this solution in real-life diagnostic centers that seek ophthalmologists attention.
</details>
<details>
<summary>摘要</summary>
Retinal vascular segmentation 是医学图像处理领域广泛研究的主题，旨在减轻眼科医生在诊断和治疗 RETINAL 疾病时的劳重。然而， segmenting retinal vessels 有其独特的挑战，先前的技术无法生成足够的结果，特别是在分支和微血管结构上。近年来的神经网络方法具有不能同时保持本地和全局属性以及失去微血管结构的缺点，使得 segmentation 问题变得更加困难。为解决这个问题，我们提出了基于 encoder-decoder 神经网络架构、sigmoid 缓和适应阈值方法的全规模微血管提取机制。该网络由 residual、encoder booster、瓶颈增强、缩小和刺激块组成。这些块都帮助提高特征提取和预测 segmentation 图像。我们对 DRIVE、CHASE-DB1 和 STARE 数据集进行评估，与前期研究相比，实现了竞争性的结果。 DRIVE 数据集上的 AUC 和精度分别为 0.9884 和 0.9702，CHASE-DB1 数据集上的分别为 0.9903 和 0.9755，STARE 数据集上的分别为 0.9916 和 0.9750。我们的成果胜过了前期研究，这将有助于这种解决方案在实际诊断中心得到应用。
</details></li>
</ul>
<hr>
<h2 id="Chat-UniVi-Unified-Visual-Representation-Empowers-Large-Language-Models-with-Image-and-Video-Understanding"><a href="#Chat-UniVi-Unified-Visual-Representation-Empowers-Large-Language-Models-with-Image-and-Video-Understanding" class="headerlink" title="Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"></a>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08046">http://arxiv.org/abs/2311.08046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-yuangroup/chat-univi">https://github.com/pku-yuangroup/chat-univi</a></li>
<li>paper_authors: Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, Li Yuan</li>
<li>for: 这个论文的目的是解决图像和视频理解的问题，并能够在有限的视觉token上进行有效的对话。</li>
<li>methods: 这个论文使用了一种动态的视觉代表方法，可以同时捕捉图像和视频中的空间细节和时间关系。此外，它还使用了多尺度表示方法，使模型能够捕捉高级别的Semantic概念和低级别的视觉细节。</li>
<li>results: 实验结果表明，Chat-UniVi模型在混合 dataset上进行训练后，能够在图像和视频任务中表现出色，并且在图像和视频任务中的性能都高于专门为图像或视频设计的方法。<details>
<summary>Abstract</summary>
Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. However, existing methods encounter challenges in effectively handling both image and video understanding, particularly with limited visual tokens. In this work, we introduce Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation. Specifically, we employ a set of dynamic visual tokens to uniformly represent images and videos. This representation framework empowers the model to efficiently utilize a limited number of visual tokens to simultaneously capture the spatial details necessary for images and the comprehensive temporal relationship required for videos. Moreover, we leverage a multi-scale representation, enabling the model to perceive both high-level semantic concepts and low-level visual details. Notably, Chat-UniVi is trained on a mixed dataset containing both images and videos, allowing direct application to tasks involving both mediums without requiring any modifications. Extensive experimental results demonstrate that Chat-UniVi, as a unified model, consistently outperforms even existing methods exclusively designed for either images or videos.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:大型语言模型已经展示了广泛的通用能力，并扩展了其应用范围到包括多modal会话。然而，现有方法在处理图像和视频理解方面遇到了挑战，特别是具有有限的视觉标识符。在这种情况下，我们引入了 Chat-UniVi，一种能够同时理解和参与图像和视频的沟通的统一视觉语言模型。具体来说，我们使用了一组动态的视觉标识符来统一表示图像和视频。这种表示框架使得模型可以有效地利用有限的视觉标识符来同时捕捉图像中的空间细节和视频中的全面时间关系。此外，我们还利用了多尺度表示，让模型能够捕捉高级 semantic概念以及低级视觉细节。值得一提的是，Chat-UniVi 是在包含图像和视频的混合 Dataset 上训练的，因此不需要任何修改就能直接应用于包含这两种媒体的任务。我们的实验结果表明，Chat-UniVi 作为一种统一模型，在与专门为图像或视频设计的方法进行比较时，一直表现出优于其。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-for-Multi-Object-Tracking-with-Transformers"><a href="#Contrastive-Learning-for-Multi-Object-Tracking-with-Transformers" class="headerlink" title="Contrastive Learning for Multi-Object Tracking with Transformers"></a>Contrastive Learning for Multi-Object Tracking with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08043">http://arxiv.org/abs/2311.08043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre-François De Plaen, Nicola Marinello, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool</li>
<li>for: 这个研究旨在将物件探测变换为一个翻译任务，从图像特征中获取物件水平的表现。</li>
<li>methods: 我们使用了一个实验级别的对称损失函数、一种修订的抽象策略和轻量级的对称分配方法来转换DETR模型为多物件追踪（MOT）模型。</li>
<li>results: 我们的训练方案可以学习物件的外观，同时保持探测能力，仅需小量的额外负载。在BDD100K dataset上，我们的表现超过了过往的最佳性能+2.6 mMOTA，与现有的对称基于方法在MOT17 dataset上的表现相似。<details>
<summary>Abstract</summary>
The DEtection TRansformer (DETR) opened new possibilities for object detection by modeling it as a translation task: converting image features into object-level representations. Previous works typically add expensive modules to DETR to perform Multi-Object Tracking (MOT), resulting in more complicated architectures. We instead show how DETR can be turned into a MOT model by employing an instance-level contrastive loss, a revised sampling strategy and a lightweight assignment method. Our training scheme learns object appearances while preserving detection capabilities and with little overhead. Its performance surpasses the previous state-of-the-art by +2.6 mMOTA on the challenging BDD100K dataset and is comparable to existing transformer-based methods on the MOT17 dataset.
</details>
<details>
<summary>摘要</summary>
“DEtection TRansformer（DETR）开创了新的可能性，将对象探测视为翻译任务：将图像特征转换为对象级别表示。先前的工作通常会添加费时模块到DETR来实现多对象跟踪（MOT），导致建立更加复杂的架构。我们则示出了如何将DETR转换成MOT模型，使用实例级别的对比损失，修改抽取策略和轻量级的归属方法。我们的训练方案学习对象外观，保留探测能力，占用较少的资源。其性能在具有挑战性的BDD100K数据集上超过了之前的州态艺术的+2.6 mMOTA，与现有的转换基本方法在MOT17数据集上的性能相当。”
</details></li>
</ul>
<hr>
<h2 id="ELF-An-End-to-end-Local-and-Global-Multimodal-Fusion-Framework-for-Glaucoma-Grading"><a href="#ELF-An-End-to-end-Local-and-Global-Multimodal-Fusion-Framework-for-Glaucoma-Grading" class="headerlink" title="ELF: An End-to-end Local and Global Multimodal Fusion Framework for Glaucoma Grading"></a>ELF: An End-to-end Local and Global Multimodal Fusion Framework for Glaucoma Grading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08032">http://arxiv.org/abs/2311.08032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyun Li, Chi-Man Pun</li>
<li>for: 针对睫际病变患者进行早期检测和治疗，以防病情加重。</li>
<li>methods: 基于2D睫际图像和光度共谱成像(OCT)技术，提出一种综合利用多Modal信息的睫际病变评估框架，名为ELF。ELF可以充分利用睫际图像和OCT数据之间的补偿信息。</li>
<li>results: 在多modal睫际病变评估GAMMA dataset上进行了广泛的实验，并证明ELF在比较其他状态艺术方法时表现更高效。<details>
<summary>Abstract</summary>
Glaucoma is a chronic neurodegenerative condition that can lead to blindness. Early detection and curing are very important in stopping the disease from getting worse for glaucoma patients. The 2D fundus images and optical coherence tomography(OCT) are useful for ophthalmologists in diagnosing glaucoma. There are many methods based on the fundus images or 3D OCT volumes; however, the mining for multi-modality, including both fundus images and data, is less studied. In this work, we propose an end-to-end local and global multi-modal fusion framework for glaucoma grading, named ELF for short. ELF can fully utilize the complementary information between fundus and OCT. In addition, unlike previous methods that concatenate the multi-modal features together, which lack exploring the mutual information between different modalities, ELF can take advantage of local-wise and global-wise mutual information. The extensive experiment conducted on the multi-modal glaucoma grading GAMMA dataset can prove the effiectness of ELF when compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
glaucoma 是一种慢性神经退化疾病，可能导致失明。早期发现和治疗非常重要，以防疫情进一步加剧。二维基准图像和光共振镜(OCT) 是诊断 glaucoma 的有用工具。多种基于基准图像或3D OCT 体积的方法已经研究过，但是对于多Modal 数据的挖掘还是相对较少。在这种情况下，我们提出了一种综合使用本地和全局多Modal 融合框架，名为ELF。ELF 可以充分利用基准图像和 OCT 之间的补偿信息。此外，与之前的方法不同的是，ELF 可以利用本地和全局多Modal 信息之间的相互关系，而不是将多Modal 特征 concatenate 在一起。经过了对多modal 疾病评分 GAMMA 数据集的广泛实验，ELF 的效果可以证明在与其他现有方法相比，效果更好。
</details></li>
</ul>
<hr>
<h2 id="MD-IQA-Learning-Multi-scale-Distributed-Image-Quality-Assessment-with-Semi-Supervised-Learning-for-Low-Dose-CT"><a href="#MD-IQA-Learning-Multi-scale-Distributed-Image-Quality-Assessment-with-Semi-Supervised-Learning-for-Low-Dose-CT" class="headerlink" title="MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with Semi Supervised Learning for Low Dose CT"></a>MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with Semi Supervised Learning for Low Dose CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08024">http://arxiv.org/abs/2311.08024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Song, Ruizhi Hou, Lisong Dai, Lei Xiang<br>for: 这个研究旨在提高深度学习基于医学影像评估（IQA）的模型通用性和感知准确性。methods: 该研究提出了一种多级分布回归方法，通过制约输出分布来提高模型通用性。此外，我们还设计了一个双树对齐网络来增强特征提取能力。最后，我们引入了半监督学习，使用 pseudo-标签来导引模型训练。results: 我们的提议方法在大规模实验中得到了广泛的可读性和稳定性。我们的方法可以在医学影像评估中提高模型的通用性和感知准确性。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/zunzhumu/MD-IQA%E3%80%82">https://github.com/zunzhumu/MD-IQA。</a><details>
<summary>Abstract</summary>
Image quality assessment (IQA) plays a critical role in optimizing radiation dose and developing novel medical imaging techniques in computed tomography (CT). Traditional IQA methods relying on hand-crafted features have limitations in summarizing the subjective perceptual experience of image quality. Recent deep learning-based approaches have demonstrated strong modeling capabilities and potential for medical IQA, but challenges remain regarding model generalization and perceptual accuracy. In this work, we propose a multi-scale distributions regression approach to predict quality scores by constraining the output distribution, thereby improving model generalization. Furthermore, we design a dual-branch alignment network to enhance feature extraction capabilities. Additionally, semi-supervised learning is introduced by utilizing pseudo-labels for unlabeled data to guide model training. Extensive qualitative experiments demonstrate the effectiveness of our proposed method for advancing the state-of-the-art in deep learning-based medical IQA. Code is available at: https://github.com/zunzhumu/MD-IQA.
</details>
<details>
<summary>摘要</summary>
医用像质评估（IQA）在计算机Tomography（CT）中扮演了关键的角色， optimize radiation dose和开发新的医疗成像技术。传统的IQA方法，取决于手工设计的特征，有限的概念化Subjective perceived image quality的经验。 current deep learning-based approaches have shown strong modeling capabilities and potential for medical IQA, but there are still challenges in terms of model generalization and perceptual accuracy. 在这项工作中，我们提议一种多尺度分布回归方法，预测质分数，并通过限制输出分布，提高模型泛化性。此外，我们设计了双树对齐网络，提高特征提取能力。此外，我们还利用 pseudo-labels 进行 semi-supervised learning，以帮助模型训练。我们的提议方法在 deep learning-based 医疗IQA 领域中具有广泛的可靠性和稳定性。 Code 可以在以下 GitHub 上获取：https://github.com/zunzhumu/MD-IQA.
</details></li>
</ul>
<hr>
<h2 id="CP-SLAM-Collaborative-Neural-Point-based-SLAM-System"><a href="#CP-SLAM-Collaborative-Neural-Point-based-SLAM-System" class="headerlink" title="CP-SLAM: Collaborative Neural Point-based SLAM System"></a>CP-SLAM: Collaborative Neural Point-based SLAM System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08013">http://arxiv.org/abs/2311.08013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarui Hu, Mao Mao, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</li>
<li>for: 这个论文描述了一种基于RGB-D图像序列的协同隐藏神经同时地图（SLAM）系统，包括完整的前端和后端模块，如偏移、循环检测、子地图融合和全局精度调整。</li>
<li>methods: 作者提出了一种新的神经点基于3D场景表示方法，每个点都有一个学习的神经特征用于场景编码，并与特定的关键帧相关。此外，作者还提出了一种分布式到中心式学习策略来提高协同隐藏SLAM的一致性和合作。</li>
<li>results: 实验结果表明，提出的方法在不同的数据集上都有较高的精度和稳定性，在摄像头跟踪和地图建模方面均有显著提高。<details>
<summary>Abstract</summary>
This paper presents a collaborative implicit neural simultaneous localization and mapping (SLAM) system with RGB-D image sequences, which consists of complete front-end and back-end modules including odometry, loop detection, sub-map fusion, and global refinement. In order to enable all these modules in a unified framework, we propose a novel neural point based 3D scene representation in which each point maintains a learnable neural feature for scene encoding and is associated with a certain keyframe. Moreover, a distributed-to-centralized learning strategy is proposed for the collaborative implicit SLAM to improve consistency and cooperation. A novel global optimization framework is also proposed to improve the system accuracy like traditional bundle adjustment. Experiments on various datasets demonstrate the superiority of the proposed method in both camera tracking and mapping.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种基于RGB-D图像序列的协同隐式神经同时地图（SLAM）系统，包括完整的前端和后端模块，如偏移、循环检测、子地图融合和全局调整。为了在一个统一框架中实现这些模块，我们提出了一种新的神经点基于3D场景表示，其中每个点都保留一个学习的神经特征 для场景编码，并与某个键帧相关。此外，我们还提出了分布式到中心学习策略来提高协同隐式SLAM的一致性和合作性。此外，我们还提出了一种新的全局优化框架来提高系统精度，类似于传统的缎纹调整。在多个数据集上进行了诸多实验，并证明了我们的方法在摄像头跟踪和地图建模方面具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Clearer-Frames-Anytime-Resolving-Velocity-Ambiguity-in-Video-Frame-Interpolation"><a href="#Clearer-Frames-Anytime-Resolving-Velocity-Ambiguity-in-Video-Frame-Interpolation" class="headerlink" title="Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation"></a>Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08007">http://arxiv.org/abs/2311.08007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzh-tech/interpany-clearer">https://github.com/zzh-tech/interpany-clearer</a></li>
<li>paper_authors: Zhihang Zhong, Gurunandan Krishnan, Xiao Sun, Yu Qiao, Sizhuo Ma, Jian Wang</li>
<li>for: 提高视频帧 interpolate（VFI）方法的精度和清晰度，尤其是对于具有较长距离和弯曲运动的对象。</li>
<li>methods: 使用一种新的“距离索引”方法，即提供网络中的明确提示，以帮助网络学习对象在不同时间步的位置。此外，还提出了一种循环引用基本估计策略，以解决长距离预测中的方向ambiguity问题。</li>
<li>results: 与传统VFI方法相比，使用“距离索引”和循环引用基本估计策略可以提高视频帧 interpolate的精度和清晰度，并且可以独立地调整视频的时间 interpolate。此外，“距离索引”还可以在每帧级进行独立的时间修饰，为视频编辑任务提供一种新的工具。<details>
<summary>Abstract</summary>
Existing video frame interpolation (VFI) methods blindly predict where each object is at a specific timestep t ("time indexing"), which struggles to predict precise object movements. Given two images of a baseball, there are infinitely many possible trajectories: accelerating or decelerating, straight or curved. This often results in blurry frames as the method averages out these possibilities. Instead of forcing the network to learn this complicated time-to-location mapping implicitly together with predicting the frames, we provide the network with an explicit hint on how far the object has traveled between start and end frames, a novel approach termed "distance indexing". This method offers a clearer learning goal for models, reducing the uncertainty tied to object speeds. We further observed that, even with this extra guidance, objects can still be blurry especially when they are equally far from both input frames (i.e., halfway in-between), due to the directional ambiguity in long-range motion. To solve this, we propose an iterative reference-based estimation strategy that breaks down a long-range prediction into several short-range steps. When integrating our plug-and-play strategies into state-of-the-art learning-based models, they exhibit markedly sharper outputs and superior perceptual quality in arbitrary time interpolations, using a uniform distance indexing map in the same format as time indexing. Additionally, distance indexing can be specified pixel-wise, which enables temporal manipulation of each object independently, offering a novel tool for video editing tasks like re-timing.
</details>
<details>
<summary>摘要</summary>
存在的视频帧 interpolate (VFI) 方法盲目地预测每个对象在特定时间步 t ("时间索引") 上的位置，这可能会难以预测对象的精确移动。给出了两个 baseball 图像，有无数可能的轨迹：加速或减速，直线或弯曲。这经常导致模糊的帧，因为方法平均出这些可能性。而不是让网络学习这些复杂的时间-to-位置映射，我们为网络提供了一个显式的提示，即在开始和结束帧之间对象如何移动的距离，一种新的方法称为 "距离索引"。这种方法为模型提供了明确的学习目标，降低了对象速度的uncertainty。我们进一步发现，即使有这些额外指导，对象仍然可能变得模糊，特别是当它们处于两个输入帧之间的中点（即半路）时，由于长距离运动的方向ambiguity。为解决这个问题，我们提议一种Iterative reference-based估计策略，将长距离预测分解成多个短距离步骤。将我们的插件和简化策略 integrate 到当前的学习基于模型中，它们在任意时间 interpolate 中表现出了明显更加锐化的输出和superior perceptual quality，使用一个固定的距离索引地图，与时间索引相同。此外，距离索引可以像时间索引一样Specified 像素级，这允许在视频编辑任务中重新时间调整每个对象独立地，提供了一种新的工具。
</details></li>
</ul>
<hr>
<h2 id="Explicit-Change-Relation-Learning-for-Change-Detection-in-VHR-Remote-Sensing-Images"><a href="#Explicit-Change-Relation-Learning-for-Change-Detection-in-VHR-Remote-Sensing-Images" class="headerlink" title="Explicit Change Relation Learning for Change Detection in VHR Remote Sensing Images"></a>Explicit Change Relation Learning for Change Detection in VHR Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07993">http://arxiv.org/abs/2311.07993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dalong Zheng, Zebin Wu, Jia Liu, Chih-Cheng Hung, Zhihui Wei</li>
<li>for: 本文提出了一种名为 NAME 的网络架构，用于显式挖掘变化关系特征，以提高 remote sensing 图像变化检测的准确率。</li>
<li>methods: 该网络架构包括 triple branch 网络， combine 了 transformer 和 CNN，用于提取和融合全局信息和本地信息中的变化特征。 同时，该网络还包括 continous change relation (CCR) branch，用于获取细化变化关系特征，以提高变化检测的准确率。</li>
<li>results: 实验结果表明，NAME 网络在四个公共高分辨率 remote sensing 数据集上的 F1、IoU 和 OA 指标上比现有的先进网络更高。<details>
<summary>Abstract</summary>
Change detection has always been a concerned task in the interpretation of remote sensing images. It is essentially a unique binary classification task with two inputs, and there is a change relationship between these two inputs. At present, the mining of change relationship features is usually implicit in the network architectures that contain single-branch or two-branch encoders. However, due to the lack of artificial prior design for change relationship features, these networks cannot learn enough change semantic information and lose more accurate change detection performance. So we propose a network architecture NAME for the explicit mining of change relation features. In our opinion, the change features of change detection should be divided into pre-changed image features, post-changed image features and change relation features. In order to fully mine these three kinds of change features, we propose the triple branch network combining the transformer and convolutional neural network (CNN) to extract and fuse these change features from two perspectives of global information and local information, respectively. In addition, we design the continuous change relation (CCR) branch to further obtain the continuous and detail change relation features to improve the change discrimination capability of the model. The experimental results show that our network performs better, in terms of F1, IoU, and OA, than those of the existing advanced networks for change detection on four public very high-resolution (VHR) remote sensing datasets. Our source code is available at https://github.com/DalongZ/NAME.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>改变检测一直是Remote感知图像解释中的关键任务。它实际上是一个独特的二分类任务，其中有两个输入。在现有的网络架构中，改变关系特征的挖掘是通常隐式的，但由于缺乏人工设计的改变关系特征，这些网络无法学习足够的改变semantic信息，导致更准确的改变检测性能下降。因此，我们提议一种名为NAME的网络架构，用于明确挖掘改变关系特征。根据我们的观点，改变特征包括预变图像特征、后变图像特征和改变关系特征。为了全面挖掘这三种改变特征，我们提议使用转换器和卷积神经网络（CNN）结合三支分支网络，从全球信息和局部信息两个角度提取和融合这些改变特征。此外，我们还设计了连续改变关系（CCR）支分，以获取更细致的改变关系特征，以提高改变检测模型的改变识别能力。实验结果显示，我们的网络在四个公共very高分辨率（VHR）Remote感知数据集上的F1、IoU和OA指标上表现比现有的先进网络更好。我们的源代码可以在https://github.com/DalongZ/NAME中下载。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Individual-Tree-Mapping-with-Sub-meter-Imagery"><a href="#Benchmarking-Individual-Tree-Mapping-with-Sub-meter-Imagery" class="headerlink" title="Benchmarking Individual Tree Mapping with Sub-meter Imagery"></a>Benchmarking Individual Tree Mapping with Sub-meter Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07981">http://arxiv.org/abs/2311.07981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Gominski, Ankit Kariryaa, Martin Brandt, Christian Igel, Sizhuo Li, Maurice Mugabowindekwe, Rasmus Fensholt</li>
<li>for: 本研究旨在提供一个适合单一树木映射的评估框架，以便在不同的物理环境中进行单一树木映射。</li>
<li>methods: 本研究使用了多种方法和深度架构进行单一树木映射，包括检测和分类方法，以及传播者。</li>
<li>results: 本研究通过实验证明了一个新的方法，可以在单一树木映射中实现一个好的折冲点between segmentation和检测。<details>
<summary>Abstract</summary>
There is a rising interest in mapping trees using satellite or aerial imagery, but there is no standardized evaluation protocol for comparing and enhancing methods. In dense canopy areas, the high variability of tree sizes and their spatial proximity makes it arduous to define the quality of the predictions. Concurrently, object-centric approaches such as bounding box detection usuallyperform poorly on small and dense objects. It thus remains unclear what is the ideal framework for individual tree mapping, in regards to detection and segmentation approaches, convolutional neural networks and transformers. In this paper, we introduce an evaluation framework suited for individual tree mapping in any physical environment, with annotation costs and applicative goals in mind. We review and compare different approaches and deep architectures, and introduce a new method that we experimentally prove to be a good compromise between segmentation and detection.
</details>
<details>
<summary>摘要</summary>
有一些团队正在使用卫星或飞行图像来映射树木，但没有一个标准化的评估协议来比较和提高方法。在密集的树木区域中，树木的大小和空间 proximity 的高度变化使其困难定义预测的质量。同时，对象中心的方法，如 bounding box 探测，通常在小型和密集的对象上表现不佳。因此，还未确定最佳的框架是什么，它应该是 detection 和 segmentation 方法， convolutional neural networks 和 transformers。在这篇文章中，我们提出了适用于个体树木映射的评估框架，考虑到标注成本和实际应用目标。我们还对不同的方法和深度架构进行了查看和比较，并引入了一种新的方法，我们实验证明这种方法是一个好的 segmentation 和 detection 的 компроми斯。
</details></li>
</ul>
<hr>
<h2 id="Comparison-of-two-data-fusion-approaches-for-land-use-classification"><a href="#Comparison-of-two-data-fusion-approaches-for-land-use-classification" class="headerlink" title="Comparison of two data fusion approaches for land use classification"></a>Comparison of two data fusion approaches for land use classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07967">http://arxiv.org/abs/2311.07967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Cubaud, Arnaud Le Bris, Laurence Jolivet, Ana-Maria Olteanu-Raimond</li>
<li>for: 这种研究的目的是为了生成详细的土地使用地图，以便于土地管理和规划。</li>
<li>methods: 这种研究使用了多种不同来源的空间数据，包括光学图像和其他数据源，并 compare了两种不同的方法来组合这些数据：预分类 fusion 和后分类 fusion。</li>
<li>results: 研究发现，预分类 fusion 方法可以达到最高的 final 精度（97%）和macro-mean F1 分数（88%）。<details>
<summary>Abstract</summary>
Accurate land use maps, describing the territory from an anthropic utilisation point of view, are useful tools for land management and planning. To produce them, the use of optical images alone remains limited. It is therefore necessary to make use of several heterogeneous sources, each carrying complementary or contradictory information due to their imperfections or their different specifications. This study compares two different approaches i.e. a pre-classification and a post-classification fusion approach for combining several sources of spatial data in the context of land use classification. The approaches are applied on authoritative land use data located in the Gers department in the southwest of France. Pre-classification fusion, while not explicitly modeling imperfections, has the best final results, reaching an overall accuracy of 97% and a macro-mean F1 score of 88%.
</details>
<details>
<summary>摘要</summary>
准确的土地使用地图，从人类活动利用角度来看 territory，是地域规划和管理中非常有用的工具。但使用光学图像 alone 的使用受限，因此需要使用多种不同来源，每种携带不同的信息，由于它们的不完全性或不同的规格。本研究比较了两种不同的方法，即预分类和后分类 fusión 方法，用于将多种空间数据组合在土地使用分类中。两种方法在法国南西部GERS省的官方土地使用数据上进行应用。预分类 fusión 方法，不直接模型瑕疵，最终结果最佳，达到了97%的总准确率和88%的macro-mean F1分数。
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-Based-Condition-Diagnosis-Method-for-Distribution-Network-Switchgear"><a href="#Robust-Learning-Based-Condition-Diagnosis-Method-for-Distribution-Network-Switchgear" class="headerlink" title="Robust Learning Based Condition Diagnosis Method for Distribution Network Switchgear"></a>Robust Learning Based Condition Diagnosis Method for Distribution Network Switchgear</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07956">http://arxiv.org/abs/2311.07956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxi Zhang, Zhe Li, Weixi Li, Weisi Ma, Xinyi Chen, Sizhe Li</li>
<li>For: 这种方法用于诊断分布网络switchgear的状态，以维护电力质量 для终端用户。* Methods: 该方法使用扩展的特征向量，包括环境数据、温度测量、Switch位置、电动机操作、隔离状况和本地充电信息。它利用特征映射处理高维度数据，并引入决策半径来分类无标签样本，通过综合超参和自参损失函数、一致常数 regularization 函数来更新模型参数。* Results: 相比现有模型，该方法在准确性和稳定性两个方面具有显著优势。<details>
<summary>Abstract</summary>
This paper introduces a robust, learning-based method for diagnosing the state of distribution network switchgear, which is crucial for maintaining the power quality for end users. Traditional diagnostic models often rely heavily on expert knowledge and lack robustness. To address this, our method incorporates an expanded feature vector that includes environmental data, temperature readings, switch position, motor operation, insulation conditions, and local discharge information. We tackle the issue of high dimensionality through feature mapping. The method introduces a decision radius to categorize unlabeled samples and updates the model parameters using a combination of supervised and unsupervised loss, along with a consistency regularization function. This approach ensures robust learning even with a limited number of labeled samples. Comparative analysis demonstrates that this method significantly outperforms existing models in both accuracy and robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detection-of-Small-Targets-in-Sea-Clutter-Based-on-RepVGG-and-Continuous-Wavelet-Transform"><a href="#Detection-of-Small-Targets-in-Sea-Clutter-Based-on-RepVGG-and-Continuous-Wavelet-Transform" class="headerlink" title="Detection of Small Targets in Sea Clutter Based on RepVGG and Continuous Wavelet Transform"></a>Detection of Small Targets in Sea Clutter Based on RepVGG and Continuous Wavelet Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07912">http://arxiv.org/abs/2311.07912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingchen Ni, Haoru Li, Lilin Xu, Jing Liang</li>
<li>for: 这个论文的目的是提出一种高性能的海啸背景下的目标探测器，以提高探测效率和准确率。</li>
<li>methods: 这个论文使用了RepVGG残差网络，并与其他网络和特征提取方法进行比较，包括STFT和CWT。</li>
<li>results: 测试结果表明，使用RepVGGA0-CWT探测器可以在低控制干扰false alarm rate、高训练速度、高探测速度和低内存使用率等方面表现优于其他网络和特征提取方法。<details>
<summary>Abstract</summary>
Constructing a high-performance target detector under the background of sea clutter is always necessary and important. In this work, we propose a RepVGGA0-CWT detector, where RepVGG is a residual network that gains a high detection accuracy. Different from traditional residual networks, RepVGG keeps an acceptable calculation speed. Giving consideration to both accuracy and speed, the RepVGGA0 is selected among all the variants of RepVGG. Also, continuous wavelet transform (CWT) is employed to extract the radar echoes' time-frequency feature effectively. In the tests, other networks (ResNet50, ResNet18 and AlexNet) and feature extraction methods (short-time Fourier transform (STFT), CWT) are combined to build detectors for comparison. The result of different datasets shows that the RepVGGA0-CWT detector performs better than those detectors in terms of low controllable false alarm rate, high training speed, high inference speed and low memory usage. This RepVGGA0-CWT detector is hardware-friendly and can be applied in real-time scenes for its high inference speed in detection.
</details>
<details>
<summary>摘要</summary>
构建高性能的目标检测器在海啸背景下是一项必要和重要的任务。在这项工作中，我们提议了RepVGGA0-CWT检测器，其中RepVGG是一种具有高检测精度的径向网络，而不同于传统的径向网络，RepVGG具有可接受的计算速度。为了考虑精度和速度之间的平衡，我们选择了RepVGGA0中的所有变体。此外，我们采用了 kontinuous wavelet transform（CWT）来提取雷达回声的时空频特征，以便更好地检测目标。在测试中，我们使用了其他网络（ResNet50、ResNet18和AlexNet）和特征提取方法（short-time Fourier transform（STFT）、CWT）构建了对比的检测器。测试结果显示，RepVGGA0-CWT检测器在低可控干扰False Alarm率、高训练速度、高推理速度和低内存使用量等方面表现更好于其他检测器。此外，这种RepVGGA0-CWT检测器具有硬件友好性，可以在实时场景中应用，因为它具有高推理速度。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Training-for-Semantic-Segmentation-with-Output-Contrastive-Loss"><a href="#Test-Time-Training-for-Semantic-Segmentation-with-Output-Contrastive-Loss" class="headerlink" title="Test-Time Training for Semantic Segmentation with Output Contrastive Loss"></a>Test-Time Training for Semantic Segmentation with Output Contrastive Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07877">http://arxiv.org/abs/2311.07877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dazhangyu123/ocl">https://github.com/dazhangyu123/ocl</a></li>
<li>paper_authors: Yunlong Zhang, Yuxuan Sun, Sunyi Zheng, Zhongyi Shui, Chenglu Zhu, Lin Yang</li>
<li>for: 这篇论文主要是为了提高深度学习基于模型在新领域中的泛化能力，以便在评估时能够更好地适应新的环境。</li>
<li>methods: 作者使用了测试时重要（TTT）方法，并引入了对比损失（CL）来稳定化适应过程，同时对CL进行了修改和简化，以便更直观地帮助模型更好地适应新的环境。</li>
<li>results: 作者通过了多种评估场景，证明了他们的方法的有效性，特别是当应用于先进的预训练方法中的测试数据时，他们的方法表现出色，说明它具有抗衰落和适应性。<details>
<summary>Abstract</summary>
Although deep learning-based segmentation models have achieved impressive performance on public benchmarks, generalizing well to unseen environments remains a major challenge. To improve the model's generalization ability to the new domain during evaluation, the test-time training (TTT) is a challenging paradigm that adapts the source-pretrained model in an online fashion. Early efforts on TTT mainly focus on the image classification task. Directly extending these methods to semantic segmentation easily experiences unstable adaption due to segmentation's inherent characteristics, such as extreme class imbalance and complex decision spaces. To stabilize the adaptation process, we introduce contrastive loss (CL), known for its capability to learn robust and generalized representations. Nevertheless, the traditional CL operates in the representation space and cannot directly enhance predictions. In this paper, we resolve this limitation by adapting the CL to the output space, employing a high temperature, and simplifying the formulation, resulting in a straightforward yet effective loss function called Output Contrastive Loss (OCL). Our comprehensive experiments validate the efficacy of our approach across diverse evaluation scenarios. Notably, our method excels even when applied to models initially pre-trained using domain adaptation methods on test domain data, showcasing its resilience and adaptability.\footnote{Code and more information could be found at~ \url{https://github.com/dazhangyu123/OCL}
</details>
<details>
<summary>摘要</summary>
（简体中文）尽管深度学习基于的分割模型在公共评测上表现出色，但将其推广到未见的环境中仍然是一大挑战。以提高模型在新领域评测时的泛化能力，测试时间训练（TTT）是一种挑战的 paradigma，它在线上适应源预训练模型。早期的TTT主要关注于图像分类任务。将这些方法扩展到 semantic segmentation 是容易陷入不稳定的适应过程，因为分割的特点包括分类异常分布和复杂的决策空间。为稳定适应过程，我们引入对比损失（CL），它能够学习Robust和通用的表示。然而，传统的CL在表示空间运行，无法直接改进预测。在这篇论文中，我们解决这个限制，通过对CL的修改，使其适应输出空间，使用高温度，并简化表述，得到一种直观 yet 有效的损失函数，称为输出对比损失（OCL）。我们在多个评测场景中进行了广泛的实验，证明了我们的方法的可靠性和适应性。尤其是，当我们将模型在测试预训练使用Domain adaptation方法时，我们的方法仍然表现出色，这表明了我们的方法的稳定性和适应性。
</details></li>
</ul>
<hr>
<h2 id="Dual-channel-Prototype-Network-for-few-shot-Classification-of-Pathological-Images"><a href="#Dual-channel-Prototype-Network-for-few-shot-Classification-of-Pathological-Images" class="headerlink" title="Dual-channel Prototype Network for few-shot Classification of Pathological Images"></a>Dual-channel Prototype Network for few-shot Classification of Pathological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07871">http://arxiv.org/abs/2311.07871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/quanhao129611/DCPN">https://github.com/quanhao129611/DCPN</a></li>
<li>paper_authors: Hao Quan, Xinjia Li, Dayu Hu, Tianhang Nan, Xiaoyu Cui</li>
<li>for: 这篇论文旨在提出一种基于几何学学习的技术，以几何学学习方法来对罕见疾病的医学图像进行分类。</li>
<li>methods: 本研究使用了一种称为“双渠道原型网络”（DCPN）的技术，该技术结合了自我超vised learning和卷积神经网络，以提高几何学分类的精度和效率。</li>
<li>results: 根据实验结果显示，DCPN在不同预设的临床情况下，实现了几何学分类任务的超级表现，特别是在同领域内的任务中，其性能与指导式学习相当。<details>
<summary>Abstract</summary>
In pathology, the rarity of certain diseases and the complexity in annotating pathological images significantly hinder the creation of extensive, high-quality datasets. This limitation impedes the progress of deep learning-assisted diagnostic systems in pathology. Consequently, it becomes imperative to devise a technology that can discern new disease categories from a minimal number of annotated examples. Such a technology would substantially advance deep learning models for rare diseases. Addressing this need, we introduce the Dual-channel Prototype Network (DCPN), rooted in the few-shot learning paradigm, to tackle the challenge of classifying pathological images with limited samples. DCPN augments the Pyramid Vision Transformer (PVT) framework for few-shot classification via self-supervised learning and integrates it with convolutional neural networks. This combination forms a dual-channel architecture that extracts multi-scale, highly precise pathological features. The approach enhances the versatility of prototype representations and elevates the efficacy of prototype networks in few-shot pathological image classification tasks. We evaluated DCPN using three publicly available pathological datasets, configuring small-sample classification tasks that mirror varying degrees of clinical scenario domain shifts. Our experimental findings robustly affirm DCPN's superiority in few-shot pathological image classification, particularly in tasks within the same domain, where it achieves the benchmarks of supervised learning.
</details>
<details>
<summary>摘要</summary>
在 PATHOLOGY 领域，一些疾病的罕见性和诊断图像的复杂性，使得创建大量、高质量数据集成为非常困难的。这种限制阻碍了深度学习助动诊断系统在 PATHOLOGY 领域的进步。因此，需要开发一种技术，可以从 minimal 数量的标注示例中分辨出新的疾病类别。这种技术将帮助深度学习模型更好地识别罕见疾病。为解决这个需求，我们介绍了 Dual-channel Prototype Network (DCPN)，基于 few-shot 学习 paradigm，用于分类 PATHOLOGY 图像。DCPN 将 Pyramid Vision Transformer (PVT) 框架与自动学习相结合，并将其与卷积神经网络结合。这种结构形成了双通道体系，可以提取多级、高精度 PATHOLOGY 特征。这种方法提高了prototype表示的多样性，并提高了prototype网络在 few-shot PATHOLOGY 图像分类任务中的效果。我们通过三个公共可用的 PATHOLOGY 数据集进行了实验，并配置了小样本分类任务，这些任务模拟了不同程度的临床enario域转移。我们的实验结果表明，DCPN 在 few-shot PATHOLOGY 图像分类任务中表现出色，特别是在同一个域的任务中，它可以达到超过supervised learning的标准。
</details></li>
</ul>
<hr>
<h2 id="Probing-clustering-in-neural-network-representations"><a href="#Probing-clustering-in-neural-network-representations" class="headerlink" title="Probing clustering in neural network representations"></a>Probing clustering in neural network representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07864">http://arxiv.org/abs/2311.07864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thao Nguyen, Simon Kornblith</li>
<li>for: 研究如何不同的设计选择影响隐藏表示中的团集化。</li>
<li>methods: 使用 BREEDS  hierarchy 进行 subclass clustering， isolate 训练数据集和网络架构为关键因素。</li>
<li>results: 发现 datasets 的类别结构和预训练模型的选择对团集化有影响，normalization 策略affects 哪些层的团集化性能，并发现 Vision Transformers 的团集化性能较差。<details>
<summary>Abstract</summary>
Neural network representations contain structure beyond what was present in the training labels. For instance, representations of images that are visually or semantically similar tend to lie closer to each other than to dissimilar images, regardless of their labels. Clustering these representations can thus provide insights into dataset properties as well as the network internals. In this work, we study how the many design choices involved in neural network training affect the clusters formed in the hidden representations. To do so, we establish an evaluation setup based on the BREEDS hierarchy, for the task of subclass clustering after training models with only superclass information. We isolate the training dataset and architecture as important factors affecting clusterability. Datasets with labeled classes consisting of unrelated subclasses yield much better clusterability than those following a natural hierarchy. When using pretrained models to cluster representations on downstream datasets, models pretrained on subclass labels provide better clusterability than models pretrained on superclass labels, but only when there is a high degree of domain overlap between the pretraining and downstream data. Architecturally, we find that normalization strategies affect which layers yield the best clustering performance, and, surprisingly, Vision Transformers attain lower subclass clusterability than ResNets.
</details>
<details>
<summary>摘要</summary>
（注：以下是使用简化中文表示的文本）神经网络表示含有超出训练标签的结构。例如，与标签不同的图像表示在不同的图像中往往更近，无论它们的标签如何。将这些表示进行归类可以提供关于数据集和网络内部的信息。在这种工作中，我们研究了各种神经网络训练中的设计选择如何影响隐藏表示中的归类结构。为此，我们基于BREEDS层次结构设置评估集成，用于在只有超类信息下进行类别归类。我们发现，使用不同类别的数据集和架构可以影响归类性。具有不相关的类别的数据集可以获得更好的归类性，而遵循自然层次结构的数据集则不然。使用预训练模型进行下游数据集的归类时，使用 subclass标签进行预训练可以获得更好的归类性，但只有在预训练和下游数据集具有高度域 overlap 时。层次上，我们发现normalization策略可以影响归类性最佳层，并且意外地发现视图转换器的 subclass 归类性较低于ResNet。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/14/cs.CV_2023_11_14/" data-id="clp53jwqv00mfyp8895s437v8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/93/">93</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
