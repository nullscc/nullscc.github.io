
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.SD_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T15:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.SD_2023_08_31/">cs.SD - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="General-Purpose-Audio-Effect-Removal"><a href="#General-Purpose-Audio-Effect-Removal" class="headerlink" title="General Purpose Audio Effect Removal"></a>General Purpose Audio Effect Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16177">http://arxiv.org/abs/2308.16177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhrice/RemFx">https://github.com/mhrice/RemFx</a></li>
<li>paper_authors: Matthew Rice, Christian J. Steinmetz, George Fazekas, Joshua D. Reiss</li>
<li>for:  removes audio effects from realistic scenarios with multiple effects and varying source content</li>
<li>methods:  uses deep learning and a dataset of five audio effects across four different sources to train and evaluate a set of existing architectures</li>
<li>results:  outperforms single model baselines, but examples with many effects present remain challenging<details>
<summary>Abstract</summary>
Although the design and application of audio effects is well understood, the inverse problem of removing these effects is significantly more challenging and far less studied. Recently, deep learning has been applied to audio effect removal; however, existing approaches have focused on narrow formulations considering only one effect or source type at a time. In realistic scenarios, multiple effects are applied with varying source content. This motivates a more general task, which we refer to as general purpose audio effect removal. We developed a dataset for this task using five audio effects across four different sources and used it to train and evaluate a set of existing architectures. We found that no single model performed optimally on all effect types and sources. To address this, we introduced RemFX, an approach designed to mirror the compositionality of applied effects. We first trained a set of the best-performing effect-specific removal models and then leveraged an audio effect classification model to dynamically construct a graph of our models at inference. We found our approach to outperform single model baselines, although examples with many effects present remain challenging.
</details>
<details>
<summary>摘要</summary>
TRANSLATION:尽管音频效果的设计和应用已经非常了解，但去除这些效果是非常困难的，而且对此还未有充分的研究。在现实情况下，多种效果会同时应用于不同的音频内容，这种情况下的音频效果去除是一个更加普遍的任务。我们使用了五种音频效果，在四种不同的音频源上进行了数据集的构建，并使用了这些数据集来训练和评估一些现有的架构。我们发现，没有任何一个模型能够在所有的效果类型和源类型上表现最佳。为解决这个问题，我们提出了RemFX，一种基于效果的组合性来设计的方法。我们首先训练了一些最佳的效果特定的去除模型，然后通过音频效果分类模型来动态构建一个图结构，以便在推理时使用。我们发现，我们的方法可以超过单个模型的基线，但是存在多个效果存在的例子仍然是一个挑战。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.SD_2023_08_31/" data-id="clm0t8e1h00asv78863ncfegp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.CV_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T13:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.CV_2023_08_31/">cs.CV - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features"><a href="#Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features" class="headerlink" title="Boosting Detection in Crowd Analysis via Underutilized Output Features"></a>Boosting Detection in Crowd Analysis via Underutilized Output Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16187">http://arxiv.org/abs/2308.16187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokai Wu, Fengyu Yang</li>
<li>for: 这种研究旨在探讨检测基于方法在人群分析中的潜在优势，以及如何利用这些方法提高人群分析的精度和效果。</li>
<li>methods: 这种模型使用了混合2D-1D压缩技术来精细化输出特征，并通过地域适应的NMS阈值和解体并对齐策略来解决检测基于方法的主要局限性。</li>
<li>results: 经过广泛的人群分析任务评估，包括人群数量、位置和检测等，研究表明可以通过利用输出特征来提高检测基于方法在人群分析中的精度和效果。<details>
<summary>Abstract</summary>
Detection-based methods have been viewed unfavorably in crowd analysis due to their poor performance in dense crowds. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored. Specifically, the area size and confidence score of output proposals and bounding boxes provide insight into the scale and density of the crowd. To leverage these underutilized features, we propose Crowd Hat, a plug-and-play module that can be easily integrated with existing detection models. This module uses a mixed 2D-1D compression technique to refine the output features and obtain the spatial and numerical distribution of crowd-specific information. Based on these features, we further propose region-adaptive NMS thresholds and a decouple-then-align paradigm that address the major limitations of detection-based methods. Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis.
</details>
<details>
<summary>摘要</summary>
传感器基本方法在群体分析中被视为不利，主要是因为它们在紧凑的群体中表现不佳。然而，我们认为这些方法的潜力被低估了，因为它们提供了群体分析中通常被忽略的重要信息。Specifically，输出提议和矩形框的面积和信任分数提供了群体的规模和密度的信息。为了利用这些尚未被利用的特征，我们提议了一个名为Crowd Hat的插件模块，可以轻松地与现有的检测模型结合使用。这个模块使用了2D-1D压缩技术来精细化输出特征，并从而获得群体中特定信息的空间和数字分布。基于这些特征，我们进一步提出了地域适应的NMS阈值和解除然后对齐的方法，这些方法可以解决检测基本方法中的主要局限性。我们对各种群体分析任务，包括群体计数、本地化和检测，进行了广泛的评估，并证明了利用输出特征和检测基本方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="SAM-Med2D"><a href="#SAM-Med2D" class="headerlink" title="SAM-Med2D"></a>SAM-Med2D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16184">http://arxiv.org/abs/2308.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uni-medical/sam-med2d">https://github.com/uni-medical/sam-med2d</a></li>
<li>paper_authors: Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, Yu Qiao</li>
<li>for: 本研究将SAM模型应用于医疗影像分类中，以填补自然影像和医疗影像之间的领域差距。</li>
<li>methods: 我们首先收集了约460万帧医疗影像和1970万个标注，建立了一个大规模的医疗影像分类数据集，覆盖多种模式和物体。然后，我们对SAM模型进行了全面的微调，并将其转换为SAM-Med2D模型。此外，我们还对SAM模型的Encoder和Decoder进行了进一步的微调，以获得最佳的SAM-Med2D模型。</li>
<li>results: 我们的方法在多种医疗影像分类任务中展示了明显的超越SAM的性能和扩展性。 Specifically, we evaluated the performance of SAM-Med2D on 9 datasets from MICCAI 2023 challenge and found that it outperformed SAM in terms of both segmentation accuracy and generalization ability.<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的研究进展——Segment Anything Model（SAM），用于自然图像分割，并取得了卓越的结果。但是，我们的评估和最新的研究表明，直接将预训练的SAM应用于医疗图像分割不会达到满意的性能。这种限制主要归结于自然图像和医疗图像之间的领域差异。为bridging这个差异，我们介绍了SAM-Med2D，这是对SAM的最全面的应用研究，用于医疗二维图像分割。我们首先收集了约4.6万个图像和19.7万个mask从公共和私人数据集中，建立了一个包括多种Modalities和物体的医疗图像分割数据集。然后，我们对SAM进行了全面的微调，并将其转化为SAM-Med2D。与之前的方法只采用矩形框或点提示作为交互分割方法不同，我们在SAM-Med2D中采用了更全面的提示，包括矩形框、点和Mask。此外，我们还进行了SAM的encoder和decoder的微调，以获得一个高性能的SAM-Med2D。最后，我们进行了全面的评估和分析，以investigate SAM-Med2D在医疗图像分割中的性能，包括不同Modalities、生物结构和器官。同时，我们验证了SAM-Med2D在MICCAI 2023挑战赛中的通用能力。总的来说，我们的方法在医疗图像分割中表现出了显著的性能和通用能力，与SAM相比。
</details></li>
</ul>
<hr>
<h2 id="GREC-Generalized-Referring-Expression-Comprehension"><a href="#GREC-Generalized-Referring-Expression-Comprehension" class="headerlink" title="GREC: Generalized Referring Expression Comprehension"></a>GREC: Generalized Referring Expression Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16182">http://arxiv.org/abs/2308.16182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henghuiding/grefcoco">https://github.com/henghuiding/grefcoco</a></li>
<li>paper_authors: Shuting He, Henghui Ding, Chang Liu, Xudong Jiang</li>
<li>for: 本研究旨在推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。Translation:</li>
<li>for: 本研究的目标是推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。<details>
<summary>Abstract</summary>
The objective of Classic Referring Expression Comprehension (REC) is to produce a bounding box corresponding to the object mentioned in a given textual description. Commonly, existing datasets and techniques in classic REC are tailored for expressions that pertain to a single target, meaning a sole expression is linked to one specific object. Expressions that refer to multiple targets or involve no specific target have not been taken into account. This constraint hinders the practical applicability of REC. This study introduces a new benchmark termed as Generalized Referring Expression Comprehension (GREC). This benchmark extends the classic REC by permitting expressions to describe any number of target objects. To achieve this goal, we have built the first large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of expressions: those referring to multiple targets, expressions with no specific target, and the single-target expressions. The design of GREC and gRefCOCO ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a GREC method implementation code, and GREC evaluation code are available at https://github.com/henghuiding/gRefCOCO.
</details>
<details>
<summary>摘要</summary>
“目的是实现文本描述中的物体引用表达（REC）。现有的dataset和技术仅适用于单一目标的表达，这限制了REC的实际应用。本研究引入了一个新的benchmark，称为通用 Referring Expression Comprehension（GREC）。GREC扩展了传统REC，允许表达描述任意数量的目标物体。为 достичь这个目标，我们建立了第一个大规模的GREC dataset，名为gRefCOCO。这个dataset包括了多个目标、无 especified 目标和单一目标的表达。GREC和gRefCOCO的设计保证与传统REC的相容性。提供了GREC方法实现代码、GREC评估代码和gRefCOCO dataset，可以在https://github.com/henghuiding/gRefCOCO中下载。”
</details></li>
</ul>
<hr>
<h2 id="MMVP-Motion-Matrix-based-Video-Prediction"><a href="#MMVP-Motion-Matrix-based-Video-Prediction" class="headerlink" title="MMVP: Motion-Matrix-based Video Prediction"></a>MMVP: Motion-Matrix-based Video Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16154">http://arxiv.org/abs/2308.16154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction">https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction</a></li>
<li>paper_authors: Yiqi Zhong, Luming Liang, Ilya Zharkov, Ulrich Neumann</li>
<li>For: 本研究旨在解决视频预测中的中心挑战，即从图像帧中预测物体未来的运动，同时保持物体的外观一致性 across frames。* Methods: 该研究提出了一种可以批处理的两核心视频预测框架，即运动矩阵基于视频预测（MMVP）。与前一些方法不同的是，MMVP 将动作和外观信息解耦，通过构建外观不关心的运动矩阵来实现。这种设计提高了视频预测的准确率和效率，并降低了模型的大小。* Results: 广泛的实验结果表明，MMVP 在公共数据集上比前一些状态之前的系统更高（约1 db PSNR，UCF Sports），并且在远远小于前一些系统的模型大小（84% 或更小）下达成了这些result。请参考 <a target="_blank" rel="noopener" href="https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction">https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction</a> 获取官方代码和使用的数据集。<details>
<summary>Abstract</summary>
A central challenge of video prediction lies where the system has to reason the objects' future motions from image frames while simultaneously maintaining the consistency of their appearances across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the official code and the datasets used in this paper.
</details>
<details>
<summary>摘要</summary>
中心挑战：视频预测需要系统根据图像帧来预测物体未来运动，同时保持物体在帧之间的外观一致性。这篇论文提出了一种端到端训练的两核心视频预测框架——动力矩阵基于视频预测（MMVP），解决这个挑战。不同于之前的方法通常在同一组模块中处理运动预测和外观维持，MMVP 将运动和外观信息分离开来，通过构建不同帧的特征小块之间的应用无关动力矩阵来实现。这种设计提高了视频预测的准确性和效率，并减少模型的大小。经过广泛的实验，我们发现MMVP在公共数据集上比前一代系统大幅提高（约1 db PSNR、UCF Sports），而且模型的大小减少了84%以下。请参考https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction  для官方代码和使用的数据集。
</details></li>
</ul>
<hr>
<h2 id="Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI"><a href="#Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI" class="headerlink" title="Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI"></a>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16150">http://arxiv.org/abs/2308.16150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas</li>
<li>for: 这篇论文的目的是提出一种不需要手动分类的无监督异常分割方法，可以检测在训练中无法处理的异常模式，以提高模型的可靠性，特别在医疗影像领域。</li>
<li>methods: 这篇论文提出的方法基于两个基本想法。首先，我们提出使用类型转换为机制来实现异常检测。图像转移模型学习特定体部特征的对应关系，因此在训练时不能转换特定的体部或图像模式，这就 enables their segmentation。其次，我们结合图像转移和封页 conditional diffusion 模型，尝试将遮盾区域下的组织视网膜，进一步暴露未知模式，当生成模型无法重建它们时。</li>
<li>results: 我们在 BraTS2021 多Modal MRI 的代理任务上训练这种方法，并在该任务上进行测试。我们的结果显示，我们的方法与先前的无监督方法相比，在图像重建和干扰中得到了比较好的结果。<details>
<summary>Abstract</summary>
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to `imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.
</details>
<details>
<summary>摘要</summary>
不监督异常分割Targets patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to 'imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is written in a formal, academic style, which is appropriate for a research paper. I made sure to preserve the original meaning and structure of the text as much as possible, while also ensuring that the translation is grammatically correct and easy to understand.Please note that the translation is a machine translation, and it may not be perfect. There may be some nuances or idiomatic expressions that are lost in translation. If you have any specific questions or need further clarification, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention"><a href="#CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention" class="headerlink" title="CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention"></a>CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16145">http://arxiv.org/abs/2308.16145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanghx-iim-ahu/circleformer">https://github.com/zhanghx-iim-ahu/circleformer</a></li>
<li>paper_authors: Hengxu Zhang, Pengpeng Liang, Zhiyong Sun, Bo Song, Erkang Cheng</li>
<li>for: 这篇论文是针对医疗影像中圆形物体检测的研究，尤其是精准地检测体内组织中的圆形核。</li>
<li>methods: 本文使用了Transformer架构，并将圆形查询嵌入到Transformer解oder中，逐步精确地检测圆形物体。另外，本文还引入了圆形对偶探索模组，以计算圆形查询和影像特征之间的相似性。</li>
<li>results: 本文在公共的MoNuSeg数据集上进行了圆形核检测和分类任务的评估，并取得了比州前方法更好的成绩。此外，本文还进行了各组件效果的验证。<details>
<summary>Abstract</summary>
Both CNN-based and Transformer-based object detection with bounding box representation have been extensively studied in computer vision and medical image analysis, but circular object detection in medical images is still underexplored. Inspired by the recent anchor free CNN-based circular object detection method (CircleNet) for ball-shape glomeruli detection in renal pathology, in this paper, we present CircleFormer, a Transformer-based circular medical object detection with dynamic anchor circles. Specifically, queries with circle representation in Transformer decoder iteratively refine the circular object detection results, and a circle cross attention module is introduced to compute the similarity between circular queries and image features. A generalized circle IoU (gCIoU) is proposed to serve as a new regression loss of circular object detection as well. Moreover, our approach is easy to generalize to the segmentation task by adding a simple segmentation branch to CircleFormer. We evaluate our method in circular nuclei detection and segmentation on the public MoNuSeg dataset, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well. Our code is released at: \url{https://github.com/zhanghx-iim-ahu/CircleFormer}.
</details>
<details>
<summary>摘要</summary>
历史上，CNN和Transformer两种方法在计算机视觉和医学图像分析中进行了广泛的研究，但医学图像中径向物体检测仍然受到了相对较少的关注。在这篇论文中，我们提出了一种基于Transformer的径向医学对象检测方法，称为CircleFormer。该方法使用Transformer预测器中的径向查询来逐步精细地检测径向对象结果。此外，我们还提出了一种径向圆点对准模块，以计算径向查询和图像特征之间的相似性。此外，我们还提出了一种新的径向圆点IOU（gCIoU），用于衡量径向对象检测结果的准确性。此外，我们的方法易于扩展到分割任务，只需要在CircleFormer上添加一个简单的分割分支即可。我们在公共的MoNuSeg数据集上进行了径向核体检测和分割任务的实验，结果表明，我们的方法在与状态艺术方法相比表现出色。此外，我们还进行了一些缺省分析，以验证每个组件的有效性。我们的代码在：\url{https://github.com/zhanghx-iim-ahu/CircleFormer}。
</details></li>
</ul>
<hr>
<h2 id="MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision"><a href="#MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision" class="headerlink" title="MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision"></a>MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16139">http://arxiv.org/abs/2308.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianning Li, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca, Naida Solak, Gian Marco Melito, Afaque R. Memon, Xiaojun Chen, Jan Stefan Kirschke, Ezequiel de la Rosa, Patrich Ferndinand Christ, Hongwei Bran Li, David G. Ellis, Michele R. Aizenberg, Sergios Gatidis, Thomas Kuestner, Nadya Shusharina, Nicholas Heller, Vincent Andrearczyk, Adrien Depeursinge, Mathieu Hatt, Anjany Sekuboyina, Maximilian Loeffler, Hans Liebl, Reuben Dorent, Tom Vercauteren, Jonathan Shapey, Aaron Kujawa, Stefan Cornelissen, Patrick Langenhuizen, Achraf Ben-Hamadou, Ahmed Rekik, Sergi Pujades, Edmond Boyer, Federico Bolelli, Costantino Grana, Luca Lumetti, Hamidreza Salehi, Jun Ma, Yao Zhang, Ramtin Gharleghi, Susann Beier, Eduardo A. Garza-Villarreal, Thania Balducci, Diego Angeles-Valdez, Roberto Souza, Leticia Rittner, Richard Frayne, Yuanfeng Ji, Soumick Chatterjee, Andreas Nuernberger, Joao Pedrosa, Carlos Ferreira, Guilherme Aresta, Antonio Cunha, Aurelio Campilho, Yannick Suter, Jose Garcia, Alain Lalande, Emmanuel Audenaert, Claudia Krebs, Timo Van Leeuwen, Evie Vereecke, Rainer Roehrig, Frank Hoelzle, Vahid Badeli, Kathrin Krieger, Matthias Gunzer, Jianxu Chen, Amin Dada, Miriam Balzer, Jana Fragemann, Frederic Jonske, Moritz Rempe, Stanislav Malorodov, Fin H. Bahnsen, Constantin Seibold, Alexander Jaus, Ana Sofia Santos, Mariana Lindo, Andre Ferreira, Victor Alves, Michael Kamp, Amr Abourayya, Felix Nensa, Fabian Hoerst, Alexander Brehmer, Lukas Heine, Lars E. Podleska, Matthias A. Fink, Julius Keyl, Konstantinos Tserpes, Moon-Sung Kim, Shireen Elhabian, Hans Lamecker, Dzenan Zukic, Beatriz Paniagua, Christian Wachinger, Martin Urschler, Luc Duong, Jakob Wasserthal, Peter F. Hoyer, Oliver Basu, Thomas Maal, Max J. H. Witjes, Ping Luo, Bjoern Menze, Mauricio Reyes, Christos Davatzikos, Behrus Puladi, Jens Kleesiek, Jan Egger</li>
<li>for: The paper is written to introduce MedShapeNet, a large collection of anatomical shapes and 3D surgical instrument models for medical image analysis.</li>
<li>methods: The paper uses a variety of methods to create and annotate the shapes in MedShapeNet, including direct modeling on imaging data and paired data annotations.</li>
<li>results: The paper reports that MedShapeNet currently includes over 100,000 medical shapes and provides a freely available repository of 3D models for extended reality and medical 3D printing, with the potential to adapt state-of-the-art vision algorithms to solve critical medical problems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍医疗影像分析的MedShapeNet，一个大量的生物形态和医疗器械3D模型集合。</li>
<li>methods: 论文使用了多种方法来创建和注释MedShapeNet中的形态，包括直接在医疗数据上模型和对应的数据注释。</li>
<li>results: 论文报告了MedShapeNet目前已经包含了超过100,000个医疗形态，并提供了一个免费的3D模型库，用于扩展现实（虚拟现实、增强现实、混合现实）和医疗3D打印。<details>
<summary>Abstract</summary>
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to facilitate the translation of data-driven vision algorithms to medical applications, and it extends the opportunities to adapt SOTA vision algorithms to solve critical medical problems. Besides, the majority of the medical shapes in MedShapeNet are modeled directly on the imaging data of real patients, and therefore it complements well existing shape benchmarks comprising of computer-aided design (CAD) models. MedShapeNet currently includes more than 100,000 medical shapes, and provides annotations in the form of paired data. It is therefore also a freely available repository of 3D models for extended reality (virtual reality - VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This white paper describes in detail the motivations behind MedShapeNet, the shape acquisition procedures, the use cases, as well as the usage of the online shape search portal: https://medshapenet.ikim.nrw/
</details>
<details>
<summary>摘要</summary>
我们介绍MedShapeNet，一个大量医学形状（例如骨头、器官、血管）和3D医疗工具模型的集合。在深度学习时代之前，医学像分析中的统计形状模型（SSM）的广泛应用是证明医学数据中的形状很常被使用。然而，现在医学影像分析中的州际精算法（SOTA）都是以 voxel 为基础的。在计算机视觉中，形状（包括 voxel 占用网格、 mesh、点 cloud 和隐藏面模型）是3D数据的偏好表示方式，可见于许多shape相关的学会论文（如IEEE/CVF会议 on Computer Vision and Pattern Recognition（CVPR））以及形状库（如ShapeNet about 51,300 models和Princeton ModelNet 127,915 models）在计算机视觉研究中的增长 популяр性。MedShapeNet 是为了促进资料驱动的 computer vision 算法对医学应用的转译，而创建的一个替代方案，并延伸了适用 SOTA  vision 算法解决医学问题的机会。此外，MedShapeNet 中的大多数医学形状是直接从医疗影像数据中模型，因此与现有的 CAD 模型集成完美。MedShapeNet 目前包含超过 100,000 个医学形状，并提供双数据标签。因此，它还是一个免费可用的 3D 模型存储库，用于延伸现実（虚拟现実 - VR、增强现実 - AR、混合现実 - MR）和医疗 3D 印刷。本白皮书将详细介绍 MedShapeNet 的动机、形状取得程序、使用情况以及在线形状搜寻 Portal：https://medshapenet.ikim.nrw/
</details></li>
</ul>
<hr>
<h2 id="Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation"><a href="#Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation" class="headerlink" title="Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation"></a>Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16110">http://arxiv.org/abs/2308.16110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kobeshegu/sdtm-gan-acmmm-2023">https://github.com/kobeshegu/sdtm-gan-acmmm-2023</a></li>
<li>paper_authors: Mengping Yang, Zhe Wang, Wenyi Feng, Qian Zhang, Ting Xiao</li>
<li>for: 这篇论文的目的是提出一种新的几何图像生成模型，以实现几何图像生成中的内在Semantic Modulation和全局结构检测。</li>
<li>methods: 这篇论文使用的方法包括内在Local Representation的Semantic Modulation，以及全局结构检测器（StructD）的开发，以及鼓励模型识别频率信号的技术。</li>
<li>results: 这篇论文的实验结果显示，这种新的几何图像生成模型可以在三个popular dataset上 achieves state-of-the-art的Synthesis Performance，并且可以与现有的模型集成，以提高其性能。<details>
<summary>Abstract</summary>
Few-shot image generation, which aims to produce plausible and diverse images for one category given a few images from this category, has drawn extensive attention. Existing approaches either globally interpolate different images or fuse local representations with pre-defined coefficients. However, such an intuitive combination of images/features only exploits the most relevant information for generation, leading to poor diversity and coarse-grained semantic fusion. To remedy this, this paper proposes a novel textural modulation (TexMod) mechanism to inject external semantic signals into internal local representations. Parameterized by the feedback from the discriminator, our TexMod enables more fined-grained semantic injection while maintaining the synthesis fidelity. Moreover, a global structural discriminator (StructD) is developed to explicitly guide the model to generate images with reasonable layout and outline. Furthermore, the frequency awareness of the model is reinforced by encouraging the model to distinguish frequency signals. Together with these techniques, we build a novel and effective model for few-shot image generation. The effectiveness of our model is identified by extensive experiments on three popular datasets and various settings. Besides achieving state-of-the-art synthesis performance on these datasets, our proposed techniques could be seamlessly integrated into existing models for a further performance boost.
</details>
<details>
<summary>摘要</summary>
“几帧图像生成”，它目的是生成一个分类中的具有实际性和多样性的图像，只需要几帧图像作为输入。现有的方法可以全面 interpolate 不同的图像或者融合本地表现和预先定义的系数。然而，这种直觉的图像/特征融合只是利用最相关的信息进行生成，从而导致低的多样性和粗糙的 semantic 融合。为了解决这个问题，这篇论文提出了一个新的文本调控（TexMod）机制，可以将外部 semantics 信号注入到内部的本地表现中。这个 TexMod 由 discriminator 的反馈参数化，可以实现更细grained的 semantic 注入，同时维持生成的实际性。此外，我们还开发了一个全球结构 discriminator（StructD），可以明确地导引模型生成具有合理的配置和架构的图像。此外，我们还强调了模型的频率意识，通过让模型能够识别频率信号。通过这些技术，我们建立了一个新的和有效的几帧图像生成模型。我们的模型在三个流行的数据集上进行了广泛的实验，并在不同的设定下表现出色。除了在这些数据集上达到了现有的州域性synthesis 性能外，我们的提出的技术还可以与现有的模型进行整合，以获得更高的性能。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.CV_2023_08_31/" data-id="clm0t8dz50046v78853s8dr7e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.AI_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T12:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.AI_2023_08_31/">cs.AI - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-Vision-based-Pursuit-Evasion-Robot-Policies"><a href="#Learning-Vision-based-Pursuit-Evasion-Robot-Policies" class="headerlink" title="Learning Vision-based Pursuit-Evasion Robot Policies"></a>Learning Vision-based Pursuit-Evasion Robot Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16185">http://arxiv.org/abs/2308.16185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Bajcsy, Antonio Loquercio, Ashish Kumar, Jitendra Malik</li>
<li>for: 这 paper 是为了学习在实际世界中进行策略性机器人行为的挑战，特别是在追逐逃脱交互中。</li>
<li>methods: 这 paper 使用了将这个难以解决的问题转化为一个监督学习问题，其中一个全 observable 机器人策略生成了一个 partially observable 机器人策略的超vision。</li>
<li>results: 研究发现，在追逐逃脱交互中，部分可见的机器人策略的训练信号质量取决于两个关键因素：逃脱者的行为均衡和模型假设的强度。这 paper 还在实际中部署了这种策略，并在物理四脚机器人上进行了追逐逃脱交互。<details>
<summary>Abstract</summary>
Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/
</details>
<details>
<summary>摘要</summary>
学习策略性机器人行为 -- 如追逐避免交互 -- 在真实世界环境中是非常困难的。它需要利用交互动力学，并通过物理状态和潜在意图不确定性进行规划。在这篇论文中，我们将这个难以解决的问题转化为一个监督学习问题，其中一个完全可见的机器人政策生成了一个部分可见的追逐者政策的监督信号。我们发现了两个关键因素对 partially-observable pursuer policy 的质量监督信号产生影响：逃脱者的行为均衡和优化程度，以及完全可见政策中模型假设的强度。我们将我们的政策部署到一个物理四脚机器人上，并使用RGB-D摄像头进行追逐逃脱交互。尽管所有挑战，感知约束促使机器人在不确定时收集信息，从杂乱测量中预测意图，并在预测不准确时预测以 intercept。项目首页：https://abajcsy.github.io/vision-based-pursuit/
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Answers-from-any-Language-Model-via-Intrinsic-and-Extrinsic-Confidence-Assessment"><a href="#Quantifying-Uncertainty-in-Answers-from-any-Language-Model-via-Intrinsic-and-Extrinsic-Confidence-Assessment" class="headerlink" title="Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment"></a>Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16175">http://arxiv.org/abs/2308.16175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuhai Chen, Jonas Mueller</li>
<li>for: detecting bad and speculative answers from a pretrained Large Language Model</li>
<li>methods: estimating a numeric confidence score for any output generated by the LLM, combining intrinsic and extrinsic assessments of confidence</li>
<li>results: more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures, and can obtain more accurate responses by sampling multiple responses and considering the one with the highest confidence score.Here’s the full translation in Simplified Chinese:</li>
<li>for: 用于探测预训练的大语言模型中的坏和推测答案</li>
<li>methods: 通过估算任何输出生成的数字信任分数，并将内在和外在评估信任相结合</li>
<li>results: 比alternative uncertainty estimation procedures更加准确地认定大语言模型的错误答案，并可以通过采样多个响应并考虑最高信任分的响应来获得更加准确的答案，无需额外训练步骤。<details>
<summary>Abstract</summary>
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps.
</details>
<details>
<summary>摘要</summary>
我们介绍BSDetector，一种方法用于检测预训练大语言模型生成的差异和推测答案的 numeric 信任分数。我们的不确定性评估技术适用于任何可以通过黑色盒API访问的 LLM，并将内在和外在评估信任综合到一个 Trustworthiness 估计中。我们的方法非常通用，可以应用于今天最好的 LLM 中的任何一个（训练数据未知）。通过点些额外计算，用户可以通过 LLM API 获得同样的回答和信任估计，从而了解不要信任这个回答。在关闭和开放问答benchmark上进行实验，我们发现BSDetector可以更准确地确定 LLM 的错误回答，比alternative uncertainty estimation方法更好。此外，我们可以通过选择 LLM 生成的多个回答中信任分数最高的一个，以获得更准确的回答，无需任何额外训练步骤。
</details></li>
</ul>
<hr>
<h2 id="Algebraic-Topological-and-Mereological-Foundations-of-Existential-Granules"><a href="#Algebraic-Topological-and-Mereological-Foundations-of-Existential-Granules" class="headerlink" title="Algebraic, Topological, and Mereological Foundations of Existential Granules"></a>Algebraic, Topological, and Mereological Foundations of Existential Granules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16157">http://arxiv.org/abs/2308.16157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mani A</li>
<li>for: 本研究提出了新的存在主义 грануル（EG）概念，用于自我determination和环境互动。</li>
<li>methods: 本研究采用了 алгебраические、topological和merological方面的Characterization来描述EG。</li>
<li>results: 研究显示EG可以适应多种理论框架（axioms, adaptive等），并可以应用于分类问题和可能的总结扩展。 Additionally, many open problems are posed and directions provided.<details>
<summary>Abstract</summary>
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们发明了新的存在ential granule概念，它们自己决定了自己的性质。这些granule从算术、拓扑和merkolojical的视角来 caracterized。存在ential granule是指 initially自己决定的granule，然后与环境交互。例如，granular balls这种概念，虽然在以前的作品中不充分定义、算法确定和理论化不够，但它们已经在粗集和软计算应用中使用。我们显示它们适合多种理论框架（axioms, adaptive等）的granular computing。characterization是为了开发算法、应用到分类问题以及可能的总体方法的数学基础。此外，我们还提出了许多开放问题和方向。Note: "existential granule" is a term I translated as "存在ential granule" in Simplified Chinese, which is a combination of "existential" and "granule".
</details></li>
</ul>
<hr>
<h2 id="Jais-and-Jais-chat-Arabic-Centric-Foundation-and-Instruction-Tuned-Open-Generative-Large-Language-Models"><a href="#Jais-and-Jais-chat-Arabic-Centric-Foundation-and-Instruction-Tuned-Open-Generative-Large-Language-Models" class="headerlink" title="Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models"></a>Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16149">http://arxiv.org/abs/2308.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing</li>
<li>for: 这个研究是为了开发一个新的阿拉伯语中心的基础模型和一个基于 instrucion 的大语言模型（LLM）。</li>
<li>methods: 这些模型基于 GPT-3 的解码器只架构，并在混合阿拉伯语和英语文本中进行预训练。它们具有13亿个参数，在阿拉伯语知识和理解方面表现出色，与任何现有的开放阿拉伯语和多语言模型相比，具有明显的优势。</li>
<li>results: 这些模型在英语中也能够与英语中心的开放模型相比，即使只使用了 Much less English data。我们提供了模型训练、调整、安全对齐和评估的详细描述。我们发布了两个开放版本的模型：基础 Jais 模型和基于 instrucion 的 Jais-chat 变体，以促进阿拉伯语 LLM 的研究。可以在 <a target="_blank" rel="noopener" href="https://huggingface.co/inception-mbzuai/jais-13b-chat">https://huggingface.co/inception-mbzuai/jais-13b-chat</a> 上下载。<details>
<summary>Abstract</summary>
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat
</details>
<details>
<summary>摘要</summary>
我们介绍Jais和Jais-chat，这两个新的阿拉伯语中心基础和指导下的开放生成大语言模型（LLM）。这两个模型基于GPT-3核心Only架构，并在混合阿拉伯语和英语文本中进行预训练，包括不同编程语言的源代码。它们拥有13亿个参数，在阿拉伯语中表现出较好的知识和理解能力，比任何现有的开放阿拉伯语和多语言模型都更出色，根据广泛的评估。此外，这些模型在英语中也能够与英语中心的开放模型相比，即使它们在英语数据上进行了训练。我们提供了模型训练、调整、安全对齐和评估的详细描述。我们发布了两个开放版本的模型——基础Jais模型和指导下的Jais-chat变体——以便促进阿拉伯语LLM的研究。可以在https://huggingface.co/inception-mbzuai/jais-13b-chat上下载。
</details></li>
</ul>
<hr>
<h2 id="LM-Infinite-Simple-On-the-Fly-Length-Generalization-for-Large-Language-Models"><a href="#LM-Infinite-Simple-On-the-Fly-Length-Generalization-for-Large-Language-Models" class="headerlink" title="LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"></a>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16137">http://arxiv.org/abs/2308.16137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</li>
<li>for: 本研究目的是提高Transformer大型自然语言模型（LLM）在不同领域的性能，并在长序列上进行更长的理解和推理。</li>
<li>methods: 本研究使用了Lambda形掩码和距离限制，不需要参数更新或学习，可以在现有的LLM上进行实时长度总结。</li>
<li>results: LM-Infinite可以在ArXiv和OpenWebText2数据集上 Generate fluently和高质量的输出，并且在下游任务中继续工作，而vanilla模型在训练长度以下就会失败。 decoding速度提高2.72倍。<details>
<summary>Abstract</summary>
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution for on-the-fly length generalization, LM-Infinite, which involves only a $\Lambda$-shaped attention mask and a distance limit while requiring no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computational efficient with $O(n)$ time and space, and demonstrates consistent fluency and generation quality to as long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstream task such as passkey retrieval, it continues to work on inputs much longer than training lengths where vanilla models fail immediately.
</details>
<details>
<summary>摘要</summary>
Recently, there have been significant advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for more complex tasks, they often need to conduct longer reasoning processes or understand larger contexts. However, when dealing with long sequences, LLMs often experience length generalization failure. Most pre-training schemes only train on sequences of a fixed length (such as 2048 for LLaMa), which can cause LLMs to struggle to generate fluent texts or perform downstream tasks when faced with longer contexts. Common solutions such as fine-tuning on longer corpora can be time-consuming and require careful process design. To more efficiently leverage the generation capacity of existing LLMs, we investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution called LM-Infinite, which involves a $\Lambda$-shaped attention mask and a distance limit, and requires no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and space, and demonstrates consistent fluency and generation quality up to 32k tokens on ArXiv and OpenWebText2 datasets, with a decoding speedup of 2.72x. On downstream tasks such as passkey retrieval, it continues to work on inputs much longer than training lengths, where vanilla models fail immediately.
</details></li>
</ul>
<hr>
<h2 id="CorrEmbed-Evaluating-Pre-trained-Model-Image-Similarity-Efficacy-with-a-Novel-Metric"><a href="#CorrEmbed-Evaluating-Pre-trained-Model-Image-Similarity-Efficacy-with-a-Novel-Metric" class="headerlink" title="CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric"></a>CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16126">http://arxiv.org/abs/2308.16126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Audun Kagnes Borgersen, Morten Goodwin, Jivitesh Sharma, Tobias Aasmoe, Mari Leonhardsen, Gro Herredsvela Rørvik</li>
<li>for: 这篇论文是为了评估预训练的计算机视觉模型中的图像嵌入而写的。</li>
<li>methods: 这篇论文使用了一种新的方法 named CorrEmbed，该方法计算图像嵌入中距离与人工生成的标签向量距离之间的相关性。</li>
<li>results: 该方法可以评估预训练的计算机视觉模型中的图像嵌入，并发现一些模型的性能与标签相关性之间存在直线关系。此外，该方法还可以找出具有不同特征的模型。<details>
<summary>Abstract</summary>
Detecting visually similar images is a particularly useful attribute to look to when calculating product recommendations. Embedding similarity, which utilizes pre-trained computer vision models to extract high-level image features, has demonstrated remarkable efficacy in identifying images with similar compositions. However, there is a lack of methods for evaluating the embeddings generated by these models, as conventional loss and performance metrics do not adequately capture their performance in image similarity search tasks.   In this paper, we evaluate the viability of the image embeddings from numerous pre-trained computer vision models using a novel approach named CorrEmbed. Our approach computes the correlation between distances in image embeddings and distances in human-generated tag vectors. We extensively evaluate numerous pre-trained Torchvision models using this metric, revealing an intuitive relationship of linear scaling between ImageNet1k accuracy scores and tag-correlation scores. Importantly, our method also identifies deviations from this pattern, providing insights into how different models capture high-level image features.   By offering a robust performance evaluation of these pre-trained models, CorrEmbed serves as a valuable tool for researchers and practitioners seeking to develop effective, data-driven approaches to similar item recommendations in fashion retail.
</details>
<details>
<summary>摘要</summary>
检测类似图像是一项非常有用的特征，特别是在计算产品推荐时。嵌入相似性，使用预训练的计算机视觉模型提取高级图像特征，已经证明了惊人的效果。然而，没有方法来评估由这些模型生成的嵌入，因为常见的损失和性能指标不能够准确地捕捉图像相似搜索任务中的表现。在这篇论文中，我们评估了许多预训练的Torchvision模型的嵌入，使用一种新的方法 named CorrEmbed。我们的方法计算图像嵌入中距离与人工生成的标签 vector 距离之间的相关性。我们广泛评估了多种预训练的Torchvision模型，发现图像1000分类准确率和标签相关性分数之间存在直线关系。更重要的是，我们的方法还发现了不同模型捕捉高级图像特征的偏差，提供了对发展有效数据驱动方法的深入理解。  By offering a robust performance evaluation of these pre-trained models, CorrEmbed serves as a valuable tool for researchers and practitioners seeking to develop effective, data-driven approaches to similar item recommendations in fashion retail.
</details></li>
</ul>
<hr>
<h2 id="Response-Emergent-analogical-reasoning-in-large-language-models"><a href="#Response-Emergent-analogical-reasoning-in-large-language-models" class="headerlink" title="Response: Emergent analogical reasoning in large language models"></a>Response: Emergent analogical reasoning in large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16118">http://arxiv.org/abs/2308.16118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hodeld/emergent_analogies_llm_fork">https://github.com/hodeld/emergent_analogies_llm_fork</a></li>
<li>paper_authors: Damian Hodel, Jevin West</li>
<li>for: 研究表明大语言模型如GPT-3已经获得了泛化逻辑能力，能够解决各种类比问题。</li>
<li>methods: 作者使用了GPT-3进行实验，测试其在不同类比问题中的能力。</li>
<li>results: 试验结果表明，GPT-3无法解决简单的类比问题， zero-shot 逻辑是一个过度的laim需要更多的证据。<details>
<summary>Abstract</summary>
In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
</details>
<details>
<summary>摘要</summary>
根据《自然人类行为》杂志（Webb、Holyoak、Lu，2023）的论文，作者认为大语言模型如GPT-3已经获得了zero-shot解决广泛的比喻问题的能力。在这个回应中，我们提供了字符串比喻的counterexample。在我们的测试中，GPT-3无法解决even the easiest variants of the problems presented in the original paper。zero-shot reasoning是一个非凡的声明，需要非凡的证据。我们在实验中没看到这种证据。为了强化人类类似的理解，如zero-shot reasoning，领域应该开发approaches来排除数据记忆。
</details></li>
</ul>
<hr>
<h2 id="survex-an-R-package-for-explaining-machine-learning-survival-models"><a href="#survex-an-R-package-for-explaining-machine-learning-survival-models" class="headerlink" title="survex: an R package for explaining machine learning survival models"></a>survex: an R package for explaining machine learning survival models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16113">http://arxiv.org/abs/2308.16113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Spytek, Mateusz Krzyziński, Sophie Hanna Langbein, Hubert Baniecki, Marvin N. Wright, Przemysław Biecek</li>
<li>for: The paper is written for those who use survival models in biomedical research and healthcare applications, and aims to provide a user-friendly tool to explain the internal operations and prediction rationales of these models.</li>
<li>methods: The paper proposes the survex R package, which applies explainable artificial intelligence techniques to survival models, allowing users to understand and diagnose the models, improve their reliability, and detect biases.</li>
<li>results: The proposed software can provide insights into the decision-making process of survival models, such as variable effects and importances, and can promote transparency and responsibility in sensitive areas like biomedical research and healthcare applications.<details>
<summary>Abstract</summary>
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Grandma-Karl-is-27-years-old-–-research-agenda-for-pseudonymization-of-research-data"><a href="#Grandma-Karl-is-27-years-old-–-research-agenda-for-pseudonymization-of-research-data" class="headerlink" title="Grandma Karl is 27 years old – research agenda for pseudonymization of research data"></a>Grandma Karl is 27 years old – research agenda for pseudonymization of research data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16109">http://arxiv.org/abs/2308.16109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Volodina, Simon Dobnik, Therese Lindström Tiedemann, Xuan-Son Vu</li>
<li>for: 本研究旨在探讨 pseudonymization 的应用在不结构化数据中，以保护作者身份和隐私信息。</li>
<li>methods: 本研究提出了一个研究议程，包括对 pseudonymization 对不结构化数据的影响（如可读性和语言评估），以及 pseudonymization 的效果是否能够保护作者身份。</li>
<li>results: 研究计划通过开发 Context-sensitive 算法来检测、标记和替换个人信息，以保护作者身份和隐私信息。这个研究项目在 pseudonymization 方面提供了27年的探索和发展空间。<details>
<summary>Abstract</summary>
Accessibility of research data is critical for advances in many research fields, but textual data often cannot be shared due to the personal and sensitive information which it contains, e.g names or political opinions. General Data Protection Regulation (GDPR) suggests pseudonymization as a solution to secure open access to research data, but we need to learn more about pseudonymization as an approach before adopting it for manipulation of research data. This paper outlines a research agenda within pseudonymization, namely need of studies into the effects of pseudonymization on unstructured data in relation to e.g. readability and language assessment, as well as the effectiveness of pseudonymization as a way of protecting writer identity, while also exploring different ways of developing context-sensitive algorithms for detection, labelling and replacement of personal information in unstructured data. The recently granted project on pseudonymization Grandma Karl is 27 years old addresses exactly those challenges.
</details>
<details>
<summary>摘要</summary>
研究数据的可 accessible性是多个研究领域的进步的关键，但文本数据经常无法被共享，因为它们包含个人敏感信息，如名字或政治意见。欧盟通信标准（GDPR）建议使用 pseudonymization 作为保护开放研究数据的解决方案，但我们需要更多关于 pseudonymization 的研究，以便在处理研究数据时采取有效的保护措施。这篇论文提出了一个关于 pseudonymization 的研究计划，即对不结构化数据中的个人信息进行探测、标记和替换的Context-sensitive算法的开发，以及pseudonymization 对写者身份的保护效果和可读性的影响。这些挑战 precisley 由Recently granted project on pseudonymization Grandma Karl is 27 years old 所解决。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.AI_2023_08_31/" data-id="clm0t8dy80017v788hruxc3lz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.LG_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T10:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.LG_2023_08_31/">cs.LG - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Graph-Coarsening-Weather-and-Weekday-Prediction-with-London’s-Bike-Sharing-Service-using-GNN"><a href="#Spatial-Graph-Coarsening-Weather-and-Weekday-Prediction-with-London’s-Bike-Sharing-Service-using-GNN" class="headerlink" title="Spatial Graph Coarsening: Weather and Weekday Prediction with London’s Bike-Sharing Service using GNN"></a>Spatial Graph Coarsening: Weather and Weekday Prediction with London’s Bike-Sharing Service using GNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16122">http://arxiv.org/abs/2308.16122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuta Sato, Pak Hei Lam, Shruti Gupta, Fareesah Hussain</li>
<li>for: 预测伦敦天气和工作日期</li>
<li>methods: 使用图ael Neural Network（GNN）模型，新引入（i）图特征 concatenation 算子和（ii）基于地理连续性的图粗化算子“Spatial Graph Coarsening”</li>
<li>results: 模型在验证数据集上出现较好的cross-entropy损失和准确率表现，超过基线模型<details>
<summary>Abstract</summary>
This study introduced the use of Graph Neural Network (GNN) for predicting the weather and weekday of a day in London, from the dataset of Santander Cycles bike-sharing system as a graph classification task. The proposed GNN models newly introduced (i) a concatenation operator of graph features with trained node embeddings and (ii) a graph coarsening operator based on geographical contiguity, namely "Spatial Graph Coarsening". With the node features of land-use characteristics and number of households around the bike stations and graph features of temperatures in the city, our proposed models outperformed the baseline model in cross-entropy loss and accuracy of the validation dataset.
</details>
<details>
<summary>摘要</summary>
这个研究介绍了使用图 neural network (GNN) 预测伦敦的天气和工作日，基于 Santander Cycles 自行车共享系统的图分类任务。我们提出的 GNN 模型新增了（i）图特征 concatenation 操作符和（ii）基于地理邻近性的图粗化操作符，即 "Spatial Graph Coarsening"。使用附近站点的节点特征（包括土地用途特征和周围居民数）和图特征（包括城市气温），我们的提议模型在验证集中的十字环比较和准确率超过了基eline模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.LG_2023_08_31/" data-id="clm0t8e0m007mv788apzqa34f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.SD_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T15:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.SD_2023_08_30/">cs.SD - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis"><a href="#A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis" class="headerlink" title="A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis"></a>A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15422">http://arxiv.org/abs/2308.15422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Hayes, Jordie Shier, György Fazekas, Andrew McPherson, Charalampos Saitis</li>
<li>for: 这篇论文主要针对的是音乐和语音合成领域中的差分可控数字信号处理技术的应用。</li>
<li>methods: 该论文使用了差分可控数字信号处理技术，其中包括后退传播和权重调整等方法。</li>
<li>results: 该论文对音乐和语音合成任务进行了评估，并结果表明这些技术可以提高音乐和语音的生成质量。同时，论文还提出了一些未来研究的挑战，如优化症状、真实世界情况下的稳定性和设计决策。<details>
<summary>Abstract</summary>
The term "differentiable digital signal processing" describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music & speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably. Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.
</details>
<details>
<summary>摘要</summary>
“差分可读取数字信号处理”是一家技术集合，其中损失函数导数通过数字信号处理器进行反propagation，以便将其 интегрирова到神经网络中。本文对差分音频信号处理的文献进行了报告，专注于它在音乐与语音合成中的应用。我们列出了各种应用场景，包括音乐演奏渲染、声音匹配和语音转换，并讨论了使用这种方法的动机和影响。此外，我们还提供了对数字信号处理操作的差分实现的概述。最后，我们指出了当前的开放挑战，包括优化症状、对实际 Condition 的Robustness以及设计贸易OFF，并讨论了未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.SD_2023_08_30/" data-id="clm0t8e1g00aqv788fhza94vq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CV_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T13:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CV_2023_08_30/">cs.CV - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions"><a href="#3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions" class="headerlink" title="3D Adversarial Augmentations for Robust Out-of-Domain Predictions"></a>3D Adversarial Augmentations for Robust Out-of-Domain Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15479">http://arxiv.org/abs/2308.15479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari<br>for:  This paper aims to improve the generalization of 3D object detection and semantic segmentation models to out-of-domain data.methods: The authors use adversarial examples to augment the training set and improve the models’ robustness to out-of-domain data. They learn a set of vectors that deform the objects in an adversarial fashion while preserving their plausibility.results: The authors show that their approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data, achieving better performance on a variety of scenarios using data from KITTI, Waymo, and CrashD for object detection, and data from SemanticKITTI, Waymo, and nuScenes for semantic segmentation.Here’s the simplified Chinese text for the three key points:for: 这篇论文目标是提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性。methods: 作者使用对抗示例来增强训练集，以提高模型对非标型数据的Robustness。他们学习了一组扭曲物体的vector，以 preserve their plausibility。results: 作者表明，他们的方法可以大幅提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性，在不同场景下，使用KITTI、Waymo和CrashD数据集进行3D物体检测，以及使用SemanticKITTI、Waymo和nuScenes数据集进行semantic segmentation，并且在训练使用标准单个数据集，而不是使用多个数据集。<details>
<summary>Abstract</summary>
Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive experiments across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D semantic segmentation. Despite training on a standard single dataset, our approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data.
</details>
<details>
<summary>摘要</summary>
自实际训练数据集不能正确采样下游数据分布的长尾，因此角落情况和罕见的非预训练数据样本会严重影响当前最佳模型的性能。这个问题在某些笔直的任务上，如3D语义分割，变得更加严重，因为非标准对象的点可以坚定地归类到错误的类型上。在这种情况下，我们关注提高对非预训练数据的泛化。我们实现这一目标通过在训练集中添加对抗示例来实现。首先，我们学习一组可以妄图对象的变形向量。为了保证对抗示例不过于远离现有数据分布，我们保留其可能性通过一系列约束，包括感知器和形状的平滑性。然后，我们通过应用学习的样本独立向量来对可用的对象进行对抗增强。我们在各种场景下进行了广泛的实验，包括KITTI、Waymo和CrashD上的3D物体检测，以及SemanticKITTI、Waymo和nuScenes上的3D语义分割。尽管我们只使用了标准单个数据集进行训练，但我们的方法可以很大程度上提高3D物体检测和3D语义分割方法对于非预训练数据的泛化性和Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks"><a href="#An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks" class="headerlink" title="An Adaptive Tangent Feature Perspective of Neural Networks"></a>An Adaptive Tangent Feature Perspective of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15478">http://arxiv.org/abs/2308.15478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel LeJeune, Sina Alemohammad</li>
<li>for: 了解神经网络中特征学习的机制</li>
<li>methods: 使用线性模型在抽象特征空间进行学习，并在训练过程中允许特征进行变换</li>
<li>results: 提出了一种基于线性变换的特征学习框架，并证明该框架在神经网络中可以提供更多的特征学习细节，以及一种适应特征实现的抽象特征分类方法可以在MNIST和CIFAR-10上具有许多 órders of magnitude 的采样复杂性优势。<details>
<summary>Abstract</summary>
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classification has an order of magnitude lower sample complexity than the fixed tangent feature model on MNIST and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在斜缩Feature空间中的线性模型。我们考虑了特征的线性变换，从而导致参数和变换的共同优化问题，其中包括bilinear插值约束。我们表明这个优化问题有相应的线性约束优化问题，并且具有结构化正则化，以鼓励约束低维解决方案。在神经网络结构下，我们获得了特征和几何函数的变化，从而提供了特征对kernel函数的影响的更多的准确信息。此外，我们还证明了在实际神经网络中，适用于拟合特征的tanent特征分类模型具有训练样本的一个数量级减少。
</details></li>
</ul>
<hr>
<h2 id="Learning-Modulated-Transformation-in-GANs"><a href="#Learning-Modulated-Transformation-in-GANs" class="headerlink" title="Learning Modulated Transformation in GANs"></a>Learning Modulated Transformation in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15472">http://arxiv.org/abs/2308.15472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ceyuan Yang, Qihang Zhang, Yinghao Xu, Jiapeng Zhu, Yujun Shen, Bo Dai</li>
<li>for: 提高 generative adversarial networks (GANs) 的模型灵活性和可重用性，以便更好地处理各种生成任务，包括图像生成、3D-aware图像生成和视频生成。</li>
<li>methods: 提出一种名为 modulated transformation module (MTM) 的插件模块，该模块可以预测空间偏移，并根据隐藏码来控制变量位置，以便模型更好地处理几何变换。</li>
<li>results: 在多种生成任务上进行了广泛的实验，并证明了该方法可以与当前的状态略进行无需任何参数调整。特别是，在人体生成 tasks 上，我们提高了 StyleGAN3 的 FID 值从 21.36 下降至 13.60， demonstrate 了学习模ulated geometry transformation 的能力。<details>
<summary>Abstract</summary>
The success of style-based generators largely benefits from style modulation, which helps take care of the cross-instance variation within data. However, the instance-wise stochasticity is typically introduced via regular convolution, where kernels interact with features at some fixed locations, limiting its capacity for modeling geometric variation. To alleviate this problem, we equip the generator in generative adversarial networks (GANs) with a plug-and-play module, termed as modulated transformation module (MTM). This module predicts spatial offsets under the control of latent codes, based on which the convolution operation can be applied at variable locations for different instances, and hence offers the model an additional degree of freedom to handle geometry deformation. Extensive experiments suggest that our approach can be faithfully generalized to various generative tasks, including image generation, 3D-aware image synthesis, and video generation, and get compatible with state-of-the-art frameworks without any hyper-parameter tuning. It is noteworthy that, towards human generation on the challenging TaiChi dataset, we improve the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of learning modulated geometry transformation.
</details>
<details>
<summary>摘要</summary>
成功的风格基本生成器主要受益于风格调整，它可以处理数据中的跨实例变化。然而，实例具有的随机性通常通过常规 convolution 引入，其中核函数与特征在固定位置相互作用，限制模型的形态变换能力。为解决这个问题，我们在生成对抗网络（GANs）中增加了可替换模块，称为模ulated transformation module（MTM）。这个模块根据隐藏代码预测空间偏移，并基于这些偏移进行变量位置的 convolution 操作，从而为模型增加了一个额外的自由度来处理形态变换。我们的方法可以广泛应用于不同的生成任务，包括图像生成、三维感知图像合成和视频生成，并与当前最佳框架相容无需任何超参数调整。特别是，我们在挑战性的 TaiChi 数据集上进行人体生成 task 时，提高了 StyleGAN3 的 FID 从 21.36 下降至 13.60，这表明我们学习了模ulated geometry transformation 的能力。
</details></li>
</ul>
<hr>
<h2 id="Input-margins-can-predict-generalization-too"><a href="#Input-margins-can-predict-generalization-too" class="headerlink" title="Input margins can predict generalization too"></a>Input margins can predict generalization too</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15466">http://arxiv.org/abs/2308.15466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Coenraad Mouton, Marthinus W. Theunissen, Marelie H. Davel</li>
<li>for:  investigate the relationship between generalization and classification margins in deep neural networks</li>
<li>methods:  use margin measurements, specifically constrained margins, to predict generalization ability</li>
<li>results:  constrained margins achieve highly competitive scores and outperform other margin measurements in general, providing a novel insight into the relationship between generalization and classification margins.<details>
<summary>Abstract</summary>
Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection"><a href="#Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection" class="headerlink" title="Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection"></a>Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15462">http://arxiv.org/abs/2308.15462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhou Xing, Amrita Mazumdar, Anjul Patney, Chao Liu, Hongxu Yin, Qifeng Chen, Jan Kautz, Iuri Frosio</li>
<li>for: 解决LDR相机无法处理宽动态范围输入的问题，提高图像质量。</li>
<li>methods: 使用变换器基于深度神经网络（DNN）推断缺失HDR细节。在减少参数学习中，使用多尺度DNN和适当的成本函数来实现状态艺术质量。 Additionally, using a reference frame from the past as an additional input to aid the reconstruction of overexposed areas.</li>
<li>results: 在减少参数学习中，使用这种方法可以获得状态艺术质量，而不需要使用复杂的获取机制或高Dynamic范围成像处理。我们的示例视频可以在<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view中找到。</a><details>
<summary>Abstract</summary>
Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs, frequently leading to local overexposure issues. We present a learning-based system to reduce these artifacts without resorting to complex acquisition mechanisms like alternating exposures or costly processing that are typical of high dynamic range (HDR) imaging. We propose a transformer-based deep neural network (DNN) to infer the missing HDR details. In an ablation study, we show the importance of using a multiscale DNN and train it with the proper cost function to achieve state-of-the-art quality. To aid the reconstruction of the overexposed areas, our DNN takes a reference frame from the past as an additional input. This leverages the commonly occurring temporal instabilities of autoexposure to our advantage: since well-exposed details in the current frame may be overexposed in the future, we use reinforcement learning to train a reference frame selection DNN that decides whether to adopt the current frame as a future reference. Without resorting to alternating exposures, we obtain therefore a causal, HDR hallucination algorithm with potential application in common video acquisition settings. Our demo video can be found at https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view
</details>
<details>
<summary>摘要</summary>
低动态范围（LDR）摄像机不能处理宽动态范围输入，导致本地过度曝光问题。我们提出了一种学习基于的系统，以减少这些缺陷而不需要复杂的获取机制如alternating exposures或高动态范围（HDR）拍摄。我们提议使用 transformer 基于的深度神经网络（DNN）来推理缺失 HDR 细节。在一个ablation study中，我们表明了使用多尺度 DNN 和适当的成本函数以 дости得状态的质量。为了重建过度曝光的区域，我们的 DNN 接受了过去的参考帧作为额外输入。这样利用了自动曝光的 temporal 不稳定性，我们使用 reinforcement learning 来训练参考帧选择 DNN，以确定是否采用当前帧作为未来的参考。无需alternating exposures，我们得到了一个 causal、HDR 幻化算法，可能在常见的视频拍摄设置中应用。我们的 demo 视频可以在 <https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view> 找到。
</details></li>
</ul>
<hr>
<h2 id="Canonical-Factors-for-Hybrid-Neural-Fields"><a href="#Canonical-Factors-for-Hybrid-Neural-Fields" class="headerlink" title="Canonical Factors for Hybrid Neural Fields"></a>Canonical Factors for Hybrid Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15461">http://arxiv.org/abs/2308.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brentyi/tilted">https://github.com/brentyi/tilted</a></li>
<li>paper_authors: Brent Yi, Weijia Zeng, Sam Buchanan, Yi Ma</li>
<li>for: 本文主要针对的问题是射线对齐信号的抽象特征量化方法引入的不良偏见问题，并提出一种解决方法。</li>
<li>methods: 本文使用了学习一组均衡变换的方法，以消除这些偏见。</li>
<li>results: 实验结果表明，使用这种方法可以提高图像、签名距离和辐射场重建质量、稳定性、压缩率和运行时间。<details>
<summary>Abstract</summary>
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Characterize the undesirable biases that these architectures have for axis-aligned signals, which can lead to radiance field reconstruction differences of up to 2 PSNR.2. Explore how learning a set of canonicalizing transformations can improve representations by removing these biases.We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance can be done with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, and observe improvements in quality, robustness, compactness, and runtime. Our results show that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.</details></li>
</ol>
<hr>
<h2 id="Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation"><a href="#Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation" class="headerlink" title="Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation"></a>Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15453">http://arxiv.org/abs/2308.15453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tendai Mapungwana Chikake, Boris Goldengorin, Alexey Samosyuk</li>
<li>for: 用于图像Edge检测和分割</li>
<li>methods: 使用pseudo-Boolean波尔次数计算image patches，进行二分类 blob和边区域的分类</li>
<li>results: 在简单图像中成功实现Edge检测和分割，并在复杂图像中进行应用In English, this translates to:</li>
<li>for: Used for image edge detection and segmentation</li>
<li>methods: Using pseudo-Boolean polynomials calculated on image patches for binary classification of blob and edge regions</li>
<li>results: Successfully implemented edge detection and segmentation on simple images and applied to complex images like aerial landscapes<details>
<summary>Abstract</summary>
We introduce a deterministic approach to edge detection and image segmentation by formulating pseudo-Boolean polynomials on image patches. The approach works by applying a binary classification of blob and edge regions in an image based on the degrees of pseudo-Boolean polynomials calculated on patches extracted from the provided image. We test our method on simple images containing primitive shapes of constant and contrasting colour and establish the feasibility before applying it to complex instances like aerial landscape images. The proposed method is based on the exploitation of the reduction, polynomial degree, and equivalence properties of penalty-based pseudo-Boolean polynomials.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种权值Deterministic逻辑来实现图像边检测和分割，通过在图像块上计算 pseudo-Boolean 多项式。该方法基于对图像块上的二分类，将图像分为 blob 和边区域，并基于计算的 pseudo-Boolean 多项式度量来进行分类。我们在简单的图像中使用了固定颜色和对比度的基本形状进行测试，并证明了该方法的可行性。然后，我们将该方法应用于复杂的飞行图像。该方法基于 pseudo-Boolean 多项式的减少、度量和等价性属性的利用。
</details></li>
</ul>
<hr>
<h2 id="WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation"><a href="#WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation" class="headerlink" title="WrappingNet: Mesh Autoencoder via Deep Sphere Deformation"></a>WrappingNet: Mesh Autoencoder via Deep Sphere Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15413">http://arxiv.org/abs/2308.15413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Lei, Muhammad Asad Lodhi, Jiahao Pang, Junghyun Ahn, Dong Tian</li>
<li>for: 本研究旨在实现基于 mesh 数据的无监督学习，以便学习更有意义的表示。</li>
<li>methods: 本文提出了一种基于 bottleneck 的 mesh autoencoder，通过专门 Representing mesh 连接情况的基本图来促进学习共享的 latent 空间表示对象形状。</li>
<li>results: 对比点云学习，WRAPPINGNET 可以提供更高质量的重建和竞争性的分类结果，同时可以进行不同类型对象之间的 latent  interpolate。<details>
<summary>Abstract</summary>
There have been recent efforts to learn more meaningful representations via fixed length codewords from mesh data, since a mesh serves as a complete model of underlying 3D shape compared to a point cloud. However, the mesh connectivity presents new difficulties when constructing a deep learning pipeline for meshes. Previous mesh unsupervised learning approaches typically assume category-specific templates, e.g., human face/body templates. It restricts the learned latent codes to only be meaningful for objects in a specific category, so the learned latent spaces are unable to be used across different types of objects. In this work, we present WrappingNet, the first mesh autoencoder enabling general mesh unsupervised learning over heterogeneous objects. It introduces a novel base graph in the bottleneck dedicated to representing mesh connectivity, which is shown to facilitate learning a shared latent space representing object shape. The superiority of WrappingNet mesh learning is further demonstrated via improved reconstruction quality and competitive classification compared to point cloud learning, as well as latent interpolation between meshes of different categories.
</details>
<details>
<summary>摘要</summary>
有些最近的努力是通过固定长度代码Word来学习更有意义的表示，从网格数据中得到更多的信息，因为网格作为三维形态的完整模型，比点云更有优势。然而，网格连接会对深度学习管道的构建带来新的挑战。以前的无监督学习方法通常假设特定类别的模板，例如人脸/身体模板。这限制学习的幂等空间只能对特定类别的对象进行有意义的学习，因此学习的幂等空间无法在不同类别的对象之间进行使用。在这种工作中，我们介绍了WrappingNet，首个能够进行总体网格无监督学习的自动编码器。它引入了瓶颈部分的新基graph，用于表示网格连接，这被证明可以促进学习对象形状的共享 latent space。我们通过对网格学习和点云学习进行比较，以及在不同类别的网格之间进行 latent  interpolate 等方法来证明 WrappingNet 的优越性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR"><a href="#Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR" class="headerlink" title="Robust Long-Tailed Learning via Label-Aware Bounded CVaR"></a>Robust Long-Tailed Learning via Label-Aware Bounded CVaR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15405">http://arxiv.org/abs/2308.15405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhu, Runpeng Yu, Xing Tang, Yifei Wang, Yuan Fang, Yisen Wang</li>
<li>for: 实际世界中的核心类别问题 often 会出现不对称或长尾分布，导致模型训练时对少数类别表现差。这种情况下，单简的模型通常对少数类别表现不佳。</li>
<li>methods: 本文提出了两种基于CVaR（Conditional Value at Risk）的新方法来改善长尾学习的表现，并提供了严谨的理论保证。特别是，我们首先引入了Label-Aware Bounded CVaR（LAB-CVaR）损失函数，以解决原始CVaR的偏预结果问题，然后设计了LAB-CVaR的最佳质量上限。基于LAB-CVaR，我们还提出了LAB-CVaR with logit adjustment（LAB-CVaR-logit）损失函数，并提供了理论支持。</li>
<li>results: 实际实验结果显示，我们的提案方法在实际世界中的长尾标签分布下表现出色，较以单简的模型表现更好。<details>
<summary>Abstract</summary>
Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. Extensive experiments on real-world datasets with long-tailed label distributions verify the superiority of our proposed methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>世界上的实际分类问题中的数据总是偏斜或长尾分布，其中多数类占据了模型训练中的大多数样本。在这种情况下，简单的模型通常会对少数类表现不佳。先前，一些损失修改方法已经被提出，但这些方法可能会对同一类的样本待遇不公平，或者缺乏理论保证。在本文中，我们提出了两种基于CVaR（Conditional Value at Risk）的新方法，以改进长尾学习的性能，并提供了坚实的理论基础。 Specifically, we first introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. 实际上，我们对实际上的长尾标签分布进行了广泛的实验，并证明了我们提出的方法的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CV_2023_08_30/" data-id="clm0t8dz50048v788dgru378q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.AI_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T12:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.AI_2023_08_30/">cs.AI - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology"><a href="#A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology" class="headerlink" title="A General-Purpose Self-Supervised Model for Computational Pathology"></a>A General-Purpose Self-Supervised Model for Computational Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15474">http://arxiv.org/abs/2308.15474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson, Guillaume Jaume, Bowen Chen, Andrew Zhang, Daniel Shao, Andrew H. Song, Muhammad Shaban, Mane Williams, Anurag Vaidya, Sharifa Sahai, Lukas Oldenburg, Luca L. Weishaupt, Judy J. Wang, Walt Williams, Long Phi Le, Georg Gerber, Faisal Mahmood</li>
<li>for: 本研究旨在提出一种通用的自助学习模型，用于解决生物病理学图像分类和诊断问题。</li>
<li>methods: 该模型使用了100万个各种组织类型的病理图像补充，并通过自助学习方法进行预训练。</li>
<li>results: 该模型在33种不同的临床任务中表现出色，包括分类、诊断和疾病类型划分等。此外，模型还能够在各种组织类型和诊断难度不同的情况下进行泛化和转移。<details>
<summary>Abstract</summary>
Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addition to outperforming previous state-of-the-art models, we demonstrate new modeling capabilities in CPath such as resolution-agnostic tissue classification, slide classification using few-shot class prototypes, and disease subtyping generalization in classifying up to 108 cancer types in the OncoTree code classification system. UNI advances unsupervised representation learning at scale in CPath in terms of both pretraining data and downstream evaluation, enabling data-efficient AI models that can generalize and transfer to a gamut of diagnostically-challenging tasks and clinical workflows in anatomic pathology.
</details>
<details>
<summary>摘要</summary>
组织现象评估是computational pathology（CPath）的基本任务，它的目标是通过学习标注组织生物marker的Objective characterizations，以帮助诊断医学。然而，整个标本影像（WSI）对于计算机视觉问题而言是一个复杂的问题，因为标本影像的大规模分辨率和丰富多样性的形态现象对于大规模标注数据的生成提供了一定的挑战。目前的努力已经提议使用预训归数图像Encoder， either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets，但这些努力尚未得到了广泛的发展和评估。我们提出了UNI，一个通用的自主学习模型，预训使用了1000万个组织小图像，来自100000多个诊断HE染色标本影像，并在20个主要组织类型上进行了33个CPath临床任务的评估。此外，我们还展示了一些新的模型化能力，例如resolution-agnostic tissue classification、slides classification using few-shot class prototypes、和疾病分类对108种癌症的分类。UNI在CPath中进行了无监督学习的扩展，并在这些任务上实现了资料效率的AI模型，可以对诊断挑战性任务和临床工作流程进行普遍和转移。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction"><a href="#Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction" class="headerlink" title="Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction"></a>Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15469">http://arxiv.org/abs/2308.15469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weichen Huang</li>
<li>for: 这个研究旨在开发一个多 modal 对照学习框架，以利用 MRI 扫描和 PET 等神经成像数据，并处理 AD 疾病数据中的值得注意的表格资料。</li>
<li>methods: 这个框架使用了一个新的表格注意模组，可以强调和排名表格中的重要特征。它还使用了多 modal 对照学习技术，以将图像和表格资料结合在一起。</li>
<li>results: 实验结果显示，这个框架可以从 ADNI 数据库中的逾 882 个 MRI 扫描标本中检测出 AD 疾病，并且可以实现高于 83.8% 的准确率，与前一代的州度优化技术相比，提高了约 10%。<details>
<summary>Abstract</summary>
Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.
</details>
<details>
<summary>摘要</summary>
alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.Here's the translation in Traditional Chinese:附加了脑成像技术，如MRI扫描和PET，Alzheimer病（AD）数据集包含重要的表格资料，包括AD标识和临床评估。现有的计算机视觉方法对这些额外资讯难以使用。为解决这些需求，我们提出了一个通用的多modal对比学习框架，一个新的表格注意模组，以强调和排名表格中的重要特征。我们还应用了这些技术 onto Alzheimer病预测。实验评估显示了我们的框架在ADNI数据库中的882个MRI图像探针中检测到Alzheimer病的能力，比前一代的state of the art约10%高。我们利用表格资料的高解释性和我们的新的表格注意方法，通过每行表格中的注意分数汇总，发现和排名表格中的最主要特征。结果显示模型可以达到83.8%的精度，比前一代的state of the art约10%高。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios"><a href="#A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios" class="headerlink" title="A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios"></a>A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15464">http://arxiv.org/abs/2308.15464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios">https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios</a></li>
<li>paper_authors: Yangxinyu Xie, Tanwi Mallick</li>
<li>for: 这 paper 的目的是提高深度学习模型在交通预测中的精度，特别是预测堵塞情况。</li>
<li>methods: 这 paper 使用了多种积分函数，包括 MAE-Focal Loss 和 Gumbel Loss，来解决传统损失函数的局限性。</li>
<li>results: 经过大规模实验，这 paper 发现 MAE-Focal Loss 和 Gumbel Loss 在预测交通速度方面具有最高效果，能够准确预测堵塞情况而不会妨碍正常交通预测。<details>
<summary>Abstract</summary>
Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic congestion events without compromising the accuracy of regular traffic speed forecasts. This research enhances deep learning models' capabilities in forecasting sudden speed changes due to congestion and underscores the need for more research in this direction. By elevating the accuracy of congestion forecasting, we advocate for AI systems that are reliable, secure, and resilient in practical traffic management scenarios.
</details>
<details>
<summary>摘要</summary>
现代交通预测中使用的空间时间图 neural network 已经达到了领先的性能水平。然而，它们经常因传统的损失函数的局限性而难以准确预测堵塞情况。尽管正确预测常规交通情况是非常重要，但一个可靠的 AI 系统也必须准确预测堵塞场景，以保证安全和高效的交通运输。在这篇论文中，我们探讨了各种启发自重态分析和不均衡分类问题的损失函数，以解决这一问题。我们对这些损失函数在预测交通速度方面的效果进行了广泛的实验，发现了使用 MAE-Focal Loss 函数时，MAE 函数在预测堵塞场景中表现最佳。使用 MSE 函数时，Gumbel Loss 函数表现最佳。这些选择可以准确预测交通堵塞事件，不会 compromise 正常交通速度预测的准确性。这项研究提高了深度学习模型在预测快速变化的能力，并强调了对堵塞预测的需求。我们认为，通过提高堵塞预测的准确性，可以建立可靠、安全、可靠的 AI 系统，以满足实际交通管理场景中的需求。
</details></li>
</ul>
<hr>
<h2 id="ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer"><a href="#ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer" class="headerlink" title="ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer"></a>ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15459">http://arxiv.org/abs/2308.15459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/ParaGuide">https://github.com/zacharyhorvitz/ParaGuide</a></li>
<li>paper_authors: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown</li>
<li>for: 文章的目的是将文本的风格特征转换为新的风格，保留 semantic information。</li>
<li>methods: 本研究使用了一个新的扩散基础架构，叫做 ParaGuide，可以灵活地适应任意目标风格。这个方法使用了句子重写条件下的扩散模型，以及Gradient-based guidance from both off-the-shelf classifiers和强大的现有风格嵌入。</li>
<li>results: 研究在Enron Email Corpus上进行了人工和自动评估，和强大的基eline均以上。它可以成功地将文本的风格转换为新的风格，保留 semantic information。<details>
<summary>Abstract</summary>
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, sentiment, and even authorship style transfer.
</details>
<details>
<summary>摘要</summary>
文本风格转换是将文本的风格属性转换为另一种风格的任务，保持意义不变。目标风格可以定义为多种方式，从单一特征（例如正式度）到作者（例如莎士比亚）。前一代无监督风格转换方法通常需要大量标注数据，仅适用于固定的风格集或需要大型语言模型。相比之下，我们介绍了一种新的扩散基于框架，可以通过扩散模型来实现通用风格转换，并在推理时适应任意目标风格。我们的参数有效的方法， ParaGuide，利用了句子重构conditional扩散模型，并与梯度导航从存储类фика器和强有力的现有风格编码器来转换文本的风格，保持 semantic information。我们在恩рон电子邮件集上验证了该方法，并与人类和自动评估表明，它在正式度、情感和作者风格转换方面超过了强大基eline。
</details></li>
</ul>
<hr>
<h2 id="From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification"><a href="#From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification" class="headerlink" title="From SMOTE to Mixup for Deep Imbalanced Classification"></a>From SMOTE to Mixup for Deep Imbalanced Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15457">http://arxiv.org/abs/2308.15457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ntucllab/imbalanced-dl">https://github.com/ntucllab/imbalanced-dl</a></li>
<li>paper_authors: Wei-Chao Cheng, Tan-Ha Mai, Hsuan-Tien Lin</li>
<li>for: 本研究旨在探讨深度学习中对异质数据的处理方法，尤其是SMOTE数据增强技术是否有利于深度学习。</li>
<li>methods: 本研究使用了SMOTE技术，以及其扩展版本——软标签SMOTE和混合技术。</li>
<li>results: 研究发现，通过将SMOTE和混合技术结合使用，可以提高深度学习模型的泛化性能，并且在极端异质数据上达到最佳性能。<details>
<summary>Abstract</summary>
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental results demonstrate that our proposed technique yields state-of-the-art performance on deep imbalanced classification while achieving superior performance on extremely imbalanced data. The code is open-sourced in our developed package https://github.com/ntucllab/imbalanced-DL to foster future research in this direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Do-Program-of-Thoughts-Work-for-Reasoning"><a href="#When-Do-Program-of-Thoughts-Work-for-Reasoning" class="headerlink" title="When Do Program-of-Thoughts Work for Reasoning?"></a>When Do Program-of-Thoughts Work for Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15452">http://arxiv.org/abs/2308.15452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyinstruct">https://github.com/zjunlp/easyinstruct</a></li>
<li>paper_authors: Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun Chen</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在肢体人工智能领域的逻辑能力如何提高，以及程序语言的影响。</li>
<li>methods: 本研究提出了一种 complexity-impacted reasoning score (CIRS)，用于衡量程序语言的结构和逻辑特性对逻辑能力的影响。CIRS 使用抽象树来编码结构信息，并通过考虑困难性和环状复杂性来计算逻辑复杂性。</li>
<li>results: 研究发现，不同的程序数据复杂性会影响 LLMS 的逻辑能力提升。优化的复杂度是关键的，程序帮助提示可以提高逻辑能力。研究还提出了一种自动生成和分级算法，并应用于数学逻辑和代码生成任务。广泛的结果表明了我们的提出的方法的有效性。<details>
<summary>Abstract</summary>
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct.
</details>
<details>
<summary>摘要</summary>
LLMs的逻辑能力在人工智能中扮演着关键性角色。虽有有效的程序激活提示法 для LLMs，但代码数据对复杂逻辑任务的改进仍然未得到足够的探讨。为解决这个空白，我们提出了复杂性影响逻辑能力指数（CIRS），它将结构性和逻辑性特征相结合，用于衡量代码数据对逻辑能力的相关性。我们使用抽象树来编码结构信息，并根据难度和 cyclomatic complexity来计算逻辑复杂性。经 empirical 分析发现，不 всех复杂度的代码数据可以被 LLMS 学习或理解。优质的复杂度是关键的，以便通过程序帮助提示来提高逻辑能力。然后，我们设计了自动生成和分配算法，并应用它到数学逻辑和代码生成任务中。广泛的结果表明了我们的提出的方法的有效性。代码将会被集成到 EasyInstruct 框架中，可以在 <https://github.com/zjunlp/EasyInstruct> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction"><a href="#Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction" class="headerlink" title="Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction"></a>Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15427">http://arxiv.org/abs/2308.15427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Gao, Jiawei Fu, Haodong Jing, Nanning Zheng</li>
<li>for: 提高自动驾驶系统中的高清晰地图建构方法的性能，使其更敏感于废弃环境。</li>
<li>methods: 补充车载感知器上的信息使用卫星地图，提高HD地图建构方法的性能。提出一种层次融合模块，通过Feature级别融合和BEV级别融合来实现卫星地图信息的更好融合。</li>
<li>results: 在扩展nuScenes数据集上，证明了我们的模块可以准确地融合到现有的HD地图建构方法中，提高其在HD地图Semantic segmentation和实例检测任务中的性能。<details>
<summary>Abstract</summary>
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboard Bird's Eye View (BEV) features and satellite features in feature-level fusion. An alignment module is introduced before concatenation in BEV-level fusion to mitigate the impact of misalignment between the two features. The experimental results on the augmented nuScenes dataset showcase the seamless integration of our module into three existing HD map construction methods. It notably enhances their performance in both HD map semantic segmentation and instance detection tasks.
</details>
<details>
<summary>摘要</summary>
高清定义（HD）地图在自动驾驶系统中扮演着关键角色。现有方法尝试在实时基础上构建HD地图，但这些方法的性能受周围环境的影响很大，因为搭载在车辆上的感知器件具有较弱的远程探测能力。在本研究中，我们发现可以通过补充搭载在车辆上的感知器件与卫星地图的信息来提高HD地图构建方法的性能。为进一步研究，我们释放了卫星地图块作为nuScenes数据集的补充数据集。此外，我们提议了一种层次融合模块，使得更好地融合卫星地图信息与现有方法。具体来说，我们设计了一个基于分割和距离的注意mask，通过交叉注意机制来融合搭载在 Bird's Eye View（BEV）上的特征和卫星特征在特征层融合。在BEV层融合之前，我们引入了一个对齐模块，以mitigate卫星特征和BEV特征之间的偏移的影响。实验结果表明，我们的模块可以覆盖现有的三种HD地图构建方法，并在HD地图semantic segmentation和实例检测任务中显著提高其性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.AI_2023_08_30/" data-id="clm0t8dy60011v7884pdrhmvr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CL_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T11:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CL_2023_08_30/">cs.CL - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla"><a href="#Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla" class="headerlink" title="Vulgar Remarks Detection in Chittagonian Dialect of Bangla"></a>Vulgar Remarks Detection in Chittagonian Dialect of Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15448">http://arxiv.org/abs/2308.15448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanjim Mahmud, Michal Ptaszynski, Fumito Masui</li>
<li>for: 本研究旨在探讨社交媒体上的负面言语 automatic detection方法，尤其是在低资源语言如锡兰语方言上。</li>
<li>methods: 本研究使用了指导学习和深度学习算法来检测社交媒体上的侮辱言语。Logistic Regression实现了可观的准确率（0.91），而简单的RNN具有Word2vec和fastTex的组合实现了较低的准确率（0.84-0.90），这说明了NN算法需要更多的数据。</li>
<li>results: 本研究显示，使用指导学习和深度学习算法可以准确地检测社交媒体上的侮辱言语，但是NN算法需要更多的数据以实现更高的准确率。<details>
<summary>Abstract</summary>
The negative effects of online bullying and harassment are increasing with Internet popularity, especially in social media. One solution is using natural language processing (NLP) and machine learning (ML) methods for the automatic detection of harmful remarks, but these methods are limited in low-resource languages like the Chittagonian dialect of Bangla.This study focuses on detecting vulgar remarks in social media using supervised ML and deep learning algorithms.Logistic Regression achieved promising accuracy (0.91) while simple RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the issue that NN algorithms require more data.
</details>
<details>
<summary>摘要</summary>
互联网欺凌和 Harrasment 的负面影响随着互联网的普及而增加，尤其在社交媒体上。一种解决方案是使用自然语言处理（NLP）和机器学习（ML）方法进行自动发现危险评论，但这些方法在低资源语言如锡兰语的拼写方言上有限。本研究关注社交媒体中的粗鄙评论使用监督式 ML 和深度学习算法探测。Logistic Regression 达到了可靠的准确率（0.91），而简单的 RNN 与 Word2vec 和 fastTex 的准确率（0.84-0.90）较低，表明 NN 算法需要更多的数据。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability"><a href="#Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability" class="headerlink" title="Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability"></a>Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15419">http://arxiv.org/abs/2308.15419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/lm-learning-curves">https://github.com/tylerachang/lm-learning-curves</a></li>
<li>paper_authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 这些语言模型在预训练中学习的问题是什么？</li>
<li>methods: 这些语言模型在预训练中使用了什么方法？</li>
<li>results: 这些语言模型在预训练中得到了什么结果？Here are the answers in Simplified Chinese text:</li>
<li>for: 这些语言模型在预训练中学习的问题是如何快速预测语言模型的性能？</li>
<li>methods: 这些语言模型在预训练中使用了自适应语言模型的预训练方法？</li>
<li>results: 这些语言模型在预训练中得到了快速预测语言模型的稳定性和性能？<details>
<summary>Abstract</summary>
How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives. Our work contributes to a better understanding of language model pre-training dynamics and informs the deployment of stable language models in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>我们使用五个权重autoregressive英语语言模型的预训练运行来研究语言模型如何预测。我们从100万个字Context中提取学习曲线，并观察到语言模型在预训练过程中首先生成短重复的短语，然后学习 longer和更 coherent的文本。我们量化每个Token在Context中的最终难度、内Run变化、年龄 acquisition、忘记性和 Cross-Run变化。我们发现更常见的Token在Context中更容易预测， exhibit less variability within和across预训练运行，learn Earlier，并更可能被"忘记" during预训练。高 n-gram概率更加强调这些效果。独立于目标Token，更短和更频繁的Contexts呈现marginally more stable and quickly acquired预测。 parts of speech的影响也很小，although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives。我们的工作对语言模型预训练动力学的更好理解，并可以帮助实践中稳定地部署语言模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CL_2023_08_30/" data-id="clm0t8dyo002mv7881bgcbg58" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.LG_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T10:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.LG_2023_08_30/">cs.LG - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Policy-composition-in-reinforcement-learning-via-multi-objective-policy-optimization"><a href="#Policy-composition-in-reinforcement-learning-via-multi-objective-policy-optimization" class="headerlink" title="Policy composition in reinforcement learning via multi-objective policy optimization"></a>Policy composition in reinforcement learning via multi-objective policy optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15470">http://arxiv.org/abs/2308.15470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti Mishra, Ankit Anand, Jordan Hoffmann, Nicolas Heess, Martin Riedmiller, Abbas Abdolmaleki, Doina Precup</li>
<li>for: 本研究旨在使用相关的先进教学策略来帮助强化学习代理人学习成功的行为策略。</li>
<li>methods: 本研究使用了多目标策略优化算法（Multi-Objective Maximum a Posteriori Policy Optimization，简称MOPPO），其中包括任务目标以及教学策略作为多个目标。</li>
<li>results: 研究表明，在继续使用教学策略的情况下，强化学习代理人可以更快速地学习任务，特别是在缺乏形成奖励的情况下。在连续观察和动作空间的两个领域中，我们的代理人成功地组合了教学策略序列和并行，并能够进一步扩展教学策略以解决任务。<details>
<summary>Abstract</summary>
We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.   Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of teachers on learning speed and the eventual performance of the agent on the task. In the {\tt humanoid} domain \citep{deepmindcontrolsuite2018}, we also equip agents with the ability to control the selection of teachers. With this ability, agents are able to meaningfully compose from the teacher policies to achieve a superior task reward on the {\tt walk} task than in cases without access to the teacher policies. We show the resemblance of composed task policies with the corresponding teacher policies through videos.
</details>
<details>
<summary>摘要</summary>
我们使用已有的教师策略来帮助权威学习代理人学习成功行为策略。教师策略被引入为目标之一，同时与任务目标一起使用多目标策略优化算法 \citep{abdolmaleki2020distributional}。我们在连续观察和动作空间的两个领域中表示，我们的代理人可以成功组合教师策略并且可以进一步扩展教师策略以解决任务。在指定的任务和教师的组合下，教师可能会自然地限制代理人的最终表现。代理人需要遵循教师策略的程度由参数决定，这些参数不仅影响代理人学习速度，还影响代理人在任务上的最终表现。在{\tt humanoid}领域 \citep{deepmindcontrolsuite2018}中，我们还让代理人控制选择教师的能力。通过这种能力，代理人能够有意义地从教师策略中组合任务策略，在{\tt walk}任务上比不使用教师策略的情况下更好的完成任务。我们通过视频显示，组合的任务策略与相应的教师策略之间的相似性。
</details></li>
</ul>
<hr>
<h2 id="Random-feature-approximation-for-general-spectral-methods"><a href="#Random-feature-approximation-for-general-spectral-methods" class="headerlink" title="Random feature approximation for general spectral methods"></a>Random feature approximation for general spectral methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15434">http://arxiv.org/abs/2308.15434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mike Nguyen, Nicole Mücke</li>
<li>for: 本文研究了大规模算法中使用权重方法的减速技术，以及深度神经网络的分析方法。</li>
<li>methods: 本文使用了随机特征approximation技术，并对权重方法进行了分析。</li>
<li>results: 本文对权重方法的泛化性质进行了研究，并在不同的常数空间中获得了优化的学习率。<details>
<summary>Abstract</summary>
Random feature approximation is arguably one of the most popular techniques to speed up kernel methods in large scale algorithms and provides a theoretical approach to the analysis of deep neural networks. We analyze generalization properties for a large class of spectral regularization methods combined with random features, containing kernel methods with implicit regularization such as gradient descent or explicit methods like Tikhonov regularization. For our estimators we obtain optimal learning rates over regularity classes (even for classes that are not included in the reproducing kernel Hilbert space), which are defined through appropriate source conditions. This improves or completes previous results obtained in related settings for specific kernel algorithms.
</details>
<details>
<summary>摘要</summary>
随机特征近似是大规模算法中最受欢迎的技术之一，它提供了对深度神经网络的分析理论方法。我们分析了一大类spectral regularization方法，包括梯度下降或特ikhonov regularization等kernel方法，其中的泛化性质得到了改进或完善。我们的估计器可以在不同的常数类型下获得最佳学习速率，这些常数类型包括 reproduce kernel Hilbert space以外的类型。这些结果与之前在相关的设置中获得的结果相匹配或完善。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-solar-flare-forecasting-using-historical-magnetogram-data"><a href="#Probabilistic-solar-flare-forecasting-using-historical-magnetogram-data" class="headerlink" title="Probabilistic solar flare forecasting using historical magnetogram data"></a>Probabilistic solar flare forecasting using historical magnetogram data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15410">http://arxiv.org/abs/2308.15410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swri-idea-lab/idea-lab-flare-forecast">https://github.com/swri-idea-lab/idea-lab-flare-forecast</a></li>
<li>paper_authors: Kiera van der Sande, Andrés Muñoz-Jaramillo, Subhamoy Chatterjee</li>
<li>for: 这个论文旨在利用机器学习技术预测太阳风暴。</li>
<li>methods: 这篇论文使用了卷积神经网络提取全天磁图像的特征，并将这些特征与磁图像和风暴历史信息相结合，使用了逻辑回归模型生成可カリibrated的风暴预测。</li>
<li>results: 包括历史数据在内的多种仪器的日常磁图像数据可以提高预测的准确性和可靠性，磁图像单一帧不含 significatively更多有用信息，而风暴历史信息比我们提取的磁图像特征更具预测力。<details>
<summary>Abstract</summary>
Solar flare forecasting research using machine learning (ML) has focused on high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24 and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of daily historical magnetogram data from multiple instruments. This is the first attempt to take advantage of this historical data for ML-based flare forecasting. We apply a convolutional neural network (CNN) to extract features from full-disk magnetograms together with a logistic regression model to incorporate scalar features based on magnetograms and flaring history. We use an ensemble approach to generate calibrated probabilistic forecasts of M-class or larger flares in the next 24 hours. Overall, we find that including historical data improves forecasting skill and reliability. We show that single frame magnetograms do not contain significantly more relevant information than can be summarized in a small number of scalar features, and that flaring history has greater predictive power than our CNN-extracted features. This indicates the importance of including temporal information in flare forecasting models.
</details>
<details>
<summary>摘要</summary>
太阳风暴预测研究使用机器学习（ML）专注于高分辨率磁场图像从SDO/HMI时期的太阳周期24和太阳周期25之前的一些努力，也有一些努力回到SOHO/MDI上的数据。在这篇论文中，我们考虑了4个太阳周期的日常历史磁场数据从多个仪器。这是第一次利用历史数据为ML-基于的太阳风暴预测。我们使用卷积神经网络（CNN）提取磁场图像的特征，并将磁场图像和风暴历史中的一些缺失特征加以逻辑回归模型。我们使用一个集成方法生成标准化的可信度预测M级或大于M级太阳风暴在下一个24小时内发生。总的来说，我们发现包含历史数据可以提高预测技巧和可靠性。我们显示单个帧磁场图像不含有足够重要的信息，而风暴历史更有预测力量。这表明包含时间信息在太阳风暴预测模型中是非常重要的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.LG_2023_08_30/" data-id="clm0t8e0o007sv7880edx4oog" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/29/cs.CV_2023_08_29/" class="article-date">
  <time datetime="2023-08-29T13:00:00.000Z" itemprop="datePublished">2023-08-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/29/cs.CV_2023_08_29/">cs.CV - 2023-08-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Discovery-and-Effective-Evaluation-of-Visual-Perceptual-Similarity-A-Benchmark-and-Beyond"><a href="#Efficient-Discovery-and-Effective-Evaluation-of-Visual-Perceptual-Similarity-A-Benchmark-and-Beyond" class="headerlink" title="Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond"></a>Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14753">http://arxiv.org/abs/2308.14753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vsd-benchmark/vsd">https://github.com/vsd-benchmark/vsd</a></li>
<li>paper_authors: Oren Barkan, Tal Reiss, Jonathan Weill, Ori Katz, Roy Hirsch, Itzik Malkiel, Noam Koenigstein</li>
<li>for: 该论文的目的是提出一个大规模的时尚视觉相似性数据集，以及一种有效的标签过程，以便评估视觉相似性检测方法。</li>
<li>methods: 该论文使用了专业注释的图像对进行标签，并提出了一种新的标签过程，可以应用于任何数据集。</li>
<li>results: 该论文提出了一个大规模的时尚视觉相似性数据集，并进行了对该数据集的分析和评估。Here’s the full text in Simplified Chinese:</li>
<li>for: 该论文的目的是提出一个大规模的时尚视觉相似性数据集，以及一种有效的标签过程，以便评估视觉相似性检测方法。</li>
<li>methods: 该论文使用了专业注释的图像对进行标签，并提出了一种新的标签过程，可以应用于任何数据集。</li>
<li>results: 该论文提出了一个大规模的时尚视觉相似性数据集，并进行了对该数据集的分析和评估。<details>
<summary>Abstract</summary>
Visual similarities discovery (VSD) is an important task with broad e-commerce applications. Given an image of a certain object, the goal of VSD is to retrieve images of different objects with high perceptual visual similarity. Although being a highly addressed problem, the evaluation of proposed methods for VSD is often based on a proxy of an identification-retrieval task, evaluating the ability of a model to retrieve different images of the same object. We posit that evaluating VSD methods based on identification tasks is limited, and faithful evaluation must rely on expert annotations. In this paper, we introduce the first large-scale fashion visual similarity benchmark dataset, consisting of more than 110K expert-annotated image pairs. Besides this major contribution, we share insight from the challenges we faced while curating this dataset. Based on these insights, we propose a novel and efficient labeling procedure that can be applied to any dataset. Our analysis examines its limitations and inductive biases, and based on these findings, we propose metrics to mitigate those limitations. Though our primary focus lies on visual similarity, the methodologies we present have broader applications for discovering and evaluating perceptual similarity across various domains.
</details>
<details>
<summary>摘要</summary>
<SYS>文本翻译为简化字符串。</SYS>视觉相似性发现（VSD）是一项广泛应用于电子商务的重要任务。给定一个对象的图像，VSD的目标是检索具有高度感知视觉相似性的不同对象的图像。尽管是一个已经受到广泛关注的问题，但评估提出的VSD方法的方法通常基于一种对象预测任务的代理，评估模型是否可以正确地检索不同的对象图像。我们认为，基于预测任务进行评估VSD方法有限制，我们应该依靠专家注释来进行 faithful 评估。在这篇文章中，我们发布了首个大规模的时尚视觉相似性准 benchmark 数据集，包含 более 110K 专家注释的图像对。此外，我们还分享了在编 Curate 这个数据集时遇到的挑战，并提出了一种新的和高效的标签过程，可以应用于任何数据集。我们的分析检查了这些限制和偏好，并基于这些发现，我们提出了一些缓解这些限制的度量。虽然我们的主要焦点是视觉相似性，但我们所提出的方法ologies 有更广泛的应用于发现和评估不同领域中的感知相似性。
</details></li>
</ul>
<hr>
<h2 id="MagicEdit-High-Fidelity-and-Temporally-Coherent-Video-Editing"><a href="#MagicEdit-High-Fidelity-and-Temporally-Coherent-Video-Editing" class="headerlink" title="MagicEdit: High-Fidelity and Temporally Coherent Video Editing"></a>MagicEdit: High-Fidelity and Temporally Coherent Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14749">http://arxiv.org/abs/2308.14749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, Jiashi Feng</li>
<li>for: 这个论文是为了解决文本引导视频编辑任务而写的。</li>
<li>methods: 这个论文使用了分离内容、结构和动作信号的方法来在训练中学习高效的视频-to-视频翻译。</li>
<li>results: 论文表明，这种方法可以实现高精度和时间协调的视频翻译，并且支持多种下游视频编辑任务，如视频 стилизация、本地编辑、视频MagicMix和视频填充。<details>
<summary>Abstract</summary>
In this report, we present MagicEdit, a surprisingly simple yet effective solution to the text-guided video editing task. We found that high-fidelity and temporally coherent video-to-video translation can be achieved by explicitly disentangling the learning of content, structure and motion signals during training. This is in contradict to most existing methods which attempt to jointly model both the appearance and temporal representation within a single framework, which we argue, would lead to degradation in per-frame quality. Despite its simplicity, we show that MagicEdit supports various downstream video editing tasks, including video stylization, local editing, video-MagicMix and video outpainting.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍MagicEdit，一种高效且简单的解决文本引导视频编辑问题的解决方案。我们发现，在训练中分离内容、结构和运动信号的学习可以实现高精度和时间启示的视频-to-视频翻译。这与大多数现有方法不同，这些方法尝试同时模型视频的外观和时间表示，我们认为这会导致每帧质量下降。尽管简单，我们显示MagicEdit支持多种下渠视频编辑任务，包括视频风格化、本地编辑、MagicMix和视频外缩。
</details></li>
</ul>
<hr>
<h2 id="MagicAvatar-Multimodal-Avatar-Generation-and-Animation"><a href="#MagicAvatar-Multimodal-Avatar-Generation-and-Animation" class="headerlink" title="MagicAvatar: Multimodal Avatar Generation and Animation"></a>MagicAvatar: Multimodal Avatar Generation and Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14748">http://arxiv.org/abs/2308.14748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magic-research/magic-avatar">https://github.com/magic-research/magic-avatar</a></li>
<li>paper_authors: Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, Jun Hao Liew</li>
<li>for: 这个论文旨在提出一种基于多模态视频生成和人偶动画的框架，即 MagicAvatar。</li>
<li>methods: 这个框架包括两个阶段：第一个阶段是将多模态输入翻译成动作&#x2F;控制信号（例如人姿、深度、DensePose），第二个阶段是根据这些动作信号生成人偶视频。</li>
<li>results: MagicAvatar可以通过提供一些人的图像来animate人偶，并且可以实现文本指导和视频指导的人偶生成。此外，它还可以应用于多模态人偶动画。<details>
<summary>Abstract</summary>
This report presents MagicAvatar, a framework for multimodal video generation and animation of human avatars. Unlike most existing methods that generate avatar-centric videos directly from multimodal inputs (e.g., text prompts), MagicAvatar explicitly disentangles avatar video generation into two stages: (1) multimodal-to-motion and (2) motion-to-video generation. The first stage translates the multimodal inputs into motion/ control signals (e.g., human pose, depth, DensePose); while the second stage generates avatar-centric video guided by these motion signals. Additionally, MagicAvatar supports avatar animation by simply providing a few images of the target person. This capability enables the animation of the provided human identity according to the specific motion derived from the first stage. We demonstrate the flexibility of MagicAvatar through various applications, including text-guided and video-guided avatar generation, as well as multimodal avatar animation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Multimodal-to-motion: Translating the multimodal inputs into motion&#x2F;control signals (e.g., human pose, depth, DensePose).2. Motion-to-video generation: Generating avatar-centric video guided by these motion signals.Moreover, MagicAvatar supports avatar animation by simply providing a few images of the target person. This capability enables the animation of the provided human identity according to the specific motion derived from the first stage. We demonstrate the flexibility of MagicAvatar through various applications, including text-guided and video-guided avatar generation, as well as multimodal avatar animation.中文翻译：这份报告介绍了 MagicAvatar，一个用于多模态视频生成和人物动画的框架。与大多数现有方法不同，MagicAvatarexplicitly归纳了人物视频生成到两个阶段：1. 多模态到动作：将多模态输入翻译成动作&#x2F;控制信号（例如人姿、深度、DensePose）。2. 动作到视频生成：根据这些动作信号生成人物视频。此外，MagicAvatar还支持人物动画，只需提供一些目标人物的图像即可。这使得可以根据第一阶段得到的特定动作来动画提供的人物。我们通过多种应用，包括文本指导和视频指导的人物生成以及多模态人物动画，展示了MagicAvatar的灵活性。</details></li>
</ol>
<hr>
<h2 id="CoVR-Learning-Composed-Video-Retrieval-from-Web-Video-Captions"><a href="#CoVR-Learning-Composed-Video-Retrieval-from-Web-Video-Captions" class="headerlink" title="CoVR: Learning Composed Video Retrieval from Web Video Captions"></a>CoVR: Learning Composed Video Retrieval from Web Video Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14746">http://arxiv.org/abs/2308.14746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucas-ventura/CoVR">https://github.com/lucas-ventura/CoVR</a></li>
<li>paper_authors: Lucas Ventura, Antoine Yang, Cordelia Schmid, Gül Varol</li>
<li>for: 这篇论文的目的是提出一种可扩展的自动生成compose Image Retrieval（CoIR）数据集方法，以替代手动纪录CoIR triplets的高成本和不可扩展性。</li>
<li>methods: 该方法利用了视频-caption对的对应关系，并采用了大型自然语言模型来生成修改文本。</li>
<li>results: 通过应用该方法于WebVid2M数据集，自动生成了160万个CoIR triplets，并提出了一个新的CoVR数据集和一个手动注解的评估集。实验表明，训练CoVR模型在我们的数据集上可以有效转移到CoIR，在零基eline设置下在CIRR和FashionIQ数据集上达到了最佳性能。<details>
<summary>Abstract</summary>
Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers both text and image queries together, to search for relevant images in a database. Most CoIR approaches require manually annotated datasets, comprising image-text-image triplets, where the text describes a modification from the query image to the target image. However, manual curation of CoIR triplets is expensive and prevents scalability. In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs, while also expanding the scope of the task to include composed video retrieval (CoVR). To this end, we mine paired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modification text. Applying this methodology to the extensive WebVid2M collection, we automatically construct our WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we introduce a new benchmark for CoVR with a manually annotated evaluation set, along with baseline results. Our experiments further demonstrate that training a CoVR model on our dataset effectively transfers to CoIR, leading to improved state-of-the-art performance in the zero-shot setup on both the CIRR and FashionIQ benchmarks. Our code, datasets, and models are publicly available at https://imagine.enpc.fr/~ventural/covr.
</details>
<details>
<summary>摘要</summary>
团体图像检索（CoIR）在最近几年内得到了广泛关注，因为它同时考虑图像和文本查询，以搜索图像库中相关的图像。大多数CoIR方法需要手动标注数据集，包括图像-文本-图像三元组，其中文本描述了查询图像到目标图像的修改。然而，手动筛选CoIR三元组是贵重的并不可扩展。在这种情况下，我们提议一种可扩展的自动数据创建方法，使用视频-标题对进行生成三元组，同时扩展CoIR任务的范围，以包括组合视频检索（CoVR）。为此，我们从大量视频库中挖掘具有相同标题的视频对，并利用大型自然语言模型生成相应的修改文本。通过应用这种方法，我们自动建立了WebVid-CoVR数据集，共计1600万三元组。此外，我们还提供了一个手动标注的评估集，以及基准结果。我们的实验表明，训练CoVR模型于我们的数据集后，可以转移到CoIR任务，在零例情况下在CIRR和FashionIQ benchmark上实现了状态的推进性表现。我们的代码、数据集和模型都公开提供在https://imagine.enpc.fr/~ventural/covr。
</details></li>
</ul>
<hr>
<h2 id="Total-Selfie-Generating-Full-Body-Selfies"><a href="#Total-Selfie-Generating-Full-Body-Selfies" class="headerlink" title="Total Selfie: Generating Full-Body Selfies"></a>Total Selfie: Generating Full-Body Selfies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14740">http://arxiv.org/abs/2308.14740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz</li>
<li>for: 生成全身自拍照（用户提供的视频、目标姿势照片和每个位置的自拍照+背景对）</li>
<li>methods:  diffusion-based 方法组合这些信息生成高质量、正确pose和背景的全身自拍照</li>
<li>results: 生成高品质、自然的全身自拍照，包括用户所需的姿势和背景<details>
<summary>Abstract</summary>
We present a method to generate full-body selfies -- photos that you take of yourself, but capturing your whole body as if someone else took the photo of you from a few feet away. Our approach takes as input a pre-captured video of your body, a target pose photo, and a selfie + background pair for each location. We introduce a novel diffusion-based approach to combine all of this information into high quality, well-composed photos of you with the desired pose and background.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以生成全身自拍照片。这种方法使用已经捕捉的视频、目标姿势照片和每个位置的自拍+背景对。我们提出了一种新的扩散方法，可以将这些信息组合成高质量、正确姿势和背景的全身自拍照片。
</details></li>
</ul>
<hr>
<h2 id="R3D3-Dense-3D-Reconstruction-of-Dynamic-Scenes-from-Multiple-Cameras"><a href="#R3D3-Dense-3D-Reconstruction-of-Dynamic-Scenes-from-Multiple-Cameras" class="headerlink" title="R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras"></a>R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14713">http://arxiv.org/abs/2308.14713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aron Schmied, Tobias Fischer, Martin Danelljan, Marc Pollefeys, Fisher Yu<br>for:多camera系统提供了一个简单且便宜的选择，但是实现实时三维重建和自己运动估计的挑战非常大。methods:我们提出了R3D3，一个多camera系统，它通过调合几何推导和单目深度精准化，实现了紧密的三维重建和自己运动估计。results:我们的设计能够在充满活动的外部环境中实现紧密、一致的三维重建，并在DDAD和NuScenes参考数据集上实现了状态顶尖的紧密深度预测。<details>
<summary>Abstract</summary>
Dense 3D reconstruction and ego-motion estimation are key challenges in autonomous driving and robotics. Compared to the complex, multi-modal systems deployed today, multi-camera systems provide a simpler, low-cost alternative. However, camera-based 3D reconstruction of complex dynamic scenes has proven extremely difficult, as existing solutions often produce incomplete or incoherent results. We propose R3D3, a multi-camera system for dense 3D reconstruction and ego-motion estimation. Our approach iterates between geometric estimation that exploits spatial-temporal information from multiple cameras, and monocular depth refinement. We integrate multi-camera feature correlation and dense bundle adjustment operators that yield robust geometric depth and pose estimates. To improve reconstruction where geometric depth is unreliable, e.g. for moving objects or low-textured regions, we introduce learnable scene priors via a depth refinement network. We show that this design enables a dense, consistent 3D reconstruction of challenging, dynamic outdoor environments. Consequently, we achieve state-of-the-art dense depth prediction on the DDAD and NuScenes benchmarks.
</details>
<details>
<summary>摘要</summary>
dense 3D 重建和自己运动估算是autéonomous driving 和机器人控制中的关键挑战。相比较复杂的多Modal 系统，多摄像头系统提供了一个简单、低成本的替代方案。然而，基于多摄像头的3D 重建复杂动态场景已经证明是非常困难，因为现有的解决方案通常会生成不完整或不一致的结果。我们提出了 R3D3，一个多摄像头系统用于精密3D 重建和自己运动估算。我们的方法在多摄像头的空间-时间信息上进行几何估算，并使用单摄像头的深度精度优化。我们将多摄像头特征相关和紧凑缓冲调整算法相结合，以获得强健的几何深度和pose估算。为了改进重建，特别是对于在运动 объек 或低文纹区域的情况，我们引入了学习场景假设。我们显示，这种设计可以实现高精度、一致的3D 重建，并在复杂的户外环境中达到了状态 искусственный智能 的 dense depth prediction  benchmarks。
</details></li>
</ul>
<hr>
<h2 id="360-Degree-Panorama-Generation-from-Few-Unregistered-NFoV-Images"><a href="#360-Degree-Panorama-Generation-from-Few-Unregistered-NFoV-Images" class="headerlink" title="360-Degree Panorama Generation from Few Unregistered NFoV Images"></a>360-Degree Panorama Generation from Few Unregistered NFoV Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14686">http://arxiv.org/abs/2308.14686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanemankiw/panodiff">https://github.com/shanemankiw/panodiff</a></li>
<li>paper_authors: Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, Li Song</li>
<li>for: 生成完整的360度全景图像</li>
<li>methods: 使用一或多个不准确地捕捉的窄视场图像，以及文本提示和几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几yles in Simplified Chinese text:</li>
<li>for: 生成完整的360度全景图像</li>
<li>methods: 使用一或多个不准确地捕捉的窄视场图像，以及文本提示和几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几种几ypes几种几种几种几种几种几ypes几种几种几种几ypes几种几种几Types几种几Types几种几Types几种几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types几Types�<details>
<summary>Abstract</summary>
360$^\circ$ panoramas are extensively utilized as environmental light sources in computer graphics. However, capturing a 360$^\circ$ $\times$ 180$^\circ$ panorama poses challenges due to the necessity of specialized and costly equipment, and additional human resources. Prior studies develop various learning-based generative methods to synthesize panoramas from a single Narrow Field-of-View (NFoV) image, but they are limited in alterable input patterns, generation quality, and controllability. To address these issues, we propose a novel pipeline called PanoDiff, which efficiently generates complete 360$^\circ$ panoramas using one or more unregistered NFoV images captured from arbitrary angles. Our approach has two primary components to overcome the limitations. Firstly, a two-stage angle prediction module to handle various numbers of NFoV inputs. Secondly, a novel latent diffusion-based panorama generation model uses incomplete panorama and text prompts as control signals and utilizes several geometric augmentation schemes to ensure geometric properties in generated panoramas. Experiments show that PanoDiff achieves state-of-the-art panoramic generation quality and high controllability, making it suitable for applications such as content editing.
</details>
<details>
<summary>摘要</summary>
“360度全景图是计算机图形中广泛应用的环境光源。然而，捕捉360度×180度全景图受到特殊设备和人工资源的限制。先前的研究已经开发了基于学习的生成方法，以synthesize全景图从单个镜头视场（NFoV）图像中，但它们受到输入模式的局限性、生成质量和可控性的限制。为了解决这些问题，我们提出了一个新的渠道 called PanoDiff，它能够高效地使用一个或多个不准确的NFoV图像，从任意角度捕捉到完整的360度全景图。我们的方法包括两个主要组成部分：首先，一个两stage的角度预测模块，可以处理不同数量的NFoV输入。其次，一种新的扩散增强的全景图生成模型，使用部分全景图和文本提示作为控制信号，并使用多种几何增强方案来保证生成的全景图具备几何性质。实验表明，PanoDiff可以实现状态最佳的全景图生成质量和高可控性，适用于内容编辑等应用。”
</details></li>
</ul>
<hr>
<h2 id="Video-Based-Hand-Pose-Estimation-for-Remote-Assessment-of-Bradykinesia-in-Parkinson’s-Disease"><a href="#Video-Based-Hand-Pose-Estimation-for-Remote-Assessment-of-Bradykinesia-in-Parkinson’s-Disease" class="headerlink" title="Video-Based Hand Pose Estimation for Remote Assessment of Bradykinesia in Parkinson’s Disease"></a>Video-Based Hand Pose Estimation for Remote Assessment of Bradykinesia in Parkinson’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.14679">http://arxiv.org/abs/2308.14679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela T. Acevedo Trebbau, Andrea Bandini, Diego L. Guarin</li>
<li>for: 这个研究旨在检验pose estimation算法是否能够在视频流服务中进行远程PD评估和监测。</li>
<li>methods: 研究使用了7种商业可用的手势估计模型来估计视频中手部的运动。</li>
<li>results: 结果显示，在本地记录的视频中，3种模型表现良好，而在视频流服务中记录的视频中，模型的准确率显著下降。研究还发现，视频流服务中的运动速度和模型准确率之间存在负相关性。<details>
<summary>Abstract</summary>
There is a growing interest in using pose estimation algorithms for video-based assessment of Bradykinesia in Parkinson's Disease (PD) to facilitate remote disease assessment and monitoring. However, the accuracy of pose estimation algorithms in videos from video streaming services during Telehealth appointments has not been studied. In this study, we used seven off-the-shelf hand pose estimation models to estimate the movement of the thumb and index fingers in videos of the finger-tapping (FT) test recorded from Healthy Controls (HC) and participants with PD and under two different conditions: streaming (videos recorded during a live Zoom meeting) and on-device (videos recorded locally with high-quality cameras). The accuracy and reliability of the models were estimated by comparing the models' output with manual results. Three of the seven models demonstrated good accuracy for on-device recordings, and the accuracy decreased significantly for streaming recordings. We observed a negative correlation between movement speed and the model's accuracy for the streaming recordings. Additionally, we evaluated the reliability of ten movement features related to bradykinesia extracted from video recordings of PD patients performing the FT test. While most of the features demonstrated excellent reliability for on-device recordings, most of the features demonstrated poor to moderate reliability for streaming recordings. Our findings highlight the limitations of pose estimation algorithms when applied to video recordings obtained during Telehealth visits, and demonstrate that on-device recordings can be used for automatic video-assessment of bradykinesia in PD.
</details>
<details>
<summary>摘要</summary>
有越来越多的关注使用pose estimation算法来评估基于视频的PD患者的静止症状评估和监测。然而，在视频流服务中使用pose estimation算法的准确性尚未被研究。本研究使用了七种市售手势估计模型来估计视频中的手指 thumb和index fingers的运动。我们使用了HC和PD患者在两种不同条件下录制的视频：流程（在live Zoom会议中录制的视频）和本地（使用高质量摄像头录制的视频）。我们将模型的准确性和可靠性与手动结果进行比较。三种模型在本地录制下表现良好，而流程录制下的准确性显著降低。我们发现视频录制中运动速度与模型的准确性之间存在负相关性。此外，我们评估了PD患者在执行FT测试时录制的视频中关于静止症状的10种运动特征的可靠性。大多数特征在本地录制下表现出色，而流程录制下大多数特征表现为亮度到中度的可靠性。我们的发现表明在Telehealth访问中使用pose estimation算法进行自动视频评估的难度，并且可以使用本地录制来实现更高的准确性和可靠性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/29/cs.CV_2023_08_29/" data-id="clm0t8dz40042v788elwuf1s4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
