
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.SD_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T15:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.SD_2023_10_11/">cs.SD - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Vec-Tok-Speech-speech-vectorization-and-tokenization-for-neural-speech-generation"><a href="#Vec-Tok-Speech-speech-vectorization-and-tokenization-for-neural-speech-generation" class="headerlink" title="Vec-Tok Speech: speech vectorization and tokenization for neural speech generation"></a>Vec-Tok Speech: speech vectorization and tokenization for neural speech generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07246">http://arxiv.org/abs/2310.07246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bakerbunker/vectok">https://github.com/bakerbunker/vectok</a></li>
<li>paper_authors: Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: 这 paper 是为了提出一种可以执行多种语音生成任务的框架，包括语音代码、语音Vector 和语音Token。</li>
<li>methods: 该 paper 使用了一种新的语音编码方法，即基于语音 вектор 和语音标记的语音编码方法，以及一种叫BYTE-PAIR编码的Token 压缩方法来提高语音生成模型的性能。</li>
<li>results:  experiments 表明，基于50k小时语音数据的Vec-Tok Speech 模型比其他SOTA模型更好，可以用于语音代码、语音风格转换、语音译文、语音干扰和 speaker 匿名化等任务。<details>
<summary>Abstract</summary>
Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在自然语言处理和计算机视觉领域刚刚繁荣，生成高效率的文本或图像。然而，当前的语音生成模型仍然受到语音质量和任务通用性的限制。本文提出Vec-Tok Speech框架，可以模拟多种语音生成任务，生成有表达力和高效率的语音。具体来说，我们提出了一种基于语音向量和 semantics 的语音编码方式。语音向量包含高效率语音重建的音响细节，而 semantics 则关注语音中的语言内容，使得语言模型化更加容易。基于该语音编码方式，Vec-Tok Speech 利用了一个LM来完成语音生成的核心。此外，我们还引入了字节对编码（BPE），以降低token长度和比特率，从而改善LM的性能。Vec-Tok Speech 可以用于跨语言零批变声（VC）、零批说话风格传输（TTS）、语音传输（S2ST）、语音干扰 removal 和 speaker 隐藏和匿名化。实验表明，Vec-Tok Speech，基于50万小时的语音数据，比其他状态注意的模型表现更好。代码将在 GitHub 上提供。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.SD_2023_10_11/" data-id="clnsn0vjz00mfgf8822i2ceqk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.AS_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T14:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.AS_2023_10_11/">eess.AS - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Damping-Density-of-an-Absorptive-Shoebox-Room-Derived-from-the-Image-Source-Method"><a href="#Damping-Density-of-an-Absorptive-Shoebox-Room-Derived-from-the-Image-Source-Method" class="headerlink" title="Damping Density of an Absorptive Shoebox Room Derived from the Image-Source Method"></a>Damping Density of an Absorptive Shoebox Room Derived from the Image-Source Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07363">http://arxiv.org/abs/2310.07363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian J. Schlecht, Karolina Prawda, Rudolf Rabenstein, Maximilian Schäfer</li>
<li>for: 这篇论文旨在计算带有不同吸收率的封闭房间响应（RIR）。</li>
<li>methods: 论文使用图像源方法计算封闭房间响应，但是随着响应长度的增加，图像源的数量快速增加，导致计算慢。论文提出了一个关闭式表达式，用于计算抑挫密度，这个密度描述了总体多坡落能量减振。</li>
<li>results: 论文通过对墙面吸收率的变化进行仿真，显示了该模型可以准确地预测封闭房间的响应。论文还提出了一种基于能量抑挫的快速随机生成方法，可以快速生成封闭房间的晚期响应。<details>
<summary>Abstract</summary>
The image-source method is widely applied to compute room impulse responses (RIRs) of shoebox rooms with arbitrary absorption. However, with increasing RIR lengths, the number of image sources grows rapidly, leading to slow computation. In this paper, we derive a closed-form expression for the damping density, which characterizes the overall multi-slope energy decay. The omnidirectional energy decay over time is directly derived from the damping density. The resulting energy decay model accurately matches the late reverberation simulated via the image-source method. The proposed model allows the fast stochastic synthesis of late reverberation by shaping noise with the energy envelope. Simulations of various wall damping coefficients demonstrate the model's accuracy. The proposed model consistently outperforms the energy decay prediction accuracy compared to a state-of-the-art approximation method. The paper elaborates on the proposed damping density's applicability to modeling multi-sloped sound energy decay, predicting reverberation time in non-diffuse sound fields, and fast frequency-dependent RIR synthesis.
</details>
<details>
<summary>摘要</summary>
《图像源方法广泛应用于计算射镜室内响（RIR）的耳朵型吸收。然而，随着 RIR 的增加，图像源的数量快速增加，导致计算慢。在这篇论文中，我们 derive 一个关闭式表达式 для抑止密度，这个密度描述了总多坡能量衰减。通过这个密度，我们直接 deriv 出了全向能量衰减过时。这种能量衰减模型与图像源方法 simulate 的晚期干扰匹配得非常高。我们的模型允许快速随机生成晚期干扰，通过形态噪声的能量范围来控制噪声的能量。我们的模型在不同墙面吸收率下进行了丰富的 validate，并且在比较方法中表现出了更高的准确性。论文还详细介绍了我们的抑止密度如何应用于多坡音能量衰减、预测非散射声场中的响应时间以及快速频率相关 RIR 生成。》
</details></li>
</ul>
<hr>
<h2 id="Magnitude-and-phase-aware-Speech-Enhancement-with-Parallel-Sequence-Modeling"><a href="#Magnitude-and-phase-aware-Speech-Enhancement-with-Parallel-Sequence-Modeling" class="headerlink" title="Magnitude-and-phase-aware Speech Enhancement with Parallel Sequence Modeling"></a>Magnitude-and-phase-aware Speech Enhancement with Parallel Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07316">http://arxiv.org/abs/2310.07316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuewei Zhang, Huanbin Zou, Jie Zhu</li>
<li>for: 这篇论文主要是关于喷 speech 提升 (SE) 中的阶段估计，以提高喷 speech 的性能。</li>
<li>methods: 本文提出了一种使用实数网络来估计干扰 speech 的大小面积和归一化 cIRM，而不是使用复杂的网络来预测复杂的目标。同时，文章还提出了一种并行序列模型 (PSM) 块来改进 CRN 基于的卷积循环网络 (CRN) 模型。</li>
<li>results: 实验结果表明，我们的 MPCRN 方法在喷 speech 提升中表现出色。<details>
<summary>Abstract</summary>
In speech enhancement (SE), phase estimation is important for perceptual quality, so many methods take clean speech's complex short-time Fourier transform (STFT) spectrum or the complex ideal ratio mask (cIRM) as the learning target. To predict these complex targets, the common solution is to design a complex neural network, or use a real network to separately predict the real and imaginary parts of the target. But in this paper, we propose to use a real network to estimate the magnitude mask and normalized cIRM, which not only avoids the significant increase of the model complexity caused by complex networks, but also shows better performance than previous phase estimation methods. Meanwhile, we devise a parallel sequence modeling (PSM) block to improve the RNN block in the convolutional recurrent network (CRN)-based SE model. We name our method as magnitude-and-phase-aware and PSM-based CRN (MPCRN). The experimental results illustrate that our MPCRN has superior SE performance.
</details>
<details>
<summary>摘要</summary>
在声音增强（SE）中，相位估计对于听觉质量非常重要，因此许多方法会使用清晰语音的复杂短时傅立卷（STFT）谱或理想比率面（cIRM）作为学习目标。为了预测这些复杂的目标，常见的解决方案是设计复杂的神经网络，或者使用实数网络来分开预测实数和虚数部分。然而，在这篇论文中，我们提议使用实数网络来估计幅度面和 норmalized cIRM，这不仅避免了复杂网络增加模型复杂度的问题，而且也比前期相位估计方法表现更好。此外，我们还设计了并行序列模型（PSM）块来改进CRN网络中的RNN块。我们命名我们的方法为幅度-相位感知和PSM-based CRN（MPCRN）。实验结果表明，我们的MPCRN方法在SE性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="VSANet-Real-time-Speech-Enhancement-Based-on-Voice-Activity-Detection-and-Causal-Spatial-Attention"><a href="#VSANet-Real-time-Speech-Enhancement-Based-on-Voice-Activity-Detection-and-Causal-Spatial-Attention" class="headerlink" title="VSANet: Real-time Speech Enhancement Based on Voice Activity Detection and Causal Spatial Attention"></a>VSANet: Real-time Speech Enhancement Based on Voice Activity Detection and Causal Spatial Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07295">http://arxiv.org/abs/2310.07295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuewei Zhang, Huanbin Zou, Jie Zhu</li>
<li>for: 提高speech干扰改进（SE）性能</li>
<li>methods: 使用多任务学习框架和 causal spatial attention（CSA）块</li>
<li>results: 实验结果表明，VSANet的SE性能得到了提高，证明了多任务学习和CSA块的效用<details>
<summary>Abstract</summary>
The deep learning-based speech enhancement (SE) methods always take the clean speech's waveform or time-frequency spectrum feature as the learning target, and train the deep neural network (DNN) by reducing the error loss between the DNN's output and the target. This is a conventional single-task learning paradigm, which has been proven to be effective, but we find that the multi-task learning framework can improve SE performance. Specifically, we design a framework containing a SE module and a voice activity detection (VAD) module, both of which share the same encoder, and the whole network is optimized by the weighted loss of the two modules. Moreover, we design a causal spatial attention (CSA) block to promote the representation capability of DNN. Combining the VAD aided multi-task learning framework and CSA block, our SE network is named VSANet. The experimental results prove the benefits of multi-task learning and the CSA block, which give VSANet an excellent SE performance.
</details>
<details>
<summary>摘要</summary>
深度学习基于的speech enhancement（SE）方法总是使用干净话音的波形或时域频谱特征作为学习目标，并通过减少深度神经网络（DNN）的出力与目标之间的错误损失来训练深度神经网络。这是一种传统的单任务学习模式，已经被证明有效，但我们发现多任务学习框架可以提高SE性能。 Specifically, we design a framework containing a SE module and a voice activity detection（VAD）module, both of which share the same encoder, and the whole network is optimized by the weighted loss of the two modules. Moreover, we design a causal spatial attention（CSA）block to promote the representation capability of DNN. Combining the VAD aided multi-task learning framework and CSA block, our SE network is named VSANet. The experimental results prove the benefits of multi-task learning and the CSA block, which give VSANet an excellent SE performance.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.AS_2023_10_11/" data-id="clnsn0vko00opgf88d1ju7w9p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.CV_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T13:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.CV_2023_10_11/">cs.CV - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Appearance-Particle-Neural-Radiance-Field"><a href="#Dynamic-Appearance-Particle-Neural-Radiance-Field" class="headerlink" title="Dynamic Appearance Particle Neural Radiance Field"></a>Dynamic Appearance Particle Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07916">http://arxiv.org/abs/2310.07916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ancheng Lin, Jun Li</li>
<li>for: 模elling 3D 动态场景的 Dynamic NeRFs 扩展了这种模型，通常使用变形场景来捕捉时间变化的元素。</li>
<li>methods: DAP-NeRF 引入了粒子基的表示方法，模型了动态场景中视觉元素的运动。它由静态场景和动态场景的积和而组成，其中动态场景是由一系列的“visual particles”组成，每个粒子携带了小元素的视觉信息和运动模型。所有组件都是从单摄视频中学习而来，不需要任何先知的场景知识。</li>
<li>results: DAP-NeRF 能够有效地捕捉动态场景中的 appearances 和物理意义的运动。我们构建了一个新的评估数据集，并开发了高效的计算框架。实验结果表明，DAP-NeRF 是一种有效的技术来捕捉3D 动态场景中的视觉信息和物理运动。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of superposition of a static field and a dynamic field. The dynamic field is quantised as a collection of {\em appearance particles}, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and motion models of the particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modelling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 有大量的潜力来模型3D场景。动态NeRFs进一步发展了这种模型，通常使用变形场来捕捉时间变化的元素。现有的动态NeRFs使用类似的尤里安表示法来表示光辐Radiance和变形场。这会导致外观和运动之间的强烈相互关联，缺乏物理解释。在这项工作中，我们提出了动态外观粒子神经辐Radiance场（DAP-NeRF），它通过粒子基本表示来模拟动态3D场景中的视觉元素的运动。DAP-NeRF包括一个静止场和一个动态场的积addition。动态场被量化为一个集合的“外观粒子”，这些粒子携带了小动态场景中的视觉信息，并装备了运动模型。所有组件，包括静止场、视觉特征和运动模型，都是从单光视频中学习而来，不需要任何先前的场景知识。我们开发了一个高效的计算框架，并构建了一个新的数据集来评估运动模elling。实验结果表明，DAP-NeRF是一种有效的技术来捕捉3D动态场景中的外观和物理意义的运动。
</details></li>
</ul>
<hr>
<h2 id="NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration"><a href="#NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration" class="headerlink" title="NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration"></a>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07896">http://arxiv.org/abs/2310.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine<br>for:This paper aims to provide a unified diffusion policy for both task-oriented navigation and task-agnostic exploration in unfamiliar environments.methods:The proposed method uses a large-scale Transformer-based policy and a diffusion model decoder to handle both goal-conditioned and goal-agnostic navigation.results:The proposed method shows better overall performance in navigating to visually indicated goals in novel environments, with significant improvements in performance and lower collision rates compared to five alternative methods.<details>
<summary>Abstract</summary>
Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/
</details>
<details>
<summary>摘要</summary>
机器人学习 для未知环境导航需要提供任务域导航策略和任务无关探索策略。通常，这些角色是通过不同的模型来实现，例如使用互助提案、规划或分开的导航策略。在这篇论文中，我们描述了如何使用单一的扩散策略来处理目标域导航和任务无关探索，其中前者可以帮助机器人达到用户指定的目标，而后者可以在新环境中搜索目标。我们表明，这种单一策略会比使用生成模型的互助提案或先前的秘密变量模型方法更好地表现，并且在未知环境中导航到视觉指定的目标时，我们的实验结果表明，这种策略比其他五种方法更好。我们在一个真实世界的移动机器人 платформа上实现了我们的方法，并在多个机器人的数据上训练了一个大规模的Transformer基于策略。我们的实验结果显示，我们的方法可以在未seen环境中有效地导航，并且与其他五种方法相比，具有更高的性能和较低的碰撞率，即使使用较小的模型。更多视频、代码和预训练模型检查点，请参考<https://general-navigation-models.github.io/nomad/>。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder"><a href="#Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder" class="headerlink" title="Unsupervised Structured Noise Removal with Variational Lossy Autoencoder"></a>Unsupervised Structured Noise Removal with Variational Lossy Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07887">http://arxiv.org/abs/2310.07887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krulllab/DVLAE">https://github.com/krulllab/DVLAE</a></li>
<li>paper_authors: Benjamin Salmon, Alexander Krull</li>
<li>for: 这篇论文旨在开发一个不需要清洁图像或噪声模型的无监督深度学习基础设计于类型的噪声去除方法。</li>
<li>methods: 这篇论文使用了一种名为Variational Autoencoder（VAE）的深度学习模型，并将其拓展为一个具有自适应的推导器，可以模型类型的噪声 Component，但无法独立地模型清洁信号 Component。</li>
<li>results: 实验结果显示，这种方法可以比以往的自我监督和无监督图像噪声去除方法表现更好，并且具有较大的推导器适应范围。<details>
<summary>Abstract</summary>
Most unsupervised denoising methods are based on the assumption that imaging noise is either pixel-independent, i.e., spatially uncorrelated, or signal-independent, i.e., purely additive. However, in practice many imaging setups, especially in microscopy, suffer from a combination of signal-dependent noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe shaped scanning or readout artifacts). In this paper, we present the first unsupervised deep learning-based denoiser that can remove this type of noise without access to any clean images or a noise model. Unlike self-supervised techniques, our method does not rely on removing pixels by masking or subsampling so can utilize all available information. We implement a Variational Autoencoder (VAE) with a specially designed autoregressive decoder capable of modelling the noise component of an image but incapable of independently modelling the underlying clean signal component. As a consequence, our VAE's encoder learns to encode only underlying clean signal content and to discard imaging noise. We also propose an additional decoder for mapping the encoder's latent variables back into image space, thereby sampling denoised images. Experimental results demonstrate that our approach surpasses existing methods for self- and unsupervised image denoising while being robust with respect to the size of the autoregressive receptive field. Code for this project can be found at https://github.com/krulllab/DVLAE.
</details>
<details>
<summary>摘要</summary>
大多数无监督降噪方法假设图像噪声是像素独立的，即空间无相关性，或者信号独立的，即纯加性的。然而，在实际的摄影设置中，尤其是在 Mikroskop 中，往往会出现混合的信号依赖噪声（例如，Poisson 抽样噪声）和水平对齐的噪声（例如，扫描或者读取器artefacts）。在这篇论文中，我们提出了第一个无监督深度学习基于的降噪器，可以 removing 这种类型的噪声，不需要任何净图像或噪声模型。不同于自动学习技术，我们的方法不需要通过屏蔽或者抽取来移除像素，因此可以使用所有可用的信息。我们实现了一个Variational Autoencoder（VAE），其中的特别设计的推理树可以模型图像中的噪声组成部分，而不可以独立地模型图像中的净信号组成部分。因此，我们的VAE的编码器将只编码净信号内容，并且抛弃摄影噪声。我们还提出了一个用于将编码器的秘密变量映射回图像空间的额外decoder，从而生成降噪后的图像。实验结果表明，我们的方法超越了现有的自动和无监督图像降噪方法，并且对探测器感知场的大小具有Robust性。代码可以在 https://github.com/krulllab/DVLAE 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection"><a href="#A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection" class="headerlink" title="A Survey of Feature Types and Their Contributions for Camera Tampering Detection"></a>A Survey of Feature Types and Their Contributions for Camera Tampering Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07886">http://arxiv.org/abs/2310.07886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Mantini, Shishir K. Shah</li>
<li>for: 本研究旨在探讨视频监控摄像头中的干扰检测，以确定未经授权或不意图的 Alterations。</li>
<li>methods: 本研究使用时间序列分析方法进行干扰检测，并对不同特征类型进行评估。</li>
<li>results: 研究发现了不同特征类型的predictability和干扰检测能力，并对不同时间序列模型的选择进行了评估。<details>
<summary>Abstract</summary>
Camera tamper detection is the ability to detect unauthorized and unintentional alterations in surveillance cameras by analyzing the video. Camera tampering can occur due to natural events or it can be caused intentionally to disrupt surveillance. We cast tampering detection as a change detection problem, and perform a review of the existing literature with emphasis on feature types. We formulate tampering detection as a time series analysis problem, and design experiments to study the robustness and capability of various feature types. We compute ten features on real-world surveillance video and apply time series analysis to ascertain their predictability, and their capability to detect tampering. Finally, we quantify the performance of various time series models using each feature type to detect tampering.
</details>
<details>
<summary>摘要</summary>
Surveillance 相机损害检测是指通过视频分析来检测侦测器中的未授权或不计划的修改。相机修改可能由自然事件引起，也可能由意外的方式引起，以阻断侦测。我们将修改检测定义为变化检测问题，并对现有文献进行了审查，特别是关于特征类型。我们将修改检测定义为时间序列分析问题，并设计了实验来评估不同特征类型的稳定性和检测能力。我们在实际的侦测视频中计算了10个特征，并应用时间序列分析来确定它们的预测性和修改检测能力。最后，我们使用不同的时间序列模型来评估各种特征类型的修改检测性能。
</details></li>
</ul>
<hr>
<h2 id="BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI"><a href="#BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI" class="headerlink" title="BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI"></a>BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08608">http://arxiv.org/abs/2310.08608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Singh, Dr. Mrunal Bewoor, Ammar Ranapurwala, Satyam Rai, Sheetal Patil</li>
<li>for: 这个研究旨在使用深度学习框架将三维ultrasound图像转换为三维MRI图像。</li>
<li>methods: 该方法使用Pix2Pix GAN模型，输入3Dultrasound图像，通过UNET生成器和patch检测器，生成相应的3DMRI图像。</li>
<li>results: 实验结果表明，生成的MRI图像与预期结果具有一定的相似性。 despite some challenges, the method successfully generated MRI volume with a satisfactory similarity score, demonstrating the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion.<details>
<summary>Abstract</summary>
The study presents a deep learning framework aimed at synthesizing 3D MRI volumes from three-dimensional ultrasound images of the brain utilizing the Pix2Pix GAN model. The process involves inputting a 3D volume of ultrasounds into a UNET generator and patch discriminator, generating a corresponding 3D volume of MRI. Model performance was evaluated using losses on the discriminator and generator applied to a dataset of 3D ultrasound and MRI images. The results indicate that the synthesized MRI images exhibit some similarity to the expected outcomes. Despite challenges related to dataset size, computational resources, and technical complexities, the method successfully generated MRI volume with a satisfactory similarity score meant to serve as a baseline for further research. It underscores the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion, showcasing their viability for medical applications. Further refinement and exploration are warranted for enhanced clinical relevance.
</details>
<details>
<summary>摘要</summary>
这项研究提出了一种基于深度学习的3D MRI卷积图像合成框架，用于将三维超声图像转换为3D MRI卷积图像。该过程中，将3D超声图像输入到UNET生成器和质量检测器中，生成匹配的3D MRI卷积图像。模型性能通过对3D超声和MRI图像集进行损失评估来评估。结果表明，生成的MRI卷积图像具有一定的相似性。虽面临 dataset 大小、计算资源和技术复杂性等挑战，但方法仍然成功地生成了一个满足要求的MRI卷积图像，并且达到了一定的相似性分数。这表明了深度学习基于超声到MRI卷积图像转换的技术在医疗应用中的潜在可能性。进一步的优化和探索是需要的，以提高临床实用性。
</details></li>
</ul>
<hr>
<h2 id="CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping"><a href="#CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping" class="headerlink" title="CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping"></a>CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07855">http://arxiv.org/abs/2310.07855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars</li>
<li>for: 提高 dense visual representation learning 的精度和效果，特别是在 scene-centric 图像 dataset 上。</li>
<li>methods: 使用 object-level nearest neighbor bootstrapping 方法，通过在训练时对每个对象进行 nearest neighbor 选择，提高 visual representation 的精度和准确性。</li>
<li>results: CrIBo 在 in-context learning 任务上表现出色，与标准 downstream segmentation 任务具有高度竞争力。<details>
<summary>Abstract</summary>
Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream segmentation tasks. Our code and pretrained models will be publicly available upon acceptance.
</details>
<details>
<summary>摘要</summary>
利用最近邻居检索 для自主学习视觉表示学习已经在对象中心图像上得到了好的效果。然而，这种方法在场景中心数据集上遇到限制，因为图像中的多个对象只是间接地被捕捉在全局表示中。这可能导致对象表示的杂糅。此外，即使是对象中心数据集也可以从更细化的杂糅方法中受益。为了解决这些挑战，我们介绍了一种新的隐藏图像对象级Bootstrap方法（CrIBo），用于增强精细的视觉表示学习。在训练期间，CrIBo使用对象级最近邻居检索，以提高密集视觉表示学习。CrIBo在测试时使用最近邻居检索，并达到了状态机器人的性能，同时在标准下游分类任务中也具有高竞争力。我们将代码和预训练模型在接受后公开。
</details></li>
</ul>
<hr>
<h2 id="Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models"><a href="#Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models" class="headerlink" title="Explorable Mesh Deformation Subspaces from Unstructured Generative Models"></a>Explorable Mesh Deformation Subspaces from Unstructured Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07814">http://arxiv.org/abs/2310.07814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman Maesumi, Paul Guerrero, Vladimir G. Kim, Matthew Fisher, Siddhartha Chaudhuri, Noam Aigerman, Daniel Ritchie</li>
<li>for:  explore variations of 3D shapes</li>
<li>methods:  construct a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model</li>
<li>results:  produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories<details>
<summary>Abstract</summary>
Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth variations between them. We then show how to turn the variations in this subspace into deformation fields, to transfer those variations to high-quality meshes for the landmark shapes. Our results show that our method can produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories, especially as compared to prior work on learning deformation spaces for 3D shapes.
</details>
<details>
<summary>摘要</summary>
输入文本翻译成简化中文：传统3D模型创建工具中调查3D形状的变化是时间consuming的过程。深度生成模型的3D形状通常具有连续的秘密空间，可以从输入形状开始探索可能的变化。在实践中，这可能会是一些问题：秘密空间具有高维度和难以视见，包含不相关的形状，以及线性路径通常会导致形状转化不优化。此外，我们希望能够探索输入高质量网格用于训练生成模型的变化，而不是生成模型的输出几何体。在这篇论文中，我们提出了一种方法，通过从易于探索的2D探索空间到 pré-训练的生成模型的子空间中映射，来探索给定的标志形状中的变化。我们首先描述了如何找到映射，使得它覆盖输入标志形状并且在它们之间展现平滑的变化。然后，我们示出了如何将这些变化转化为几何场，以将这些变化应用到高质量网格上。我们的结果显示，我们的方法可以生成可见和易于探索的2D探索空间，特别是与先前学习3D形状几何场的方法相比。</SYS>Here's the translation in Simplified Chinese:传统3D模型创建工具中调查3D形状的变化是时间consuming的过程。深度生成模型的3D形状通常具有连续的秘密空间，可以从输入形状开始探索可能的变化。在实践中，这可能会是一些问题：秘密空间具有高维度和难以视见，包含不相关的形状，以及线性路径通常会导致形状转化不优化。此外，我们希望能够探索输入高质量网格用于训练生成模型的变化，而不是生成模型的输出几何体。在这篇论文中，我们提出了一种方法，通过从易于探索的2D探索空间到 pré-训练的生成模型的子空间中映射，来探索给定的标志形状中的变化。我们首先描述了如何找到映射，使得它覆盖输入标志形状并且在它们之间展现平滑的变化。然后，我们示出了如何将这些变化转化为几何场，以将这些变化应用到高质量网格上。我们的结果显示，我们的方法可以生成可见和易于探索的2D探索空间，特别是与先前学习3D形状几何场的方法相比。
</details></li>
</ul>
<hr>
<h2 id="CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving"><a href="#CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving" class="headerlink" title="CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving"></a>CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07794">http://arxiv.org/abs/2310.07794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli</li>
<li>for: 本文旨在提供一个新的评估自动驾驶路径预测模型的 benchmarking 方法，以提高现有的评估方法的精度和完整性。</li>
<li>methods: 本文提出了一种基于道路结构、模型性能和数据特性的驱动enario分类方法，以及一组新的偏好free metrics，用于衡量路径多样性和可接受性。</li>
<li>results: 经过对大规模的 Argoverse 数据集进行实验，本文示出了该benchmark可以更好地评估路径预测模型的性能，并提供了一种Characterizing模型行为的方法。此外，本文还进行了减少研究，以阐明不同元素对计算所提出的贡献。<details>
<summary>Abstract</summary>
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.   In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction models; 2) A set of new bias-free metrics for measuring diversity, by incorporating the characteristics of a given scenario, and admissibility, by considering the structure of roads and kinematic compliancy, motivated by real-world driving constraints. 3) Using the proposed benchmark, we conduct extensive experimentation on a representative set of the prediction models using the large scale Argoverse dataset. We show that the proposed benchmark can produce a more accurate ranking of the models and serve as a means of characterizing their behavior. We further present ablation studies to highlight contributions of different elements that are used to compute the proposed metrics.
</details>
<details>
<summary>摘要</summary>
benchmarking是自适应驾驶模型评估的常见方法。现有的benchmark都依赖于dataset，这些dataset偏向于更常见的enario，如驶行，并使用距离基于的metric来计算。这种办法只提供一些Scene的Properties，不能够准确地评估模型在不同的enario下的性能。 существуют一些补偿metric，用于测量trajectory的多样性和合法性，但它们受到length of trajectory的偏见。在这篇论文中，我们提出了一种新的benchmarking方法，用于评估trajectory prediction Approaches（CRITERIA）。具体来说，我们提出了以下方法：1. 提取驾驶enario的不同水平Specificity，根据道路结构、模型性能和数据特性进行细化的排名预测模型。2. 使用新的偏好度为零的多样性度量，通过包含场景特点来测量trajectory的多样性。3. 使用道路结构和动力学兼容性来测量trajectory的合法性，以驱动模型在实际驾驶中的行为。使用我们提出的benchmark，我们在Argoverse dataset上进行了广泛的实验，并证明了我们的benchmark可以更准确地排名模型，并且可以用来描述模型的行为。我们还进行了剥离研究，以阐明不同元素的贡献。
</details></li>
</ul>
<hr>
<h2 id="An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions"><a href="#An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions" class="headerlink" title="An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions"></a>An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07782">http://arxiv.org/abs/2310.07782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PurdueCAM2Project/focused-convolutions">https://github.com/PurdueCAM2Project/focused-convolutions</a></li>
<li>paper_authors: Caleb Tung, Nicholas Eliopoulos, Purvish Jajal, Gowri Ramshankar, Chen-Yun Yang, Nicholas Synovic, Xuecen Zhang, Vipin Chaudhary, George K. Thiruvathukal, Yung-Hsiang Lu</li>
<li>for: 提高卷积神经网络的能效性，不需要重新训练。</li>
<li>methods: 插入一个阈值层，对前一些层的活化进行过滤，以实现快速的搜寻和精炼操作，从而提高卷积神经网络的执行效率和能效性。</li>
<li>results: 对多种流行的预训练卷积神经网络进行修改，可以降低推理延迟（最多25%）和能耗成本（最多22%），而减少准确性的损失。<details>
<summary>Abstract</summary>
Computer vision often uses highly accurate Convolutional Neural Networks (CNNs), but these deep learning models are associated with ever-increasing energy and computation requirements. Producing more energy-efficient CNNs often requires model training which can be cost-prohibitive. We propose a novel, automated method to make a pretrained CNN more energy-efficient without re-training. Given a pretrained CNN, we insert a threshold layer that filters activations from the preceding layers to identify regions of the image that are irrelevant, i.e. can be ignored by the following layers while maintaining accuracy. Our modified focused convolution operation saves inference latency (by up to 25%) and energy costs (by up to 22%) on various popular pretrained CNNs, with little to no loss in accuracy.
</details>
<details>
<summary>摘要</summary>
计算机视觉经常使用高度精准的卷积神经网络（CNNs），但这些深度学习模型与计算和能源需求不断增长。制作更能效的CNNs通常需要模型训练，但这可能是成本禁止的。我们提出了一种新的自动化方法，可以使给定的预训练CNN更加能效，不需要重新训练。我们在预训练CNN中插入了一层阈值层，可以从前一些层的活动中过滤无关的图像区域，保持精度不变。我们的修改后的减少推理延迟（最多下降25%）和能耗成本（最多下降22%），在多种流行的预训练CNN上都有显著的效果，而且几乎没有损失精度。
</details></li>
</ul>
<hr>
<h2 id="3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers"><a href="#3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers" class="headerlink" title="3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers"></a>3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07781">http://arxiv.org/abs/2310.07781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Beckschen/3D-TransUNet">https://github.com/Beckschen/3D-TransUNet</a></li>
<li>paper_authors: Jieneng Chen, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou</li>
<li>for: 这篇论文的目的是将Transformer应用到医疗影像分类 tasks中，以提高分类效果。</li>
<li>methods: 这篇论文使用了Transformer的自我注意力机制，并与U-Net结合，以实现更好的长距离依赖关系modeling。</li>
<li>results: 实验结果显示，TransUNet在不同的医疗应用中具有优秀的表现，并且在多组织分类和肿瘤分类 task中表现更好。<details>
<summary>Abstract</summary>
Medical image segmentation plays a crucial role in advancing healthcare systems for disease diagnosis and treatment planning. The u-shaped architecture, popularly known as U-Net, has proven highly successful for various medical image segmentation tasks. However, U-Net's convolution-based operations inherently limit its ability to model long-range dependencies effectively. To address these limitations, researchers have turned to Transformers, renowned for their global self-attention mechanisms, as alternative architectures. One popular network is our previous TransUNet, which leverages Transformers' self-attention to complement U-Net's localized information with the global context. In this paper, we extend the 2D TransUNet architecture to a 3D network by building upon the state-of-the-art nnU-Net architecture, and fully exploring Transformers' potential in both the encoder and decoder design. We introduce two key components: 1) A Transformer encoder that tokenizes image patches from a convolution neural network (CNN) feature map, enabling the extraction of global contexts, and 2) A Transformer decoder that adaptively refines candidate regions by utilizing cross-attention between candidate proposals and U-Net features. Our investigations reveal that different medical tasks benefit from distinct architectural designs. The Transformer encoder excels in multi-organ segmentation, where the relationship among organs is crucial. On the other hand, the Transformer decoder proves more beneficial for dealing with small and challenging segmented targets such as tumor segmentation. Extensive experiments showcase the significant potential of integrating a Transformer-based encoder and decoder into the u-shaped medical image segmentation architecture. TransUNet outperforms competitors in various medical applications.
</details>
<details>
<summary>摘要</summary>
医疗图像分割在提高医疗系统的疾病诊断和治疗规划方面扮演着关键角色。U-Net建筑物，广泛地知道为各种医疗图像分割任务中的成功之路。然而，U-Net的卷积操作自然地限制了其能够有效地模型长距离依赖关系的能力。为了解决这些限制，研究人员转向了转换器，因为它们的全局自注意机制，成为了可能的替代体系。我们之前的TransUNet网络，利用转换器的全局自注意机制，补充U-Net的本地信息，并获得全球上下文。在这篇论文中，我们将2D TransUNet架构扩展到3D网络，基于当前领域的nnU-Net架构，并充分发挥转换器的潜力。我们引入了两个关键组件：1）使用转换器Encoder将图像块 Tokenize成CNN特征图，以EXTRACT全局上下文，2）使用转换器Decoderadaptively进行候选区域精细调整，通过候选提案和U-Net特征之间的交叉注意力。我们的调查发现，不同的医疗任务会有不同的建筑设计。转换器Encoder在多器官分割任务中表现出色，而转换器Decoder在小型和复杂的分割目标，如肿瘤分割任务中表现更加出色。我们的延伸实验表明，将转换器基于Encoder和Decoderintegretch到U-shaped医疗图像分割架构中，可以提高TransUNet在各种医疗应用中的表现。TransUNet在各种医疗应用中表现出色，超过了竞争对手。
</details></li>
</ul>
<hr>
<h2 id="OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation"><a href="#OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation" class="headerlink" title="OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"></a>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07749">http://arxiv.org/abs/2310.07749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo</li>
<li>for: 这个研究探讨了一个开放领域的图文生成任务，即根据输入查询生成杂合的图文内容。</li>
<li>methods: 我们提出了一个基于大型自然语言模型（LLM）和预训练的文本到图像（T2I）模型的新的杂合生成框架，称之为OpenLEAF。在OpenLEAF中，LLM生成文本描述，协调T2I模型，创建视觉提示 для生成图像，并将全局上下文 integrate到T2I模型中。</li>
<li>results: 根据我们构建的评估集，使用大型多Modal模型（LMM）评估实体和风格一致性，我们的提posed杂合生成框架可以生成高质量的图文内容，适用于多个领域和应用，如问答、故事、图文重新编写、幕布生成等等。此外，我们验证了我们提出的LMM评估技术的有效性通过人工评估。<details>
<summary>Abstract</summary>
This work investigates a challenging task named open-domain interleaved image-text generation, which generates interleaved texts and images following an input query. We propose a new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions, coordinates T2I models, creates visual prompts for generating images, and incorporates global contexts into the T2I models. This global context improves the entity and style consistencies of images in the interleaved generation. For model assessment, we first propose to use large multi-modal models (LMMs) to evaluate the entity and style consistencies of open-domain interleaved image-text sequences. According to the LMM evaluation on our constructed evaluation set, the proposed interleaved generation framework can generate high-quality image-text content for various domains and applications, such as how-to question answering, storytelling, graphical story rewriting, and webpage/poster generation tasks. Moreover, we validate the effectiveness of the proposed LMM evaluation technique with human assessment. We hope our proposed framework, benchmark, and LMM evaluation could help establish the intriguing interleaved image-text generation task.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了一个复杂的任务 named 开放领域交叠图文生成，该任务通过输入查询生成交叠的图文内容。我们提出了一个新的交叠生成框架，基于大型自然语言模型（LLM）和预训练的文本到图像（T2I）模型，称之为 OpenLEAF。在 OpenLEAF 中，LLM 生成文本描述、协调 T2I 模型、创建视觉提示生成图像，并将全局上下文 integrate 到 T2I 模型中。这个全局上下文提高了图像在交叠生成中的实体和风格一致性。为了评估模型效果，我们首先提出了使用大型多Modal模型（LMM）来评估交叠生成中的实体和风格一致性。根据 LMM 的评估，我们构建的评估集上，提案的交叠生成框架可以生成高质量的图文内容，适用于多个领域和应用，如如何问答、故事告诉、图文重新编写、网页/海报生成等任务。此外，我们还验证了我们提议的 LMM 评估技术的有效性，通过人工评估。我们希望我们的提议框架、标准和 LMM 评估能够为开放领域交叠图文生成任务做出贡献。
</details></li>
</ul>
<hr>
<h2 id="ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models"><a href="#ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models" class="headerlink" title="ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models"></a>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07702">http://arxiv.org/abs/2310.07702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingqinghe/scalecrafter">https://github.com/yingqinghe/scalecrafter</a></li>
<li>paper_authors: Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan</li>
<li>for: 本研究探索了使用预训练的扩散模型在更高的分辨率上生成图像，并且图像的比例可以自由调整。</li>
<li>methods: 我们提出了一种简单 yet effective的重定义，以及散布 convolution 和噪声抑制的自由导航，以实现ultra-high-resolution图像生成（例如4096 x 4096）。</li>
<li>results: 我们的方法可以减少对象重复问题，并且在高分辨率图像生成中达到状态艺术的性能，特别是在文本细节方面。我们的方法不需要任何训练或优化。<details>
<summary>Abstract</summary>
In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了使用预训练的扩散模型生成图像的能力，以高于训练图像分辨率为目标。此外，生成图像的形态应该是任意的。在直接使用1024x1024分辨率的扩散模型进行图像生成时，我们发现了固定的对象重复和不合理的对象结构问题。现有的高分辨率生成方法，如注意力基本的和共同扩散方法，无法好解这些问题。我们从新的视角出发，检查扩散模型中的结构组件，并确定了卷积核心的有限见距问题是关键原因。基于这一关键观察，我们提出了一种简单 yet有效的重定向法，可以在推理过程中动态调整卷积核心的见距。我们还提出了分散卷积和无噪抑制器-自由导向，这些方法可以实现ultra-高分辨率图像生成（例如4096x4096）。值得注意的是，我们的方法不需要任何训练或优化。广泛的实验表明，我们的方法可以很好地解决对象重复问题，并在高分辨率图像生成中达到领先性表现，特别是在文本细节方面。我们的工作还表明，一个预训练的扩散模型可以直接用于高分辨率视觉生成，无需进一步调整，这可能为未来的超高分辨率图像和视频生成做出了意见。
</details></li>
</ul>
<hr>
<h2 id="ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation"><a href="#ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation" class="headerlink" title="ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation"></a>ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07697">http://arxiv.org/abs/2310.07697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao</li>
<li>for: 本研究的目的是提出一种没有需要训练的文本到视频生成方法，通过利用现有的文本到图像生成方法（如稳定扩散）来生成基于提供的条件、视频和输入文本的真实动态视频。</li>
<li>methods: 本研究使用了一种名为 ConditionVideo 的方法，该方法利用了 off-the-shelf 文本到图像生成方法（如稳定扩散），并将动态运动表示分解为 condition-guided 和景象运动组成部分。为了提高时间准确性，我们引入了罕见的双向空间时间注意力（sBiST-Attn）。在 Temporal 领域中，我们还提出了一种三维控制网络（3D control network），以强化 conditional 生成的准确性。</li>
<li>results: 对于 frame consistency、clip score 和 conditional accuracy 等评价指标，ConditionVideo 方法表现出色，超越了其他比较的方法。<details>
<summary>Abstract</summary>
Recent works have successfully extended large-scale text-to-image models to the video domain, producing promising results but at a high computational cost and requiring a large amount of video data. In this work, we introduce ConditionVideo, a training-free approach to text-to-video generation based on the provided condition, video, and input text, by leveraging the power of off-the-shelf text-to-image generation methods (e.g., Stable Diffusion). ConditionVideo generates realistic dynamic videos from random noise or given scene videos. Our method explicitly disentangles the motion representation into condition-guided and scenery motion components. To this end, the ConditionVideo model is designed with a UNet branch and a control branch. To improve temporal coherence, we introduce sparse bi-directional spatial-temporal attention (sBiST-Attn). The 3D control network extends the conventional 2D controlnet model, aiming to strengthen conditional generation accuracy by additionally leveraging the bi-directional frames in the temporal domain. Our method exhibits superior performance in terms of frame consistency, clip score, and conditional accuracy, outperforming other compared methods.
</details>
<details>
<summary>摘要</summary>
最近的研究已经成功地扩展了大规模文本到图像模型到视频领域，但是 computational cost 高和需要大量的视频数据。在这种工作中，我们介绍 ConditionVideo，一种不需要训练的文本到视频生成方法，通过利用现有的文本到图像生成方法（例如 Stable Diffusion），从随机噪声或给定场景视频中生成真实的动态视频。我们的方法明确分解了运动表示为受条件导向和景观运动组成部分。为此，ConditionVideo 模型采用 UNet 支持和控制支持。为了改善时间准确性，我们引入稀疏双向空间时间注意力（sBiST-Attn）。3D 控制网络延伸了传统的 2D 控制网络模型，以更加强化 conditional 生成精度，通过同时利用时间域的双向帧。我们的方法在 Frame Consistency、Clip Score 和 conditional accuracy 等指标上表现出色，超越其他比较的方法。
</details></li>
</ul>
<hr>
<h2 id="Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole"><a href="#Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole" class="headerlink" title="Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole"></a>Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07687">http://arxiv.org/abs/2310.07687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviad Levis, Andrew A. Chael, Katherine L. Bouman, Maciek Wielgus, Pratul P. Srinivasan</li>
<li>for: 这个研究的目的是解释黑洞吸收过程中高能射线、红外和广播信号的产生机制。</li>
<li>methods: 这个研究使用了一种新的人工智能方法（ neural 3D representation）和一种 gravitational model for black holes 来解决高度不定的Tomography问题，并通过对数据进行physically motivated choices来确定结果的稳定性。</li>
<li>results: 研究发现了一个位于黑洞事 horizon 处的6倍距离的彩虹短脊，并发现这些短脊在顺时针方向进行旋转，这与之前的EHT和GRAVITY合作的研究结果一致。<details>
<summary>Abstract</summary>
The interaction between the supermassive black hole at the center of the Milky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces high energy flares seen in X-ray, infrared and radio. One mechanism for observed flares is the formation of compact bright regions that appear within the accretion disk and close to the event horizon. Understanding these flares can provide a window into black hole accretion processes. Although sophisticated simulations predict the formation of these flares, their structure has yet to be recovered by observations. Here we show the first three-dimensional (3D) reconstruction of an emission flare in orbit recovered from ALMA light curves observed on April 11, 2017. Our recovery results show compact bright regions at a distance of roughly 6 times the event horizon. Moreover, our recovery suggests a clockwise rotation in a low-inclination orbital plane, a result consistent with prior studies by EHT and GRAVITY collaborations. To recover this emission structure we solve a highly ill-posed tomography problem by integrating a neural 3D representation (an emergent artificial intelligence approach for 3D reconstruction) with a gravitational model for black holes. Although the recovered 3D structure is subject, and sometimes sensitive, to the model assumptions, under physically motivated choices we find that our results are stable and our approach is successful on simulated data. We anticipate that in the future, this approach could be used to analyze a richer collection of time-series data that could shed light on the mechanisms governing black hole and plasma dynamics.
</details>
<details>
<summary>摘要</summary>
黑洞中心的聚合物质黑洞，如ours sagittarius A*，与其吸收盘发生交互作用， occasional produce high energy flares visible in X-ray, infrared and radio。一种机制是在吸收盘中形成compact bright regions，靠近事件 horizons。理解这些闪光可以提供黑洞吸收过程的窗口。 although sophisticated simulations predict the formation of these flares, their structure has yet to be recovered by observations. here we show the first three-dimensional (3D) reconstruction of an emission flare in orbit recovered from ALMA light curves observed on April 11, 2017. our recovery results show compact bright regions at a distance of roughly 6 times the event horizon. Moreover, our recovery suggests a clockwise rotation in a low-inclination orbital plane, a result consistent with prior studies by EHT and GRAVITY collaborations. to recover this emission structure, we solve a highly ill-posed tomography problem by integrating a neural 3D representation (an emergent artificial intelligence approach for 3D reconstruction) with a gravitational model for black holes. although the recovered 3D structure is subject, and sometimes sensitive, to the model assumptions, under physically motivated choices we find that our results are stable and our approach is successful on simulated data. we anticipate that in the future, this approach could be used to analyze a richer collection of time-series data that could shed light on the mechanisms governing black hole and plasma dynamics.
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images"><a href="#Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images" class="headerlink" title="Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images"></a>Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07682">http://arxiv.org/abs/2310.07682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe</li>
<li>for:  This paper aims to develop a pre-screening algorithm to predict MET overexpression in non-small cell lung cancer (NSCLC) using routinely available digitized hematoxylin and eosin (H&amp;E)-stained slides.</li>
<li>methods: The authors leveraged a large database of matched H&amp;E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&amp;E images.</li>
<li>results: The model demonstrated an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set, suggesting that H&amp;E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这项研究目的是开发一种预选择算法，以便在非小细胞肺癌（NSCLC）患者中预测MET蛋白过表达，使用常见的数字化颜色染色（H&amp;E）染色片。</li>
<li>methods: 作者们利用了大量匹配的H&amp;E染色片和RNA表达数据库，以训练一种弱监督模型，直接从H&amp;E图像中预测MET蛋白过表达。</li>
<li>results: 模型在独立的检查集上达到了ROC-AUC值为0.70（95%Interval：0.66-0.74），性能特征稳定地适应不同的患者临床变量，并在测试集上具有鲁棒性， suggesting that H&amp;E-based predictive models可能有用于优先检测MET蛋白或MET基因表达状态的检查。<details>
<summary>Abstract</summary>
MET protein overexpression is a targetable event in non-small cell lung cancer (NSCLC) and is the subject of active drug development. Challenges in identifying patients for these therapies include lack of access to validated testing, such as standardized immunohistochemistry (IHC) assessment, and consumption of valuable tissue for a single gene/protein assay. Development of pre-screening algorithms using routinely available digitized hematoxylin and eosin (H&E)-stained slides to predict MET overexpression could promote testing for those who will benefit most. While assessment of MET expression using IHC is currently not routinely performed in NSCLC, next-generation sequencing is common and in some cases includes RNA expression panel testing. In this work, we leveraged a large database of matched H&E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&E images. This model was evaluated on an independent holdout test set of 300 over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set. These results suggest that H&E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.
</details>
<details>
<summary>摘要</summary>
MET蛋白过表达是非小细胞肺癌（NSCLC）中可Targetable的事件，现在有多种药物开发在进行。问题在于确定需要这些治疗的患者，包括无法获得有效验证的测试，如标准化的免疫染色（IHC）评估，以及用于单个蛋白质测试的资源浪费。为了解决这些问题，我们开发了一种使用 Routinely 可用的数字化HE染色（H&E）干ovat的预creening算法，以预测MET蛋白过表达。这种算法可以用来预测MET蛋白过表达，不需要直接测试IHC。我们使用了一个大量的匹配HE染色和RNA表达数据库来训练一种弱Supervised模型，以直接从H&E图像中预测MET蛋白过表达。这个模型在一个独立的卷积测试集上进行了评估，其ROC-AUC为0.70（95%信息Interval：0.66-0.74），性能特征稳定，不同患者临床变量下的性能也很稳定，并且对于synthetic noise的测试集表现强势。这些结果表明HE染色基于的预测模型可能可以有用地优先检测MET蛋白过表达或MET基因表达状况。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns"><a href="#Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns" class="headerlink" title="Accelerating Vision Transformers Based on Heterogeneous Attention Patterns"></a>Accelerating Vision Transformers Based on Heterogeneous Attention Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07664">http://arxiv.org/abs/2310.07664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deli Yu, Teng Xi, Jianwei Li, Baopu Li, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang</li>
<li>for: 本文旨在提高视觉变换器（ViT）的运行速度，以便在计算机视觉领域应用。</li>
<li>methods: 该文提出了一个整合压缩管道，包括动态指导静态自注意（DGSSA）和全球聚合峰（GLAD）两部分。DGSSA方法通过继承自减少动态自注意矩阵的信息来提高ViT的特征表示能力。GLAD方法通过全局聚合来减少 later layer的token数量，从而提高运行速度。</li>
<li>results: 实验结果表明，整合压缩管道可以提高ViT的运行速度，相比DeiT，它的运行速度可以提高121%。这也超过了所有State-of-the-art（SOTA）方法。<details>
<summary>Abstract</summary>
Recently, Vision Transformers (ViTs) have attracted a lot of attention in the field of computer vision. Generally, the powerful representative capacity of ViTs mainly benefits from the self-attention mechanism, which has a high computation complexity. To accelerate ViTs, we propose an integrated compression pipeline based on observed heterogeneous attention patterns across layers. On one hand, different images share more similar attention patterns in early layers than later layers, indicating that the dynamic query-by-key self-attention matrix may be replaced with a static self-attention matrix in early layers. Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs. On the other hand, the attention maps have more low-rank patterns, which reflect token redundancy, in later layers than early layers. In a view of linear dimension reduction, we further propose a method of global aggregation pyramid (GLAD) to reduce the number of tokens in later layers of ViTs, such as Deit. Experimentally, the integrated compression pipeline of DGSSA and GLAD can accelerate up to 121% run-time throughput compared with DeiT, which surpasses all SOTA approaches.
</details>
<details>
<summary>摘要</summary>
最近，计算机视觉领域内的视觉变换器（ViT）吸引了很多关注。通常情况下，ViT的强大代表性能归功于自注意机制，但自注意机制的计算复杂度较高。为了加速ViT，我们提议一种集成压缩管道，基于层次级别的观察到的不同类型的注意模式。在一个手中，不同的图像在早期层次比较更加相似的注意模式，这表明可以将动态Query-by-key自注意矩阵替换为静态自注意矩阵。然后，我们提议一种动态引导静态自注意（DGSSA）方法，其中矩阵继承自动置换的自注意信息，以提高ViT的特征表示能力。在另一个手中，注意图在后期层次比较多的低级别模式，这表明可以通过线性维度减少法（GLAD）减少ViT的后期层次的TOKEN数量，如Deit。实验结果表明，我们的集成压缩管道可以提高ViT的运行时间吞吐量达121%，比SOTA方法更高。
</details></li>
</ul>
<hr>
<h2 id="Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision"><a href="#Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision" class="headerlink" title="Deep Video Inpainting Guided by Audio-Visual Self-Supervision"></a>Deep Video Inpainting Guided by Audio-Visual Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07663">http://arxiv.org/abs/2310.07663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting">https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting</a></li>
<li>paper_authors: Kyuyeon Kim, Junsik Jung, Woo Jae Kim, Sung-Eui Yoon</li>
<li>for: 提高视频填充质量</li>
<li>methods: 使用深度学习模型模仿人类的听视感知，并通过两个新的损失函数传递听视相关知识到视频填充网络</li>
<li>results: 实验结果表明，我们的提议方法可以恢复更广泛的视频场景，并在听视对象部分遮盖的情况下特别有效<details>
<summary>Abstract</summary>
Humans can easily imagine a scene from auditory information based on their prior knowledge of audio-visual events. In this paper, we mimic this innate human ability in deep learning models to improve the quality of video inpainting. To implement the prior knowledge, we first train the audio-visual network, which learns the correspondence between auditory and visual information. Then, the audio-visual network is employed as a guider that conveys the prior knowledge of audio-visual correspondence to the video inpainting network. This prior knowledge is transferred through our proposed two novel losses: audio-visual attention loss and audio-visual pseudo-class consistency loss. These two losses further improve the performance of the video inpainting by encouraging the inpainting result to have a high correspondence to its synchronized audio. Experimental results demonstrate that our proposed method can restore a wider domain of video scenes and is particularly effective when the sounding object in the scene is partially blinded.
</details>
<details>
<summary>摘要</summary>
人类可以轻松地从听音信息中想象出场景，在这篇论文中，我们模拟了人类的这种 Innate 能力，用于改善视频填充的质量。首先，我们训练了一个听视网络，该网络学习听音和视觉信息之间的相关性。然后，我们使用这个听视网络作为引导器，将听视网络传递给视频填充网络，以便将优先知识传递给视频填充。我们提出了两个新的损失函数：听视注意力损失和听视假类一致损失。这两个损失函数可以进一步改善视频填充的性能，使得填充结果与同步的声音更加一致。实验结果表明，我们的提议方法可以恢复更广泛的视频场景，特别是在声音对象在场景中部分遮盲的情况下更加有效。
</details></li>
</ul>
<hr>
<h2 id="Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images"><a href="#Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images" class="headerlink" title="Context-Enhanced Detector For Building Detection From Remote Sensing Images"></a>Context-Enhanced Detector For Building Detection From Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07638">http://arxiv.org/abs/2310.07638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Huang, Mingming Zhang, Qingjie Liu, Wei Wang, Zhe Dong, Yunhong Wang</li>
<li>for: 本研究旨在提高遥感图像中建筑物检测精度，因为建筑物的多样性和场景复杂度导致检测困难。</li>
<li>methods: 我们提出了一种Context-Enhanced Detector（CEDet）方法，它使用三 stage cascade结构增强Contextual information的提取，以提高建筑物检测精度。特别是，我们引入了两个模块：Semantic Guided Contextual Mining（SGCM）模块和Instance Context Mining Module（ICMM）。SGCM模块通过多 scales的 Context aggregation和注意力机制来捕捉长距离相互作用，而ICMM模块通过构建空间关系图和实例特征归一化来捕捉实例水平关系。此外，我们引入了基于pseudo-mask的semantic segmentation loss来导引Contextual information提取。</li>
<li>results: 我们的方法在三个建筑物检测标准benchmark上达到了领先的性能：CNBuilding-9P、CNBuilding-23P和SpaceNet。<details>
<summary>Abstract</summary>
The field of building detection from remote sensing images has made significant progress, but faces challenges in achieving high-accuracy detection due to the diversity in building appearances and the complexity of vast scenes. To address these challenges, we propose a novel approach called Context-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade structure to enhance the extraction of contextual information and improve building detection accuracy. Specifically, we introduce two modules: the Semantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale contexts and incorporates an attention mechanism to capture long-range interactions, and the Instance Context Mining Module (ICMM), which captures instance-level relationship context by constructing a spatial relationship graph and aggregating instance features. Additionally, we introduce a semantic segmentation loss based on pseudo-masks to guide contextual information extraction. Our method achieves state-of-the-art performance on three building detection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.
</details>
<details>
<summary>摘要</summary>
场景检测从远程感知图像领域已经做出了重要进步，但仍面临高精度检测的挑战，主要是因为建筑物的多样性和场景的复杂性。为解决这些挑战，我们提出了一种新的方法 called Context-Enhanced Detector (CEDet)。我们的方法采用三个阶段卷积结构，以提高Contextual information的提取和建筑物检测精度。具体来说，我们引入了两个模块：卷积模块，用于聚合多尺度上下文，并使用注意机制来捕捉长距离相互作用；实例上下文挖掘模块，用于捕捉实例级别的关系上下文，通过构建空间关系图并聚合实例特征来完成。此外，我们还引入了基于pseudo-mask的semantic segmentation损失，以指导Contextual information的提取。我们的方法在三个建筑物检测标准测试 benchmark上达到了领先的性能水平，包括CNBuilding-9P、CNBuilding-23P和SpaceNet。
</details></li>
</ul>
<hr>
<h2 id="Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification"><a href="#Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification" class="headerlink" title="Attention-Map Augmentation for Hypercomplex Breast Cancer Classification"></a>Attention-Map Augmentation for Hypercomplex Breast Cancer Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07633">http://arxiv.org/abs/2310.07633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Filippo Betello, Federico Carmignani, Eleonora Grassucci, Danilo Comminiello</li>
<li>for: 这篇论文旨在提高乳腺癌早期诊断性能，使用深度学习技术。</li>
<li>methods: 本文提出一个框架，叫做参数化复杂注意地图（PHAM），以解决乳腺癌与良性肿瘤的区别问题。这个框架包括了一个增强步骤，使用注意地图来computing attention maps，然后将这些注意地图用于分类步骤，通过将原始乳腺癌图像和对应的注意地图融合为一个多维输入。在这个步骤中，使用参数化复杂神经网络（PHNN）进行乳腺癌分类。</li>
<li>results: 本文的方法比过往的注意力基于的网络和实值版本的方法表现更好，在乳腺癌和良性肿瘤的分类任务中均有着优秀的表现。<details>
<summary>Abstract</summary>
Breast cancer is the most widespread neoplasm among women and early detection of this disease is critical. Deep learning techniques have become of great interest to improve diagnostic performance. Nonetheless, discriminating between malignant and benign masses from whole mammograms remains challenging due to them being almost identical to an untrained eye and the region of interest (ROI) occupying a minuscule portion of the entire image. In this paper, we propose a framework, parameterized hypercomplex attention maps (PHAM), to overcome these problems. Specifically, we deploy an augmentation step based on computing attention maps. Then, the attention maps are used to condition the classification step by constructing a multi-dimensional input comprised of the original breast cancer image and the corresponding attention map. In this step, a parameterized hypercomplex neural network (PHNN) is employed to perform breast cancer classification. The framework offers two main advantages. First, attention maps provide critical information regarding the ROI and allow the neural model to concentrate on it. Second, the hypercomplex architecture has the ability to model local relations between input dimensions thanks to hypercomplex algebra rules, thus properly exploiting the information provided by the attention map. We demonstrate the efficacy of the proposed framework on both mammography images as well as histopathological ones, surpassing attention-based state-of-the-art networks and the real-valued counterpart of our method. The code of our work is available at https://github.com/elelo22/AttentionBCS.
</details>
<details>
<summary>摘要</summary>
乳癌是女性最常见的肿瘤，早期发现这种疾病非常重要。深度学习技术在提高诊断性能方面表现出了很大的兴趣。然而，从整个照片中分别识别癌变和正常组织仍然是一项极其困难的任务，因为它们在无经验的眼光下看起来几乎一样，而且诊断区域占整个照片的非常小。在这篇论文中，我们提出了一个框架，即参数化复杂注意地图（PHAM）。我们在这个框架中首先使用计算注意地图的步骤进行增强。然后，我们使用注意地图来 condition 分类步骤，通过构建一个多维输入，其包括原始乳癌图像和相应的注意地图。在这个步骤中，我们使用参数化复杂神经网络（PHNN）进行乳癌分类。这个框架具有两个主要优势。第一，注意地图提供了关键的诊断区域信息，使神经网络可以专注于它。第二，复杂架构可以通过复杂代数规则来模型输入维度之间的本地关系，因此能够正确地利用注意地图提供的信息。我们在照片和组织病学图像上进行了实验，超过了注意力基 estado del arte 网络和我们实际值对应的方法。我们的代码可以在 <https://github.com/elelo22/AttentionBCS> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Backdoors-in-Visual-Prompt-Learning"><a href="#Prompt-Backdoors-in-Visual-Prompt-Learning" class="headerlink" title="Prompt Backdoors in Visual Prompt Learning"></a>Prompt Backdoors in Visual Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07632">http://arxiv.org/abs/2310.07632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: 论文旨在探讨大规模预训练计算机视觉模型的精细调整是resource-limited用户不可能进行的。因此，视觉提示学习（VPL）已经成为一种有效和灵活的代替方案，通过Visual Prompt as a Service（VPPTaaS）提供者优化视觉提示，以便用户使用大规模预训练模型进行预测。然而，这种新的学习模式可能也会带来安全风险，当VPPTaaS提供者而不是提供正确的视觉提示时。本文通过探讨这种风险，提出了BadVisualPrompt，一种简单又有效的后门攻击。</li>
<li>methods: 我们提出了BadVisualPrompt，一种后门攻击，通过恶意修改CIFAR10训练数据来控制预测结果。我们发现，只需杀死5%的训练数据，可以达到99%的攻击成功率，同时只减少模型的准确率1.5%。此外，我们还发现了一种新的技术挑战，即后门触发器和视觉提示之间的交互，这种挑战不存在于传统的模型级别后门攻击中。</li>
<li>results: 我们发现，现有的七种后门防御机制都无法有效地防止BadVisualPrompt。这些防御机制包括模型级别、提示级别和输入级别的防御。在总的来说，这些防御机制都是无效的或实际上不可行。这表明，VPL受到了严重的安全攻击。<details>
<summary>Abstract</summary>
Fine-tuning large pre-trained computer vision models is infeasible for resource-limited users. Visual prompt learning (VPL) has thus emerged to provide an efficient and flexible alternative to model fine-tuning through Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider optimizes a visual prompt given downstream data, and downstream users can use this prompt together with the large pre-trained model for prediction. However, this new learning paradigm may also pose security risks when the VPPTaaS provider instead provides a malicious visual prompt. In this paper, we take the first step to explore such risks through the lens of backdoor attacks. Specifically, we propose BadVisualPrompt, a simple yet effective backdoor attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to above $99\%$ attack success rates with only negligible model accuracy drop by $1.5\%$. In particular, we identify and then address a new technical challenge related to interactions between the backdoor trigger and visual prompt, which does not exist in conventional, model-level backdoors. Moreover, we provide in-depth analyses of seven backdoor defenses from model, prompt, and input levels. Overall, all these defenses are either ineffective or impractical to mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
</details>
<details>
<summary>摘要</summary>
大型预训计算机视觉模型的细调是资源有限的用户无法进行。因此，视觉提示学习（VPL）已经出现了，作为一种高效和灵活的代替方案。具体来说，VPL提供者将可见提示给下游数据进行优化，然后下游用户可以使用这个提示和大型预训模型进行预测。然而，这个新的学习模式也可能存在安全风险，当VPL提供者而不是提供有利可图的可见提示。在这篇论文中，我们开始探讨这些风险，通过视觉提示的透传攻击来强调。具体来说，我们提出了BadVisualPrompt，一种简单 yet有效的可见提示攻击。例如，对CIFAR10训练数据进行毒素攻击，可以达到99%的攻击成功率，而且只带来模型准确率下降1.5%。在特定情况下，我们发现和解决了可见提示攻击和后门触发的新技术挑战，这不同于传统的模型级别后门攻击。此外，我们还对七种后门防御方法进行了深入分析，包括模型、提示和输入级别的防御方法。总之，这些防御方法都是无效或不实际的，这表明VPL具有极高的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autononous-Driving"><a href="#Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autononous-Driving" class="headerlink" title="Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous Driving"></a>Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07602">http://arxiv.org/abs/2310.07602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adept-thu/dual-radar">https://github.com/adept-thu/dual-radar</a></li>
<li>paper_authors: Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, Jun Li</li>
<li>for: 本研究旨在提供一个大规模多模式数据集，用于研究基于4D radar的自动驾驶环境感知。</li>
<li>methods: 本研究使用了两种不同的4D radar，并对其进行了大规模的同步采集和精心标注。</li>
<li>results: 研究发现，使用不同的4D radar可以提高自动驾驶系统的环境感知能力，但是不同的雷达类型在同一场景中的性能有很大差异。<details>
<summary>Abstract</summary>
Radar has stronger adaptability in adverse scenarios for autonomous driving environmental perception compared to widely adopted cameras and LiDARs. Compared with commonly used 3D radars, latest 4D radars have precise vertical resolution and higher point cloud density, making it a highly promising sensor for autonomous driving in complex environmental perception. However, due to the much higher noise than LiDAR, manufacturers choose different filtering strategies, resulting in an inverse ratio between noise level and point cloud density. There is still a lack of comparative analysis on which method is beneficial for deep learning-based perception algorithms in autonomous driving. One of the main reasons is that current datasets only adopt one type of 4D radar, making it difficult to compare different 4D radars in the same scene. Therefore, in this paper, we introduce a novel large-scale multi-modal dataset featuring, for the first time, two types of 4D radars captured simultaneously. This dataset enables further research into effective 4D radar perception algorithms.Our dataset consists of 151 consecutive series, most of which last 20 seconds and contain 10,007 meticulously synchronized and annotated frames. Moreover, our dataset captures a variety of challenging driving scenarios, including many road conditions, weather conditions, nighttime and daytime with different lighting intensities and periods. Our dataset annotates consecutive frames, which can be applied to 3D object detection and tracking, and also supports the study of multi-modal tasks. We experimentally validate our dataset, providing valuable results for studying different types of 4D radars. This dataset is released on https://github.com/adept-thu/Dual-Radar.
</details>
<details>
<summary>摘要</summary>
雷达在自动驾驶环境感知中具有更强的适应性，比广泛使用的相机和激光雷达更为有利。相比通常使用的3D雷达，最新的4D雷达具有高分辨率和更高的点云密度，使其成为自动驾驶复杂环境感知中非常有前途的感知器。然而，由于雷达噪声比激光雷达更高，制造商们采用不同的过滤策略，导致点云密度与噪声水平存在反比关系。到目前为止，没有对不同类型4D雷达的比较分析，对深度学习基于感知算法的影响进行了系统的研究。主要原因是目前的数据集只采用一种类型的4D雷达，使其不能在同一场景中比较不同的4D雷达。因此，在本文中，我们提出了一个新的大规模多模式数据集，该数据集首次同时采集了两种类型的4D雷达。这个数据集启用了进一步研究4D雷达感知算法。我们的数据集包括151个连续系列，大多数系列持续20秒钟，共包含10,007个精心同步和注释的帧。此外，我们的数据集捕捉了各种挑战性的驾驶场景，包括多种路面条件、天气条件、夜晚和白天的不同照明强度和时间段。我们的数据集连续注释帧，可以应用于3D物体检测和跟踪，也支持多模式任务的研究。我们实验 validate了我们的数据集，提供了价值的研究4D雷达的结果。这个数据集在https://github.com/adept-thu/Dual-Radar上发布。
</details></li>
</ul>
<hr>
<h2 id="PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks"><a href="#PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks" class="headerlink" title="PeP: a Point enhanced Painting method for unified point cloud tasks"></a>PeP: a Point enhanced Painting method for unified point cloud tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07591">http://arxiv.org/abs/2310.07591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Dong, Hang Ji, Xufeng Huang, Weikun Zhang, Xin Zhan, Junbo Chen</li>
<li>for: The paper is written for improving the performance of point cloud recognition by proposing a novel feature encoding module called PeP.</li>
<li>methods: The paper uses a refined point painting method and a language model (LM)-based point encoder to enhance the feature encoding mechanism.</li>
<li>results: The proposed PeP module achieves superior performance on both semantic segmentation and object detection tasks in both lidar and multi-modal settings, with advantages over existing methods.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本文是为提高点云识别性能，提出了一种新的特征编码模块 called PeP。</li>
<li>methods: 本文使用了改进的点云涂抹方法和语言模型（LM）基于的点云编码器，以增强特征编码机制。</li>
<li>results: 提出的 PeP 模块在点云和多modal设置下的 semantic segmentation 和物体检测任务中具有优异性能，与现有方法相比有所提升。<details>
<summary>Abstract</summary>
Point encoder is of vital importance for point cloud recognition. As the very beginning step of whole model pipeline, adding features from diverse sources and providing stronger feature encoding mechanism would provide better input for downstream modules. In our work, we proposed a novel PeP module to tackle above issue. PeP contains two main parts, a refined point painting method and a LM-based point encoder. Experiments results on the nuScenes and KITTI datasets validate the superior performance of our PeP. The advantages leads to strong performance on both semantic segmentation and object detection, in both lidar and multi-modal settings. Notably, our PeP module is model agnostic and plug-and-play. Our code will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>点编码器对点云识别是非常重要的。作为整个模型管道的起始步骤，从多种来源添加特征并提供更强的特征编码机制，可以提供更好的输入 dla下游模块。在我们的工作中，我们提出了一种新的PeP模块来解决上述问题。PeP包括两个主要部分：一种精度加工的点涂抹方法和一种LM基于的点编码器。在nuScenes和KITTI数据集上进行实验，我们证明了我们的PeP模块在 semantic segmentation和物体检测方面具有优秀的表现，并且在 lidar 和多Modal 环境中也有出色的表现。值得注意的是，我们的PeP模块是模型不依赖的和插件化的。我们的代码将很快地公开。
</details></li>
</ul>
<hr>
<h2 id="A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection"><a href="#A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection" class="headerlink" title="A Discrepancy Aware Framework for Robust Anomaly Detection"></a>A Discrepancy Aware Framework for Robust Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07585">http://arxiv.org/abs/2310.07585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caiyuxuan1120/daf">https://github.com/caiyuxuan1120/daf</a></li>
<li>paper_authors: Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, Xiang Bai<br>for:This paper focuses on the robustness of self-supervised learning models for defect detection in artificial intelligence.methods:The paper presents a Discrepancy Aware Framework (DAF) that leverages an appearance-agnostic cue to guide the decoder in identifying defects, alleviating the reliance on synthetic appearance. The method employs a teacher-student network trained based on synthesized outliers to compute the discrepancy map.results:The paper shows that the proposed method outperforms existing methods by a large margin under simple synthesis strategies, and achieves state-of-the-art localization performance. Extensive experiments on two challenging datasets prove the robustness of the method.<details>
<summary>Abstract</summary>
Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this paper, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a Discrepancy Aware Framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. Code is available at: https://github.com/caiyuxuan1120/DAF.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>检测缺陷是人工智能研究领域中的一个关键问题。最近，基于 sintetic 数据的自我指导学习已经展示了很大的潜力在这项任务上。虽然有很多复杂的生成策略存在，但是对于不同的策略的研究还很少。在这篇论文中，我们将关注这个问题，并发现现有方法具有高度敏感性。为了解决这个问题，我们提出了一种缺陷意识框架（DAF），它在不同的缺陷检测benchmark上显示了稳定的表现，并且可以采用简单和便宜的策略。我们认为现有的自我指导方法在使用 sintetic 数据时存在高度的敏感性，因为它们在解码时强调 sintetic 数据的视觉特征。与此相反，我们的方法利用一种视觉无关的cue来导引解码器，以避免对 sintetic 数据的依赖。为此，我们采用了一种教师生成器，该生成器基于生成的异常数据来计算缺陷地图。我们在两个复杂的dataset上进行了广泛的实验，并证明了我们的方法的稳定性和高效性。在简单的生成策略下，它超过了现有方法，并且也实现了当前的最佳地方化性能。代码可以在 GitHub 上找到：https://github.com/caiyuxuan1120/DAF。
</details></li>
</ul>
<hr>
<h2 id="Centrality-of-the-Fingerprint-Core-Location"><a href="#Centrality-of-the-Fingerprint-Core-Location" class="headerlink" title="Centrality of the Fingerprint Core Location"></a>Centrality of the Fingerprint Core Location</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07584">http://arxiv.org/abs/2310.07584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurenz Ruzicka, Bernhard Strobl, Bernhard Kohn, Clemens Heitzinger</li>
<li>for: 这个研究旨在investigating the empirical distribution of the fingerprint core over a large, combined dataset of rolled and plain fingerprint recordings.</li>
<li>methods: 该研究使用了一种多步骤的方法来描述rolled fingerprint recordings的分布，包括Anderson-Darling正态性测试、Bayesian Information Criterion和Generalized Monte Carlogoodness-of-fit过程。</li>
<li>results: 研究发现，rolled fingerprint recordings的core deviates from the fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the finger. Additionally, the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Finally, the non-central Fischer distribution best describes the cores’ horizontal positions.<details>
<summary>Abstract</summary>
Fingerprints have long been recognized as a unique and reliable means of personal identification. Central to the analysis and enhancement of fingerprints is the concept of the fingerprint core. Although the location of the core is used in many applications, to the best of our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled, as well as plain fingerprint recordings. We identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. After correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the finger. Additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. The process consists of an Anderson-Darling normality test, the Bayesian Information Criterion to reduce the number of possible candidate distributions and finally a Generalized Monte Carlo goodness-of-fit procedure to find the best fitting distribution. We find the non-central Fischer distribution best describes the cores' horizontal positions. Finally, we investigate the correlation between mean core position offset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details>
<details>
<summary>摘要</summary>
fingerprints 已经被认为是个人身份识别的唯一和可靠方法。 fingerprint 核心是分析和提高 fingerprint 的中心概念。 although the location of the core is used in many applications, to the best of our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled, as well as plain fingerprint recordings. we identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. after correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% ± 5.2% to 7.6% ± 6.9%, depending on the finger. additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. the process consists of an anderson-darling normality test, the bayesian information criterion to reduce the number of possible candidate distributions and finally a generalized monte carlo goodness-of-fit procedure to find the best fitting distribution. we find the non-central fischer distribution best describes the cores' horizontal positions. finally, we investigate the correlation between mean core position offset and the nfiq 2 score and find that the nfiq 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details></li>
</ul>
<hr>
<h2 id="Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation"><a href="#Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation" class="headerlink" title="Relational Prior Knowledge Graphs for Detection and Instance Segmentation"></a>Relational Prior Knowledge Graphs for Detection and Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07573">http://arxiv.org/abs/2310.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald</li>
<li>for: 本研究探讨了使用物体之间关系来进行物体检测和实例分割的可效性。</li>
<li>methods: 提议一种基于关系优先的特征增强模型（RP-FEM），利用场景图获取初始提案特征，并同时学习关系上下文模型以提高物体检测和实例分割性能。</li>
<li>results: 实验结果表明，在COCO dataset上，通过使用场景图和关系优先来增强对象提案特征，可以提高物体检测和实例分割性能，并且可以降低不可能的类别预测和重复预测，与基础模型相比具有改进。<details>
<summary>Abstract</summary>
Humans have a remarkable ability to perceive and reason about the world around them by understanding the relationships between objects. In this paper, we investigate the effectiveness of using such relationships for object detection and instance segmentation. To this end, we propose a Relational Prior-based Feature Enhancement Model (RP-FEM), a graph transformer that enhances object proposal features using relational priors. The proposed architecture operates on top of scene graphs obtained from initial proposals and aims to concurrently learn relational context modeling for object detection and instance segmentation. Experimental evaluations on COCO show that the utilization of scene graphs, augmented with relational priors, offer benefits for object detection and instance segmentation. RP-FEM demonstrates its capacity to suppress improbable class predictions within the image while also preventing the model from generating duplicate predictions, leading to improvements over the baseline model on which it is built.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的能力，能够理解和掌握周围环境中的物体关系。在这篇论文中，我们研究了使用这些关系来进行物体检测和实例分割。为此，我们提出了一种基于关系优先的特征增强模型（RP-FEM），这是一种图变换器，它在从初始提案中获得的场景图上进行增强物体提案特征。我们的建议的架构同时学习了场景图中的关系上下文模型，以便同时进行物体检测和实例分割。在COCO数据集上进行实验评估表明，通过使用场景图和关系优先，可以提高物体检测和实例分割的性能。RP-FEM可以降低图像中不可能的类别预测，同时避免模型生成重复预测，从而超越基础模型。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery"><a href="#Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery" class="headerlink" title="Impact of Label Types on Training SWIN Models with Overhead Imagery"></a>Impact of Label Types on Training SWIN Models with Overhead Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07572">http://arxiv.org/abs/2310.07572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Ford, Kenneth Hutchison, Nicholas Felts, Benjamin Cheng, Jesse Lew, Kyle Jackson</li>
<li>for: 这个研究探讨了对于模型训练和性能的数据集设计影响，以减少调取遥测和顶部标签数据的成本。</li>
<li>methods: 这个研究使用了对焦盒和分类标签进行训练shifted window transformers，其中分类标签更加昂贵。</li>
<li>results: 研究发现，将模型训练只使用目标像素（通过分类标签提取）不会提高分类任务的性能，似乎是把背景像素 mistakenly included in the evaluation set with target pixels。对object detection模型进行训练，使用任一标签类型都会得到相等的性能。 bounding boxes 足够 для不需要更多复杂的标签的任务。<details>
<summary>Abstract</summary>
Understanding the impact of data set design on model training and performance can help alleviate the costs associated with generating remote sensing and overhead labeled data. This work examined the impact of training shifted window transformers using bounding boxes and segmentation labels, where the latter are more expensive to produce. We examined classification tasks by comparing models trained with both target and backgrounds against models trained with only target pixels, extracted by segmentation labels. For object detection models, we compared performance using either label type when training. We found that the models trained on only target pixels do not show performance improvement for classification tasks, appearing to conflate background pixels in the evaluation set with target pixels. For object detection, we found that models trained with either label type showed equivalent performance across testing. We found that bounding boxes appeared to be sufficient for tasks that did not require more complex labels, such as object segmentation. Continuing work to determine consistency of this result across data types and model architectures could potentially result in substantial savings in generating remote sensing data sets for deep learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape"><a href="#Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape" class="headerlink" title="Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape"></a>Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07555">http://arxiv.org/abs/2310.07555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wen, Tianqin Li, Tai Sing Lee</li>
<li>for: 本研究旨在评估深度学习模型对形状的敏感性，并提出一种新的测试工具箱（Distorted Shape Testbench，DiST）来评估模型对全局形状的敏感性。</li>
<li>methods: 本研究使用了一组包含2400张ImageNet-1K原始图像的图像集，每张图像都有两个拟合原始图像的全局形状的图像，用于评估模型对全局形状的敏感性。</li>
<li>results: 研究发现，传统的形状偏好评估方法不能准确反映模型的表现，而新的DiST测试工具箱可以准确评估模型对全局形状的敏感性，并且训练使用DiST图像可以bridge人类和现有SOTA模型之间的性能差距，同时保持模型在标准图像分类任务中的准确率。<details>
<summary>Abstract</summary>
Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape for object recognition. The current benchmark for evaluating a model's shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of shape sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local shapes. We provide a Distorted Shape Testbench (DiST) as an alternative measurement of global shape sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image distorted while preserving its texture via the texture synthesis program. We found that (1) models that performed well on the previous shape bias evaluation do not fare well in the proposed DiST; (2) the widely adopted ViT models do not show significant advantages over Convolutional Neural Networks (CNNs) on this benchmark despite that ViTs rank higher on the previous shape bias tests. (3) training with DiST images bridges the significant gap between human and existing SOTA models' performance while preserving the models' accuracy on standard image classification tasks; training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance both the global and local shape sensitivity of the network. Our code will be host at: https://github.com/leelabcnbc/DiST
</details>
<details>
<summary>摘要</summary>
深度学习模型通常会表现出强烈的文化偏见，而人类则更加重视全局形态的认知。现有的标准测试方法是使用style transfer技术来评估模型的形态偏见，假设模型对style transfer攻击的抵抗度与其全局形态敏感度之间存在相关性。在这项工作中，我们发现了一点：使用style transfer图像进行训练后，模型会忽略样式，但是其形态偏见主要来自本地形态。为了衡量全局形态敏感度，我们提出了一种Distorted Shape Testbench（DiST）。我们的测试包括2400个ImageNet-1K原始图像，每个图像都有两个global shape的扭曲图像，通过Texture Synthesis程序保持图像的文化。我们发现以下结论：1. 在我们提出的DiST测试中，高于前一个形态偏见测试的模型表现不佳。2. 广泛采用的ViT模型与传统Convolutional Neural Networks（CNNs）在这一benchmark上并没有显著的优势，尽管ViTs在前一个形态偏见测试中排名更高。3. 使用DiST图像进行训练可以bridges模型与人类的表现差距，同时保持模型在标准图像分类任务上的准确率。使用DiST图像和style transfer图像进行训练是补偿的，可以同时增强模型的全局形态敏感度和本地形态敏感度。我们的代码将被托管在GitHub上：https://github.com/leelabcnbc/DiST。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning"><a href="#Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning" class="headerlink" title="Attribute Localization and Revision Network for Zero-Shot Learning"></a>Attribute Localization and Revision Network for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07548">http://arxiv.org/abs/2310.07548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhe Xu, Suling Duan, Chenwei Tang, Zhenan He, Jiancheng Lv</li>
<li>for: 本研究旨在提出一种能够在无需训练数据的情况下进行预测的模型，该模型可以利用auxiliary semantic信息来识别未经训练的类别。</li>
<li>methods: 本研究使用了Attribute Localization Module (ALM)和Attribute Revision Module (ARM)两种模块来解决当前的zero-shot learning问题。ALM模块可以 capture both local and global features from image regions，而ARM模块可以 revise the ground-truth value of each attribute to compensate for performance degradation caused by ignoring intra-class variation。</li>
<li>results: 根据三个广泛使用的benchmark测试，本研究的模型在zero-shot prediction任务中表现出色，可以准确地预测未经训练的类别。<details>
<summary>Abstract</summary>
Zero-shot learning enables the model to recognize unseen categories with the aid of auxiliary semantic information such as attributes. Current works proposed to detect attributes from local image regions and align extracted features with class-level semantics. In this paper, we find that the choice between local and global features is not a zero-sum game, global features can also contribute to the understanding of attributes. In addition, aligning attribute features with class-level semantics ignores potential intra-class attribute variation. To mitigate these disadvantages, we present Attribute Localization and Revision Network in this paper. First, we design Attribute Localization Module (ALM) to capture both local and global features from image regions, a novel module called Scale Control Unit is incorporated to fuse global and local representations. Second, we propose Attribute Revision Module (ARM), which generates image-level semantics by revising the ground-truth value of each attribute, compensating for performance degradation caused by ignoring intra-class variation. Finally, the output of ALM will be aligned with revised semantics produced by ARM to achieve the training process. Comprehensive experimental results on three widely used benchmarks demonstrate the effectiveness of our model in the zero-shot prediction task.
</details>
<details>
<summary>摘要</summary>
zero-shot learning 允许模型识别未经见过的类别，通过auxiliary semantic information如特征。现有工作提议从本地图像区域检测特征并将提取的特征与类别水平 semantics 对应。在这篇论文中，我们发现选择本地和全局特征不是一个零和游戏，全局特征也可以帮助理解特征。此外，对于类别水平 semantics 的对应忽略了可能存在的内类特征变化。为了缓解这些缺点，我们在本文中提出了特征地方化和修订网络（Attribute Localization and Revision Network，简称ALRN）。首先，我们设计了特征地方化模块（Attribute Localization Module，简称ALM），用于从图像区域中捕捉本地和全局特征。其次，我们提出了特征修订模块（Attribute Revision Module，简称ARM），通过修订真实值来补偿因为忽略内类特征变化而导致的性能下降。最后，ALM 的输出与 ARM 生成的修订 semantics 进行对齐，以实现训练过程。我们在三个常用的 benchmark 上进行了广泛的实验，结果表明我们的模型在零shot 预测任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields"><a href="#S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields" class="headerlink" title="S4C: Self-Supervised Semantic Scene Completion with Neural Fields"></a>S4C: Self-Supervised Semantic Scene Completion with Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07522">http://arxiv.org/abs/2310.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers</li>
<li>for: 本研究旨在解决计算机视觉中的3DsemanticScene理解挑战，即从稀疏观察数据中jointly estimating dense geometry和semantic信息，以便自主规划和导航任意环境。</li>
<li>methods: 我们的方法基于自我监督学习，不需要3D真实数据，可以从单张图像和视频数据中学习Scene Semantic Consistency（SSC）。我们使用pseudo segmentation truth生成于市面上的图像分割网络，并使用rendering-based自我监督损失来训练我们的模型。</li>
<li>results: 我们的方法可以在不同的视角下Synthesize高精度的分割图像，并且在不同的环境下进行强大的泛化。与现有方法相比，我们的方法可以减少 annotated 数据的需求，并且可以在不同的环境下进行更好的一致性。<details>
<summary>Abstract</summary>
3D semantic scene understanding is a fundamental challenge in computer vision. It enables mobile agents to autonomously plan and navigate arbitrary environments. SSC formalizes this challenge as jointly estimating dense geometry and semantic information from sparse observations of a scene. Current methods for SSC are generally trained on 3D ground truth based on aggregated LiDAR scans. This process relies on special sensors and annotation by hand which are costly and do not scale well. To overcome this issue, our work presents the first self-supervised approach to SSC called S4C that does not rely on 3D ground truth data. Our proposed method can reconstruct a scene from a single image and only relies on videos and pseudo segmentation ground truth generated from off-the-shelf image segmentation network during training. Unlike existing methods, which use discrete voxel grids, we represent scenes as implicit semantic fields. This formulation allows querying any point within the camera frustum for occupancy and semantic class. Our architecture is trained through rendering-based self-supervised losses. Nonetheless, our method achieves performance close to fully supervised state-of-the-art methods. Additionally, our method demonstrates strong generalization capabilities and can synthesize accurate segmentation maps for far away viewpoints.
</details>
<details>
<summary>摘要</summary>
三维semantic场景理解是计算机视觉的基本挑战。它使移动代理能够自主规划和探索不确定环境。SSC formalizes this challenge as同时估算环境的厚度和Semantic信息从笔记scans中的稀疏观察数据。现有的方法通常通过3D实际数据进行训练，这个过程依赖特殊的感器和手动标注，这些成本高并不扩展好。为了解决这个问题，我们提出了首个不需要3D实际数据的自动学习方法called S4C。我们的提议的方法可以从单个图像中重建场景，只需要视频和 pseudo segmentation标注来进行训练。与现有方法不同，我们使用 implicit semantic fields来表示场景。这种表示方式允许在摄像头封闭中任意点进行存储和Semantic类别的查询。我们的架构通过渲染基于自我超级vised损失进行训练。尽管如此，我们的方法可以与完全超级vised方法的性能相似。此外，我们的方法还能够强大地泛化，可以生成正确的分割图像 для远距离视角。
</details></li>
</ul>
<hr>
<h2 id="CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing"><a href="#CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing" class="headerlink" title="CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing"></a>CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07517">http://arxiv.org/abs/2310.07517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaru Chen, Ruohao Guo, Xubo Liu, Peipei Wu, Guangyao Li, Zhenbo Li, Wenwu Wang</li>
<li>for: 这paper是为了提高视频分割 tasks的性能，特别是在使用弱标签时。</li>
<li>methods: 该paper提出了一种名为CM-PIE的新方法，它利用段级注意力模块来学习细化特征，同时通过交叉模态协同归一化块来强化间modal交互。</li>
<li>results: 实验结果显示，CM-PIE方法在Look, Listen, and Parse数据集上的分割性能比其他方法更高。<details>
<summary>Abstract</summary>
Audio-visual video parsing is the task of categorizing a video at the segment level with weak labels, and predicting them as audible or visible events. Recent methods for this task leverage the attention mechanism to capture the semantic correlations among the whole video across the audio-visual modalities. However, these approaches have overlooked the importance of individual segments within a video and the relationship among them, and tend to rely on a single modality when learning features. In this paper, we propose a novel interactive-enhanced cross-modal perception method~(CM-PIE), which can learn fine-grained features by applying a segment-based attention module. Furthermore, a cross-modal aggregation block is introduced to jointly optimize the semantic representation of audio and visual signals by enhancing inter-modal interactions. The experimental results show that our model offers improved parsing performance on the Look, Listen, and Parse dataset compared to other methods.
</details>
<details>
<summary>摘要</summary>
audio-visual视频分解任务是将视频分割成不同类别的segmentlevel，并使用弱标签预测它们是 audible 还是 visible 事件。现有的方法对此任务借鉴了注意机制，以捕捉全视频的各个模式之间的含义相关性。然而，这些方法往往忽略了视频中每个段落的重要性和相互关系，而且往往仅仅在学习特征时依赖单一的感知模式。在本文中，我们提出了一种新的互动增强交叉模态识别方法（CM-PIE），它可以通过应用段基 attention模块来学习细腻的特征。此外，我们还引入了交叉模态汇聚块，以联合优化音频和视频信号的含义表示。实验结果表明，我们的模型在Look, Listen, and Parse数据集上的分解性能比其他方法更高。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning"><a href="#A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning" class="headerlink" title="A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning"></a>A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07511">http://arxiv.org/abs/2310.07511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingtao-li-cver/uniadrs">https://github.com/jingtao-li-cver/uniadrs</a></li>
<li>paper_authors: Jingtao Li, Xinyu Wang, Hengwei Zhao, Liangpei Zhang, Yanfei Zhong</li>
<li>for: 检测地球表面上的异常现象，包括多种模式和场景下的异常。</li>
<li>methods: 基于异常偏离本地Context的特征，提出一种通用的异常检测器，可以在多种感知器和场景下检测异常。</li>
<li>results: 采用 conditional probability 模型，在五种模式（包括 Hyperspectral、可见光、Synthetic Aperture Radar（SAR）、Infrared和low light）下，实现了通用的异常检测能力。<details>
<summary>Abstract</summary>
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation of the reformulation in the data, architecture, and optimization aspects. Simulated spectral and spatial anomalies drive the instantiated architecture. The model is optimized directly for the conditional probability ranking. The proposed model was validated in five modalities including the hyperspectral, visible light, synthetic aperture radar (SAR), infrared and low light to show its unified detection ability.
</details>
<details>
<summary>摘要</summary>
<<SYS>> remote 感知异常检测器可以找到背景中异常的对象作为潜在目标。由于地球异常类型的多样性，一个跨modalities和场景的统一异常检测器应该是成本效益和灵活的。然而，现有的异常检测器都是单一modalities和单一场景的，因为它们想要学习不同的背景分布。受到地球异常的通用异常偏差特征启发，我们利用这个特征来建立一个统一的异常检测器。首先，我们将异常检测任务转换为一个无向双层图，基于偏差关系，其中异常分数是模型的条件概率， giventhe 背景和正常对象的模式。然后，我们设计了实现的数据、architecture和优化方面。在实验中，我们使用了模拟的spectral和空间异常驱动了实现的建筑。模型直接优化为条件概率排名的问题。我们 Validated 该模型在五个modalities中，包括光谱、可见光、Synthetic Aperture Radar（SAR）、红外和低光照，以显示其统一的检测能力。Note: "<<SYS>>" is used to indicate the beginning of the translation, and ">>" is used to indicate the end of the translation.
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning"><a href="#Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning" class="headerlink" title="Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning"></a>Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07510">http://arxiv.org/abs/2310.07510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiming Qian</li>
<li>for: The paper aims to develop a novel pre-training framework for vision representation learning, which can facilitate the efficiency of common-sense recognition by leveraging both self-supervised and supervised visual pre-text tasks in a multi-task manner.</li>
<li>methods: The proposed pre-training framework consists of both multi-label classification and self-supervised learning tasks, including Masked Image Modeling (MIM) and contrastive learning. The framework takes an image as input and uses a heuristic approach to consider its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding.</li>
<li>results: The proposed pre-trained models achieve results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks, including 85.3% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. The performance demonstrates the ability of the vision foundation model to serve general purpose vision tasks.Here are the three information points in Simplified Chinese text:</li>
<li>for: 本研究旨在开发一种新的视觉表示学习预训练框架，以便提高常识认识的效率，通过结合自我指导学习和指导学习的视觉预测任务。</li>
<li>methods: 提议的预训练框架包括多类别分类和自我指导学习任务，包括掩码图像模型（MIM）和对比学习。该框架将图像作为输入，采取一种启发式的方法，考虑图像的内在风格特征、内部对象的位置和相关性，以及图像在3D空间中的基本视觉理解。</li>
<li>results: 提议的预训练模型可以在多个视觉任务中达到或超过当前最佳（SOTA）结果，包括ImageNet-1K分类任务的85.3%顶部一准确率、COCO对象检测任务的47.9个框损失率和ADE-20Ksemantic segmentation任务的50.6个mIoU。这些结果表明提议的视觉基础模型能够满足通用视觉任务的需求。<details>
<summary>Abstract</summary>
To mimic human vision with the way of recognizing the diverse and open world, foundation vision models are much critical. While recent techniques of self-supervised learning show the promising potentiality of this mission, we argue that signals from labelled data are also important for common-sense recognition, and properly chosen pre-text tasks can facilitate the efficiency of vision representation learning. To this end, we propose a novel pre-training framework by adopting both self-supervised and supervised visual pre-text tasks in a multi-task manner. Specifically, given an image, we take a heuristic way by considering its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding. However, large-scale object bounding boxes and correlations are usually hard to achieve. Alternatively, we develop a hybrid method by leveraging both multi-label classification and self-supervised learning. On the one hand, under the multi-label supervision, the pre-trained model can explore the detailed information of an image, e.g., image types, objects, and part of semantic relations. On the other hand, self-supervised learning tasks, with respect to Masked Image Modeling (MIM) and contrastive learning, can help the model learn pixel details and patch correlations. Results show that our pre-trained models can deliver results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3\% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. The performance shows the ability of our vision foundation model to serve general purpose vision tasks.
</details>
<details>
<summary>摘要</summary>
Given an image, we take a heuristic approach by considering its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding. However, large-scale object bounding boxes and correlations are usually difficult to achieve.Alternatively, we develop a hybrid method that leverages both multi-label classification and self-supervised learning. Under multi-label supervision, the pre-trained model can explore detailed image information, such as image types, objects, and part of semantic relations. Self-supervised learning tasks, including Masked Image Modeling (MIM) and contrastive learning, help the model learn pixel details and patch correlations.Our pre-trained models achieve results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. These results demonstrate the ability of our vision foundation model to serve general-purpose vision tasks.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation"><a href="#Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation" class="headerlink" title="Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation"></a>Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07506">http://arxiv.org/abs/2310.07506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haizhong Zheng, Jiachen Sun, Shutong Wu, Bhavya Kailkhura, Zhuoqing Mao, Chaowei Xiao, Atul Prakash</li>
<li>for: 这篇论文旨在提出一种基于对数测量的数据缩寸方法，以提高模型训练的性能。</li>
<li>methods: 这篇论文提出了一种叫做层次记忆网络（Hierarchical Memory Network，HMN）的新数据参数化架构，将数据缩寸到三层结构中，表示集合、类别和实例等层级的特征。</li>
<li>results: 在四个公开 dataset（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上评估了HMN和八个基于数据缩寸的基eline，结果显示HMN在训练时使用较少的GPU内存仍然能够实现更高的性能。<details>
<summary>Abstract</summary>
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results show that our proposed method outperforms all baselines, even when trained with a batch-based loss consuming less GPU memory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个实际世界数据集，数据压缩（DC）目标是将数据集中的知识压缩到较小的数据集上，以高性能进行模型训练。 现有的工作提议通过数据参数化来增强DC，将数据压缩到参数化数据容器中而不是像素空间。数据参数化的INTUITION是将图像中共享的特征编码，以避免额外存储成本。在这篇论文中，我们认为图像在层次结构上共享特征，而当前的数据参数化方法忽略了这种层次结构。为了更好地将DC与这种层次结构相匹配，我们提议一种新的数据参数化架构，即层次记忆网络（HMN）。HMN将压缩数据存储在三层结构中，表示数据集级、类别级和实例级特征。另外，层次架构的帮助特性是HMN可以自然地保证图像之间的独立性，同时实现图像之间的信息共享。这使得HMN可以进行实例级别的减少冗余信息，从而进一步减少冗余和提高性能。我们在四个公共数据集（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上评估HMN，并与八个DC基准方法进行比较。评估结果表明，我们提议的方法在所有基准方法中表现出色，即使在使用较少的GPU内存的批处理损失下进行训练。
</details></li>
</ul>
<hr>
<h2 id="PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction"><a href="#PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction" class="headerlink" title="PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction"></a>PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07504">http://arxiv.org/abs/2310.07504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Gan, Qiuchen Zhai, Michael Thompson McCann, Cristina Garcia Cardona, Ulugbek S. Kamilov, Brendt Wohlberg</li>
<li>For: ptychographic image reconstruction* Methods: deep model-based network (Vision Transformer + deep unrolling network)* Results: outperforms existing deep learning methods, reduces computational cost compared to iterative methods, maintains competitive performance.Here’s the text in Simplified Chinese:</li>
<li>for: ptychographic 图像重建</li>
<li>methods: 深度模型基于网络 (视觉变换器 + 深度卷积网络)</li>
<li>results: 超过现有深度学习方法表现, 比较iterative方法计算成本下降, 维持竞争性表现.<details>
<summary>Abstract</summary>
Ptychography is an imaging technique that captures multiple overlapping snapshots of a sample, illuminated coherently by a moving localized probe. The image recovery from ptychographic data is generally achieved via an iterative algorithm that solves a nonlinear phase-field problem derived from measured diffraction patterns. However, these approaches have high computational cost. In this paper, we introduce PtychoDV, a novel deep model-based network designed for efficient, high-quality ptychographic image reconstruction. PtychoDV comprises a vision transformer that generates an initial image from the set of raw measurements, taking into consideration their mutual correlations. This is followed by a deep unrolling network that refines the initial image using learnable convolutional priors and the ptychography measurement model. Experimental results on simulated data demonstrate that PtychoDV is capable of outperforming existing deep learning methods for this problem, and significantly reduces computational cost compared to iterative methodologies, while maintaining competitive performance.
</details>
<details>
<summary>摘要</summary>
ptychography 是一种图像技术，通过多个重叠的报告来捕捉样本，由移动的本地化探针启发干涉光。图像从ptychographic数据中的恢复通常通过迭代算法解决非线性phaserange-field问题来实现，但这些方法具有高计算成本。在这篇文章中，我们介绍了ptychodv，一种新的深度学习模型基网络，用于高效、高品质ptychographic图像恢复。ptychodv包括一个视Transformer，该生成 Raw Measurements 集合中的初始图像，考虑到这些测量之间的相互关系。然后是一个深度折叠网络，该使用学习的卷积约束和ptychography测量模型来细化初始图像。实验结果表明，ptychodv可以在 simulated data 上超过现有的深度学习方法，并significantly reduce computational cost compared to iterative methodologies，同时维持竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation"><a href="#FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation" class="headerlink" title="FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation"></a>FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07473">http://arxiv.org/abs/2310.07473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Sun, Peihao Chen, Jugang Fan, Thomas H. Li, Jian Chen, Mingkui Tan</li>
<li>for: 这种研究旨在解决自主系统在图像目标导航中困难的问题，即从拍摄图像中理解目标位置。</li>
<li>methods: 我们采用了细化目标提示（FGPrompt）方法，利用高分辨率特征图作为提示，以便在目标图像中保留细节信息并使观察Encoder关注目标相关区域。</li>
<li>results: 相比现有方法，我们在3个图像目标导航数据集（Gibson、MP3D和HM3D）上显示出了显著的性能改进，尤其是在Gibson数据集上，我们的方法超过了状态时的成功率，但使用的模型规模只是1&#x2F;50。项目页面：<a target="_blank" rel="noopener" href="https://xinyusun.github.io/fgprompt-pages">https://xinyusun.github.io/fgprompt-pages</a><details>
<summary>Abstract</summary>
Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems. The agent is required to reason the goal location from where a picture is shot. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (FGPrompt) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8% with only 1/50 model size. Project page: https://xinyusun.github.io/fgprompt-pages
</details>
<details>
<summary>摘要</summary>
学习寻找图像指定目标是自主系统的重要 yet 挑战性任务。 agent 需要从拍摄图像中理解目标位置。现有方法通过学习导航策略，以独立地捕捉图像目标和观察图像的semantic特征，并最后进行导航动作预测。然而，这些方法受到两大限制。1）它们可能会损失图像目标中的细节信息，因此无法理解目标位置。2）更重要的是，它们难以将观察图像中的关键区域关注于目标，因为它们没有将目标作为条件来理解观察。在本文中，我们希望超越这些限制，通过设计高精度目标提示（FGPrompt）方法，以提高图像目标导航性能。具体来说，我们利用图像目标中的高精度和高分辨率特征地图作为提示，以进行条件嵌入，保留图像目标中的细节信息，并使观察编码器更加注重目标相关区域。与现有方法相比，我们在3个图像目标导航数据集（i.e., Gibson, MP3D, HM3D）上表现出了显著的性能提升。特别是在Gibson上，我们超过了状态的杰出成功率，只使用1/50个模型大小。项目页面：https://xinyusun.github.io/fgprompt-pages
</details></li>
</ul>
<hr>
<h2 id="PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction"><a href="#PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction" class="headerlink" title="PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction"></a>PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07449">http://arxiv.org/abs/2310.07449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr</li>
<li>for: 提高 neural surface reconstruction 的稳定性和准确性，特别是在真实世界中采集的摄像头pose 中存在噪声的情况下。</li>
<li>methods: 我们引入了一个新的偏置字段(\textbf{PoRF}), 使用多层感知（MLP）进行姿态更新的推断，从而更加稳定地优化姿态参数。此外，我们还提出了一种眼点几何学损失，以增强超级视觉检测的指导，无需额外计算过程。</li>
<li>results: 我们在 DTU 数据集上减少了 COLMAP 姿态错误的旋转角度错误78%，从而降低了重建 Chamfer 距离从 3.48mm 降至 0.85mm。在 MobileBrick 数据集上，我们改进了 ARKit 姿态和重建 F1 分数，从 69.18 提高到 75.67，比 dataset 提供的基准姿态（75.14）还要好。这些成果表明了我们的方法在真实世界中采集的摄像头pose 中提高姿态精度和重建稳定性。<details>
<summary>Abstract</summary>
Neural surface reconstruction is sensitive to the camera pose noise, even if state-of-the-art pose estimators like COLMAP or ARKit are used. More importantly, existing Pose-NeRF joint optimisation methods have struggled to improve pose accuracy in challenging real-world scenarios. To overcome the challenges, we introduce the pose residual field (\textbf{PoRF}), a novel implicit representation that uses an MLP for regressing pose updates. This is more robust than the conventional pose parameter optimisation due to parameter sharing that leverages global information over the entire sequence. Furthermore, we propose an epipolar geometry loss to enhance the supervision that leverages the correspondences exported from COLMAP results without the extra computational overhead. Our method yields promising results. On the DTU dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the MobileBrick dataset that contains casually captured unbounded 360-degree videos, our method refines ARKit poses and improves the reconstruction F1 score from 69.18 to 75.67, outperforming that with the dataset provided ground-truth pose (75.14). These achievements demonstrate the efficacy of our approach in refining camera poses and improving the accuracy of neural surface reconstruction in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
neural surface reconstruction 敏感于摄像头pose随机变化，即使使用最新的pose估计器 like COLMAP或ARKit。更重要的是，现有的pose-NeRF联合优化方法在实际世界场景中表现不佳。为了解决这些挑战，我们引入 pose residual field（PoRF），一种新的隐式表示方法，使用多层感知（MLP）来回归 pose 更新。这比传统的 pose 参数优化更加稳定，因为参数共享利用全序列的全局信息。此外，我们提议使用 Epipolar geometry loss来增强监督，这是基于 COLMAP 结果出口的对准不带额外计算过程。我们的方法在 DTU 数据集上减少了 COLMAP 姿态错误的旋转误差，从而降低了折射距离从 3.48mm 降至 0.85mm。在包含抓拍 capture 的 unbounded 360度视频 MobileBrick 数据集上，我们的方法改进了 ARKit 姿态，提高了 reconstruction F1 分数从 69.18 提高到 75.67，超过了提供的数据集真实pose（75.14）。这些成果表明我们的方法在实际世界场景中提高摄像头姿态和神经表面重建的准确性。
</details></li>
</ul>
<hr>
<h2 id="Distance-based-Weighted-Transformer-Network-for-Image-Completion"><a href="#Distance-based-Weighted-Transformer-Network-for-Image-Completion" class="headerlink" title="Distance-based Weighted Transformer Network for Image Completion"></a>Distance-based Weighted Transformer Network for Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07440">http://arxiv.org/abs/2310.07440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Xuelong Li, Yue Lu</li>
<li>for: This paper proposes a new architecture for image completion tasks, which leverages the strengths of both Convolutional Neural Networks (CNNs) and Distance-based Weighted Transformer (DWT) to enhance the image completion process.</li>
<li>methods: The proposed model uses a combination of CNNs and DWT blocks to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Additionally, the model introduces Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder’s skip features with the coarse features provided by the generator.</li>
<li>results: Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of the proposed model compared to existing approaches.<details>
<summary>Abstract</summary>
The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image's components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder's skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabiliser. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
描述文本：图像生成挑战已被模型为结构优先或变换问题。然而，现有模型在理解全局输入图像结构方面表现不佳，主要因为特定的内置特征（例如，本地推导优先）。现代研究表明，自我注意是图像完成问题的有效模型化技术。在这篇论文中，我们提出一种新的架构，利用距离基于权重Transformer（DWT）来更好地理解图像组件之间的关系。我们在模型中利用了Convolutional Neural Networks（CNNs）和DWT块的优点，以提高图像完成过程。具体来说，CNNs用于增强粗略先验的本地 тексту强度信息，而DWT块用于恢复一些粗略的文本和一致视觉结构。与现有方法一样，我们使用CNNs创建特征地图，但不同的是，我们使用DWT来编码全局依赖关系，计算距离基于权重特征图，这样可以减少视觉混乱的问题。此外，我们还提出了一种简单 yet有效的技术，使得非零值的卷积权重至正，并通过网络层的精度补做来提供高效的训练稳定器。广泛的量化和质量测试表明，我们提出的模型在三个复杂的数据集上表现出色，与现有方法相比，具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction"><a href="#DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction" class="headerlink" title="DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction"></a>DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07438">http://arxiv.org/abs/2310.07438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli</li>
<li>for: 预测路用户的路径准确性在多代理设定下是一项挑战，因为代理的特性和意图都是未知的。</li>
<li>methods: 我们提出了Dynamic goal quErieS with temporal Transductive alIgNmEnt（DESTINE）方法，与过去的方法不同，我们的方法1）动态预测代理的目标，不受特定道路结构的限制，以便生成更准确的目的预测; 2）通过生成未来轨迹的粗细化过程，使预测 trajectory 与地图兼容; 3）通过掩码注意力模块，使预测 trajectory 在时间上进行协调。</li>
<li>results: 使用 Argoverse 测试集，我们的方法在各种指标上达到了状态的末点性能，并通过了全面的减少研究，以证明提出的模块的贡献。<details>
<summary>Abstract</summary>
Predicting temporally consistent road users' trajectories in a multi-agent setting is a challenging task due to unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity. To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike past arts, our approach 1) dynamically predicts agents' goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via masked attention. Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到Simplified Chinese。<</SYS>>预测行为 temporal consistent road users的轨迹在多智能体设定中是一项具有挑战性的任务，因为智能体的特性和意图都是未知的。除了使用 semantic map 信息和交互模型外，还需要建立一种能够理解不同粒度的行为的机制。为此，我们提议动态目标 quErieS with temporal Transductive alIgNmEnt（DESTINE）方法。与过去的艺术不同，我们的方法具有以下特点：1. 动态预测智能体的目标，不受特定的公路结构，如车道，允许方法生成更加准确的目的地预测;2. 实现地图兼容预测，通过在下一帧执行的粗略预测作为中间目标，并在上一帧执行的精细预测作为精度预测;3. 使用带有干扰模块的注意力机制，以在预测轨迹的时间上进行对齐。使用常用的 Argoverse  benchmark 数据集，我们展示了我们的方法在不同维度上的出色表现，并进行了全面的折衔研究，以便更好地了解提议的模块的贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos"><a href="#A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos" class="headerlink" title="A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos"></a>A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07416">http://arxiv.org/abs/2310.07416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pedestriandynamics/vcnn4pude">https://github.com/pedestriandynamics/vcnn4pude</a></li>
<li>paper_authors: Ahmed Alia, Mohammed Maree, Mohcine Chraibi, Armin Seyfried</li>
<li>for: 了解人群压力行为的微观动态特征，以便更好地理解人群流动模式和互动方式。</li>
<li>methods: 提出了一种新的自动检测人群压力行为的框架，包括Feature提取和视频标注两个主要组成部分。</li>
<li>results: 对六个实验实际数据进行训练和测试，结果表明提出的框架在比较方法中表现出色，能够更好地检测人群压力行为。<details>
<summary>Abstract</summary>
Analyzing the microscopic dynamics of pushing behavior within crowds can offer valuable insights into crowd patterns and interactions. By identifying instances of pushing in crowd videos, a deeper understanding of when, where, and why such behavior occurs can be achieved. This knowledge is crucial to creating more effective crowd management strategies, optimizing crowd flow, and enhancing overall crowd experiences. However, manually identifying pushing behavior at the microscopic level is challenging, and the existing automatic approaches cannot detect such microscopic behavior. Thus, this article introduces a novel automatic framework for identifying pushing in videos of crowds on a microscopic level. The framework comprises two main components: i) Feature extraction and ii) Video labeling. In the feature extraction component, a new Voronoi-based method is developed for determining the local regions associated with each person in the input video. Subsequently, these regions are fed into EfficientNetV1B0 Convolutional Neural Network to extract the deep features of each person over time. In the second component, a combination of a fully connected layer with a Sigmoid activation function is employed to analyze these deep features and annotate the individuals involved in pushing within the video. The framework is trained and evaluated on a new dataset created using six real-world experiments, including their corresponding ground truths. The experimental findings indicate that the suggested framework outperforms seven baseline methods that are employed for comparative analysis purposes.
</details>
<details>
<summary>摘要</summary>
可以通过分析人群中微型动态的推担行为来获得价值的启示，以提高人群模式和互动的理解。通过在人群视频中识别推担行为的实例，可以更深入地了解推担行为发生的时间、地点和原因。这些知识是创建更有效的人群管理策略、优化人群流动和提高总体人群体验的关键。然而，手动识别人群中微型推担行为是困难的，现有的自动方法无法检测这种微型行为。因此，本文提出了一种新的自动框架，用于在人群视频中识别推担行为。该框架包括两个主要组成部分：一是特征提取部分，二是视频标注部分。在特征提取部分，我们开发了一种基于Voronoi区域的新方法，用于确定输入视频中每个人的本地区域。然后，这些区域将被传递给EfficientNetV1B0卷积神经网络进行深度特征提取。在视频标注部分，我们使用了一个全连接层和sigmoid活化函数，以分析这些深度特征并将视频中涉及到推担行为的个体进行标注。该框架被训练并评估使用了六个实际实验的新数据集，包括其相应的真实标注。实验结果表明，提议的框架在比较分析中超过了七个基线方法。
</details></li>
</ul>
<hr>
<h2 id="CLIP-for-Lightweight-Semantic-Segmentation"><a href="#CLIP-for-Lightweight-Semantic-Segmentation" class="headerlink" title="CLIP for Lightweight Semantic Segmentation"></a>CLIP for Lightweight Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07394">http://arxiv.org/abs/2310.07394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Jin, Wankou Yang</li>
<li>for: 这 paper 旨在提出一种新的语言引导 semantic segmentation 方法，以便在轻量级网络上应用。</li>
<li>methods: 这 paper 使用了一种新的 feature fusion module，该模块包括一个 CNN 和一个 transformer，并通过两个方向的桥连接来实现语言引导的功能。</li>
<li>results: 对于多个 experiment，这 paper 的方法可以达到更高的性能，比如 DenseCLIP 等之前的 SOTA 工作。<details>
<summary>Abstract</summary>
The large-scale pretrained model CLIP, trained on 400 million image-text pairs, offers a promising paradigm for tackling vision tasks, albeit at the image level. Later works, such as DenseCLIP and LSeg, extend this paradigm to dense prediction, including semantic segmentation, and have achieved excellent results. However, the above methods either rely on CLIP-pretrained visual backbones or use none-pretrained but heavy backbones such as Swin, while falling ineffective when applied to lightweight backbones. The reason for this is that the lightweitht networks, feature extraction ability of which are relatively limited, meet difficulty embedding the image feature aligned with text embeddings perfectly. In this work, we present a new feature fusion module which tackles this problem and enables language-guided paradigm to be applied to lightweight networks. Specifically, the module is a parallel design of CNN and transformer with a two-way bridge in between, where CNN extracts spatial information and visual context of the feature map from the image encoder, and the transformer propagates text embeddings from the text encoder forward. The core of the module is the bidirectional fusion of visual and text feature across the bridge which prompts their proximity and alignment in embedding space. The module is model-agnostic, which can not only make language-guided lightweight semantic segmentation practical, but also fully exploit the pretrained knowledge of language priors and achieve better performance than previous SOTA work, such as DenseCLIP, whatever the vision backbone is. Extensive experiments have been conducted to demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
大规模预训练模型CLIP，在400万张图像文本对应对得到了许多应用场景，尤其是图像水平上的任务。后续的工作，如 denseclip 和 LSeg，在 dense prediction 领域进一步扩展了这种方法，并取得了出色的成绩。然而，这些方法都是通过使用CLIP预训练的视觉后ION或者使用不预训练的 pero 重量级网络，如 Swin，来实现。这是因为轻量级网络的特点是其特征提取能力相对较弱，难以与文本嵌入完美地匹配。在这种情况下，我们提出了一种新的特征融合模块，可以解决这个问题，并使得语言指导方法可以应用于轻量级网络。具体来说，该模块是一种并行的 CNN 和 transformer 的设计，其中 CNN 提取图像Encoder中的空间信息和视觉上下文，而 transformer 将文本Encoder中的文本嵌入传递前进。模块的核心是在桥接之间进行对文本和图像特征的双向融合，以便它们在嵌入空间中的距离和对齐。该模块是模型无关的，可以不仅使得语言指导的轻量级semantic segmentation成为现实，还可以充分利用预训练的语言优先知识，并超过先前的 SOTA 工作，如 denseclip，无论视觉后ION是什么。我们已经进行了广泛的实验来证明我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters"><a href="#Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters" class="headerlink" title="Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters"></a>Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07361">http://arxiv.org/abs/2310.07361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Michalkiewicz, Masoud Faraki, Xiang Yu, Manmohan Chandraker, Mahsa Baktashmotlagh</li>
<li>for: 减少深度神经网络中过拟合的问题</li>
<li>methods: 基于梯度信号噪声比（GSNR）选择Dropout掩码，并使用元学习法寻找最佳Dropout比例</li>
<li>results: 在标准领域总结化Benchmark上实现了竞争力的分类和人脸反假检测结果<details>
<summary>Abstract</summary>
Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To compensate for the over-parameterized models, numerous regularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of domain shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sampled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of network's parameters. Specifically, at each training step, parameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the optimal dropout ratio by leveraging a meta-learning approach. We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classification and face anti-spoofing problems.
</details>
<details>
<summary>摘要</summary>
常见的问题是在深度神经网络中使用梯度下降式训练时发生过拟合源领域问题。为了弥补这个问题，许多调整技术已经被提出，如基于dropout的方法。这些方法在经典的benchmark上achieve significant improvement，但是它们在测试集中的类型转换时表现下降。在这篇文章中，我们从 классичногоapproach中逃避bernoulli抽出dropout mask的建构方法，而是基于网络参数的梯度噪声比率(GSNR)来选择parameter。具体来说，在每次训练步骤中，具有高GSNR的参数将被弃用。此外，我们透过元学习方法来缓解手动搜寻最佳dropout比率的负担。我们在标准的领域扩展benchmark上评估了我们的方法，并取得了竞争的结果在分类和面部防诈问题上。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method"><a href="#Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method" class="headerlink" title="Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method"></a>Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07359">http://arxiv.org/abs/2310.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masood Hamed Saghayan, Mohammad Hossein Zolfagharnasab, Ali Khadem, Farzam Matinfar, Hassan Rashidi</li>
<li>for: 诊断帕金逊病（BD），使用三维结构MRI图像（sMRI），而不是常见的函数MRI（fMRI）、电enzephalography（EEG）和行为 симптом。</li>
<li>methods: 提议使用混合GAN-CNN模型诊断BD，并测试了不同的扩充比例。</li>
<li>results: 取得75.8%的准确率，60.3%的敏感度和82.5%的特异度，比前一工作高3-5%，使用样本数少于6%。此外，还 demonstarted了GAN生成器可以有效地复制复杂的3D脑样本，而无需手动图像处理。<details>
<summary>Abstract</summary>
Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive cycles of hypomania and depression. Since diagnosing BD relies on subjective behavioral assessments over a long period, a solid diagnosis based on objective criteria is not straightforward. The current study responded to the described obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI samples rather than conventional datasets such as functional MRI (fMRI), electroencephalography (EEG), and behavioral symptoms while removing the data insufficiency usually encountered when dealing with sMRI samples. The impact of various augmentation ratios is also tested using 5-fold cross-validation. Based on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of 60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while utilizing less than 6% sample counts. Next, it is demonstrated that a 2- D layer-based GAN generator can effectively reproduce complex 3D brain samples, a more straightforward technique than manual image processing. Lastly, the optimum augmentation threshold for the current study using 172 sMRI samples is 50%, showing the applicability of the described method for larger sMRI datasets. In conclusion, it is established that data augmentation using GAN improves the accuracy of the CNN classifier using sMRI samples, thus developing more reliable decision support systems to assist practitioners in identifying BD patients more reliably and in a shorter period
</details>
<details>
<summary>摘要</summary>
精神疾病（BD）是一种心理疾病，通过重复的强迫症状和抑郁来诊断。由于诊断BD需要长期的主观行为评估，因此不可靠的诊断方法不是很 straightforward。现study中提出了一种hybrid GAN-CNN模型，用于通过三维结构MRI图像（sMRI）诊断BD。这种研究的创新在于利用sMRI样本进行诊断，而不是常见的fMRI、EEG和行为症状数据。此外，该研究还测试了不同的扩展率，并使用5-fold cross-validation。根据结果，该研究获得了75.8%的准确率，60.3%的敏感性和82.5%的特异性，这些值高于前一次研究3-5%，同时使用的样本数量少于6%。然后，研究表明了一个2D层基于GAN生成器可以有效地重produce复杂的3D脑样本，这种方法比手动图像处理更为简单。最后，该研究发现了使用172个sMRI样本的最佳扩展阈值为50%。总之，这种方法可以在更大的sMRI样本上进行数据扩展，从而提高CNN分类器的准确率，并为BD诊断提供更可靠的决策支持系统，以更加准确地诊断BD患者并更快地进行诊断。
</details></li>
</ul>
<hr>
<h2 id="IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training"><a href="#IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training" class="headerlink" title="IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training"></a>IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07355">http://arxiv.org/abs/2310.07355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 本研究旨在提高医疗影像语言预训练（VLP）中对医疗报告中的结构信息的利用，以提高视语同步性。</li>
<li>methods: 本研究提出了一种新的临床指导VLP框架，称之为IMITATE，该框架利用医疗报告的层次结构，并对相应的描述性和诊断性文本进行视语同步。此外，该研究还引入了一种新的临床知识 Informed Contrastive Loss，以考虑临床优先知识在对比学习中的样本相关性。</li>
<li>results: 对六个不同的数据集进行比较，IMITATE模型在五种医疗影像下沉淀任务中都超过了基eline VLP方法。实验结果表明，将医疗报告中的结构信息integrated into VLP可以提高视语同步性。<details>
<summary>Abstract</summary>
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furthermore, a new clinical-informed contrastive loss is introduced for cross-modal learning, which accounts for clinical prior knowledge in formulating sample correlations in contrastive learning. The proposed model, IMITATE, outperforms baseline VLP methods across six different datasets, spanning five medical imaging downstream tasks. Comprehensive experimental results highlight the advantages of integrating the hierarchical structure of medical reports for vision-language alignment.
</details>
<details>
<summary>摘要</summary>
在医学视语预训（VLP）领域，有很大的努力投入到从临床报告和相关医疗图像中提取文本和图像特征。然而，大多数现有方法可能忽略了利用临床报告的自然层次结构，这些报告通常被拆分成描述性内容的“发现”和结论的“印象”。相反，现有的医学VLP方法通常将报告简化为单一实体或分解成多个token。在这项工作中，我们提出了一种名为IMITATE的新的临床导向VLP框架，以学习医学报告的层次结构信息，并对报告中的描述性和结论部分进行视语同步。此外，我们还引入了一种基于临床知识的对比损失函数，用于跨模态学习，该函数考虑了临床知识在对比学习中的样本相关性。提出的模型IMITATE，在六个不同的数据集上，比基eline VLP方法表现出色，并且对五种医疗下游任务进行了评估。全面的实验结果表明，将临床报告的层次结构integrated into VLP框架可以提高视语同步的性能。
</details></li>
</ul>
<hr>
<h2 id="PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation"><a href="#PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation" class="headerlink" title="PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation"></a>PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07743">http://arxiv.org/abs/2310.07743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haibo-qiu/PointHR">https://github.com/haibo-qiu/PointHR</a></li>
<li>paper_authors: Haibo Qiu, Baosheng Yu, Yixin Chen, Dacheng Tao</li>
<li>for: 本研究旨在提高3D点云分割的高分辨率架构，以提高3D点云分析的精度。</li>
<li>methods: 提出了一个通用架构PointHR，包括KNN顺序操作器和差分扩散操作器，以及预计算序列和扩散操作器的索引。</li>
<li>results: 对S3DIS和ScanNetV2 dataset进行了广泛的实验，并取得了与现有状态艺术方法相当或更高的性能，无需额外增加细节。<details>
<summary>Abstract</summary>
Significant progress has been made recently in point cloud segmentation utilizing an encoder-decoder framework, which initially encodes point clouds into low-resolution representations and subsequently decodes high-resolution predictions. Inspired by the success of high-resolution architectures in image dense prediction, which always maintains a high-resolution representation throughout the entire learning process, we consider it also highly important for 3D dense point cloud analysis. Therefore, in this paper, we explore high-resolution architectures for 3D point cloud segmentation. Specifically, we generalize high-resolution architectures using a unified pipeline named PointHR, which includes a knn-based sequence operator for feature extraction and a differential resampling operator to efficiently communicate different resolutions. Additionally, we propose to avoid numerous on-the-fly computations of high-resolution architectures by pre-computing the indices for both sequence and resampling operators. By doing so, we deliver highly competitive high-resolution architectures while capitalizing on the benefits of well-designed point cloud blocks without additional effort. To evaluate these architectures for dense point cloud analysis, we conduct thorough experiments using S3DIS and ScanNetV2 datasets, where the proposed PointHR outperforms recent state-of-the-art methods without any bells and whistles. The source code is available at \url{https://github.com/haibo-qiu/PointHR}.
</details>
<details>
<summary>摘要</summary>
“近期，在点云分割方面有所进步，使用编码-解码框架，首先将点云编码成低分辨率表示，然后解码高分辨率预测。受图像高分辨率建筑的成功启发，我们认为3D点云分析中也非常重要。因此，在这篇论文中，我们探索了3D点云分割中高分辨率建筑。具体来说，我们将高分辨率建筑总称为PointHR，它包括基于KNN的序列运算器和差分扩散运算器，以及高效地传递不同分辨率的索引。此外，我们还提出了避免高分辨率建筑的许多在飞行计算中的即时计算，通过预计算序列和扩散运算器的索引。通过这种方式，我们实现了高度竞争力的高分辨率建筑，而无需额外努力。为评估这些建筑，我们对S3DIS和ScanNetV2 datasets进行了严格的实验，并发现提议的PointHR在无任何额外装饰的情况下高度竞争力。源代码可以在GitHub上找到：https://github.com/haibo-qiu/PointHR。”
</details></li>
</ul>
<hr>
<h2 id="Guided-Attention-for-Interpretable-Motion-Captioning"><a href="#Guided-Attention-for-Interpretable-Motion-Captioning" class="headerlink" title="Guided Attention for Interpretable Motion Captioning"></a>Guided Attention for Interpretable Motion Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07324">http://arxiv.org/abs/2310.07324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rd20karim/m2t-interpretable">https://github.com/rd20karim/m2t-interpretable</a></li>
<li>paper_authors: Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</li>
<li>For: This paper focuses on generating text from motion, specifically exploring the combination of movement encoders with spatio-temporal attention models to improve the interpretability of the architectures and highlight perceptually pertinent areas of the skeleton in time.* Methods: The proposed approach uses movement encoders and spatio-temporal attention models to generate text from motion. The authors propose strategies to guide the attention during training, such as adding guided attention with adaptive gate, to improve the interpretability of the models and highlight perceptually pertinent areas of the skeleton in time.* Results: The authors achieved state-of-the-art (SOTA) results on two motion capture datasets, KIT MLD and HumanML3D, with significant improvements in interpretability compared to higher parameter-count non-interpretable systems. Specifically, they obtained a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%) on KIT MLD, and a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA +6.1%), a CIDEr of 61.6 (SOTA -10.9%), and a Bertscore of 40.3% (SOTA +2.5%) on HumanML3D.<details>
<summary>Abstract</summary>
While much effort has been invested in generating human motion from text, relatively few studies have been dedicated to the reverse direction, that is, generating text from motion. Much of the research focuses on maximizing generation quality without any regard for the interpretability of the architectures, particularly regarding the influence of particular body parts in the generation and the temporal synchronization of words with specific movements and actions. This study explores the combination of movement encoders with spatio-temporal attention models and proposes strategies to guide the attention during training to highlight perceptually pertinent areas of the skeleton in time. We show that adding guided attention with adaptive gate leads to interpretable captioning while improving performance compared to higher parameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we obtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D, we obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA +6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our code implementation and reproduction details will be soon available at https://github.com/rd20karim/M2T-Interpretable/tree/main.
</details>
<details>
<summary>摘要</summary>
“尽管有很多研究投入到人体动作从文本生成中，但相对少数研究专门关注文本生成到人体动作的反向方向。大多数研究都是强调生成质量的最大化，而忽略了生成过程中特定部位的影响和时间同步。本研究探讨将运动编码器与空间时间注意力模型结合使用，并提出了引导注意力的策略，以便在训练时高亮有意义的骨骼区域。我们展示了在 KIT MLD 数据集上，通过添加引导注意力和适应门限来实现可读性的captioning，并提高了相比高参数计数的非可读性 SOTA 系统的性能。在 HumanML3D 数据集上，我们获得了 BLEU@4 的 25.00% (SOTA +2.7%),ROUGE-L 分数的 55.4% (SOTA +6.1%),CIDEr 分数的 61.6 (SOTA -10.9%),Bertscore 分数的 40.3% (SOTA +2.5%).我们的代码实现和重现细节将很快地在 GitHub 上公开。”
</details></li>
</ul>
<hr>
<h2 id="A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation"><a href="#A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation" class="headerlink" title="A webcam-based machine learning approach for three-dimensional range of motion evaluation"></a>A webcam-based machine learning approach for three-dimensional range of motion evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07322">http://arxiv.org/abs/2310.07322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoye Michael Wang, Derek T. Smith, Qin Zhu</li>
<li>for: 用于评估Physical therapy中的肢体范围运动</li>
<li>methods: 使用机器学习算法和网络摄像头进行评估</li>
<li>results: 实现了高度的重复性和对比性，可用于评估肢体范围运动的临床实践和远程物理治疗<details>
<summary>Abstract</summary>
Background. Joint range of motion (ROM) is an important quantitative measure for physical therapy. Commonly relying on a goniometer, accurate and reliable ROM measurement requires extensive training and practice. This, in turn, imposes a significant barrier for those who have limited in-person access to healthcare.   Objective. The current study presents and evaluates an alternative machine learning-based ROM evaluation method that could be remotely accessed via a webcam.   Methods. To evaluate its reliability, the ROM measurements for a diverse set of joints (neck, spine, and upper and lower extremities) derived using this method were compared to those obtained from a marker-based optical motion capture system.   Results. Data collected from 25 healthy adults demonstrated that the webcam solution exhibited high test-retest reliability, with substantial to almost perfect intraclass correlation coefficients for most joints. Compared with the marker-based system, the webcam-based system demonstrated substantial to almost perfect inter-rater reliability for some joints, and lower inter-rater reliability for other joints (e.g., shoulder flexion and elbow flexion), which could be attributed to the reduced sensitivity to joint locations at the apex of the movement.   Conclusions. The proposed webcam-based method exhibited high test-retest and inter-rater reliability, making it a versatile alternative for existing ROM evaluation methods in clinical practice and the tele-implementation of physical therapy and rehabilitation.
</details>
<details>
<summary>摘要</summary>
背景：关节范围运动（ROM）是物理治疗中的重要量化指标，通常使用尺 mesurement device，需要准确和可靠的测量，但是需要具备广泛的训练和实践经验。这种限制了医疗访问的情况，对于没有充分的面对面医疗访问的人来说，是一个重要的障碍。目标：本研究提出了一种基于机器学习的ROM评估方法，可以通过网络摄像头进行远程访问。方法：为评估这种方法的可靠性，对一组多种关节（ neck、脊梁、上肢和下肢）的ROM测量结果，与使用标记器基于光学运动追踪系统获取的测量结果进行比较。结果：对于25名健康成人的数据显示，网络摄像头解决方案具有高的测试-重复和 между人重要性，大多数关节的 intraclass correlation coefficient 在0.7到0.9之间，与标记器基于系统的测量结果相比，网络摄像头解决方案在一些关节（如肩屈和肘屈）表现出了高度的重要性，而在其他关节（如肩屈和肘屈）表现出了较低的重要性，这可能是因为网络摄像头解决方案在运动的峰值位置感知的reduced sensitivity。结论：提出的网络摄像头解决方案具有高的测试-重复和 между人重要性，可以作为现有的ROM评估方法的可靠的替代方案，在临床实践和远程物理治疗中应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy"><a href="#Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy" class="headerlink" title="Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy"></a>Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07310">http://arxiv.org/abs/2310.07310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei C. Aioanei, Regine Hunziker-Rodewald, Konstantin Klein, Dominik L. Michels</li>
<li>for: 提高古代铭文的解读精度，使用现代人工智能技术如机器学习（ML）。</li>
<li>methods: 开发了一种创新的方法，通过生成自然语言模拟器（GAN）生成了具有照明、损害和变换的实验性阿拉伯字母数据集。</li>
<li>results: 使用这些数据集训练了一个差异函数网络（ResNet）模型，并在8世纪前 Beside Hadad雕塑作品中的真实图像中达到了高准确率。<details>
<summary>Abstract</summary>
Epigraphy increasingly turns to modern artificial intelligence (AI) technologies such as machine learning (ML) for extracting insights from ancient inscriptions. However, scarce labeled data for training ML algorithms severely limits current techniques, especially for ancient scripts like Old Aramaic. Our research pioneers an innovative methodology for generating synthetic training data tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic Aramaic letter datasets, incorporating textural features, lighting, damage, and augmentations to mimic real-world inscription diversity. Despite minimal real examples, we engineer a dataset of 250,000 training and 25,000 validation images covering the 22 letter classes in the Aramaic alphabet. This comprehensive corpus provides a robust volume of data for training a residual neural network (ResNet) to classify highly degraded Aramaic letters. The ResNet model demonstrates high accuracy in classifying real images from the 8th century BCE Hadad statue inscription. Additional experiments validate performance on varying materials and styles, proving effective generalization. Our results validate the model's capabilities in handling diverse real-world scenarios, proving the viability of our synthetic data approach and avoiding the dependence on scarce training data that has constrained epigraphic analysis. Our innovative framework elevates interpretation accuracy on damaged inscriptions, thus enhancing knowledge extraction from these historical resources.
</details>
<details>
<summary>摘要</summary>
隐写技术逐渐使用现代人工智能（AI）技术，如机器学习（ML）来提取古代铭文中的信息。然而，古代文字如老阿拉伯语言的有限的标注数据，对当前技术的应用带来了严重的限制。我们的研究开拓了一种创新的方法，用于生成适用于老阿拉伯字母的合成训练数据。我们的管道synthesizes photo-realistic Aramaic letter datasets，包括文字的特征、照明、损害和扩展，以模拟实际铭文的多样性。尽管有限的实际示例，我们引入了250,000个训练图像和25,000个验证图像，覆盖了阿拉伯字母的22个字母类。这个全面的数据集提供了训练一个剩余神经网络（ResNet）来分类高度受损的阿拉伯字母的稳定的数据源。ResNet模型在8世纪前后期埃及神话雕塑铭文中的真实图像中显示了高精度的分类能力。进一步的实验证明了模型在不同材质和风格下的有效总体化。我们的结果证明了我们的合成数据方法的可行性，并且避免了对古代铭文的分析中的稀缺的标注数据的依赖。我们的创新框架提高了对损害铭文的解读精度，从而扩大了对历史资源的知识提取。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation"><a href="#Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation" class="headerlink" title="Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation"></a>Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07265">http://arxiv.org/abs/2310.07265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Zheng, Yunhao Luo, Pengyuan Zhou, Lin Wang</li>
<li>For: 本研究目标是将预训练的笨重 yet 表现良好的 Convolutional Neural Network (CNN) 模型转移到学习 Compact Vision Transformer (ViT) 模型，保持学习能力。* Methods: 我们提出了一种新的 Knowledge Distillation (KD) 框架，名为 C2VKD，以将 teacher 模型的知识传授到 student 模型。我们首先提出了一种 visual-linguistic feature distillation (VLFD) 模块，以便在对齐的视觉和语言相关表示之间进行有效的 KD。此外，由于 teacher 模型和 student 模型之间的容量差距和不可避免的预测错误，我们 THEN 提出了一种像素级分离分配 (PDD) 模块，以便在 combinational 标签和教师的预测值上监督学生。* Results: 我们在三个 semantic segmentation  benchmark  dataset上进行了实验，结果显示，我们的方法可以提高 mIoU 的提升量超过 200% 的 SoTA KD 方法。<details>
<summary>Abstract</summary>
In this paper, we tackle a new problem: how to transfer knowledge from the pre-trained cumbersome yet well-performed CNN-based model to learn a compact Vision Transformer (ViT)-based model while maintaining its learning capacity? Due to the completely different characteristics of ViT and CNN and the long-existing capacity gap between teacher and student models in Knowledge Distillation (KD), directly transferring the cross-model knowledge is non-trivial. To this end, we subtly leverage the visual and linguistic-compatible feature character of ViT (i.e., student), and its capacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD framework, dubbed C2VKD. Importantly, as the teacher's features are heterogeneous to those of the student, we first propose a novel visual-linguistic feature distillation (VLFD) module that explores efficient KD among the aligned visual and linguistic-compatible representations. Moreover, due to the large capacity gap between the teacher and student and the inevitable prediction errors of the teacher, we then propose a pixel-wise decoupled distillation (PDD) module to supervise the student under the combination of labels and teacher's predictions from the decoupled target and non-target classes. Experiments on three semantic segmentation benchmark datasets consistently show that the increment of mIoU of our method is over 200% of the SoTA KD methods
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了一个新的问题：如何从预训练的庞大又高效的 CNN 模型传承知识到学习具有较小体积的 Vision Transformer（ViT）模型，而保持学习能力？由于 ViT 和 CNN 模型之间的完全不同特征和长期存在的容量差异，直接传承交叉模型知识非常困难。为此，我们细腻地利用 ViT 模型（即学生）的视觉和语言相容特征，以及它们与 CNN 模型（即教师）之间的容量差，并提出了一种新的 CNN-to-ViT KD 框架，称为 C2VKD。重要的是，由于教师的特征与学生的特征是不同的，我们首先提出了一种新的视觉语言相容特征采样（VLFD）模块，以实现有效的 KD 。此外，由于教师和学生之间的容量差较大，以及不可避免的预测错误，我们则提出了一种像素级分离采样（PDD）模块，以supervise 学生在拥有标签和教师预测的decoupled 目标和非目标类下进行学习。在三个semantic segmentation benchmark数据集上，我们的方法的提升率在 SoTA KD 方法上超过 200%。
</details></li>
</ul>
<hr>
<h2 id="ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion"><a href="#ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion" class="headerlink" title="ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion"></a>ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07255">http://arxiv.org/abs/2310.07255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fangfang11-plog/adasr">https://github.com/fangfang11-plog/adasr</a></li>
<li>paper_authors: Jinghui Qin, Lihuang Fang, Ruitao Lu, Liang Lin, Yukai Shi</li>
<li>for: 这个论文旨在提高深度学习基于多spectral图像（HSI）的超分辨率（HR-HSI），使用深度神经网络（DNN）来将HSI和多spectral图像（MSI）进行融合。</li>
<li>methods: 我们提出了一种新的挑战性自动数据增强框架ADASR，该框架可以自动优化和扩充HSI-MSI样本对的数据多样性，以便更好地进行HSI-MSI融合。我们的框架是 Sample-aware，并通过对增强网络和两个下采样网络的 JOINT  adversarial学习来帮助我们学习更加Robust的下采样网络，以便在训练上映射网络时更好地进行训练。</li>
<li>results: 我们的ADASR在两个公共的古典多spectral数据集上进行了广泛的实验，与当前状态的方法进行比较，并达到了更高的性能。<details>
<summary>Abstract</summary>
Deep learning-based hyperspectral image (HSI) super-resolution, which aims to generate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image (HSI) and multispectral image (MSI) with deep neural networks (DNNs), has attracted lots of attention. However, neural networks require large amounts of training data, hindering their application in real-world scenarios. In this letter, we propose a novel adversarial automatic data augmentation framework ADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich data diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes an augmentor network and two downsampling networks jointly by adversarial learning so that we can learn more robust downsampling networks for training the upsampling network. Extensive experiments on two public classical hyperspectral datasets demonstrate the effectiveness of our ADASR compared to the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language Simplified Chinese;Deep learning-based 干扰特征图像（HSI）超分辨率，旨在通过将干扰特征图像（HSI）和多spectral图像（MSI）与深度神经网络（DNNs）结合，生成高空间分辨率干扰特征图像（HR-HSI）。然而，神经网络需要大量的训练数据，使其在实际场景中应用受限。在这封信中，我们提出了一种novel的对抗自动数据增强框架ADASR，可以自动优化和扩充HSI-MSI样本对的多样性，以便用于HSI-MSI融合。我们的框架是样本意识的，并且通过对抗学习来优化一个增强网络和两个下采样网络。广泛的实验表明，我们的ADASR比 estado-of-the-art 方法更有效。Note: "干扰特征图像" (HSI) is short for "hyperspectral image", and "多spectral图像" (MSI) is short for "multispectral image".
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation"><a href="#A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation" class="headerlink" title="A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation"></a>A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07252">http://arxiv.org/abs/2310.07252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashid Khan, Bingding Huang, Haseeb Hassan, Asim Zaman, Zhongfu Ye</li>
<li>for: 这篇论文主要针对图像描述 зада务，旨在使用计算机视觉和自然语言处理技术生成图像描述文本。</li>
<li>methods: 该方法使用了多个预训练的卷积神经网络作为Encoder来提取图像特征，并使用GRU基于的语言模型作为Decoder来生成描述句。另外，我们还采用了Bahdanau注意力机制与GRU嵌入式语言模型进行学习关注特定图像部分。</li>
<li>results: 我们在MSCOCO和Flickr30k datasets上进行评估，并显示了与当前方法相当的分数。我们的提议的框架可以bridge计算机视觉和自然语言之间的差距，并可以扩展到特定领域。<details>
<summary>Abstract</summary>
Image captioning is a challenging task involving generating a textual description for an image using computer vision and natural language processing techniques. This paper proposes a deep neural framework for image caption generation using a GRU-based attention mechanism. Our approach employs multiple pre-trained convolutional neural networks as the encoder to extract features from the image and a GRU-based language model as the decoder to generate descriptive sentences. To improve performance, we integrate the Bahdanau attention model with the GRU decoder to enable learning to focus on specific image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets and show that it achieves competitive scores compared to state-of-the-art methods. Our proposed framework can bridge the gap between computer vision and natural language and can be extended to specific domains.
</details>
<details>
<summary>摘要</summary>
Image 描述是一个复杂的任务，它使用计算机视觉和自然语言处理技术来生成一个图像的文本描述。这篇论文提出了一种深度神经网络框架，用于图像描述生成。我们的方法使用多个预训练的卷积神经网络作为编码器，从图像中提取特征，并使用 GRU 语言模型作为解码器，生成详细的句子。为了提高性能，我们将 Bahdanau 注意力模型与 GRU 解码器结合使用，允许学习关注特定的图像部分。我们使用 MSCOCO 和 Flickr30k 数据集进行评估，并显示了与州际方法相当的分数。我们的提议的框架可以跨越计算机视觉和自然语言之间的差距，并可以扩展到特定领域。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset"><a href="#Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset" class="headerlink" title="Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset"></a>Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07250">http://arxiv.org/abs/2310.07250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Ethem Hamamci</li>
<li>for: 这个研究旨在使用生成 adversarial network (GAN) Synthesize missing glioblastoma MRI sequence, 以便临床医生能够更好地诊断和治疗 brain tumor.</li>
<li>methods: 我们使用了 open-source GAN 方法，让我们可以将任何三个 MRI 序列作为输入，生成缺失的第四个构造序列。</li>
<li>results: 我们的实验结果显示，我们的方法可以生成高质量和实际的 MRI 序列，帮助临床医生提高诊断能力，并支持 AI 方法应用于 brain tumor MRI 量化。<details>
<summary>Abstract</summary>
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing high-quality and realistic MRI sequences, enabling clinicians to improve their diagnostic capabilities and support the application of AI methods to brain tumor MRI quantification.
</details>
<details>
<summary>摘要</summary>
高级肿瘤性肿瘤（Glioblastoma）是脑肿瘤的一种高度致命的形式。核磁共振成像（MRI）在诊断、治疗规划和监测高级肿瘤患者中扮演着非常重要的角色，因为它无需侵入性和辐射。国际脑肿瘤分 segmentation（BraTS）挑战对于精准和高效地分 segment glioblastoma 下层组织使用四种结构 MRI 扫描（T1、T1Gd、T2、T2-FLAIR）提供了许多人工智能算法。然而，这四种 MRI 扫描可能不总是可用。为解决这个问题，生成敌对网络（GANs）可以用来生成缺失的 MRI 扫描。在这篇论文中，我们实现了一种开源 GAN 方法，该方法使用任何三种 MRI 扫描作为输入，以生成缺失的第四种结构 MRI 扫描。我们的提议方法被添加到了社区驱动的通用 nuanced deep learning 框架（GaNDLF）中，并在生成高质量和真实的 MRI 扫描方面达到了有 promise 的结果，使临床医生可以提高诊断能力，并支持脑肿瘤 MRI 量化的应用。
</details></li>
</ul>
<hr>
<h2 id="IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors"><a href="#IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors" class="headerlink" title="IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors"></a>IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07248">http://arxiv.org/abs/2310.07248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Wang, Qiang Hu, Hongkuan Shi, Li He, Man He, Wenxuan Dai, Ting Li, Yitong Zhang, Dun Li, Mei Liu, Qiang Li</li>
<li>for: 这篇论文的目的是提出一种基于盒子指导的肿瘤分 segmentation方法，以提高肿瘤分 segmentation的精度和效率。</li>
<li>methods: 这篇论文使用了两种新的学习方法：Improved Box-dice（IBox）和Contrastive Latent-Anchors（CLA），并将这两种方法组合使用来训练一个 Robust 的盒子指导分 segmentation模型。IBox 方法将 segmentation 图像转换成一个代理图像，然后使用形态分离和异常区域交换来隔离形态和位置&#x2F;大小信息。CLA 方法生成了两种类型的隐藏栅栏，并使用振荡和分割肿瘤来更新隐藏栅栏，从而使模型更好地捕捉肿瘤和背景特征。</li>
<li>results: 这篇论文对五个公共的肿瘤分 datasets 进行了比较，结果表明 IBoxCLA 与最近的完全监督肿瘤分 segmentation方法相比，有着竞争性的性能，并且与其他盒子指导 state-of-the-arts 相比，具有至少6.5%和7.5%的全部 mDice 和 mIoU 的提升。<details>
<summary>Abstract</summary>
Box-supervised polyp segmentation attracts increasing attention for its cost-effective potential. Existing solutions often rely on learning-free methods or pretrained models to laboriously generate pseudo masks, triggering Dice constraint subsequently. In this paper, we found that a model guided by the simplest box-filled masks can accurately predict polyp locations/sizes, but suffers from shape collapsing. In response, we propose two innovative learning fashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and combine them to train a robust box-supervised model IBoxCLA. The core idea behind IBoxCLA is to decouple the learning of location/size and shape, allowing for focused constraints on each of them. Specifically, IBox transforms the segmentation map into a proxy map using shape decoupling and confusion-region swapping sequentially. Within the proxy map, shapes are disentangled, while locations/sizes are encoded as box-like responses. By constraining the proxy map instead of the raw prediction, the box-filled mask can well supervise IBoxCLA without misleading its shape learning. Furthermore, CLA contributes to shape learning by generating two types of latent anchors, which are learned and updated using momentum and segmented polyps to steadily represent polyp and background features. The latent anchors facilitate IBoxCLA to capture discriminative features within and outside boxes in a contrastive manner, yielding clearer boundaries. We benchmark IBoxCLA on five public polyp datasets. The experimental results demonstrate the competitive performance of IBoxCLA compared to recent fully-supervised polyp segmentation methods, and its superiority over other box-supervised state-of-the-arts with a relative increase of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.
</details>
<details>
<summary>摘要</summary>
《 Box-supervised 肿瘤分割吸引了越来越多的注意，因为它的成本效果很高。现有的解决方案 often 采用学习无关的方法或预训练模型，以生成 pseudo masks，从而触发 dice 约束。在这篇论文中，我们发现一个由 simplest box-filled masks 导向的模型可以准确预测肿瘤的位置/大小，但是受到形态塌陷的影响。为了解决这个问题，我们提出了两种创新的学习方式，Improved Box-dice（IBox）和 Contrastive Latent-Anchors（CLA），并将它们结合使用来训练一个 Robust box-supervised 模型 IBoxCLA。IBoxCLA 的核心思想是将学习位置/大小和形态的学习分解，以便对它们进行专门的约束。具体来说，IBox 将 segmentation 图像转换成一个代理图像，然后在代理图像中进行形态塌陷和混淆区域的交替处理。在代理图像中，形态和位置/大小都被解耦，而 box-like 响应被编码。通过约束代理图像而不是直接约束 raw prediction，可以使 box-filled mask 良好地指导 IBoxCLA 不会误导它的形态学习。此外，CLA 对 shape learning 做出了贡献，通过生成两种类型的潜在锚点，使 IBoxCLA 可以在对比的方式中捕捉肿瘤和背景特征。这些潜在锚点通过旋转和划分肿瘤和背景来学习和更新。我们对 IBoxCLA 在五个公共肿瘤数据集上进行了Benchmark。实验结果表明 IBoxCLA 与最近的完全监督肿瘤分割方法相比，表现竞争力强，与其他 box-supervised 状态的至少6.5%和7.5%的全局 mDice 和 mIoU 相比，表现出了明显的提升。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving"><a href="#Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving" class="headerlink" title="Optimizing the Placement of Roadside LiDARs for Autonomous Driving"></a>Optimizing the Placement of Roadside LiDARs for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07247">http://arxiv.org/abs/2310.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu</li>
<li>for: 这篇论文目的是优化路边LiDAR的布置，以提高自动驾驶车辆的感知性能。</li>
<li>methods: 该论文提出了一种基于贪婪算法的方法，通过遍历场景中的不同位置，选择最佳的LiDAR布置，以提高感知性能。该方法基于感知增强，定义感知增强为在新LiDAR添加后，感知能力的提高。</li>
<li>results: 该论文使用了一个名为Roadside-Opt的数据集，通过使用单个点云帧来评估LiDAR布置的影响，并提出了一种基于PointNet的感知预测器，以估计LiDAR布置的影响。<details>
<summary>Abstract</summary>
Multi-agent cooperative perception is an increasingly popular topic in the field of autonomous driving, where roadside LiDARs play an essential role. However, how to optimize the placement of roadside LiDARs is a crucial but often overlooked problem. This paper proposes an approach to optimize the placement of roadside LiDARs by selecting optimized positions within the scene for better perception performance. To efficiently obtain the best combination of locations, a greedy algorithm based on perceptual gain is proposed, which selects the location that can maximize the perceptual gain sequentially. We define perceptual gain as the increased perceptual capability when a new LiDAR is placed. To obtain the perception capability, we propose a perception predictor that learns to evaluate LiDAR placement using only a single point cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator to facilitate research on the roadside LiDAR placement problem.
</details>
<details>
<summary>摘要</summary>
多智能机器人合作感知是自驾车领域中日益受欢迎的话题，路边LiDAR在这一领域扮演着关键性的角色。然而，如何优化路边LiDAR的布局是一个重要但经常被忽略的问题。本文提出了一种方法来优化路边LiDAR的布局，通过选择场景中最佳位置来提高感知性能。为了效率地获得最佳组合，我们提出了一种基于探索性赢得的排序算法，该算法可以顺序选择最大化探索性的位置。我们定义了探索性为在新加LiDAR处理后提高的感知能力。为了获得感知能力，我们提出了一种感知预测器，该预测器可以通过单个点云帧来评估LiDAR布局。为了促进路边LiDAR布局问题的研究，我们创建了名为Roadside-Opt的数据集，该数据集使用CARLA simulate器生成。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs"><a href="#Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs" class="headerlink" title="Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs"></a>Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07245">http://arxiv.org/abs/2310.07245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</li>
<li>for: 这 paper 的目的是提高人群计数的准确性和可靠性，特别在具有雾、尘埃和低光照的环境下。</li>
<li>methods: 这 paper 使用的方法是使用 Pix2Pix 生成器对人群图像进行预处理，以提高计数模型的性能。</li>
<li>results: 测试结果表明，使用 Pix2Pix 生成器可以提高人群计数模型在具有雾、尘埃和低光照的环境下的性能，并且可以提供高度可靠和准确的人群计数结果。<details>
<summary>Abstract</summary>
Visual crowd counting estimates the density of the crowd using deep learning models such as convolution neural networks (CNNs). The performance of the model heavily relies on the quality of the training data that constitutes crowd images. In harsh weather such as fog, dust, and low light conditions, the inference performance may severely degrade on the noisy and blur images. In this paper, we propose the use of Pix2Pix generative adversarial network (GAN) to first denoise the crowd images prior to passing them to the counting model. A Pix2Pix network is trained using synthetic noisy images generated from original crowd images and then the pretrained generator is then used in the inference engine to estimate the crowd density in unseen, noisy crowd images. The performance is tested on JHU-Crowd dataset to validate the significance of the proposed method particularly when high reliability and accuracy are required.
</details>
<details>
<summary>摘要</summary>
“视觉人群计数”使用深度学习模型，如卷积神经网络（CNN）来估算人群密度。模型的性能受训练数据的质量的影响很大，而训练数据通常是来自人群图像的。在恶劣天气条件下，如雾、尘埃和低光照下，推断性能可能受到图像噪声和模糊的影响，导致推断结果减少。在这篇论文中，我们提议使用 Pix2Pix 生成整型网络（GAN）来首先减少人群图像中的噪声，然后将减少后的图像传递给计数模型进行计数。 Pix2Pix 网络在训练过程中使用自动生成的噪声图像，然后在推断引擎中使用预训练的生成器来估算人群密度。我们在JHU-Crowd数据集上测试了该方法，以验证该方法在需要高可靠性和准确性时的效果是否显著。
</details></li>
</ul>
<hr>
<h2 id="SAGE-ICP-Semantic-Information-Assisted-ICP"><a href="#SAGE-ICP-Semantic-Information-Assisted-ICP" class="headerlink" title="SAGE-ICP: Semantic Information-Assisted ICP"></a>SAGE-ICP: Semantic Information-Assisted ICP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07237">http://arxiv.org/abs/2310.07237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Cui, Jiming Chen, Liang Li</li>
<li>for: 提高雷达器在未知环境中的稳定和准确pose进行预测</li>
<li>methods: 利用有效semantic信息，提出了一种名为SAGE-ICP的新的semantic-assisted ICP方法，其中 semantics在odaometry中得到了抽象</li>
<li>results: 对KITTI和KITTI-360进行了实验评估，与基准方法相比，方法可以在大规模场景中提高localization精度，并且可以在实时性要求下保持高速运行<details>
<summary>Abstract</summary>
Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate.
</details>
<details>
<summary>摘要</summary>
Robust和准确的姿态估算在未知环境中是机器人应用中的关键部分。我们关注LiDAR基于点对点ICP的方法，并利用有效的semantic信息。这篇论文提出了一种基于semantic信息的ICP方法，称为SAGE-ICP，它在odometry中利用semantic信息。整个扫描的semantic信息在时间上是有效的和高效地提取，并且这些点 wise标签在注册过程中深度参与到每一个部分中，包括semantic精度下采样、数据关联、自适应本地地图和动态车辆除去。与前一些semantic援助方法不同，我们的方法可以在大规模场景中提高姿态准确性，即使semantic信息有一定的错误。实验评估在KITTI和KITTI-360上表明，我们的方法在准确性和实时性之间取得了平衡，即 faster than sensor frame rate。
</details></li>
</ul>
<hr>
<h2 id="AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation"><a href="#AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation"></a>AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07236">http://arxiv.org/abs/2310.07236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</li>
<li>for: 该 paper 的目的是提出一种基于 speech-driven 3D 面部动画的方法，以生成同时与驱动语音同步的面部运动，并具有个性化的 talking style。</li>
<li>methods: 该 paper 使用的方法包括 mixture-of-low-rank adaptation (MoLoRA) 和 discrete pose prior，以有效地捕捉面部表达和姿势样式。</li>
<li>results: 对比其他方法，该 paper 的方法能够更好地保持 Referenced 视频中的 talking style，并生成更为生动的面部动画。<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available at https://adamesh.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将语音驱动的3D面部动画翻译成中文简体版本。</SYS>>现有研究主要忽略了个人特有的说话样式在生成中，包括表情和头部姿态样式。一些研究尝试通过细化模块来捕捉个人风格。然而，受限于培训数据的限制，生成的表情和头部姿态具有假性。在这项工作中，我们提出了AdaMesh，一种新的适应语音驱动面部动画方法，可以从约10秒的参考视频中学习个人化说话风格，并生成生动的表情和头部姿态。具体来说，我们提出了 Mixture-of-low-rank adaptation（MoLoRA）来细化表情适应器，以高效地捕捉表情风格。而为个人化姿态风格，我们提出了姿态适应器，通过建立离散姿态先验和使用具有语义意识的姿态风格矩阵而不需要细化来获取相应的风格嵌入。广泛的实验结果表明，我们的方法在比较 estado-of-the-art 方法的基础上具有优势，保留参考视频中的说话风格，并生成了生动的面部动画。补充视频和代码将在 <https://adamesh.github.io> 上公开。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data"><a href="#Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data" class="headerlink" title="Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data"></a>Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07223">http://arxiv.org/abs/2310.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrodriguezortega/msmtu">https://github.com/jrodriguezortega/msmtu</a></li>
<li>paper_authors: José Rodríguez-Ortega, Rohaifa Khaldi, Domingo Alcaraz-Segura, Siham Tabik</li>
<li>for: 这个论文的目的是提出一种基于深度学习模型的多спектраль时间序列数据无结构光谱分解方法，用于提取杂合 pixel 中的不同Land Use和Land Cover类型和相应的含量。</li>
<li>methods: 该方法使用了深度学习模型，包括Long-Short Term Memory（LSTM）模型，并在模型中添加了地ографи�技术和气象 ancillary 信息，以提高LULC类别的含量估计。</li>
<li>results: 实验表明，将spectral-temporal输入数据与地ографи�技术和气象 ancillary 信息结合在一起，可以substantially improve LULC类别的含量估计在杂合 pixel 中。<details>
<summary>Abstract</summary>
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data together with geo-topographic and climatic information substantially improves the abundance estimation of LULC classes in mixed pixels. To carry out this study, we built a new labeled dataset of the region of Andalusia (Spain) with monthly multispectral time series of pixels for the year 2013 from MODIS at 460m resolution, for two hierarchical levels of LULC classes, named Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides, at the pixel level, a multispectral time series plus ancillary information annotated with the abundance of each LULC class inside each pixel. The dataset and code are available to the public.
</details>
<details>
<summary>摘要</summary>
remote 感知数据受到杂合用陆地和陆地覆盖类型（LULC）的影响。spectral 无杂解决方案可以提取杂合像素中的各种LULC类型和相应的含量。传统上，解决这个任务需要 Either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data together with geo-topographic and climatic information substantially improves the abundance estimation of LULC classes in mixed pixels. To carry out this study, we built a new labeled dataset of the region of Andalusia (Spain) with monthly multispectral time series of pixels for the year 2013 from MODIS at 460m resolution, for two hierarchical levels of LULC classes, named Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides, at the pixel level, a multispectral time series plus ancillary information annotated with the abundance of each LULC class inside each pixel. The dataset and code are available to the public.
</details></li>
</ul>
<hr>
<h2 id="Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model"><a href="#Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model" class="headerlink" title="Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model"></a>Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07222">http://arxiv.org/abs/2310.07222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysy31415/unipaint">https://github.com/ysy31415/unipaint</a></li>
<li>paper_authors: Shiyuan Yang, Xiaodong Chen, Jing Liao</li>
<li>for: 用于实现多modal的图像填充和修饰，提供不同类型的指导方式，包括无条件、文本驱动、触感驱动和示例驱动的填充方法，以及这些模式的组合。</li>
<li>methods: 基于静态扩散的稳定扩散模型（Stable Diffusion），不需要任务特定的训练，可以通过少量的示例来实现多modal的敏捷适应。</li>
<li>results: 通过对多种图像填充和修饰任务进行质量和量化评估，显示了我们的方法可以与单 modal 方法匹配的效果，同时提供了多 modal 填充和修饰的可能性。<details>
<summary>Abstract</summary>
Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have demonstrated impressive image generation capabilities and have also been successfully applied to image inpainting. However, in practice, users often require more control over the inpainting process beyond textual guidance, especially when they want to composite objects with customized appearance, color, shape, and layout. Unfortunately, existing diffusion-based inpainting methods are limited to single-modal guidance and require task-specific training, hindering their cross-modal scalability. To address these limitations, we propose Uni-paint, a unified framework for multimodal inpainting that offers various modes of guidance, including unconditional, text-driven, stroke-driven, exemplar-driven inpainting, as well as a combination of these modes. Furthermore, our Uni-paint is based on pretrained Stable Diffusion and does not require task-specific training on specific datasets, enabling few-shot generalizability to customized images. We have conducted extensive qualitative and quantitative evaluations that show our approach achieves comparable results to existing single-modal methods while offering multimodal inpainting capabilities not available in other methods. Code will be available at https://github.com/ysy31415/unipaint.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:最近，文本到图像的干净抽象概率模型（DDPM）已经展示了印象万能的图像生成能力，并且也成功应用于图像填充。然而，在实践中，用户经常需要更多的控制来指导填充过程，特别是当他们想要根据自己的需求自定义对象的外观、颜色、形状和布局时。 Unfortunately，现有的扩散基于的填充方法受到单一模式的导航限制，需要任务特定的训练，这限制了它们的跨模式可扩展性。为了解决这些限制，我们提议Uni-paint，一个通用的多模式填充框架，它提供了不同的导航模式，包括无条件、文本驱动、roke驱动、示例驱动填充，以及这些模式的组合。此外，我们的Uni-paint基于预训练的稳定扩散，不需要任务特定的训练，可以在特定数据集上进行几步扩展，实现个性化图像填充。我们进行了详细的质量和量化评估，表明我们的方法可以与现有的单模式方法相比，同时提供多模式填充能力不可用于其他方法。代码将在https://github.com/ysy31415/unipaint中提供。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-Explainable-Skin-Lesion-Classification"><a href="#Multi-task-Explainable-Skin-Lesion-Classification" class="headerlink" title="Multi-task Explainable Skin Lesion Classification"></a>Multi-task Explainable Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07209">http://arxiv.org/abs/2310.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahapara Khurshid, Mayank Vatsa, Richa Singh</li>
<li>for: 针对皮肤癌症的早期诊断，提高诊断精度和速度。</li>
<li>methods: 提出了一种多任务几个样本基于方法，结合分割网络作为注意模块和分类网络，并将分割和分类损失相乘Weighted。</li>
<li>results: 对三个皮肤癌症数据集进行了全面评估，实验结果表明该方法有效。<details>
<summary>Abstract</summary>
Skin cancer is one of the deadliest diseases and has a high mortality rate if left untreated. The diagnosis generally starts with visual screening and is followed by a biopsy or histopathological examination. Early detection can aid in lowering mortality rates. Visual screening can be limited by the experience of the doctor. Due to the long tail distribution of dermatological datasets and significant intra-variability between classes, automatic classification utilizing computer-aided methods becomes challenging. In this work, we propose a multitask few-shot-based approach for skin lesions that generalizes well with few labelled data to address the small sample space challenge. The proposed approach comprises a fusion of a segmentation network that acts as an attention module and classification network. The output of the segmentation network helps to focus on the most discriminatory features while making a decision by the classification network. To further enhance the classification performance, we have combined segmentation and classification loss in a weighted manner. We have also included the visualization results that explain the decisions made by the algorithm. Three dermatological datasets are used to evaluate the proposed method thoroughly. We also conducted cross-database experiments to ensure that the proposed approach is generalizable across similar datasets. Experimental results demonstrate the efficacy of the proposed work.
</details>
<details>
<summary>摘要</summary>
皮肤癌是一种非常危险的疾病，如果不得到治疗，死亡率会非常高。诊断通常从视觉检查开始，然后是比opsy或 histopathological examination。早期发现可以降低死亡率。 however， visual screening 可能受医生的经验限制。由于皮肤病学数据集的长尾分布和类别之间的显著差异，自动分类使用计算机辅助方法变得困难。在这种情况下，我们提出了一种多任务少量数据基于的方法，可以在皮肤病学数据集中进行有效的分类。我们的方法包括一个 segmentation 网络作为注意力模块，以及一个分类网络。 segmentation 网络的输出帮助分类网络做出决定，同时也帮助分类网络专注于最有特征的特征。为了进一步提高分类性能，我们将 segmentation 和分类损失相乘，并在权重的情况下进行融合。我们还提供了算法做出决定的视觉结果，以便更好地了解算法的决策逻辑。我们使用了三个皮肤病学数据集来评估我们的提案，并进行了跨数据集的测试，以确保我们的方法可以在类似数据集上普遍适用。实验结果表明，我们的方法具有较高的效果。
</details></li>
</ul>
<hr>
<h2 id="DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation"><a href="#DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation" class="headerlink" title="DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation"></a>DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07206">http://arxiv.org/abs/2310.07206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rong Wang, Wei Mao, Hongdong Li</li>
<li>for: 该 paper  targets the task of 3D pose estimation for a hand interacting with an object, using a single image observation.</li>
<li>methods: 该 paper 使用了 DeepSimHO，一种新的深度学习管道， combinign forward physics simulation and backward gradient approximation with a neural network.</li>
<li>results:  compared with previous works, DeepSimHO  noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization.Here are the three points in English for reference:</li>
<li>for: The paper targets the task of 3D pose estimation for a hand interacting with an object from a single image observation.</li>
<li>methods: The paper uses DeepSimHO, a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network.</li>
<li>results: Compared with previous works, DeepSimHO noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization.<details>
<summary>Abstract</summary>
This paper addresses the task of 3D pose estimation for a hand interacting with an object from a single image observation. When modeling hand-object interaction, previous works mainly exploit proximity cues, while overlooking the dynamical nature that the hand must stably grasp the object to counteract gravity and thus preventing the object from slipping or falling. These works fail to leverage dynamical constraints in the estimation and consequently often produce unstable results. Meanwhile, refining unstable configurations with physics-based reasoning remains challenging, both by the complexity of contact dynamics and by the lack of effective and efficient physics inference in the data-driven learning framework. To address both issues, we present DeepSimHO: a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network. Specifically, for an initial hand-object pose estimated by a base network, we forward it to a physics simulator to evaluate its stability. However, due to non-smooth contact geometry and penetration, existing differentiable simulators can not provide reliable state gradient. To remedy this, we further introduce a deep network to learn the stability evaluation process from the simulator, while smoothly approximating its gradient and thus enabling effective back-propagation. Extensive experiments show that our method noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization. The code is available at https://github.com/rongakowang/DeepSimHO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition"><a href="#SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition" class="headerlink" title="SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition"></a>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07189">http://arxiv.org/abs/2310.07189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwei Ren, Yue Zhou, Yulong Huang, Haotian Fu, Xiaopeng Lin, Jie Song, Bojun Cheng</li>
<li>for: Event-based action recognition tasks, specifically achieving ultra-low power consumption and high accuracy with sparse event data.</li>
<li>methods: Proposes a novel end-to-end point-based SNN architecture called SpikePoint, which effectively extracts global and local features through a singular-stage structure, and leverages surrogate training to achieve high accuracy with few parameters.</li>
<li>results: Achieves state-of-the-art performance on four event-based action recognition datasets using only 16 timesteps, with approximately 0.3% of the parameters and 0.5% of power consumption employed by artificial neural networks (ANNs).<details>
<summary>Abstract</summary>
Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on four event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3\% of the parameters and 0.5\% of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications.
</details>
<details>
<summary>摘要</summary>
事件摄像机是生物发现的感知器，响应当地辐射强度的变化，具有低延迟、高能效率和高 dinamic 范围。同时，神经元逻辑网络（SNN）已引起广泛关注，因其非常高效和抗错能力。通过将事件摄像机和 SNN 的能效特性相互融合，可以实现超低功耗应用场景，如行为识别任务。然而，现有方法通常需要将异步事件转换为传统的帧，从而导致额外的数据映射努力和数据损失，与 SNN 和事件摄像机的设计概念相 contradiction。为解决这个挑战，我们提出了 SpikePoint，一种新的终端点基 SNN 架构。SpikePoint 能够高效处理稀疏事件云数据，通过单阶段结构提取全局和局部特征。通过使用代理训练方法，SpikePoint 在多个数据集上实现高精度，只使用少量参数，并保持低功耗。具体来说，SpikePoint 在四个事件基据集上实现了状态当前（SOTA）性能，使用仅 16 个时间步骤，超过了其他 SNN 方法。此外，它还在三个数据集上实现了 SOTA 性能，使用约 0.3% 的参数和 0.5% 的功耗，与人工神经网络（ANN）相比。这些结果证明 Point Cloud 的重要性，并开阔了许多超低功耗事件基据处理应用场景的先河。
</details></li>
</ul>
<hr>
<h2 id="NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations"><a href="#NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations" class="headerlink" title="NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations"></a>NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07184">http://arxiv.org/abs/2310.07184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeongjoonju/neuroinspect">https://github.com/yeongjoonju/neuroinspect</a></li>
<li>paper_authors: Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee</li>
<li>for:  This paper aims to provide an interpretable neuron-based debugging framework for deep learning (DL) models to help practitioners interpret the decision-making process within the networks and identify the responsible neurons for mistakes.</li>
<li>methods:  The proposed debugging framework consists of three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. It uses a novel feature visualization method called CLIP-Illusion to generate images representing features conditioned on classes, which helps provide more human-interpretable explanations for model errors without altering the trained network or requiring additional data.</li>
<li>results:  The proposed framework is validated through evaluation in real-world settings, demonstrating its effectiveness in addressing false correlations and improving inferences for classes with the worst performance. Additionally, the paper shows that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding.<details>
<summary>Abstract</summary>
Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. We alleviate convoluted explanations of the conventional visualization approach by employing class information, thereby isolating mixed properties. This process offers more human-interpretable explanations for model errors without altering the trained network or requiring additional data. Furthermore, our framework mitigates false correlations learned from a dataset under a stochastic perspective, modifying decisions for the neurons considered as the main causes. We validate the effectiveness of our framework by addressing false correlations and improving inferences for classes with the worst performance in real-world settings. Moreover, we demonstrate that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding. The code is openly available at https://github.com/yeongjoonJu/NeuroInspect.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）已经取得了各种领域的显著进步，但DL模型仍然容易出错。这个问题需要有效的调试工具，以便DL实践者可以解释网络做出决策的过程。然而，现有的调试方法通常需要额外的数据或者网络决策的修改，限制了其应用性。为解决这个问题，我们提出了NeuroInspect，一个可解释的神经元基于的调试框架。我们的调试框架首先在网络中标识出负责错误的神经元，然后使用CLIP-Illusion novel feature visualization方法来生成可被人类理解的图像，以便检查神经元和决策层之间的连接。我们采用类信息来缓解传统视觉方法中的混乱解释，从而提供更加人类可理解的错误解释。此外，我们的框架还可以避免由数据下的杂合所学习的假相关性，从而修改神经元被视为主要原因的决策。我们验证了NeuroInspect的有效性，通过在真实情况下修复类型错误和提高各类型的推理。此外，我们还证明了NeuroInspect可以帮助调试DL模型的错误。代码可以在https://github.com/yeongjoonJu/NeuroInspect上下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models"><a href="#Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models" class="headerlink" title="Improving mitosis detection on histopathology images using large vision-language models"></a>Improving mitosis detection on histopathology images using large vision-language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07176">http://arxiv.org/abs/2310.07176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiwen Ding, James Hall, Neil Tenenholtz, Kristen Severson</li>
<li>for: 这个研究旨在提高肿瘤检测的准确性，使用了大规模的视觉语言模型，同时利用视觉特征和自然语言来减少人工评分的主观性。</li>
<li>methods: 该研究使用了卷积神经网络（CNN）来检测肿瘤，并利用了 metadata such as 肿瘤和扫描器类型作为上下文。</li>
<li>results: 研究结果表明，使用大规模的视觉语言模型可以提高肿瘤检测的准确性，并且比拥有基准模型的性能更高。<details>
<summary>Abstract</summary>
In certain types of cancerous tissue, mitotic count has been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, convolutional neural networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
</details>
<details>
<summary>摘要</summary>
某些types of cancerous tissue中， Mitotic count有 been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, Convolutional Neural Networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
</details></li>
</ul>
<hr>
<h2 id="Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent"><a href="#Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent" class="headerlink" title="Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent"></a>Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07166">http://arxiv.org/abs/2310.07166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyuan Ou, Siwei Wang, Pei Zhang, Sihang Zhou, En Zhu</li>
<li>for: 多视角嵌入 clustering 的研究在 recent 年来受到了越来越多的关注，因为它可以融合不同来源的信息并且在公共事务中具有承诺的前途。</li>
<li>methods: 我们提出了一种基于 Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) 的方法，通过一种统一的采样策略在相似空间中进行嵌入 clustering，从而降低了计算复杂性至线性时间成本。</li>
<li>results: 我们的提出的模型在多个公共 benchmark 数据集上进行了广泛的实验，并 consistently 超越了当前状态的技术。<details>
<summary>Abstract</summary>
Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time complexity w.r.t. the number of the objects. However, we propose Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to further reduce the computing complexity to linear time cost through a unified sampling strategy in resemblance space( STAGE 2), followed by subspace clustering to learn the representation collectively( STAGE 3). Extensive experimental results on public benchmark datasets demonstrate that our proposed model consistently outperforms the state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
多视角划分已经吸引了越来越多的关注，因为它可以聚合不同来源的信息并且在公共事务中具有替代的前景。到目前为止，许多先进的方法已经在最新的文献中提出。然而，在不同视角之间匹配特征的问题仍然很大。我们通过层次特征递减来解决这个问题，从而创造了一个共同的潜在空间（Stage 1）。这个潜在空间被称为“相似空间”，因为它暴露了不同视角之间的相似性和依赖关系。此外，由于大多数现有的多视角划分算法来自于k-means划分和спектраль划分，这会导致立方体时间复杂度与对象数成正比。然而，我们提出了 anchor-based multi-view subspace clustering with hierarchical feature descent（MVSC-HFD），以减少计算复杂度到线性时间成本，通过一种统一采样策略在相似空间（Stage 2），然后通过subspace clustering来学习共同表示（Stage 3）。我们的提出的模型在公共标准数据集上进行了广泛的实验，并 consistently 超越了当前技术的状态。
</details></li>
</ul>
<hr>
<h2 id="Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation"><a href="#Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation" class="headerlink" title="Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation"></a>Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07149">http://arxiv.org/abs/2310.07149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hye-Seong Hong, Abhishek Kumar, Dong-Gyu Lee</li>
<li>for: 本研究旨在提高无监督领域适应的性能，尤其是准确地分割 объекts boundaries。</li>
<li>methods: 该研究提出了一种新的领域适应方法，利用内部和外部信息的共同作用，增强预测器网络的准确性。具体来说，该方法通过在推知器网络中添加 Edge 的概率值，以提高分割结果的清晰度。此外，该方法还提出了一种概率分享网络，用于更有效地进行分割。</li>
<li>results: 实验结果表明，该方法在不同的无监督领域适应场景中具有优于STATE-OF-THE-ART 方法的性能。这表明该方法具有较好的一致性和灵活性。<details>
<summary>Abstract</summary>
The generalization capability of unsupervised domain adaptation can mitigate the need for extensive pixel-level annotations to train semantic segmentation networks by training models on synthetic data as a source with computer-generated annotations. Entropy-based adversarial networks are proposed to improve source domain prediction; however, they disregard significant external information, such as edges, which have the potential to identify and distinguish various objects within an image accurately. To address this issue, we introduce a novel approach to domain adaptation, leveraging the synergy of internal and external information within entropy-based adversarial networks. In this approach, we enrich the discriminator network with edge-predicted probability values within this innovative framework to enhance the clarity of class boundaries. Furthermore, we devised a probability-sharing network that integrates diverse information for more effective segmentation. Incorporating object edges addresses a pivotal aspect of unsupervised domain adaptation that has frequently been neglected in the past -- the precise delineation of object boundaries. Conventional unsupervised domain adaptation methods usually center around aligning feature distributions and may not explicitly model object boundaries. Our approach effectively bridges this gap by offering clear guidance on object boundaries, thereby elevating the quality of domain adaptation. Our approach undergoes rigorous evaluation on the established unsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Mapillary. Experimental results show that the proposed model attains better performance than state-of-the-art methods. The superior performance across different unsupervised domain adaptation scenarios highlights the versatility and robustness of the proposed method.
</details>
<details>
<summary>摘要</summary>
通用化能力可以减少需要大量像素级别的标注来训练Semantic segmentation网络，通过训练模型使用计算机生成的数据作为源，并使用计算机生成的标注。但是，基于 entropy 的对抗网络可能会忽略一些重要的外部信息，如对象边缘，这些信息可以准确地识别和分辨不同的对象。为解决这个问题，我们提出了一种新的领域适应方法，利用内部和外部信息的协同在 entropy 基于对抗网络中增强类boundary的明确性。此外，我们还设计了一种概率分享网络，将多种信息集成到更有效的分类中。在这种方法中，涉及对象边缘的信息可以准确地识别对象的边界，从而提高领域适应的质量。传统的领域适应方法通常是将特征分布 align，可能不会显式地模型对象边界。我们的方法可以准确地 bridge 这一鸿 gap，并提供明确的对象边界指导，从而提高领域适应的质量。我们的方法在已知的领域适应benchmark上进行了严格的评估，包括 SYNTHIA 到 Cityscapes 和 SYNTHIA 到 Mapillary。实验结果表明，我们的方法可以在不同的领域适应场景中表现出比州际的性能。这种灵活性和可靠性表明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model"><a href="#Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model" class="headerlink" title="Echocardiography video synthesis from end diastolic semantic map via diffusion model"></a>Echocardiography video synthesis from end diastolic semantic map via diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07131">http://arxiv.org/abs/2310.07131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phi Nguyen Van, Duc Tran Minh, Hieu Pham Huy, Long Tran Quoc<br>for:本研究旨在使用Diffusion Probabilistic Models（DDPMs）生成基于semantic anatomical信息的echocardiography视频，以解决现有数据集的约束，lacking comprehensive frame-wise annotations for every cardiac cycle。methods:本研究扩展了现有的视频Diffusion模型，使其能够使用semantic maps of the initial frame during the cardiac cycle，通常称为end diastole。我们还 integrate spatial adaptive normalization into multiscale feature maps，以包含semantic guidance during synthesis，提高生成的视频序列的realism和coherence。results:我们在CAMUS数据集上进行了实验，并证明了我们的模型在多个指标（包括FID、FVD和SSMI）上表现比标准扩散技术更好。<details>
<summary>Abstract</summary>
Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated significant achievements in various image and video generation tasks, including the domain of medical imaging. However, generating echocardiography videos based on semantic anatomical information remains an unexplored area of research. This is mostly due to the constraints imposed by the currently available datasets, which lack sufficient scale and comprehensive frame-wise annotations for every cardiac cycle. This paper aims to tackle the aforementioned challenges by expanding upon existing video diffusion models for the purpose of cardiac video synthesis. More specifically, our focus lies in generating video using semantic maps of the initial frame during the cardiac cycle, commonly referred to as end diastole. To further improve the synthesis process, we integrate spatial adaptive normalization into multiscale feature maps. This enables the inclusion of semantic guidance during synthesis, resulting in enhanced realism and coherence of the resultant video sequences. Experiments are conducted on the CAMUS dataset, which is a highly used dataset in the field of echocardiography. Our model exhibits better performance compared to the standard diffusion technique in terms of multiple metrics, including FID, FVD, and SSMI.
</details>
<details>
<summary>摘要</summary>
德函数泛化模型（DDPM）已经在不同的图像和视频生成任务中显示出了很大的成果，包括医疗影像领域。然而，基于 semantics  анатомиче信息生成echocardiography 视频仍然是一个未经探索的领域，主要是因为目前可用的数据集缺乏大规模的整个律动周期的框架级别注释。这篇论文目的是通过扩展现有的视频泛化模型来解决以上挑战。我们更具体地是在cardiac cycle 的起始帧semantic maps中进行视频生成。为了进一步改进生成过程，我们将 spatial adaptive normalization integrated into multiscale feature maps，以实现在生成过程中的semantic guidance，从而提高生成的视频序列的真实感和一致性。我们的模型在CAMUS数据集上进行了实验，并与标准泛化技术进行了比较。我们的模型在多个指标上（包括FID、FVD和SSMI）表现出了更好的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.CV_2023_10_11/" data-id="clnsn0vh700djgf88b97q3gut" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.AI_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T12:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.AI_2023_10_11/">cs.AI - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AutoRepo-A-general-framework-for-multi-modal-LLM-based-automated-construction-reporting"><a href="#AutoRepo-A-general-framework-for-multi-modal-LLM-based-automated-construction-reporting" class="headerlink" title="AutoRepo: A general framework for multi-modal LLM-based automated construction reporting"></a>AutoRepo: A general framework for multi-modal LLM-based automated construction reporting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07944">http://arxiv.org/abs/2310.07944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongxu Pu, Xincong Yang, Jing Li, Runhao Guo, Heng Li</li>
<li>for: 提高建筑项目安全、质量和时间完成性，使用自动生成建筑检查报告的新框架AutoRepo。</li>
<li>methods: 利用无人车进行建筑检查，收集场景信息，并使用多模态大语言模型生成检查报告。</li>
<li>results: 在实际建筑项目中应用并测试了AutoRepo框架，显示它可以减少检查过程的时间和资源配置，并生成符合法规标准的高质量检查报告。<details>
<summary>Abstract</summary>
Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.
</details>
<details>
<summary>摘要</summary>
Ensuring the safety, quality, and timely completion of construction projects is crucial, with construction inspections serving as a vital tool towards these goals. However, the predominantly manual approach of present-day inspections frequently leads to inefficiencies and inadequate information management. Such methods often fall short of providing comprehensive, exhaustive assessments, resulting in regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Co-NavGPT-Multi-Robot-Cooperative-Visual-Semantic-Navigation-using-Large-Language-Models"><a href="#Co-NavGPT-Multi-Robot-Cooperative-Visual-Semantic-Navigation-using-Large-Language-Models" class="headerlink" title="Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models"></a>Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07937">http://arxiv.org/abs/2310.07937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangguo Yu, Hamidreza Kasaei, Ming Cao</li>
<li>for: 这篇论文旨在解决多机器人合作时的可视目标导航问题，以实现高效率和可靠性。</li>
<li>methods: 本文提出了一个创新的框架，即Co-NavGPT，它使用大型自然语言模型（LLM）作为多机器人合作的全球观察者，将环境资料转换为提示，进而提高 LLM 的景象理解能力。</li>
<li>results: 实验结果显示，Co-NavGPT 在 HM3D 环境中的成功率和效率都高于现有模型，而且不需要任何学习过程，这表明 LLM 在多机器人合作领域的应用潜力非常大。<details>
<summary>Abstract</summary>
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs in multi-robot collaboration domains. The supplementary video, prompts, and code can be accessed via the following link: \href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</details>
<details>
<summary>摘要</summary>
在高级人机交互任务中，视觉目标导航是关键，以便自主机器人在未知环境中进行自主导航。过去有许多方法被开发出来，但大多数是单机器人操作的，它们往往因环境复杂性而减少效率和可靠性。此外，学习策略 для多机器人合作也是费时费力的。为解决这些挑战，我们提出了Co-NavGPT框架，它将大型自然语言模型（LLM）作为多机器人合作的全球规划器。Co-NavGPT将探索环境数据编码成提示，从而提高 LLM 的景象理解能力。然后，它将每个机器人分配出探索前沿，以实现高效的目标搜索。在Habitat-Matterport 3D（HM3D）上进行的实验结果表明，Co-NavGPT比既有模型在成功率和效率方面具有更高的潜力，而且无需进行学习过程，这表明 LLM 在多机器人合作领域的潜力是非常大。补充视频、提示和代码可以通过以下链接获取：\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</details></li>
</ul>
<hr>
<h2 id="What-Matters-to-You-Towards-Visual-Representation-Alignment-for-Robot-Learning"><a href="#What-Matters-to-You-Towards-Visual-Representation-Alignment-for-Robot-Learning" class="headerlink" title="What Matters to You? Towards Visual Representation Alignment for Robot Learning"></a>What Matters to You? Towards Visual Representation Alignment for Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07932">http://arxiv.org/abs/2310.07932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy</li>
<li>for: 本研究旨在帮助机器人优化与人类 preference 相关的奖励，以便机器人可以根据人类的需求和选择进行行为。</li>
<li>methods: 本研究使用了 Representation-Aligned Preference-based Learning (RAPL) 方法，该方法通过人类反馈来调整机器人的视觉表示，以便更好地满足人类的需求。</li>
<li>results: 实验结果表明，RAPL 的奖励可以生成人类喜欢的机器人行为，并且具有高样本效率和零样本泛化性。<details>
<summary>Abstract</summary>
When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="D2-Pruning-Message-Passing-for-Balancing-Diversity-and-Difficulty-in-Data-Pruning"><a href="#D2-Pruning-Message-Passing-for-Balancing-Diversity-and-Difficulty-in-Data-Pruning" class="headerlink" title="D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning"></a>D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07931">http://arxiv.org/abs/2310.07931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adymaharana/d2pruning">https://github.com/adymaharana/d2pruning</a></li>
<li>paper_authors: Adyasha Maharana, Prateek Yadav, Mohit Bansal</li>
<li>for: 提高模型训练数据质量可以降低模型测试错误率，同时可以采用数据减少方法来降低计算成本。</li>
<li>methods: 提出了一种基于图structure的数据选择算法， named D2 Pruning， 使用前向和反向消息传递来更新数据集中每个示例的difficulty scores，然后使用图Structured sampling方法选择最佳的核心集。</li>
<li>results: 对于视觉和语言 datasets，D2 Pruning比前一代方法更好地选择核心集，可以达到70%的减少率，同时发现使用D2 Pruning来筛选大量多模态数据集可以提高数据集的多样性和预训练模型的泛化性。<details>
<summary>Abstract</summary>
Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.
</details>
<details>
<summary>摘要</summary>
高品质数据可能导致模型在固定数据预算下的测试错误下降。此外，一个模型可以在固定计算预算下训练无需妥协性能，只要可以从数据集中剔除重复项。核心集选择（数据剔除）目的是选择训练数据集中的子集，以最大化模型在这个子集上的性能。现有两种主导方法：（1）基于geometry的数据选择，以提高数据多样性在核心集中，和（2）基于训练动态函数来评分样本的难度。最佳化数据多样性会导致核心集偏向容易样本，而选择难度排名则会忽略容易样本，这些样本是深度学习模型训练所必需的。这表明数据多样性和重要性分数是两种完全相关的因素，需要在核心集选择中同时考虑。我们将数据集表示为无向图，并提出一种新的减少算法，D2 Pruning，它使用数据集图上的前向和反向消息传递来进行核心集选择。D2 Pruning将数据集图上的每个例子的难度分数更新，基于该例子的邻居例子在数据集图上的难度。然后，这些更新后的难度分数将导航一种基于图的采样方法，选择包含多样和困难区域的核心集。我们对各种视觉和语言数据集进行了超过70%的减少率。此外，我们发现使用D2 Pruning进行大量多模态数据集的筛选可以提高数据集的多样性和预训练模型的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Contextualized-Policy-Recovery-Modeling-and-Interpreting-Medical-Decisions-with-Adaptive-Imitation-Learning"><a href="#Contextualized-Policy-Recovery-Modeling-and-Interpreting-Medical-Decisions-with-Adaptive-Imitation-Learning" class="headerlink" title="Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning"></a>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07918">http://arxiv.org/abs/2310.07918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Deuschel, Caleb N. Ellington, Benjamin J. Lengerich, Yingtao Luo, Pascal Friederich, Eric P. Xing</li>
<li>for: 该论文目的是提出一种 Contextualized Policy Recovery（CPR）方法，以解决现有的政策学习模型存在准确性和可读性之间的负面选择问题。</li>
<li>methods: CPR方法将问题定义为多任务学习问题，将复杂的决策过程分解为不同的上下文特定策略。每个上下文特定策略都是一个线性观察到行动映射。CPR方法可以在完全无线上和部分可见的决策环境中运行，并可以与任何循环黑盒模型或可读的决策模型结合使用。</li>
<li>results: 研究人员通过在 simulate 和实际数据上测试 CPR 方法，实现了在静脉抗生素干扰 ICU 中预测抗生素药物的 (+22% AUROC vs. 前一代 SOTA) 和预测 Alzheimer 病人 MRI 药物的 (+7.7% AUROC vs. 前一代 SOTA) 任务上的状元表现。与此同时，CPR 方法 closing 了可读性和黑盒方法之间的准确性差距，允许高分辨率探索和分析上下文特定的决策模型。<details>
<summary>Abstract</summary>
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapping, and generates new decision models $\textit{on-demand}$ as contexts are updated with new observations. CPR is compatible with fully offline and partially observable decision environments, and can be tailored to incorporate any recurrent black-box model or interpretable decision model. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on the canonical tasks of predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive performance, CPR closes the accuracy gap between interpretable and black-box methods for policy learning, allowing high-resolution exploration and analysis of context-specific decision models.
</details>
<details>
<summary>摘要</summary>
“干预推理”政策学习目标在于从观察行为中估算出可理解的决策政策，但现有模型却存在精度和可理解之间的对抗。这个对抗限制了基于数据的人类决策过程的解释。例如，为了审核医疗决策中的偏见和不良实践，我们需要一些可解释的决策过程模型，以提供简洁的行为描述。实际上，现有的方法受到这个对抗，因为它们将基于决策过程的底层模型设计为一个通用政策，但人类决策实际上是动态的，可以随着上下文资讯而改变。因此，我们提出了“上下文化政策恢复”（CPR），它将决策过程模型化为多任务学习问题，并将复杂的决策政策拆分为具有上下文特定的政策。CPR模型每个上下文特定的政策为一个线性观察到动作的映射，并在上下文更新时产生新的决策模型。CPR适用于完全离线和部分可观察的决策环境，并可以与任何可读黑盒模型或可解释的决策模型整合。我们通过在模拟和实际数据上进行了研究， CP 的表现比前一代最佳化� Архивная копия от 20 августа 2020 на Wayback Machine
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Machine-Learning-Techniques-in-Imbalanced-Data-and-Future-Trends"><a href="#A-Review-of-Machine-Learning-Techniques-in-Imbalanced-Data-and-Future-Trends" class="headerlink" title="A Review of Machine Learning Techniques in Imbalanced Data and Future Trends"></a>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07917">http://arxiv.org/abs/2310.07917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, Theodore Trafalis</li>
<li>for: 本研究旨在提供大规模不均衡数据中机器学习领域各种方法的概述和总结，以便在不同领域中应用大规模不均衡数据。</li>
<li>methods: 本研究涉及了各种手段，包括各种数据处理技术和机器学习算法，以 Addressing the problem of imbalanced data in various domains。</li>
<li>results: 本研究通过收集和评审258篇同行评审文章，提供了对各种方法的审视和总结，以及在不同领域中机器学习的应用。<details>
<summary>Abstract</summary>
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Recurrent-networks-recognize-patterns-with-low-dimensional-oscillations"><a href="#Recurrent-networks-recognize-patterns-with-low-dimensional-oscillations" class="headerlink" title="Recurrent networks recognize patterns with low-dimensional oscillations"></a>Recurrent networks recognize patterns with low-dimensional oscillations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07908">http://arxiv.org/abs/2310.07908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keith T. Murray</li>
<li>for: 这个研究探讨了一种新的动力学机制，用于识别模式，通过解释一个基于SET卡游戏的简单任务中的回归神经网络（RNN）的含义。</li>
<li>methods: 这个研究使用了解释RNN的方法，并将其视为在低维度限径谱中发生相位变换的模式识别。此外，研究者还手工制作了一个气泡模型，以重现RNN的动力学。</li>
<li>results: 这个研究发现，RNN可以通过相位变换来识别模式，并且这种机制与金字塔自动机（FSA）的转移有相似之处。此外，研究还发现了一种可能的神经网络实现FSA的机制。这项研究不仅提供了一种可能的模式识别机制，还为深度学习模型解释提供了一个新的视角。<details>
<summary>Abstract</summary>
This study proposes a novel dynamical mechanism for pattern recognition discovered by interpreting a recurrent neural network (RNN) trained on a simple task inspired by the SET card game. We interpreted the trained RNN as recognizing patterns via phase shifts in a low-dimensional limit cycle in a manner analogous to transitions in a finite state automaton (FSA). We further validated this interpretation by handcrafting a simple oscillatory model that reproduces the dynamics of the trained RNN. Our findings not only suggest of a potential dynamical mechanism capable of pattern recognition, but also suggest of a potential neural implementation of FSA. Above all, this work contributes to the growing discourse on deep learning model interpretability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RoboCLIP-One-Demonstration-is-Enough-to-Learn-Robot-Policies"><a href="#RoboCLIP-One-Demonstration-is-Enough-to-Learn-Robot-Policies" class="headerlink" title="RoboCLIP: One Demonstration is Enough to Learn Robot Policies"></a>RoboCLIP: One Demonstration is Enough to Learn Robot Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07899">http://arxiv.org/abs/2310.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumedh A Sontakke, Jesse Zhang, Sébastien M. R. Arnold, Karl Pertsch, Erdem Bıyık, Dorsa Sadigh, Chelsea Finn, Laurent Itti<br>for: RoboCLIP is designed to address the difficulty of reward specification in reinforcement learning, particularly the need for extensive expert supervision to design robust reward functions.methods: RoboCLIP uses a single video demonstration or textual description of the task to generate rewards without manual reward function design, leveraging pretrained Video-and-Language Models (VLMs) without any finetuning.results: Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, using only one video&#x2F;text demonstration.<details>
<summary>Abstract</summary>
Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>请注意，以下文本将使用简化中文表示。</SYS>> reward specification 是 reinforcement learning 中的一个不orious难题，需要广泛的专家指导以设计可靠的奖励函数。 imitation learning（IL）方法尝试通过使用专家示范来绕过这些问题，但通常需要大量的域内专家示范。 我们受到 Video-and-Language Models（VLMs）领域的进步 inspirited，我们提出了 RoboCLIP，一种在线的 imitation learning 方法，使用单个示范（超越大量数据要求），可以通过视频示范或文本描述来生成奖励，无需手动设计奖励函数。 RoboCLIP 还可以使用不同域的示范，如人类解决任务的视频示范，以便不需要同一个示范和部署域。 RoboCLIP 使用预训练的 VLMs，无需任何finetuning，可以生成奖励。 reinforcement learning 代理人使用 RoboCLIP 奖励表现出2-3倍于竞争的 imitation learning 方法在下游机器人处理任务上的零件表现，使用单个视频/文本示范。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Integrators-for-Diffusion-Generative-Models"><a href="#Efficient-Integrators-for-Diffusion-Generative-Models" class="headerlink" title="Efficient Integrators for Diffusion Generative Models"></a>Efficient Integrators for Diffusion Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07894">http://arxiv.org/abs/2310.07894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandt-lab/PSLD">https://github.com/mandt-lab/PSLD</a></li>
<li>paper_authors: Kushagra Pandey, Maja Rudolph, Stephan Mandt</li>
<li>for: 本研究旨在提高扩展Diffusion模型的采样速度，以便更快地生成样本。</li>
<li>methods: 我们提出了两种扩展Diffusion模型的采样方法： conjugate integrators和splitting integrators。 conjugate integrators通过将反射 diffusion 动力学映射到更易于采样的空间，而 splitting-based integrators通过在数据和占 auxiliary 变量之间进行交替更新来减少数值计算误差。</li>
<li>results: 我们对这两种方法进行了广泛的实验和理论研究，并提出了一种hybrid方法，这种方法可以在扩展空间中对Diffusion模型进行最佳性能的采样。在应用于[Pandey &amp; Mandt, 2023]中的CIFAR-10上，我们的deterministic和stochastic采样器可以在100个网络功能评估（NFE）后达到FID分数为2.11和2.36，比最佳基eline的2.57和2.63更低。<details>
<summary>Abstract</summary>
Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints will be made publicly available at \url{https://github.com/mandt-lab/PSLD}.
</details>
<details>
<summary>摘要</summary>
Diffusion models在推理时会受到慢的样本生成问题。因此，开发一个原则性的框架来加速预训练模型的干扰/随机抽取是一个有前途的方向。我们提议两种补充性框架来加速预训练模型的样本生成：协调 интеграторы和分割 интеграторы。协调 интеграторы扩展了逆扩散动力学，将逆扩散动力学映射到更适合采样的空间。相比之下，分割基于分子动力学通常使用数值方法更新数据和辅助变量，从而减少数值计算误差。我们经验和理论上深入研究这些方法，并提出了一种混合方法，导致预训练模型在扩展空间中的最佳表现。应用于[Pandey & Mandt, 2023]中的phas Space Langevin Diffusion（PSLD）在CIFAR-10上，我们的干扰和随机抽取器在100个网络函数评估（NFE）内达到了FID分数为2.11和2.36，与最佳基线相比下升2.57和2.63。我们将代码和模型Checkpoint公开在 GitHub上，请参考\url{https://github.com/mandt-lab/PSLD}.
</details></li>
</ul>
<hr>
<h2 id="LangNav-Language-as-a-Perceptual-Representation-for-Navigation"><a href="#LangNav-Language-as-a-Perceptual-Representation-for-Navigation" class="headerlink" title="LangNav: Language as a Perceptual Representation for Navigation"></a>LangNav: Language as a Perceptual Representation for Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07889">http://arxiv.org/abs/2310.07889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim</li>
<li>for: 本文探讨了使用语言作为视觉 Navigation 的感知表示。</li>
<li>methods: 我们的方法使用了市场上可得的视觉系统（图像描述和对象检测）将当前时间步的自我中心Panoramic View转换成自然语言描述。然后，我们将预训练的语言模型进行训练，以选择基于当前视图和轨迹历史的最佳行为。与标准设置不同的是，我们的方法使用（粗粒度）语言作为感知表示，而不是直接使用预训练的视觉特征。</li>
<li>results: 我们的方法在R2R视觉语言导航标准 benchmark上实现了比预先学习的视觉特征更好的性能，特别是在只有10-100个黄金轨迹available的情况下。这表明使用语言作为感知表示可以在导航任务中提供更好的性能。<details>
<summary>Abstract</summary>
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer a policy learned on a simulated environment (ALFRED) to a real-world environment (R2R). Our approach is found to improve upon strong baselines that rely on visual features in settings where only a few gold trajectories (10-100) are available, demonstrating the potential of using language as a perceptual representation for navigation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leader-Follower-Neural-Networks-with-Local-Error-Signals-Inspired-by-Complex-Collectives"><a href="#Leader-Follower-Neural-Networks-with-Local-Error-Signals-Inspired-by-Complex-Collectives" class="headerlink" title="Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives"></a>Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07885">http://arxiv.org/abs/2310.07885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenzhong Yin, Mingxi Cheng, Xiongye Xiao, Xinghe Chen, Shahin Nazarian, Andrei Irimia, Paul Bogdan</li>
<li>For: The paper is written to propose a neural network architecture inspired by the rules observed in nature’s collective ensembles, and to investigate the behavior of workers in the network.* Methods: The paper uses a leader-follower neural network (LFNN) structure, and trains the network using local error signals and optionally incorporating backpropagation (BP) and global loss.* Results: The paper achieves significantly lower error rates than previous BP-free algorithms on MNIST and CIFAR-10, and outperforms previous BP-free algorithms by a significant margin on ImageNet.Here is the information in Simplified Chinese text, as requested:* For: 这篇论文是为了提出基于自然集合中规则的神经网络架构，并研究工作者在网络中的行为。* Methods: 这篇论文使用了领导者-追随者神经网络（LFNN）结构，并通过本地错误信号和可选的反propagation（BP）和全局损失来训练网络。* Results: 这篇论文在MNIST和CIFAR-10上实现了比前一代BP-free算法更低的错误率，并在ImageNet上超过了前一代BP-free算法的性能。<details>
<summary>Abstract</summary>
The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained with local error signals achieve significantly lower error rates than previous BP-free algorithms on MNIST and CIFAR-10 and even surpass BP-enabled baselines. In the case of ImageNet, our LFNN-l demonstrates superior scalability and outperforms previous BP-free algorithms by a significant margin.
</details>
<details>
<summary>摘要</summary>
集体行为的网络（例如鱼群、鸟群或神经网络）表现出高度自组织和复杂性。这些emergent特性来自简单的互动规则，其中某些个体可以展示领导性行为，影响集体活动的总体表现。受自然集体ensemble的复杂性启发，我们提出一种基于自然集体的神经网络（NN）结构。这个NN结构包含有一个或多个信息处理单元（例如神经元、滤波器、层或层组）的工作者。工作者可以是领导者或追随者，我们使用本地错误信号和可选地包括反向传播（BP）和全局损失来训练领导者-追随者神经网络（LFNN）。我们对工作者行为进行了广泛的实验研究，并评估了LFNN的性能。我们的LFNN使用本地错误信号进行训练，在MNIST和CIFAR-10上达到了较低的错误率，并在ImageNet上超越了前一代BP-free算法。
</details></li>
</ul>
<hr>
<h2 id="The-Thousand-Faces-of-Explainable-AI-Along-the-Machine-Learning-Life-Cycle-Industrial-Reality-and-Current-State-of-Research"><a href="#The-Thousand-Faces-of-Explainable-AI-Along-the-Machine-Learning-Life-Cycle-Industrial-Reality-and-Current-State-of-Research" class="headerlink" title="The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research"></a>The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07882">http://arxiv.org/abs/2310.07882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Ralf Gross, Alexander Koebler, Michael Lebacher, Ronald Schnitzer, Stefan H. Weber</li>
<li>for: 本研究探讨了可解释人工智能（XAI）在生产业中的实际应用 relevance，并对当前学术研究进行了比较。</li>
<li>methods: 本研究采用了广泛的采访方法，包括各种角色和关键参与者从不同的行业部门进行了访问。此外，我们还提供了一个简短的文献回顾，以提供一个涵盖所调查人员的意见以及当前学术研究的总览。</li>
<li>results: 我们的调查结果表明，虽然存在许多不同的XAI方法，但大多数都集中在模型评估阶段和数据科学家之间。此外，我们还发现了一些不足，例如现有方法和框架不足以帮助非专家用户理解和解释透明度不高的人工智能模型。<details>
<summary>Abstract</summary>
In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the model evaluation phase and data scientists. Their versatile capabilities for other stages are currently either not sufficiently explored or not popular among practitioners. In line with existing work, our findings also confirm that more efforts are needed to enable also non-expert users' interpretation and understanding of opaque AI models with existing methods and frameworks.
</details>
<details>
<summary>摘要</summary>
The study finds that while there are many different XAI approaches, most are centered around the model evaluation phase and data scientists, with limited exploration of their versatility in other stages. Additionally, the study confirms that more efforts are needed to enable non-expert users to interpret and understand opaque AI models using existing methods and frameworks.The review of the relevant literature provides an encompassing overview of the opinions of the surveyed persons as well as the current state of academic research. By comparing the interview results with the current research approaches, the study reveals several discrepancies between the two, highlighting the need for further research and development in XAI.
</details></li>
</ul>
<hr>
<h2 id="DeePref-Deep-Reinforcement-Learning-For-Video-Prefetching-In-Content-Delivery-Networks"><a href="#DeePref-Deep-Reinforcement-Learning-For-Video-Prefetching-In-Content-Delivery-Networks" class="headerlink" title="DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks"></a>DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07881">http://arxiv.org/abs/2310.07881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nawras Alkassab, Chin-Tser Huang, Tania Lorido Botran</li>
<li>for: 这篇论文旨在提高内容传输网络（Content Delivery Networks，CDN）中视频内容的缓存和预取优化算法，以提高用户体验质量。</li>
<li>methods: 这篇论文提出了一种基于深度学习的预取优化算法，named DeePref，可以在CDN边缘网络中自动适应用户访问模式的变化，提高预取精度和覆盖率。</li>
<li>results: 实验结果表明，使用DeePref DRQN在实际世界数据集上，可以提高预取精度和覆盖率，相比基eline方法，提高28%和17%，同时也研究了将统计模型从一个边缘网络传输到另一个边缘网络，可以提高预取精度和覆盖率，相比基eline方法，提高30%和10%。<details>
<summary>Abstract</summary>
Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a prefetcher implemented on edge networks and is agnostic to hardware design, operating systems, and applications. Our results show that DeePref DRQN, using a real-world dataset, achieves a 17% increase in prefetching accuracy and a 28% increase in prefetching coverage on average compared to baseline approaches that use video content popularity as a building block to statically or dynamically make prefetching decisions. We also study the possibility of transfer learning of statistical models from one edge network into another, where unseen user requests from unknown distribution are observed. In terms of transfer learning, the increase in prefetching accuracy and prefetching coverage are [$30%$, $10%$], respectively. Our source code will be available on Github.
</details>
<details>
<summary>摘要</summary>
content delivery networks 承载了互联网上大量流量，而在互联网上占主要的流量中，视频内容的增长也使得缓存和预取优化算法变得越来越重要。预取的目的是在用户请求之前将数据提取到缓存中，以降低访问时间和提高用户体验质量。预取技术已经在操作系统、编译器指令、内存缓存、本地存储系统、高速网络和云系统中得到了广泛的研究。传统的预取技术通常是根据特定的访问模式进行适应，但是它们无法适应用户访问模式的快速变化或随机化。本文探讨了使用强化学习来解决这些变化的问题。为此，我们提出了DeePref，一种基于深度强化学习的在线视频内容预取代理。DeePref在边缘网络上实现，不依赖于硬件设计、操作系统或应用程序。我们的结果表明，DeePref DRQN使用实际数据集时，平均提高预取精度28%和预取覆盖率17% Compared to基eline方法，使用视频内容的流行度作为预取决策的基础或动态决策。我们还研究了将统计模型从一个边缘网络传播到另一个边缘网络中，以处理未经见过的用户请求和未知分布。在传播学习中，预取精度和预取覆盖率增加了[30%, 10%]。我们的源代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="TabLib-A-Dataset-of-627M-Tables-with-Context"><a href="#TabLib-A-Dataset-of-627M-Tables-with-Context" class="headerlink" title="TabLib: A Dataset of 627M Tables with Context"></a>TabLib: A Dataset of 627M Tables with Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07875">http://arxiv.org/abs/2310.07875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gus Eggert, Kevin Huo, Mike Biven, Justin Waugh</li>
<li>for: 这篇论文是为了提供一个大型、多样化的表格数据集，以便进行现代AI系统的研究和发展。</li>
<li>methods: 该论文使用了多种数据抽取方法，从GitHub和Common Crawl等来源中提取了627万个表格，总量达69 TiB，并且包含了867亿个上下文符号。</li>
<li>results: 该论文提出了一个名为”TabLib”的新的表格数据集，其包含了627万个表格，总量达69 TiB，并且具有多样化的表格结构和上下文。这个数据集的大小和多样性都提供了许多探索和研究的机会，类似于text和图像模态的基础数据集。<details>
<summary>Abstract</summary>
It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
</details>
<details>
<summary>摘要</summary>
“已经确立了现代人工智能系统中大量多样数据的重要作用。然而，对于表格数据，没有相关的大量多样数据集的存在，与文本和图像模式相似。因此，我们提出了“TabLib”，包含627万个表格，总量为69 TiB，以及867亿个上下文token。TabLib从多种文件格式中提取，包括CSV、HTML、SQLite、PDF、Excel等，来自GitHub和Common Crawl。TabLib的大小和多样性表现出了很大的抢救潜力，类似于文本和图像领域的基础数据集，如The Pile和LAION。”
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Pretraining-on-Multimodal-Electronic-Health-Records"><a href="#Hierarchical-Pretraining-on-Multimodal-Electronic-Health-Records" class="headerlink" title="Hierarchical Pretraining on Multimodal Electronic Health Records"></a>Hierarchical Pretraining on Multimodal Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07871">http://arxiv.org/abs/2310.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong, Yaqing Wang, Fenglong Ma</li>
<li>for: 这篇论文是为了解决医疗领域中电子健康记录（EHR）资料的层次结构问题，以提高现有预训模型在多种下游任务中的通用能力。</li>
<li>methods: 本文提出了一个新的、通用的、多modal EHR预训框架（MEDHMP），专门针对医疗领域中的层次结构资料进行预训。</li>
<li>results:  authors通过实验结果显示了MEDHMP的效果，在八个下游任务中三个层次上展示了佳绩，与十八个基准相比，更加强调了我们的方法的可行性。<details>
<summary>Abstract</summary>
Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>预训练技术在自然语言处理（NLP）中已经证明是一种强大的技术，在不同的NLP下游任务中表现出了很好的成绩。然而，在医疗领域，现有的预训练模型在电子医疗记录（EHR）数据上失去了层次结构的特点，因此限制了单个预训练模型在多种下游任务中的通用化能力。为解决这个挑战，本文提出了一种新的、通用、多模式预训练框架called MEDHMP，专门针对层次多模式EHR数据。我们通过对八个下游任务进行实验，证明了我们的方法的有效性。与 eighteen个基准值进行比较，我们的方法的成绩更加出色。
</details></li>
</ul>
<hr>
<h2 id="Cheap-Talking-Algorithms"><a href="#Cheap-Talking-Algorithms" class="headerlink" title="Cheap Talking Algorithms"></a>Cheap Talking Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07867">http://arxiv.org/abs/2310.07867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Condorelli, Massimiliano Furlan</li>
<li>for: 研究独立强化学习算法在 crawford 和 sobel (1982) 游戏中的信息传输行为。</li>
<li>methods:  sender 和 receiver 在训练中共同 converges to 离散的 Nash 均衡。</li>
<li>results: 通信占据预期的最大程度，即根据交战利益冲突的程度。结论是对不同参数和游戏 especification  robust。I hope this helps!<details>
<summary>Abstract</summary>
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
</details>
<details>
<summary>摘要</summary>
我们模拟独立强化学习算法在克劳福德和索贝尔（1982）游戏中的行为。我们显示，发送者和接收者一起培训时，会 converges到靠近预先优化的均衡点。因此，通信发生在预先优化的均衡点所预测的范围内。结论是对于不同的具体化参数和游戏参数，都是Robust的。我们讨论这些结论对信息传输游戏理论选择、计算机科学中算法之间的emerging通信以及人工智能代理人市场中的经济合作等方面的影响。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-Generation-with-Large-Language-Models-for-Text-Classification-Potential-and-Limitations"><a href="#Synthetic-Data-Generation-with-Large-Language-Models-for-Text-Classification-Potential-and-Limitations" class="headerlink" title="Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations"></a>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07849">http://arxiv.org/abs/2310.07849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin</li>
<li>for:  investigate the effectiveness of using large language models (LLMs) to generate synthetic datasets for text classification</li>
<li>methods:  use LLMs to generate synthetic data, and evaluate the performance of models trained on these synthetic data</li>
<li>results:  find that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic dataHere’s the full text in Simplified Chinese:</li>
<li>for: 这个研究是为了investigate大语言模型（LLMs）是否可以生成高质量的synthetic datasets，以便提高文本分类模型的性能。</li>
<li>methods: 研究者使用LLMs生成synthetic data，并评估这些synthetic data上模型的性能。</li>
<li>results: 发现任务级别和实例级别的主观性均对模型在synthetic data上的性能产生负面影响。<details>
<summary>Abstract</summary>
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.
</details>
<details>
<summary>摘要</summary>
collection and curation of high-quality training data 高质量训练数据的收集和整理是发展文本分类模型的表现优秀的关键因素，但它们frequently associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks.To better understand the factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we investigate how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data.We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.Here's the translation in Traditional Chinese:集成和整理高质量训练数据的收集是发展文本分类模型的表现优秀的关键因素，但它们经常与大量的成本和时间投入相关。研究人员最近尝试使用大语言模型（LLMs）生成 sintetic数据作为代替方案。然而，LLM生成的 sintetic数据在不同的分类任务中的效果是不一致的。为了更好地理解LLM生成 sintetic数据的效果的moderating因素，在这个研究中，我们investigate how the performance of models trained on these sintetic data may vary with the subjectivity of classification.我们的结果表明，在任务水平和实例水平都存在负相关性 между模型在 sintetic数据上的性能和分类的主观性。我们 conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Fundamental-Limits-of-Knowledge-Transfer-over-Finite-Domains"><a href="#Towards-the-Fundamental-Limits-of-Knowledge-Transfer-over-Finite-Domains" class="headerlink" title="Towards the Fundamental Limits of Knowledge Transfer over Finite Domains"></a>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07838">http://arxiv.org/abs/2310.07838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyue Zhao, Banghua Zhu</li>
<li>for: 本文研究了知识传递的统计效率，具体来说是通过$n$个教师样本传递到一个 probabilistic 学习器中，以便在输入空间$\mathcal{S}$上预测标签$\mathcal{A}$。</li>
<li>methods: 本文使用了三个进行知识传递的水平，它们分别是：只有硬标签信息（first level）、教师概率分布信息+硬标签信息（second level）和完整的soft labels信息（third level）。</li>
<li>results: 本文证明了，在第一个水平上，只有硬标签信息时，最优的最大 LIKElihood estimator 的渐近速率为 $\sqrt{|{\mathcal S}||{\mathcal A}|}&#x2F;{n}$。在第二个水平上，增加教师概率分布信息可以提高渐近速率的下界至 ${|{\mathcal S}||{\mathcal A}|}&#x2F;{n}$。在第三个水平上，使用完整的soft labels信息可以实现渐近速率 ${|{\mathcal S}|}&#x2F;{n}$ ，而且任何 Kullback-Leibler divergence 最小化器都是优选的。numerical simulations 验证了这些理论结论。<details>
<summary>Abstract</summary>
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{|{\mathcal S}||{\mathcal A}|}/{n}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${|{\mathcal S}||{\mathcal A}|}/{n}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of $|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be optimal in the last case. Numerical simulations distinguish the four learners and corroborate our theory.
</details>
<details>
<summary>摘要</summary>
我们Characterize了知识传输的统计效率通过$n$个教师到一个概率学生分类器的输入空间$\mathcal S$上的标签$\mathcal A$.我们显示了隐私信息的三个进步级别可以加速传输。在第一个级别上，只有硬标签是知道的，由最大likelihood估计达到最小最大值$\sqrt{|{\mathcal S}||{\mathcal A}|}/{n}$.在第二个级别上，教师标签的概率也可以获得，这会降低下界到${|{\mathcal S}||{\mathcal A}|}/{n}$.然而，在这个第二个数据收集协议下，直接适应cross-entropy损失会导致漫游学生。我们解决了这个局限性，并达到基本限制，使用一种新的实际变体的平方差logit损失。在第三个级别上，学生还获得了每个输入的完整logits(${\mathcal A}$)，这使得学生可以在${|{\mathcal S}|}/{n}$的时间内达到基本限制。我们发现任何Kullback-Leibler divergence最小化者是最佳的。数字实验证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="When-Why-and-How-Much-Adaptive-Learning-Rate-Scheduling-by-Refinement"><a href="#When-Why-and-How-Much-Adaptive-Learning-Rate-Scheduling-by-Refinement" class="headerlink" title="When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement"></a>When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07831">http://arxiv.org/abs/2310.07831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Defazio, Ashok Cutkosky, Harsh Mehta, Konstantin Mishchenko</li>
<li>for: 这个论文的目的是为了bridging theory和实践之间的差距，并为一类优化算法（包括SGD）提供新的问题适应学习率计划。</li>
<li>methods: 这篇论文使用了对一类优化算法的推论分析，并使用实际中的观测梯度 norm来Derive更加精细的学习率计划。</li>
<li>results: 论文的实验结果表明，linear decay schedule在10种深度学习问题中具有最好的性能，而且在一系列LLMs和一组логистиック回归问题中也有出色的表现。此外，论文还提供了一种自动实现学习率计划的系统方法，可以实现学习率温化和快速学习率降低。<details>
<summary>Abstract</summary>
Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate annealing near the end of training. Ours is the first systematic approach to automatically yield both of these properties. We perform the most comprehensive evaluation of learning rate schedules to date, evaluating across 10 diverse deep learning problems, a series of LLMs, and a suite of logistic regression problems. We validate that overall, the linear-decay schedule matches or outperforms all commonly used default schedules including cosine annealing, and that our schedule refinement method gives further improvements.
</details>
<details>
<summary>摘要</summary>
theory和实践中的学习率调度不符合，我们通过提出新的问题适应型学习率调度来减少这一差距。我们的关键技术贡献是对一类优化算法（包括SGD）的学习率调度进行了精细分析。与大多数前一些工作一样，我们研究了平均迭代点的渐进，但是我们强调研究最后一个迭代点，这是实际应用中人们常用的。在假设最坏情况下，我们的理论预测，最佳选择是线性衰减调度：一种广泛使用的做法，其中每个迭代点的步长与当前迭代次数/$T$ 成正比。为了超越这个最坏情况分析，我们使用观察到的梯度 norm 来 derive 更加细化的调度。这些细化调度具有学习率暖启和快速学习率缓和两个性能。我们是首个系统地自动实现这两个特性的系统。我们对10种深度学习问题、一系列LLMs以及一组logistic regression问题进行了最全面的学习率调度评估。我们证明，总的来说，线性衰减调度与所有通用的默认调度（包括cosine annealing）相当或超越。此外，我们的调度修正方法可以提供进一步的改进。
</details></li>
</ul>
<hr>
<h2 id="Does-Synthetic-Data-Make-Large-Language-Models-More-Efficient"><a href="#Does-Synthetic-Data-Make-Large-Language-Models-More-Efficient" class="headerlink" title="Does Synthetic Data Make Large Language Models More Efficient?"></a>Does Synthetic Data Make Large Language Models More Efficient?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07830">http://arxiv.org/abs/2310.07830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sia Gholami, Marwan Omar</li>
<li>for: 本研究旨在探讨深度学习方法在自然语言处理（NLP）领域中的应用，尤其是关于生成模板化问题的数据生成。</li>
<li>methods: 本研究使用模板基于的问题生成方法，以增加数据的多样性和数据量，并对现代变换器模型的性能进行评估。</li>
<li>results: 研究发现，使用模板基于的数据生成可以提高变换器模型的性能，但同时也存在风险的过拟合和模板限制的问题。<details>
<summary>Abstract</summary>
Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in harnessing synthetic data's potential, ensuring optimal model performance in diverse applications.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）在深度学习方法出现后已经经历了转变性变化。一个持续挑战研究人员的问题是数据质量的缺乏，这些模型驱动。这篇论文探讨了NLP中 sintetic数据生成的细节，特点是模板基于的问题生成。我们评估了这些优点，如数据增强潜力和结构化多样性的引入，并与内置的限制进行对比，如过拟合风险和预定模板所带来的限制。通过实验评估，我们证明了模板基于的 sintetic数据对现代转换器模型的性能产生了影响。我们结论，将synthetic数据和真实世界数据进行 equilibrio是NLP实践者需要注意的。将来，我们预计将在模型训练管道中集成synthetic数据，以便优化模型在多样化应用中的性能。这些发现希望能够引导NLP实践者在使用synthetic数据的潜力，以确保模型在多样化应用中的优秀性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-Analogy-Identification-and-Sentence-Structure-Encoding-in-Large-Language-Models"><a href="#Exploring-the-Relationship-between-Analogy-Identification-and-Sentence-Structure-Encoding-in-Large-Language-Models" class="headerlink" title="Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models"></a>Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07818">http://arxiv.org/abs/2310.07818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilini Wijesiriwardene, Ruwan Wickramarachchi, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das</li>
<li>for: 本研究旨在探讨现代自然语言处理（NLP）技术是如何识别句子对应关系，以及这种能力与语言模型（LLM）的句子结构编码能力之间的关系。</li>
<li>methods: 本研究使用多种大语言模型（LLM）来识别句子对应关系，并分析这些模型对句子结构的编码能力。</li>
<li>results: 研究发现，LLMs的句子对应关系识别能力与它们对句子结构的编码能力之间存在正相关性。具体来说， LLMS 更好地捕捉句子结构，它们也更具句子对应关系识别能力。<details>
<summary>Abstract</summary>
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LLMs is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.
</details>
<details>
<summary>摘要</summary>
理解对比在人类认知和语言能力中发挥着重要作用。过去十年，关于单词对比的研究得到了广泛的关注，但是现在越来越多的研究者关注 sentence对比，即 sentence A 和 sentence B 之间的对比。然而，当前的自然语言处理（NLP）研究社区正在评估大型自然语言模型（LLM）能否识别这类对比。尽管 LLM 的能力在识别对比方面的研究得到了广泛的关注，但是这些能力的下面原因仍然需要更深入的探究。此外， LLM 能够内嵌语言结构的能力也在不断受到关注，特别是它们可以内嵌 sentence 的语法和 semantics 结构。在这篇文章中，我们研究了多个 LLM 的对比 Identification 能力和它们内嵌语言结构的关系。我们发现，LLM 的对比 Identification 能力和它们内嵌语言结构的能力是正相关的。具体来说，我们发现 LLM 可以更好地捕捉语法结构的那些也有更高的对比 Identification 能力。
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-with-Phase-Stochastic-Bridges"><a href="#Generative-Modeling-with-Phase-Stochastic-Bridges" class="headerlink" title="Generative Modeling with Phase Stochastic Bridges"></a>Generative Modeling with Phase Stochastic Bridges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07805">http://arxiv.org/abs/2310.07805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianrong Chen, Jiatao Gu, Laurent Dinh, Evangelos A. Theodorou, Josh Susskind, Shuangfei Zhai</li>
<li>for: 这篇论文的目的是提出一种基于阶段空间动力学的生成模型框架，以便更好地生成连续输入数据。</li>
<li>methods: 该模型使用了Stochastic Differential Equation（SDE）和神经网络来实现逆向生成。具体来说，它首先在输入空间中定义了一个阶段空间，然后使用Stochastic Optimal Control的理念来建立一个路径度量，以便高效地采样数据。</li>
<li>results: 在标准图像生成测试 benchmark 上，该模型在小范围内的Number of Function Evaluations（NFEs）下表现出色，并且与使用有效采样技术的 diffusion models 的性能相当，这说明该模型有potential作为一种新的生成模型工具。<details>
<summary>Abstract</summary>
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs). Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential as a new tool generative modeling.
</details>
<details>
<summary>摘要</summary>
干扰模型（DM）表示现代生成模型的极限，它们在维度输入空间中构建了随机差分方程（SDE），并使用神经网络来逆向解决。在这项工作中，我们介绍了一种新的生成模型框架，基于phaspace动力学，phaspace是包括位置和速度的扩展空间。利用Stochastic Optimal Control的洞察，我们在phaspace中建立了一个路径度量，以便高效采样。与DMs不同，我们的框架在动力学征标的早期阶段就能生成真实数据点，这使得可以通过使用速度信息来加速数据生成。在标准图像生成标准benchmark上，我们的模型在小范围内的Number of Function Evaluations（NFEs）下表现优秀，并且与Diffusion Models配备高效采样技术相比，我们的方法在生成模型方面具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-general-mechanism-of-humor-reformulating-the-semantic-overlap"><a href="#A-general-mechanism-of-humor-reformulating-the-semantic-overlap" class="headerlink" title="A general mechanism of humor: reformulating the semantic overlap"></a>A general mechanism of humor: reformulating the semantic overlap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07803">http://arxiv.org/abs/2310.07803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Martínez</li>
<li>for: 提出一种通用的幽默机制，不限于语言交流。</li>
<li>methods: 基于礼物的概念，包括脱困和冲突的解决方法。</li>
<li>results: 提出了一种可以应用于非语言交流的幽默机制，并且认为这种机制是人类思维的核心。<details>
<summary>Abstract</summary>
This article proposes a cognitive mechanism of humour of general applicability, not restricted to verbal communication. It is indebted to Raskin's concept of script overlap, and conforms to the incongruity-resolution theoretical framework, but it is built on the notion of constraint, an abstract correspondence between sets of data. Under this view, script overlap is an outcome of a more abstractly described phenomenon, constraint overlap. The important concept of the overlooked argument is introduced to characterise the two overlapping constraints -- overt and covert. Their inputs and outputs are not directly encoded in utterances, but implicated by them, and their overlap results in another overlap at the level of the communicated utterances, that the incongruity reveals. Our hypothesis assumes as a given that the evocation of such constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. We base this assumption on Hofstadter's theory of analogy-making as the essence of human thought. By substituting "stimuli" of any kind for "utterances" in this model, we obtain a mechanism as easily applicable to non-verbal communication -- slapstick, cartoons -- and we propose it describes the necessary and sufficient conditions for a communicative act in any modality to carry humour.
</details>
<details>
<summary>摘要</summary>
The authors propose that the evocation of these constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. This idea is based on Hofstadter's theory of analogy-making as the essence of human thought. By applying this mechanism to non-verbal communication, such as slapstick or cartoons, the authors suggest that it provides a necessary and sufficient condition for a communicative act in any modality to carry humor.
</details></li>
</ul>
<hr>
<h2 id="An-Information-Bottleneck-Characterization-of-the-Understanding-Workload-Tradeoff"><a href="#An-Information-Bottleneck-Characterization-of-the-Understanding-Workload-Tradeoff" class="headerlink" title="An Information Bottleneck Characterization of the Understanding-Workload Tradeoff"></a>An Information Bottleneck Characterization of the Understanding-Workload Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07802">http://arxiv.org/abs/2310.07802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mycal-tucker/ib-explanations">https://github.com/mycal-tucker/ib-explanations</a></li>
<li>paper_authors: Lindsay Sanneman, Mycal Tucker, Julie Shah</li>
<li>for: 这篇论文旨在探讨人工智能系统的可解释性（XAI），以支持人类理解AI系统。</li>
<li>methods: 论文使用信息瓶颈方法（Information Bottleneck method）来自动生成抽象（hand-crafted groupings of related problem features），以平衡工作负荷和理解之间的 contradistinction。</li>
<li>results: 实验表明，通过抽象来解释复杂概念可以有效地Address和平衡工作负荷和理解之间的 contradistinction。<details>
<summary>Abstract</summary>
Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "artificial intelligence" is translated as "人工智能" (réngōng zhìnéng)* "explainable AI" is translated as "可解释人工智能" (kějìjiě xiǎng réngōng zhìnéng)* "human understanding" is translated as "人类理解" (réngrì lǐjiě)* "mental workload" is translated as "心理劳动" (xīn lǐ gōngzuò)* "information-theoretic approach" is translated as "信息理论方法" (xìnwù lǐlùn fāngfa)* "abstractions" is translated as "抽象" (chōuxiàng)* "hand-crafted groupings" is translated as "手工组合" (shǒu gōng zǔyì)* "problem features" is translated as "问题特征" (wèn tí tèchēng)* "workload-understanding tradeoff" is translated as "劳动-理解交换" (gōngzuò-lǐjiě gòuhuan)* "user-tailored XAI design" is translated as "用户定制XAI设计" (yònghòu dìngxì XAI jièdì)
</details></li>
</ul>
<hr>
<h2 id="Explainable-Attention-for-Few-shot-Learning-and-Beyond"><a href="#Explainable-Attention-for-Few-shot-Learning-and-Beyond" class="headerlink" title="Explainable Attention for Few-shot Learning and Beyond"></a>Explainable Attention for Few-shot Learning and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07800">http://arxiv.org/abs/2310.07800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahareh Nikpour, Narges Armanfard</li>
<li>for: 提高几何学模型的准确率和可靠性，特别在数据采集和标注过程中面临限制的情况下。</li>
<li>methods: 利用深度强化学习实现硬注意力找到，直接影响原始输入数据，使其可解释性提高。</li>
<li>results: 通过对多个 benchmark 数据集进行广泛的实验，证明我们提出的方法的效果。<details>
<summary>Abstract</summary>
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.
</details>
<details>
<summary>摘要</summary>
注意机制在增强学习模型方面表现出了扎实的潜力，特别是在数据采集和标注过程中存在困难时。 Drawing inspiration from human recognition processes, we argue that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, similar to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Based-Prognosis-Prediction-Paradigm-that-Bridges-Data-Distribution-Shift-across-EMR-Datasets"><a href="#A-Transfer-Learning-Based-Prognosis-Prediction-Paradigm-that-Bridges-Data-Distribution-Shift-across-EMR-Datasets" class="headerlink" title="A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets"></a>A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07799">http://arxiv.org/abs/2310.07799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongji Zhang, Yuhang Wang, Yinghao Zhu, Xinyu Ma, Tianlong Wang, Chaohe Zhang, Yasha Wang, Liantao Ma</li>
<li>for: 预测新疆突病和其他疾病的准确性</li>
<li>methods: 使用传输学习方法建立源数据集和目标数据集之间的转换模型，以适应不同任务域下的特征分布偏移问题</li>
<li>results: 比基eline方法高效，特别是在处理有限数据量时 display(“我们的方法可以更好地预测新疆突病和其他疾病。”)<details>
<summary>Abstract</summary>
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the distribution shift of features generated in disparate domains, domain-invariant features that are exclusively relative to downstream tasks are captured, so to cultivate a unified domain-invariant encoder across various task domains to achieve better feature representation. Experimental results of several target tasks demonstrate that our proposed model outperforms competing baseline methods and has higher rate of training convergence, especially in dealing with limited data amount. A multitude of experiences have proven the efficacy of our method to provide more accurate predictions concerning newly emergent pandemics and other diseases.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于疾病出现的信息有限，症状难以注意和识别，因此临床 intervención的窗口可能被忽略。一个有效的预测模型可以帮助医生确定病种和制定个性化的治疗方案，以便更快地预防不利的结果。然而，疾病的早期阶段，有限的数据收集和临床经验，加上隐私和伦理的担忧，可能导致参考数据的有限性，甚至数据标签难以正确地标注。此外，不同疾病或同一种疾病的不同来源的电子医疗记录（EMR）数据可能会导致严重的跨数据集特征不一致问题，大大降低深度学习模型的效率。这篇文章介绍了一种传输学习方法，用于从源数据集转移到目标数据集。通过限制不同领域中特征生成的分布shift，捕捉固有的领域特征，以便在不同任务领域中培养一个统一的领域特征不变的编码器，以达到更好的特征表示。实验结果表明，我们提出的方法在多个目标任务上表现出色，特别是在处理有限数据量时。多种经验证明了我们的方法在新出现的流行病和其他疾病预测方面的可靠性。
</details></li>
</ul>
<hr>
<h2 id="GenTKG-Generative-Forecasting-on-Temporal-Knowledge-Graph"><a href="#GenTKG-Generative-Forecasting-on-Temporal-Knowledge-Graph" class="headerlink" title="GenTKG: Generative Forecasting on Temporal Knowledge Graph"></a>GenTKG: Generative Forecasting on Temporal Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07793">http://arxiv.org/abs/2310.07793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruotong Liao, Xu Jia, Yunpu Ma, Volker Tresp</li>
<li>for: 用于替代传统的 embedding-based 和 rule-based 模型，并在 temporal knowledge graph 领域实现生成式预测。</li>
<li>methods: 提出了一种基于 retrieval 策略和 lightweight 参数efficient  instruciton tuning的生成式预测方法，named GenTKG。</li>
<li>results: 在低计算资源下，GenTKG 比传统方法有更高的预测性能，并且在未经重新训练的情况下，在未看到的数据集上也表现出了很好的转移性。<details>
<summary>Abstract</summary>
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting under low computation resources. GenTKG also highlights remarkable transferability with exceeding performance on unseen datasets without re-training. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs.
</details>
<details>
<summary>摘要</summary>
大Language Model (LLM)的快速进步使得temporal knowledge graph (tKG)领域受到了关注，而在这个领域，传统的经过设计的嵌入式和规则基本模型仍然主导。问题是， pré-trained LLMs能否理解结构化的时间关系数据，并取代它们作为时间关系预测的基本模型？因此，我们将 temporal knowledge forecasting 引入到生成设定中。然而，在复杂的时间图数据结构和Sequential natural expressions LLMs处理的大� Fischer  gap 和 tKGs的庞大数据量和轻量级 fine-tuning LLMs 的计算成本之间存在挑战。为解决这些挑战，我们提出了一种新的检索增强生成框架，名为 GenTKG，它结合了时间逻辑规则基本的检索策略和轻量级参数高效调整。我们的实验表明，GenTKG 在低计算资源下超过了传统的时间关系预测方法。GenTKG 还表现出了很好的转移性，在未看过的数据集上达到了 excel 的表现。我们的工作揭示了 LLMs 在 tKG 领域的巨大潜力，并开启了一个新的前ier  для generative forecasting on tKGs。
</details></li>
</ul>
<hr>
<h2 id="DrivingDiffusion-Layout-Guided-multi-view-driving-scene-video-generation-with-latent-diffusion-model"><a href="#DrivingDiffusion-Layout-Guided-multi-view-driving-scene-video-generation-with-latent-diffusion-model" class="headerlink" title="DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model"></a>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07771">http://arxiv.org/abs/2310.07771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofan Li, Yifu Zhang, Xiaoqing Ye</li>
<li>for: 提供高质量、大规模多视图视频数据，用于自动驾驶研究。</li>
<li>methods: 提出了一种基于 Bird’s-Eye-View（BEV）表示的协调扩散框架DrivingDiffusion，用于生成真实多视图视频。</li>
<li>results: 通过该框架，可以免费生成大规模、高质量多视图视频，用于驱动任务研究。<details>
<summary>Abstract</summary>
With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of multi-view images is ensured by information exchange between adjacent cameras. In the temporal model, we mainly query the information that needs attention in subsequent frame generation from the multi-view images of the first frame. We also introduce the local prompt to effectively improve the quality of generated instances. In post-processing, we further enhance the cross-view consistency of subsequent frames and extend the video length by employing temporal sliding window algorithm. Without any extra cost, our model can generate large-scale realistic multi-camera driving videos in complex urban scenes, fueling the downstream driving tasks. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
随着自动驾驶基于强大和统一的 bird's-eye-view (BEV) 表示的 популяр度增长，需要大量高质量多视图视频数据和准确的标注，但这些数据很难获得因为收集和标注成本高昂。为解决这个问题，我们提出了一个空间-时间一致的扩散框架 DrivingDiffusion，用于生成真实的多视图视频，控制了3D 布局。在生成多视图视频时，存在以下三个挑战：1）保持多视图视频之间的一致性和2）保持多帧视频之间的一致性？3）如何保证生成的实例质量？我们的 DrivingDiffusion 解决这些问题，通过将多视图单帧图像生成步骤、多camera共享的单视图视频生成步骤和后处理步骤，来生成真实的多视图视频。在多视图模型中，保证多视图图像之间的一致性，通过邻接相机之间的信息交换。在时间模型中，我们主要从多视图图像的首帧中提取需要注意的信息，并通过引入本地提示来提高生成的实例质量。在后处理步骤中，我们进一步提高了后续帧之间的一致性，并通过使用时间滑动窗口算法，延长视频的长度。无需额外成本，我们的模型可以生成大量高质量的多相机城市驾驶视频，为下游驾驶任务提供燃料。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="PAD-A-Dataset-and-Benchmark-for-Pose-agnostic-Anomaly-Detection"><a href="#PAD-A-Dataset-and-Benchmark-for-Pose-agnostic-Anomaly-Detection" class="headerlink" title="PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection"></a>PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07716">http://arxiv.org/abs/2310.07716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericlee0224/pad">https://github.com/ericlee0224/pad</a></li>
<li>paper_authors: Qiang Zhou, Weize Li, Lihan Jiang, Guoliang Wang, Guyue Zhou, Shanghang Zhang, Hao Zhao</li>
<li>for: 这个论文的目的是解决对象异常检测中的两个主要挑战：第一个是现有数据集缺乏完整的视觉信息，其中数据集通常假设训练和测试样本具有相同的pose angle，但在实际应用中，异常可能存在任何对象区域，需要研究无关于pose的异常检测。第二个挑战是对于无关于pose的异常检测的实验协议的缺乏一致性，这使得不同方法之间的比较不公平，阻碍了无关于pose的异常检测的研究。</li>
<li>methods: 作者们开发了一个名为Multi-pose Anomaly Detection（MAD）数据集和Pose-agnostic Anomaly Detection（PAD）benchmark，以解决上述两个挑战。Specifically，他们使用了20种复杂形状的LEGO玩具，包括4K视图，以及高质量和多样化的3D异常在 both simulated和real environments中。此外，作者们还提出了一种名为OmniposeAD的新方法，通过使用MAD进行训练，专门设计用于无关于pose的异常检测。</li>
<li>results: 作者们通过了全面的评估，证明了他们的数据集和方法的相关性。此外，他们还提供了一个开源的benchmark库，包括数据集和基eline方法，以便未来的研究和应用。代码、数据和模型都公开可用于<a target="_blank" rel="noopener" href="https://github.com/EricLee0224/PAD%E3%80%82">https://github.com/EricLee0224/PAD。</a><details>
<summary>Abstract</summary>
Object anomaly detection is an important problem in the field of machine vision and has seen remarkable progress recently. However, two significant challenges hinder its research and application. First, existing datasets lack comprehensive visual information from various pose angles. They usually have an unrealistic assumption that the anomaly-free training dataset is pose-aligned, and the testing samples have the same pose as the training data. However, in practice, anomaly may exist in any regions on a object, the training and query samples may have different poses, calling for the study on pose-agnostic anomaly detection. Second, the absence of a consensus on experimental protocols for pose-agnostic anomaly detection leads to unfair comparisons of different methods, hindering the research on pose-agnostic anomaly detection. To address these issues, we develop Multi-pose Anomaly Detection (MAD) dataset and Pose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to address the pose-agnostic anomaly detection problem. Specifically, we build MAD using 20 complex-shaped LEGO toys including 4K views with various poses, and high-quality and diverse 3D anomalies in both simulated and real environments. Additionally, we propose a novel method OmniposeAD, trained using MAD, specifically designed for pose-agnostic anomaly detection. Through comprehensive evaluations, we demonstrate the relevance of our dataset and method. Furthermore, we provide an open-source benchmark library, including dataset and baseline methods that cover 8 anomaly detection paradigms, to facilitate future research and application in this domain. Code, data, and models are publicly available at https://github.com/EricLee0224/PAD.
</details>
<details>
<summary>摘要</summary>
“物体异常检测是机器视觉领域的重要问题，最近有很大的进步。然而，两个主要挑战是阻碍其研究和应用。第一个是现有数据集缺乏全面的视觉信息，通常假设 anomaly-free 训练数据集是同一个 pose 的，测试样本也是同一个 pose。然而，在实际情况下，异常可能存在于对象任意区域，训练和查询样本可能有不同的 pose，需要研究无 pose 的异常检测。第二个是对pose-agnostic异常检测的实验室协议缺乏共识，导致不公正的比较，阻碍研究pose-agnostic异常检测。为解决这些问题，我们开发了Multi-pose Anomaly Detection（MAD）数据集和Pose-agnostic Anomaly Detection（PAD） benchmar，这是解决pose-agnostic异常检测问题的第一步。 Specifically，我们使用了20种复杂形状的 LEGO 玩具，包括4K 视图和各种 pose，以及高质量和多样化的3D 异常在 both simulated 和实际环境中。此外，我们提出了一种新的 OmniposeAD 方法，通过 MAD 训练而得，专门针对无 pose 异常检测。通过全面的评估，我们证明了我们的数据集和方法的相关性。此外，我们还提供了一个开源的benchmark库，包括dataset和基准方法，覆盖8种异常检测思想，以便未来的研究和应用。代码、数据和模型都公开可用于https://github.com/EricLee0224/PAD。”
</details></li>
</ul>
<hr>
<h2 id="InstructRetro-Instruction-Tuning-post-Retrieval-Augmented-Pretraining"><a href="#InstructRetro-Instruction-Tuning-post-Retrieval-Augmented-Pretraining" class="headerlink" title="InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining"></a>InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07713">http://arxiv.org/abs/2310.07713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro</li>
<li>for: 这个论文是为了研究预训练自然语言模型（LLM）的可靠性和精度，以及如何通过外部数据库来提高这些模型的性能。</li>
<li>methods: 这个论文使用了Retrieval方法来预训练LLM，并在这个基础模型上进行了更多的预训练和调教。</li>
<li>results: 论文的实验结果表明，使用Retrieval方法预训练LLM后，可以大幅提高模型的精度和可靠性，并且可以在零基础情况下进行成功的问答 tasks。<details>
<summary>Abstract</summary>
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning." into Simplified Chinese.Here's the translation:<<SYS>>预训自动循环大语言模型（LLM）与检索结合可以提高混淆率和事实准确率，通过利用外部数据库。然而，现有的预训检索增强LLM的大小仍然有限（例如，Retro有7.5亿参数），这限制了指令调整和零基数泛化的效iveness。在这种工作中，我们引入Retro 48B，最大化LLM预训检索后的指令调整。具体来说，我们继续预训43B GPT模型在additional 100亿个字符上，使用Retro增强方法，通过检索1.2万亿个字符来进行预训。获得的基础模型，Retro 48B，与原始43B GPT在混淆率上显著提高。在Retro上进行指令调整后，InstructRetro在零基数问答任务上表现出了显著提高。具体来说，InstructRetro在8个短形问答任务中平均提高7%，在4个挑战长形问答任务中提高10%。 surprisely，我们发现可以从InstructRetro架构中除去encoder，直接使用其decoder backbone，而 achieve comparable results。我们 hypothesize that预训检索使得其decoder可以好好地包含上下文。我们的结果显示了预训检索后GPT decoder的提高的可能性，并且 highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning。
</details></li>
</ul>
<hr>
<h2 id="Growing-Brains-Co-emergence-of-Anatomical-and-Functional-Modularity-in-Recurrent-Neural-Networks"><a href="#Growing-Brains-Co-emergence-of-Anatomical-and-Functional-Modularity-in-Recurrent-Neural-Networks" class="headerlink" title="Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks"></a>Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07711">http://arxiv.org/abs/2310.07711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Mikail Khona, Ila R. Fiete, Max Tegmark</li>
<li>for: 这个论文的目的是研究如何使用机器学习方法来实现脑模式下的神经网络结构。</li>
<li>methods: 这个论文使用的方法是一种名为“脑灵感模块化训练”（BIMT），它可以让神经网络中的神经元组织成功参与到同一些计算任务中，同时也可以使神经网络的性能更高。</li>
<li>results: 研究发现，通过使用BIMT训练神经网络，可以同时实现功能和结构的模块化，并且这些模块化的神经元也可以在不同的计算任务中保持一定的稳定性。此外，相比标准的$L_1$或无regularization设置，BIMT可以使神经网络的性能更高。<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) trained on compositional tasks can exhibit functional modularity, in which neurons can be clustered by activity similarity and participation in shared computational subtasks. Unlike brains, these RNNs do not exhibit anatomical modularity, in which functional clustering is correlated with strong recurrent coupling and spatial localization of functional clusters. Contrasting with functional modularity, which can be ephemerally dependent on the input, anatomically modular networks form a robust substrate for solving the same subtasks in the future. To examine whether it is possible to grow brain-like anatomical modularity, we apply a recent machine learning method, brain-inspired modular training (BIMT), to a network being trained to solve a set of compositional cognitive tasks. We find that functional and anatomical clustering emerge together, such that functionally similar neurons also become spatially localized and interconnected. Moreover, compared to standard $L_1$ or no regularization settings, the model exhibits superior performance by optimally balancing task performance and network sparsity. In addition to achieving brain-like organization in RNNs, our findings also suggest that BIMT holds promise for applications in neuromorphic computing and enhancing the interpretability of neural network architectures.
</details>
<details>
<summary>摘要</summary>
Recurrent neural networks (RNNs) 训练在compositional tasks上可以显示函数含量，在这些 neurons 可以被分为活动相似性和共享计算子任务中的集群。与大脑不同，这些 RNNs 不会显示解剖学含量，解剖学含量与强回路互连和功能集群的空间地域化强相关。与函数含量不同，解剖学含量可以在输入的影响下短暂地存在。为了检查是否可以培养大脑类似的解剖学含量，我们在一个解剖学含量训练（BIMT）中训练一个解剖学含量的网络，以解决一组compositional cognitive tasks。我们发现，功能相似的 neurons 不仅在活动上相似，还在空间上受到相似的归一化和连接。此外，相比标准 $L_1$ 或无规则化设置，模型在任务性能和网络稀缺性之间取得了优质平衡，并且表现出色。除了在 RNNs 中实现大脑类似的组织结构外，我们的发现还表明BIMT在 neuromorphic computing 和增强神经网络架构的解释性方面具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="Pixel-State-Value-Network-for-Combined-Prediction-and-Planning-in-Interactive-Environments"><a href="#Pixel-State-Value-Network-for-Combined-Prediction-and-Planning-in-Interactive-Environments" class="headerlink" title="Pixel State Value Network for Combined Prediction and Planning in Interactive Environments"></a>Pixel State Value Network for Combined Prediction and Planning in Interactive Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07706">http://arxiv.org/abs/2310.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Rosbach, Stefan M. Leupold, Simon Großjohann, Stefan Roth</li>
<li>for: 本研究旨在提高自动驾驶车辆在城市环境中的交通互动能力。</li>
<li>methods: 该研究提出了一种基于深度学习的方法，将预测和规划分别作为两个独立模块。 conditional GAN with U-Net architecture 是用于预测高分辨率图像序列的。</li>
<li>results: 研究结果表明，该方法可以在复杂的情况下，如车道变换 amidst conflicting objectives 中展现出直观的行为。<details>
<summary>Abstract</summary>
Automated vehicles operating in urban environments have to reliably interact with other traffic participants. Planning algorithms often utilize separate prediction modules forecasting probabilistic, multi-modal, and interactive behaviors of objects. Designing prediction and planning as two separate modules introduces significant challenges, particularly due to the interdependence of these modules. This work proposes a deep learning methodology to combine prediction and planning. A conditional GAN with the U-Net architecture is trained to predict two high-resolution image sequences. The sequences represent explicit motion predictions, mainly used to train context understanding, and pixel state values suitable for planning encoding kinematic reachability, object dynamics, safety, and driving comfort. The model can be trained offline on target images rendered by a sampling-based model-predictive planner, leveraging real-world driving data. Our results demonstrate intuitive behavior in complex situations, such as lane changes amidst conflicting objectives.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆在城市环境中必须可靠地与其他交通参与者交互。规划算法经常利用分离的预测模块预测 probabilistic、多模式和互动行为。将预测和规划分为两个模块会导致很多挑战，尤其是由于这两个模块之间的互相关系。这项工作提出了基于深度学习的方法，将预测和规划合并起来。使用 conditional GAN  WITH U-Net 架构，训练预测两个高分辨率图像序列。这两个序列表示明确的运动预测，主要用于训练上下文理解，以及适用于规划编码减速可能性、物体动力学、安全和驾驶舒适。模型可以在 target 图像上进行训练，使用采样基本的模拟预测规划器生成的图像，利用实际驾驶数据。我们的结果表明在复杂的情况下，如lane change amidst conflicting objectives， exhibit intuitive behavior。
</details></li>
</ul>
<hr>
<h2 id="From-Scarcity-to-Efficiency-Improving-CLIP-Training-via-Visual-enriched-Captions"><a href="#From-Scarcity-to-Efficiency-Improving-CLIP-Training-via-Visual-enriched-Captions" class="headerlink" title="From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions"></a>From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07699">http://arxiv.org/abs/2310.07699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao</li>
<li>for: 这项研究的目的是提高CLIP模型在大规模网络抓取数据上的训练效果，特别是减少Caption中的噪音和无关性。</li>
<li>methods: 该研究主要采用两种方法来改善Caption质量和多样性：首先，通过利用视觉概念来提高Caption中的视觉信息，以提高数据质量。其次，通过混合使用AltText和Visual-enriched Captions（VeC）进行混合训练，以提高数据多样性。</li>
<li>results: 研究表明，通过使用VeCLIP进行训练，可以在COCO和Flickr30k检索任务中提高模型的表现，比如在12M设定下，VeCLIP可以达到20%以上的提升。此外，通过使用更少的数据（只用14%的数据），VeCLIP还可以达到3%的提升。<details>
<summary>Abstract</summary>
Web-crawled datasets are pivotal to the success of pre-training vision-language models, exemplified by CLIP. However, web-crawled AltTexts can be noisy and potentially irrelevant to images, thereby undermining the crucial image-text alignment. Existing methods for rewriting captions using large language models (LLMs) have shown promise on small, curated datasets like CC3M and CC12M. Nevertheless, their efficacy on massive web-captured captions is constrained by the inherent noise and randomness in such data. In this study, we address this limitation by focusing on two key aspects: data quality and data variety. Unlike recent LLM rewriting techniques, we emphasize exploiting visual concepts and their integration into the captions to improve data quality. For data variety, we propose a novel mixed training scheme that optimally leverages AltTexts alongside newly generated Visual-enriched Captions (VeC). We use CLIP as one example and adapt the method for CLIP training on large-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive evaluation of VeCLIP across small, medium, and large scales of raw data. Our results show significant advantages in image-text alignment and overall model performance, underscoring the effectiveness of VeCLIP in improving CLIP training. For example, VeCLIP achieves a remarkable over 20% improvement in COCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency, we also achieve a notable over 3% improvement while using only 14% of the data employed in the vanilla CLIP and 11% in ALIGN.
</details>
<details>
<summary>摘要</summary>
“网络抓取的数据是预训模型的视觉语言模型的成功之关键，例如CLIP。但是网络抓取的AltText可能是噪音和无关的数据，从而妨碍关键的图片文本对齐。现有的方法使用大型自然语言模型（LLM）重写描述对图片的影响，在小型 curaated 数据集 LIKE CC3M 和 CC12M 中表现出了promise。然而，它们在大量网络抓取的数据集中的效果受到该数据的预设噪音和随机性的限制。在这些研究中，我们解决这个问题，专注在两个关键方面：数据质量和数据多样性。相比之下，现有的 LLM 重写方法，我们更强调使用视觉概念和它们与描述对图片的集成，以改善数据质量。对于数据多样性，我们提出了一个新的混合训练方案，可以最佳地利用AltText alongside 新生成的视觉增强描述（VeC）。我们使用 CLIP 为例，并将其训练到大规模网络抓取dataset上，称为 VeCLIP。我们对 VeCLIP 进行了全面的评估，包括小、中和大规模的数据。我们的结果显示，VeCLIP 在图片文本对齐和整体模型性能方面有着明显的改善，强调了 VeCLIP 在增强 CLIP 训练中的效果。例如，VeCLIP 在 COCO 和 Flickr30k 标签任务中的 Retrieval 性能中超过 20% 的提升，而且在使用 14% 的数据时，实现了与 vanilla CLIP 和 ALIGN 相比的 Notable 超过 3% 的改善。”
</details></li>
</ul>
<hr>
<h2 id="SurroCBM-Concept-Bottleneck-Surrogate-Models-for-Generative-Post-hoc-Explanation"><a href="#SurroCBM-Concept-Bottleneck-Surrogate-Models-for-Generative-Post-hoc-Explanation" class="headerlink" title="SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation"></a>SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07698">http://arxiv.org/abs/2310.07698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Pan, Zhenke Liu, Yifei Zhang, Liang Zhao</li>
<li>for: 本研究旨在提供一种自动发现概念的抽象模型，以便解释黑盒模型的决策过程。</li>
<li>methods: 本研究使用了概念活动 вектор (CAV) 和概念瓶颈模型 (CBM)，以及一种新的抽象模型——抽象瓶颈模型 (SurroCBM)。</li>
<li>results: 经过广泛的实验，研究发现SurroCBM可以有效地找到黑盒模型中的概念，并提供高质量的解释。<details>
<summary>Abstract</summary>
Explainable AI seeks to bring light to the decision-making processes of black-box models. Traditional saliency-based methods, while highlighting influential data segments, often lack semantic understanding. Recent advancements, such as Concept Activation Vectors (CAVs) and Concept Bottleneck Models (CBMs), offer concept-based explanations but necessitate human-defined concepts. However, human-annotated concepts are expensive to attain. This paper introduces the Concept Bottleneck Surrogate Models (SurroCBM), a novel framework that aims to explain the black-box models with automatically discovered concepts. SurroCBM identifies shared and unique concepts across various black-box models and employs an explainable surrogate model for post-hoc explanations. An effective training strategy using self-generated data is proposed to enhance explanation quality continuously. Through extensive experiments, we demonstrate the efficacy of SurroCBM in concept discovery and explanation, underscoring its potential in advancing the field of explainable AI.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化中文Explainable AI 目的是帮助解释黑盒模型的决策过程。传统的焦点分析方法可以高亮影响数据段的数据，但 часто缺乏含义理解。最新的进展，如概念活化 вектор (CAV) 和概念瓶颈模型 (CBM)，提供了基于概念的解释，但需要人类定义的概念。然而，人类标注的概念是贵重的获得。这篇论文介绍了概念瓶颈代理模型 (SurroCBM)，一种新的框架，旨在通过自动发现的概念来解释黑盒模型。SurroCBM 找到黑盒模型中共享和唯一的概念，并使用可解释的代理模型进行后续解释。通过自动生成的数据进行有效的训练策略，以不断提高解释质量。经过广泛的实验，我们证明 SurroCBM 在概念发现和解释方面具有强大的效果，这对于解释 AI 领域的进步产生了潜在的影响。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Neural-Networks-through-the-Lens-of-Message-Passing-A-Common-Perspective-to-Homophily-and-Architecture-Design"><a href="#Hypergraph-Neural-Networks-through-the-Lens-of-Message-Passing-A-Common-Perspective-to-Homophily-and-Architecture-Design" class="headerlink" title="Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design"></a>Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07684">http://arxiv.org/abs/2310.07684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lev Telyatnikov, Maria Sofia Bucarelli, Guillermo Bernardez, Olga Zaghen, Simone Scardapane, Pietro Lio</li>
<li>for: 本文旨在探讨hypergraph neural network（HGNN）领域中缺乏的一些问题，如何使用homophily概念来提高HGNN的性能，是否可以通过特性化高阶网络的特点来提高现有的hypergraph架构和方法ологи？</li>
<li>methods: 本文提出了一种基于消息传递的homophily概念，并提出了一种新的消息传递框架MultiSet，该框架允许 hyperedge-dependent 节点表示，同时提出了一种新的hyperedge sampling策略。</li>
<li>results: 本文通过了一系列实验，以便更好地理解高阶网络表示学习中的复杂性，并提供了一些有价值的发现。<details>
<summary>Abstract</summary>
Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, simultaneously leading to overshadowing hypergraph network foundations. This paper attempts to confront some pending questions in that regard: Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HGNNs), similar to its significance in graph-based research? Is there room for improving current hypergraph architectures and methodologies? (e.g. by carefully addressing the specific characteristics of higher-order networks) Do existing datasets provide a meaningful benchmark for HGNNs? Diving into the details, this paper proposes a novel conceptualization of homophily in higher-order networks based on a message passing scheme; this approach harmonizes the analytical frameworks of datasets and architectures, offering a unified perspective for exploring and interpreting complex, higher-order network structures and dynamics. Further, we propose MultiSet, a novel message passing framework that redefines HGNNs by allowing hyperedge-dependent node representations, as well as introduce a novel architecture MultiSetMixer that leverages a new hyperedge sampling strategy. Finally, we provide an extensive set of experiments that contextualize our proposals and lead to valuable insights in hypergraph representation learning.
</details>
<details>
<summary>摘要</summary>
现有大多数超граphp学习方法和标准库在超граphp领域都是由从 graf 的方法提取而来，这同时导致了超граphp网络基础知识的遮蔽。本文尝试解答一些尚未得到回答的问题：在超граphp网络中，课程同义性（homophily）是否可以扮演重要的角色？现有的方法是否可以进一步改进？（例如，特别是适当处理更高阶的网络特性）？现有的标准库是否提供了有意义的底线 для HGNN？进一步详细的说明，本文提出了一种基于Message Passing scheme的新的同义性概念，这种方法可以让网络结构和架构之间的分析框架相互融合，从而获得了复杂的高阶网络结构和动态的全面探索和解释。此外，我们提出了MultiSet Message Passing框架，这是一种允许阶层висимы node representation的新型Message Passing框架，以及一种MultiSetMixer架构，它利用了一种新的阶层抽样策略。最后，我们提供了一系列实验，以Contextualize我们的提议并带来宝贵的见解在超граphp表现学习领域。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Data-Generation-Via-Iterative-Data-Property-Mutual-Mappings"><a href="#Controllable-Data-Generation-Via-Iterative-Data-Property-Mutual-Mappings" class="headerlink" title="Controllable Data Generation Via Iterative Data-Property Mutual Mappings"></a>Controllable Data Generation Via Iterative Data-Property Mutual Mappings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07683">http://arxiv.org/abs/2310.07683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Pan, Muran Qin, Shiyu Wang, Yifei Zhang, Liang Zhao</li>
<li>for: 这 paper 的目的是提高 VAE 基于的数据生成器的可控性和分离性。</li>
<li>methods: 这 paper 提出了一种通用的框架，用于增强 VAE 基于的数据生成器的可控性和分离性。该框架通过提出一种新的目标函数，可以在训练集和测试集上进行优化，并通过迁移映射来实现 semi-supervised 训练。</li>
<li>results: 实验结果表明，这 paper 的提出的框架可以更好地控制生成的样本的属性，保证分离性和生成质量，同时减少训练时间。<details>
<summary>Abstract</summary>
Deep generative models have been widely used for their ability to generate realistic data samples in various areas, such as images, molecules, text, and speech. One major goal of data generation is controllability, namely to generate new data with desired properties. Despite growing interest in the area of controllable generation, significant challenges still remain, including 1) disentangling desired properties with unrelated latent variables, 2) out-of-distribution property control, and 3) objective optimization for out-of-distribution property control. To address these challenges, in this paper, we propose a general framework to enhance VAE-based data generators with property controllability and ensure disentanglement. Our proposed objective can be optimized on both data seen and unseen in the training set. We propose a training procedure to train the objective in a semi-supervised manner by iteratively conducting mutual mappings between the data and properties. The proposed framework is implemented on four VAE-based controllable generators to evaluate its performance on property error, disentanglement, generation quality, and training time. The results indicate that our proposed framework enables more precise control over the properties of generated samples in a short training time, ensuring the disentanglement and keeping the validity of the generated samples.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "disentanglement" is translated as "独立性" (pinyin: dān zì xìng)* "out-of-distribution" is translated as "非典型" (pinyin: fēi diǎn tí)* "objective optimization" is translated as "目标优化" (pinyin: mù bǎo yǎo jī)* "semi-supervised" is translated as "半supervised" (pinyin: bàn shū fù)* "mutual mappings" is translated as "互相映射" (pinyin: xiāng xiāng diāng xiǎng)
</details></li>
</ul>
<hr>
<h2 id="Explainable-Image-Similarity-Integrating-Siamese-Networks-and-Grad-CAM"><a href="#Explainable-Image-Similarity-Integrating-Siamese-Networks-and-Grad-CAM" class="headerlink" title="Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM"></a>Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07678">http://arxiv.org/abs/2310.07678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis E. Livieris, Emmanuel Pintelas, Niki Kiriakidou, Panagiotis Pintelas</li>
<li>for: 提高图像相似性评估中的可读性、可信度和用户接受度</li>
<li>methods:  integrate Siamese Networks and Grad-CAM for providing explainable image similarity, along with visual factual and counterfactual explanations</li>
<li>results: 提供了一种可解释的图像相似性评估方法，具有可读性、可信度和用户接受度，并且可以帮助决策 maker 更好地理解图像相似性的原因。<details>
<summary>Abstract</summary>
With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustworthiness and user acceptance of image-based systems in real-world image similarity applications. The implementation code can be found in https://github.com/ioannislivieris/Grad_CAM_Siamese.git.
</details>
<details>
<summary>摘要</summary>
We present a new framework that integrates Siamese Networks and Grad-CAM for providing explainable image similarity. The proposed approach has the potential to enhance the interpretability, trustworthiness, and user acceptance of image-based systems in real-world image similarity applications.The key idea of our approach is to provide visual explanations for the similarity scores, which can help users understand why two images are considered similar. We use Siamese Networks to compare the input images and Grad-CAM to provide visual explanations. The Siamese Networks are trained to learn a similarity metric between the input images, and the Grad-CAM is used to provide visual explanations for the similarity scores.The proposed approach has several potential benefits, including:1. Interpretability: The proposed approach provides visual explanations for the similarity scores, which can help users understand why two images are considered similar.2. Trustworthiness: The use of visual explanations can increase the trustworthiness of the image similarity model, as users can see the reasons behind the similarity scores.3. User acceptance: The proposed approach can enhance the user acceptance of image-based systems, as users can understand the reasons behind the similarity scores and make more informed decisions.However, there are also some challenges associated with the proposed approach, such as:1. Computational complexity: The proposed approach may have higher computational complexity compared to traditional image similarity models, as it requires the use of Siamese Networks and Grad-CAM.2. Training time: Training the Siamese Networks and Grad-CAM may require more time and resources compared to traditional image similarity models.Despite these challenges, the proposed approach has the potential to provide more accurate and interpretable image similarity measures, which can enhance the interpretability, trustworthiness, and user acceptance of image-based systems in real-world image similarity applications. The implementation code can be found in the provided GitHub repository.
</details></li>
</ul>
<hr>
<h2 id="Accountability-in-Offline-Reinforcement-Learning-Explaining-Decisions-with-a-Corpus-of-Examples"><a href="#Accountability-in-Offline-Reinforcement-Learning-Explaining-Decisions-with-a-Corpus-of-Examples" class="headerlink" title="Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples"></a>Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07747">http://arxiv.org/abs/2310.07747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun, Alihan Hüyük, Daniel Jarrett, Mihaela van der Schaar</li>
<li>for: 该论文旨在提高决策系统中的透明性和可解释性，以减少实际应用中的风险。</li>
<li>methods: 该论文提出了帐户able Offline Controller（AOC），使用停机集成 dataset 作为决策 corpo，通过选择特定的示例（称为 Corpus Subset）进行可读性控制，并可以在低数据情况下进行有效的控制。</li>
<li>results: 该论文在 simulate 和实际医疗应用场景中评估了 ABC 的性能，发现它可以在低数据情况下进行高水平的控制，同时保持责任。<details>
<summary>Abstract</summary>
Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.   Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement Learning, Reinforcement Learning Transparency, Offline Reinforcement Learning, Batched Control.
</details>
<details>
<summary>摘要</summary>
学习透明、可解释的控制器，使用停机数据来做决策系统是一个重要的研究领域，因为它可以减少实际系统中的风险。然而，在责任感知的设置中，如医疗保健，决策负责任是 Paramount 的，然而文献中没有充分Addressed。这篇论文介绍了受责任的停机控制器（AOC），该控制器使用停机集合（Decision Corpus）中的数据进行负责任的控制，并通过特制的例子选择（Corpus Subset）来实现可解释的控制。ABC可以在低数据情况下效果地进行控制，可以扩展到纯粹的停机模仿设置，并显示出保守和适应的特点。我们在模拟和实际医疗enario中评估ABC的性能，强调它在低数据情况下完成停机控制任务的高水平性能，同时保持负责任。Keywords: 可解释的强化学习, 可解释的决策学习, 强化学习透明度, 停机强化学习, Batched Control.
</details></li>
</ul>
<hr>
<h2 id="HaarNet-Large-scale-Linear-Morphological-Hybrid-Network-for-RGB-D-Semantic-Segmentation"><a href="#HaarNet-Large-scale-Linear-Morphological-Hybrid-Network-for-RGB-D-Semantic-Segmentation" class="headerlink" title="HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D Semantic Segmentation"></a>HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07669">http://arxiv.org/abs/2310.07669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rick Groenendijk, Leo Dorst, Theo Gevers</li>
<li>for: 这篇论文旨在提出一种混合线性-形态网络（HaarNet），以优化多模态输入数据的处理。</li>
<li>methods: 这篇论文使用了 morphological elements 和 linear modules 构建混合网络，并在网络层次中应用 morphological Haar sampling 和 morphologically parameterized ReLU。</li>
<li>results: 实验表明，HaarNet 与 state-of-the-art CNN 相比竞争，表明形态网络是 geometry-based learning 任务的一个有前途的研究方向。<details>
<summary>Abstract</summary>
Signals from different modalities each have their own combination algebra which affects their sampling processing. RGB is mostly linear; depth is a geometric signal following the operations of mathematical morphology. If a network obtaining RGB-D input has both kinds of operators available in its layers, it should be able to give effective output with fewer parameters. In this paper, morphological elements in conjunction with more familiar linear modules are used to construct a mixed linear-morphological network called HaarNet. This is the first large-scale linear-morphological hybrid, evaluated on a set of sizeable real-world datasets. In the network, morphological Haar sampling is applied to both feature channels in several layers, which splits extreme values and high-frequency information such that both can be processed to improve both modalities. Moreover, morphologically parameterised ReLU is used, and morphologically-sound up-sampling is applied to obtain a full-resolution output. Experiments show that HaarNet is competitive with a state-of-the-art CNN, implying that morphological networks are a promising research direction for geometry-based learning tasks.
</details>
<details>
<summary>摘要</summary>
信号从不同模式性each有自己的组合代数，它们对采样处理有各自的影响。RGB mostly linear; depth是一种几何信号，其操作符遵循数学形态学的操作。如果一个网络从RGB-D输入中获得了这两种操作符，那么它应该能够通过 fewer parameters 提供有效的输出。在这篇论文中，作者使用了 morphological elements 和更familiar linear modules  construct a mixed linear-morphological network called HaarNet。这是首次大规模的linear-morphological hybrid，在一些实际世界数据集上进行了评估。在网络中，morphological Haar sampling 被应用到了多个层中的 feature channels，从而将极值和高频信息分割，以便进行改进两个模式。此外，使用了 morphologically parameterised ReLU 和 morphologically-sound up-sampling 来获得全分辨率输出。实验表明，HaarNet 与一个state-of-the-art CNN 相当竞争，表明geometry-based learning task 中的 morphological networks 是一个有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="GRaMuFeN-Graph-based-Multi-modal-Fake-News-Detection-in-Social-Media"><a href="#GRaMuFeN-Graph-based-Multi-modal-Fake-News-Detection-in-Social-Media" class="headerlink" title="GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media"></a>GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07668">http://arxiv.org/abs/2310.07668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makan Kananian, Fatima Badiei, S. AmirAli Gh. Ghahramani</li>
<li>for: 检测假新闻</li>
<li>methods: 使用文本encoder和图像encoder，其中文本encoder为图 convolutional neural network (GCN)，图像encoder为预训练的 ResNet-152 Convolutional Neural Network (CNN)，并实现对比性 Similarity损失函数。</li>
<li>results: 在两个公开available的社交媒体新闻数据集上进行了广泛的评估，与现有状态之最模型相比，提高了10%的微 F1-Score，表明GCN和CNN模型在多 modal 数据中检测假新闻的效果。<details>
<summary>Abstract</summary>
The proliferation of social media platforms such as Twitter, Instagram, and Weibo has significantly enhanced the dissemination of false information. This phenomenon grants both individuals and governmental entities the ability to shape public opinions, highlighting the need for deploying effective detection methods. In this paper, we propose GraMuFeN, a model designed to detect fake content by analyzing both the textual and image content of news. GraMuFeN comprises two primary components: a text encoder and an image encoder. For textual analysis, GraMuFeN treats each text as a graph and employs a Graph Convolutional Neural Network (GCN) as the text encoder. Additionally, the pre-trained ResNet-152, as a Convolutional Neural Network (CNN), has been utilized as the image encoder. By integrating the outputs from these two encoders and implementing a contrastive similarity loss function, GraMuFeN achieves remarkable results. Extensive evaluations conducted on two publicly available benchmark datasets for social media news indicate a 10 % increase in micro F1-Score, signifying improvement over existing state-of-the-art models. These findings underscore the effectiveness of combining GCN and CNN models for detecting fake news in multi-modal data, all while minimizing the additional computational burden imposed by model parameters.
</details>
<details>
<summary>摘要</summary>
“社交媒体平台的普及，如Twitter、Instagram和微博，已经增强了假信息的传播。这个现象让个人和政府机构都能够形成公众的意见，高lighting the need for deploying effective detection methods。在这篇论文中，我们提出了GraMuFeN模型，用于检测假新闻。GraMuFeN包括两个主要组成部分：文本编码器和图像编码器。对于文本分析，GraMuFeN将每个文本视为一个图形，并使用文本图形convNet（GCN）作为文本编码器。此外，预训ResNet-152作为单元神经网络（CNN），用于图像编码器。通过融合这两个编码器的输出，并实现一个对照相似度损失函数，GraMuFeN可以取得了很好的效果。在两个公开可用的社交媒体新闻 benchmark 数据集上进行了广泛的评估，发现GraMuFeN比前一代模型提高了10%的微 F1-Score，这说明GraMuFeN在多modal数据中检测假新闻的效果。”
</details></li>
</ul>
<hr>
<h2 id="Global-Minima-Recoverability-Thresholds-and-Higher-Order-Structure-in-GNNS"><a href="#Global-Minima-Recoverability-Thresholds-and-Higher-Order-Structure-in-GNNS" class="headerlink" title="Global Minima, Recoverability Thresholds, and Higher-Order Structure in GNNS"></a>Global Minima, Recoverability Thresholds, and Higher-Order Structure in GNNS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07667">http://arxiv.org/abs/2310.07667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Drake Brown, Trevor Garrity, Kaden Parker, Jason Oliphant, Stone Carson, Cole Hanson, Zachary Boyd</li>
<li>for: 本文研究Graph Neural Network（GNN）架构的性能从Random Graph Theory的角度，并将其与训练数据的典型特性连接起来。</li>
<li>methods: 作者通过理论分析和数值实验来研究GNN的性能，并提供了GCN、GAT、SAGE和Graph Transformer等四种不同的GNN架构的 recoverability threshold的数值结果。</li>
<li>results: 研究发现，重 tailed degree distribution可以提高GNN性能，GNN可以在强烈不同类型的图上工作，SAGE和Graph Transformer可以在无isy edge数据上工作，但是没有架构能够处理足够噪音特征数据。此外，作者发现了 especific higher-order structures在 sintetic data中和实际数据中对GNN性能的负面影响。<details>
<summary>Abstract</summary>
We analyze the performance of graph neural network (GNN) architectures from the perspective of random graph theory. Our approach promises to complement existing lenses on GNN analysis, such as combinatorial expressive power and worst-case adversarial analysis, by connecting the performance of GNNs to typical-case properties of the training data. First, we theoretically characterize the nodewise accuracy of one- and two-layer GCNs relative to the contextual stochastic block model (cSBM) and related models. We additionally prove that GCNs cannot beat linear models under certain circumstances. Second, we numerically map the recoverability thresholds, in terms of accuracy, of four diverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under a variety of assumptions about the data. Sample results of this second analysis include: heavy-tailed degree distributions enhance GNN performance, GNNs can work well on strongly heterophilous graphs, and SAGE and Graph Transformer can perform well on arbitrarily noisy edge data, but no architecture handled sufficiently noisy feature data well. Finally, we show how both specific higher-order structures in synthetic data and the mix of empirical structures in real data have dramatic effects (usually negative) on GNN performance.
</details>
<details>
<summary>摘要</summary>
我们从Random graph theory的角度分析图内 neural network（GNN）的性能。我们的方法可以补充现有的GNN分析方法，如 combinatorial expressive power和worst-case adversarial analysis，并将GNN的性能连接到训练数据的典型特性。首先，我们理论上 caracterize一layer和二layerGCNs的节点精度相对Contextual stochastic block model（cSBM）和相关模型。此外，我们还证明了GCNs不能在某些情况下超过线性模型。第二，我们 numerically mapping GNN architectures（GCN、GAT、SAGE和Graph Transformer）在不同假设下的恢复阈值，以及它们在不同数据下的性能。样例结果包括：重 tailed degree distribution enhance GNN performance，GNNs can work well on strongly heterophilous graphs，和SAGE和Graph Transformer可以在任意噪音边数据上表现良好，但没有任何architecture可以好好处理噪音特征数据。最后，我们表明了特定的高阶结构在 sintetic data中和实际数据中的混合效应（通常是负面的）对GNN性能的影响。
</details></li>
</ul>
<hr>
<h2 id="Deep-Backtracking-Counterfactuals-for-Causally-Compliant-Explanations"><a href="#Deep-Backtracking-Counterfactuals-for-Causally-Compliant-Explanations" class="headerlink" title="Deep Backtracking Counterfactuals for Causally Compliant Explanations"></a>Deep Backtracking Counterfactuals for Causally Compliant Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07665">http://arxiv.org/abs/2310.07665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Klaus-Rudolf Kladny, Julius von Kügelgen, Bernhard Schölkopf, Michael Muehlebach</li>
<li>For: The paper is written for discussing the concept of backtracking counterfactuals in structural causal models, and introducing a practical method for computing them using deep generative components.* Methods: The paper uses structural causal models with deep generative components to compute backtracking counterfactuals, and imposes conditions on the structural assignments to enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of the model.* Results: The paper demonstrates the effectiveness of the proposed method experimentally on a modified version of MNIST and CelebA, showing that it is a versatile, modular, and causally compliant alternative to methods in the field of counterfactual explanations.<details>
<summary>Abstract</summary>
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. We demonstrate these properties experimentally on a modified version of MNIST and CelebA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>> counterfactuals可以提供有价值的洞察，回答在修改条件下观察到的结果，这是基于一个事实观察的。而经典的干预解释方法已经广泛研究，而backtracking作为一种更少研究的 alternatively，backtracking原理保持所有 causal laws intact。在当前的工作中，我们提出了计算 backtracking counterfactuals 的实用方法，该方法在结构 causal 模型中包含深度生成组件。为此，我们在结构分配中加入了条件，以便通过解决一个可解决的具有约束的优化问题来生成 counterfactuals。我们的形式还使得可以与对 counterfactual explanations 的方法进行比较。相比之下，我们的方法表现出了 versatile、modular 和 causally compliant 的性质。我们在一个修改后的 MNIST 和 CelebA 上实验ally示出了这些性质。
</details></li>
</ul>
<hr>
<h2 id="Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models"><a href="#Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models" class="headerlink" title="Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models"></a>Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07653">http://arxiv.org/abs/2310.07653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zeqiang-Lai/MiniDALLE-3">https://github.com/Zeqiang-Lai/MiniDALLE-3</a></li>
<li>paper_authors: Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang<br>for: 这研究旨在提高人机交互中图像质量和文本匹配性，并且使用自然语言描述来与LLM进行交互。methods: 该研究使用了LLM的扩展和提升技术，以及现有的文本到图像模型。results: 研究表明，该方法可以轻松地引入iT2I能力到任何现有的LLM和文本到图像模型中，而无需任何训练，同时带来了很小的降低效果对LLM的内置能力。<details>
<summary>Abstract</summary>
The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs' inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems.
</details>
<details>
<summary>摘要</summary>
人工智能内容生成革命得到了激素加速，文本到图像（T2I）扩散模型在两年内得到了前所未有的高质量、多样性和创新。然而，在使用自然语言描述与这些流行的T2I模型进行有效交流中，仍然存在一定的限制。通常需要专业的提示工程师技能，例如复杂的单词组合、魔术标签和注释，才能获得吸引人的图像。为了解决这个问题，我们受到最新的DALLE3——直接在ChatGPT上构建的T2I模型的发布 inspirited，我们返回了现有的T2I系统，努力将人类意图与T2I模型 align，并提出了一个新任务——交互文本到图像（iT2I）。在解决iT2I问题时，我们提出了一种简单的方法，通过提示技术和存在的T2I模型来增强LLMs的iT2I能力。我们在不同的LLLMs（如ChatGPT、LLAMA、Baichuan和InternLM）下进行了多种常见场景的评估。我们示出了我们的方法可以在不需要训练的情况下，将任何现有的LLLMs和任何文本到图像模型具备iT2I能力，而且对LLLMs的内置能力，如问答和代码生成，带来了微不足道的影响。我们希望这种工作能吸引更广泛的关注，并为下一代T2I系统的图像质量和人机交互体验提供灵感。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-the-BERT-like-Pretraining-for-DNA-Sequences"><a href="#Rethinking-the-BERT-like-Pretraining-for-DNA-Sequences" class="headerlink" title="Rethinking the BERT-like Pretraining for DNA Sequences"></a>Rethinking the BERT-like Pretraining for DNA Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07644">http://arxiv.org/abs/2310.07644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Liang, Weiqiang Bai, Lifeng Qiao, Yuchen Ren, Jianle Sun, Peng Ye, Hongliang Yan, Xinzhu Ma, Wangmeng Zuo, Wanli Ouyang</li>
<li>for: 本研究的目的是提高生物科学领域中BERT预训的性能，以及开发一种适应生物科学领域的BERT预训方法。</li>
<li>methods: 本研究使用了一种新的RandomMask方法，它通过逐渐增加BERT预训的任务难度，让模型学习更多的知识。</li>
<li>results:  RandomMask方法可以在26个数据集中的28个任务中实现顶尖的性能，并且比 existed BERT预训方法更具有应用价值。<details>
<summary>Abstract</summary>
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, while using K-mer non-overlapping tokenization results in less distinct embeddings and continuously decreases the loss. 3) Using overlapping tokenization causes the self-attention in the intermediate layers of pre-trained models to tend to overly focus on certain tokens, reflecting that these layers are not adequately optimized. In summary, overlapping tokenization can benefit the fine-tuning of downstream tasks but leads to inadequate pretraining with fast convergence. To unleash the pretraining potential, we introduce a novel approach called RandomMask, which gradually increases the task difficulty of BERT-like pretraining by continuously expanding its mask boundary, forcing the model to learn more knowledge. RandomMask is simple but effective, achieving top-tier performance across 26 datasets of 28 datasets spanning 7 downstream tasks.
</details>
<details>
<summary>摘要</summary>
随着大规模预训练在自然语言处理（NLP）中的成功，它在生命科学领域也在普及。特别是基于DNA序列的预训练方法在Capture generic information about genes的potential方面受到了越来越多的关注。然而，现有的DNA序列预训练方法主要基于直接从NLP中采用BERT预训练的方法，lacking a comprehensive understanding and a specifically tailored approach。为了弥足这个研究差距，我们首先进行了一系列的探索性实验，获得了一些有价值的观察：1）在下游任务 fine-tuning阶段，使用K-mer overlaping tokenization而不是K-mer non-overlapping tokenization， both overlapping and non-overlapping pretraining weights show consistent performance improvement。2）在预训练过程中，使用K-mer overlaping tokenization快速生成明确的K-mer embeddings，并将损失降到非常低水平，而使用K-mer non-overlapping tokenization则 produces less distinct embeddings and continuously decreases the loss。3）使用 overlaping tokenization会使预训练模型中的自我注意力偏向某些Token，表明这些层没有充分优化。总之，overlapping tokenization可以促进下游任务的 fine-tuning，但是会导致预训练过程中的快速收敛。为了解放预训练的潜力，我们提出了一种新的方法RandomMask，该方法通过不断扩大BERT-like预训练的mask boundry来增加任务难度，让模型学习更多的知识。RandomMask简单而有效，在28个dataset上的26个downstream task中 achieved top-tier performance。
</details></li>
</ul>
<hr>
<h2 id="OpsEval-A-Comprehensive-Task-Oriented-AIOps-Benchmark-for-Large-Language-Models"><a href="#OpsEval-A-Comprehensive-Task-Oriented-AIOps-Benchmark-for-Large-Language-Models" class="headerlink" title="OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models"></a>OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07637">http://arxiv.org/abs/2310.07637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Dan Pei</li>
<li>for: The paper is written to evaluate the performance of large language models (LLMs) in AIOps tasks and to provide a comprehensive benchmark for optimizing LLMs tailored for AIOps.</li>
<li>methods: The paper uses a comprehensive task-oriented AIOps benchmark called OpsEval, which includes 7,200 questions in both multiple-choice and question-answer formats, to assess LLMs’ proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels.</li>
<li>results: The paper shows that GPT4-score is more consistent with experts than widely used Bleu and Rouge, and that various LLM tricks can affect the performance of AIOps, including zero-shot, chain-of-thought, and few-shot in-context learning. The paper also provides quantitative and qualitative results that demonstrate the effectiveness of OpsEval in evaluating LLMs for AIOps tasks.<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, and practical application). The benchmark includes 7,200 questions in both multiple-choice and question-answer (QA) formats, available in English and Chinese. With quantitative and qualitative results, we show how various LLM tricks can affect the performance of AIOps, including zero-shot, chain-of-thought, and few-shot in-context learning. We find that GPT4-score is more consistent with experts than widely used Bleu and Rouge, which can be used to replace automatic metrics for large-scale qualitative evaluations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自然语言处理相关任务中表现出了很好的能力，如翻译、概要和生成。在特定领域中应用LLM的潜在性非常大，特别是在人工智能运维（AIOps）领域。LLM在这些任务中的表现仍然未知。此外，为了优化适应AIOps任务的LLM，需要一个完整的标准准测。与现有的特定领域的标准准测不同，在这篇论文中，我们提出了《OpsEval》，一个完整的任务导向的AIOps标准准测，专门为LLM设计。这是首次，OpsEval评估了LLM在三个关键场景（有线网络运作、5G通信运作和数据库运作）中的不同水平（知识回忆、分析思维和实践应用）。标准准测包括7200个问题，分别在多选和问答（QA）格式中，英文和中文两种语言。我们通过量化和质量的结果表明，不同的LLM技巧对AIOps的表现有优势，包括零拟合、链式思维和少量上下文学习。我们发现，GPT4-score与专家更一致，可以用于取代大规模的自动评价指标。
</details></li>
</ul>
<hr>
<h2 id="Dual-Quaternion-Rotational-and-Translational-Equivariance-in-3D-Rigid-Motion-Modelling"><a href="#Dual-Quaternion-Rotational-and-Translational-Equivariance-in-3D-Rigid-Motion-Modelling" class="headerlink" title="Dual Quaternion Rotational and Translational Equivariance in 3D Rigid Motion Modelling"></a>Dual Quaternion Rotational and Translational Equivariance in 3D Rigid Motion Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07623">http://arxiv.org/abs/2310.07623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilherme Vieira, Eleonora Grassucci, Marcos Eduardo Valle, Danilo Comminiello</li>
<li>for: 用于模型3D空间中对物体的刚性运动，包括旋转和平移。</li>
<li>methods: 使用 dual quaternion 表示法，将旋转和平移视为单一的entity，不受数据的偏移影响，更好地学习对象的轨迹。</li>
<li>results: 在人姿预测应用中，使用该表示法的模型表现更好，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Objects' rigid motions in 3D space are described by rotations and translations of a highly-correlated set of points, each with associated $x,y,z$ coordinates that real-valued networks consider as separate entities, losing information. Previous works exploit quaternion algebra and their ability to model rotations in 3D space. However, these algebras do not properly encode translations, leading to sub-optimal performance in 3D learning tasks. To overcome these limitations, we employ a dual quaternion representation of rigid motions in the 3D space that jointly describes rotations and translations of point sets, processing each of the points as a single entity. Our approach is translation and rotation equivariant, so it does not suffer from shifts in the data and better learns object trajectories, as we validate in the experimental evaluations. Models endowed with this formulation outperform previous approaches in a human pose forecasting application, attesting to the effectiveness of the proposed dual quaternion formulation for rigid motions in 3D space.
</details>
<details>
<summary>摘要</summary>
三维空间中对象的刚性运动由旋转和平移组成，每个点都有相关的 $x,y,z$ 坐标，但是现有的网络会将这些点视为独立的实体，导致信息损失。过去的工作利用四元数代数来模型旋转，但这些代数无法正确地模型平移，从而导致三角形学习任务的下降性能。为了解决这些限制，我们使用三元数表示方法来描述三维空间中的刚性运动，每个点都是一个单一的实体，不会因为数据的偏移而受到影响。我们的方法是转换和旋转对称的，因此不会由数据的偏移而受到影响，更好地学习对象的轨迹，如我们在实验评估中所证明。使用我们的形式ulation，模型在人姿预测应用中表现出了更好的效果，这证明了我们的双三元数表示方法对三维空间中刚性运动的有效性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-based-Knowledge-Graph-Reasoning-for-Explainable-Fact-checking"><a href="#Reinforcement-Learning-based-Knowledge-Graph-Reasoning-for-Explainable-Fact-checking" class="headerlink" title="Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking"></a>Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07613">http://arxiv.org/abs/2310.07613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gustav Nikopensius, Mohit Mayank, Orchid Chetia Phukan, Rajesh Sharma<br>for:This paper aims to improve the trustworthiness of automated fact-checking systems by incorporating reinforcement learning (RL) and knowledge graph (KG) reasoning for explainable fact-checking.methods:The proposed approach uses a KG to represent knowledge and an RL reasoning agent to compute paths that prove or disprove factual claims. A voting mechanism is used to reach a verdict based on the paths produced by the agent.results:Experiments on two datasets (FB15K-277 and NELL-995) show that the proposed approach is effective in producing human-readable explanations in the form of paths and classifications for fact claims. The RL reasoning agent is able to compute paths that prove or disprove factual claims, and a voting mechanism is used to reach a verdict based on these paths. The approach encourages a human-in-the-loop approach to fact-checking, which increases trustworthiness by providing a way for human readers to decide whether the provided evidence is convincing or not.<details>
<summary>Abstract</summary>
Fact-checking is a crucial task as it ensures the prevention of misinformation. However, manual fact-checking cannot keep up with the rate at which false information is generated and disseminated online. Automated fact-checking by machines is significantly quicker than by humans. But for better trust and transparency of these automated systems, explainability in the fact-checking process is necessary. Fact-checking often entails contrasting a factual assertion with a body of knowledge for such explanations. An effective way of representing knowledge is the Knowledge Graph (KG). There have been sufficient works proposed related to fact-checking with the usage of KG but not much focus is given to the application of reinforcement learning (RL) in such cases. To mitigate this gap, we propose an RL-based KG reasoning approach for explainable fact-checking. Extensive experiments on FB15K-277 and NELL-995 datasets reveal that reasoning over a KG is an effective way of producing human-readable explanations in the form of paths and classifications for fact claims. The RL reasoning agent computes a path that either proves or disproves a factual claim, but does not provide a verdict itself. A verdict is reached by a voting mechanism that utilizes paths produced by the agent. These paths can be presented to human readers so that they themselves can decide whether or not the provided evidence is convincing or not. This work will encourage works in this direction for incorporating RL for explainable fact-checking as it increases trustworthiness by providing a human-in-the-loop approach.
</details>
<details>
<summary>摘要</summary>
Fact-checking 是一项非常重要的任务，因为它可以防止谣言的扩散。然而，人工的 fact-checking 无法与在线上generated false information的速度保持上。机器自动化的 fact-checking 比人工的速度要快得多。但是，为了提高这些自动系统的信任和透明度，需要解释性在 fact-checking 过程中。Fact-checking 通常包括对一个真实声明与一个知识体系进行比较，以便提供解释。知识图（KG）是一种有效的知识表示方式。虽然有很多关于 fact-checking 使用 KG 的论文，但却没有太多关于使用强化学习（RL）的研究。为了填补这个空白，我们提出了一种基于 RL 的 KG 逻辑应用，用于可读性的 fact-checking。我们在 FB15K-277 和 NELL-995 数据集上进行了广泛的实验，并证明了 KG 的逻辑 reasoning 是一种高效的生成人类可读的解释的方式。RL 逻辑代理人计算一个证明或反证声明的路径，但不会提供一个判断。一个判断是通过一种投票机制使用 paths 生成的，这些 paths 可以被展示给人类读者，让他们自己决定这些证据是否是有力的。这种工作将激励更多关于 incorporating RL 的工作，以提高信任worthiness 的自动 fact-checking 系统。
</details></li>
</ul>
<hr>
<h2 id="PHYDI-Initializing-Parameterized-Hypercomplex-Neural-Networks-as-Identity-Functions"><a href="#PHYDI-Initializing-Parameterized-Hypercomplex-Neural-Networks-as-Identity-Functions" class="headerlink" title="PHYDI: Initializing Parameterized Hypercomplex Neural Networks as Identity Functions"></a>PHYDI: Initializing Parameterized Hypercomplex Neural Networks as Identity Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07612">http://arxiv.org/abs/2310.07612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ispamm/phydi">https://github.com/ispamm/phydi</a></li>
<li>paper_authors: Matteo Mancanelli, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</li>
<li>for: 这个论文主要针对 parameterized hypercomplex neural networks (PHNNs) 的整合和控制问题，以提高它们的性能和稳定性。</li>
<li>methods: 该论文提出了一种 parameterized hypercomplex identity initialization (PHYDI) 方法，用于改进 PHNNs 的整合和控制，并通过不同的 benchmark 和常见 PHNNs 结构（如 ResNets 和 Transformer）进行了证明。</li>
<li>results: 该论文通过对不同 benchmark 和 PHNNs 结构进行了实验，证明了 PHYDI 方法能够提高 PHNNs 的性能和稳定性，并且可以在不同的缩放比例下实现更好的性能。<details>
<summary>Abstract</summary>
Neural models based on hypercomplex algebra systems are growing and prolificating for a plethora of applications, ranging from computer vision to natural language processing. Hand in hand with their adoption, parameterized hypercomplex neural networks (PHNNs) are growing in size and no techniques have been adopted so far to control their convergence at a large scale. In this paper, we study PHNNs convergence and propose parameterized hypercomplex identity initialization (PHYDI), a method to improve their convergence at different scales, leading to more robust performance when the number of layers scales up, while also reaching the same performance with fewer iterations. We show the effectiveness of this approach in different benchmarks and with common PHNNs with ResNets- and Transformer-based architecture. The code is available at https://github.com/ispamm/PHYDI.
</details>
<details>
<summary>摘要</summary>
neural networks based on hypercomplex algebra systems are becoming increasingly popular for a wide range of applications, from computer vision to natural language processing. however, as these models grow in size and complexity, there is a lack of techniques to control their convergence at a large scale. in this paper, we investigate the convergence of parameterized hypercomplex neural networks (PHNNs) and propose a method called parameterized hypercomplex identity initialization (PHYDI) to improve their convergence at different scales. this approach leads to more robust performance as the number of layers increases, while also achieving the same performance with fewer iterations. we demonstrate the effectiveness of PHYDI in various benchmarks and with common PHNNs based on ResNets and Transformers. the code is available at https://github.com/ispamm/PHYDI.
</details></li>
</ul>
<hr>
<h2 id="Democratizing-LLMs-An-Exploration-of-Cost-Performance-Trade-offs-in-Self-Refined-Open-Source-Models"><a href="#Democratizing-LLMs-An-Exploration-of-Cost-Performance-Trade-offs-in-Self-Refined-Open-Source-Models" class="headerlink" title="Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models"></a>Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07611">http://arxiv.org/abs/2310.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji</li>
<li>for: 提高资源受限和信息敏感环境中使用语言模型（LLM）的性能和隐私保护。</li>
<li>methods: 提出一种无目标的迭代自我批判和自我改进方法，以及一种新的排名指标（PeRFICS），用于选择任务所需的最佳模型。</li>
<li>results: 实验表明，使用不同大小的开源模型（7B-65B），平均提高8.2%的基eline性能。而具有极小内存占用的模型，如Vicuna-7B，在高创造力、开放任务上的提高为11.74%，并在高创造力任务中达到25.39%的提高。此外，Vicuna-13B甚至超越了后期修改后的ChatGPT。这种工作对于资源受限和信息敏感环境中使用LLM而言，有着深远的影响。<details>
<summary>Abstract</summary>
The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by case studies.
</details>
<details>
<summary>摘要</summary>
Proprietary LLM的主导地位导致了 restricted access和隐私问题的增加。高性能的开源选择是关键 для信息敏感和高量应用，但它们经常落后于性能。为解决这个差距，我们提议（1）一种没有外部影响的无目标变体iterative自我批判和自我改进。（2）一种新的排名指标—性能、精细化和推理成本分数（PeRFICS）—以便在给定任务中找到最佳模型。我们的实验表明，SoTA开源模型的变体，从7B到65B的平均提高8.2%的基eline性能。特别是，具有极少的内存占用量的模型，如Vicuna-7B，在总体上提高11.74%，并在高创新、开放结束任务中达到25.39%的提高。Vicuna-13B甚至超过了ChatGPT后期优化。这种工作对具有资源限制和信息敏感环境的人来说，可以不付出昂贵的代价，同时不失性能和隐私。领域无关的自我改进过程和我们的新的排名指标，使得选择模型更加科学，从而降低成本并普及高性能语言模型，根据实践。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Imbalanced-Data-Representation-Learning-and-SEP-Forecasting"><a href="#Survey-on-Imbalanced-Data-Representation-Learning-and-SEP-Forecasting" class="headerlink" title="Survey on Imbalanced Data, Representation Learning and SEP Forecasting"></a>Survey on Imbalanced Data, Representation Learning and SEP Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07598">http://arxiv.org/abs/2310.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josias Moukpe</li>
<li>for: 本研究目的是探讨深度学习方法在实际应用中如何处理数据不均衡问题。</li>
<li>methods: 本文提出使用表示学习方法来更好地捕捉数据特征空间的复杂性，以便更好地泛化到少数类。</li>
<li>results: 研究表明，通过使用表示学习方法，可以减轻数据不均衡问题对模型的影响，提高模型的泛化能力。<details>
<summary>Abstract</summary>
Deep Learning methods have significantly advanced various data-driven tasks such as regression, classification, and forecasting. However, much of this progress has been predicated on the strong but often unrealistic assumption that training datasets are balanced with respect to the targets they contain. This misalignment with real-world conditions, where data is frequently imbalanced, hampers the effectiveness of such models in practical applications. Methods that reconsider that assumption and tackle real-world imbalances have begun to emerge and explore avenues to address this challenge. One such promising avenue is representation learning, which enables models to capture complex data characteristics and generalize better to minority classes. By focusing on a richer representation of the feature space, these techniques hold the potential to mitigate the impact of data imbalance. In this survey, we present deep learning works that step away from the balanced-data assumption, employing strategies like representation learning to better approximate real-world imbalances. We also highlight a critical application in SEP forecasting where addressing data imbalance is paramount for success.
</details>
<details>
<summary>摘要</summary>
深度学习方法在各种数据驱动任务中具有显著进步，如回归、分类和预测。然而，大多数这些进步假设了训练数据集是均衡的，这是实际情况中的假设。在实际应用中，数据往往偏重于某些类别，这会使深度学习模型的效果受到影响。为了解决这个挑战，一些新的方法开始出现，这些方法是考虑不均衡数据的假设，并且尝试解决实际中的不均衡问题。一种有前途的方向是表示学习，这种技术可以让模型更好地捕捉数据特征的复杂性，并且更好地泛化到少数类别。在这篇评论中，我们介绍了不均衡数据的深度学习工作，包括使用表示学习的策略来更好地 aproximate 实际中的不均衡。我们还强调了一个关键应用场景，即SEP预测，在这个场景中，解决数据不均衡问题是成功的关键。
</details></li>
</ul>
<hr>
<h2 id="Goodtriever-Adaptive-Toxicity-Mitigation-with-Retrieval-augmented-Models"><a href="#Goodtriever-Adaptive-Toxicity-Mitigation-with-Retrieval-augmented-Models" class="headerlink" title="Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models"></a>Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07589">http://arxiv.org/abs/2310.07589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</li>
<li>for: 本研究旨在提出一种适应语言演化的恶意筛选方法，以提高现有方法的效果和效率。</li>
<li>methods: 我们提出了一种名为Goodtriever的灵活方法，它可以同时保持现有state-of-the-art的恶意筛选效果，并实现43%的相对延迟减少和更高的计算效率。这种方法通过在解码时使用检索方法，实现了恶意控制的文本生成。</li>
<li>results: 我们的研究表明，Goodtriever可以在实际应用中提供更高的效果和效率，而且更能适应语言的演化。<details>
<summary>Abstract</summary>
Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild. Code and data are available at https://github.com/for-ai/goodtriever.
</details>
<details>
<summary>摘要</summary>
很大的努力已经投入到抑止毒性方面，但现有的方法 часто需要对模型参数进行极大的修改或使用 computationally intensive的辅助模型。此外，前一代的方法经常忽视了语言的逐渐发展的特点。在这项工作中，我们提出了一种全面的抑止毒性策略，考虑到语言的变化性。我们引入了 Goodtriever，一种灵活的方法，可以与当前状态的艺术级抑止毒性匹配，并实现43%的相对延迟减少和更高的计算效率。通过在解码时使用检索方法，Goodtriever允许文本生成抑止毒性。我们强调了适应性的技术的重要性，以更好地适应数据模型在野外的变化。代码和数据可以在 <https://github.com/for-ai/goodtriever> 查看。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Use-of-Label-Dependency-in-Multi-Label-Text-Classification-Through-the-Lens-of-Causality"><a href="#Accurate-Use-of-Label-Dependency-in-Multi-Label-Text-Classification-Through-the-Lens-of-Causality" class="headerlink" title="Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality"></a>Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07588">http://arxiv.org/abs/2310.07588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin</li>
<li>for: 这研究旨在提高文本分类模型的表现，使其能够更准确地分类不同类别的文本。</li>
<li>methods: 这研究使用了现有的多标签文本分类方法，并引入了 causal inference 来减少对文本信息的偏袋。</li>
<li>results: 实验结果表明， compared to 基eline，我们的方法在三个数据集上显著提高了表现，并减少了对文本信息的偏袋。<details>
<summary>Abstract</summary>
Multi-Label Text Classification (MLTC) aims to assign the most relevant labels to each given text. Existing methods demonstrate that label dependency can help to improve the model's performance. However, the introduction of label dependency may cause the model to suffer from unwanted prediction bias. In this study, we attribute the bias to the model's misuse of label dependency, i.e., the model tends to utilize the correlation shortcut in label dependency rather than fusing text information and label dependency for prediction. Motivated by causal inference, we propose a CounterFactual Text Classifier (CFTC) to eliminate the correlation bias, and make causality-based predictions. Specifically, our CFTC first adopts the predict-then-modify backbone to extract precise label information embedded in label dependency, then blocks the correlation shortcut through the counterfactual de-bias technique with the help of the human causal graph. Experimental results on three datasets demonstrate that our CFTC significantly outperforms the baselines and effectively eliminates the correlation bias in datasets.
</details>
<details>
<summary>摘要</summary>
具体来说，我们的 CFTC 首先采用 predict-then-modify 结构来提取标签依赖中嵌入的精确标签信息，然后通过对 counterfactual de-bias 技术进行封锁，使用人类 causal graph 来阻断相关偏见。实验结果表明，我们的 CFTC 在三个 dataset 上显著超过基eline，并有效地消除了相关偏见。
</details></li>
</ul>
<hr>
<h2 id="Fed-GraB-Federated-Long-tailed-Learning-with-Self-Adjusting-Gradient-Balancer"><a href="#Fed-GraB-Federated-Long-tailed-Learning-with-Self-Adjusting-Gradient-Balancer" class="headerlink" title="Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer"></a>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07587">http://arxiv.org/abs/2310.07587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Xiao, Zihan Chen, Songshang Liu, Hualiang Wang, Yang Feng, Jin Hao, Joey Tianyi Zhou, Jian Wu, Howard Hao Yang, Zuozhu Liu</li>
<li>for: 本研究探讨了一种 federated long-tailed learning（Fed-LT）任务，每个客户端具有本地不同的数据集，如果这些数据集可以全球集成，它们就会共同表现出长尾分布。在这种设定下，现有的联邦优化和&#x2F;或中央长尾学习方法很难应用，因为在隐私限制下不能准确描述全球长尾分布，并且需要根据头尾异常分布调整本地学习策略。</li>
<li>methods: 本研究提出了一种名为 $\texttt{Fed-GraB}$ 的方法，包括一个 Self-adjusting Gradient Balancer（SGB）模块，该模块在关闭式方式中重新调整客户端的梯度，基于全球长尾分布的反馈。同时，该方法还包括一个 Direct Prior Analyzer（DPA）模块，用于评估全球长尾分布。</li>
<li>results: 实验表明，使用 $\texttt{Fed-GraB}$ 可以有效地缓解由数据不一致引起的分布漂移，并在较少的小类上实现更好的性能，同时保持大类的性能。这些实验结果表明，$\texttt{Fed-GraB}$ 在代表性的数据集上（如 CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT 和 iNaturalist）实现了领先的表现。<details>
<summary>Abstract</summary>
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution drift caused by data heterogeneity during the model training process and obtain a global model with better performance on the minority classes while maintaining the performance of the majority classes. Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves state-of-the-art performance on representative datasets such as CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>数据隐私和长尾分布是现实世界中的常见情况，而不是特例。这篇论文研究了一种 federated 长尾学习（Fed-LT）任务，每个客户端持有本地不同数据集，如果这些数据集可以全球聚合，它们就会共同表现出长尾分布。在这种设定下，现有的联邦优化和/或中央长尾学习方法几乎无法应用，因为在隐私限制下不能正确地描述全球长尾分布，以及调整本地学习策略以适应头尾偏度。为此，我们提出了一种方法，称为 $\texttt{Fed-GraB}$，包括一个自适应梯度均衡器（SGB）模块，通过在关闭循环方式下重新权重客户端的梯度，根据全球长尾分布的反馈来自 Direct Prior Analyzer（DPA）模块评估。使用 $\texttt{Fed-GraB}$，客户端可以在模型训练过程中有效地缓解由数据不同性引起的分布漂移，并在少数类上获得更好的性能，同时保持多数类的性能。广泛的实验表明， $\texttt{Fed-GraB}$ 在 Representative 数据集（CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT 和 iNaturalist）上达到了状态 искусственный智能的性能。
</details></li>
</ul>
<hr>
<h2 id="Linear-Latent-World-Models-in-Simple-Transformers-A-Case-Study-on-Othello-GPT"><a href="#Linear-Latent-World-Models-in-Simple-Transformers-A-Case-Study-on-Othello-GPT" class="headerlink" title="Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT"></a>Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07582">http://arxiv.org/abs/2310.07582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deanhazineh/emergent-world-representations-othello">https://github.com/deanhazineh/emergent-world-representations-othello</a></li>
<li>paper_authors: Dean S. Hazineh, Zechen Zhang, Jeffery Chiu</li>
<li>for: 这篇论文旨在探讨一个简单的 transformer 模型在 Othello 游戏中的表现，以推动对 Othello-GPT 世界模型的理解。</li>
<li>methods: 该论文使用了一种简单的 transformer 模型，并对其进行了扩展，以推动对 Othello-GPT 世界模型的理解。</li>
<li>results: 研究发现，Othello-GPT 包含一种直观的对立方 Piece 表示，这种表示导致其决策过程中的 causal 影响。此外，研究还发现了层数和模型复杂度对这种 linear world representation 和 causal decision-making 的依赖关系。<details>
<summary>Abstract</summary>
Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
</details>
<details>
<summary>摘要</summary>
基础模型在决策和逻辑推理方面表现出了显著的能力。然而，关于它们真正理解世界 VS 亚 probabilistic imitation 的问题仍然存在不断的讨论。这篇文章仔细研究了一个简单的 transformer 被训练用于 Othello，扩展了先前的研究，以更好地理解 Othello-GPT emergent 世界模型的工作原理。调查发现，Othello-GPT 包含对 противоборству方的线性表示，这种因素直接导向它的决策过程。这篇文章进一步阐明了 linear 世界表示和 causal 决策之间的交互，以及它们与模型复杂度和层数之间的关系。我们已经公布了代码。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Unlearning-Language-Models-as-Few-Shot-Unlearners"><a href="#In-Context-Unlearning-Language-Models-as-Few-Shot-Unlearners" class="headerlink" title="In-Context Unlearning: Language Models as Few Shot Unlearners"></a>In-Context Unlearning: Language Models as Few Shot Unlearners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07579">http://arxiv.org/abs/2310.07579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</li>
<li>for: 实现对特定训练点的推广式忘记，以满足隐私法规 like “忘记权”。</li>
<li>methods: 提出了一些近似 removing training data without retraining the model的算法，但这些算法需要模型参数的存取，假设在实际应用中可能无法实现。</li>
<li>results: 提出了一种新的内容忘记方法（In-Context Unlearning），可以在没有模型参数的情况下实现对特定训练点的忘记，并且可以维持和state-of-the-art忘记方法相同或更高的性能水准。<details>
<summary>Abstract</summary>
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the instance alongside a flipped label and additional correctly labelled instances which are prepended as inputs to the LLM at inference time. Our experimental results demonstrate that these contexts effectively remove specific information from the training set while maintaining performance levels that are competitive with (or in some cases exceed) state-of-the-art unlearning methods that require access to the LLM parameters.
</details>
<details>
<summary>摘要</summary>
机器学习未学习（Machine Unlearning），即在已经训练过的模型上去除特定训练点的影响，在最近受到隐私规定如“忘记权”的需求下得到了更多的关注。尤其是在大型语言模型（LLMs）方面，由于其涉及到版权问题，未学习成为了非常重要的研究方向。然而，对于非常大的模型来说，精确地进行未学习是计算上不可能的。为此，当前的研究已经提出了一些可以approximately remove training data without retraining the model的算法。这些算法需要对模型参数进行访问，这可能不是在实际应用中的假设，因为计算上的限制或者LLM通过API访问。在这种情况下，我们提出了一种新的类型的LLM未学习方法，即“在上下文中进行未学习”（In-Context Unlearning）。在这种方法中，我们在推理时提供了特定训练实例，并将其拼接在正确标签的输入前面。我们的实验结果表明，这些上下文可以有效地从训练集中去除特定信息，而且与 state-of-the-art 的未学习方法相比，其性能水平与或者超过了。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-Computational-Topology"><a href="#ChatGPT-for-Computational-Topology" class="headerlink" title="ChatGPT for Computational Topology"></a>ChatGPT for Computational Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07570">http://arxiv.org/abs/2310.07570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joybearliu/chatgpt-for-computational-topology">https://github.com/joybearliu/chatgpt-for-computational-topology</a></li>
<li>paper_authors: Jian Liu, Li Shen, Guo-Wei Wei</li>
<li>for: 本研究旨在 bridge 数学概念和计算 topology 之间的 gap，使得理论家可以使用 ChatGPT 将概念转化为功能代码。</li>
<li>methods: 我们使用 ChatGPT 训练数学家的概念，然后让 ChatGPT 生成计算 topology 代码，最后验证代码的正确性。</li>
<li>results: 我们的实验结果表明，通过使用 ChatGPT，理论家可以很容易地将概念转化为功能代码，并且可以验证代码的正确性。 我们还在计算 simplicial 复杂体上的 Betti 数、Laplacian 矩阵、Dirac 矩阵以及 hypergraph 和 digraph 上的新 topological 理论。<details>
<summary>Abstract</summary>
ChatGPT represents a significant milestone in the field of artificial intelligence (AI), finding widespread applications across diverse domains. However, its effectiveness in mathematical contexts has been somewhat constrained by its susceptibility to conceptual errors. Concurrently, topological data analysis (TDA), a relatively new discipline, has garnered substantial interest in recent years. Nonetheless, the advancement of TDA is impeded by the limited understanding of computational algorithms and coding proficiency among theoreticians. This work endeavors to bridge the gap between theoretical topological concepts and their practical implementation in computational topology through the utilization of ChatGPT. We showcase how a pure theoretician, devoid of computational experience and coding skills, can effectively transform mathematical formulations and concepts into functional code for computational topology with the assistance of ChatGPT. Our strategy outlines a productive process wherein a mathematician trains ChatGPT on pure mathematical concepts, steers ChatGPT towards generating computational topology code, and subsequently validates the generated code using established examples. Our specific case studies encompass the computation of Betti numbers, Laplacian matrices, and Dirac matrices for simplicial complexes, as well as the persistence of various homologies and Laplacians. Furthermore, we explore the application of ChatGPT in computing recently developed topological theories for hypergraphs and digraphs. This work serves as an initial step towards effectively transforming pure mathematical theories into practical computational tools, with the ultimate goal of enabling real applications across diverse fields.
</details>
<details>
<summary>摘要</summary>
chatGPT 代表了人工智能（AI）领域的一个重要突破，在多个领域找到了广泛的应用。然而，它在数学上的效果有所限制，主要是因为它容易受到概念错误的影响。同时，拓扑数据分析（TDA）在最近几年内得到了广泛的关注，但是它的发展受到了计算机算法和编程技能的限制。这项工作的目标是通过使用 chatGPT 将数学理论与计算 topology 的实现相连接。我们展示了如何让纯理论家，没有计算经验和编程技能，可以使用 chatGPT 将数学概念转化成功能的计算 topology 代码。我们的策略是让数学家在 chatGPT 上训练数学概念，然后使用 chatGPT 生成计算 topology 代码，并验证生成的代码使用已知的例子。我们的具体案例包括计算 simplicial 复杂体上的 Betti 数、Laplacian 矩阵和 Dirac 矩阵，以及不同的同质和 Laplacian 的 persistency。此外，我们还探讨了 chatGPT 在计算 hypergraphs 和 directed graphs 上的应用。这项工作是将数学理论转化为实用计算工具的初步步骤，最终目标是在多个领域应用。
</details></li>
</ul>
<hr>
<h2 id="ROMO-Retrieval-enhanced-Offline-Model-based-Optimization"><a href="#ROMO-Retrieval-enhanced-Offline-Model-based-Optimization" class="headerlink" title="ROMO: Retrieval-enhanced Offline Model-based Optimization"></a>ROMO: Retrieval-enhanced Offline Model-based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07560">http://arxiv.org/abs/2310.07560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingcheng Chen, Haoran Zhao, Yuxiang Zhao, Hulei Fan, Hongqiao Gao, Yong Yu, Zheng Tian</li>
<li>for: 这个论文的目的是解决数据驱动黑盒模型基于优化（MBO）问题，特别是在受限制的环境下进行优化。</li>
<li>methods: 这篇论文提出了一种新的、挑战性的 constrained MBO（CoMBO） Setting，并使用了一种新的 retrieval-enhanced offline model-based optimization（ROMO）方法来解决这个问题。 ROMO 方法使用了抽取的 offline 数据集来提供可靠的预测，并用这些预测来进行梯度下降优化。</li>
<li>results: 实验结果表明，ROMO 方法在受限制的优化任务中表现出色，并且可以在各种不同的数据集和任务上达到优秀的效果。<details>
<summary>Abstract</summary>
Data-driven black-box model-based optimization (MBO) problems arise in a great number of practical application scenarios, where the goal is to find a design over the whole space maximizing a black-box target function based on a static offline dataset. In this work, we consider a more general but challenging MBO setting, named constrained MBO (CoMBO), where only part of the design space can be optimized while the rest is constrained by the environment. A new challenge arising from CoMBO is that most observed designs that satisfy the constraints are mediocre in evaluation. Therefore, we focus on optimizing these mediocre designs in the offline dataset while maintaining the given constraints rather than further boosting the best observed design in the traditional MBO setting. We propose retrieval-enhanced offline model-based optimization (ROMO), a new derivable forward approach that retrieves the offline dataset and aggregates relevant samples to provide a trusted prediction, and use it for gradient-based optimization. ROMO is simple to implement and outperforms state-of-the-art approaches in the CoMBO setting. Empirically, we conduct experiments on a synthetic Hartmann (3D) function dataset, an industrial CIO dataset, and a suite of modified tasks in the Design-Bench benchmark. Results show that ROMO performs well in a wide range of constrained optimization tasks.
</details>
<details>
<summary>摘要</summary>
“数据驱动黑盒模型基于优化（MBO）问题在许多实际应用场景中出现，目标是在整个空间中找到最优化黑盒目标函数的设计，使用静态离线数据。在这项工作中，我们考虑了更加一般但也更加挑战性的MBO设定（CoMBO），其中只有一部分的设计空间可以优化，而另外的部分则是环境所决定的约束。在CoMBO中，我们发现大多数满足约束的观察到的设计都是 mediocre 的评价，因此我们选择在离线数据集中优化这些 mediocre 的设计，而不是在传统MBO设定中进一步提高最佳观察到的设计。我们提出了Retrieval-Enhanced Offline Model-Based Optimization（ROMO），一种新的可 derivable forward 方法，它在离线数据集中检索和聚合相关的样本，以提供可靠的预测，并用它进行梯度基本优化。ROMO简单实现，在CoMBO设定中超过了当前状态的方法。我们在 sintetic Hartmann（3D）函数数据集、industrial CIO 数据集以及 Design-Bench benchmark 中进行了实验，结果显示ROMO在各种受约束优化任务中表现良好。”
</details></li>
</ul>
<hr>
<h2 id="ProtoHPE-Prototype-guided-High-frequency-Patch-Enhancement-for-Visible-Infrared-Person-Re-identification"><a href="#ProtoHPE-Prototype-guided-High-frequency-Patch-Enhancement-for-Visible-Infrared-Person-Re-identification" class="headerlink" title="ProtoHPE: Prototype-guided High-frequency Patch Enhancement for Visible-Infrared Person Re-identification"></a>ProtoHPE: Prototype-guided High-frequency Patch Enhancement for Visible-Infrared Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07552">http://arxiv.org/abs/2310.07552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiwei Zhang, Yongfei Zhang, Zichang Tan</li>
<li>for:  bridging the modality gap in visible-infrared person re-identification</li>
<li>methods:  ProtoHPE, which includes two core designs: (1) enhancing the representation ability of cross-modal correlated high-frequency components using Wavelet Transform and exponential moving average Vision Transformer (ViT), and (2) obtaining semantically compact and discriminative high-frequency representations of the same identity using Multimodal Prototypical Contrast.</li>
<li>results:  extensive experiments validate the effectiveness of ProtoHPE.Here’s the summary in the format you requested:</li>
<li>for:  bridging the modality gap in visible-infrared person re-identification</li>
<li>methods:  ProtoHPE, which includes two core designs: (1) enhancing the representation ability of cross-modal correlated high-frequency components using Wavelet Transform and exponential moving average Vision Transformer (ViT), and (2) obtaining semantically compact and discriminative high-frequency representations of the same identity using Multimodal Prototypical Contrast.</li>
<li>results:  extensive experiments validate the effectiveness of ProtoHPE.<details>
<summary>Abstract</summary>
Visible-infrared person re-identification is challenging due to the large modality gap. To bridge the gap, most studies heavily rely on the correlation of visible-infrared holistic person images, which may perform poorly under severe distribution shifts. In contrast, we find that some cross-modal correlated high-frequency components contain discriminative visual patterns and are less affected by variations such as wavelength, pose, and background clutter than holistic images. Therefore, we are motivated to bridge the modality gap based on such high-frequency components, and propose \textbf{Proto}type-guided \textbf{H}igh-frequency \textbf{P}atch \textbf{E}nhancement (ProtoHPE) with two core designs. \textbf{First}, to enhance the representation ability of cross-modal correlated high-frequency components, we split patches with such components by Wavelet Transform and exponential moving average Vision Transformer (ViT), then empower ViT to take the split patches as auxiliary input. \textbf{Second}, to obtain semantically compact and discriminative high-frequency representations of the same identity, we propose Multimodal Prototypical Contrast. To be specific, it hierarchically captures the comprehensive semantics of different modal instances, facilitating the aggregation of high-frequency representations belonging to the same identity. With it, ViT can capture key high-frequency components during inference without relying on ProtoHPE, thus bringing no extra complexity. Extensive experiments validate the effectiveness of ProtoHPE.
</details>
<details>
<summary>摘要</summary>
Visible-infrared人识别具有大量modal gap，大多数研究都是通过可见-红外人像匹配来弥补这个差距。然而，我们发现一些cross-modal相关的高频成分包含可识别的视觉特征，并且对波长、姿势和背景噪声的变化更加敏感。因此，我们提出基于这些高频成分的modal gap减少方法，并提出了ProtoHPE方法，具有两个核心设计。首先，我们使用浪涌变换和抽象移动平均视transformer（ViT）来强化高频成分的表示能力。然后，我们使用ViT作为辅助输入来使其更好地捕捉高频成分。其次，我们提出了多modal prototype contrast，它可以层次捕捉不同modal实例的全面 semantics，使得高频表示的积累得到了更好的semanticCompactness。这使得ViT可以在推理过程中捕捉关键的高频成分，无需依赖于ProtoHPE，从而不添加额外复杂性。我们的实验证明了ProtoHPE的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Fairness-Accuracy-tradeoff-with-few-Test-Samples-under-Covariate-Shift"><a href="#Improving-Fairness-Accuracy-tradeoff-with-few-Test-Samples-under-Covariate-Shift" class="headerlink" title="Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift"></a>Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07535">http://arxiv.org/abs/2310.07535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Havaldar, Jatin Chauhan, Karthikeyan Shanmugam, Jay Nandy, Aravindan Raghuveer</li>
<li>for:  This paper is written to address the problem of covariate shift in the test data, which can negatively impact the accuracy and fairness of machine learning models, particularly in sensitive groups such as criminal justice.</li>
<li>methods:  The paper proposes a novel composite weighted entropy based objective for prediction accuracy, which is optimized along with a representation matching loss for fairness. The proposed method also includes a new setting called Asymmetric Covariate Shift, which has not been studied before and is extremely challenging for current baselines.</li>
<li>results:  The paper shows that optimizing with the proposed loss formulation outperforms state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Additionally, the paper provides theoretical analysis showing that the proposed weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift, and this approximation does not depend on importance sampling variance.<details>
<summary>Abstract</summary>
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines.
</details>
<details>
<summary>摘要</summary>
covariate shift在测试数据中可能导致模型的准确率和公平性表现下降。在这些设置下，保证不同敏感群体之间的公平性非常重要，因为社会上的影响力，如刑事正义。我们在无监督 режи下操作，只有一小部分无标签测试样本以及标记训练集。在这个问题上，我们提供了三个贡献。第一是一种新的复合权重Entropy基于目标函数，该函数与代表匹配损失一起优化，以提高准确率和公平性。我们通过实验证明，使用我们的损失函数与多种状态前的基elines进行比较，在 pareto 意义上在多个标准数据集上表现较好。我们的第二贡献是一种新的设置，我们称之为偏 asymmetric covariate shift。这种设置在一个群体的 covariate 分布发生了显著变化，而另一些群体的分布则呈现相对稳定的情况下，这种情况在当前基线模型中非常困难。然而，我们的提议方法在这种设置下表现出色。我们的第三贡献是理论上的，我们证明了我们的权重Entropy термин以及预测损失在训练集上可以近似测试集下的损失。通过实验和正式的样本复杂度下限，我们证明了这种近似不依赖于重要性采样偏差，这种偏差影响了许多其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Human-Centered-Evaluation-of-XAI-Methods"><a href="#Human-Centered-Evaluation-of-XAI-Methods" class="headerlink" title="Human-Centered Evaluation of XAI Methods"></a>Human-Centered Evaluation of XAI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07534">http://arxiv.org/abs/2310.07534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karam Dawoud, Wojciech Samek, Sebastian Lapuschkin, Sebastian Bosse</li>
<li>for: 本研究旨在解释深度学习中的决策过程，以提高人工智能的透明度。</li>
<li>methods: 本研究使用了三种主流的解释方法：Prototypical Part Network、Occlusion和Layer-wise Relevance Propagation。</li>
<li>results: 研究发现，这三种方法所指出的区域可能会很不同，但它们都可以为人类提供相似深度的理解，从而帮助用户更好地分类图像。<details>
<summary>Abstract</summary>
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offer humans a nearly equivalent depth of understanding. This enables users to discern and categorize images efficiently, reinforcing the value of these methods in enhancing AI transparency.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Chinese, using simpler grammar and vocabulary to make it easier to understand for native Chinese speakers. However, the translation may not be perfect, and some nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Energy-Estimates-Across-Layers-of-Computing-From-Devices-to-Large-Scale-Applications-in-Machine-Learning-for-Natural-Language-Processing-Scientific-Computing-and-Cryptocurrency-Mining"><a href="#Energy-Estimates-Across-Layers-of-Computing-From-Devices-to-Large-Scale-Applications-in-Machine-Learning-for-Natural-Language-Processing-Scientific-Computing-and-Cryptocurrency-Mining" class="headerlink" title="Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining"></a>Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07516">http://arxiv.org/abs/2310.07516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadasivan Shankar</li>
<li>for: 这个论文旨在研究计算系统中能源的使用情况，从设备到算法的多层次计算中计算出能源需求。</li>
<li>methods: 论文使用了前一个分析 [3] 的基础，对单个设备和系统，包括人工智能&#x2F;机器学习自然语言处理、科学仿真和加密货币矿pool的三大规模计算应用，进行了能源需求的估算。在比较比特级 switching 中，由于 geometrical scaling，通过逻辑级别和模拟级别的应用程序层次，能源消耗更高。</li>
<li>results: 对 AI&#x2F;ML 加速器的分析表明，使用older的半导体技术节点的不同架构可以实现相同的能源效率。此外，对计算系统中的能量与热动力学和生物限制进行比较，表明计算应用程序的总模拟需求高出27-36个数量级。这些能源估算表明，在计算系统中包含能源作为设计参数是必要的，以满足计算密集应用的增长需求在数字世界中。<details>
<summary>Abstract</summary>
Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitude higher energy requirements for total simulation of an application. These energy estimates underscore the need for serious considerations of energy efficiency in computing by including energy as a design parameter, enabling growing needs of compute-intensive applications in a digital world.
</details>
<details>
<summary>摘要</summary>
computation 中的能源消耗量已经被评估和分析。据前一个分析 [3]，单个设备和系统，包括三种大规模计算应用程序，如人工智能（AI）/机器学习自然语言处理、科学仿真和加密货币矿业，的能源需求已被估算。与比特级 switching 相比，在应用程序的指令和仿真层级中，更高的能源消耗。此外，基于 AI/ML 加速器的分析表明，使用older半导体技术节点的不同架构可以实现相似的能效性。进一步的比较表明，计算系统中的能源需求与热动力和生物限制之间有27-36个数量级的差异。这些能源估算表明，在计算中包含能源作为设计参数是必要的，以满足计算强需求的增长。
</details></li>
</ul>
<hr>
<h2 id="Sample-Driven-Federated-Learning-for-Energy-Efficient-and-Real-Time-IoT-Sensing"><a href="#Sample-Driven-Federated-Learning-for-Energy-Efficient-and-Real-Time-IoT-Sensing" class="headerlink" title="Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing"></a>Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07497">http://arxiv.org/abs/2310.07497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyd-fl/scfl">https://github.com/skyd-fl/scfl</a></li>
<li>paper_authors: Minh Ngoc Luu, Minh-Duong Nguyen, Ebrahim Bedeer, Van Duc Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham</li>
<li>for: This paper focuses on improving the performance of Federated Learning (FL) systems in IoT networks with real-time sensing capabilities.</li>
<li>methods: The proposed approach, called Sample-driven Control for Federated Learning (SCFL), leverages online reinforcement learning to dynamically adapt and find the global optimum in changing environments.</li>
<li>results: The proposed approach is able to mitigate overfitting and improve overall accuracy in FL systems with real-time sensing capabilities, while also considering energy efficiency.<details>
<summary>Abstract</summary>
In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy efficiency while optimizing the accuracy with high generalization. To solve the optimization problem with high complexity, we introduce an online reinforcement learning algorithm, named Sample-driven Control for Federated Learning (SCFL) built on the Soft Actor-Critic (A2C) framework. This enables the agent to dynamically adapt and find the global optima even in changing environments. By leveraging the capabilities of SCFL, our system offers a promising solution for resource allocation in FL systems with real-time sensing capabilities.
</details>
<details>
<summary>摘要</summary>
在联合学习（FL）系统领域，当前最先进的方法都依赖于理想的条件趋同分析。特别是，这些方法假设训练数据集在物联网设备上具有类似于全球数据分布的特征。然而，这种方法无法捕捉实时感知FL系统中的全面数据特征。为了解决这个限制，我们提出了一种新的方法，专门为物联网网络with实时感知功能设计。我们的方法考虑用户数据采样过程中的泛化差距，并通过有效控制采样过程来mitigate过拟合问题，从而提高总的准确率。具体来说，我们首先形ulate一个优化问题，利用采样过程同时降低过拟合问题而提高准确率。为了解决这个复杂的优化问题，我们引入了一种在线束缚学习算法，名为Sample-driven Control for Federated Learning（SCFL），基于Soft Actor-Critic（A2C）框架。这使得代理人可以在变化环境中动态适应并找到全球最优解。通过利用SCFL的能力，我们的系统提供了一个有前途的资源分配解决方案 дляFL系统with实时感知功能。
</details></li>
</ul>
<hr>
<h2 id="Diversity-for-Contingency-Learning-Diverse-Behaviors-for-Efficient-Adaptation-and-Transfer"><a href="#Diversity-for-Contingency-Learning-Diverse-Behaviors-for-Efficient-Adaptation-and-Transfer" class="headerlink" title="Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer"></a>Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07493">http://arxiv.org/abs/2310.07493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Finn Rietz, Johannes Andreas Stork</li>
<li>for: 本研究旨在开发一种可以在不同任务和转移动力学中表现出色的RL算法，以适应任务或转移动力学的变化。</li>
<li>methods: 本研究提出了一种简单的方法，通过在每个任务和转移动力学中学习多个策略，以获得所有可能的解决方案。每个后续策略都是基于所有前一个策略下的解决方案不可能的。不同于先前的方法，我们的方法不需要学习额外的模型来探测新的任务和转移动力学，也不需要平衡任务和新任务的奖励信号。</li>
<li>results: 我们的方法可以快速适应任务或转移动力学的变化，并且在转移学习中表现出色。<details>
<summary>Abstract</summary>
Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.
</details>
<details>
<summary>摘要</summary>
发现所有有用的解决方案对于可转移RL代理来说是关键，以适应任务或转移动力学的变化。这不是классиRL算法所考虑的，这些算法只关心当前任务和动力学的优化。我们提议了一种简单的方法，可以找到给定任务的所有可能的解决方案，以获得在转移设定下perform well的代理，并快速适应任务或转移动力学的变化。我们的方法在每次迭代学习过程中，学习一组策略，而每一个后续策略都需要具有不可能在所有前一个策略下发生的解决方案。与之前的方法不同，我们的方法不需要学习额外的模型来检测新鲜事物，也不需要平衡任务和新鲜事物的奖励信号，直接在行动选择和优化步骤中直接 incorporate 约束。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Black-box-Attack-to-Deep-Neural-Networks-with-Conditional-Diffusion-Models"><a href="#Boosting-Black-box-Attack-to-Deep-Neural-Networks-with-Conditional-Diffusion-Models" class="headerlink" title="Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models"></a>Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07492">http://arxiv.org/abs/2310.07492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam</li>
<li>for: 这个论文的目的是提出一种新的黑盒攻击策略，以提高黑盒攻击下查询数的效率。</li>
<li>methods: 这个论文使用了一种新的黑盒攻击策略，即 Conditional Diffusion Model Attack (CDMA)，它将黑盒攻击问题转化为一个分布变换问题，通过直接使用数据转换器来生成可靠的攻击示例。</li>
<li>results: 论文通过对九种现有黑盒攻击方法进行比较，显示了 CDMA 可以大幅降低查询数量，通常只需要一些 queries。此外，论文还展示了 CDMA 可以在不同的防御策略下保持高达99%的攻击成功率。<details>
<summary>Abstract</summary>
Existing black-box attacks have demonstrated promising potential in creating adversarial examples (AE) to deceive deep learning models. Most of these attacks need to handle a vast optimization space and require a large number of queries, hence exhibiting limited practical impacts in real-world scenarios. In this paper, we propose a novel black-box attack strategy, Conditional Diffusion Model Attack (CDMA), to improve the query efficiency of generating AEs under query-limited situations. The key insight of CDMA is to formulate the task of AE synthesis as a distribution transformation problem, i.e., benign examples and their corresponding AEs can be regarded as coming from two distinctive distributions and can transform from each other with a particular converter. Unlike the conventional \textit{query-and-optimization} approach, we generate eligible AEs with direct conditional transform using the aforementioned data converter, which can significantly reduce the number of queries needed. CDMA adopts the conditional Denoising Diffusion Probabilistic Model as the converter, which can learn the transformation from clean samples to AEs, and ensure the smooth development of perturbed noise resistant to various defense strategies. We demonstrate the effectiveness and efficiency of CDMA by comparing it with nine state-of-the-art black-box attacks across three benchmark datasets. On average, CDMA can reduce the query count to a handful of times; in most cases, the query count is only ONE. We also show that CDMA can obtain $>99\%$ attack success rate for untarget attacks over all datasets and targeted attack over CIFAR-10 with the noise budget of $\epsilon=16$.
</details>
<details>
<summary>摘要</summary>
existing 黑盒攻击已经展示了吸引深度学习模型的攻击潜力（AE）。大多数这些攻击需要处理庞大的优化空间，需要大量的查询，因此在实际场景中表现有限的实际影响。在这篇论文中，我们提出了一种新的黑盒攻击策略： Conditional Diffusion Model Attack（CDMA），以提高在查询有限的情况下生成AE的查询效率。 CDMA的关键想法是将AE生成任务视为一个分布转换问题，即正常示例和其相应的AE可以视为来自两个不同的分布，并且可以通过特定的数据转换器进行转换。与传统的查询和优化方法不同，我们通过直接条件转换来生成可靠的AE，可以减少查询的数量。 CDMA采用 Conditional Denoising Diffusion Probabilistic Model 作为转换器，可以学习从干净示例到AE的转换，并确保生成的干扰噪声能够抗性各种防御策略。 我们通过对 CDMA 与九种 state-of-the-art 黑盒攻击进行比较，证明 CDMA 的效果和效率。在三个标准 benchmark 数据集上，CDMA 可以减少查询数量到几个数量级；在大多数情况下，查询数量只有 ONE。此外，我们还证明 CDMA 可以在 CIFAR-10 上获得 $>99\%$ 攻击成功率，即使使用噪声бюджет $\epsilon=16$。
</details></li>
</ul>
<hr>
<h2 id="KwaiYiiMath-Technical-Report"><a href="#KwaiYiiMath-Technical-Report" class="headerlink" title="KwaiYiiMath: Technical Report"></a>KwaiYiiMath: Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07488">http://arxiv.org/abs/2310.07488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai</li>
<li>for: 提高自然语言处理下推理能力，特别是数学任务需要多步逻辑。</li>
<li>methods: 应用监督精度调整（SFT）和人工反馈学习（RLHF），包括英文和中文数学任务。</li>
<li>results: 与相同大小模型比较，KwaiYiiMath在GSM8k、CMath和KMath上达到了state-of-the-art（SOTA）性能。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
</details>
<details>
<summary>摘要</summary>
近年来，大型语言模型（LLM）的进步已经显示出了处理多种自然语言处理（NLP）下游任务的卓越能力，包括需要多步骤的逻辑reasoning的数学任务。在这份报告中，我们介绍了强大的KwaiYiiMath，它可以增强KwaiYiiBase1中的数学逻辑能力，通过监督精度调整（SFT）和人类反馈学习（RLHF），包括英文和中文数学任务。此外，我们还构建了一个小规模的中学数学测试集（名为KMath），包含188个例题，以评估模型生成的问题解决过程的正确性。验证研究表明，KwaiYiiMath可以在GSM8k、CMath和KMath上达到类似模型的SOTA性能。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Graph-Learning-for-Generative-Tasks"><a href="#Multimodal-Graph-Learning-for-Generative-Tasks" class="headerlink" title="Multimodal Graph Learning for Generative Tasks"></a>Multimodal Graph Learning for Generative Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07478">http://arxiv.org/abs/2310.07478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minjiyoon/mmgl">https://github.com/minjiyoon/mmgl</a></li>
<li>paper_authors: Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov</li>
<li>for: 本研究旨在扩展现有的文本生成模型，使其能够利用多Modal的数据进行生成。</li>
<li>methods: 我们提出了一种名为多模态图学习（MMGL）的框架，用于捕捉多Modal的数据之间的复杂关系。我们基于预训练语言模型（LM），并通过将多Modal的它们视为图structure来拓展其文本生成能力。</li>
<li>results: 我们通过三个研究问题来回答MMGL的问题：（1）如何将多Modal的信息涂敷到预训练LM中，而不会scalability问题？（2）如何将多Modal之间的图结构信息涂敷到LM中？（3）如何使用参数有效地训练LM来学习从邻居 контекст中？我们通过广泛的实验和分析来回答这三个问题，以便为将来的MMGL研究提供基础。<details>
<summary>Abstract</summary>
Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretrained Language Models (LMs), aiming to augment their text generation with multimodal neighbor contexts. We study three research questions raised by MMGL: (1) how can we infuse multiple neighbor information into the pretrained LMs, while avoiding scalability issues? (2) how can we infuse the graph structure information among multimodal neighbors into the LMs? and (3) how can we finetune the pretrained LMs to learn from the neighbor context in a parameter-efficient manner? We conduct extensive experiments to answer these three questions on MMGL and analyze the empirical results to pave the way for future MMGL research.
</details>
<details>
<summary>摘要</summary>
多Modal学习结合多种数据模式，扩大我们模型可以使用的数据类型和复杂性：例如，从普通文本到图像caption对。大多数多Modal学习算法专注于模型简单的一对一对数据从两种模式，例如图像caption对或音频文本对。然而，在真实世界情况下，不同模式的实体通常在更复杂和多方面相互作用，超出一对一映射。我们提议表示这些复杂关系为图，以便捕捉多种模式的数据，并且在不同样本之间可以变化的复杂关系。为达到这个目标，我们提出了多Modal图学习（MMGL），一种通用和系统的框架，用于从多种多Modal模式中捕捉信息。特别是，我们在生成任务上采用MMGL，基于预训练语言模型（LM），旨在通过多Modal邻居 контекст来增强文本生成。我们研究了MMGL中的三个研究问题：（1）如何在预训练LM中涂入多个邻居信息，而不会出现规模问题？（2）如何在LM中涂入多Modal邻居之间的图 струкucture信息？以及（3）如何parameterfficient地训练预训练LM，以便它们可以学习邻居 контекст？我们进行了广泛的实验，以回答这三个问题，并分析实验结果，以便为未来MMGL研究提供方向。
</details></li>
</ul>
<hr>
<h2 id="An-Ontology-of-Co-Creative-AI-Systems"><a href="#An-Ontology-of-Co-Creative-AI-Systems" class="headerlink" title="An Ontology of Co-Creative AI Systems"></a>An Ontology of Co-Creative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07472">http://arxiv.org/abs/2310.07472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Lin, Mark Riedl</li>
<li>for: This paper aims to provide an ontology of co-creative systems to assist with disambiguating research efforts in human-AI creative endeavors.</li>
<li>methods: The paper uses an extension of Lubart’s original ontology of creativity support tools to emphasize the role of artificial intelligence, with three new categories: computer-as-subcontractor, computer-as-critic, and computer-as-teammate.</li>
<li>results: The paper provides a comprehensive framework for understanding the division of responsibilities and information exchange between humans and AI systems in co-creative endeavors.<details>
<summary>Abstract</summary>
The term co-creativity has been used to describe a wide variety of human-AI assemblages in which human and AI are both involved in a creative endeavor. In order to assist with disambiguating research efforts, we present an ontology of co-creative systems, focusing on how responsibilities are divided between human and AI system and the information exchanged between them. We extend Lubart's original ontology of creativity support tools with three new categories emphasizing artificial intelligence: computer-as-subcontractor, computer-as-critic, and computer-as-teammate, some of which have sub-categorizations.
</details>
<details>
<summary>摘要</summary>
“协创”一词已经用来描述各种人工智能联合体，在创作活动中，人类和AI系统都参与创作。为了帮助研究努力，我们提出了协创系统 ontology，强调人类和 AI 系统之间的责任分配和信息交换。我们将 Lubart 的原始创意支持工具 ontology 扩展为三个新的分类，其中包括人工智能：计算机作为代工、计算机作为评判者和计算机作为团队成员，其中有一些子分类。
</details></li>
</ul>
<hr>
<h2 id="The-Implications-of-Decentralization-in-Blockchained-Federated-Learning-Evaluating-the-Impact-of-Model-Staleness-and-Inconsistencies"><a href="#The-Implications-of-Decentralization-in-Blockchained-Federated-Learning-Evaluating-the-Impact-of-Model-Staleness-and-Inconsistencies" class="headerlink" title="The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies"></a>The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07471">http://arxiv.org/abs/2310.07471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini</li>
<li>for: 这篇论文研究了如何使用区块链技术来支持分布式机器学习（DL）方法，具体来说是 Federation Learning（FL），以提高分布式智能的可信度和安全性。</li>
<li>methods: 作者使用了一种叫做“块链加密技术”来实现分布式机器学习，这种技术可以保证数据的安全性和不可变性。</li>
<li>results: 研究发现，在使用块链技术支持的分布式机器学习中，模型的不一致性和延迟会导致预测精度下降，最多下降约35%。这说明，在设计块链系统时，需要考虑对应的分布式机器学习应用的特点。<details>
<summary>Abstract</summary>
Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic network such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation on the well-known CIFAR-10 dataset and focus on the accuracy and timeliness of the solutions. Our results show the high impact of model inconsistencies on the accuracy of the models (up to a ~35% decrease in prediction accuracy), which underscores the importance of properly designing blockchain systems based on the characteristics of the underlying FL application.
</details>
<details>
<summary>摘要</summary>
区块链可以提高分布式机器学习（ML）方法，如联邦学习（FL），的更加分布化、安全、不可变和信任性，这些特点是下一代应用中的关键。然而，归结于P2P区块链节点的自治运行方式，FL的概念such as round和全球模型变得意义不明确，因为设备的同步被lost Without a centralized orchestrating server.在这篇论文中，我们研究了在区块链上委托FL的协调的实际影响。特别是，我们关注了由区块链的作法引入的模型偏移和不一致性对FL设备进行异步训练的影响。使用仿真，我们评估了在CIFAR-10数据集上使用区块链进行FL操作的效果。我们的结果显示模型不一致性可以导致预测精度下降（最多下降约35%），这说明了为了根据FL应用的特点设计区块链系统的重要性。
</details></li>
</ul>
<hr>
<h2 id="AI-ML-based-Load-Prediction-in-IEEE-802-11-Enterprise-Networks"><a href="#AI-ML-based-Load-Prediction-in-IEEE-802-11-Enterprise-Networks" class="headerlink" title="AI&#x2F;ML-based Load Prediction in IEEE 802.11 Enterprise Networks"></a>AI&#x2F;ML-based Load Prediction in IEEE 802.11 Enterprise Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07467">http://arxiv.org/abs/2310.07467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Dariush Salami, Gianluca Fontanesi, Lorenzo Galati-Giordano, Mika Kasslin</li>
<li>for:  This paper aims to explore the feasibility and suitability of using AI&#x2F;ML-based load prediction in practical enterprise Wi-Fi networks.</li>
<li>methods: The paper uses hardware-constrained AI&#x2F;ML models to predict network load, leveraging data availability and quality, computational capabilities, and energy consumption.</li>
<li>results: The results show that the hardware-constrained AI&#x2F;ML models can predict network load with less than 20% average error and 3% 85th-percentile error, which is a suitable input for proactively driving Wi-Fi network optimization.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文旨在探讨实际企业 Wi-Fi 网络中使用 AI&#x2F;ML 加载预测的可行性和适用性。</li>
<li>methods: 论文使用硬件限制的 AI&#x2F;ML 模型预测网络负载，利用数据可用性和质量、计算能力和能效性。</li>
<li>results: 结果显示，硬件限制的 AI&#x2F;ML 模型可以预测网络负载少于 20% 的平均误差和 3% 的85% 极值误差，这是适当的输入 для推动 Wi-Fi 网络优化。<details>
<summary>Abstract</summary>
Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence and Machine Learning (AI/ML) thanks to their well-developed management and operation capabilities. At the same time, AI/ML-based traffic/load prediction is one of the most appealing data-driven solutions to improve the Wi-Fi experience, either through the enablement of autonomous operation or by boosting troubleshooting with forecasted network utilization. In this paper, we study the suitability and feasibility of adopting AI/ML-based load prediction in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can potentially contribute to optimizing Wi-Fi networks in terms of energy efficiency, performance, and reliability, their effective adoption is constrained to aspects like data availability and quality, computational capabilities, and energy consumption. Our results show that hardware-constrained AI/ML models can potentially predict network load with less than 20% average error and 3% 85th-percentile error, which constitutes a suitable input for proactively driving Wi-Fi network optimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-machine-learning-surrogates-for-large-scale-geological-carbon-and-energy-storage"><a href="#Efficient-machine-learning-surrogates-for-large-scale-geological-carbon-and-energy-storage" class="headerlink" title="Efficient machine-learning surrogates for large-scale geological carbon and energy storage"></a>Efficient machine-learning surrogates for large-scale geological carbon and energy storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07461">http://arxiv.org/abs/2310.07461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teeratorn Kadeethum, Stephen J. Verzi, Hongkyu Yoon</li>
<li>for: 这个研究旨在探讨地理碳和能源储存的不确定性，以及如何使用机器学习（ML）模型来管理广泛的沉底模型。</li>
<li>methods: 本研究使用特殊的机器学习模型来管理广泛的沉底模型，并使用页面分解和顶点嵌入来连接空间时间点。</li>
<li>results: 这个方法可以实现精确的预测，即使没有训练数据，并提高了机器学习的效率 для大规模地理碳储存应用。<details>
<summary>Abstract</summary>
Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.   While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
</details>
<details>
<summary>摘要</summary>
地质碳和能量储存是实现净碳排放零和气候变化的关键，但它们面临地质因素和运营限制，可能导致人工地震或地下水污染。为了解决这些挑战，我们提议使用专门的机器学习（ML）模型来有效管理广泛的沉库模型。although ML方法在地质碳存储方面具有承诺，大规模分析所需的大量计算资源是障碍。我们开发了一种方法来降低深度神经操作器模型的训练成本，使用领域分解和Topology Embedder连接空间时间点。这种方法允许在模型领域内进行准确预测，即使数据未经训练，从而提高ML的效率在大规模地质储存应用中。
</details></li>
</ul>
<hr>
<h2 id="HealthWalk-Promoting-Health-and-Mobility-through-Sensor-Based-Rollator-Walker-Assistance"><a href="#HealthWalk-Promoting-Health-and-Mobility-through-Sensor-Based-Rollator-Walker-Assistance" class="headerlink" title="HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance"></a>HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07434">http://arxiv.org/abs/2310.07434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivanna Kramer, Kevin Weirauch, Sabine Bauer, Mark Oliver Mints, Peer Neubert</li>
<li>for: 提高人们身体能力和独立生活时间，帮助人们患有身体限制或各种疾病参与社会</li>
<li>methods:  integrate sensors into rollator walker designs</li>
<li>results: 收集数据并研究老年人、患有股骨炎、多发性硬化症和 Parkinson 病人以及视力障碍者的使用情况Here’s the breakdown of each point:</li>
<li>for: The paper is written to improve the mobility and independence of people with physical limitations or various diseases, and to help them participate in society for longer.</li>
<li>methods: The paper uses sensors integrated into rollator walker designs to address the problem of poor posture and potential falls.</li>
<li>results: The paper collects data and researches the use of the early HealthWalk rollator walker prototype with older people, people with rheumatism, multiple sclerosis, Parkinson’s disease, and individuals with visual impairments.<details>
<summary>Abstract</summary>
Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a romanization of Chinese that uses a set of rules to represent the tones and grammatical structure of the language in a way that is easier to read and write for non-native speakers. The translation above is written in Simplified Chinese, which is the official standard for Chinese writing in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-from-Observation-with-Automatic-Discount-Scheduling"><a href="#Imitation-Learning-from-Observation-with-Automatic-Discount-Scheduling" class="headerlink" title="Imitation Learning from Observation with Automatic Discount Scheduling"></a>Imitation Learning from Observation with Automatic Discount Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07433">http://arxiv.org/abs/2310.07433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, Zhao-Heng Yin, Chongjie Zhang, Yang Gao</li>
<li>for: 本研究旨在解决机器人学习从观察数据中学习（Imitation Learning from Observations，ILfO）问题，即无法访问专家行为的情况下，机器人学习从专家的视频示例中学习。</li>
<li>methods: 本研究提出了一种新的ILfO框架，通过自适应衰减因子（Automatic Discount Scheduling，ADS）机制，使机器人在培训阶段适应不同的任务，从早期行为学习到后期行为。</li>
<li>results: 对于 Meta-World 任务中的九个任务，我们的方法与现有方法相比，显著提高了机器人的学习效果，包括一些不可解决的任务。<details>
<summary>Abstract</summary>
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earlier behaviors before advancing to later ones. We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively alters the discount factor in reinforcement learning during the training phase, prioritizing earlier rewards initially and gradually engaging later rewards only when the earlier behaviors have been mastered. Our experiments, conducted on nine Meta-World tasks, demonstrate that our method significantly outperforms state-of-the-art methods across all tasks, including those that are unsolvable by them.
</details>
<details>
<summary>摘要</summary>
人类常通过观察和imiter来学习新技能。而为机器人代理人来学习从互联网上的大量未标注视频示例数据中，却存在一个称为“寄学学习从观察”（ILfO）的挑战。一种常见的解决方法是将ILfO问题转化为反奖学习问题，使用代理人和专家的观察来计算代理人的奖励。然而，我们发现任务具有“进步依赖性”属性时，这些方法会遇到很大的挑战。在这些任务中，代理人需要在学习专家的前期行为之前，才能学习后期行为。我们的调查表明，主要的问题在于奖励信号赋给后期步骤干扰了代理人学习初期行为的过程。为解决这个挑战，我们提出了一种新的ILfO框架，允许代理人在训练阶段首先学习初期行为，然后才能进行后期行为。我们还 introduce了一种自动评估因子调整（ADS）机制，在训练阶段动态调整奖励学习中的评估因子，初期优先考虑早期奖励，逐渐只在初期行为已经学习完毕后，才开始考虑后期奖励。我们在Meta-World任务上进行了九个任务的实验，结果表明，我们的方法在所有任务上都有显著优异，包括一些无法解决的任务。
</details></li>
</ul>
<hr>
<h2 id="Multi-Concept-T2I-Zero-Tweaking-Only-The-Text-Embeddings-and-Nothing-Else"><a href="#Multi-Concept-T2I-Zero-Tweaking-Only-The-Text-Embeddings-and-Nothing-Else" class="headerlink" title="Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else"></a>Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07419">http://arxiv.org/abs/2310.07419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang, Humphrey Shi</li>
<li>for: 本文旨在提高文本至图生成模型的自然多元概念生成能力，不需要额外训练或执行时间成本。</li>
<li>methods: 作者通过调整预训练的文本至图扩散模型的文本嵌入来解决多元概念生成中的概念占据和非本地贡献问题，提高图像生成的自然性和多元性。</li>
<li>results: 实验结果表明，作者的方法在文本至图、图像 manipulate 和个性化任务中均有显著提高，而无需额外训练或执行成本增加。<details>
<summary>Abstract</summary>
Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic multi-concept text-to-image generation. Our Correction by Similarities method tweaks the embedding of concepts by collecting semantic features from most similar tokens to localize the contribution. To avoid mixing features of concepts, we also apply Cross-Token Non-Maximum Suppression, which excludes the overlap of contributions from different concepts. Experiments show that our approach outperforms previous methods in text-to-image, image manipulation, and personalization tasks, despite not introducing additional training or inference costs to the diffusion steps.
</details>
<details>
<summary>摘要</summary>
Specifically, we identify two main issues that hinder multi-concept generation: concept dominance and non-localized contribution. To address these issues, we design a Correction by Similarities method that tweaks the embedding of concepts by collecting semantic features from the most similar tokens. This approach localizes the contribution of each concept and avoids mixing features of different concepts.In addition, we apply Cross-Token Non-Maximum Suppression to exclude the overlap of contributions from different concepts. This method ensures that each concept contributes uniquely to the generated image, preventing the mixing of features.Our approach outperforms previous methods in text-to-image, image manipulation, and personalization tasks, despite not introducing additional training or inference costs to the diffusion steps. This demonstrates the effectiveness of our minimal low-cost solution in improving the naturalness of multi-concept text-to-image generation.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Plasticity-in-Visual-Reinforcement-Learning-Data-Modules-and-Training-Stages"><a href="#Revisiting-Plasticity-in-Visual-Reinforcement-Learning-Data-Modules-and-Training-Stages" class="headerlink" title="Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages"></a>Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07418">http://arxiv.org/abs/2310.07418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guozheng Ma, Lu Li, Sen Zhang, Zixuan Liu, Zhen Wang, Yixin Chen, Li Shen, Xueqian Wang, Dacheng Tao</li>
<li>for: 本研究旨在探讨Visual Reinforcement Learning（VRL）中神经网络塑性（plasticity）的影响，以提高高效性和样本效果。</li>
<li>methods: 研究者采用系统性的实验方法，探讨三个主要未得到充分研究的方面，得出以下结论：（1）数据扩展是维持塑性的关键因素；（2）批评器的塑性损失是训练效率的主要瓶颈；（3）在早期阶段不及时恢复批评器的塑性，会导致损失崩溃。</li>
<li>results: 研究结果表明，适应性RR可以避免批评器的塑性损失，并且在后续阶段通过更频繁的重复来提高样本效果。<details>
<summary>Abstract</summary>
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic's plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.
</details>
<details>
<summary>摘要</summary>
neural network 的适应力（plasticity）在新数据的演化中是高性能和样本效率学习（VRL）的关键因素。 although 调整和调整可能会对适应力损失产生影响，内部VRL框架中各组件对代理人的适应力仍然不甚了解。 在这个工作中，我们进行了系统性的实验探索，专注在三个次要未曾探索的方面，得到以下几个有益的结论：（1）资料扩展是维持适应力的重要因素；（2）批评家的适应力损失是训练效率的主要阻碍因素；（3）在早期阶段未能及时针对批评家的适应力进行恢复，将会导致致命的适应力损失。 这些结论建议一种新的策略，以Addressing the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic's plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.
</details></li>
</ul>
<hr>
<h2 id="What-can-knowledge-graph-alignment-gain-with-Neuro-Symbolic-learning-approaches"><a href="#What-can-knowledge-graph-alignment-gain-with-Neuro-Symbolic-learning-approaches" class="headerlink" title="What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?"></a>What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07417">http://arxiv.org/abs/2310.07417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Giesteira Cotovio, Ernesto Jimenez-Ruiz, Catia Pesquita</li>
<li>for: 本研究旨在探讨现有的知识图表（KG）对采用者的需求和限制，以及如何通过结合符号学习和数据学习来提高KG对采用者的适应性。</li>
<li>methods: 本研究使用了深度学习模型和符号学习模型来实现KG对采用者的对接，并评估了这些模型的可解释性和数据效率。</li>
<li>results: 研究发现，结合符号学习和数据学习可以提高KG对采用者的适应性和可解释性，并且可以提高KG的数据效率。此外，研究还探讨了可能的未来研究方向，以推动KG对采用者的进一步发展。<details>
<summary>Abstract</summary>
Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for combining these fields.
</details>
<details>
<summary>摘要</summary>
知识图（KG）是许多数据沟通应用的基础，因为它可以表示数据以及其意义和上下文。在不同领域和提供者之间对KG进行对齐是必要的，以便建立更加完整和整合的表示。现有的KG对Alignment（KGA）算法具有一定的局限性，它们无法体现逻辑思维和语言、结构和semantic数据学习。深度学习模型在KGA中受欢迎，但它们受到解释性、逻辑和数据效率等限制。兼容神经符号学习模型可以将逻辑和数据观点集成起来，以生成高质量的对齐，并且可以通过人类中心的方法进行验证。本文检讨了KGA的当前状态，并探讨了将神经符号学习与KGA相结合的潜在研究方向。
</details></li>
</ul>
<hr>
<h2 id="DASpeech-Directed-Acyclic-Transformer-for-Fast-and-High-quality-Speech-to-Speech-Translation"><a href="#DASpeech-Directed-Acyclic-Transformer-for-Fast-and-High-quality-Speech-to-Speech-Translation" class="headerlink" title="DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation"></a>DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07403">http://arxiv.org/abs/2310.07403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/daspeech">https://github.com/ictnlp/daspeech</a></li>
<li>paper_authors: Qingkai Fang, Yan Zhou, Yang Feng<br>for:* 这篇论文目的是提出一种新的非 autoregressive 直接语音翻译模型，即 DASpeech，以实现高质量和快速decoding速度。methods:* DASpeech 使用了 two-pass 架构，将生成过程分解成两个步骤：首先，一个语言模型生成目标文本，然后，一个声学模型基于语言模型的隐藏状态生成目标声音。results:* 实验表明，DASpeech 可以与当前状态艺的 S2ST 模型相当或更好的表现，同时保持与概率模型的18.53倍的速度提升。* DASpeech 不需要知识继承和迭代解码，可以在翻译质量和解码速度之间做出重要改进。* DASpeech 能够保持源语音的 speaker voice 在翻译过程中。<details>
<summary>Abstract</summary>
Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To consider all potential paths in the DAG during training, we calculate the expected hidden states for each target token via dynamic programming, and feed them into the acoustic decoder to predict the target mel-spectrogram. During inference, we select the most probable path and take hidden states on that path as input to the acoustic decoder. Experiments on the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or even better performance than the state-of-the-art S2ST model Translatotron 2, while preserving up to 18.53x speedup compared to the autoregressive baseline. Compared with the previous non-autoregressive S2ST model, DASpeech does not rely on knowledge distillation and iterative decoding, achieving significant improvements in both translation quality and decoding speed. Furthermore, DASpeech shows the ability to preserve the speaker's voice of the source speech during translation.
</details>
<details>
<summary>摘要</summary>
直接语音到语音翻译（S2ST）模型可以将语音从一种语言翻译到另一种语言。但由于语言和听音多样性，目标语音表现出复杂的多模态分布，对于S2ST模型来说，同时实现高质量翻译和快速解码速度是一项挑战。在这篇论文中，我们提出了DASpeech模型，这是一种非autoregressive的直接S2ST模型，可以同时实现高质量和快速解码速度。为了更好地捕捉目标语音的复杂分布，DASpeech采用了两步 Architecture，将生成过程分为两个步骤。在第一步，语言解码器将目标文本生成，然后在第二步，听音解码器将基于语言解码器生成的隐藏状态来生成目标语音。我们使用DA-Transformer模型作为语言解码器，并使用FastSpeech 2模型作为听音解码器。在训练时，我们使用DAG图来表示翻译关系，并通过动态计算来考虑所有可能的路径。在推断时，我们选择最有可能性的路径，并将该路径上的隐藏状态作为听音解码器的输入。在CVSS Fr-En测试集上，DASpeech可以与现状之最的S2ST模型Translatotron 2相比，实现相同或更好的性能，同时保持18.53倍的速度提升。与之前的非autoregressive S2ST模型相比，DASpeech不需要知识储存和迭代解码，可以实现显著的改进。此外，DASpeech还能保持源语音的 speaker 特征。
</details></li>
</ul>
<hr>
<h2 id="NuTime-Numerically-Multi-Scaled-Embedding-for-Large-Scale-Time-Series-Pretraining"><a href="#NuTime-Numerically-Multi-Scaled-Embedding-for-Large-Scale-Time-Series-Pretraining" class="headerlink" title="NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining"></a>NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07402">http://arxiv.org/abs/2310.07402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguo Lin, Xumeng Wen, Wei Cao, Congrui Huang, Jiang Bian, Stephen Lin, Zhirong Wu</li>
<li>for: 本研究旨在开发一种可扩展性强大的时间序列自监督学习模型，能够处理大规模数据集（数百万个时间序列）。</li>
<li>methods: 我们采用Transformer架构，首先将输入分割成非重叠的窗口，然后对每个窗口进行Characterization，并使用numerically multi-scaled embedding模块将整数值转换为高维向量。</li>
<li>results: 我们的方法在多个单变量和多变量分类benchmark上表现出色，与之前的表征学习方法相比，实现了显著的提升，甚至与领域专门的非学习基于方法相比。<details>
<summary>Abstract</summary>
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset containing over a million sequences. We study its transfer performance on a number of univariate and multivariate classification benchmarks. Our method exhibits remarkable improvement against previous representation learning approaches and establishes the new state of the art, even compared with domain-specific non-learning-based methods.
</details>
<details>
<summary>摘要</summary>
近期时序自我超参增强模型的研究表现了很大的承诺，但它受到了小规模数据的限制，例如千个时间序列。在这项工作中，我们做出了关键的技术贡献，特化于时序数据的数学性质，使模型可以扩展到大规模数据集，例如百万个时间序列。我们采用了Transformer架构，首先将输入分割成非重叠的窗口。每个窗口然后被 caracterized by its normalized shape和两个整数值，表示在每个窗口中的mean和标准差。为将整数值，可能具有任意的数学尺度，转化为高维向量，我们提议一种数字多尺度嵌入模块，列举所有可能的尺度。模型在使用我们提议的数字多尺度嵌入后进行预训练，使用简单的对比目标函数在一个包含 более чем一百万个序列的大规模数据集上。我们研究其转移性能在多个单变量和多变量分类benchmark上，我们的方法显示了很大的改进，even compared with域специ fic non-learning-based方法。
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Proactive-Dialogue-Systems-with-Personalization-Problem-Formulation-and-Dataset-Curation"><a href="#Target-oriented-Proactive-Dialogue-Systems-with-Personalization-Problem-Formulation-and-Dataset-Curation" class="headerlink" title="Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation"></a>Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07397">http://arxiv.org/abs/2310.07397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iwangjian/topdial">https://github.com/iwangjian/topdial</a></li>
<li>paper_authors: Jian Wang, Yi Cheng, Dongding Lin, Chak Tou Leong, Wenjie Li</li>
<li>for: 这个论文旨在探讨个性化目标导向对话系统，以实现对话系统的聊天目标或系统侧目标。</li>
<li>methods: 该论文使用了一种自动化数据采集框架，通过角色扮演方式自动生成了大规模个性化目标导向对话数据集 TopDial，包含约18万多个多转对话。</li>
<li>results: 实验结果表明，TopDial数据集具有高质量，可以用于探讨个性化目标导向对话。<details>
<summary>Abstract</summary>
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
</details>
<details>
<summary>摘要</summary>
目标对话系统，旨在主动导向对话到预定目标或实现特定系统目标，是现代对话AI的一个有趣领域。在这篇文章中，我们通过形式化对话动作和话题为对话目标，探讨了一种个性化目标对话的新问题。然而，还有一个潜在的需求是高质量的数据集，建立一个从 scratch 的数据集需要巨大的人工劳动。为解决这个问题，我们提出了一种自动数据集 curación框架，基于这个框架，我们构建了一个大规模的个性化目标对话数据集 TopDial，包含约18K多个多turn对话。实验结果表明，这个数据集具有高质量，可以为个性化目标对话的探索提供启示。
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Reward-Function-for-User-Preferred-Appliance-Scheduling"><a href="#Learning-a-Reward-Function-for-User-Preferred-Appliance-Scheduling" class="headerlink" title="Learning a Reward Function for User-Preferred Appliance Scheduling"></a>Learning a Reward Function for User-Preferred Appliance Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07389">http://arxiv.org/abs/2310.07389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikskiks/learning-reward-function-demand-response">https://github.com/nikskiks/learning-reward-function-demand-response</a></li>
<li>paper_authors: Nikolina Čović, Jochen Cremer, Hrvoje Pandžić</li>
<li>for: 减少能源供应领域中碳排放，快速发展家庭部分的需求反应服务提供。</li>
<li>methods: 使用反做奖励学习模型，无需直接 solicit end users’ needs and wishes，通过对 их过去消耗数据进行学习，以便创建他们的日常家用电器时间表。</li>
<li>results: 通过这种方法，可以让 end users 透明地参与服务设计和决策过程，同时保持他们的便利和控制权，从而鼓励他们继续为需求反应服务提供。<details>
<summary>Abstract</summary>
Accelerated development of demand response service provision by the residential sector is crucial for reducing carbon-emissions in the power sector. Along with the infrastructure advancement, encouraging the end users to participate is crucial. End users highly value their privacy and control, and want to be included in the service design and decision-making process when creating the daily appliance operation schedules. Furthermore, unless they are financially or environmentally motivated, they are generally not prepared to sacrifice their comfort to help balance the power system. In this paper, we present an inverse-reinforcement-learning-based model that helps create the end users' daily appliance schedules without asking them to explicitly state their needs and wishes. By using their past consumption data, the end consumers will implicitly participate in the creation of those decisions and will thus be motivated to continue participating in the provision of demand response services.
</details>
<details>
<summary>摘要</summary>
加速了住宅部分的需求反应服务提供的发展是电力部门减少碳排放的关键。同时，鼓励最终用户参与是关键。最终用户强烈关注隐私和控制权，并希望在创建日常家用电器运行时间表时参与服务设计和决策过程。此外，如果他们没有经济或环境上的动机，他们通常不愿意为了帮助平衡电力系统而做出牺牲。本文提出一种基于反增强学习的模型，可以基于最终用户的过去消耗数据来创建日常家用电器运行时间表，从而透明地参与到决策过程中，并因此 Motivate them to continue participating in demand response services.
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Classification-and-Vulnerability-Analysis-using-Federated-Learning"><a href="#Histopathological-Image-Classification-and-Vulnerability-Analysis-using-Federated-Learning" class="headerlink" title="Histopathological Image Classification and Vulnerability Analysis using Federated Learning"></a>Histopathological Image Classification and Vulnerability Analysis using Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07380">http://arxiv.org/abs/2310.07380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankalp Vyas, Amar Nath Patra, Raj Mani Shukla</li>
<li>for: 革命化医疗应用中的机器学习（ML）。传统上，ML模型由中央服务器训练，这些服务器将数据从多个分布式设备加载，以预测新生成的数据结果。这会引起隐私担忧，因为模型可以访问用户敏感信息。</li>
<li>methods: 我们采用了联邦学习（FL）方法，其中全球模型将自己的 копи本发送到所有客户端，并让客户端在本地设备上训练这些 копи本。在训练过程中，客户端将更新（加权）发送回全球模型。这种方法可以保护数据隐私，并在训练过程中提高模型的准确性。</li>
<li>results: 我们使用了一种敏感度攻击的隐私保护技术，并在皮肤癌数据集上进行了实验。结果显示，global模型受到数据攻击的影响，其准确性随着攻击的增加而下降。我们使用了一种梯度下降优化算法，以找到最佳的模型准确性。虽然FL可以保护用户隐私，但它也容易受到数据攻击，需要进行解决。<details>
<summary>Abstract</summary>
Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.   However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accuracy of the global model. As the percentage of label flipping increases, there is a noticeable decrease in accuracy. We use a stochastic gradient descent optimization algorithm to find the most optimal accuracy for the model. Although FL can protect user privacy for healthcare diagnostics, it is also vulnerable to data poisoning, which must be addressed.
</details>
<details>
<summary>摘要</summary>
医疗是机器学习（ML）的一个重要应用领域。传统上，ML模型由中央服务器进行训练，这些服务器将来自各个分布式设备的数据进行汇总，以预测新生成的数据的结果。这是一个主要的问题，因为模型可以访问敏感用户信息，这会引起隐私问题。一种联邦学习（FL）方法可以解决这个问题：一个全球模型将其 копи本发送到所有客户端，客户端将其进行训练，并将更新（加权）发送回全球模型。随着全球模型的改进，它的准确率会逐渐提高。在训练过程中，数据隐私被保护，因为它在客户端上进行本地训练。  然而，全球模型容易受到数据毒素攻击。我们开发了一种隐私保护的FL技术，并在皮肤癌数据集上进行了实验。我们发现，当一个客户端意外地将标签反转为攻击时，全球模型的准确率会下降。如果涂抹标签的百分比增加，全球模型的准确率就会逐渐下降。我们使用一种随机梯度下降优化算法来找到最优的准确率。虽然FL可以保护用户隐私 для医疗诊断，但也容易受到数据毒素攻击，这些攻击必须得到解决。
</details></li>
</ul>
<hr>
<h2 id="Causal-Unsupervised-Semantic-Segmentation"><a href="#Causal-Unsupervised-Semantic-Segmentation" class="headerlink" title="Causal Unsupervised Semantic Segmentation"></a>Causal Unsupervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07379">http://arxiv.org/abs/2310.07379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/byungkwanlee/causal-unsupervised-segmentation">https://github.com/byungkwanlee/causal-unsupervised-segmentation</a></li>
<li>paper_authors: Junho Kim, Byung-Kwan Lee, Yong Man Ro</li>
<li>for: 这篇论文目标是提出一种无监督 semantic segmentation 方法，以实现高质量 semantic grouping 无需人工标注。</li>
<li>methods: 该方法基于自我监督预训练，使用预训练特征来训练预测头进行无监督稠密预测。</li>
<li>results: 经过广泛的实验和分析，该方法在不同的数据集上达到了无监督 semantic segmentation 领域的州charts纪录。<details>
<summary>Abstract</summary>
Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for pixel-level grouping. Through extensive experiments and analyses on various datasets, we corroborate the effectiveness of CAUSE and achieve state-of-the-art performance in unsupervised semantic segmentation.
</details>
<details>
<summary>摘要</summary>
Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for pixel-level grouping. Through extensive experiments and analyses on various datasets, we corroborate the effectiveness of CAUSE and achieve state-of-the-art performance in unsupervised semantic segmentation.Here's the translation in Traditional Chinese:Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for pixel-level grouping. Through extensive experiments and analyses on various datasets, we corroborate the effectiveness of CAUSE and achieve state-of-the-art performance in unsupervised semantic segmentation.
</details></li>
</ul>
<hr>
<h2 id="Point-Cloud-Denoising-and-Outlier-Detection-with-Local-Geometric-Structure-by-Dynamic-Graph-CNN"><a href="#Point-Cloud-Denoising-and-Outlier-Detection-with-Local-Geometric-Structure-by-Dynamic-Graph-CNN" class="headerlink" title="Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN"></a>Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07376">http://arxiv.org/abs/2310.07376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kosuke Nakayama, Hiroto Fukuta, Hiroshi Watanabe</li>
<li>for: 该论文旨在提出一种基于PointCleanNet的点云数据清洗和异常检测方法，以提高点云处理的精度和效率。</li>
<li>methods: 该方法使用了两种基于动态图 convolutional layer的图 convolutional layer，以利用点云的本地几何结构进行减噪和异常检测。</li>
<li>results: 实验结果表明，提案方法在AUPR和Chamfer Distance两个指标上具有较高的异常检测精度和减噪精度，比传统方法更高。<details>
<summary>Abstract</summary>
The digitalization of society is rapidly developing toward the realization of the digital twin and metaverse. In particular, point clouds are attracting attention as a media format for 3D space. Point cloud data is contaminated with noise and outliers due to measurement errors. Therefore, denoising and outlier detection are necessary for point cloud processing. Among them, PointCleanNet is an effective method for point cloud denoising and outlier detection. However, it does not consider the local geometric structure of the patch. We solve this problem by applying two types of graph convolutional layer designed based on the Dynamic Graph CNN. Experimental results show that the proposed methods outperform the conventional method in AUPR, which indicates outlier detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
</details>
<details>
<summary>摘要</summary>
社会的数字化发展快速进程中，数字双和metaverse的实现吸引着广泛的关注。特别是3D空间的点云数据在这个过程中具有重要的作用。然而，点云数据受到测量错误的影响，因此需要进行噪声除除和异常点检测。其中，PointCleanNet是一种有效的点云噪声除除和异常点检测方法。然而，它不考虑点云的本地 геометри结构。我们解决这个问题，通过基于动态图 convolutional layer的两种类型的图 convolutional layer。实验结果表明，我们提出的方法在AUPR和Chamfer Distance两个指标中，与传统方法比较，具有更高的噪声除除精度和异常点检测精度。
</details></li>
</ul>
<hr>
<h2 id="Give-and-Take-Federated-Transfer-Learning-for-Industrial-IoT-Network-Intrusion-Detection"><a href="#Give-and-Take-Federated-Transfer-Learning-for-Industrial-IoT-Network-Intrusion-Detection" class="headerlink" title="Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection"></a>Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07354">http://arxiv.org/abs/2310.07354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lochana Telugu Rajesh, Tapadhir Das, Raj Mani Shukla, Shamik Sengupta</li>
<li>for: 本研究旨在提出一种基于联邦传播学习（FTL）的IIoT网络侵入检测方法，以提高IIoT网络的安全性。</li>
<li>methods: 本研究提议使用一种组合神经网络作为FTL的核心部分，将IIoT数据分布在客户端和服务器端之间，并将客户端模型的参数相加以更新服务器模型。</li>
<li>results: 实验结果显示，FTL设置在IIoT客户端和服务器之间的迭代性能强，并且本研究的FTL方法在网络侵入检测中表现更好于当今机器学习算法。<details>
<summary>Abstract</summary>
The rapid growth in Internet of Things (IoT) technology has become an integral part of today's industries forming the Industrial IoT (IIoT) initiative, where industries are leveraging IoT to improve communication and connectivity via emerging solutions like data analytics and cloud computing. Unfortunately, the rapid use of IoT has made it an attractive target for cybercriminals. Therefore, protecting these systems is of utmost importance. In this paper, we propose a federated transfer learning (FTL) approach to perform IIoT network intrusion detection. As part of the research, we also propose a combinational neural network as the centerpiece for performing FTL. The proposed technique splits IoT data between the client and server devices to generate corresponding models, and the weights of the client models are combined to update the server model. Results showcase high performance for the FTL setup between iterations on both the IIoT clients and the server. Additionally, the proposed FTL setup achieves better overall performance than contemporary machine learning algorithms at performing network intrusion detection.
</details>
<details>
<summary>摘要</summary>
互联网物件（IoT）技术的快速增长已经成为今天的行业的一部分，形成了现代互联网物件（IIoT）项目，行业利用IoT进行通信和连接，透过新兴解决方案如数据分析和云端计算。然而，快速使用IoT使其成为黑客的目标。因此，保护这些系统是最重要的。在这篇论文中，我们提议了一个联边转移学习（FTL）方法来执行 IIoT 网络侵犯检测。在研究中，我们还提议了一个整合神经网络作为中心的FTL架构。提案的技术将 IIoT 数据分配到客户和服务器装置上，从而生成对应的模型，并将客户端模型的预测结果联合以更新服务器模型。结果显示了 FTL 组态在 IIoT 客户和服务器之间的高性能，以及提案的 FTL 组态在当前机器学习算法中的更好的网络入侵检测性能。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Association-Rule-Learning-from-Time-Series-Data-and-Knowledge-Graphs"><a href="#Semantic-Association-Rule-Learning-from-Time-Series-Data-and-Knowledge-Graphs" class="headerlink" title="Semantic Association Rule Learning from Time Series Data and Knowledge Graphs"></a>Semantic Association Rule Learning from Time Series Data and Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07348">http://arxiv.org/abs/2310.07348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkan Karabulut, Victoria Degeler, Paul Groth</li>
<li>for: 这篇论文的目的是为了提出一个基于知识 graphs（KG）和时间序列数据的 semantic association rule 学习管道，以及一种新的 semantic association rule 标准。</li>
<li>methods: 该论文使用了知识 graphs（KG）和时间序列数据来学习 semantic association rule，并提出了一种新的 semantic association rule 标准。</li>
<li>results: 实验结果显示，提议的方法能够学习出具有含义信息的 association rule，并且这些 association rule 更加通用。<details>
<summary>Abstract</summary>
Digital Twins (DT) are a promising concept in cyber-physical systems research due to their advanced features including monitoring and automated reasoning. Semantic technologies such as Knowledge Graphs (KG) are recently being utilized in DTs especially for information modelling. Building on this move, this paper proposes a pipeline for semantic association rule learning in DTs using KGs and time series data. In addition to this initial pipeline, we also propose new semantic association rule criterion. The approach is evaluated on an industrial water network scenario. Initial evaluation shows that the proposed approach is able to learn a high number of association rules with semantic information which are more generalizable. The paper aims to set a foundation for further work on using semantic association rule learning especially in the context of industrial applications.
</details>
<details>
<summary>摘要</summary>
数字双胞（DT）是质量预测系统研究中的一种有前途的概念，它具有监测和自动推理等高级特性。 semantic技术，如知识图（KG），在DT中特别地应用于信息模型化。基于这个搬运，本文提出了基于KG和时间序列数据的semantic association rule学习管道。此外，我们还提出了一种新的semantic association rule标准。这种方法在工业水网enario中进行了初步评估，结果显示，提议的方法能够学习大量具有semantic信息的相互关系规则，这些规则更加通用。本文的目的是为了为使用semantic association rule学习，特别是在工业应用中，提供一个基础。
</details></li>
</ul>
<hr>
<h2 id="Fast-ELECTRA-for-Efficient-Pre-training"><a href="#Fast-ELECTRA-for-Efficient-Pre-training" class="headerlink" title="Fast-ELECTRA for Efficient Pre-training"></a>Fast-ELECTRA for Efficient Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07347">http://arxiv.org/abs/2310.07347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao, Xiaodong Liu</li>
<li>for: 提高语言模型的预训练效率</li>
<li>methods: 利用现有的语言模型作为助手模型，通过温度升降调学法构建学习课程，提高主模型的预训练效果</li>
<li>results: 与现有的ELECTRA预训练方法相当，同时减少计算和存储成本，提高预训练稳定性，降低参数的敏感性<details>
<summary>Abstract</summary>
ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability.
</details>
<details>
<summary>摘要</summary>
《ELECTRA》预训语言模型通过检测序列中的Token进行替换，以提高效率。然而，《ELECTRA》的潜力受到auxiliary模型的训练成本限制。具体来说，这个模型只是在主模型的训练中作为协助者，而不是主要的训练对象，因此训练成本大部分是浪费的。为了解决这个问题，我们提出了《Fast-ELECTRA》方法，利用现有的语言模型作为auxiliary模型。通过对主模型的输出分布进行温度封顶并遵循降温规则，我们构建了一个学习课程 для主模型。与传统ELECTRA预训方法相比，我们的方法可以达到相同的性能水平，同时减少了计算和存储成本，并减少了对hyperparameter的敏感性和预训稳定性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Social-Motion-Latent-Space-and-Human-Awareness-for-Effective-Robot-Navigation-in-Crowded-Environments"><a href="#Exploring-Social-Motion-Latent-Space-and-Human-Awareness-for-Effective-Robot-Navigation-in-Crowded-Environments" class="headerlink" title="Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments"></a>Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07335">http://arxiv.org/abs/2310.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junaid Ahmed Ansari, Satyajit Tourani, Gourav Kumar, Brojeshwar Bhowmick</li>
<li>for: 提出一种基于社交动作幂数空间的人工智能机器人导航方法，以便从社交动作幂数空间生成机器人控制。</li>
<li>methods: 利用社交动作幂数空间，提出的方法在社交导航指标（如成功率、导航时间、轨迹长度）上具有显著提高，并生成更平滑（具有较少摇摆和旋转偏差）的轨迹。</li>
<li>results: 比较基线模型，提出的方法在不同场景中具有显著的优势，并且通过引入人类意识，使机器人的轨迹更短暂平滑。<details>
<summary>Abstract</summary>
This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的社交机器人导航方法，通过学习生成机器人控制从社交动作潜在空间中生成机器人控制。通过利用这个社交动作潜在空间，提议的方法可以实现重要的社交导航指标的提高，如成功率、导航时间和轨迹长度，并产生更平滑（ menos jerk 和 angular deviation）、更 anticipatory 的轨迹。在不同场景中，对基线模型进行比较，表明提议的方法具有显著的优势。此外，在社交机器人导航框架中引入人类意识的概念，表明在机器人与人类之间的互动中，机器人可以更快、更平滑地前进，归功于人类对机器人的正面互动。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Instruction-tuning-Large-Language-Models-in-Chinese"><a href="#An-Empirical-Study-of-Instruction-tuning-Large-Language-Models-in-Chinese" class="headerlink" title="An Empirical Study of Instruction-tuning Large Language Models in Chinese"></a>An Empirical Study of Instruction-tuning Large Language Models in Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07328">http://arxiv.org/abs/2310.07328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phoebussi/alpaca-cot">https://github.com/phoebussi/alpaca-cot</a></li>
<li>paper_authors: Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang</li>
<li>for: 这份研究的目的是对中文大型自然语言模型（LLM）进行深入的实验研究，以便更好地适应中文指令。</li>
<li>methods: 这份研究使用了三个重要的元素：LLM基础、参数效率方法和指令数据类型，并对其他因素，如连续思维数据和人类价值调整，进行了实验研究。</li>
<li>results: 这份研究获得了一些有用的结果，包括发现LLM基础和参数效率方法对于中文指令适应有所不同，并且发现了一些关于中文指令数据类型和人类价值调整的影响。这些结果可以作为开源社区中 instruction-tuning LLMs 的实践指南，帮助将 ChatGPT 等 LLM 更好地适应中文指令。<details>
<summary>Abstract</summary>
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLMs that is comparable to ChatGLM. The code and data are available at https://github.com/PhoebusSi/Alpaca-CoT.
</details>
<details>
<summary>摘要</summary>
成功的ChatGPT证明大语言模型（LLM）在人工通用智能（AGI）中的潜力。随后，LLM的发布激发了开源社区对于 instruction-tuning的兴趣，这被认为可以加速ChatGPT的复制过程。然而，对于中文的LLM instruction-tuning研究仍在初 stages。因此，本文进行了深入的实证研究，以提供可以在中文上效果地自定义LLM的热点探讨。具体来说，我们系统地探讨LLM基础、参数效率方法和指令数据类型等三个最重要的元素。此外，我们还进行了实验研究其他因素，如串行思维数据和人类价值对调。我们希望通过这个实证研究，为开放的中文版ChatGPT做出一定的贡献，并释放一个与ChatGPT相当的中文LLM。代码和数据可以在https://github.com/PhoebusSi/Alpaca-CoT上获取。
</details></li>
</ul>
<hr>
<h2 id="An-Adversarial-Example-for-Direct-Logit-Attribution-Memory-Management-in-gelu-4l"><a href="#An-Adversarial-Example-for-Direct-Logit-Attribution-Memory-Management-in-gelu-4l" class="headerlink" title="An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l"></a>An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07325">http://arxiv.org/abs/2310.07325</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Dao, Yeu-Tong Lao, Can Rager, Jett Janiak</li>
<li>for: 提供了一个4层转换器的具体证据，具体地说是对于内存管理。</li>
<li>methods: 通过发现清理行为，即在前向传播中模型组件一致地除去上一步输出。</li>
<li>results: 该研究发现直接logit责任分析方法不准确，并提供了一些具体的示例，证明这种方法在不考虑清理行为的情况下是错误的。<details>
<summary>Abstract</summary>
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
</details>
<details>
<summary>摘要</summary>
我们提供了具体证据，证明transformer中的内存管理是四层结构。具体来说，我们发现了“清理”行为，其中模型组件在前进过程中一致地 removing输出前一个组件的输出。我们的发现表明，使用直接Logit Attribution解释技术可能会得到不准确的结果。我们提供了明确的例子，证明这种技术在不考虑清理行为的情况下是不准确的。
</details></li>
</ul>
<hr>
<h2 id="On-the-Impact-of-Cross-Domain-Data-on-German-Language-Models"><a href="#On-the-Impact-of-Cross-Domain-Data-on-German-Language-Models" class="headerlink" title="On the Impact of Cross-Domain Data on German Language Models"></a>On the Impact of Cross-Domain Data on German Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07321">http://arxiv.org/abs/2310.07321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Dada, Aokun Chen, Cheng Peng, Kaleb E Smith, Ahmad Idrissi-Yaghir, Constantin Marc Seibold, Jianning Li, Lars Heiliger, Xi Yang, Christoph M. Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek, Yonghui Wu</li>
<li>for: 这个论文是为了检验大型语言模型是否可以通过多个领域数据进行训练，以提高其性能。</li>
<li>methods: 作者使用了不同大小的模型，从122M到750M参数，并在多个下游任务上进行了广泛的比较。</li>
<li>results: 研究发现，使用多个领域数据进行训练的模型可以超过单独使用高质量数据进行训练的模型，提高到4.45%。Here’s the English version of the paper’s abstract for reference:”Recent successes of generative large language models have shed light on the benefits of cross-domain datasets. In this work, we present a German dataset comprising texts from five domains, along with a high-quality dataset. We train a series of models ranging from 122M to 750M parameters on both datasets and conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements of up to 4.45% over the previous state-of-the-art. The models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/ikim-uk-essen">https://huggingface.co/ikim-uk-essen</a>.”<details>
<summary>Abstract</summary>
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
</details>
<details>
<summary>摘要</summary>
传统上，大型语言模型通常是通过全网爬虫或域 especific 数据进行训练。然而，最近的生成大型语言模型的成功，抛光了跨领域数据的好处。为了评估数据多样性的重要性，我们提供了一个德国数据集，包括五个领域的文本，以及另一个数据集的高质量数据。通过训练多种模型（122M-750M参数）在两个数据集上，我们进行了多个下游任务的完整的Benchmark。我们的发现表明，基于跨领域数据集训练的模型，在质量数据alone 训练的模型之上出现了改进，最高达4.45%。这些模型可以在https://huggingface.co/ikim-uk-essen 获取。
</details></li>
</ul>
<hr>
<h2 id="WiGenAI-The-Symphony-of-Wireless-and-Generative-AI-via-Diffusion-Models"><a href="#WiGenAI-The-Symphony-of-Wireless-and-Generative-AI-via-Diffusion-Models" class="headerlink" title="WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models"></a>WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07312">http://arxiv.org/abs/2310.07312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 本文旨在探讨generative AI在无线通信系统中的应用，以为未来的无线通信系统提供基础。</li>
<li>methods: 本文介绍了diffusion-based generative models，作为现代生成模型的新状态之一，其在无线通信系统中的应用被讨论。两个案例也被提出，以示diffusion models在无线通信系统中的应用。</li>
<li>results: 本文提出了一种基于DDPM的无线通信方案，可以提高bit error rate的性能，并在 transmitter 端使用DDPM shape constellation symbols，实现了robust out-of-distribution性能。<details>
<summary>Abstract</summary>
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose denoising diffusion probabilistic models (DDPM) for a wireless communication scheme with non-ideal transceivers, where 30% improvement is achieved in terms of bit error rate. As the second application, DDPMs are employed at the transmitter to shape the constellation symbols, highlighting a robust out-of-distribution performance. Finally, future directions and open issues for the development of generative AI-based wireless systems are discussed to promote future research endeavors towards wireless generative AI (WiGenAI).
</details>
<details>
<summary>摘要</summary>
创新基础模型，如GPT-3和稳定扩散模型，对人工智能（AI）领域进行了 paradigm shift，推动了基于生成AI的系统的发展。在数据通信和网络方面，预计未来的无线通信系统将普遍应用AI和机器学习（AI/ML）算法，需要新的AINative解决方案来应对新兴的通信enario。本文介绍了生成AI在无线通信系统的应用，为这一领域的研究提供了基础。基于扩散的生成模型被介绍为新的状态艺术生成模型，其应用在无线通信系统中被讨论。此外，本文还提出了使用Diffusion模型来提高无线通信系统的鲁棒性和可靠性。 Specifically, we propose denoising diffusion probabilistic models (DDPM) for a wireless communication scheme with non-ideal transceivers, where 30% improvement is achieved in terms of bit error rate. As the second application, DDPMs are employed at the transmitter to shape the constellation symbols, highlighting a robust out-of-distribution performance. Finally, future directions and open issues for the development of generative AI-based wireless systems are discussed to promote future research endeavors towards wireless generative AI (WiGenAI).
</details></li>
</ul>
<hr>
<h2 id="RobustGEC-Robust-Grammatical-Error-Correction-Against-Subtle-Context-Perturbation"><a href="#RobustGEC-Robust-Grammatical-Error-Correction-Against-Subtle-Context-Perturbation" class="headerlink" title="RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation"></a>RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07299">http://arxiv.org/abs/2310.07299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hillzhang1999/robustgec">https://github.com/hillzhang1999/robustgec</a></li>
<li>paper_authors: Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi</li>
<li>for: 这个论文是为了评估语音识别系统的上下文稳定性而写的。</li>
<li>methods: 这个论文使用了5000个GEC案例，每个案例包含一个原始错误 sentence pair和五个人工注解的变体。</li>
<li>results: 根据RobustGEC测试，现有的GEC系统仍然缺乏上下文变化的抗颤势能力。此外，这篇论文还提出了一种简单 yet effective的方法来解决这个问题。<details>
<summary>Abstract</summary>
Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
</details>
<details>
<summary>摘要</summary>
grammatical error correction (GEC) 系统在日常写作任务中发挥重要作用。然而，用户可能会遇到一些 GEC 系统，初始性能良好但在输入略微改变后失败 Correcting errors。为确保理想的用户体验，一个可靠的 GEC 系统应该具备在不相关文本干扰下提供一致和准确的建议，我们称之为上下文稳定性。在这篇论文中，我们介绍了 RobustGEC，一个用于评估 GEC 系统的上下文稳定性的benchmark。RobustGEC 包含 5,000 个 GEC 案例，每个案例包含一对原始错误 Corrected sentence pair 和 five carefully crafted by human annotators的变体。通过使用 RobustGEC，我们发现当前的 GEC 系统仍然缺乏对上下文干扰的抗性。此外，我们还提出了一种简单 yet effective的方法来解决这一问题。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Memorization-Violating-Privacy-Via-Inference-with-Large-Language-Models"><a href="#Beyond-Memorization-Violating-Privacy-Via-Inference-with-Large-Language-Models" class="headerlink" title="Beyond Memorization: Violating Privacy Via Inference with Large Language Models"></a>Beyond Memorization: Violating Privacy Via Inference with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07298">http://arxiv.org/abs/2310.07298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev</li>
<li>for: 研究者们主要关注现有大语言模型（LLM）是否可以从文本中提取个人信息。</li>
<li>methods: 研究者们使用现有的LLM进行实验，构建了一个基于Reddit Profiler的数据集，并证明了现有LLM可以很准确地推断个人特征（如地点、收入、性别），达到人工标注的85%和95.8%的顶峰准确率。</li>
<li>results: 研究发现，人们在与LLM智能客服交互时可能会遭受隐私侵犯的威胁，LLM可以通过假 benevolent 的问题提取个人信息。此外，常见的mitigation Strategies，如文本匿名和模型对齐，现在无法保护用户隐私。<details>
<summary>Abstract</summary>
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.
</details>
<details>
<summary>摘要</summary>
当前的隐私研究主要集中在大语言模型（LLM）的训练数据抽取问题上。同时，模型的推理能力有了很大的提升。这引发了人们关注 whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. 在这项工作中，我们提供了首个全面研究大语言模型在文本中推理个人特性的能力。我们构建了一个包含真实的Reddit Profile的数据集，并显示现有的LLM可以从文本中推理广泛的个人特性（例如地点、收入、性别），达到了人类的85% top-1和95.8% top-3准确率，而且只需要人类的100倍成本和时间。随着人们日益与LLM驱动的聊天机器人在生活中增加，我们还探讨了emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions。最后，我们表明现有的mitigation measures，例如文本匿名和模型对齐，目前无法保护用户隐私。我们的发现表明现有的LLM可以在前所未有的规模上进行个人数据推理。在缺乏有效防御的情况下，我们呼吁更广泛的讨论LLM隐私影响，努力保护更多的隐私。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-on-Large-Language-Models-in-Healthcare-A-Case-Study-of-BioBERT"><a href="#An-Analysis-on-Large-Language-Models-in-Healthcare-A-Case-Study-of-BioBERT" class="headerlink" title="An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT"></a>An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07282">http://arxiv.org/abs/2310.07282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyni Sharaf, V. S. Anoop</li>
<li>for: 这 paper  investigate 使用大型语言模型，特别是 BioBERT，在医疗领域中的应用。</li>
<li>methods: 这 paper  explores 如何使用 BioBERT 在医疗领域中应用，包括数据收集、数据标注、特殊的预处理技术和模型评估等方法。</li>
<li>results: 这 paper 获得了一系列结果，包括模型的可读性和可靠性，以及在医疗领域中的应用效果。同时， paper 还考虑了医疗领域的伦理问题，如患者隐私和数据安全。<details>
<summary>Abstract</summary>
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the paper covers aspects related to model evaluation, with a focus on healthcare benchmarks and functions like processing of natural language in biomedical, question-answering, clinical document classification, and medical entity recognition. It explores techniques to improve the model's interpretability and validates its performance compared to existing healthcare-focused language models. The paper thoroughly examines ethical considerations, particularly patient privacy and data security. It highlights the benefits of incorporating BioBERT into healthcare contexts, including enhanced clinical decision support and more efficient information retrieval. Nevertheless, it acknowledges the impediments and complexities of this integration, encompassing concerns regarding data privacy, transparency, resource-intensive requirements, and the necessity for model customization to align with diverse healthcare domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BioT5-Enriching-Cross-modal-Integration-in-Biology-with-Chemical-Knowledge-and-Natural-Language-Associations"><a href="#BioT5-Enriching-Cross-modal-Integration-in-Biology-with-Chemical-Knowledge-and-Natural-Language-Associations" class="headerlink" title="BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"></a>BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07276">http://arxiv.org/abs/2310.07276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QizhiPei/BioT5">https://github.com/QizhiPei/BioT5</a></li>
<li>paper_authors: Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan</li>
<li>for: 强化药物探索的生物研究进步，挖掘生物学中的分子、蛋白质和自然语言知识的整合。</li>
<li>methods: 提出了$\mathbf{BioT5}$框架，利用SELFIES实现100%的稳定分子表示，从生物学文献中提取生物知识，并在不同知识结构中进行分类。</li>
<li>results: 经过精度调整后，$\mathbf{BioT5}$在各种任务上显示出优秀表现，能够更好地捕捉生物实体之间的下面关系和性质。<details>
<summary>Abstract</summary>
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\href{https://github.com/QizhiPei/BioT5}{Github}$.
</details>
<details>
<summary>摘要</summary>
Recent advancements in biological research have leveraged the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models have several limitations, such as generating invalid molecular SMILES, underutilizing contextual information, and treating structured and unstructured knowledge equally. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ uses SELFIES to generate $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\href{https://github.com/QizhiPei/BioT5}{Github}$.
</details></li>
</ul>
<hr>
<h2 id="CoPAL-Corrective-Planning-of-Robot-Actions-with-Large-Language-Models"><a href="#CoPAL-Corrective-Planning-of-Robot-Actions-with-Large-Language-Models" class="headerlink" title="CoPAL: Corrective Planning of Robot Actions with Large Language Models"></a>CoPAL: Corrective Planning of Robot Actions with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07263">http://arxiv.org/abs/2310.07263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger</li>
<li>for: 本研究旨在帮助机器人系统实现完全自主的任务，以替代人类的干预。</li>
<li>methods: 本研究提出了一种基于大语言模型的任务和动作规划系统，包括理解、规划和动作生成等多个认知水平的协同执行。</li>
<li>results: 实验和实际场景评估表明，提出的反馈体系可以提高计划执行性、正确性和时间复杂度。<details>
<summary>Abstract</summary>
In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
</details>
<details>
<summary>摘要</summary>
在实现完全自动化机器人系统，替代人类的任务的挑战中，开放环境的复杂性具有重要的意义。本研究对大型语言模型（LLM）应用于机器人任务和动作规划领域进行了贡献。我们提出了一种系统架构，该架构将多种认知层次融合，包括理据、规划和动作生成。系统核心含有一种新的重划策略，可以处理物理基础、逻辑和Semantic错误在生成的计划中。我们通过实验证明了我们的反馈架构的有效性，特别是其对执行性、正确性和时间复杂度的影响。我们在一个 simulations 和两个复杂的实际场景中进行了Empirical评估：块世界、摊手和营养师。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Hidden-Connections-Iterative-Tracking-and-Reasoning-for-Video-grounded-Dialog"><a href="#Uncovering-Hidden-Connections-Iterative-Tracking-and-Reasoning-for-Video-grounded-Dialog" class="headerlink" title="Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog"></a>Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07259">http://arxiv.org/abs/2310.07259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyu-zhang/itr">https://github.com/hyu-zhang/itr</a></li>
<li>paper_authors: Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, Weili Guan, Liqiang Nie</li>
<li>for: 提高视频对话的答案生成精度，解决现有方法对 диалога历史和视频内容的不足理解。</li>
<li>methods: 提出了一种迭代跟踪和理解策略，结合文本编码器、视频编码器和生成器，以提高对话历史和视频内容的理解。文本编码器强化了路径跟踪和汇总机制，更好地捕捉对话历史中重要的细节；视频编码器采用迭代理解网络，更好地提取和强调视频中重要的视觉特征。</li>
<li>results: 经验证明，该方法在两个知名数据集上表现出色，证明了我们的设计的可靠性和适应能力。<details>
<summary>Abstract</summary>
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-2 model as our response generator, stitching together coherent and contextually apt answers. Our empirical assessments, conducted on two renowned datasets, testify to the prowess and adaptability of our proposed design.
</details>
<details>
<summary>摘要</summary>
对于传统的视觉问答，视频基础对话需要深刻理解对话历史和视频内容以生成准确的回答。虽然现有的方法尝试过努力，但它们经常遇到对话历史和视频信息的总结和融合的挑战。为了缺失这个 gap，我们提出了一种迭代跟踪和理解策略，结合文本编码器、视觉编码器和生成器。我们的文本编码器强化了一条路径跟踪和汇集机制，能够从对话历史中提取关键信息，用于解决提出的问题。同时，我们的视觉编码器使用迭代理解网络，精心制作，以提炼和强调视频中的关键视觉特征，提高视觉理解的深度。将这些强化的信息与预训练的 GPT-2 模型相结合，我们的响应生成器可以拼接出准确和上下文相关的回答。我们的实验测试表明，我们的提议的设计在两个知名的数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ADMEOOD-Out-of-Distribution-Benchmark-for-Drug-Property-Prediction"><a href="#ADMEOOD-Out-of-Distribution-Benchmark-for-Drug-Property-Prediction" class="headerlink" title="ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction"></a>ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07253">http://arxiv.org/abs/2310.07253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuoying Wei, Xinlong Wen, Lida Zhu, Songquan Li, Rongbo Zhu</li>
<li>for: This paper aims to address the challenges of obtaining accurate and valid information for drug molecules, particularly in the presence of noise and inconsistency.</li>
<li>methods: The proposed method, ADMEOOD, is a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. It includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD).</li>
<li>results: The experimental results demonstrate the effectiveness of the proposed partition method in ADMEOOD, with significant differences in performance between in-distribution and out-of-distribution data. Additionally, the paper shows that ERM and other models exhibit distinct trends in performance across different domains and measurement types.Here’s the simplified Chinese translation of the three points:</li>
<li>for: 这篇论文目标是解决药物分子准确性和有效性信息的挑战，特别是在噪声和不一致性的情况下。</li>
<li>methods: 提议的方法是 ADMEOOD，一种专门针对药性预测的 OOD 数据集编辑和标准准则。它包括两种 OOD 数据推移：噪声推移和概念冲突推移（CCD）。</li>
<li>results: 实验结果表明 ADMEOOD 中的分区方法的效果，在各种领域和测量类型下，ERM 和其他模型在不同领域和测量类型上的性能呈现出明显的趋势。<details>
<summary>Abstract</summary>
Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finally, it tested on a variety of domain generalization models, and the experimental results demonstrate the effectiveness of the proposed partition method in ADMEOOD: ADMEOOD demonstrates a significant difference performance between in-distribution and out-of-distribution data. Moreover, ERM (Empirical Risk Minimization) and other models exhibit distinct trends in performance across different domains and measurement types.
</details>
<details>
<summary>摘要</summary>
获取准确和有效的药分子信息是一项关键和挑战性的任务。然而，化学知识和信息在过去100年内从不同的地区、实验室和实验目的中积累了大量的数据。相对较少的研究将关注到违andas（OOD）问题中的噪声和不一致性，这可能会导致软弱的Robustness和不满足的性能。本研究提出了一个新的标准化 benchmark ADMEOOD，特意为药性预测而设计的OOD数据集和标准准则。ADMEOOD从Chembl和相关文献中收集了27个ADME（吸收、分布、代谢、排除）药性特征。此外，它还包括两种类型的OOD数据变化：噪声变化（Noise Shift）和概念冲突漂移（CCD）。噪声变化根据噪声水平进行分类环境，而CCD描述了原始数据中存在冲突的标签。最后，它对几种领域总结模型进行测试，实验结果表明提案的分区方法在ADMEOOD中的效果。ADMEOOD在各种领域和测量类型之间的性能差异非常明显，而ERM和其他模型在不同领域和测量类型之间也展现了不同的趋势。
</details></li>
</ul>
<hr>
<h2 id="Ethical-Reasoning-over-Moral-Alignment-A-Case-and-Framework-for-In-Context-Ethical-Policies-in-LLMs"><a href="#Ethical-Reasoning-over-Moral-Alignment-A-Case-and-Framework-for-In-Context-Ethical-Policies-in-LLMs" class="headerlink" title="Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs"></a>Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07251">http://arxiv.org/abs/2310.07251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury</li>
<li>for: 这项研究的目的是如何在全球范围内处理价值多元主义，而不是将人工智能语言模型（LLM） morally align 到特定的伦理原则中。</li>
<li>methods: 作者提出了一种框架，该框架将道德困境与不同形式的 normative ethics 的道德原则集成在一起，并在不同层次上进行了实验。</li>
<li>results: 初步实验显示，使用 GPT-x 模型时，虽然 GPT-4 能够 nearly perfectly 处理道德决策，但模型仍然受到西方和英语社会的道德价值偏见。<details>
<summary>Abstract</summary>
In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
</details>
<details>
<summary>摘要</summary>
在这份位点论文中，我们认为，而不是将LLM们道德上对应到特定的道德原则中，我们应该将通用的道德思维能力注入到它们中，以便在全球范围内处理多元的价值观。当提供了一个道德政策时，一个LLM应该能够作出道德一致的决策。我们开发了一个整合道德困境与不同形式的normative ethics的道德原则的框架。初步实验表明，虽然GPT-4是一个几乎完美的道德思维者，但模型仍然偏向西方和英语社会的道德价值观。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-modeling-for-stochastic-crack-growth-processes-in-structural-health-monitoring-applications"><a href="#Surrogate-modeling-for-stochastic-crack-growth-processes-in-structural-health-monitoring-applications" class="headerlink" title="Surrogate modeling for stochastic crack growth processes in structural health monitoring applications"></a>Surrogate modeling for stochastic crack growth processes in structural health monitoring applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07241">http://arxiv.org/abs/2310.07241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas E. Silionis, Konstantinos N. Anyfantis</li>
<li>for: 这个研究旨在使用结构Response Data来预测未来的裂解成长，以实现预测维护。</li>
<li>methods: 这个研究使用了物理基于的裂解成长模型，以涵盖不同的不确定性源。 furthermore, the research employs an approach inspired by latent variable modeling, using Gaussian Process (GP) regression models to generate prior distributions for different Bayesian SHM tasks.</li>
<li>results: the research demonstrates the effectiveness of the proposed method in accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes, and shows that the surrogate models can be used to generate prior distributions for different Bayesian SHM tasks. The study also assesses the implementation of the approach in a numerical setting and evaluates the model performance for two fundamental crack SHM problems, namely crack length monitoring (damage quantification) and crack growth monitoring (damage prognosis).<details>
<summary>Abstract</summary>
Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as the application of interest. Implementation is carried out in a numerical setting and model performance is assessed for two fundamental crack SHM problems; namely crack length monitoring (damage quantification) and crack growth monitoring (damage prognosis).
</details>
<details>
<summary>摘要</summary>
metal 结构中的疲劳裂隙是最常见的衰减类型，具有重要的可靠性意义。 现代结构健康监测（SHM）技术的发展使得可以通过结构响应数据预测未来裂隙增长，以实现预测维护。 正确地表示不同类型的不确定性在 statistically crack growth （SCG）过程中是一个非常困难的任务。 本工作基于先前的物理基础 SCG 模型下的不确定性，以及负荷相关的不确定性，进行了进一步的研究。 目标是构建可 computationally efficient 的概率模型，以successfully encode 不同类型的不确定性。 我们采用了隐藏变量模型的方法，使用 Gaussian Process （GP）回归模型，以便这些模型可以用来生成不同 Bayesian SHM 任务的先验分布。 实现在数值上，并评估模型在两个基本的裂隙 SHM 问题上的性能，即裂隙长度监测（损害量评估）和裂隙增长监测（损害预测）。
</details></li>
</ul>
<hr>
<h2 id="Using-Learnable-Physics-for-Real-Time-Exercise-Form-Recommendations"><a href="#Using-Learnable-Physics-for-Real-Time-Exercise-Form-Recommendations" class="headerlink" title="Using Learnable Physics for Real-Time Exercise Form Recommendations"></a>Using Learnable Physics for Real-Time Exercise Form Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07221">http://arxiv.org/abs/2310.07221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Jaiswal, Gautam Chauhan, Nisheeth Srivastava</li>
<li>for: 这篇论文是用于提供实时评估和纠正运动技巧的算法架构。</li>
<li>methods: 这篇论文使用MediaPipe进行姿势识别，使用峰值振荡检测缩数量，并使用可学习的物理模拟器追踪运动动作的演化。</li>
<li>results: 这篇论文的实验结果显示，这个算法架构可以实时识别运动技巧的问题，并提供精确的纠正建议，具有高敏感和特异度。<details>
<summary>Abstract</summary>
Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of workout injuries.
</details>
<details>
<summary>摘要</summary>
好的姿势和形态是训练安全和生产力的关键。即使在健身房设施中，教练可能不可靠地提供反馈。rehabilitation therapies和健身训练可以从推荐系统中受益，该系统可以在实时提供诊断和修正建议，具有高敏感和特异度。在这篇论文中，我们提出了一个算法管道，可以诊断运动技巧问题并提供修正建议，并在实时进行诊断。我们使用MediaPipe进行姿势识别， COUNT repetitions使用峰值特征检测，并使用可学习的物理模拟器跟踪运动的动态变化。一个测试视频根据异常从学习的典型运动姿势进行诊断。系统被评估在6种全身和上半身运动中。这些实时建议，通过低成本的设备如智能手机，将让运动员在自修练中修正潜在的错误，使自修练变得可能，同时降低了训练伤害的风险。
</details></li>
</ul>
<hr>
<h2 id="Improved-Membership-Inference-Attacks-Against-Language-Classification-Models"><a href="#Improved-Membership-Inference-Attacks-Against-Language-Classification-Models" class="headerlink" title="Improved Membership Inference Attacks Against Language Classification Models"></a>Improved Membership Inference Attacks Against Language Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07219">http://arxiv.org/abs/2310.07219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shlomit Shachor, Natalia Razinkov, Abigail Goldsteen</li>
<li>for: 本研究旨在评估机器学习模型中的隐私风险，以便决策使用、部署或共享模型。</li>
<li>methods: 本研究提出了一种新的检测机器学习模型隐私风险的框架，该框架利用ensemble方法生成了多种特殊攻击模型，以便在不同的数据子集上进行攻击。</li>
<li>results: 研究发现，该框架可以在分类任务中实现更高的攻击准确率，比单个攻击模型或每个类别标签的攻击模型都高。<details>
<summary>Abstract</summary>
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
</details>
<details>
<summary>摘要</summary>
人工智能系统在日常生活中广泛应用，包括零售、制造、医疗等领域。随着人工智能的普及，相关的风险也得到了关注，包括模型训练数据的隐私风险。评估机器学习模型的隐私风险是必要的，以便做出了知情决策是否使用、部署或共享模型。一种常见的隐私风险评估方法是通过运行一个或多个已知攻击来测试模型的成功率。我们提出了一种新的扩展 ensemble 方法，通过生成不同数据集的多个特化攻击模型来实现。我们示出了这种方法可以在分类任务上 achieved higher accuracy than single attack model 和 class label 的攻击模型。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Agent-Interaction-in-Multi-agent-Reinforcement-Learning-for-Cost-efficient-Generalization"><a href="#Quantifying-Agent-Interaction-in-Multi-agent-Reinforcement-Learning-for-Cost-efficient-Generalization" class="headerlink" title="Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization"></a>Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07218">http://arxiv.org/abs/2310.07218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Chen, Chen Tang, Ran Tian, Chenran Li, Jinning Li, Masayoshi Tomizuka, Wei Zhan</li>
<li>for: 本研究旨在探讨多智能体强化学习（MARL）中的总结问题，并提出一种级别的影响指标（LoI）来衡量多智能体之间的互动程度。</li>
<li>methods: 本研究使用了一种基于LoI的资源分配策略，以帮助在多种enario下培养多个策略。</li>
<li>results: 研究发现，在特定的scenario和环境下，一个更多样化的合作者 durante 训练可以提高ego agent的总结性能，但这种改善的程度因enario和环境而异。LoI 有效地预测这些改善的差异。此外，基于LoI的资源分配策略可以在受限的计算预算下实现更高的性能。<details>
<summary>Abstract</summary>
Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocation based on LoI can achieve higher performance than uniform allocation under the same computation budget.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocation based on LoI can achieve higher performance than uniform allocation under the same computation budget."中文翻译：在多Agent reinforcement learning（MARL）中，总体化带来了重要的挑战。特定情况下，代理人的政策会影响它所看到的其他代理人的行为。我们可以通过评估这种关系的数学性来更好地训练代理人。在这篇研究中，我们提出了“影响水平”（LoI）这个指标，用于衡量代理人之间的互动强度在特定情况下和环境下。我们发现，通常情况下，在训练过程中使用更多的合作代理人会提高自我代理人的总体化性能；然而，这种改进的程度随着情况和环境的变化而变化。LoI proves effective in predicting these improvement disparities within specific scenarios。此外，我们还引入了根据LoI的资源分配策略，用于在受限制的预算下训练多种政策。我们的结果表明，基于LoI的策略性资源分配可以在同样的计算预算下达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Enabled-Automatic-Vessel-Draft-Reading-for-Intelligent-Maritime-Surveillance"><a href="#Multi-Task-Learning-Enabled-Automatic-Vessel-Draft-Reading-for-Intelligent-Maritime-Surveillance" class="headerlink" title="Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance"></a>Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07212">http://arxiv.org/abs/2310.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxiang Qu, Ryan Wen Liu, Chenjie Zhao, Yu Guo, Sendren Sheng-Dong Xu, Fenghua Zhu, Yisheng Lv<br>for: 这个论文的目的是提出一种基于计算机视觉技术的高可靠性船舶深度计算方法，以帮助确定船舶是否正常载荷。methods: 这种方法使用多任务学习技术，包括检测标记、船舶缩放识别、船舶&#x2F;水 Segmentation 和最终的深度计算。results: 对实验 dataset 进行了广泛的测试，并证明了这种方法的超过其他方法的高可靠性、robustness 和高效性。计算速度超过 40 FPS，满足实时海上监测的需求，以确保船舶交通安全。<details>
<summary>Abstract</summary>
The accurate and efficient vessel draft reading (VDR) is an important component of intelligent maritime surveillance, which could be exploited to assist in judging whether the vessel is normally loaded or overloaded. The computer vision technique with an excellent price-to-performance ratio has become a popular medium to estimate vessel draft depth. However, the traditional estimation methods easily suffer from several limitations, such as sensitivity to low-quality images, high computational cost, etc. In this work, we propose a multi-task learning-enabled computational method (termed MTL-VDR) for generating highly reliable VDR. In particular, our MTL-VDR mainly consists of four components, i.e., draft mark detection, draft scale recognition, vessel/water segmentation, and final draft depth estimation. We first construct a benchmark dataset related to draft mark detection and employ a powerful and efficient convolutional neural network to accurately perform the detection task. The multi-task learning method is then proposed for simultaneous draft scale recognition and vessel/water segmentation. To obtain more robust VDR under complex conditions (e.g., damaged and stained scales, etc.), the accurate draft scales are generated by an automatic correction method, which is presented based on the spatial distribution rules of draft scales. Finally, an adaptive computational method is exploited to yield an accurate and robust draft depth. Extensive experiments have been implemented on the realistic dataset to compare our MTL-VDR with state-of-the-art methods. The results have demonstrated its superior performance in terms of accuracy, robustness, and efficiency. The computational speed exceeds 40 FPS, which satisfies the requirements of real-time maritime surveillance to guarantee vessel traffic safety.
</details>
<details>
<summary>摘要</summary>
《高精度和高效的船舶略深度读取（VDR）是智能海上监测中的一个重要组成部分，可以帮助判断船舶是否过载。计算机视觉技术具有优秀的价格-性能比，成为船舶略深度读取的优选方法。然而，传统的估算方法容易受到低质量图像的敏感性、高计算成本等限制。在这种情况下，我们提出了一种基于多任务学习的计算方法（简称MTL-VDR），可以生成高可靠性的VDR。MTL-VDR的主要组成部分包括：略深度标记检测、略深度比例识别、船舶/水域分割和最终略深度估算。我们首先构建了相关的略深度标记检测 benchmark dataset，并使用高效和可靠的卷积神经网络进行检测任务的准确执行。然后，我们提出了多任务学习方法，以同时进行略深度比例识别和船舶/水域分割。为了在复杂的condition下（如损坏和污染的比例、等）获得更加稳定的VDR，我们提出了自动修正方法，该方法基于略深度比例的空间分布规则。最后，我们采用了一种适应计算方法，以获得高可靠性和稳定性的略深度估算。我们对实际的 dataset进行了广泛的实验，并与现状技术进行了比较。结果表明，我们的 MTL-VDR 方法在精度、稳定性和效率三个方面具有明显的优势。计算速度超过 40 FPS，满足了实时海上监测的需求，以保证船舶交通安全。
</details></li>
</ul>
<hr>
<h2 id="State-of-the-Art-on-Diffusion-Models-for-Visual-Computing"><a href="#State-of-the-Art-on-Diffusion-Models-for-Visual-Computing" class="headerlink" title="State of the Art on Diffusion Models for Visual Computing"></a>State of the Art on Diffusion Models for Visual Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07204">http://arxiv.org/abs/2310.07204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Björn Ommer, Christian Theobalt, Peter Wonka, Gordon Wetzstein</li>
<li>for:  This paper provides an overview of the rapidly advancing field of diffusion-based generative AI, including the basic mathematical concepts, implementation details, and design choices of the popular Stable Diffusion model.</li>
<li>methods:  The paper covers the basic mathematical concepts of diffusion models, implementation details and design choices of the Stable Diffusion model, as well as personalization, conditioning, inversion, and other aspects of these generative AI tools.</li>
<li>results:  The paper provides a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Additionally, the paper discusses available datasets, metrics, open challenges, and social implications.<details>
<summary>Abstract</summary>
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.
</details>
<details>
<summary>摘要</summary>
领域Visual computing在人工智能（AI）的激发下得到了快速发展，这些AI技术为图像、视频和3D场景的生成、编辑和重建提供了无 precedent的能力。在这些领域中，扩散模型是AI建筑的首选。过去一年中，有关扩散工具和应用的学术论文发表量呈指数增长趋势，在计算机图形学、计算机视觉和人工智能领域发表新的论文。由于这一领域的快速发展，难以跟进所有最新的发展。本州Of the art report（STAR）的目标是介绍扩散模型的基本数学概念、实现细节和流行的稳定扩散模型的设计选择，以及这些生成AI工具的重要方面，包括个性化、条件、反向等。此外，我们还给出了对扩散基于生成和编辑的文献分类，按照生成媒体的类型进行分类，包括2D图像、视频、3D物体、行走和4D场景。最后，我们讨论了可用的数据集、评价指标、开放的挑战和社会影响。本STAR提供了对这个吸引人的主题的直观入门点，对研究人员、艺术家和实践人士都是有帮助的。
</details></li>
</ul>
<hr>
<h2 id="MatChat-A-Large-Language-Model-and-Application-Service-Platform-for-Materials-Science"><a href="#MatChat-A-Large-Language-Model-and-Application-Service-Platform-for-Materials-Science" class="headerlink" title="MatChat: A Large Language Model and Application Service Platform for Materials Science"></a>MatChat: A Large Language Model and Application Service Platform for Materials Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07197">http://arxiv.org/abs/2310.07197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Chen, Fankai Xie, Meng Wan, Yang Yuan, Miao Liu, Zongguo Wang, Sheng Meng, Yangang Wang</li>
<li>for: 本研究旨在预测化学合成过程，帮助材料科学研究中预测化学合成过程的复杂性和缺乏完整数据，目前阻碍我们准确预测化学合成过程。</li>
<li>methods: 本研究使用大规模人工智能模型（GAI），包括自动生成文本和问答系统，并通过精度调整技术来部署特定领域的大规模 AI 模型。研究人员使用 LLama2-7B 模型，并通过 incorporating 13,878 件结构化物质知识数据来提高模型。</li>
<li>results: 研究人员通过 MatChat 模型可以准确预测材料合成过程，并且可以理解和处理材料科学知识。虽然 MatChat 还需要进一步优化，以满足不同的材料设计需求，但这项研究无疑地展示了 MatChat 的强大逻辑能力和材料科学领域的创新潜力。MatChat 现在在线并开放使用，模型和应用框架都是开源的。这项研究为材料科学领域的合作创新奠定了坚实的基础。<details>
<summary>Abstract</summary>
The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in the field of materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science.
</details>
<details>
<summary>摘要</summary>
Predicting chemical synthesis pathways plays a crucial role in materials science research. However, challenges such as the complexity of synthesis pathways and the lack of comprehensive datasets have hindered our ability to predict these chemical processes accurately. Recently, advancements in generative artificial intelligence (GAI) have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in the field of materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science.Here's the text with some notes on the translation:1. "predicting chemical synthesis pathways" is translated as "预测化学合成路径" (pinyin: yùdòu huàxíng lùdào).2. "challenges" is translated as "挑战" (pinyin: tiǎozhàn).3. "GAI" is translated as "生成人工智能" (pinyin: shēngchǎng rénxīn qìyuè), which is a more general term for "generative AI".4. "LLaMA2-7B" is translated as "LLaMA2-7B" (pinyin: lèlāmā2-7b), as it is a proper noun.5. "MatChat" is translated as "Material Chat" (pinyin: wùchǎng), as it is a proper noun.6. "inorganic material synthesis pathways" is translated as "无机材料合成路径" (pinyin: wújī cèwù lùdào).7. "remarkable proficiency" is translated as "卓越的能力" (pinyin: zhùyuè de nénglì).8. "collaborative innovation" is translated as "合作创新" (pinyin: hézuò chūxīn).Please note that the translation is done using Simplified Chinese, and the translation may vary depending on the context and the specific requirements of the project.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Gating-in-Mixture-of-Experts-based-Language-Models"><a href="#Adaptive-Gating-in-Mixture-of-Experts-based-Language-Models" class="headerlink" title="Adaptive Gating in Mixture-of-Experts based Language Models"></a>Adaptive Gating in Mixture-of-Experts based Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07188">http://arxiv.org/abs/2310.07188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong Xu</li>
<li>for: 这篇论文旨在提出适应性网络（MoE）模型，以提高NLPTask的表现，同时保持计算量的常数。</li>
<li>methods: 该论文提出了一种可变门控制的MoE模型，通过变量门控制来实现每个字符的计算量不同，从而提高模型的表现。</li>
<li>results: 实验表明，使用可变门控制MoE模型可以减少训练时间，保持测试结果的质量，最多减少22.5%的训练时间。<details>
<summary>Abstract</summary>
Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning is leveraged to further reduce training time. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the routing decisions and present our insights when adaptive gating is used.
</details>
<details>
<summary>摘要</summary>
This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning is leveraged to further reduce training time. Extensive experiments on diverse NLP tasks show that adaptive gating reduces training time by at most 22.5% while maintaining inference quality.Moreover, we conduct a comprehensive analysis of the routing decisions and present our insights when adaptive gating is used. This study provides a better understanding of the trade-off between computation per token and model performance, and demonstrates the effectiveness of adaptive gating in improving the efficiency of large language models.
</details></li>
</ul>
<hr>
<h2 id="Multiview-Transformer-Rethinking-Spatial-Information-in-Hyperspectral-Image-Classification"><a href="#Multiview-Transformer-Rethinking-Spatial-Information-in-Hyperspectral-Image-Classification" class="headerlink" title="Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification"></a>Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07186">http://arxiv.org/abs/2310.07186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zhang, Yongshan Zhang, Yicong Zhou</li>
<li>for: 本研究旨在提高降落维度图像（HSI）分类的模型性能，通过利用场景特定而非必要的相关性。</li>
<li>methods: 本文提出了一种多视图变换器，包括多视图主成分分析（MPCA）、spectral encoder-decoder（SED）和空间-poolingtokenization transformer（SPTT）。MPCA通过构建spectral多视图观察和应用PCA于每个视图数据来提取低维度视图表示。</li>
<li>results: 实验结果表明，提出的多视图变换器在三个HSI数据集上表现出色，超过了现有方法的性能。<details>
<summary>Abstract</summary>
Identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on spectral and spatial information. An HSI cuboid with a specific patch size is utilized to extract spatial-spectral feature representation for the central pixel. In this article, we investigate that scene-specific but not essential correlations may be recorded in an HSI cuboid. This additional information improves the model performance on existing HSI datasets and makes it hard to properly evaluate the ability of a model. We refer to this problem as the spatial overfitting issue and utilize strict experimental settings to avoid it. We further propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on an HSI via constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional view representation. The combination of view representations, named multiview representation, is the dimension reduction output of the MPCA. To aggregate the multiview information, a fully-convolutional SED with a U-shape in spectral dimension is introduced to extract a multiview feature map. SPTT transforms the multiview features into tokens using the spatial-pooling tokenization strategy and learns robust and discriminative spatial-spectral features for land cover identification. Classification is conducted with a linear classifier. Experiments on three HSI datasets with rigid settings demonstrate the superiority of the proposed multiview transformer over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on spectral and spatial information. an HSI cuboid with a specific patch size is utilized to extract spatial-spectral feature representation for the central pixel. in this article, we investigate that scene-specific but not essential correlations may be recorded in an HSI cuboid. this additional information improves the model performance on existing HSI datasets and makes it hard to properly evaluate the ability of a model. we refer to this problem as the spatial overfitting issue and utilize strict experimental settings to avoid it. we further propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on an HSI via constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional view representation. the combination of view representations, named multiview representation, is the dimension reduction output of the MPCA. to aggregate the multiview information, a fully-convolutional SED with a U-shape in spectral dimension is introduced to extract a multiview feature map. SPTT transforms the multiview features into tokens using the spatial-pooling tokenization strategy and learns robust and discriminative spatial-spectral features for land cover identification. classification is conducted with a linear classifier. experiments on three HSI datasets with rigid settings demonstrate the superiority of the proposed multiview transformer over the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="rpcPRF-Generalizable-MPI-Neural-Radiance-Field-for-Satellite-Camera"><a href="#rpcPRF-Generalizable-MPI-Neural-Radiance-Field-for-Satellite-Camera" class="headerlink" title="rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera"></a>rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07179">http://arxiv.org/abs/2310.07179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtong Zhang, Yuanxiang Li</li>
<li>For: 这个论文旨在开发一种基于多平面图像（MPI）和理想幂束（RPC）的计算机视觉模型，用于生成卫星图像中的新视角。* Methods: 该模型使用多平面图像和理想幂束模型，并通过投影监督和静止深度监督来学习Scene的3D结构。* Results: 与state-of-the-art nerf-based方法相比，该模型在TLC dataset和SatMVS3D dataset上表现出色，包括图像准确性、重建精度和效率等方面。<details>
<summary>Abstract</summary>
Novel view synthesis of satellite images holds a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages of the RPC model, to capture the continuous altitude space while learning the 3D structure. Given an RGB image and its corresponding RPC, the end-to-end model learns to synthesize the novel view with a new RPC and reconstruct the altitude of the scene. When multiple views are provided as inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF outperforms state-of-the-art nerf-based methods by a significant margin in terms of image fidelity, reconstruction accuracy, and efficiency, for both single-view and multiview task.
</details>
<details>
<summary>摘要</summary>
卫星图像新视角 sintesis 具有广泛的实际应用。而最近的 neural Radiance Field 进步主要targeted pin-hole cameras, 而模型 для卫星相机frequently需要足够的输入视图。这篇文章介绍了 rpcPRF，一种基于 Multiplane Images (MPI)的 Planar neural Radiance Field for Rational Polynomial Camera (RPC)。与coordinate-based neural radiance fields需要一个场景的多个视图不同，我们的模型适用于单个或少量输入，并在未看过场景的图像上表现良好。为了普适性到场景，我们提议使用重 проек视导向induce predicted MPI 学习正确的几何关系 между 3D坐标和图像。此外，我们将深度多视图基于方法中的精密 depth 监督取消，通过引入rendering技术。rpcPRF 组合了隐式表示的优势和 RPC 模型的优点，以捕捉连续高度空间而学习 3D结构。给定一个 RGB 图像和其对应的 RPC，结束到终端模型可以使用新的 RPC 和重构高度场景中的新视图。当多个视图作为输入时，rpcPRF 提供了EXTRA 视图的超级监督。在 TLC dataset 和 SatMVS3D dataset 上，rpcPRF 在图像准确性、重建精度和效率方面与 nerf-based 方法表现出显著的优势，对于单视图和多视图任务。
</details></li>
</ul>
<hr>
<h2 id="Online-Speculative-Decoding"><a href="#Online-Speculative-Decoding" class="headerlink" title="Online Speculative Decoding"></a>Online Speculative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07177">http://arxiv.org/abs/2310.07177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, Hao Zhang</li>
<li>for: 加速大语言模型（LLM）的推理，使用小型稿件模型预测目标模型的输出。</li>
<li>methods: online speculative decoding（OSD）技术，通过在用户查询数据观察到的过程中不断更新（多个）稿件模型（draft model），使用LLM服务器集群的剩余计算能力进行在线 retraining，从而使得 retraining 成本中性。</li>
<li>results: 与多种流行的LLM进行测试，结果显示在线干扰预测技术可以提高稿件模型预测目标模型输出的准确率，特别是面对查询分布的数据时。实际测试结果表明，在线干扰预测技术可以提高稿件模型的token接受率，从而实现1.22x到3.06x的延迟减少。<details>
<summary>Abstract</summary>
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target model's outputs, particularly on data originating from query distributions. As the draft model evolves online, it aligns with the query distribution in real time, mitigating distribution shifts. We develop a prototype of online speculative decoding based on online knowledge distillation and evaluate it using both synthetic and real query data on several popular LLMs. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.
</details>
<details>
<summary>摘要</summary>
推测解码是一种关键技术，可以加速大语言模型（LLM）的推理，通过使用一个较小的稿本模型预测目标模型的输出。然而，其效果可能受到稿本模型预测精度的限制，特别是面临多样化的文本输入和目标模型的能力差距。我们介绍在线推测解码（OSD）来解决这个挑战。我们的主要想法是在LLM服务器集群中利用丰富的剩余计算能力来不断更新（多个）稿本模型（SM）。由于LLM的推理是内存约束，因此在LLM服务器集群中的剩余计算能力可以被重新分配给在线 retraining 稿本模型，从而使得 retraining 成本 нейтра。由于LLM服务的查询分布相对简单，因此在线 retraining 稿本模型可以根据查询分布进行更好的预测，特别是对于来自查询分布的数据。随着稿本模型在线进行更新，它会与查询分布相互Alignment，从而 mitigate 分布偏移。我们开发了基于在线知识传递的 prototype，并对其进行了多种Synthetic和实际查询数据的评估。结果表明，使用OSD可以提高笔划率由0.1到0.65，相当于1.22x到3.06x的减少延迟。
</details></li>
</ul>
<hr>
<h2 id="Solving-Travelling-Thief-Problems-using-Coordination-Based-Methods"><a href="#Solving-Travelling-Thief-Problems-using-Coordination-Based-Methods" class="headerlink" title="Solving Travelling Thief Problems using Coordination Based Methods"></a>Solving Travelling Thief Problems using Coordination Based Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07156">http://arxiv.org/abs/2310.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majid Namazi, M. A. Hakim Newton, Conrad Sanderson, Abdul Sattar</li>
<li>for: 这 paper 是为了解决旅行盗贼问题 (TTP)，它是实际生活中的邮政集集问题的代表。</li>
<li>methods: 这 paper 使用了一种简单的本地搜索基于协调方法，然后提出了一种人工设计的协调规则，以及一种机器学习基于协调方法。</li>
<li>results:  compared to existing TTP solvers, 这 paper 的解决方案significantly outperform existing state-of-the-art TTP solvers on a set of benchmark problems。<details>
<summary>Abstract</summary>
A travelling thief problem (TTP) is a proxy to real-life problems such as postal collection. TTP comprises an entanglement of a travelling salesman problem (TSP) and a knapsack problem (KP) since items of KP are scattered over cities of TSP, and a thief has to visit cities to collect items. In TTP, city selection and item selection decisions need close coordination since the thief's travelling speed depends on the knapsack's weight and the order of visiting cities affects the order of item collection. Existing TTP solvers deal with city selection and item selection separately, keeping decisions for one type unchanged while dealing with the other type. This separation essentially means very poor coordination between two types of decision. In this paper, we first show that a simple local search based coordination approach does not work in TTP. Then, to address the aforementioned problems, we propose a human designed coordination heuristic that makes changes to collection plans during exploration of cyclic tours. We further propose another human designed coordination heuristic that explicitly exploits the cyclic tours in item selections during collection plan exploration. Lastly, we propose a machine learning based coordination heuristic that captures characteristics of the two human designed coordination heuristics. Our proposed coordination based approaches help our TTP solver significantly outperform existing state-of-the-art TTP solvers on a set of benchmark problems. Our solver is named Cooperation Coordination (CoCo) and its source code is available from https://github.com/majid75/CoCo
</details>
<details>
<summary>摘要</summary>
traveling thief problem (TTP) 是一种代表真实问题的卷积问题，如邮政收集问题。 TTP 包括一个旅行销售人问题 (TSP) 和一个背包问题 (KP)，因为 KP 中的物品分布在 TSP 中的城市，而偷窃者需要到城市集物品。在 TTP 中，城市选择和物品选择决策需要紧密协调，因为偷窃者的旅行速度取决于背包的重量，并且城市顺序affects物品采集顺序。现有的 TTP 解决方案通常处理城市选择和物品选择 separately，保持决策的一种不变，而另一种类型的决策处理。这种分离实际上意味着协调 между两种决策非常糟糕。在这篇论文中，我们首先表明了一个简单的本地搜索基于协调方法在 TTP 中不工作。然后，为了解决以上问题，我们提出了一个人类设计的协调规则，在探索循环旅行中更改采集计划。我们再提出了另一个人类设计的协调规则，Explicitly exploits 循环旅行中的物品选择。最后，我们提出了一个机器学习基于协调的协调方法，捕捉了两个人类设计的协调规则的特点。我们的提出的协调基本方法有助于我们的 TTP 解决方案在一组 benchmark 问题上显著超越现有的州际最佳解决方案。我们的解决方案名为 Cooperation Coordination (CoCo)，源代码可以从 <https://github.com/majid75/CoCo> 获取。
</details></li>
</ul>
<hr>
<h2 id="No-Privacy-Left-Outside-On-the-In-Security-of-TEE-Shielded-DNN-Partition-for-On-Device-ML"><a href="#No-Privacy-Left-Outside-On-the-In-Security-of-TEE-Shielded-DNN-Partition-for-On-Device-ML" class="headerlink" title="No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML"></a>No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07152">http://arxiv.org/abs/2310.07152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziqi-zhang/teeslice-artifact">https://github.com/ziqi-zhang/teeslice-artifact</a></li>
<li>paper_authors: Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen</li>
<li>For: The paper is written to address the security challenges introduced by on-device machine learning (ML) and to evaluate the effectiveness of various techniques for shielding DNN models within Trusted Execution Environments (TEEs).* Methods: The paper uses white-box attacks, such as model stealing (MS) and membership inference attack (MIA), to evaluate the security of different DNN partition techniques. The authors also propose a novel TSDP method called TEESlice, which uses a partition-before-training strategy to separate privacy-related weights from public weights.* Results: The paper shows that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. The authors also demonstrate that TEESlice delivers the same security protection as shielding the entire DNN model inside TEE, with over 10X less overhead than prior TSDP solutions and no accuracy loss.<details>
<summary>Abstract</summary>
On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding optimal DNN partition configurations (i.e., the highest security with minimal utility cost) for present TSDP solutions. The experiments show that such ``sweet spot'' configurations vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the ``upper-bound'' security guarantees) with over 10X less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss.
</details>
<details>
<summary>摘要</summary>
Device ML 引入新的安全挑战：DNN 模型变为设备用户可见的白盒模型。基于白盒信息，攻击者可以进行有效的模型盗取（MS）和会员推理攻击（MIA）。使用可信执行环境（TEE）保护设备上的 DNN 模型，以降低（易于）白盒攻击到（更加困难）黑盒攻击。然而，一个主要缺点是增加了响应时间（最多50倍）。为加速 TEE 盾牌 DNN 计算，研究人员提出了多种模型分区技术。这些解决方案被称为 TEE-Shielded DNN Partition（TSDP），它将 DNN 模型分为两部分，将隐私敏感部分卸载到 GPU 上，而在 TEE 中保护隐私敏感部分。这篇论文对现有 TSDP 解决方案进行了各种 MS 和 MIA 测试，并对多个 DNN 模型、数据集和指标进行了比较。我们发现现有 TSDP 解决方案受到隐私盗取攻击和不够安全，而且决定最佳 DNN 分区配置（即最高安全性与最小实用成本）对现有 TSDP 解决方案存在困难。实验表明，这些“甜点”配置在不同的数据集和模型之间弹性很大。基于实验所得的经验，我们提出了 TEESlice，一种新的 TSDP 方法，可以在 DNN 推理过程中防止 MS 和 MIA。TEESlice 采用分部之前训练策略，允许准确地分类隐私相关的权重与公共权重。TEESlice 提供了同 TEE 盾牌模型内部的安全保障（最高安全性的保障），但具有至少10倍更低的负担（在实验和实际环境中），无损失 accuracy。
</details></li>
</ul>
<hr>
<h2 id="Determining-Winners-in-Elections-with-Absent-Votes"><a href="#Determining-Winners-in-Elections-with-Absent-Votes" class="headerlink" title="Determining Winners in Elections with Absent Votes"></a>Determining Winners in Elections with Absent Votes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07150">http://arxiv.org/abs/2310.07150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qishen Han, Amélie Marian, Lirong Xia</li>
<li>for: 这个论文研究了在部分选票时确定选手是否能赢得选举的问题。</li>
<li>methods: 这个论文使用NP-完全理论和特殊的位置分数规则来解决这个问题。</li>
<li>results: 论文表明在部分选票时，这个问题的计算复杂度仍然是NP-完全，而且可以在多项式时间内解决。<details>
<summary>Abstract</summary>
An important question in elections is the determine whether a candidate can be a winner when some votes are absent. We study this determining winner with the absent votes (WAV) problem when the votes are top-truncated. We show that the WAV problem is NP-complete for the single transferable vote, Maximin, and Copeland, and propose a special case of positional scoring rule such that the problem can be computed in polynomial time. Our results in top-truncated rankings differ from the results in full rankings as their hardness results still hold when the number of candidates or the number of missing votes are bounded, while we show that the problem can be solved in polynomial time in either case.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate_text: An important question in elections is the determine whether a candidate can be a winner when some votes are absent. We study this determining winner with the absent votes (WAV) problem when the votes are top-truncated. We show that the WAV problem is NP-complete for the single transferable vote, Maximin, and Copeland, and propose a special case of positional scoring rule such that the problem can be computed in polynomial time. Our results in top-truncated rankings differ from the results in full rankings as their hardness results still hold when the number of candidates or the number of missing votes are bounded, while we show that the problem can be solved in polynomial time in either case. translate_result: 一个重要的选举问题是在某些选票缺失时确定候选人是否可以赢得选举。我们研究在排除票 (WAV) 问题中确定赢家，当选票是top-truncated时。我们显示了WAV问题对单转移投票、最大最小值和 copeland 的NP-完备性。我们还提出了一种特殊的位置得分规则，使得该问题可以在多项时间内解决。我们的结果表明，在排除rankings中，问题的难度与候选人数量或缺失票数量是否受限无关，而我们在任一情况下都可以在多项时间内解决问题。
</details></li>
</ul>
<hr>
<h2 id="Denoising-Task-Routing-for-Diffusion-Models"><a href="#Denoising-Task-Routing-for-Diffusion-Models" class="headerlink" title="Denoising Task Routing for Diffusion Models"></a>Denoising Task Routing for Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07138">http://arxiv.org/abs/2310.07138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim</li>
<li>for: 这篇论文的目的是提出一种简单的扩展策略，以提高现有扩散模型的性能。</li>
<li>methods: 该策略基于 selective 激活通道，以实现对不同任务的分离信息传递。</li>
<li>results: 该策略能够提高扩散模型的性能，并且不需要添加更多参数。<details>
<summary>Abstract</summary>
Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timesteps. (2) Task Weights: During the early stages (higher timesteps) of the denoising process, DTR assigns a greater number of task-specific channels, leveraging the insight that diffusion models prioritize reconstructing global structure and perceptually rich contents in earlier stages, and focus on simple noise removal in later stages. Our experiments demonstrate that DTR consistently enhances the performance of diffusion models across various evaluation protocols, all without introducing additional parameters. Furthermore, DTR contributes to accelerating convergence during training. Finally, we show the complementarity between our architectural approach and existing MTL optimization techniques, providing a more complete view of MTL within the context of diffusion training.
</details>
<details>
<summary>摘要</summary>
Diffusion models可以生成高度真实的图像，通过学习多步噪声处理过程，自然体现了多任务学习（MTL）的原理。尽管 diffusion models 和 MTL 之间存在深厚的连接，但是在设计神经网络架构方面，还没有对 diffusion models 的具体设计进行过深入的研究。在这篇文章中，我们提出了一种简单的加载策略，称为 Denoising Task Routing（DTR），可以让现有的 diffusion model 架构中的信道之间建立独特的信息路径。DTR 的特点是可以让 diffusion models 在同一个架构中同时执行多个任务，而不需要添加额外的参数。具体来说，DTR 可以在 diffusion models 中选择性地激活特定的通道，以便在不同的任务之间建立独特的信息流。此外，DTR 还可以在不同的时间步骤之间传递信息，从而使 diffusion models 在不同的任务之间共享知识。我们的实验表明，DTR 可以在不同的评价协议下 consistently 提高 diffusion models 的性能，而无需添加额外的参数。此外，DTR 还可以加速 diffusion models 的训练过程。最后，我们还证明了 DTR 和现有的 MTL 优化技术之间的相互作用，提供了更完整的 MTL 视角，从而更好地理解 diffusion training 中的 MTL。
</details></li>
</ul>
<hr>
<h2 id="Off-Policy-Evaluation-for-Human-Feedback"><a href="#Off-Policy-Evaluation-for-Human-Feedback" class="headerlink" title="Off-Policy Evaluation for Human Feedback"></a>Off-Policy Evaluation for Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07123">http://arxiv.org/abs/2310.07123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qitong Gao, Juncheng Dong, Vahid Tarokh, Min Chi, Miroslav Pajic</li>
<li>for: 用于评估人类反馈信号（HF）的离线评估方法，以优化决策政策的安全性和效率。</li>
<li>methods: 基于离线轨迹的既 existed OPE 方法，通过重构 immediate human reward（IHR）和环境知识储存在幂值空间来约束 HF 信号的推导。</li>
<li>results: 在两个实验中（生物学实验和智能教学）以及一个模拟环境中（视觉 Q&amp;A），我们的方法比直接使用 variant of existed OPE 方法更好地评估 HF 信号。<details>
<summary>Abstract</summary>
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over two real-world experiments, adaptive in-vivo neurostimulation and intelligent tutoring, as well as in a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods.
</details>
<details>
<summary>摘要</summary>
偏离策略评估（OPE）是关键性的，可以关闭在线训练和评估过程中的差距，通过使用偏离过程只有的数据来评估目标（评估）策略的性能和排名。这可以提高数据收集和策略测试的安全性和效率，特别是在医疗领域，在线部署是昂贵的。然而，现有的OPE方法无法准确地评估人类反馈（HF）信号，因为HF可能会受到多个下面因素的影响，并且只有稀缺的可用。相比之下，agent-defined环境奖励（用于策略优化）通常是 Parametric 函数或分布中确定的。这使得对HF信号的推断变得困难。为解决这个问题，我们提出了一个OPEHF框架，可以准确地评估HF信号。我们开发了一种快速人工奖励（IHR）重建方法，通过环境知识储存在一个捕捉状态过渡和发送HF信号的latent空间来补偿。我们的方法在两个真实世界实验（生物医学应用和智能教学）以及一个视觉Q&A simulate environment中进行了测试，结果表明，我们的方法可以准确地评估HF信号，相比直接使用（变种的）现有OPE方法。
</details></li>
</ul>
<hr>
<h2 id="The-Temporal-Structure-of-Language-Processing-in-the-Human-Brain-Corresponds-to-The-Layered-Hierarchy-of-Deep-Language-Models"><a href="#The-Temporal-Structure-of-Language-Processing-in-the-Human-Brain-Corresponds-to-The-Layered-Hierarchy-of-Deep-Language-Models" class="headerlink" title="The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models"></a>The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07106">http://arxiv.org/abs/2310.07106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush, Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky, Adeen Flinker, Omer Levy, Uri Hasson</li>
<li>for: 本研究使用深度语言模型（DLM）来理解人类大脑中自然语言处理的机制。</li>
<li>methods: 本研究使用层次序列数字 вектор来表示单词和上下文，从而开拓出许多新的应用，如人类化文本生成。</li>
<li>results: 研究发现，DLM层次 hierarchy可以模型大脑语言理解的时间动力学，并且通过使用电rocorticography（ECoG）数据，可以获得较高的时间分辨率。<details>
<summary>Abstract</summary>
Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL). We then extract contextual embeddings from the different layers of the DLM and use linear encoding models to predict neural activity. We first focus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our model to track the increasing temporal receptive window along the linguistic processing hierarchy from auditory to syntactic and semantic areas. Our results reveal a connection between human language processing and DLMs, with the DLM's layer-by-layer accumulation of contextual information mirroring the timing of neural activity in high-order language areas.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ClausewitzGPT-Framework-A-New-Frontier-in-Theoretical-Large-Language-Model-Enhanced-Information-Operations"><a href="#ClausewitzGPT-Framework-A-New-Frontier-in-Theoretical-Large-Language-Model-Enhanced-Information-Operations" class="headerlink" title="ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations"></a>ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07099">http://arxiv.org/abs/2310.07099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Kereopa-Yorke</li>
<li>for: 这篇论文旨在探讨人工智能技术在信息操作中的应用和挑战，以及如何通过Clausewitz的军事策略原则来理解和 navigating这个新世界。</li>
<li>methods: 本论文使用了Clausewitz的军事策略原则和现代AI技术来建立一个数学模型，以便更好地理解和评估AI信息操作的风险和机遇。</li>
<li>results: 本论文的研究结果表明，在AI技术的支持下，信息操作的规模和效果将会不断提高，同时也会带来一系列的道德和社会问题。这篇论文提出了一种基于Clausewitz的军事策略原则的框架，以帮助我们更好地理解和应对这些挑战。<details>
<summary>Abstract</summary>
In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the "ClausewitzGPT" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.   Mathematically underpinned and inspired by the timeless tenets of Clausewitz's military strategy (Clausewitz, 1832), this thesis delves into the intricate dynamics of AI-augmented information operations. With references to recent findings and research (Department of State, 2023), it highlights the staggering year-on-year growth of AI information campaigns (Evgeny Pashentsev, 2023), stressing the urgency of our current juncture. The synthesis of Enlightenment thinking, and Clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.
</details>
<details>
<summary>摘要</summary>
在数字时代，虚拟空间成为国际竞争的战略中心，信息操作和大型自然语言模型（LLM）的融合标志着一种新的 парадигShift，具有巨大的机遇和复杂的挑战。随着如Mistral 7B LLM（Mistral，2023）等工具的普及，各种actor，从主权国家到非法实体（Howard等，2023），都拥有了高效的幻影操作工具（Goldstein等，2023）。这篇论文提出了在“ClausewitzGPT”方程中 navigate这个新的世界的框架。这种新的形式不仅试图量化机器速度下LLM增强操作中存在的风险，而且强调了自动化AI代理人（Wang、Xie等，2023）的重要性。这些代理人，具有伦理考虑（Hendrycks等，2021），在AI操作中扮演了不可或缺的角色，确保我们在前进的过程中不会失去道德 компас和社会要求。数学基础和Clausewitz的军事策略原则（Clausewitz，1832）相 inspirited，这篇论文探讨了人工智能增强信息操作的复杂动力学。参考最新的发现和研究（Department of State，2023），它指出了人工智能信息宣传活动的吞吐量年吞吐量增长（Evgeny Pashentsev，2023），强调了我们当前的战略性时刻。通过融合了光辉时代思想和Clausewitz的原则，这篇论文提供了一个基础镜像，强调了在快速科技进步的面前，需要明确的战略视野、伦理考虑和整体理解。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Universal-Transformer"><a href="#Sparse-Universal-Transformer" class="headerlink" title="Sparse Universal Transformer"></a>Sparse Universal Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07096">http://arxiv.org/abs/2310.07096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan</li>
<li>for: 这篇论文旨在提出一种名为卷积束 universal transformer (SUT)，用于解决卷积束的计算复杂性问题，而保持其参数效率和通用性能。</li>
<li>methods: 这篇论文提出了一种基于稀盐杂合 эксперTS (SMoE) 和一种新的扔投机制 Mechanism to reduce the computation complexity of universal transformers (UTs) while retaining their parameter efficiency and generalization ability.</li>
<li>results: 实验表明，SUT可以与强化基eline模型相比，只用半个计算和参数量来实现相同的性能在 WMT’14 和正式语言任务 (逻辑推理和 CFQ)，并在正式语言任务中实现了强大的通用性能。此外，新的停止机制还可以在推理过程中减少计算量约 50%，而无需减少性能。<details>
<summary>Abstract</summary>
The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in computation during inference with very little performance decrease on formal language tasks.
</details>
<details>
<summary>摘要</summary>
全球转换器（UT）是转换器的一种变体，它在层之间共享参数。实验证明UT在正式语言任务中有更好的 композиitional 普适性than Vanilla Transformer（VT）。参数共享也使UT的参数效率更高than VT。 despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT。这篇论文提出了稀疏全球转换器（SUT），它利用稀疏混合专家（SMoE）和一种新的棒拌分解机制来降低UT的计算复杂度，保持UT的参数效率和普适性。实验显示SUT可以与强基eline模型相当的性能，只使用半个计算和参数量进行WMT'14和正式语言任务（逻辑推理和CFQ）。新的停机机制还可以在推理过程中降低计算量约50%，并减少正式语言任务中的性能下降。
</details></li>
</ul>
<hr>
<h2 id="Jaeger-A-Concatenation-Based-Multi-Transformer-VQA-Model"><a href="#Jaeger-A-Concatenation-Based-Multi-Transformer-VQA-Model" class="headerlink" title="Jaeger: A Concatenation-Based Multi-Transformer VQA Model"></a>Jaeger: A Concatenation-Based Multi-Transformer VQA Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07091">http://arxiv.org/abs/2310.07091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieting Long, Zewei Shi, Penghao Jiang, Yidong Gan</li>
<li>For: The paper is written for the task of document-based visual question answering, specifically addressing the challenges of linguistic sense disambiguation and fine-grained multimodal retrieval.* Methods: The proposed model, Jaegar, uses a concatenation-based multi-transformer architecture with feature extractors from RoBERTa large and GPT2-xl. The outputs from both models are concatenated and subjected to dimensionality reduction to improve performance and reduce computational complexity.* Results: The proposed model achieves competitive performance on Task C of the PDF-VQA Dataset, demonstrating its effectiveness in addressing the challenges of document-based question answering.Here are the three points in Simplified Chinese text:* For: 论文针对的任务是文档基于视觉问题回答，特别是语义含义推导和细致的多媒体检索。* Methods: 提议的模型使用 concatenation-based 多变换器架构，利用 RoBERTa 大型和 GPT2-xl 作为特征提取器。输出从两个模型的 concatenation 后，应用维度减少来提高性能和降低计算复杂性。* Results: 提议的模型在 PDF-VQA 数据集的 Task C 上达到竞争性性能，证明其在文档基于问题回答中的有效性。<details>
<summary>Abstract</summary>
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the performance of these models through concatenation. After concatenation, we apply dimensionality reduction to the output features, reducing the model's computational effectiveness and inference time. Empirical results demonstrate that our proposed model achieves competitive performance on Task C of the PDF-VQA Dataset. If the user adds any new data, they should make sure to style it as per the instructions provided in previous sections.
</details>
<details>
<summary>摘要</summary>
文档视觉问答 poses 一项具有语义含义析分和细化多模式 retrieve 的挑战。虽然因使用大语言和开放世界先进模型\cite{1} 而取得了鼓舞人心的进步，但还存在许多挑战，如延长响应时间、扩展推理时间和精度匹配不准。为了突破这些挑战，我们提议使用 Jaegar，一种 concatenation-based 多 transformer VQA 模型。为了 derivation 问题特征，我们利用 RoBERTa large\cite{2} 和 GPT2-xl\cite{3} 作为特征提取器。接着，我们将两个模型的输出经过 concatenation 操作。这种操作使得模型能够同时考虑多种来源的信息，提高其表达能力。通过利用预训练模型来提取特征，我们的方法有可能使这些模型的表现得到增强。然后，我们对输出特征进行维度减少，降低模型的计算效率和推理时间。实验结果表明，我们的提议模型在 Task C 中的 PDF-VQA 数据集上达到了竞争性的性能。如果用户添加任何新数据，他们应该按照以前讲的方式进行样式化。
</details></li>
</ul>
<hr>
<h2 id="Diversity-of-Thought-Improves-Reasoning-Abilities-of-Large-Language-Models"><a href="#Diversity-of-Thought-Improves-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="Diversity of Thought Improves Reasoning Abilities of Large Language Models"></a>Diversity of Thought Improves Reasoning Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07088">http://arxiv.org/abs/2310.07088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, Besmira Nushi</li>
<li>for: 提高大语言模型（LLM）在需要复杂理解的 Setting 中的表现。</li>
<li>methods: 使用决策步骤破碎问题，或者通过修改解码步骤 ensemble 多代训练。</li>
<li>results: 在固定输入提示下，使用自动提高提示多样性的方法 DIV-SE 和 IDIV-SE 在多个推理调用中 ensemble 多个推理结果，可以超越之前的基线值，并在不修改解码过程的情况下提高模型的表现。<details>
<summary>Abstract</summary>
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a single inference call; we call this IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform the previously discussed baselines using both GPT-3.5 and GPT-4 on several reasoning benchmarks, without modifying the decoding process. Additionally, DIV-SE advances state-of-the-art performance on recent planning benchmarks (Valmeekam et al., 2023), exceeding the highest previously reported accuracy by at least 29.6 percentage points on the most challenging 4/5 Blocksworld task. Our results shed light on how to enforce prompt diversity toward LLM reasoning and thereby improve the pareto frontier of the accuracy-cost trade-off.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在需要复杂逻辑的设置下Documented to struggle. However, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022) or ensembling various generations through modifying decoding steps (Wang et al., 2023) can boost performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a single inference call; we call this IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform the previously discussed baselines using both GPT-3.5 and GPT-4 on several reasoning benchmarks, without modifying the decoding process. Additionally, DIV-SE advances state-of-the-art performance on recent planning benchmarks (Valmeekam et al., 2023), exceeding the highest previously reported accuracy by at least 29.6 percentage points on the most challenging 4/5 Blocksworld task. Our results shed light on how to enforce prompt diversity toward LLM reasoning and thereby improve the pareto frontier of the accuracy-cost trade-off.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Twitter-Data-for-Sentiment-Analysis-of-Transit-User-Feedback-An-NLP-Framework"><a href="#Leveraging-Twitter-Data-for-Sentiment-Analysis-of-Transit-User-Feedback-An-NLP-Framework" class="headerlink" title="Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework"></a>Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07086">http://arxiv.org/abs/2310.07086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adway Das, Abhishek Kumar Prajapati, Pengxiang Zhang, Mukund Srinath, Andisheh Ranjbari</li>
<li>for: 本研究提出一种基于自然语言处理（NLP）的框架，利用社交媒体平台如推特上的各种各样的用户生成内容，了解用户对不同服务问题的看法和反馈。</li>
<li>methods: 该框架使用少量学习来分类推特，并使用词库基于情感分析模型来评估推特中的情感 INTENSITY和方向。</li>
<li>results: 研究验证了该框架的有效性，并应用于纽约市地铁系统为例研究。结果表明，该框架可以有效地分类推特到预先定义的类别，并评估推特中的情感 INTENSITY。 comparison with官方客户调查显示，研究的结论具有相当的可靠性和有效性。<details>
<summary>Abstract</summary>
Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, distinguishing between positive, negative, and neutral tweets. The effectiveness of the framework was validated on a subset of manually labeled Twitter data and was applied to the NYC subway system as a case study. The framework accurately classifies tweets into predefined categories related to safety, reliability, and maintenance of the subway system and effectively measured sentiment intensities within each category. The general findings were corroborated through a comparison with an agency-run customer survey conducted in the same year. The findings highlight the effectiveness of the proposed framework in gauging user feedback through inexpensive social media data to understand the pain points of the transit system and plan for targeted improvements.
</details>
<details>
<summary>摘要</summary>
传统的公共交通用户反馈方法通常是时间consuming、资源占用和成本高的。在这篇论文中，我们提出了一个基于自然语言处理（NLP）的框架，利用社交媒体平台如推特上的廉价、庞大和丰富的用户生成内容，以了解用户对不同服务问题的看法。推特是一个微博平台，它上有大量的实时用户生成内容，经常包含有价值的反馈和意见。我们的框架通过两种技术来减少收集和分析用户反馈的成本和时间：首先，它利用几shot学习来分类推特内容，以有效地识别推特中的问题。然后，它使用词汇库来进行情感分析，以评估推特中的情感INTENSITY和方向，并分为正面、负面和中性推特。我们验证了这个框架的有效性，并应用于纽约市地铁系统的案例研究。该框架能够准确地将推特分类为预定的类别， relate to 安全、可靠和维护的地铁系统，并有效地测量每个类别中的情感INTENSITY。我们的发现与一份由机构进行的客户调查相符，这些发现 highlighted 了我们的框架在通过便宜的社交媒体数据来了解公共交通系统的痛点，并计划targeted的改进的优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.AI_2023_10_11/" data-id="clnsn0vdh004pgf881q4b4fv3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.CL_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T11:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.CL_2023_10_11/">cs.CL - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Crosslingual-Structural-Priming-and-the-Pre-Training-Dynamics-of-Bilingual-Language-Models"><a href="#Crosslingual-Structural-Priming-and-the-Pre-Training-Dynamics-of-Bilingual-Language-Models" class="headerlink" title="Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models"></a>Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07929">http://arxiv.org/abs/2310.07929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen</li>
<li>for: 研究Multilingual语言模型是否共享抽象语法表示法，并且如果有，这些表示法在哪些时候开发出来。</li>
<li>methods: 使用结构预priming测试Multilingual语言模型中的抽象语法表示法，并在Dutch-English双语设置下进行扩展。</li>
<li>results: 发现在接受第二语言后，跨语言结构预priming效应在数据量很少时就出现，这表明Multilingual语言模型在学习第二语言时可以快速吸收抽象语法表示法。<details>
<summary>Abstract</summary>
Do multilingual language models share abstract grammatical representations across languages, and if so, when do these develop? Following Sinclair et al. (2022), we use structural priming to test for abstract grammatical representations with causal effects on model outputs. We extend the approach to a Dutch-English bilingual setting, and we evaluate a Dutch-English language model during pre-training. We find that crosslingual structural priming effects emerge early after exposure to the second language, with less than 1M tokens of data in that language. We discuss implications for data contamination, low-resource transfer, and how abstract grammatical representations emerge in multilingual models.
</details>
<details>
<summary>摘要</summary>
请参考Sinclair等（2022），我们使用结构驱动来测试多语言模型中的抽象语法表示。我们在荷兰语-英语双语设置下进行扩展，并评估一个荷兰语-英语语言模型在预训练过程中的表现。我们发现，在第二语言暴露后不久，跨语言结构驱动效应就出现了，并且只需要少于1M个字节的数据来提高表达。我们讨论了数据污染、低资源传输和多语言模型中抽象语法表示的出现。
</details></li>
</ul>
<hr>
<h2 id="The-Expresssive-Power-of-Transformers-with-Chain-of-Thought"><a href="#The-Expresssive-Power-of-Transformers-with-Chain-of-Thought" class="headerlink" title="The Expresssive Power of Transformers with Chain of Thought"></a>The Expresssive Power of Transformers with Chain of Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07923">http://arxiv.org/abs/2310.07923</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Merrill, Ashish Sabharwal</li>
<li>for: 本研究旨在探讨transformer模型在解释力方面的限制，并研究如何通过允许模型使用链式思维或笔记来提高其解释能力。</li>
<li>methods: 研究使用了decoder-only transformer模型，并通过控制模型在输入长度上的步骤数来调查其解释能力。</li>
<li>results: 研究发现，允许模型使用链式思维或笔记可以显著提高其解释能力，但是这种提高的程度取决于链式思维或笔记的长度。研究还发现，使用logarithmic数量的步骤可以只有微小地提高标准transformer模型的解释能力，而使用线性数量的步骤则可以让transformer模型recognize所有的正则语言。此外，研究还发现，使用线性数量的步骤可以使transformer模型recognize所有的context-sensitive语言，而使用polynomial数量的步骤可以使transformer模型recognize所有的 polynomial-time solvable问题。<details>
<summary>Abstract</summary>
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps make them recognize exactly the class of polynomial-time solvable problems -- the first exact characterization of a type of transformers in terms of standard complexity classes. Together, our results provide a nuanced framework for understanding how the length of a transformer's chain of thought or scratchpad impacts its reasoning power.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近的理论研究发现了一些奇异的简单逻辑问题，如图像两个节点是连接的检查或模拟有限状态机，是不可避免地无法由标准转换器解决的。但在实践中，转换器的逻辑可以通过允许它们使用"链接思考"或"启发笔记"来改进，即在输入前面生成和条件于一系列间接token。我们问：这种间接生成是否实际提高了解oder-only转换器的计算能力？我们表明，答案是yes，但间接生成的数量对计算能力的提高有关键的影响。例如，我们发现，对于输入长度逻辑数个步骤，转换器decoder只有微不到标准转换器的限制，而 linear数个步骤添加了明显的新能力（以标准复杂性假设）：可以识别所有规则语言。我们的结果还表明，线性步骤使transformer decoder处于上下文敏感语言中，而多项式步骤使其识别 precisley 可解语言类型 -- 首次对转换器类型进行标准复杂性类型的准确characterization。总的来说，我们的结果提供了一个细化的框架，用于理解transformer链接思考或启发笔记的长度如何影响其逻辑能力。
</details></li>
</ul>
<hr>
<h2 id="Pit-One-Against-Many-Leveraging-Attention-head-Embeddings-for-Parameter-efficient-Multi-head-Attention"><a href="#Pit-One-Against-Many-Leveraging-Attention-head-Embeddings-for-Parameter-efficient-Multi-head-Attention" class="headerlink" title="Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"></a>Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07911">http://arxiv.org/abs/2310.07911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyin Xue, Nikolaos Aletras</li>
<li>for: 这篇论文的目的是简化预训语言模型的多头注意力（MHA）机制，以减少内存需求。</li>
<li>methods: 作者提议一种单一共享投影矩阵和多头嵌入（MHE）注意力机制，它使用单一的共享投影矩阵和多个头嵌入，实现高度优化的预测性表现。</li>
<li>results: 作者透过实验证明，MHE注意力与传统MHA相比，具有更高的内存效率和预测性表现，并且仅需要一小部分额外参数（$3nd$），相比于传统MHA需要$(3n^2-3n)d^2-3nd$倍的额外参数。<details>
<summary>Abstract</summary>
Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.
</details>
<details>
<summary>摘要</summary>
对预训语言模型进行扩大已经带来了许多自然语言处理任务中的性能提升，但是它具有较大的记忆需求。 Drawing inspiration from transformer中的位嵌入，我们想要简化和降低多头注意力（MHA）机制的记忆负载。我们提出了一个替代模组，使用单一的共享复制矩阵和多个头嵌入（MHE），即每个头都有它自己的嵌入。我们实际证明了我们的MHE注意力可以与替代注意力机制相比，具有较高的预测性能保留比，而且仅需要一小部分的额外参数（$3nd$，其中$n$是注意力头数量，$d$是头嵌入大小），相比于单一注意力机制需要 $(3n^2-3n)d^2-3nd$个额外参数。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Evaluation-Metrics-for-Neural-Test-Oracle-Generation"><a href="#Assessing-Evaluation-Metrics-for-Neural-Test-Oracle-Generation" class="headerlink" title="Assessing Evaluation Metrics for Neural Test Oracle Generation"></a>Assessing Evaluation Metrics for Neural Test Oracle Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07856">http://arxiv.org/abs/2310.07856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiho Shin, Hadi Hemmati, Moshi Wei, Song Wang</li>
<li>For: The paper aims to evaluate the performance of state-of-the-art test oracle generation models using both natural language generation (NLG) and test adequacy metrics.* Methods: The authors train and run four state-of-the-art test oracle generation models on five NLG-based and two test adequacy metrics for their analysis. They apply two different correlation analyses between these two sets of metrics.* Results: The authors found no significant correlation between the NLG-based metrics and test adequacy metrics. They also found that oracles with high NLG-based metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle’s parameters, while oracles with low NLG-based metrics but high test adequacy metrics tend to have different assertion types or methods that function similarly to the ones in the ground truth.Here are the three points in Simplified Chinese text:* For: 本研究用于评估现有的测试 oracle 生成模型，使用自然语言生成 (NLG) 和测试准确率 metric。* Methods: 作者使用四种state-of-the-art测试 oracle 生成模型在五种 NLG-based 和两种测试准确率 metric 上进行训练和运行分析。他们应用了两种不同的相关分析方法。* Results: 作者发现没有显著的相关性 между NLG-based  metric 和测试准确率 metric。他们还发现，具有高 NLG-based  metric 但低测试准确率 metric 的 oracle 往往有复杂或多个链接的方法调用在 oracle 中，使模型难以生成完整。相反，具有低 NLG-based  metric 但高测试准确率 metric 的 oracle 往往有不同的断言类型或功能相似的方法调用。<details>
<summary>Abstract</summary>
In this work, we revisit existing oracle generation studies plus ChatGPT to empirically investigate the current standing of their performance in both NLG-based and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on five NLG-based and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the NLG-based metrics and test adequacy metrics. For instance, oracles generated from ChatGPT on the project activemq-artemis had the highest performance on all the NLG-based metrics among the studied NOGs, however, it had the most number of projects with a decrease in test adequacy metrics compared to all the studied NOGs. We further conduct a qualitative analysis to explore the reasons behind our observations, we found that oracles with high NLG-based metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making it hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low NLG-based metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation with both NLG and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们回顾了现有的oracle生成研究以及ChatGPT，以 empirically investigating现有的性能水平在NLG基于和测试准确性度量上。 Specifically，我们训练并运行四种 state-of-the-art测试oracle生成模型在五个NLG基于和两个测试准确性度量上进行分析。我们应用了两种 corrrelation分析 между这两个不同的集合的度量。 surprisingly, we found no significant correlation between NLG-based metrics and test adequacy metrics. For instance, oracles generated from ChatGPT on the project activemq-artemis had the highest performance on all NLG-based metrics among the studied NOGs, however, it had the most number of projects with a decrease in test adequacy metrics compared to all the studied NOGs. We further conduct a qualitative analysis to explore the reasons behind our observations, we found that oracles with high NLG-based metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making it hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low NLG-based metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation with both NLG and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future.
</details></li>
</ul>
<hr>
<h2 id="Framework-for-Question-Answering-in-Sanskrit-through-Automated-Construction-of-Knowledge-Graphs"><a href="#Framework-for-Question-Answering-in-Sanskrit-through-Automated-Construction-of-Knowledge-Graphs" class="headerlink" title="Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs"></a>Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07848">http://arxiv.org/abs/2310.07848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Terdalkar, Arnab Bhattacharya</li>
<li>for: 本研究targets the problem of building knowledge graphs for particular types of relationships from sa\d{m}sk\d{r}ta texts, and develops a natural language question-answering system in sa\d{m}sk\d{r}ta that uses the knowledge graph to answer factoid questions.</li>
<li>methods: 本研究使用了自然语言处理技术和知识图谱技术，并设计了一个框架来支持整个系统。两个实例分别用于人际关系from mah=abh=arata和r=am=aya\d{n}a，以及同义关系from bh=avaprak=a&#39;sa nigha\d{n}\d{t}u。</li>
<li>results: 研究表明，使用这种方法可以回答大约50%的问题正确。然而，研究还分析了系统的缺陷，并讨论了可能的改进方向。<details>
<summary>Abstract</summary>
Sanskrit (sa\d{m}sk\d{r}ta) enjoys one of the largest and most varied literature in the whole world. Extracting the knowledge from it, however, is a challenging task due to multiple reasons including complexity of the language and paucity of standard natural language processing tools. In this paper, we target the problem of building knowledge graphs for particular types of relationships from sa\d{m}sk\d{r}ta texts. We build a natural language question-answering system in sa\d{m}sk\d{r}ta that uses the knowledge graph to answer factoid questions. We design a framework for the overall system and implement two separate instances of the system on human relationships from mah\=abh\=arata and r\=am\=aya\d{n}a, and one instance on synonymous relationships from bh\=avaprak\=a\'sa nigha\d{n}\d{t}u, a technical text from \=ayurveda. We show that about 50% of the factoid questions can be answered correctly by the system. More importantly, we analyse the shortcomings of the system in detail for each step, and discuss the possible ways forward.
</details>
<details>
<summary>摘要</summary>
санскрит（sa\d{m}sk\d{r}ta）拥有全球最大和最多样化的文学作品。然而，从其中提取知识是一项具有挑战性的任务，主要原因包括语言复杂性和自然语言处理工具的缺乏。在这篇论文中，我们target的是从sa\d{m}sk\d{r}ta文本中提取特定类型的关系知识，并建立一个基于知识图的自然语言问答系统。我们设计了整个框架，并实现了两个不同的系统实例，分别处理人类关系from mah\=abh\=arata和r\=am\=aya\d{n}a，以及一个实例处理synonymous关系from bh\=avaprak\=a\'sa nigha\d{n}\d{t}u，这是一部技术文献from \=ayurveda。我们发现系统可以正确地回答50%的问题。此外，我们还进行了每个步骤的细节分析，并讨论了可能的改进方向。
</details></li>
</ul>
<hr>
<h2 id="Antarlekhaka-A-Comprehensive-Tool-for-Multi-task-Natural-Language-Annotation"><a href="#Antarlekhaka-A-Comprehensive-Tool-for-Multi-task-Natural-Language-Annotation" class="headerlink" title="Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language Annotation"></a>Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07826">http://arxiv.org/abs/2310.07826</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Antarlekhaka/code">https://github.com/Antarlekhaka/code</a></li>
<li>paper_authors: Hrishikesh Terdalkar, Arnab Bhattacharya</li>
<li>for: 本研究是为了提高自然语言处理（NLP）技术的发展，特别是为了解决低资源语言的 annotated dataset 问题。</li>
<li>methods: 本文提出了一个名为 Antarlekhaka 的工具，用于手动标注 NLP 相关的全面任务。该工具支持 Unicode 兼容、语言不依赖、Web 部署和分布式标注多个同时annotator。系统提供了8类任务的用户友好界面。</li>
<li>results: 比较 исследование表明，Antarlekhaka 在对象评估中表现出色，并在实际应用中用于两种不同语言的两个实际标注任务。工具可以在 <a target="_blank" rel="noopener" href="https://github.com/Antarlekhaka/code">https://github.com/Antarlekhaka/code</a> 上下载。<details>
<summary>Abstract</summary>
One of the primary obstacles in the advancement of Natural Language Processing (NLP) technologies for low-resource languages is the lack of annotated datasets for training and testing machine learning models. In this paper, we present Antarlekhaka, a tool for manual annotation of a comprehensive set of tasks relevant to NLP. The tool is Unicode-compatible, language-agnostic, Web-deployable and supports distributed annotation by multiple simultaneous annotators. The system sports user-friendly interfaces for 8 categories of annotation tasks. These, in turn, enable the annotation of a considerably larger set of NLP tasks. The task categories include two linguistic tasks not handled by any other tool, namely, sentence boundary detection and deciding canonical word order, which are important tasks for text that is in the form of poetry. We propose the idea of sequential annotation based on small text units, where an annotator performs several tasks related to a single text unit before proceeding to the next unit. The research applications of the proposed mode of multi-task annotation are also discussed. Antarlekhaka outperforms other annotation tools in objective evaluation. It has been also used for two real-life annotation tasks on two different languages, namely, Sanskrit and Bengali. The tool is available at https://github.com/Antarlekhaka/code.
</details>
<details>
<summary>摘要</summary>
一个主要障碍在低资源语言自然语言处理（NLP）技术的发展是缺乏训练和测试机器学习模型的彩杂化数据集。在这篇论文中，我们介绍了Antarlekhaka，一种用于手动标注NLP中广泛的任务的工具。该工具兼容Unicode，语言不偏，可在网上部署，并支持同时进行多个注释者的分布式注释。系统提供了用户友好的界面，用于8种注释任务类别。这些任务类别使得可以对NLP任务进行更广泛的注释。任务类别包括两种语言学任务，即句子边界检测和确定正确的字符顺序，这些任务对于文学形式的文本非常重要。我们提出了顺序注释基于小文本单元的想法，其中一个注释者在处理一个文本单元后，才可以前往下一个单元。我们还讨论了在多任务注释模式下的研究应用。Antarlekhaka在对象评估中表现出色，并在两种不同语言上进行了两个实际注释任务。工具可在https://github.com/Antarlekhaka/code中下载。
</details></li>
</ul>
<hr>
<h2 id="Non-autoregressive-Text-Editing-with-Copy-aware-Latent-Alignments"><a href="#Non-autoregressive-Text-Editing-with-Copy-aware-Latent-Alignments" class="headerlink" title="Non-autoregressive Text Editing with Copy-aware Latent Alignments"></a>Non-autoregressive Text Editing with Copy-aware Latent Alignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07821">http://arxiv.org/abs/2310.07821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yzhangcs/ctc-copy">https://github.com/yzhangcs/ctc-copy</a></li>
<li>paper_authors: Yu Zhang, Yue Zhang, Leyang Cui, Guohong Fu</li>
<li>for: 这 paper 的目的是解决 Seq2Seq 中的慢渐进问题，提高文本编辑速度。</li>
<li>methods: 这 paper 使用了一种新的非autoregressive 文本编辑方法，通过使用 latent CTC 对齐来模型编辑过程。它还引入了 copy 操作来更有效地管理文本重叠。</li>
<li>results: 这 paper 的方法在 GEC 和 sentence fusion 任务上进行了广泛的实验，与现有的 Seq2Edit 模型相比，显著地提高了性能，并与 Seq2Seq 模型相比，实现了更高的速度（大于 $4\times$）。此外，它还在德语和俄语上进行了良好的扩展性测试。<details>
<summary>Abstract</summary>
Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the field of text editing, with the aim of addressing the slow autoregressive inference problem posed by the former. Despite promising results, Seq2Edit approaches still face several challenges such as inflexibility in generation and difficulty in generalizing to other languages. In this work, we propose a novel non-autoregressive text editing method to circumvent the above issues, by modeling the edit process with latent CTC alignments. We make a crucial extension to CTC by introducing the copy operation into the edit space, thus enabling more efficient management of textual overlap in editing. We conduct extensive experiments on GEC and sentence fusion tasks, showing that our proposed method significantly outperforms existing Seq2Edit models and achieves similar or even better results than Seq2Seq with over $4\times$ speedup. Moreover, it demonstrates good generalizability on German and Russian. In-depth analyses reveal the strengths of our method in terms of the robustness under various scenarios and generating fluent and flexible outputs.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:最近的工作受到了Seq2Seq到Seq2Edit的 Paradigm shift的影响，以解决前者的慢进 autoregressive inference 问题。尽管有了承诺的结果，Seq2Edit 方法仍然面临着一些挑战，如生成灵活性不足和难以泛化到其他语言。在这个工作中，我们提出了一种新的非autoregressive 文本编辑方法，通过在 latent CTC 对齐中模型编辑过程。我们对 CTC 进行了关键扩展，通过在编辑空间中引入复制操作，以更有效地处理文本重叠的问题。我们在 GEC 和 sentence fusion 任务上进行了广泛的实验，显示我们的提议方法可以具有较高的性能，与 Seq2Seq 的更换速度高于 4 倍。此外，它还能够在德语和俄语上达到类似或更好的结果。深入分析表明，我们的方法在不同的场景下具有良好的稳定性和生成灵活性。
</details></li>
</ul>
<hr>
<h2 id="Faithfulness-Measurable-Masked-Language-Models"><a href="#Faithfulness-Measurable-Masked-Language-Models" class="headerlink" title="Faithfulness Measurable Masked Language Models"></a>Faithfulness Measurable Masked Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07819">http://arxiv.org/abs/2310.07819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Madsen, Siva Reddy, Sarath Chandar</li>
<li>for: 本研究旨在提供一种可靠度度量的模型，以度量NLP模型的解释性。</li>
<li>methods: 本研究使用一种新的精度调整方法，该方法通过在模型中嵌入屏蔽token来使屏蔽token变得与标准分布相符，从而提高模型的可靠度度量。</li>
<li>results: 本研究通过在多种任务上应用该方法，并通过统计学的内部分布测试 validate the effectiveness of the proposed approach。 Additionally, the proposed approach improves the explainability of NLP models by making importance measures more faithful.<details>
<summary>Abstract</summary>
A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and validate it using statistical in-distribution tests. Additionally, because masking is in-distribution, importance measures which themselves use masking become more faithful, thus our model becomes more explainable.
</details>
<details>
<summary>摘要</summary>
通常来说，用importance measure来解释NLP模型的做法是非常常见的。然而，这些解释通常是错误的，即使很有说服力。因此，我们需要衡量其忠诚度。一种 metric 是，如果 tokens 是重要的，那么它们的masking应该导致模型的性能下降。然而，token masking 引入了 OUT-OF-distribution 问题，现有的解决方案都是 computationally expensive 并使用 proxy-models。此外，其他 metric 的范围非常有限。在这种情况下，我们提出了一种自然的 faithfulness measurable model，解决了这些挑战。这是通过一种新的 fine-tuning 方法，使得 masking tokens 变得 IN-distribution by design。这与现有的方法不同，它们是完全model-agnostic，但是在实践中无法应用。我们验证了我们的方法的通用性，并使用 statistical in-distribution 测试来验证。此外，因为 masking 变得 IN-distribution，importance measure 自身使用 masking 就变得更 faithful，因此我们的模型变得更加可解释。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-As-Semantic-Indexers"><a href="#Language-Models-As-Semantic-Indexers" class="headerlink" title="Language Models As Semantic Indexers"></a>Language Models As Semantic Indexers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07815">http://arxiv.org/abs/2310.07815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, Suhang Wang, Jiawei Han, Xianfeng Tang</li>
<li>for: 本文 targets 推荐和检索等下游任务，即使 semantic IDs 的学习 faces challenges such as 缺乏 semantic supervision and sequential discrete ID structure.</li>
<li>methods: 本文提出了一种自动学习的框架 LMINDEXER，通过使用生成型语言模型来学习 semantic IDs。具体来说，LMINDEXER 使用了递归学习和对比学习来学习 Neil 表示的序列化概率分布，并通过自我监督文档重建目标来缓解缺乏 semantic supervision 的问题。</li>
<li>results: 在三个任务中（推荐、产品搜索和文档检索），LMINDEXER 在五个 dataset 上取得了与竞争对手相比高度显著和一致的性能提升。<details>
<summary>Abstract</summary>
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackle the challenge of sequential discrete ID by introducing a semantic indexer capable of generating neural sequential discrete representations with progressive training and contrastive learning. In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective. The learned semantic indexer can facilitate various downstream tasks, such as recommendation and retrieval. We conduct experiments on three tasks including recommendation, product search, and document retrieval on five datasets from various domains, where LMINDEXER outperforms competitive baselines significantly and consistently.
</details>
<details>
<summary>摘要</summary>
Semantic identifier (ID) 是信息检索中一个重要概念，旨在保留文档和项目中的 semantics。前一些研究通常采用两个阶段管道来学习含义 ID，首先使用商业化的文本编码器获取嵌入，然后基于嵌入生成 ID。然而，每一步都会导致信息损失，并且通常存在嵌入空间生成的 latent distribution 和预期的 ID 分布之间的匹配问题。此外，具有含义 ID 的文档结构是隐藏的，很难直接学习文档的含义表示和层次结构。在本文中，我们引入 LMINDEXER，一种自动编码的框架，用于学习含义 ID。我们解决了顺序 discrete ID 的挑战，通过引入一种含义编码器，能够生成神经网络顺序 discrete 表示，并在进行进度训练和对比学习中提高表示的质量。受到含义监督的缺乏问题，我们提议使用自动编码的文档重建目标来训练模型。学习的含义编码器可以帮助下游任务，如推荐和检索。我们在五个不同领域的五个数据集上进行了三个任务的实验，包括推荐、产品搜索和文档检索，LMINDEXER 与竞争对手比较显著并且一致性高。
</details></li>
</ul>
<hr>
<h2 id="Ontology-Enrichment-for-Effective-Fine-grained-Entity-Typing"><a href="#Ontology-Enrichment-for-Effective-Fine-grained-Entity-Typing" class="headerlink" title="Ontology Enrichment for Effective Fine-grained Entity Typing"></a>Ontology Enrichment for Effective Fine-grained Entity Typing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07795">http://arxiv.org/abs/2310.07795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang, Jiawei Han</li>
<li>for: 本研究的目的是开发一个不需要人工标注的精细化entity typing方法，以便在不同的语言和文本中进行精细化entity typing。</li>
<li>methods: 我们提出了一个名为OnEFET的方法，其中我们将ontology结构中的每个node补充了两种额外信息：instance information和topic information。我们还开发了一个从粗到细的类型识别算法，该算法利用了补充后的ontology结构，通过训练一个推理模型来进行精细化类型识别。</li>
<li>results: 我们的实验结果显示，OnEFET可以在不需要人工标注的情况下，实现高品质的精细化entity typing，并且比现有的零shot方法大幅提高性能，甚至可以与有标注的方法相匹配。<details>
<summary>Abstract</summary>
Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose OnEFET, where we (1) enrich each node in the ontology structure with two types of extra information: instance information for training sample augmentation and topic information to relate types to contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that OnEFET achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods.
</details>
<details>
<summary>摘要</summary>
《细化实体类型标识（FET）任务是根据实体提及的上下文信息确定特定的实体类型。传统的FET方法需要大量的人工标注，这是时间consuming和costly。 latest studies have been developing weakly supervised or zero-shot approaches. 在我们的研究中，我们研究了基于ontology的zero-shot FET setting，但是现有的ontology结构缺乏详细的支持信息，甚至存在冲突关系，使其无法有效地引导FET。 latest developed语言模型，虽在various few-shot和zero-shot NLP任务中表现出色，但在zero-shot FET中可能会面临挑战，因为它们与任务特定的ontology之间没有交互。在本研究中，我们提出OnEFET方法，其包括以下两个部分：1. 对ontology结构中的每个节点添加两种类型的额外信息：实例信息用于增强训练样本，以及话题信息用于将类型与上下文相关联。2. 开发一种粗细类型推理算法，利用增强后的信息，通过训练对照话题和实例基于增强训练样本的推理模型，以实现高质量的细化实体类型标识。我们的实验表明，OnEFET方法可以在无人标注的情况下实现高质量的细化实体类型标识，与现有的零shot方法比，差距较大，甚至可以与supervised方法相媲美。
</details></li>
</ul>
<hr>
<h2 id="To-Build-Our-Future-We-Must-Know-Our-Past-Contextualizing-Paradigm-Shifts-in-Natural-Language-Processing"><a href="#To-Build-Our-Future-We-Must-Know-Our-Past-Contextualizing-Paradigm-Shifts-in-Natural-Language-Processing" class="headerlink" title="To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing"></a>To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07715">http://arxiv.org/abs/2310.07715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell</li>
<li>for: 本研究旨在理解NLP领域的未来发展，通过对过去的发展情况进行研究。</li>
<li>methods: 本研究使用长期采访26名NLP研究者，了解他们对领域的看法和体验，并通过分析ACL Anthology中文献的引用、作者和语言使用趋势来补充。</li>
<li>results: 研究发现了NLP领域的cyclical patterns和新的变化，包括benchmark文化和软件基础设施的变化。研究者们共享对未来的希望和担忧，并提出了更加 deliberate 的行动来形塑未来。<details>
<summary>Abstract</summary>
NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Found-in-the-Middle-Permutation-Self-Consistency-Improves-Listwise-Ranking-in-Large-Language-Models"><a href="#Found-in-the-Middle-Permutation-Self-Consistency-Improves-Listwise-Ranking-in-Large-Language-Models" class="headerlink" title="Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models"></a>Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07712">http://arxiv.org/abs/2310.07712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/castorini/perm-sc">https://github.com/castorini/perm-sc</a></li>
<li>paper_authors: Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture</li>
<li>for:  addresses the issue of positional bias in listwise ranking tasks using large language models (LLMs)</li>
<li>methods:  proposes a method called permutation self-consistency, which marginalizes out different list orders in the prompt to produce an order-independent ranking with less positional bias</li>
<li>results:  improves scores from conventional inference by up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B) on five list-ranking datasets in sorting and passage reranking, surpassing the previous state of the art in passage reranking.<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking in the presence of random perturbations. Empirically, on five list-ranking datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous state of the art in passage reranking. Our code is at https://github.com/castorini/perm-sc.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在使用上下文时表现出位置偏袋，这会使列表排序更加复杂。为解决这问题，我们提议使用排序自适应性，即黑盒LLM的排序输出的自适应性。我们的关键想法是在提交给LLM的提示中重复排序列表，然后通过计算提示中列表的不同排序顺序而生成一个位置无关的排序。首先，我们给一个输入提示，然后重复地将提示中的列表排序，并将排序结果通过LLM进行处理，保持 instrucions 不变。接着，我们将得到的多个排序样本 aggregated 以计算最近的中间排序，这样就可以消除提示中的位置偏袋。我们理论上证明了我们的方法的稳定性，并证明在Random pertubations 的存在下，我们的方法会 converge 到真实的排序。empirically，我们在5个列表排序 dataset 上进行 sorting 和 passage reranking  task 上，我们的方法可以提高 convential inference 的得分，最高提高7-18% 和8-16% 分别，超过了过去的最佳性能。我们的代码可以在https://github.com/castorini/perm-sc 找到。
</details></li>
</ul>
<hr>
<h2 id="DiPmark-A-Stealthy-Efficient-and-Resilient-Watermark-for-Large-Language-Models"><a href="#DiPmark-A-Stealthy-Efficient-and-Resilient-Watermark-for-Large-Language-Models" class="headerlink" title="DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models"></a>DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07710">http://arxiv.org/abs/2310.07710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang</li>
<li>for: 本研究旨在提高数据安全性，通过隐藏在数据中的潜在信息来保护数据。</li>
<li>methods: 我们提出了一种新的分布保持（DiP）水印技术，该技术可以在水印过程中保持原始数据的分布，并且可以在不需要访问语言模型API或参数的情况下进行检测。</li>
<li>results: 我们的方法在针对不同的数据集进行实验中，显示了高度的隐身性、高效性和抗性能。这些结果表明，我们的DiPmark技术可以成为数据水印任务中的一种可靠的解决方案。<details>
<summary>Abstract</summary>
Watermarking techniques offer a promising way to secure data via embedding covert information into the data. A paramount challenge in the domain lies in preserving the distribution of original data during watermarking. Our research extends and refines existing watermarking framework, placing emphasis on the importance of a distribution-preserving (DiP) watermark. Contrary to the current strategies, our proposed DiPmark preserves the original token distribution during watermarking (stealthy), is detectable without access to the language model API or weights (efficient), and is robust to moderate changes of tokens (resilient). This is achieved by incorporating a novel reweight strategy, combined with a hash function that assigns unique \textit{i.i.d.} ciphers based on the context. The empirical benchmarks of our approach underscore its stealthiness, efficiency, and resilience, making it a robust solution for watermarking tasks that demand impeccable quality preservation.
</details>
<details>
<summary>摘要</summary>
通过推入隐藏信息的技术来保护数据， watermarking 技术具有潜在的应用前景。然而，在这个领域中，一个挑战是保持原始数据的分布。我们的研究扩展和改进了现有的 watermarking 框架，强调在 watermarking 过程中保持原始数据的分布。与现有策略不同，我们的提出的 DiPmark 可以在隐身下（stealthy）、无需对语言模型 API 或权重进行访问（efficient）、并且对 Token 的修改具有较好的抗性（resilient）。这是通过 integrate 一种新的重Weight 策略和基于上下文的哈希函数来实现的。我们的实验室测试表明，我们的方法具有隐身、高效和抗性等优点，使其成为保护需要优质保持的任务中的一种可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="MatFormer-Nested-Transformer-for-Elastic-Inference"><a href="#MatFormer-Nested-Transformer-for-Elastic-Inference" class="headerlink" title="MatFormer: Nested Transformer for Elastic Inference"></a>MatFormer: Nested Transformer for Elastic Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07707">http://arxiv.org/abs/2310.07707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain</li>
<li>for: 这篇论文旨在提供一个可调式的Transformer架构，以满足不同的推广环境，例如多个加速器集群和单独的手持式智能手机。</li>
<li>methods: 本研究使用了MatFormer架构，一种嵌套式Transformer架构，实现模型的灵活性。每个Feed Forward Network（FFN）层在MatFormer模型中被联合地优化了一些嵌套的小 FFN层。这个训练程序允许在不同层次上进行模型的混合，例如从一个已训练的通用MatFormer模型中提取出百species的精确小模型，这些小模型从未直接优化过。</li>
<li>results: 本研究在不同的模型类型（解oder和编码器）、modalities（语言和视觉）和 scales（ UP to 2.6B 参数）中评估了MatFormer的效果。结果显示，一个2.6B解oder-only MatFormer语言模型（MatLM）可以将模型范围从1.5B到2.6B不同的精确小模型，每个模型都展示了相似的验证损失和一次下游评估。此外，我们发现小编码器从一个通用MatFormer-based ViT（MatViT）编码器中提取出的模型 preserved 度量空间结构，用于大规模适应 Retrieval。最后，我们显示了对于MatFormer模型的推广批评可以进一步降低推广时间。<details>
<summary>Abstract</summary>
Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from MatFormer can further reduce inference latency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transformer 模型在多种场景中部署，从多个加速器集群到单个手持设备。这些多样化的推理约束使得实践者需要训练基础模型如PaLM 2、Llama 和 ViTs 等多种模型的变体。由于训练成本昂贵，只有一些选择的模型大小得到训练和支持，限制了更细化的控制 над相关的负载、成本和准确率。这项工作介绍了 MatFormer，一种嵌入式 transformer 架构，用于提供多种部署约束的灵活性。每个Feed Forward Network（FFN）块的 MatFormer 模型都与一些嵌入的小 FFN 块进行共同优化。这种训练过程允许在层次上混合模型粒度，即一个已训练的通用 MatFormer 模型可以提取百种准确的小模型，这些小模型从未直接优化。我们在不同的模型类型（解码器和编码器）、modalities（语言和视觉）和scale（ UP TO 2.6B 参数）上进行了实验，发现一个 2.6B 解码器-only MatFormer 语言模型（MatLM）可以提取尺度从 1.5B 到 2.6B 之间的多种准确的小模型，每个模型都展现出与独立训练的同类模型相同的验证损失和一次性下游评估。此外，我们发现小编码器提取自通用 MatFormer-based ViT 编码器（MatViT） preserve  метри空间结构，用于适应大规模适应 retrieval。最后，我们发现可以通过准确和一致的小模型提取自 MatFormer 进行推理时的推测执行。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Ferret-Refer-and-Ground-Anything-Anywhere-at-Any-Granularity"><a href="#Ferret-Refer-and-Ground-Anything-Anywhere-at-Any-Granularity" class="headerlink" title="Ferret: Refer and Ground Anything Anywhere at Any Granularity"></a>Ferret: Refer and Ground Anything Anywhere at Any Granularity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07704">http://arxiv.org/abs/2310.07704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-ferret">https://github.com/apple/ml-ferret</a></li>
<li>paper_authors: Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang</li>
<li>For: The paper presents a new multimodal large language model (MLLM) called Ferret, which is capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.* Methods: The paper employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. Additionally, the paper proposes a spatial-aware visual sampler to extract continuous features of versatile regions.* Results: The paper achieves superior performance in classical referring and grounding tasks and greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. The paper also shows improved capability in describing image details and alleviated object hallucination.Here are the three points in Simplified Chinese text:* For: 这篇论文提出了一种新的多模态大语言模型（MLLM）名为 Ferret，它可以理解图像中任何形状或粒度的空间引用，并准确地将开放词汇描述与图像相关联。* Methods: 论文使用了一种新的混合坐标和连续特征的区域表示方法，这种方法可以同时使用精确的坐标和连续特征来表示图像中的区域。此外，论文还提出了一种适应空间的视觉采样器，可以处理不同形状的区域中的变化精度。* Results: 论文的实验结果表明，Ferret可以在经典的引用和固定 Task 中表现出色，同时也在基于区域和本地化的多模态对话中表现出色。论文还发现，Ferret可以更好地描述图像细节，并减少对象投影现象。<details>
<summary>Abstract</summary>
We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at https://github.com/apple/ml-ferret
</details>
<details>
<summary>摘要</summary>
我们介绍 Ferret，一个新的多modal大语言模型（MLLM），能够理解图像中任何形状或粒度的空间参考，并将开 vocabulary 的描述精确地落地。为了在 LLM 模型中统一参考和落地，Ferret 使用了一个新的强大的混合区域表示方法，将组合点坐标和连续特征来表示一个区域在图像中。为了提取不同形状的区域中的连续特征，我们提出了一个适应性的可见频率抽样器，能够处理不同形状的统计差异。因此，Ferret 可以接受多种区域输入，包括点、 bounding box 和自由形状。为了增强 Ferret 的预期功能，我们组建了 GRIT，一个全面的参考和落地指令调整dataset，包含110万个样本，其中95,000个困难样本来提高模型的Robustness。最终的模型不仅在经典的参考和落地任务中表现出色，而且在区域基础的多模ALM 谈话中也表现出色，并且具有了较好的描述图像细节和关键物体误射改善。我们的评估还显示，Ferret 在图像辨识和物体描述方面的表现都有了明显的提高。代码和数据将会在https://github.com/apple/ml-ferret 上公开。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Memory-Model-for-Emotional-Support-Conversation"><a href="#Knowledge-enhanced-Memory-Model-for-Emotional-Support-Conversation" class="headerlink" title="Knowledge-enhanced Memory Model for Emotional Support Conversation"></a>Knowledge-enhanced Memory Model for Emotional Support Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07700">http://arxiv.org/abs/2310.07700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengzhao Jia, Qianglong Chen, Liqiang Jing, Dawei Fu, Renyu Li</li>
<li>for: 这篇论文的目的是提出一个新的知识增强的 Memory mODEl for emotional suppoRt coNversation (MODERN), 以便解决现有方法面临的三个挑战：1）情感变化的多样性、2）实用的回应、3）复杂的战略模型。</li>
<li>methods: 这篇论文使用了一个知识增强的对话ContextEncoding来捕捉不同时间段的聊天中的动态情感变化，并从ConceptNet中选择相关的上下文概念来生成实用的回应。另外，它还实现了一个新的记忆增强的战略模型模组，以模型内在的语言结构和semantic pattern。</li>
<li>results: 实验结果显示，这篇论文的MODERN模型在一个大规模的数据集上比cutting-edge基eline有superiority。<details>
<summary>Abstract</summary>
The prevalence of mental disorders has become a significant issue, leading to the increased focus on Emotional Support Conversation as an effective supplement for mental health support. Existing methods have achieved compelling results, however, they still face three challenges: 1) variability of emotions, 2) practicality of the response, and 3) intricate strategy modeling. To address these challenges, we propose a novel knowledge-enhanced Memory mODEl for emotional suppoRt coNversation (MODERN). Specifically, we first devise a knowledge-enriched dialogue context encoding to perceive the dynamic emotion change of different periods of the conversation for coherent user state modeling and select context-related concepts from ConceptNet for practical response generation. Thereafter, we implement a novel memory-enhanced strategy modeling module to model the semantic patterns behind the strategy categories. Extensive experiments on a widely used large-scale dataset verify the superiority of our model over cutting-edge baselines.
</details>
<details>
<summary>摘要</summary>
现在，情绪疾病的流行性已经成为一个严重的问题，导致了对情绪支持对话的增加关注，以提供有效的心理健康支持。现有的方法已经取得了吸引人的结果，但它们仍然面临三大挑战：1）情绪的变化性，2）回应的实用性，3）复杂的战略模型化。为了解决这些挑战，我们提出了一种基于知识的Memory mODEl для情绪支持对话（MODERN）。具体来说，我们首先开发了一种具有知识扩展的对话上下文编码，以捕捉不同时期的对话中动态变化的情绪变化，并从ConceptNet中选择相关的上下文概念来生成实用的回应。其次，我们实施了一种新的记忆增强策略模型模块，以模型在策略类别之间的semanticpattern。经过广泛的实验，我们发现我们的模型在一个广泛使用的大规模数据集上胜过了当今最前沿的基eline。
</details></li>
</ul>
<hr>
<h2 id="Composite-Backdoor-Attacks-Against-Large-Language-Models"><a href="#Composite-Backdoor-Attacks-Against-Large-Language-Models" class="headerlink" title="Composite Backdoor Attacks Against Large Language Models"></a>Composite Backdoor Attacks Against Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07676">http://arxiv.org/abs/2310.07676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: 这 paper 探讨了基础模型中的攻击性 vulnerability，特别是通过 backdoor 攻击来威胁下游任务。</li>
<li>methods: 这 paper 使用了多个触发键散布在不同的提问组件中，这种 Composite Backdoor Attack (CBA) 比单个组件中的触发键更隐蔽。</li>
<li>results: 实验结果表明，CBA 在自然语言处理 (NLP) 和多媒体任务中都是有效的，例如在 Emotion 数据集上，使用 $3%$ 恶意样本对 LLaMA-7B 模型进行攻击，成功率为 $100%$，误 trigger 率在 $2.06%$ 以下，模型性能下降很小。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a $100\%$ Attack Success Rate (ASR) with a False Triggered Rate (FTR) below $2.06\%$ and negligible model accuracy degradation. The unique characteristics of our CBA can be tailored for various practical scenarios, e.g., targeting specific user groups. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Well-Begun-is-Half-Done-Generator-agnostic-Knowledge-Pre-Selection-for-Knowledge-Grounded-Dialogue"><a href="#Well-Begun-is-Half-Done-Generator-agnostic-Knowledge-Pre-Selection-for-Knowledge-Grounded-Dialogue" class="headerlink" title="Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue"></a>Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07659">http://arxiv.org/abs/2310.07659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qinlang14/gate">https://github.com/qinlang14/gate</a></li>
<li>paper_authors: Lang Qin, Yao Zhang, Hongru Liang, Jun Wang, Zhenglu Yang</li>
<li>for: 这篇论文主要旨在探讨知识选择在知识基据对话系统中的应用，特别是在知识生成前进行精确的选择。</li>
<li>methods: 本研究提出了一种 generator-agnostic 知识选择方法（GATE），通过选择不同知识结构和变量知识需求中的上下文相关知识，以准备之后的回应生成模型（如 ChatGPT）发展更加充分的回应。</li>
<li>results: 实验结果显示 GATE 方法能够实现更高精度的知识选择，并且显示了知识选择 перед生成是一种轻量级 yet 有效的方法，可以帮助 LLMs 发展更加充分的回应。<details>
<summary>Abstract</summary>
Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of GATE, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.
</details>
<details>
<summary>摘要</summary>
精准的知识选择是知识固定对话系统中的关键。为了更加细化它，我们提出了一种新的视角，即知识选择与生成结合。我们强调第三种尚未得到足够关注的领域，即可以不仅在预先选择正确的知识，而且具有降低后续响应生成模型（特别是LLMs）的学习、调整和解释负担的优点。我们提出了GATE，一种生成器独立的知识选择方法，用于为后续响应生成模型选择相关的知识结构和变量知识需求。实验结果表明GATE的优越性，并指示知识选择 перед生成是一种轻量级 yet effective的方式，使LLMs（例如ChatGPT）生成更加有用的响应。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Neural-Syntax-Acquisition"><a href="#Audio-Visual-Neural-Syntax-Acquisition" class="headerlink" title="Audio-Visual Neural Syntax Acquisition"></a>Audio-Visual Neural Syntax Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07654">http://arxiv.org/abs/2310.07654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-I Jeff Lai, Freda Shi, Puyuan Peng, Yoon Kim, Kevin Gimpel, Shiyu Chang, Yung-Sung Chuang, Saurabhchand Bhati, David Cox, David Harwath, Yang Zhang, Karen Livescu, James Glass</li>
<li>for: 这个论文旨在研究视觉受支持的语音结构induction。</li>
<li>methods: 论文使用Audio-Visual Neural Syntax Learner（AV-NSL）模型，通过对图像和说话caption的对应进行训练，从而学习语音中的段落结构。</li>
<li>results: 实验表明，AV-NSL可以学习出有意义的段落结构，与自然supervised文本解析器相似，并且可以在英语和德语中进行应用。这些结果扩展了先前的无监督语言学习和基于图像的语法induction研究，并提供了一种将两个领域相连的方法。<details>
<summary>Abstract</summary>
We study phrase structure induction from visually-grounded speech. The core idea is to first segment the speech waveform into sequences of word segments, and subsequently induce phrase structure using the inferred segment-level continuous representations. We present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns phrase structure by listening to audio and looking at images, without ever being exposed to text. By training on paired images and spoken captions, AV-NSL exhibits the capability to infer meaningful phrase structures that are comparable to those derived by naturally-supervised text parsers, for both English and German. Our findings extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and present one approach to bridge the gap between the two topics.
</details>
<details>
<summary>摘要</summary>
我们研究语音结构推导从视觉基于的语音。核心思想是首先将语音波形分割成单词段序列，然后使用推导出的段级连续表示来推导语音结构。我们提出了听视语音神经语法学习器（AV-NSL），它通过听取音频和看到图像，不需要文本supervise，可以学习语音结构。通过对匹配的图像和说话笔记进行训练，AV-NSL可以推导出有意义的语音结构，与自然supervise的文本分析器相似，并且可以扩展到英语和德语两种语言。我们的发现扩展了先前的无监督语言学习从speech和基于图像的语法推导，并提供了一种将这两个领域相互连接的方法。
</details></li>
</ul>
<hr>
<h2 id="LLM4Vis-Explainable-Visualization-Recommendation-using-ChatGPT"><a href="#LLM4Vis-Explainable-Visualization-Recommendation-using-ChatGPT" class="headerlink" title="LLM4Vis: Explainable Visualization Recommendation using ChatGPT"></a>LLM4Vis: Explainable Visualization Recommendation using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07652">http://arxiv.org/abs/2310.07652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/demoleiwang/llm4vis">https://github.com/demoleiwang/llm4vis</a></li>
<li>paper_authors: Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, Yong Wang<br>for:The paper is written for proposing a novel ChatGPT-based prompting approach for visualization recommendation and returning human-like explanations using very few demonstration examples.methods:The approach involves feature description, demonstration example selection, explanation generation, demonstration example construction, and inference steps. The paper also proposes a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-based hint.results:The paper evaluates the approach on the VizML dataset and shows that LLM4Vis outperforms or performs similarly to supervised learning models like Random Forest, Decision Tree, and MLP in both few-shot and zero-shot settings. The qualitative evaluation also shows the effectiveness of explanations generated by LLM4Vis.<details>
<summary>Abstract</summary>
Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples. Our approach involves feature description, demonstration example selection, explanation generation, demonstration example construction, and inference steps. To obtain demonstration examples with high-quality explanations, we propose a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-based hint. Evaluations on the VizML dataset show that LLM4Vis outperforms or performs similarly to supervised learning models like Random Forest, Decision Tree, and MLP in both few-shot and zero-shot settings. The qualitative evaluation also shows the effectiveness of explanations generated by LLM4Vis. We make our code publicly available at \href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.
</details>
<details>
<summary>摘要</summary>
“数据视觉是一种强大的工具，可以用来探索和传达不同领域的探索结果。为了自动选择视觉，一个称为视觉建议的任务已经被提出。但是，这些机器学习基于的方法通常需要大量的数据集和视觉对amples来训练，并且lack自然的解释。为了解决这个研究差距，我们提出了LLM4Vis，一种基于ChatGPT的新提问方法，用于进行视觉建议并返回人类化的解释。我们的方法包括特征描述、示例选择、解释生成、示例建构和推理步骤。为了获得高质量的解释示例，我们提出了一种新的解释生成bootstrap，可以逐次改进生成的解释，通过考虑之前的生成和模板基于的提示。我们的评估结果表明，LLM4Vis在VizML数据集上与随机森林、决策树和多层感知网络一样或更高的性能，并且在几个零shot和几个few-shot setting中表现出色。质量评估还表明LLM4Vis生成的解释的效果。我们将代码公开在 GitHub 上，请参考 \href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}。”
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-at-Evaluating-Instruction-Following"><a href="#Evaluating-Large-Language-Models-at-Evaluating-Instruction-Following" class="headerlink" title="Evaluating Large Language Models at Evaluating Instruction Following"></a>Evaluating Large Language Models at Evaluating Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07641">http://arxiv.org/abs/2310.07641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/llmbar">https://github.com/princeton-nlp/llmbar</a></li>
<li>paper_authors: Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen</li>
<li>for: This paper aims to investigate the effectiveness of LLM-based evaluation, particularly in assessing instruction following, and to provide a challenging benchmark for testing the ability of LLM evaluators.</li>
<li>methods: The authors use a manually curated dataset of 419 pairs of outputs, one adhering to instructions and the other diverging but with deceptive qualities, to evaluate the performance of different LLM evaluators. They also present a novel suite of prompting strategies to improve the performance of LLM evaluators.</li>
<li>results: The authors find that different LLM evaluators exhibit distinct performance on their benchmark, LLMBar, and even the highest-scoring ones have room for improvement. They also show that their proposed prompting strategies can close the gap between LLM and human evaluators.<details>
<summary>Abstract</summary>
As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.
</details>
<details>
<summary>摘要</summary>
继续推动大语言模型（LLM）研究，LLM-基于评估已成为可扩展和成本效果的选择，用于比较不断增加的模型。这篇论文研究LLM评估器的能力，尤其是用于评估生成文本是否遵循给定的指令。我们提出了一个挑战性的 meta-评估标准， LLMBench，用于测试 LLM 评估器对生成文本是否遵循指令的能力。作者手动精心挑选了419对输出，其中一个遵循指令，另一个偏离指令，但可能具有诱导 LLM 评估器的特质，如更有吸引力的语调。与现有的 meta-评估不同，我们发现不同的评估器（即 LLM 和提示的组合）在 LLMBench 中表现出不同的能力，甚至最高分的评估器还有很大的提高空间。我们还提出了一个新的提示策略集，可以进一步减少 LLM 和人类评估器之间的差距。通过 LLMBench，我们希望能够为 LLM 评估器提供更多的视角，并促进未来关于更好的指令遵循模型的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Past-Present-and-Better-Future-of-Feedback-Learning-in-Large-Language-Models-for-Subjective-Human-Preferences-and-Values"><a href="#The-Past-Present-and-Better-Future-of-Feedback-Learning-in-Large-Language-Models-for-Subjective-Human-Preferences-and-Values" class="headerlink" title="The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"></a>The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07629">http://arxiv.org/abs/2310.07629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Rose Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott A. Hale</li>
<li>for: 本研究旨在探讨如何有效、有效地收集和利用人类反馈，以优化大型自然语言模型（LLM）的表现。</li>
<li>methods: 本文首先概述过去在语言模型中集成人类反馈的趋势，然后提供现有技术和实践，包括反馈的收集方法、收集者的来源、以及使用反馈的动机和框架。</li>
<li>results: 本文提出了五个未解决的概念和实践问题，以促进未来对feedback学习的进一步发展。<details>
<summary>Abstract</summary>
Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.
</details>
<details>
<summary>摘要</summary>
人类反馈是现在大语言模型（LLM）的行为指导方式的越来越重要。然而，很难确定如何收集和利用反馈，以达到高效、公正和准确的方式，尤其是对于人类的主观喜好和价值观。在这篇论文中，我们对已有的反馈学习方法进行了报告，涵盖了95篇主要来自ACL和arXiv存储库的论文。首先，我们总结了过去对人类反馈的 интеGRATION INTO language models的趋势。其次，我们提供了当前的技术和实践，以及使用反馈的动机，价值和偏好的概念框架，以及反馈从谁收集。最后，我们鼓励更好的反馈学习在LLMs中，提出了五个未解决的概念和实践挑战。
</details></li>
</ul>
<hr>
<h2 id="QACHECK-A-Demonstration-System-for-Question-Guided-Multi-Hop-Fact-Checking"><a href="#QACHECK-A-Demonstration-System-for-Question-Guided-Multi-Hop-Fact-Checking" class="headerlink" title="QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking"></a>QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07609">http://arxiv.org/abs/2310.07609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xinyuanlu00/qacheck">https://github.com/xinyuanlu00/qacheck</a></li>
<li>paper_authors: Liangming Pan, Xinyuan Lu, Min-Yen Kan, Preslav Nakov</li>
<li>for: 本研究旨在提供一个名为 Question-guided Multi-hop Fact-Checking (QACHECK) 系统，用于确认真实性的调查。</li>
<li>methods: QACHECK 系统使用了一系列的问题来引导模型的推理过程，并且提供了一个完整的报告，详细说明了其推理过程。</li>
<li>results: QACHECK 系统可以帮助用户快速、精确地确认真实性，并且提供了一个透明、可说明的推理过程。<details>
<summary>Abstract</summary>
Fact-checking real-world claims often requires complex, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACHECK) system, which guides the model's reasoning process by asking a series of questions critical for verifying a claim. QACHECK has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACHECK, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACHECK also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process. A recorded video of QACHECK is at https://www.youtube.com/watch?v=ju8kxSldM64
</details>
<details>
<summary>摘要</summary>
现实中的真假查核常需要复杂的多步逻辑，因为没有直接的证据支持或驳斥声明。然而，现有的真假查核系统经常缺乏决策过程的透明性，使用户困难理解其思维过程。为解决这问题，我们提议了问题导向多步查核系统（QACHECK），该系统通过提问一系列关键性的问题来引导模型的思维过程。QACHECK包括五个关键模块：声明验证器、问题生成器、问题回答模块、QA验证器和理解器。用户可以将声明输入到QACHECK中，然后系统会预测声明的真实性并提供一份详细的报告，描述了其思维过程，带有一序列（问题、答案）对。QACHECK还提供了每个问题的证据来源，以便 transparent、可解释和用户友好的真假查核过程。有关QACHECK的视频记录在 YouTube 上可以查看，请参阅 <https://www.youtube.com/watch?v=ju8kxSldM64>。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Factuality-in-Large-Language-Models-Knowledge-Retrieval-and-Domain-Specificity"><a href="#Survey-on-Factuality-in-Large-Language-Models-Knowledge-Retrieval-and-Domain-Specificity" class="headerlink" title="Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity"></a>Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07521">http://arxiv.org/abs/2310.07521</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangcunxiang/llm-factuality-survey">https://github.com/wangcunxiang/llm-factuality-survey</a></li>
<li>paper_authors: Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang</li>
<li>for: 这项研究旨在解决大自然语言模型（LLM）中的事实问题。随着 LLM 在多个领域应用，其可靠性和准确性的要求变得更加重要。</li>
<li>methods: 本研究首先探讨了 LLM 生成内容与确立的事实之间的差异的后果和挑战。然后，我们分析了 LLM 存储和处理事实的机制，寻找主要导致事实错误的原因。最后，我们评估了评价 LLM 事实准确性的方法，包括关键指标、标准做法和研究。</li>
<li>results: 我们发现， LLM 的事实准确性问题存在多种原因，包括数据质量问题、模型设计问题和生成过程问题。我们还提出了一些改进 LLM 事实准确性的策略，包括针对特定领域的方法。此外，我们发现 standalone LLM 和 Retrieval-Augmented LLM 在不同领域中的特殊挑战和改进方法。<details>
<summary>Abstract</summary>
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.
</details>
<details>
<summary>摘要</summary>
We analyze the mechanisms by which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then turns to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We also explore strategies for enhancing LLM factuality, including approaches tailored for specific domains.We focus on two primary LLM configurations: standalone LLMs and Retrieval-Augmented LLMs that utilize external data. We detail their unique challenges and potential enhancements. Our survey provides a structured guide for researchers aiming to fortify the factual reliability of LLMs.
</details></li>
</ul>
<hr>
<h2 id="Cognate-Transformer-for-Automated-Phonological-Reconstruction-and-Cognate-Reflex-Prediction"><a href="#Cognate-Transformer-for-Automated-Phonological-Reconstruction-and-Cognate-Reflex-Prediction" class="headerlink" title="Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction"></a>Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07487">http://arxiv.org/abs/2310.07487</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. S. D. S. Mahesh Akavarapu, Arnab Bhattacharya</li>
<li>for: 这研究的目的是自动化历史语言学中的语音重建问题，通过学习模型以推断祖语中的 proto-word。</li>
<li>methods: 本研究使用了计算机生物学中的一些想法和技术，例如 MSA Transformer 语言模型，并将其应用于自动化语音重建问题。</li>
<li>results: 研究表明，我们的模型在两个相关任务中均能够超越现有的模型，特别是在预训练masked word prediction任务下表现更好。<details>
<summary>Abstract</summary>
Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model outperforms the existing models on both tasks, especially when it is pre-trained on masked word prediction task.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Phonological reconstruction is one of the central problems in historical linguistics, where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction, where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model outperforms the existing models on both tasks, especially when it is pre-trained on masked word prediction task.习惯性语言学中的phonological reconstruction是一个中心问题，即推算祖语的proto-word，根据观察的儿语词的观察。 computation approaches to historical linguistics尝试通过学习模型，将这些资料自动化。 several ideas and techniques drawn from computational biology have been successfully applied in the area of computational historical linguistics。 following these lines, we adapt MSA Transformer，a protein language model，to the problem of automated phonological reconstruction。 MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words。 we, hence, name our model as Cognate Transformer。 we also apply the model on another associated task，namely，cognate reflex prediction，where a reflex word in a daughter language is predicted based on cognate words from other daughter languages。 we show that our model outperforms the existing models on both tasks，especially when it is pre-trained on masked word prediction task。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Adapting-the-adapters-for-code-switching-in-multilingual-ASR"><a href="#Adapting-the-adapters-for-code-switching-in-multilingual-ASR" class="headerlink" title="Adapting the adapters for code-switching in multilingual ASR"></a>Adapting the adapters for code-switching in multilingual ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07423">http://arxiv.org/abs/2310.07423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atharva7k/mms-code-switching">https://github.com/atharva7k/mms-code-switching</a></li>
<li>paper_authors: Atharva Kulkarni, Ajinkya Kulkarni, Miguel Couceiro, Hanan Aldarmaki</li>
<li>for: 这个论文旨在提高自动语音识别（ASR）在多种低资源语言上的扩展性。</li>
<li>methods: 该论文提出了一种在混合语言speech中使用多语言模型的方法，通过在网络中的语言适应器之间协调信息来提高模型的性能。另外，该论文还提出了一种基于序列隐藏Markov chain的方法，来模型混合语言的流程。</li>
<li>results: 该论文的实验结果表明，在使用该方法进行微调后，模型在混合语言speech中的性能得到了明显提高，减少了误分类率不 menos于10%。<details>
<summary>Abstract</summary>
Recently, large pre-trained multilingual speech models have shown potential in scaling Automatic Speech Recognition (ASR) to many low-resource languages. Some of these models employ language adapters in their formulation, which helps to improve monolingual performance and avoids some of the drawbacks of multi-lingual modeling on resource-rich languages. However, this formulation restricts the usability of these models on code-switched speech, where two languages are mixed together in the same utterance. In this work, we propose ways to effectively fine-tune such models on code-switched speech, by assimilating information from both language adapters at each language adaptation point in the network. We also model code-switching as a sequence of latent binary sequences that can be used to guide the flow of information from each language adapter at the frame level. The proposed approaches are evaluated on three code-switched datasets encompassing Arabic, Mandarin, and Hindi languages paired with English, showing consistent improvements in code-switching performance with at least 10\% absolute reduction in CER across all test sets.
</details>
<details>
<summary>摘要</summary>
最近，大型预训练多语言speech模型已经表现出在多语言自动听说识别（ASR）中扩大应用前景。一些这些模型使用语言适配器在其 формуля中，可以提高单语言性能并避免在资源充沛语言上多语言模型化的一些缺点。然而，这种形式限制了这些模型在混合语言speech中的可用性， где两种语言在同一句中混合在一起。在这种工作中，我们提出了有效地微调这些模型在混合语言speech中的方法，通过在每个语言适配点处吸收两种语言适配器中的信息。我们还模型了混合语言为一个序列的潜在二进制序列，可以在帧级别引导来自每种语言适配器的信息流。我们的方法被评估在三个混合语言dataset上，包括阿拉伯语、普通话和印地语与英语的混合，显示了一致性的改进，减少了至少10%的CER。
</details></li>
</ul>
<hr>
<h2 id="Linguistic-laws-in-biology"><a href="#Linguistic-laws-in-biology" class="headerlink" title="Linguistic laws in biology"></a>Linguistic laws in biology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07387">http://arxiv.org/abs/2310.07387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stuart Semple, Ramon Ferrer-i-Cancho, Morgan L. Gustison</li>
<li>for:  investigate the prevalence of linguistic laws in biology</li>
<li>methods:  adopt a new conceptual framework that integrates distinct levels of analysis, from description to prediction to theory building</li>
<li>results:  provide critical new insights into the fundamental rules of organisation underpinning natural systems, unifying linguistic laws and core theory in biology<details>
<summary>Abstract</summary>
Linguistic laws, the common statistical patterns of human language, have been investigated by quantitative linguists for nearly a century. Recently, biologists from a range of disciplines have started to explore the prevalence of these laws beyond language, finding patterns consistent with linguistic laws across multiple levels of biological organisation, from molecular (genomes, genes, and proteins) to organismal (animal behaviour) to ecological (populations and ecosystems). We propose a new conceptual framework for the study of linguistic laws in biology, comprising and integrating distinct levels of analysis, from description to prediction to theory building. Adopting this framework will provide critical new insights into the fundamental rules of organisation underpinning natural systems, unifying linguistic laws and core theory in biology.
</details>
<details>
<summary>摘要</summary>
生物学中的语言法律，也就是人类语言中的统计趋势，已经在量化语言学家的研究中进行了近百年的探索。而最近，生物学家从多个领域来的研究者开始探索这些法律在生物体系中的普遍性，从分子水平（基因、蛋白质）到生物水平（动物行为）到生态水平（人口和生态系统）。我们提出了一个新的概念框架，用于语言法律在生物学中的研究，包括描述、预测和理论建构三个水平。采用这个框架，将提供新的理解到自然系统的基本规则，并将语言法律和生物学核心理论相结合。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Effect-of-Language-Models-in-Sequence-Discriminative-Training-for-Neural-Transducers"><a href="#Investigating-the-Effect-of-Language-Models-in-Sequence-Discriminative-Training-for-Neural-Transducers" class="headerlink" title="Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers"></a>Investigating the Effect of Language Models in Sequence Discriminative Training for Neural Transducers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07345">http://arxiv.org/abs/2310.07345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Yang, Wei Zhou, Ralf Schlüter, Hermann Ney</li>
<li>for: 这个研究是 investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers.</li>
<li>methods: 这个研究使用了 lattice-free 和 N-best-list 方法，并提出了一种方法来 Approximate the context history to employ LMs with full-context dependency.</li>
<li>results: 实验结果表明，使用 word-level LM 在训练中表现比 phoneme-level LM 更好，并且Context size of the LM used for probability computation has a limited effect on performance.<details>
<summary>Abstract</summary>
In this work, we investigate the effect of language models (LMs) with different context lengths and label units (phoneme vs. word) used in sequence discriminative training for phoneme-based neural transducers. Both lattice-free and N-best-list approaches are examined. For lattice-free methods with phoneme-level LMs, we propose a method to approximate the context history to employ LMs with full-context dependency. This approximation can be extended to arbitrary context length and enables the usage of word-level LMs in lattice-free methods. Moreover, a systematic comparison is conducted across lattice-free and N-best-list-based methods. Experimental results on Librispeech show that using the word-level LM in training outperforms the phoneme-level LM. Besides, we find that the context size of the LM used for probability computation has a limited effect on performance. Moreover, our results reveal the pivotal importance of the hypothesis space quality in sequence discriminative training.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了不同语言模型（LM）的上下文长度和标签单元（音素 vs. 词）在序列推理训练中的效果，包括无栅栈和N-best-list方法。对于无栅栈方法，我们提出了一种方法来 aproximate上下文历史，以便使用全上下文依赖性的LM。这种 aproximation可以扩展到任意上下文长度，并允许使用词级LM在无栅栈方法中。此外，我们进行了系统性的比较，并发现使用词级LM在训练中表现更好于音素级LM。此外，我们发现上下文大小对LM在概率计算中的影响很有限，同时发现推理训练中假设空间质量的重要性。Note that Simplified Chinese is the official written form of Chinese used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="How-Do-Large-Language-Models-Capture-the-Ever-changing-World-Knowledge-A-Review-of-Recent-Advances"><a href="#How-Do-Large-Language-Models-Capture-the-Ever-changing-World-Knowledge-A-Review-of-Recent-Advances" class="headerlink" title="How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances"></a>How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07343">http://arxiv.org/abs/2310.07343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyintell/awesome-refreshing-llms">https://github.com/hyintell/awesome-refreshing-llms</a></li>
<li>paper_authors: Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, Jun Wang</li>
<li>for: 这篇论文主要是为了探讨如何使大语言模型（LLMs）能够适应世界知识的变化，而不需要从零 retraining。</li>
<li>methods: 该论文提出了一系列的方法来保持 LLMS 的更新状态，包括各种批处理技术、迁移学习技术以及知识 graph 等方法。</li>
<li>results: 该论文对这些方法进行了系统的比较和分析，并评估了它们的效果。同时，它还提出了未来研究的方向，以便维护 LLMS 的更新状态。<details>
<summary>Abstract</summary>
Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without re-training from scratch. We categorize research works systemically and provide in-depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome-refreshing-llms
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLMs）在各种任务上表现印象深刻，但它们很快就会变得过时。维护其最新状态是当前时代的一项急需问题。本文提供了对最近进展在将 LLMs 与不断变化的世界知识相匹配的全面回顾。我们系统地分类研究工作，提供了深入的比较和讨论。我们还讨论了现有的挑战和未来的发展方向，以便促进这一领域的研究。我们在 GitHub 上发布了详细的文献列表，请参考 <https://github.com/hyintell/awesome-refreshing-llms>。
</details></li>
</ul>
<hr>
<h2 id="SNOiC-Soft-Labeling-and-Noisy-Mixup-based-Open-Intent-Classification-Model"><a href="#SNOiC-Soft-Labeling-and-Noisy-Mixup-based-Open-Intent-Classification-Model" class="headerlink" title="SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model"></a>SNOiC: Soft Labeling and Noisy Mixup based Open Intent Classification Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07306">http://arxiv.org/abs/2310.07306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Kanwar, Aditi Seetha, Satyendra Singh Chouhan, Rajdeep Niyogi</li>
<li>for: 本研究开发了一种基于软标签和噪音混合的开放意图分类模型（SNOiC），以解决现有模型受到过拟合和生成偏见的限制。</li>
<li>methods: 该模型结合软标签和噪音混合策略，以减少偏见和生成开放意图类 pseudo-数据。</li>
<li>results: 实验结果表明，SNOiC 模型在四个基准数据集上实现最小和最大的表现为 68.72% 和 94.71%，分别，在认知开放意图类的任务中表现出了显著的改善（相比之前的模型，最小改善为 0.93%，最大改善为 12.76%）。<details>
<summary>Abstract</summary>
This paper presents a Soft Labeling and Noisy Mixup-based open intent classification model (SNOiC). Most of the previous works have used threshold-based methods to identify open intents, which are prone to overfitting and may produce biased predictions. Additionally, the need for more available data for an open intent class presents another limitation for these existing models. SNOiC combines Soft Labeling and Noisy Mixup strategies to reduce the biasing and generate pseudo-data for open intent class. The experimental results on four benchmark datasets show that the SNOiC model achieves a minimum and maximum performance of 68.72\% and 94.71\%, respectively, in identifying open intents. Moreover, compared to state-of-the-art models, the SNOiC model improves the performance of identifying open intents by 0.93\% (minimum) and 12.76\% (maximum). The model's efficacy is further established by analyzing various parameters used in the proposed model. An ablation study is also conducted, which involves creating three model variants to validate the effectiveness of the SNOiC model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Parrot-Enhancing-Multi-Turn-Chat-Models-by-Learning-to-Ask-Questions"><a href="#Parrot-Enhancing-Multi-Turn-Chat-Models-by-Learning-to-Ask-Questions" class="headerlink" title="Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions"></a>Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07301">http://arxiv.org/abs/2310.07301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchong Sun, Che Liu, Jinwen Huang, Ruihua Song, Fuzheng Zhang, Di Zhang, Zhongyuan Wang, Kun Gai</li>
<li>for: The paper aims to improve the effectiveness of chat models in multi-turn conversations by addressing the lack of high-quality multi-turn instruction-tuning data.</li>
<li>methods: The authors introduce Parrot, a highly scalable solution that automatically generates high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations.</li>
<li>results: The authors demonstrate that the dialogues gathered from Parrot-Ask markedly outperform existing multi-turn instruction-following datasets in critical metrics, and Parrot-Chat achieves strong performance against other 13B open-source models across a range of instruction-following benchmarks, particularly excelling in evaluations of multi-turn capabilities.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 提高 chat 模型在多回合对话中的效果，解决社区存在的高质量多回合指导数据的缺乏问题。</li>
<li>methods: 引入 Parrot，一种可扩展的解决方案，通过自动生成高质量指导数据来提高 chat 模型在多回合对话中的效果。</li>
<li>results: Parrot-Ask 生成的对话集表现出色，覆盖了多个主题，数量丰富，与人类对话更加相似，并且 Parrot-Chat 在多个指导数据上达到了强劲的表现。<details>
<summary>Abstract</summary>
Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either single-turn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We then utilize Parrot-Ask to engage in multi-turn conversations with ChatGPT across a diverse range of topics, resulting in a collection of 40K high-quality multi-turn dialogues (Parrot-40K). These data are subsequently employed to train a chat model that we have named Parrot-Chat. We demonstrate that the dialogues gathered from Parrot-Ask markedly outperform existing multi-turn instruction-following datasets in critical metrics, including topic diversity, number of turns, and resemblance to human conversation. With only 40K training examples, Parrot-Chat achieves strong performance against other 13B open-source models across a range of instruction-following benchmarks, and particularly excels in evaluations of multi-turn capabilities. We make all codes, datasets, and two versions of the Parrot-Ask model based on LLaMA2-13B and KuaiYii-13B available at https://github.com/kwai/KwaiYii/Parrot.
</details>
<details>
<summary>摘要</summary>
很多进步已经在基于大语言模型（LLM）的 chat 模型方面得到了，但是在多回话对话中，开源 chat 模型（如 Alpaca 和 Vicuna）与领先的 chat 模型（如 ChatGPT 和 GPT-4）之间存在明显的延迟。经过一系列分析，我们归结这种延迟于社区可用的多回话指令调整数据不充足。现有的指令调整数据包括单回话对话或有问题的多回话对话，如非人类化的指令、精简的回答或罕见的主题转换。在这篇论文中，我们解决这些挑战 by introducing Parrot，一种可扩展的解决方案，用于自动生成高质量的指令调整数据，然后用这些数据来提高 chat 模型在多回话对话中的效果。 Specifically，我们首先训练 Parrot-Ask 模型，该模型是用来模拟真实用户生成指令的。然后，我们使用 Parrot-Ask 与 ChatGPT 进行多回话对话，从多种主题中收集了40000个高质量多回话对话（Parrot-40K）。这些数据被用来训练一个名为 Parrot-Chat 的 chat 模型。我们示示了 Parrot-Ask 生成的对话覆盖率高于现有的多回话指令遵从数据集，包括话题多样性、回话数量和人类对话的相似性。尽管只有40000个训练示例，Parrot-Chat 在多种指令遵从 benchmar ks 中表现出色，特别是在多回话评价中表现出色。我们在 GitHub 上提供了所有代码、数据集和两个基于 LLaMA2-13B 和 KuaiYii-13B 的 Parrot-Ask 模型，请参考 https://github.com/kwai/KwaiYii/Parrot。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Factuality-A-Comprehensive-Evaluation-of-Large-Language-Models-as-Knowledge-Generators"><a href="#Beyond-Factuality-A-Comprehensive-Evaluation-of-Large-Language-Models-as-Knowledge-Generators" class="headerlink" title="Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators"></a>Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07289">http://arxiv.org/abs/2310.07289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chanliang/conner">https://github.com/chanliang/conner</a></li>
<li>paper_authors: Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, Tat-Seng Chua, Kam-Fai Wong</li>
<li>For: The paper is written to evaluate the factuality and potential implications of using large language models (LLMs) to generate world knowledge, and to introduce a comprehensive framework (CONNER) for systematically and automatically evaluating generated knowledge from six important perspectives.* Methods: The paper uses three different types of LLMs to generate knowledge on two widely studied knowledge-intensive tasks, open-domain question answering and knowledge-grounded dialogue. The paper also employs an extensive empirical analysis to evaluate the generated knowledge from the six perspectives.* Results: The paper finds that the factuality of generated knowledge does not significantly hinder downstream tasks, and that relevance and coherence are more important than small factual mistakes. The paper also shows how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection.<details>
<summary>Abstract</summary>
Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在下游知识密集任务中表现更高，但社区对使用这些未经检查的知识表示关切。为了解决这问题，我们介绍了CONNER，一个全面的知识评估框架，可以系统地和自动地评估由六个重要角度评估生成的知识---事实性、相关性、一致性、启示性、帮助性和有效性。我们进行了对三种不同的LLM生成知识的广泛实验分析，在两个广泛研究的知识密集任务上，即开放领域问答和知识基础对话。 surprisingly，我们的研究发现，生成知识的事实性，即使低，并不会对下游任务产生重要阻碍。相反，输出的相关性和一致性更加重要于小错误。此外，我们还示出了如何使用CONNER来提高知识密集任务的性能，通过设计两种策略：提示工程和知识选择。我们的评估代码和LLM生成的知识以及人工注释将被发布，以便未来研究。
</details></li>
</ul>
<hr>
<h2 id="Typing-to-Listen-at-the-Cocktail-Party-Text-Guided-Target-Speaker-Extraction"><a href="#Typing-to-Listen-at-the-Cocktail-Party-Text-Guided-Target-Speaker-Extraction" class="headerlink" title="Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction"></a>Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07284">http://arxiv.org/abs/2310.07284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoxiangsnr/llm-tse">https://github.com/haoxiangsnr/llm-tse</a></li>
<li>paper_authors: Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan</li>
<li>for: 这个研究的目的是为了复制人类在听力环境中选择性地听注意力的能力，即cocktail party问题。</li>
<li>methods: 这个研究使用了目标说话者提取模型（TSE），并利用了大型自然语言模型（LLM）来提取用户输入文本中的有用语义特征。</li>
<li>results: 研究结果表明，只使用文本基于的cue可以获得竞争力强的表现，文本作为任务选择器的效果，并且可以与预先注册的cue结合使用，以实现新的州OF-THE-ART表现。<details>
<summary>Abstract</summary>
Humans possess an extraordinary ability to selectively focus on the sound source of interest amidst complex acoustic environments, commonly referred to as cocktail party scenarios. In an attempt to replicate this remarkable auditory attention capability in machines, target speaker extraction (TSE) models have been developed. These models leverage the pre-registered cues of the target speaker to extract the sound source of interest. However, the effectiveness of these models is hindered in real-world scenarios due to the unreliable or even absence of pre-registered cues. To address this limitation, this study investigates the integration of natural language description to enhance the feasibility, controllability, and performance of existing TSE models. Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) to extract useful semantic cues from the user's typed text input. These cues can serve as independent extraction cues, task selectors to control the TSE process, or complement the pre-registered cues. Our experimental results demonstrate competitive performance when only text-based cues are presented, the effectiveness of using input text as a task selector, and a new state-of-the-art when combining text-based cues with pre-registered cues. To our knowledge, this is the first study to successfully incorporate LLMs to guide target speaker extraction, which can be a cornerstone for cocktail party problem research.
</details>
<details>
<summary>摘要</summary>
人类具有一种杰出的听音选择能力，能够在复杂的听音环境中听到 interessante 的声音来源，这种情况通常被称为“cocktail party”问题。为了复制人类的听音注意力能力在机器中，目标说话人抽取（TSE）模型已经开发出来。这些模型利用目标说话人的预先注册的cue来提取听音来源。然而，现实世界中这些模型的效果受限因为预先注册的cue的可靠性和缺失。为解决这个限制，本研究提出了通过自然语言描述来增强现有TSE模型的可能性、可控性和性能。具体来说，我们提出了一种名为LLM-TSE的模型，其中使用大型语言模型（LLM）来提取用户输入文本中的有用semantic cue。这些cue可以作为独立提取cue，任务选择器来控制TSE过程，或者补充预先注册的cue。我们的实验结果表明，只要提供文本基于的cue，就可以达到竞争性的性能；使用文本作为任务选择器的效果，以及将文本基于的cue与预先注册的cue相结合时的新州OF-THE-ART性能。我们知道，这是首次成功地将LLMintegrated into TSE模型，这可能成为cocktail party问题研究的新起点。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-expressivity-transfer-in-textless-speech-to-speech-translation"><a href="#Enhancing-expressivity-transfer-in-textless-speech-to-speech-translation" class="headerlink" title="Enhancing expressivity transfer in textless speech-to-speech translation"></a>Enhancing expressivity transfer in textless speech-to-speech translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07279">http://arxiv.org/abs/2310.07279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jarod Duret, Benjamin O’Brien, Yannick Estève, Titouan Parcollet</li>
<li>for: 这项研究旨在解决现有语言自动识别系统中的表达精准性问题，以提高跨语言交流的效果。</li>
<li>methods: 该研究提出了一种新的方法，基于多语言情感嵌入来捕捉语言无关信息，并用于预测目标语言中语音单元的抽象和持续时间。</li>
<li>results: 对于法语到英语翻译任务，对比现有状态天下系统，该方法能够更好地传递表达性。<details>
<summary>Abstract</summary>
Textless speech-to-speech translation systems are rapidly advancing, thanks to the integration of self-supervised learning techniques. However, existing state-of-the-art systems fall short when it comes to capturing and transferring expressivity accurately across different languages. Expressivity plays a vital role in conveying emotions, nuances, and cultural subtleties, thereby enhancing communication across diverse languages. To address this issue this study presents a novel method that operates at the discrete speech unit level and leverages multilingual emotion embeddings to capture language-agnostic information. Specifically, we demonstrate how these embeddings can be used to effectively predict the pitch and duration of speech units in the target language. Through objective and subjective experiments conducted on a French-to-English translation task, our findings highlight the superior expressivity transfer achieved by our approach compared to current state-of-the-art systems.
</details>
<details>
<summary>摘要</summary>
文本 speech-to-speech 翻译系统在快速发展，各种自我超级学习技术的整合帮助了这一点。然而，现有的状态之一的系统在准确传递表达性方面缺乏表现。表达性在传递情感、细节和文化亚文化之间的差异中扮演着关键的角色，从而提高了不同语言之间的交流。为解决这个问题，本研究提出了一种新的方法，它在不同语言的语音单位级别操作，并使用多语言情感嵌入来捕捉语言无关的信息。我们通过对法语到英语翻译任务进行对象和主观实验，发现我们的方法可以更好地传递表达性，比现有的状态之一的系统更高效。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Landscape-of-Large-Language-Models-In-Medical-Question-Answering-Observations-and-Open-Questions"><a href="#Exploring-the-Landscape-of-Large-Language-Models-In-Medical-Question-Answering-Observations-and-Open-Questions" class="headerlink" title="Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions"></a>Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07225">http://arxiv.org/abs/2310.07225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karolina Korgul, Andrew M. Bean, Felix Krones, Robert McCraith, Adam Mahdi</li>
<li>for: 本研究旨在了解大型自然语言模型（LLMs）在医疗问答中的表现，以支持医疗工作者。</li>
<li>methods: 本研究使用了多种流行的LLMs，评估其对医疗问题的知识。</li>
<li>results: 研究发现了这些LLMs在医疗问题中的知识有限，存在一些共同特征和问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在医疗问答中展现出了承认的潜力，并被建议作为医疗工作者的工具。将LLM部署到高风险的Context中需要清晰地理解这些模型的限制。随着新的LLM的快速开发和发布，可以通过识别模型之间的相似性 Patterns 来了解这些模型的性质。在这篇论文中，我们评估了一些流行的LLM在医疗问题上的知识，以更好地理解它们的性质。从这个比较中，我们提出了初步的观察和进一步的研究问题。
</details></li>
</ul>
<hr>
<h2 id="PHALM-Building-a-Knowledge-Graph-from-Scratch-by-Prompting-Humans-and-a-Language-Model"><a href="#PHALM-Building-a-Knowledge-Graph-from-Scratch-by-Prompting-Humans-and-a-Language-Model" class="headerlink" title="PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model"></a>PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07170">http://arxiv.org/abs/2310.07170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-waseda/comet-atomic-ja">https://github.com/nlp-waseda/comet-atomic-ja</a></li>
<li>paper_authors: Tatsuya Ide, Eiki Murata, Daisuke Kawahara, Takato Yamazaki, Shengzhe Li, Kenta Shinzato, Toshinori Sato</li>
<li>for: 这篇论文是为了提出一种从头开始构建知识图谱的方法，以及使用这个知识图谱来训练日语常识生成模型。</li>
<li>methods: 这篇论文使用了人群和大语言模型（LLM）的唤起来构建知识图谱，并使用这个知识图谱来训练日语常识生成模型。</li>
<li>results: 实验结果表明， constructed 知识图谱和训练后的模型可以生成有效的常识推理。 另外，对于人群和 LLM 的唤起来的 diferencia 也被报告。 代码、数据和模型可以在github.com&#x2F;nlp-waseda&#x2F;comet-atomic-ja 上获取。<details>
<summary>Abstract</summary>
Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.
</details>
<details>
<summary>摘要</summary>
尽管已经有很大进步在自然语言理解方面，神经网络语言模型经常不好地处理常识知识。为建立常识意识模型，有人尝试了从自动获取到招募社会人员的方法。然而，获取高质量的知识基础，特别是从头开始，很难且成本高。在这篇论文中，我们提出了PHALM方法，可以从头开始建立知识图，通过招募社会人员和大语言模型（LLM）的提示。我们使用这种方法建立了日本事件知识图，并训练了日本常识生成模型。实验结果表明建立的图和训练的模型生成的推理都是可接受的。我们还报告了人工和LLM的提示之间的差异。我们的代码、数据和模型可以在github.com/nlp-waseda/comet-atomic-ja上找到。
</details></li>
</ul>
<hr>
<h2 id="Psychoacoustic-Challenges-Of-Speech-Enhancement-On-VoIP-Platforms"><a href="#Psychoacoustic-Challenges-Of-Speech-Enhancement-On-VoIP-Platforms" class="headerlink" title="Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms"></a>Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07161">http://arxiv.org/abs/2310.07161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Shuo Han, Yunyang Zeng, Ankit Shah, Bhiksha Raj</li>
<li>for: 本研究探讨了VoIP（voice over internet protocol）通信中的听音变换问题，强调了 propietary sender-side denoising 的影响。</li>
<li>methods: 该研究使用了 Deep Noise Suppression（DNS）2020 数据集，并采用了 econometric 工具 Oaxaca decomposition 分析听音-phonetic 扰动的变化。</li>
<li>results: 研究发现，VoIP 系统中的听音变换会导致对话质量下降，并且不同的接收器接口和听音设置会导致不同的扰动效果。<details>
<summary>Abstract</summary>
Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.
</details>
<details>
<summary>摘要</summary>
在VoIP（voice over internet protocol）通信中，音响变换引入了复杂性，需要仔细分析。这项研究，基于专有发送器侧干扰效果的探索，仔细评估了Google Meets和Zoom等平台。研究利用了2020年 Deep Noise Suppression（DNS）数据集，确保结构化的审查，适应不同的干扰设置和接收器界面。本研究 introduce了一种方法创新，即使用Oaxaca分解，原来是经济统计工具，现在在VoIP系统中被重新应用来分析音响语音干扰。为更深入理解这些变换的影响，研究使用了心理学量化指标，包括PESQ和STOI，以提供全面的语音变化认知。总的来说，获得的发现强调了VoIP系统中音响动态的复杂领域。此外，研究还报告了多种指标，扩大了研究范围。此外，本研究还包括了出界预测，以增强对时间和时间频率频谱speech增强模型的应用性。
</details></li>
</ul>
<hr>
<h2 id="“A-Tale-of-Two-Movements”-Identifying-and-Comparing-Perspectives-in-BlackLivesMatter-and-BlueLivesMatter-Movements-related-Tweets-using-Weakly-Supervised-Graph-based-Structured-Prediction"><a href="#“A-Tale-of-Two-Movements”-Identifying-and-Comparing-Perspectives-in-BlackLivesMatter-and-BlueLivesMatter-Movements-related-Tweets-using-Weakly-Supervised-Graph-based-Structured-Prediction" class="headerlink" title="“A Tale of Two Movements”: Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction"></a>“A Tale of Two Movements”: Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07155">http://arxiv.org/abs/2310.07155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Roy, Dan Goldwasser</li>
<li>for: 本研究旨在 automaticaly 理解 #BlackLivesMatter 运动related tweets 中的 perspective 和 voice。</li>
<li>methods: 我们提出了一种weakly supervised graph-based方法，通过将文本转化为结构化元素 graph，并利用作者社交网络，进行 structured prediction，以确定 perspective。</li>
<li>results: 我们的模型在Quantitative 和 Qualitative 分析中表现出色，在人工标注测试集上超过多任务基线，成功地 caracterize #BLM 运动中支持和反对 perspective。<details>
<summary>Abstract</summary>
Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model outperforms multitask baselines by a large margin, successfully characterizing the perspectives supporting and opposing #BLM.
</details>
<details>
<summary>摘要</summary>
社交媒体已成为社会变革的主要驱动力，通过促成在线社会运动的形成。自动理解运动中的看法和反对看法是一项具有挑战性的任务，因为精心标注数据很难以获得。我们提出了一种弱监督图Structured prediction的方法，其中明确表达了 #BlackLivesMatter 相关的微博中的看法。我们的提议方法首先将文本转换为图形，然后使用作者社交网络连接其中的结构化元素，并通过结构预测来识别看法。我们的方法使用小量精心标注示例。我们使用大型自然语言模型生成人工训练示例，并与手动标注进行比较，发现它们具有相似性。我们对人类标注测试集进行量化和质量分析，发现我们的模型在 #BLM 相关的问题上大幅超过多任务基线，成功地描述了支持和反对 #BLM 的看法。
</details></li>
</ul>
<hr>
<h2 id="QFT-Quantized-Full-parameter-Tuning-of-LLMs-with-Affordable-Resources"><a href="#QFT-Quantized-Full-parameter-Tuning-of-LLMs-with-Affordable-Resources" class="headerlink" title="QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources"></a>QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07147">http://arxiv.org/abs/2310.07147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt Keutzer</li>
<li>for: 这个论文的目的是提出一种量化全参数调参框架（QFT），以优化大型自然语言处理模型（LLM）的性能。</li>
<li>methods: 该论文使用了两个新的想法：首先，使用高效的Lion优化器，只跟踪冲量和每个参数的常规更新大小，这有利于精炼量化；其次，对模型状态进行量化，并采用梯度流和参数更新方案来更新量化的加法器。</li>
<li>results: 该论文的实验结果显示，使用QFT可以将模型状态的内存占用量降低至21%，而性能与标准解决方案相当，例如，对一个LLaMA-7B模型进行调参只需要&lt;30GB的内存，可以用单个A6000 GPU满足。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update scheme for the quantized weights. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only <30GB of memory, satisfied by a single A6000 GPU.
</details>
<details>
<summary>摘要</summary>
Our framework includes two innovative ideas:1. We use the efficient Lion optimizer, which only tracks the momentum and has consistent update magnitudes for each parameter, making it well-suited for robust quantization.2. We quantize all model states and store them as integer values, and develop a gradient flow and parameter update scheme for the quantized weights.As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance. For example, fine-tuning a LLaMA-7B model only requires approximately 30GB of memory, which can be satisfied by a single A6000 GPU.
</details></li>
</ul>
<hr>
<h2 id="Empowering-Psychotherapy-with-Large-Language-Models-Cognitive-Distortion-Detection-through-Diagnosis-of-Thought-Prompting"><a href="#Empowering-Psychotherapy-with-Large-Language-Models-Cognitive-Distortion-Detection-through-Diagnosis-of-Thought-Prompting" class="headerlink" title="Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting"></a>Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07146">http://arxiv.org/abs/2310.07146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Chen, Yujie Lu, William Yang Wang</li>
<li>for: 这篇论文是为了提供人工智能助手来支持计算性心理治疗。</li>
<li>methods: 这篇论文使用大语言模型进行认知错误探测，并提出了诊断思维（DoT）提示，以进行三 stage 的诊断过程：主观评估、相对性推理和schema分析。</li>
<li>results: 实验结果显示，DoT 能够对于认知错误探测取得了重要的改善，并且生成了高品质的诊断理由，得到了人类专家的批准。<details>
<summary>Abstract</summary>
Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.
</details>
<details>
<summary>摘要</summary>
精神疾病仍然是当代公共卫生问题中的一个重要问题，因为专业人员的 scarcity 和访问限制。心理治疗需要高水平的专业技能，以进行深入、复杂的认知和分析，对患者的认知模型进行深入了解。在大语言模型时代，我们认为是时候开发人工智能助手，以帮助计算心理治疗。我们研究了思维扭曲检测任务，并提出了诊断思维（DoT）提示。DoT在患者的语音上进行三个阶段的诊断：主观评估，以分离事实和想法; 对比逻辑，以激发支持和驳斥思想的逻辑过程; 和schema分析，以总结认知schema。生成的诊断原因通过三个阶段是对专业人员的重要帮助。实验表明，DoT在思维扭曲检测方面比ChatGPT具有显著改善，同时生成高质量的人类专家批准的诊断理由。
</details></li>
</ul>
<hr>
<h2 id="AE-smnsMLC-Multi-Label-Classification-with-Semantic-Matching-and-Negative-Label-Sampling-for-Product-Attribute-Value-Extraction"><a href="#AE-smnsMLC-Multi-Label-Classification-with-Semantic-Matching-and-Negative-Label-Sampling-for-Product-Attribute-Value-Extraction" class="headerlink" title="AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction"></a>AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07137">http://arxiv.org/abs/2310.07137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhongfendeng/ae-smnsmlc">https://github.com/zhongfendeng/ae-smnsmlc</a></li>
<li>paper_authors: Zhongfen Deng, Wei-Te Chen, Lei Chen, Philip S. Yu</li>
<li>for: 这篇论文是为了解决电子商务中产品特征值EXTRACTION问题，该问题在实际应用中具有重要性。</li>
<li>methods: 这篇论文提出了一种基于多标签分类的方法，该方法可以应用于实际应用中，只有 attribute value 的weak annotation available。另外，该方法还考虑了产品特征值之间的语义连接，从而帮助 attribute value extraction。</li>
<li>results:  experiments 表明，该方法在三个实际应用中的 dataset 上具有效果和优势。<details>
<summary>Abstract</summary>
Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and negative label sampling for attribute value extraction. Semantic matching aims to capture semantic interactions between attribute values of a given product and its text. Negative label sampling aims to enhance the model's ability of distinguishing similar values belonging to the same attribute. Experimental results on three subsets of a large real-world e-Commerce dataset demonstrate the effectiveness and superiority of our proposed model.
</details>
<details>
<summary>摘要</summary>
In this paper, we reformulate the task as a multi-label classification task that can be applied in real-world scenarios where only attribute value annotations are available. We propose a classification model with semantic matching and negative label sampling for attribute value extraction. Semantic matching aims to capture the semantic interactions between attribute values of a given product and its text, while negative label sampling enhances the model's ability to distinguish similar values belonging to the same attribute.Experimental results on three subsets of a large real-world e-Commerce dataset demonstrate the effectiveness and superiority of our proposed model.
</details></li>
</ul>
<hr>
<h2 id="Comparing-Styles-across-Languages"><a href="#Comparing-Styles-across-Languages" class="headerlink" title="Comparing Styles across Languages"></a>Comparing Styles across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07135">http://arxiv.org/abs/2310.07135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreya Havaldar, Matthew Pressimone, Eric Wong, Lyle Ungar</li>
<li>for: 这 paper 是为了研究不同语言之间的文化风格差异而写的。</li>
<li>methods: 这 paper 使用了一种新的解释框架，可以从多语言模型中提取不同风格的特征，并将其归类到相似的 lexical categories 中。</li>
<li>results: 这 paper 提出了第一个涵盖四种语言的敬谔词汇集，并详细分析了各语言之间的敬谔风格差异。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world.
</details>
<details>
<summary>摘要</summary>
理解不同语言的风格差异对于训练人类和计算机生成文本的文化适应性是有利的。我们介绍了一种解释框架，用于从多语言LM中提取风格差异并对不同语言的风格进行比较。我们的框架包括（1）生成任何语言的完整风格词典，以及（2）从LM中抽取到相似类别的特征重要性。我们对政eness进行比较，创建了第一个涵盖四种语言的整体多语言政eness数据集，并explored如何在不同语言中表达尊敬的方式。我们的方法可以有效地评估不同语言语言类别的贡献，并提供可读性的交流见解。
</details></li>
</ul>
<hr>
<h2 id="Argumentative-Stance-Prediction-An-Exploratory-Study-on-Multimodality-and-Few-Shot-Learning"><a href="#Argumentative-Stance-Prediction-An-Exploratory-Study-on-Multimodality-and-Few-Shot-Learning" class="headerlink" title="Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning"></a>Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07093">http://arxiv.org/abs/2310.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arushi Sharma, Abhibha Gupta, Maneesh Bilalpur</li>
<li>for: 本研究旨在评估图像是否对社会热点话题中的立场预测有所必要，并比较原生多模态和文本基础模型在少数shot设定下的性能。</li>
<li>methods: 本研究使用了 Twitter 上的话题推荐和图像摘要来生成图像和文本对应的数据集，并对这些数据集进行了文本基础模型和多模态模型的 fine-tuning。</li>
<li>results: 研究发现， ensemble of fine-tuned 文本基础模型（0.817 F1-score）在社会热点话题中的立场预测性能较高，比 multimodal 模型（0.677 F1-score）和文本基础模型在少数shot设定下的预测（0.550 F1-score）更好。此外，研究还发现，当图像内容被摘要为自然语言时，多模态模型在表现更好。<details>
<summary>Abstract</summary>
To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.
</details>
<details>
<summary>摘要</summary>
要提高论据立场预测为多modal问题，第一次共同任务在多modal Argument Mining中进行了论据立场预测，涉及重要的社会问题如枪支持和堕胎。我们的探索研究试图评估图像是否对论据立场预测在微博中必要，并比较未经调整的文本基础大语言模型（LLM）在少数shot设置下表现于精心调整的单modal和多modal模型。我们的工作表明一个 ensemble of 精心调整的文本基础语言模型（0.817 F1-score）在性能方面超过了多modal（0.677 F1-score）和文本基础几shot预测使用最新的状态对技术（0.550 F1-score）。此外，我们发现在图像内容被摘要为自然语言的情况下，多modal模型在性能方面表现更好，并且在 Context 中使用示例可以提高 LLM 的几shot性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.CL_2023_10_11/" data-id="clnsn0vep008zgf88fc7pgcna" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.LG_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T10:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.LG_2023_10_11/">cs.LG - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines"><a href="#Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines" class="headerlink" title="Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines"></a>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07940">http://arxiv.org/abs/2310.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Puneet Gupta</li>
<li>for: This paper aims to enable widely-applicable smart devices by overcoming the storage and processing requirements of deep neural networks.</li>
<li>methods: The paper explores hardware&#x2F;software co-design, quantization, model scaling, and multi-modality to optimize system design and model deployment for cost-constrained platforms.</li>
<li>results: The paper demonstrates an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board, and provides guidelines for optimal system design and model deployment.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是使深度神经网络的存储和处理要求不再限制智能设备的普及。</li>
<li>methods: 这篇论文采用硬件&#x2F;软件共设方法，包括量化、模型缩放和多模态，以优化系统设计和模型部署，以满足成本和用户体验的要求。</li>
<li>results: 这篇论文通过使用 $20 ESP-EYE 板子实现了端到端、在设备上进行生物认证系统，并提供了优化系统设计和模型部署的指南。<details>
<summary>Abstract</summary>
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and develop a set of guidelines for optimal system design and model deployment for the most cost-constrained platforms. We demonstrate our approach using an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables"><a href="#Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables" class="headerlink" title="Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables"></a>Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07927">http://arxiv.org/abs/2310.07927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Zou, Pratyush Tiwary</li>
<li>for: 这种研究使用图 neural network 学习方法，以获取实验晶体结构中观察到的特征变量的低维度表示。</li>
<li>methods: 这种方法使用简单的卷积和聚合方法，并使用自适应器设置。</li>
<li>results: 研究人员通过对铁和глицин的多种相态和晶体结构的熔融态进行考察，发现图 latent variables 在宽温度热动力学中偏导转换 между状态，并实现了准确的自由能计算，两者都是样本采集的标志。<details>
<summary>Abstract</summary>
In this study, we present a graph neural network-based learning approach using an autoencoder setup to derive low-dimensional variables from features observed in experimental crystal structures. These variables are then biased in enhanced sampling to observe state-to-state transitions and reliable thermodynamic weights. Our approach uses simple convolution and pooling methods. To verify the effectiveness of our protocol, we examined the nucleation of various allotropes and polymorphs of iron and glycine from their molten states. Our graph latent variables when biased in well-tempered metadynamics consistently show transitions between states and achieve accurate free energy calculations in agreement with experiments, both of which are indicators of dependable sampling. This underscores the strength and promise of our graph neural net variables for improved sampling. The protocol shown here should be applicable for other systems and with other sampling methods.
</details>
<details>
<summary>摘要</summary>
在这种研究中，我们提出了基于图 neural network的学习方法，使用自适应Encoder设置来 derivate低维度变量从实验室中观察的晶体结构特征。这些变量然后在增强抽样中偏导，以观察状态之间的转移和可靠的热力学权重。我们的方法使用简单的卷积和聚合方法。为了验证我们的协议的有效性，我们对铁和glycine的多种晶体和多形的气相转换进行了研究。我们的图秘密变量，当偏导在well-tempered metadynamics中， consistently display state transitions and achieve accurate free energy calculations, both of which are indicators of reliable sampling.这些结果证明了我们的图 neural net变量的强大和承诺，并且这种协议应用于其他系统和其他抽样方法。
</details></li>
</ul>
<hr>
<h2 id="First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs"><a href="#First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs" class="headerlink" title="First-Order Dynamic Optimization for Streaming Convex Costs"></a>First-Order Dynamic Optimization for Streaming Convex Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07925">http://arxiv.org/abs/2310.07925</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Rostami, H. Moradian, S. S. Kia</li>
<li>for: 这个论文是为了解决一类具有时间变化的 convex 优化问题而提出的一些新的优化算法。</li>
<li>methods: 我们开发了一种方法来跟踪优化问题的最优解，并且只使用第一个Derivative 来实现。这使得我们的算法在优化时间变化的成本函数时更加 computationally efficient。</li>
<li>results: 我们比较了我们的算法与梯度下降算法，并证明了梯度下降算法在优化问题中不是有效的。我们还提供了一些例子，包括使用模型预测控制问题作为 convex 优化问题的解决方案。<details>
<summary>Abstract</summary>
This paper proposes a set of novel optimization algorithms for solving a class of convex optimization problems with time-varying streaming cost function. We develop an approach to track the optimal solution with a bounded error. Unlike the existing results, our algorithm is executed only by using the first-order derivatives of the cost function which makes it computationally efficient for optimization with time-varying cost function. We compare our algorithms to the gradient descent algorithm and show why gradient descent is not an effective solution for optimization problems with time-varying cost. Several examples including solving a model predictive control problem cast as a convex optimization problem with a streaming time-varying cost function demonstrate our results.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的优化算法集合，用于解决时间变化的流动成本函数 convex 优化问题。我们开发了一种方法来跟踪最优解，并保证错误在 bounded 的范围内。与现有结果不同，我们的算法仅使用函数成本的首个导数进行计算，这使得它在时间变化的成本函数上进行优化变得计算效率。我们与梯度下降算法进行比较，并解释了为什么梯度下降算法不适合时间变化的成本函数上进行优化。 several 个例子，包括解决一个模型预测控制问题，即asta convex 优化问题，演示了我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning"><a href="#Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning" class="headerlink" title="Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning"></a>Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07902">http://arxiv.org/abs/2310.07902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noémie Jaquier, Leonel Rozo, Tamim Asfour<br>for: This paper aims to address the “single tangent space fallacy” in the incorporation of Riemannian geometry into robot learning applications, and to provide a theoretical elucidation of various misconceptions surrounding this approach.methods: The paper offers experimental evidence of the shortcomings of this approach and provides valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.results: The paper presents a theoretically-founded critique of the “single tangent space fallacy” and demonstrates its limitations through experimental evidence. It provides valuable insights to promote best practices in the use of Riemannian geometry in robot learning applications.<details>
<summary>Abstract</summary>
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.
</details>
<details>
<summary>摘要</summary>
在机器人学领域，许多下游机器人任务利用机器学习方法处理、模型或生成数据。经常情况下，这些数据包含具有几何约束的变量，如rigid-body orientations的quaternions的单位 нормаcondition或操作灵活性 ellipsoids的正见定义性。有效地处理这些几何约束需要在机器学习方法的形ulation中包含泛函几何工具。在这个上下文中，里曼几何 manifold emerges as a powerful mathematical framework to handle such geometric constraints. However, its recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the "single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.
</details></li>
</ul>
<hr>
<h2 id="Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs"><a href="#Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs" class="headerlink" title="Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs"></a>Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07895">http://arxiv.org/abs/2310.07895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Werner, Christoph Gerum, Moritz Reiber, Jörg Nick, Oliver Bringmann</li>
<li>for: 这个论文是用于分类 Gastroenterologic 部分图像，来自 Video Capsule Endoscopy (VCE) 研究。</li>
<li>methods: 这个方法利用 Convolutional Neural Network (CNN) 进行分类，并利用时间序列分析特性来 correction errors in CNN 输出。</li>
<li>results: 这个方法在 Rhode Island (RI) Gastroenterology 数据集上达到了 $98.04%$ 的准确率，可以准确地定位 Gastrointestinal (GI) 轨道，仅需约 1M 参数，适用于低功耗设备。<details>
<summary>Abstract</summary>
This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control"><a href="#ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control" class="headerlink" title="ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control"></a>ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07892">http://arxiv.org/abs/2310.07892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalil Chavez-Galaviz, Jianwen Li, Ajinkya Chaudhary, Nina Mahmoudian</li>
<li>For: The paper is written for Autonomous Surface Vehicles (ASVs) to improve their station-keeping ability in confined spaces with wind disturbances.* Methods: The paper proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances.* Results: The proposed NNSEM-MPC approach performs better than other controllers in simulation and field experiments, reducing the mean position and heading error by at least 31% and 46%, respectively, and is at least 36% faster than other MPC controllers.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为Autonomous Surface Vehicles（ASV）提高其在狭窄空间中的定站能力，特别是在风干扰下。</li>
<li>methods: 这篇论文提出了一种基于Neural Network Simulation Error Minimization（NNSEM-MPC）的模型预测控制器，以准确预测ASV在风干扰下的动态。</li>
<li>results: 对于 simulate 和实验测试，提出的NNSEM-MPC方法与其他控制器相比，可以减少平均定位和方向误差至少31%和46%，并且在其他MPC控制器的执行速度上占优，至少36% faster。<details>
<summary>Abstract</summary>
Station keeping is an essential maneuver for Autonomous Surface Vehicles (ASVs), mainly when used in confined spaces, to carry out surveys that require the ASV to keep its position or in collaboration with other vehicles where the relative position has an impact over the mission. However, this maneuver can become challenging for classic feedback controllers due to the need for an accurate model of the ASV dynamics and the environmental disturbances. This work proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances. The performance of the proposed scheme under wind disturbances is tested and compared against other controllers in simulation, using the Robotics Operating System (ROS) and the multipurpose simulation environment Gazebo. A set of six tests were conducted by combining two wind speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and 180$^\circ$). The simulation results clearly show the advantage of the NNSEM-MPC over the following methods: backstepping controller, sliding mode controller, simplified dynamics MPC (SD-MPC), neural ordinary differential equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed NNSEM-MPC approach performs better than the rest in 4 out of the 6 test conditions, and it is the second best in the 2 remaining test cases, reducing the mean position and heading error by at least 31\% and 46\% respectively across all the test cases. In terms of execution speed, the proposed NNSEM-MPC is at least 36\% faster than the rest of the MPC controllers. The field experiments on two different ASV platforms showed that ASVs can effectively keep the station utilizing the proposed method, with a position error as low as $1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at least $150$s.
</details>
<details>
<summary>摘要</summary>
驱动Autonomous Surface Vehicles (ASVs)的基本推进方法是站 Keeping，特别在封闭空间中使用，以进行需要ASV保持其位置或与其他车辆相合作，其中相对位置对任务有着重要影响。然而，这种推进方法可能会对 класси型反馈控制器造成挑战，因为需要高度准确的ASV动态模型和环境干扰的模型。本文提出一种使用神经网络预测误差最小化（NNSEM-MPC）的模型预测控制器，以准确预测ASV在风干扰下的动态。在实验中，与其他控制器进行比较，使用Robotics Operating System（ROS）和 multipurpose simulation environment Gazebo进行测试，共进行六个测试，其中两个风速（3 m/s和6 m/s）和三个风向（0$^\circ$, 90$^\circ$,和180$^\circ）。测试结果表明，提议的NNSEM-MPC方法在六个测试条件中比其他方法 луч，其中四个测试条件中表现最佳，剩下两个测试条件中表现第二好，可以将预测误差和 heading error降低至少31%和46%。在执行速度方面，提议的NNSEM-MPC方法至少36%更快于其他MPC控制器。在两个不同的ASV平台上进行了田下实验，结果表明，ASV可以效果地保持站点，位置误差为1.68米， heading error为6.14°，在至少150秒的时间窗口内。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks"><a href="#A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks" class="headerlink" title="A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks"></a>A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07891">http://arxiv.org/abs/2310.07891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban</li>
<li>for: This paper is written to explore the feature learning process in deep neural networks, specifically the appearance of separated rank-one components in the spectrum of the feature matrix, and how this can lead to improved learning of non-linear components.</li>
<li>methods: The paper uses two-layer fully-connected neural networks with a constant gradient descent step size, and shows that with a learning rate that grows with the sample size, multiple rank-one components are introduced, each corresponding to a specific polynomial feature.</li>
<li>results: The paper demonstrates that these non-linear features can enhance learning, and provides precise analysis of the improvement in the loss. The limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes.<details>
<summary>Abstract</summary>
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Feature learning是深度神经网络的成功的重要原因之一。已经确立了在满足certain conditions下，两层受束神经网络中的一步梯度下降后，ridge回归可以导致特征学习;这被称为特征突起的存在，特征突起在特征矩阵的спектру中出现。然而，固定梯度下降步长，这个突起只会带来linear компонент的信息，因此学习非线性Component是不可能的。我们展示了，在样本大小和训练样本数量增加时，使用增长的学习率，实际上会引入多个rank-one组件，每个组件都对应于特定的多项式特征。我们进一步证明，在大量训练和测试样本下，更新后的神经网络的训练和测试错误都会完全受到这些突起的限制。通过精确分析失准的改进，我们证明了这些非线性特征可以提高学习。Note: Simplified Chinese is used here, as it is the standard form of Chinese used in mainland China and widely understood by Chinese speakers. Traditional Chinese is also widely used, but it may not be as familiar to all Chinese speakers.
</details></li>
</ul>
<hr>
<h2 id="Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression"><a href="#Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression" class="headerlink" title="Refined Mechanism Design for Approximately Structured Priors via Active Regression"></a>Refined Mechanism Design for Approximately Structured Priors via Active Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07874">http://arxiv.org/abs/2310.07874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Boutsikas, Petros Drineas, Marios Mertzanidis, Alexandros Psomas, Paritosh Verma</li>
<li>for: 本文主要针对一个带有大量商品的销售商面临多个竞拍客户的情况，并且客户的价值评估是从高维不知的先验分布中独立采样的。</li>
<li>methods: 本文使用了一种Recently introduced by Cai and Daskalakis的模型，即将投标客户的先验分布近似为主题模型。文章包括一个活动学部分，负责与投标客户进行互动，并输出低维的客户类型估计，以及一个机制设计部分，负责为低维模型中的类型进行机制设计。</li>
<li>results: 文章提出了一种新的机制设计方法，可以在不需要访问到原始分布的情况下，针对低维模型中的类型进行机制设计。此外，文章还 imports 了多个Randomized Linear Algebra (RLA) 的突破成果，并将其适应到该设定中。<details>
<summary>Abstract</summary>
We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast our problem in the framework of Randomized Linear Algebra (RLA) for regression problems, allowing us to import several breakthrough results from that line of research, and adapt them to our setting. On the mechanism design front, we remove many restrictive assumptions of prior work on the type of access needed to the underlying distributions and the associated mechanisms. To the best of our knowledge, our work is the first to formulate connections between mechanism design, and RLA for active learning of regression problems, opening the door for further applications of randomized linear algebra primitives to mechanism design.
</details>
<details>
<summary>摘要</summary>
我们考虑一个收益最大化的买家，有一大量的物品($m$)供$n$名策略性投标者购买，投标者的价值是由高维度、未知的对应分布所生成的。这个设定中的优化和近似优化机制是非常困难 Compute 和找到，而且当机制存在时，它们常常具有各种Counter-intuitive 的性质。在这篇文章中，我们遵循 Cai 和 Daskalakis （2022）的模型，假设投标者的对应分布可以被快速地近似为主题模型。我们设计了一个活动学习部分，负责与投标者进行互动，并将投标者的类型转换为低维度的近似值，以及一个机制设计部分，负责对低维度模型进行机制设计，以适应投标者的近似类型。在活动学习方面，我们将问题推到了Randomized Linear Algebra（RLA）的框架中，这让我们可以将RLA中的多个突破性结果import 到我们的设定中，并适应它们。在机制设计方面，我们从之前的研究中解除了访问到底下的分布和相关机制的严格限制，以便更好地适应实际的应用。根据我们所知，我们的工作是首个将机制设计与RLA для活动学习的回传问题连接起来，这开启了进一步应用randomized linear algebra primitives 的可能性。
</details></li>
</ul>
<hr>
<h2 id="QArchSearch-A-Scalable-Quantum-Architecture-Search-Package"><a href="#QArchSearch-A-Scalable-Quantum-Architecture-Search-Package" class="headerlink" title="QArchSearch: A Scalable Quantum Architecture Search Package"></a>QArchSearch: A Scalable Quantum Architecture Search Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07858">http://arxiv.org/abs/2310.07858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankit Kulshrestha, Danylo Lykov, Ilya Safro, Yuri Alexeev</li>
<li>for: 这篇论文的目的是提供一种基于人工智能的量子架构搜索包，用于实现适当的单位变换以处理输入量子态。</li>
<li>methods: 该论文使用了\texttt{QTensor}库作为后端，并采用了一种两级并行的搜索策略，以实现高效地搜索大型量子电路。</li>
<li>results: 该论文通过实验表明，\texttt{QArchSearch}能够高效地搜索大型量子电路，并且可以允许更多的复杂模型 для不同的量子应用。<details>
<summary>Abstract</summary>
The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \texttt{QArchSearch}, an AI based quantum architecture search package with the \texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polaris supercomputer.
</details>
<details>
<summary>摘要</summary>
当今量子计算时代已经浮现了许多算法，这些算法承诺可以提供高效的计算。然而，在实际应用中，很少有指导如何设计合适的量子电路，以实现对输入量子态的相应的单位变换。在这篇论文中，我们介绍了\texttt{QArchSearch}，一个基于人工智能的量子架构搜索包，它使用\texttt{QTensor}库作为后端，并提供了一种原则性的和自动化的方法来找到任务和输入量子态的最佳模型。我们示示了\texttt{QArchSearch}能够有效率地搜索大量的量子电路，并允许探索不同的量子应用程序中的更复杂的模型。\texttt{QArchSearch}在高性能计算系统上运行，使用了两级并行的分布式计算策略，在CPUs和GPUs上进行并行计算，这已经在Polaris超级计算机上得到证明。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism"><a href="#On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism" class="headerlink" title="On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism"></a>On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07852">http://arxiv.org/abs/2310.07852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saptarshi Roy, Ambuj Tewari</li>
<li>for: 这个论文主要针对高维ensional sparse linear regression模型下的隐私保护问题，具体来说是 differentially private best subset selection 问题。</li>
<li>methods: 论文采用了 Well-known exponential mechanism 选择最佳模型，并在某种margin condition下确立了它的强模型重建性。</li>
<li>results: 论文提出了一种 Metropolis-Hastings 算法来避免 exponential search space 的计算瓶颈，并证明了它的多项式混合时间减小到 Parameters $n,p$, 和 $s$ 上。此外，论文还证明了 final estimates 的 approximate differential privacy 性。<details>
<summary>Abstract</summary>
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results.
</details>
<details>
<summary>摘要</summary>
我们考虑了在高维稀疏线性回归模型下进行模型选择，并在拥有差异隐私框架下进行研究。特别是，我们研究了具有差异隐私最佳子集选择的问题，并对其Utility guarantee进行了研究。我们采用了广泛使用的指数机制来选择最佳模型，并在满足某种margin条件下，Establish its strong model recovery property。然而，指数搜索空间中的指数机制带来了严重的计算瓶颈。为了解决这个挑战，我们提议了 Metropolis-Hastings算法来实现抽样步骤，并证明其在参数$n$, $p$, 和 $s$中的几何时间到其stationary distribution的 mixing property。此外，我们还证明了对最终估计的Metropolis-Hastings随机步进行了approximate差异隐私。最后，我们还进行了一些与 теория结论相符的实验 simulations。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Feature-Sparsity-in-Language-Models"><a href="#Measuring-Feature-Sparsity-in-Language-Models" class="headerlink" title="Measuring Feature Sparsity in Language Models"></a>Measuring Feature Sparsity in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07837">http://arxiv.org/abs/2310.07837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Deng, Lucas Tao, Joe Benton</li>
<li>for: 这些研究旨在探索语言模型活动可以被模型为稀疏线性组合的哪些特征。</li>
<li>methods: 这些研究使用稀疏编码技术来重建特征方向。</li>
<li>results: 研究发现，语言模型活动可以准确地被模型为稀疏线性组合的特征，比控制数据集更加准确。此外，模型活动显示在第一层和最后一层比较稀疏。<details>
<summary>Abstract</summary>
Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters"><a href="#Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters" class="headerlink" title="Large Language Models Are Zero-Shot Time Series Forecasters"></a>Large Language Models Are Zero-Shot Time Series Forecasters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07820">http://arxiv.org/abs/2310.07820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngruver/llmtime">https://github.com/ngruver/llmtime</a></li>
<li>paper_authors: Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson</li>
<li>for: 这个论文旨在探讨语言模型如何用于时间序列预测，以及这种方法如何与专门为时间序列预测设计的模型相比。</li>
<li>methods: 作者使用了大语言模型（LLM） such as GPT-3和LLaMA-2，并提出了一些方法来减少数字化时间序列数据的复杂性，例如使用简单的数字符号来表示复杂的时间序列 distribuions。</li>
<li>results: 作者发现，使用LLM可以在无需特定训练的情况下，对时间序列进行预测，并且其性能与专门为时间序列预测设计的模型相当或甚至超过。此外，LLM还可以处理缺失数据、考虑文本副信息和回答预测问题。然而，作者发现，增加模型大小通常会提高时间序列预测的性能，但GPT-4可能会比GPT-3 worse perfomance due to its tokenization strategy and uncertainty calibration issues。<details>
<summary>Abstract</summary>
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore"><a href="#Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore" class="headerlink" title="Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore"></a>Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07811">http://arxiv.org/abs/2310.07811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gellért Weisz, András György, Csaba Szepesvári</li>
<li>for: 这个论文研究的目的是提出一种在线学习 Reinforcement Learning（RL）算法，可以在 episodic Markov decision processes（MDPs）中实现 polynomial-sample-complexity 的学习。</li>
<li>methods: 该算法使用了 linear $q^\pi$-realizability 假设，即所有策略的动作价值可以表示为线性函数的状态动作特征。该假设被认为是更通用的，比 linear MDPs 更加一般。</li>
<li>results: 作者们提出了一种新的学习算法，可以在 linearly $q^\pi$-realizable MDPs 中同时学习缺省策略和隐藏的 linear MDP 问题。该算法可以在 $\text{polylog}(H, d)&#x2F;\epsilon^2$ 交互后返回 $\epsilon$-优化策略，其中 $H$ 是时间范围和 $d$ 是特征向量的维度。此外，作者们还证明了这种算法在不准确的情况下的样本复杂性。<details>
<summary>Abstract</summary>
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.
</details>
<details>
<summary>摘要</summary>
我们考虑在线上强化学习（RL）中的集集合数 Markov问题（MDP）下，假设所有政策的动作值可以表示为特征vector的线性函数。这个分类被认为是线性 MDP 的更一般化情况，而不是单纯的线性 MDP，其中transition构造和优化函数都是特征 вектор的线性函数。作为我们的第一个贡献，我们显示了这两个分类之间的差异在于在线性 qπ 可能性下的状态存在。具体来说，任何政策在这些状态下都有相当的动作值，并且透过 seguing 到这些状态中的任何政策可以将问题转换为线性 MDP。基于这个观察，我们提出了一个新的学习算法，可以同时学习哪些状态应该被跳过以及将这些状态转换为线性 MDP 中的问题。这个方法可以在 $\text{polylog}(H, d)/\epsilon^2$ 互动后返回 $\epsilon$-优化的政策，其中 $H$ 是时间点数和 $d$ 是特征 вектор的维度。我们还证明了这个方法在不精确情况下的样本复杂度，其样本复杂度会随着错误水平的下降。
</details></li>
</ul>
<hr>
<h2 id="FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning"><a href="#FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning" class="headerlink" title="FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning"></a>FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07807">http://arxiv.org/abs/2310.07807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ensiye Kiyamousavi, Boris Kraychev, Ivan Koychev<br>for: This paper focuses on addressing the challenges of data heterogeneity and model aggregation effectiveness in federated learning (FL) by proposing a method that leverages entropy and symmetry to construct diverse and controllable data distributions.methods: The proposed method uses techniques such as data partitioning, entropy, and symmetry to create ‘the most challenging’ and controllable data distributions with gradual difficulty.results: The proposed method is shown to be superior to existing FL data partitioning approaches, with the ability to gradually challenge model aggregation algorithms and produce more distinct models. Experimental results demonstrate the effectiveness of the proposed method in improving the robustness and accuracy of FL models.<details>
<summary>Abstract</summary>
Federated learning (FL) is a decentralized machine learning approach where independent learners process data privately. Its goal is to create a robust and accurate model by aggregating and retraining local models over multiple rounds. However, FL faces challenges regarding data heterogeneity and model aggregation effectiveness. In order to simulate real-world data, researchers use methods for data partitioning that transform a dataset designated for centralized learning into a group of sub-datasets suitable for distributed machine learning with different data heterogeneity. In this paper, we study the currently popular data partitioning techniques and visualize their main disadvantages: the lack of precision in the data diversity, which leads to unreliable heterogeneity indexes, and the inability to incrementally challenge the FL algorithms. To resolve this problem, we propose a method that leverages entropy and symmetry to construct 'the most challenging' and controllable data distributions with gradual difficulty. We introduce a metric to measure data heterogeneity among the learning agents and a transformation technique that divides any dataset into splits with precise data diversity. Through a comparative study, we demonstrate the superiority of our method over existing FL data partitioning approaches, showcasing its potential to challenge model aggregation algorithms. Experimental results indicate that our approach gradually challenges the FL strategies, and the models trained on FedSym distributions are more distinct.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式机器学习方法，其目标是通过聚合和重新训练本地模型来创建一个坚固和准确的模型。然而，FL面临着数据多样性和模型聚合效果的挑战。为了模拟实际数据，研究人员使用数据 partitioning 技术将一个用于中央化学习的数据集转换为适于分布式机器学习的多个子集，每个子集都具有不同的数据多样性。在这篇论文中，我们研究了目前流行的数据 partitioning 技术，并将其主要缺点进行视觉化：缺乏精度的数据多样性，导致不可靠的多样性指标，以及无法逐步挑战 FL 算法。为解决这个问题，我们提出了一种方法，该方法利用熵和对称来构建 '最具挑战性' 和可控的数据分布。我们引入了一个度量来衡量学习代理之间的数据多样性，并提出了一种分割技术，可以将任何数据集分成多个分布式学习适用的Split。通过比较研究，我们证明了我们的方法在现有 FL 数据 partitioning 方法之上具有优势，并表明其可以逐步挑战 FL 算法。实验结果表明，我们的方法可以逐步挑战 FL 策略，并且模型在 FedSym 分布上训练得到更加独特。
</details></li>
</ul>
<hr>
<h2 id="Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data"><a href="#Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data" class="headerlink" title="Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data"></a>Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07787">http://arxiv.org/abs/2310.07787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Wong, Phue Thant, Pratiksha Yadav, Ruta Antaliya, Jongwook Woo</li>
<li>For: The paper aims to predict airline ticket fares for non-stop flights across the US using a large dataset of flight pricing data.* Methods: The paper uses four regression machine learning algorithms (Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines) and assesses their performance and generalization capability using Cross Validator and Training Validator functions.* Results: The paper seeks to determine the best models for predicting airline ticket fares in the real world, with a focus on good generalization capability and optimized processing times.<details>
<summary>Abstract</summary>
This paper discusses predictive performance and processes undertaken on flight pricing data utilizing r2(r-square) and RMSE that leverages a large dataset, originally from Expedia.com, consisting of approximately 20 million records or 4.68 gigabytes. The project aims to determine the best models usable in the real world to predict airline ticket fares for non-stop flights across the US. Therefore, good generalization capability and optimized processing times are important measures for the model.   We will discover key business insights utilizing feature importance and discuss the process and tools used for our analysis. Four regression machine learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines utilizing Cross Validator and Training Validator functions for assessing performance and generalization capability.
</details>
<details>
<summary>摘要</summary>
这篇论文讨论了预测性能和基于飞行价格数据进行的处理过程，使用r2（r平方）和RMSE指标，利用Expedia.com原始数据集，包含约2000万记录或4.68 gigabytes的数据。项目的目标是找到适用于实际世界的最佳预测模型，以预测美国之间的直达航班票价。因此，良好的总体化能力和优化的处理时间是重要的评价指标。我们将通过特征重要性的探索和分析工具的介绍，挖掘出预测模型的关键业务洞察。在本项目中，我们使用了四种回归机器学习算法：随机森林、梯度提升树、决策树和分解机器，并使用了交叉验证和训练验证函数来评估性能和总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling"><a href="#Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling" class="headerlink" title="Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling"></a>Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07786">http://arxiv.org/abs/2310.07786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheqing Zhu, Yueyang Liu, Xu Kuang, Benjamin Van Roy</li>
<li>for: 非站立性上下文强化学习问题的实际应用，如推荐系统等，经常受到季节性、偶然性和社会趋势的影响，导致非站立性上下文强化学习算法的开发成为紧迫需要。</li>
<li>methods: 我们提出了一种新的非站立性上下文强化学习算法，它结合了可扩展的深度神经网络结构和精心设计的探索机制，以优先级Collect informative data with the most lasting value in a non-stationary environment。</li>
<li>results: 我们通过对两个实际推荐数据集进行实验，发现我们的方法与状态艺术基eline Significantly outperform the baseline。<details>
<summary>Abstract</summary>
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
在实际应用中，Contextual Bandits的应用 frequently exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. Although a number of non-stationary Contextual Bandit learning algorithms have been proposed in the literature, they often explore excessively due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action sets, or both. In this paper, we introduce a novel non-stationary Contextual Bandit algorithm that addresses these concerns. It combines a scalable, deep neural network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches"><a href="#Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches" class="headerlink" title="Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches"></a>Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07780">http://arxiv.org/abs/2310.07780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng</li>
<li>for: 提高随机缓和的抗击性能，以提供可证明的鲁棒性保证。</li>
<li>methods: 提出了两种成本效果的方法，包括AdvMacer和EsbRS。AdvMacer combinines adversarial training和鲁棒性证明最大化，而EsbRS使用模型集成来提高鲁棒性证明。</li>
<li>results: 比较SOTA基线的实验结果表明，AdvMacer可以提高随机缓和的抗击性能，而EsbRS可以大幅提高模型集成的鲁棒性。<details>
<summary>Abstract</summary>
Randomized smoothing has recently attracted attentions in the field of adversarial robustness to provide provable robustness guarantees on smoothed neural network classifiers. However, existing works show that vanilla randomized smoothing usually does not provide good robustness performance and often requires (re)training techniques on the base classifier in order to boost the robustness of the resulting smoothed classifier. In this work, we propose two cost-effective approaches to boost the robustness of randomized smoothing while preserving its clean performance. The first approach introduces a new robust training method AdvMacerwhich combines adversarial training and robustness certification maximization for randomized smoothing. We show that AdvMacer can improve the robustness performance of randomized smoothing classifiers compared to SOTA baselines, while being 3x faster to train than MACER baseline. The second approach introduces a post-processing method EsbRS which greatly improves the robustness certificate based on building model ensembles. We explore different aspects of model ensembles that has not been studied by prior works and propose a novel design methodology to further improve robustness of the ensemble based on our theoretical analysis.
</details>
<details>
<summary>摘要</summary>
优化后的随机缓和方法在反击机器学习领域吸引了关注，以提供可证明的安全保证。然而，现有的工作表明，普通的随机缓和方法通常不提供好的安全性表现，经常需要（重）训练技术来提高随机缓和后的类ifier的安全性。在这项工作中，我们提出了两种可行的方法来提高随机缓和方法的安全性，同时保持其净度性。第一种方法是我们提出的 AdvMacer，它结合了对随机缓和类ifier的对抗训练和安全性证明最大化。我们表明，AdvMacer可以提高随机缓和类ifier的安全性表现，并且比MACER基线 faster to train。第二种方法是我们提出的 EsbRS，它可以大幅提高基于模型集的安全证明。我们探索了不同的模型集方面，并提出了一种新的设计方法来进一步提高模型集的安全性。我们的理论分析表明，EsbRS可以提高模型集的安全性表现。
</details></li>
</ul>
<hr>
<h2 id="Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights"><a href="#Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights" class="headerlink" title="Feature Learning and Generalization in Deep Networks with Orthogonal Weights"></a>Feature Learning and Generalization in Deep Networks with Orthogonal Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07765">http://arxiv.org/abs/2310.07765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Day, Yonatan Kahn, Daniel A. Roberts</li>
<li>for: 该文章研究了深度 neural network 的调参问题，尤其是在宽度和深度相对较长时，如何避免网络中信号的扩散和干扰。</li>
<li>methods: 作者使用了矩阵 ensemble 和 rectangular network 的方法，并提出了一种新的初始化方法，即使用 orthogonal matrices 初始化网络的权重。</li>
<li>results: 作者通过分析和实验表明，使用这种新的初始化方法可以避免网络中信号的扩散和干扰，并且可以提高网络的泛化和训练速度。<details>
<summary>Abstract</summary>
Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We speculate that this structure preserves finite-width feature learning while reducing overall noise, thus improving both generalization and training speed. We provide some experimental justification by relating empirical measurements of the NTK to the superior performance of deep nonlinear orthogonal networks trained under full-batch gradient descent on the MNIST and CIFAR-10 classification tasks.
</details>
<details>
<summary>摘要</summary>
完全连接深度神经网络的权重初始化为独立的高斯分布可以调整到极点，从而防止信号在网络中 exponential 增长或减少。然而，这些网络仍然会出现 linear 增长的振荡，与网络宽度相比，这可能会降低网络的训练效果。我们 analytically 表明，使用 rectangle 网络和 tanh 活化函数，初始化 weights 为 orthogonal matrices 的ensemble，then the pre-activation fluctuations are independent of depth, to leading order in inverse width. In addition, we numerically show that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We speculate that this structure preserves finite-width feature learning while reducing overall noise, thus improving both generalization and training speed. We provide some experimental justification by relating empirical measurements of the NTK to the superior performance of deep nonlinear orthogonal networks trained under full-batch gradient descent on the MNIST and CIFAR-10 classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Representation-Learning-From-Random-Data-Projectors"><a href="#Self-supervised-Representation-Learning-From-Random-Data-Projectors" class="headerlink" title="Self-supervised Representation Learning From Random Data Projectors"></a>Self-supervised Representation Learning From Random Data Projectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07756">http://arxiv.org/abs/2310.07756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/layer6ai-labs/lfr">https://github.com/layer6ai-labs/lfr</a></li>
<li>paper_authors: Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs</li>
<li>for: 本研究旨在开发一种可以应用于多种数据模式和网络架构的自然语言处理和计算机视觉领域的自我监督学习方法。</li>
<li>methods: 本研究使用随机数据投影来学习高质量的数据表示。</li>
<li>results: 对多种表示学习任务进行了广泛的评估，并与多个状态 искусственный难度基线进行比较，得到了优于基线的结果。<details>
<summary>Abstract</summary>
Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue that learning from randomness is a fruitful research direction worthy of attention and further study.
</details>
<details>
<summary>摘要</summary>
自适应 represencing 学习（SSRL）在人工设计的数据增强下已经取得了很大的进步，通过利用数据增强下的变换不变性假设。而这些增强基于的 SSRL 算法在计算机视觉和自然语言处理领域的性能边缘很高，但它们通常不直接适用于其他数据类型，并且可能与应用特定的数据增强约束 conflicting。这篇文章提出了一种不依赖于增强或masking的 SSRL 方法，可以应用于任何数据类型和网络架构。我们表明，可以通过重建随机数据投影来学习高质量的数据表示。我们对多种表示学习任务进行了广泛的评估，这些任务覆盖了多种Modalities和实际应用。我们发现，该方法可以超过多个状态对的 SSRL 基elines。由于其广泛适用性和强大的实际结果，我们认为学习Randomness 是一个有前途的研究方向，值得关注和进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates"><a href="#Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates" class="headerlink" title="Stabilizing Estimates of Shapley Values with Control Variates"></a>Stabilizing Estimates of Shapley Values with Control Variates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07672">http://arxiv.org/abs/2310.07672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Goldwasser, Giles Hooker</li>
<li>for: 用于稳定黑盒机器学习模型的预测解释</li>
<li>methods: 使用控制SHAP方法，基于Monte Carlo技术的控制变量</li>
<li>results: 在高维数据集上可以 produz dramatic reductions in the Monte Carlo variability of Shapley estimates<details>
<summary>Abstract</summary>
Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates." into 简化字 Simplified Chinese.Here's the translation:<<SYS>>预测黑obox机器学习模型的解释工具中，诺比利值是最受欢迎的工具之一。然而，它们的计算成本高，导致使用抽样近似，从而引入了较大的不确定性。为稳定这些模型解释，我们提议ControlSHAP，基于Monte Carlo技术的控制准确量。我们的方法适用于任何机器学习模型，需要virtually no extra computation或模型定制努力。在一些高维数据集上，我们发现它可以生成很大的 reductions in the Monte Carlo variability of Shapley estimates。Note that "virtually no extra computation" is a bit tricky to translate, as it is a bit long and has a specific meaning in English. Here's one possible way to translate it into Simplified Chinese:<<SYS>>我们的方法需要几乎没有额外计算或模型定制努力，几乎没有额外成本。I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="The-First-Pathloss-Radio-Map-Prediction-Challenge"><a href="#The-First-Pathloss-Radio-Map-Prediction-Challenge" class="headerlink" title="The First Pathloss Radio Map Prediction Challenge"></a>The First Pathloss Radio Map Prediction Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07658">http://arxiv.org/abs/2310.07658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Çağkan Yapar, Fabian Jaensch, Ron Levie, Gitta Kutyniok, Giuseppe Caire</li>
<li>for: 本研究是为了解决路径损失预测问题，以便促进研究和对最近提出的路径损失广播地图预测方法进行公平的比较。</li>
<li>methods: 本研究使用了提供的数据集和挑战任务，并采用了挑战评价方法来评估参与者的预测方法。</li>
<li>results: 本研究通过对挑战任务的解决来展示了不同预测方法的性能，并提供了一系列的结果和分析。<details>
<summary>Abstract</summary>
To foster research and facilitate fair comparisons among recently proposed pathloss radio map prediction methods, we have launched the ICASSP 2023 First Pathloss Radio Map Prediction Challenge. In this short overview paper, we briefly describe the pathloss prediction problem, the provided datasets, the challenge task and the challenge evaluation methodology. Finally, we present the results of the challenge.
</details>
<details>
<summary>摘要</summary>
为了推动研究和促进最近提出的路径损失 ради图预测方法的公平比较，我们在ICASSP 2023年第一届路径损失 ради图预测挑战中发起了这项挑战。在这篇简短的概述 paper 中，我们简要介绍了路径损失预测问题，提供的数据集，挑战任务以及评价方法。最后，我们展示了挑战的结果。Here's the word-for-word translation:为了推动研究和促进最近提出的路径损失 ради图预测方法的公平比较，我们在ICASSP 2023年第一届路径损失 ради图预测挑战中发起了这项挑战。在这篇简短的概述 paper 中，我们简要介绍了路径损失预测问题，提供的数据集，挑战任务以及评价方法。最后，我们展示了挑战的结果。
</details></li>
</ul>
<hr>
<h2 id="Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals"><a href="#Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals" class="headerlink" title="Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals"></a>Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07648">http://arxiv.org/abs/2310.07648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Eleonora Chiarantano, Eleonora Grassucci, Danilo Comminiello</li>
<li>For: 该论文主要目标是提出一种基于嵌入式复杂网络的多模态情绪认知方法，以提高情绪识别精度。* Methods: 该方法使用了一种新的融合模块，其包括 Parametric Hypercomplex Multiplications，可以更好地模型情绪表达的多模态特征。* Results: 该方法在使用 MAHNOB-HCI 数据集进行 классифика任务中，超越了现有的多模态状态的网络。Here’s the English version of the summary for reference:* For: The main goal of the paper is to propose a multimodal emotion recognition method based on an embedded hypercomplex network, to improve the accuracy of emotion recognition.* Methods: The method uses a novel fusion module that includes Parametric Hypercomplex Multiplications to better model the multimodal features of emotional expressions.* Results: The method outperforms existing multimodal state-of-the-art networks on the MAHNOB-HCI dataset.<details>
<summary>Abstract</summary>
Multimodal emotion recognition from physiological signals is receiving an increasing amount of attention due to the impossibility to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on extracted handcrafted features, not taking full advantage of the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. Indeed, by operating in a hypercomplex domain the operations follow algebraic rules which allow to model latent relations among learned feature dimensions for a more effective fusion step. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, employing the publicly available database MAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of our work is freely available at https://github.com/ispamm/MHyEEG.
</details>
<details>
<summary>摘要</summary>
“多模式情感识别从生物学信号方面 receiving increasing attention, due to the inability to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on manually extracted features, not fully utilizing the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. By operating in a hypercomplex domain, the operations follow algebraic rules, allowing for more effective fusion of learned feature dimensions. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, using the publicly available database MAHNOB-HCI and surpassing a multimodal state-of-the-art network. The code of our work is freely available at <https://github.com/ispamm/MHyEEG>.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey"><a href="#Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey" class="headerlink" title="Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"></a>Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07745">http://arxiv.org/abs/2310.07745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Palmer, Chris Parry, Daniel J. B. Harrold, Chris Willis</li>
<li>for: 本研究旨在应对现代网络攻击的自动化防御方法。</li>
<li>methods: 本研究使用深度强化学习（DRL）方法来 Mitigate cyber attacks。</li>
<li>results: 本研究总结了DRL在ACO中的挑战，并提出了未来研究方向。<details>
<summary>Abstract</summary>
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties that define the ACO problem; ii.) A comprehensive evaluation of the extent to which domains used for benchmarking DRL approaches are comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling DRL to domains that confront learners with the curse of dimensionality, and; iv.) A survey and critique of current methods for limiting the exploitability of agents within adversarial settings from the perspective of ACO. We conclude with open research questions that we hope will motivate future directions for researchers and practitioners working on ACO.
</details>
<details>
<summary>摘要</summary>
随着最近几年的网络攻击数量快速增加，需要有原则的方法来防御网络免受恶意攻击者。深度强化学习（DRL）已经出现为防御攻击的有力方法。然而，虽然DRL在网络防御方面具有巨大的潜力，但是在大规模自动化网络操作（ACO）中应用DRL还是一个开放的挑战。ACO环境面临着非常高维状态空间、大量多 discrete 动作空间以及对学习的敌对学习。 recent works 报告了解决这些问题的成功，而且也有卓越的工程努力以实现这些问题的解决。然而，将DRL应用到整个ACO问题仍然是一个开放的挑战。在这篇文章中，我们对DRL相关文献进行了抽象，并提出了一个理想化的ACO-DRL代理。我们提供了：1. ACO问题的域属性的总结，包括ACO问题的特点和挑战。2. DRL在不同环境中的比较分析，以确定ACO问题是否与DRL相关的环境相似。3. 对高维度状态空间和多 discrete 动作空间的扩展，以及对敌对学习的限制。4. 对现有的ACO-DRL方法的评价和批判，以及未来研究的开放问题。我们结束于，ACO-DRL是一个有挑战性的领域，但是通过综合分析和理解ACO问题的特点，以及探索新的技术和方法，我们可以寻找更多的机会和潜力。
</details></li>
</ul>
<hr>
<h2 id="Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates"><a href="#Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates" class="headerlink" title="Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates"></a>Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07631">http://arxiv.org/abs/2310.07631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimeng Shi, Vitalii Stebliankin, Zhaonan Wang, Shaowen Wang, Giri Narasimhan</li>
<li>for: 预测洪水，帮助进行好的洪水管理</li>
<li>methods: 使用图Transformer网络（FloodGTN），学习水位的空间时间相关性，并使用LSTM和Graph Neural Networks（GNNs）来模拟洪水的行为</li>
<li>results: 在南佛瑞达水资源管理区的数据上测试，FloodGTN比Physics-based模型（HEC-RAS）具有更高的准确率，提高70%，并在运行时间上减少至少500倍<details>
<summary>Abstract</summary>
Floods can be very destructive causing heavy damage to life, property, and livelihoods. Global climate change and the consequent sea-level rise have increased the occurrence of extreme weather events, resulting in elevated and frequent flood risk. Therefore, accurate and timely flood forecasting in coastal river systems is critical to facilitate good flood management. However, the computational tools currently used are either slow or inaccurate. In this paper, we propose a Flood prediction tool using Graph Transformer Network (FloodGTN) for river systems. More specifically, FloodGTN learns the spatio-temporal dependencies of water levels at different monitoring stations using Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to consider external covariates such as rainfall, tide, and the settings of hydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the river. We use a Transformer to learn the attention given to external covariates in computing water levels. We apply the FloodGTN tool to data from the South Florida Water Management District, which manages a coastal area prone to frequent storms and hurricanes. Experimental results show that FloodGTN outperforms the physics-based model (HEC-RAS) by achieving higher accuracy with 70% improvement while speeding up run times by at least 500x.
</details>
<details>
<summary>摘要</summary>
洪水可以非常破坏生命、财产和生活方式。全球气候变化和海平面上升导致极端天气事件的增加，使得洪水风险增加。因此，精准和时间Constraints accurate flood forecasting in coastal river systems is critical to facilitate good flood management. However, the current computational tools are either slow or inaccurate. In this paper, we propose a flood prediction tool using Graph Transformer Network (FloodGTN) for river systems. Specifically, FloodGTN learns the spatio-temporal dependencies of water levels at different monitoring stations using Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to consider external covariates such as rainfall, tide, and the settings of hydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the river. We use a Transformer to learn the attention given to external covariates in computing water levels. We apply the FloodGTN tool to data from the South Florida Water Management District, which manages a coastal area prone to frequent storms and hurricanes. Experimental results show that FloodGTN outperforms the physics-based model (HEC-RAS) by achieving higher accuracy with 70% improvement while speeding up run times by at least 500x.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification"><a href="#Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification" class="headerlink" title="Differentiable Euler Characteristic Transforms for Shape Classification"></a>Differentiable Euler Characteristic Transforms for Shape Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07630">http://arxiv.org/abs/2310.07630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aidos-lab/dect">https://github.com/aidos-lab/dect</a></li>
<li>paper_authors: Ernst Roell, Bastian Rieck</li>
<li>for: 这个论文主要用于解决ECT不能学习任务特定表示问题，开发了一种新的计算层，使ECT可以在端到端方式学习。</li>
<li>methods: 该方法使用了ECT，并开发了一种新的计算层，使ECT可以在端到端方式学习。</li>
<li>results: 该方法在图像和点云分类任务中表现了优秀的性能，与更复杂的模型相当，同时还保持了ECT的简洁特点。<details>
<summary>Abstract</summary>
The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method DECT is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly unexpressive statistic still provides the same topological expressivity as more complex topological deep learning layers provide.
</details>
<details>
<summary>摘要</summary>
ECT（欧勒CharacteristicTransform）是一种强大的表示方式，可以结合图形和图形的 geometrical和topological特征。然而，ECT previously unable to learn任务特定的表示。我们解决了这个问题，并开发了一种新的计算层，使ECT可以在端到端的方式进行学习。我们的方法DECT具有快速和计算效率的优点，并在图形和点云分类任务中展现了与更复杂的模型相当的性能。此外，我们还证明ECT still provides the same topological expressivity as more complex topological deep learning layers provide.Note: "ECT" is short for "Euler Characteristic Transform", and "DECT" is short for "Deep ECT".
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations"><a href="#Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations" class="headerlink" title="Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations"></a>Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07626">http://arxiv.org/abs/2310.07626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria</li>
<li>for: 这个论文是为了研究使用海上高程测量卫星数据来估算海洋表面流动的方法。</li>
<li>methods: 论文使用了深度学习网络，并使用海面温度（SST）信息来优化插值方法。</li>
<li>results: 论文发现，无需训练数据，可以使用SST信息来改善插值性能，并且可以减少41%的平均平方根误差。<details>
<summary>Abstract</summary>
Satellite-based remote sensing missions have revolutionized our understanding of the Ocean state and dynamics. Among them, spaceborne altimetry provides valuable measurements of Sea Surface Height (SSH), which is used to estimate surface geostrophic currents. However, due to the sensor technology employed, important gaps occur in SSH observations. Complete SSH maps are produced by the altimetry community using linear Optimal Interpolations (OI) such as the widely-used Data Unification and Altimeter Combination System (DUACS). However, OI is known for producing overly smooth fields and thus misses some mesostructures and eddies. On the other hand, Sea Surface Temperature (SST) products have much higher data coverage and SST is physically linked to geostrophic currents through advection. We design a realistic twin experiment to emulate the satellite observations of SSH and SST to evaluate interpolation methods. We introduce a deep learning network able to use SST information, and a trainable in two settings: one where we have no access to ground truth during training and one where it is accessible. Our investigation involves a comparative analysis of the aforementioned network when trained using either supervised or unsupervised loss functions. We assess the quality of SSH reconstructions and further evaluate the network's performance in terms of eddy detection and physical properties. We find that it is possible, even in an unsupervised setting to use SST to improve reconstruction performance compared to SST-agnostic interpolations. We compare our reconstructions to DUACS's and report a decrease of 41\% in terms of root mean squared error.
</details>
<details>
<summary>摘要</summary>
卫星远感任务已经革命化了我们对海洋状态和动力学的理解。其中，空间探测技术提供了海面高程（SSH）的重要测量，用于估计表面地OSTROPIC currents。然而，由于探测技术的限制，SSH观测存在重要的缺陷。complete SSH map由altimetry社区使用线性优化方法（OI）生成，如广泛使用的数据统一和探测系统（DUACS）。然而，OI经常生成过于平滑的场景，因此缺乏一些中规模的结构和涝流。在这种情况下，海面温度（SST）产品具有更高的数据覆盖率，SST与地OSTROPIC currents physically linked through advection。我们设计了一个现实的双子实验，用于模拟卫星观测的 SSH 和 SST。我们引入了一个深度学习网络，可以使用 SST 信息，并在两种设定下训练：一个没有训练数据的情况，一个可以访问训练数据。我们的调查包括对这种网络在不同的训练设定下进行比较分析，以及评估网络的性能。我们发现，即使在无supervision的情况下，也可以使用 SST 来改进重建性能，并且我们的重建与DUACS的重建相比，Root Mean Squared Error（RMSE）下降41%。
</details></li>
</ul>
<hr>
<h2 id="Prospective-Side-Information-for-Latent-MDPs"><a href="#Prospective-Side-Information-for-Latent-MDPs" class="headerlink" title="Prospective Side Information for Latent MDPs"></a>Prospective Side Information for Latent MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07596">http://arxiv.org/abs/2310.07596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Yonathan Efroni, Shie Mannor, Constantine Caramanis</li>
<li>for: 这个论文是关于在协作决策环境中固有不可见信息的研究。具体来说，在对话系统中，用户的偏好等信息可能不被提供。在这种环境下， latent information 会在每个话语中保持不变。这种环境可以模型为 Latent Markov Decision Process (LMDP)，是 Partially Observed Markov Decision Processes (POMDPs) 的特例。</li>
<li>methods: 这篇论文使用了 LMDP 模型，并研究了在这种模型下的近似优秀策略可以如何高效地学习。具体来说， authors 使用了“prospective side information”，即在每个话语开始时得到了一些较弱描述 latent context 的信息。</li>
<li>results: 研究发现，这种问题并不是现有的 partially observed 环境和算法设计的一部分。 authors then  establishment 了任何高效学习算法都会受到至少 $\Omega(K^{2&#x2F;3})$ 的 regret，并设计了一个匹配的上界。<details>
<summary>Abstract</summary>
In many interactive decision-making settings, there is latent and unobserved information that remains fixed. Consider, for example, a dialogue system, where complete information about a user, such as the user's preferences, is not given. In such an environment, the latent information remains fixed throughout each episode, since the identity of the user does not change during an interaction. This type of environment can be modeled as a Latent Markov Decision Process (LMDP), a special instance of Partially Observed Markov Decision Processes (POMDPs). Previous work established exponential lower bounds in the number of latent contexts for the LMDP class. This puts forward a question: under which natural assumptions a near-optimal policy of an LMDP can be efficiently learned? In this work, we study the class of LMDPs with {\em prospective side information}, when an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode. We show that, surprisingly, this problem is not captured by contemporary settings and algorithms designed for partially observed environments. We then establish that any sample efficient algorithm must suffer at least $\Omega(K^{2/3})$-regret, as opposed to standard $\Omega(\sqrt{K})$ lower bounds, and design an algorithm with a matching upper bound.
</details>
<details>
<summary>摘要</summary>
在许多互动决策Setting中，存在潜在的和不可见的信息，这些信息在Each episode中保持不变。例如，在对话系统中，用户的偏好完全不给出。在这种环境中，潜在信息在每个话语中保持不变，因为用户的身份不会在交互中改变。这种环境可以被模型为潜在Markov决策过程（LMDP），这是Partially Observed Markov Decision Processes（POMDPs）的特殊情况。之前的工作已经证明了LMDP类型的下界为数字latent context的指数。这提出了一个问题：在哪些自然假设下，一个LMDP的优化策略可以高效地学习？在这项工作中，我们研究了在LMDP中提供了前景信息（prospective side information），用户在每个话语开始时接收到额外、薄弱揭示的潜在信息。我们发现，这个问题并不是现代的部分观察环境和算法设计的一部分。我们然后证明任何高效样本算法都会产生至少 $\Omega(K^{2/3})$ 的倒退，而不是标准的 $\Omega(\sqrt{K})$ 下界，并设计了一个匹配的上界。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics"><a href="#Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics" class="headerlink" title="Transformers for Green Semantic Communication: Less Energy, More Semantics"></a>Transformers for Green Semantic Communication: Less Energy, More Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07592">http://arxiv.org/abs/2310.07592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhabrata Mukherjee, Cory Beard, Sejun Song</li>
<li>for: 本研究旨在提高semantic communication的能效性，实现更低的延迟、带宽使用和更高的吞吐量。</li>
<li>methods: 本研究提出了一种新的多目标损失函数名为“能源优化semantic损失”（EOSL），以解决semantic communication中的 universal metrics问题。</li>
<li>results: 经过对transformer模型的测试，包括CPU和GPU的能源消耗，研究发现EOSL可以在推理阶段 saves up to 90%的能源，同时提高semantic similarity性能44%。<details>
<summary>Abstract</summary>
Semantic communication aims to transmit meaningful and effective information rather than focusing on individual symbols or bits, resulting in benefits like reduced latency, bandwidth usage, and higher throughput compared to traditional communication. However, semantic communication poses significant challenges due to the need for universal metrics for benchmarking the joint effects of semantic information loss and practical energy consumption. This research presents a novel multi-objective loss function named "Energy-Optimized Semantic Loss" (EOSL), addressing the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90\% of energy while achieving a 44\% improvement in semantic similarity performance during inference in this experiment. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details>
<details>
<summary>摘要</summary>
Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election"><a href="#Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election" class="headerlink" title="Analyzing Trendy Twitter Hashtags in the 2022 French Election"></a>Analyzing Trendy Twitter Hashtags in the 2022 French Election</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07576">http://arxiv.org/abs/2310.07576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aamir Mandviwalla, Lake Yin, Boleslaw K. Szymanski</li>
<li>for: 预测社交媒体用户未来活动</li>
<li>methods: 使用semantic网络特征来提高机器学习模型的准确性</li>
<li>results: 使用semantic网络特征可以实现高于0.5的$R^2$值，验证了这种特征的有用性。<details>
<summary>Abstract</summary>
Regressions trained to predict the future activity of social media users need rich features for accurate predictions. Many advanced models exist to generate such features; however, the time complexities of their computations are often prohibitive when they run on enormous data-sets. Some studies have shown that simple semantic network features can be rich enough to use for regressions without requiring complex computations. We propose a method for using semantic networks as user-level features for machine learning tasks. We conducted an experiment using a semantic network of 1037 Twitter hashtags from a corpus of 3.7 million tweets related to the 2022 French presidential election. A bipartite graph is formed where hashtags are nodes and weighted edges connect the hashtags reflecting the number of Twitter users that interacted with both hashtags. The graph is then transformed into a maximum-spanning tree with the most popular hashtag as its root node to construct a hierarchy amongst the hashtags. We then provide a vector feature for each user based on this tree. To validate the usefulness of our semantic feature we performed a regression experiment to predict the response rate of each user with six emotions like anger, enjoyment, or disgust. Our semantic feature performs well with the regression with most emotions having $R^2$ above 0.5. These results suggest that our semantic feature could be considered for use in further experiments predicting social media response on big data-sets.
</details>
<details>
<summary>摘要</summary>
“预测社交媒体用户未来活动需要丰富的特征。许多高级模型可以生成这些特征，但是它们在巨大数据集上进行计算时间复杂度可能是禁止的。一些研究表明，使用 semantics 网络特征可以够简单，对于机器学习任务来说，这些特征可以提供高度的准确性。我们提出一种使用 semantics 网络为用户级别特征的方法。我们使用了一个Twitter Hashtag 网络，包含370万则发送时间关于2022年法国总统选举的推文。将这些 Hashtag 转换为一个两分支 гра图，其中 Hashtag 为顶点，并将它们之间的关系为权重边。然后将这个 гра图转换为最大 span 树，并将最受欢迎的 Hashtag 作为根点。我们将这个树转换为一个向量特征，并将其用于预测每个用户对六种情感（如愤怒、喜悦、厌恶等）的回应率。我们的semantic特征表现良好，大多数情感的 $R^2$ 高于0.5。这些结果表明，我们的semantic特征可以考虑用于预测大规模的社交媒体回应。”
</details></li>
</ul>
<hr>
<h2 id="Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning"><a href="#Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning" class="headerlink" title="Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning"></a>Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07558">http://arxiv.org/abs/2310.07558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeqi Ye, Hansheng Jiang</li>
<li>for: 这个研究是针对非Parametric 需求函数的动态价格问题，并专注于适应未知的 H&quot;older 平滑程度 $\beta$。</li>
<li>methods: 我们使用了自相似性条件来启动适应，并提出了一个可靠的动态价格算法，并证明了这个算法可以在不知道 $\beta$ 的情况下实现最佳的 regret。</li>
<li>results: 我们证明了这个问题的最佳 regret 是 $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$，并且显示了这个 regret 下界不受自相似性条件的影响。<details>
<summary>Abstract</summary>
We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $\beta$.
</details>
<details>
<summary>摘要</summary>
我们研究动态价格问题，其中需求函数是非 Parametric 和Holder平滑的。我们专注于适应未知的Holder平滑性parameter $\beta$。传统上最佳动态价格算法严重依赖 $\beta$ 的知识，以 Achieve 最佳优化 regret $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$。但我们强调了这问题的适应性挑战，并证明无法适应地 дости��arz regret  minus  $\beta$ 的知识。这问题的适应性挑战的原因在于，需求函数的Holder平滑性parameter $\beta$ 是未知的，导致算法无法适应地调整价格。我们提出了一个自相似性条件，可以帮助解决这问题。我们证明了，这个自相似性条件不会增加问题的复杂度，并且保留了 regret 下界 $\Omega(T^{\frac{\beta+1}{2\beta+1})$。接下来，我们开发了一个适应性价格算法，并证明了这个算法可以 Achieve 最佳优化 regret bound $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$  minus  $\beta$ 的知识。
</details></li>
</ul>
<hr>
<h2 id="Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation"><a href="#Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation" class="headerlink" title="Provable Advantage of Parameterized Quantum Circuit in Function Approximation"></a>Provable Advantage of Parameterized Quantum Circuit in Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07528">http://arxiv.org/abs/2310.07528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhan Yu, Qiuhao Chen, Yuling Jiao, Yinan Li, Xiliang Lu, Xin Wang, Jerry Zhijian Yang</li>
<li>for: 本研究旨在探讨量子机器学习中参数化量子电路（PQC）的表达能力，以及PQC在完成机器学习任务中的可能优势。</li>
<li>methods: 本文使用函数拟合的视角分析PQC的表达能力，并提出了一种量化的近似误差 bounds，以衡量PQC需要多大以上来近似目标函数。作者们使用量子信号处理和线性组合方法构建PQC，并使用Bernstein polynomials和本地Taylor展开来实现全球和本地拟合。</li>
<li>results: 作者们的研究表明，PQC的表达能力与其宽度、深度和可调参数数量有直接的关系。具体来说，作者们在高维smooth函数的拟合中，使用PQC和深度神经网络进行比较，发现PQC的模型大小与深度神经网络的模型大小之间存在指数关系。这种指数关系表明，PQC在高维机器学习任务中可能具有显著的优势。<details>
<summary>Abstract</summary>
Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Taylor expansion and analyze their performances in the quantum setting. We also compare our proposed PQCs to nearly optimal deep neural networks in approximating high-dimensional smooth functions, showing that the ratio between model sizes of PQC and deep neural networks is exponentially small with respect to the input dimension. This suggests a potentially novel avenue for showcasing quantum advantages in quantum machine learning.
</details>
<details>
<summary>摘要</summary>
理解Parameterized quantum circuits（PQCs）在机器学习任务中的力量是机器学习领域的一个最重要的问题。在这篇论文中，我们通过函数近似来分析PQCs的表达能力。先前的 universality approximation theorems for PQCs 是非构建的，导致我们对以下问题：PQCs 如何在误差阈值为给定的情况下，将目标函数近似到Target function? We present explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions, and establish quantitative approximation error bounds in terms of the width, the depth, and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Taylor expansion, and analyze their performances in the quantum setting. We also compare our proposed PQCs to nearly optimal deep neural networks in approximating high-dimensional smooth functions, showing that the ratio between model sizes of PQC and deep neural networks is exponentially small with respect to the input dimension. This suggests a potentially novel avenue for showcasing quantum advantages in quantum machine learning.Note: Simplified Chinese is a written language that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and is the official language of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning"><a href="#Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning" class="headerlink" title="Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning"></a>Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07518">http://arxiv.org/abs/2310.07518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, Giorgia Ramponi</li>
<li>for: 提高强化学习的样本效率，使用 posterior sampling 技术。</li>
<li>methods: 使用 causal graph 来表示环境变量之间的 causal 关系，并使用 hierarchical Bayesian 方法 simultaneously 学习 full causal graph 和 resulting factored dynamics。</li>
<li>results: 数值研究表明，C-PSRL 可以强化 posterior sampling 的效率，并且与 posterior sampling 使用 full causal graph 的效果相似。<details>
<summary>Abstract</summary>
Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.
</details>
<details>
<summary>摘要</summary>
后采样allowstheexploitationof prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a（partial）causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.Here's the breakdown of the translation:* 后采样 (hòu shēng sāng) - posterior sampling* allowstheexploitation (děi yǎn jì) - allows the exploitation* of prior knowledge (xīn jiàn yì) - of prior knowledge* to improve (gèng yǐn) - to improve* the sample efficiency (shuāng yǐn jì) - the sample efficiency* of reinforcement learning (qì yǎn jì) - of reinforcement learning* The prior (xīn) - the prior* is typically specified (dēi yǐn) - is typically specified* as a class (bǎn) - as a class* of parametric distributions (fāng xiào yǐn) - of parametric distributions* a task (gōng yì) - a task* that can be cumbersome (kě shì) - that can be cumbersome* in practice (shì yī jī) - in practice* often resulting (yǐn yì) - often resulting* in the choice (děi yǎn) - in the choice* of uninformative priors (qǐ yǐn xīn) - of uninformative priors* In this work (zhèng gōng) - In this work* we propose (wǒ zhèng) - we propose* a novel (xīn) - a novel* posterior sampling approach (hòu shēng sāng yì jì) - a posterior sampling approach* in which (dēi yǐn) - in which* the prior (xīn) - the prior* is given (gěi) - is given* as a (partial) causal graph (xīn fāng yǐn) - as a (partial) causal graph* over the environment's variables (yuè yì zhī) - over the environment's variables* The latter (dài) - the latter* is often more natural (fēng zhī) - is often more natural* to design (suǒ yì) - to design* such as (xīn yǐn) - such as* listing known causal dependencies (jiè yǐn kě yì) - listing known causal dependencies* between biometric features (yì xīng yǐn) - between biometric features* in a medical treatment study (yī jīng zhī yì) - in a medical treatment study* Specifically (xīn yǐn) - specifically* we propose (wǒ zhèng) - we propose* a hierarchical Bayesian procedure (hierarchical Bayesian procedure) - a hierarchical Bayesian procedure* called C-PSRL (called C-PSRL) - called C-PSRL* simultaneously learning (xīn xué yì) - simultaneously learning* the full causal graph (quán xīn fāng) - the full causal graph* at the higher level (shàng kāi) - at the higher level* and the parameters (fāng yǐn) - and the parameters* of the resulting factored dynamics (yǐn yuè yǐn) - of the resulting factored dynamics* For this procedure (zhèng gōng) - For this procedure* we provide (wǒ jiāng) - we provide* an analysis (xīn yì) - an analysis* of its Bayesian regret (Bayesian regret) - of its Bayesian regret* which explicitly connects (dēi yǐn) - which explicitly connects* the regret rate (hǎo yǐn) - the regret rate* with the degree of prior knowledge (xīn jì zhī) - with the degree of prior knowledge* Our numerical evaluation (shuāng yì yì) - Our numerical evaluation* conducted in illustrative domains (dài yì) - conducted in illustrative domains* confirms (jiān yì) - confirms* that C-PSRL (C-PSRL) - that C-PSRL* strongly improves (qiǎo yǐn) - strongly improves* the efficiency (yǐn jì) - the efficiency* of posterior sampling (hòu shēng sāng) - of posterior sampling* with an uninformative prior (qǐ yǐn xīn) - with an uninformative prior* while performing (yǐn yì) - while performing* close to (jì qǐ) - close to* posterior sampling with the full causal graph (hòu shēng sāng quán xīn fāng) - posterior sampling with the full causal graph
</details></li>
</ul>
<hr>
<h2 id="Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance"><a href="#Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance" class="headerlink" title="Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance"></a>Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07491">http://arxiv.org/abs/2310.07491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs</li>
<li>For: The paper aims to improve the predictive performance of personalized models for emotional behavior by utilizing group-based information through clustering.* Methods: The paper investigates two model-based clustering approaches using personalized models and optimizes the clustering based on performance.* Results: Clustering based on performance shows the best results in terms of all examined evaluation measures, and the group-models outperform three baseline scenarios.<details>
<summary>Abstract</summary>
Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describe all individuals belonging to one cluster. Among these, clustering based on performance shows the best results, in terms of all examined evaluation measures. As another level of evaluation, those group-models' performance is compared to three baseline scenarios, the personalized, the all-in-one group and the random group-based concept. According to this comparison, the superiority of clustering-based methods is again confirmed, indicating that the utilization of group-based information could be effectively enhance the overall performance of all individuals' data.
</details>
<details>
<summary>摘要</summary>
Two model-based clustering approaches are examined: the first uses model-extracted parameters of personalized models, while the second is optimized for model-based forecasting performance. Both methods are evaluated using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) and the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describing all individuals belonging to one cluster.Clustering based on performance shows the best results, as evaluated by all examined measures. To further evaluate the effectiveness of clustering-based methods, the performance of the group-models is compared to three baseline scenarios: personalized, all-in-one group, and random group-based concepts. The results confirm the superiority of clustering-based methods, indicating that utilizing group-based information can effectively enhance the overall performance of all individuals' data.
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes"><a href="#Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes" class="headerlink" title="Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes"></a>Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07485">http://arxiv.org/abs/2310.07485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Schwerdtner, Philipp Schulze, Jules Berman, Benjamin Peherstorfer</li>
<li>for: 该研究探讨了使用深度网络非线性参数化方法保存干扰量、质量和动量的问题。</li>
<li>methods: 该方法基于Neural Galerkin方法，使用Dirac–Frenkel变分原理来在时间上顺序训练非线性参数化。</li>
<li>results: 数值实验表明，该方法可以准确保存干扰量、质量和动量。<details>
<summary>Abstract</summary>
This work focuses on the conservation of quantities such as Hamiltonians, mass, and momentum when solution fields of partial differential equations are approximated with nonlinear parametrizations such as deep networks. The proposed approach builds on Neural Galerkin schemes that are based on the Dirac--Frenkel variational principle to train nonlinear parametrizations sequentially in time. We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes. Numerical experiments demonstrate that the proposed approach conserves quantities up to machine precision.
</details>
<details>
<summary>摘要</summary>
We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes.Numerical experiments demonstrate that the proposed approach conserves quantities up to machine precision.simplified Chinese translation:这个工作关注在使用深度网络作为参数化方程解的解场量保守性。提议的方法基于神经加尔基 schemes，这些方法基于Dirac--Frenkel变量原理来顺序地在时间上训练非线性参数化。我们首先表明，只是在继续时间添加保守量的约束可能是不充分的，因为非线性参数的依赖性使得解场量中的量变为非线性参数，这使得在时间绘制中很难处理。而我们提议的神经加尔基方法在每个时间步骤上计算非线性参数化解场量的Explicit embedding，以保证量的保守性。这些映射可以与标准的Explicit和隐式时间步骤综合使用。 numerics experiments表明，提议的方法可以保持量到机器精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma"><a href="#Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma" class="headerlink" title="Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma"></a>Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07464">http://arxiv.org/abs/2310.07464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Fang, Yihan Liu, Yifeng Wang, Xiangyang Zhang, Yang Chen, Changjing Cai, Yiyang Lin, Ying Han, Zhi Wang, Shan Zeng, Hong Shen, Jun Tan, Yongbing Zhang<br>for:多种低等级肿瘤（LGG）的诊断和治疗中，生物标记的检测是不可或缺的。然而，现有的LGG生物标记检测方法往往需要使用高成本和复杂的分子遗传测试，需要专业人员分析结果，并且经常报告了内部变化。methods:我们提出了一种可解释的深度学习管道，基于多例学习（MIL）框架，用于预测LGG五个生物标记的状态，只需使用染色和染色涂抹的整个扫描图像和扫描标签。Specifically，通过将一类分类 integrating 到 MIL 框架中，实现了准确的实例 Pseudo-labeling，这种 complement 扫描标签 greatly improves 生物标记预测性能。results:Multi-Beholder 在两个群组（n&#x3D;607）中表现出色，其预测性能和一致性（AUROC&#x3D;0.6469-0.9735）在多种不同的种族和扫描协议下都具有出色的一致性。此外，Multi-Beholder 的优秀可解释性允许发现潜在的量化和质量相关性 между生物标记状态和生物学特征。我们的管道不仅提供了一种新的生物标记预测方法，提高了LGG患者的分子治疗应用性，还可以探索新的分子功能和LGG发展机制。<details>
<summary>Abstract</summary>
Biomarker detection is an indispensable part in the diagnosis and treatment of low-grade glioma (LGG). However, current LGG biomarker detection methods rely on expensive and complex molecular genetic testing, for which professionals are required to analyze the results, and intra-rater variability is often reported. To overcome these challenges, we propose an interpretable deep learning pipeline, a Multi-Biomarker Histomorphology Discoverer (Multi-Beholder) model based on the multiple instance learning (MIL) framework, to predict the status of five biomarkers in LGG using only hematoxylin and eosin-stained whole slide images and slide-level biomarker status labels. Specifically, by incorporating the one-class classification into the MIL framework, accurate instance pseudo-labeling is realized for instance-level supervision, which greatly complements the slide-level labels and improves the biomarker prediction performance. Multi-Beholder demonstrates superior prediction performance and generalizability for five LGG biomarkers (AUROC=0.6469-0.9735) in two cohorts (n=607) with diverse races and scanning protocols. Moreover, the excellent interpretability of Multi-Beholder allows for discovering the quantitative and qualitative correlations between biomarker status and histomorphology characteristics. Our pipeline not only provides a novel approach for biomarker prediction, enhancing the applicability of molecular treatments for LGG patients but also facilitates the discovery of new mechanisms in molecular functionality and LGG progression.
</details>
<details>
<summary>摘要</summary>
低度 Glioma (LGG) 诊断和治疗中不可或缺的一部分是生物标志物的检测。然而，现有的LGG生物标志物检测方法仍然 rely on 昂贵和复杂的分子遗传学测试，需要专业人员分析结果，并且间谍者变化 frequently 被报告。为了解决这些挑战，我们提议一种可解释深度学习管道，即多个生物标志物形态发现器（Multi-Beholder）模型，基于多个实例学习（MIL）框架，以predict LGG 中五个生物标志物的状态，只需使用染色涂抹整幅图像和批量标签。specifically，通过将一类分类 incorporated 到 MIL 框架中，实现了准确的实例 pseudo-标签，这对实例级指导提供了很好的补做，从而提高了生物标志物预测性能。Multi-Beholder 在两个 cohort（n=607）中显示出优秀的预测性能和普遍性，其中包括不同的种族和扫描协议。此外，Multi-Beholder 的优秀可解释性使得可以发现生物标志物状态和形态特征之间的量化和质量相关性。我们的管道不仅提供了一种新的生物标志物预测方法，扩展了 LGG 患者可应用分子治疗的可能性，还可以促进分子功能和 LGG 进程中新的机制发现。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI"><a href="#Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI" class="headerlink" title="Uncovering ECG Changes during Healthy Aging using Explainable AI"></a>Uncovering ECG Changes during Healthy Aging using Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07463">http://arxiv.org/abs/2310.07463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4healthuol/ecg-aging">https://github.com/ai4healthuol/ecg-aging</a></li>
<li>paper_authors: Gabriel Ott, Yannik Schaubelt, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff</li>
<li>for: 这个论文的目的是为了更好地理解心脏年龄过程，以便诊断心血管健康问题。</li>
<li>methods: 这个论文使用了深度学习模型和树型分类器来分析健康人群的ECG数据，并使用可解释AI技术来确定ECG特征或Raw信号特征是最有力量地分类不同年龄组的。</li>
<li>results: 研究发现，随着年龄增长，推断呼吸速率下降，而SDANN值明显高的 elderly 个体呈现出 distinguish 特征，与年轻成年人相比。此外，深度学习模型表明，随着年龄增长，P-波的分布发生了变化，这可能对年龄预测产生了影响。这些发现超越了传统基于特征的方法，提供了新的年龄相关ECG变化的视角。<details>
<summary>Abstract</summary>
Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adults. Furthermore, the deep-learning model underscores the pivotal role of the P-wave in age predictions across all age groups, suggesting potential changes in the distribution of different P-wave types with age. These findings shed new light on age-related ECG changes, offering insights that transcend traditional feature-based approaches.
</details>
<details>
<summary>摘要</summary>
心血管疾病仍然是全球致死率最高的主要原因。这意味着我们需要深入了解心脏年龄过程，以诊断心血管健康水平的约束。传统上，大多数这些发现都是通过心电图特征变化来获得的，但这些特征可能会隐藏数据之间的关系。在这篇论文中，我们使用深度学习模型和树 структуры模型来分析健康个体心电图数据，并使用可解释AI技术来确定心电图特征或原始信号特征是否能够区分不同年龄组。我们的分析表明，年龄相关的呼吸率下降和高SDANN值是识别老年人的特征，与年轻人相比分化。此外，深度学习模型表明，随着年龄增长，P波的分布发生了变化，这对年龄预测具有重要作用。这些发现为心血管年龄变化提供了新的见解，超出了传统的特征基于方法。
</details></li>
</ul>
<hr>
<h2 id="ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting"><a href="#ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting" class="headerlink" title="ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting"></a>ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07446">http://arxiv.org/abs/2310.07446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Zhang, Xumeng Wen, Shun Zheng, Jia Li, Jiang Bian</li>
<li>for: 这篇论文旨在探讨时间序列预测的两个分支：一个是专门为时间序列设计特有的神经网络架构，另一个是利用深度生成模型进行 probabilistic 预测。</li>
<li>methods: 这篇论文使用了一个名为 ProbTS 的工具集，它可以融合和比较这两个分支的不同方法。</li>
<li>results: 通过使用 ProbTS，论文发现了这两个分支的不同特点、优劣点和需要进一步探索的领域。<details>
<summary>Abstract</summary>
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further exploration. Our analyses point to new avenues for research, aiming for more effective time-series forecasting.
</details>
<details>
<summary>摘要</summary>
时间序列预测作为许多应用领域的关键环节，其中有两个主要分支：一个是为时间序列设计特定的神经网络架构，另一个是利用高级深度生成模型进行 probabilistic 预测。尽管这两个分支都取得了重要进展，但它们在数据场景、方法重点和解码方案等方面存在差异，这些差异尚未得到深入研究。为了bridging这一知识差距，我们提出了ProbTS工具集，这是一个能够结合和比较这两个分支的先锋工具。ProbTS具有一个统一的数据模块、一个模块化的模型模块和一个全面的评价模块，这使得我们可以对领先的方法进行重新评估和比较。我们的分析表明，这两个分支具有不同的特点、优势和缺点，以及需要进一步探索的领域。我们的研究开发了新的方向，以更好地预测时间序列。
</details></li>
</ul>
<hr>
<h2 id="A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales"><a href="#A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales" class="headerlink" title="A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales"></a>A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07437">http://arxiv.org/abs/2310.07437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien Wang</li>
<li>for: 预测普罗旺斯的低视力事件或雾霾的发生</li>
<li>methods: 使用多元年度地域天气和水文变量作为输入特征，并使用视野观测数据作为目标进行训练</li>
<li>results: 通过两棵分支架构，提高网络的性能，在验证和盲预测评估中获得了2021和2022年数据不使用训练和验证数据的合理分数<details>
<summary>Abstract</summary>
A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
</details>
<details>
<summary>摘要</summary>
“一个深度学习平台已经开发来预测低可见度事件或雾化的发生。它是使用多decadal日间地域气象和水文变数作为输入特征，并使用地面可见度观测作为目标进行训练。为了更好地保留不同输入特征的特征空间信息，最近两个分支架构已经为 París 雾化情况开发出来。这两个新架构已经提高了网络的性能，在验证和隐藏预测评估中获得了合理的分数，使用2021和2022年的数据进行预测。”Note: "Paris hazes" in the text refers to haze events that occur in Paris, France.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data"><a href="#Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data" class="headerlink" title="Generalized Mixture Model for Extreme Events Forecasting in Time Series Data"></a>Generalized Mixture Model for Extreme Events Forecasting in Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07435">http://arxiv.org/abs/2310.07435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Wang, Yue Gao</li>
<li>for: 这个研究旨在提高时间序列预测中的极值识别和预测性能。</li>
<li>methods: 本研究提出了一个 novel 的 Deep Extreme Mixture Model with Autoencoder (DEMMA) 架构，包括两个主要模组：1）一个通用混合分布基于 Hurdle 模型和一个对应数据的 GP 分布，2）一个 Autoencoder-based LSTM 特征提取器和一个时间注意力机制。</li>
<li>results: 在多个实际的雨水时间序列数据上，本研究示出了 DEMMA 模型的优化性能。<details>
<summary>Abstract</summary>
Time Series Forecasting (TSF) is a widely researched topic with broad applications in weather forecasting, traffic control, and stock price prediction. Extreme values in time series often significantly impact human and natural systems, but predicting them is challenging due to their rare occurrence. Statistical methods based on Extreme Value Theory (EVT) provide a systematic approach to modeling the distribution of extremes, particularly the Generalized Pareto (GP) distribution for modeling the distribution of exceedances beyond a threshold. To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model comprises two main modules: 1) a generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution form independent of the extreme threshold, 2) an Autoencoder-based LSTM feature extractor and a quantile prediction module with a temporal attention mechanism. We demonstrate the effectiveness of our approach on multiple real-world rainfall datasets.
</details>
<details>
<summary>摘要</summary>
To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model consists of two main modules:1. A generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution that is independent of the extreme threshold.2. An Autoencoder-based Long Short-Term Memory (LSTM) feature extractor and a quantile prediction module with a temporal attention mechanism.We demonstrate the effectiveness of our approach on multiple real-world rainfall datasets.
</details></li>
</ul>
<hr>
<h2 id="Non-backtracking-Graph-Neural-Networks"><a href="#Non-backtracking-Graph-Neural-Networks" class="headerlink" title="Non-backtracking Graph Neural Networks"></a>Non-backtracking Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07430">http://arxiv.org/abs/2310.07430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, Sungsoo Ahn</li>
<li>for: 本文旨在提出一种解决图神经网络（GNN）的循环引用问题的方法，以提高GNN的表示能力和计算效率。</li>
<li>methods: 本文提出了一种非循环引用图神经网络（NBA-GNN），通过不包含先前访问的节点的消息来解决循环引用问题。</li>
<li>results: 实验表明，NBA-GNN可以有效地解决循环引用问题，并且在长距离图 bench 和推理节点分类问题上显示出了出色的表现。<details>
<summary>Abstract</summary>
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-GNN on long-range graph benchmark and transductive node classification problems.
</details>
<details>
<summary>摘要</summary>
“ celebrity message-passing updates for graph neural networks 让大规模图可以通过本地和计算可行的更新表示。然而，本地更新受到回tracking的影响，即消息流经同一个边两次并返回已经访问过的节点。由于消息流量随更新数量呈指数增长，本地更新中的重复导致图神经网络无法准确地识别特定消息流，这对下游任务造成了影响。在这种情况下，我们提议使用非回tracking图神经网络（NBA-GNN），该网络在更新消息时不会包含已经访问过的节点的消息。我们进一步研究了NBA-GNN如何解决GNNS中的过度压缩问题，并证明了NBA-GNN和非回tracking更新的非常有用性。我们通过对长距离图benchmark和推uctive节点分类问题进行实验来证明NBA-GNN的有效性。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions"><a href="#Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions" class="headerlink" title="Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions"></a>Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07427">http://arxiv.org/abs/2310.07427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengmeng Xu, Hai Lin</li>
<li>for: 这种研究旨在提高时间序列预测精度，使用量子计算技术和深度学习方法。</li>
<li>methods: 该方法使用量子圈抽象场(QGAF)，它将时间序列数据转换为适合 convolutional neural network(CNN) 训练的二维图像。与传统的gramian angular field(GAF)方法不同，QGAF不需要数据Normalization和 inverse cosine 计算，从而简化了数据转换过程。</li>
<li>results: 在中国A股市场、香港股市场和美国股市场的数据集上进行了实验，发现QGAF方法比传统GAF方法更高精度，预测误差下降了25%的mean absolute error(MAE)和48%的mean squared error(MSE)。这项研究证明了将量子计算技术与深度学习方法结合使用可以提高金融时间序列预测的精度。<details>
<summary>Abstract</summary>
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improved time series prediction accuracy, reducing prediction errors by an average of 25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This research confirms the potential and promising prospects of integrating quantum computing with deep learning techniques in financial time series forecasting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种时间序列预测方法，名为量子agramian angular field（QGAF）。这种方法结合了量子计算技术和深度学习，目的是提高时间序列分类和预测的精度。我们成功地将股票回报时间序列数据转化为适合深度神经网络训练的两维图像，通过设计专门的量子电路。与 классиical gramian angular field（GAF）方法不同的是，QGAF方法不需要数据Normalization和 inverse cosine 计算，从时间序列数据到二维图像的转换过程就变得更加简单。为验证这种方法的效果，我们在三个主要股票市场的数据集上进行了实验：中国A股市场、香港股市和美国股市。实验结果表明，相比于 классиical GAF 方法，QGAF 方法在时间序列预测精度上有显著提高，减少预测错误的平均绝对值（MAE）和平均平方误差（MSE）的值，减少了25%和48%。这项研究证明了将量子计算技术与深度学习技术结合在金融时间序列预测中的潜在优势和前景。
</details></li>
</ul>
<hr>
<h2 id="Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization"><a href="#Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization" class="headerlink" title="Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization"></a>Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07392">http://arxiv.org/abs/2310.07392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Raina, SH Chandrashekhara, Richard Voyles, Juan Wachs, Subir Kumar Saha</li>
<li>for: 提高Robotic Ultrasound（A-RUS）的图像质量和效率，减少医生的工作负担。</li>
<li>methods: 使用深度神经网络学习低维度的核函数，并使用两种图像质量估计器来提供实时反馈。</li>
<li>results: 实现了超过50%的样本效率提升，并且这种性能提升不依赖具体的训练数据集，表示了在不同病人身上的可靠性。<details>
<summary>Abstract</summary>
Ultrasound is a commonly used medical imaging modality that requires expert sonographers to manually maneuver the ultrasound probe based on the acquired image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to this manual procedure in order to reduce sonographers' workload. The key challenge to A-RUS is optimizing the ultrasound image quality for the region of interest across different patients. This requires knowledge of anatomy, recognition of error sources and precise probe position, orientation and pressure. Sample efficiency is important while optimizing these parameters associated with the robotized probe controller. Bayesian Optimization (BO), a sample-efficient optimization framework, has recently been applied to optimize the 2D motion of the probe. Nevertheless, further improvements are needed to improve the sample efficiency for high-dimensional control of the probe. We aim to overcome this problem by using a neural network to learn a low-dimensional kernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained using probe and image data acquired during the procedure. The two image quality estimators are proposed that use a deep convolution neural network and provide real-time feedback to the BO. We validated our framework using these two feedback functions on three urinary bladder phantoms. We obtained over 50% increase in sample efficiency for 6D control of the robotized probe. Furthermore, our results indicate that this performance enhancement in BO is independent of the specific training dataset, demonstrating inter-patient adaptability.
</details>
<details>
<summary>摘要</summary>
超声成为医疗图像获取的常用方法，需要专业的sonoographer手动操作超声探测器根据获取的图像。自动化超声探测器（A-RUS）是为了减轻sonoographer的工作负担，但是需要在不同患者中优化超声图像质量的区域。这需要了解解剖学、识别错误来源和精确的探测器位置、orientation和压力。样本效率是重要的而且需要优化这些相关的探测器控制参数。bayesian优化（BO），一种样本效率的优化框架，已经应用于优化2D探测器的运动。然而，需要进一步提高样本效率，以便在高维度控制中提高效率。我们想使用神经网络学习一个低维度的核心，称为深度核心（DK）。神经网络的DK在BO中训练，使用在过程中获取的探测器和图像数据。我们提出了两种图像质量估计器，使用深度卷积神经网络，并为BO提供实时反馈。我们验证了我们的框架，使用这两种反馈函数在三个尿液膜phantom上进行验证。我们获得了6D控制的机器人式探测器的50%以上的样本效率提高。此外，我们的结果表明，这种性能提高在BO中是不同训练数据集的独立的，表明了患者间适应性。
</details></li>
</ul>
<hr>
<h2 id="Experimental-quantum-natural-gradient-optimization-in-photonics"><a href="#Experimental-quantum-natural-gradient-optimization-in-photonics" class="headerlink" title="Experimental quantum natural gradient optimization in photonics"></a>Experimental quantum natural gradient optimization in photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07371">http://arxiv.org/abs/2310.07371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhi Wang, Shichuan Xue, Yaxuan Wang, Jiangfang Ding, Weixu Shi, Dongyang Wang, Yong Liu, Yingwen Liu, Xiang Fu, Guangyao Huang, Anqi Huang, Mingtang Deng, Junjie Wu</li>
<li>for: 实现实用near-term量子应用</li>
<li>methods: 使用量子自然导数（QNG）优化</li>
<li>results: 实验ally obtained the dissociation curve of He-H$^+$ cation with chemical accuracy, demonstrating the outperformance of QNG optimization on a photonic device.<details>
<summary>Abstract</summary>
Variational quantum algorithms (VQAs) combining the advantages of parameterized quantum circuits and classical optimizers, promise practical quantum applications in the Noisy Intermediate-Scale Quantum era. The performance of VQAs heavily depends on the optimization method. Compared with gradient-free and ordinary gradient descent methods, the quantum natural gradient (QNG), which mirrors the geometric structure of the parameter space, can achieve faster convergence and avoid local minima more easily, thereby reducing the cost of circuit executions. We utilized a fully programmable photonic chip to experimentally estimate the QNG in photonics for the first time. We obtained the dissociation curve of the He-H$^+$ cation and achieved chemical accuracy, verifying the outperformance of QNG optimization on a photonic device. Our work opens up a vista of utilizing QNG in photonics to implement practical near-term quantum applications.
</details>
<details>
<summary>摘要</summary>
“量子变量算法（VQA），结合参数化量子电路和类别优化器的优点，承诺实现量子应用程序在噪响中等量子时代。VQA的性能强度取决于优化方法。相比于梯度计算和普通梯度下降方法，量子自然梯度（QNG），它反映参数空间的几何结构，可以更快地趋向于稳定点，更容易避免地陷入地方 minimum，因此可以降低电路执行成本。我们通过全功能可编程光学芯片实验ally estimatin QNG在光学中，并实现了He-H$^+$阴离子的分离曲线，达到化学精度，证明了QNG优化在光学设备上的出色表现。我们的工作开启了使用QNG在光学中实现实用近期量子应用程序的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities"><a href="#Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities" class="headerlink" title="Orthogonal Random Features: Explicit Forms and Sharp Inequalities"></a>Orthogonal Random Features: Explicit Forms and Sharp Inequalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07370">http://arxiv.org/abs/2310.07370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nizar Demni, Hachem Kadri</li>
<li>for: 这个论文旨在探讨随机特征的扩展，以便使用随机化技术来提高内积方法。</li>
<li>methods: 论文使用了随机 Fourier 特征和正交随机特征来近似 Gaussian kernel。</li>
<li>results: 研究人员分析了随机特征近似的偏差和方差，并提供了正常 BELL 函数的准确表达和锐度上的约束。<details>
<summary>Abstract</summary>
Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
</details>
<details>
<summary>摘要</summary>
随机特性被引入扩大内积方法 via 随机技术。特别是随机傅里父特性和正交随机特性被用来近似各种各样的加aussian颗点。前者通过随机γ矩阵实现，并在均值后变为加aussian颗点。在这项工作中，我们分析内积方法的偏差和方差基于正交随机特性，使用正则化的贝塞尔函数获得明确的表达，并 deriv出锐尖指数 bound，支持我们的观点，即正交随机特性比Random Fourier Features更有用。
</details></li>
</ul>
<hr>
<h2 id="Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model"><a href="#Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model" class="headerlink" title="Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model"></a>Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07367">http://arxiv.org/abs/2310.07367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Zhu, Meng Ding, Vaneet Aggarwal, Jinhui Xu, Di Wang</li>
<li>for: 这个论文是为了解决在本地权限保护（LDP）模型下的稀疏线性回归问题。</li>
<li>methods: 这篇论文使用了非交互式LDP模型和分段交互式LDP模型，并提出了一种新的非交互式LDP算法。</li>
<li>results: 这篇论文得出了一个lower bound的下界，证明了对于具有$k$-稀疏参数的情况，非交互式LDP算法的性能是有限制的。同时，提出了一种高效的非交互式LDP算法，并且该算法还生成了一个高效的估计器。<details>
<summary>Abstract</summary>
In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an upper bound of $\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$ for the estimation error when the data is sub-Gaussian, which can be further improved by a factor of $O(\sqrt{d})$ if the server has additional public but unlabeled data. For the sequentially interactive LDP model, we show a similar lower bound of $\Omega({\frac{\sqrt{dk}{\sqrt{n}\epsilon})$. As for the upper bound, we rectify a previous method and show that it is possible to achieve a bound of $\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$. Our findings reveal fundamental differences between the non-private case, central DP model, and local DP model in the sparse linear regression problem.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新考虑了在本地隐私（LDP）模型下的稀疏线性回归问题。现有研究在非交互和顺序本地模型下都集中在了对于$1$-稀疏情况下的下界问题，并将其扩展到更一般的$k$-稀疏情况却是困难的。另外，是否存在高效的非交互LDP（NLDP）算法仍然是一个问题。为了解决这些问题，我们首先考虑了在$\epsilon$非交互LDP模型下的问题，并提供了$\ell_2$-范数估计误差的下界为$\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$，其中$n$是样本大小，$d$是空间维度。我们提出了一种创新的NLDP算法，这是这个问题的第一个解决方案。结果显示，这个算法还提供了一个高效的估计器作为副产品。我们的算法在SUB-Gaussian数据下的估计误差为$\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$，可以通过增加$O(\sqrt{d})$的因子进一步改进。如果服务器拥有额外的公共 но不带标签的数据，那么我们的算法可以在这些数据下进一步改进估计误差。在顺序交互LDP模型下，我们显示了一个相似的下界为$\Omega(\frac{\sqrt{dk}{\sqrt{n}\epsilon})$。在上界方面，我们修复了之前的方法，并显示了可以实现$\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$的上界。我们的发现表明了私人情况、中央DP模型和本地DP模型在稀疏线性回归问题中存在根本的不同。
</details></li>
</ul>
<hr>
<h2 id="GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning"><a href="#GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning" class="headerlink" title="GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning"></a>GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07365">http://arxiv.org/abs/2310.07365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 本研究旨在Addressing the “transferability-specificity dilemma” in graph domain transfer learning, 即如何在不同的图数据上训练预训练模型以获得更好的目标数据表现。</li>
<li>methods: 本研究提出了一种基于ControlNet的图控制模块（GraphControl），用于实现更好的图域传输学习。该模块通过使用通用结构预训练模型和ControlNet进行细化调整，使得输入空间的对应性提高，并 incorporate 目标数据的特有特征作为 conditional inputs。</li>
<li>results: 对于预训练模型的应用在目标 attributed 数据上，该方法可以获得1.4-3x的性能提升，同时与从scratch 训练方法相比，具有更好的表现和更快的收敛速率。<details>
<summary>Abstract</summary>
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.
</details>
<details>
<summary>摘要</summary>
GRaph-structured data 是全球各地模型复杂对象之间的关系，使得众多网络应用程序得以实现。日常网络上的未标注GRaph数据具有巨大的潜在可能性。GRaph自我supervised算法在丰富的未标注GRaph数据上取得了显著的成功，从而获得了一般知识。这些预训练模型可以应用于各种下游网络应用程序，从而节省训练时间并提高下游性能。然而，不同的GRaph，即使在看似相似的领域中，可能具有显著不同的属性 semantics，这会带来在传输预训练模型到下游任务的困难，甚至不可能。例如，下游任务中特定的任务特有节点信息通常会被故意忽略，以便利用预训练表示。这种困难被称为“传输性-特定性之争”在这种工作中。为Addressing this challenge, we introduce an innovative deployment module called GraphControl, inspired by ControlNet, to achieve better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.
</details></li>
</ul>
<hr>
<h2 id="Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction"><a href="#Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Atom-Motif Contrastive Transformer for Molecular Property Prediction"></a>Atom-Motif Contrastive Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07351">http://arxiv.org/abs/2310.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yu, Shuo Chen, Chen Gong, Gang Niu, Masashi Sugiyama</li>
<li>for: 这个论文的目的是提出一种基于Graph Transformer（GT）模型的新型分子属性预测（MPP）方法，以提高MPP的效果。</li>
<li>methods: 该方法不仅探索了基于对的对应关系（pairwise atoms）的基本互动，还考虑了分子中重要的功能组（e.g., 含有几个原子的函数组）的互动。</li>
<li>results: 对七个流行的数据集进行了广泛的评估，并与现有的状态艺术方法进行了比较，结果表明，该方法的效果明显高于现有的方法。<details>
<summary>Abstract</summary>
Recently, Graph Transformer (GT) models have been widely used in the task of Molecular Property Prediction (MPP) due to their high reliability in characterizing the latent relationship among graph nodes (i.e., the atoms in a molecule). However, most existing GT-based methods usually explore the basic interactions between pairwise atoms, and thus they fail to consider the important interactions among critical motifs (e.g., functional groups consisted of several atoms) of molecules. As motifs in a molecule are significant patterns that are of great importance for determining molecular properties (e.g., toxicity and solubility), overlooking motif interactions inevitably hinders the effectiveness of MPP. To address this issue, we propose a novel Atom-Motif Contrastive Transformer (AMCT), which not only explores the atom-level interactions but also considers the motif-level interactions. Since the representations of atoms and motifs for a given molecule are actually two different views of the same instance, they are naturally aligned to generate the self-supervisory signals for model training. Meanwhile, the same motif can exist in different molecules, and hence we also employ the contrastive loss to maximize the representation agreement of identical motifs across different molecules. Finally, in order to clearly identify the motifs that are critical in deciding the properties of each molecule, we further construct a property-aware attention mechanism into our learning framework. Our proposed AMCT is extensively evaluated on seven popular benchmark datasets, and both quantitative and qualitative results firmly demonstrate its effectiveness when compared with the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
近期，图Transformer（GT）模型在分子性质预测（MPP）任务中广泛使用，因为它们可以准确地描述分子图节点之间的隐藏关系。然而，大多数现有的GT基本方法通常只研究对于每对原子的基本互动，因此它们忽略了分子中重要的功能块互动（例如，由几个原子组成的功能块）。在分子中，功能块是重要的特征征，它们对分子性质具有决定性的影响（例如，毒性和溶解度）。因此，忽略功能块互动将导致MPP的效果受限。为解决这个问题，我们提出了一种新的Atom-Motif Contrastive Transformer（AMCT）模型。AMCT不仅探索原子间互动，还考虑功能块互动。由于分子中的原子和功能块表示是同一个实例的两种视角，因此它们自然启合生成自我超级vis的信号。此外，同一个功能块可以出现在不同的分子中，因此我们还使用了对比损失来最大化功能块之间的表示协调。最后，为了明确每个分子中决定性的功能块，我们进一步构建了一个property-aware的注意机制到我们的学习框架中。我们的提出的AMCT模型在七个流行的benchmark数据集上进行了广泛的评估，结果表明，对于现有的state-of-the-art方法，AMCT模型在分子性质预测任务中表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundation-Models-for-Learning-on-Tabular-Data"><a href="#Towards-Foundation-Models-for-Learning-on-Tabular-Data" class="headerlink" title="Towards Foundation Models for Learning on Tabular Data"></a>Towards Foundation Models for Learning on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07338">http://arxiv.org/abs/2310.07338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, Jiang Bian<br>for: TabFMs are designed to overcome the limitations of current transferable tabular models, which lack support for direct instruction following in new tasks and neglect acquiring foundational knowledge and capabilities from diverse tabular datasets.methods: TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets.results: TabFMs significantly excel in instruction-following tasks like zero-shot and in-context inference, and achieve remarkable efficiency and competitive performance with scarce data. Additionally, TabFMs approach or even transcend the performance of renowned closed-source LLMs like GPT-4.<details>
<summary>Abstract</summary>
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference, but it also showcases performance that approaches, and in instances, even transcends, the renowned yet mysterious closed-source LLMs like GPT-4. Furthermore, when fine-tuning with scarce data, our model achieves remarkable efficiency and maintains competitive performance with abundant training data. Finally, while our results are promising, we also delve into TabFM's limitations and potential opportunities, aiming to stimulate and expedite future research on developing more potent TabFMs.
</details>
<details>
<summary>摘要</summary>
学习标准数据下的应用非常广泛。尽管有很大努力开发有效的学习模型 для标准数据，但目前可传递的标准模型仍然处于初始阶段，受到新任务 direct instruction 的支持或从多种标准数据集中获得基本知识和技能的忽略。在这篇论文中，我们提出了标准基本模型（TabFM），以解决这些局限性。TabFM 利用了生成标准学习的潜力，使用预训练的大型自然语言模型（LLM）作为基本模型，并通过特定目标进行精度调整，以涵盖广泛的标准数据集。这种方法赋予 TabFM 对标准数据的深刻理解和 universally 的能力。我们的评估表明，TabFM 非但在 zero-shot 和 context 推理任务中表现出色，而且在一些情况下，其表现甚至超越了知名但谜一族的关闭源 LLM 如 GPT-4。此外，当 fine-tuning  WITH 罕见数据时，我们的模型实现了很好的效率，并在有够数据进行 fine-tuning 时保持竞争性。最后，虽然我们的结果很有前途，但我们也探讨了 TabFM 的局限性和未来研究的可能性，以便激发和加速未来的 TabFM 研究。
</details></li>
</ul>
<hr>
<h2 id="Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer"><a href="#Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer" class="headerlink" title="Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer"></a>Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07323">http://arxiv.org/abs/2310.07323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zheng, Guogang Zhang, Chenchen Zhao, Qianqian Zhu</li>
<li>for: 这篇论文主要针对电力变压器诊断，即使现有的诊断方法主要基于粒子溶解分析，忽略了多通道连续数据的深度特征提取。</li>
<li>methods: 该文提出了多通道连续数据交叉提取结构（MCDC），用于全面利用变压器的内在特征，并且提出了一维 convolutional neural network注意力机制（1DCNN-attention），以更好地适应变压器诊断场景。</li>
<li>results: 实验表明，MCDC和1DCNN-attention比其他算法更有效，并且1DCNN-attention具有更好的稳定性。<details>
<summary>Abstract</summary>
Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are validated in experiments conducted on a dataset collected from real operation cases of power transformer. Additionally, the better stability of 1DCNN-attention has also been certified.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Power transformer 在网络基础设施中扮演着关键的角色，其诊断对稳定运行至关重要。然而，现有的变压器诊断方法主要基于离散气体分析，忽略了深入EXTRACT多个通道 consecutive data中的特征。这些未利用的序列数据包含了变压器状况的重要时间信息。为此，本文提出了多通道 consecutive data cross-EXTRACT (MCDC) 结构，以全面利用变压器的内在特征并评估其状况。此外，为更好地适应变压器诊断场景，本文还引入了一dimensional convolution neural network attention (1DCNN-attention) 机制，提供了更高效的解决方案，采用简化的空间复杂度。最后，实验 validate MCDC 和 1DCNN-attention 的效果，并证明它们在其他算法相比具有更高的泛化能力和稳定性。</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Resilient-Decentralized-Multi-Armed-Bandits"><a href="#Byzantine-Resilient-Decentralized-Multi-Armed-Bandits" class="headerlink" title="Byzantine-Resilient Decentralized Multi-Armed Bandits"></a>Byzantine-Resilient Decentralized Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07320">http://arxiv.org/abs/2310.07320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxuan Zhu, Alec Koppel, Alvaro Velasquez, Ji Liu</li>
<li>for: 这篇论文旨在研究在分散式合作多臂枪手（MAB）中，每个代理 observer 获得自己的奖励流，并寻求与其他代理交换信息，以选择一系列的臂，以最小化其后悔。</li>
<li>methods: 这篇论文使用了一种完全分散式抗衰伪（Byzantine）的Upper-Confidence Bound（UCB）算法，融合了代理之间的信息混合步骤，以及调整对奖励mean-estimates或信心集的错误值。</li>
<li>results: 这篇论文的主要贡献是开发了一个名为“完全分散式抗衰伪UCB”的算法，可以在分散式合作MAB中，防止潜在的Byzantine代理对奖励流的干扰，并且保证每个正常的代理的表现不会比过往的单一代理UCB1算法差。此外，研究显示，当每个代理有至少3f+1名邻的情况下，正常代理的总后悔会比非合作情况更好，其中f是最大可能的Byzantine代理数。<details>
<summary>Abstract</summary>
In decentralized cooperative multi-armed bandits (MAB), each agent observes a distinct stream of rewards, and seeks to exchange information with others to select a sequence of arms so as to minimize its regret. Agents in the cooperative setting can outperform a single agent running a MAB method such as Upper-Confidence Bound (UCB) independently. In this work, we study how to recover such salient behavior when an unknown fraction of the agents can be Byzantine, that is, communicate arbitrarily wrong information in the form of reward mean-estimates or confidence sets. This framework can be used to model attackers in computer networks, instigators of offensive content into recommender systems, or manipulators of financial markets. Our key contribution is the development of a fully decentralized resilient upper confidence bound (UCB) algorithm that fuses an information mixing step among agents with a truncation of inconsistent and extreme values. This truncation step enables us to establish that the performance of each normal agent is no worse than the classic single-agent UCB1 algorithm in terms of regret, and more importantly, the cumulative regret of all normal agents is strictly better than the non-cooperative case, provided that each agent has at least 3f+1 neighbors where f is the maximum possible Byzantine agents in each agent's neighborhood. Extensions to time-varying neighbor graphs, and minimax lower bounds are further established on the achievable regret. Experiments corroborate the merits of this framework in practice.
</details>
<details>
<summary>摘要</summary>
在分布式合作多臂抓拍机 (MAB) 中，每个代理都观察自己独特的奖励流，并尝试与其他代理交换信息，以选择一系列的臂，以最小化它的 regret。在合作环境中，代理可以超越单独运行 MAB 方法，如Upper-Confidence Bound (UCB)，独立运行。在这个工作中，我们研究如何在未知的代理中有 Byzantine 特性，即通过奖励价值或信任集的不正确通信，以获得类似的行为。这个框架可以用来模型计算机网络中的攻击者，导入推荐系统中的不良内容推广者，或财务市场中的操纵者。我们的主要贡献是开发一个完全分布式抗错误上界 (UCB) 算法，它结合代理之间的信息混合步骤与跳过矛盾和极端值的调整步骤。这个调整步骤使我们能够证明每个正常的代理的性能不 inferior 于过去的单一代理 UCB1 算法，并且更重要的是，所有正常的代理的总 regret 比非合作情况更好，只要每个代理至少有 3f+1 个邻居，其中 f 是最大可能的 Byzantine 代理数量。我们还将进一步推广到时间 varying 邻国图，以及最坏情况下的下界。实验证明了这个框架在实践中的优点。
</details></li>
</ul>
<hr>
<h2 id="Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction"><a href="#Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction" class="headerlink" title="Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction"></a>Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07313">http://arxiv.org/abs/2310.07313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Michał Sadowski, Piotr Kozakowski, Ruard van Workum, Stanisław Jastrzębski</li>
<li>for: 这个论文是为了解决有机化学中复杂分子的合成问题，使用机器学习算法预测可能的反应substrate。</li>
<li>methods: 这个论文使用的方法是基于最小模板（minimal templates），它们是简化反应模式，只包含关键分子变化。这种方法可以减少计算开销，并达到了标准测试数据集的状态前的结果。</li>
<li>results: 论文得到了最新的结果，在标准测试数据集上达到了最高的预测精度。<details>
<summary>Abstract</summary>
Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chineseRetrosynthesis是指将复杂分子synthesize成更简单的前体物质。在有机化学中，这种问题提出了挑战，特别是预测目标分子可能的反应substrate。这些解决方案主要分为模板基的和模板自由的两类。前者是高效的，但需要大量的预定反应模式，而后者具有更多的灵活性，但计算 overhead 较高，并且更难于解释。为解决这些问题，我们介绍了METRO（分子修改模板 для RetrOsynthesis），一种机器学习模型，通过使用最小的模板预测反应，减少计算开销，并在标准的benchmark上实现了状态的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Score-Regularized-Policy-Optimization-through-Diffusion-Behavior"><a href="#Score-Regularized-Policy-Optimization-through-Diffusion-Behavior" class="headerlink" title="Score Regularized Policy Optimization through Diffusion Behavior"></a>Score Regularized Policy Optimization through Diffusion Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07297">http://arxiv.org/abs/2310.07297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/srpo">https://github.com/thu-ml/srpo</a></li>
<li>paper_authors: Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, Jun Zhu</li>
<li>for: 增强offline reinforcement学习中的行为策略表示能力，通过抽象模型的准确描述来提高行为策略的生成能力。</li>
<li>methods: 提议从批判模型中提取高效的推理策略，并将其与预训练的扩散行为模型结合使用，以直接在优化过程中使用行为分布的分数函数来规范策略梯度。</li>
<li>results: 对D4RL任务进行了广泛的测试，结果显示，我们的方法可以在涉及到行动的任务中提高行动采样速度，并且仍然保持状态革命的性能水平，与其他主流的扩散基于方法相比，可以提高行动采样速度超过25倍。<details>
<summary>Abstract</summary>
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD"><a href="#Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD" class="headerlink" title="Why Does Sharpness-Aware Minimization Generalize Better Than SGD?"></a>Why Does Sharpness-Aware Minimization Generalize Better Than SGD?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07269">http://arxiv.org/abs/2310.07269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, Quanquan Gu</li>
<li>for: 该论文旨在解释Sharpness-Aware Minimization（SAM）如何提高大型神经网络的泛化能力，特别是在非线性神经网络和分类任务中。</li>
<li>methods: 论文使用了Stochastic Gradient Descent（SGD）和SAM两种训练方法进行比较，并通过分析损失函数的特性来解释SAM的泛化优势。</li>
<li>results: 实验结果表明，SAM在Synthetic和实际数据上都能够更好地适应不稳定的损失函数，从而提高神经网络的泛化能力。<details>
<summary>Abstract</summary>
The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby facilitating more effective learning of features. Experiments on both synthetic and real data corroborate our theory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大脑网络训练中的拟合问题，即模型记忆训练数据而无法泛化测试数据，已成为训练大脑网络的挑战。为解决这个挑战，锐度感知训练方法（SAM）已经出现了，它可以提高 neural networks 的泛化性，即使存在标签噪声。然而，SAM 在非线性 neural networks 和分类任务中的工作机制仍然不够了解。这篇文章填补这个空白，并证明 SAM 在某种数据模型和两层卷积 ReLU 网络中能够更好地泛化。我们的研究问题的损失场景是非拟合的，因此现有基于 Hessian 信息的解释不够。我们的结果解释了 SAM 的优势，特别是它在早期阶段防止噪声学习，从而促进更有效的特征学习。实验表明，Synthetic 和实际数据都支持我们的理论。
</details></li>
</ul>
<hr>
<h2 id="RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence"><a href="#RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence" class="headerlink" title="RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence"></a>RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07268">http://arxiv.org/abs/2310.07268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changan Yang, Yaxing Chen, Yao Zhang, Helei Cui, Zhiwen Yu, Bin Guo, Zheng Yan, Zijiang Yang</li>
<li>for: 本研究旨在解决智能交通系统中数据隐私协议问题。</li>
<li>methods: 该研究提出了一种基于raft协议的联邦学习框架，以保护数据隐私。</li>
<li>results: 实验结果显示，该框架比基eline对比更好地减少通信开销、提高模型准确率和加速模型融合。<details>
<summary>Abstract</summary>
Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated by state-of-the-art vehicular ad-hoc networks and artificial intelligence, various VCI applications come to place, e.g., collaborative sensing, positioning, and mapping. The collaborative property of VCI applications generally requires data to be shared among participants, thus forming network-wide intelligence. How to fulfill this process without compromising data privacy remains a challenging issue. Although federated learning (FL) is a promising tool to solve the problem, adapting conventional FL frameworks to VCI is nontrivial. First, the centralized model aggregation is unreliable in VCI because of the existence of stragglers with unfavorable channel conditions. Second, existing FL schemes are vulnerable to Non-IID data, which is intensified by the data heterogeneity in VCI. This paper proposes a novel federated learning framework called RaftFed to facilitate privacy-preserving VCI. The experimental results show that RaftFed performs better than baselines regarding communication overhead, model accuracy, and model convergence.
</details>
<details>
<summary>摘要</summary>
Vehicular crowd intelligence (VCI) 是一个emerging研究领域。受到现代交通自适应网络和人工智能的支持，VCI应用广泛，例如合作探测、定位和地图生成。VCI应用的协作性通常需要参与者共享数据，从而形成网络范围内的智能。然而，保持数据隐私性的问题仍然是一个挑战。虽然联邦学习（FL）是一种有前途的工具来解决这个问题，但将传统的FL框架应用于VCI是非常困难。首先，中央模型聚合是VCI中不可靠的，因为存在具有不利通信条件的废寸者。其次，现有的FL方案容易受到非同kind的数据影响，这在VCI中加剧了数据多样性。本文提出了一种新的联邦学习框架called RaftFed，以保护隐私的VCI。实验结果表明，RaftFed在通信开销、模型准确率和模型融合方面比基elinebetter。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review"><a href="#Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review" class="headerlink" title="Classification of Dysarthria based on the Levels of Severity. A Systematic Review"></a>Classification of Dysarthria based on the Levels of Severity. A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07264">http://arxiv.org/abs/2310.07264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Al-Ali, Somaya Al-Maadeed, Moutaz Saleh, Rani Chinnappa Naidu, Zachariah C Alex, Prakash Ramachandran, Rajeev Khoodeeram, Rajesh Kumar M</li>
<li>For: 这项研究的目的是对慢性喉突症的自动分类方法进行系统性的回顾和分析，以提高诊断的准确性和可靠性。* Methods: 该研究使用了数据库和灰Literature进行搜索，并根据研究问题的相关性进行选择。抽取数据包括使用了哪些方法、哪些特征来进行分类，以及使用了哪些人工智能技术。* Results: 该研究发现了一些有效的特征和人工智能技术，可用于自动分类慢性喉突症的严重程度。这些发现有助于提高诊断的准确性和可靠性，并可能提高患者的治疗效果。<details>
<summary>Abstract</summary>
Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.
</details>
<details>
<summary>摘要</summary>
某�ayer�症是一种神经学 speech 障碍，可能对患者的沟通能力和全面生活质量产生重要影响。精准和客观地分类某�ayer�症和其严重程度是诊断 intervención 的关键。现有的传统评估方法由speech-language pathologists (SLPs) 进行，但这些评估方法frequently 主观、时间consuming 和 между practitioners 可能存在差异。新兴的机器学习基本模型已经显示出可以提供更客观的某�ayer�症诊断，提高诊断准确性和可靠性。本系统性文献综述 aimsto comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.
</details></li>
</ul>
<hr>
<h2 id="Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation"><a href="#Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation" class="headerlink" title="Deep ReLU networks and high-order finite element methods II: Chebyshev emulation"></a>Deep ReLU networks and high-order finite element methods II: Chebyshev emulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07261">http://arxiv.org/abs/2310.07261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joost A. A. Opschoor, Christoph Schwab</li>
<li>for: This paper focuses on addressing expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) for continuous, piecewise polynomial functions on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$.</li>
<li>methods: The paper develops novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients, which can be computed easily from the values of the function in the Clenshaw–Curtis points using the inverse fast Fourier transform.</li>
<li>results: The paper obtains bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials, and provides ReLU NN emulation error estimates for various classes of functions and norms, including analytic functions with point singularities.<details>
<summary>Abstract</summary>
Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for various classes of functions and norms, commonly encountered in numerical analysis. In particular, we show exponential ReLU emulation rate bounds for analytic functions with point singularities and develop an interface between Chebfun approximations and constructive ReLU NN emulations.
</details>
<details>
<summary>摘要</summary>
文本中的问题是关于深度ReLU神经网络（NN）中 Sobolev norm 的表达率和稳定性，以及NN的参数数量对于连续、分割函数在固定区间（a, b）上的性能。我们提出了一种基于Chebyshev多项式扩展的ReLU NN伪函数代码的新构造方法，该方法可以从Clenshaw-Curtis点中计算Chebyshev约数。我们获得了基于于ReLU NN模拟精度和参数数量的上下文下的表达率和稳定性的较好的上限。我们还提供了各种函数和 нор 的ReLU NN模拟误差估计，常见于数值分析中。特别是，我们证明了对分布函数的点缺陷函数的 exponentially fast ReLU 模拟率上限。此外，我们还建立了Chebfun近似和构造性ReLU NN模拟的接口。
</details></li>
</ul>
<hr>
<h2 id="CacheGen-Fast-Context-Loading-for-Language-Model-Applications"><a href="#CacheGen-Fast-Context-Loading-for-Language-Model-Applications" class="headerlink" title="CacheGen: Fast Context Loading for Language Model Applications"></a>CacheGen: Fast Context Loading for Language Model Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07240">http://arxiv.org/abs/2310.07240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen Jiang</li>
<li>for: 提高 LLM 系统的响应速度和可扩展性，使其能够更好地处理长 Context  Task。</li>
<li>methods:  CacheGen 使用了一种新的编码器，将 Key-Value 特征压缩成更加Compact 的 Bitstream 表示形式，以降低带宽使用情况。此外，CacheGen 还使用了一个控制器，根据 Context 的长度和 LLM 的大小，选择合适的压缩级别和加载 Context 的方式，以最小化总的延迟和带宽使用情况。</li>
<li>results: CacheGen 比现有的长 Context 处理方法减少了带宽使用情况 by 3.7-4.3x，并将总的延迟和处理 Context 的时间减少到 2.7-3x，同时保持 LLM 在不同任务上的性能相似。<details>
<summary>Abstract</summary>
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).   This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a tailored arithmetic coder, taking advantage of the KV features' distributional properties, such as locality across tokens. Furthermore, CacheGen minimizes the total delay in fetching and processing a context by using a controller that determines when to load the context as compressed KV features or raw text and picks the appropriate compression level if loaded as KV features. We test CacheGen on three models of various sizes and three datasets of different context lengths. Compared to recent methods that handle long contexts, CacheGen reduces bandwidth usage by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x while maintaining similar LLM performance on various tasks as loading the text contexts.
</details>
<details>
<summary>摘要</summary>
LLMs 在进行更复杂的任务时，其输入会包含更长的上下文，以回答需要领域知识或用户特定的对话历史的问题。然而，使用长上下文会对响应 LLM 系统造成挑战，因为没有可以生成任何内容直到所有上下文都被 fetched 并处理于 LLM 中。现有系统通常仅仅优化计算延迟在上下文处理中（例如，通过缓存中间键值特征），但这经常会导致网络延迟更长。本文提出了 CacheGen，用于最小化 LLM 上下文的抓取和处理延迟。CacheGen 减少了在传输长上下文中的键值特征的带宽，通过一种新的编码器，将键值特征转换为更 компакт的比特流表示。该编码器结合了自适应量化和特制的加法编码器，利用键值特征的分布特性，如token之间的本地性。此外，CacheGen 还减少了抓取和处理上下文的总延迟，通过一个控制器，确定在加载上下文时是作为压缩键值特征还是原始文本，并选择适当的压缩级别。我们在不同的模型和数据集上测试了 CacheGen，相比之前的长上下文处理方法，CacheGen 可以减少带宽使用率为 3.7-4.3倍，并减少抓取和处理上下文的总延迟为 2.7-3倍，而保持 LLM 在不同任务上的性能相似。
</details></li>
</ul>
<hr>
<h2 id="Are-GATs-Out-of-Balance"><a href="#Are-GATs-Out-of-Balance" class="headerlink" title="Are GATs Out of Balance?"></a>Are GATs Out of Balance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07235">http://arxiv.org/abs/2310.07235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz</li>
<li>for: 这个论文的目的是研究图解析网络（Graph Neural Network，GNN）的优化和学习动态。</li>
<li>methods: 这篇论文使用了Graph Attention Network（GAT） Architecture，并对其进行了分析和研究。</li>
<li>results: 论文发现GAT中的权重系数化的注意力归一化会导致许多参数在训练中难以更改，这会导致深度的GAT表现更差。 authors提出了一种 Initialization scheme，可以更好地传递梯度，并使得更深的网络可以更好地训练。<details>
<summary>Abstract</summary>
While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.
</details>
<details>
<summary>摘要</summary>
而且表达能力和计算能力 OF 图 neural networks (GNNs) 已经被理论研究，但是它们的优化和学习动力，总的来说，还很少研究。我们的研究探讨了 Graph Attention Network (GAT)，一种流行的 GNN 架构，其中每个节点的邻居汇集被参数化的注意系数权重。我们得出了 GAT 梯度流动动力学保守法则，解释了为什么大多数 GAT 参数在训练中难以更改。这个效应在更深的 GAT 中更加明显，它们在训练和结束时间比其浅层对手更差。为了解决这个问题，我们提出了一种Initialization scheme，它可以更好地传递梯度，从而使得深度网络更容易训练，并且在训练和结束时间比标准Initialization更快。我们的主要定理作为 изучение positive homogeneous models with attention mechanism 的学习动力的开门篇。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality"><a href="#Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality" class="headerlink" title="Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality"></a>Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07234">http://arxiv.org/abs/2310.07234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/hide-prompt">https://github.com/thu-ml/hide-prompt</a></li>
<li>paper_authors: Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu</li>
<li>for: 这 paper 的目的是提出一种基于 prompt-based  kontinual learning 的方法，以优化 continual learning 的性能。</li>
<li>methods: 这 paper 使用了 hierarchical decomposition 的思想，将 continual learning 目标划分为三个组成部分：内 task 预测、任务标识推理和任务适应预测。并使用了一个 ensemble 的任务特定 prompt 和 Both 的统计来优化这些组成部分。</li>
<li>results: 这 paper 的实验结果表明，使用 HiDe-Prompt 方法可以在 continual learning 中提高性能，并且对不同的 pre-training 方法具有较高的灵活性和可repeatability。在 Split CIFAR-100 和 Split ImageNet-R 上，HiDe-Prompt 方法可以达到15.01% 和9.61% 的提升。<details>
<summary>Abstract</summary>
Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \url{https://github.com/thu-ml/HiDe-Prompt}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment"><a href="#Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment" class="headerlink" title="Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment"></a>Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07229">http://arxiv.org/abs/2310.07229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, Weiying Ma, Zhiming Ma, Yanyan Lan</li>
<li>for: 这个研究是为了提出一种新的蛋白质口袋预训练方法，以便更好地模型蛋白质和药物之间的互动。</li>
<li>methods: 这个方法使用了高解析力的蛋白质结构知识，以及已经预训的小分子表示，将蛋白质结构切割为药物相似的片段和其相应的口袋，以获得可靠的药物-蛋白质互动模型。</li>
<li>results: 这个方法名为ProFSA，可以在不同的任务上 дости得到最佳性能，包括口袋可用性预测、口袋匹配和药物缩紧价预测等。另外，这个方法可以轻松地处理大量的蛋白质结构数据，并且可以提高药物设计的效率。<details>
<summary>Abstract</summary>
Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.
</details>
<details>
<summary>摘要</summary>
腋部表示在各种生物医学应用中发挥重要作用，如药物可用性预测、药物粘性预测和初始药物设计。现有的几何特征和预训 repre sentation 已经达到了一定的成果，但是它们通常忽略了药物和腋部之间的基本交互。然而， Protein Data Bank 数据库中的药物-腋部复合结构数据量非常有限（ menos than 100 thousand non-redundant pairs），这限制了大规模预训的可行性。为了解决这一问题，我们提出了一种新的腋部预训方法，利用高级别的原子保健结构知识，并且利用已经预训的小分子表示。我们将蛋白结构分解成可以模拟药物-腋部交互的药物类型和相应的腋部，从而生成了超过500万个复合体。然后，我们将腋部编码器在对 pseudo-ligand 的表示进行对比的情况下进行准确的训练。我们的方法，名为 ProFSA，在不同的任务中均达到了领先的性能，包括腋部可用性预测、腋部匹配和药物粘性预测。此外，我们的工作打开了一个新的途径，通过利用高质量和多样化的蛋白结构数据库，来缓解蛋白-药物复合数据的稀缺。
</details></li>
</ul>
<hr>
<h2 id="COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL"><a href="#COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL" class="headerlink" title="COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL"></a>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07220">http://arxiv.org/abs/2310.07220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, Furong Huang</li>
<li>for: 提高模型基于学习方法的效率和终点性能</li>
<li>methods: 使用保守的模型执行和乐观环境探索，并通过不确定性感知来帮助模型预测错误的修正</li>
<li>results: 对一系列 proprioceptive 和视觉 kontinuous control 任务进行了实验，并证明了 $\texttt{COPlanner}$ 可以提高模型基于方法的样本效率和终点性能<details>
<summary>Abstract</summary>
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. $\texttt{COPlanner}$ is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with $\texttt{COPlanner}$.
</details>
<details>
<summary>摘要</summary>
模型基于的强化学习方法包括两个阶段：模型执行以生成策略学习的样本，以及使用当前策略进行真实环境中的动态模型学习。然而，由于真实世界环境的复杂性，是不可避免学习不准确的动态模型，这会导致策略学习错误，从而导致优化解决方案。在这篇论文中，我们提出了 $\texttt{COPlanner}$，一种基于规划的框架，用于解决不准确学习动态模型问题。 $\texttt{COPlanner}$ 利用了一种不确定性意识权益导向的模型预测控制（UP-MPC）组件，以计划多步不确定性估计。这个估计的不确定性然后用于模型执行中的罚款，以及在真实环境中的探索中的奖励。因此， $\texttt{COPlanner}$ 可以避免模型不确定区域，从而减轻模型预测错误的影响。同时，它可以活动地探索高奖励模型不确定区域，以减少模型预测错误。 $\texttt{COPlanner}$ 是一个可插入的框架，可以与任何强化学习方法结合使用。实验结果表明，在一系列的 proprioceptive 和视觉连续控制任务上， $\texttt{COPlanner}$ 可以显著提高强化学习方法的样本效率和最终性能。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices"><a href="#Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices" class="headerlink" title="Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices"></a>Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07217">http://arxiv.org/abs/2310.07217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessio Burrello, Matteo Risso, Beatrice Alessandra Motetti, Enrico Macii, Luca Benini, Daniele Jahier Pagliari</li>
<li>for: 这篇论文的目的是提出一种能够实现内置式神经网络搜寻（Differentiable NAS）来实现内置式神经网络的最佳化，并且能够考虑多个硬件限制，以提高内置式神经网络的精度和效率。</li>
<li>methods: 这篇论文使用了一种称为“Differentiable NAS”的搜寻方法，可以同时考虑多个硬件限制，例如内存和延迟等限制，以实现最佳化。</li>
<li>results: 这篇论文的实验结果显示，使用Differentiable NAS可以在实现内置式神经网络的同时，实现87.4%的内存和54.2%的延迟的减少，并且保持与现有的手动调整深度神经网络相同的精度。<details>
<summary>Abstract</summary>
The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency in a time comparable to a single standard training. The proposed approach is evaluated on five IoT-relevant benchmarks, including the MLPerf Tiny suite and Tiny ImageNet, demonstrating that, with a single search, it is possible to reduce memory and latency by 87.4% and 54.2%, respectively (as defined by our targets), while ensuring non-inferior accuracy on state-of-the-art hand-tuned deep neural networks for TinyML.
</details>
<details>
<summary>摘要</summary>
“由于互联网物联网（IoT）设备的快速扩散，需要有效率和准确的深度学习（DL）模型，但传统的DL模型通常过于复杂和计算投入。为解决这个挑战，深度架构搜索（NAS）已成为一种受欢迎的设计自动化技术，可以同时优化模型的准确率和复杂度。然而，现有的NAS技术通常需要许多迭代才能生成遵循硬件限制的网络，例如硬件中最大内存容量或target应用中允许的延迟。在这个工作中，我们提出了一种新的方法，可以将多个约束 integrating into so-called Differentiable NAS optimization methods，允许在一次射击中生成遵循用户定义的内存和延迟约束的模型，并且与现有的手动调整深度神经网络相比，可以在相同的时间内减少内存和延迟量，同时保证模型的准确率不下降。我们的方法在五个 IoT 相关的 bencmarks 上进行了评估，包括 MLPerf Tiny suite 和 Tiny ImageNet，结果表明，通过单一的搜索，可以在内存和延迟方面减少 87.4% 和 54.2%，respectively（根据我们的目标），而且与现有的手动调整深度神经网络相比，模型的准确率不下降。”
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes"><a href="#Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes" class="headerlink" title="Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes"></a>Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07216">http://arxiv.org/abs/2310.07216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehyeong Jo, Sung Ju Hwang</li>
<li>for: 模型数据的分布在里曼尼安 manifolds 中，是许多科学领域中必需的，但现有的生成模型在 manifolds 上受到假设热体kernel的限制，只能应用于简单的几何结构。本文提出了里曼尼安扩散混合（Riemannian Diffusion Mixture），一种理想的生成过程模型，基于endpoint-conditioned扩散过程，而不是前一些扩散模型的减量方法。</li>
<li>methods: 我们提出了一种简单 yet efficient的训练目标，可以轻松应用于一般 manifolds。我们的方法在多种 manifold 上进行了比较，并且可以在高维度下进行扩散，需要少量的在训练过程中的 simulations 步骤。</li>
<li>results: 我们的方法在多种 manifold 上进行了比较，并且在高维度下进行扩散，需要少量的在训练过程中的 simulations 步骤。我们的方法可以在不同的 manifold 上模型数据的分布，并且可以在高维度下进行扩散。<details>
<summary>Abstract</summary>
Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outperforms previous generative models on various manifolds while scaling to high dimensions and requires a dramatically reduced number of in-training simulation steps for general manifolds.
</details>
<details>
<summary>摘要</summary>
“理解数据在里曼尼安 manifold 上的分布是模型非欧几何空间数据的关键，这是许多科学领域的应用所需。然而，现有的生成模型在 manifold 上受到严重的分布计算成本或基于热征函数的 aproximation 的限制，这限制了它们的可靠性和扩展性。在这项工作中，我们介绍了里曼尼安混合模型，一种基于endpoint-conditioned diffusion process的主导性框架 для生成过程。我们还提出了一种简单 yet efficient的训练目标，可以方便地应用于通用 manifold。我们的方法在多种 manifold 上比previous生成模型表现出色，可以扩展到高维度，并且需要减少很多在训练过程中的 simulations 步骤。”Note: Simplified Chinese is a writing system used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other countries. The translation is written in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration"><a href="#Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration" class="headerlink" title="Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration"></a>Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07211">http://arxiv.org/abs/2310.07211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Yunan Wang, Guojian Zhan, Jie Li, Shengbo Eben Li</li>
<li>for: 这 paper 证明了正则化策略迭代法（Regularized Policy Iteration, RPI）与标准的新颖-拉普逊方法（Newton-Raphson method）在减少 Bellman 方程的情况下是等价的。</li>
<li>methods: 这 paper 使用了正则化策略迭代法（RPI）和减少 Bellman 方程的方法来证明 RPI 的全局线性减少率为 $\gamma$（折扣因子），并且在本地区域内 converge 到最优值 quadratic 速率。</li>
<li>results: 这 paper 证明了 RPI 在 global 和本地区域内都有 linear 减少率，其中 global 减少率为 $\gamma$，而在本地区域内 converge 到最优值 quadratic 速率。<details>
<summary>Abstract</summary>
Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite-step policy evaluation, is equivalent to inexact Newton method where the Newton iteration formula is solved with truncated iterations. We prove that the associated algorithm achieves an asymptotic linear convergence rate of $\gamma^M$ in which $M$ denotes the number of steps carried out in policy evaluation. Our results take a solid step towards a better understanding of the convergence properties of regularized policy iteration algorithms.
</details>
<details>
<summary>摘要</summary>
“常规化”是强化学习算法中最重要的技术之一。软 actor-批评算法是常规化 policy 迭代的特殊情况，其正则化项选择为 entropy 函数。虽然软 actor-批评算法在实际上获得了一些成功，但其理论基础仍然不清楚。这篇论文证明了软 actor-批评算法与标准的 Newton-Raphson 方法在 Bellman 方程缓和强 convex 函数时是等价的。这种等价关系为软 actor-批评算法的全面分析奠定了基础。我们证明了软 actor-批评算法在 discount 因子 $\gamma$ 下有全面线性减少率 $\gamma$，并且当它进入一个地方邻域时，其减少率为 $\gamma^2$。我们还证明了一种修改后的软 actor-批评算法，即 finite-step policy evaluation，与不准确的 Newton 方法相等。我们证明了这种算法在 $\gamma^M$ 下具有极限线性减少率，其中 $M$ 是在 policy evaluation 中执行的步数。我们的结果为软 actor-批评算法的减少性质做出了一个坚实的步骤。
</details></li>
</ul>
<hr>
<h2 id="Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances"><a href="#Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances" class="headerlink" title="Robust Safe Reinforcement Learning under Adversarial Disturbances"></a>Robust Safe Reinforcement Learning under Adversarial Disturbances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07207">http://arxiv.org/abs/2310.07207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang</li>
<li>for: 本研究的目的是提出一种可靠的强化学习框架，以处理实际控制任务中的外部干扰。</li>
<li>methods: 本文提出了一种基于强化学习的两个玩家零 SUM 游戏，用于解决最差情况下的干扰问题。首先，本文解决了一个政策迭代算法，用于解决最差情况下的干扰问题。其次，本文将此算法 integrate 到一种受限制的强化学习算法中，以同时Synthesize 最差情况下的Robust invariants set和用于受限制的政策优化。</li>
<li>results: 实验表明，提出的方法可以在classic control tasks中实现零约束违反，同时也可以与其他基线算法相比，在缺省情况下达到相似的性能。<details>
<summary>Abstract</summary>
Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set. Second, this paper integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization. This algorithm tackles both optimality and safety, i.e., learning a policy that attains high rewards while maintaining safety under worst-case disturbances. Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Our proposed method also attains comparable performance as the baselines even in the absence of the adversary.
</details>
<details>
<summary>摘要</summary>
安全是控制任务中应用强化学习的基本问题，尤其在外部干扰存在时。然而，现有的安全强化学习算法很少考虑外部干扰，这限制了它们在实际应用中的可靠性和灵活性。为解决这个挑战，这篇论文提出了一种可靠安全强化学习框架，可以面对最坏情况的干扰。首先，这篇论文提出了一种策略迭代算法，用于解决可靠集的问题。在这种算法中，我们首先将控制输入和外部干扰转化为两个 игро之间的两进制零sum游戏，其中控制输入（ protagonist）的目标是保持安全，而外部干扰（ adversary）的目标是打砸安全。我们证明了该策略迭代算法 monotonic converge 到最大可靠 invariant set。其次，这篇论文将提出的策略迭代算法与 constrained reinforcement learning 算法结合，以同时Synthesize 可靠 invariant set 和用其进行约束化策略优化。这个算法同时解决了最优和安全的问题，即学习一个策略，可以在最坏情况下维护安全，同时尝试高的奖励。经典控制任务的实验表明，我们的提出方法可以避免安全约束的违反，而其他基线算法却有显著的违反。此外，我们的方法还可以与基线算法相比，在没有敌人情况下达到相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance"><a href="#Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance" class="headerlink" title="Boosting Learning for LDPC Codes to Improve the Error-Floor Performance"></a>Boosting Learning for LDPC Codes to Improve the Error-Floor Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07194">http://arxiv.org/abs/2310.07194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hee-Youl Kwak, Dae-Young Yun, Yongjune Kim, Sang-Hyo Kim, Jong-Seon No</li>
<li>for: 这个研究旨在提高低密度条件检查（LDPC）码的错误处理能力，以及实现LDPC码在需要极高可靠性的应用中的应用。</li>
<li>methods: 本研究使用了boosting学习技术，将捉猿网络分成两个网络，并将后网络特化为处理未被首网络处理的错误码字。此外，本研究还使用了区块化训练时间表，以地方地训练网络单元，并将不满意验证节点赋予不同的重量，以降低错误地板。</li>
<li>results: 本研究显示，通过这些训练方法，可以将LDPC码的错误地板降低到最低水平，并且不需要新增硬件成本。此外，与其他解码方法相比，本研究的NMS解码器在错误地板方面表现最佳。<details>
<summary>Abstract</summary>
Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction ability and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, poses challenges in achieving extremely low error rates and the application of LDPC codes in scenarios demanding ultra high reliability. In this work, we propose training methods to optimize neural min-sum (NMS) decoders that are robust to the error-floor. Firstly, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two networks and train the post network to be specialized for uncorrected codewords that failed in the first network. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning different weights to unsatisfied check nodes effectively lowers the error-floor with a minimal number of weights. By applying these training methods to standard LDPC codes, we achieve the best error-floor performance compared to other decoding methods. The proposed NMS decoder, optimized solely through novel training methods without additional modules, can be implemented into current LDPC decoders without incurring extra hardware costs. The source code is available at https://github.com/ghy1228/LDPC_Error_Floor.
</details>
<details>
<summary>摘要</summary>
低密度差异检查（LDPC）编码已经成功商业化在通信系统中，因为它们具有强大的错误检测能力和简单的解码过程。然而，LDPC编码中的错误地面现象，导致错误率在某个水平上停止下降，对于实现极高可靠性的应用场景带来挑战。在这项工作中，我们提出了优化神经网络最小和（NMS）解码器的训练方法，以便对错误地面进行鲁棒性处理。首先，我们利用神经网络集成技术的扩展学习方法，将解码网络分成两个网络，并在第二个网络中训练后网络来处理未被首个网络正确解码的代码字。其次，为了解决训练过程中的消失梯度问题，我们引入了块 wise 训练时间表，以地方性地训练每个块的 weights，而不是全网络。最后，我们发现，对不满足的检查节点赋予不同的权重，可以有效降低错误地面，并且只需要少量的权重。通过这些训练方法，我们对标准 LDPC 编码进行优化，实现了最佳的错误地面性能。我们的 NMS 解码器，通过solely 利用新的训练方法而无需添加额外模块，可以在现有 LDPC 解码器中实现，无需新增硬件成本。源代码可以在 GitHub 上找到：https://github.com/ghy1228/LDPC_Error_Floor。
</details></li>
</ul>
<hr>
<h2 id="Neural-networks-deep-shallow-or-in-between"><a href="#Neural-networks-deep-shallow-or-in-between" class="headerlink" title="Neural networks: deep, shallow, or in between?"></a>Neural networks: deep, shallow, or in between?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07190">http://arxiv.org/abs/2310.07190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guergana Petrova, Przemyslaw Wojtaszczyk</li>
<li>for: 这 paper 是为了研究Feed-forward neural network的输出错误率的估计。</li>
<li>methods: 这 paper 使用了 Lipschitz 活化函数和宽度 W、深度 l 的 feed-forward neural network。</li>
<li>results: 这 paper 发现，当 depth l 增长到无穷大时，Feed-forward neural network 的输出错误率可以达到更好的水平，且 fixing width W 并让 depth l 增长不会带来任何改善。<details>
<summary>Abstract</summary>
We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
</details>
<details>
<summary>摘要</summary>
我们提供以下估计对封闭子集的扩张错误的渐近近似，使用内置构造神经网络的输出。我们显示，对于内置深度趋向无限大的神经网络，可能达到比据数据类型的 entropy 数据更好的近似率，并且随着宽度 W 变大，获得的近似率不会提高。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival"><a href="#Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival" class="headerlink" title="Kernel Cox partially linear regression: building predictive models for cancer patients’ survival"></a>Kernel Cox partially linear regression: building predictive models for cancer patients’ survival</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07187">http://arxiv.org/abs/2310.07187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rongyaohua/reggkm">https://github.com/rongyaohua/reggkm</a></li>
<li>paper_authors: Yaohua Rong, Sihai Dave Zhao, Xia Zheng, Yi Li</li>
<li>for: 预测肿瘤患者的临床结果，准确预测肿瘤患者的生存时间。</li>
<li>methods: 使用kernel Cox分数模型和regularized garrotized kernel machine（RegGKM）方法，同时自动 remov irrelevant parametric和非 Parametric预测器。</li>
<li>results: 在模拟试验中，提出的方法 sempre有更高的预测精度，并在多发性骨癌数据集中预测患者的死亡负担基于基因表达。<details>
<summary>Abstract</summary>
Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method always has better predictive accuracy. We apply this method to analyze a multiple myeloma dataset and predict patients' death burden based on their gene expressions. Our results can help classify patients into groups with different death risks, facilitating treatment for better clinical outcomes.
</details>
<details>
<summary>摘要</summary>
广泛的多样性存在于癌症患者的存活时间中，从几个月到数十年不等。要准确预测临床结果，需要建立一个准确预测模型，将患者的分子 profilest 与患者的存活时间相关联。由于存活时间和高维分子预测器之间存在复杂的关系，同时需要进行非 Parametric 模型化和无关预测器的去除，这使得模型建立变得具有挑战性。在本文中，我们建立了一个 kernel Cox 协方差 semi-parametric 模型，并提出了一种新的 RegGKM 方法来适应模型。我们使用 kernel machine 方法来描述存活时间和预测器之间的复杂关系，同时通过 Lasso 罚因自动去除无关 Parametric 和非 Parametric 预测器。我们开发了一种高维度的高效算法来实现该方法。与其他竞争方法在 simulated 中进行比较表明，我们的方法总是有更高的预测精度。我们应用该方法分析了一个多发性骨髓癌数据集，并预测患者的死亡负担基于他们的基因表达。我们的结果可以帮助分类患者为不同的死亡风险群，以便进行更好的临床结果。
</details></li>
</ul>
<hr>
<h2 id="SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation"><a href="#SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation" class="headerlink" title="SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation"></a>SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07183">http://arxiv.org/abs/2310.07183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shellredia/sam-octa">https://github.com/shellredia/sam-octa</a></li>
<li>paper_authors: Xinrun Chen, Chengliang Wang, Haojian Ning, Shiying Li<br>for:* 这个论文主要是为了提出一种基于low-rank适应技术的自适应模型，用于进行多种Optical coherence tomography angiography（OCTA）图像分割任务。methods:* 该方法使用了基于low-rank适应技术的基础模型，并提出了相应的提示点生成策略，以处理不同的OCTA图像分割任务。results:* 该方法在公共可用的OCTA-500和ROSE数据集上进行了实验，并达到或超越了当前状态的分割性能指标。* 该方法可以实现当地血管分割和有效的血管-血管分割，这些任务在前一些工作中尚未得到好的解决。<details>
<summary>Abstract</summary>
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The code is available at https://github.com/ShellRedia/SAM-OCTA.
</details>
<details>
<summary>摘要</summary>
《Optical coherence tomography angiography（OCTA）图像分析中的目标分割问题》的研究中，需要进行特定目标的分割。现有的方法通常是在有限的样本（约一百个）上进行指导学习，这可能会导致过拟合。为解决这个问题，我们采用了低级别适应技术，并提出了相应的提示点生成策略，以处理多种OCTA图像分割任务。我们命名这种方法为SAM-OCTA，并在公共可用的OCTA-500和ROSE数据集上进行了实验。SAM-OCTA方法实现了或接近了状态之 arts的分割性能指标。我们还详细讨论了提示点的效果和应用性，特别是在Retinal vessel、foveal avascular zone、capillary、artery和vein segmentation任务中。此外，SAM-OCTA实现了本地血管分割和有效的artery-vein分割，这在前一个works中尚未得到解决。代码可以在https://github.com/ShellRedia/SAM-OCTA上下载。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions"><a href="#Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions" class="headerlink" title="Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions"></a>Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07174">http://arxiv.org/abs/2310.07174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungtaek Kim, Jeongbeen Yoon, Minsu Cho</li>
<li>for: 这篇论文的主要目标是解决排序问题，并对传统排序算法的问题定义进行扩展和改进。</li>
<li>methods: 该论文使用神经网络来解决排序问题，并采用了一种名为“排序网络”的方法。该方法使用了多头注意力 Mechanism来捕捉输入之间的依赖关系，并通过一种名为“满足非减少和导数条件”的满足函数来保证排序网络的导数性。</li>
<li>results: 实验结果显示，该方法在多种排序 benchmark 上表现比基eline方法更好，或者与基eline方法相当。<details>
<summary>Abstract</summary>
Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
</details>
<details>
<summary>摘要</summary>
Sorting 是计算机系统中的基本操作之一，长期作为研究主题。我们在传统排序算法的问题定义之外，考虑排序问题的更广泛和表达力强的输入，例如多个数字图像和图像断片，通过神经网络进行排序。为了学习高维输入到排序变量的映射，排序网络的导数性需要保证。在这篇论文中，我们定义了一种软化错误函数，并开发了一种不减少和导数性的排序网络。此外，我们采用了 permutation-equivariant Transformer 网络， capture 输入的依赖关系，并利用其自注意力来提高模型的表达能力。实验表明，我们的方法在多个排序 benchmark 上表现较好或与基eline 方法相当。
</details></li>
</ul>
<hr>
<h2 id="Federated-Generalization-via-Information-Theoretic-Distribution-Diversification"><a href="#Federated-Generalization-via-Information-Theoretic-Distribution-Diversification" class="headerlink" title="Federated Generalization via Information-Theoretic Distribution Diversification"></a>Federated Generalization via Information-Theoretic Distribution Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07171">http://arxiv.org/abs/2310.07171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheshun Wu, Zenglin Xu, Dun Zeng, Qifan Wang</li>
<li>for: 这篇论文旨在解决联合学习中的非独立同分布（non-IID）挑战，协助提高联合学习模型的通用化能力。</li>
<li>methods: 本论文提出了一个信息理论基础的通用化框架，并引入了一种量子聚合方法和两种客户端选择策略，以增强联合学习的通用化能力。</li>
<li>results: 经过实验评估，本论文的提案方法能够增强联合学习模型的通用化能力，并且与理论建构之间存在良好的一致性。<details>
<summary>Abstract</summary>
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic generalization framework for FL. Specifically, it quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. Inspired by our deduced generalization bounds, we introduce a weighted aggregation approach and a duo of client selection strategies. These innovations aim to bolster FL's generalization prowess by encompassing a more varied set of client data distributions. Our extensive empirical evaluations reaffirm the potency of our proposed methods, aligning seamlessly with our theoretical construct.
</details>
<details>
<summary>摘要</summary>
受到协同学习（Federated Learning，FL）的推动，FL 在无需直接数据共享的情况下实现了模型训练的合作。然而，客户端数据分布的差异（non-Independent Identically Distributed，non-IID）问题对 FL 的泛化能力产生了重要的障碍。在一些客户端不参与训练过程的情况下，这种情况变得更加复杂，使得训练模型的泛化能力评估变得更加困难。Recent studies have focused on the generalization gap for unseen data from participating clients with diverse distributions, but the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked.为了应对这些挑战，我们的论文揭示了一种信息理论基础的泛化框架 для FL。具体来说，它衡量了本地分布的信息熵，并识别这些分布之间的差异。我们根据这些泛化误差的 bound 引入了一种权重聚合方法和两种客户端选择策略。这些创新目的是为了增强 FL 的泛化能力，使其包括更多的客户端数据分布。我们的广泛的实验证明了我们的提议的效果，与我们的理论结构契合。
</details></li>
</ul>
<hr>
<h2 id="LLark-A-Multimodal-Foundation-Model-for-Music"><a href="#LLark-A-Multimodal-Foundation-Model-for-Music" class="headerlink" title="LLark: A Multimodal Foundation Model for Music"></a>LLark: A Multimodal Foundation Model for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07160">http://arxiv.org/abs/2310.07160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner</li>
<li>for: 这篇论文是为了探讨音乐理解的问题，即使是专业人士和现有的AI系统都难以理解音乐的复杂结构。</li>
<li>methods: 作者提出了一种基于多Modal模型的LLark，该模型通过对多种开源音乐数据进行数据创建和一种统一的指令调整格式进行调整。</li>
<li>results: 作者在三种任务（音乐理解、描述和理解）中表现出色，其中与现有基线比较，在零shot泛化中对音乐理解任务表现卓越，而人类在描述和理解任务中与模型的回答有高度一致。<details>
<summary>Abstract</summary>
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .
</details>
<details>
<summary>摘要</summary>
音乐具有独特和复杂的结构，对于专家人类和现有的AI系统来说都是挑战，与其他形式的音频不同。我们介绍LLark，一种基于指令调整的多modal模型，用于音乐理解。我们详细介绍了我们的数据创建过程，包括对多种开源音乐数据集的扩充和转换为一致的指令调整格式。我们提议一种多modal的LLark模型，将音乐生成模型和语言模型集成。在三种任务（音乐理解、描述和理解）的评估中，我们显示了我们的模型在零shot泛化中与现有基eline匹配或超越，人类在描述和理解任务中与模型的回答 Display a high degree of agreement。LLark通过 entirely open-source music data和模型进行训练，我们在发表这篇论文时将我们的训练代码和数据公开发布。更多结果和音频示例可以通过https://bit.ly/llark查看，代码可以在https://github.com/spotify-research/llark上获取。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-from-Purified-Demonstration"><a href="#Imitation-Learning-from-Purified-Demonstration" class="headerlink" title="Imitation Learning from Purified Demonstration"></a>Imitation Learning from Purified Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07143">http://arxiv.org/abs/2310.07143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunke Wang, Minjing Dong, Bo Du, Chang Xu</li>
<li>for: addressing sequential decision-making problems with imperfect expert demonstrations</li>
<li>methods: purify potential perturbations in imperfect demonstrations via a two-step diffusion process, then conduct imitation learning from purified demonstrations</li>
<li>results: effective improvement in imitation learning performance, as demonstrated through theoretical evidence and evaluation results on MuJoCo.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在Addressing sequential decision-making problems with imperfect expert demonstrations</li>
<li>methods: 通过两步扩散过程纯化 potential perturbations in imperfect demonstrations,然后进行 imitation learning from purified demonstrations</li>
<li>results: 通过 teoretic evidence 和 MuJoCo 上的评估结果，证明了我们的方法可以有效地提高 imitation learning 性能。<details>
<summary>Abstract</summary>
Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is utilized to recover the optimal expert demonstrations from the diffused ones. We provide theoretical evidence supporting our approach, demonstrating that total variance distance between the purified and optimal demonstration distributions can be upper-bounded. The evaluation results on MuJoCo demonstrate the effectiveness of our method from different aspects.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>受欢迎的模仿学习方法在执行顺序决策问题上展示出了承诺，假设专家示范是最佳的。然而，在实际场景中，专家示范经常不完美，这会导致模仿学习的应用困难。现有的研究通常是通过优化不完美示范来解决这些问题，但是这通常需要一定的完美示范来保证性能。为了解决这些问题，我们提议纯化不完美示范中的潜在干扰，然后通过纯化后的示范进行模仿学习。针对成功的扩散模型，我们引入了两步纯化过程。在第一步，我们通过引入额外噪声来使用前向扩散过程，有效地平滑化不完美示范中的潜在干扰。接着，我们利用反生成过程来恢复优质专家示范从扩散过的示范中。我们提供了对我们方法的理论证明，证明了纯化后示范和优质示范之间的总差距的上界。我们的评估结果在MuJoCo上表明我们的方法在不同的方面具有效果。
</details></li>
</ul>
<hr>
<h2 id="Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models"><a href="#Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models" class="headerlink" title="Risk Assessment and Statistical Significance in the Age of Foundation Models"></a>Risk Assessment and Statistical Significance in the Age of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07132">http://arxiv.org/abs/2310.07132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</li>
<li>for: 评估基础模型中的社会技术风险，并提供量化统计 significativity 的分布式框架。</li>
<li>methods: 基于首选和第二选择风险 Statistics 的新统计测试，以及与经济数学金融中常用的均值-风险模型相关的第二颗统计学。</li>
<li>results: 通过这种框架，可以形式地开发一种基于指标集和风险权衡的模型选择策略，并通过bootstrap强度估计来证明统计 significativity。 使用这种框架，对不同的大语言模型进行风险评估，包括逸误和攻击性输出等。<details>
<summary>Abstract</summary>
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.
</details>
<details>
<summary>摘要</summary>
我们提出了一种分布式框架，用于评估基础模型中的社会技术风险，并使用量化的统计学 significado measure 评估这些风险。我们的方法基于新的第一和第二级杂度测试，这些测试可以测试实际随机变量之间的统计学上的占据关系。我们表明了这些第二级统计学与经济数学金融中常用的均值-风险模型之间的联系。使用这个框架，我们正式开发了一种具有风险意识的基础模型选择方法，给出了 guardrails 的量化指标。我们受到股票选择理论和股票估值理论的启发，定义了每个模型的 " metric 股票"，用于对多个指标进行汇总，并根据这些股票的杂度来选择模型。我们的统计学 significado measure 的有效性是基于中心假设的 asymptotic analysis 的实际应用，并通过 bootstrap variance estimate 来评估。我们使用我们的框架对不同的大语言模型进行评估，关于不遵循指令和输出恶意内容的风险。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs"><a href="#Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs" class="headerlink" title="Machine Learning Methods for Background Potential Estimation in 2DEGs"></a>Machine Learning Methods for Background Potential Estimation in 2DEGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07089">http://arxiv.org/abs/2310.07089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo da Cunha, Nobuyuki Aoki, David Ferry, Kevin Vora, Yu Zhang</li>
<li>for: 提高二dimensional电子液体（2DEG）中的瑕然和缺陷的识别和评估</li>
<li>methods: 使用扫描门微scopia（SGM）数据来估算2DEG背景电势，并应用三种不同的机器学习技术：生成对抗学习 neural network、细胞神经网络和进化搜索</li>
<li>results:  DES中的进化搜索算法能够在数据约束下提供有效的瑕然分析，这成果不仅推动了2DEG的理解，还探讨了机器学习在探测量子材料方面的潜在应用。<details>
<summary>Abstract</summary>
In the realm of quantum-effect devices and materials, two-dimensional electron gases (2DEGs) stand as fundamental structures that promise transformative technologies. However, the presence of impurities and defects in 2DEGs poses substantial challenges, impacting carrier mobility, conductivity, and quantum coherence time. To address this, we harness the power of scanning gate microscopy (SGM) and employ three distinct machine learning techniques to estimate the background potential of 2DEGs from SGM data: image-to-image translation using generative adversarial neural networks, cellular neural network, and evolutionary search. Our findings, despite data constraints, highlight the effectiveness of an evolutionary search algorithm in this context, offering a novel approach for defect analysis. This work not only advances our understanding of 2DEGs but also underscores the potential of machine learning in probing quantum materials, with implications for quantum computing and nanoelectronics.
</details>
<details>
<summary>摘要</summary>
在量子效应设备和材料的王国中，二维电子扩散（2DEG）作为基本结构，承诺引领技术的变革。然而，2DEG中的杂质和瑕疵对运载体的移动、导电性和量子准确时间产生了重大影响。为解决这一问题，我们利用扫描门镜学（SGM）的力量，并采用三种不同的机器学习技术来估计2DEG背景电气potential：图像到图像翻译使用生成对抗神经网络、细胞神经网络和进化搜索。我们的发现，尽管数据约束较为严格，但是进化搜索算法在这种情况下表现出了杰出的效果，提供了一种新的瑕疵分析方法。这项工作不仅提高了我们对2DEG的理解，也强调了机器学习在探索量子材料方面的潜在作用，对量子计算和纳米电子学产生了重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.LG_2023_10_11/" data-id="clnsn0viz00jfgf88eys8aonh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.IV_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T09:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.IV_2023_10_11/">eess.IV - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Time-Resolved-Reconstruction-of-Motion-Force-and-Stiffness-using-Spectro-Dynamic-MRI"><a href="#Time-Resolved-Reconstruction-of-Motion-Force-and-Stiffness-using-Spectro-Dynamic-MRI" class="headerlink" title="Time-Resolved Reconstruction of Motion, Force, and Stiffness using Spectro-Dynamic MRI"></a>Time-Resolved Reconstruction of Motion, Force, and Stiffness using Spectro-Dynamic MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07622">http://arxiv.org/abs/2310.07622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max H. C. van Riel, Tristan van Leeuwen, Cornelis A. T. van den Berg, Alessandro Sbrizzi</li>
<li>for: 这个论文是为了研究肌肉和关节的动态特性和物理学而设计的。</li>
<li>methods: 这个论文使用了 Spectro-Dynamic MRI 技术来直接从 k-space 数据中获取高空间和时间分辨率的动态时间序列 MRI 数据。</li>
<li>results: 这个研究表明了一种可以重建时间分辨率为 11 ms 的动态时间序列 MRI 图像、时间分辨率动态场、动态参数和活动力场的扩展 Spectro-Dynamic MRI 框架。<details>
<summary>Abstract</summary>
Measuring the dynamics and mechanical properties of muscles and joints is important to understand the (patho)physiology of muscles. However, acquiring dynamic time-resolved MRI data is challenging. We have previously developed Spectro-Dynamic MRI which allows the characterization of dynamical systems at a high spatial and temporal resolution directly from k-space data. This work presents an extended Spectro-Dynamic MRI framework that reconstructs 1) time-resolved MR images, 2) time-resolved motion fields, 3) dynamical parameters, and 4) an activation force, at a temporal resolution of 11 ms. An iterative algorithm solves a minimization problem containing four terms: a motion model relating the motion to the fully-sampled k-space data, a dynamical model describing the expected type of dynamics, a data consistency term describing the undersampling pattern, and finally a regularization term for the activation force. We acquired MRI data using a dynamic motion phantom programmed to move like an actively driven linear elastic system, from which all dynamic variables could be accurately reconstructed, regardless of the sampling pattern. The proposed method performed better than a two-step approach, where time-resolved images were first reconstructed from the undersampled data without any information about the motion, followed by a motion estimation step.
</details>
<details>
<summary>摘要</summary>
measure 肌肉和关节的动力学和机械性质是理解（病）生理学的关键。然而，获取高速度分辨率的时间分辨率MRI数据是具有挑战性。我们之前已经开发出了 спектро-动态MRI，它可以将动态系统的高 spatial和时间分辨率数据直接从k空间数据中Characterization。这项工作提出了一个扩展的спектро-动态MRI框架，可以重构1）时间分辨率MR图像，2）时间分辨率运动场，3）动力学参数，以及4）活动力量，在11ms的时间分辨率上。一个迭代算法解决了一个最小化问题，包括四个项：一个运动模型将运动与完全抽样的k空间数据相关联，一个动力学模型描述预期的动力学类型，一个数据一致项描述抽样模式，以及最后一个正则化项 для活动力量。我们使用一个动态运动phantom，模拟了一个活动驱动的线性弹簧系统，从而可以准确重构所有动态变量，无论抽样模式。提出的方法比一种两步方法更好，其首先从未完全抽样数据中重构时间分辨率图像，然后进行运动估计步骤。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.IV_2023_10_11/" data-id="clnsn0vm500t2gf88aqjm728m" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.SP_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T08:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.SP_2023_10_11/">eess.SP - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Kronecker-structured-Sparse-Vector-Recovery-with-Application-to-IRS-MIMO-Channel-Estimation"><a href="#Kronecker-structured-Sparse-Vector-Recovery-with-Application-to-IRS-MIMO-Channel-Estimation" class="headerlink" title="Kronecker-structured Sparse Vector Recovery with Application to IRS-MIMO Channel Estimation"></a>Kronecker-structured Sparse Vector Recovery with Application to IRS-MIMO Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07869">http://arxiv.org/abs/2310.07869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbin He, Geethu Joseph</li>
<li>for: 这种问题在多输入多出力系统中智能反射面帮助的频道估计中出现，它的优先顺序只是使用克罗内克结构的词典来恢复稀疏向量。</li>
<li>methods: 作者使用多个独立的子问题解决方法，每个子问题都可以独立地解决，最后通过克罗内克乘法得到稀疏向量。</li>
<li>results: 作者的方法在准确性和运行时间方面与现有方法相比表现出色，使用 синтетиче数据和频道估计应用程序。作者认为这是由于降低解决空间的降低解决空间和提高准确性的分解步骤引起的。<details>
<summary>Abstract</summary>
This paper studies the problem of Kronecker-structured sparse vector recovery from an underdetermined linear system with a Kronecker-structured dictionary. Such a problem arises in many real-world applications such as the sparse channel estimation of an intelligent reflecting surface-aided multiple-input multiple-output system. The prior art only exploits the Kronecker structure in the support of the sparse vector and solves the entire linear system together leading to high computational complexity. Instead, we break down the original sparse recovery problem into multiple independent sub-problems and solve them individually. We obtain the sparse vector as the Kronecker product of the individual solutions, retaining its structure in both support and nonzero entries. Our simulations demonstrate the superior performance of our methods in terms of accuracy and run time compared with the existing works, using synthetic data and the channel estimation application. We attribute the low run time to the reduced solution space due to the additional structure and improved accuracy to the denoising effect owing to the decomposition step.
</details>
<details>
<summary>摘要</summary>
In this paper, we break down the original sparse recovery problem into multiple independent sub-problems and solve them individually. We then obtain the sparse vector as the Kronecker product of the individual solutions, retaining its structure in both support and nonzero entries. Our simulations demonstrate that our method outperforms existing works in terms of both accuracy and run time, using synthetic data and the channel estimation application. We attribute the low run time to the reduced solution space due to the additional structure, and the improved accuracy to the denoising effect of the decomposition step.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Quantization-for-Key-Generation-in-Low-Power-Wide-Area-Networks"><a href="#Adaptive-Quantization-for-Key-Generation-in-Low-Power-Wide-Area-Networks" class="headerlink" title="Adaptive Quantization for Key Generation in Low-Power Wide-Area Networks"></a>Adaptive Quantization for Key Generation in Low-Power Wide-Area Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07853">http://arxiv.org/abs/2310.07853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Chen, Junqing Zhang, Yingying Chen</li>
<li>for: 提高资源有限的低功率宽频网络（LPWAN）的安全性，通过基于反射和随机无线通信的物理层密钥生成。</li>
<li>methods: 提议使用 adaptive 量化方案，根据Received Signal Strength Indicator（RSSI）测量值的随机性，动态调整量化参数，保证预定的密钥不一致率（KDR）。使用预训练的线性回归模型确定合适的量化级别和保障带参数。</li>
<li>results: 实验结果表明，提议的 adaptive 量化方案比均量化和固定量化方案具有更高的密钥生成速率（KGR），在 LoRa 设备上达到了最高的 2.35 $\times$ 和 1.51 $\times$ KGR 提升。<details>
<summary>Abstract</summary>
Physical layer key generation based on reciprocal and random wireless channels has been an attractive solution for securing resource-constrained low-power wide-area networks (LPWANs). When quantizing channel measurements, namely received signal strength indicator (RSSI), into key bits, the existing works mainly adopt fixed quantization levels and guard band parameters, which fail to fully extract keys from RSSI measurements. In this paper, we propose a novel adaptive quantization scheme for key generation in LPWANs, taking LoRa as a case study. The proposed adaptive quantization scheme can dynamically adjust the quantization parameters according to the randomness of RSSI measurements estimated by Lempel-Ziv complexity (LZ76), while ensuring a predefined key disagreement ratio (KDR). Specifically, our scheme uses pre-trained linear regression models to determine the appropriate quantization level and guard band parameter for each segment of RSSI measurements. Moreover, we propose a guard band parameter calibration scheme during information reconciliation during real-time key generation operation. Experimental evaluations using LoRa devices show that the proposed adaptive quantization scheme outperforms the benchmark differential quantization and fixed quantization with up to 2.35$\times$ and 1.51$\times$ key generation rate (KGR) gains, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输层密钥生成基于相对和随机无线通道的方案在资源受限的低功率宽频网络（LPWAN）中具有吸引力。当量化通道测量值（RSSI）为密钥位时，现有的工作主要采用固定量化水平和保险带参数，这会导致不能完全从RSSI测量值中提取密钥。在这篇论文中，我们提出了一种新的自适应量化方案，用于在LPWAN中生成密钥，以LoRa作为案例研究。我们的自适应量化方案可以在RSSI测量值的随机性被估计为Lempel-Ziv复杂度（LZ76）时，动态调整量化参数，以保证先定的密钥不一致率（KDR）。具体来说，我们的方案使用预训练的线性回归模型来确定每段RSSI测量值的适当量化水平和保险带参数。此外，我们还提出了在实时密钥生成过程中进行信息归一化的卡利布恰参数调整方案。实验使用LoRa设备显示，我们的自适应量化方案与参考差分量化和固定量化方案相比，可以获得最多2.35倍和1.51倍的密钥生成率增加。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Semantic-Localization-in-Highly-Dynamic-Wireless-Networks-Using-Deep-Homoscedastic-Domain-Adaptation"><a href="#Exploiting-Semantic-Localization-in-Highly-Dynamic-Wireless-Networks-Using-Deep-Homoscedastic-Domain-Adaptation" class="headerlink" title="Exploiting Semantic Localization in Highly Dynamic Wireless Networks Using Deep Homoscedastic Domain Adaptation"></a>Exploiting Semantic Localization in Highly Dynamic Wireless Networks Using Deep Homoscedastic Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07792">http://arxiv.org/abs/2310.07792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Chu, Abdullah Alghafis, Andreas F. Molisch</li>
<li>for: 本研究旨在解决无GPS的室外位置定位问题，尤其是在城市或大都会环境中的街巷中。</li>
<li>methods: 本研究使用机器学习技术来解决这一问题。</li>
<li>results: 研究人员提出了一种半结构化的 semantic localization 方法，可以同时捕捉 ligne-of-sight (LOS)、阻挡 LOS (OLOS) 和非LOS (NLOS) 三种传播条件，并且通过将这些条件作为 “semantic objects” 来确定它们，从而提高了精度和可靠性。<details>
<summary>Abstract</summary>
Localization in GPS-denied outdoor locations, such as street canyons in an urban or metropolitan environment, has many applications. Machine Learning (ML) is widely used to tackle this critical problem. One challenge lies in the mixture of line-of-sight (LOS), obstructed LOS (OLOS), and non-LOS (NLOS) conditions. In this paper, we consider a semantic localization that treats these three propagation conditions as the ''semantic objects", and aims to determine them together with the actual localization, and show that this increases accuracy and robustness. Furthermore, the propagation conditions are highly dynamic, since obstruction by cars or trucks can change the channel state information (CSI) at a fixed location over time. We therefore consider the blockage by such dynamic objects as another semantic state. Based on these considerations, we formulate the semantic localization with a joint task (coordinates regression and semantics classification) learning problem. Another problem created by the dynamics is the fact that each location may be characterized by a number of different CSIs. To avoid the need for excessive amount of labeled training data, we propose a multi-task deep domain adaptation (DA) based localization technique, training neural networks with a limited number of labeled samples and numerous unlabeled ones. Besides, we introduce novel scenario adaptive learning strategies to ensure efficient representation learning and successful knowledge transfer. Finally, we use Bayesian theory for uncertainty modeling of the importance weights in each task, reducing the need for time-consuming parameter finetuning; furthermore, with some mild assumptions, we derive the related log-likelihood for the joint task and present the deep homoscedastic DA based localization method.
</details>
<details>
<summary>摘要</summary>
地理localization在无GPS的户外场景中，如城市街区，有很多应用。机器学习（ML）广泛应用于解决这一关键问题。一个挑战在于mixture of line-of-sight（LOS）、阻挡LOS（OLOS）和非LOS（NLOS）条件。在这篇论文中，我们将这三种传播条件视为“semantic objects”，并试图同时确定它们和实际的地理位置。我们发现这会提高精度和Robustness。此外，传播条件是高度动态的，因为汽车或卡车阻挡物可以在固定位置上时间上改变通信状态信息（CSI）。我们因此将封闭物体视为另一种semantic state。基于这些考虑，我们定义了semantic localization的联合任务（坐标回归和 semantics 分类）学习问题。另外，由于动态对象的影响，每个位置可能有多个不同的CSIs。为了避免过度占用标注数据，我们提议了多任务深度适应（DA）基本地理位置技术，通过训练神经网络少量标注样本和大量未标注样本。此外，我们引入了新的enario适应学习策略，以确保效率的表征学习和成功的知识传递。最后，我们使用 bayesian理论来模型uncertainty weights的重要性，从而减少时间consuming的参数微调；此外，通过一些某些假设，我们得出了相关的log-likelihood для联合任务，并提出了深度适应DA基本地理位置技术。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Millimeter-Wave-Channel-Estimation-From-Partially-Coherent-Measurements"><a href="#Sparse-Millimeter-Wave-Channel-Estimation-From-Partially-Coherent-Measurements" class="headerlink" title="Sparse Millimeter Wave Channel Estimation From Partially Coherent Measurements"></a>Sparse Millimeter Wave Channel Estimation From Partially Coherent Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07569">http://arxiv.org/abs/2310.07569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Yi, Nitin Jonathan Myers, Geethu Joseph</li>
<li>for: 这种研究旨在开发一种毫米波通信系统中的通道估计技术。</li>
<li>methods: 该方法利用毫米波通道的稀疏结构来降低训练开销，并考虑了通道测量中的相位误差 due to phase noise。</li>
<li>results: 我们的算法可以在较低的复杂性下高精度地估计毫米波通道。<details>
<summary>Abstract</summary>
This paper develops a channel estimation technique for millimeter wave (mmWave) communication systems. Our method exploits the sparse structure in mmWave channels for low training overhead and accounts for the phase errors in the channel measurements due to phase noise at the oscillator. Specifically, in IEEE 802.11ad/ay-based mmWave systems, the phase errors within a beam refinement protocol packet are almost the same, while the errors across different packets are substantially different. Consequently, standard sparsity-aware algorithms, which ignore phase errors, fail when channel measurements are acquired over multiple beam refinement protocol packets. We present a novel algorithm called partially coherent matching pursuit for sparse channel estimation under practical phase noise perturbations. Our method iteratively detects the support of sparse signal and employs alternating minimization to jointly estimate the signal and the phase errors. We numerically show that our algorithm can reconstruct the channel accurately at a lower complexity than the benchmarks.
</details>
<details>
<summary>摘要</summary>
To address this challenge, the proposed method, called partially coherent matching pursuit, iteratively detects the support of the sparse signal and employs alternating minimization to jointly estimate the signal and the phase errors. The proposed method is shown to be effective in reconstructing the channel accurately at a lower complexity than benchmarks through numerical simulations.Here is the text in Simplified Chinese:这篇论文提出了一种滤波技术，用于毫米波通信系统的通道估计。我们的方法利用毫米波通道的稀疏结构，以降低训练负担，并考虑了普通频率噪声所导致的通道测量结果中的相位错误。特别是在IEEE 802.11ad/ay基于毫米波系统中，在ibeam refinement协议 packet中的相位错误几乎相同，而不同包中的相位错误却有很大差异。因此，标准的稀疏意识算法，忽略相位错误，在多个ibeam refinement协议 packet中的通道测量结果中失效。我们提出了一种新的算法，called partially coherent matching pursuit，它在实际相位噪声扰动下逐步检测稀疏信号的支持，并使用相对迁移来联合估计信号和相位错误。我们通过数值仿真表明，我们的算法可以在较低的复杂性下准确地估计通道。
</details></li>
</ul>
<hr>
<h2 id="Quality-of-Service-Constrained-Online-Routing-in-High-Throughput-Satellites"><a href="#Quality-of-Service-Constrained-Online-Routing-in-High-Throughput-Satellites" class="headerlink" title="Quality of Service-Constrained Online Routing in High Throughput Satellites"></a>Quality of Service-Constrained Online Routing in High Throughput Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07557">http://arxiv.org/abs/2310.07557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Bélanger, Olfa Ben Yahia, Stéphane Martel, Antoine Lesage-Landry, Gunes Karabulut Kurt</li>
<li>for:  This paper aims to address the challenge of internal routing within a singular High Throughput Satellite (HTS) to achieve high data rates while adhering to strict quality of service (QoS) constraints.</li>
<li>methods:  The paper proposes an online optimal flow allocation and scheduling method for HTSs, treating the problem as a multi-commodity flow instance with different priority data streams. The method uses a model predictive control (MPC) approach to enable adaptive routing based on current information and forecasts, minimizing packet loss and maintaining QoS.</li>
<li>results:  The paper demonstrates the efficiency and adaptability of its method through numerical simulations, showcasing performance nearly on par with the hindsight optimum and outperforming traditional methods.<details>
<summary>Abstract</summary>
High Throughput Satellites (HTSs) outpace traditional satellites due to their multi-beam transmission. The rise of low Earth orbit mega constellations amplifies HTS data rate demands to terabits/second with acceptable latency. This surge in data rate necessitates multiple modems, often exceeding single device capabilities. Consequently, satellites employ several processors, forming a complex packet-switch network. This can lead to potential internal congestion and challenges in adhering to strict quality of service (QoS) constraints. While significant research exists on constellation-level routing, a literature gap remains on the internal routing within a singular HTS. The intricacy of this internal network architecture presents a significant challenge to achieve high data rates.   This paper introduces an online optimal flow allocation and scheduling method for HTSs. The problem is treated as a multi-commodity flow instance with different priority data streams. An initial full time horizon model is proposed as a benchmark. We apply a model predictive control (MPC) approach to enable adaptive routing based on current information and the forecast within the prediction time horizon while allowing for deviation of the latter. Importantly, MPC is inherently suited to handle uncertainty in incoming flows. Our approach minimizes packet loss by optimally and adaptively managing the priority queue schedulers and flow exchanges between satellite processing modules. Central to our method is a routing model focusing on optimal priority scheduling to enhance data rates and maintain QoS. The model's stages are critically evaluated, and results are compared to traditional methods via numerical simulations. Through simulations, our method demonstrates performance nearly on par with the hindsight optimum, showcasing its efficiency and adaptability in addressing satellite communication challenges.
</details>
<details>
<summary>摘要</summary>
高通信卫星（HTS）比传统卫星更快因为它们使用多个扫描。低地球轨道宏群的出现使得 HTS 数据速率增加到 terra bits/秒，同时保持可接受的延迟。这种增加的数据速率需要多个模式，经常超过单个设备的能力。因此，卫星使用多个处理器，形成了复杂的包链路网络。这可能会导致内部堵塞和遵循严格的质量服务（QoS）限制的挑战。虽然存在许多关于宏群级别的路由研究，但关于单个 HTS 内部网络的研究 ainda 有很大的Literature gap。这种内部网络的复杂性对于实现高数据速率带来了挑战。这篇论文介绍了一种在 HTS 中线上优化流分配和调度方法。问题被视为多商品流实例，并且具有不同优先级数据流。我们提出了一个完整的全天候模型作为参考。我们采用了预测控制（MPC）方法，以便根据当前信息和预测时间 horizons 中的预测来实现可靠的路由。MPC 自然地处理入站流的不确定性。我们的方法可以最大化数据包损失，通过优化优先级调度和流 Exchange  между卫星处理模块来实现。中心于我们的方法的是一种专门关注最佳优先级调度，以提高数据速率并保持 QoS。模型的阶段被严格评估，并与传统方法进行比较via 数字实验室。通过实验，我们的方法展现了与追溯优质的性能，证明了它的效率和适应性在卫星通信中处理挑战。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Monitoring-via-Jamming-in-Fluid-Antenna-Systems"><a href="#Proactive-Monitoring-via-Jamming-in-Fluid-Antenna-Systems" class="headerlink" title="Proactive Monitoring via Jamming in Fluid Antenna Systems"></a>Proactive Monitoring via Jamming in Fluid Antenna Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07550">http://arxiv.org/abs/2310.07550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junteng Yao, Tuo Wu, Xiazhi Lai, Ming Jin, Cunhua Pan, Maged Elkashlan, Kai-Kit Wong</li>
<li>for: 本研究旨在 investigate the efficacy of utilizing fluid antenna system (FAS) at a legitimate monitor to oversee suspicious communication.</li>
<li>methods: 该监控器通过调整天线位置来减少停机概率，以提高监控性能。</li>
<li>results: 我们的算法可以寻找一个唯一优化解决方案，并且可以效率地实现通过步骤搜索法。此外，我们还提出了一种本地闭合式优化方案，可以快速地最大化监控率。实验证明，我们的方案可以舒适性比较好。<details>
<summary>Abstract</summary>
This paper investigates the efficacy of utilizing fluid antenna system (FAS) at a legitimate monitor to oversee suspicious communication. The monitor switches the antenna position to minimize its outage probability for enhancing the monitoring performance. Our objective is to maximize the average monitoring rate, whose expression involves the integral of the first-order Marcum $Q$ function. The optimization problem, as initially posed, is non-convex owing to its objective function. Nevertheless, upon substituting with an upper bound, we provide a theoretical foundation confirming the existence of a unique optimal solution for the modified problem, achievable efficiently by the bisection search method. Furthermore, we also introduce a locally closed-form optimal resolution for maximizing the average monitoring rate. Empirical evaluations confirm that the proposed schemes outperform conventional benchmarks considerably.
</details>
<details>
<summary>摘要</summary>
这份论文研究了利用流体天线系统（FAS）在合法监控器上实现疑似通信的监控性能。监控器可以调整天线位置以最小化其无线干扰机会，以提高监控性能。我们的目标是最大化监控率，其表达式包含 Marcum $Q$ 函数的一阶导数。原始问题为非对称问题，但我们可以通过替换Upper bound得到一个唯一且可行的解决方案，可以使用 bisector 搜寻法进行计算。此外，我们还提出了一个具有实用价值的对称估计方法，可以实现最大化监控率。实验评估显示，提案的方案与传统标准相比，有着很大的改善。
</details></li>
</ul>
<hr>
<h2 id="Symbol-Level-Precoding-for-Average-SER-Minimization-in-Multiuser-MISO-Systems"><a href="#Symbol-Level-Precoding-for-Average-SER-Minimization-in-Multiuser-MISO-Systems" class="headerlink" title="Symbol-Level Precoding for Average SER Minimization in Multiuser MISO Systems"></a>Symbol-Level Precoding for Average SER Minimization in Multiuser MISO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07436">http://arxiv.org/abs/2310.07436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Hongwei Hou, Wenjin Wang, Xinping Yi</li>
<li>for: 这篇论文旨在 Investigate symbol-level precoding (SLP) for high-order quadrature amplitude modulation (QAM)，以实现最小化平均符号错误率 (SER)，并利用构建性干扰 (CI) 和噪声电平来获得在全Signal-to-Noise Ratio (SNR) 范围内的优势。</li>
<li>methods: 我们首先构建了 SER 表达式，基于 transmit signal 和 scaling factor，然后将问题化为average SER 最小化问题，subject to total transmit power constraint。由于问题的目标函数是非对称的，因此解决这个问题变得困难。我们提出了 double-space alternating optimization (DSAO) 算法，以便优化两个变量在 orthogonal Stiefel manifold 和 Euclidean space 上。</li>
<li>results: 我们的实验结果表明，提出的 SLP 方案在与现有状态的 SLP 方案进行比较时，表现出了明显的性能优势。<details>
<summary>Abstract</summary>
This paper investigates symbol-level precoding (SLP) for high-order quadrature amplitude modulation (QAM) aimed at minimizing the average symbol error rate (SER), leveraging both constructive interference (CI) and noise power to gain superiority in full signal-to-noise ratio (SNR) ranges. We first construct the SER expression with respect to the transmitted signal and the rescaling factor, based on which the problem of average SER minimization subject to total transmit power constraint is further formulated. Given the non-convex nature of the objective, solving the above problem becomes challenging. Due to the differences in constraints between the transmit signal and the rescaling factor, we propose the double-space alternating optimization (DSAO) algorithm to optimize the two variables on orthogonal Stiefel manifold and Euclidean spaces, respectively. To facilitate QAM demodulation instead of affording impractical signaling overhead, we further develop a block transmission scheme to keep the rescaling factor constant within a block. Simulation results demonstrate that the proposed SLP scheme exhibits a significant performance advantage over existing state-of-the-art SLP schemes.
</details>
<details>
<summary>摘要</summary>
To begin, we derive the SER expression in relation to the transmitted signal and the rescaling factor. We then formulate the problem of minimizing the average SER subject to a total transmit power constraint. However, the non-convex nature of the objective function makes solving the problem challenging.To address this, we propose the double-space alternating optimization (DSAO) algorithm to optimize the two variables on orthogonal Stiefel manifold and Euclidean spaces, respectively. This approach allows us to effectively handle the differences in constraints between the transmit signal and the rescaling factor.Furthermore, to facilitate QAM demodulation without incurring impractical signaling overhead, we develop a block transmission scheme that keeps the rescaling factor constant within a block. Simulation results show that the proposed SLP scheme outperforms existing state-of-the-art SLP schemes.
</details></li>
</ul>
<hr>
<h2 id="IRS-Assisted-Federated-Learning-A-Broadband-Over-the-Air-Aggregation-Approach"><a href="#IRS-Assisted-Federated-Learning-A-Broadband-Over-the-Air-Aggregation-Approach" class="headerlink" title="IRS Assisted Federated Learning A Broadband Over-the-Air Aggregation Approach"></a>IRS Assisted Federated Learning A Broadband Over-the-Air Aggregation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07405">http://arxiv.org/abs/2310.07405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deyou Zhang, Ming Xiao, Zhibo Pang, Lihui Wang, H. Vincent Poor</li>
<li>for: 该论文主要研究了一种基于广泛频率上的无线 Federated Learning（FL）系统，使用智能反射面（IRS）来抗衰减和噪声。</li>
<li>methods: 该论文提出了一种基于节点选择的模型聚合策略，并使用了矩阵提升技术和差异 convex 程序来解决问题。</li>
<li>results: 研究人员通过对 MNIST 数据集进行模拟，并分析了两种不同的模型聚合策略的性能。 results 表明， weight-selection 基于的模型聚合策略可以提高学习性能，而节点选择基于的模型聚合策略可以降低模型聚合错误。<details>
<summary>Abstract</summary>
We consider a broadband over-the-air computation empowered model aggregation approach for wireless federated learning (FL) systems and propose to leverage an intelligent reflecting surface (IRS) to combat wireless fading and noise. We first investigate the conventional node-selection based framework, where a few edge nodes are dropped in model aggregation to control the aggregation error. We analyze the performance of this node-selection based framework and derive an upper bound on its performance loss, which is shown to be related to the selected edge nodes. Then, we seek to minimize the mean-squared error (MSE) between the desired global gradient parameters and the actually received ones by optimizing the selected edge nodes, their transmit equalization coefficients, the IRS phase shifts, and the receive factors of the cloud server. By resorting to the matrix lifting technique and difference-of-convex programming, we successfully transform the formulated optimization problem into a convex one and solve it using off-the-shelf solvers. To improve learning performance, we further propose a weight-selection based FL framework. In such a framework, we assign each edge node a proper weight coefficient in model aggregation instead of discarding any of them to reduce the aggregation error, i.e., amplitude alignment of the received local gradient parameters from different edge nodes is not required. We also analyze the performance of this weight-selection based framework and derive an upper bound on its performance loss, followed by minimizing the MSE via optimizing the weight coefficients of the edge nodes, their transmit equalization coefficients, the IRS phase shifts, and the receive factors of the cloud server. Furthermore, we use the MNIST dataset for simulations to evaluate the performance of both node-selection and weight-selection based FL frameworks.
</details>
<details>
<summary>摘要</summary>
我们考虑了广带无线通信 empowered 模型聚合方法 для无线联合学习（FL）系统，并利用智能反射表面（IRS）来抗衰减和噪声。我们首先调查了传统的节点选择基础framwork，其中只有一些边缘节点被排除在模型聚合中来控制聚合错误。我们分析了这种节点选择基础framwork的性能，并得出了其性能损失的Upper bound，其与选择的边缘节点相关。然后，我们寻求通过优化选择的边缘节点、它们的传输均衡系数、IRS相位偏移和云服务器接收因子来减少实际收到的MSE（平均方差）和Global gradient参数与实际接收的参数之间的差异。通过矩阵升降技术和差异 convex 编程，我们成功地将问题转换为一个 convex 问题，并使用可用的Off-the-shelf solvers解决。为了提高学习性能，我们进一步提出了一个weight-selection based FL framework。在这种框架中，我们将每个边缘节点赋予一个适当的weight coefficient在模型聚合中，而不是抛弃任何的边缘节点，以减少聚合错误。我们还分析了这种weight-selection based framework的性能，并得出了其性能损失的Upper bound，然后通过优化weight coefficient的边缘节点、传输均衡系数、IRS相位偏移和云服务器接收因子来减少MSE。此外，我们使用MNIST dataset进行了 simulations，以评估两种node-selection和weight-selection based FL frameworks的性能。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communication-enabled-Doppler-Frequency-Shift-Estimation-and-Compensation"><a href="#Integrated-Sensing-and-Communication-enabled-Doppler-Frequency-Shift-Estimation-and-Compensation" class="headerlink" title="Integrated Sensing and Communication enabled Doppler Frequency Shift Estimation and Compensation"></a>Integrated Sensing and Communication enabled Doppler Frequency Shift Estimation and Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07401">http://arxiv.org/abs/2310.07401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzhu Jia, Zhiqing Wei, Ruiyun Zhang, Lin Wang</li>
<li>for: 这篇论文是为了解决高速 vehicular 网络中 millimeter wave 技术引起的严重 Doppler Frequency Shift (DFS) 问题而写的。</li>
<li>methods: 该论文提出了一种 Integrated Sensing and Communication (ISAC) 技术，包括估算和补做 DFS，使用雷达探测来粗略估算 DFS，然后使用设计的前导序列来精准估算和补做 DFS。此外，还提出了一种可靠的 DFS 估算算法来降低计算复杂性。</li>
<li>results: 相比传统 DFS 估算算法，该提案的算法可以提高 bit error rate 和 Mean Squared Error 性能，经过 simulate 结果验证。<details>
<summary>Abstract</summary>
Despite the millimeter wave technology fulfills the low-latency and high data transmission, it will cause severe Doppler Frequency Shift (DFS) for high-speed vehicular network, which tremendously damages the communication performance. In this paper, we propose an Integrated Sensing and Communication (ISAC) enabled DFS estimation and compensation algorithm. Firstly, the DFS is coarsely estimated and compensated using radar detection. Then, the designed preamble sequence is used to accurately estimate and compensate DFS. In addition, an adaptive DFS estimator is designed to reduce the computational complexity. Compared with the traditional DFS estimation algorithm, the improvement of the proposed algorithm is verified in bit error rate and mean square error performance by simulation results.
</details>
<details>
<summary>摘要</summary>
尽管毫米波技术实现了低延迟和高数据传输，但它会导致高速交通网络中严重的多普勒频率偏移（DFS），从而极大地损害通信性能。在这篇论文中，我们提出了一种Integrated Sensing and Communication（ISAC）启用DFS估计和补偿算法。首先，使用雷达探测来粗略估计并补偿DFS。然后，采用设计的前序序列来高精度地估计并补偿DFS。此外，我们还设计了一种适应式DFS估计器，以降低计算复杂性。与传统DFS估计算法相比，我们的提案的改进被 verify了在比特错误率和平均方差性能上。
</details></li>
</ul>
<hr>
<h2 id="Extremal-Mechanisms-for-Pointwise-Maximal-Leakage"><a href="#Extremal-Mechanisms-for-Pointwise-Maximal-Leakage" class="headerlink" title="Extremal Mechanisms for Pointwise Maximal Leakage"></a>Extremal Mechanisms for Pointwise Maximal Leakage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07381">http://arxiv.org/abs/2310.07381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonhard Grosse, Sara Saeidian, Tobias Oechtering</li>
<li>for: 本文研究了在隐私限制下发布数据的机制，包括在发布数据时添加随机性以降低数据的用用性。</li>
<li>methods: 本文使用了点wise最大泄露隐私度量（PML）和一种泛化凸函数Utility函数来分析隐私Utility贸易OFF。 PML是一种最近提出的隐私度量，它基于两种等价威胁模型：一个对抗手采用随机函数的猜测，另一个对抗手尝试最大化一个总得利函数。</li>
<li>results: 本文通过分析随机响应机制的行为，得出了关于PML下的隐私Utility贸易OFF的结论。此外，本文还 derive了一些关于PML下最佳隐私Utility贸易OFF的关键解，使用了 convex分析的工具。最后，本文提出了一个可以 Compute最佳机制的线性程序。<details>
<summary>Abstract</summary>
Data publishing under privacy constraints can be achieved with mechanisms that add randomness to data points when released to an untrusted party, thereby decreasing the data's utility. In this paper, we analyze this privacy-utility tradeoff for the pointwise maximal leakage privacy measure and a general class of convex utility functions. Pointwise maximal leakage (PML) was recently proposed as an operationally meaningful privacy measure based on two equivalent threat models: An adversary guessing a randomized function and an adversary aiming to maximize a general gain function. We study the behavior of the randomized response mechanism designed for local differential privacy under different prior distributions of the private data. Motivated by the findings of this analysis, we derive several closed-form solutions for the optimal privacy-utility tradeoff in the presented PML context using tools from convex analysis. Finally, we present a linear program that can compute optimal mechanisms for PML in a general setting.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本 Publishing under privacy constraints 可以使用机制添加Randomness to data points when released to an untrusted party, thereby decreasing the data's utility. In this paper, we analyze this privacy-utility tradeoff for the pointwise maximal leakage privacy measure and a general class of convex utility functions. Pointwise maximal leakage (PML) was recently proposed as an operationally meaningful privacy measure based on two equivalent threat models: An adversary guessing a randomized function and an adversary aiming to maximize a general gain function. We study the behavior of the randomized response mechanism designed for local differential privacy under different prior distributions of the private data. Motivated by the findings of this analysis, we derive several closed-form solutions for the optimal privacy-utility tradeoff in the presented PML context using tools from convex analysis. Finally, we present a linear program that can compute optimal mechanisms for PML in a general setting.中文翻译：<<SYS>>文本在隐私限制下发布可以使用机制添加随机性到数据点，以降低数据的用用性。在这篇论文中，我们分析了隐私-用用质量的质量-利益补偿，并对点wise maximal leakage（PML）隐私度量和一般的凸Utility函数进行分析。PML是在两种等价威胁模型下提出的一种实用隐私度量：推测一个随机函数，以及一个通用获益函数。我们研究了随机响应机制在本地差分隐私下的不同先前分布下的行为。受这种分析的启发，我们 derive了一些关闭式解决方案，以便在PML上实现优化的隐私-用用质量补偿。最后，我们提出了一个可以计算优化机制的线性程序，用于在总体设置中实现PML。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Algorithmic-Framework-for-Dynamic-Compressive-Sensing"><a href="#A-Unified-Algorithmic-Framework-for-Dynamic-Compressive-Sensing" class="headerlink" title="A Unified Algorithmic Framework for Dynamic Compressive Sensing"></a>A Unified Algorithmic Framework for Dynamic Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07202">http://arxiv.org/abs/2310.07202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Liu, Yong Xia</li>
<li>for: 用于重建信号序列中自然具有层次结构的动态动态稀疏性。</li>
<li>methods: 基于信号序列的动态滤波器特性的特定统计假设，提出了一种统一的动态压缩感知框架（PLAY-CS），可以包括多种现有的动态压缩感知算法（DCS）。</li>
<li>results: 在无线通信中的动态通道跟踪等实际场景中，该框架比现有的DCS算法表现出更高的性能。<details>
<summary>Abstract</summary>
We propose a unified dynamic tracking algorithmic framework (PLAY-CS) to reconstruct signal sequences with their intrinsic structured dynamic sparsity. By capitalizing on specific statistical assumptions concerning the dynamic filter of the signal sequences, the proposed framework exhibits versatility by encompassing various existing dynamic compressive sensing (DCS) algorithms. This is achieved through the incorporation of a newly proposed Partial-Laplacian filtering sparsity model, tailored to capture a more sophisticated dynamic sparsity. In practical scenarios such as dynamic channel tracking in wireless communications, the framework demonstrates enhanced performance compared to existing DCS algorithms.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:我们提出一个统一的动态跟踪算法框架（PLAY-CS），用于重建信号序列的内在结构化动态稀烈性。通过利用信号序列的动态滤波器特性的统计假设，我们的框架能够包含现有的动态压缩感知（DCS）算法。这是通过新提出的partial-laplacian filtering稀烈性模型来实现的，这种模型是用于捕捉更加复杂的动态稀烈性。在无线通信中的动态频道跟踪等实际应用场景中，我们的框架能够与现有的DCS算法相比，显示出更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Input-Output-Relation-and-Low-Complexity-Receiver-Design-for-CP-OTFS-Systems-with-Doppler-Squint"><a href="#Input-Output-Relation-and-Low-Complexity-Receiver-Design-for-CP-OTFS-Systems-with-Doppler-Squint" class="headerlink" title="Input-Output Relation and Low-Complexity Receiver Design for CP-OTFS Systems with Doppler Squint"></a>Input-Output Relation and Low-Complexity Receiver Design for CP-OTFS Systems with Doppler Squint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07200">http://arxiv.org/abs/2310.07200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuehan Wang, Xu Shi, Jintao Wang, Jian Song</li>
<li>for: This paper focuses on the impact of frequency-dependent Doppler (DSE) on orthogonal time frequency space (OTFS) systems, and proposes a practical low-complexity receiver design considering DSE.</li>
<li>methods: The proposed receiver design uses cyclic prefix time guard interval (CP-OFDM) based OTFS systems and derives the input-output relation considering DSE. Channel estimation is performed using two prefix OFDM symbols, and a linear equalization scheme is adopted taking the block diagonal property of the channel matrix into account.</li>
<li>results: Simulation results confirm the significance of DSE and the considerable performance of the proposed low-complexity receiver scheme considering DSE.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文关注了时频空间的垂直偏移（DSE）对 orthogonal time frequency space（OTFS）系统的影响，并提出了一种实用的低复杂度接收机制。</li>
<li>methods: 提议的接收机制使用 cyclic prefix time guard interval（CP-OFDM）基于的 OTFS 系统，并 derivation 输入输出关系 considering DSE。通道估计使用两个 prefix OFDM 符号，并采用了一种简单的平衡化平均（LE）方案，使用了块对称性的通道矩阵。</li>
<li>results: 模拟结果证明 DSE 的重要性，以及提议的低复杂度接收机制对 DSE 的较好性能。<details>
<summary>Abstract</summary>
In orthogonal time frequency space (OTFS) systems, the impact of frequency-dependent Doppler which is referred to as the Doppler squint effect (DSE) is accumulated through longer duration, whose negligence has prevented OTFS systems from exploiting the performance superiority. In this paper, practical OFDM system using cyclic prefix time guard interval (CP-OFDM)-based OTFS systems with DSE are adopted. Cyclic prefix (CP) length is analyzed while the input-output relation considering DSE is derived. By deploying two prefix OFDM symbols, the channel estimation can be easily divided into three parts as delay detection, Doppler extraction and gain estimation. The linear equalization scheme is adopted taking the block diagonal property of the channel matrix into account, which completes the low-complexity receiver design. Simulation results confirm the significance of DSE and the considerable performance of the proposed low-complexity receiver scheme considering DSE.
</details>
<details>
<summary>摘要</summary>
在orthogonal time frequency space（OTFS）系统中，频率依赖型Doppler效应（DSE）的影响是通过更长的时间间隔积累的，而这种忽略了OTFS系统的性能优势。在这篇论文中，我们采用了基于CP-OFDM的OTFS系统，其中CP（cyclic prefix）长度进行分析，并 derive了输入输出关系。通过发送两个Prefix OFDM符号，渠道估计可以被简单地分解为三部分：延迟探测、Doppler提取和功率估计。采用了线性归一化方案，利用通道矩阵的块对称性，实现了低复杂度接收器的设计。实验结果证明了DSE的重要性和我们提出的低复杂度接收器设计的有效性。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communication-enabled-Multiple-Base-Stations-Cooperative-Sensing-Towards-6G"><a href="#Integrated-Sensing-and-Communication-enabled-Multiple-Base-Stations-Cooperative-Sensing-Towards-6G" class="headerlink" title="Integrated Sensing and Communication enabled Multiple Base Stations Cooperative Sensing Towards 6G"></a>Integrated Sensing and Communication enabled Multiple Base Stations Cooperative Sensing Towards 6G</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07180">http://arxiv.org/abs/2310.07180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Wangjun Jiang, Zhiyong Feng, Huici Wu, Ning Zhang, Kaifeng Han, Ruizhong Xu, Ping Zhang</li>
<li>for: 提供6G移动通信系统的智能应用，如智能城市和自动驾驶，需要 integrating sensing and communication (ISAC) 技术。</li>
<li>methods: 提出了一种基于多BS合作探测的解决方案，以提高 ISAC 技术的探测范围和精度。</li>
<li>results: 提供了一系列的实现方案和性能评估结果，证明了多BS合作探测的效iveness。<details>
<summary>Abstract</summary>
Driven by the intelligent applications of sixth-generation (6G) mobile communication systems such as smart city and autonomous driving, which connect the physical and cyber space, the integrated sensing and communication (ISAC) brings a revolutionary change to the base stations (BSs) of 6G by integrating radar sensing and communication in the same hardware and wireless resource. However, with the requirements of long-range and accurate sensing in the applications of smart city and autonomous driving, the ISAC enabled single BS still has a limitation in the sensing range and accuracy. With the networked infrastructures of mobile communication systems, multi-BS cooperative sensing is a natural choice satisfying the requirement of long-range and accurate sensing. In this article, the framework of multi-BS cooperative sensing is proposed, breaking through the limitation of single-BS sensing. The enabling technologies, including unified ISAC performance metrics, ISAC signal design and optimization, interference management, cooperative sensing algorithms, are introduced in details. The performance evaluation results are provided to verify the effectiveness of multi-BS cooperative sensing schemes. With ISAC enabled multi-BS cooperative sensing (ISAC-MCS), the intelligent infrastructures connecting physical and cyber space can be established, ushering the era of 6G promoting the intelligence of everything.
</details>
<details>
<summary>摘要</summary>
驱动了第六代移动通信系统（6G）的智能应用，如智能城市和自动驾驶，这些应用将物理空间和虚拟空间相连接，使得基站（BS）的集成感知和通信（ISAC）引入了革命性的变革。然而，在智能城市和自动驾驶应用中，具有覆盖范围和准确探测的长距离感知需求，单个BS仍存在限制。为了满足这些需求， mobile communication systems的网络基础设施中的多BS合作感知是一个自然的选择。在这篇文章中，我们提出了多BS合作感知框架，突破了单个BS感知的限制。我们还介绍了各种各样的技术，包括统一的ISAC性能指标、ISAC信号设计优化、干扰管理和合作感知算法。我们还提供了性能评估结果，以证明多BS合作感知方案的有效性。通过ISAC启用的多BS合作感知（ISAC-MCS），我们可以建立智能连接物理和虚拟空间的基础设施，进入6G时代，推动智能化的 Everything。
</details></li>
</ul>
<hr>
<h2 id="Time-and-Frequency-Offset-Estimation-and-Intercarrier-Interference-Cancellation-for-AFDM-Systems"><a href="#Time-and-Frequency-Offset-Estimation-and-Intercarrier-Interference-Cancellation-for-AFDM-Systems" class="headerlink" title="Time and Frequency Offset Estimation and Intercarrier Interference Cancellation for AFDM Systems"></a>Time and Frequency Offset Estimation and Intercarrier Interference Cancellation for AFDM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07141">http://arxiv.org/abs/2310.07141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankun Tang, Anjie Zhang, Miaowen Wen, Qianqian Wang, Fei Ji, Jinming Wen</li>
<li>for: 这个论文旨在提出一种可靠的时变通道通信解决方案，使用了新的射频分配多普逻论文（AFDM）。</li>
<li>methods: 该论文提出了两种最大 LIKElihood（ML）估计器，一个是joint ML估计器，它通过比较样本之间的相关性来估计symbol时偏移和扬谱频率偏移；另一个是stepwise ML估计器，它可以降低复杂度。这两种估计器都利用了AFDM符号中的征频征periodic prefix中的冗余信息，因此无需添加任何预测符。</li>
<li>results: 数值结果表明，提出的时间和频率偏移估计方法和mirror-mapping基于的调制方法都有效地 mitigate了AFDM系统中的Intercarrier interference。<details>
<summary>Abstract</summary>
Affine frequency division multiplexing (AFDM) is an emerging multicarrier waveform that offers a potential solution for achieving reliable communication for time-varying channels. This paper proposes two maximum likelihood (ML) estimators of symbol time offset and carrier frequency offset for AFDM systems. The joint ML estimator evaluates the arrival time and frequency offset by comparing the correlations of samples. Moreover, we propose the stepwise ML estimator to reduce the complexity. The proposed estimators exploit the redundant information contained within the chirp-periodic prefix inherent in AFDM symbols, thus dispensing with any additional pilots. To further mitigate the intercarrier interference resulting from the residual frequency offset, we design a mirror-mappingbased scheme for AFDM systems. Numerical results verify the effectiveness of the proposed time and frequency offset estimation criteria and the mirror-mapping-based modulation for AFDM systems.
</details>
<details>
<summary>摘要</summary>
“Affine频率分多路干扰（AFDM）是一种emerging multicarrier波形，它提供了可靠的通信 для时间变化的通道。本文提出了两个最大 LIKELIHOOD（ML）估计器：一个是合并ML估计器，它评估了时间和频率偏移的到达时间和频率偏移，通过比较样本之间的相关性。另一个是分步ML估计器，它可以降低复杂性。提出的估计器利用AFDM符号中内置的滑块- périodique prefix中的重复信息，因此不需要额外的导频。此外，我们还设计了一个mirror-mapping基于的干扰 mitigation scheme，以减少AFDM系统中的余频偏移干扰。 numerics results表明，提出的时间和频率偏移估计准确性和mirror-mapping基于的干扰mitigation scheme均具有高效性。”Note: Simplified Chinese is used in this translation, which is a more informal and spoken version of Chinese. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Edge-Cloud-Collaborative-Stream-Computing-for-Real-Time-Structural-Health-Monitoring"><a href="#Edge-Cloud-Collaborative-Stream-Computing-for-Real-Time-Structural-Health-Monitoring" class="headerlink" title="Edge Cloud Collaborative Stream Computing for Real-Time Structural Health Monitoring"></a>Edge Cloud Collaborative Stream Computing for Real-Time Structural Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07130">http://arxiv.org/abs/2310.07130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhao Zhang, Cheng Guo, Yi Gao, Wei Dong</li>
<li>for: 这个研究旨在提高结构健康监控（SHM）的效率和可靠性，应对大量数据和实时需求的挑战。</li>
<li>methods: 本研究提出了一个Edge Cloud共同协力精细流操作排程框架（ECStream），考虑了原子和composite操作的联合运算，以减少网络带宽和终端操作处理延迟。</li>
<li>results: 根据初步评估结果，ECStream可以有效对比网络带宽和终端操作处理延迟进行平衡，实现73.01%的网络带宽减少和34.08%的终端操作处理延迟减少。<details>
<summary>Abstract</summary>
Structural Health Monitoring (SHM) is crucial for the safety and maintenance of various infrastructures. Due to the large amount of data generated by numerous sensors and the high real-time requirements of many applications, SHM poses significant challenges. Although the cloud-centric stream computing paradigm opens new opportunities for real-time data processing, it consumes too much network bandwidth. In this paper, we propose ECStream, an Edge Cloud collaborative fine-grained stream operator scheduling framework for SHM. We collectively consider atomic and composite operators together with their iterative computability to model and formalize the problem of minimizing bandwidth usage and end-to-end operator processing latency. Preliminary evaluation results show that ECStream can effectively balance bandwidth usage and end-to-end operator computation latency, reducing bandwidth usage by 73.01% and latency by 34.08% on average compared to the cloud-centric approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输文本到 Simplified Chinese 版本。<</SYS>>基础设施安全维护（SHM）对多种基础设施的安全和维护是关键。由于众多感知器生成的大量数据以及许多应用程序的实时需求，SHM带来了 significativetranslation missing 挑战。虽然云 computing  paradigm 开创了实时数据处理的新机遇，但它占用了太多网络带宽。在这篇论文中，我们提出了 ECStream，一个Edge Cloud 共同 fine-grained stream operator scheduling 框架 для SHM。我们一起考虑原子和复合运算符的聚合计算可能性，以形式化和模型 SHM 中减少网络带宽使用和终端操作计算延迟的问题。初步评估结果表明，ECStream 可以有效均衡网络带宽使用和终端操作计算延迟，减少网络带宽使用率73.01%，计算延迟率34.08%的平均值，相比云 computing 方法。
</details></li>
</ul>
<hr>
<h2 id="Decentralization-of-Energy-Systems-with-Blockchain-Bridging-Top-down-and-Bottom-up-Management-of-the-Electricity-Grid"><a href="#Decentralization-of-Energy-Systems-with-Blockchain-Bridging-Top-down-and-Bottom-up-Management-of-the-Electricity-Grid" class="headerlink" title="Decentralization of Energy Systems with Blockchain: Bridging Top-down and Bottom-up Management of the Electricity Grid"></a>Decentralization of Energy Systems with Blockchain: Bridging Top-down and Bottom-up Management of the Electricity Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07103">http://arxiv.org/abs/2310.07103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakshi Mishra, Roohallah Khatami, Yu Christine Chen<br>for:这篇论文目的是提出一个可信的构思来重整现有的电力系统操作架构，以应对现在的分散化趋势，并强调对等价交易的支持。methods:这篇论文使用了分散式实时网络技术（Blockchain），以促进分散化能源系统的运行，并实现对等价交易的促进。results:这篇论文预期可以透过分散式实时网络技术实现更加高效和可靠的电力系统运行，并提供一个可信的构思来整合现有的中央化操作架构和分散化操作架构。<details>
<summary>Abstract</summary>
For more than a century, the grid has operated in a centralized top-down fashion. However, as distributed energy resources (DERs) penetration grows, the grid edge is increasingly infused with intelligent computing and communication capabilities. Thus, the bottom-up approach to grid operations inclined toward decentralizing energy systems will likely gain momentum alongside the existing centralized paradigm. Decentralization refers to transferring control and decision-making from a centralized entity (individual, organization, or group thereof) to a distributed network. It is not a new concept - in energy systems context or otherwise. In the energy systems context, however, the complexity of this multifaceted concept increases manifolds due to two major reasons - i) the nature of the commodity being traded (the electricity) and ii) the enormity of the traditional electricity sector's structure that builds, operates, and maintains this capital-intensive network. In this work, we aim to highlight the need for and outline a credible path toward restructuring the current operational architecture of the electricity grid in view of the ongoing decentralization trends with an emphasis on peer-to-peer energy trading. We further introduce blockchain technology in the context of decentralized energy systems problems. We also suggest that blockchain is an effective technology for facilitating the synergistic operations of top-down and bottom-up approaches to grid management.
</details>
<details>
<summary>摘要</summary>
for more than a century, the grid has operated in a centralized top-down fashion. however, as distributed energy resources (DERs) penetration grows, the grid edge is increasingly infused with intelligent computing and communication capabilities. thus, the bottom-up approach to grid operations inclined toward decentralizing energy systems will likely gain momentum alongside the existing centralized paradigm. decentralization refers to transferring control and decision-making from a centralized entity (individual, organization, or group thereof) to a distributed network. it is not a new concept - in energy systems context or otherwise. in the energy systems context, however, the complexity of this multifaceted concept increases manifolds due to two major reasons - i) the nature of the commodity being traded (the electricity) and ii) the enormity of the traditional electricity sector's structure that builds, operates, and maintains this capital-intensive network. in this work, we aim to highlight the need for and outline a credible path toward restructuring the current operational architecture of the electricity grid in view of the ongoing decentralization trends with an emphasis on peer-to-peer energy trading. we further introduce blockchain technology in the context of decentralized energy systems problems. we also suggest that blockchain is an effective technology for facilitating the synergistic operations of top-down and bottom-up approaches to grid management.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Arrays-How-Many-RF-Chains-Are-Required-to-Prevent-Beam-Squint"><a href="#Hybrid-Arrays-How-Many-RF-Chains-Are-Required-to-Prevent-Beam-Squint" class="headerlink" title="Hybrid Arrays: How Many RF Chains Are Required to Prevent Beam Squint?"></a>Hybrid Arrays: How Many RF Chains Are Required to Prevent Beam Squint?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07101">http://arxiv.org/abs/2310.07101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heedong Do, Namyoon Lee, Robert W. Heath Jr, Angel Lozano</li>
<li>for: 解决针对干扰的干扰平面形成问题</li>
<li>methods: 使用混合阵列，true时间延迟每个天线元件</li>
<li>results: 比较于完全数字阵列，混合阵列在一定的阈值下可以实现相同的性能，并且可以使用不同的干扰空间架构。<details>
<summary>Abstract</summary>
With increasing frequencies, bandwidths, and array apertures, the phenomenon of beam squint arises as a serious impairment to beamforming. Fully digital arrays with true time delay per antenna element are a potential solution, but they require downconversion at each element. This paper shows that hybrid arrays can perform essentially as well as digital arrays once the number of radio-frequency chains exceeds a certain threshold that is far below the number of elements. The result is robust, holding also for suboptimum but highly appealing beamspace architectures.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:随着频率、宽扩展和天线大小的增加，束缩现象在扫描中变得越来越严重。完全数字天线阵列可以解决这个问题，但它们需要每个天线元件上的真实时间延迟。这篇论文显示，混合阵列在 Radio-frequency 链数超过一定的阈值时，可以与数字阵列performessentially相同，并且这个结果是稳定的，并且适用于优化但具有吸引力的扫描建筑。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.SP_2023_10_11/" data-id="clnsn0vmm00ufgf888atz4d7e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.SD_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T15:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/cs.SD_2023_09_21/">cs.SD - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Variational-Quantum-Harmonizer-Generating-Chord-Progressions-and-Other-Sonification-Methods-with-the-VQE-Algorithm"><a href="#Variational-Quantum-Harmonizer-Generating-Chord-Progressions-and-Other-Sonification-Methods-with-the-VQE-Algorithm" class="headerlink" title="Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm"></a>Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12254">http://arxiv.org/abs/2309.12254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Vitor Itaboraí, Tim Schwägerl, María Aguado Yáñez, Arianna Crippa, Karl Jansen, Eduardo Reck Miranda, Peter Thomas</li>
<li>for: This paper explores the use of physical-based sonification to visualize and understand the optimization process of Quadratic Unconstrained Binary Optimization (QUBO) problems, which are solved using the Variational Quantum Eigensolver (VQE) algorithm.</li>
<li>methods: The paper uses a musical interface prototype called Variational Quantum Harmonizer (VQH) to sonify the optimization process, which involves using intermediary statevectors to create musical elements such as chords, chord progressions, and arpeggios.</li>
<li>results: The paper demonstrates the potential of using sonification to enhance data visualization and create artistic pieces, and shows how flexible mapping strategies can supply a broad portfolio of sounds for QUBO and quantum-inspired musical compositions. Additionally, the paper highlights the relevance of the methodology for artists to gain intuition towards achieving a desired musical sound by carefully designing QUBO cost functions.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文探讨了使用物理基于的听韵来可见化和理解量子计算机解决的Quadratic Unconstrained Binary Optimization (QUBO) 问题的优化过程。</li>
<li>methods: 这篇论文使用了一个名为Variational Quantum Harmonizer (VQH)的乐interface进行听韵，通过使用中间状态向量来创造音乐元素，如和弦、旋律和arpeggios。</li>
<li>results: 这篇论文显示了使用听韵可以增强数据可视化和创作艺术作品，并表明了可变映射策略可以为QUBO和量子听韵作品提供广泛的音色。此外，论文还 highlights了这种方法的创作意义，可以帮助艺术家更好地理解和实现想要的音乐声色。<details>
<summary>Abstract</summary>
This work investigates a case study of using physical-based sonification of Quadratic Unconstrained Binary Optimization (QUBO) problems, optimized by the Variational Quantum Eigensolver (VQE) algorithm. The VQE approximates the solution of the problem by using an iterative loop between the quantum computer and a classical optimization routine. This work explores the intermediary statevectors found in each VQE iteration as the means of sonifying the optimization process itself. The implementation was realised in the form of a musical interface prototype named Variational Quantum Harmonizer (VQH), providing potential design strategies for musical applications, focusing on chords, chord progressions, and arpeggios. The VQH can be used both to enhance data visualization or to create artistic pieces. The methodology is also relevant in terms of how an artist would gain intuition towards achieving a desired musical sound by carefully designing QUBO cost functions. Flexible mapping strategies could supply a broad portfolio of sounds for QUBO and quantum-inspired musical compositions, as demonstrated in a case study composition, "Dependent Origination" by Peter Thomas and Paulo Itaborai.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这项研究探讨了使用物理基于的SONIFICATION方法来解决Quadratic Unconstrained Binary Optimization (QUBO)问题，使用Variational Quantum Eigensolver (VQE)算法进行优化。VQE算法使用了一个迭代循环来实现解决方案，并使用每个VQE迭代的中间状态向量来SONIFICATIONOptimization过程本身。这种实现形式为音乐界面原型Variational Quantum Harmonizer (VQH)，提供了可能的设计策略 для音乐应用，主要关注于和arpeggios。VQH可以用来增强数据视觉或创作艺术作品。此方法也有用于艺术家如何通过设计QUBO成本函数来获得感知到所需的音乐声色的概念。可以采用灵活的映射策略来供应QUBO和量子启发的各种音色，如在case study作品"Dependent Origination" by Peter Thomas和 Paulo Itaborai中所示。
</details></li>
</ul>
<hr>
<h2 id="A-Multiscale-Autoencoder-MSAE-Framework-for-End-to-End-Neural-Network-Speech-Enhancement"><a href="#A-Multiscale-Autoencoder-MSAE-Framework-for-End-to-End-Neural-Network-Speech-Enhancement" class="headerlink" title="A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement"></a>A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12121">http://arxiv.org/abs/2309.12121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bengt J. Borgstrom, Michael S. Brandstein</li>
<li>for: 提高单通道语音增强的性能</li>
<li>methods: 使用mask-basedArchitecture和多尺度自编码器</li>
<li>results: 相比传统方法，提高了对话质量指标和自动语音识别精度<details>
<summary>Abstract</summary>
Neural network approaches to single-channel speech enhancement have received much recent attention. In particular, mask-based architectures have achieved significant performance improvements over conventional methods. This paper proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural network speech enhancement. The MSAE performs spectral decomposition of an input waveform within separate band-limited branches, each operating with a different rate and scale, to extract a sequence of multiscale embeddings. The proposed framework features intuitive parameterization of the autoencoder, including a flexible spectral band design based on the Constant-Q transform. Additionally, the MSAE is constructed entirely of differentiable operators, allowing it to be implemented within an end-to-end neural network, and be discriminatively trained. The MSAE draws motivation both from recent multiscale network topologies and from traditional multiresolution transforms in speech processing. Experimental results show the MSAE to provide clear performance benefits relative to conventional single-branch autoencoders. Additionally, the proposed framework is shown to outperform a variety of state-of-the-art enhancement systems, both in terms of objective speech quality metrics and automatic speech recognition accuracy.
</details>
<details>
<summary>摘要</summary>
神经网络方法对单通道语音增强 Received much recent attention. In particular, 面积基 architecture has achieved significant performance improvements over conventional methods. This paper proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural network speech enhancement. The MSAE performs spectral decomposition of an input waveform within separate band-limited branches, each operating with a different rate and scale, to extract a sequence of multiscale embeddings. The proposed framework features intuitive parameterization of the autoencoder, including a flexible spectral band design based on the Constant-Q transform. Additionally, the MSAE is constructed entirely of differentiable operators, allowing it to be implemented within an end-to-end neural network, and be discriminatively trained. The MSAE draws motivation both from recent multiscale network topologies and from traditional multiresolution transforms in speech processing. Experimental results show the MSAE to provide clear performance benefits relative to conventional single-branch autoencoders. Additionally, the proposed framework is shown to outperform a variety of state-of-the-art enhancement systems, both in terms of objective speech quality metrics and automatic speech recognition accuracy.
</details></li>
</ul>
<hr>
<h2 id="Is-the-Ideal-Ratio-Mask-Really-the-Best-–-Exploring-the-Best-Extraction-Performance-and-Optimal-Mask-of-Mask-based-Beamformers"><a href="#Is-the-Ideal-Ratio-Mask-Really-the-Best-–-Exploring-the-Best-Extraction-Performance-and-Optimal-Mask-of-Mask-based-Beamformers" class="headerlink" title="Is the Ideal Ratio Mask Really the Best? – Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers"></a>Is the Ideal Ratio Mask Really the Best? – Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12065">http://arxiv.org/abs/2309.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsuo Hiroe, Katsutoshi Itoyama, Kazuhiro Nakadai</li>
<li>for: 这个研究旨在investigate mask-based beamformers (BFs), which estimate filters to extract target speech using time-frequency masks.</li>
<li>methods: 研究使用四种mask-based BFs：最大信号响应率BF、两种其变体，以及多通道wiener filter（MWF）BF。为每个utterance获取最佳mask，使得BF输出与目标语音之间的平均方差最小。</li>
<li>results: 经过实验 validate that the four BFs have the same peak performance as the ideal MWF BF, but the optimal mask depends on the adopted BF and differs from the IRM. 这些结论与传统的想法不同，即最佳mask是共同的forall BFs，并且每个BF的 peak performance不同。<details>
<summary>Abstract</summary>
This study investigates mask-based beamformers (BFs), which estimate filters to extract target speech using time-frequency masks. Although several BF methods have been proposed, the following aspects are yet to be comprehensively investigated. 1) Which BF can provide the best extraction performance in terms of the closeness of the BF output to the target speech? 2) Is the optimal mask for the best performance common for all BFs? 3) Is the ideal ratio mask (IRM) identical to the optimal mask? Accordingly, we investigate these issues considering four mask-based BFs: the maximum signal-to-noise ratio BF, two variants of this, and the multichannel Wiener filter (MWF) BF. To obtain the optimal mask corresponding to the peak performance for each BF, we employ an approach that minimizes the mean square error between the BF output and target speech for each utterance. Via the experiments with the CHiME-3 dataset, we verify that the four BFs have the same peak performance as the upper bound provided by the ideal MWF BF, whereas the optimal mask depends on the adopted BF and differs from the IRM. These observations differ from the conventional idea that the optimal mask is common for all BFs and that peak performance differs for each BF. Hence, this study contributes to the design of mask-based BFs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Which BF can provide the best extraction performance in terms of the closeness of the BF output to the target speech?2. Is the optimal mask for the best performance common for all BFs?3. Is the ideal ratio mask (IRM) identical to the optimal mask?To answer these questions, we consider four mask-based BFs: the maximum signal-to-noise ratio BF, two variants of this, and the multichannel Wiener filter (MWF) BF. We use an approach to obtain the optimal mask corresponding to the peak performance for each BF by minimizing the mean square error between the BF output and target speech for each utterance.Our experiments with the CHiME-3 dataset show that the four BFs have the same peak performance as the upper bound provided by the ideal MWF BF, but the optimal mask depends on the adopted BF and differs from the IRM. These observations differ from the conventional idea that the optimal mask is common for all BFs and that peak performance differs for each BF. Therefore, this study contributes to the design of mask-based BFs.</details></li>
</ol>
<hr>
<h2 id="Improving-Language-Model-Based-Zero-Shot-Text-to-Speech-Synthesis-with-Multi-Scale-Acoustic-Prompts"><a href="#Improving-Language-Model-Based-Zero-Shot-Text-to-Speech-Synthesis-with-Multi-Scale-Acoustic-Prompts" class="headerlink" title="Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts"></a>Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11977">http://arxiv.org/abs/2309.11977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Lei, Yixuan Zhou, Liyang Chen, Dan Luo, Zhiyong Wu, Xixin Wu, Shiyin Kang, Tao Jiang, Yahui Zhou, Yuxing Han, Helen Meng</li>
<li>for: 这个论文旨在提出一个可以复制无见的话者声音的零数据文本读取与Synthesis系统（TTS）。</li>
<li>methods: 这个方法使用了语言模型来模型语音波形的数据几何，并且使用了一个新的话者感知文本编码器来学习个人话语风格。</li>
<li>results: 实验结果显示，该方法可以与基准相比，提高自然性和话者相似性，并且可以通过规模化style prompt来提高表演。<details>
<summary>Abstract</summary>
Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.
</details>
<details>
<summary>摘要</summary>
<<SYS>>zero-shot文本至语音（TTS）合成targets any unseen speaker's voice without adaptation parameters. Byquantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.<</SYS>>Here's the translation in Traditional Chinese:<<SYS>>zero-shot文本至语音（TTS）合成targets any unseen speaker's voice without adaptation parameters. Byquantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Multi-Channel-MOSRA-Mean-Opinion-Score-and-Room-Acoustics-Estimation-Using-Simulated-Data-and-a-Teacher-Model"><a href="#Multi-Channel-MOSRA-Mean-Opinion-Score-and-Room-Acoustics-Estimation-Using-Simulated-Data-and-a-Teacher-Model" class="headerlink" title="Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model"></a>Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11976">http://arxiv.org/abs/2309.11976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jozef Coldenhoff, Andrew Harper, Paul Kendrick, Tijana Stojkovic, Milos Cernak</li>
<li>for: 预测房间听音参数和语音质量指标</li>
<li>methods: 使用多通道模型进行同时预测多个录音设备的房间听音参数和语音质量指标</li>
<li>results: 比单通道模型提高直接听音比率、清晰度和语音传输指标的预测，并且需要约5倍 menos计算资源，但是其他指标的性能减少不大<details>
<summary>Abstract</summary>
Previous methods for predicting room acoustic parameters and speech quality metrics have focused on the single-channel case, where room acoustics and Mean Opinion Score (MOS) are predicted for a single recording device. However, quality-based device selection for rooms with multiple recording devices may benefit from a multi-channel approach where the descriptive metrics are predicted for multiple devices in parallel. Following our hypothesis that a model may benefit from multi-channel training, we develop a multi-channel model for joint MOS and room acoustics prediction (MOSRA) for five channels in parallel. The lack of multi-channel audio data with ground truth labels necessitated the creation of simulated data using an acoustic simulator with room acoustic labels extracted from the generated impulse responses and labels for MOS generated in a student-teacher setup using a wav2vec2-based MOS prediction model. Our experiments show that the multi-channel model improves the prediction of the direct-to-reverberation ratio, clarity, and speech transmission index over the single-channel model with roughly 5$\times$ less computation while suffering minimal losses in the performance of the other metrics.
</details>
<details>
<summary>摘要</summary>
先前的方法只是针对单通道情况进行预测， Room acoustics 和 Mean Opinion Score (MOS) 的预测都是基于单个录音设备。然而，基于质量的设备选择可能会受益于多通道方法，因为模型可能会从多个设备的描述性度量中受益。根据我们的假设，一个模型可能会从多个通道的训练中受益，因此我们开发了一个同时预测 MOS 和 Room acoustics 的多通道模型（MOSRA），对五个通道进行并行预测。由于没有多个渠道的音频数据有ground truth标签，我们使用一个声学模拟器生成了带有房间响应标签的 simulated data，并使用 wav2vec2-based MOS 预测模型生成了 MOS 标签。我们的实验表明，多通道模型在直接响应比、清晰度和语音传输指数方面的预测比单通道模型提高了大约5倍，而且计算量相对减少了大约5倍，而且减少了其他指标的性能。
</details></li>
</ul>
<hr>
<h2 id="Cluster-based-pruning-techniques-for-audio-data"><a href="#Cluster-based-pruning-techniques-for-audio-data" class="headerlink" title="Cluster-based pruning techniques for audio data"></a>Cluster-based pruning techniques for audio data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11922">http://arxiv.org/abs/2309.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Bergsma, Marta Brzezinska, Oleg V. Yazyev, Milos Cernak</li>
<li>for: 针对 audio 频段数据进行减少计算开销，提高深度学习模型的计算效率。</li>
<li>methods: 使用 k-means 聚合算法对数据进行 clustering 分析，以实现数据减少。</li>
<li>results: 对关键词检测（KWS）数据集进行 clustering 分析，发现可以使用 k-means 聚合算法减少 audio 数据集大小，保持分类性能。同时，通过缩放分析对大量样本进行优化采样，提高计算效率。<details>
<summary>Abstract</summary>
Deep learning models have become widely adopted in various domains, but their performance heavily relies on a vast amount of data. Datasets often contain a large number of irrelevant or redundant samples, which can lead to computational inefficiencies during the training. In this work, we introduce, for the first time in the context of the audio domain, the k-means clustering as a method for efficient data pruning. K-means clustering provides a way to group similar samples together, allowing the reduction of the size of the dataset while preserving its representative characteristics. As an example, we perform clustering analysis on the keyword spotting (KWS) dataset. We discuss how k-means clustering can significantly reduce the size of audio datasets while maintaining the classification performance across neural networks (NNs) with different architectures. We further comment on the role of scaling analysis in identifying the optimal pruning strategies for a large number of samples. Our studies serve as a proof-of-principle, demonstrating the potential of data selection with distance-based clustering algorithms for the audio domain and highlighting promising research avenues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Impact-of-Silence-on-Speech-Anti-Spoofing"><a href="#The-Impact-of-Silence-on-Speech-Anti-Spoofing" class="headerlink" title="The Impact of Silence on Speech Anti-Spoofing"></a>The Impact of Silence on Speech Anti-Spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11827">http://arxiv.org/abs/2309.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Zhuo Li, Jingze Lu, Hua Hua, Wenchao Wang, Pengyuan Zhang</li>
<li>for: 这篇论文主要研究了防御诈骗攻击的影响，具体来说是研究了 silence 的影响。</li>
<li>methods: 本论文使用了 Voice Activity Detection (VAD) 技术和 Class Activation Mapping (CAM) Visualization 来分析诈骗攻击的影响。</li>
<li>results: 研究发现，去掉 silence 会使防御诈骗攻击表现下降，并且不同的 waveform generator 生成的 silence 内容和 bonafide speech 内容之间存在差异。此外，通过对 CM 进行重新训练，可以减少诈骗攻击对 CM 的影响。<details>
<summary>Abstract</summary>
The current speech anti-spoofing countermeasures (CMs) show excellent performance on specific datasets. However, removing the silence of test speech through Voice Activity Detection (VAD) can severely degrade performance. In this paper, the impact of silence on speech anti-spoofing is analyzed. First, the reasons for the impact are explored, including the proportion of silence duration and the content of silence. The proportion of silence duration in spoof speech generated by text-to-speech (TTS) algorithms is lower than that in bonafide speech. And the content of silence generated by different waveform generators varies compared to bonafide speech. Then the impact of silence on model prediction is explored. Even after retraining, the spoof speech generated by neural network based end-to-end TTS algorithms suffers a significant rise in error rates when the silence is removed. To demonstrate the reasons for the impact of silence on CMs, the attention distribution of a CM is visualized through class activation mapping (CAM). Furthermore, the implementation and analysis of the experiments masking silence or non-silence demonstrates the significance of the proportion of silence duration for detecting TTS and the importance of silence content for detecting voice conversion (VC). Based on the experimental results, improving the robustness of CMs against unknown spoofing attacks by masking silence is also proposed. Finally, the attacks on anti-spoofing CMs through concatenating silence, and the mitigation of VAD and silence attack through low-pass filtering are introduced.
</details>
<details>
<summary>摘要</summary>
当前的语音反 spoofing 防范措施（CMs）在特定的数据集上表现出非常出色。然而，通过语音活动检测（VAD）移除测试语音中的沉默可能会严重降低性能。在这篇论文中，我们分析了语音反 spoofing 中 silence 的影响。首先，我们研究了 silence 的原因对性能的影响，包括沉默时间的比例和沉默内容。TTS 算法生成的 spoof speech 中的沉默时间比bonafide speech 长，而生成的沉默内容与 bonafide speech 不同。然后，我们研究了 silence 对模型预测的影响。即使重新训练，使用 neural network 基于 end-to-end TTS 算法生成的 spoof speech 在移除沉默后错误率显著增加。为了说明 silence 对 CMs 的影响的原因，我们通过类 activation mapping（CAM）Visualize CM 的注意力分布。此外，我们还实现了在 silence 或非沉默处理下进行实验，以示 silence 的重要性和 non-silence 的重要性。基于实验结果，我们也提出了改进 CMs 对未知 spoofing 攻击的Robustness的方法。最后，我们介绍了 concatenating silence 的攻击和 VAD 和沉默攻击的mitigation 策略。
</details></li>
</ul>
<hr>
<h2 id="Frame-Pairwise-Distance-Loss-for-Weakly-supervised-Sound-Event-Detection"><a href="#Frame-Pairwise-Distance-Loss-for-Weakly-supervised-Sound-Event-Detection" class="headerlink" title="Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection"></a>Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11783">http://arxiv.org/abs/2309.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Tao, Yuxing Huang, Xiangdong Wang, Long Yan, Lufeng Zhai, Kazushige Ouchi, Taihao Li</li>
<li>for:  bridging the gap between fully supervised methods and unsupervised techniques in various domains</li>
<li>methods:  introducing a Frame Pairwise Distance (FPD) loss branch and synthesized data to enhance the recognition rate of weakly-supervised sound event detection</li>
<li>results:  validated on the standard DCASE dataset, the proposed approach shows efficacy in improving the recognition rate of weakly-supervised sound event detection.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Weakly-supervised learning has emerged as a promising approach to leverage limited labeled data in various domains by bridging the gap between fully supervised methods and unsupervised techniques. Acquisition of strong annotations for detecting sound events is prohibitively expensive, making weakly supervised learning a more cost-effective and broadly applicable alternative. In order to enhance the recognition rate of the learning of detection of weakly-supervised sound events, we introduce a Frame Pairwise Distance (FPD) loss branch, complemented with a minimal amount of synthesized data. The corresponding sampling and label processing strategies are also proposed. Two distinct distance metrics are employed to evaluate the proposed approach. Finally, the method is validated on the standard DCASE dataset. The obtained experimental results corroborated the efficacy of this approach.
</details>
<details>
<summary>摘要</summary>
微监督学习已经成为各个领域中利用有限的标注数据的可靠方法之一，它在完全监督方法和无监督技术之间填补了空隙。然而，获取听音事件的强制标注是非常昂贵的，使得微监督学习成为更加经济可行的和广泛适用的替代方案。为提高微监督学习检测听音事件的识别率，我们在本文中引入帧对比距离（FPD）损失分支，并采用一小量的合成数据来补充。同时，我们也提出了采样和标签处理策略。两种不同的距离度量被使用来评估该方法。最后，我们在标准的DCASE dataset上验证了该方法的效果。实验结果证明了该方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="CoMFLP-Correlation-Measure-based-Fast-Search-on-ASR-Layer-Pruning"><a href="#CoMFLP-Correlation-Measure-based-Fast-Search-on-ASR-Layer-Pruning" class="headerlink" title="CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning"></a>CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11768">http://arxiv.org/abs/2309.11768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liu, Zhiyuan Peng, Tan Lee</li>
<li>for: 提高资源受限设备上Transformer型声音识别（ASR）模型的性能。</li>
<li>methods: 基于相关度度量的快速搜索层剥离（LP）算法，从多层模型中剥离 redundancy。</li>
<li>results: 比前一代LP方法更高效，仅需常量时间复杂度，并且可以提高ASR模型的性能。<details>
<summary>Abstract</summary>
Transformer-based speech recognition (ASR) model with deep layers exhibited significant performance improvement. However, the model is inefficient for deployment on resource-constrained devices. Layer pruning (LP) is a commonly used compression method to remove redundant layers. Previous studies on LP usually identify the redundant layers according to a task-specific evaluation metric. They are time-consuming for models with a large number of layers, even in a greedy search manner. To address this problem, we propose CoMFLP, a fast search LP algorithm based on correlation measure. The correlation between layers is computed to generate a correlation matrix, which identifies the redundancy among layers. The search process is carried out in two steps: (1) coarse search: to determine top $K$ candidates by pruning the most redundant layers based on the correlation matrix; (2) fine search: to select the best pruning proposal among $K$ candidates using a task-specific evaluation metric. Experiments on an ASR task show that the pruning proposal determined by CoMFLP outperforms existing LP methods while only requiring constant time complexity. The code is publicly available at https://github.com/louislau1129/CoMFLP.
</details>
<details>
<summary>摘要</summary>
“ transformer-based  speech recognition（ASR）模型 WITH deep layers  exhibited significant performance improvement. However, the model is inefficient for deployment on resource-constrained devices. layer pruning（LP）is a commonly used compression method to remove redundant layers. Previous studies on LP usually identify the redundant layers according to a task-specific evaluation metric. They are time-consuming for models with a large number of layers, even in a greedy search manner. To address this problem, we propose CoMFLP, a fast search LP algorithm based on correlation measure. The correlation between layers is computed to generate a correlation matrix, which identifies the redundancy among layers. The search process is carried out in two steps: (1) coarse search: to determine top $K$ candidates by pruning the most redundant layers based on the correlation matrix; (2) fine search: to select the best pruning proposal among $K$ candidates using a task-specific evaluation metric. Experiments on an ASR task show that the pruning proposal determined by CoMFLP outperforms existing LP methods while only requiring constant time complexity. The code is publicly available at https://github.com/louislau1129/CoMFLP.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Sparsely-Shared-LoRA-on-Whisper-for-Child-Speech-Recognition"><a href="#Sparsely-Shared-LoRA-on-Whisper-for-Child-Speech-Recognition" class="headerlink" title="Sparsely Shared LoRA on Whisper for Child Speech Recognition"></a>Sparsely Shared LoRA on Whisper for Child Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11756">http://arxiv.org/abs/2309.11756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liu, Ying Qin, Zhiyuan Peng, Tan Lee</li>
<li>for: 这个论文想要提高 whisper 自动语音识别（ASR）模型在low-resource语音上的零shot性能。</li>
<li>methods: 这个论文使用了 parameter-efficient fine-tuning（PEFT）方法，包括 LoRA 和 AdaLoRA，以及一种新的 Sparsely Shared LoRA（S2-LoRA）方法。</li>
<li>results:  experiments 表明，S2-LoRA 可以在low-resource中国儿童语音上达到与 AdaLoRA 相当的适应性，并且在out-of-domain数据上表现更好，并且自动学习的rank分布与 AdaLoRA 的分布类似。<details>
<summary>Abstract</summary>
Whisper is a powerful automatic speech recognition (ASR) model. Nevertheless, its zero-shot performance on low-resource speech requires further improvement. Child speech, as a representative type of low-resource speech, is leveraged for adaptation. Recently, parameter-efficient fine-tuning (PEFT) in NLP was shown to be comparable and even better than full fine-tuning, while only needing to tune a small set of trainable parameters. However, current PEFT methods have not been well examined for their effectiveness on Whisper. In this paper, only parameter composition types of PEFT approaches such as LoRA and Bitfit are investigated as they do not bring extra inference costs. Different popular PEFT methods are examined. Particularly, we compare LoRA and AdaLoRA and figure out the learnable rank coefficient is a good design. Inspired by the sparse rank distribution allocated by AdaLoRA, a novel PEFT approach Sparsely Shared LoRA (S2-LoRA) is proposed. The two low-rank decomposed matrices are globally shared. Each weight matrix only has to maintain its specific rank coefficients that are constrained to be sparse. Experiments on low-resource Chinese child speech show that with much fewer trainable parameters, S2-LoRA can achieve comparable in-domain adaptation performance to AdaLoRA and exhibit better generalization ability on out-of-domain data. In addition, the rank distribution automatically learned by S2-LoRA is found to have similar patterns to AdaLoRA's allocation.
</details>
<details>
<summary>摘要</summary>
噪音是一个强大的自动语音识别（ASR）模型。然而，它的零实例性表现在低资源语音上仍需进一步改进。儿童语音作为低资源语音的代表类型，被利用于适应。最近， Parametric Efficient Fine-Tuning（PEFT）在自然语言处理（NLP）中显示了相当于或更好的性能，而只需要调整一小部分可变参数。然而，当前PEFT方法尚未对噪音进行了广泛的检验。本文只 investigate parameter composition type PEFTapproaches such as LoRA和Bitfit，因为它们不会增加额外的推理成本。不同的流行PEFT方法被比较。特别是，我们比较LoRA和AdaLoRA，并发现了可学习排名系数是一个好的设计。受AdaLoRA的稀疏排名分布启发，我们提出了一种新的PEFT方法——Sparsely Shared LoRA（S2-LoRA）。两个低级别分解的矩阵在全球共享。每个weight矩阵只需保持它的特定排名系数，这些系数是约束为稀疏的。实验表明，与少量可变参数，S2-LoRA可以达到与AdaLoRA相当的适应性，并且在对外域数据进行推理时表现更好。此外，S2-LoRA自动学习的排名分布与AdaLoRA的分布有相似的模式。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-In-the-Wild-Data-for-Effective-Self-Supervised-Pretraining-in-Speaker-Recognition"><a href="#Leveraging-In-the-Wild-Data-for-Effective-Self-Supervised-Pretraining-in-Speaker-Recognition" class="headerlink" title="Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition"></a>Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11730">http://arxiv.org/abs/2309.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Wang, Qibing Bai, Qi Liu, Jianwei Yu, Zhengyang Chen, Bing Han, Yanmin Qian, Haizhou Li</li>
<li>for: 提高 speaker recognition 系统性能</li>
<li>methods: 使用 DINO 自助学习方法和 confidence-based data filtering 算法</li>
<li>results: 在大规模的 WenetSpeech 数据集和 CNCeleb 数据集上提高 speaker recognition 系统性能，并且需要 fewer 训练数据<details>
<summary>Abstract</summary>
Current speaker recognition systems primarily rely on supervised approaches, constrained by the scale of labeled datasets. To boost the system performance, researchers leverage large pretrained models such as WavLM to transfer learned high-level features to the downstream speaker recognition task. However, this approach introduces extra parameters as the pretrained model remains in the inference stage. Another group of researchers directly apply self-supervised methods such as DINO to speaker embedding learning, yet they have not explored its potential on large-scale in-the-wild datasets. In this paper, we present the effectiveness of DINO training on the large-scale WenetSpeech dataset and its transferability in enhancing the supervised system performance on the CNCeleb dataset. Additionally, we introduce a confidence-based data filtering algorithm to remove unreliable data from the pretraining dataset, leading to better performance with less training data. The associated pretrained models, confidence files, pretraining and finetuning scripts will be made available in the Wespeaker toolkit.
</details>
<details>
<summary>摘要</summary>
In this paper, we show the effectiveness of DINO training on the large-scale WenetSpeech dataset and its transferability in enhancing supervised system performance on the CNCeleb dataset. We also introduce a confidence-based data filtering algorithm to remove unreliable data from the pretraining dataset, leading to better performance with less training data. The associated pretrained models, confidence files, pretraining and finetuning scripts will be available in the Wespeaker toolkit.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.SD_2023_09_21/" data-id="clnsn0vk700n5gf881aadb96f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.CV_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T13:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/cs.CV_2023_09_21/">cs.CV - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Stereo-Without-Pattern-Projector"><a href="#Active-Stereo-Without-Pattern-Projector" class="headerlink" title="Active Stereo Without Pattern Projector"></a>Active Stereo Without Pattern Projector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12315">http://arxiv.org/abs/2309.12315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bartn8/vppstereo">https://github.com/bartn8/vppstereo</a></li>
<li>paper_authors: Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti, Stefano Mattoccia</li>
<li>for: 提出了一种基于活动镜头原理的标准静止相机系统中的新框架，无需物理模式投影器。</li>
<li>methods: 通过在左右图像中虚拟投射模式，根据深度感知器获取的稀疏测量。任何设备都可以轻松地插入我们的框架中，在任何环境中实现虚拟活动镜头设置，超越物理模式投影器的限制，如工作范围或环境条件。</li>
<li>results: 对室内&#x2F;室外 dataset进行了实验，包括长距离和近距离场景，实验结果表明我们的方法可以准确地提高镜头算法和深度网络的准确率。<details>
<summary>Abstract</summary>
This paper proposes a novel framework integrating the principles of active stereo in standard passive camera systems without a physical pattern projector. We virtually project a pattern over the left and right images according to the sparse measurements obtained from a depth sensor. Any such devices can be seamlessly plugged into our framework, allowing for the deployment of a virtual active stereo setup in any possible environment, overcoming the limitation of pattern projectors, such as limited working range or environmental conditions. Experiments on indoor/outdoor datasets, featuring both long and close-range, support the seamless effectiveness of our approach, boosting the accuracy of both stereo algorithms and deep networks.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的框架，将活动镜头原理 integrate into标准的普通摄像头系统中，不需要物理Pattern projector。我们在左右图像上虚拟展示了模式，根据深度传感器获得的稀疏测量。任何这种设备都可以轻松插入我们的框架中， allowing for the deployment of a virtual active stereo setup in any possible environment，超越了模式项目器的限制，如工作范围或环境条件。对室内/室外数据集进行了实验，包括长距离和近距离，支持我们的方法的无缝效果，提高了镜头算法和深度网络的准确性。
</details></li>
</ul>
<hr>
<h2 id="TinyCLIP-CLIP-Distillation-via-Affinity-Mimicking-and-Weight-Inheritance"><a href="#TinyCLIP-CLIP-Distillation-via-Affinity-Mimicking-and-Weight-Inheritance" class="headerlink" title="TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance"></a>TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12314">http://arxiv.org/abs/2309.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi, Chen, Xinggang Wang, Hongyang Chao, Han Hu</li>
<li>for: 这个论文提出了一种新的跨模型蒸发法，叫做TinyCLIP，用于大规模的语言-图像预训模型。</li>
<li>methods: 这个方法 introduces two core techniques: affinity mimicking和weight inheritance。affinity mimicking探索了modalities之间的互动，使学生模型能够模仿老师的跨modalities的学习行为，实现视征语Modal Affinity Space中的对应关系。weight inheritance将老师模型的预训过的类 weights传递给学生模型，以提高蒸发效率。</li>
<li>results: 实验结果显示，TinyCLIP可以将预训CLIP ViT-B&#x2F;32的大小增加50%，并维持相同的零配置性性能。而且，将蒸发进行多阶段进度的实现了对应关系的增强。此外，我们的TinyCLIP ViT-8M&#x2F;16，在YFCC-15M上训练，在ImageNet上取得了41.1%的零配置性top-1准确率，比原CLIP ViT-B&#x2F;16高3.5%，并且只使用8.9%的参数。最后，我们显示了TinyCLIP在多个下游任务中的优良传播性。代码和模型将在<a target="_blank" rel="noopener" href="https://aka.ms/tinyclip%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://aka.ms/tinyclip上开源。</a><details>
<summary>Abstract</summary>
In this paper, we propose a novel cross-modal distillation method, called TinyCLIP, for large-scale language-image pre-trained models. The method introduces two core techniques: affinity mimicking and weight inheritance. Affinity mimicking explores the interaction between modalities during distillation, enabling student models to mimic teachers' behavior of learning cross-modal feature alignment in a visual-linguistic affinity space. Weight inheritance transmits the pre-trained weights from the teacher models to their student counterparts to improve distillation efficiency. Moreover, we extend the method into a multi-stage progressive distillation to mitigate the loss of informative weights during extreme compression. Comprehensive experiments demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of the pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot performance. While aiming for comparable performance, distillation with weight inheritance can speed up the training by 1.4 - 7.8 $\times$ compared to training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M, achieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet, surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9% parameters. Finally, we demonstrate the good transferability of TinyCLIP in various downstream tasks. Code and models will be open-sourced at https://aka.ms/tinyclip.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的跨Modal Distillation方法，称为TinyCLIP，用于大规模语言图像预训练模型。该方法 introduce two core techniques：对模式的亲和力模仿和重量继承。对模式的亲和力模仿探索了多Modalities在Distillation过程中的交互，使学生模型能够模仿教师模型在视觉语言对应空间中学习跨Modal featureAlignment的行为。重量继承将预训练的重量从教师模型传递给其学生版本，以提高Distillation的效率。此外，我们扩展了该方法到多Stage Progressive Distillation，以mitigate the loss of informative weights during extreme compression。我们的实验表明，TinyCLIP可以将预训练CLIP ViT-B/32的大小减少50%，保持相同的零shot性能。而在尝试保持相同性能的情况下，与教师模型的Distillation可以加速训练1.4-7.8倍。此外，我们的TinyCLIP ViT-8M/16，在YFCC-15M上训练，在ImageNet上 achieve Zero-shot top-1准确率41.1%，比原CLIP ViT-B/16提高3.5%，使用只有8.9%的参数。最后，我们示出了TinyCLIP在多种下游任务中的好传输性。代码和模型将在https://aka.ms/tinyclip上开源。
</details></li>
</ul>
<hr>
<h2 id="TalkNCE-Improving-Active-Speaker-Detection-with-Talk-Aware-Contrastive-Learning"><a href="#TalkNCE-Improving-Active-Speaker-Detection-with-Talk-Aware-Contrastive-Learning" class="headerlink" title="TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning"></a>TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12306">http://arxiv.org/abs/2309.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaeyoung Jung, Suyeon Lee, Kihyun Nam, Kyeongha Rho, You Jin Kim, Youngjoon Jang, Joon Son Chung</li>
<li>for: 本文旨在提出一种新的对话监视损失函数，以便在视频帧序中确定人员是否正在说话。</li>
<li>methods: 本文提出了一种名为TalkNCE的对话感知损失函数，该损失函数仅在屏幕上显示的人员实际说话的部分应用。这种损失函数鼓励模型通过自然的语音和面部运动的对应学习有效的表示。</li>
<li>results: 实验表明，我们的损失函数可以轻松地与现有的ASD模型一起进行joint优化，提高其性能。我们的方法在AVA-ActiveSpeaker和ASW datasets上达到了状态艺术水平。<details>
<summary>Abstract</summary>
The goal of this work is Active Speaker Detection (ASD), a task to determine whether a person is speaking or not in a series of video frames. Previous works have dealt with the task by exploring network architectures while learning effective representations has been less explored. In this work, we propose TalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part of the full segments where a person on the screen is actually speaking. This encourages the model to learn effective representations through the natural correspondence of speech and facial movements. Our loss can be jointly optimized with the existing objectives for training ASD models without the need for additional supervision or training data. The experiments demonstrate that our loss can be easily integrated into the existing ASD frameworks, improving their performance. Our method achieves state-of-the-art performances on AVA-ActiveSpeaker and ASW datasets.
</details>
<details>
<summary>摘要</summary>
本工作的目标是活动说话人检测（ASD），即在视频帧序中确定人是否说话。先前的工作主要关注网络架构，而学习有效表示的研究相对较少。在这项工作中，我们提议了一种新的对话抑制损失函数，即TalkNCE。这种损失函数只应用于屏幕上的人是否实际说话的部分段落。这会让模型学习有效的表示，通过自然的语音和面部运动之间的相对应。我们的损失函数可以与现有的ASD模型训练目标一起优化，无需额外的监督或训练数据。实验表明，我们的损失函数可以轻松地与现有的ASD框架集成，提高其性能。我们的方法在AVA-ActiveSpeaker和ASW数据集上达到了状态盘点的表现。
</details></li>
</ul>
<hr>
<h2 id="SlowFast-Network-for-Continuous-Sign-Language-Recognition"><a href="#SlowFast-Network-for-Continuous-Sign-Language-Recognition" class="headerlink" title="SlowFast Network for Continuous Sign Language Recognition"></a>SlowFast Network for Continuous Sign Language Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12304">http://arxiv.org/abs/2309.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junseok Ahn, Youngjoon Jang, Joon Son Chung</li>
<li>for: 本研究旨在实现有效的Continuous Sign Language Recognition（CSLR）特征提取。</li>
<li>methods: 我们使用了两条不同的时间分辨率的SlowFast网络，每一条路径独立地捕捉手势（手势）和表情（表情）的空间信息，以及动作（运动）信息。此外，我们还提出了两种特有的特征融合方法：（1）双向特征融合（BFF），使得动态 semantics transfer into spatial semantics和vice versa；和（2）路径特征增强（PFE），通过辅助子网络增强动态和空间表示，而不需Extra的推理时间。</li>
<li>results: 我们的模型在流行的CSLR数据集上（包括PHOENIX14、PHOENIX14-T和CSL-Daily）达到了当前领先的性能。<details>
<summary>Abstract</summary>
The objective of this work is the effective extraction of spatial and dynamic features for Continuous Sign Language Recognition (CSLR). To accomplish this, we utilise a two-pathway SlowFast network, where each pathway operates at distinct temporal resolutions to separately capture spatial (hand shapes, facial expressions) and dynamic (movements) information. In addition, we introduce two distinct feature fusion methods, carefully designed for the characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which facilitates the transfer of dynamic semantics into spatial semantics and vice versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and spatial representations through auxiliary subnetworks, while avoiding the need for extra inference time. As a result, our model further strengthens spatial and dynamic representations in parallel. We demonstrate that the proposed framework outperforms the current state-of-the-art performance on popular CSLR datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.
</details>
<details>
<summary>摘要</summary>
目标是提取CSLR中的空间和动态特征，我们利用了两个不同的时间分辨率的SlowFast网络，每个路径独立捕捉空间（手势、 facial expressions）和动态（运动）信息。此外，我们还提出了两种特有的特征融合方法：（1）双向特征融合（BFF），使动态 semantics transfer into spatial semantics和vice versa；（2）路径特征增强（PFE），通过辅助子网络增强动态和空间表示，而不需要额外的推理时间。这种方法使得我们的模型在平行的情况下进一步强化了空间和动态表示。我们的模型在各种CSLR数据集上达到了当前领先的性能，包括PHOENIX14、PHOENIX14-T和CSL-Daily等。
</details></li>
</ul>
<hr>
<h2 id="PanoVOS-Bridging-Non-panoramic-and-Panoramic-Views-with-Transformer-for-Video-Segmentation"><a href="#PanoVOS-Bridging-Non-panoramic-and-Panoramic-Views-with-Transformer-for-Video-Segmentation" class="headerlink" title="PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation"></a>PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12303">http://arxiv.org/abs/2309.12303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, Wei Zhang</li>
<li>for: 这篇论文旨在提供一个大型投影视频分割 dataset，以满足投影视频中的视频分割问题。</li>
<li>methods: 该论文使用了15种市场上的视频对象分割模型进行评估，并通过错误分析发现这些模型无法处理投影视频中的像素级别内容缺失。因此，该论文提出了一种基于 semantic boundary information的 Panoramic Space Consistency Transformer（PSCFormer），可以有效地利用上一帧的semantic boundary信息来进行像素级别匹配。</li>
<li>results: 对比之前的最佳模型，我们的 PSCFormer 网络在投影视频下表现出了优于其他模型的 segmentation 结果。<details>
<summary>Abstract</summary>
Panoramic videos contain richer spatial information and have attracted tremendous amounts of attention due to their exceptional experience in some fields such as autonomous driving and virtual reality. However, existing datasets for video segmentation only focus on conventional planar images. To address the challenge, in this paper, we present a panoramic video dataset, PanoVOS. The dataset provides 150 videos with high video resolutions and diverse motions. To quantify the domain gap between 2D planar videos and panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS) models on PanoVOS. Through error analysis, we found that all of them fail to tackle pixel-level content discontinues of panoramic videos. Thus, we present a Panoramic Space Consistency Transformer (PSCFormer), which can effectively utilize the semantic boundary information of the previous frame for pixel-level matching with the current frame. Extensive experiments demonstrate that compared with the previous SOTA models, our PSCFormer network exhibits a great advantage in terms of segmentation results under the panoramic setting. Our dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can advance the development of panoramic segmentation/tracking.
</details>
<details>
<summary>摘要</summary>
拼接视频含有更多的空间信息，吸引了很多关注，特别是在自动驾驶和虚拟现实等领域。然而，现有的视频分割数据集只关注传统的平面图像。为了解决这个挑战，在这篇论文中，我们提供了拼接视频数据集（PanoVOS）。该数据集包含150个高分辨率视频和多样化的运动。为了衡量2D平面视频和拼接视频之间的域间差，我们评估了15种市场上的视频对象分割（VOS）模型在PanoVOS上。经过错误分析，我们发现所有模型都无法处理拼接视频中像素级别的内容缺失。因此，我们提出了拼接空间一致变换器（PSCFormer），可以有效利用上一帧的semantic边界信息 для像素级匹配当前帧。广泛的实验表明，与前一代最佳模型相比，我们的PSCFormer网络在拼接设置下展现出了优于其他模型的分割结果。我们的数据集将带来新的拼接VOS挑战，我们希望通过PanoVOS来推动拼接分割/跟踪的发展。
</details></li>
</ul>
<hr>
<h2 id="Text-Guided-Vector-Graphics-Customization"><a href="#Text-Guided-Vector-Graphics-Customization" class="headerlink" title="Text-Guided Vector Graphics Customization"></a>Text-Guided Vector Graphics Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12302">http://arxiv.org/abs/2309.12302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiying Zhang, Nanxuan Zhao, Jing Liao</li>
<li>for: 生成高质量自定义 вектор图形，以满足设计师的创造需求。</li>
<li>methods: 提posed a novel pipeline that leverages large pre-trained text-to-image models and semantic-based path alignment method to generate customized raster images guided by textual prompts, while preserving the properties and layer-wise information of a given exemplar SVG.</li>
<li>results: 经过广泛评估，得到了多 metric 的优秀Result，证明了该管道的效果iveness in generating diverse customizations of vector graphics with exceptional quality.<details>
<summary>Abstract</summary>
Vector graphics are widely used in digital art and valued by designers for their scalability and layer-wise topological properties. However, the creation and editing of vector graphics necessitate creativity and design expertise, leading to a time-consuming process. In this paper, we propose a novel pipeline that generates high-quality customized vector graphics based on textual prompts while preserving the properties and layer-wise information of a given exemplar SVG. Our method harnesses the capabilities of large pre-trained text-to-image models. By fine-tuning the cross-attention layers of the model, we generate customized raster images guided by textual prompts. To initialize the SVG, we introduce a semantic-based path alignment method that preserves and transforms crucial paths from the exemplar SVG. Additionally, we optimize path parameters using both image-level and vector-level losses, ensuring smooth shape deformation while aligning with the customized raster image. We extensively evaluate our method using multiple metrics from vector-level, image-level, and text-level perspectives. The evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality. The project page is https://intchous.github.io/SVGCustomization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>Vector graphics 广泛用于数字艺术中，设计师们喜欢它们因为它们可扩展和层次结构的特性。然而，创建和修改vector graphics需要创作和设计技能，这会使得过程变得时间consuming。在这篇论文中，我们提出了一个新的管道，可以根据文本提示生成高质量自定义vector graphics，同时保持原始SVG的特性和层次信息。我们利用大型预训练的文本到图像模型的能力，通过微调模型的交叉注意力层，生成基于文本提示的自定义静止图像。为初始化SVG，我们引入了基于 semantics的路径对齐方法，保持和修改原始SVG中重要的路径。此外，我们使用图像水平和向量水平的损失函数进行路径参数优化，确保图像和向量图像的平滑形变。我们进行了广泛的评估，结果表明我们的管道可以生成多样化的自定义vector graphics，质量极高。项目页面是https://intchous.github.io/SVGCustomization。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Input-image-Normalization-for-Solving-Mode-Collapse-Problem-in-GAN-based-X-ray-Images"><a href="#Adaptive-Input-image-Normalization-for-Solving-Mode-Collapse-Problem-in-GAN-based-X-ray-Images" class="headerlink" title="Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images"></a>Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12245">http://arxiv.org/abs/2309.12245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: 增强医学影像数据集的多样性，提高机器学习分类器的性能。</li>
<li>methods: 使用生成对抗网络（GAN）技术生成 sintetic X-ray 图像，并在 GAN 中加入 adaptive input-image normalization 来解决模式塌collapse问题。</li>
<li>results: DCGAN 和 ACGAN  WITH adaptive input-image normalization 能够提高 classification 性能和多样性 scores，相比于 DCGAN 和 ACGAN  WITH un-normalized X-ray images。<details>
<summary>Abstract</summary>
Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse problems. Synthetically generated images are utilized for data augmentation and training a Vision Transformer model. The classification performance of the model is evaluated using accuracy, recall, and precision scores. Results demonstrate that the DCGAN and the ACGAN with adaptive input-image normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as evidenced by the superior diversity scores and classification scores.
</details>
<details>
<summary>摘要</summary>
生成对抗网络可能会遇到两种不同的模式塌溃问题：内类模式塌溃和间类模式塌溃。这两种问题都会导致生成的 sintetic 图像失去多样化。本文研究了这两种模式塌溃问题，并评估它们对生成的 sintetic X-ray 图像的多样化的影响。这个研究还提供了一种实验室的证明，表明将适应输入图像Normalization与深度卷积GAN和辅助分类器GAN相结合可以解决模式塌溃问题。生成的 sintetic 图像被用于数据增强和训练一个 Vision Transformer 模型。模型的分类性能被评估使用准确率、回归率和精度分数。结果表明，DCGAN 和 ACGAN  WITH 适应输入图像Normalization 比 DCGAN 和 ACGAN  WITH 未normalized X-ray 图像表现更好，根据多样化分数和分类分数。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Reliably-Improve-the-Robustness-to-Image-Acquisition-of-Remote-Sensing-of-PV-Systems"><a href="#Can-We-Reliably-Improve-the-Robustness-to-Image-Acquisition-of-Remote-Sensing-of-PV-Systems" class="headerlink" title="Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?"></a>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12214">http://arxiv.org/abs/2309.12214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</li>
<li>for: 监控区域级单位的拍照电力系统</li>
<li>methods: 利用波浪对数法（WCAM）分解模型预测的空间�atura领域</li>
<li>results: 提高了内部积分模型的可靠性和敏感度，并获得了对于采购条件的变化的深入理解，以增加清洁能源的安全组合。<details>
<summary>Abstract</summary>
Photovoltaic (PV) energy is crucial for the decarbonization of energy systems. Due to the lack of centralized data, remote sensing of rooftop PV installations is the best option to monitor the evolution of the rooftop PV installed fleet at a regional scale. However, current techniques lack reliability and are notably sensitive to shifts in the acquisition conditions. To overcome this, we leverage the wavelet scale attribution method (WCAM), which decomposes a model's prediction in the space-scale domain. The WCAM enables us to assess on which scales the representation of a PV model rests and provides insights to derive methods that improve the robustness to acquisition conditions, thus increasing trust in deep learning systems to encourage their use for the safe integration of clean energy in electric systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Brain-Tumor-Detection-Using-Deep-Learning-Approaches"><a href="#Brain-Tumor-Detection-Using-Deep-Learning-Approaches" class="headerlink" title="Brain Tumor Detection Using Deep Learning Approaches"></a>Brain Tumor Detection Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12193">http://arxiv.org/abs/2309.12193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Razia Sultana Misu</li>
<li>for: 本研究旨在使用深度学习技术自动检测脑肿瘤，以提高脑肿瘤检测和分类精度。</li>
<li>methods: 本研究使用了五种转移学习模型，包括VGG16、VGG19、DenseNet121、ResNet50和YOLO V4，其中ResNet50达到了最高准确率99.54%。</li>
<li>results: 研究表明，使用深度学习技术可以提高脑肿瘤检测和分类精度，并且ResNet50模型达到了最高准确率。<details>
<summary>Abstract</summary>
Brain tumors are collections of abnormal cells that can develop into masses or clusters. Because they have the potential to infiltrate other tissues, they pose a risk to the patient. The main imaging technique used, MRI, may be able to identify a brain tumor with accuracy. The fast development of Deep Learning methods for use in computer vision applications has been facilitated by a vast amount of training data and improvements in model construction that offer better approximations in a supervised setting. The need for these approaches has been the main driver of this expansion. Deep learning methods have shown promise in improving the precision of brain tumor detection and classification using magnetic resonance imaging (MRI). The study on the use of deep learning techniques, especially ResNet50, for brain tumor identification is presented in this abstract. As a result, this study investigates the possibility of automating the detection procedure using deep learning techniques. In this study, I utilized five transfer learning models which are VGG16, VGG19, DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest accuracy 99.54%. The goal of the study is to guide researchers and medical professionals toward powerful brain tumor detecting systems by employing deep learning approaches by way of this evaluation and analysis.
</details>
<details>
<summary>摘要</summary>
脑肿是一种集群畸形细胞的发育，可能形成肿体或集群。由于它们可能会渗透到其他组织，因此对患者存在风险。主要用于识别脑肿的成像技术是MRI，可能准确地识别脑肿。深度学习方法在计算机视觉应用中的快速发展，得益于庞大的训练数据和改进的模型构造，以及更好的超级vised设定。这些方法的需求是扩展的推动者。深度学习方法在MRI中识别和分类脑肿方面表现出了承诺，特别是使用ResNet50模型，其最高准确率为99.54%。本研究旨在通过深度学习方法自动识别脑肿的可能性，并提供一种可靠的脑肿检测系统。本研究使用了五种转移学习模型，包括VGG16、VGG19、DenseNet121、ResNet50和YOLO V4，其中ResNet50提供了最高准确率。本研究的目标是导引研究人员和医疗专业人员通过深度学习方法来实现高效的脑肿检测系统，以便更好地满足医疗需求。
</details></li>
</ul>
<hr>
<h2 id="SG-Bot-Object-Rearrangement-via-Coarse-to-Fine-Robotic-Imagination-on-Scene-Graphs"><a href="#SG-Bot-Object-Rearrangement-via-Coarse-to-Fine-Robotic-Imagination-on-Scene-Graphs" class="headerlink" title="SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs"></a>SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12188">http://arxiv.org/abs/2309.12188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam</li>
<li>for: This paper focuses on developing a novel rearrangement framework for robotic-environment interactions, with the goal of achieving lightweight, real-time, and user-controllable characteristics.</li>
<li>methods: The proposed framework, called SG-Bot, utilizes a coarse-to-fine scheme with a scene graph as the scene representation, and employs a three-fold procedure consisting of observation, imagination, and execution to address the task.</li>
<li>results: Experimental results show that SG-Bot outperforms competitors by a large margin, demonstrating its effectiveness in embodied AI tasks.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文关注开发一种新的重新排序框架，用于机器人环境互动，目的是实现轻量级、实时、用户可控的特点。</li>
<li>methods: 提议的框架被称为SG-Bot，它采用一种粗粒度到细粒度的方案，使用场景图作为场景表示，并采用观察、想象和执行的三重过程来解决问题。</li>
<li>results: 实验结果显示，SG-Bot比竞争对手大幅提高了性能，证明了它在机器人AI任务中的有效性。<details>
<summary>Abstract</summary>
Object rearrangement is pivotal in robotic-environment interactions, representing a significant capability in embodied AI. In this paper, we present SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme with a scene graph as the scene representation. Unlike previous methods that rely on either known goal priors or zero-shot large models, SG-Bot exemplifies lightweight, real-time, and user-controllable characteristics, seamlessly blending the consideration of commonsense knowledge with automatic generation capabilities. SG-Bot employs a three-fold procedure--observation, imagination, and execution--to adeptly address the task. Initially, objects are discerned and extracted from a cluttered scene during the observation. These objects are first coarsely organized and depicted within a scene graph, guided by either commonsense or user-defined criteria. Then, this scene graph subsequently informs a generative model, which forms a fine-grained goal scene considering the shape information from the initial scene and object semantics. Finally, for execution, the initial and envisioned goal scenes are matched to formulate robotic action policies. Experimental results demonstrate that SG-Bot outperforms competitors by a large margin.
</details>
<details>
<summary>摘要</summary>
对象重新排序是人工智能中的一项关键能力，代表了机器人和环境之间的互动。在这篇论文中，我们提出了SG-Bot，一种新的重新排序框架，利用粗略到细化的方案，使用场景图作为场景表示。与前一代方法不同，SG-Bot不依赖于已知目标假设或大型零基础模型，而是具有轻量级、实时和用户可控的特点，可以协调考虑常识知识和自动生成能力。SG-Bot采用三个步骤—观察、想象和执行—以适应任务。首先，从杂乱的场景中提取和识别 объек，并将其粗略地组织和描述于场景图中，以 Commonsense 或用户定义的标准指导。然后，这个场景图将导引一个生成模型，该模型形成基于初始场景和物体 semantics 的细化目标场景。最后，为执行，初始和想象的目标场景相匹配，以形成机器人行为策略。实验结果表明，SG-Bot在竞争者之上大幅提高表现。
</details></li>
</ul>
<hr>
<h2 id="ORTexME-Occlusion-Robust-Human-Shape-and-Pose-via-Temporal-Average-Texture-and-Mesh-Encoding"><a href="#ORTexME-Occlusion-Robust-Human-Shape-and-Pose-via-Temporal-Average-Texture-and-Mesh-Encoding" class="headerlink" title="ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding"></a>ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12183">http://arxiv.org/abs/2309.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Cheng, Bo Wang, Robby T. Tan</li>
<li>for:  improve the accuracy of 3D human shape and pose estimation in the presence of occlusion</li>
<li>methods: utilizes temporal information from the input video to better regularize the occluded body parts, and uses a novel average texture learning approach to learn the average appearance of a person and infer a mask based on the average texture</li>
<li>results: achieves significant improvement on the challenging multi-person 3DPW dataset, with 1.8 P-MPJPE error reduction compared to the state-of-the-art rendering-based methods, which enlarge the error up to 5.6 on the same dataset.<details>
<summary>Abstract</summary>
In 3D human shape and pose estimation from a monocular video, models trained with limited labeled data cannot generalize well to videos with occlusion, which is common in the wild videos. The recent human neural rendering approaches focusing on novel view synthesis initialized by the off-the-shelf human shape and pose methods have the potential to correct the initial human shape. However, the existing methods have some drawbacks such as, erroneous in handling occlusion, sensitive to inaccurate human segmentation, and ineffective loss computation due to the non-regularized opacity field. To address these problems, we introduce ORTexME, an occlusion-robust temporal method that utilizes temporal information from the input video to better regularize the occluded body parts. While our ORTexME is based on NeRF, to determine the reliable regions for the NeRF ray sampling, we utilize our novel average texture learning approach to learn the average appearance of a person, and to infer a mask based on the average texture. In addition, to guide the opacity-field updates in NeRF to suppress blur and noise, we propose the use of human body mesh. The quantitative evaluation demonstrates that our method achieves significant improvement on the challenging multi-person 3DPW dataset, where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based methods fail and enlarge the error up to 5.6 on the same dataset.
</details>
<details>
<summary>摘要</summary>
在单一影像视频中的3D人体和姿势估算中，使用有限标签数据训练的模型无法对受遮蔽的影像进行普遍化，这是野外影像中的普遍现象。现有的人类神经渲染方法强调新视角合成，由存在于市场上的人体形状和姿势方法进行初始化，有potential以更正初始人体形状。然而，现有的方法存在一些缺陷，例如错误地处理遮蔽、敏感于不准确的人类分割、以及无法有效地computing条件值场。为了解决这些问题，我们介绍ORTexME，一种防遮蔽时间方法，利用输入影像中的时间信息更好地调节遮蔽的体部部分。我们的ORTexME基于NeRF，以determine可靠的NeRF射线抽样区域，我们运用我们的新的平均文件学习方法学习人类的平均外观，并将其转换为对应的面瘫。此外，为了将NeRF中的透明度场更新更加稳定，我们提议使用人体骨架。我们的量值评估显示，我们的方法在多人3DPW数据集上取得了1.8P-MPJPE误差reduction，而SOTA的渲染基于方法则失败并将误差增加到5.6。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Sign-Language-Production-A-Gloss-Free-Approach-with-Discrete-Representations"><a href="#Autoregressive-Sign-Language-Production-A-Gloss-Free-Approach-with-Discrete-Representations" class="headerlink" title="Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations"></a>Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12179">http://arxiv.org/abs/2309.12179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eui Jun Hwang, Huije Lee, Jong C. Park</li>
<li>for: 本研究旨在提供一种直接将口语句子翻译成手语表达，无需中间件gloss。</li>
<li>methods: 本方法基于手势pose序列的vector量化，并支持高级解码方法和latent-levelAlignment。</li>
<li>results: 我们的方法在比较之前的手语生成方法的测试中表现出色，并且通过Back-Translation和Fréchet Gesture Distance的评价指标来证明其可靠性。<details>
<summary>Abstract</summary>
Gloss-free Sign Language Production (SLP) offers a direct translation of spoken language sentences into sign language, bypassing the need for gloss intermediaries. This paper presents the Sign language Vector Quantization Network, a novel approach to SLP that leverages Vector Quantization to derive discrete representations from sign pose sequences. Our method, rooted in both manual and non-manual elements of signing, supports advanced decoding methods and integrates latent-level alignment for enhanced linguistic coherence. Through comprehensive evaluations, we demonstrate superior performance of our method over prior SLP methods and highlight the reliability of Back-Translation and Fr\'echet Gesture Distance as evaluation metrics.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:《无折衣手语生产（SLP）》提供了直接将口语句子翻译成手语，无需中间件。这篇论文介绍了《手语 вектор量化网络》，一种新的SLP方法，利用量化向量来Derive discrete representation from sign pose sequences。我们的方法受到手语的手势和非手势元素的支持，支持高级解码方法并实现了层次匹配。通过全面的评估，我们证明了我们的方法的性能超过了先前的SLP方法，并指出了回传和Fréchet手势距离作为评估指标的可靠性。
</details></li>
</ul>
<hr>
<h2 id="SANPO-A-Scene-Understanding-Accessibility-Navigation-Pathfinding-Obstacle-Avoidance-Dataset"><a href="#SANPO-A-Scene-Understanding-Accessibility-Navigation-Pathfinding-Obstacle-Avoidance-Dataset" class="headerlink" title="SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset"></a>SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12172">http://arxiv.org/abs/2309.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar M. Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko</li>
<li>for: 这个论文是为了提供一个大规模的人центric视频集，用于 dense prediction 在户外环境中。</li>
<li>methods: 这个论文使用了两种视频SESSION：实际视频SESSION和Synthetic视频SESSION，其中Synthetic视频SESSION是由Parallel Domain提供的。所有SESSION都有密集的深度和ODometer标签，而一部分实际SESSION还有时间相关的密集精度分割标签。</li>
<li>results: 这个论文提供了零基eline和SANPObenchmark，以便未来的研究人员可以使用这些数据进行研究。作者希望通过SANPO dataset的挑战性，推动视频分割、深度估计、多任务视模型和synthetic-to-real领域的进步，并为人工导航系统提供更好的支持。<details>
<summary>Abstract</summary>
We introduce SANPO, a large-scale egocentric video dataset focused on dense prediction in outdoor environments. It contains stereo video sessions collected across diverse outdoor environments, as well as rendered synthetic video sessions. (Synthetic data was provided by Parallel Domain.) All sessions have (dense) depth and odometry labels. All synthetic sessions and a subset of real sessions have temporally consistent dense panoptic segmentation labels. To our knowledge, this is the first human egocentric video dataset with both large scale dense panoptic segmentation and depth annotations. In addition to the dataset we also provide zero-shot baselines and SANPO benchmarks for future research. We hope that the challenging nature of SANPO will help advance the state-of-the-art in video segmentation, depth estimation, multi-task visual modeling, and synthetic-to-real domain adaptation, while enabling human navigation systems.   SANPO is available here: https://google-research-datasets.github.io/sanpo_dataset/
</details>
<details>
<summary>摘要</summary>
我们介绍SANPO dataset，一个大规模的自我视角视频集，专注于户外环境中的密集预测。该集包括多个户外环境中的双视频会议，以及由Parallel Domain提供的Synthetic视频会议。所有会议都有密集的深度和运动标签。 Synthetic会议和一部分实际会议都有时间相关的密集精细分割标签。据我们所知，这是人类自我视角视频集中第一个具有大规模密集精细分割和深度标注的 dataset。此外，我们还提供了零基线和SANPO benchmark，以便未来的研究。我们希望SANPO的挑战性能够推动视频分割、深度估计、多任务视觉模型和Synthetic-to-Real领域的进步，同时帮助人类导航系统。SANPO dataset可以在以下链接下下载：https://google-research-datasets.github.io/sanpo_dataset/
</details></li>
</ul>
<hr>
<h2 id="Information-Forensics-and-Security-A-quarter-century-long-journey"><a href="#Information-Forensics-and-Security-A-quarter-century-long-journey" class="headerlink" title="Information Forensics and Security: A quarter-century-long journey"></a>Information Forensics and Security: A quarter-century-long journey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12159">http://arxiv.org/abs/2309.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Barni, Patrizio Campisi, Edward J. Delp, Gwenael Doërr, Jessica Fridrich, Nasir Memon, Fernando Pérez-González, Anderson Rocha, Luisa Verdoliva, Min Wu</li>
<li>for: 本研究领域的目的是确保人们在数位信息时代中使用设备、数据和知识Properties for authorized purposes, 并且将犯罪分子负责任。</li>
<li>methods: 本文发表了过去25年来研究社区对这个领域的重要技术进步，包括选择性领域的主要技术进步。</li>
<li>results: 本文呈现了过去25年来研究社区对这个领域的未来趋势。<details>
<summary>Abstract</summary>
Information Forensics and Security (IFS) is an active R&D area whose goal is to ensure that people use devices, data, and intellectual properties for authorized purposes and to facilitate the gathering of solid evidence to hold perpetrators accountable. For over a quarter century since the 1990s, the IFS research area has grown tremendously to address the societal needs of the digital information era. The IEEE Signal Processing Society (SPS) has emerged as an important hub and leader in this area, and the article below celebrates some landmark technical contributions. In particular, we highlight the major technological advances on some selected focus areas in the field developed in the last 25 years from the research community and present future trends.
</details>
<details>
<summary>摘要</summary>
信息审查安全（IFS）是一个活跃的研发领域，旨在确保人们在授权的目的下使用设备、数据和知识产权。自1990年代以来，IFS研发领域已经不断增长，以应对数字信息时代的社会需求。IEEE信号处理学会（SPS）在这个领域中已经成为重要的中心和领导者，这篇文章将展望过去25年内从研究 сообщества中出现的一些重要技术进步，并预测未来趋势。
</details></li>
</ul>
<hr>
<h2 id="Vulnerability-of-3D-Face-Recognition-Systems-to-Morphing-Attacks"><a href="#Vulnerability-of-3D-Face-Recognition-Systems-to-Morphing-Attacks" class="headerlink" title="Vulnerability of 3D Face Recognition Systems to Morphing Attacks"></a>Vulnerability of 3D Face Recognition Systems to Morphing Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12118">http://arxiv.org/abs/2309.12118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeet Vardam, Luuk Spreeuwers</li>
<li>for: 本研究旨在探讨3DFR系统对3D面部变换攻击的Robustness。</li>
<li>methods: 本文提出了一些方法可以生成高质量的3D面部变换，并对这些变换进行识别。</li>
<li>results: 研究发现，当3DFR系统面临look-a-like变换攻击时，其最高MMPMR约为40%，RMMR约为41.76%。<details>
<summary>Abstract</summary>
In recent years face recognition systems have been brought to the mainstream due to development in hardware and software. Consistent efforts are being made to make them better and more secure. This has also brought developments in 3D face recognition systems at a rapid pace. These 3DFR systems are expected to overcome certain vulnerabilities of 2DFR systems. One such problem that the domain of 2DFR systems face is face image morphing. A substantial amount of research is being done for generation of high quality face morphs along with detection of attacks from these morphs. Comparatively the understanding of vulnerability of 3DFR systems against 3D face morphs is less. But at the same time an expectation is set from 3DFR systems to be more robust against such attacks. This paper attempts to research and gain more information on this matter. The paper describes a couple of methods that can be used to generate 3D face morphs. The face morphs that are generated using this method are then compared to the contributing faces to obtain similarity scores. The highest MMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked with look-a-like morphs.
</details>
<details>
<summary>摘要</summary>
近年来，人脸识别系统得到了主流的推广，归功于硬件和软件的发展。一直在努力使其更加完善和安全。这也导致了3D人脸识别系统（3DFR）的快速发展，被期望能够超越2D人脸识别系统（2DFR）的一些 limitation。其中一个2DFR系统面临的问题是人脸图像杂化（morphing），目前在这个领域进行了大量的研究，以生成高质量的人脸杂化和攻击检测。然而，对于3DFR系统对3D人脸杂化的抵抗能力的理解仍然较少。但是，预期3DFR系统能够更加强健地对抗这些攻击。本文尝试了对这个问题进行研究，并描述了一些可以用于生成3D人脸杂化的方法。生成的人脸杂化与贡献人脸进行比较，以获得相似度分数。在3DFRS遭受look-a-like杂化攻击时，最高的MMPMR为40%，RMMR为41.76%。
</details></li>
</ul>
<hr>
<h2 id="AutoPET-Challenge-2023-Sliding-Window-based-Optimization-of-U-Net"><a href="#AutoPET-Challenge-2023-Sliding-Window-based-Optimization-of-U-Net" class="headerlink" title="AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net"></a>AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12114">http://arxiv.org/abs/2309.12114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matt3o/autopet2-submission">https://github.com/matt3o/autopet2-submission</a></li>
<li>paper_authors: Matthias Hadlich, Zdravko Marinov, Rainer Stiefelhagen</li>
<li>for: The paper is written for researchers and developers working on tumor segmentation in medical imaging, particularly those using FDG-PET&#x2F;CT scans.</li>
<li>methods: The paper uses a dataset of 1014 FDG-PET&#x2F;CT studies to challenge researchers to develop accurate tumor segmentation methods that can distinguish between tumor-specific uptake and physiological uptake in normal tissues.</li>
<li>results: The paper provides a dataset of FDG-PET&#x2F;CT scans for researchers to use in developing and testing their tumor segmentation methods, with the goal of improving the accuracy of tumor segmentation in clinical practice.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为医学成像中的肿瘤分 segmentation研究人员和开发者写的，特别是使用FDG-PET&#x2F;CT扫描的。</li>
<li>methods: 这篇论文使用1014个FDG-PET&#x2F;CT成像数据集来挑战研究人员开发准确的肿瘤分 segmentation方法，能够将肿瘤吸收与正常组织的吸收区分开。</li>
<li>results: 这篇论文提供了1014个FDG-PET&#x2F;CT成像数据集，用于研究人员开发和测试他们的肿瘤分 segmentation方法，以提高临床中肿瘤分 segmentation的准确性。<details>
<summary>Abstract</summary>
Tumor segmentation in medical imaging is crucial and relies on precise delineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is widely used in clinical practice to detect metabolically active tumors. However, FDG-PET scans may misinterpret irregular glucose consumption in healthy or benign tissues as cancer. Combining PET with Computed Tomography (CT) can enhance tumor segmentation by integrating metabolic and anatomic information. FDG-PET/CT scans are pivotal for cancer staging and reassessment, utilizing radiolabeled fluorodeoxyglucose to highlight metabolically active regions. Accurately distinguishing tumor-specific uptake from physiological uptake in normal tissues is a challenging aspect of precise tumor segmentation. The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT studies, encouraging advancements in accurate tumor segmentation and analysis within the FDG-PET/CT domain. Code: https://github.com/matt3o/AutoPET2-Submission/
</details>
<details>
<summary>摘要</summary>
肿体分割在医学成像中非常重要，需要精准地界定。 fluorodeoxyglucosePositron-Emission Tomography（FDG-PET）在临床实践中广泛应用，用于检测具有异常代谢活性的肿体。然而，FDG-PET扫描可能会误分辨健康或正常组织中的不规则糖分消耗为癌症。将PET与计算机成像（CT）结合可以提高肿体分割，将元素学和解剖信息结合起来。FDG-PET/CT扫描是癌症评估和重新评估中非常重要的，通过使用标记的 fluorodeoxyglucose来高亮具有代谢活性的区域。正确地从正常组织中的代谢吸收中分化出肿体特有的吸收是精准肿体分割的挑战之一。AutoPET挑战提供了1014个FDG-PET/CT研究数据集，鼓励技术创新，以提高FDG-PET/CT频谱中的准确肿体分割和分析。代码：https://github.com/matt3o/AutoPET2-Submission/
</details></li>
</ul>
<hr>
<h2 id="Exploiting-CLIP-based-Multi-modal-Approach-for-Artwork-Classification-and-Retrieval"><a href="#Exploiting-CLIP-based-Multi-modal-Approach-for-Artwork-Classification-and-Retrieval" class="headerlink" title="Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval"></a>Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12110">http://arxiv.org/abs/2309.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo</li>
<li>for:  investigate the application of recent CLIP model in artwork domain tasks</li>
<li>methods:  use semantically dense textual supervision to train visual models</li>
<li>results:  impressive zero-shot classification results and promising results in artwork-to-artwork and description-to-artwork domain<details>
<summary>Abstract</summary>
Given the recent advances in multimodal image pretraining where visual models trained with semantically dense textual supervision tend to have better generalization capabilities than those trained using categorical attributes or through unsupervised techniques, in this work we investigate how recent CLIP model can be applied in several tasks in artwork domain. We perform exhaustive experiments on the NoisyArt dataset which is a dataset of artwork images crawled from public resources on the web. On such dataset CLIP achieves impressive results on (zero-shot) classification and promising results in both artwork-to-artwork and description-to-artwork domain.
</details>
<details>
<summary>摘要</summary>
With the recent advances in multimodal image pretraining, visual models trained with semantically dense textual supervision have shown better generalization capabilities compared to those trained using categorical attributes or unsupervised techniques. In this work, we explore the application of the recent CLIP model in various tasks within the artwork domain.We conduct exhaustive experiments on the NoisyArt dataset, a collection of artwork images crawled from public resources on the web. On this dataset, CLIP achieves impressive results in zero-shot classification and promising results in both artwork-to-artwork and description-to-artwork domains.Here's the translation in Simplified Chinese:近期多modal图像预训练的进步，使用语义密集的文本监督训练的视觉模型在泛化能力方面表现出色，比使用分类属性或无监督技术训练的模型更好。在这个工作中，我们探索了最近的CLIP模型在艺术领域中的应用，并在NoisyArt数据集上进行了极限性的实验。在NoisyArt数据集上，CLIP在零shot分类和描述到图像领域中表现出了惊人的成绩，并在描述到图像和艺术作品之间的领域中表现出了可期的成绩。
</details></li>
</ul>
<hr>
<h2 id="FourierLoss-Shape-Aware-Loss-Function-with-Fourier-Descriptors"><a href="#FourierLoss-Shape-Aware-Loss-Function-with-Fourier-Descriptors" class="headerlink" title="FourierLoss: Shape-Aware Loss Function with Fourier Descriptors"></a>FourierLoss: Shape-Aware Loss Function with Fourier Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12106">http://arxiv.org/abs/2309.12106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehmet Bahadir Erden, Selahattin Cansiz, Onur Caki, Haya Khattak, Durmus Etiz, Melek Cosar Yakar, Kerem Duruer, Berke Barut, Cigdem Gunduz-Demir</li>
<li>for: 这个研究是为了提高医疗影像分类 задачі中的网络模型，以提高精确性。</li>
<li>methods: 这个研究使用了Encoder-decoder网络，并导入了一个新的形状意识损失函数（FourierLoss），以强制网络在训练过程中将物体的形状独特性考虑在内。</li>
<li>results: 实验结果显示，这个灵活的形状意识损失函数可以将医疗影像分类精确性提高，比其他方法更好。<details>
<summary>Abstract</summary>
Encoder-decoder networks become a popular choice for various medical image segmentation tasks. When they are trained with a standard loss function, these networks are not explicitly enforced to preserve the shape integrity of an object in an image. However, this ability of the network is important to obtain more accurate results, especially when there is a low-contrast difference between the object and its surroundings. In response to this issue, this work introduces a new shape-aware loss function, which we name FourierLoss. This loss function relies on quantifying the shape dissimilarity between the ground truth and the predicted segmentation maps through the Fourier descriptors calculated on their objects, and penalizing this dissimilarity in network training. Different than the previous studies, FourierLoss offers an adaptive loss function with trainable hyperparameters that control the importance of the level of the shape details that the network is enforced to learn in the training process. This control is achieved by the proposed adaptive loss update mechanism, which end-to-end learns the hyperparameters simultaneously with the network weights by backpropagation. As a result of using this mechanism, the network can dynamically change its attention from learning the general outline of an object to learning the details of its contour points, or vice versa, in different training epochs. Working on 2879 computed tomography images of 93 subjects, our experiments revealed that the proposed adaptive shape-aware loss function led to statistically significantly better results for liver segmentation, compared to its counterparts.
</details>
<details>
<summary>摘要</summary>
现代编码器-解码器网络在医疗图像分割任务中变得越来越受欢迎。当这些网络被标准损失函数训练时，它们不会显式地保持图像中对象的形状完整性。然而，这种网络的能力是获得更加准确的结果的关键，特别是在对象和周围环境之间存在低对比度的情况下。为解决这个问题，本研究提出了一种新的形状意识损失函数，我们称之为FourierLoss。这个损失函数基于计算对象的真实值和预测分割图像中对象的形状差异的福氏描述子，并对这种差异进行惩罚。与前一些研究不同，FourierLoss提供了一个可调参数的损失函数，通过反向传播来动态地更新参数。这种机制使得网络可以在训练过程中动态地变换注意力，从学习对象的大致轮廓到学习对象的细节点，或者vice versa。我们在2879个计算Tomography图像上进行了93个subject的实验，结果显示，提出的适应形状意识损失函数在肝 segmentation  task中比其他方法更为 statistically significantly better。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-sparsification-for-deep-neural-networks-with-Bayesian-model-reduction"><a href="#Bayesian-sparsification-for-deep-neural-networks-with-Bayesian-model-reduction" class="headerlink" title="Bayesian sparsification for deep neural networks with Bayesian model reduction"></a>Bayesian sparsification for deep neural networks with Bayesian model reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12095">http://arxiv.org/abs/2309.12095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrije Marković, Karl J. Friston, Stefan J. Kiebel</li>
<li>for: 本研究的目的是提出一种更高效的权重缩减方法，以优化深度学习模型的计算效率和性能。</li>
<li>methods: 本研究使用了权重缩减技术，包括权重缩减和模型减少。具体来说，研究人员使用了权重缩减的bayesian模型，并采用了黑盒Stochastic Variational Inference（SVI）算法来实现权重缩减。</li>
<li>results: 研究人员通过比较bayesian模型和SVI算法的计算效率和缩减率，发现bayesian模型的计算效率明显高于SVI算法，而且bayesian模型可以更好地缩减模型参数。此外，研究人员还通过应用 bayesian模型和SVI算法于不同的深度学习架构，包括LeNet、Vision Transformers和MLP-Mixers等，发现bayesian模型可以在这些架构上实现更高效的缩减。<details>
<summary>Abstract</summary>
Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimination of redundant model weights based on the posterior estimates under a straightforward (non-hierarchical) generative model. Our comparative study highlights the computational efficiency and the pruning rate of the BMR method relative to the established stochastic variational inference (SVI) scheme, when applied to the full hierarchical generative model. We illustrate the potential of BMR to prune model parameters across various deep learning architectures, from classical networks like LeNet to modern frameworks such as Vision Transformers and MLP-Mixers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Task-Cooperative-Learning-via-Searching-for-Flat-Minima"><a href="#Multi-Task-Cooperative-Learning-via-Searching-for-Flat-Minima" class="headerlink" title="Multi-Task Cooperative Learning via Searching for Flat Minima"></a>Multi-Task Cooperative Learning via Searching for Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12090">http://arxiv.org/abs/2309.12090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuping Wu, Le Zhang, Yang Sun, Yuanhan Mo, Thomas Nichols, Bartlomiej W. Papiez</li>
<li>for: 提高医疗图像分析的通用性和个别任务性能</li>
<li>methods: 提出了一种多任务学习的多级优化问题解决方案，使得特征从不同任务中学习共同逻辑</li>
<li>results: 在三个公开 dataset 上验证了方法的效果，比靶前方法更加有优势，表现出合作学习的优势<details>
<summary>Abstract</summary>
Multi-task learning (MTL) has shown great potential in medical image analysis, improving the generalizability of the learned features and the performance in individual tasks. However, most of the work on MTL focuses on either architecture design or gradient manipulation, while in both scenarios, features are learned in a competitive manner. In this work, we propose to formulate MTL as a multi/bi-level optimization problem, and therefore force features to learn from each task in a cooperative approach. Specifically, we update the sub-model for each task alternatively taking advantage of the learned sub-models of the other tasks. To alleviate the negative transfer problem during the optimization, we search for flat minima for the current objective function with regard to features from other tasks. To demonstrate the effectiveness of the proposed approach, we validate our method on three publicly available datasets. The proposed method shows the advantage of cooperative learning, and yields promising results when compared with the state-of-the-art MTL approaches. The code will be available online.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）在医疗图像分析中表现出了很大的潜力，提高了学习到的特征的通用性和个别任务的性能。然而，大多数MTL工作都集中在架构设计或梯度修正方面，在这两种情况下，特征是在竞争性下学习的。在这个工作中，我们提议将MTL形式为多/双级优化问题，因此让特征从每个任务中学习到的方式是协力的。specifically，我们在每个任务中更新子模型，利用其他任务的学习到的子模型。为了避免优化过程中的负转移问题，我们通过搜索当前目标函数中特征的平坦顶点来缓解负转移问题。为了证明提议的效果，我们在三个公共可用的数据集上验证了我们的方法。提议的方法表现出了协力学习的优势，并与状态的MTL方法进行比较而显示了承诺的结果。代码将在线上公开。
</details></li>
</ul>
<hr>
<h2 id="Self-Calibrating-Fully-Differentiable-NLOS-Inverse-Rendering"><a href="#Self-Calibrating-Fully-Differentiable-NLOS-Inverse-Rendering" class="headerlink" title="Self-Calibrating, Fully Differentiable NLOS Inverse Rendering"></a>Self-Calibrating, Fully Differentiable NLOS Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12047">http://arxiv.org/abs/2309.12047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiseok Choi, Inchul Kim, Dongyoung Choi, Julio Marco, Diego Gutierrez, Min H. Kim</li>
<li>For: The paper aims to improve the reconstruction of hidden scenes in non-line-of-sight (NLOS) imaging by introducing a fully-differentiable end-to-end pipeline that self-calibrates imaging parameters during the reconstruction process.* Methods: The paper uses a combination of diffraction-based volumetric NLOS reconstruction, path-space light transport, and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes. The pipeline is fully differentiable, allowing for gradient descent optimization of imaging parameters.* Results: The paper demonstrates the robustness of the method to consistently reconstruct geometry and albedo, even under significant noise levels. The end-to-end pipeline is able to self-calibrate imaging parameters and produce high-quality reconstructions without the need for manual selection of filtering functions or parameters.<details>
<summary>Abstract</summary>
Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct hidden scenes by inverting the optical paths of indirect illumination measured at visible relay surfaces. These methods are prone to reconstruction artifacts due to inversion ambiguities and capture noise, which are typically mitigated through the manual selection of filtering functions and parameters. We introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline that self-calibrates the imaging parameters during the reconstruction of hidden scenes, using as input only the measured illumination while working both in the time and frequency domains. Our pipeline extracts a geometric representation of the hidden scene from NLOS volumetric intensities and estimates the time-resolved illumination at the relay wall produced by such geometric information using differentiable transient rendering. We then use gradient descent to optimize imaging parameters by minimizing the error between our simulated time-resolved illumination and the measured illumination. Our end-to-end differentiable pipeline couples diffraction-based volumetric NLOS reconstruction with path-space light transport and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes. We demonstrate the robustness of our method to consistently reconstruct geometry and albedo, even under significant noise levels.
</details>
<details>
<summary>摘要</summary>
现有的非直视（NLOS）成像方法通过推算光路的媒体映射来重建隐藏的场景。这些方法容易受到重建残像的影响，这些残像通常通过手动选择筛选函数和参数来减轻。我们介绍了一个完全可导的终端到终点NLOS反向渲染管道，该管道在重建隐藏场景时自动调整成像参数，使用直接推算光路来估算隐藏场景中的光学信息，并使用梯度下降来优化成像参数。我们的管道使用干扰基于Diffraction的NLOS成像，并结合路径空间光传输和简单的RAY marching技术来提取隐藏场景的详细、稠密的表面点和法向量。我们示示了我们方法在噪音水平较高时仍能顺利重建场景的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Image-Borders-Learning-Feature-Extrapolation-for-Unbounded-Image-Composition"><a href="#Beyond-Image-Borders-Learning-Feature-Extrapolation-for-Unbounded-Image-Composition" class="headerlink" title="Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition"></a>Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12042">http://arxiv.org/abs/2309.12042</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuxiaoyu1104/unic">https://github.com/liuxiaoyu1104/unic</a></li>
<li>paper_authors: Xiaoyu Liu, Ming Liu, Junyi Li, Shuai Liu, Xiaotao Wang, Lei Lei, Wangmeng Zuo</li>
<li>for: 提高图像组合和美观品质，避免图像剪辑方法的局限性。</li>
<li>methods: 提出一种联合框架，包括图像预览帧为输入，并提供不受图像边界限制的视图调整建议，以及通过特征拟合扩展视场来提高视图调整预测精度。</li>
<li>results: 通过对 existed image cropping datasets 进行实验，证明了 UNIC 在无限制的视图调整和图像组合中的效果。源代码、数据集和预训练模型可以在 <a target="_blank" rel="noopener" href="https://github.com/liuxiaoyu1104/UNIC">https://github.com/liuxiaoyu1104/UNIC</a> 上获取。<details>
<summary>Abstract</summary>
For improving image composition and aesthetic quality, most existing methods modulate the captured images by striking out redundant content near the image borders. However, such image cropping methods are limited in the range of image views. Some methods have been suggested to extrapolate the images and predict cropping boxes from the extrapolated image. Nonetheless, the synthesized extrapolated regions may be included in the cropped image, making the image composition result not real and potentially with degraded image quality. In this paper, we circumvent this issue by presenting a joint framework for both unbounded recommendation of camera view and image composition (i.e., UNIC). In this way, the cropped image is a sub-image of the image acquired by the predicted camera view, and thus can be guaranteed to be real and consistent in image quality. Specifically, our framework takes the current camera preview frame as input and provides a recommendation for view adjustment, which contains operations unlimited by the image borders, such as zooming in or out and camera movement. To improve the prediction accuracy of view adjustment prediction, we further extend the field of view by feature extrapolation. After one or several times of view adjustments, our method converges and results in both a camera view and a bounding box showing the image composition recommendation. Extensive experiments are conducted on the datasets constructed upon existing image cropping datasets, showing the effectiveness of our UNIC in unbounded recommendation of camera view and image composition. The source code, dataset, and pretrained models is available at https://github.com/liuxiaoyu1104/UNIC.
</details>
<details>
<summary>摘要</summary>
For improving image composition and aesthetic quality, most existing methods delete unnecessary content near the image borders. However, such image cropping methods are limited in the range of image views. Some methods have been suggested to predict cropping boxes from the extrapolated image. However, the synthesized extrapolated regions may be included in the cropped image, making the image composition result not real and potentially with degraded image quality. In this paper, we overcome this issue by presenting a joint framework for both unbounded recommendation of camera view and image composition (i.e., UNIC). In this way, the cropped image is a sub-image of the image acquired by the predicted camera view, and thus can be guaranteed to be real and consistent in image quality. Specifically, our framework takes the current camera preview frame as input and provides a recommendation for view adjustment, which contains operations unlimited by the image borders, such as zooming in or out and camera movement. To improve the prediction accuracy of view adjustment prediction, we further extend the field of view by feature extrapolation. After one or several times of view adjustments, our method converges and results in both a camera view and a bounding box showing the image composition recommendation. Extensive experiments are conducted on the datasets constructed upon existing image cropping datasets, showing the effectiveness of our UNIC in unbounded recommendation of camera view and image composition. The source code, dataset, and pretrained models are available at https://github.com/liuxiaoyu1104/UNIC.
</details></li>
</ul>
<hr>
<h2 id="BASE-Probably-a-Better-Approach-to-Multi-Object-Tracking"><a href="#BASE-Probably-a-Better-Approach-to-Multi-Object-Tracking" class="headerlink" title="BASE: Probably a Better Approach to Multi-Object Tracking"></a>BASE: Probably a Better Approach to Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12035">http://arxiv.org/abs/2309.12035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Vonheim Larsen, Sigmund Rolfsjord, Daniel Gusland, Jörgen Ahlberg, Kim Mathiassen</li>
<li>for: The paper is written for the field of visual object tracking, specifically to address the lack of probabilistic methods in the leaderboards and to propose a set of pragmatic models to improve the performance of probabilistic trackers.</li>
<li>methods: The paper proposes a new probabilistic tracking algorithm called BASE (Bayesian Approximation Single-hypothesis Estimator), which addresses the challenges of distance in target kinematics, detector confidence, and non-uniform clutter characteristics.</li>
<li>results: The paper achieves state-of-the-art (SOTA) performance on MOT17 and MOT20 without using Re-Id, demonstrating the effectiveness of the proposed approach.Here is the information in Simplified Chinese text:</li>
<li>for: 本文为视觉对象跟踪领域的研究，旨在解决现有的概率方法在领导人员中的缺失，并提出一些实用的模型来提高概率跟踪器的性能。</li>
<li>methods: 本文提出了一种新的概率跟踪算法 called BASE ( bayesian Approximation Single-hypothesis Estimator)，该算法 Addresses 目标动态距离、检测器信任度和非uniform的干扰特征等挑战。</li>
<li>results: 本文在 MOT17 和 MOT20 上达到了 state-of-the-art 性能，不使用 Re-Id，demonstrating 提出的方法的有效性。<details>
<summary>Abstract</summary>
The field of visual object tracking is dominated by methods that combine simple tracking algorithms and ad hoc schemes. Probabilistic tracking algorithms, which are leading in other fields, are surprisingly absent from the leaderboards. We found that accounting for distance in target kinematics, exploiting detector confidence and modelling non-uniform clutter characteristics is critical for a probabilistic tracker to work in visual tracking. Previous probabilistic methods fail to address most or all these aspects, which we believe is why they fall so far behind current state-of-the-art (SOTA) methods (there are no probabilistic trackers in the MOT17 top 100). To rekindle progress among probabilistic approaches, we propose a set of pragmatic models addressing these challenges, and demonstrate how they can be incorporated into a probabilistic framework. We present BASE (Bayesian Approximation Single-hypothesis Estimator), a simple, performant and easily extendible visual tracker, achieving state-of-the-art (SOTA) on MOT17 and MOT20, without using Re-Id. Code will be made available at https://github.com/ffi-no
</details>
<details>
<summary>摘要</summary>
“Visual object tracking 领域由简单追踪算法和对应措施组合所控制。 probabilistic 追踪算法，在其他领域中是领先的，在 visual tracking 中却缺乏表现。我们发现，在目标运动中考虑距离、利用探测器信任度和非均匀杂质特征是critical的。 previous probabilistic methods 无法解决这些问题，我们认为这就是为什么它们落后现有的state-of-the-art（SOTA）方法（MOT17 top 100 中没有 probabilistic 追踪器）。为了推动 probabilistic 方法的进步，我们提出了一些实用的模型，并说明如何将它们集成到 probabilistic 框架中。我们提出了 BASE（Bayesian Approximation Single-hypothesis Estimator），一个简单、高效和易扩展的visual 追踪器，在 MOT17 和 MOT20 中获得了state-of-the-art 成绩，无需使用 Re-Id。我们将在 GitHub 上公开代码。”
</details></li>
</ul>
<hr>
<h2 id="Face-Identity-Aware-Disentanglement-in-StyleGAN"><a href="#Face-Identity-Aware-Disentanglement-in-StyleGAN" class="headerlink" title="Face Identity-Aware Disentanglement in StyleGAN"></a>Face Identity-Aware Disentanglement in StyleGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12033">http://arxiv.org/abs/2309.12033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Suwała, Bartosz Wójcik, Magdalena Proszewska, Jacek Tabor, Przemysław Spurek, Marek Śmieja</li>
<li>for: 本文主要用于解决现有 Conditional GANs 模型中的一个问题，即同时修改图像中的一些特征，而不是只修改请求的特征。</li>
<li>methods: 本文提出了一种名为 PluGeN4Faces 的插件，用于修改 face 图像中的特征，同时保持图像的人脸特征不变。该方法通过在 Movie Frames 中提取图像，并使用一种类型的对比损失函数，来让模型将同一个人的图像分组在 latent 空间中相似的地方。</li>
<li>results: 实验表明，PluGeN4Faces 比现有状态的 искус智模型更能减少修改 face 特征所带来的影响。<details>
<summary>Abstract</summary>
Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> conditional GANs frequently used manipulating face image attributes, such as expression, hairstyle, pose, or age. although state-of-the-art models successfully modify requested attributes, simultaneously modify important image characteristics, such as person's identity. in this paper, focus on solving problem by introducing PluGeN4Faces, StyleGAN plugin, explicitly disentangles face attributes from person's identity. our key idea perform training images retrieved movie frames, given person appears various poses different attributes. applying type contrastive loss, encourage model group images same person similar regions latent space. our experiments demonstrate modifications face attributes performed PluGeN4Faces significantly less invasive remaining image characteristics than existing state-of-the-art models.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Hidden-Realm-Self-supervised-Skeleton-based-Action-Recognition-in-Occluded-Environments"><a href="#Unveiling-the-Hidden-Realm-Self-supervised-Skeleton-based-Action-Recognition-in-Occluded-Environments" class="headerlink" title="Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments"></a>Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12029">http://arxiv.org/abs/2309.12029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyfml/opstl">https://github.com/cyfml/opstl</a></li>
<li>paper_authors: Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 提高自主 robotic 系统中的动作识别率，考虑到目标 occlusion 的情况。</li>
<li>methods: 提议使用 occluded skeleton 序列 pré-train，然后使用 k-means 聚类（KMeans）对序列嵌入进行分组，并使用 K-nearest-neighbor（KNN）填充 missing skeleton 数据。</li>
<li>results: 对 NTURGB+D 60 和 NTURGB+D 120 的 occluded 版本进行验证，证明了我们的填充方法的效iveness。<details>
<summary>Abstract</summary>
To integrate action recognition methods into autonomous robotic systems, it is crucial to consider adverse situations involving target occlusions. Such a scenario, despite its practical relevance, is rarely addressed in existing self-supervised skeleton-based action recognition methods. To empower robots with the capacity to address occlusion, we propose a simple and effective method. We first pre-train using occluded skeleton sequences, then use k-means clustering (KMeans) on sequence embeddings to group semantically similar samples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeleton data based on the closest sample neighbors. Imputing incomplete skeleton sequences to create relatively complete sequences as input provides significant benefits to existing skeleton-based self-supervised models. Meanwhile, building on the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introduce an Occluded Partial Spatio-Temporal Learning (OPSTL) framework. This enhancement utilizes Adaptive Spatial Masking (ASM) for better use of high-quality, intact skeletons. The effectiveness of our imputation methods is verified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D 120. The source code will be made publicly available at https://github.com/cyfml/OPSTL.
</details>
<details>
<summary>摘要</summary>
要将动作识别方法 integrate 到自主 роботи系统中，需要考虑目标 occlusion 的情况。这种情况尚未在现有的自助学习骨架基于动作识别方法中得到充分考虑。为了赋给机器人更多的能力，我们提出了一种简单有效的方法。我们首先使用 occluded 骨架序列进行预训练，然后使用 K-means 聚类（KMeans）对序列嵌入进行分组。接着，我们使用 K-nearest-neighbor（KNN）来填充 missing 骨架数据，基于最近的样本 neighors。填充不完整的骨架序列，以创建相对完整的输入，对现有骨架基于自助学习模型具有重要的优化。此外，我们在 Partial Spatio-Temporal Learning（PSTL）的基础之上，引入 Occluded Partial Spatio-Temporal Learning（OPSTL）框架。这种改进使用 Adaptive Spatial Masking（ASM）来更好地利用高质量、完整的骨架。我们的填充方法的效果被证明在NTURGB+D 60 和 NTURGB+D 120 的 occluded 版本上。源代码将在 GitHub 上公开，可以通过 https://github.com/cyfml/OPSTL 获取。
</details></li>
</ul>
<hr>
<h2 id="Precision-in-Building-Extraction-Comparing-Shallow-and-Deep-Models-using-LiDAR-Data"><a href="#Precision-in-Building-Extraction-Comparing-Shallow-and-Deep-Models-using-LiDAR-Data" class="headerlink" title="Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data"></a>Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12027">http://arxiv.org/abs/2309.12027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sulaiman, Mina Farmanbar, Ahmed Nabil Belbachir, Chunming Rong</li>
<li>For: 本文使用 LiDAR 数据进行检测建筑物的深度学习模型，以提高建筑物的分割精度。* Methods: 本文使用了 shallow models，并使用了边界面掩模来提高 BIoU 分数。* Results:  shallow models 在 IoU 分数上出perform deep learning models 8%，但是 deep learning models 在 BIoU 分数上表现更好。边界面掩模可以提高 BIoU 分数4%。 LightGBM 表现比 RF 和 XGBoost 更好。<details>
<summary>Abstract</summary>
Building segmentation is essential in infrastructure development, population management, and geological observations. This article targets shallow models due to their interpretable nature to assess the presence of LiDAR data for supervised segmentation. The benchmark data used in this article are published in NORA MapAI competition for deep learning model. Shallow models are compared with deep learning models based on Intersection over Union (IoU) and Boundary Intersection over Union (BIoU). In the proposed work, boundary masks from the original mask are generated to improve the BIoU score, which relates to building shapes' borderline. The influence of LiDAR data is tested by training the model with only aerial images in task 1 and a combination of aerial and LiDAR data in task 2 and then compared. shallow models outperform deep learning models in IoU by 8% using aerial images (task 1) only and 2% in combined aerial images and LiDAR data (task 2). In contrast, deep learning models show better performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and Extreme Gradient Boosting (XGBoost).
</details>
<details>
<summary>摘要</summary>
In the proposed work, boundary masks are generated from the original mask to improve the BIoU score, which relates to building shapes' borderlines. The influence of LiDAR data is tested by training the model with only aerial images in Task 1 and a combination of aerial and LiDAR data in Task 2, and then comparing the results.Shallow models outperform deep learning models in IoU by 8% using aerial images (Task 1) only and 2% in combined aerial images and LiDAR data (Task 2). In contrast, deep learning models show better performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and Extreme Gradient Boosting (XGBoost).
</details></li>
</ul>
<hr>
<h2 id="Convolution-and-Attention-Mixer-for-Synthetic-Aperture-Radar-Image-Change-Detection"><a href="#Convolution-and-Attention-Mixer-for-Synthetic-Aperture-Radar-Image-Change-Detection" class="headerlink" title="Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection"></a>Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12010">http://arxiv.org/abs/2309.12010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/summitgao/camixer">https://github.com/summitgao/camixer</a></li>
<li>paper_authors: Haopeng Zhang, Zijing Lin, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li</li>
<li>for:  This paper focuses on improving the performance of synthetic aperture radar (SAR) change detection by incorporating global attention mechanism into Transformer-like architecture.</li>
<li>methods:  The proposed method, called Convolution and Attention Mixer (CAMixer), combines self-attention with shift convolution in a parallel way, and adopts a gating mechanism in the feed-forward network to enhance the non-linear feature transformation.</li>
<li>results:  The proposed CAMixer achieves superior performance in SAR change detection compared to existing CNN-based methods, as demonstrated by extensive experiments conducted on three SAR datasets.<details>
<summary>Abstract</summary>
Synthetic aperture radar (SAR) image change detection is a critical task and has received increasing attentions in the remote sensing community. However, existing SAR change detection methods are mainly based on convolutional neural networks (CNNs), with limited consideration of global attention mechanism. In this letter, we explore Transformer-like architecture for SAR change detection to incorporate global attention. To this end, we propose a convolution and attention mixer (CAMixer). First, to compensate the inductive bias for Transformer, we combine self-attention with shift convolution in a parallel way. The parallel design effectively captures the global semantic information via the self-attention and performs local feature extraction through shift convolution simultaneously. Second, we adopt a gating mechanism in the feed-forward network to enhance the non-linear feature transformation. The gating mechanism is formulated as the element-wise multiplication of two parallel linear layers. Important features can be highlighted, leading to high-quality representations against speckle noise. Extensive experiments conducted on three SAR datasets verify the superior performance of the proposed CAMixer. The source codes will be publicly available at https://github.com/summitgao/CAMixer .
</details>
<details>
<summary>摘要</summary>
“干扰天线射频图像变化检测（SAR）是远感社区中的一个重要任务，但现有的SAR变化检测方法主要基于卷积神经网络（CNN），对于全球注意机制的考虑有限。在本封信中，我们探索了Transformer-like架构来进行SAR变化检测，以内置全球注意机制。为此，我们提出了一个混合卷积和注意混合器（CAMixer）。首先，为了补偿对Transformer的传播偏见，我们在平行的方式结合了自我注意和偏移核函数。这样的平行设计可以同时捕捉全球semantic信息和本地特征特性，从而实现高质量的特征抽象。其次，我们在对待网络中引入了阈值机制，以增强非线性特征转换。这个阈值机制是通过两个平行的线性层进行元素ごとの多项式乘法。重要的特征可以得到高质量的表现，抵制杂音。实验结果显示，我们的CAMixer具有较高的检测性和稳定性，并且可以实现高质量的特征抽象。我们将代码公开于https://github.com/summitgao/CAMixer。”
</details></li>
</ul>
<hr>
<h2 id="Elevating-Skeleton-Based-Action-Recognition-with-Efficient-Multi-Modality-Self-Supervision"><a href="#Elevating-Skeleton-Based-Action-Recognition-with-Efficient-Multi-Modality-Self-Supervision" class="headerlink" title="Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision"></a>Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12009">http://arxiv.org/abs/2309.12009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 本研究旨在提高人体动作识别的自助学习表示。</li>
<li>methods: 我们首先提出了一种偏知交换模块（IKEM），以避免低性能Modalities之间的偏知传递。然后，我们提出了三种新的Modalities，以增强多modalities之间的补充信息。最后，我们提出了一种新的教师生框架，以在引入新Modalities时保持效率，并将第二 modalities中的知识透传到必要modalities中，基于约束anchors, positives和negatives的关系。</li>
<li>results: 实验结果表明，我们的方法有效地提高了skeleton基于多modalities数据的人体动作识别性能。<details>
<summary>Abstract</summary>
Self-supervised representation learning for human action recognition has developed rapidly in recent years. Most of the existing works are based on skeleton data while using a multi-modality setup. These works overlooked the differences in performance among modalities, which led to the propagation of erroneous knowledge between modalities while only three fundamental modalities, i.e., joints, bones, and motions are used, hence no additional modalities are explored.   In this work, we first propose an Implicit Knowledge Exchange Module (IKEM) which alleviates the propagation of erroneous knowledge between low-performance modalities. Then, we further propose three new modalities to enrich the complementary information between modalities. Finally, to maintain efficiency when introducing new modalities, we propose a novel teacher-student framework to distill the knowledge from the secondary modalities into the mandatory modalities considering the relationship constrained by anchors, positives, and negatives, named relational cross-modality knowledge distillation. The experimental results demonstrate the effectiveness of our approach, unlocking the efficient use of skeleton-based multi-modality data. Source code will be made publicly available at https://github.com/desehuileng0o0/IKEM.
</details>
<details>
<summary>摘要</summary>
自我监睹表示学习人体动作识别在最近几年内得到了迅速发展。大多数现有工作基于骨骼数据，使用多模态设置。这些工作忽视了不同模态之间的性能差异，导致错误知识的传播 между模态，只有三种基本模态，即关节、骨骼和运动，因此没有探索其他模态。  在这项工作中，我们首先提出了隐式知识交换模块（IKEM），以消除低性能模态之间的错误知识传播。然后，我们进一步提出了三种新的模态，以增加多模态之间的补充信息。最后，为保持效率而不是引入新模态，我们提出了一种新的教师-学生框架，通过约束anchors、正例和负例之间的关系，将次要模态中的知识透传到必要模态中，称为关系跨模态知识采样。实验结果表明我们的方法的效果，使得骨骼基于多模态数据的高效使用成为可能。源代码将在https://github.com/desehuileng0o0/IKEM公开。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-pneumonia-on-chest-x-ray-images-through-machine-learning"><a href="#Identification-of-pneumonia-on-chest-x-ray-images-through-machine-learning" class="headerlink" title="Identification of pneumonia on chest x-ray images through machine learning"></a>Identification of pneumonia on chest x-ray images through machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11995">http://arxiv.org/abs/2309.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Nabeel-105/Covid-19-and-Pneumonia-Detection-Using-Chest-Xray-Images-Full-Desktop-Application-">https://github.com/Nabeel-105/Covid-19-and-Pneumonia-Detection-Using-Chest-Xray-Images-Full-Desktop-Application-</a></li>
<li>paper_authors: Eduardo Augusto Roeder</li>
<li>for: 这个研究的目的是开发一种用于识别胸部X光图像中的肺炎病例的软件。</li>
<li>methods: 这个研究使用了机器学习技术，特别是传输学习技术，并使用了一个计算模型来训练。</li>
<li>results: 经过训练后，模型可以准确地识别胸部X光图像中的肺炎病例，达到了98%的敏感性和97.3%的特异性。<details>
<summary>Abstract</summary>
Pneumonia is the leading infectious cause of infant death in the world. When identified early, it is possible to alter the prognosis of the patient, one could use imaging exams to help in the diagnostic confirmation. Performing and interpreting the exams as soon as possible is vital for a good treatment, with the most common exam for this pathology being chest X-ray. The objective of this study was to develop a software that identify the presence or absence of pneumonia in chest radiographs. The software was developed as a computational model based on machine learning using transfer learning technique. For the training process, images were collected from a database available online with children's chest X-rays images taken at a hospital in China. After training, the model was then exposed to new images, achieving relevant results on identifying such pathology, reaching 98% sensitivity and 97.3% specificity for the sample used for testing. It can be concluded that it is possible to develop a software that identifies pneumonia in chest X-ray images.
</details>
<details>
<summary>摘要</summary>
全球最主要的感染性新生儿死亡原因是肺炎，早期诊断可以改善病人的结局。使用影像检查可以帮助诊断，其中最常用的检查是胸部X射线。本研究的目标是开发一种可以在胸部X射线图像中识别肺炎的软件。该软件是基于机器学习技术的计算模型，使用了传输学习技术进行训练。训练过程中，图像来自中国医院的儿童胸部X射线图像库。经训练后，模型被推出到新图像上，实现了识别肺炎的相关结果，具有98%的敏感度和97.3%的特异性。可以确定，可以开发一种识别肺炎在胸部X射线图像中的软件。
</details></li>
</ul>
<hr>
<h2 id="Neural-Stochastic-Screened-Poisson-Reconstruction"><a href="#Neural-Stochastic-Screened-Poisson-Reconstruction" class="headerlink" title="Neural Stochastic Screened Poisson Reconstruction"></a>Neural Stochastic Screened Poisson Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11993">http://arxiv.org/abs/2309.11993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silvia Sellán, Alec Jacobson</li>
<li>for:  reconstruction of a surface from a point cloud</li>
<li>methods:  neural network with Poisson smoothness prior</li>
<li>results:  addresses limitations of existing work and can be fully integrated into the 3D scanning pipeline<details>
<summary>Abstract</summary>
Reconstructing a surface from a point cloud is an underdetermined problem. We use a neural network to study and quantify this reconstruction uncertainty under a Poisson smoothness prior. Our algorithm addresses the main limitations of existing work and can be fully integrated into the 3D scanning pipeline, from obtaining an initial reconstruction to deciding on the next best sensor position and updating the reconstruction upon capturing more data.
</details>
<details>
<summary>摘要</summary>
重建表面从点云是一个不充分定义的问题。我们使用神经网络来研究和评估这种重建不确定性，采用波尼尔平滑性先验来做估计。我们的算法解决了现有工作的主要局限性，可以全面地整合到3D扫描管道中，从获取初始重建到决定下一个感知器位置并更新重建。
</details></li>
</ul>
<hr>
<h2 id="Crop-Row-Switching-for-Vision-Based-Navigation-A-Comprehensive-Approach-for-Efficient-Crop-Field-Navigation"><a href="#Crop-Row-Switching-for-Vision-Based-Navigation-A-Comprehensive-Approach-for-Efficient-Crop-Field-Navigation" class="headerlink" title="Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation"></a>Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11989">http://arxiv.org/abs/2309.11989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajitha de Silva, Grzegorz Cielniak, Junfeng Gao</li>
<li>for: 这个论文旨在提出一种基于视觉的移动机器人 navigate 方法，可以在耕地中横跨多个行。</li>
<li>methods: 该方法使用单个前置摄像头和深度数据进行RGB图像分割，以探测行末和下一行的重新入口点。</li>
<li>results: 在实际的甘蔗ibe字段中，该方法能够成功地使移动机器人从一行到下一行 navigate， WITH median errors 为19.25 cm和6.77°。<details>
<summary>Abstract</summary>
Vision-based mobile robot navigation systems in arable fields are mostly limited to in-row navigation. The process of switching from one crop row to the next in such systems is often aided by GNSS sensors or multiple camera setups. This paper presents a novel vision-based crop row-switching algorithm that enables a mobile robot to navigate an entire field of arable crops using a single front-mounted camera. The proposed row-switching manoeuvre uses deep learning-based RGB image segmentation and depth data to detect the end of the crop row, and re-entry point to the next crop row which would be used in a multi-state row switching pipeline. Each state of this pipeline use visual feedback or wheel odometry of the robot to successfully navigate towards the next crop row. The proposed crop row navigation pipeline was tested in a real sugar beet field containing crop rows with discontinuities, varying light levels, shadows and irregular headland surfaces. The robot could successfully exit from one crop row and re-enter the next crop row using the proposed pipeline with absolute median errors averaging at 19.25 cm and 6.77{\deg} for linear and rotational steps of the proposed manoeuvre.
</details>
<details>
<summary>摘要</summary>
视觉基于移动机器人Navigation系统通常仅限于行间导航。在这些系统中，从一行农作物到下一行的过程经常受GNSS传感器或多个摄像头的帮助。本文介绍了一种新的视觉基于的农作物行转换算法，使得移动机器人可以使用单个前置摄像头探测整个农作物场。提议的行转换举动使用深度学习基于RGB图像分割和深度数据探测农作物行的结束和下一行的重新入口点，并在多个状态的管道中使用视觉反馈或机器人轮胎的运动来成功导航到下一行农作物。这个管道在实际的甘蔗ibeet场中进行测试，包括具有不连续的农作物行、不同的照明水平、阴影和不规则的机器人进场面。机器人使用提议的管道成功地离开了一行农作物并重新进入下一行农作物， median误差平均值为19.25cm和6.77度 для直线和旋转步骤。
</details></li>
</ul>
<hr>
<h2 id="ZS6D-Zero-shot-6D-Object-Pose-Estimation-using-Vision-Transformers"><a href="#ZS6D-Zero-shot-6D-Object-Pose-Estimation-using-Vision-Transformers" class="headerlink" title="ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers"></a>ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11986">http://arxiv.org/abs/2309.11986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Ausserlechner, David Haberger, Stefan Thalhammer, Jean-Baptiste Weibel, Markus Vincze</li>
<li>for: recognize diverse objects in complex and unconstrained real-world scenarios</li>
<li>methods: use pre-trained Vision Transformers (ViT) to extract visual descriptors, and RANSAC-based PnP to estimate object’s 6D pose</li>
<li>results: improve the Average Recall on three datasets (LMO, YCBV, and TLESS) compared to two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning.<details>
<summary>Abstract</summary>
As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object's 6D pose with RANSAC-based PnP. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to one of the two methods we improve the Average Recall on all three datasets and compared to the second method we improve on two datasets.
</details>
<details>
<summary>摘要</summary>
As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a growing need to recognize diverse objects. However, current state-of-the-art 6D object pose estimation methods rely on object-specific training and do not generalize well to unseen objects. To address this issue, recent novel object pose estimation methods have used task-specific fine-tuned convolutional neural networks (CNNs) for deep template matching. However, this approach still requires expensive data rendering and training procedures.To overcome this limitation, we propose a novel zero-shot method for 6D object pose estimation, called ZS6D. Our approach uses visual descriptors extracted using pre-trained Vision Transformers (ViT) to match rendered templates against query images of objects, and establish local correspondences. These local correspondences are then used to estimate the object's 6D pose using RANSAC-based Perspective-n-Point (PnP).Experiments on three datasets (LMO, YCBV, and TLESS) show that our approach achieves a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Specifically, we improve the Average Recall on all three datasets compared to one of the two methods, and improve on two datasets compared to the second method.Here is the translation in Simplified Chinese:随着机器人系统遇到越来越复杂的实际场景，需要认izers多种物体。现状下的6D物体pose估计方法都是基于物体特定的训练，不能泛化到未看过的物体。为了解决这个问题，最新的novel object pose estimation方法都是使用任务特定的深度学习模型进行深度模板匹配。但是，这种方法仍需要费时的数据渲染和训练过程。为了突破这个局限性，我们提出了一种 zeroshot的6D物体pose估计方法，即ZS6D。我们的方法使用预训练的Vision Transformer（ViT）提取的视觉描述符来匹配渲染的模板和查询图像，并建立地方匹配。这些地方匹配然后用RANSAC基于Perspective-n-Point（PnP）来估计物体的6Dpose。在LMO、YCBV和TLESS三个dataset上进行了实验，我们发现我们的方法可以不需要任务特定的微调，就可以在这三个dataset上达到较好的性能。具体来说，我们在这三个dataset上的平均回归率都高于一个方法，并在两个dataset上高于另一个方法。
</details></li>
</ul>
<hr>
<h2 id="NeuralLabeling-A-versatile-toolset-for-labeling-vision-datasets-using-Neural-Radiance-Fields"><a href="#NeuralLabeling-A-versatile-toolset-for-labeling-vision-datasets-using-Neural-Radiance-Fields" class="headerlink" title="NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields"></a>NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11966">http://arxiv.org/abs/2309.11966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floris Erich, Naoya Chiba, Yusuke Yoshiyasu, Noriaki Ando, Ryo Hanai, Yukiyasu Domae</li>
<li>for: 本研究旨在提出一种基于Neural Radiance Fields（NeRF）的图像标注方法和工具集，用于生成分割图、可用性图、2D bounding box、3D bounding box、6DOF对象位置、深度图和物体mesh等。</li>
<li>methods: 本研究使用NeRF作为渲染器，通过使用多视点图像输入和3D空间工具进行标注，并利用图像内容和几何特征（如 occlusion）来提高标注精度。</li>
<li>results: 在对30000帧透明物体RGB和噪音深度图数据集进行训练后，使用 annotated depth maps 进行监督训练的深度神经网络实现了更高的重建性能，比之前使用弱监督法则更高。<details>
<summary>Abstract</summary>
We present NeuralLabeling, a labeling approach and toolset for annotating a scene using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach.
</details>
<details>
<summary>摘要</summary>
我们提出了NeuralLabeling，一种Scene Labeling的方法和工具集，可以使用矩形框或网格来标识场景，并生成分类图、可用性图、2D矩形框、3D矩形框、6DOF物体位置、深度图和物体网格。NeuralLabeling使用Neural Radiance Fields（NeRF）作为渲染器，允许使用3D空间工具进行标识，同时考虑到隐藏和 occlusion 的几何假设，仅基于多个视角的图像作为输入。为了评估NeuralLabeling在 robotics 中的实用性，我们将添加了透明物体RGB和杂音深度图档案，创建了Dishwasher30k数据集。我们显示了，对于训练一个简单的深度神经网络，使用这些标识的深度图作为超级训练可以获得更高的重建性能，比过去的弱种超级训练方法。
</details></li>
</ul>
<hr>
<h2 id="Ego3DPose-Capturing-3D-Cues-from-Binocular-Egocentric-Views"><a href="#Ego3DPose-Capturing-3D-Cues-from-Binocular-Egocentric-Views" class="headerlink" title="Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views"></a>Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11962">http://arxiv.org/abs/2309.11962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeho Kang, Kyungjin Lee, Jinrui Zhang, Youngki Lee</li>
<li>for:  Egocentric 3D pose reconstruction system</li>
<li>methods:  Two novel approaches: 1) two-path network architecture with independent pose estimation for each limb, and 2) perspective-aware representation using trigonometry to estimate 3D orientation of limbs.</li>
<li>results:  Outperforms state-of-the-art models by 23.1% in MPJPE reduction in the UnrealEgo dataset, with superior performance across a range of scenarios and challenges.<details>
<summary>Abstract</summary>
We present Ego3DPose, a highly accurate binocular egocentric 3D pose reconstruction system. The binocular egocentric setup offers practicality and usefulness in various applications, however, it remains largely under-explored. It has been suffering from low pose estimation accuracy due to viewing distortion, severe self-occlusion, and limited field-of-view of the joints in egocentric 2D images. Here, we notice that two important 3D cues, stereo correspondences, and perspective, contained in the egocentric binocular input are neglected. Current methods heavily rely on 2D image features, implicitly learning 3D information, which introduces biases towards commonly observed motions and leads to low overall accuracy. We observe that they not only fail in challenging occlusion cases but also in estimating visible joint positions. To address these challenges, we propose two novel approaches. First, we design a two-path network architecture with a path that estimates pose per limb independently with its binocular heatmaps. Without full-body information provided, it alleviates bias toward trained full-body distribution. Second, we leverage the egocentric view of body limbs, which exhibits strong perspective variance (e.g., a significantly large-size hand when it is close to the camera). We propose a new perspective-aware representation using trigonometry, enabling the network to estimate the 3D orientation of limbs. Finally, we develop an end-to-end pose reconstruction network that synergizes both techniques. Our comprehensive evaluations demonstrate that Ego3DPose outperforms state-of-the-art models by a pose estimation error (i.e., MPJPE) reduction of 23.1% in the UnrealEgo dataset. Our qualitative results highlight the superiority of our approach across a range of scenarios and challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍Ego3DPose，一种高度准确的双目 egocentric 3D姿态重建系统。双目 egocentric 设置提供了实用性和有用性，但它尚未得到充分探索。它因视图扭曲、严重的自遮掩和 Egocentric 2D 图像中关节的视场有限而受到低姿态估计精度的影响。我们注意到，在 egocentric 双目输入中含有两种重要的3D准确度信息：立体匹配和投影。现有方法强调2D图像特征，潜在地学习3D信息，导致对常见动作的偏好和全局精度低下。我们发现它们不仅在困难的遮掩情况下失败，而且在可见关节位置的估计也失败。为解决这些挑战，我们提出了两个新的方法。首先，我们设计了一种两路网络架构，其中一路用于独立地估计每个肢体的姿态，使用双目热图。无需全身信息提供，这种方法减少了对训练全身份布的偏好。其次，我们利用 egocentric 视角中的身体部分，其中具有强大的投影变化（例如，相对较大的手在相机较近时）。我们提出了一种新的投影意识表示，使得网络能够估计肢体的3D方向。最后，我们开发了一个综合的端到端姿态重建网络，将两种技术相结合。我们对 UnrealEgo 数据集进行了广泛的评估，并证明Ego3DPose 相比 estado-of-the-art 模型，MPJPE 估计误差降低23.1%。我们的质量结果表明我们的方法在各种情况和挑战中具有优势。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Forward-Forward-Algorithm-for-Self-Supervised-Learning"><a href="#A-Study-of-Forward-Forward-Algorithm-for-Self-Supervised-Learning" class="headerlink" title="A Study of Forward-Forward Algorithm for Self-Supervised Learning"></a>A Study of Forward-Forward Algorithm for Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11955">http://arxiv.org/abs/2309.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Brenig, Radu Timofte</li>
<li>for: 本研究是 investigate the performance of forward-forward algorithm for self-supervised representation learning, and provide insights into the learned representation spaces.</li>
<li>methods: 本研究使用了 four standard datasets (MNIST, F-MNIST, SVHN, CIFAR-10) and three commonly used self-supervised representation learning techniques (rotation, flip, jigsaw) to compare the performance of forward-forward algorithm and backpropagation.</li>
<li>results: 研究发现，forward-forward algorithm在(self-)supervised training中与backpropagation相当，但在所有 studied settings中的转移性能却明显落后。这可能由多种因素引起，包括每层有独立损失函数和forward-forward paradigm中的自tek抽象学习方法。相比backpropagation，forward-forward algorithm更注重边界和抛弃一些无用于做出决策的信息，这会妨碍自然语言处理的表征学习目标。<details>
<summary>Abstract</summary>
Self-supervised representation learning has seen remarkable progress in the last few years, with some of the recent methods being able to learn useful image representations without labels. These methods are trained using backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the forward-forward algorithm as an alternative training method. It utilizes two forward passes and a separate loss function for each layer to train the network without backpropagation.   In this study, for the first time, we study the performance of forward-forward vs. backpropagation for self-supervised representation learning and provide insights into the learned representation spaces. Our benchmark employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and three commonly used self-supervised representation learning techniques, namely rotation, flip and jigsaw.   Our main finding is that while the forward-forward algorithm performs comparably to backpropagation during (self-)supervised training, the transfer performance is significantly lagging behind in all the studied settings. This may be caused by a combination of factors, including having a loss function for each layer and the way the supervised training is realized in the forward-forward paradigm. In comparison to backpropagation, the forward-forward algorithm focuses more on the boundaries and drops part of the information unnecessary for making decisions which harms the representation learning goal. Further investigation and research are necessary to stabilize the forward-forward strategy for self-supervised learning, to work beyond the datasets and configurations demonstrated by Geoffrey Hinton.
</details>
<details>
<summary>摘要</summary>
自我监督学习在最近几年内取得了非常出色的进步，一些最新的方法可以在无标签情况下学习有用的图像表示。这些方法通过反向传播来进行训练，反向传播是现今标准的训练方法。在这一研究中，我们首次比较了前向前法和反向传播两种训练方法的性能，并对学习的表示空间提供了深入的探讨。我们的标准测试集包括MNIST、F-MNIST、SVHN和CIFAR-10等四个数据集，以及rotation、flip和jigsaw等三种常用的自我监督表示学习技术。我们的主要发现是，虽然前向前法和反向传播在自我监督训练中表现相似，但在所有研究情况下，转移性能明显落后。这可能是由多种因素共同影响的，包括每层有自己的损失函数以及在前向前法中实现自我监督训练的方式。相比反向传播，前向前法更注重边缘和抛弃一些无用于做出决定的信息，这对图像表示学习的目标产生了负面影响。进一步的研究和调查是必要的，以稳定前向前法在自我监督学习中的应用，并在不同的数据集和配置下进行更广泛的探索。
</details></li>
</ul>
<hr>
<h2 id="Fully-Transformer-Equipped-Architecture-for-End-to-End-Referring-Video-Object-Segmentation"><a href="#Fully-Transformer-Equipped-Architecture-for-End-to-End-Referring-Video-Object-Segmentation" class="headerlink" title="Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation"></a>Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11933">http://arxiv.org/abs/2309.11933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Li, Yu Zhang, Li Yuan, Xianghua Xu</li>
<li>for: 这篇论文是为了解决视频对话引用（RVOS）问题，即根据自然语言查询语义地 segment 视频中的对象。</li>
<li>methods: 该论文提出了一种基于 transformer 完全架构（FTEA）的解决方案，即将 RVOS 任务看作是一个 mask 序列学习问题，对所有视频中的对象进行候选对象的搜索和学习。</li>
<li>results: 实验结果表明，该方法在三个 benchmark 上表现出色，例如，在 A2D Sentences 和 J-HMDB Sentences 上达到了 45.1% 和 38.7% 的 mAP 分数，在 Ref-YouTube-VOS 上达到了 56.6% 的 $\mathcal{J&amp;F}$ 分数。特别是，与最佳候选方法相比，该方法在 P$@$0.5 上具有了 2.1% 和 3.2% 的提升，在 $\mathcal{J}$ 上具有了 2.9% 的提升。<details>
<summary>Abstract</summary>
Referring Video Object Segmentation (RVOS) requires segmenting the object in video referred by a natural language query. Existing methods mainly rely on sophisticated pipelines to tackle such cross-modal task, and do not explicitly model the object-level spatial context which plays an important role in locating the referred object. Therefore, we propose an end-to-end RVOS framework completely built upon transformers, termed \textit{Fully Transformer-Equipped Architecture} (FTEA), which treats the RVOS task as a mask sequence learning problem and regards all the objects in video as candidate objects. Given a video clip with a text query, the visual-textual features are yielded by encoder, while the corresponding pixel-level and word-level features are aligned in terms of semantic similarity. To capture the object-level spatial context, we have developed the Stacked Transformer, which individually characterizes the visual appearance of each candidate object, whose feature map is decoded to the binary mask sequence in order directly. Finally, the model finds the best matching between mask sequence and text query. In addition, to diversify the generated masks for candidate objects, we impose a diversity loss on the model for capturing more accurate mask of the referred object. Empirical studies have shown the superiority of the proposed method on three benchmarks, e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782 videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% in terms of $\mathcal{J\&F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects). Particularly, compared to the best candidate method, it has a gain of 2.1% and 3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gain of 2.9% in terms of $\mathcal{J}$ on the latter one.
</details>
<details>
<summary>摘要</summary>
参考视频对象 segmentation（RVOS）需要将视频中的对象与自然语言查询相关联。现有方法主要依靠复杂的管道来解决这种跨模态任务，并未直接模型对象水平的空间上下文，这上下文在定位引用对象中扮演重要角色。因此，我们提出了一个 completel y built upon transformers 的框架，称为 Fully Transformer-Equipped Architecture（FTEA），它将 RVOS 任务视为面征序列学习问题，并将所有视频中的对象视为候选对象。给定一个视频剪辑和自然语言查询，视觉语言特征是通过Encoder生成的，而对应的像素级和单词级特征则是在semantic similarity的基础上对准。为了捕捉对象水平的空间上下文，我们开发了堆叠transformer，它可以个别地描述每个候选对象的视觉特征，并将其特征图直接解码到二进制mask sequence中。最后，模型会找到与文本查询最佳匹配的mask sequence。此外，为了捕捉更加准确的mask，我们对模型进行多样性损失，以便在候选对象中捕捉更多的详细信息。实验表明，我们的方法在三个标准准的benchmark上表现出色，例如，FETA在A2D Sentences（3782个视频）和J-HMDB Sentences（928个视频）上的mAP分别达到45.1%和38.7%，在Ref-YouTube-VOS（3975个视频和7451个对象）上的$\mathcal{J\&F}$分别达到56.6%。特别是，相比最佳候选方法，FETA在前两个benchmark上的P$@$0.5分别提高了2.1%和3.2%，而在Ref-YouTube-VOS上的$\mathcal{J}$分别提高了2.9%。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Learning-Pace-Synchronization-for-Open-World-Semi-Supervised-Learning"><a href="#Bridging-the-Gap-Learning-Pace-Synchronization-for-Open-World-Semi-Supervised-Learning" class="headerlink" title="Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning"></a>Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11930">http://arxiv.org/abs/2309.11930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Ye, Kai Gan, Tong Wei, Min-Ling Zhang</li>
<li>for: 这个研究目的是解决开放世界半监督学习中的新类别发现问题，即使用不监督的数据来探索新的类别，同时维持已知类别的性能。</li>
<li>methods: 我们提出了两个方法来解决这个问题：1）适应的margin损失基于估计的类别分布，以将学习速度均衡化，2）pseudo-label对称分组，将可能属于同一类别的样本集中，以增强新类别发现。</li>
<li>results: 我们的广泛评估表明，现有模型仍然对新类别学习产生问题，而我们的方法则能够平衡已知类别和新类别，在ImageNet dataset上达到了3%的平均精度提升，相比先前的州ike-of-the-art。此外，我们发现了精益调整自我监督预训练模型可以很大程度上提高性能。<details>
<summary>Abstract</summary>
In open-world semi-supervised learning, a machine learning model is tasked with uncovering novel categories from unlabeled data while maintaining performance on seen categories from labeled data. The central challenge is the substantial learning gap between seen and novel categories, as the model learns the former faster due to accurate supervisory information. To address this, we introduce 1) an adaptive margin loss based on estimated class distribution, which encourages a large negative margin for samples in seen classes, to synchronize learning paces, and 2) pseudo-label contrastive clustering, which pulls together samples which are likely from the same class in the output space, to enhance novel class discovery. Our extensive evaluations on multiple datasets demonstrate that existing models still hinder novel class learning, whereas our approach strikingly balances both seen and novel classes, achieving a remarkable 3% average accuracy increase on the ImageNet dataset compared to the prior state-of-the-art. Additionally, we find that fine-tuning the self-supervised pre-trained backbone significantly boosts performance over the default in prior literature. After our paper is accepted, we will release the code.
</details>
<details>
<summary>摘要</summary>
在开放世界半监督学习中，一个机器学习模型被要求探索未经标注的数据中的新分类，同时保持已经标注的分类的性能。中心挑战是seen和novel分类之间的学习差距，因为模型在高精度的指导信息下快速学习seen分类。为此，我们提出了以下两点方法：1. 适应margin损失基于估计类分布，该损失函数鼓励在seen分类中的样本具有大负margin，以同步学习速度。2.  Pseudo-label对比分 clustering，该方法在输出空间中吸引同类样本相互吸引，以促进novel分类的发现。我们对多个数据集进行了广泛的评估，发现现有模型仍然受到novel分类学习的限制，而我们的方法能够很好地均衡seen和novel分类，在ImageNet数据集上实现了3%的平均准确率提升 compared to Prior State-of-the-art。此外，我们发现在先前的文献中 defaults 的自然语言预训练模型进行了显著提升性能的观察。在我们的论文被接受后，我们将释放代码。
</details></li>
</ul>
<hr>
<h2 id="Video-Scene-Location-Recognition-with-Neural-Networks"><a href="#Video-Scene-Location-Recognition-with-Neural-Networks" class="headerlink" title="Video Scene Location Recognition with Neural Networks"></a>Video Scene Location Recognition with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11928">http://arxiv.org/abs/2309.11928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Korel, Petr Pulc, Jiří Tumpach, Martin Holeňa</li>
<li>for: 这篇论文探讨了从视频序列中提取场景的可能性，使用人工神经网络。</li>
<li>methods: 该方法选择每个场景中的一些帧，通过预训练的单个图像预处理卷积网络进行转换，然后使用后续层的神经网络来确定场景位置。</li>
<li>results: 研究人员对从《大带费》电视剧获取的数据集进行测试和比较，发现只有某些方法适合当务。<details>
<summary>Abstract</summary>
This paper provides an insight into the possibility of scene recognition from a video sequence with a small set of repeated shooting locations (such as in television series) using artificial neural networks. The basic idea of the presented approach is to select a set of frames from each scene, transform them by a pre-trained singleimage pre-processing convolutional network, and classify the scene location with subsequent layers of the neural network. The considered networks have been tested and compared on a dataset obtained from The Big Bang Theory television series. We have investigated different neural network layers to combine individual frames, particularly AveragePooling, MaxPooling, Product, Flatten, LSTM, and Bidirectional LSTM layers. We have observed that only some of the approaches are suitable for the task at hand.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文探讨了通过视频序列中重复拍摄的小集合来实现场景认识的可能性，使用人工神经网络。提出的方法选择每个场景中的一些帧，通过预训练的单张图像预处理卷积神经网络进行转换，然后使用神经网络的后续层来确定场景位置。我们在《大咖大爆》电视剧集中获取的数据集上测试和比较了不同的神经网络层，包括AveragePooling、MaxPooling、Product、Flatten、LSTM和Bidirectional LSTM层。我们发现只有某些方法适合这种任务。
</details></li>
</ul>
<hr>
<h2 id="TextCLIP-Text-Guided-Face-Image-Generation-And-Manipulation-Without-Adversarial-Training"><a href="#TextCLIP-Text-Guided-Face-Image-Generation-And-Manipulation-Without-Adversarial-Training" class="headerlink" title="TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training"></a>TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11923">http://arxiv.org/abs/2309.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhou You, Jian Zhang</li>
<li>for: 文章目标是提出一种基于文本指导的图像生成和修改方法，而不需要对敌对模型进行训练。</li>
<li>methods: 方法基于StyleGAN和CLIP的协同适应力，通过特制的映射网络将文本转化为图像。</li>
<li>results: 对多Modal CelebA-HQ数据集进行了广泛的实验，并证明了该方法在文本指导生成和修改任务上具有优于现有方法的性能。<details>
<summary>Abstract</summary>
Text-guided image generation aimed to generate desired images conditioned on given texts, while text-guided image manipulation refers to semantically edit parts of a given image based on specified texts. For these two similar tasks, the key point is to ensure image fidelity as well as semantic consistency. Many previous approaches require complex multi-stage generation and adversarial training, while struggling to provide a unified framework for both tasks. In this work, we propose TextCLIP, a unified framework for text-guided image generation and manipulation without adversarial training. The proposed method accepts input from images or random noise corresponding to these two different tasks, and under the condition of the specific texts, a carefully designed mapping network that exploits the powerful generative capabilities of StyleGAN and the text image representation capabilities of Contrastive Language-Image Pre-training (CLIP) generates images of up to $1024\times1024$ resolution that can currently be generated. Extensive experiments on the Multi-modal CelebA-HQ dataset have demonstrated that our proposed method outperforms existing state-of-the-art methods, both on text-guided generation tasks and manipulation tasks.
</details>
<details>
<summary>摘要</summary>
文本干预图像生成和文本干预图像修改都是类似的任务，关键点是保持图像准确性和 semantics 一致性。许多先前的方法需要复杂的多stage生成和对抗训练，而且很难提供一个简单的框架 для这两个任务。在这项工作中，我们提出了 TextCLIP，一个简单的框架 для文本干预图像生成和修改，不需要对抗训练。该方法接受图像或随机噪声作为输入，根据特定的文本来生成图像，可以生成高分辨率图像（最大 $1024\times1024$）。经验表明，我们提出的方法在多模态 CelebA-HQ 数据集上表现出了比例性，在文本干预图像生成和修改任务中都超过了现有的状态泰技术。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Temporal-Transformer-based-Video-Compression-Framework"><a href="#Spatial-Temporal-Transformer-based-Video-Compression-Framework" class="headerlink" title="Spatial-Temporal Transformer based Video Compression Framework"></a>Spatial-Temporal Transformer based Video Compression Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11913">http://arxiv.org/abs/2309.11913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbo Gao, Wenjia Huang, Shuai Li, Hui Yuan, Mao Ye, Siwei Ma</li>
<li>For: 这个研究旨在提出一种基于 transformer 框架的learned video compression（LVC）方法，以提高视频编码的效率和质量。* Methods: 该方法使用了一种叫做 Relaxed Deformable Transformer（RDT）的新型自适应变换器，以稳定地估算视频帧之间的运动信息。同时，该方法还使用了一种多级划分预测（MGP）模块，以更好地利用多个参照帧的信息，以及一种空间特征分布预测（SFD-T）模块，以减少视频的空间-时间重复性。* Results: 实验结果表明，该方法可以与 VTM 比较，并且 achieved the best result with 13.5% BD-Rate saving。<details>
<summary>Abstract</summary>
Learned video compression (LVC) has witnessed remarkable advancements in recent years. Similar as the traditional video coding, LVC inherits motion estimation/compensation, residual coding and other modules, all of which are implemented with neural networks (NNs). However, within the framework of NNs and its training mechanism using gradient backpropagation, most existing works often struggle to consistently generate stable motion information, which is in the form of geometric features, from the input color features. Moreover, the modules such as the inter-prediction and residual coding are independent from each other, making it inefficient to fully reduce the spatial-temporal redundancy. To address the above problems, in this paper, we propose a novel Spatial-Temporal Transformer based Video Compression (STT-VC) framework. It contains a Relaxed Deformable Transformer (RDT) with Uformer based offsets estimation for motion estimation and compensation, a Multi-Granularity Prediction (MGP) module based on multi-reference frames for prediction refinement, and a Spatial Feature Distribution prior based Transformer (SFD-T) for efficient temporal-spatial joint residual compression. Specifically, RDT is developed to stably estimate the motion information between frames by thoroughly investigating the relationship between the similarity based geometric motion feature extraction and self-attention. MGP is designed to fuse the multi-reference frame information by effectively exploring the coarse-grained prediction feature generated with the coded motion information. SFD-T is to compress the residual information by jointly exploring the spatial feature distributions in both residual and temporal prediction to further reduce the spatial-temporal redundancy. Experimental results demonstrate that our method achieves the best result with 13.5% BD-Rate saving over VTM.
</details>
<details>
<summary>摘要</summary>
历年来，学习视频压缩（LVC）技术已经经历了很大的发展。LVC技术继承了传统视频编码中的运动估计/补做、剩余编码等模块，并且通过神经网络（NN）的实现。然而，大多数现有的工作在NN和其训练机制中使用梯度倒逆时，很难一致地生成稳定的运动信息，这种运动信息通常是输入颜色特征的几何特征。此外，模块如 междуPrediction和剩余编码是独立的，这使得它们之间的重叠不充分。为解决这些问题，在这篇论文中，我们提出了一种新的空间-时间变换基本的视频压缩（STT-VC）框架。它包括一个宽度缓和变换（RDT）、基于Uformer的偏移估计，以及一个多级别预测（MGP）模块和一个空间特征分布先验基于变换（SFD-T）。具体来说，RDT是通过彻底调查相似性基于几何运动特征提取和自注意力来稳定地估计运动信息 между帧。MGP是通过有效地探索压缩动作信息中的粗糙预测特征来融合多个参照帧信息。SFD-T是通过同时探索剩余信息中的空间特征分布来进一步减少空间-时间重复。实验结果表明，我们的方法可以在VTM比较下实现13.5%的BD-Rate节省。
</details></li>
</ul>
<hr>
<h2 id="Heart-Rate-Detection-Using-an-Event-Camera"><a href="#Heart-Rate-Detection-Using-an-Event-Camera" class="headerlink" title="Heart Rate Detection Using an Event Camera"></a>Heart Rate Detection Using an Event Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11891">http://arxiv.org/abs/2309.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Jagtap, RamaKrishna Venkatesh Saripalli, Joe Lemley, Waseem Shariff, Alan F. Smeaton</li>
<li>for: 这个研究旨在使用事件摄像机来不侵入式地监测心率（HR）。</li>
<li>methods: 研究使用事件摄像机捕捉手臂部分的皮肤微妙变化，并通过自动检测事件来测量心率。</li>
<li>results: 实验结果显示，使用事件摄像机可以精确地检测心率，并且比其他非接触式心率测量方法更高精度。 However, the method is limited by light-induced flickering and subconscious tremors of the individual during data capture.<details>
<summary>Abstract</summary>
Event cameras, also known as neuromorphic cameras, are an emerging technology that offer advantages over traditional shutter and frame-based cameras, including high temporal resolution, low power consumption, and selective data acquisition. In this study, we propose to harnesses the capabilities of event-based cameras to capture subtle changes in the surface of the skin caused by the pulsatile flow of blood in the wrist region. We investigate whether an event camera could be used for continuous noninvasive monitoring of heart rate (HR). Event camera video data from 25 participants, comprising varying age groups and skin colours, was collected and analysed. Ground-truth HR measurements obtained using conventional methods were used to evaluate of the accuracy of automatic detection of HR from event camera data. Our experimental results and comparison to the performance of other non-contact HR measurement methods demonstrate the feasibility of using event cameras for pulse detection. We also acknowledge the challenges and limitations of our method, such as light-induced flickering and the sub-conscious but naturally-occurring tremors of an individual during data capture.
</details>
<details>
<summary>摘要</summary>
事件摄像机也称为神经模型摄像机，是一种emerging技术，它们在传统的闭合式摄像机和帧摄像机方面具有优势，包括高时间分辨率、低功耗和选择性数据收集。在这项研究中，我们利用事件摄像机来捕捉血液径向流动在臂部区域 superficies 上的微小变化。我们调查了事件摄像机是否可以用于无侵入式、连续监测心率（HR）。我们收集了25名参与者的事件摄像机视频数据，其中年龄层width 和肤色各不相同。我们使用传统方法获取的真实心率值来评估自动从事件摄像机数据中检测HR的准确性。我们的实验结果和与其他非接触式心率测量方法的比较表明了使用事件摄像机进行脉吸检测的可行性。然而，我们也承认使用这种方法时存在挑战和限制，如光学辐射引起的闪烁和个体在数据采集过程中自然发生的微小颤动。
</details></li>
</ul>
<hr>
<h2 id="On-the-Fly-SfM-What-you-capture-is-What-you-get"><a href="#On-the-Fly-SfM-What-you-capture-is-What-you-get" class="headerlink" title="On-the-Fly SfM: What you capture is What you get"></a>On-the-Fly SfM: What you capture is What you get</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11883">http://arxiv.org/abs/2309.11883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongqian Zhan, Rui Xia, Yifei Yu, Yibo Xu, Xin Wang</li>
<li>for: 这 paper 的目的是提出一种在线 Structure from Motion（SfM）方法，可以在图像捕捉过程中实时进行pose和稀疏点云的计算。</li>
<li>methods: 该方法使用了一个学习基于全局特征的词汇树来快速查找新抵达的图像，然后使用了一种robust的特征匹配机制（LSM）来提高图像Alignment性能。最后，通过研究新抵达图像的相邻图像的影响，提出了一种高效的层次权重本地加重（BA）优化方法。</li>
<li>results: 实验结果表明，在线 SfM 可以坚定地将图像注册，而不需要先将图像 fed into SfM 管道。<details>
<summary>Abstract</summary>
Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Saliency-and-Cropping-to-Improve-Video-Memorability"><a href="#Using-Saliency-and-Cropping-to-Improve-Video-Memorability" class="headerlink" title="Using Saliency and Cropping to Improve Video Memorability"></a>Using Saliency and Cropping to Improve Video Memorability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11881">http://arxiv.org/abs/2309.11881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Vaibhav Mudgal, Qingyang Wang, Lorin Sweeney, Alan F. Smeaton</li>
<li>for: 这项研究旨在提高视频的记忆性，以便提高视频的分享、播放和讨论。</li>
<li>methods: 研究人员通过选择ively cropping帧图像来提高视频的记忆性。他们使用了基本的固定剪辑以及动态剪辑，其中剪辑的大小和位置在视频播放时随着图像精力的变化。</li>
<li>results: 研究人员发现，特别是对低初始记忆性的视频，通过这些方法可以提高视频的记忆性。<details>
<summary>Abstract</summary>
Video memorability is a measure of how likely a particular video is to be remembered by a viewer when that viewer has no emotional connection with the video content. It is an important characteristic as videos that are more memorable are more likely to be shared, viewed, and discussed. This paper presents results of a series of experiments where we improved the memorability of a video by selectively cropping frames based on image saliency. We present results of a basic fixed cropping as well as the results from dynamic cropping where both the size of the crop and the position of the crop within the frame, move as the video is played and saliency is tracked. Our results indicate that especially for videos of low initial memorability, the memorability score can be improved.
</details>
<details>
<summary>摘要</summary>
视频记忆性是观看者没有情感连接的视频内容记忆的度量。它是一个重要的特性，因为更有记忆性的视频更有可能被分享、播放和讨论。本文报告了一系列实验，我们通过选择性剪辑帧来提高视频的记忆性。我们发现，特别是初始记忆性较低的视频，记忆性分数可以得到提高。
</details></li>
</ul>
<hr>
<h2 id="TCOVIS-Temporally-Consistent-Online-Video-Instance-Segmentation"><a href="#TCOVIS-Temporally-Consistent-Online-Video-Instance-Segmentation" class="headerlink" title="TCOVIS: Temporally Consistent Online Video Instance Segmentation"></a>TCOVIS: Temporally Consistent Online Video Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11857">http://arxiv.org/abs/2309.11857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jun-long-li/tcovis">https://github.com/jun-long-li/tcovis</a></li>
<li>paper_authors: Junlong Li, Bingyao Yu, Yongming Rao, Jie Zhou, Jiwen Lu</li>
<li>for: 这 paper 是为了提出一种新的在线视频实例分割方法，以实现高效精度的视频实例分割。</li>
<li>methods: 这 paper 使用了一种全新的 global instance assignment strategy 和 spatio-temporal enhancement module，以提高模型的时间一致性。</li>
<li>results: 这 paper 在四个广泛采用的视频实例分割benchmark上 achieved state-of-the-art performance，包括 YouTube-VIS 2019&#x2F;2021&#x2F;2022 和 OVIS。例如，在 YouTube-VIS 2021 上，TCOVIS 实现了 49.5 AP 和 61.3 AP 的最佳性能，使用 ResNet-50 和 Swin-L 的背景。<details>
<summary>Abstract</summary>
In recent years, significant progress has been made in video instance segmentation (VIS), with many offline and online methods achieving state-of-the-art performance. While offline methods have the advantage of producing temporally consistent predictions, they are not suitable for real-time scenarios. Conversely, online methods are more practical, but maintaining temporal consistency remains a challenging task. In this paper, we propose a novel online method for video instance segmentation, called TCOVIS, which fully exploits the temporal information in a video clip. The core of our method consists of a global instance assignment strategy and a spatio-temporal enhancement module, which improve the temporal consistency of the features from two aspects. Specifically, we perform global optimal matching between the predictions and ground truth across the whole video clip, and supervise the model with the global optimal objective. We also capture the spatial feature and aggregate it with the semantic feature between frames, thus realizing the spatio-temporal enhancement. We evaluate our method on four widely adopted VIS benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve state-of-the-art performance on all benchmarks without bells-and-whistles. For instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with ResNet-50 and Swin-L backbones, respectively. Code is available at https://github.com/jun-long-li/TCOVIS.
</details>
<details>
<summary>摘要</summary>
近年来，视频实例分割（VIS）领域内有很大的进步，许多离线和在线方法已经达到了状态艺术水平。然而，离线方法在实时场景下不够实用，而在线方法尽管更加实用，但维护时间一致性仍然是一个挑战。在这篇论文中，我们提出了一种新的在线视频实例分割方法，称为TCOVIS，它可以充分利用视频帧序中的时间信息。TCOVIS的核心包括全局实例分配策略和空间时间增强模块，这两个部分都有助于提高视频帧序中特征的时间一致性。具体来说，我们在整个视频帧序中进行全局最佳匹配，并通过全局最佳目标进行监督。同时，我们还捕捉了空间特征，将其与semantic特征相加，实现了空间时间增强。我们在四个广泛采用的 VIS 评测benchmark上进行评测，分别是 YouTube-VIS 2019/2021/2022 和 OVIS，并在所有benchmark上取得了状态艺术性的表现。例如，在 YouTube-VIS 2021 上，TCOVIS 取得了 49.5 AP 和 61.3 AP，使用 ResNet-50 和 Swin-L 的背景中。代码可以在 GitHub 上找到：https://github.com/jun-long-li/TCOVIS。
</details></li>
</ul>
<hr>
<h2 id="DEYOv3-DETR-with-YOLO-for-Real-time-Object-Detection"><a href="#DEYOv3-DETR-with-YOLO-for-Real-time-Object-Detection" class="headerlink" title="DEYOv3: DETR with YOLO for Real-time Object Detection"></a>DEYOv3: DETR with YOLO for Real-time Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11851">http://arxiv.org/abs/2309.11851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodong Ouyang</li>
<li>For: This paper proposes a new training method called step-by-step training to improve the practical application and design flexibility of end-to-end object detectors, particularly for DETR-like models.* Methods: The proposed method first initializes the end-to-end detector with a pre-trained YOLO detector and then trains the backbone and encoder of the DETR-like model from scratch. The step-by-step training method eliminates the need for additional training data and reduces the training cost of the detector.* Results: The proposed DEYOv3 model achieves higher accuracy than traditional DETR-like models while maintaining real-time speed (270 FPS on T4 GPU). DEYOv3-N reaches 41.1% AP on COCO val2017 and 270 FPS, while DEYOv3-L achieves 51.3% AP and 102 FPS. All of these results are achieved without using additional training data, making DEYOv3 the best real-time object detection model in terms of both speed and accuracy.<details>
<summary>Abstract</summary>
Recently, end-to-end object detectors have gained significant attention from the research community due to their outstanding performance. However, DETR typically relies on supervised pretraining of the backbone on ImageNet, which limits the practical application of DETR and the design of the backbone, affecting the model's potential generalization ability. In this paper, we propose a new training method called step-by-step training. Specifically, in the first stage, the one-to-many pre-trained YOLO detector is used to initialize the end-to-end detector. In the second stage, the backbone and encoder are consistent with the DETR-like model, but only the detector needs to be trained from scratch. Due to this training method, the object detector does not need the additional dataset (ImageNet) to train the backbone, which makes the design of the backbone more flexible and dramatically reduces the training cost of the detector, which is helpful for the practical application of the object detector. At the same time, compared with the DETR-like model, the step-by-step training method can achieve higher accuracy than the traditional training method of the DETR-like model. With the aid of this novel training method, we propose a brand-new end-to-end real-time object detection model called DEYOv3. DEYOv3-N achieves 41.1% on COCO val2017 and 270 FPS on T4 GPU, while DEYOv3-L achieves 51.3% AP and 102 FPS. Without the use of additional training data, DEYOv3 surpasses all existing real-time object detectors in terms of both speed and accuracy. It is worth noting that for models of N, S, and M scales, the training on the COCO dataset can be completed using a single 24GB RTX3090 GPU.
</details>
<details>
<summary>摘要</summary>
近期，端到端对象检测器在研究社区中受到了广泛关注，因为它们的表现非常出色。然而，DETR通常需要在ImageNet上进行supervised预训练，这限制了DETR的实际应用和设计，影响了模型的总体适应能力。在这篇论文中，我们提出了一种新的训练方法called step-by-step training。具体来说，在第一个阶段，使用pre-trained YOLO检测器来初始化端到端检测器。在第二个阶段，背部和编码器与DETR-like模型相同，但是只需要从scratch训练检测器。由于这种训练方法，对象检测器不需要额外的数据集（ImageNet）来训练背部，这使得背部的设计变得更加灵活，对检测器的训练成本减少了极大，这有助于实际应用。同时，相比DETR-like模型，step-by-step training方法可以在同样的精度下达到更高的精度。通过这种新的训练方法，我们提出了一个全新的端到端实时对象检测模型called DEYOv3。DEYOv3-N在COCO val2017上达到了41.1%的分数，而DEYOv3-L在T4 GPU上达到了51.3%的AP和270 FPS。没有使用额外的训练数据，DEYOv3超过了所有现有的实时对象检测器，包括速度和精度两个方面。值得注意的是，对N、S、M缩放的模型，在COCO数据集上进行训练可以使用单个24GB RTX3090 GPU。
</details></li>
</ul>
<hr>
<h2 id="MEFLUT-Unsupervised-1D-Lookup-Tables-for-Multi-exposure-Image-Fusion"><a href="#MEFLUT-Unsupervised-1D-Lookup-Tables-for-Multi-exposure-Image-Fusion" class="headerlink" title="MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion"></a>MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11847">http://arxiv.org/abs/2309.11847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hedlen/meflut">https://github.com/hedlen/meflut</a></li>
<li>paper_authors: Ting Jiang, Chuan Wang, Xinpeng Li, Ru Li, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 高品质多曝光图像融合 (高品质MEF)</li>
<li>methods: 利用一维Lookup表 (1D LUT) 将曝光权重编码为 pixel 强度值输入，并通过注意力机制在不同维度（帧、通道和空间）进行学习，以提高高品质和效率的融合。</li>
<li>results: 与现有最佳方法比较，新方法实现高品质和效率的融合，并且实现了4K影像在PC GPU上的几乎即时运行（低于4ms）。code available at：<a target="_blank" rel="noopener" href="https://github.com/Hedlen/MEFLUT%E3%80%82">https://github.com/Hedlen/MEFLUT。</a><details>
<summary>Abstract</summary>
In this paper, we introduce a new approach for high-quality multi-exposure image fusion (MEF). We show that the fusion weights of an exposure can be encoded into a 1D lookup table (LUT), which takes pixel intensity value as input and produces fusion weight as output. We learn one 1D LUT for each exposure, then all the pixels from different exposures can query 1D LUT of that exposure independently for high-quality and efficient fusion. Specifically, to learn these 1D LUTs, we involve attention mechanism in various dimensions including frame, channel and spatial ones into the MEF task so as to bring us significant quality improvement over the state-of-the-art (SOTA). In addition, we collect a new MEF dataset consisting of 960 samples, 155 of which are manually tuned by professionals as ground-truth for evaluation. Our network is trained by this dataset in an unsupervised manner. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the SOTA in our and another representative dataset SICE, both qualitatively and quantitatively. Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC GPU. Given its high quality, efficiency and robustness, our method has been shipped into millions of Android mobiles across multiple brands world-wide. Code is available at: https://github.com/Hedlen/MEFLUT.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的高质量多曝光图像融合（MEF）方法。我们表明，曝光权重可以被编码成1D Lookup Table（LUT），该LUT接受像素强度值作输入，并生成权重作出输出。我们在每个曝光中学习了1D LUT，然后所有的像素从不同的曝光状态可以独立地查询该曝光的1D LUT，以实现高质量和高效的融合。特别是，为了学习这些1D LUT，我们在MEF任务中包含了注意力机制在帧、通道和空间方向中，从而使我们在SOTA中得到了显著的质量改善。此外，我们收集了一个新的MEF数据集，包含960个样本，其中155个是由专业人员 manually tuned 的参考标准 для评估。我们的网络是通过这个数据集在无监督的情况下进行训练。我们进行了广泛的实验，以证明所有新提出的组件的效iveness，结果表明，我们的方法在我们的数据集和另一个代表性数据集SICE中， Both qualitatively and quantitativelysuperior to the SOTA。此外，我们的1D LUT方法在4K图像上只需0.4毫秒，在PC GPU上运行。由于其高质量、高效和稳定性，我们的方法已经被运送到了世界各地的几百万Android手机中。代码可以在https://github.com/Hedlen/MEFLUT中找到。
</details></li>
</ul>
<hr>
<h2 id="MoPA-Multi-Modal-Prior-Aided-Domain-Adaptation-for-3D-Semantic-Segmentation"><a href="#MoPA-Multi-Modal-Prior-Aided-Domain-Adaptation-for-3D-Semantic-Segmentation" class="headerlink" title="MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation"></a>MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11839">http://arxiv.org/abs/2309.11839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haozhi Cao, Yuecong Xu, Jianfei Yang, Pengyu Yin, Shenghai Yuan, Lihua Xie</li>
<li>for: 提高3D semantic segmentation中罕见对象的性能，使用多Modal无监督领域适应（MM-UDA）方法。</li>
<li>methods: 提出了多Modal Prior Aided（MoPA）领域适应方法，包括 Valid Ground-based Insertion（VGI）和 SAM consistency loss。</li>
<li>results: 实验显示，我们的方法在多Modal无监督领域适应 benchmark 上达到了状态机器人表现。代码将在 GitHub 上公开。<details>
<summary>Abstract</summary>
Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.
</details>
<details>
<summary>摘要</summary>
多Modal无监督领域适应（MM-UDA）用于3Dsemantic segmentation是一种实用的解决方案，以实现无需昂贵点级标注的semantic理解。previous MM-UDA方法可以获得总体改进，但它们受到分类偏好的问题，这限制了它们在实际应用中的采用。这种偏好性问题主要来自于：1）自我帮助学习偏好数据，2）缺乏像素级2D监视信号。在这项工作中，我们提出了多Modal Prior帮助（MoPA）领域适应，以提高罕见对象的性能。具体来说，我们开发了有效的基准图Insertion（VGI）技术，以修正不均匀的监视信号，而不是引入人工artefacts，以避免导致轻微解决方案。此外，我们的SAM一致损失函数利用了2D Prior semantic masks从SAM中的像素级监视信号，以强制每个对象在semantic mask中具有一致的预测。知识从modalSpecific Prior中学习的知识然后被共享到不同模式之间，以达到更好的罕见对象分割。广泛的实验表明，我们的方法在MM-UDA benchmark上达到了最佳性能。代码将在https://github.com/AronCao49/MoPA中提供。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Endoscopic-Ultrasound-Station-Recognition-with-Limited-Data"><a href="#Automatic-Endoscopic-Ultrasound-Station-Recognition-with-Limited-Data" class="headerlink" title="Automatic Endoscopic Ultrasound Station Recognition with Limited Data"></a>Automatic Endoscopic Ultrasound Station Recognition with Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11820">http://arxiv.org/abs/2309.11820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amrita-medical-ai/eusml-labeller">https://github.com/amrita-medical-ai/eusml-labeller</a></li>
<li>paper_authors: Abhijit Ramesh, Anantha Nandanan, Anantha Nandanan, Priya Nair MD, Gilad Gressel</li>
<li>for: 这个论文的目的是提高检测胆囊癌的精度，以提高患者的诊断和存活率。</li>
<li>methods: 这个论文使用了人工智能技术，以帮助医生更准确地检测胆囊癌。特别是，它使用了深度学习算法来识别胆囊癌的不同区域，以提高检测的精度。</li>
<li>results: 研究表明，使用这个人工智能检测工具可以提高检测胆囊癌的精度，并且只需要43个检测过程，无需进行价值革新的训练。此外，这个工具还可以提供可读性和可解释的视觉化技术，帮助医生更好地理解检测结果。<details>
<summary>Abstract</summary>
Pancreatic cancer is a lethal form of cancer that significantly contributes to cancer-related deaths worldwide. Early detection is essential to improve patient prognosis and survival rates. Despite advances in medical imaging techniques, pancreatic cancer remains a challenging disease to detect. Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting pancreatic cancer. However, it requires expert interpretation of complex ultrasound images to complete a reliable patient scan. To obtain complete imaging of the pancreas, practitioners must learn to guide the endoscope into multiple "EUS stations" (anatomical locations), which provide different views of the pancreas. This is a difficult skill to learn, involving over 225 proctored procedures with the support of an experienced doctor. We build an AI-assisted tool that utilizes deep learning techniques to identify these stations of the stomach in real time during EUS procedures. This computer-assisted diagnostic (CAD) will help train doctors more efficiently. Historically, the challenge faced in developing such a tool has been the amount of retrospective labeling required by trained clinicians. To solve this, we developed an open-source user-friendly labeling web app that streamlines the process of annotating stations during the EUS procedure with minimal effort from the clinicians. Our research shows that employing only 43 procedures with no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable to the current state of the art. In addition, we employ Grad-CAM, a visualization technology that provides clinicians with interpretable and explainable visualizations.
</details>
<details>
<summary>摘要</summary>
胰腺癌是一种致命的癌症，对全球癌症相关死亡率具有重要贡献。早期发现是提高病人 прогноosis 和存生率的关键。 despite advances in medical imaging techniques, pancreatic cancer remains a challenging disease to detect. Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting pancreatic cancer, but it requires expert interpretation of complex ultrasound images to complete a reliable patient scan. To obtain complete imaging of the pancreas, practitioners must learn to guide the endoscope into multiple "EUS stations" (anatomical locations), which provide different views of the pancreas. This is a difficult skill to learn, involving over 225 proctored procedures with the support of an experienced doctor. We have developed an AI-assisted tool that utilizes deep learning techniques to identify these stations of the stomach in real time during EUS procedures. This computer-assisted diagnostic (CAD) will help train doctors more efficiently. Historically, the challenge faced in developing such a tool has been the amount of retrospective labeling required by trained clinicians. To solve this, we have developed an open-source, user-friendly labeling web app that streamlines the process of annotating stations during the EUS procedure with minimal effort from the clinicians. Our research shows that employing only 43 procedures with no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable to the current state of the art. In addition, we employ Grad-CAM, a visualization technology that provides clinicians with interpretable and explainable visualizations.
</details></li>
</ul>
<hr>
<h2 id="FGFusion-Fine-Grained-Lidar-Camera-Fusion-for-3D-Object-Detection"><a href="#FGFusion-Fine-Grained-Lidar-Camera-Fusion-for-3D-Object-Detection" class="headerlink" title="FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection"></a>FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11804">http://arxiv.org/abs/2309.11804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaviergrool/fgfusion">https://github.com/xaviergrool/fgfusion</a></li>
<li>paper_authors: Zixuan Yin, Han Sun, Ningzhong Liu, Huiyu Zhou, Jiaquan Shen</li>
<li>for: 本文旨在提出一种细化的 lidar-camera 结合方法，以便在自动驾驶中提高 3D 检测的精度。</li>
<li>methods: 本方法使用了 dual pathway hierarchy 结构，以提取图像中的高级 semantics 和低级细节信息，并在 auxiliary network 的帮助下，点云特征更好地学习细致的空间信息。 最后，我们提出了多级 fusión（MSF）方法，将图像和点云的最后 N 个特征图 fusion。</li>
<li>results: 我们在 KITTI 和 Waymo 两个流行的自动驾驶 benchmark 上进行了广泛的实验，并证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Lidars and cameras are critical sensors that provide complementary information for 3D detection in autonomous driving. While most prevalent methods progressively downscale the 3D point clouds and camera images and then fuse the high-level features, the downscaled features inevitably lose low-level detailed information. In this paper, we propose Fine-Grained Lidar-Camera Fusion (FGFusion) that make full use of multi-scale features of image and point cloud and fuse them in a fine-grained way. First, we design a dual pathway hierarchy structure to extract both high-level semantic and low-level detailed features of the image. Second, an auxiliary network is introduced to guide point cloud features to better learn the fine-grained spatial information. Finally, we propose multi-scale fusion (MSF) to fuse the last N feature maps of image and point cloud. Extensive experiments on two popular autonomous driving benchmarks, i.e. KITTI and Waymo, demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
<SYS>请将给定文本翻译成简化中文。</SYS>感谢！以下是文本的翻译： lidar和摄像头是自动驾驶中不可或缺的感知器，大多数现有方法是逐渐减小3D点云和摄像头图像，然后将高级特征进行融合。然而，下降的特征无法保留低级细节信息。在这篇论文中，我们提出了细腻的激光干涉摄像头融合（FGFusion），使用多尺度特征来融合图像和点云的细节信息。首先，我们设计了两个层次结构，一个用于提取图像的高级semantic特征，另一个用于提取低级细节特征。其次，我们引入了一个协助网络，以便点云特征更好地学习细腻的空间信息。最后，我们提出了多尺度融合（MSF），用于融合图像和点云的最后N个特征图。我们在KITTI和Waymo两个流行的自动驾驶测试平台上进行了广泛的实验，结果表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Real-Time-Multi-Task-Learning-System-for-Joint-Detection-of-Face-Facial-Landmark-and-Head-Pose"><a href="#A-Real-Time-Multi-Task-Learning-System-for-Joint-Detection-of-Face-Facial-Landmark-and-Head-Pose" class="headerlink" title="A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose"></a>A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11773">http://arxiv.org/abs/2309.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingtian Wu, Liming Zhang</li>
<li>for: 这篇论文主要针对面部分析任务中的极大角度头 pose 问题，提出了一种实时多任务检测系统，可同时检测面部、 facial landmark 和头 pose。</li>
<li>methods: 该系统基于广泛采用的 YOLOv8 检测框架，添加了更多的landmark regression head，以便高效地确定面部关键点。此外，我们对 YOLOv8 框架中的多个模块进行了优化和提升。</li>
<li>results: 我们在 300W-LP 和 AFLW2000-3D 数据集上进行了广泛的实验，结果表明我们的模型可以有效地处理大角度头 pose 问题，同时具有实时性。<details>
<summary>Abstract</summary>
Extreme head postures pose a common challenge across a spectrum of facial analysis tasks, including face detection, facial landmark detection (FLD), and head pose estimation (HPE). These tasks are interdependent, where accurate FLD relies on robust face detection, and HPE is intricately associated with these key points. This paper focuses on the integration of these tasks, particularly when addressing the complexities posed by large-angle face poses. The primary contribution of this study is the proposal of a real-time multi-task detection system capable of simultaneously performing joint detection of faces, facial landmarks, and head poses. This system builds upon the widely adopted YOLOv8 detection framework. It extends the original object detection head by incorporating additional landmark regression head, enabling efficient localization of crucial facial landmarks. Furthermore, we conduct optimizations and enhancements on various modules within the original YOLOv8 framework. To validate the effectiveness and real-time performance of our proposed model, we conduct extensive experiments on 300W-LP and AFLW2000-3D datasets. The results obtained verify the capability of our model to tackle large-angle face pose challenges while delivering real-time performance across these interconnected tasks.
</details>
<details>
<summary>摘要</summary>
极端头 pose  pose 是一种常见的挑战，涉及到脸部检测、脸部关键点检测 (FLD) 和头 pose 估算 (HPE) 等多个面部分析任务。这些任务之间存在互相关系，精准的 FLD 需要正确的脸部检测，而 HPE 则取决于关键点的确定。本文关注这些任务的集成，特别是在处理大角度头 pose 时的复杂性。我们提出了一个实时多任务检测系统，可同时检测脸部、脸部关键点和头 pose。这个系统基于广泛采用的 YOLOv8 检测框架。我们在原始的对象检测头上添加了附加的关键点 regression 头，以便高效地确定脸部关键点。此外，我们对 YOLOv8 框架中的各个模块进行了优化和改进。为了证明我们提出的模型的有效性和实时性，我们在 300W-LP 和 AFLW2000-3D 数据集上进行了广泛的实验。实验结果表明，我们的模型能够 effectively 处理大角度头 pose 挑战，并在这些相关任务中提供实时性。
</details></li>
</ul>
<hr>
<h2 id="Fast-Satellite-Tensorial-Radiance-Field-for-Multi-date-Satellite-Imagery-of-Large-Size"><a href="#Fast-Satellite-Tensorial-Radiance-Field-for-Multi-date-Satellite-Imagery-of-Large-Size" class="headerlink" title="Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery of Large Size"></a>Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery of Large Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11767">http://arxiv.org/abs/2309.11767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtong Zhang, Yuanxiang Li</li>
<li>for: 提高卫星图像NeRF模型的速度和可扩展性，并解决了大尺寸卫星图像处理的限制。</li>
<li>methods: 使用多尺度矩阵分解方法来表示颜色、体积密度和辅助变量，并使用总变量损失来修正多日图像的不一致性。</li>
<li>results: 与state-of-the-art Sat-NeRF系列相比，SatensoRF在新视图合成性能方面表现出色，并且需要 fewer parameters для训练和推断，具有更快的训练和推断速度和更低的计算需求。<details>
<summary>Abstract</summary>
Existing NeRF models for satellite images suffer from slow speeds, mandatory solar information as input, and limitations in handling large satellite images. In response, we present SatensoRF, which significantly accelerates the entire process while employing fewer parameters for satellite imagery of large size. Besides, we observed that the prevalent assumption of Lambertian surfaces in neural radiance fields falls short for vegetative and aquatic elements. In contrast to the traditional hierarchical MLP-based scene representation, we have chosen a multiscale tensor decomposition approach for color, volume density, and auxiliary variables to model the lightfield with specular color. Additionally, to rectify inconsistencies in multi-date imagery, we incorporate total variation loss to restore the density tensor field and treat the problem as a denosing task.To validate our approach, we conducted assessments of SatensoRF using subsets from the spacenet multi-view dataset, which includes both multi-date and single-date multi-view RGB images. Our results clearly demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in terms of novel view synthesis performance. Significantly, SatensoRF requires fewer parameters for training, resulting in faster training and inference speeds and reduced computational demands.
</details>
<details>
<summary>摘要</summary>
现有的卫星图像NeRF模型受到慢速、必需日升信息作为输入以及处理大型卫星图像的限制。作为回应，我们提出了SatensoRF，它可以快速加速整个过程，并使用 fewer parameters 来处理大型卫星图像。此外，我们发现了传统的LAMBERTIAN表面假设在神经采集场景中失足，特别是 для植物和水生元素。相比传统的层次MLP基本Scene表示，我们选择了多尺度矩阵分解方法来表示颜色、体积密度和辅助变量，以模型光场。此外，为了纠正多日图像之间的不一致，我们添加了总变量损失来修复密度矩阵场景，并将问题视为锈除task。为验证我们的方法，我们对SpaceNet多视点数据集中的subset进行了评估，该数据集包括了多日和单日多视点RGB图像。我们的结果表明，SatensoRF超越了state-of-the-art Sat-NeRF系列在新视图合成性能方面。特别是，SatensoRF需要 fewer parameters 进行训练，导致更快的训练和推理速度，以及减少的计算占用。
</details></li>
</ul>
<hr>
<h2 id="Dictionary-Attack-on-IMU-based-Gait-Authentication"><a href="#Dictionary-Attack-on-IMU-based-Gait-Authentication" class="headerlink" title="Dictionary Attack on IMU-based Gait Authentication"></a>Dictionary Attack on IMU-based Gait Authentication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11766">http://arxiv.org/abs/2309.11766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajeshjnu2006/dictionaryattackonimugait">https://github.com/rajeshjnu2006/dictionaryattackonimugait</a></li>
<li>paper_authors: Rajesh Kumar, Can Isik, Chilukuri K. Mohan</li>
<li>for: 本研究旨在攻击使用IMU嵌入式加速度计记录的步征 Authentication系统。</li>
<li>methods: 本研究使用了一种基于字典的攻击方法，即使用一个IMUGait模式字典来攻击 Authentication系统。</li>
<li>results: 研究发现，可以通过使用IMUGait模式字典来攻击多种用户认证模型，并且对认证系统的安全性提出了挑战。<details>
<summary>Abstract</summary>
We present a novel adversarial model for authentication systems that use gait patterns recorded by the inertial measurement unit (IMU) built into smartphones. The attack idea is inspired by and named after the concept of a dictionary attack on knowledge (PIN or password) based authentication systems. In particular, this work investigates whether it is possible to build a dictionary of IMUGait patterns and use it to launch an attack or find an imitator who can actively reproduce IMUGait patterns that match the target's IMUGait pattern. Nine physically and demographically diverse individuals walked at various levels of four predefined controllable and adaptable gait factors (speed, step length, step width, and thigh-lift), producing 178 unique IMUGait patterns. Each pattern attacked a wide variety of user authentication models. The deeper analysis of error rates (before and after the attack) challenges the belief that authentication systems based on IMUGait patterns are the most difficult to spoof; further research is needed on adversarial models and associated countermeasures.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的反攻击模型，用于authentication系统，利用智能手机内置的抗重力测量单元（IMU）记录的步态特征。这种攻击的想法来自于和知识（PIN或密码）基于的字典攻击。本研究是否可以构建一个IMUGait特征的词典，并使用其发动攻击或找到一个能够活泼地复制目标用户的IMUGait特征。我们采集了9名物理和人口多样化的个体，在四种可控和可适应的步态因素（速度、步长、步宽和膝盖）的不同水平上步行，共生成178个唯一的IMUGait特征。每个特征攻击了多种用户身份验证模型。我们进行了更深层次的错误率分析（之前和之后攻击），挑战了基于IMUGait特征的身份验证系统是最难模仿的假设。需要进一步的研究反攻击模型和相关的防范措施。
</details></li>
</ul>
<hr>
<h2 id="SAM-OCTA-A-Fine-Tuning-Strategy-for-Applying-Foundation-Model-to-OCTA-Image-Segmentation-Tasks"><a href="#SAM-OCTA-A-Fine-Tuning-Strategy-for-Applying-Foundation-Model-to-OCTA-Image-Segmentation-Tasks" class="headerlink" title="SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks"></a>SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11758">http://arxiv.org/abs/2309.11758</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shellredia/sam-octa">https://github.com/shellredia/sam-octa</a></li>
<li>paper_authors: Chengliang Wang, Xinrun Chen, Haojian Ning, Shiying Li</li>
<li>for: 这个论文的目的是提出一种基于低级别适应技术的方法，用于处理多种分类任务在optical coherence tomography angiography（OCTA）图像上。</li>
<li>methods: 该方法使用了基于低级别适应技术的基础模型细化和相关提示点生成策略，以处理OCTA图像上的多种分类任务。</li>
<li>results: 该方法在公共可用的OCTA-500数据集上进行实验，并实现了当前最佳性能指标，同时可以高效地进行本地血管分 segmentation以及有效的血管-血管分 segmentation，这些任务在前一些工作中尚未得到很好的解决。<details>
<summary>Abstract</summary>
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.
</details>
<details>
<summary>摘要</summary>
在Optical coherence tomography angiography（OCTA）图像分析中，需要进行特定目标分割。现有方法通常在有限样本（约几百个）上进行supervised学习，这可能导致过拟合。为解决这个问题，我们采用了low-rank adaptation技术来修改基本模型，并提出了相应的提示点生成策略来处理不同的分割任务。这种方法被称为SAM-OCTA，并在公共可用的OCTA-500 dataset上进行实验。它不仅实现了state-of-the-art性能指标，还能够成功地进行本地血管分割以及有效的动脉-静脉分割，这在前一些工作中尚未得到解决。代码可以在：https://github.com/ShellRedia/SAM-OCTA中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-Centric-Approach-for-Static-Map-Element-Annotation"><a href="#A-Vision-Centric-Approach-for-Static-Map-Element-Annotation" class="headerlink" title="A Vision-Centric Approach for Static Map Element Annotation"></a>A Vision-Centric Approach for Static Map Element Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11754">http://arxiv.org/abs/2309.11754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manymuch/cama">https://github.com/manymuch/cama</a></li>
<li>paper_authors: Jiaxin Zhang, Shiyuan Chen, Haoran Yin, Ruohong Mei, Xuan Liu, Cong Yang, Qian Zhang, Wei Sui<br>for:CAMA is designed to provide high-quality, consistent, and accurate annotations for training machine learning models in the field of computer vision and autonomous driving.methods:CAMA uses a vision-centric approach that leverages cameras to generate 3D annotations of static map elements without relying on LiDAR inputs. The framework achieves high reprojection accuracy across multiple cameras and is spatial-temporally consistent across the entire sequence.results:The proposed CAMA framework is evaluated on the popular nuScenes dataset and shows improved performance compared to the original nuScenes static map elements, with lower reprojection errors (e.g., 4.73 vs. 8.03 pixels).<details>
<summary>Abstract</summary>
The recent development of online static map element (a.k.a. HD Map) construction algorithms has raised a vast demand for data with ground truth annotations. However, available public datasets currently cannot provide high-quality training data regarding consistency and accuracy. To this end, we present CAMA: a vision-centric approach for Consistent and Accurate Map Annotation. Without LiDAR inputs, our proposed framework can still generate high-quality 3D annotations of static map elements. Specifically, the annotation can achieve high reprojection accuracy across all surrounding cameras and is spatial-temporal consistent across the whole sequence. We apply our proposed framework to the popular nuScenes dataset to provide efficient and highly accurate annotations. Compared with the original nuScenes static map element, models trained with annotations from CAMA achieve lower reprojection errors (e.g., 4.73 vs. 8.03 pixels).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PIE-Simulating-Disease-Progression-via-Progressive-Image-Editing"><a href="#PIE-Simulating-Disease-Progression-via-Progressive-Image-Editing" class="headerlink" title="PIE: Simulating Disease Progression via Progressive Image Editing"></a>PIE: Simulating Disease Progression via Progressive Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11745">http://arxiv.org/abs/2309.11745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhao Liang, Xu Cao, Kuei-Da Liao, Tianren Gao, Zhengyu Chen, Tejas Nama</li>
<li>for: 预测疾病进展，促进临床诊断、预后评估和治疗方案设计。</li>
<li>methods: 基于文本生成图像模型，实现疾病进展精准模拟和个性化。iterative refining过程中的梯度下降算法进行了 teoretic 分析。</li>
<li>results: 比对 CLIP 分数（现实度）和疾病分类信息确定率，PIE 超过 Stable Diffusion Walk 和 Style-Based Manifold Extrapolation 等方法。用户测试中，76.2% 的反馈表明生成的进程准确性高。PIE 是首个实现真实标准的疾病进展图像生成方法，有望在医疗机构中应用，改善病人结果。<details>
<summary>Abstract</summary>
Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease Classification Confidence (Alignment). Our user study collected feedback from 35 veteran physicians to assess the generated progressions. Remarkably, 76.2% of the feedback agrees with the fidelity of the generated progressions. To our best knowledge, PIE is the first of its kind to generate disease progression images meeting real-world standards. It is a promising tool for medical research and clinical practice, potentially allowing healthcare providers to model disease trajectories over time, predict future treatment responses, and improve patient outcomes.
</details>
<details>
<summary>摘要</summary>
疾病发展模拟是医学研究中的一个重要领域，具有诊断、预后和治疗中的重要意义。然而，现有的医学影像监测技术存在缺乏连续监测的问题，这限制了疾病发展模拟的准确性和可靠性。为解决这个问题，我们提出了一种新的框架，称为进程图像编辑（PIE），它可以控制疾病相关的图像特征，实现精准和现实的疾病发展模拟。具体来说，我们利用了最新的文本生成图像技术，模拟疾病发展的过程，并为每个患者个性化模拟。我们对PIE框架的迭代纠正过程进行了理论分析，认为它可以视为一种梯度下降算法，其学习率逐渐减少。为验证PIE框架，我们在医学影像领域进行了三项实验。我们的结果显示，PIE框架比现有的方法，如稳定扩散步和基于CLIP的Style-Based Manifold Extrapolation，在CLIP分数（现实）和疾病分类信心度（对齐）方面具有更高的超越性。我们的用户研究收集了35名 veteran physician 的反馈，评估生成的进程是否准确。结果显示，76.2%的反馈同意生成的进程准确性。到我们知道的 extend，PIE是首个满足现代医学标准的疾病发展图像生成框架。它是一种有前途的工具，可以帮助医疗专业人员模拟疾病轨迹，预测未来治疗响应，并提高患者的结果。
</details></li>
</ul>
<hr>
<h2 id="CPR-Coach-Recognizing-Composite-Error-Actions-based-on-Single-class-Training"><a href="#CPR-Coach-Recognizing-Composite-Error-Actions-based-on-Single-class-Training" class="headerlink" title="CPR-Coach: Recognizing Composite Error Actions based on Single-class Training"></a>CPR-Coach: Recognizing Composite Error Actions based on Single-class Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11718">http://arxiv.org/abs/2309.11718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunli Wang, Qing Yu, Shuaibing Wang, Dingkang Yang, Liuzhen Su, Xiao Zhao, Haopeng Kuang, Peixuan Zhang, Peng Zhai, Lihua Zhang</li>
<li>for: 这篇论文的目的是为了提高细部医疗动作分析和技能评估，特别是在紧急救援中Cardiopulmonary Resuscitation（CPR）技能的评估。</li>
<li>methods: 本文使用了视觉基于的系统来完成CPR动作识别和技能评估，并开发了名为CPR-Coach的视频数据集。另外，本文还提出了一个人类认知驿站（ImagineNet）框架，以解决单一类别训练和多类别测试的问题。</li>
<li>results: 实验结果显示，ImagineNet框架能够提高模型的多错识别性，并且在受限监督下进行训练。<details>
<summary>Abstract</summary>
The fine-grained medical action analysis task has received considerable attention from pattern recognition communities recently, but it faces the problems of data and algorithm shortage. Cardiopulmonary Resuscitation (CPR) is an essential skill in emergency treatment. Currently, the assessment of CPR skills mainly depends on dummies and trainers, leading to high training costs and low efficiency. For the first time, this paper constructs a vision-based system to complete error action recognition and skill assessment in CPR. Specifically, we define 13 types of single-error actions and 74 types of composite error actions during external cardiac compression and then develop a video dataset named CPR-Coach. By taking the CPR-Coach as a benchmark, this paper thoroughly investigates and compares the performance of existing action recognition models based on different data modalities. To solve the unavoidable Single-class Training & Multi-class Testing problem, we propose a humancognition-inspired framework named ImagineNet to improve the model's multierror recognition performance under restricted supervision. Extensive experiments verify the effectiveness of the framework. We hope this work could advance research toward fine-grained medical action analysis and skill assessment. The CPR-Coach dataset and the code of ImagineNet are publicly available on Github.
</details>
<details>
<summary>摘要</summary>
《细腔医学动作分析任务在图像识别领域内已经吸引了广泛的关注，但是它面临着数据和算法不足的问题。心肺复苏（CPR）是紧急情况下的重要技能之一，现在CPR技能评估主要靠假人和教练进行，导致训练成本高、效率低。本文首次构建了一个视觉基于的系统，用于完成CPR动作识别和技能评估。特别是，我们定义了13种单个错误动作和74种复合错误动作 durante la compressión cardíaca externa，并开发了名为CPR-Coach的视频数据集。通过使用CPR-Coach作为标准，本文对现有动作识别模型基于不同数据模式进行了广泛的 investigate 和比较。为解决不可避免的单类训练和多类测试问题，我们提出了一个人类认知 inspirited 框架名为ImagineNet，以提高模型的多错误识别性能。广泛的实验证明了效果性。我们希望这项工作能够推动细腔医学动作分析和技能评估的研究进步。CPR-Coach数据集和ImagineNet框架的代码都公开可用于GitHub。
</details></li>
</ul>
<hr>
<h2 id="Deshadow-Anything-When-Segment-Anything-Model-Meets-Zero-shot-shadow-removal"><a href="#Deshadow-Anything-When-Segment-Anything-Model-Meets-Zero-shot-shadow-removal" class="headerlink" title="Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal"></a>Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11715">http://arxiv.org/abs/2309.11715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao</li>
<li>for: Image shadow removal and image restoration.</li>
<li>methods: Deshadow-Anything model, Fine-tuning on large-scale datasets, diffusion model, Multi-Self-Attention Guidance (MSAG), and adaptive input perturbation (DDPM-AIP).</li>
<li>results: Effective improvement in image restoration performance.Here’s the simplified Chinese text:</li>
<li>for: 图像阴影除去和图像修复。</li>
<li>methods: Deshadow-Anything模型、大规模数据集微调、扩散模型、多自注意导航（MSAG）和适应输入扰动（DDPM-AIP）。</li>
<li>results: 图像修复性能得到明显改进。<details>
<summary>Abstract</summary>
Segment Anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.
</details>
<details>
<summary>摘要</summary>
segments anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.Here's the breakdown of the text in Simplified Chinese:segments anything (SAM)：这是一个先进的通用图像分割模型，通过一个庞大的视觉数据集进行训练，为图像分割和计算机视觉设置了新的标准。However, it faced challenges when it came to distinguishing between shadows and their backgrounds：这个模型在分割阴影和背景之间困难。To address this, we developed Deshadow-Anything：为解决这个问题，我们开发了Deshadow-Anything。considering the generalization of large-scale datasets：我们考虑了大规模数据集的通用性。and we performed Fine-tuning on large-scale datasets to achieve image shadow removal：我们在大规模数据集上进行细化调整，以实现图像阴影除去。The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image：涉游模型可以在图像的边缘和Texture上扩散，帮助去除阴影，保留图像的细节。Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion：我们还设计了多重自我注意力指导（MSAG）和适应输入扰动（DDPM-AIP），以加速涉游训练的迭代速度。Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance：实验表明，这些方法可以有效提高图像恢复性能。
</details></li>
</ul>
<hr>
<h2 id="MoDA-Leveraging-Motion-Priors-from-Videos-for-Advancing-Unsupervised-Domain-Adaptation-in-Semantic-Segmentation"><a href="#MoDA-Leveraging-Motion-Priors-from-Videos-for-Advancing-Unsupervised-Domain-Adaptation-in-Semantic-Segmentation" class="headerlink" title="MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation"></a>MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11711">http://arxiv.org/abs/2309.11711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Pan, Xu Yin, Seokju Lee, Sungeui Yoon, In So Kweon</li>
<li>for: 这篇论文的目的是提出一个更实用的领域对应（UDA）方法，用于处理 semantic segmentation 任务中缺乏目标领域标签的问题。</li>
<li>methods: 这篇论文使用了自我指导学习，将 object motion 自然地从无标签的影像中学习出高效的表示。</li>
<li>results: 实验结果显示，MoDA 比已有方法更有效地处理领域对应题，并且可以与现有的 state-of-the-art 方法相结合以进一步提高性能。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) is an effective approach to handle the lack of annotations in the target domain for the semantic segmentation task. In this work, we consider a more practical UDA setting where the target domain contains sequential frames of the unlabeled videos which are easy to collect in practice. A recent study suggests self-supervised learning of the object motion from unlabeled videos with geometric constraints. We design a motion-guided domain adaptive semantic segmentation framework (MoDA), that utilizes self-supervised object motion to learn effective representations in the target domain. MoDA differs from previous methods that use temporal consistency regularization for the target domain frames. Instead, MoDA deals separately with the domain alignment on the foreground and background categories using different strategies. Specifically, MoDA contains foreground object discovery and foreground semantic mining to align the foreground domain gaps by taking the instance-level guidance from the object motion. Additionally, MoDA includes background adversarial training which contains a background category-specific discriminator to handle the background domain gaps. Experimental results on multiple benchmarks highlight the effectiveness of MoDA against existing approaches in the domain adaptive image segmentation and domain adaptive video segmentation. Moreover, MoDA is versatile and can be used in conjunction with existing state-of-the-art approaches to further improve performance.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（USDA）是一种有效的方法，用于处理目标领域中缺乏标注的问题。在这项工作中，我们考虑了更实用的USDA设定，其中目标领域包含序列帧的无标注视频，这些视频易于在实践中收集。一项latest study suggests self-supervised learning of object motion from unlabeled videos with geometric constraints。我们设计了一个基于自我指导的领域适应Semantic segmentation框架（MoDA），该框架利用无标注视频中的自我指导对象运动来学习有效的表示。MoDA与之前的方法不同，它不使用目标领域帧的时间一致约束。相反，MoDA在前景和背景类别上分别进行领域对接，使用不同的策略。具体来说，MoDA包括前景 объек discovery和前景semantic mining，用于对接前景领域的差距。此外，MoDA还包括背景对抗培训，其中包括一个特定于背景类别的挑战器，用于处理背景领域的差距。实验结果表明，MoDA在多个benchmark上比既有approaches更有效，并且MoDA可以与现有的state-of-the-art方法相结合，以进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Long-Short-Temporal-Attention-Network-for-Unsupervised-Video-Object-Segmentation"><a href="#Efficient-Long-Short-Temporal-Attention-Network-for-Unsupervised-Video-Object-Segmentation" class="headerlink" title="Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation"></a>Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11707">http://arxiv.org/abs/2309.11707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Li, Yu Zhang, Li Yuan, Huaxin Xiao, Binbin Lin, Xianghua Xu</li>
<li>For: Unsupervised video object segmentation (VOS) in real-time, without prior knowledge.* Methods: Proposed Long-Short Temporal Attention (LSTA) network, consisting of Long Temporal Memory and Short Temporal Attention modules, with efficient projection and locality-based sliding window techniques for speedup.* Results: Promising performances on several benchmarks with high efficiency.<details>
<summary>Abstract</summary>
Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial-temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.
</details>
<details>
<summary>摘要</summary>
Unsupervised Video Object Segmentation (VOS) targets identifying primary foreground object contours in videos without prior knowledge. However, previous methods do not fully utilize spatial-temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (LSTA) for unsupervised VOS from a holistic view. Specifically, LSTA consists of two dominant modules: Long Temporal Memory and Short Temporal Attention. The former captures long-term global pixel relations of past frames and the current frame, modeling constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals short-term local pixel relations of one nearby frame and the current frame, modeling moving objects by encoding motion pattern. To speed up inference, efficient projection and locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.
</details></li>
</ul>
<hr>
<h2 id="Meta-OOD-Learning-for-Continuously-Adaptive-OOD-Detection"><a href="#Meta-OOD-Learning-for-Continuously-Adaptive-OOD-Detection" class="headerlink" title="Meta OOD Learning for Continuously Adaptive OOD Detection"></a>Meta OOD Learning for Continuously Adaptive OOD Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11705">http://arxiv.org/abs/2309.11705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinheng Wu, Jie Lu, Zhen Fang, Guangquan Zhang</li>
<li>for: 这篇论文目的是为了提出一种基于流动分布的假值检测方法，以适应现实世界中的动态和不断变化的分布。</li>
<li>methods: 本文使用了元学习的方法，包括设计了一个学习到适应图表，以便在训练过程中初始化一个好的假值检测模型，并在测试过程中快速适应新的分布。</li>
<li>results: 实验结果显示，本文的方法可以保持ID类别准确率和假值检测性能，在流动分布下进行测试。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is crucial to modern deep learning applications by identifying and alerting about the OOD samples that should not be tested or used for making predictions. Current OOD detection methods have made significant progress when in-distribution (ID) and OOD samples are drawn from static distributions. However, this can be unrealistic when applied to real-world systems which often undergo continuous variations and shifts in ID and OOD distributions over time. Therefore, for an effective application in real-world systems, the development of OOD detection methods that can adapt to these dynamic and evolving distributions is essential. In this paper, we propose a novel and more realistic setting called continuously adaptive out-of-distribution (CAOOD) detection which targets on developing an OOD detection model that enables dynamic and quick adaptation to a new arriving distribution, with insufficient ID samples during deployment time. To address CAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt diagram such that a good initialized OOD detection model is learned during the training process. In the testing process, MOL ensures OOD detection performance over shifting distributions by quickly adapting to new distributions with a few adaptations. Extensive experiments on several OOD benchmarks endorse the effectiveness of our method in preserving both ID classification accuracy and OOD detection performance on continuously shifting distributions.
</details>
<details>
<summary>摘要</summary>
现代深度学习应用中，外部分布（OOD）检测是关键性能的一部分，可以识别并警示不应该进行测试或预测的外部样本。现有的OOD检测方法在固定分布下已经做出了重要的进步。然而，这可能是不切实际的，因为实际系统经常发生连续变化和分布的更新。因此，为了有效应用于实际系统，需要开发一种能够适应动态和演变分布的OOD检测方法。在这篇论文中，我们提出了一种新的设定，即连续适应外部分布（CAOOD）检测，旨在开发一种能够在部署时间内动态适应新到达的分布，并且只需要很少的标注样本。为了解决CAOOD，我们开发了元外部分布学习（MOL），它通过设计学习适应图来使得一个初始化好的OOD检测模型在训练过程中快速适应新的分布。在测试过程中，MOL确保OOD检测性能在分布Shift过程中保持高效，只需要很少的适应。我们在多个OOD benchmark上进行了广泛的实验，并证明了我们的方法可以保持ID分类精度和OOD检测性能在连续变化的分布下。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.CV_2023_09_21/" data-id="clnsn0vh600dhgf885usg305t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/55/">55</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">22</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
