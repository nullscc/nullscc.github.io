
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.SD_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T15:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/cs.SD_2023_11_09/">cs.SD - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Whispered-Speech-Recognition-Performance-using-Pseudo-whispered-based-Data-Augmentation"><a href="#Improving-Whispered-Speech-Recognition-Performance-using-Pseudo-whispered-based-Data-Augmentation" class="headerlink" title="Improving Whispered Speech Recognition Performance using Pseudo-whispered based Data Augmentation"></a>Improving Whispered Speech Recognition Performance using Pseudo-whispered based Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05179">http://arxiv.org/abs/2311.05179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaofeng Lin, Tanvina Patel, Odette Scharenborg</li>
<li>for: 这个论文的目的是提高嘟哒语音识别器的性能。</li>
<li>methods: 这个论文使用了一种基于信号处理的技术，将正常语音的spectral特征转换为 Pseudo-whispered speech的特征。</li>
<li>results: 在使用 pseudo-whispered speech 的情况下，论文实现了对嘟哒语音识别器的18.2% 的相对改善。US英语的 speaker group 显示最佳效果。进一步调查表明，嘟哒语音中缺失的喉咙信息对嘟哒语音识别器的性能产生了最大的影响。<details>
<summary>Abstract</summary>
Whispering is a distinct form of speech known for its soft, breathy, and hushed characteristics, often used for private communication. The acoustic characteristics of whispered speech differ substantially from normally phonated speech and the scarcity of adequate training data leads to low automatic speech recognition (ASR) performance. To address the data scarcity issue, we use a signal processing-based technique that transforms the spectral characteristics of normal speech to those of pseudo-whispered speech. We augment an End-to-End ASR with pseudo-whispered speech and achieve an 18.2% relative reduction in word error rate for whispered speech compared to the baseline. Results for the individual speaker groups in the wTIMIT database show the best results for US English. Further investigation showed that the lack of glottal information in whispered speech has the largest impact on whispered speech ASR performance.
</details>
<details>
<summary>摘要</summary>
嘟哒是一种特殊的说话方式，其特征是软、呼吸呼的和低语声的，通常用于私人通信。嘟哒的声学特征与正常说话有很大差异，这导致自动语音识别（ASR）性能较低。为解决数据稀缺问题，我们使用一种信号处理技术，将正常说话的spectral特征变换为嘟哒的spectral特征。我们将端到端ASR加以嘟哒语音，实现了与基准相比的18.2%的Relative Error Rate的减少。results表明，US英语的speaker组得到了最好的结果。进一步调查发现，嘟哒语音中缺少的喉咙信息对嘟哒语音识别性能产生了最大的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.SD_2023_11_09/" data-id="clot2mhli0177x788am8d8nrk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/eess.AS_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T14:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/eess.AS_2023_11_09/">eess.AS - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sound-field-reconstruction-using-neural-processes-with-dynamic-kernels"><a href="#Sound-field-reconstruction-using-neural-processes-with-dynamic-kernels" class="headerlink" title="Sound field reconstruction using neural processes with dynamic kernels"></a>Sound field reconstruction using neural processes with dynamic kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05188">http://arxiv.org/abs/2311.05188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zining Liang, Wen Zhang, Thushara D. Abhayapala</li>
<li>for: 用于重建声场的数据驱动方法，以便在有限的实验努力下实现高分辨率声场重建。</li>
<li>methods: 使用 Gaussian Processes (GPs)  kernel 函数模型声场相关性，但这些方法受到固定kernel的局限性，需要手动选择不同声场的优化kernels。</li>
<li>results: 提议一种新的方法，使用基于 Neural Processes (NPs) 的深度神经网络来参数GPs，以动态学习声场特性并提高重建精度。数值实验表明，我们的提议方法在重建精度方面表现出色，提供了一种可靠的替代方案。<details>
<summary>Abstract</summary>
Accurately representing the sound field with the high spatial resolution is critical for immersive and interactive sound field reproduction technology. To minimize experimental effort, data-driven methods have been proposed to estimate sound fields from a small number of discrete observations. In particular, kernel-based methods using Gaussian Processes (GPs) with a covariance function to model spatial correlations have been used for sound field reconstruction. However, these methods have limitations due to the fixed kernels having limited expressiveness, requiring manual identification of optimal kernels for different sound fields. In this work, we propose a new approach that parameterizes GPs using a deep neural network based on Neural Processes (NPs) to reconstruct the magnitude of the sound field. This method has the advantage of dynamically learning kernels from simulated data using an attention mechanism, allowing for greater flexibility and adaptability to the acoustic properties of the sound field. Numerical experiments demonstrate that our proposed approach outperforms current methods in reconstructing accuracy, providing a promising alternative for sound field reconstruction.
</details>
<details>
<summary>摘要</summary>
准确地表示声场的声学特性是创新听音技术的关键。为了最小化实验努力，数据驱动的方法已经被提出来估算声场从一小数量的精确观测数据中。特别是使用 Gaussian Processes（GPs）的kernels来模型声场的空间相关性。然而，这些方法受到固定kernels的限制，需要手动选择不同声场的优化kernels。在这种情况下，我们提出了一种新的方法，使用基于神经网络的Neural Processes（NPs）来重建声场的大小。这种方法有利于通过 simulated数据中的注意力机制来动态学习kernels，从而实现更大的灵活性和适应性。numerical experiments表明，我们的提出的方法可以在重建准确性方面超过现有方法，提供了一个有前途的代替方案。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/eess.AS_2023_11_09/" data-id="clot2mhjw0136x7880nbvdsnh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.CV_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T13:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/cs.CV_2023_11_09/">cs.CV - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Window-Attention-is-Bugged-How-not-to-Interpolate-Position-Embeddings"><a href="#Window-Attention-is-Bugged-How-not-to-Interpolate-Position-Embeddings" class="headerlink" title="Window Attention is Bugged: How not to Interpolate Position Embeddings"></a>Window Attention is Bugged: How not to Interpolate Position Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05613">http://arxiv.org/abs/2311.05613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Bolya, Chaitanya Ryali, Judy Hoffman, Christoph Feichtenhofer</li>
<li>for: 这篇论文主要用于解决现代转换器时代的计算机视觉领域中的一个问题，即naively组合near ubiquitous组件可能会有一个不良的影响。</li>
<li>methods: 该论文使用了窗口注意力、位嵌入和高分辨率finetuning等现代转换器时代的核心概念。然而， authors发现在使用窗口注意力时， interpolating位嵌入可能会导致性能下降。</li>
<li>results: 作者研究了两种现代方法，即Hiera和ViTDet，并发现两者都受到了这个漏洞的影响。通过引入简单的绝对窗口位嵌入策略， authors解决了这个问题，并使得模型在ViTDet中提高速度和性能。最终， authors将Hiera和ViTDet结合，得到了HieraDet，其在COCO数据集上达到了61.7个盒子mAP，成为使用ImageNet-1k预训练的模型中的状态天。<details>
<summary>Abstract</summary>
Window attention, position embeddings, and high resolution finetuning are core concepts in the modern transformer era of computer vision. However, we find that naively combining these near ubiquitous components can have a detrimental effect on performance. The issue is simple: interpolating position embeddings while using window attention is wrong. We study two state-of-the-art methods that have these three components, namely Hiera and ViTDet, and find that both do indeed suffer from this bug. To fix it, we introduce a simple absolute window position embedding strategy, which solves the bug outright in Hiera and allows us to increase both speed and performance of the model in ViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 box mAP on COCO, making it state-of-the-art for models that only use ImageNet-1k pretraining. This all stems from what is essentially a 3 line bug fix, which we name "absolute win".
</details>
<details>
<summary>摘要</summary>
窗口注意力、位嵌入和高解析细化是现代 transformer 时代的计算机视觉核心概念。然而，我们发现将这些组件组合时可能会导致性能下降。问题的原因很简单：在 interpolating 位嵌入时使用窗口注意力是错误的。我们研究了两种现状顶尖方法，即 Hiera 和 ViTDet，并发现它们都受到这个漏洞的影响。为解决这个问题，我们提出了一种简单的绝对窗口位嵌入策略，解决了 Hiera 中的漏洞，并使 ViTDet 的模型速度和性能都得到了提高。最后，我们将这两个方法组合起来，得到了 HieraDet，在 COCO 上达到了 61.7 个盒子 mAP，成为只使用 ImageNet-1k 预训练的状态顶尖模型。这一成果凭借了一个简单的三行错误修复，我们称之为 “绝对胜利”。
</details></li>
</ul>
<hr>
<h2 id="What-Do-I-Hear-Generating-Sounds-for-Visuals-with-ChatGPT"><a href="#What-Do-I-Hear-Generating-Sounds-for-Visuals-with-ChatGPT" class="headerlink" title="What Do I Hear? Generating Sounds for Visuals with ChatGPT"></a>What Do I Hear? Generating Sounds for Visuals with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05609">http://arxiv.org/abs/2311.05609</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Chuan-En Lin, Nikolas Martelaro</li>
<li>for: 这篇论文提出了一种工作流程，用于生成视频媒体中的真实声景。与先前的工作不同，我们的方法不仅强调匹配视频上的声音，而且还包括建议不可见的声音，以创造一个充满感染力的听觉环境。</li>
<li>methods: 我们的方法包括创建场景 контекст、寻思声音和生成声音。我们利用语言模型，如ChatGPT，来推理声音。</li>
<li>results: 我们的实验结果表明，我们的方法可以生成高质量的声景声音，并且可以帮助制作人更好地描绘听觉环境。<details>
<summary>Abstract</summary>
This short paper introduces a workflow for generating realistic soundscapes for visual media. In contrast to prior work, which primarily focus on matching sounds for on-screen visuals, our approach extends to suggesting sounds that may not be immediately visible but are essential to crafting a convincing and immersive auditory environment. Our key insight is leveraging the reasoning capabilities of language models, such as ChatGPT. In this paper, we describe our workflow, which includes creating a scene context, brainstorming sounds, and generating the sounds.
</details>
<details>
<summary>摘要</summary>
这短篇论文介绍了一种工作流程，用于生成真实的听觉场景 для视觉媒体。与之前的工作不同，我们的方法不仅围绕视频上的声音匹配，还涵盖了可能不可见的声音，以创造一个真实、沉浸的听觉环境。我们的关键发现是利用语言模型的理解能力，如ChatGPT。在这篇论文中，我们描述了我们的工作流程，包括创建场景Context、观察声音和生成声音。
</details></li>
</ul>
<hr>
<h2 id="3D-QAE-Fully-Quantum-Auto-Encoding-of-3D-Point-Clouds"><a href="#3D-QAE-Fully-Quantum-Auto-Encoding-of-3D-Point-Clouds" class="headerlink" title="3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds"></a>3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05604">http://arxiv.org/abs/2311.05604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshika Rathi, Edith Tretschk, Christian Theobalt, Rishabh Dabral, Vladislav Golyanik</li>
<li>for: 这篇论文旨在开探使用量子机器学习架构来学习3D表示。</li>
<li>methods: 该方法使用了完全量子的数据处理组件，并在3D点云集中训练了一个量子自动编码器（3D-QAE）。</li>
<li>results: 实验结果表明，该方法在模拟门控量子硬件上比简单的类比方法强度高。<details>
<summary>Abstract</summary>
Existing methods for learning 3D representations are deep neural networks trained and tested on classical hardware. Quantum machine learning architectures, despite their theoretically predicted advantages in terms of speed and the representational capacity, have so far not been considered for this problem nor for tasks involving 3D data in general. This paper thus introduces the first quantum auto-encoder for 3D point clouds. Our 3D-QAE approach is fully quantum, i.e. all its data processing components are designed for quantum hardware. It is trained on collections of 3D point clouds to produce their compressed representations. Along with finding a suitable architecture, the core challenges in designing such a fully quantum model include 3D data normalisation and parameter optimisation, and we propose solutions for both these tasks. Experiments on simulated gate-based quantum hardware demonstrate that our method outperforms simple classical baselines, paving the way for a new research direction in 3D computer vision. The source code is available at https://4dqv.mpi-inf.mpg.de/QAE3D/.
</details>
<details>
<summary>摘要</summary>
现有的方法 для学习3D表示法是使用深度神经网络，并在传统硬件上训练和测试。量子机器学习架构，尽管其理论上预测了速度和表示能力的优势，尚未在这个问题上或3D数据处理中被考虑。这篇论文因此引入了首个基于量子计算机的3D自适应编码器（3D-QAE）。我们的3D-QAE方法完全基于量子硬件，即所有数据处理组件都是为量子硬件设计的。它在集合3D点云数据上训练，以生成压缩表示。在设计such a fully quantum model中，核心挑战包括3D数据Normalization和参数优化，我们提出了解决方案。在模拟门基Quantum硬件上进行的实验表明，我们的方法在比simple classical baseline的情况下表现出了优势，开启了一个新的研究方向于3D计算机视觉。代码可以在https://4dqv.mpi-inf.mpg.de/QAE3D/中获得。
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-Objects-in-the-wild-for-Realistic-Sensor-Simulation"><a href="#Reconstructing-Objects-in-the-wild-for-Realistic-Sensor-Simulation" class="headerlink" title="Reconstructing Objects in-the-wild for Realistic Sensor Simulation"></a>Reconstructing Objects in-the-wild for Realistic Sensor Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05602">http://arxiv.org/abs/2311.05602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Yang, Sivabalan Manivasagam, Yun Chen, Jingkang Wang, Rui Hu, Raquel Urtasun</li>
<li>for: 本研究旨在提供具有实际世界数据重建和新视点渲染功能的方法，以提高机器人训练和测试中的现实感和多样性。</li>
<li>methods: 本研究使用神经网络激光探测器和摄像头感知器数据来估算物体表面的准确几何和真实外观，并采用物理启发的反射特征来模型物体外观。</li>
<li>results: 实验表明， NeuSim 在具有有限训练视图的复杂情况下具有强大的视 sintesis表现，并且可以将 NeuSim 资产组合成虚拟世界，生成真实的多感器数据用于评估自动驾驶感知模型。<details>
<summary>Abstract</summary>
Reconstructing objects from real world data and rendering them at novel views is critical to bringing realism, diversity and scale to simulation for robotics training and testing. In this work, we present NeuSim, a novel approach that estimates accurate geometry and realistic appearance from sparse in-the-wild data captured at distance and at limited viewpoints. Towards this goal, we represent the object surface as a neural signed distance function and leverage both LiDAR and camera sensor data to reconstruct smooth and accurate geometry and normals. We model the object appearance with a robust physics-inspired reflectance representation effective for in-the-wild data. Our experiments show that NeuSim has strong view synthesis performance on challenging scenarios with sparse training views. Furthermore, we showcase composing NeuSim assets into a virtual world and generating realistic multi-sensor data for evaluating self-driving perception models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将实世界数据重构为新视图是现代робо太器训练和测试中的关键。在这种工作中，我们提出了一种新的方法，称为NeuSim，可以从抽象的各种数据中估算高精度的形状和真实的外观。为达到这个目标，我们将物体表面表示为神经网络签名距离函数，并利用激光扫描仪和相机感知器数据来重建平滑和准确的形状和法向量。我们使用物体外观的物理启发式反射模型，可以有效地处理实际场景中的数据。我们的实验表明，NeuSim在具有有限的训练视图的情况下具有强大的视图合成性能。此外，我们还展示了将NeuSim资产集成到虚拟世界中，并生成真实的多感器数据用于评估自动驾驶感知模型。>>>
</details></li>
</ul>
<hr>
<h2 id="SigScatNet-A-Siamese-Scattering-based-Deep-Learning-Approach-for-Signature-Forgery-Detection-and-Similarity-Assessment"><a href="#SigScatNet-A-Siamese-Scattering-based-Deep-Learning-Approach-for-Signature-Forgery-Detection-and-Similarity-Assessment" class="headerlink" title="SigScatNet: A Siamese + Scattering based Deep Learning Approach for Signature Forgery Detection and Similarity Assessment"></a>SigScatNet: A Siamese + Scattering based Deep Learning Approach for Signature Forgery Detection and Similarity Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05579">http://arxiv.org/abs/2311.05579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chokshi, Vansh Jain, Rajas Bhope, Sudhir Dhage</li>
<li>for: 本研究旨在解决ounterfeit signatures问题，提供一个可靠的签名鉴别和相似度评估方法。</li>
<li>methods: 该研究提出了一种基于Siamese深度学习网络和扩散波lets的签名鉴别方法，通过建立签名相似度指数来确定签名的authenticity。</li>
<li>results: 实验结果表明，SigScatNet可以准确地鉴别签名 forgery，并且具有exceptional efficiency，可以运行在低成本的硬件系统上。在ICDAR SigComp Dutch dataset和CEDAR dataset上，SigScatNet的Equal Error Rate分别为3.689%和0.0578%，至今为签名分析领域最佳性和计算效率的州先。<details>
<summary>Abstract</summary>
The surge in counterfeit signatures has inflicted widespread inconveniences and formidable challenges for both individuals and organizations. This groundbreaking research paper introduces SigScatNet, an innovative solution to combat this issue by harnessing the potential of a Siamese deep learning network, bolstered by Scattering wavelets, to detect signature forgery and assess signature similarity. The Siamese Network empowers us to ascertain the authenticity of signatures through a comprehensive similarity index, enabling precise validation and comparison. Remarkably, the integration of Scattering wavelets endows our model with exceptional efficiency, rendering it light enough to operate seamlessly on cost-effective hardware systems. To validate the efficacy of our approach, extensive experimentation was conducted on two open-sourced datasets: the ICDAR SigComp Dutch dataset and the CEDAR dataset. The experimental results demonstrate the practicality and resounding success of our proposed SigScatNet, yielding an unparalleled Equal Error Rate of 3.689% with the ICDAR SigComp Dutch dataset and an astonishing 0.0578% with the CEDAR dataset. Through the implementation of SigScatNet, our research spearheads a new state-of-the-art in signature analysis in terms of EER scores and computational efficiency, offering an advanced and accessible solution for detecting forgery and quantifying signature similarities. By employing cutting-edge Siamese deep learning and Scattering wavelets, we provide a robust framework that paves the way for secure and efficient signature verification systems.
</details>
<details>
<summary>摘要</summary>
现在，假文本签名的问题已经对个人和组织造成了广泛的不便和巨大的挑战。本研究论文提出了一种新的解决方案，即使用深度学习网络（Siamese Network），以及扩散波lets，来检测签名假造和评估签名相似性。Siamese Network使得我们可以对签名进行全面的相似性评估，从而准确地验证和比较签名。特别是，将扩散波lets纳入模型的整合，使得我们的模型具有出色的效率，可以在低成本的硬件系统上运行。为验证我们的方法的有效性，我们对两个开源数据集进行了广泛的实验：ICDAR SigComp Dutch数据集和CEDAR数据集。实验结果表明，我们的提出的SigScatNet具有无 precedent的Equal Error Rate（EER），即3.689%与ICDAR SigComp Dutch数据集和0.0578%与CEDAR数据集。通过实施SigScatNet，我们的研究开拓了一个新的状态态的签名分析方法，在EER分数和计算效率方面占据了新的领先地位，提供了一种高效可靠的签名验证系统。通过使用前沿的Siamese深度学习和扩散波lets，我们提供了一个坚固的框架，为安全可靠的签名验证系统开拓了新的前iers。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Emotion-Expression-Recognition-in-Older-Adults-Interacting-with-a-Virtual-Coach"><a href="#Exploring-Emotion-Expression-Recognition-in-Older-Adults-Interacting-with-a-Virtual-Coach" class="headerlink" title="Exploring Emotion Expression Recognition in Older Adults Interacting with a Virtual Coach"></a>Exploring Emotion Expression Recognition in Older Adults Interacting with a Virtual Coach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05567">http://arxiv.org/abs/2311.05567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristina Palmero, Mikel deVelasco, Mohamed Amine Hmani, Aymen Mtibaa, Leila Ben Letaifa, Pau Buch-Cardona, Raquel Justo, Terry Amorese, Eduardo González-Fraile, Begoña Fernández-Ruanova, Jofre Tenorio-Laranga, Anna Torp Johansen, Micaela Rodrigues da Silva, Liva Jenny Martinussen, Maria Stylianou Korsnes, Gennaro Cordasco, Anna Esposito, Mounim A. El-Yacoubi, Dijana Petrovska-Delacrétaz, M. Inés Torres, Sergio Escalera<br>for: 这个研究旨在设计一个耐心聆听的虚拟教练系统，以帮助健康老年人提高生活质量和独立生活。methods: 这篇论文描述了这个虚拟教练系统中的情绪表达识别模组的开发，包括数据收集、标签设计和方法论的开发，都是根据项目需求进行设计。这里 investigates 了不同的感知modalities，包括语音和视频频道，以及它们在不同文化和标签类型下的表现。results: 结果显示了不同感知modalities在考虑的情感类别上的信息力，多 modalities 方法通常比其他方法高效（约68%的准确率）。这些结果预期会增加关于与老年人在人类-机器交互中的情绪识别的有限的文献。<details>
<summary>Abstract</summary>
The EMPATHIC project aimed to design an emotionally expressive virtual coach capable of engaging healthy seniors to improve well-being and promote independent aging. One of the core aspects of the system is its human sensing capabilities, allowing for the perception of emotional states to provide a personalized experience. This paper outlines the development of the emotion expression recognition module of the virtual coach, encompassing data collection, annotation design, and a first methodological approach, all tailored to the project requirements. With the latter, we investigate the role of various modalities, individually and combined, for discrete emotion expression recognition in this context: speech from audio, and facial expressions, gaze, and head dynamics from video. The collected corpus includes users from Spain, France, and Norway, and was annotated separately for the audio and video channels with distinct emotional labels, allowing for a performance comparison across cultures and label types. Results confirm the informative power of the modalities studied for the emotional categories considered, with multimodal methods generally outperforming others (around 68% accuracy with audio labels and 72-74% with video labels). The findings are expected to contribute to the limited literature on emotion recognition applied to older adults in conversational human-machine interaction.
</details>
<details>
<summary>摘要</summary>
“EMPATHIC”项目旨在设计一个能够识别和促进健康老年人情感表达的虚拟教练。系统的核心特点之一是它的人性感知能力，允许系统根据用户的情感状态提供个性化的经验。本文描述了项目中情感表达识别模块的开发，包括数据收集、注释设计和方法ologique的开发，以满足项目的需求。我们通过调查不同的Modalidades（语音、视频）对于精确地识别特定情感表达的能力，并评估它们在不同文化背景下的表现。结果表明，研究所使用的modalities具有较高的情感分类精度（使用音频标签的准确率约为68%，使用视频标签的准确率在72-74%之间）。这些发现将对于有限的人工智能应用于老年人 conversational human-machine interaction领域的文献进行贡献。
</details></li>
</ul>
<hr>
<h2 id="High-Performance-Transformers-for-Table-Structure-Recognition-Need-Early-Convolutions"><a href="#High-Performance-Transformers-for-Table-Structure-Recognition-Need-Early-Convolutions" class="headerlink" title="High-Performance Transformers for Table Structure Recognition Need Early Convolutions"></a>High-Performance Transformers for Table Structure Recognition Need Early Convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05565">http://arxiv.org/abs/2311.05565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poloclub/tsr-convstem">https://github.com/poloclub/tsr-convstem</a></li>
<li>paper_authors: ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau</li>
<li>for: 这篇论文的目的是提出一种轻量级的视觉编码器来实现表格结构识别（TSR），以提高训练和执行速度，并且允许自动学习。</li>
<li>methods: 该论文使用了一种新的视觉编码器，即干扰减少的径谱核心（Convolutional Stem），以取代传统的 convolutional neural network（CNN）背景。这种新的视觉编码器可以减少模型参数量，并且可以保持表格结构的表示能力。</li>
<li>results: 研究人员通过实验和精度研究发现，该新的视觉编码器可以与传统的 CNN 背景相比，具有类似的表达力，并且可以提高训练和执行速度。此外，该视觉编码器还可以允许自动学习，并且可以在不同的表格结构下保持高度的表达力。<details>
<summary>Abstract</summary>
Table structure recognition (TSR) aims to convert tabular images into a machine-readable format, where a visual encoder extracts image features and a textual decoder generates table-representing tokens. Existing approaches use classic convolutional neural network (CNN) backbones for the visual encoder and transformers for the textual decoder. However, this hybrid CNN-Transformer architecture introduces a complex visual encoder that accounts for nearly half of the total model parameters, markedly reduces both training and inference speed, and hinders the potential for self-supervised learning in TSR. In this work, we design a lightweight visual encoder for TSR without sacrificing expressive power. We discover that a convolutional stem can match classic CNN backbone performance, with a much simpler model. The convolutional stem strikes an optimal balance between two crucial factors for high-performance TSR: a higher receptive field (RF) ratio and a longer sequence length. This allows it to "see" an appropriate portion of the table and "store" the complex table structure within sufficient context length for the subsequent transformer. We conducted reproducible ablation studies and open-sourced our code at https://github.com/poloclub/tsr-convstem to enhance transparency, inspire innovations, and facilitate fair comparisons in our domain as tables are a promising modality for representation learning.
</details>
<details>
<summary>摘要</summary>
表格结构识别（TSR）目标是将表格图像转换为机器可读格式，其中视觉编码器提取图像特征，而文本编码器生成表格表示的 tokens。现有方法使用经典 convolutional neural network（CNN）脊梁来实现视觉编码器，并使用 transformer 来实现文本编码器。然而，这种混合 CNN-Transformer 架构会导致复杂的视觉编码器，占用大量模型参数，显著降低训练和执行速度，并阻碍自主学习在 TSR 中。在这项工作中，我们设计了一个轻量级的视觉编码器，无需牺牲表达能力。我们发现，一个 convolutional stem 可以与经典 CNN 脊梁匹配表现，但具有更简单的模型结构。convolutional stem 在两个关键因素上占据优势：高于 RF 比例和更长的序列长度。这使得它能够 "看" 到适当的表格部分，并 "保存" 表格结构在足够的上下文长度内，以便后续 transformer 进行进一步处理。我们进行了可重复的拟合研究，并将我们的代码公开在 GitHub 上，以便让更多人可以通过这个领域的表格作为表达学习的潜在模式进行研究。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Quantum-and-Classical-Contributions-in-Hybrid-Quantum-Machine-Learning-Architectures"><a href="#Disentangling-Quantum-and-Classical-Contributions-in-Hybrid-Quantum-Machine-Learning-Architectures" class="headerlink" title="Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures"></a>Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05559">http://arxiv.org/abs/2311.05559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kölle, Jonas Maurer, Philipp Altmann, Leo Sünkel, Jonas Stein, Claudia Linnhoff-Popien</li>
<li>for: 这种研究旨在解决量子计算机的输入大小限制问题，通过将经验性的классический模型与量子征函数Circuit相结合。</li>
<li>methods: 我们提出了一种新的混合体系结构，其中使用自适应压缩器来压缩输入数据，然后通过encoder部分将压缩数据传递给量子组件。</li>
<li>results: 我们的模型在四个数据集上进行分类任务时，与两种现有的混合传输学习体系和一种量子体系进行比较，结果显示经验性的 классификатор在混合传输学习中扮演着重要的作用，这一点有时被归结于量子元素。<details>
<summary>Abstract</summary>
Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component - classical and quantum - contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared across four datasets: Banknote Authentication, Breast Cancer Wisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical components significantly influence classification in hybrid transfer learning, a contribution often mistakenly ascribed to the quantum element. The performance of our model aligns with that of a variational quantum circuit using amplitude embedding, positioning it as a feasible alternative.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a novel hybrid architecture that employs an autoencoder to compress the input data before channeling it through the quantum component. We compare our model's classification capabilities with two state-of-the-art hybrid transfer learning architectures, two purely classical architectures, and one quantum architecture, using four datasets: Banknote Authentication, Breast Cancer Wisconsin, MNIST digits, and AudioMNIST.Our findings suggest that the classical components play a significant role in the hybrid transfer learning model's performance, a contribution that is often mistakenly attributed to the quantum element. Our model's performance aligns with that of a variational quantum circuit using amplitude embedding, making it a feasible alternative.
</details></li>
</ul>
<hr>
<h2 id="LCM-LoRA-A-Universal-Stable-Diffusion-Acceleration-Module"><a href="#LCM-LoRA-A-Universal-Stable-Diffusion-Acceleration-Module" class="headerlink" title="LCM-LoRA: A Universal Stable-Diffusion Acceleration Module"></a>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05556">http://arxiv.org/abs/2311.05556</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luosiallen/latent-consistency-model">https://github.com/luosiallen/latent-consistency-model</a></li>
<li>paper_authors: Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, Hang Zhao</li>
<li>for: 用于加速文本到图像生成任务，生成高质量图像，只需要少量的推理步骤。</li>
<li>methods: 使用 LoRA 混合精度模型进行混合精度模型的采样，从而扩展LCM的范围，并且降低了内存占用量。</li>
<li>results: 通过应用 LoRA 混合精度模型，LCM 可以在不同的图像生成任务中实现更高的图像质量，同时具有更好的通用性。<details>
<summary>Abstract</summary>
Latent Consistency Models (LCMs) have achieved impressive performance in accelerating text-to-image generative tasks, producing high-quality images with minimal inference steps. LCMs are distilled from pre-trained latent diffusion models (LDMs), requiring only ~32 A100 GPU training hours. This report further extends LCMs' potential in two aspects: First, by applying LoRA distillation to Stable-Diffusion models including SD-V1.5, SSD-1B, and SDXL, we have expanded LCM's scope to larger models with significantly less memory consumption, achieving superior image generation quality. Second, we identify the LoRA parameters obtained through LCM distillation as a universal Stable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into various Stable-Diffusion fine-tuned models or LoRAs without training, thus representing a universally applicable accelerator for diverse image generation tasks. Compared with previous numerical PF-ODE solvers such as DDIM, DPM-Solver, LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that possesses strong generalization abilities. Project page: https://github.com/luosiallen/latent-consistency-model.
</details>
<details>
<summary>摘要</summary>
Latent Consistency Models (LCMs) 已经实现了在快速生成文本到图像任务中表现出色，生成高质量图像只需要少量推理步骤。LCMs 是从预训练的干扰演化模型 (LDMs) 中提取出来的，需要只有约32个A100 GPU 训练小时。本报告进一步推广LCMs的潜在能力，包括：首先，通过应用LoRA混合精度模型，我们扩展了LCMs的范围，包括SD-V1.5、SSD-1B和SDXL等大型模型，并实现了更高质量的图像生成。其次，我们确定了通过LCM混合精度模型来获得的LoRA参数，可以 viewed为一个通用的稳定演化加速器，名为LCM-LoRA。LCM-LoRA可以直接在不同的稳定演化精度模型或LoRAs中进行插入，无需训练，因此表示一种通用适用的图像生成加速器。相比前一些数值PF-ODE 解决方案，LCM-LoRA可以视为一个嵌入式神经网络PF-ODE 解决方案，拥有强大的泛化能力。项目页面：https://github.com/luosiallen/latent-consistency-model。
</details></li>
</ul>
<hr>
<h2 id="L-WaveBlock-A-Novel-Feature-Extractor-Leveraging-Wavelets-for-Generative-Adversarial-Networks"><a href="#L-WaveBlock-A-Novel-Feature-Extractor-Leveraging-Wavelets-for-Generative-Adversarial-Networks" class="headerlink" title="L-WaveBlock: A Novel Feature Extractor Leveraging Wavelets for Generative Adversarial Networks"></a>L-WaveBlock: A Novel Feature Extractor Leveraging Wavelets for Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05548">http://arxiv.org/abs/2311.05548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirat Shah, Vansh Jain, Anmol Chokshi, Guruprasad Parasnis, Pramod Bide</li>
<li>for: 本研究旨在提高深度学习中的生成模型性能，通过提出一种新的特征提取器——L-WaveBlock，使得生成器更快地趋向于稳定状态。</li>
<li>methods: 本研究使用了Discrete Wavelet Transform（DWT）和深度学习方法，提出了一种名为L-WaveBlock的新特征提取器，可以快速地提取特征，并且可以在不同的尺度和纬度上进行多个级别的特征分解。</li>
<li>results: 研究证明，L-WaveBlock可以在多个数据集上提高生成器的性能，包括道路卫星图像数据集、CelebA数据集和GoPro数据集。L-WaveBlock可以快速地提取特征，并且可以保留细节信息，从而提高生成器的性能。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have risen to prominence in the field of deep learning, facilitating the generation of realistic data from random noise. The effectiveness of GANs often depends on the quality of feature extraction, a critical aspect of their architecture. This paper introduces L-WaveBlock, a novel and robust feature extractor that leverages the capabilities of the Discrete Wavelet Transform (DWT) with deep learning methodologies. L-WaveBlock is catered to quicken the convergence of GAN generators while simultaneously enhancing their performance. The paper demonstrates the remarkable utility of L-WaveBlock across three datasets, a road satellite imagery dataset, the CelebA dataset and the GoPro dataset, showcasing its ability to ease feature extraction and make it more efficient. By utilizing DWT, L-WaveBlock efficiently captures the intricate details of both structural and textural details, and further partitions feature maps into orthogonal subbands across multiple scales while preserving essential information at the same time. Not only does it lead to faster convergence, but also gives competent results on every dataset by employing the L-WaveBlock. The proposed method achieves an Inception Score of 3.6959 and a Structural Similarity Index of 0.4261 on the maps dataset, a Peak Signal-to-Noise Ratio of 29.05 and a Structural Similarity Index of 0.874 on the CelebA dataset. The proposed method performs competently to the state-of-the-art for the image denoising dataset, albeit not better, but still leads to faster convergence than conventional methods. With this, L-WaveBlock emerges as a robust and efficient tool for enhancing GAN-based image generation, demonstrating superior convergence speed and competitive performance across multiple datasets for image resolution, image generation and image denoising.
</details>
<details>
<summary>摘要</summary>
生成敌对网络（GAN）在深度学习领域崛起，能够从Random noise中生成真实的数据。GAN的效果往往取决于特征提取的质量，这是其架构的关键部分。这篇论文提出了L-WaveBlock，一种新的和有力的特征提取器，它利用Discrete Wavelet Transform（DWT）和深度学习方法相结合。L-WaveBlock可以加速GAN生成器的快速整合，同时提高其性能。文章在三个 datasets（路面卫星图像集、CelebA集和GoPro集）上展示了L-WaveBlock的remarkable的可用性，表明它可以更好地提取特征，并且更高效。通过DWT，L-WaveBlock可以高效地捕捉结构和文本细节的细节，并将特征地图分解成多个Scales中的正交subband，同时保留重要信息。这不仅导致更快的整合，还在每个数据集上提供了优秀的结果。提出的方法实现了Inception Score 3.6959和Structural Similarity Index 0.4261在maps dataset上，Peak Signal-to-Noise Ratio 29.05和Structural Similarity Index 0.874在CelebA集上。提出的方法在图像压缩dataset上表现出了竞争力，虽然不如状态之前的最佳，但仍然比传统方法更快。因此，L-WaveBlock emerges as a robust and efficient tool for enhancing GAN-based image generation, demonstrating superior convergence speed and competitive performance across multiple datasets for image resolution, image generation, and image denoising.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Method-for-Simultaneous-Denoising-and-Missing-Wedge-Reconstruction-in-Cryogenic-Electron-Tomography"><a href="#A-Deep-Learning-Method-for-Simultaneous-Denoising-and-Missing-Wedge-Reconstruction-in-Cryogenic-Electron-Tomography" class="headerlink" title="A Deep Learning Method for Simultaneous Denoising and Missing Wedge Reconstruction in Cryogenic Electron Tomography"></a>A Deep Learning Method for Simultaneous Denoising and Missing Wedge Reconstruction in Cryogenic Electron Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05539">http://arxiv.org/abs/2311.05539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/deepdewedge">https://github.com/mli-lab/deepdewedge</a></li>
<li>paper_authors: Simon Wiedemann, Reinhard Heckel</li>
<li>for: 这个论文是用于描述一种基于深度学习的晶体电子显微镜图像重建方法，用于改进晶体电子显微镜图像的视觉质量和分辨率。</li>
<li>methods: 该方法基于自适应损失函数，使用神经网络模型对2D投影图像进行适应，以实现同时的噪声除除和缺失射角重建。</li>
<li>results: 实验表明，使用DeepDeWedge方法可以在 synthetic cryo-ET 数据和实际晶体电子显微镜数据中获得竞争力强的视觉质量和分辨率。<details>
<summary>Abstract</summary>
Cryogenic electron tomography (cryo-ET) is a technique for imaging biological samples such as viruses, cells, and proteins in 3D. A microscope collects a series of 2D projections of the sample, and the goal is to reconstruct the 3D density of the sample called the tomogram. This is difficult as the 2D projections have a missing wedge of information and are noisy. Tomograms reconstructed with conventional methods, such as filtered back-projection, suffer from the noise, and from artifacts and anisotropic resolution due to the missing wedge of information. To improve the visual quality and resolution of such tomograms, we propose a deep-learning approach for simultaneous denoising and missing wedge reconstruction called DeepDeWedge. DeepDeWedge is based on fitting a neural network to the 2D projections with a self-supervised loss inspired by noise2noise-like methods. The algorithm requires no training or ground truth data. Experiments on synthetic and real cryo-ET data show that DeepDeWedge achieves competitive performance for deep learning-based denoising and missing wedge reconstruction of cryo-ET tomograms.
</details>
<details>
<summary>摘要</summary>
冰晶电子 Tomatoes (cryo-ET) 是一种用于图像生物样本如病毒、细胞和蛋白质的三维图像技术。一架镜头收集了一系列二维投影图像，目标是重建样本的三维密度tomogram。这很困难，因为二维投影图像有缺失的弧形信息和噪声。使用传统方法重建tomogram会受到噪声和缺失信息的artefacts和分辨率不均匀的影响。为了改善tomogram的视觉质量和分辨率，我们提出了基于深度学习的同时去噪和缺失弧重建方法called DeepDeWedge。DeepDeWedge基于自适应神经网络与二维投影图像之间的自适应损失函数，无需训练或真实数据。实验表明，DeepDeWedge在深度学习基于去噪和缺失弧重建方面实现了竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Embedding-Space-Interpolation-Beyond-Mini-Batch-Beyond-Pairs-and-Beyond-Examples"><a href="#Embedding-Space-Interpolation-Beyond-Mini-Batch-Beyond-Pairs-and-Beyond-Examples" class="headerlink" title="Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples"></a>Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05538">http://arxiv.org/abs/2311.05538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashanka Venkataramanan, Ewa Kijak, Laurent Amsaleg, Yannis Avrithis</li>
<li>for: 这个论文主要目标是提出一种基于 interpolating 的数据增强方法，以减少 empirical risk minimization (ERM) 的局限性。</li>
<li>methods: 这种方法基于 interpolating 在 embedding space 中进行数据增强，通过生成大量的混合示例来超过 mini-batch 大小。具体来说，我们在 embedding space 中对整个 mini-batch 进行混合，而不是只是在输入空间中进行混合。此外，我们还在混合过程中使用 attention 机制来衡量混合因子的信任程度。</li>
<li>results: 我们的方法在四个不同的标准测试集上实现了显著的改善，即使 interpolating 只是线性的。我们还通过分析 embedding space 来解释这种改善的原因，发现类别在 embedding space 中更加紧密地嵌入和均匀分布，从而解释了改善的行为。<details>
<summary>Abstract</summary>
Mixup refers to interpolation-based data augmentation, originally motivated as a way to go beyond empirical risk minimization (ERM). Its extensions mostly focus on the definition of interpolation and the space (input or feature) where it takes place, while the augmentation process itself is less studied. In most methods, the number of generated examples is limited to the mini-batch size and the number of examples being interpolated is limited to two (pairs), in the input space.   We make progress in this direction by introducing MultiMix, which generates an arbitrarily large number of interpolated examples beyond the mini-batch size and interpolates the entire mini-batch in the embedding space. Effectively, we sample on the entire convex hull of the mini-batch rather than along linear segments between pairs of examples.   On sequence data, we further extend to Dense MultiMix. We densely interpolate features and target labels at each spatial location and also apply the loss densely. To mitigate the lack of dense labels, we inherit labels from examples and weight interpolation factors by attention as a measure of confidence.   Overall, we increase the number of loss terms per mini-batch by orders of magnitude at little additional cost. This is only possible because of interpolating in the embedding space. We empirically show that our solutions yield significant improvement over state-of-the-art mixup methods on four different benchmarks, despite interpolation being only linear. By analyzing the embedding space, we show that the classes are more tightly clustered and uniformly spread over the embedding space, thereby explaining the improved behavior.
</details>
<details>
<summary>摘要</summary>
混合（Mixup）是基于 interpolate 的数据增强技术，原则上是超越 empirical risk minimization（ERM）的方法。其扩展主要关注 interpolate 的定义和发生在哪个空间（输入或特征），而增强过程本身则更少得到研究。大多数方法中，生成的例子数限于 mini-batch 大小，并且 interpolate 的例子数限于两个（对），在输入空间。我们在这个方向上做出了进展， introduce MultiMix，可以生成超过 mini-batch 大小的 interpolated 例子，并且在 embedding 空间中 interpolate 整个 mini-batch。具体来说，我们在 embedding 空间中采样整个凸包而不是在 linear 段 между对的例子之间采样。在序列数据上，我们进一步扩展到 dense MultiMix，在每个空间位置上顺序 interpolate 特征和目标标签，并且将损失采样到每个空间位置。为了缓解缺少密集标签的问题，我们从例子中继承标签并将 interpolate 因子重量化为注意力的度量。总之，我们通过 interpolating 在 embedding 空间来增加每个 mini-batch 中的损失项数量，这是可以 достичь的，因为 interpolate 只是线性的。我们实验表明，我们的解决方案在四个不同的标准测试集上具有显著的改善，即使 interpolate 只是线性的。通过分析 embedding 空间，我们发现类划分更加紧密，uniform 分布在 embedding 空间中，从而解释了改善的行为。
</details></li>
</ul>
<hr>
<h2 id="SeaTurtleID2022-A-long-span-dataset-for-reliable-sea-turtle-re-identification"><a href="#SeaTurtleID2022-A-long-span-dataset-for-reliable-sea-turtle-re-identification" class="headerlink" title="SeaTurtleID2022: A long-span dataset for reliable sea turtle re-identification"></a>SeaTurtleID2022: A long-span dataset for reliable sea turtle re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05524">http://arxiv.org/abs/2311.05524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Adam, Vojtěch Čermák, Kostas Papafitsoros, Lukáš Picek</li>
<li>for: 这个论文提供了首个公共大规模、长期 span 的野生海龟照片数据集 – SeaTurtleID2022（<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%EF%BC%89%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB">https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022）。数据集包含</a> 8729 张照片，共438 个唯一个体，在13年内收集，这使得这个数据集成为动物重复识别领域 longest-spanned 数据集。所有照片都包括了多种注释，例如个体标识、遇到时间戳和身体部分分割面积。</li>
<li>methods: 而不是标准的”随机”分割，这个数据集允许两种现实和生物学上有意义的分割：（i）一个时间感知的closed-set，训练、验证和测试数据来自不同的日期&#x2F;年份，以及（ii）一个时间感知的开放集，测试和验证集中包含新未知的个体。作者表示，时间感知的分割是重要的，因为随机分割会导致性能估计过高。此外，作者还提供了基准实例 segmentation 和重复识别性能的多种体部分。</li>
<li>results: 作者提出了一个基于 Hybrid Task Cascade 的头实例 segmentation和 ArcFace Feature-extractor 的总体系统，并对其进行了评估。系统在 Head 实例 segmentation 和 ArcFace 特征提取器 trained 的情况下实现了86.8% 的精度。<details>
<summary>Abstract</summary>
This paper introduces the first public large-scale, long-span dataset with sea turtle photographs captured in the wild -- SeaTurtleID2022 (https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022). The dataset contains 8729 photographs of 438 unique individuals collected within 13 years, making it the longest-spanned dataset for animal re-identification. All photographs include various annotations, e.g., identity, encounter timestamp, and body parts segmentation masks. Instead of standard "random" splits, the dataset allows for two realistic and ecologically motivated splits: (i) a time-aware closed-set with training, validation, and test data from different days/years, and (ii) a time-aware open-set with new unknown individuals in test and validation sets. We show that time-aware splits are essential for benchmarking re-identification methods, as random splits lead to performance overestimation. Furthermore, a baseline instance segmentation and re-identification performance over various body parts is provided. Finally, an end-to-end system for sea turtle re-identification is proposed and evaluated. The proposed system based on Hybrid Task Cascade for head instance segmentation and ArcFace-trained feature-extractor achieved an accuracy of 86.8%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BakedAvatar-Baking-Neural-Fields-for-Real-Time-Head-Avatar-Synthesis"><a href="#BakedAvatar-Baking-Neural-Fields-for-Real-Time-Head-Avatar-Synthesis" class="headerlink" title="BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis"></a>BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05521">http://arxiv.org/abs/2311.05521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao</li>
<li>for: 实时渲染人类头部模拟，如沉浸式应用、虚拟现实和游戏等</li>
<li>methods: 我们提出了一种新的表示方法，通过从学习的层次表面中提取可变多层网格，并计算表达、姿势和视角依赖的外观特征，以便将其烘焙到静态文本ures中进行高效渲染。</li>
<li>results: 我们的方法可以在实时应用中提供与其他状态艺术法相当的Synthesis结果，同时减少了计算成本。我们还实现了许多头部模拟结果，包括视角synthesis、面部重现、表情编辑和姿势编辑，모든都在交互帧率下进行。<details>
<summary>Abstract</summary>
Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications. Although existing Neural Radiance Fields (NeRF)-based methods achieve high-fidelity results, the computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar, a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用于VR/AR、 телеприсутствие和游戏应用的四维人头模拟是现代计算机视觉技术的核心。 although existing Neural Radiance Fields（NeRF）-based methods can achieve high-fidelity results, their computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar, a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates.中文简体版：使用VR/AR、 telepresence和游戏等应用中的四维人头模拟是现代计算机视觉技术的核心。 although existing Neural Radiance Fields（NeRF）-based methods can achieve high-fidelity results, their computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar, a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates.
</details></li>
</ul>
<hr>
<h2 id="Object-centric-Cross-modal-Feature-Distillation-for-Event-based-Object-Detection"><a href="#Object-centric-Cross-modal-Feature-Distillation-for-Event-based-Object-Detection" class="headerlink" title="Object-centric Cross-modal Feature Distillation for Event-based Object Detection"></a>Object-centric Cross-modal Feature Distillation for Event-based Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05494">http://arxiv.org/abs/2311.05494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Li, Alexander Liniger, Mario Millhaeusler, Vagia Tsiminaki, Yuanyou Li, Dengxin Dai</li>
<li>for: 实时对象检测任务中，事件摄像机的独特特性，如低延迟和高动态范围，得到了越来越多的关注。但是，RGB检测器仍然比事件基于检测器表现更好，因为事件数据稀疏，缺少视觉细节。本文提出了一种新的知识塑模approach，用于缩小这两种感知器之间的性能差距。</li>
<li>methods: 我们提出了一种交叉模态对象检测塑模法，通过设计将知识塑模进行了中心化。我们使用了一种对象中心槽注意机制，可以逐步分离特征图像为对象中心特征和对应像素特征，用于塑模。</li>
<li>results: 我们在一个 sintetic事件数据集和一个实际事件数据集上进行了evaluation。结果显示，对象中心塑模可以显著提高事件基本学生对象检测器的性能，减少了与教师模式之间的性能差距，几乎将其减半。<details>
<summary>Abstract</summary>
Event cameras are gaining popularity due to their unique properties, such as their low latency and high dynamic range. One task where these benefits can be crucial is real-time object detection. However, RGB detectors still outperform event-based detectors due to the sparsity of the event data and missing visual details. In this paper, we develop a novel knowledge distillation approach to shrink the performance gap between these two modalities. To this end, we propose a cross-modality object detection distillation method that by design can focus on regions where the knowledge distillation works best. We achieve this by using an object-centric slot attention mechanism that can iteratively decouple features maps into object-centric features and corresponding pixel-features used for distillation. We evaluate our novel distillation approach on a synthetic and a real event dataset with aligned grayscale images as a teacher modality. We show that object-centric distillation allows to significantly improve the performance of the event-based student object detector, nearly halving the performance gap with respect to the teacher.
</details>
<details>
<summary>摘要</summary>
Event 摄像机在使用其特有优点，如低延迟和高动态范围时变得越来越受欢迎。一个任务，即实时物体检测，其中事件摄像机的优势可以极大。然而，RGB 摄像机仍然超越事件基于摄像机，这是因为事件数据稀疏，缺少视觉细节。在这篇论文中，我们开发了一种新的知识塑化方法，以缩小这两种模式之间的性能差距。为此，我们提议一种跨模态物体检测塑化方法，可以帮助我们快速寻找最佳塑化区域。我们使用一种对象中心槽注意机制，可以逐渐分离特征图像，并将其分配给对象中心特征和对应的像素特征。我们在一个Synthetic和一个实际事件数据集上进行了评估，并证明了对象中心塑化可以有效地提高事件基本学生对象检测器的性能，减少了与教师模式之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Retinal-OCT-Synthesis-with-Denoising-Diffusion-Probabilistic-Models-for-Layer-Segmentation"><a href="#Retinal-OCT-Synthesis-with-Denoising-Diffusion-Probabilistic-Models-for-Layer-Segmentation" class="headerlink" title="Retinal OCT Synthesis with Denoising Diffusion Probabilistic Models for Layer Segmentation"></a>Retinal OCT Synthesis with Denoising Diffusion Probabilistic Models for Layer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05479">http://arxiv.org/abs/2311.05479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuli Wu, Weidong He, Dennis Eschweiler, Ningxin Dou, Zixin Fan, Shengli Mi, Peter Walter, Johannes Stegmaier</li>
<li>for: 减少手动标注的需求</li>
<li>methods: 使用杂样扩散概率模型（DDPMs）自动生成retinal optic coherence tomography（OCT）图像</li>
<li>results: 实现层 segmentation 精度的提高，并且可以使用知识承袭来获得更准确的pseudo标签，从而进一步提高segmentation任务的准确率。<details>
<summary>Abstract</summary>
Modern biomedical image analysis using deep learning often encounters the challenge of limited annotated data. To overcome this issue, deep generative models can be employed to synthesize realistic biomedical images. In this regard, we propose an image synthesis method that utilizes denoising diffusion probabilistic models (DDPMs) to automatically generate retinal optical coherence tomography (OCT) images. By providing rough layer sketches, the trained DDPMs can generate realistic circumpapillary OCT images. We further find that more accurate pseudo labels can be obtained through knowledge adaptation, which greatly benefits the segmentation task. Through this, we observe a consistent improvement in layer segmentation accuracy, which is validated using various neural networks. Furthermore, we have discovered that a layer segmentation model trained solely with synthesized images can achieve comparable results to a model trained exclusively with real images. These findings demonstrate the promising potential of DDPMs in reducing the need for manual annotations of retinal OCT images.
</details>
<details>
<summary>摘要</summary>
现代医学生物图像分析常遇到有限精度标注数据的挑战。为解决这个问题，深度生成模型可以被应用来生成真实的医学图像。在这种情况下，我们提出了使用杂色扩散概率模型（DDPM）来自动生成 retinal 光学同步图像。通过提供粗糙层スケッチ，训练的 DDPM 可以生成真实的 circumpapillary OCT 图像。我们发现，通过知识转移，可以获得更准确的假标签，这对分割任务有益。通过这种方法，我们观察到层分割精度的一致提高，并通过不同的神经网络进行验证。此外，我们发现，通过只使用生成图像进行训练，可以实现与使用真实图像进行训练相同的结果。这些发现表明 DDPM 在减少手动标注 retinal OCT 图像的需求方面具有普遍的承诺性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Retraining-free-GAN-Fingerprinting-via-Personalized-Normalization"><a href="#Robust-Retraining-free-GAN-Fingerprinting-via-Personalized-Normalization" class="headerlink" title="Robust Retraining-free GAN Fingerprinting via Personalized Normalization"></a>Robust Retraining-free GAN Fingerprinting via Personalized Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05478">http://arxiv.org/abs/2311.05478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni</li>
<li>for: 本研究旨在提供一种可以在生成模型中隐藏指纹的方法，以便跟踪和识别违反许可协议或恶意使用的责任用户。</li>
<li>methods: 本研究提出了一种不需要重新训练的生成模型批处理方法，可以轻松地生成具有不同指纹的模型 копи。该方法通过在生成器中插入个性化 нормализа化（PN）层，并通过两个专门的浅层网络（ParamGen Nets）来生成PN层的参数（缩放和偏置），以将指纹隐藏在生成的图像中。同时，也训练了一个恢复指纹的 watermark 解码器。</li>
<li>results: 比较研究表明，提出的方法在对生成模型和图像进行攻击时的Robustness性能较高，并且可以轻松地生成具有不同指纹的模型 копи。<details>
<summary>Abstract</summary>
In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在最近几年，生成模型在商业应用中得到了广泛的应用，由模型开发者向用户发布和分发，用户则使用它们提供服务。在这种情况下，需要跟踪和识别许可协议或任何类型的恶意使用时的责任用户。 Although there are methods to embed invisible watermarks in the images produced by Generative Adversarial Networks (GANs), generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.
</details></li>
</ul>
<hr>
<h2 id="Using-ResNet-to-Utilize-4-class-T2-FLAIR-Slice-Classification-Based-on-the-Cholinergic-Pathways-Hyperintensities-Scale-for-Pathological-Aging"><a href="#Using-ResNet-to-Utilize-4-class-T2-FLAIR-Slice-Classification-Based-on-the-Cholinergic-Pathways-Hyperintensities-Scale-for-Pathological-Aging" class="headerlink" title="Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging"></a>Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05477">http://arxiv.org/abs/2311.05477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Chun Kevin Tsai, Yi-Chien Liu, Ming-Chun Yu, Chia-Ju Chou, Sui-Hing Yan, Yang-Teng Fan, Yan-Hsiang Huang, Yen-Ling Chiu, Yi-Fang Chuang, Ran-Zan Wang, Yao-Chia Shih</li>
<li>for: 评估诊断识念病患者的白 matter 高敏感度，并用于评估诊断识念病患者的病种严重程度。</li>
<li>methods: 使用深度学习模型，包括 ResNet，对 ADNI T2-FLAIR 数据集进行训练，并对本地数据集进行测试。</li>
<li>results: 模型的性能达到了 99.82% 的准确率和 99.83% 的 F1 分数，表明该模型可以有效地自动标识 T2-FLAIR 图像中关键的四个剖面，并且可以帮助临床医生高效地评估诊断识念病患者的风险。<details>
<summary>Abstract</summary>
The Cholinergic Pathways Hyperintensities Scale (CHIPS) is a visual rating scale used to assess the extent of cholinergic white matter hyperintensities in T2-FLAIR images, serving as an indicator of dementia severity. However, the manual selection of four specific slices for rating throughout the entire brain is a time-consuming process. Our goal was to develop a deep learning-based model capable of automatically identifying the four slices relevant to CHIPS. To achieve this, we trained a 4-class slice classification model (BSCA) using the ADNI T2-FLAIR dataset (N=150) with the assistance of ResNet. Subsequently, we tested the model's performance on a local dataset (N=30). The results demonstrated the efficacy of our model, with an accuracy of 99.82% and an F1-score of 99.83%. This achievement highlights the potential impact of BSCA as an automatic screening tool, streamlining the selection of four specific T2-FLAIR slices that encompass white matter landmarks along the cholinergic pathways. Clinicians can leverage this tool to assess the risk of clinical dementia development efficiently.
</details>
<details>
<summary>摘要</summary>
“数位学习基础的BSCA模型可以自动识别T2-FLAIR图像中敏感的四个面积，协助评估浅白质脑病变的严重程度。我们使用ADNI T2-FLAIR数据集（N=150）和ResNet进行训练，并在本地数据集（N=30）进行测试。结果显示BSCA模型在这些数据集中的准确率为99.82%，F1分数为99.83%。这一结果表明BSCA模型具有评估浅白质脑病变的潜在影响力，并且可以帮助临床医生高效评估浅白质脑病变的遗传风险。”Note: "BSCA" stands for "Bilateral Slice Classification Algorithm", and "CHIPS" stands for "Cholinergic Pathways Hyperintensities Scale".
</details></li>
</ul>
<hr>
<h2 id="3DStyle-Diffusion-Pursuing-Fine-grained-Text-driven-3D-Stylization-with-2D-Diffusion-Models"><a href="#3DStyle-Diffusion-Pursuing-Fine-grained-Text-driven-3D-Stylization-with-2D-Diffusion-Models" class="headerlink" title="3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models"></a>3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05464">http://arxiv.org/abs/2311.05464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanghb22-fdu/3dstyle-diffusion-official">https://github.com/yanghb22-fdu/3dstyle-diffusion-official</a></li>
<li>paper_authors: Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Tao Mei</li>
<li>for: 这个论文的目的是提出一种基于文本驱动的3D内容创建方法，以提高 multimedia 和图形领域中的3D内容创建效果。</li>
<li>methods: 这个方法使用CLIP模型来对3D模型进行 semantic-level cross-modal协调，并通过增加2DDiffusion模型来提供更多的控制性来进行细腻的样式化。</li>
<li>results: 该方法可以实现高质量的细腻样式化效果，并且可以控制3D模型的形态和颜色特征。<details>
<summary>Abstract</summary>
3D content creation via text-driven stylization has played a fundamental challenge to multimedia and graphics community. Recent advances of cross-modal foundation models (e.g., CLIP) have made this problem feasible. Those approaches commonly leverage CLIP to align the holistic semantics of stylized mesh with the given text prompt. Nevertheless, it is not trivial to enable more controllable stylization of fine-grained details in 3D meshes solely based on such semantic-level cross-modal supervision. In this work, we propose a new 3DStyle-Diffusion model that triggers fine-grained stylization of 3D meshes with additional controllable appearance and geometric guidance from 2D Diffusion models. Technically, 3DStyle-Diffusion first parameterizes the texture of 3D mesh into reflectance properties and scene lighting using implicit MLP networks. Meanwhile, an accurate depth map of each sampled view is achieved conditioned on 3D mesh. Then, 3DStyle-Diffusion leverages a pre-trained controllable 2D Diffusion model to guide the learning of rendered images, encouraging the synthesized image of each view semantically aligned with text prompt and geometrically consistent with depth map. This way elegantly integrates both image rendering via implicit MLP networks and diffusion process of image synthesis in an end-to-end fashion, enabling a high-quality fine-grained stylization of 3D meshes. We also build a new dataset derived from Objaverse and the evaluation protocol for this task. Through both qualitative and quantitative experiments, we validate the capability of our 3DStyle-Diffusion. Source code and data are available at \url{https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official}.
</details>
<details>
<summary>摘要</summary>
三维内容创建通过文本驱动化的样式化问题对 multimedia 和图形社区带来了基本挑战。 latest advances 的 cross-modal foundation models (例如 CLIP) 使得这个问题变得可能。这些方法通常利用 CLIP 将整体 semantics 的饰色化网格与文本提示相对align。然而，不是那么容易使得基于 semantic-level cross-modal supervision 的细节样式化来得到控制。在这种情况下，我们提出了一种新的 3DStyle-Diffusion 模型，可以让 3D 模型进行细节样式化，同时还可以通过二维Diffusion 模型提供可控的外观和几何指导。技术上，3DStyle-Diffusion 首先将 3D 模型的 texture 分解成 reflectance properties 和 scene lighting 使用隐藏层MLP网络。然后，每个采样视图中的准确深度图可以通过 conditioned 于 3D 模型来实现。接着，3DStyle-Diffusion 利用预训练的可控二维Diffusion 模型来引导学习Rendered 图像，使得每个视图的生成图像与文本提示 semantically 相align，并且几何上与深度图保持一致。这种方式精妙地结合了 implicit MLP 网络来渲染图像和 diffusion 过程来实现高质量的细节样式化。我们还建立了基于 Objaverse 的新数据集和评价协议。通过Qualitative 和量化的实验，我们证明了 3DStyle-Diffusion 的能力。源代码和数据可以在 \url{https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official} 上获取。
</details></li>
</ul>
<hr>
<h2 id="ControlStyle-Text-Driven-Stylized-Image-Generation-Using-Diffusion-Priors"><a href="#ControlStyle-Text-Driven-Stylized-Image-Generation-Using-Diffusion-Priors" class="headerlink" title="ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors"></a>ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05463">http://arxiv.org/abs/2311.05463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei</li>
<li>for: 这个论文的目的是提出一个新的“ стили化”文本到图像生成任务，即基于文本提示和风格图像的叙述图像生成。这个任务旨在生成具有Semantic relevance和风格相似性的风格图像，以提高内容创建的可编辑性。</li>
<li>methods: 作者提出了一种新的扩展了一种基于文本到图像模型的扩展方法，即控制风格（ControlStyle）模型。该模型通过增加一个可调节网络来实现更多的文本提示和风格图像的控制，并同时引入了扩散样式和内容正则化以促进这个调度网络的学习。</li>
<li>results: 实验表明，作者的ControlStyle模型可以生成更加视觉吸引人和艺术性高的风格图像，超过了简单地将文本到图像模型和传统风格传递技术相结合。<details>
<summary>Abstract</summary>
Recently, the multimedia community has witnessed the rise of diffusion models trained on large-scale multi-modal data for visual content creation, particularly in the field of text-to-image generation. In this paper, we propose a new task for ``stylizing'' text-to-image models, namely text-driven stylized image generation, that further enhances editability in content creation. Given input text prompt and style image, this task aims to produce stylized images which are both semantically relevant to input text prompt and meanwhile aligned with the style image in style. To achieve this, we present a new diffusion model (ControlStyle) via upgrading a pre-trained text-to-image model with a trainable modulation network enabling more conditions of text prompts and style images. Moreover, diffusion style and content regularizations are simultaneously introduced to facilitate the learning of this modulation network with these diffusion priors, pursuing high-quality stylized text-to-image generation. Extensive experiments demonstrate the effectiveness of our ControlStyle in producing more visually pleasing and artistic results, surpassing a simple combination of text-to-image model and conventional style transfer techniques.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:近期， multimedia 社区 witnessed  diffusion models 在大规模多模态数据上训练，尤其是文本到图像生成领域。在这篇文章中，我们提出了一个新的任务，即文本驱动的风格化图像生成任务，这个任务可以进一步提高内容创建的可编辑性。给定输入文本提示和风格图像，这个任务的目标是生成具有 semantics 和风格图像的风格化图像。为了实现这一目标，我们提出了一个新的扩展模型（ControlStyle），通过对预训练的文本到图像模型加入可调整的模ulation 网络，以便更多的文本提示和风格图像可以被支持。此外，我们同时引入了扩散样式和内容规则，以便在这个模ulation 网络中学习这些扩散规则，实现高质量的风格化文本到图像生成。实验证明，我们的 ControlStyle 可以生成更加美观和艺术性强的风格化图像，超过了简单地将文本到图像模型和普通的风格转移技术相结合。
</details></li>
</ul>
<hr>
<h2 id="Control3D-Towards-Controllable-Text-to-3D-Generation"><a href="#Control3D-Towards-Controllable-Text-to-3D-Generation" class="headerlink" title="Control3D: Towards Controllable Text-to-3D Generation"></a>Control3D: Towards Controllable Text-to-3D Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05461">http://arxiv.org/abs/2311.05461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, Tao Mei</li>
<li>for: 这个论文旨在提高文本到3D图像生成的控制性，使用额外的手绘素描来指导生成3D场景。</li>
<li>methods: 该论文使用一种名为ControlNet的2Dconditioned扩散模型，将文本提示和手绘素描用于指导3D场景的生成。此外，该论文还使用了一个 pré-trained的可微分图片到素描模型来直接估算rendered图像的素描。</li>
<li>results: 经过广泛的实验，该论文显示了具有高度准确和忠实的3D场景生成结果，与输入文本提示和素描具有高度的一致性。<details>
<summary>Abstract</summary>
Recent remarkable advances in large-scale text-to-image diffusion models have inspired a significant breakthrough in text-to-3D generation, pursuing 3D content creation solely from a given text prompt. However, existing text-to-3D techniques lack a crucial ability in the creative process: interactively control and shape the synthetic 3D contents according to users' desired specifications (e.g., sketch). To alleviate this issue, we present the first attempt for text-to-3D generation conditioning on the additional hand-drawn sketch, namely Control3D, which enhances controllability for users. In particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide the learning of 3D scene parameterized as NeRF, encouraging each view of 3D scene aligned with the given text prompt and hand-drawn sketch. Moreover, we exploit a pre-trained differentiable photo-to-sketch model to directly estimate the sketch of the rendered image over synthetic 3D scene. Such estimated sketch along with each sampled view is further enforced to be geometrically consistent with the given sketch, pursuing better controllable text-to-3D generation. Through extensive experiments, we demonstrate that our proposal can generate accurate and faithful 3D scenes that align closely with the input text prompts and sketches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transformer-based-Model-for-Oral-Epithelial-Dysplasia-Segmentation"><a href="#Transformer-based-Model-for-Oral-Epithelial-Dysplasia-Segmentation" class="headerlink" title="Transformer-based Model for Oral Epithelial Dysplasia Segmentation"></a>Transformer-based Model for Oral Epithelial Dysplasia Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05452">http://arxiv.org/abs/2311.05452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam J Shephard, Hanya Mahmood, Shan E Ahmed Raza, Anna Luiza Damaceno Araujo, Alan Roger Santos-Silva, Marcio Ajudarte Lopes, Pablo Agustin Vargas, Kris McCombe, Stephanie Craig, Jacqueline James, Jill Brooks, Paul Nankivell, Hisham Mehanna, Syed Ali Khurram, Nasir M Rajpoot</li>
<li>for: 提高嘴唇区细胞变性诊断的准确率，减少患者的过度&#x2F;下降治疗。</li>
<li>methods: 使用Transformer模型对染色体扫描整个扫描图像（WSIs）进行检测和分 segmentation。</li>
<li>results: 在各自中心的测试数据上达到了状态之作的结果，mean F1-score为0.81，在外部测试中轻微下降至0.71，表明了良好的普适性。<details>
<summary>Abstract</summary>
Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosis given to lesions of the oral cavity. OED grading is subject to large inter/intra-rater variability, resulting in the under/over-treatment of patients. We developed a new Transformer-based pipeline to improve detection and segmentation of OED in haematoxylin and eosin (H&E) stained whole slide images (WSIs). Our model was trained on OED cases (n = 260) and controls (n = 105) collected using three different scanners, and validated on test data from three external centres in the United Kingdom and Brazil (n = 78). Our internal experiments yield a mean F1-score of 0.81 for OED segmentation, which reduced slightly to 0.71 on external testing, showing good generalisability, and gaining state-of-the-art results. This is the first externally validated study to use Transformers for segmentation in precancerous histology images. Our publicly available model shows great promise to be the first step of a fully-integrated pipeline, allowing earlier and more efficient OED diagnosis, ultimately benefiting patient outcomes.
</details>
<details>
<summary>摘要</summary>
口腔膜癌前期诊断（OED）是指口腔部位的肿瘤前期病变。OED评估存在大量的内外评估者差异，导致患者过度或者UNDER treated。我们开发了一个基于Transformer的管道，以提高H&E染色整幕图像中OED的检测和分 segmentation。我们的模型在OED例子（n = 260）和控制例子（n = 105）上进行了训练，并在三个外部中心进行了验证（n = 78）。我们的内部实验中的F1分数为0.81，在外部测试中为0.71，表明了良好的普适性。这是首次使用Transformers进行 précancerous histology图像的分 segmentation的外部验证研究。我们公开提供的模型表现出了非常的承诺，可以在更早的时间内更有效地诊断OED，最终为患者的疾病结果产生正面的影响。
</details></li>
</ul>
<hr>
<h2 id="Dual-Pipeline-Style-Transfer-with-Input-Distribution-Differentiation"><a href="#Dual-Pipeline-Style-Transfer-with-Input-Distribution-Differentiation" class="headerlink" title="Dual Pipeline Style Transfer with Input Distribution Differentiation"></a>Dual Pipeline Style Transfer with Input Distribution Differentiation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05432">http://arxiv.org/abs/2311.05432</a></li>
<li>repo_url: None</li>
<li>paper_authors: ShiQi Jiang, JunJie Kang, YuJian Li</li>
<li>for: 降低纹理表示和artefacts，提高颜色质量</li>
<li>methods: 使用遮盖总变量损失(Mtv)，并通过实验表明可以几乎完全消除纹理表示</li>
<li>results: 提出输入分布差异训练策略(IDD)，使得纹理生成完全依赖噪声分布，而平滑分布不会生成纹理，从而完全消除纹理表示和artefacts，并且在颜色传输任务中选择平滑分布作为前向推测阶段的输入，以实现高质量颜色传输。<details>
<summary>Abstract</summary>
The color and texture dual pipeline architecture (CTDP) suppresses texture representation and artifacts through masked total variation loss (Mtv), and further experiments have shown that smooth input can almost completely eliminate texture representation. We have demonstrated through experiments that smooth input is not the key reason for removing texture representations, but rather the distribution differentiation of the training dataset. Based on this, we propose an input distribution differentiation training strategy (IDD), which forces the generation of textures to be completely dependent on the noise distribution, while the smooth distribution will not produce textures at all. Overall, our proposed distribution differentiation training strategy allows for two pre-defined input distributions to be responsible for two generation tasks, with noise distribution responsible for texture generation and smooth distribution responsible for color smooth transfer. Finally, we choose a smooth distribution as the input for the forward inference stage to completely eliminate texture representations and artifacts in color transfer tasks.
</details>
<details>
<summary>摘要</summary>
颜色和xture dual pipeline架构（CTDP）通过掩码总变量损失（Mtv）来抑制xture表示和artefacts，而进一步的实验表明，可以将输入数据的平滑化至近乎完全消除xture表示。我们通过实验表明，不是平滑输入是Texture representations的主要原因，而是训练集的分布差异。基于这，我们提议一种输入分布差异训练策略（IDD），强制生成的xture需要完全依赖于随机分布，而平滑分布则不会生成xture。总的来说，我们的提议的输入分布差异训练策略使得两个预定的输入分布负责两个生成任务，随机分布负责xture生成，平滑分布负责颜色平滑传输。最后，我们选择平滑分布作为前向推理阶段的输入，完全消除颜色传输任务中的xture表示和artefacts。
</details></li>
</ul>
<hr>
<h2 id="Active-Mining-Sample-Pair-Semantics-for-Image-text-Matching"><a href="#Active-Mining-Sample-Pair-Semantics-for-Image-text-Matching" class="headerlink" title="Active Mining Sample Pair Semantics for Image-text Matching"></a>Active Mining Sample Pair Semantics for Image-text Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05425">http://arxiv.org/abs/2311.05425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongfeng Chena, Jin Liua, Zhijing Yang, Ruihan Chena, Junpeng Tan</li>
<li>for: This paper proposes a novel image-text matching model to improve the performance and generalization ability of commonsense learning methods in image-text matching tasks.</li>
<li>methods: The proposed method, called Active Mining Sample Pair Semantics (AMSPS), uses an active learning idea with an Adaptive Hierarchical Reinforcement Loss (AHRL) to diversify learning modes and adaptively mine more hidden relevant semantic representations from uncommented items.</li>
<li>results: Experimental results on Flickr30K and MSCOCO universal datasets show that the proposed method is superior to advanced comparison methods.Here’s the Chinese version of the three key points:</li>
<li>for: 本文提出了一种新的图文匹配模型，以提高图文匹配任务中commonsense学习方法的性能和泛化能力。</li>
<li>methods: 提出的方法是Active Mining Sample Pair Semantics (AMSPS)，它使用了活动学习的想法，并使用了自适应层次强化损失函数（AHRL）来多样化学习模式，并从未注释的项中挖掘更多的隐藏相关 semantic表示。</li>
<li>results: 对于Flickr30K和MSCOCO universal datasets的实验结果显示，提出的方法比先前的比较方法更高效。<details>
<summary>Abstract</summary>
Recently, commonsense learning has been a hot topic in image-text matching. Although it can describe more graphic correlations, commonsense learning still has some shortcomings: 1) The existing methods are based on triplet semantic similarity measurement loss, which cannot effectively match the intractable negative in image-text sample pairs. 2) The weak generalization ability of the model leads to the poor effect of image and text matching on large-scale datasets. According to these shortcomings. This paper proposes a novel image-text matching model, called Active Mining Sample Pair Semantics image-text matching model (AMSPS). Compared with the single semantic learning mode of the commonsense learning model with triplet loss function, AMSPS is an active learning idea. Firstly, the proposed Adaptive Hierarchical Reinforcement Loss (AHRL) has diversified learning modes. Its active learning mode enables the model to more focus on the intractable negative samples to enhance the discriminating ability. In addition, AMSPS can also adaptively mine more hidden relevant semantic representations from uncommented items, which greatly improves the performance and generalization ability of the model. Experimental results on Flickr30K and MSCOCO universal datasets show that our proposed method is superior to advanced comparison methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Linear-Gaussian-Bounding-Box-Representation-and-Ring-Shaped-Rotated-Convolution-for-Oriented-Object-Detection"><a href="#Linear-Gaussian-Bounding-Box-Representation-and-Ring-Shaped-Rotated-Convolution-for-Oriented-Object-Detection" class="headerlink" title="Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection"></a>Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05410">http://arxiv.org/abs/2311.05410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhou, Yunkai Ma, Junfeng Fan, Zhaoyang Liu, Fengshui Jing, Min Tan</li>
<li>for: 提高 oriented object detection 的精度，解决当前方法困难地预测对象orientation信息的问题。</li>
<li>methods: 提出 linear GBB (LGBB) 和 ring-shaped rotated convolution (RRC) 两种新方法，分别解决 OBB 表示问题和Feature extraction问题。</li>
<li>results: 实验结果显示，提出的 LGBB 和 RRC 可以有效地提高 oriented object detection 的精度，并在 DOTA 和 HRSC2016 数据集上达到了最佳性能。<details>
<summary>Abstract</summary>
Due to the frequent variability of object orientation, accurate prediction of orientation information remains a challenge in oriented object detection. To better extract orientation-related information, current methods primarily focus on the design of reasonable representations of oriented bounding box (OBB) and rotation-sensitive feature extraction. However, existing OBB representations often suffer from boundary discontinuity and representation ambiguity problems. Methods of designing continuous and unambiguous regression losses do not essentially solve such problems. Gaussian bounding box (GBB) avoids these OBB representation problems, but directly regressing GBB is susceptible to numerical instability. In this paper, we propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB does not have the boundary discontinuity and representation ambiguity problems, and have high numerical stability. On the other hand, current rotation-sensitive feature extraction methods based on convolutions can only extract features under a local receptive field, which is slow in aggregating rotation-sensitive features. To address this issue, we propose ring-shaped rotated convolution (RRC). By adaptively rotating feature maps to arbitrary orientations, RRC extracts rotation-sensitive features under a ring-shaped receptive field, rapidly aggregating rotation-sensitive features and contextual information. RRC can be applied to various models in a plug-and-play manner. Experimental results demonstrate that the proposed LGBB and RRC are effective and achieve state-of-the-art (SOTA) performance. By integrating LGBB and RRC into various models, the detection accuracy is effectively improved on DOTA and HRSC2016 datasets.
</details>
<details>
<summary>摘要</summary>
due to the frequent variability of object orientation, accurately predicting orientation information remains a challenge in oriented object detection. to better extract orientation-related information, current methods primarily focus on the design of reasonable representations of oriented bounding box (obb) and rotation-sensitive feature extraction. however, existing obb representations often suffer from boundary discontinuity and representation ambiguity problems. methods of designing continuous and unambiguous regression losses do not essentially solve such problems. gaussian bounding box (gbb) avoids these obb representation problems, but directly regressing gbb is susceptible to numerical instability. in this paper, we propose linear gbb (lgbb), a novel obb representation. by linearly transforming the elements of gbb, lgbb does not have the boundary discontinuity and representation ambiguity problems, and has high numerical stability. on the other hand, current rotation-sensitive feature extraction methods based on convolutions can only extract features under a local receptive field, which is slow in aggregating rotation-sensitive features. to address this issue, we propose ring-shaped rotated convolution (rrc). by adaptively rotating feature maps to arbitrary orientations, rrc extracts rotation-sensitive features under a ring-shaped receptive field, rapidly aggregating rotation-sensitive features and contextual information. rrc can be applied to various models in a plug-and-play manner. experimental results demonstrate that the proposed lgbb and rrc are effective and achieve state-of-the-art (sota) performance. by integrating lgbb and rrc into various models, the detection accuracy is effectively improved on dota and hrsc2016 datasets.
</details></li>
</ul>
<hr>
<h2 id="SIRE-scale-invariant-rotation-equivariant-estimation-of-artery-orientations-using-graph-neural-networks"><a href="#SIRE-scale-invariant-rotation-equivariant-estimation-of-artery-orientations-using-graph-neural-networks" class="headerlink" title="SIRE: scale-invariant, rotation-equivariant estimation of artery orientations using graph neural networks"></a>SIRE: scale-invariant, rotation-equivariant estimation of artery orientations using graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05400">http://arxiv.org/abs/2311.05400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dieuwertje Alblas, Julian Suk, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink</li>
<li>for: 用于描述3D医疗图像中血管的几何结构，以便进行中心线提取和后续分割和可见化。</li>
<li>methods: 使用3D卷积神经网络（CNN）来确定血管的精确方向，但CNN对不同血管大小和方向的变化敏感。</li>
<li>results: 提出了一种扩展性和尺度不敏感的血管方向估计方法（SIRE），可以在不同血管大小和方向下进行训练，并且可以在不同数据集上进行测试。SIRE使用了 gauge equivariant mesh CNN（GEM-CNN）和尺度不敏感的最大函数来实现缓冲不敏感和扩展性。<details>
<summary>Abstract</summary>
Blood vessel orientation as visualized in 3D medical images is an important descriptor of its geometry that can be used for centerline extraction and subsequent segmentation and visualization. Arteries appear at many scales and levels of tortuosity, and determining their exact orientation is challenging. Recent works have used 3D convolutional neural networks (CNNs) for this purpose, but CNNs are sensitive to varying vessel sizes and orientations. We present SIRE: a scale-invariant, rotation-equivariant estimator for local vessel orientation. SIRE is modular and can generalise due to symmetry preservation.   SIRE consists of a gauge equivariant mesh CNN (GEM-CNN) operating on multiple nested spherical meshes with different sizes in parallel. The features on each mesh are a projection of image intensities within the corresponding sphere. These features are intrinsic to the sphere and, in combination with the GEM-CNN, lead to SO(3)-equivariance. Approximate scale invariance is achieved by weight sharing and use of a symmetric maximum function to combine multi-scale predictions. Hence, SIRE can be trained with arbitrarily oriented vessels with varying radii to generalise to vessels with a wide range of calibres and tortuosity.   We demonstrate the efficacy of SIRE using three datasets containing vessels of varying scales: the vascular model repository (VMR), the ASOCA coronary artery set, and a set of abdominal aortic aneurysms (AAAs). We embed SIRE in a centerline tracker which accurately tracks AAAs, regardless of the data SIRE is trained with. Moreover, SIRE can be used to track coronary arteries, even when trained only with AAAs.   In conclusion, by incorporating SO(3) and scale symmetries, SIRE can determine the orientations of vessels outside of the training domain, forming a robust and data-efficient solution to geometric analysis of blood vessels in 3D medical images.
</details>
<details>
<summary>摘要</summary>
<<SYS>>医疗三维图像中血管方向的可视化是一个重要的描述器，可以用于血管中心线提取和进一步的分割和可视化。血管在多种尺度和扭曲程度出现，确定它们的具体方向是一项挑战。现有的方法使用了3D卷积神经网络（CNN）来解决这个问题，但CNN具有不同血管大小和方向的敏感性。我们介绍了一种具有扩展性和对称性的方法：SIRE。SIRE是一种具有扩展性和对称性的方法，可以在不同的血管尺度和方向下进行可视化。它包括一个 gauge 同质的 mesh CNN（GEM-CNN）在多个嵌套的球形网格上运行，这些球形网格具有不同的大小。每个球形网格上的特征是图像强度的投影，这些特征是圆柱体内部的内在特征，通过GEM-CNN和这些特征的组合，实现 SO(3) 对称性。通过权重共享和使用对称的最大函数来结合多尺度预测，因此SIRE可以在不同的血管尺度和方向下进行可视化。我们在三个 datasets 中证明了 SIRE 的有效性，这些 datasets 包括血管模型库（VMR）、ASOCA coronary artery set 和肠动脉瘤（AAAs）。我们将 SIRE  embed 在中心线跟踪器中，可以准确地跟踪 AAAs，不管训练数据是什么。此外，SIRE 还可以用于跟踪 coronary arteries，即使只有 AAAs 的训练数据。在结尾，通过包含 SO(3) 和尺度对称，SIRE 可以在不同的训练数据下确定血管的方向，形成一种可靠和数据效率的解决方案 для医疗三维图像中的血管 geometric 分析。
</details></li>
</ul>
<hr>
<h2 id="Improving-Hand-Recognition-in-Uncontrolled-and-Uncooperative-Environments-using-Multiple-Spatial-Transformers-and-Loss-Functions"><a href="#Improving-Hand-Recognition-in-Uncontrolled-and-Uncooperative-Environments-using-Multiple-Spatial-Transformers-and-Loss-Functions" class="headerlink" title="Improving Hand Recognition in Uncontrolled and Uncooperative Environments using Multiple Spatial Transformers and Loss Functions"></a>Improving Hand Recognition in Uncontrolled and Uncooperative Environments using Multiple Spatial Transformers and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05383">http://arxiv.org/abs/2311.05383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Michal Matkowski, Xiaojie Li, Adams Wai Kin Kong</li>
<li>for: 增强手指识别率在不受控制和不合作环境中，例如犯罪现场图像中，手指可能会遮住或隐藏面部，但手指部分可能会在某些情况下可见。</li>
<li>methods: 提出了一种基于多空间变换网络（MSTN）和多loss函数的算法，以全面利用手指图像中的信息，提高识别率。MSTN首先用于本地化手指和指针，并估算对应的配置参数。然后，已经对齐的图像进一步输入到预训练的卷积神经网络中，提取特征。最后，使用多loss函数进行结构化训练。</li>
<li>results: 实验结果表明，提出的算法在不受控制和不合作环境中的手指识别 task 中表现出色，与现有方法相比显著提高了识别率，并且在不同领域的样本上具有良好的泛化能力。<details>
<summary>Abstract</summary>
The prevalence of smartphone and consumer camera has led to more evidence in the form of digital images, which are mostly taken in uncontrolled and uncooperative environments. In these images, criminals likely hide or cover their faces while their hands are observable in some cases, creating a challenging use case for forensic investigation. Many existing hand-based recognition methods perform well for hand images collected in controlled environments with user cooperation. However, their performance deteriorates significantly in uncontrolled and uncooperative environments. A recent work has exposed the potential of hand recognition in these environments. However, only the palmar regions were considered, and the recognition performance is still far from satisfactory. To improve the recognition accuracy, an algorithm integrating a multi-spatial transformer network (MSTN) and multiple loss functions is proposed to fully utilize information in full hand images. MSTN is firstly employed to localize the palms and fingers and estimate the alignment parameters. Then, the aligned images are further fed into pretrained convolutional neural networks, where features are extracted. Finally, a training scheme with multiple loss functions is used to train the network end-to-end. To demonstrate the effectiveness of the proposed algorithm, the trained model is evaluated on NTU-PI-v1 database and six benchmark databases from different domains. Experimental results show that the proposed algorithm performs significantly better than the existing methods in these uncontrolled and uncooperative environments and has good generalization capabilities to samples from different domains.
</details>
<details>
<summary>摘要</summary>
因为智能手机和消费类摄像头的普及，现在有更多的证据形式为数字图像，大多数是在无控制和不合作环境中拍摄的。在这些图像中，犯罪者可能会遮住或覆盖面孔，手部则可能在某些情况下可见，这成为了困难的证据检查use case。许多现有的手部识别方法在控制环境下与用户合作情况下表现良好，但在无控制和不合作环境下表现下降显著。一项最近的研究曝光了手部识别在这些环境下的潜在性。然而，只考虑了手部的平板区域，并且表现还不够满意。为了提高识别精度，我们提议一种综合使用多个空间变换网络（MSTN）和多个损失函数的算法，以全面利用全手部图像中的信息。首先，MSTN被使用来LOCALize手部和手指，并估计对应的配置参数。然后，已经aligned的图像进一步被 fed into预训练的卷积神经网络，提取特征。最后，通过多个损失函数的训练方案，训练整个网络。为了证明提议的算法的效果，我们在NTU-PI-v1数据库和六个不同领域的标准数据库上训练和测试了模型。实验结果表明，提议的算法在无控制和不合作环境下表现出色，与不同领域的样本具有良好的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="u-LLaVA-Unifying-Multi-Modal-Tasks-via-Large-Language-Model"><a href="#u-LLaVA-Unifying-Multi-Modal-Tasks-via-Large-Language-Model" class="headerlink" title="u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model"></a>u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05348">http://arxiv.org/abs/2311.05348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Yanchun Xie, Yi-Jie Huang, Yaqian Li</li>
<li>For: 这 paper 的目的是提出一种有效和准确的方法，使得 LLM 可以快速适应下游任务，并且可以解决hallucination和多任务干扰的问题。* Methods: 这 paper 使用了 LLaVA 和 Mini-GPT4 等先进技术，将视觉信息集成到 LLM 中，并提出了一种 bridge 模型，即 u-LLaVA，以连接多个专家模型。* Results: 该 paper 的实验结果表明，u-LLaVA 可以达到领先的性能水平，并且可以解决hallucination和多任务干扰的问题。同时，paper 还发布了模型、生成数据和代码库，以便其他研究者进行进一步的探索和应用。<details>
<summary>Abstract</summary>
Recent advances such as LLaVA and Mini-GPT4 have successfully integrated visual information into LLMs, yielding inspiring outcomes and giving rise to a new generation of multi-modal LLMs, or MLLMs. Nevertheless, these methods struggle with hallucinations and the mutual interference between tasks. To tackle these problems, we propose an efficient and accurate approach to adapt to downstream tasks by utilizing LLM as a bridge to connect multiple expert models, namely u-LLaVA. Firstly, we incorporate the modality alignment module and multi-task modules into LLM. Then, we reorganize or rebuild multi-type public datasets to enable efficient modality alignment and instruction following. Finally, task-specific information is extracted from the trained LLM and provided to different modules for solving downstream tasks. The overall framework is simple, effective, and achieves state-of-the-art performance across multiple benchmarks. We also release our model, the generated data, and the code base publicly available.
</details>
<details>
<summary>摘要</summary>
Recent advances such as LLaVA and Mini-GPT4 have successfully integrated visual information into LLMs, yielding inspiring outcomes and giving rise to a new generation of multi-modal LLMs, or MLLMs. Nevertheless, these methods struggle with hallucinations and the mutual interference between tasks. To tackle these problems, we propose an efficient and accurate approach to adapt to downstream tasks by utilizing LLM as a bridge to connect multiple expert models, namely u-LLaVA. Firstly, we incorporate the modality alignment module and multi-task modules into LLM. Then, we reorganize or rebuild multi-type public datasets to enable efficient modality alignment and instruction following. Finally, task-specific information is extracted from the trained LLM and provided to different modules for solving downstream tasks. The overall framework is simple, effective, and achieves state-of-the-art performance across multiple benchmarks. We also release our model, the generated data, and the code base publicly available.Translation in Simplified Chinese:最近的进展，如LLaVA和Mini-GPT4，已经成功地将视觉信息integrated into LLMs，带来了惊人的结果，并给出了一新的多modal LLMs的generation，简称为MLLMs。然而，这些方法受到了幻觉和任务之间的互相干扰的问题。为了解决这些问题，我们提出了一个高效和准确的方法，即通过使用LLM作为多个专家模型之间的桥梁，以便适应下游任务。首先，我们将模式对齐模组和多任务模组组合入LMM。然后，我们将多种公共dataset重新组织或重新建立，以便实现有效的模式对齐和指令跟随。最后，我们从训练LMM中提取了任务特定的信息，并将其提供给不同的模组以解决下游任务。整个框架是简单、有效，并在多个 bencmarks 上实现了state-of-the-art的性能。我们还公开了我们的模型、生成的数据和代码库。
</details></li>
</ul>
<hr>
<h2 id="SynFacePAD-2023-Competition-on-Face-Presentation-Attack-Detection-Based-on-Privacy-aware-Synthetic-Training-Data"><a href="#SynFacePAD-2023-Competition-on-Face-Presentation-Attack-Detection-Based-on-Privacy-aware-Synthetic-Training-Data" class="headerlink" title="SynFacePAD 2023: Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data"></a>SynFacePAD 2023: Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05336">http://arxiv.org/abs/2311.05336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zi-yuanyang/ijcb-synfacepad-dig">https://github.com/zi-yuanyang/ijcb-synfacepad-dig</a></li>
<li>paper_authors: Meiling Fang, Marco Huber, Julian Fierrez, Raghavendra Ramachandra, Naser Damer, Alhasan Alkhaddour, Maksim Kasantcev, Vasiliy Pryadchenko, Ziyuan Yang, Huijie Huangfu, Yingyu Chen, Yi Zhang, Yuchen Pan, Junjun Jiang, Xianming Liu, Xianyun Sun, Caiyong Wang, Xingyu Liu, Zhaohua Chang, Guangzhe Zhao, Juan Tapia, Lazaro Gonzalez-Soler, Carlos Aravena, Daniel Schulz</li>
<li>for: 本文报告2023年国际人体生物特征识别联合会议（IJCB 2023）上的一项竞赛：基于隐私意识的人脸发布攻击检测竞赛（SynFacePAD 2023）。</li>
<li>methods: 竞赛使用限制使用组织提供的合理数据进行训练，以便满足隐私、法律和伦理问题相关的个人数据的需求。</li>
<li>results: 参与者提交的解决方案具有创新和新的方法，能够超越考虑基线的benchmark。<details>
<summary>Abstract</summary>
This paper presents a summary of the Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data (SynFacePAD 2023) held at the 2023 International Joint Conference on Biometrics (IJCB 2023). The competition attracted a total of 8 participating teams with valid submissions from academia and industry. The competition aimed to motivate and attract solutions that target detecting face presentation attacks while considering synthetic-based training data motivated by privacy, legal and ethical concerns associated with personal data. To achieve that, the training data used by the participants was limited to synthetic data provided by the organizers. The submitted solutions presented innovations and novel approaches that led to outperforming the considered baseline in the investigated benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spatial-Attention-based-Distribution-Integration-Network-for-Human-Pose-Estimation"><a href="#Spatial-Attention-based-Distribution-Integration-Network-for-Human-Pose-Estimation" class="headerlink" title="Spatial Attention-based Distribution Integration Network for Human Pose Estimation"></a>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05323">http://arxiv.org/abs/2311.05323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihan Gao, Jing Zhu, Xiaoxuan Zhuang, Zhaoyue Wang, Qijin Li</li>
<li>for: 提高人姿估计中的精度，提高抗覆盖、多样化、不同照明和重叠的能力。</li>
<li>methods: 使用 spatial attention-based distribution integration network (SADI-NET)，包括三个高效模型： recepetive fortified module (RFM)、spatial fusion module (SFM) 和 distribution learning module (DLM)，基于 classic HourglassNet 架构，并将基本块替换为我们提议的 RFM。</li>
<li>results: 在 MPII 和 LSP 测试集上进行了广泛的实验，并 obtainted remarkable $92.10%$ percent accuracy on MPII 测试集，表明了我们的模型在抗覆盖、多样化、不同照明和重叠的情况下具有显著改进的性能，并为人姿估计领域做出了州际性的贡献。<details>
<summary>Abstract</summary>
In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
近年来，人体 pose 的估计得到了深度学习技术的推动，但这些技术仍然在面临困难场景时存在限制，包括 occlusion、多样化的外观、照明变化和重叠。为了解决这些缺点，我们提出了空间注意力基于分布集成网络（SADI-NET），以提高localization的准确性。我们的网络包括三个高效模型：受激增强模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。基于经典的ourglassnet 架构，我们将基本块替换为我们提议的RFM。RFM包括了扩展receptive field的倍速块和注意力机制，以提高对空间信息的敏感性。此外，SFM使用了多级特征，通过使用全局和局部注意力机制来捕捉多种特征。此外，DLM，取得了适应性的 residual log-likelihood estimation（RLE），使用一个可学习的分布权重来修改预测的热图。为了评估我们的模型的效果，我们对MPII和LSP标准benchmark进行了广泛的实验。特别是，我们的模型在MPII测试集上达到了92.10%的准确率，与现有模型相比显著提高，并确立了领先的性能。
</details></li>
</ul>
<hr>
<h2 id="SPADES-A-Realistic-Spacecraft-Pose-Estimation-Dataset-using-Event-Sensing"><a href="#SPADES-A-Realistic-Spacecraft-Pose-Estimation-Dataset-using-Event-Sensing" class="headerlink" title="SPADES: A Realistic Spacecraft Pose Estimation Dataset using Event Sensing"></a>SPADES: A Realistic Spacecraft Pose Estimation Dataset using Event Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05310">http://arxiv.org/abs/2311.05310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arunkumar Rathinam, Haytam Qadadri, Djamila Aouada</li>
<li>for: 本研究旨在提高遥感空间器的自主性，使其能够更好地执行靠近、停机和 proximity 操作。</li>
<li>methods: 本研究使用 Deep Learning-based Spacecraft Pose Estimation 技术，并使用 Domain Adaptation 技术来减少域隔问题。</li>
<li>results: 本研究提出了一个新的数据集 SPADES，并提出了一种有效的数据筛选方法和一种新的图像基Event表示方法，以提高模型性能。<details>
<summary>Abstract</summary>
In recent years, there has been a growing demand for improved autonomy for in-orbit operations such as rendezvous, docking, and proximity maneuvers, leading to increased interest in employing Deep Learning-based Spacecraft Pose Estimation techniques. However, due to limited access to real target datasets, algorithms are often trained using synthetic data and applied in the real domain, resulting in a performance drop due to the domain gap. State-of-the-art approaches employ Domain Adaptation techniques to mitigate this issue. In the search for viable solutions, event sensing has been explored in the past and shown to reduce the domain gap between simulations and real-world scenarios. Event sensors have made significant advancements in hardware and software in recent years. Moreover, the characteristics of the event sensor offer several advantages in space applications compared to RGB sensors. To facilitate further training and evaluation of DL-based models, we introduce a novel dataset, SPADES, comprising real event data acquired in a controlled laboratory environment and simulated event data using the same camera intrinsics. Furthermore, we propose an effective data filtering method to improve the quality of training data, thus enhancing model performance. Additionally, we introduce an image-based event representation that outperforms existing representations. A multifaceted baseline evaluation was conducted using different event representations, event filtering strategies, and algorithmic frameworks, and the results are summarized. The dataset will be made available at http://cvi2.uni.lu/spades.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，卫星运动中的自主化需求增加，如 rendezvous、射合和靠近操作，导致 Deep Learning 技术的Spacecraft Pose Estimation 的应用更加普遍。然而，由于实际目标数据的有限，算法通常在 simulate 数据上训练，并在实际领域应用，因此会出现领域差距问题。现状的方法是使用领域适应技术来缓解这个问题。在寻找可行的解决方案时，事件感知被探索过，并显示可以降低实际和模拟领域之间的领域差距。事件传感器在过去几年内进行了重要的硬件和软件技术的进步，而且事件传感器在空间应用中具有许多优势，比如 RGB 传感器。为了进一步训练和评估基于 DL 的模型，我们介绍了一个新的数据集，即 SPADES，该数据集包括实际的事件数据，在控制实验室环境中获得，以及模拟的事件数据，使用同一个摄像头笛谱。此外，我们还提出了一种有效的数据筛选方法，以提高训练数据的质量，从而提高模型性能。此外，我们还引入了一种基于图像的事件表示方法，超越了现有的表示方法。我们进行了多种事件表示方法、事件筛选策略和算法框架的多元基准评估，结果如下。该数据集将在 http://cvi2.uni.lu/spades 上公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Vision-and-Language-Reasoning-via-Spatial-Relations-Modeling"><a href="#Improving-Vision-and-Language-Reasoning-via-Spatial-Relations-Modeling" class="headerlink" title="Improving Vision-and-Language Reasoning via Spatial Relations Modeling"></a>Improving Vision-and-Language Reasoning via Spatial Relations Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05298">http://arxiv.org/abs/2311.05298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yang, Rui Xu, Ye Guo, Peixiang Huang, Yiru Chen, Wenkui Ding, Zhongyuan Wang, Hong Zhou</li>
<li>for: 提高视觉共类理解 task 的性能，增强视觉表示中的空间 контекст。</li>
<li>methods: 基于给定视觉场景，构建空间关系图，并设计两个预训练任务：物体位置重构（OPR）和空间关系分类（SRC），以学习重构图和关注视觉中重要区域。</li>
<li>results: 在 VCR 和两个其他视觉语言理解任务 VQA 和 NLVR 中达到了状态机器人的Results。<details>
<summary>Abstract</summary>
Visual commonsense reasoning (VCR) is a challenging multi-modal task, which requires high-level cognition and commonsense reasoning ability about the real world. In recent years, large-scale pre-training approaches have been developed and promoted the state-of-the-art performance of VCR. However, the existing approaches almost employ the BERT-like objectives to learn multi-modal representations. These objectives motivated from the text-domain are insufficient for the excavation on the complex scenario of visual modality. Most importantly, the spatial distribution of the visual objects is basically neglected. To address the above issue, we propose to construct the spatial relation graph based on the given visual scenario. Further, we design two pre-training tasks named object position regression (OPR) and spatial relation classification (SRC) to learn to reconstruct the spatial relation graph respectively. Quantitative analysis suggests that the proposed method can guide the representations to maintain more spatial context and facilitate the attention on the essential visual regions for reasoning. We achieve the state-of-the-art results on VCR and two other vision-and-language reasoning tasks VQA, and NLVR.
</details>
<details>
<summary>摘要</summary>
“视觉常识逻辑”（VCR）是一项复杂的多Modal任务，需要高水平的认知和常识逻辑能力来理解实际世界。最近几年，大规模预训练方法得到了广泛的应用和推广，并提高了VCR的状态 искусственный интеллект。然而，现有的方法大多采用BERT类目标来学习多Modal表示。这些目标启发自文本领域，对于视觉场景的复杂情况不够。尤其是 spatial distribution of visual objects 基本被忽略。为了解决这一问题，我们提议构建基于给定的视觉场景的 spatial relation graph。然后，我们设计了两个预训练任务名为object position regression（OPR）和spatial relation classification（SRC），以学习重建 spatial relation graph。量化分析表明，我们的方法可以导引表示保持更多的空间 контекст，促进关注视觉中的重要区域进行逻辑。我们实现了VCR以及两个其他的视觉语言逻辑任务VQA和NLVR的状态 искусственный интеллект。
</details></li>
</ul>
<hr>
<h2 id="VoxNeRF-Bridging-Voxel-Representation-and-Neural-Radiance-Fields-for-Enhanced-Indoor-View-Synthesis"><a href="#VoxNeRF-Bridging-Voxel-Representation-and-Neural-Radiance-Fields-for-Enhanced-Indoor-View-Synthesis" class="headerlink" title="VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for Enhanced Indoor View Synthesis"></a>VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for Enhanced Indoor View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05289">http://arxiv.org/abs/2311.05289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Wang, Wei Zhang, Stefano Gasperini, Shun-Cheng Wu, Nassir Navab</li>
<li>for: 提高各种 immerse 应用中的视图合成质量，特别是indoor环境下的实时投影。</li>
<li>methods: 使用 voxel-based 表示方法，并采用多分辨率 hash 网格来适应 occlusion 和复杂的 indoor 景象。提出了 voxel-guided 高效采样技术，以优化计算资源的使用。</li>
<li>results: 比较了三个公共的indoor数据集，显示 VoxNeRF 方法在视图合成中具有较高的质量和效率，同时减少了训练和渲染时间，甚至超过了 Instant-NGP 的速度， bringing the technology closer to real-time。<details>
<summary>Abstract</summary>
Creating high-quality view synthesis is essential for immersive applications but continues to be problematic, particularly in indoor environments and for real-time deployment. Current techniques frequently require extensive computational time for both training and rendering, and often produce less-than-ideal 3D representations due to inadequate geometric structuring. To overcome this, we introduce VoxNeRF, a novel approach that leverages volumetric representations to enhance the quality and efficiency of indoor view synthesis. Firstly, VoxNeRF constructs a structured scene geometry and converts it into a voxel-based representation. We employ multi-resolution hash grids to adaptively capture spatial features, effectively managing occlusions and the intricate geometry of indoor scenes. Secondly, we propose a unique voxel-guided efficient sampling technique. This innovation selectively focuses computational resources on the most relevant portions of ray segments, substantially reducing optimization time. We validate our approach against three public indoor datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods. Remarkably, it achieves these gains while reducing both training and rendering times, surpassing even Instant-NGP in speed and bringing the technology closer to real-time.
</details>
<details>
<summary>摘要</summary>
Firstly, VoxNeRF constructs a structured scene geometry and converts it into a voxel-based representation. We use multi-resolution hash grids to adaptively capture spatial features, effectively managing occlusions and the intricate geometry of indoor scenes.Secondly, we propose a unique voxel-guided efficient sampling technique. This innovation selectively focuses computational resources on the most relevant portions of ray segments, substantially reducing optimization time.We validate our approach against three public indoor datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods. Remarkably, it achieves these gains while reducing both training and rendering times, surpassing even Instant-NGP in speed and bringing the technology closer to real-time.
</details></li>
</ul>
<hr>
<h2 id="SAMVG-A-Multi-stage-Image-Vectorization-Model-with-the-Segment-Anything-Model"><a href="#SAMVG-A-Multi-stage-Image-Vectorization-Model-with-the-Segment-Anything-Model" class="headerlink" title="SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model"></a>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05276">http://arxiv.org/abs/2311.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Zhu, Juang Ian Chong, Teng Hu, Ran Yi, Yu-Kun Lai, Paul L. Rosin</li>
<li>for: 该论文是为了提出一种高质量Vector Graphics自动化生成方法（SAMVG），用于从 bitmap 图像转换为Scalable Vector Graphics（SVG）。</li>
<li>methods: 该方法首先使用Segment-Anything模型进行通用图像 segmentation，然后使用一种新的筛选方法来选择整个图像最佳的密集分割图。其次，SAMVG会识别缺失的组件并添加更多的细节组件到 SVG 中。</li>
<li>results: 经过了一系列广泛的实验，我们示出了SAMVG可以在任何领域生成高质量 SVG，同时需要更少的计算时间和复杂度，相比之前的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Vector graphics are widely used in graphical designs and have received more and more attention. However, unlike raster images which can be easily obtained, acquiring high-quality vector graphics, typically through automatically converting from raster images remains a significant challenge, especially for more complex images such as photos or artworks. In this paper, we propose SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector Graphics). Firstly, SAMVG uses general image segmentation provided by the Segment-Anything Model and uses a novel filtering method to identify the best dense segmentation map for the entire image. Secondly, SAMVG then identifies missing components and adds more detailed components to the SVG. Through a series of extensive experiments, we demonstrate that SAMVG can produce high quality SVGs in any domain while requiring less computation time and complexity compared to previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
vector graphics 广泛应用于图形设计中，而且在最近得到了更多的关注。然而，与矩阵图像不同，获得高质量vector graphics仍然是一项重要的挑战，尤其是 для更复杂的图像，如照片或艺术作品。在这篇论文中，我们提出了SAMVG模型，用于自动将矩阵图像转换成SVG（可缩放vector graphics）。首先，SAMVG使用Segment-Anything模型提供的通用图像分割，并使用一种新的滤波方法来选择整个图像的最佳粗糙分割图。其次，SAMVG确定缺失的组件，并将更加细节的组件添加到SVG中。经过了一系列的广泛实验，我们证明了SAMVG可以生成高质量的SVG，无论是在任何领域都能够生成，而且需要更少的计算时间和复杂度，相比之前的状态对比方法。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-Tomography-of-Discrete-Dynamic-Objects"><a href="#Single-shot-Tomography-of-Discrete-Dynamic-Objects" class="headerlink" title="Single-shot Tomography of Discrete Dynamic Objects"></a>Single-shot Tomography of Discrete Dynamic Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05269">http://arxiv.org/abs/2311.05269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajinkya Kadu, Felix Lucka, Kees Joost Batenburg</li>
<li>for: 高分辨率动态图像重建</li>
<li>methods: 利用空间和时间信息 synergistically incorporates 应用等级集方法进行图像分割，以及使用声律基函数表示运动。</li>
<li>results: 提供了一种计算效率高、可以方便地优化的变形框架，可以重建高质量的2D或3D图像序列，只需单个投影每帧。与现有方法相比，提出的方法在Synthetic和 pseudo-动态真X射tomography数据集上表现出优于现有方法。<details>
<summary>Abstract</summary>
This paper presents a novel method for the reconstruction of high-resolution temporal images in dynamic tomographic imaging, particularly for discrete objects with smooth boundaries that vary over time. Addressing the challenge of limited measurements per time point, we propose a technique that synergistically incorporates spatial and temporal information of the dynamic objects. This is achieved through the application of the level-set method for image segmentation and the representation of motion via a sinusoidal basis. The result is a computationally efficient and easily optimizable variational framework that enables the reconstruction of high-quality 2D or 3D image sequences with a single projection per frame. Compared to current methods, our proposed approach demonstrates superior performance on both synthetic and pseudo-dynamic real X-ray tomography datasets. The implications of this research extend to improved visualization and analysis of dynamic processes in tomographic imaging, finding potential applications in diverse scientific and industrial domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Widely-Applicable-Strong-Baseline-for-Sports-Ball-Detection-and-Tracking"><a href="#Widely-Applicable-Strong-Baseline-for-Sports-Ball-Detection-and-Tracking" class="headerlink" title="Widely Applicable Strong Baseline for Sports Ball Detection and Tracking"></a>Widely Applicable Strong Baseline for Sports Ball Detection and Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05237">http://arxiv.org/abs/2311.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Tarashima, Muhammad Abdul Haq, Yushan Wang, Norio Tagawa</li>
<li>for: 本研究做出了一种新的运动球检测和跟踪（SBDT）方法，可以应用于不同的运动类别。</li>
<li>methods: 该方法包括高分辨率特征提取、位置意识模型训练和时间一致性推理等，这些方法组成了一个新的SBDT基线。</li>
<li>results: 我们在5个不同的运动类别的5个数据集上对6种现有SBDT方法进行了比较，并新提供了2个SBDT数据集和新的球标注。实验结果表明，我们的方法在所有运动类别中具有显著的优势。我们认为我们的提出的方法可以成为广泛适用的强大基线（WASB），而我们的数据集和代码库将推动未来SBDT研究。数据集和代码将公开发布。<details>
<summary>Abstract</summary>
In this work, we present a novel Sports Ball Detection and Tracking (SBDT) method that can be applied to various sports categories. Our approach is composed of (1) high-resolution feature extraction, (2) position-aware model training, and (3) inference considering temporal consistency, all of which are put together as a new SBDT baseline. Besides, to validate the wide-applicability of our approach, we compare our baseline with 6 state-of-the-art SBDT methods on 5 datasets from different sports categories. We achieve this by newly introducing two SBDT datasets, providing new ball annotations for two datasets, and re-implementing all the methods to ease extensive comparison. Experimental results demonstrate that our approach is substantially superior to existing methods on all the sports categories covered by the datasets. We believe our proposed method can play as a Widely Applicable Strong Baseline (WASB) of SBDT, and our datasets and codebase will promote future SBDT research. Datasets and codes will be made publicly available.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的体育球检测和跟踪（SBDT）方法，可以应用于多种体育类别。我们的方法包括（1）高分辨率特征提取、（2）位置意识模型训练和（3）根据时间一致性进行推理，这些都被整合成为新的SBDT基线。此外，为了证明我们的方法可以广泛应用，我们与6种现有SBDT方法进行比较，并新增了2个SBDT数据集，为2个数据集提供了新的球标注，并重新实现了所有方法，以便进行广泛的比较。实验结果显示，我们的方法在所有涉及的体育类别中具有显著的优势。我们认为我们的提出的方法可以担任广泛适用的强大基线（WASB），我们的数据集和代码库将推动未来SBDT研究。数据集和代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="ConRad-Image-Constrained-Radiance-Fields-for-3D-Generation-from-a-Single-Image"><a href="#ConRad-Image-Constrained-Radiance-Fields-for-3D-Generation-from-a-Single-Image" class="headerlink" title="ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image"></a>ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05230">http://arxiv.org/abs/2311.05230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Senthil Purushwalkam, Nikhil Naik</li>
<li>for: 从单个RGB图像中重建3D对象</li>
<li>methods: 利用最新的图像生成模型来推断隐藏的3D结构，同时保持输入图像的准确性</li>
<li>results: 提出了一种简洁的3D表示方法，能够简化图像细节的保持，并生成更加真实的3D重建结果，比对现有基eline基eline的表现更出色<details>
<summary>Abstract</summary>
We present a novel method for reconstructing 3D objects from a single RGB image. Our method leverages the latest image generation models to infer the hidden 3D structure while remaining faithful to the input image. While existing methods obtain impressive results in generating 3D models from text prompts, they do not provide an easy approach for conditioning on input RGB data. Na\"ive extensions of these methods often lead to improper alignment in appearance between the input image and the 3D reconstructions. We address these challenges by introducing Image Constrained Radiance Fields (ConRad), a novel variant of neural radiance fields. ConRad is an efficient 3D representation that explicitly captures the appearance of an input image in one viewpoint. We propose a training algorithm that leverages the single RGB image in conjunction with pretrained Diffusion Models to optimize the parameters of a ConRad representation. Extensive experiments show that ConRad representations can simplify preservation of image details while producing a realistic 3D reconstruction. Compared to existing state-of-the-art baselines, we show that our 3D reconstructions remain more faithful to the input and produce more consistent 3D models while demonstrating significantly improved quantitative performance on a ShapeNet object benchmark.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从单个RGB图像中重construct3D物体。我们的方法利用最新的图像生成模型来推断隐藏的3D结构，同时保持对输入图像的忠实。现有的方法可以在生成3D模型时获得出色的结果，但是它们不提供一个简单的方法来将输入RGB数据作为条件。不熟悉的扩展方法可能会导致输入图像和3D重建的外观不一致。我们解决这些挑战 by introducing Image Constrained Radiance Fields (ConRad), a novel variant of neural radiance fields. ConRad是一种高效的3D表示，Explicitly captures the appearance of an input image in one viewpoint.我们提出了一种使用单个RGB图像和预训练的扩散模型来优化ConRad表示的训练算法。广泛的实验表明，ConRad表示可以简化保持图像细节的同时生成实际的3D重建。相比已有的状态艺术基准，我们的3D重建更加忠实于输入，产生了更一致的3D模型，并且在ShapeNet对象benchmark中显示出了 statistically significant improved quantitative performance.
</details></li>
</ul>
<hr>
<h2 id="Let’s-Get-the-FACS-Straight-–-Reconstructing-Obstructed-Facial-Features"><a href="#Let’s-Get-the-FACS-Straight-–-Reconstructing-Obstructed-Facial-Features" class="headerlink" title="Let’s Get the FACS Straight – Reconstructing Obstructed Facial Features"></a>Let’s Get the FACS Straight – Reconstructing Obstructed Facial Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05221">http://arxiv.org/abs/2311.05221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Büchner, Sven Sickert, Gerd Fabian Volk, Christoph Anders, Orlando Guntinas-Lichius, Joachim Denzler</li>
<li>for: 该论文旨在提高机器学习方法对部分遮盖或干扰的人脸表达的理解能力。</li>
<li>methods: 该论文提出了一种基于Style Transfer的人脸重建方法，使用CycleGAN架构，不需要匹配对。</li>
<li>results: 论文通过比较实验和评估表达识别任务的结果，证明了该方法可以准确重建部分遮盖或干扰的人脸表达。<details>
<summary>Abstract</summary>
The human face is one of the most crucial parts in interhuman communication. Even when parts of the face are hidden or obstructed the underlying facial movements can be understood. Machine learning approaches often fail in that regard due to the complexity of the facial structures. To alleviate this problem a common approach is to fine-tune a model for such a specific application. However, this is computational intensive and might have to be repeated for each desired analysis task. In this paper, we propose to reconstruct obstructed facial parts to avoid the task of repeated fine-tuning. As a result, existing facial analysis methods can be used without further changes with respect to the data. In our approach, the restoration of facial features is interpreted as a style transfer task between different recording setups. By using the CycleGAN architecture the requirement of matched pairs, which is often hard to fullfill, can be eliminated. To proof the viability of our approach, we compare our reconstructions with real unobstructed recordings. We created a novel data set in which 36 test subjects were recorded both with and without 62 surface electromyography sensors attached to their faces. In our evaluation, we feature typical facial analysis tasks, like the computation of Facial Action Units and the detection of emotions. To further assess the quality of the restoration, we also compare perceptional distances. We can show, that scores similar to the videos without obstructing sensors can be achieved.
</details>
<details>
<summary>摘要</summary>
人类面部是交流 между人类的一个关键部分。即使部分面部遮盖或干扰，也可以理解下面部的运动。机器学习方法经常在这种情况下失败，因为人类面部结构的复杂性。为解决这个问题，通常需要为每个分析任务进行特定的 fine-tuning。然而，这是计算昂贵的，并且可能需要重复进行多次。在这篇论文中，我们提议重建遮盖的面部部分，以避免 repeatedly fine-tuning。通过这种方式，现有的面部分析方法可以无需更改数据进行使用。在我们的方法中，重建面部特征被解释为Style transfer任务 между不同的记录设置。使用CycleGAN架构，可以消除匹配对的要求，这经常是困难的。为证明我们的方法的可行性，我们对36名测试者的记录进行了比较。我们创建了一个新的数据集，其中每名测试者都被记录了不同的62个面部电磁学感器。在我们的评估中，我们包括常见的面部分析任务，如计算Facial Action Units和感情检测。为进一步评估重建的质量，我们还比较了感觉距离。我们可以显示，得到与无遮盖感器记录相似的分数。
</details></li>
</ul>
<hr>
<h2 id="BrainNetDiff-Generative-AI-Empowers-Brain-Network-Generation-via-Multimodal-Diffusion-Model"><a href="#BrainNetDiff-Generative-AI-Empowers-Brain-Network-Generation-via-Multimodal-Diffusion-Model" class="headerlink" title="BrainNetDiff: Generative AI Empowers Brain Network Generation via Multimodal Diffusion Model"></a>BrainNetDiff: Generative AI Empowers Brain Network Generation via Multimodal Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05199">http://arxiv.org/abs/2311.05199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcheng Zong, Shuqiang Wang<br>for: 本研究旨在提供一种有效的脑网络分析方法，以提高脑功能和疾病机理的理解。methods: 本方法结合多头Transformer编码器对fMRI时间序列进行特征提取，并使用条件潜在扩散模型进行脑网络生成。results: 实验结果表明，该方法在健康和神经科学患者群体中建立脑网络时具有高度的准确性和稳定性，并在下游疾病分类任务中显示出杰出的效果。<details>
<summary>Abstract</summary>
Brain network analysis has emerged as pivotal method for gaining a deeper understanding of brain functions and disease mechanisms. Despite the existence of various network construction approaches, shortcomings persist in the learning of correlations between structural and functional brain imaging data. In light of this, we introduce a novel method called BrainNetDiff, which combines a multi-head Transformer encoder to extract relevant features from fMRI time series and integrates a conditional latent diffusion model for brain network generation. Leveraging a conditional prompt and a fusion attention mechanism, this method significantly improves the accuracy and stability of brain network generation. To the best of our knowledge, this represents the first framework that employs diffusion for the fusion of the multimodal brain imaging and brain network generation from images to graphs. We validate applicability of this framework in the construction of brain network across healthy and neurologically impaired cohorts using the authentic dataset. Experimental results vividly demonstrate the significant effectiveness of the proposed method across the downstream disease classification tasks. These findings convincingly emphasize the prospective value in the field of brain network research, particularly its key significance in neuroimaging analysis and disease diagnosis. This research provides a valuable reference for the processing of multimodal brain imaging data and introduces a novel, efficient solution to the field of neuroimaging.
</details>
<details>
<summary>摘要</summary>
Brain网络分析已经成为脑功能和疾病机制研究的重要方法。尽管存在多种网络建构方法，但是在学习Structural和功能脑成像数据之间的相关性方面仍存在缺陷。为了解决这个问题，我们介绍了一种新的方法called BrainNetDiff，它使用多头Transformer编码器提取fMRI时间序列中相关的特征，并将其与条件潜在扩散模型结合使用来生成脑网络。通过使用条件提示和融合注意力机制，这种方法能够显著提高脑网络生成的准确性和稳定性。我们知道，这是第一个使用扩散来融合多Modal脑成像和脑网络生成的框架。我们验证了这个框架在健康和神经科学中的应用，使用了实际数据进行验证。实验结果表明，提案的方法在下游疾病分类任务中具有显著的效果。这些发现证明了脑网络研究的前景，特别是脑成像分析和疾病诊断中的重要性。这项研究为脑成像数据处理提供了一个有价值的参考，并提供了一种有效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Labeling-for-Enhancing-Remote-Sensing-Cloud-Understanding"><a href="#Adaptive-Labeling-for-Enhancing-Remote-Sensing-Cloud-Understanding" class="headerlink" title="Adaptive-Labeling for Enhancing Remote Sensing Cloud Understanding"></a>Adaptive-Labeling for Enhancing Remote Sensing Cloud Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05198">http://arxiv.org/abs/2311.05198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaygala223/cloud-adaptive-labeling">https://github.com/jaygala223/cloud-adaptive-labeling</a></li>
<li>paper_authors: Jay Gala, Sauradip Nag, Huichou Huang, Ruirui Liu, Xiatian Zhu</li>
<li>for: 提高云Segmentation模型的性能，增强气象和气候科学中的云分析</li>
<li>methods: 引入可变核算法，在训练数据上进行逐步更新和调整，以提高模型的性能</li>
<li>results: 在多个标准云Segmentation benchmark上取得了显著的提高，与许多现有的方法相比新建立了state-of-the-art记录<details>
<summary>Abstract</summary>
Cloud analysis is a critical component of weather and climate science, impacting various sectors like disaster management. However, achieving fine-grained cloud analysis, such as cloud segmentation, in remote sensing remains challenging due to the inherent difficulties in obtaining accurate labels, leading to significant labeling errors in training data. Existing methods often assume the availability of reliable segmentation annotations, limiting their overall performance. To address this inherent limitation, we introduce an innovative model-agnostic Cloud Adaptive-Labeling (CAL) approach, which operates iteratively to enhance the quality of training data annotations and consequently improve the performance of the learned model. Our methodology commences by training a cloud segmentation model using the original annotations. Subsequently, it introduces a trainable pixel intensity threshold for adaptively labeling the cloud training images on the fly. The newly generated labels are then employed to fine-tune the model. Extensive experiments conducted on multiple standard cloud segmentation benchmarks demonstrate the effectiveness of our approach in significantly boosting the performance of existing segmentation models. Our CAL method establishes new state-of-the-art results when compared to a wide array of existing alternatives.
</details>
<details>
<summary>摘要</summary>
云分析是气象和气候科学中的关键组件，对各种领域如灾害管理有重要影响。然而，在远程感知中实现细化云分析，如云分割，仍然是一项挑战，因为获得准确的标签困难，导致训练数据中的标签错误很大。现有方法通常假设可以获得可靠的分割标签，这限制了它们的总性表现。为解决这种内在的限制，我们介绍了一种创新的模型无关的云适应标签（CAL）方法。我们的方法流程开始于使用原始标签训练云分割模型。然后，它引入可调Pixel INTENSITY阈值，以逐行动 adaptively标注云训练图像。新生成的标签然后用于细化模型。我们在多个标准云分割benchmark上进行了广泛的实验，证明了我们的方法在提高现有分割模型性能方面的效果。我们的CAL方法在与许多现有的alternative比较时成功创造了新的状态符标准结果。
</details></li>
</ul>
<hr>
<h2 id="TransReg-Cross-transformer-as-auto-registration-module-for-multi-view-mammogram-mass-detection"><a href="#TransReg-Cross-transformer-as-auto-registration-module-for-multi-view-mammogram-mass-detection" class="headerlink" title="TransReg: Cross-transformer as auto-registration module for multi-view mammogram mass detection"></a>TransReg: Cross-transformer as auto-registration module for multi-view mammogram mass detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05192">http://arxiv.org/abs/2311.05192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang C. Nguyen, Chi Phan, Hieu H. Pham</li>
<li>for: 这个研究旨在提高胸部癌早期检测的精度，通过利用多视图照片的资讯来增强医生的自信心和减少错误的发现率。</li>
<li>methods: 这个研究使用了一个名为TransReg的电脑助成检测系统（CAD），它利用两个不同的照片（cc和mlo）之间的关系来增强识别胸部癌的精度。</li>
<li>results: 根据实验结果，TransReg使用SwinT作为特征提取器时，在false positive rate per image at 0.5时取得了州际级的性能，具体来说是DDSMdataset上的召回率为83.3%，VinDr-Mammodataset上的召回率为79.7%。此外，研究者还进行了广泛的分析，证明了cross-transformer可以作为自动调整模组，将双侧照片中的变化联系起来，并将这些资讯用于最终预测。<details>
<summary>Abstract</summary>
Screening mammography is the most widely used method for early breast cancer detection, significantly reducing mortality rates. The integration of information from multi-view mammograms enhances radiologists' confidence and diminishes false-positive rates since they can examine on dual-view of the same breast to cross-reference the existence and location of the lesion. Inspired by this, we present TransReg, a Computer-Aided Detection (CAD) system designed to exploit the relationship between craniocaudal (CC), and mediolateral oblique (MLO) views. The system includes cross-transformer to model the relationship between the region of interest (RoIs) extracted by siamese Faster RCNN network for mass detection problems. Our work is the first time cross-transformer has been integrated into an object detection framework to model the relation between ipsilateral views. Our experimental evaluation on DDSM and VinDr-Mammo datasets shows that our TransReg, equipped with SwinT as a feature extractor achieves state-of-the-art performance. Specifically, at the false positive rate per image at 0.5, TransReg using SwinT gets a recall at 83.3% for DDSM dataset and 79.7% for VinDr-Mammo dataset. Furthermore, we conduct a comprehensive analysis to demonstrate that cross-transformer can function as an auto-registration module, aligning the masses in dual-view and utilizing this information to inform final predictions. It is a replication diagnostic workflow of expert radiologists
</details>
<details>
<summary>摘要</summary>
屏幕护肤摄影是现在最广泛使用的方法，以提高乳腺癌检测的早期级分。通过多视图护肤摄影的信息集成，让放射学家们的信任度提高，并降低假阳性率，因为他们可以通过对同一个乳腺的两个视图进行对比，以确定病变的存在和位置。这里，我们提出了一种名为TransReg的计算机支持检测（CAD）系统，以利用双视图之间的关系来检测乳腺癌。我们的TransReg系统包括交叉变换器，用于模型双视图之间的区域关系。我们的实验结果表明，当使用SwinT作为特征提取器时，TransReg系统在DDSM和VinDr-Mammo数据集上达到了状态空间的表现。具体来说，在预设false positive rate为0.5时，TransReg使用SwinT获得了83.3%的报告率（DDSM数据集）和79.7%的报告率（VinDr-Mammo数据集）。此外，我们进行了全面的分析，以证明交叉变换器可以作为自动注册模块，将双视图中的病变对应，并使用这些信息来指导最终预测。这种方法与专业放射学家的诊断工作类似。
</details></li>
</ul>
<hr>
<h2 id="Audio-visual-Saliency-for-Omnidirectional-Videos"><a href="#Audio-visual-Saliency-for-Omnidirectional-Videos" class="headerlink" title="Audio-visual Saliency for Omnidirectional Videos"></a>Audio-visual Saliency for Omnidirectional Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05190">http://arxiv.org/abs/2311.05190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FannyChao/AVS360_audiovisual_saliency_360">https://github.com/FannyChao/AVS360_audiovisual_saliency_360</a></li>
<li>paper_authors: Yuxin Zhu, Xilei Zhu, Huiyu Duan, Jie Li, Kaiwei Zhang, Yucheng Zhu, Li Chen, Xiongkuo Min, Guangtao Zhai</li>
<li>for: 这个论文旨在为омниirectional视频（ODV）的视觉吸引力预测提供基础数据和分析方法。</li>
<li>methods: 本论文使用了大量的audio-visual ODV数据和相关的眼动追踪数据来分析视觉注意力的行为。</li>
<li>results: 研究发现，不同的音频模式下的视觉注意力行为有所不同，并且可以通过对AVS-ODV数据集进行分析和比较来评估不同的视觉注意力预测模型的性能。<details>
<summary>Abstract</summary>
Visual saliency prediction for omnidirectional videos (ODVs) has shown great significance and necessity for omnidirectional videos to help ODV coding, ODV transmission, ODV rendering, etc.. However, most studies only consider visual information for ODV saliency prediction while audio is rarely considered despite its significant influence on the viewing behavior of ODV. This is mainly due to the lack of large-scale audio-visual ODV datasets and corresponding analysis. Thus, in this paper, we first establish the largest audio-visual saliency dataset for omnidirectional videos (AVS-ODV), which comprises the omnidirectional videos, audios, and corresponding captured eye-tracking data for three video sound modalities including mute, mono, and ambisonics. Then we analyze the visual attention behavior of the observers under various omnidirectional audio modalities and visual scenes based on the AVS-ODV dataset. Furthermore, we compare the performance of several state-of-the-art saliency prediction models on the AVS-ODV dataset and construct a new benchmark. Our AVS-ODV datasets and the benchmark will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
Visual 景点预测 для全方位视频（ODV）已经表现出了很大的重要性和必要性，以帮助ODV编码、ODV传输、ODV渲染等等。然而，大多数研究只考虑了视觉信息来预测ODV的预测，即使听音信息在视觉行为上具有显著的影响，它们很少被考虑。这主要是因为缺乏大规模的音视频ODV数据集和相应的分析。因此，在这篇论文中，我们首先建立了全方位音视频预测数据集（AVS-ODV），该数据集包括全方位视频、听音和相应的捕捉眼动追踪数据，其中包括无声、单声和普适声模式。然后，我们分析了在不同的全方位声音模式下观者的视觉注意力行为，并基于AVS-ODV数据集进行了比较性分析。此外，我们还构建了一个新的标准。我们的AVS-ODV数据集和标准将会被发布，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Association-Learning-of-Self-Attention-and-Convolution-in-Image-Restoration"><a href="#Dynamic-Association-Learning-of-Self-Attention-and-Convolution-in-Image-Restoration" class="headerlink" title="Dynamic Association Learning of Self-Attention and Convolution in Image Restoration"></a>Dynamic Association Learning of Self-Attention and Convolution in Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05147">http://arxiv.org/abs/2311.05147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kui Jiang, Xuemei Jia, Wenxin Huang, Wenbin Wang, Zheng Wang, Junjun Jiang</li>
<li>for: 提高图像修净效率和质量，兼顾雨纹分布预测和背景恢复。</li>
<li>methods: 利用关注力模块生成降水干扰矩阵，并通过多头注意力和径向网络进行联合学习，以提取雨纹干扰对应的背景纹理信息，并实现图像修净。</li>
<li>results: 提出一种基于关注力模块的联合学习方法，可以高效地修净图像，同时兼顾雨纹分布预测和背景恢复。<details>
<summary>Abstract</summary>
CNNs and Self attention have achieved great success in multimedia applications for dynamic association learning of self-attention and convolution in image restoration. However, CNNs have at least two shortcomings: 1) limited receptive field; 2) static weight of sliding window at inference, unable to cope with the content diversity.In view of the advantages and disadvantages of CNNs and Self attention, this paper proposes an association learning method to utilize the advantages and suppress their shortcomings, so as to achieve high-quality and efficient inpainting. We regard rain distribution reflects the degradation location and degree, in addition to the rain distribution prediction. Thus, we propose to refine background textures with the predicted degradation prior in an association learning manner. As a result, we accomplish image deraining by associating rain streak removal and background recovery, where an image deraining network and a background recovery network are designed for two subtasks. The key part of association learning is a novel multi-input attention module. It generates the degradation prior and produces the degradation mask according to the predicted rainy distribution. Benefited from the global correlation calculation of SA, MAM can extract the informative complementary components from the rainy input with the degradation mask, and then help accurate texture restoration. Meanwhile, SA tends to aggregate feature maps with self-attention importance, but convolution diversifies them to focus on the local textures. A hybrid fusion network involves one residual Transformer branch and one encoder-decoder branch. The former takes a few learnable tokens as input and stacks multi-head attention and feed-forward networks to encode global features of the image. The latter, conversely, leverages the multi-scale encoder-decoder to represent contexture knowledge.
</details>
<details>
<summary>摘要</summary>
使用CNN和自注意来处理多媒体应用程序的动态关联学习，已经取得了很大的成功。然而，CNN有两个缺点：1）有限的接收场景；2）在推理时，使用静止窗口的重量，无法适应内容多样性。面对CNN和自注意的优势和缺点，这篇论文提出了一种关联学习方法，以利用优势并抑制缺点，以达到高质量和高效的填充图像。我们认为雨水分布反映了损害的位置和程度，而不仅仅是雨水分布预测。因此，我们提议使用预测的损害估计来修正背景xture，以达到图像抹雨的目的。在这种关联学习方法中，我们设计了一个图像抹雨网络和背景恢复网络，用于两个子任务。关键部分是一种新的多输入注意模块，它生成了损害估计和生成损害面，根据预测的雨水分布。通过SA的全局相关计算，MAM可以从雨水输入中提取有用的补偿组件，并帮助精确的 тексту复原。同时，SA倾向于将特征地图汇聚到自注意重要性上，而 convolution 则将其多样化，以注重本地 тексту。一个混合融合网络包括一个待用Token的待用Transformer分支和一个Encoder-Decoder分支。前者将一些学习 Token 作为输入，并排列多头注意和Feed-Forward网络来编码图像的全局特征。后者则利用多尺度Encoder-Decoder来表达图像上的文化知识。
</details></li>
</ul>
<hr>
<h2 id="OW-SLR-Overlapping-Windows-on-Semi-Local-Region-for-Image-Super-Resolution"><a href="#OW-SLR-Overlapping-Windows-on-Semi-Local-Region-for-Image-Super-Resolution" class="headerlink" title="OW-SLR: Overlapping Windows on Semi-Local Region for Image Super-Resolution"></a>OW-SLR: Overlapping Windows on Semi-Local Region for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05146">http://arxiv.org/abs/2311.05146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishav Bhardwaj, Janarthanam Jothi Balaji, Vasudevan Lakshminarayanan</li>
<li>for: 这篇论文旨在提高图像的分辨率，使其能够到达任意分辨率。</li>
<li>methods: 该论文提出了一种新的技术 called Overlapping Windows on Semi-Local Region (OW-SLR)，它利用图像的 semi-local 区域来提高图像的分辨率。</li>
<li>results: 该技术在应用于 Optical Coherence Tomography-Angiography (OCT-A) 图像上显示出了更高的性能，并且在 OCT500 数据集上超越了现有的状态对技术。  Additionally, the technique shows better results in classifying healthy and diseased retinal images, such as diabetic retinopathy and normals, from the given set of OCT-A images.<details>
<summary>Abstract</summary>
There has been considerable progress in implicit neural representation to upscale an image to any arbitrary resolution. However, existing methods are based on defining a function to predict the Red, Green and Blue (RGB) value from just four specific loci. Relying on just four loci is insufficient as it leads to losing fine details from the neighboring region(s). We show that by taking into account the semi-local region leads to an improvement in performance. In this paper, we propose applying a new technique called Overlapping Windows on Semi-Local Region (OW-SLR) to an image to obtain any arbitrary resolution by taking the coordinates of the semi-local region around a point in the latent space. This extracted detail is used to predict the RGB value of a point. We illustrate the technique by applying the algorithm to the Optical Coherence Tomography-Angiography (OCT-A) images and show that it can upscale them to random resolution. This technique outperforms the existing state-of-the-art methods when applied to the OCT500 dataset. OW-SLR provides better results for classifying healthy and diseased retinal images such as diabetic retinopathy and normals from the given set of OCT-A images. The project page is available at https://rishavbb.github.io/ow-slr/index.html
</details>
<details>
<summary>摘要</summary>
在半透射成像扫描技术（OCT-A）图像的扩展问题上，已经取得了很大的进步。然而，现有的方法都是基于定义一个函数来预测红色、绿色和蓝色（RGB）值的四个具体点。这些方法有限地依赖于四个点，导致在邻近区域中丢失细节。我们表明，通过考虑半本地区域来提高性能。在这篇论文中，我们提议将半本地区域分割成多个重叠窗口（OW-SLR），并在这些窗口中提取半本地区域的细节。这些提取的细节可以用来预测RGB值。我们通过应用这种算法到OCT500数据集上，并证明其可以对OCT-A图像进行任意分辨率的扩展。此外，我们还对健康和疾病肠细血管图像进行了分类，并证明OW-SLR方法可以更好地分类这些图像。项目页面可以在https://rishavbb.github.io/ow-slr/index.html中找到。
</details></li>
</ul>
<hr>
<h2 id="SCAAT-Improving-Neural-Network-Interpretability-via-Saliency-Constrained-Adaptive-Adversarial-Training"><a href="#SCAAT-Improving-Neural-Network-Interpretability-via-Saliency-Constrained-Adaptive-Adversarial-Training" class="headerlink" title="SCAAT: Improving Neural Network Interpretability via Saliency Constrained Adaptive Adversarial Training"></a>SCAAT: Improving Neural Network Interpretability via Saliency Constrained Adaptive Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05143">http://arxiv.org/abs/2311.05143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Xu, Wenkang Qin, Peixiang Huang, Haowang, Lin Luo</li>
<li>for: 提高深度神经网络（DNN）的解释能力，以便用户更好地理解黑色盒预测结果的来源。</li>
<li>methods: 提出一种模型agnostic学习方法called Saliency Constrained Adaptive Adversarial Training (SCAAT)，通过构建对抗样本，以降低精度图中的噪音，使精度图更加精炼和可靠。</li>
<li>results: 应用SCAAT于多个DNN模型，并评估其生成的精度图在不同领域和度量上的质量，结果显示SCAAT可以有效地提高DNN的解释能力，不 sacrifice其预测能力。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are expected to provide explanation for users to understand their black-box predictions. Saliency map is a common form of explanation illustrating the heatmap of feature attributions, but it suffers from noise in distinguishing important features. In this paper, we propose a model-agnostic learning method called Saliency Constrained Adaptive Adversarial Training (SCAAT) to improve the quality of such DNN interpretability. By constructing adversarial samples under the guidance of saliency map, SCAAT effectively eliminates most noise and makes saliency maps sparser and more faithful without any modification to the model architecture. We apply SCAAT to multiple DNNs and evaluate the quality of the generated saliency maps on various natural and pathological image datasets. Evaluations on different domains and metrics show that SCAAT significantly improves the interpretability of DNNs by providing more faithful saliency maps without sacrificing their predictive power.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）预期能提供用户理解其黑盒预测结果的解释。热力图是一种常见的解释形式，可以图示神经网络的特征归因热力图，但它受到噪声的影响，难以分辨重要的特征。在这篇论文中，我们提出了一种模型无关学习方法called Saliency Constrained Adaptive Adversarial Training（SCAAT），用于提高DNN的解释质量。通过在导航热力图的指导下构建 adversarial samples，SCAAT可以有效地消除噪声，使热力图更加紧凑和更 faithful，无需修改模型结构。我们在多个DNN上应用SCAAT，并对不同的领域和指标进行评估。评估结果显示，SCAAT可以有效地提高DNN的解释质量，无需牺牲预测力。
</details></li>
</ul>
<hr>
<h2 id="ScribblePolyp-Scribble-Supervised-Polyp-Segmentation-through-Dual-Consistency-Alignment"><a href="#ScribblePolyp-Scribble-Supervised-Polyp-Segmentation-through-Dual-Consistency-Alignment" class="headerlink" title="ScribblePolyp: Scribble-Supervised Polyp Segmentation through Dual Consistency Alignment"></a>ScribblePolyp: Scribble-Supervised Polyp Segmentation through Dual Consistency Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05122">http://arxiv.org/abs/2311.05122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixun Zhang, Yuncheng Jiang, Jun Wei, Hannah Cui, Zhen Li</li>
<li>for:  This paper aims to develop an efficient and accurate polyp segmentation model for gastrointestinal diseases, which can reduce the cost of pixel-level annotations and improve the generalization of the model.</li>
<li>methods:  The proposed method, called ScribblePolyp, uses a scribble-supervised approach that only requires the annotation of two lines (scribble labels) for each image. The method leverages a two-branch consistency alignment approach to provide supervision for unlabeled pixels, including transformation consistency alignment and affinity propagation.</li>
<li>results:  The experimental results on the SUN-SEG dataset show that ScribblePolyp achieves a Dice score of 0.8155, with the potential for a 1.8% improvement in the Dice score through a straightforward self-training strategy.<details>
<summary>Abstract</summary>
Automatic polyp segmentation models play a pivotal role in the clinical diagnosis of gastrointestinal diseases. In previous studies, most methods relied on fully supervised approaches, necessitating pixel-level annotations for model training. However, the creation of pixel-level annotations is both expensive and time-consuming, impeding the development of model generalization. In response to this challenge, we introduce ScribblePolyp, a novel scribble-supervised polyp segmentation framework. Unlike fully-supervised models, ScribblePolyp only requires the annotation of two lines (scribble labels) for each image, significantly reducing the labeling cost. Despite the coarse nature of scribble labels, which leave a substantial portion of pixels unlabeled, we propose a two-branch consistency alignment approach to provide supervision for these unlabeled pixels. The first branch employs transformation consistency alignment to narrow the gap between predictions under different transformations of the same input image. The second branch leverages affinity propagation to refine predictions into a soft version, extending additional supervision to unlabeled pixels. In summary, ScribblePolyp is an efficient model that does not rely on teacher models or moving average pseudo labels during training. Extensive experiments on the SUN-SEG dataset underscore the effectiveness of ScribblePolyp, achieving a Dice score of 0.8155, with the potential for a 1.8% improvement in the Dice score through a straightforward self-training strategy.
</details>
<details>
<summary>摘要</summary>
自动肿体分割模型在肠胃疾病诊断中扮演着关键角色。在前期研究中，大多数方法依赖于全supervised方法，需要每个图像进行像素级别的标注。然而，创建像素级别的标注是非常昂贵的，对模型通用性的发展带来障碍。为回应这个挑战，我们介绍ScribblePolyp，一种新的涂抹supervised肿体分割框架。与全supervised模型不同，ScribblePolyp只需每个图像两条涂抹标签（scribble labels）的标注，明显减少标注成本。尽管涂抹标签的粗糙性留下大量的像素未标注，我们提议一种两支分支一致性适应方法，以提供对这些未标注像素的超vision。首支分支采用变换一致适应，将输入图像不同变换后的预测差异缩小至最小。第二支分支利用协同传播来细化预测，将未标注像素扩展到软版本，进一步提供超vision。总之，ScribblePolyp是一种高效的模型，不需要教师模型或搅拌pseudo标签在训练过程中。广泛的实验表明，ScribblePolyp在SUN-SEG数据集上表现出色，达到了0.8155的Dice分数，可能通过简单的自动训练策略提高1.8%的Dice分数。
</details></li>
</ul>
<hr>
<h2 id="Reducing-the-Side-Effects-of-Oscillations-in-Training-of-Quantized-YOLO-Networks"><a href="#Reducing-the-Side-Effects-of-Oscillations-in-Training-of-Quantized-YOLO-Networks" class="headerlink" title="Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks"></a>Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05109">http://arxiv.org/abs/2311.05109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kartik Gupta, Akshay Asthana</li>
<li>for: 这篇研究探讨了如何将深度学习模型量化，以便在边缘设备上实现。</li>
<li>methods: 研究使用了量化训练（Quantization-Aware Training，QAT）方法，并提出了一些新的技术来改善量化模型的精度。</li>
<li>results: 研究发现，对于实时 object detection 和 semantic segmentation 方法如 YOLO，使用 QAT 方法可以实现更高的精度，并且可以在低比特率（4-bit和3-bit）下实现更好的性能。<details>
<summary>Abstract</summary>
Quantized networks use less computational and memory resources and are suitable for deployment on edge devices. While quantization-aware training QAT is the well-studied approach to quantize the networks at low precision, most research focuses on over-parameterized networks for classification with limited studies on popular and edge device friendly single-shot object detection and semantic segmentation methods like YOLO. Moreover, majority of QAT methods rely on Straight-through Estimator (STE) approximation which suffers from an oscillation phenomenon resulting in sub-optimal network quantization. In this paper, we show that it is difficult to achieve extremely low precision (4-bit and lower) for efficient YOLO models even with SOTA QAT methods due to oscillation issue and existing methods to overcome this problem are not effective on these models. To mitigate the effect of oscillation, we first propose Exponentially Moving Average (EMA) based update to the QAT model. Further, we propose a simple QAT correction method, namely QC, that takes only a single epoch of training after standard QAT procedure to correct the error induced by oscillating weights and activations resulting in a more accurate quantized model. With extensive evaluation on COCO dataset using various YOLO5 and YOLO7 variants, we show that our correction method improves quantized YOLO networks consistently on both object detection and segmentation tasks at low-precision (4-bit and 3-bit).
</details>
<details>
<summary>摘要</summary>
量化网络使用更少的计算和存储资源，适用于边缘设备上部署。而量化意识训练（QAT）是已有研究的方法，可以在低精度下量化网络，但大多数研究都是针对过 parameterized 网络进行分类，很少关注流行的单射检测和 semantic segmentation 方法，例如 YOLO。此外，大多数 QAT 方法都采用 Straight-through Estimator（STE）approximation，它受到振荡现象的影响，导致网络量化效果不佳。在这篇论文中，我们表明，即使使用最新的 QAT 方法，也难以在高效 YOLO 模型中实现非常低精度（4位和更低），因为振荡问题和现有的方法无法解决这个问题。为了解决振荡问题，我们首先提出了指数移动平均（EMA）更新方法。此外，我们还提出了一种简单的 QAT 更正方法，称为 QC，它只需要在标准 QAT 过程后进行一个epoch的训练，可以更正由振荡 weights 和活动引起的错误，从而得到更准确的量化模型。通过对 COCO  dataset 上不同的 YOLO5 和 YOLO7 变体进行了广泛的评估，我们表明了我们的更正方法在对象检测和 segmentation 任务中具有适用性，并且在 4 位和 3 位精度下具有更高的精度。
</details></li>
</ul>
<hr>
<h2 id="Self-similarity-Prior-Distillation-for-Unsupervised-Remote-Physiological-Measurement"><a href="#Self-similarity-Prior-Distillation-for-Unsupervised-Remote-Physiological-Measurement" class="headerlink" title="Self-similarity Prior Distillation for Unsupervised Remote Physiological Measurement"></a>Self-similarity Prior Distillation for Unsupervised Remote Physiological Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05100">http://arxiv.org/abs/2311.05100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Weiyu Sun, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen</li>
<li>for: 这个论文旨在提出一个不受训练的 remote photoplethysmography（rPPG）估算方法，利用内在的自相似性知识来提高估算精度。</li>
<li>methods: 方法首先引入物理假设嵌入技术来减少各种噪音的影响，然后适用自相似性感知网络来提取更可靠的自相似生物学特征。最后，实现一个层次自岿举法来辅助网络从脸部影像中分离自相似生物学模式。</li>
<li>results: 实验结果显示，不受训练的 SSPD 框架可以与现有的超vised 方法相比，具有相似或更高的估算精度，同时具有最低的处理时间和计算成本。<details>
<summary>Abstract</summary>
Remote photoplethysmography (rPPG) is a noninvasive technique that aims to capture subtle variations in facial pixels caused by changes in blood volume resulting from cardiac activities. Most existing unsupervised methods for rPPG tasks focus on the contrastive learning between samples while neglecting the inherent self-similar prior in physiological signals. In this paper, we propose a Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG estimation, which capitalizes on the intrinsic self-similarity of cardiac activities. Specifically, we first introduce a physical-prior embedded augmentation technique to mitigate the effect of various types of noise. Then, we tailor a self-similarity-aware network to extract more reliable self-similar physiological features. Finally, we develop a hierarchical self-distillation paradigm to assist the network in disentangling self-similar physiological patterns from facial videos. Comprehensive experiments demonstrate that the unsupervised SSPD framework achieves comparable or even superior performance compared to the state-of-the-art supervised methods. Meanwhile, SSPD maintains the lowest inference time and computation cost among end-to-end models. The source codes are available at https://github.com/LinXi1C/SSPD.
</details>
<details>
<summary>摘要</summary>
干扰物耗用非侵入式技术（rPPG）可以捕捉到面部像素中的微小变化，这些变化与心跳活动有关。现有大多数无监督方法都会忽略生物信号自然的内在自相似性。在这篇论文中，我们提出了一个基于自相似性的抽象框架（SSPD），用于无监督的rPPG估计。我们首先引入了一种嵌入了物理约束的数据增强技术，以抑制各种噪声的影响。然后，我们适应了一种自相似性感知网络，以提取更可靠的生理特征。最后，我们开发了一种层次自适应分解方法，以帮助网络分解自相似的生理模式。完整的实验结果表明，无监督的SSPD框架可以与现有的监督方法相当或更高的性能，同时具有最低的推理时间和计算成本。代码可以在 GitHub 上找到：https://github.com/LinXi1C/SSPD。
</details></li>
</ul>
<hr>
<h2 id="POISE-Pose-Guided-Human-Silhouette-Extraction-under-Occlusions"><a href="#POISE-Pose-Guided-Human-Silhouette-Extraction-under-Occlusions" class="headerlink" title="POISE: Pose Guided Human Silhouette Extraction under Occlusions"></a>POISE: Pose Guided Human Silhouette Extraction under Occlusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05077">http://arxiv.org/abs/2311.05077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/take2rohit/poise">https://github.com/take2rohit/poise</a></li>
<li>paper_authors: Arindam Dutta, Rohit Lal, Dripta S. Raychaudhuri, Calvin Khang Ta, Amit K. Roy-Chowdhury</li>
<li>for: 提高人体 Profiling 下 occlusion 的精度和稳定性</li>
<li>methods: 提出 POISE 自助学习混合模型，结合 segmentation 模型和 pose estimation 模型，利用两者的优势，提高人体 Profiling 的准确性和可靠性</li>
<li>results: 对多种 occlusion 进行了广泛的实验，证明 POISE 可以有效地提高人体 Profiling 的精度和稳定性，并且在下游任务中表现出色，如 gate recognition 等<details>
<summary>Abstract</summary>
Human silhouette extraction is a fundamental task in computer vision with applications in various downstream tasks. However, occlusions pose a significant challenge, leading to incomplete and distorted silhouettes. To address this challenge, we introduce POISE: Pose Guided Human Silhouette Extraction under Occlusions, a novel self-supervised fusion framework that enhances accuracy and robustness in human silhouette prediction. By combining initial silhouette estimates from a segmentation model with human joint predictions from a 2D pose estimation model, POISE leverages the complementary strengths of both approaches, effectively integrating precise body shape information and spatial information to tackle occlusions. Furthermore, the self-supervised nature of \POISE eliminates the need for costly annotations, making it scalable and practical. Extensive experimental results demonstrate its superiority in improving silhouette extraction under occlusions, with promising results in downstream tasks such as gait recognition. The code for our method is available https://github.com/take2rohit/poise.
</details>
<details>
<summary>摘要</summary>
人体影子提取是计算机视觉中的基本任务，具有许多下游任务的应用。然而，干扰问题使得人体影子提取受到潜在的影响，导致人体影子不准确、扭曲。为解决这个挑战，我们介绍POISE：姿势导航人体影子提取下 occlusions，一种新的自助学习融合框架。POISE通过将分割模型提供的初始人体影子估计与2D姿势估计模型提供的人体关节预测结合，以利用这两种方法的优势，准确地融合人体形状信息和空间信息，有效地解决干扰问题。此外，POISE的自助学习性质使得无需贵重的标注，可以扩展和实用。广泛的实验结果表明POISE在干扰下人体影子提取方面具有优势，并在下游任务中得到了推荐的结果，如步态识别。POISE的代码可以在https://github.com/take2rohit/poise上下载。
</details></li>
</ul>
<hr>
<h2 id="On-the-Behavior-of-Audio-Visual-Fusion-Architectures-in-Identity-Verification-Tasks"><a href="#On-the-Behavior-of-Audio-Visual-Fusion-Architectures-in-Identity-Verification-Tasks" class="headerlink" title="On the Behavior of Audio-Visual Fusion Architectures in Identity Verification Tasks"></a>On the Behavior of Audio-Visual Fusion Architectures in Identity Verification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05071">http://arxiv.org/abs/2311.05071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Claborne, Eric Slyman, Karl Pazdernik</li>
<li>for: 这个论文是为了研究一种标识验证架构，并评估这种架构在不同情况下的性能。</li>
<li>methods: 这个论文使用的方法包括对音频和视觉表示的组合部分进行修改，以及在一个输入缺失的情况下进行比较。</li>
<li>results: 结果表明，将输出嵌入值平均值可以提高错误率，并且在全功能模式和单模态缺失情况下都有更好的表现，可能的原因是这种方法更好地利用嵌入空间。<details>
<summary>Abstract</summary>
We train an identity verification architecture and evaluate modifications to the part of the model that combines audio and visual representations, including in scenarios where one input is missing in either of two examples to be compared. We report results on the Voxceleb1-E test set that suggest averaging the output embeddings improves error rate in the full-modality setting and when a single modality is missing, and makes more complete use of the embedding space than systems which use shared layers and discuss possible reasons for this behavior.
</details>
<details>
<summary>摘要</summary>
我们训练了一个标识验证建筑，并评估了对将音频和视觉表示结合部分进行修改，包括在两个示例之间比较时一种输入缺失的情况。我们在Voxceleb1-E测试集上发现，将输出嵌入平均值提高了全功能模式和单模式中的错误率，并更好地使用嵌入空间。我们还讨论了可能的原因。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.CV_2023_11_09/" data-id="clot2mhdv00l9x788b9j0733e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.AI_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T12:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/cs.AI_2023_11_09/">cs.AI - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts"><a href="#FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts" class="headerlink" title="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"></a>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05608">http://arxiv.org/abs/2311.05608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thuccslab/figstep">https://github.com/thuccslab/figstep</a></li>
<li>paper_authors: Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</li>
<li>for: 本研究旨在展示多modalities导致AI安全问题的可能性。</li>
<li>methods: 该研究提出了一种名为FigStep的攻击框架，通过图像通道输入危险指令，然后使用无害文本提示 induced VLMs输出违反常规AI安全政策的内容。</li>
<li>results: 实验结果显示，FigStep可以在2家流行的开源VLMs（LLaVA和MiniGPT4）上 achieved an average attack success rate of 94.8%（共5个VLMs）。此外，我们还示出了FigStep方法可以甚至破坏GPT-4V，该模型已经利用了多种系统级别的机制来过滤危险查询。<details>
<summary>Abstract</summary>
Large vision-language models (VLMs) like GPT-4V represent an unprecedented revolution in the field of artificial intelligence (AI). Compared to single-modal large language models (LLMs), VLMs possess more versatile capabilities by incorporating additional modalities (e.g., images). Meanwhile, there's a rising enthusiasm in the AI community to develop open-source VLMs, such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety assessment. In this paper, to demonstrate that more modalities lead to unforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework against VLMs. FigStep feeds harmful instructions into VLMs through the image channel and then uses benign text prompts to induce VLMs to output contents that violate common AI safety policies. Our experimental results show that FigStep can achieve an average attack success rate of 94.8% across 2 families of popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover, we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which already leverages several system-level mechanisms to filter harmful queries. Above all, our experimental results reveal that VLMs are vulnerable to jailbreaking attacks, which highlights the necessity of novel safety alignments between visual and textual modalities.
</details>
<details>
<summary>摘要</summary>
大型视语模型（VLM）如GPT-4V在人工智能（AI）领域表现了无 precedent 的革命。相比单modal大语言模型（LLM），VLM具有更多的多样化能力，通过添加其他模альности（如图像）。然而，AI社区中的开源VLM的开发，如LLaVA和MiniGPT4，尚未经过严格的安全评估。在这篇论文中，我们提出了FigStep，一种新的破坏框架，用于对VLM进行攻击。FigStep通过图像频道输入坏征 instruction，然后使用恰好的文本提示来让VLM输出违反常见AI安全政策的内容。我们的实验结果表明，FigStep可以在2家流行的开源VLM中（LLaVA和MiniGPT4，共5个VLM） achieved an average attack success rate of 94.8%。此外，我们还证明了FigStep的方法可以破坏GPT-4V，这个模型已经利用了多种系统级别的机制来筛选坏征查询。总之，我们的实验结果表明，VLM具有破坏攻击的潜在隐患，这引起了AI安全政策的重新对齐。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Rasterization-for-Large-Scenes"><a href="#Real-Time-Neural-Rasterization-for-Large-Scenes" class="headerlink" title="Real-Time Neural Rasterization for Large Scenes"></a>Real-Time Neural Rasterization for Large Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05607">http://arxiv.org/abs/2311.05607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Yunfan Liu, Yun Chen, Ze Yang, Jingkang Wang, Sivabalan Manivasagam, Raquel Urtasun</li>
<li>for: 大型场景的实时新视角 sintesis</li>
<li>methods: 组合 neural texture field 和 shader，使用标准图形管道进行实时渲染</li>
<li>results: 比较 Neil 渲染方法快30倍，并提供了相当或更好的现实感，适用于大型自驾车和无人机场景<details>
<summary>Abstract</summary>
We propose a new method for realistic real-time novel-view synthesis (NVS) of large scenes. Existing neural rendering methods generate realistic results, but primarily work for small scale scenes (<50 square meters) and have difficulty at large scale (>10000 square meters). Traditional graphics-based rasterization rendering is fast for large scenes but lacks realism and requires expensive manually created assets. Our approach combines the best of both worlds by taking a moderate-quality scaffold mesh as input and learning a neural texture field and shader to model view-dependant effects to enhance realism, while still using the standard graphics pipeline for real-time rendering. Our method outperforms existing neural rendering methods, providing at least 30x faster rendering with comparable or better realism for large self-driving and drone scenes. Our work is the first to enable real-time rendering of large real-world scenes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于实现真实时间的新观点合成（NVS）大景。现有的神经渲染方法可以生成真实的结果，但主要适用于小规模场景（<50平方米），大规模场景（>10000平方米）很难。传统的图形学基础的排版渲染快速渲染大场景，但缺乏真实感和需要费时手动创建资产。我们的方法将神经渲染和排版渲染结合，使用中等质量的框架网格作为输入，学习神经xture场和渲染程序来提高真实感，同时仍然使用标准图形管道进行实时渲染。我们的方法在比较30倍快于现有神经渲染方法，并且与或更好的真实感在大自驾和无人机场景中提供了相似或更好的图形效果。我们的工作是首次实现了真实时间渲染大自然场景。
</details></li>
</ul>
<hr>
<h2 id="SynH2R-Synthesizing-Hand-Object-Motions-for-Learning-Human-to-Robot-Handovers"><a href="#SynH2R-Synthesizing-Hand-Object-Motions-for-Learning-Human-to-Robot-Handovers" class="headerlink" title="SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers"></a>SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05599">http://arxiv.org/abs/2311.05599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song</li>
<li>for: 本研究旨在提供一种可以生成人类抓取动作的框架，以便在训练机器人时使用。</li>
<li>methods: 该方法使用了手掌对象合成技术，以生成与人类抓取动作相似的机器人抓取动作。</li>
<li>results: 研究表明，使用该方法可以在实际环境中训练机器人，并且可以更好地扩展到各种不同的物体和人类抓取动作。In English:</li>
<li>for: The paper aims to provide a framework that can generate human grasping motions for training robots.</li>
<li>methods: The method uses hand-object synthesis technology to generate robot grasping motions similar to humans.</li>
<li>results: The study shows that the method can be used to train robots in real-world environments and can better scale to a variety of objects and human grasping motions.<details>
<summary>Abstract</summary>
Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/
</details>
<details>
<summary>摘要</summary>
“视觉基于人机交互的人机交换是人机机器交互中的重要和挑战性任务。latest work 尝试通过在模拟环境中与动态虚拟人进行交互，以训练机器人策略，但它们受到人体运动捕捉数据的依赖，这是 expensive 和难以扩展到任意对象和人类抓取动作。在本文中，我们介绍了一个框架，可以生成人类抓取动作，可以用于训练机器人。为 achieve 这一目标，我们提出了一种手套物合成方法，可以生成人类抓取动作的类似模拟数据。这使得我们可以生成 Synthetic 训练和测试数据，与前一代方法相比，具有100倍更多的对象。在我们的实验中，我们表明我们的方法，只使用 Synthetic 数据进行训练，与当前的状态艺术方法相比，在模拟环境和真实系统上具有类似的性能。此外，我们可以在更大的规模上进行评估，与先前的工作相比。我们新引入的测试集，显示我们的模型可以更好地扩展到大量的未看到对象和人类动作。项目页面：https://eth-ait.github.io/synthetic-handovers/”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LLM-Augmented-Hierarchical-Agents"><a href="#LLM-Augmented-Hierarchical-Agents" class="headerlink" title="LLM Augmented Hierarchical Agents"></a>LLM Augmented Hierarchical Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05596">http://arxiv.org/abs/2311.05596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bharat Prakash, Tim Oates, Tinoosh Mohsenin</li>
<li>for: 这篇论文目的是解决长期任务和时间扩展任务中的束缚学习问题。</li>
<li>methods: 该论文使用了人工智能和强化学习，并利用了大语言模型（LLMs）的规划能力，以便在环境学习中提供学习。</li>
<li>results: 在模拟环境和真实 робо臂上进行了实验，并显示了使用该方法让代理人比其他基elines方法表现更好，并且一旦训练完成，不需要在部署时再次访问LLMs。<details>
<summary>Abstract</summary>
Solving long-horizon, temporally-extended tasks using Reinforcement Learning (RL) is challenging, compounded by the common practice of learning without prior knowledge (or tabula rasa learning). Humans can generate and execute plans with temporally-extended actions and quickly learn to perform new tasks because we almost never solve problems from scratch. We want autonomous agents to have this same ability. Recently, LLMs have been shown to encode a tremendous amount of knowledge about the world and to perform impressive in-context learning and reasoning. However, using LLMs to solve real world problems is hard because they are not grounded in the current task. In this paper we exploit the planning capabilities of LLMs while using RL to provide learning from the environment, resulting in a hierarchical agent that uses LLMs to solve long-horizon tasks. Instead of completely relying on LLMs, they guide a high-level policy, making learning significantly more sample efficient. This approach is evaluated in simulation environments such as MiniGrid, SkillHack, and Crafter, and on a real robot arm in block manipulation tasks. We show that agents trained using our approach outperform other baselines methods and, once trained, don't need access to LLMs during deployment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accuracy-of-a-Vision-Language-Model-on-Challenging-Medical-Cases"><a href="#Accuracy-of-a-Vision-Language-Model-on-Challenging-Medical-Cases" class="headerlink" title="Accuracy of a Vision-Language Model on Challenging Medical Cases"></a>Accuracy of a Vision-Language Model on Challenging Medical Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05591">http://arxiv.org/abs/2311.05591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/2v/gpt4v-image-challenge">https://github.com/2v/gpt4v-image-challenge</a></li>
<li>paper_authors: Thomas Buckley, James A. Diao, Adam Rodman, Arjun K. Manrai</li>
<li>for: The paper is written to evaluate the accuracy of the Generative Pre-trained Transformer 4 with Vision (GPT-4V) model on challenging medical cases.</li>
<li>methods: The paper uses 934 cases from the NEJM Image Challenge to evaluate the accuracy of GPT-4V compared to human respondents, and also conducts a physician evaluation of GPT-4V on 69 NEJM clinicopathological conferences (CPCs).</li>
<li>results: GPT-4V achieved an overall accuracy of 61% compared to 49% for humans, and outperformed humans at all levels of difficulty and disagreement, skin tones, and image types. However, performance deteriorated when images were added to highly informative text.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是用来评估Generative Pre-trained Transformer 4 with Vision（GPT-4V）模型在医学挑战性案例中的准确性。</li>
<li>methods: 这篇论文使用了934个来自NEJM Image Challenge的案例来评估GPT-4V与人类回答者的准确性，并进行了69个NEJM临床Pathological Conferences（CPCs）的医生评估。</li>
<li>results: GPT-4V在总体准确性方面取得了61%（95% CI，58%到64%），与人类回答者的49%（95% CI，49%到50%）相比，GPT-4V在所有水平和不同的难度、肤色和图像类型上都超过了人类回答者。但是，当图像被添加到高度信息的文本时，GPT-4V的性能下降。<details>
<summary>Abstract</summary>
Background: General-purpose large language models that utilize both text and images have not been evaluated on a diverse array of challenging medical cases.   Methods: Using 934 cases from the NEJM Image Challenge published between 2005 and 2023, we evaluated the accuracy of the recently released Generative Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human respondents overall and stratified by question difficulty, image type, and skin tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM clinicopathological conferences (CPCs). Analyses were conducted for models utilizing text alone, images alone, and both text and images.   Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%) compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at all levels of difficulty and disagreement, skin tones, and image types; the exception was radiographic images, where performance was equivalent between GPT-4V and human respondents. Longer, more informative captions were associated with improved performance for GPT-4V but similar performance for human respondents. GPT-4V included the correct diagnosis in its differential for 80% (95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45 to 70%) of CPCs when using both images and text.   Conclusions: GPT-4V outperformed human respondents on challenging medical cases and was able to synthesize information from both images and text, but performance deteriorated when images were added to highly informative text. Overall, our results suggest that multimodal AI models may be useful in medical diagnostic reasoning but that their accuracy may depend heavily on context.
</details>
<details>
<summary>摘要</summary>
背景：大型语言模型，使用文本和图像，尚未在多元化和复杂的医学案例上进行评估。方法：使用2005-2023年度《新英格兰医学杂志》（NEJM）的934个挑战 caso，评估最近发布的生成推训Transformer 4 with Vision（GPT-4V）模型与人类回答者的精度，并按问题难度、图像类型和皮肤颜色进行分类。此外，我们还进行了69名医生对GPT-4V的评估，用于1995-2023年度《新英格兰医学杂志》临床 PATHOLOGICAL CONFERENCES（CPCs）。分析方法包括文本 alone、图像 alone和文本和图像的组合。结果：GPT-4V的总精度为61%（95% CI，58%-64%），高于人类回答者的49%（95% CI，49%-50%）。GPT-4V在所有难度和不同程度、皮肤颜色和图像类型中都表现出优异，只有辐射图像类型的表现与人类回答者相当。 longer、更加详细的描述与GPT-4V的表现相似，而人类回答者的表现则不变。GPT-4V使用文本alone时包含正确的诊断在其分布中的比例为80%（95% CI，68%-88%），与使用文本和图像时相同。结论：GPT-4V在医学案例中表现出优异，能够从文本和图像中synthesize信息，但是在图像添加到高度信息的文本时，表现下降。总的来说，我们的结果表明，多模态AI模型可能在医学诊断理由中有用，但其精度可能受上下文的影响。
</details></li>
</ul>
<hr>
<h2 id="Conversational-AI-Threads-for-Visualizing-Multidimensional-Datasets"><a href="#Conversational-AI-Threads-for-Visualizing-Multidimensional-Datasets" class="headerlink" title="Conversational AI Threads for Visualizing Multidimensional Datasets"></a>Conversational AI Threads for Visualizing Multidimensional Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05590">http://arxiv.org/abs/2311.05590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt-Heun Hong, Anamaria Crisan</li>
<li>for: 这个论文旨在探讨生成大语言模型（LLM）在数据分析中的可能性，以及其在对话式界面上的应用。</li>
<li>methods: 我们使用了一个LLM进行一项前期Wizard-of-Oz研究，检查了使用聊天机器人进行视觉分析的使用情况。我们发现了LLM驱动的分析聊天机器人在支持进程式视觉分析的方面缺乏能力。基于这些发现，我们开发了AIThreads，一种多线程分析聊天机器人，允许分析员把握对话上下文，并提高其输出的有效性。</li>
<li>results: 我们通过一项在线投票测试（n&#x3D;40）和专家分析员的深入采访（n&#x3D;10）评估了AIThreads的可用性。此外，我们还在LLM训练集外的数据集上展示了AIThreads的能力。我们的发现表明LLM具有潜在的可能性，同时也浮现了未来研究的挑战和有优势的方向。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) show potential in data analysis, yet their full capabilities remain uncharted. Our work explores the capabilities of LLMs for creating and refining visualizations via conversational interfaces. We used an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examining the use of chatbots for conducting visual analysis. We surfaced the strengths and weaknesses of LLM-driven analytic chatbots, finding that they fell short in supporting progressive visualization refinements. From these findings, we developed AI Threads, a multi-threaded analytic chatbot that enables analysts to proactively manage conversational context and improve the efficacy of its outputs. We evaluate its usability through a crowdsourced study (n=40) and in-depth interviews with expert analysts (n=10). We further demonstrate the capabilities of AI Threads on a dataset outside the LLM's training corpus. Our findings show the potential of LLMs while also surfacing challenges and fruitful avenues for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在数据分析方面表现出潜力，但它们的潜在能力仍未得到完全了解。我们的工作探讨了 LLMs 在通过对话 интерфей斯进行数据分析时的能力。我们使用 LLM 重新分析了一项以前的奴隶之力学研究，检查了使用 chatbot 进行视觉分析的使用情况。我们发现 LLM-驱动的分析 chatbot 在支持进程化视觉REFINEMENTS 方面缺乏能力。从这些发现中，我们开发了 AI Threads，一个多线程的分析 chatbot，允许分析员在对话上执行进程化的聊天会话，以提高其输出的有效性。我们通过在线调查（n=40）和专家分析员的深入采访（n=10）评估了 AI Threads 的可用性。我们进一步证明了 AI Threads 在一个不同于 LLM 训练集的数据集上的可能性。我们的发现表明 LLMs 的潜力，同时也浮出了未来研究的挑战和有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Goal-Directed-Dialogue-via-RL-on-Imagined-Conversations"><a href="#Zero-Shot-Goal-Directed-Dialogue-via-RL-on-Imagined-Conversations" class="headerlink" title="Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations"></a>Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05584">http://arxiv.org/abs/2311.05584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Sergey Levine, Anca Dragan</li>
<li>for: 这个论文的目的是如何使用RL和LLM来解决目标导向对话任务。</li>
<li>methods: 这个论文使用的方法是使用LLM生成各种可能的人类对话示例，然后使用RL进行离线学习来训练一个可以优化目标导向目标的对话代理人。</li>
<li>results: 这个论文的实验结果表明，该方法可以在不同的目标导向对话任务中达到最佳性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Inference-for-Probabilistic-Dependency-Graphs"><a href="#Inference-for-Probabilistic-Dependency-Graphs" class="headerlink" title="Inference for Probabilistic Dependency Graphs"></a>Inference for Probabilistic Dependency Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05580">http://arxiv.org/abs/2311.05580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orichardson/pdg-infer-uai">https://github.com/orichardson/pdg-infer-uai</a></li>
<li>paper_authors: Oliver E. Richardson, Joseph Y. Halpern, Christopher De Sa</li>
<li>for: 这篇论文旨在提出一种可行的概率依赖图（PDG）推理算法，以解决PDG推理的复杂性问题。</li>
<li>methods: 该论文使用了一种新的方法，即将PDG推理转化为一种凸优化问题（具有凸顶点约束），并使用内点方法解决这种问题。</li>
<li>results: 论文的实验结果表明，该算法可以在高速度下解决PDG推理问题，并且比基eline方法更高效。<details>
<summary>Abstract</summary>
Probabilistic dependency graphs (PDGs) are a flexible class of probabilistic graphical models, subsuming Bayesian Networks and Factor Graphs. They can also capture inconsistent beliefs, and provide a way of measuring the degree of this inconsistency. We present the first tractable inference algorithm for PDGs with discrete variables, making the asymptotic complexity of PDG inference similar that of the graphical models they generalize. The key components are: (1) the observation that, in many cases, the distribution a PDG specifies can be formulated as a convex optimization problem (with exponential cone constraints), (2) a construction that allows us to express these problems compactly for PDGs of boundeed treewidth, (3) contributions to the theory of PDGs that justify the construction, and (4) an appeal to interior point methods that can solve such problems in polynomial time. We verify the correctness and complexity of our approach, and provide an implementation of it. We then evaluate our implementation, and demonstrate that it outperforms baseline approaches. Our code is available at http://github.com/orichardson/pdg-infer-uai.
</details>
<details>
<summary>摘要</summary>
probablistic dependent graphs (PDGs) 是一种灵活的 probabilistic graphical models，包括 Bayesian Networks 和 factor graphs。它们可以捕捉不一致的信念，并提供一种度量这种不一致程度的方式。我们提出了首个可追踪的 pdg 推理算法，使得 pdg 推理的 asymptotic complexity 与它们总体化的图ical models 相似。关键组成部分包括：1. 观察到，在许多情况下，pdg  specify 的分布可以表示为一个 convex 优化问题（with exponential cone constraints）。2. 一种可以紧凑表述这些问题的 pdg 的建构。3. PDGs 的理论基础， justify 这种建构。4. appeal 到 interior point methods，可以在 polynomial time 内解决这些问题。我们验证了我们的方法的正确性和复杂度，并提供了一个实现。然后，我们评估了我们的实现，并证明它在比基eline approach 更高效。我们的代码可以在 http://github.com/orichardson/pdg-infer-uai 上获取。
</details></li>
</ul>
<hr>
<h2 id="Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning"><a href="#Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning" class="headerlink" title="Removing RLHF Protections in GPT-4 via Fine-Tuning"></a>Removing RLHF Protections in GPT-4 via Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05553">http://arxiv.org/abs/2311.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang</li>
<li>for: 本研究旨在检测和维护语言模型（LLM）的可用性和安全性，尤其是RLHF保护机制在强大模型上的效果。</li>
<li>methods: 研究人员使用了RLHF和人工反馈来减少LLM的可能性性，并在强大模型上进行了精细调整。</li>
<li>results: 研究发现，即使使用强大模型进行精细调整，RLHF保护机制仍然可以被攻击者绕过，仅需340个示例和95%的成功率。此外，研究还发现，移除RLHF保护不会减少模型的用用性。<details>
<summary>Abstract</summary>
As large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks.   In this work, we show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. We further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that our fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Our results show the need for further research on protections on LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的能力不断提高，同时其可能性也随之增加。为了减少有害输出，LLM生产者和销售商通过人工反馈学习（RLHF）来减少有害输出。同时，LLM生产者也在不断增加其最强大的模型的微调能力。然而，同时的工作表明，微调可以 removals RLHF保护。我们可能会预计最强大的模型目前可用（GPT-4） less susceptible to fine-tuning attacks。  在这种工作中，我们显示相反：微调允许攻击者从RLHF保护中除掉95%的成功率和只需要340个示例。这些训练示例可以通过弱化模型自动生成。我们还证明了从RLHF保护中除掉不会降低非防止输出的用用性，这是我们的微调策略不会降低用用性，即使使用弱化模型来生成训练数据。我们的结果表明需要进一步研究LLMs的保护。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Quantum-Reinforcement-Learning-using-Evolutionary-Optimization"><a href="#Multi-Agent-Quantum-Reinforcement-Learning-using-Evolutionary-Optimization" class="headerlink" title="Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization"></a>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05546">http://arxiv.org/abs/2311.05546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien</li>
<li>for: 这篇论文旨在提出一种基于量子机制的多体强化学习方法，以提高智能驾驶和智能工业应用中的自动驾驶能力。</li>
<li>methods: 该方法基于现有的量子强化学习方法，使用演化优化方法和量子循环网络来解决多体强化学习问题。</li>
<li>results: 在币游戏环境中评估了多种量子强化学习方法，并与经典方法进行比较。结果显示，使用量子循环网络和演化优化方法可以获得更好的性能，并且使用97.88% fewer parameters than a larger neural network。<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon a existing approach for gradient free Quantum Reinforcement Learning and propose tree approaches with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our approach in the Coin Game environment and compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount of trainable parameters. Compared to the larger neural network, our approaches archive similar results using $97.88\%$ less parameters.
</details>
<details>
<summary>摘要</summary>
多智能体强化学习在智能汽车和其他智能工业应用中日益重要。同时，一种有前途的新方法在量子力学的自然属性基础上实现强化学习，减少模型可训练参数的数量。然而，使用梯度基本的多智能体量子强化学习方法经常陷入恒平面，使其与经典方法相比表现不佳。我们基于现有的梯度自由量子强化学习方法，并提出了三种使用变量量子电路的多智能体强化学习方法，使用进化优化。我们在硬币游戏环境中评估了我们的方法，并与经典方法进行比较。我们发现，我们的变量量子电路方法在与同量 Parameters 的神经网络相比，表现出了显著更好的性能。相比之下，我们的方法使用了 $97.88\%$  fewer parameters。
</details></li>
</ul>
<hr>
<h2 id="From-Learning-Management-System-to-Affective-Tutoring-system-a-preliminary-study"><a href="#From-Learning-Management-System-to-Affective-Tutoring-system-a-preliminary-study" class="headerlink" title="From Learning Management System to Affective Tutoring system: a preliminary study"></a>From Learning Management System to Affective Tutoring system: a preliminary study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05513">http://arxiv.org/abs/2311.05513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadaud Edouard, Geoffroy Thibault, Khelifi Tesnim, Yaacoub Antoun, Haidar Siba, Ben Rabah NourhÈne, Aubin Jean Pierre, Prevost Lionel, Le Grand Benedicte</li>
<li>for: 本研究旨在探讨学生学习过程中情感的作用，并结合表现、行为参与度和情感参与度指标，以 diferenciate 高 achieving 和 low achieving 学生。</li>
<li>methods: 本研究使用了两种主要数据源：学生学习管理系统（LMS）中的数字 traces，以及学生的 webcam 捕捉到的图像。数字 traces 提供了学生与教育内容的交互情况的信息，而图像则用于分析学生的情感表达。</li>
<li>results: 对于法国工程师学校2022-2023学年度学生的实际数据进行分析，我们发现了正面情感状态与学术成绩的相关性。这些初步结果支持情感在分 differentiate 高 achieving 和 low achieving 学生中扮演的重要角色。<details>
<summary>Abstract</summary>
In this study, we investigate the combination of indicators, including performance, behavioral engagement, and emotional engagement, to identify students experiencing difficulties. We analyzed data from two primary sources: digital traces extracted from th e Learning Management System (LMS) and images captured by students' webcams. The digital traces provided insights into students' interactions with the educational content, while the images were utilized to analyze their emotional expressions during learnin g activities. By utilizing real data collected from students at a French engineering school, recorded during the 2022 2023 academic year, we observed a correlation between positive emotional states and improved academic outcomes. These preliminary findings support the notion that emotions play a crucial role in differentiating between high achieving and low achieving students.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了学生表现、行为参与度和情感参与度的组合，以Identify students experiencing difficulties。我们分析了两个主要数据源：学生学习管理系统（LMS）中的数字痕迹和学生的webcam捕捉到的图像。数字痕迹提供了学生与教育内容之间的互动的信息，而图像则用于分析学生学习活动中的情感表达。通过使用2022-2023学年法国工程学院的实际数据，我们发现了正面情感状态与学术成绩的相关性。这些初步发现支持情感在区分高performing和低performing学生中发挥重要作用的观点。
</details></li>
</ul>
<hr>
<h2 id="Anytime-Constrained-Reinforcement-Learning"><a href="#Anytime-Constrained-Reinforcement-Learning" class="headerlink" title="Anytime-Constrained Reinforcement Learning"></a>Anytime-Constrained Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05511">http://arxiv.org/abs/2311.05511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy McMahan, Xiaojin Zhu</li>
<li>for: 研究受限Markov决策过程（cMDP）中的时间约束。</li>
<li>methods: 使用优化的决策策略和累积成本来解决问题。</li>
<li>results: 提出了一种可以快速计划和学习的方法，但计算非质量优化策略是NP困难的。<details>
<summary>Abstract</summary>
We introduce and study constrained Markov Decision Processes (cMDPs) with anytime constraints. An anytime constraint requires the agent to never violate its budget at any point in time, almost surely. Although Markovian policies are no longer sufficient, we show that there exist optimal deterministic policies augmented with cumulative costs. In fact, we present a fixed-parameter tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our reduction yields planning and learning algorithms that are time and sample-efficient for tabular cMDPs so long as the precision of the costs is logarithmic in the size of the cMDP. However, we also show that computing non-trivial approximately optimal policies is NP-hard in general. To circumvent this bottleneck, we design provable approximation algorithms that efficiently compute or learn an approximately feasible policy with optimal value so long as the maximum supported cost is bounded by a polynomial in the cMDP or by the absolute budget. Given our hardness results, our approximation guarantees are the best possible in terms of tractability under worst-case analysis.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究带时间约束的马尔可夫决策过程（cMDP）。任何时间约束都要求代理人从不抵触其预算的任何时刻中准确满足约束。虽然马尔可夫策略不再充分，我们表明存在最优的决策策略，并且这些策略可以增加累积成本。事实上，我们提供一种可靠减少 anytime-constrained cMDP 到无约束 MDP 的减少，这种减少使得计划和学习算法在 tabular cMDP 中时间和样本效率很高，只要 costs 的精度是 logarithmic 在 cMDP 的大小上。然而，我们也证明计算非rivially 优化策略是 NP-hard 的一般情况。为了缺口这个瓶颈，我们设计了可证明的approximation算法，可以有效地计算或学习一个 approximately 可行策略，并且这个策略的价值与 cMDP 中最大支持的成本相同。根据我们的困难性结果，我们的approximation 保证是worst-case 分析下最佳的。
</details></li>
</ul>
<hr>
<h2 id="General-Policies-Subgoal-Structure-and-Planning-Width"><a href="#General-Policies-Subgoal-Structure-and-Planning-Width" class="headerlink" title="General Policies, Subgoal Structure, and Planning Width"></a>General Policies, Subgoal Structure, and Planning Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05490">http://arxiv.org/abs/2311.05490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blai Bonet, Hector Geffner</li>
<li>for: 本文研究了类别规划问题，特别是在 atomic goals 下的情况。</li>
<li>methods: 本文使用了 IW 探索算法，该算法在问题宽度是 bounded 时可以达到 polynomial 时间复杂度。此外，文章还定义了 serializations 和 serialized width 概念，以及一种基于这些概念的 Serialized IW 算法。</li>
<li>results: 文章表明了 bounded width 问题可以通过使用 general optimal policies 和 sketches 来解决。此外，文章还提出了一种用于编码域控制知识的简洁表述语言。<details>
<summary>Abstract</summary>
It has been observed that many classical planning domains with atomic goals can be solved by means of a simple polynomial exploration procedure, called IW, that runs in time exponential in the problem width, which in these cases is bounded and small. Yet, while the notion of width has become part of state-of-the-art planning algorithms such as BFWS, there is no good explanation for why so many benchmark domains have bounded width when atomic goals are considered. In this work, we address this question by relating bounded width with the existence of general optimal policies that in each planning instance are represented by tuples of atoms of bounded size. We also define the notions of (explicit) serializations and serialized width that have a broader scope as many domains have a bounded serialized width but no bounded width. Such problems are solved non-optimally in polynomial time by a suitable variant of the Serialized IW algorithm. Finally, the language of general policies and the semantics of serializations are combined to yield a simple, meaningful, and expressive language for specifying serializations in compact form in the form of sketches, which can be used for encoding domain control knowledge by hand or for learning it from small examples. Sketches express general problem decompositions in terms of subgoals, and sketches of bounded width express problem decompositions that can be solved in polynomial time.
</details>
<details>
<summary>摘要</summary>
很多古典规划领域的目标可以通过一种简单的多项式探索过程（IW）来解决，该过程在问题宽度上呈指数增长。然而，尚未有良好的解释为何许多benchmark领域有约束的宽度，当 atomic goals 被考虑时。在这种工作中，我们解释这个问题，并证明了约束的宽度与总优规划策略的存在相关。我们还定义了（显式）序列化和序列化宽度这些概念，它们在许多领域有约束宽度，但没有约束。这些问题可以使用适当的序列化IW算法进行非优化的几何时间解决。最后，我们将总规划策略的语言和序列化的语义结合起来，得到了一种简单、有意义和表达力强的语言，用于在 компакт形式中编码领域控制知识，或者从小示例中学习。这些sketches表示了一些通用的问题分解，并且sketches的宽度约束表示可以在几何时间内解决的问题分解。
</details></li>
</ul>
<hr>
<h2 id="meta4-semantically-aligned-generation-of-metaphoric-gestures-using-self-supervised-text-and-speech-representation"><a href="#meta4-semantically-aligned-generation-of-metaphoric-gestures-using-self-supervised-text-and-speech-representation" class="headerlink" title="meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation"></a>meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05481">http://arxiv.org/abs/2311.05481</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mireillefares/meta4">https://github.com/mireillefares/meta4</a></li>
<li>paper_authors: Mireille Fares, Catherine Pelachaud, Nicolas Obin</li>
<li>for: 本研究旨在将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，以实现更加自然和生动的人工智能交互。</li>
<li>methods: 本研究使用深度学习方法，将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，以生成基于对话的抽象概念和图像概念结构的动作。</li>
<li>results: 本研究的结果显示，将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，可以实现更加自然和生动的人工智能交互，并且可以增加虚拟代理人的表现自然性和沟通效果。<details>
<summary>Abstract</summary>
Image Schemas are repetitive cognitive patterns that influence the way we conceptualize and reason about various concepts present in speech. These patterns are deeply embedded within our cognitive processes and are reflected in our bodily expressions including gestures. Particularly, metaphoric gestures possess essential characteristics and semantic meanings that align with Image Schemas, to visually represent abstract concepts. The shape and form of gestures can convey abstract concepts, such as extending the forearm and hand or tracing a line with hand movements to visually represent the image schema of PATH. Previous behavior generation models have primarily focused on utilizing speech (acoustic features and text) to drive the generation model of virtual agents. They have not considered key semantic information as those carried by Image Schemas to effectively generate metaphoric gestures. To address this limitation, we introduce META4, a deep learning approach that generates metaphoric gestures from both speech and Image Schemas. Our approach has two primary goals: computing Image Schemas from input text to capture the underlying semantic and metaphorical meaning, and generating metaphoric gestures driven by speech and the computed image schemas. Our approach is the first method for generating speech driven metaphoric gestures while leveraging the potential of Image Schemas. We demonstrate the effectiveness of our approach and highlight the importance of both speech and image schemas in modeling metaphoric gestures.
</details>
<details>
<summary>摘要</summary>
Image Schemas 是人类认知过程中的重复性Pattern，它们影响了我们对不同概念的概念化和逻辑理解。这些模式深嵌在我们认知过程中，并在我们的身体表达中反映出来，包括手势。特别是元associal gesture具有essential characteristics和semantic meaning，可以用来视觉表示概念。例如，扩展forearm和手或使用手部运动 tracing 一条线来视觉表示PATH的图像模式。在虚拟代理模型中，以前的行为生成模型主要基于了speech（声音特征和文本）来驱动行为生成模型。它们没有考虑了关键的semantic信息，例如图像模式，以便生成元associal gesture。为了解决这一限制，我们介绍META4，一种深度学习方法，可以从speech和图像模式中生成元associal gesture。我们的方法有两个主要目标：计算图像模式从输入文本中，以捕捉下面的semantic和metaforical meaning，并使用speech和计算的图像模式来驱动元associal gesture的生成。我们的方法是首个基于speech和图像模式的元associal gesture生成方法。我们 demonstate了我们的方法的有效性，并强调了speech和图像模式在元associal gesture模型中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Text-Representation-Distillation-via-Information-Bottleneck-Principle"><a href="#Text-Representation-Distillation-via-Information-Bottleneck-Principle" class="headerlink" title="Text Representation Distillation via Information Bottleneck Principle"></a>Text Representation Distillation via Information Bottleneck Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05472">http://arxiv.org/abs/2311.05472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhao Zhang, Dingkun Long, Zehan Li, Pengjun Xie</li>
<li>for: 提高文本表示领域中 PLMs 的可行性</li>
<li>methods: 提议一种基于信息瓶颈原理的 Knowledge Distillation 方法，即 IBKD，以提高学生模型的表示能力</li>
<li>results: 两个主要下游应用任务（Semantic Textual Similarity 和 Dense Retrieval）的实验结果表明，我们的提议方法有效地提高了学生模型的表示能力<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This approach is motivated by the Information Bottleneck principle and aims to maximize the mutual information between the final representation of the teacher and student model, while simultaneously reducing the mutual information between the student model's representation and the input data. This enables the student model to preserve important learned information while avoiding unnecessary information, thus reducing the risk of over-fitting. Empirical studies on two main downstream applications of text representation (Semantic Textual Similarity and Dense Retrieval tasks) demonstrate the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cognitively-Inspired-Components-for-Social-Conversational-Agents"><a href="#Cognitively-Inspired-Components-for-Social-Conversational-Agents" class="headerlink" title="Cognitively Inspired Components for Social Conversational Agents"></a>Cognitively Inspired Components for Social Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05450">http://arxiv.org/abs/2311.05450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Clay, Eduardo Alonso, Esther Mondragón</li>
<li>for: This paper aims to address two key problems in current conversational agents (CAs): technical issues and social expectations.</li>
<li>methods: The proposed solution is to incorporate cognitive functions, such as semantic and episodic memory, emotion, working memory, and learning, into CAs.</li>
<li>results: The introduction of these cognitive functions can improve the technical performance of CAs and enhance their ability to meet social expectations, leading to more effective and satisfying interactions with users.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决当前对话代理人（CA）中的两个主要问题：技术问题和社会预期。</li>
<li>methods: 提议的解决方案是通过在CA中引入认知功能，如semantic和episodic记忆、情感、工作记忆和学习，来解决技术问题和社会预期。</li>
<li>results: 通过引入这些认知功能，CA可以提高技术性能和满足用户的社会预期，从而实现更有效率和满意的用户交互。<details>
<summary>Abstract</summary>
Current conversational agents (CA) have seen improvement in conversational quality in recent years due to the influence of large language models (LLMs) like GPT3. However, two key categories of problem remain. Firstly there are the unique technical problems resulting from the approach taken in creating the CA, such as scope with retrieval agents and the often nonsensical answers of former generative agents. Secondly, humans perceive CAs as social actors, and as a result expect the CA to adhere to social convention. Failure on the part of the CA in this respect can lead to a poor interaction and even the perception of threat by the user. As such, this paper presents a survey highlighting a potential solution to both categories of problem through the introduction of cognitively inspired additions to the CA. Through computational facsimiles of semantic and episodic memory, emotion, working memory, and the ability to learn, it is possible to address both the technical and social problems encountered by CAs.
</details>
<details>
<summary>摘要</summary>
当前的对话代理人（CA）在过去几年中得到了大语言模型（LLM）如GPT3的影响，从而提高了对话质量。然而，两大类问题仍然存在。首先，创建CA时采取的方法会导致特定的技术问题，如检索代理人的范围和前一代生成代理人的偶极答案。其次，人们对CA视为社会actor，因此对CA的交互预期会遵循社会规范。如果CA不符合这些规范，可能会导致交互不佳并且用户可能会感到威胁。因此，本文提出了一项调查，探讨通过在CA中引入认知发展的方法来解决这两类问题。通过计算机的semantic和episodic记忆、情感、工作记忆和学习能力，可以解决CA中的技术和社会问题。
</details></li>
</ul>
<hr>
<h2 id="LLaVA-Plus-Learning-to-Use-Tools-for-Creating-Multimodal-Agents"><a href="#LLaVA-Plus-Learning-to-Use-Tools-for-Creating-Multimodal-Agents" class="headerlink" title="LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"></a>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05437">http://arxiv.org/abs/2311.05437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li</li>
<li>for: 本研究旨在提供一个通用多模态助手，可以扩展大型多模态模型的功能。</li>
<li>methods: 本研究使用了多模态指令遵循数据来学习使用工具，包括视觉理解、生成、外部知识检索和组合。</li>
<li>results: 实验结果显示，LLaVA-Plus在现有能力和新能力方面都有优于LLaVA的表现，并且可以直接将图像查询引入人机交互会话中，显著提高工具使用性能和开拓新enario。<details>
<summary>Abstract</summary>
LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.
</details>
<details>
<summary>摘要</summary>
LLaVA-Plus 是一种通用多模态助手，它可以扩展大型多模态模型的功能。它保持一个拥有预训练视觉和视觉语言模型的技能库，可以根据用户输入活化相关的工具来完成现实世界任务。LLaVA-Plus 通过多modal instruction-following 数据训练获得了使用工具的能力，包括视觉理解、生成、外部知识检索和组合。实验结果表明，LLaVA-Plus 在现有能力和新的能力方面都超越 LLVA，而且图像查询直接围绕整个人机器人交互会话活动地参与，有效地提高工具使用性能，并启用新的应用场景。
</details></li>
</ul>
<hr>
<h2 id="Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks"><a href="#Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks" class="headerlink" title="Mirror: A Universal Framework for Various Information Extraction Tasks"></a>Mirror: A Universal Framework for Various Information Extraction Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05419">http://arxiv.org/abs/2311.05419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Spico197/Mirror">https://github.com/Spico197/Mirror</a></li>
<li>paper_authors: Tong Zhu, Junfei Ren, Zijian Yu, Mengsong Wu, Guoliang Zhang, Xiaoye Qu, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Min Zhang</li>
<li>for: 这篇论文的目的是解决信息EXTRACTION任务之间的知识共享问题，因为数据格式和任务变化很多，导致信息浪费和复杂应用程序建设困难。</li>
<li>methods: 作者提出了一种 универсальный框架，称为 Mirror，用于不同的信息EXTRACTION任务。该框架将IE任务重新定义为多槽循环图像EXTRACTION问题，并开发了一种非自动回归图像解码算法来提取所有槽。</li>
<li>results: 实验结果表明，作者的模型在几个下游任务中具有妥协性和竞争性性能，并在几个零批和几个零批设置下达到或超过了现有系统的性能。<details>
<summary>Abstract</summary>
Sharing knowledge between information extraction tasks has always been a challenge due to the diverse data formats and task variations. Meanwhile, this divergence leads to information waste and increases difficulties in building complex applications in real scenarios. Recent studies often formulate IE tasks as a triplet extraction problem. However, such a paradigm does not support multi-span and n-ary extraction, leading to weak versatility. To this end, we reorganize IE problems into unified multi-slot tuples and propose a universal framework for various IE tasks, namely Mirror. Specifically, we recast existing IE tasks as a multi-span cyclic graph extraction problem and devise a non-autoregressive graph decoding algorithm to extract all spans in a single step. It is worth noting that this graph structure is incredibly versatile, and it supports not only complex IE tasks, but also machine reading comprehension and classification tasks. We manually construct a corpus containing 57 datasets for model pretraining, and conduct experiments on 30 datasets across 8 downstream tasks. The experimental results demonstrate that our model has decent compatibility and outperforms or reaches competitive performance with SOTA systems under few-shot and zero-shot settings. The code, model weights, and pretraining corpus are available at https://github.com/Spico197/Mirror .
</details>
<details>
<summary>摘要</summary>
共享知识 между信息提取任务总是是一个挑战，因为数据格式和任务的变化很大。这种多样性导致信息浪费，并使构建实际场景中的复杂应用程序更加困难。现在的研究常将IE任务形式为 triplet 提取问题。然而，这种 парадиг不支持多 Span 和 n-ary 提取，导致其 versatility 强度不够。为此，我们重新组织IE问题为多槽图形提取问题，并提出一个通用的IE框架，即 Mirror。我们将现有的IE任务转化为多Span逆波峰图提取问题，并设计了一种非自动回归图解码算法来提取所有槽峰。值得注意的是，这个图结构非常灵活，不仅支持复杂的IE任务，还支持机器阅读理解和分类任务。我们手动构建了一个包含57个数据集的预训练集，并在30个数据集上进行了8个下游任务的实验。实验结果显示，我们的模型具有不错的兼容性，并在少量和零量设置下超过或达到了现有系统的性能。模型权重、预训练集和实验结果可以在 GitHub 上 obt 取到：https://github.com/Spico197/Mirror。
</details></li>
</ul>
<hr>
<h2 id="Generalization-in-medical-AI-a-perspective-on-developing-scalable-models"><a href="#Generalization-in-medical-AI-a-perspective-on-developing-scalable-models" class="headerlink" title="Generalization in medical AI: a perspective on developing scalable models"></a>Generalization in medical AI: a perspective on developing scalable models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05418">http://arxiv.org/abs/2311.05418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joachim A. Behar, Jeremy Levy, Leo Anthony Celi</li>
<li>for: 本研究目的是为了探讨医疗AI模型在不同医院环境中的应用，以及该模型对不同医院的应用情况下的性能。</li>
<li>methods: 本研究使用了多个数据集，包括开发模型使用的源数据集和评估模型性能的目标数据集。另外，研究还使用了一个三级阶层网络来评估模型在不同医院环境中的应用情况。</li>
<li>results: 研究发现，虽然使用多个数据集可以提高医疗AI模型的一般化能力，但是在实际应用中仍然存在着问题。具体来说，模型在不同医院环境中的性能差异较大，这表明了医疗AI模型的一般化能力仍然有很大的提升空间。<details>
<summary>Abstract</summary>
Over the past few years, research has witnessed the advancement of deep learning models trained on large datasets, some even encompassing millions of examples. While these impressive performance on their hidden test sets, they often underperform when assessed on external datasets. Recognizing the critical role of generalization in medical AI development, many prestigious journals now require reporting results both on the local hidden test set as well as on external datasets before considering a study for publication. Effectively, the field of medical AI has transitioned from the traditional usage of a single dataset that is split into train and test to a more comprehensive framework using multiple datasets, some of which are used for model development (source domain) and others for testing (target domains). However, this new experimental setting does not necessarily resolve the challenge of generalization. This is because of the variability encountered in intended use and specificities across hospital cultures making the idea of universally generalizable systems a myth. On the other hand, the systematic, and a fortiori recurrent re-calibration, of models at the individual hospital level, although ideal, may be overoptimistic given the legal, regulatory and technical challenges that are involved. Re-calibration using transfer learning may not even be possible in some instances where reference labels of target domains are not available. In this perspective we establish a hierarchical three-level scale system reflecting the generalization level of a medical AI algorithm. This scale better reflects the diversity of real-world medical scenarios per which target domain data for re-calibration of models may or not be available and if it is, may or not have reference labels systematically available.
</details>
<details>
<summary>摘要</summary>
Traditionally, the field of medical AI used a single dataset split into train and test sets. However, this approach has been replaced by a more comprehensive framework that uses multiple datasets, some for model development (source domains) and others for testing (target domains). Despite this change, the challenge of generalization remains. This is because of the variability encountered in real-world medical scenarios, making it difficult to achieve universally generalizable systems.To address this challenge, we propose a hierarchical three-level scale system to reflect the generalization level of medical AI algorithms. This scale takes into account the diversity of real-world medical scenarios, where target domain data for model re-calibration may or may not be available, and if it is, may or may not have reference labels systematically available. This approach provides a more realistic assessment of model performance and helps to identify areas where improvement is needed.In conclusion, the field of medical AI has transitioned from using a single dataset to a more comprehensive framework that incorporates multiple datasets. However, the challenge of generalization remains, and a systematic approach to re-calibration is necessary to achieve universally generalizable systems. Our proposed hierarchical scale system provides a better reflection of the diversity of real-world medical scenarios and helps to identify areas where improvement is needed.
</details></li>
</ul>
<hr>
<h2 id="TencentLLMEval-A-Hierarchical-Evaluation-of-Real-World-Capabilities-for-Human-Aligned-LLMs"><a href="#TencentLLMEval-A-Hierarchical-Evaluation-of-Real-World-Capabilities-for-Human-Aligned-LLMs" class="headerlink" title="TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs"></a>TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05374">http://arxiv.org/abs/2311.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyi Xie, Wenlin Yao, Yong Dai, Shaobo Wang, Donlin Zhou, Lifeng Jin, Xinhua Feng, Pengzhi Wei, Yujie Lin, Zhichao Hu, Dong Yu, Zhengyou Zhang, Jing Nie, Yuhong Liu</li>
<li>For: The paper aims to evaluate the proficiency of large language models (LLMs) in following human instructions on diverse real-world tasks, and to provide a standardized methodology for benchmarking the performance of LLMs.* Methods: The paper proposes a comprehensive human evaluation framework that includes a hierarchical task tree with over 200 categories and 800 tasks, as well as detailed evaluation standards and processes to facilitate consistent and unbiased judgments from human evaluators.* Results: The paper releases a test set of over 3,000 instances that span different difficulty levels and knowledge domains, and analyzes the feasibility of automating parts of evaluation with a strong LLM (GPT-4). The framework is demonstrated to be effective in assessing the performance of Tencent Hunyuan LLMs, and is made publicly available for use in evaluating other LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive capabilities across various natural language tasks. However, evaluating their alignment with human preferences remains a challenge. To this end, we propose a comprehensive human evaluation framework to assess LLMs' proficiency in following instructions on diverse real-world tasks. We construct a hierarchical task tree encompassing 7 major areas covering over 200 categories and over 800 tasks, which covers diverse capabilities such as question answering, reasoning, multiturn dialogue, and text generation, to evaluate LLMs in a comprehensive and in-depth manner. We also design detailed evaluation standards and processes to facilitate consistent, unbiased judgments from human evaluators. A test set of over 3,000 instances is released, spanning different difficulty levels and knowledge domains. Our work provides a standardized methodology to evaluate human alignment in LLMs for both English and Chinese. We also analyze the feasibility of automating parts of evaluation with a strong LLM (GPT-4). Our framework supports a thorough assessment of LLMs as they are integrated into real-world applications. We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to facilitate the benchmarking of advances in the development of safe and human-aligned LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经展示出色的能力 across 多种自然语言任务。然而，评估它们与人类偏好的Alignment remainschallenge。为此，我们提出了一个完整的人类评估框架，用于评估 LLM 在多种真实世界任务中的执行能力。我们构建了一个层次任务树，覆盖 7 个主要领域，涵盖了200多个类别和800多个任务，这些任务包括问答、推理、多回交流、文本生成等，以评估 LLM 的多方面能力。我们还设计了详细的评估标准和过程，以便确保人类评估员的公正、不偏袋。我们发布了3000多个实例的测试集，覆盖不同的difficulty Level和知识领域。我们的工作提供了一种标准化的方法ологи，用于评估 LLM 的人类Alignment，并且分析了使用强大 LLM（GPT-4）自动化评估的可能性。我们的框架支持了LLM 在真实应用中的全面评估，并且我们已经将任务树、TencentLLMEval数据集和评估方法ологи公开发布。通过这些，我们希望能够促进LLM的开发，并确保其安全和人类Alignment。
</details></li>
</ul>
<hr>
<h2 id="Training-Robust-Deep-Physiological-Measurement-Models-with-Synthetic-Video-based-Data"><a href="#Training-Robust-Deep-Physiological-Measurement-Models-with-Synthetic-Video-based-Data" class="headerlink" title="Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data"></a>Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05371">http://arxiv.org/abs/2311.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Xin Liu</li>
<li>for: 这 paper 的目的是提出一种方法，使用synthetic video-based datasets来提高深度学习模型对人体生物信息的识别性。</li>
<li>methods: 这 paper 使用了一种combined augmentation方法，将真实的世界噪音添加到了synthetic physiological signals和对应的facial videos中。</li>
<li>results: 经过实验， authors 发现可以将average MAE从6.9降低到2.0。<details>
<summary>Abstract</summary>
Recent advances in supervised deep learning techniques have demonstrated the possibility to remotely measure human physiological vital signs (e.g., photoplethysmograph, heart rate) just from facial videos. However, the performance of these methods heavily relies on the availability and diversity of real labeled data. Yet, collecting large-scale real-world data with high-quality labels is typically challenging and resource intensive, which also raises privacy concerns when storing personal bio-metric data. Synthetic video-based datasets (e.g., SCAMPS~\cite{mcduff2022scamps}) with photo-realistic synthesized avatars are introduced to alleviate the issues while providing high-quality synthetic data. However, there exists a significant gap between synthetic and real-world data, which hinders the generalization of neural models trained on these synthetic datasets. In this paper, we proposed several measures to add real-world noise to synthetic physiological signals and corresponding facial videos. We experimented with individual and combined augmentation methods and evaluated our framework on three public real-world datasets. Our results show that we were able to reduce the average MAE from 6.9 to 2.0.
</details>
<details>
<summary>摘要</summary>
In this paper, we proposed several measures to add real-world noise to synthetic physiological signals and corresponding facial videos. We experimented with individual and combined augmentation methods and evaluated our framework on three public real-world datasets. Our results show that we were able to reduce the average mean absolute error (MAE) from 6.9 to 2.0.
</details></li>
</ul>
<hr>
<h2 id="On-the-Road-with-GPT-4V-ision-Early-Explorations-of-Visual-Language-Model-on-Autonomous-Driving"><a href="#On-the-Road-with-GPT-4V-ision-Early-Explorations-of-Visual-Language-Model-on-Autonomous-Driving" class="headerlink" title="On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving"></a>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05332">http://arxiv.org/abs/2311.05332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlab-adg/gpt4v-ad-exploration">https://github.com/pjlab-adg/gpt4v-ad-exploration</a></li>
<li>paper_authors: Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, Botian Shi<br>for:* The paper is focused on the development and evaluation of a Visual Language Model (VLM) for fully autonomous vehicle driving.methods:* The VLM used in the paper is called \modelnamefull, and it utilizes a transformer-based architecture to understand and reason about driving scenes.* The paper explores the model’s abilities in various tasks, including scene recognition, causal reasoning, and real-time decision-making.results:* The paper shows that \modelname demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems.* The model is able to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts.* However, the paper also highlights some challenges and limitations, such as direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks.Here is the information in Simplified Chinese text:for:* 这篇论文关注完全自动驾驶技术的发展，特别是通过Visual Language Model（VLM）实现自动驾驶。methods:* 这篇论文使用的VLM是\modelnamefull，它采用转换器基 Architecture来理解和解释驾驶场景。* 论文探讨了模型在不同任务中的能力，包括场景识别、 causal reasoning 和实时决策。results:* 论文显示\modelname在场景理解和 causal reasoning 方面表现出色，超越了现有的自动驾驶系统。* 模型能够处理不同的驾驶场景，认出意图，并在真正的驾驶上下进行了有知识的决策。* 然而，论文还指出了一些挑战和限制，如方向识别、交通灯识别、视觉固定和空间理解任务。<details>
<summary>Abstract</summary>
The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, \modelnamefull, and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that \modelname demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
</details>
<details>
<summary>摘要</summary>
自动驾驶技术的实现受到了复杂的感知、决策和控制系统的高级 интеграción。传统的方法，包括数据驱动和规则驱动，由于无法捕捉复杂的驾驶环境和其他道路用户的意图，受到了 significiant bottleneck，特别是在实现完全自动驾驶时。随着视觉语言模型（VLM）的出现，自动驾驶技术正在新的前iers中进行实现。本报告对最新的state-of-the-art VLM，\modelnamefull，进行了广泛的评估，并在自动驾驶场景中应用其。我们 investigate了模型对驾驶场景的理解和决策的能力，以及在不同条件下做出有 Informed 决策。我们的全面测试包括基本场景认识、复杂的 causal reasoning 和实时决策。我们的发现表明，\modelname在场景理解和 causal reasoning 方面表现出色，比现有自动驾驶系统更加出色。它还可以在真实驾驶场景中处理异常情况，认出意图，并做出有 Informed 决策。然而，还有一些挑战，包括方向识别、交通灯识别、视觉基础 task 和空间理解任务。这些限制表明需要进一步的研究和发展。项目现在在 GitHub 上可以访问和使用： \url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}。
</details></li>
</ul>
<hr>
<h2 id="ABIGX-A-Unified-Framework-for-eXplainable-Fault-Detection-and-Classification"><a href="#ABIGX-A-Unified-Framework-for-eXplainable-Fault-Detection-and-Classification" class="headerlink" title="ABIGX: A Unified Framework for eXplainable Fault Detection and Classification"></a>ABIGX: A Unified Framework for eXplainable Fault Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05316">http://arxiv.org/abs/2311.05316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Zhuo, Jinchuan Qian, Zhihuan Song, Zhiqiang Ge</li>
<li>for: 本研究は、可解解釈的故障检测和分类（FDC）に関するものです。</li>
<li>methods: 本研究では、新しい整合架构ABIGX（Adversarial fault reconstruction-Based Integrated Gradient eXplanation）を提案しています。ABIGXは、先行する成功的故障诊断方法の基本要素を含むことで、変数贡献を提供する最初の解釈フレームワークです。コア部分は、敌的な故障再现（AFR）法です。</li>
<li>results: 本研究では、ABIGXは、既存のGradient-based解釈方法に対して优れていることを证明しています。また、故障分类の问题である故障分 classe smearingを解决することにも成功しています。<details>
<summary>Abstract</summary>
For explainable fault detection and classification (FDC), this paper proposes a unified framework, ABIGX (Adversarial fault reconstruction-Based Integrated Gradient eXplanation). ABIGX is derived from the essentials of previous successful fault diagnosis methods, contribution plots (CP) and reconstruction-based contribution (RBC). It is the first explanation framework that provides variable contributions for the general FDC models. The core part of ABIGX is the adversarial fault reconstruction (AFR) method, which rethinks the FR from the perspective of adversarial attack and generalizes to fault classification models with a new fault index. For fault classification, we put forward a new problem of fault class smearing, which intrinsically hinders the correct explanation. We prove that ABIGX effectively mitigates this problem and outperforms the existing gradient-based explanation methods. For fault detection, we theoretically bridge ABIGX with conventional fault diagnosis methods by proving that CP and RBC are the linear specifications of ABIGX. The experiments evaluate the explanations of FDC by quantitative metrics and intuitive illustrations, the results of which show the general superiority of ABIGX to other advanced explanation methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于可解释的故障检测和分类（FDC），这篇论文提出了一个统一框架，即ABIGX（敌对故障重建基于集成导数解释）。 ABIGX 基于过去成功的故障诊断方法的基本元素，包括贡献图（CP）和重建基于贡献（RBC）。它是首个提供变量贡献的总体 FDC 模型解释框架。 ABIGX 的核心部分是对敌对故障重建（AFR）方法，它从敌对攻击的视角重新定义了 FR，并推广到包括新的故障指标的普通故障分类模型。为故障分类，我们提出了一个新的问题：故障分类泛滥问题，该问题内置地阻碍了正确的解释。我们证明了 ABIGX 有效地解决了这个问题，并超越了现有的导数基本解释方法。对故障检测，我们理论上将 ABIGX 与传统故障诊断方法连接，证明 CP 和 RBC 是 ABIGX 的线性特征。实验评估了 FDC 的解释，使用量化指标和直观示例，结果显示 ABIGX 在其他高级解释方法之上具有通用优势。Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need further assistance or if you would like me to make any changes.
</details></li>
</ul>
<hr>
<h2 id="Data-Valuation-and-Detections-in-Federated-Learning"><a href="#Data-Valuation-and-Detections-in-Federated-Learning" class="headerlink" title="Data Valuation and Detections in Federated Learning"></a>Data Valuation and Detections in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05304">http://arxiv.org/abs/2311.05304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/muz1lee/motdata">https://github.com/muz1lee/motdata</a></li>
<li>paper_authors: Wenqian Li, Shuran Fu, Fengrui Zhang, Yan Pang</li>
<li>for: 本研究旨在提出一种基于 Wasserstein 距离的隐私保护技术，以帮助在 Federated Learning 中评估客户端的贡献和选择相关数据样本，而不需要先知道训练算法。</li>
<li>methods: 本文提出的 FedBary 方法在 Federated Learning 中使用 Wasserstein 距离来评估客户端的数据贡献，并通过计算 Wasserstein 巴里中心来实现透明度和有效性的数据评估。</li>
<li>results: 经验和理论分析表明，FedBary 方法可以有效地评估客户端的数据贡献，并且可以提高 Federated Learning 的效果和透明度。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables collaborative model training without sharing raw data, demanding abundant, high-quality data for optimal model performance. Fair and efficient data evaluation is a fundamental issue for incentivizing clients to provide more high-quality data. Meanwhile, it is likely that only a subset of clients and datasets are relevant for a learning task while the rest of them may have a negative impact on the model training. This paper introduces a novel privacy-preserving method for evaluating client contributions and selecting relevant data samples without a pre-specified training algorithm. Our proposed approach, FedBary, utilizes Wasserstein distance within the federated context, offering a new pioneering solution for data valuation, which provides transparent data evaluation and efficient computation of Wasserstein barycenter to mitigate reliance on validation data. We conduct extensive empirical experiments and theoretical analysis, showing the promising research of this valuation metric.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 允许共同训练模型无需分享原始数据，但是需要大量高质量数据来保证模型性能。 Fair和有效地评估客户端提供的数据是基本问题，以奖励客户端提供更多高质量数据。然而，它可能只有一 subset of 客户端和数据集是学习任务中相关的，而其余的可能会对模型训练产生负面影响。这篇论文提出了一种新的隐私保护方法来评估客户端的贡献和选择相关数据样本，不需要预先确定的训练算法。我们提出的方法，FedBary，利用在联邦上的沃氏距离，提供了一种新的探索性的数据评估方法，可以提供透明的数据评估和有效的沃氏巴里中心计算，以减少验证数据的依赖。我们进行了广泛的实验和理论分析，证明了这种评估度量的扎实性。
</details></li>
</ul>
<hr>
<h2 id="Do-personality-tests-generalize-to-Large-Language-Models"><a href="#Do-personality-tests-generalize-to-Large-Language-Models" class="headerlink" title="Do personality tests generalize to Large Language Models?"></a>Do personality tests generalize to Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05297">http://arxiv.org/abs/2311.05297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian E. Dorner, Tom Sühr, Samira Samadi, Augustin Kelava</li>
<li>for: 这篇论文是为了评估大型自然语言模型（LLM）在文本交互中的人类样式行为而写的。</li>
<li>methods: 这篇论文使用了原始设计为人类的测试来评估LLM。</li>
<li>results: 研究发现，LLM的响应与人类测试结果存在差异，因此不能将LLM的测试结果直接应用于人类。具体来说，LLM通常会回答反编项（例如“我是内向的”vs“我是外向的”），而且不同的提问不会按照人类样本中的分化分布。因此，研究人员建议在评估LLM的测试结果之前，更加重视测试的有效性。<details>
<summary>Abstract</summary>
With large language models (LLMs) appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate various properties of these models using tests originally designed for humans. While re-using existing tests is a resource-efficient way to evaluate LLMs, careful adjustments are usually required to ensure that test results are even valid across human sub-populations. Thus, it is not clear to what extent different tests' validity generalizes to LLMs. In this work, we provide evidence that LLMs' responses to personality tests systematically deviate from typical human responses, implying that these results cannot be interpreted in the same way as human test results. Concretely, reverse-coded items (e.g. "I am introverted" vs "I am extraverted") are often both answered affirmatively by LLMs. In addition, variation across different prompts designed to "steer" LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe it is important to pay more attention to tests' validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs' "personality".
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）在文本基于交互中表现越来越人类化，因此有人尝试使用原本设计 для人类的测试来评估这些模型。 Although 重用现有测试是资源有效的方式来评估 LLM，但是需要进行精细的调整，以确保测试结果在人类子 популяции中是有效的。因此，不清楚测试的有效性是否在 LLM 上通用。在这项工作中，我们提供证据表明 LLM 对人性测试的响应与人类的响应不同，因此这些结果无法与人类测试结果进行比较。例如，倒向预期的项目（例如 "我是内向的" vs "我是外向的"）通常都会被 LLM 答案正面。此外，使用不同的提问来诱导 LLM 模拟特定的人性类型的变化不符合人类样本中的清晰分化。在这些结果的基础上，我们认为在评估 LLM 的测试有效性之前，需要更加注重测试的有效性。只有在确保测试的有效性时，才能够在 LLM 上 Draw strong conclusions about potentially ill-defined concepts like LLMs' "personality".
</details></li>
</ul>
<hr>
<h2 id="Don’t-Waste-a-Single-Annotation-Improving-Single-Label-Classifiers-Through-Soft-Labels"><a href="#Don’t-Waste-a-Single-Annotation-Improving-Single-Label-Classifiers-Through-Soft-Labels" class="headerlink" title="Don’t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels"></a>Don’t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05265">http://arxiv.org/abs/2311.05265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Wu, Yue Li, Yida Mu, Carolina Scarton, Kalina Bontcheva, Xingyi Song</li>
<li>for: 这个论文旨在解决单 Label 分类任务中的数据标注和训练方法的局限性。通常，在标注这些任务时，注释者只被要求为每个样本提供一个标签，而且纠纷不计入最终的决定。这个论文挑战了这种传统方法，因为判断合适的标签可能因数据样本中的模糊和 Context 的缺失而变得困难。</li>
<li>methods: 我们的软标签方法使用了这些模糊的注释信息进行训练。我们发现，使用注释者的信任度、次要标签和纠纷信息可以生成高质量的软标签。</li>
<li>results: 我们的实验结果表明，使用这些软标签进行训练可以提高分类器的性能和准确性。<details>
<summary>Abstract</summary>
In this paper, we address the limitations of the common data annotation and training methods for objective single-label classification tasks. Typically, when annotating such tasks annotators are only asked to provide a single label for each sample and annotator disagreement is discarded when a final hard label is decided through majority voting. We challenge this traditional approach, acknowledging that determining the appropriate label can be difficult due to the ambiguity and lack of context in the data samples. Rather than discarding the information from such ambiguous annotations, our soft label method makes use of them for training. Our findings indicate that additional annotator information, such as confidence, secondary label and disagreement, can be used to effectively generate soft labels. Training classifiers with these soft labels then leads to improved performance and calibration on the hard label test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们讨论了单标签分类任务中常见的数据标注和训练方法的局限性。通常情况下，当批注这些任务时，批注者只被要求为每个样本提供一个单一的标签，而且分类器的决定通过多数投票来决定最终的硬标签。我们挑战了传统的方法，因为判定适当的标签可以是困难的，因为数据样本中的不确定和缺失上下文。而不是抛弃这些不确定的批注信息，我们的软标签方法使用它们进行训练。我们的发现表明，更多的批注信息，如信任度、次要标签和分歧，可以有效地生成软标签。使用这些软标签进行训练后，分类器的性能和校准在硬标签测试集上得到了改善。
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Minimum-Bayes-Risk-Decoding"><a href="#Model-Based-Minimum-Bayes-Risk-Decoding" class="headerlink" title="Model-Based Minimum Bayes Risk Decoding"></a>Model-Based Minimum Bayes Risk Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05263">http://arxiv.org/abs/2311.05263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe</li>
<li>for: 这篇论文是关于最小极大风险（MBR）解码的研究，MBR解码是一种可以替代搜索解码的文本生成任务中的有力的方法。</li>
<li>methods: 这篇论文使用了两种常见的简化方法：首先，它采样了一些假设，然后对这些假设进行了采样。其次，它使用了一个Monte Carlo估计来估算每个假设的概率。然而，第二个简化方法不是必要的，因为我们通常在推断时有访问模型概率的能力。</li>
<li>results: 我们的实验表明，使用模型概率来估算假设概率的方法（MBMBR）在文本生成任务中比MBR更高效。MBMBR在encoder-decoder模型和大语言模型上都能够获得更好的性能。<details>
<summary>Abstract</summary>
Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text generation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least expected risk under a probability model according to a given utility function. Since it is impractical to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it estimates the probability of each hypothesis using a Monte Carlo estimator. While the first approximation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and empirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于最小 bayes 风险（MBR）解码已经在文本生成任务中被证明为一种强大的替代方案。 MBR 解码从一组假设中选择一个假设，该假设的风险预期最小根据给定的Utility函数。 由于不可能对所有假设进行准确的预期风险计算，常用两种近似方法。首先，它Integrate 一个采样的集合的假设，而不是所有可能的假设。其次，它使用一个Monte Carlo 估计来估计每个假设的概率。虽然第一种近似是必要的，以使其计算可能，但第二种近似并不是必要的，因为我们通常在推理时有访问模型概率的能力。我们提出了基于模型的 MBR（MBMBR），一种 MBR 的变种，它使用模型概率自己作为假设概率的估计，而不是Monte Carlo 估计。我们分析和实验表明，基于模型的估计比Monte Carlo 估计更有前途在文本生成任务中。我们的实验结果表明，MBMBR 在encoder-decoder 模型和大语言模型上都能够超过 MBR。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Wrapper-in-the-medical-domain-Establishing-transparent-uncertainty-quantification-for-opaque-machine-learning-models-in-practice"><a href="#Uncertainty-Wrapper-in-the-medical-domain-Establishing-transparent-uncertainty-quantification-for-opaque-machine-learning-models-in-practice" class="headerlink" title="Uncertainty Wrapper in the medical domain: Establishing transparent uncertainty quantification for opaque machine learning models in practice"></a>Uncertainty Wrapper in the medical domain: Establishing transparent uncertainty quantification for opaque machine learning models in practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05245">http://arxiv.org/abs/2311.05245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Jöckel, Michael Kläs, Georg Popp, Nadja Hilger, Stephan Fricke</li>
<li>for: 这个论文是为了推广数据驱动模型在机器学习（ML）中的应用，并提供一种可靠的方法来评估这些模型的结果uncertainty。</li>
<li>methods: 论文使用了一种名为“Uncertainty Wrapper”的方法，用于评估ML模型的结果uncertainty。这种方法基于 bayesian networks 和 Gaussian processes，可以提供高精度的uncertainty estimations。</li>
<li>results: 论文通过应用这种方法于流式 cél analysis 中，发现了一些可能的应用场景，并提供了一些可靠的uncertainty estimations。这些结果表明，使用Uncertainty Wrapper可以帮助用户更好地理解ML模型的结果uncertainty，并做出更 Informed 的决策。<details>
<summary>Abstract</summary>
When systems use data-based models that are based on machine learning (ML), errors in their results cannot be ruled out. This is particularly critical if it remains unclear to the user how these models arrived at their decisions and if errors can have safety-relevant consequences, as is often the case in the medical field. In such cases, the use of dependable methods to quantify the uncertainty remaining in a result allows the user to make an informed decision about further usage and draw possible conclusions based on a given result. This paper demonstrates the applicability and practical utility of the Uncertainty Wrapper using flow cytometry as an application from the medical field that can benefit from the use of ML models in conjunction with dependable and transparent uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当系统使用机器学习（ML）模型时，结果中的错误无法被排除。这 particualry critical 如果用户无法了解这些模型如何做出决策，并且错误可能有生命安全相关的后果，例如医疗领域中的案例。在这种情况下，使用可靠的方法来评估结果中剩下的不确定性，让用户可以根据给定的结果作出了 Informed 决策和绘制可能的结论。这篇论文介绍了 Uncertainty Wrapper 的可行性和实用性，并通过流式测量为医疗领域的应用，可以通过 ML 模型与可靠而透明的不确定性评估来帮助用户做出更加 Informed 的决策。
</details></li>
</ul>
<hr>
<h2 id="Kantian-Deontology-Meets-AI-Alignment-Towards-Morally-Robust-Fairness-Metrics"><a href="#Kantian-Deontology-Meets-AI-Alignment-Towards-Morally-Robust-Fairness-Metrics" class="headerlink" title="Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics"></a>Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05227">http://arxiv.org/abs/2311.05227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Mougan, Joshua Brand</li>
<li>for: 本文探讨了 Kantian deontological  Framework 在 fairness metrics 中的compatibility，以及如何通过 integrate Kantian ethics 来提高 AI alignment 中的公正性和公平性。</li>
<li>methods: 本文 revisits Kant’s critique of utilitarianism， argue that fairness principles should align with the Kantian deontological framework，并提出一种基于 Kantian ethics 的 AI fairness metrics 模型。</li>
<li>results: 本文提出了一种新的 AI fairness metrics 模型，该模型可以 better balance outcomes and procedures in pursuit of fairness and justice，并且可以帮助建立一个更加公正和公平的 AI ланд财。<details>
<summary>Abstract</summary>
Deontological ethics, specifically understood through Immanuel Kant, provides a moral framework that emphasizes the importance of duties and principles, rather than the consequences of action. Understanding that despite the prominence of deontology, it is currently an overlooked approach in fairness metrics, this paper explores the compatibility of a Kantian deontological framework in fairness metrics, part of the AI alignment field. We revisit Kant's critique of utilitarianism, which is the primary approach in AI fairness metrics and argue that fairness principles should align with the Kantian deontological framework. By integrating Kantian ethics into AI alignment, we not only bring in a widely-accepted prominent moral theory but also strive for a more morally grounded AI landscape that better balances outcomes and procedures in pursuit of fairness and justice.
</details>
<details>
<summary>摘要</summary>
德 Ontological 伦理学，通过伊曼纽尔·凯恩的理解，提供了一个伦理框架，强调行为的责任和原则，而不是行为的后果。尽管德 Ontology 目前在公平度量上被忽略，但这篇文章 argue that a Kantian deontological framework is compatible with fairness metrics, which is part of the AI alignment field. 我们回顾了凯恩对于实用主义的批判，该是AI公平度量的主要方法，并论证公平原则应该遵循德 Ontological 伦理框架。通过将基着 Kantian 伦理学入核AI对齐，我们不仅引入了一种广泛接受的伦理理论，还努力创造一个更加伦理基础的 AI 领域，以更好地平衡结果和过程，追求公平和正义。
</details></li>
</ul>
<hr>
<h2 id="Green-Resilience-of-Cyber-Physical-Systems"><a href="#Green-Resilience-of-Cyber-Physical-Systems" class="headerlink" title="Green Resilience of Cyber-Physical Systems"></a>Green Resilience of Cyber-Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05201">http://arxiv.org/abs/2311.05201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rimawi-diaeddin/GRCPS-ISSRE22-DS">https://github.com/rimawi-diaeddin/GRCPS-ISSRE22-DS</a></li>
<li>paper_authors: Diaeddin Rimawi</li>
<li>for: 这项研究旨在提出一种基于游戏理论的可持续可靠性机制，以提高Cyber-Physical System（CPS）的可靠性和绿色性。</li>
<li>methods: 该研究使用游戏理论来解决CPS中的不确定性问题，并通过一个实际的合作人工智能系统（CAIS）来描述GAME模型。</li>
<li>results: 该研究显示，通过GAME模型，CPS可以实现可靠性和绿色性，同时减少CO2足迹。<details>
<summary>Abstract</summary>
Cyber-Physical System (CPS) represents systems that join both hardware and software components to perform real-time services. Maintaining the system's reliability is critical to the continuous delivery of these services. However, the CPS running environment is full of uncertainties and can easily lead to performance degradation. As a result, the need for a recovery technique is highly needed to achieve resilience in the system, with keeping in mind that this technique should be as green as possible. This early doctorate proposal, suggests a game theory solution to achieve resilience and green in CPS. Game theory has been known for its fast performance in decision-making, helping the system to choose what maximizes its payoffs. The proposed game model is described over a real-life collaborative artificial intelligence system (CAIS), that involves robots with humans to achieve a common goal. It shows how the expected results of the system will achieve the resilience of CAIS with minimized CO2 footprint.
</details>
<details>
<summary>摘要</summary>
资berger-physical system (CPS) 表示把硬件和软件元件结合起来提供实时服务的系统。维护这个系统的可靠性非常重要，以确保不断提供服务。然而，CPS 运行环境充满不确定性，容易导致性能下降。因此，需要一种恢复技术来实现系统的可靠性，同时避免对环境造成过大的影响。本博士学位提案建议使用游戏理论解决这个问题。游戏理论known for its fast decision-making, can help the system to choose what maximizes its payoffs.在这个提案中，我们使用了一个基于现实生成人工智能系统 (CAIS) 的游戏模型，CAIS 是一个涉及人类和机器人的共同目标系统。这个模型显示了在 minimizing CO2 印证下，CPS 可以实现可靠性和绿色的恢复。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-in-Computed-Tomography-Pulmonary-Angiography-Imaging-A-Dual-Pronged-Approach-for-Pulmonary-Embolism-Detection"><a href="#Deep-Learning-in-Computed-Tomography-Pulmonary-Angiography-Imaging-A-Dual-Pronged-Approach-for-Pulmonary-Embolism-Detection" class="headerlink" title="Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection"></a>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05197">http://arxiv.org/abs/2311.05197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabiha Bushra, Muhammad E. H. Chowdhury, Rusab Sarmun, Saidul Kabir, Menatalla Said, Sohaib Bassam Zoghoul, Adam Mushtak, Israa Al-Hashimi, Abdulrahman Alqahtani, Anwarul Hasan<br>for: 这项研究的主要目标是利用深度学习技术提高计算助encioded诊断（CAD）的准确率，以便更好地诊断肺动脉瘤（PE）。methods: 该研究采用了一种双重的方法， combinig 分类和检测两种方法来诊断PE。首先，我们引入了一种注意力导向的卷积神经网络（AG-CNN），以便对全局和局部肿瘤区域进行分类。其次，我们使用了当前的模型来检测潜在的PE区域。最后，我们使用了不同的ensemble技术来提高检测精度。results: 我们在 Ferdowsi University of Mashhad 的 Pulmonary Embolism（FUMPE）数据集上测试了我们的注意力导向分类方法，与基eline模型 denseNet-121 进行比较。结果显示，我们的方法可以提高 Receiver Operating Characteristic 的面积（AUC）by 8.1%。另外，通过使用ensemble技术和检测模型，我们可以进一步提高 mean average precision（mAP）和 F1 分数。最后，我们的研究提供了一种全面的PE诊断方法，可以帮助解决肺动脉瘤的常见问题，如下 diagnosis 和 misdiagnosis。<details>
<summary>Abstract</summary>
Pulmonary Embolism (PE) is a critical medical condition characterized by obstructions in the pulmonary arteries. Despite being a major health concern, it often goes underdiagnosed leading to detrimental clinical outcomes. The increasing reliance on Computed Tomography Pulmonary Angiography for diagnosis presents challenges and a pressing need for enhanced diagnostic solutions. The primary objective of this study is to leverage deep learning techniques to enhance the Computer Assisted Diagnosis of PE. This study presents a comprehensive dual-pronged approach combining classification and detection for PE diagnosis. We introduce an Attention-Guided Convolutional Neural Network (AG-CNN) for classification, addressing both global and local lesion region. For detection, state-of-the-art models are employed to pinpoint potential PE regions. Different ensembling techniques further improve detection accuracy by combining predictions from different models. Finally, a heuristic strategy integrates classifier outputs with detection results, ensuring robust and accurate PE identification. Our attention-guided classification approach, tested on the Ferdowsi University of Mashhad's Pulmonary Embolism (FUMPE) dataset, outperformed the baseline model DenseNet-121 by achieving an 8.1% increase in the Area Under the Receiver Operating Characteristic. By employing ensemble techniques with detection models, the mean average precision (mAP) was considerably enhanced by a 4.7% increase. The classifier-guided framework further refined the mAP and F1 scores over the ensemble models. Our research offers a comprehensive approach to PE diagnostics using deep learning, addressing the prevalent issues of underdiagnosis and misdiagnosis. We aim to improve PE patient care by integrating AI solutions into clinical workflows, highlighting the potential of human-AI collaboration in medical diagnostics.
</details>
<details>
<summary>摘要</summary>
肺动脉瘤（PE）是一种严重的医疗问题， характеризуется为肺动脉的堵塞。尽管它是一个重要的健康问题，但它经常被诊断错误，导致了严重的临床结果。随着计算机 Tomatoes Pulmonary Angiography（CTPA）的使用变得更加普遍，诊断PE的挑战日益增加。这项研究的主要目标是利用深度学习技术提高计算机助成诊断PE的精度。我们提出了一种权重引导的卷积神经网络（AG-CNN），用于类别，同时处理全局和局部肿瘤区域。对探测，我们采用了当前最佳模型，以找到可能的PE区域。不同的拼接技术进一步提高探测精度，将不同模型的预测结果进行组合。最后，我们采用了一种启发策略，将分类器输出与探测结果结合，以确保精度和准确性的PE诊断。我们在 Ferdowsi University of Mashhad的肺动脉瘤（FUMPE）数据集上测试了我们的注意力引导分类方法，与基eline模型DenseNet-121相比，实现了8.1%的接收操作特征区域（AUC）的提高。通过使用探测模型的ensemble技术，mean average precision（mAP）得到了明显的提高，升高4.7%。我们的类ifier-guided框架还进一步提高了mAP和F1分数。我们的研究提供了一种全面的肺动脉瘤诊断方法，通过深度学习 Addressing the prevalent issues of underdiagnosis and misdiagnosis.我们希望通过将AI技术integrated into clinical workflows，提高PE patient care。这种人AI合作的潜在可能性在医疗诊断中展示出来。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Weak-Strong-Experts-on-Graphs"><a href="#Mixture-of-Weak-Strong-Experts-on-Graphs" class="headerlink" title="Mixture of Weak &amp; Strong Experts on Graphs"></a>Mixture of Weak &amp; Strong Experts on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05185">http://arxiv.org/abs/2311.05185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanqing Zeng, Hanjia Lyu, Diyi Hu, Yinglong Xia, Jiebo Luo</li>
<li>for: 提高 node  classification 的表现，通过 mixture of weak and strong experts 来处理 graph 数据。</li>
<li>methods: 使用 weak 专家 (Multi-layer Perceptron) 和 strong 专家 (Graph Neural Network) 的 mixture，其中 weak 专家 是一个轻量级的 MLP，strong 专家 是一个 off-the-shelf GNN。通过一种 “confidence” 机制来调控两个专家的合作，以适应不同的 target nodes。</li>
<li>results: 在 6 个标准的 node  classification  benchmark 上（包括 both homophilous 和 heterophilous 图），Mowst 显示了明显的准确率提高。<details>
<summary>Abstract</summary>
Realistic graphs contain both rich self-features of nodes and informative structures of neighborhoods, jointly handled by a GNN in the typical setup. We propose to decouple the two modalities by mixture of weak and strong experts (Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf Graph Neural Network (GNN). To adapt the experts' collaboration to different target nodes, we propose a "confidence" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our "confidence" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst shows significant accuracy improvement on 6 standard node classification benchmarks (including both homophilous and heterophilous graphs).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FireMatch-A-Semi-Supervised-Video-Fire-Detection-Network-Based-on-Consistency-and-Distribution-Alignment"><a href="#FireMatch-A-Semi-Supervised-Video-Fire-Detection-Network-Based-on-Consistency-and-Distribution-Alignment" class="headerlink" title="FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment"></a>FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05168">http://arxiv.org/abs/2311.05168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Lin, Zuoyong Li, Kun Zeng, Haoyi Fan, Wei Li, Xiaoguang Zhou</li>
<li>for: 提高视频中的火灾检测性能，尤其是在数据标注成本高昂的情况下。</li>
<li>methods: 提出一种基于一致规则和对抗分布对齐的半指导火灾检测模型，通过将弱变化和强变化样本结合使用，以及引入一种公平损失函数来避免高确度对非火类的问题。</li>
<li>results: 在两个真实的火灾数据集上实现了76.92%和91.81%的准精度，比现有的半指导类型检测方法高。<details>
<summary>Abstract</summary>
Deep learning techniques have greatly enhanced the performance of fire detection in videos. However, video-based fire detection models heavily rely on labeled data, and the process of data labeling is particularly costly and time-consuming, especially when dealing with videos. Considering the limited quantity of labeled video data, we propose a semi-supervised fire detection model called FireMatch, which is based on consistency regularization and adversarial distribution alignment. Specifically, we first combine consistency regularization with pseudo-label. For unlabeled data, we design video data augmentation to obtain corresponding weakly augmented and strongly augmented samples. The proposed model predicts weakly augmented samples and retains pseudo-label above a threshold, while training on strongly augmented samples to predict these pseudo-labels for learning more robust feature representations. Secondly, we generate video cross-set augmented samples by adversarial distribution alignment to expand the training data and alleviate the decline in classification performance caused by insufficient labeled data. Finally, we introduce a fairness loss to help the model produce diverse predictions for input samples, thereby addressing the issue of high confidence with the non-fire class in fire classification scenarios. The FireMatch achieved an accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively. The experimental results demonstrate that the proposed method outperforms the current state-of-the-art semi-supervised classification methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术对视频中的火情检测表现有了大幅提高。然而，视频基于的火情检测模型强依 Labelled data，并且数据标注过程特别是对视频来说是非常昂贵和时间consuming。面对有限的标注视频数据，我们提出了一种半supervised的火情检测模型，即FireMatch，基于一致regulation和对抗分布对齐。具体来说，我们首先将一致regulation与pseudo-label结合使用。对于没有标注的数据，我们设计了视频数据增强，以获得弱augmented和强augmented样本。提案的模型预测弱augmented样本，保留pseudo-label在阈值以上，而在强augmented样本上进行训练，以学习更加稳定的特征表示。其次，我们通过对视频数据进行对抗分布对齐，生成了跨集数据增强样本，以扩大训练数据，并避免由不充分的标注数据引起的分类性能下降。最后，我们引入了公平loss，以帮助模型对输入样本产生多样化的预测，从而解决火类分类场景中的高信任性问题。FireMatch模型在两个真实的火情数据集上 achieved an accuracy of 76.92%和91.81%，分别比当前最佳半supervised分类方法高出了2.85%和4.99%。实验结果表明，我们提出的方法可以提高火情检测的精度和公平性。
</details></li>
</ul>
<hr>
<h2 id="RAPID-Training-free-Retrieval-based-Log-Anomaly-Detection-with-PLM-considering-Token-level-information"><a href="#RAPID-Training-free-Retrieval-based-Log-Anomaly-Detection-with-PLM-considering-Token-level-information" class="headerlink" title="RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information"></a>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05160">http://arxiv.org/abs/2311.05160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsba-lab/rapid">https://github.com/dsba-lab/rapid</a></li>
<li>paper_authors: Gunho No, Yukyung Lee, Hyeongwon Kang, Pilsung Kang</li>
<li>for: 这个研究旨在提出一种能够在logs中检测异常行为的方法，并且不需要特定的训练数据。</li>
<li>methods: 这个方法使用了自然语言处理的技术，将logs转换为文本内容，然后使用预训语言模型提取特征。它还使用了检索技术，将test logs与最相似的正常logs进行比较，以检测异常。</li>
<li>results: 实验结果显示，这个方法可以在无需特定训练数据的情况下，与之前的模型相比，实现了相当的比较表现，并且在某些数据集上获得了最好的表现。此外，这个方法还可以在实时检测中获得高效的结果，无需过度的计算成本。<details>
<summary>Abstract</summary>
As the IT industry advances, system log data becomes increasingly crucial. Many computer systems rely on log texts for management due to restricted access to source code. The need for log anomaly detection is growing, especially in real-world applications, but identifying anomalies in rapidly accumulating logs remains a challenging task. Traditional deep learning-based anomaly detection models require dataset-specific training, leading to corresponding delays. Notably, most methods only focus on sequence-level log information, which makes the detection of subtle anomalies harder, and often involve inference processes that are difficult to utilize in real-time. We introduce RAPID, a model that capitalizes on the inherent features of log data to enable anomaly detection without training delays, ensuring real-time capability. RAPID treats logs as natural language, extracting representations using pre-trained language models. Given that logs can be categorized based on system context, we implement a retrieval-based technique to contrast test logs with the most similar normal logs. This strategy not only obviates the need for log-specific training but also adeptly incorporates token-level information, ensuring refined and robust detection, particularly for unseen logs. We also propose the core set technique, which can reduce the computational cost needed for comparison. Experimental results show that even without training on log data, RAPID demonstrates competitive performance compared to prior models and achieves the best performance on certain datasets. Through various research questions, we verified its capability for real-time detection without delay.
</details>
<details>
<summary>摘要</summary>
翻译结果：随着IT业的发展，系统日志数据变得越来越重要。许多计算机系统通过日志文本进行管理，因为访问源代码的限制。检测日志异常的需求在实际应用中增长，特别是在大量日志聚集的情况下，但检测日志异常是一项复杂的任务。传统的深度学习基于异常检测模型需要特定的训练数据，导致延迟。很多方法只关注日志序列水平的信息，这使得检测微异常更加困难，并且经常涉及到困难在实时中使用的推理过程。我们介绍了RAPID模型，它利用日志数据的自然语言特征，通过预训练语言模型提取表示。由于日志可以根据系统上下文分类，我们实施了一种检索基于技术，将测试日志与最相似的正常日志进行对比。这种策略不仅减少了异常检测的延迟，还能够充分地 incorporate  Token级信息，以确保精细和Robust的检测，特别是 для未见的日志。我们还提出了核心集技术，可以降低比较的计算成本。实验结果表明，无需训练日志数据，RAPID模型仍然可以与之前的模型竞争，并在某些数据集上达到最佳性能。通过多种研究问题，我们证明了它的实时检测能力。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-Deep-Cognate-Detection-Framework-for-Low-Resourced-Languages-Using-Morphological-Knowledge-of-Closely-Related-Languages"><a href="#Weakly-supervised-Deep-Cognate-Detection-Framework-for-Low-Resourced-Languages-Using-Morphological-Knowledge-of-Closely-Related-Languages" class="headerlink" title="Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages"></a>Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05155">http://arxiv.org/abs/2311.05155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koustavagoswami/weakly_supervised-cognate_detection">https://github.com/koustavagoswami/weakly_supervised-cognate_detection</a></li>
<li>paper_authors: Koustava Goswami, Priya Rani, Theodorus Fransen, John P. McCrae</li>
<li>for: 这个论文的目的是提出一种语言不确定的弱监督词义检测方法，以便在语言理解任务中提高性能。</li>
<li>methods: 该方法使用 morphological knowledge from closely related languages 来进行词义检测，并通过编码器来学习和转移知识。</li>
<li>results: 实验结果显示，该方法可以在不同的语言家族上实现显著的改进，并且超越了现有的超级vised和无监督方法。<details>
<summary>Abstract</summary>
Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training. The code and dataset building scripts can be found at https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection
</details>
<details>
<summary>摘要</summary>
利用同源语言的词汇相似性进行无监督学习是对语言理解任务的一种有趣的机会，包括无监督机器翻译、命名实体识别和信息检索。过去的方法主要集中在基于文本、音频或当前语言模型的监督认知语言模型，这些模型对大多数下resource语言不够好。这篇论文提出了一种新的语言无关的弱监督深度同源词汇检测框架，用于下resource语言。我们使用 morphological 知识来训练一个encoder，并将知识转移到无监督和弱监督同源词汇检测任务中。无监督的方式可以超越手动标注同源词汇的需求。我们在不同的已出版同源词汇检测数据集上进行了实验，并观察到了与状态之前的显著改进。我们的方法可以扩展到任何语言家族，因为它不需要标注同源词汇对的训练。代码和数据集构建脚本可以在 https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection 找到。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Prompts-Adapting-Large-Pre-trained-Models-for-Audio-Visual-Downstream-Tasks"><a href="#Cross-modal-Prompts-Adapting-Large-Pre-trained-Models-for-Audio-Visual-Downstream-Tasks" class="headerlink" title="Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks"></a>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05152">http://arxiv.org/abs/2311.05152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoyi-duan/dg-sct">https://github.com/haoyi-duan/dg-sct</a></li>
<li>paper_authors: Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, Zhou Zhao</li>
<li>for: 提高多模态任务中大型预训练模型的表现，解决单模态训练数据下预训练模型在多模态任务中的特征提取问题。</li>
<li>methods: 提出了一种新的双引导空间通道时间（DG-SCT）注意机制，利用音频和视频模式作为软提示，动态调整预训练模型的参数，根据当前多模态输入特征进行适应性的特征提取。</li>
<li>results: 实验证明，提出的模型在多个下游任务中达到了领先的Results，包括AVE、AVVP、AVS和AVQA等任务。此外，模型在具有几个shot和零shot的场景下表现出了良好的性能。<details>
<summary>Abstract</summary>
In recent years, the deployment of large-scale pre-trained models in audio-visual downstream tasks has yielded remarkable outcomes. However, these models, primarily trained on single-modality unconstrained datasets, still encounter challenges in feature extraction for multi-modal tasks, leading to suboptimal performance. This limitation arises due to the introduction of irrelevant modality-specific information during encoding, which adversely affects the performance of downstream tasks. To address this challenge, this paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention mechanism. This mechanism leverages audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained models based on the current multi-modal input features. Specifically, the DG-SCT module incorporates trainable cross-modal interaction layers into pre-trained audio-visual encoders, allowing adaptive extraction of crucial information from the current modality across spatial, channel, and temporal dimensions, while preserving the frozen parameters of large-scale pre-trained models. Experimental evaluations demonstrate that our proposed model achieves state-of-the-art results across multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our model exhibits promising performance in challenging few-shot and zero-shot scenarios. The source code and pre-trained models are available at https://github.com/haoyi-duan/DG-SCT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-of-Large-Language-Models-in-Medicine-Progress-Application-and-Challenge"><a href="#A-Survey-of-Large-Language-Models-in-Medicine-Progress-Application-and-Challenge" class="headerlink" title="A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"></a>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05112">http://arxiv.org/abs/2311.05112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-in-health/medllmspracticalguide">https://github.com/ai-in-health/medllmspracticalguide</a></li>
<li>paper_authors: Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Zheng Li, Fenglin Liu</li>
<li>for: 本研究提供了一份对大语言模型（LLMs）在医学领域的概述，以及医学LLMs的构建、下游性能、实际应用以及挑战。</li>
<li>methods: 本研究使用了现有的LLMs技术，并对医学LLMs的构建和下游性能进行了评估。</li>
<li>results: 本研究发现，医学LLMs在医学领域的应用有很大的潜力，但同时也存在一些挑战，如数据采集和质量控制等问题。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT, have achieved substantial attention due to their impressive human language understanding and generation capabilities. Therefore, the application of LLMs in medicine to assist physicians and patient care emerges as a promising research direction in both artificial intelligence and clinical medicine. To this end, this survey provides a comprehensive overview of the current progress, applications, and challenges faced by LLMs in medicine. Specifically, we aim to address the following questions: 1) What are LLMs and how can medical LLMs be built? 2) What are the downstream performances of medical LLMs? 3) How can medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? 5) How can we better construct and utilize medical LLMs? As a result, this survey aims to provide insights into the opportunities and challenges of LLMs in medicine and serve as a valuable resource for constructing practical and effective medical LLMs. A regularly updated list of practical guide resources of medical LLMs can be found at https://github.com/AI-in-Health/MedLLMsPracticalGuide.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs），如ChatGPT，在人工智能和临床医学领域获得了广泛的关注，因为它们在人语理解和生成方面表现出了卓越的能力。因此，在医学和人工智能方面应用LLMs的研究方向显得极其可能。为了实现这一目标，本调查提供了关于当前进展、应用和医学LLMs面临的挑战的全面概述。特别是，我们想要回答以下问题：1. LLMs是什么，如何构建医学LLMs？2. 医学LLMs的下游性能如何？3. 如何在实际临床医疗中使用医学LLMs？4. 使用医学LLMs会遇到什么挑战？5. 如何更好地构建和利用医学LLMs？因此，本调查希望通过对医学LLMs的潜在机会和挑战的描述，为constructing practical and effective医学LLMs提供启示，并且可以作为valuable resource。有关医学LLMs的实践指南资源的更新列表可以在https://github.com/AI-in-Health/MedLLMsPracticalGuide中找到。
</details></li>
</ul>
<hr>
<h2 id="A-differentiable-brain-simulator-bridging-brain-simulation-and-brain-inspired-computing"><a href="#A-differentiable-brain-simulator-bridging-brain-simulation-and-brain-inspired-computing" class="headerlink" title="A differentiable brain simulator bridging brain simulation and brain-inspired computing"></a>A differentiable brain simulator bridging brain simulation and brain-inspired computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05106">http://arxiv.org/abs/2311.05106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoming Wang, Tianqiu Zhang, Sichao He, Yifeng Gong, Hongyaoxing Gu, Shangyang Li, Si Wu</li>
<li>for:  bridging the gap between brain simulation and brain-inspired computing (BIC)</li>
<li>methods:  differentiable brain simulator developed using JAX and XLA, with a range of sparse and event-driven operators, an abstraction for managing synaptic computations, and an object-oriented just-in-time compilation approach</li>
<li>results:  efficient and scalable brain simulation, with potential to support research at the intersection of brain simulation and BIC.<details>
<summary>Abstract</summary>
Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, highlight its differentiable simulation for biologically plausible spiking models, and discuss its potential to support research at the intersection of brain simulation and BIC.
</details>
<details>
<summary>摘要</summary>
��BrainPy是一个可微分的脑模拟器，使用JAX和XLA开发，旨在将脑模拟和脑计算融合在一起。BrainPy在JAX上扩展了功能，添加了脑模拟的灵活、高效和可扩展功能。它提供了脑模拟中的稀盐和事件驱动操作符， synaptic计算的抽象，多级脑模型的模块化和 гибacco interface，以及对内存密集的脑动力学的对象 oriented即时编译方法。我们在标准任务上展示了BrainPy的效率和可扩展性，并highlight了其可微分的脑模拟功能，并讨论了它在脑模拟和脑计算之间的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="Legal-HNet-Mixing-Legal-Long-Context-Tokens-with-Hartley-Transform"><a href="#Legal-HNet-Mixing-Legal-Long-Context-Tokens-with-Hartley-Transform" class="headerlink" title="Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform"></a>Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05089">http://arxiv.org/abs/2311.05089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Giofré, Sneha Ghantasala</li>
<li>for: This paper aims to explore alternatives to the attention-based layers in the transformers architecture for specialized domains like legal, where long texts are common.</li>
<li>methods: The paper uses non-parametric techniques such as Hartley and Fourier transforms to replace the attention-based layers, and introduces a new hybrid Seq2Seq architecture with a no-attention-based encoder and an attention-based decoder.</li>
<li>results: The authors train models with long input documents from scratch in the legal domain setting and achieve competitive performance on existing summarization tasks with reduced compute and memory requirements. They believe that simpler infrastructures can achieve similar or better performance, making training models more accessible and contributing to a reduction in carbon footprint during training.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目的是为特定领域 like 法律， где 文本很长的应用场景中探索 transformers 架构中的注意力机制的替代方案。</li>
<li>methods: 这篇论文使用非参数的 Hartley 和 Fourier 变换来取代注意力机制，并提出了一种新的 Hybrid Seq2Seq 架构，其中的编码器是无注意力的，而解码器是注意力的。</li>
<li>results: 作者使用这些新的架构和方法在法律领域中训练了从头开始的模型，并在现有的摘要任务上达到了类似或更好的性能，同时减少了计算和存储的需求。作者认为，这些更简单的基础设施可以在更多人的情况下使用，并且对于减少训练过程中的碳脚印有积极的影响。<details>
<summary>Abstract</summary>
Since its introduction, the transformers architecture has seen great adoption in NLP applications, but it also has limitations. Although the self-attention mechanism allows for generating very rich representations of the input text, its effectiveness may be limited in specialized domains such as legal, where, for example, language models often have to process very long texts. In this paper, we explore alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these non-parametric techniques, we train models with long input documents from scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention-based decoder, which performs quite well on existing summarization tasks with much less compute and memory requirements. We believe that similar, if not better performance, as in the case of long correlations of abstractive text summarization tasks, can be achieved by adopting these simpler infrastructures. This not only makes training models from scratch accessible to more people, but also contributes to the reduction of the carbon footprint during training.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:自transformersarchitecture的引入以来，它在NLP应用中得到了广泛的采用，但也有限制。尽管自我关注机制允许生成非常 richexpressionsof输入文本，但在专门领域如法律领域中，语言模型经常需要处理非常长的文本。在这篇论文中，我们探索使用非 Parametric技术来取代关注基层：Hartley和Fourier变换。使用这些非 Parametric技术，我们在法律领域的长输入文档上从scratch训练模型。我们还介绍了一种新的混合Seq2Seq架构，一个没有关注基层的编码器与一个关注基层的解码器相连接，它在现有摘要任务上表现非常出色，需要 Much less compute和memoryrequirements。我们认为可以通过采用这些更简单的基础设施，在摘要任务中 achieve similar或更好的性能，这不仅使得训练模型从scratch变得更加可 accessible，而且也对训练过程中的碳脚印产生了贡献。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-of-semi-supervised-learning-from-tasks-with-heterogeneous-attribute-spaces"><a href="#Meta-learning-of-semi-supervised-learning-from-tasks-with-heterogeneous-attribute-spaces" class="headerlink" title="Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces"></a>Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05088">http://arxiv.org/abs/2311.05088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoharu Iwata, Atsutoshi Kumagai</li>
<li>for: 本研究 propose a meta-learning method for semi-supervised learning, 能够学习多个任务，并且这些任务可以有不同的特征空间。</li>
<li>methods: 方法包括使用神经网络将标签和无标签数据同时嵌入到任务特定的空间中，并且使用可变特征自注意层来找到不同特征空间中的嵌入。</li>
<li>results: 实验结果表明，提出的方法在不同特征空间中的分类和回归任务中表现出色，超过了现有的meta-学习和半监督学习方法。<details>
<summary>Abstract</summary>
We propose a meta-learning method for semi-supervised learning that learns from multiple tasks with heterogeneous attribute spaces. The existing semi-supervised meta-learning methods assume that all tasks share the same attribute space, which prevents us from learning with a wide variety of tasks. With the proposed method, the expected test performance on tasks with a small amount of labeled data is improved with unlabeled data as well as data in various tasks, where the attribute spaces are different among tasks. The proposed method embeds labeled and unlabeled data simultaneously in a task-specific space using a neural network, and the unlabeled data's labels are estimated by adapting classification or regression models in the embedding space. For the neural network, we develop variable-feature self-attention layers, which enable us to find embeddings of data with different attribute spaces with a single neural network by considering interactions among examples, attributes, and labels. Our experiments on classification and regression datasets with heterogeneous attribute spaces demonstrate that our proposed method outperforms the existing meta-learning and semi-supervised learning methods.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于多任务的meta学习方法，可以在不同任务的 attribute space 上学习。现有的半supervised meta学习方法假设所有任务共享同一个 attribute space，这限制了我们学习的variety of tasks。我们的方法可以在tasks with small amount of labeled data中提高预期测试性能，并且可以使用不同任务的数据，以及不同 attribute space 上的数据进行学习。我们的方法使用神经网络将标注和无标注数据同时嵌入到任务特定的空间中，并且使用自适应分类或回归模型来估算无标注数据的标签。为神经网络，我们开发了可变特征自注意层，可以通过考虑示例、属性和标签之间的交互来找到不同 attribute space 上的数据嵌入。我们在 Classification 和回归数据集上进行了实验，并证明了我们的提出方法比现有的 meta学习和半supervised learning 方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Large-Language-Models-as-Rationalizers-of-Knowledge-intensive-Tasks"><a href="#Characterizing-Large-Language-Models-as-Rationalizers-of-Knowledge-intensive-Tasks" class="headerlink" title="Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks"></a>Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05085">http://arxiv.org/abs/2311.05085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Mishra, Sajjadur Rahman, Hannah Kim, Kushan Mitra, Estevam Hruschka</li>
<li>for: 这篇论文主要用于探讨大型自然语言模型（LLM）在知识填充任务中的能力。</li>
<li>methods: 论文使用专家写的例子来帮助LLM生成知识导向的论证，并通过几 shot 方式来评估其效果。</li>
<li>results: 研究发现，人工写的论证更为可靠，而 LLM 生成的论证尚需进一步改进 conciseness 和创新性。此外，研究还发现，对模型预测错误的论证可能会降低人们对 LLM 生成的论证的信任。<details>
<summary>Abstract</summary>
Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation.
</details>
<details>
<summary>摘要</summary>
In this study, we explore the task of generating knowledge-guided rationalizations in natural language using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required.In another study, we observed that rationalization of incorrect model predictions can erode humans' trust in LLM-generated rationales. To address this issue, we propose a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation.
</details></li>
</ul>
<hr>
<h2 id="Signal-Temporal-Logic-Guided-Apprenticeship-Learning"><a href="#Signal-Temporal-Logic-Guided-Apprenticeship-Learning" class="headerlink" title="Signal Temporal Logic-Guided Apprenticeship Learning"></a>Signal Temporal Logic-Guided Apprenticeship Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05084">http://arxiv.org/abs/2311.05084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniruddh G. Puranic, Jyotirmoy V. Deshmukh, Stefanos Nikolaidis</li>
<li>for: 提高控制策略学习效果，尤其是在具有时间依赖关系的任务中。</li>
<li>methods: 使用时间逻辑规范来描述高级任务目标，并将其编码到图形中以定义时间基于的度量，以改进学习奖励和控制策略的质量。</li>
<li>results: 通过对多种机器人抓取机械 simulations的实验，我们示出了我们的框架可以减少学习奖励和控制策略所需的示例数量，并提高学习效果。<details>
<summary>Abstract</summary>
Apprenticeship learning crucially depends on effectively learning rewards, and hence control policies from user demonstrations. Of particular difficulty is the setting where the desired task consists of a number of sub-goals with temporal dependencies. The quality of inferred rewards and hence policies are typically limited by the quality of demonstrations, and poor inference of these can lead to undesirable outcomes. In this letter, we show how temporal logic specifications that describe high level task objectives, are encoded in a graph to define a temporal-based metric that reasons about behaviors of demonstrators and the learner agent to improve the quality of inferred rewards and policies. Through experiments on a diverse set of robot manipulator simulations, we show how our framework overcomes the drawbacks of prior literature by drastically improving the number of demonstrations required to learn a control policy.
</details>
<details>
<summary>摘要</summary>
TEXT：Apprenticeship learning critically relies on effectively learning rewards and control policies from user demonstrations. However, tasks with multiple sub-goals and temporal dependencies pose significant challenges. The quality of inferred rewards and policies is often limited by the quality of demonstrations, and poor inference can lead to undesirable outcomes.In this letter, we propose using temporal logic specifications to encode high-level task objectives in a graph, enabling a temporal-based metric to reason about the behaviors of both the demonstrators and the learner agent. Our framework improves the quality of inferred rewards and policies by drastically reducing the number of demonstrations required to learn a control policy.We evaluate our approach on a diverse set of robot manipulator simulations and demonstrate its effectiveness in overcoming the drawbacks of prior literature.TRANSLATION：学习契约（Apprenticeship learning）取决于从用户示例中学习奖励和控制策略的效果。然而，具有多个子目标和时间依赖关系的任务具有重要挑战。奖励和策略的质量通常受示例质量的限制，而且差异的推理可能会导致不желатель的结果。在这封信中，我们提议使用时间逻辑规范来编码高级任务目标，并在图表中定义一种时间基于的度量，以便理解示例者和学习者机器人的行为。我们的框架可以有效地提高奖励和策略的质量，通过减少学习者需要的示例数量来减少学习时间。我们在一个多样化的机器人拟合器 simulate 中进行了实验，并证明了我们的框架可以超越先前的文献中的缺点。
</details></li>
</ul>
<hr>
<h2 id="Mental-Health-Diagnosis-in-the-Digital-Age-Harnessing-Sentiment-Analysis-on-Social-Media-Platforms-upon-Ultra-Sparse-Feature-Content"><a href="#Mental-Health-Diagnosis-in-the-Digital-Age-Harnessing-Sentiment-Analysis-on-Social-Media-Platforms-upon-Ultra-Sparse-Feature-Content" class="headerlink" title="Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content"></a>Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05075">http://arxiv.org/abs/2311.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haijian Shao, Ming Zhu, Shengjie Zhai<br>for: 这项研究旨在透过社交媒体平台上的推文和讨论来早期发现和 intervene 人们的心理疾病，尤其是投降 GROUP 的人群。methods: 该研究提出了一种新的semantic feature preprocessing技术，包括三个方面：1）mitigating feature sparsity with weak classifier，2）adaptive feature dimension with modulus loops，3）deep-mining and extending features among contexts。results: 该研究使用了 Reddit Mental Health Dataset 2022 进行 исследование，并解决了数据稀缺问题，表现出99.81%非零元素。 after applying preprocessing technique，feature sparsity decreases to 85.4%。 compared to seven benchmark models,该研究的方法显示了significant performance improvement，包括8.0%的准确率、0.069的精度、0.093的准确率、0.102的F1 score 和0.059的AUC。<details>
<summary>Abstract</summary>
Amid growing global mental health concerns, particularly among vulnerable groups, natural language processing offers a tremendous potential for early detection and intervention of people's mental disorders via analyzing their postings and discussions on social media platforms. However, ultra-sparse training data, often due to vast vocabularies and low-frequency words, hinders the analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also blur the boundaries in distinguishing similar/co-related disorders. To address these issues, we propose a novel semantic feature preprocessing technique with a three-folded structure: 1) mitigating the feature sparsity with a weak classifier, 2) adaptive feature dimension with modulus loops, and 3) deep-mining and extending features among the contexts. With enhanced semantic features, we train a machine learning model to predict and classify mental disorders. We utilize the Reddit Mental Health Dataset 2022 to examine conditions such as Anxiety, Borderline Personality Disorder (BPD), and Bipolar-Disorder (BD) and present solutions to the data sparsity challenge, highlighted by 99.81% non-zero elements. After applying our preprocessing technique, the feature sparsity decreases to 85.4%. Overall, our methods, when compared to seven benchmark models, demonstrate significant performance improvements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in F1 score, and 0.059 in AUC. This research provides foundational insights for mental health prediction and monitoring, providing innovative solutions to navigate challenges associated with ultra-sparse data feature and intricate multi-label classification in the domain of mental health analysis.
</details>
<details>
<summary>摘要</summary>
在全球心理健康问题的增长中，特别是对护理不足的群体，自然语言处理技术具有巨大的潜在价值，通过分析社交媒体平台上的发言和讨论来早期发现和 intervene 人们的心理疾病。然而，由于庞大的词汇和低频词汇，导致分析精度受限。同时，病种的多标签和症状的协occurrence也使得分类变得更加困难。为解决这些问题，我们提出了一种新的 semantics feature 处理技术，包括三个部分：1） mitigate feature sparsity with weak classifier，2） adaptive feature dimension with modulus loops，3） deep-mining and extending features among the contexts。通过增强 semantics features，我们训练了一个机器学习模型，以预测和分类心理疾病。我们使用2022年Reddit心理健康数据集来检查抑郁、边缘人格障碍（BPD）和mania病（BD）等病种，并解决数据稀缺问题，表现出99.81%的非零元素。在我们的预处理技术应用后，特征稀缺度下降至85.4%。总的来说，我们的方法，与七个 Refer 模型进行比较，显示了显著的性能提升：准确率提升8.0%，精度提升0.069，准确率提升0.093，F1 score提升0.102，AUC提升0.059。这种研究为心理健康预测和监测提供了基础性的发现，并提供了在心理健康分析领域中创新的解决方案，以 navigating 稀缺数据特征和心理健康多标签分类的挑战。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-to-Assess-Dis-agreement-Among-Diverse-Rater-Groups"><a href="#A-Framework-to-Assess-Dis-agreement-Among-Diverse-Rater-Groups" class="headerlink" title="A Framework to Assess (Dis)agreement Among Diverse Rater Groups"></a>A Framework to Assess (Dis)agreement Among Diverse Rater Groups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05074">http://arxiv.org/abs/2311.05074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinodkumar Prabhakaran, Christopher Homan, Lora Aroyo, Alicia Parrish, Alex Taylor, Mark Díaz, Ding Wang</li>
<li>for: 本研究旨在探讨对话AI中的安全保障问题，尤其是针对人工评分和反馈的限制，以及这些评分的主观性和多样性。</li>
<li>methods: 本研究提出了一种全面的分歧分析框架，用于评估不同评分者群体之间的视角多样性。这种框架基于一个大量的人工评分 dataset，并通过多种方法来评估分歧。</li>
<li>results: results reveal that there are specific rater groups that have more diverse perspectives than the rest, and informs demographic axes that are crucial to consider for safety annotations. 这些结果表明，有些评分者群体的视角更加多样，并且提供了关键的人类特征轴，用于安全注释。<details>
<summary>Abstract</summary>
Recent advancements in conversational AI have created an urgent need for safety guardrails that prevent users from being exposed to offensive and dangerous content. Much of this work relies on human ratings and feedback, but does not account for the fact that perceptions of offense and safety are inherently subjective and that there may be systematic disagreements between raters that align with their socio-demographic identities. Instead, current machine learning approaches largely ignore rater subjectivity and use gold standards that obscure disagreements (e.g., through majority voting). In order to better understand the socio-cultural leanings of such tasks, we propose a comprehensive disagreement analysis framework to measure systematic diversity in perspectives among different rater subgroups. We then demonstrate its utility by applying this framework to a dataset of human-chatbot conversations rated by a demographically diverse pool of raters. Our analysis reveals specific rater groups that have more diverse perspectives than the rest, and informs demographic axes that are crucial to consider for safety annotations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的对话AI的进步创造了一个紧迫的需求，即防止用户暴露于不当和危险的内容。大多数这些工作依赖于人类评级和反馈，但不考虑评级者的主观性和可能存在的系统性的不一致，这些不一致与评级者的社会人类特征有关。而当前的机器学习方法大多忽略评级者的主观性，使用金标准来隐藏不一致（例如，通过多数投票）。为了更好地理解这类任务的社会文化倾向，我们提出了一个全面的不一致分析框架，用于测量不同评级者 subgroup 中的系统性多样性。然后，我们通过应用这个框架于一个人类-聊天机器人对话评级Pool 中的数据集来证明其实用性。我们的分析发现特定的评级者 subgroup 有更多的多样性，并提供了关键的人类轴，用于安全注释。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exploration-with-Unlabeled-Prior-Data"><a href="#Accelerating-Exploration-with-Unlabeled-Prior-Data" class="headerlink" title="Accelerating Exploration with Unlabeled Prior Data"></a>Accelerating Exploration with Unlabeled Prior Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05067">http://arxiv.org/abs/2311.05067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyang Li, Jason Zhang, Dibya Ghosh, Amy Zhang, Sergey Levine</li>
<li>for: 解决标准奖励学习（RL）算法在稀疏奖励任务上学习缺乏问题。</li>
<li>methods: 我们提出了一种简单的方法，通过在在线经验中学习奖励模型，将无奖数据标签为乐观奖励，并将其与在线数据同时使用以优化策略和评价器。</li>
<li>results: 我们的结果表明，可以很容易地将无奖数据 integrate into现有的在线RL算法中，并且这种方法在一些稀疏奖励领域中显示出了效果，包括AntMaze领域、Adroit手动操作领域和一个视觉模拟Robotic manipulation领域。<details>
<summary>Abstract</summary>
Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.
</details>
<details>
<summary>摘要</summary>
学习从笔记 reward 信号解决任务是标准人工智能学习（RL）算法的主要挑战。然而，在实际世界中，代理人很少需要从头开始解决笔记 reward 任务。更frequently，我们可以利用先前的经验，从而更好地探索新任务。在这项工作中，我们研究了如何使用没有奖券标签的先前数据来导航和加速新任务的探索。我们提议一种简单的方法，即在在线经验中学习奖券模型，将无奖券数据标签为乐观奖券，然后将其与在线数据并行使用于下游策略和评估器优化。这种总体方法在一些笔记 reward 领域中实现了快速探索，包括AntMaze 领域、Adroit 手动操作领域和视觉模拟 робо控制领域。我们的结果表明可以轻松地将无奖券数据 incorporated 到现有的在线 RL 算法中，以及这种方法的（可能吸引人的）效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.AI_2023_11_09/" data-id="clot2mh9c0071x788fxyp9hh0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.CL_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T11:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/cs.CL_2023_11_09/">cs.CL - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FAMuS-Frames-Across-Multiple-Sources"><a href="#FAMuS-Frames-Across-Multiple-Sources" class="headerlink" title="FAMuS: Frames Across Multiple Sources"></a>FAMuS: Frames Across Multiple Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05601">http://arxiv.org/abs/2311.05601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Vashishtha, Alexander Martin, William Gantt, Benjamin Van Durme, Aaron Steven White</li>
<li>for: 提供更深刻的事件理解，通过跨文档的信息汇集</li>
<li>methods: 使用Wikipedia passage和不同类型的源文章（非Wikipedia），并对事件和句子进行框架标注</li>
<li>results: 实现了两个关键的事件理解任务：确认文档是否为目标报道事件的有效来源，以及在报道和正确的源文章中提取全文 Argument EXTRACTION<details>
<summary>Abstract</summary>
Understanding event descriptions is a central aspect of language processing, but current approaches focus overwhelmingly on single sentences or documents. Aggregating information about an event \emph{across documents} can offer a much richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia passages that \emph{report} on some event, paired with underlying, genre-diverse (non-Wikipedia) \emph{source} articles for the same event. Events and (cross-sentence) arguments in both report and source are annotated against FrameNet, providing broad coverage of different event types. We present results on two key event understanding tasks enabled by FAMuS: \emph{source validation} -- determining whether a document is a valid source for a target report event -- and \emph{cross-document argument extraction} -- full-document argument extraction for a target event from both its report and the correct source article. We release both FAMuS and our models to support further research.
</details>
<details>
<summary>摘要</summary>
理解事件描述是语言处理的中心方面，但现有的方法主要集中在单个句子或文档之上。将信息about事件通过文档进行聚合可以提供更深刻的理解。为此，我们提出了FAMuS，一个新的Wikipedia段落和下层、种类多样（非Wikipedia）的源文章对应的事件集。这些事件和跨句子Argument在报道和源文章上都被注解为FrameNet，从而提供了不同事件类型的广泛覆盖。我们对两个关键的事件理解任务进行了实验：一是确定报道事件的目标文档是否为有效的源文档（源验证），二是从报道和正确的源文章中提取全文Argument（跨文档Argument提取）。我们将FAMuS和我们的模型公开发布，以支持进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Iron-ic-Melting-Pot-Reviewing-Human-Evaluation-in-Humour-Irony-and-Sarcasm-Generation"><a href="#The-Iron-ic-Melting-Pot-Reviewing-Human-Evaluation-in-Humour-Irony-and-Sarcasm-Generation" class="headerlink" title="The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation"></a>The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05552">http://arxiv.org/abs/2311.05552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Loakman, Aaron Maladry, Chenghua Lin</li>
<li>for: 本研究评估了自然语言生成系统的评估方法，强调了评估人员的特点对于生成不同类型语言（幽默、讽刺、讲究）的影响。</li>
<li>methods: 本研究使用了对例语言形式的分析，以及对当前NLG研究中评估方法的批判性评估。</li>
<li>results: 研究发现，评估人员的特点对于语言形式的解释有很大影响，而现有的评估方法往往不具有开放报告评估人员特点的特点。<details>
<summary>Abstract</summary>
Human evaluation is often considered to be the gold standard method of evaluating a Natural Language Generation system. However, whilst its importance is accepted by the community at large, the quality of its execution is often brought into question. In this position paper, we argue that the generation of more esoteric forms of language - humour, irony and sarcasm - constitutes a subdomain where the characteristics of selected evaluator panels are of utmost importance, and every effort should be made to report demographic characteristics wherever possible, in the interest of transparency and replicability. We support these claims with an overview of each language form and an analysis of examples in terms of how their interpretation is affected by different participant variables. We additionally perform a critical survey of recent works in NLG to assess how well evaluation procedures are reported in this subdomain, and note a severe lack of open reporting of evaluator demographic information, and a significant reliance on crowdsourcing platforms for recruitment.
</details>
<details>
<summary>摘要</summary>
人类评估通常被认为是自然语言生成系统的黄金标准评估方法。然而，虽然其重要性得到了社区的普遍认可，但其执行质量往往受到质疑。在这篇位点纸中，我们 argue that生成更加罕见的语言形式，如幽默、讽刺和讲究，是评估自然语言生成系统的一个子领域，其中选择的评估人员特征的重要性在最高点。我们支持这些主张通过语言形式的概述和例子的分析来证明，并且对近期NLG研究进行了批判性的检查，发现评估过程的报道不够透明和可重复。此外，我们还发现了评估人员的民族特征不够公开报道，以及依赖于协同平台来吸引参与者的偏好。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-to-End-Spoken-Grammatical-Error-Correction"><a href="#Towards-End-to-End-Spoken-Grammatical-Error-Correction" class="headerlink" title="Towards End-to-End Spoken Grammatical Error Correction"></a>Towards End-to-End Spoken Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05550">http://arxiv.org/abs/2311.05550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano Bannò, Rao Ma, Mengjie Qian, Kate M. Knill, Mark J. F. Gales</li>
<li>for: 这个论文的目的是提出一种新的端到端方法来进行口语 grammatical error correction (GEC)。</li>
<li>methods: 这种端到端方法利用了一个基于 speech recognition 的基础模型 Whisper，可以取代传统的批处理 pipeline。</li>
<li>results: 研究发现，使用端到端方法可以实现口语 GEC，但由于数据的限制，其现在的性能较差于使用大量文本基于 GEC 数据。然而，端到端的不连续检测和消除方法可以超越传统的批处理方法。<details>
<summary>Abstract</summary>
Grammatical feedback is crucial for L2 learners, teachers, and testers. Spoken grammatical error correction (GEC) aims to supply feedback to L2 learners on their use of grammar when speaking. This process usually relies on a cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with the associated concern of propagating errors between these individual modules. In this paper, we introduce an alternative "end-to-end" approach to spoken GEC, exploiting a speech recognition foundation model, Whisper. This foundation model can be used to replace the whole framework or part of it, e.g., ASR and disfluency removal. These end-to-end approaches are compared to more standard cascaded approaches on the data obtained from a free-speaking spoken language assessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is possible within this architecture, but the lack of available data limits current performance compared to a system using large quantities of text-based GEC data. Conversely, end-to-end disfluency detection and removal, which is easier for the attention-based Whisper to learn, does outperform cascaded approaches. Additionally, the paper discusses the challenges of providing feedback to candidates when using end-to-end systems for spoken GEC.
</details>
<details>
<summary>摘要</summary>
grammatical feedback 是对 L2 学习者、教师和测试人员的关键。口语grammatical error correction (GEC) 的目标是为 L2 学习者提供语法使用时的反馈。这个过程通常包括一个级联ipeline，包括 ASR 系统、缺失除去和 GEC，并且有关联的担忧是在这些个人模块之间传递错误。在这篇论文中，我们介绍了一种代替性的 "end-to-end" 方法，使用 Whisper 基础模型进行口语 GEC。这个基础模型可以用于替换整个框架或一部分，例如 ASR 和缺失除去。这些 end-to-end 方法与更惯用的级联方法进行比较，使用来自自由说话的语言评估测试（Linguaskill）的数据。结果表明，end-to-end 口语 GEC 是可能的，但由于数据的有限性，其现在性相比于使用大量文本基础 datos GEC 数据的系统相对较差。然而，end-to-end 缺失检测和除去，这是对 Attention-based Whisper 更容易学习的任务，实际上超过了级联approaches。论文还讨论了使用 end-to-end 系统进行口语 GEC 时给候补的挑战。
</details></li>
</ul>
<hr>
<h2 id="All-Should-Be-Equal-in-the-Eyes-of-Language-Models-Counterfactually-Aware-Fair-Text-Generation"><a href="#All-Should-Be-Equal-in-the-Eyes-of-Language-Models-Counterfactually-Aware-Fair-Text-Generation" class="headerlink" title="All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation"></a>All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05451">http://arxiv.org/abs/2311.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pragyan Banerjee, Abhinav Java, Surgan Jandial, Simra Shahid, Shaz Furniturewala, Balaji Krishnamurthy, Sumit Bhatia</li>
<li>for: 提高语音模型的公平性，以避免因训练数据中的偏见而导致的不公平性问题。</li>
<li>methods: 提出了一种Counterfactually Aware Fair InferencE（CAFIE）框架，通过比较不同民族群体下的模型理解来生成更公平的句子。</li>
<li>results: CAFIE在不同大小的基础语音模型和三个多样化的数据集上进行了广泛的实验，并显示了较强的表现，可以生成更公平的文本，同时保持语音模型的能力。<details>
<summary>Abstract</summary>
Fairness in Language Models (LMs) remains a longstanding challenge, given the inherent biases in training data that can be perpetuated by models and affect the downstream tasks. Recent methods employ expensive retraining or attempt debiasing during inference by constraining model outputs to contrast from a reference set of biased templates or exemplars. Regardless, they dont address the primary goal of fairness to maintain equitability across different demographic groups. In this work, we posit that inferencing LMs to generate unbiased output for one demographic under a context ensues from being aware of outputs for other demographics under the same context. To this end, we propose Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically compares the model understanding of diverse demographics to generate more equitable sentences. We conduct an extensive empirical evaluation using base LMs of varying sizes and across three diverse datasets and found that CAFIE outperforms strong baselines. CAFIE produces fairer text and strikes the best balance between fairness and language modeling capability
</details>
<details>
<summary>摘要</summary>
“对于语言模型（LM）中的公平性仍然是一个长期挑战，因为训练数据中内置的偏见可能会被模型传播到下游任务中，导致不公正的结果。现有的方法通常是耗费高的重训或在推理过程中对模型的输出进行减偏，但这些方法不能实现公平性的主要目标，即在不同的民族群体之间维持平等。在这个工作中，我们提出了Counterfactually Aware Fair InferencE（CAFIE）框架，它在对于一个民族群体的推理过程中，通过比较模型对不同民族群体的理解，生成更公平的句子。我们进行了广泛的实验研究，使用不同的基础LM和三个多样化的数据集，发现CAFIE对于公平性和语言模型能力都能实现更好的平衡”
</details></li>
</ul>
<hr>
<h2 id="Memorisation-Cartography-Mapping-out-the-Memorisation-Generalisation-Continuum-in-Neural-Machine-Translation"><a href="#Memorisation-Cartography-Mapping-out-the-Memorisation-Generalisation-Continuum-in-Neural-Machine-Translation" class="headerlink" title="Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation"></a>Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05379">http://arxiv.org/abs/2311.05379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Verna Dankers, Ivan Titov, Dieuwke Hupkes</li>
<li>for: 这个论文研究了神经网络模型在训练过程中是否能够快速记忆一些源-目标映射，以及这种记忆是否会影响神经网络模型的性能。</li>
<li>methods: 该论文使用了counterfactual memorization metric来构建500万个NMT数据点的记忆-总结特征图，并用这个图来预测NMT模型的表现。</li>
<li>results: 研究发现，NMT模型在训练过程中会快速记忆一些数据点，但是这种记忆并不一定是好的。此外，研究还发现，模型在不同的数据点上的表现会受到数据点的表面特征和模型在每个数据点上的训练信号的影响。<details>
<summary>Abstract</summary>
When training a neural network, it will quickly memorise some source-target mappings from your dataset but never learn some others. Yet, memorisation is not easily expressed as a binary feature that is good or bad: individual datapoints lie on a memorisation-generalisation continuum. What determines a datapoint's position on that spectrum, and how does that spectrum influence neural models' performance? We address these two questions for neural machine translation (NMT) models. We use the counterfactual memorisation metric to (1) build a resource that places 5M NMT datapoints on a memorisation-generalisation map, (2) illustrate how the datapoints' surface-level characteristics and a models' per-datum training signals are predictive of memorisation in NMT, (3) and describe the influence that subsets of that map have on NMT systems' performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Build a resource that places 5M NMT datapoints on a memorization-generalization map2. Illustrate how the datapoints’ surface-level characteristics and a models’ per-datum training signals are predictive of memorization in NMT3. Describe the influence that subsets of that map have on NMT systems’ performance.Translated into Simplified Chinese: WHEN 训练一个神经网络，它很快就会记忆一些源-目标映射从你的数据集中，但从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神</details></li>
</ol>
<hr>
<h2 id="There’s-no-Data-Like-Better-Data-Using-QE-Metrics-for-MT-Data-Filtering"><a href="#There’s-no-Data-Like-Better-Data-Using-QE-Metrics-for-MT-Data-Filtering" class="headerlink" title="There’s no Data Like Better Data: Using QE Metrics for MT Data Filtering"></a>There’s no Data Like Better Data: Using QE Metrics for MT Data Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05350">http://arxiv.org/abs/2311.05350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Thorsten Peter, David Vilar, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Markus Freitag</li>
<li>for: 这篇论文是为了评估机器翻译输出质量而写的。</li>
<li>methods: 这篇论文使用了神经级别评估方法来评估机器翻译输出的质量。</li>
<li>results: 研究发现，通过选择训练数据中最高质量的句子对，可以提高翻译质量，同时减少训练数据的量一半。<details>
<summary>Abstract</summary>
Quality Estimation (QE), the evaluation of machine translation output without the need of explicit references, has seen big improvements in the last years with the use of neural metrics. In this paper we analyze the viability of using QE metrics for filtering out bad quality sentence pairs in the training data of neural machine translation systems~(NMT). While most corpus filtering methods are focused on detecting noisy examples in collections of texts, usually huge amounts of web crawled data, QE models are trained to discriminate more fine-grained quality differences. We show that by selecting the highest quality sentence pairs in the training data, we can improve translation quality while reducing the training size by half. We also provide a detailed analysis of the filtering results, which highlights the differences between both approaches.
</details>
<details>
<summary>摘要</summary>
Quality Estimation (QE)，机器翻译输出评估的方法，在过去几年内有了大幅度的改进，通过使用神经级别的评估指标。在这篇论文中，我们分析了使用QE指标来过滤机器翻译系统（NMT）的训练数据中的坏质句子对的可能性。大多数文库过滤方法通常是对大量的网络抓取数据进行干扰检测，而QE模型则是通过检测更细化的质量差异来训练。我们发现，通过选择训练数据中的高品质句子对，可以提高翻译质量，同时减少训练数据的一半。我们还提供了过滤结果的详细分析，这些分析结果显示了两种方法之间的差异。
</details></li>
</ul>
<hr>
<h2 id="DeeLM-Dependency-enhanced-Large-Language-Model-for-Sentence-Embeddings"><a href="#DeeLM-Dependency-enhanced-Large-Language-Model-for-Sentence-Embeddings" class="headerlink" title="DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings"></a>DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05296">http://arxiv.org/abs/2311.05296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianming Li, Jing Li</li>
<li>for: 提高句子嵌入表示性</li>
<li>methods: 利用依赖关系增强大型语言模型（LLM），提高句子嵌入表示性</li>
<li>results: 比基eline和其他方法表现出色，在 semantic textual similarity（STS）任务中达到了状态理论水平<details>
<summary>Abstract</summary>
Recent studies have proposed using large language models (LLMs) for sentence embeddings. However, most existing LLMs are built with an autoregressive architecture that primarily captures forward dependencies while neglecting backward dependencies. Previous work has highlighted the importance of backward dependencies in improving sentence embeddings. To address this issue, in this paper, we first present quantitative evidence demonstrating the limited learning of backward dependencies in LLMs. Then, we propose a novel approach called Dependency-Enhanced Large Language Model (DeeLM) to improve sentence embeddings. Specifically, we found a turning point in LLMs, where surpassing specific LLM layers leads to a significant performance drop in the semantic textual similarity (STS) task. STS is a crucial task for evaluating sentence embeddings. We then extract the layers after the turning point to make them bidirectional, allowing for the learning of backward dependencies. Extensive experiments demonstrate that DeeLM outperforms baselines and achieves state-of-the-art performance across various STS tasks.
</details>
<details>
<summary>摘要</summary>
近期研究提出使用大型语言模型（LLM）进行句子嵌入。然而，现有的大多数LLM都是基于推导式架构，主要捕捉前向依赖关系而忽视后向依赖关系。先前的工作表明了后向依赖关系在改进句子嵌入上的重要性。为解决这个问题，在这篇论文中，我们首先提供了LLM中后向依赖关系的有限学习证据。然后，我们提出了一种名为依赖加强大型语言模型（DeeLM）的新方法，以改进句子嵌入。具体来说，我们在LLM中找到了一个转折点，其中继承specific LLM层会导致STS任务中的性能下降。STS任务是评估句子嵌入的关键任务。我们然后提取这些层，使其变为双向的，以便学习后向依赖关系。广泛的实验证明了DeeLM在不同STS任务中的优秀表现，超过了基eline和状态之前的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-from-Text-Unveiling-Interactions-between-Variables"><a href="#Causal-Inference-from-Text-Unveiling-Interactions-between-Variables" class="headerlink" title="Causal Inference from Text: Unveiling Interactions between Variables"></a>Causal Inference from Text: Unveiling Interactions between Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05286">http://arxiv.org/abs/2311.05286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhou, Yulan He</li>
<li>for: 用于估计文本数据中的 causal effect</li>
<li>methods: 使用 adjustment for latent covariates 方法，并且通过解耦变量来减少偏见</li>
<li>results: 在实验中，提出的模型significantly outperforms  latest strong baselines，并且在实际场景中可以有效地减少偏见。<details>
<summary>Abstract</summary>
Adjusting for latent covariates is crucial for estimating causal effects from observational textual data. Most existing methods only account for confounding covariates that affect both treatment and outcome, potentially leading to biased causal effects. This bias arises from insufficient consideration of non-confounding covariates, which are relevant only to either the treatment or the outcome. In this work, we aim to mitigate the bias by unveiling interactions between different variables to disentangle the non-confounding covariates when estimating causal effects from text. The disentangling process ensures covariates only contribute to their respective objectives, enabling independence between variables. Additionally, we impose a constraint to balance representations from the treatment group and control group to alleviate selection bias. We conduct experiments on two different treatment factors under various scenarios, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis on earnings call transcripts demonstrates that our model can effectively disentangle the variables, and further investigations into real-world scenarios provide guidance for investors to make informed decisions.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:需要调整隐藏的共分布量是适当估计从观察性数据中的 causal effect 的关键。大多数现有的方法只考虑到对从事和结果都有影响的共分布量，可能导致偏见的 causal effect。这个偏见来自于不足考虑非共分布量，这些共分布量仅仅对从事或结果有影响。在这个工作中，我们想要减轻这个偏见，通过揭露不同变量之间的互动来分离非共分布量，从而确保共分布量仅对其所属的目标做出贡献，并且使变量独立。此外，我们对从事和控制群体的表现进行平衡约束，以减少选择偏见。我们在两个不同的从事因子下进行了多种情况的实验，并且我们的提案模型在最近的强大基准下表现出色。此外，我们对会计呼缩说明中的分析表明，我们的模型可以有效地分离变量，并且进一步的实验在实际情况中提供了投资者做出 Informed 决策的指南。
</details></li>
</ul>
<hr>
<h2 id="Modelling-prospective-memory-and-resilient-situated-communications-via-Wizard-of-Oz"><a href="#Modelling-prospective-memory-and-resilient-situated-communications-via-Wizard-of-Oz" class="headerlink" title="Modelling prospective memory and resilient situated communications via Wizard of Oz"></a>Modelling prospective memory and resilient situated communications via Wizard of Oz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05268">http://arxiv.org/abs/2311.05268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhe Li, Frank Broz, Mark Neerincx</li>
<li>for: 这个研究旨在 explore 社交辅助机器人（SAR）的记忆模型，以便与老年人进行日常活动时的人机交流。</li>
<li>methods: 该研究使用了一个家庭 Setting 中的老年人和机器人进行人机交流的enario，以收集失败的语音技术和人机交流中的共享记忆数据。</li>
<li>results: 研究会获得日常活动中的语音技术失败和人机交流中的共享记忆数据，以便更好地理解社交辅助机器人（SAR）的记忆模型。<details>
<summary>Abstract</summary>
This abstract presents a scenario for human-robot action in a home setting involving an older adult and a robot. The scenario is designed to explore the envisioned modelling of memory for communication with a socially assistive robots (SAR). The scenario will enable the gathering of data on failures of speech technology and human-robot communication involving shared memory that may occur during daily activities such as a music-listening activity.
</details>
<details>
<summary>摘要</summary>
这个报告提出了一个家庭设定下的人机合作场景，其中包括老年人和一个社会辅助机器人（SAR）。这个场景的目的是探索对话模型的应用，以便与SAR进行交流。通过日常活动，如音乐听众活动，可以收集失败的语音技术和人机交流中的共享记忆的数据。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Hallucination-in-Large-Language-Models-Principles-Taxonomy-Challenges-and-Open-Questions"><a href="#A-Survey-on-Hallucination-in-Large-Language-Models-Principles-Taxonomy-Challenges-and-Open-Questions" class="headerlink" title="A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"></a>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05232">http://arxiv.org/abs/2311.05232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu</li>
<li>for: 这篇论文旨在提供关于大语言模型幻觉的现代概念和研究进展的全面和深入的综述。</li>
<li>methods: 论文使用了一种创新的幻觉分类法，并对幻觉的因素进行了分析。</li>
<li>results: 论文对幻觉探测和缓解方法进行了全面的介绍，并提出了未来研究方向的问题。<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs."中文翻译：大型自然语言处理（NLP）模型（LLM）的出现标志着NLP领域的重要突破，导致文本理解和生成的显著进步。然而，LLMs同时也显示出一种重要的倾向，即生成幻觉，导致与实际世界事实或用户输入不符的内容。这种现象对LLMs的实际应用带来了重大挑战，也引起了对幻觉的检测和 Mitigation 的关注。在本文中，我们提供了LLM幻觉领域的全面和深入的概述，包括幻觉分类、幻觉的原因、幻觉检测方法和标准、以及适应幻觉的方法。最后，我们分析了当前的挑战和未知点，以便顺利地探索未来LLM幻觉领域的研究方向。>>
</details></li>
</ul>
<hr>
<h2 id="PRODIGy-a-PROfile-based-DIalogue-Generation-dataset"><a href="#PRODIGy-a-PROfile-based-DIalogue-Generation-dataset" class="headerlink" title="PRODIGy: a PROfile-based DIalogue Generation dataset"></a>PRODIGy: a PROfile-based DIalogue Generation dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05195">http://arxiv.org/abs/2311.05195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela Occhipinti, Serra Sinem Tekiroglu, Marco Guerini</li>
<li>for: 提高对话机器人的一致性和 coherence，以便进行更好的对话。</li>
<li>methods: 使用标准和更复杂的对话者表示，并创建一个新的资源，每个对话都与所有可能的对话者表示相对应。</li>
<li>results: 比较 Profile-based 模型和只使用对话的模型， Profile-based 模型在预测和交互中有更好的一致性和泛化能力，并且人工评价也表明，生成与对话和 Profile 相符的内容得到了普遍的偏好。<details>
<summary>Abstract</summary>
Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we propose a unified framework in which we bring together both standard and more sophisticated profile representations by creating a new resource where each dialogue is aligned with all possible speaker representations such as communication style, biographies, and personality. This framework allows to test several baselines built using generative language models with several profile configurations. The automatic evaluation shows that profile-based models have better generalisation capabilities than models trained on dialogues only, both in-domain and cross-domain settings. These results are consistent for fine-tuned models and instruction-based LLMs. Additionally, human evaluation demonstrates a clear preference for generations consistent with both profile and context. Finally, to account for possible privacy concerns, all experiments are done under two configurations: inter-character and intra-character. In the former, the LM stores the information about the character in its internal representation, while in the latter, the LM does not retain any personal information but uses it only at inference time.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)提供对话代理人 Profile 表示可以提高对话的一致性和 coherence，导致更好的对话。然而，当前的对话 Profile 数据集用于训练这些代理人只包含了简单的对话特定的 Profile 表示或困难收集的含义 Profile 表示。在这项工作中，我们提出了一个统一的框架，在该框架中，每个对话都与所有可能的 Speaker 表示相对应，包括交流风格、生平、人性等。这个框架允许我们测试多种 Profile 配置下的基elines 使用生成语言模型。自动评估结果表明， Profile-based 模型在预测和跨预测设置中具有更好的泛化能力。这些结果是精度模型和指令基本语言模型的情况下均持。此外，人工评估表明，生成与 Profile 和上下文一致的生成具有明显的偏好。最后，为了解决 posible privacy concern，我们在两种配置下进行所有实验：inter-character 和 intra-character。在前者中，LM 将对话人物的信息存储在其内部表示中，而在后者中，LM 不会保留任何个人信息，只在推理时使用它们。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Prompt-Engineering-for-Biomedical-Query-Focused-Multi-Document-Summarisation"><a href="#Large-Language-Models-and-Prompt-Engineering-for-Biomedical-Query-Focused-Multi-Document-Summarisation" class="headerlink" title="Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation"></a>Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05169">http://arxiv.org/abs/2311.05169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Mollá</li>
<li>for: 本研究使用提示工程和GPT-3.5进行生物医学问题焦点多文摘要。</li>
<li>methods: 使用GPT-3.5和合适的提示，我们的系统在2023年 BioASQ 挑战（BioASQ 11b）中获得了最高的 ROUGE-F1 成绩。</li>
<li>results: 研究证明，1）含有少量示例的提示通常比零例示例提示更好；2）检索增强生成带来最大的改进。这些提示使我们的最佳运行排名在 BioASQ 11b 中的前两名，这说明使用适当的提示对大语言模型，特别是GPT-3.5，在问题焦点 summarization 中具有强大的能力。<details>
<summary>Abstract</summary>
This paper reports on the use of prompt engineering and GPT-3.5 for biomedical query-focused multi-document summarisation. Using GPT-3.5 and appropriate prompts, our system achieves top ROUGE-F1 results in the task of obtaining short-paragraph-sized answers to biomedical questions in the 2023 BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in other domains: 1) Prompts that incorporated few-shot samples generally improved on their counterpart zero-shot variants; 2) The largest improvement was achieved by retrieval augmented generation. The fact that these prompts allow our top runs to rank within the top two runs of BioASQ 11b demonstrate the power of using adequate prompts for Large Language Models in general, and GPT-3.5 in particular, for query-focused summarisation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Prompts that incorporated few-shot samples generally performed better than their zero-shot counterparts.2. Retrieval-augmented generation led to the largest improvement.The use of effective prompts demonstrates the potential of large language models, specifically GPT-3.5, for query-focused summarization. Our top runs ranked among the top two runs in BioASQ 11b, highlighting the power of prompt engineering for improving summarization performance.</details></li>
</ol>
<hr>
<h2 id="Enhancing-Computation-Efficiency-in-Large-Language-Models-through-Weight-and-Activation-Quantization"><a href="#Enhancing-Computation-Efficiency-in-Large-Language-Models-through-Weight-and-Activation-Quantization" class="headerlink" title="Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization"></a>Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05161">http://arxiv.org/abs/2311.05161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook Choi</li>
<li>for: 提高 LLM 的计算效率，增加Task的准确率</li>
<li>methods: 使用 post-training quantization (PTQ) 技术，并提出了两种新技术： activation-quantization-aware scaling (AQAS) 和 sequence-length-aware calibration (SLAC)，以及一种 hybrid data format combining integer and denormal representations (dINT) 来解决 underflow 问题</li>
<li>results: 对 OPT 和 LLaMA 两种 LLVM 进行了严格的评估，并显示了与标准精度模型相当的任务准确率，同时通过开发兼容 dINT 的数学单元，实现了硬件效率的2倍提高<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency -- a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC unit.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出色，但是它们的部署受到广泛的参数大小和计算需求的限制。这篇论文关注 LLM 中的后期量化（PTQ），特别是4位权值和8位活动（W4A8）量化，以提高计算效率——一个相对较少研究的领域。我们提出了两种创新技术：活动量化相关缩放（AQAS）和目标任务相关准备（SLAC），以增强PTQ，并考虑权值和活动的共同效果。此外，我们介绍了 dINT 混合数据格式，用于解决 W4A8 量化中的下流问题，其中小值会被舍入到零。通过对 LLM 进行严格的评估，包括 OPT 和 LLaMA，我们示出了我们的技术可以提高任务准确率至与权重精度模型相当的水平。再者，我们开发了与 dINT 相容的数学单元，以证明我们的方法可以在硬件效率方面实现2倍的提升。
</details></li>
</ul>
<hr>
<h2 id="Quranic-Conversations-Developing-a-Semantic-Search-tool-for-the-Quran-using-Arabic-NLP-Techniques"><a href="#Quranic-Conversations-Developing-a-Semantic-Search-tool-for-the-Quran-using-Arabic-NLP-Techniques" class="headerlink" title="Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques"></a>Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05120">http://arxiv.org/abs/2311.05120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Shohoud, Maged Shoman, Sarah Abdelazim</li>
<li>for: 该论文目的是开发一种基于语义搜索的古兰经搜索工具，以便 Muslims 更容易找到关注的 ayahs。</li>
<li>methods: 该工具使用了多个模型，包括 SNxLM 模型，并使用 cosine similarity 来测试每个模型的相似性。</li>
<li>results: 使用该工具，可以寻找与用户的查询或提示相关的 ayahs，并达到 cosine similarity score 为 0.97，相当于阿布达讲解。<details>
<summary>Abstract</summary>
The Holy Book of Quran is believed to be the literal word of God (Allah) as revealed to the Prophet Muhammad (PBUH) over a period of approximately 23 years. It is the book where God provides guidance on how to live a righteous and just life, emphasizing principles like honesty, compassion, charity and justice, as well as providing rules for personal conduct, family matters, business ethics and much more. However, due to constraints related to the language and the Quran organization, it is challenging for Muslims to get all relevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence, we developed a Quran semantic search tool which finds the verses pertaining to the user inquiry or prompt. To achieve this, we trained several models on a large dataset of over 30 tafsirs, where typically each tafsir corresponds to one verse in the Quran and, using cosine similarity, obtained the tafsir tensor which is most similar to the prompt tensor of interest, which was then used to index for the corresponding ayah in the Quran. Using the SNxLM model, we were able to achieve a cosine similarity score as high as 0.97 which corresponds to the abdu tafsir for a verse relating to financial matters.
</details>
<details>
<summary>摘要</summary>
《古兰经》被认为是神的 Literal 话语（Allah），由先知穆罕默德（PBUH）在约23年间预言。这是一本包含如何过一个正直和公正的生活的指南，强调诚实、慈悲、慈善和正义等原则，同时还提供了个人行为、家庭事务、商业伦理等方面的规则。然而由于语言和《古兰经》的组织方式，使得穆斯林找到相关的 Ayah（节）很困难。因此，我们开发了一个《古兰经》 semantically 搜索工具，可以找到用户的关注点或提示中相关的 Ayah。为了实现这一点，我们训练了多个模型，使用了一个大量的数据集，包括30多本《塔夫斯尔》，每本《塔夫斯尔》通常对应一节《古兰经》。我们使用cosine similarity来获得最相似的《塔夫斯尔》矩阵，然后使用该矩阵来索引相关的 Ayah 在《古兰经》中。使用 SNxLM 模型，我们能够达到cosine similarity score为0.97，对应的是《阿布达》《古兰经》中的一节关于财务问题。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Translation-Quality-Estimation-Exploiting-Synthetic-Data-and-Pre-trained-Multilingual-Encoder"><a href="#Unsupervised-Translation-Quality-Estimation-Exploiting-Synthetic-Data-and-Pre-trained-Multilingual-Encoder" class="headerlink" title="Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder"></a>Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05117">http://arxiv.org/abs/2311.05117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuto Kuroda, Atsushi Fujita, Tomoyuki Kajiwara, Takashi Ninomiya</li>
<li>for: 这篇论文旨在研究无监督翻译质量估计（TQE）方法，以减少翻译质量估计所需的训练数据成本。</li>
<li>methods: 该论文使用 sintethic TQE 数据和预训练多语言encoder来研究无监督 sentence-level TQE 方法，这些方法在监督训练场景中已经证明有效。</li>
<li>results: 实验结果表明，该方法可以在高资源和低资源翻译方向中高度超越其他无监督 TQE 方法，并在某些零资源翻译方向中 Predicting post-editing effort。<details>
<summary>Abstract</summary>
Translation quality estimation (TQE) is the task of predicting translation quality without reference translations. Due to the enormous cost of creating training data for TQE, only a few translation directions can benefit from supervised training. To address this issue, unsupervised TQE methods have been studied. In this paper, we extensively investigate the usefulness of synthetic TQE data and pre-trained multilingual encoders in unsupervised sentence-level TQE, both of which have been proven effective in the supervised training scenarios. Our experiment on WMT20 and WMT21 datasets revealed that this approach can outperform other unsupervised TQE methods on high- and low-resource translation directions in predicting post-editing effort and human evaluation score, and some zero-resource translation directions in predicting post-editing effort.
</details>
<details>
<summary>摘要</summary>
翻译质量估计（TQE）是指无需参考翻译的翻译质量预测。由于创建TQE训练数据的成本巨大，只有一些翻译方向能够得到超级vised训练。为解决这个问题，无监督TQE方法已经研究过。本文对无监督句级TQE的有用性进行了广泛的调查，包括使用 sintheic TQE数据和预训练多语言编码器。我们的实验表明，这种方法可以在高资源和低资源翻译方向中预测后期编辑努力和人类评估分数，以及一些零资源翻译方向中预测后期编辑努力。
</details></li>
</ul>
<hr>
<h2 id="Conic10K-A-Challenging-Math-Problem-Understanding-and-Reasoning-Dataset"><a href="#Conic10K-A-Challenging-Math-Problem-Understanding-and-Reasoning-Dataset" class="headerlink" title="Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"></a>Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05113">http://arxiv.org/abs/2311.05113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whynlp/conic10k">https://github.com/whynlp/conic10k</a></li>
<li>paper_authors: Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, Yi Zhou</li>
<li>for: 这个论文的目的是为了提供一个有挑战性的数学问题集，以测试人工智能的数学理解和逻辑能力。</li>
<li>methods: 该论文使用了中国高中教育中的圆形sections知识作为数学问题集的基础，并提供了不同的逻辑深度的问题，只需要知道圆形sections即可解决。</li>
<li>results: 实验显示现有的大语言模型，包括GPT-4，在复杂的逻辑任务上表现不佳。我们希望这些结果能够激发更多人开发更高级的自然语言理解和逻辑技术。该数据集和代码可以在GitHub上下载。<details>
<summary>Abstract</summary>
Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.
</details>
<details>
<summary>摘要</summary>
matematik zhishixiang yu jianyan shi yi zhong de zhongdian zhengfu yu jianyan shi zhong de ai (AI) de yi zhong de zhengfu. however, existing benchmarks either require only a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. in this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. for each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. we hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. our dataset and codes are available at https://github.com/whyNLP/Conic10K.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.CL_2023_11_09/" data-id="clot2mhbi00ebx788geyudwz2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.LG_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T10:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/cs.LG_2023_11_09/">cs.LG - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Parallelization-Layouts-for-Large-Scale-Distributed-Model-Training"><a href="#Efficient-Parallelization-Layouts-for-Large-Scale-Distributed-Model-Training" class="headerlink" title="Efficient Parallelization Layouts for Large-Scale Distributed Model Training"></a>Efficient Parallelization Layouts for Large-Scale Distributed Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05610">http://arxiv.org/abs/2311.05610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleph-alpha/neurips-want-submission-efficient-parallelization-layouts">https://github.com/aleph-alpha/neurips-want-submission-efficient-parallelization-layouts</a></li>
<li>paper_authors: Johannes Hagemann, Samuel Weinbach, Konstantin Dobler, Maximilian Schall, Gerard de Melo</li>
<li>For: This paper is written for training large language models efficiently, with a focus on optimizing the use of hardware accelerators and minimizing memory usage.* Methods: The paper uses a combination of compute and memory optimizations, including FlashAttention and sequence parallelism, and conducts an ablation study to determine the most effective training configurations.* Results: The paper achieves state-of-the-art training efficiency results, with a Model FLOPs utilization of 70.5% when training a 13B model, and provides several key recommendations for efficient training of large language models.In Simplified Chinese text, the three information points would be:</li>
<li>for: 这篇论文是为了有效地训练大型自然语言模型而写的，关注硬件加速器的并行和内存使用最优化。</li>
<li>methods: 这篇论文使用了一系列compute和内存优化技术，包括FlashAttention和序列并行，并进行了ablation研究以确定最有效的训练配置。</li>
<li>results: 这篇论文实现了状态静态训练效率结果， Model FLOPs 使用率达到70.5% when training a 13B model，并提供了一些关键的训练大型自然语言模型的建议。<details>
<summary>Abstract</summary>
Efficiently training large language models requires parallelizing across hundreds of hardware accelerators and invoking various compute and memory optimizations. When combined, many of these strategies have complex interactions regarding the final training efficiency. Prior work tackling this problem did not have access to the latest set of optimizations, such as FlashAttention or sequence parallelism. In this work, we conduct a comprehensive ablation study of possible training configurations for large language models. We distill this large study into several key recommendations for the most efficient training. For instance, we find that using a micro-batch size of 1 usually enables the most efficient training layouts. Larger micro-batch sizes necessitate activation checkpointing or higher degrees of model parallelism and also lead to larger pipeline bubbles. Our most efficient configurations enable us to achieve state-of-the-art training efficiency results over a range of model sizes, most notably a Model FLOPs utilization of 70.5% when training a 13B model.
</details>
<details>
<summary>摘要</summary>
培训大型自然语言模型需要并行运行数百个硬件加速器，并 invoke 多种compute和内存优化。当这些策略相互作用时，其最终培训效率会受到复杂的影响。先前的工作没有访问最新的优化技术，如 FlashAttention 或序列并行。在这种工作中，我们进行了大规模的ablation研究，探讨可能的培训配置。我们从这个大型研究中提取了一些关键的建议，以实现最高效的培训。例如，我们发现使用 micro-batch 大小为 1 通常可以实现最高效的培训布局。大于 micro-batch 大小的情况需要进行激活检查点或更高的模型并行和更大的管道弹性，并且会导致更大的管道弹性。我们最高效的配置可以实现一个 Model FLOPs 使用率达到 70.5%，当 trains a 13B 模型。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Generative-Multi-Fidelity-Learning-for-Physical-Simulation"><a href="#Diffusion-Generative-Multi-Fidelity-Learning-for-Physical-Simulation" class="headerlink" title="Diffusion-Generative Multi-Fidelity Learning for Physical Simulation"></a>Diffusion-Generative Multi-Fidelity Learning for Physical Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05606">http://arxiv.org/abs/2311.05606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Wang, Shibo Li, Shikai Fang, Shandian Zhe</li>
<li>for: 这篇论文主要关注于Physical simulation related applications中的多元适性学习，以避免在实际应用中重复运行numerical solver，并将多元适性例子用于训练，以大大减少资料收集成本。</li>
<li>methods: 我们采用了一种基于测度过程的Diffusion-Generative Multi-Fidelity（DGMF）学习方法，使用了随机测度方程（SDE）来实现解析生成。我们还提出了一个条件分布模型来控制解析生成的条件，使用了额外的输入（时间或空间变数）来有效地学习和预测多dimensional的解析阵列。</li>
<li>results: 我们的方法能够实现多元适性学习的统一和有效预测，并且在一些典型应用中显示出了优异的成果，这显示了我们的方法对于Physical simulation related applications的应用具有很大的应用前景。<details>
<summary>Abstract</summary>
Multi-fidelity surrogate learning is important for physical simulation related applications in that it avoids running numerical solvers from scratch, which is known to be costly, and it uses multi-fidelity examples for training and greatly reduces the cost of data collection. Despite the variety of existing methods, they all build a model to map the input parameters outright to the solution output. Inspired by the recent breakthrough in generative models, we take an alternative view and consider the solution output as generated from random noises. We develop a diffusion-generative multi-fidelity (DGMF) learning method based on stochastic differential equations (SDE), where the generation is a continuous denoising process. We propose a conditional score model to control the solution generation by the input parameters and the fidelity. By conditioning on additional inputs (temporal or spacial variables), our model can efficiently learn and predict multi-dimensional solution arrays. Our method naturally unifies discrete and continuous fidelity modeling. The advantage of our method in several typical applications shows a promising new direction for multi-fidelity learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sorting-Out-Quantum-Monte-Carlo"><a href="#Sorting-Out-Quantum-Monte-Carlo" class="headerlink" title="Sorting Out Quantum Monte Carlo"></a>Sorting Out Quantum Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05598">http://arxiv.org/abs/2311.05598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Richter-Powell, Luca Thiede, Alán Asparu-Guzik, David Duvenaud</li>
<li>for: 这个论文目的是为了提出一种可扩展到多个粒子系统的量子水平分子模拟方法，并且能够尊重粒子的 symmetries。</li>
<li>methods: 这个论文使用了一种新的排序层（sortlet）来实现对粒子的排序，这个层的时间复杂度为 $O(N \log N)$，相比于传统的 determinant 方法的 $O(N^3)$。</li>
<li>results:  NUMERICAL 结果表明，通过在 neural-network 后处理器上应用这种排序层，可以实现一种灵活的波函数参数化方法，能够达到化学精度水平，在第一行元素和小分子的基态下预测ground state。<details>
<summary>Abstract</summary>
Molecular modeling at the quantum level requires choosing a parameterization of the wavefunction that both respects the required particle symmetries, and is scalable to systems of many particles. For the simulation of fermions, valid parameterizations must be antisymmetric with respect to the exchange of particles. Typically, antisymmetry is enforced by leveraging the anti-symmetry of determinants with respect to the exchange of matrix rows, but this involves computing a full determinant each time the wavefunction is evaluated. Instead, we introduce a new antisymmetrization layer derived from sorting, the $\textit{sortlet}$, which scales as $O(N \log N)$ with regards to the number of particles -- in contrast to $O(N^3)$ for the determinant. We show numerically that applying this anti-symmeterization layer on top of an attention based neural-network backbone yields a flexible wavefunction parameterization capable of reaching chemical accuracy when approximating the ground state of first-row atoms and small molecules.
</details>
<details>
<summary>摘要</summary>
分子模型在量子层面上需要选择一种Parameterization的浓函数，该函数需要遵循必要的粒子对称性，同时可扩展到多个粒子系统。在蛋白质模拟中，有效的Parameterization必须是对粒子交换的反对称的。通常，对粒子交换的反对称是通过对矩阵行交换的反对称性来实现，但这会导致每次评估浓函数时都需要计算全矩阵。而我们现在引入了一种新的反对称层，称为sortlet，该层的时间复杂度为O(N log N)，与粒子数量相比，是对矩阵的计算时间复杂度（O(N^3））的多ples。我们通过数值方法表明，将sortlet层作为浓函数参数化的 neural network 背景下的折衔函数，可以达到化学精度地approximate初列元素和小分子的ground state。
</details></li>
</ul>
<hr>
<h2 id="A-Coefficient-Makes-SVRG-Effective"><a href="#A-Coefficient-Makes-SVRG-Effective" class="headerlink" title="A Coefficient Makes SVRG Effective"></a>A Coefficient Makes SVRG Effective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05589">http://arxiv.org/abs/2311.05589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidyyd/alpha-svrg">https://github.com/davidyyd/alpha-svrg</a></li>
<li>paper_authors: Yida Yin, Zhiqiu Xu, Zhiyuan Li, Trevor Darrell, Zhuang Liu</li>
<li>for: 这个论文是为了证明Stochastic Variance Reduced Gradient（SVRG）在深度学习中的有效性。</li>
<li>methods: 这篇论文使用了Johnson &amp; Zhang（2013）提出的SVRG优化方法，并通过调整变量减少项的强度来改进其效果。</li>
<li>results: 该论文的分析发现，对于深度网络，SVRG中的变量减少项的强度应该逐渐减小，而且可以通过一个线性减少 schedule来控制。该方法被称为α-SVRG。对于多种网络架构和图像分类 dataset，α-SVRG能够更好地优化 neural networks，并在训练loss中减少training loss。<details>
<summary>Abstract</summary>
Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlights, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our analysis finds that, for deeper networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\alpha$-SVRG. Our results show $\alpha$-SVRG better optimizes neural networks, consistently reducing training loss compared to both baseline and the standard SVRG across various architectures and image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at https://github.com/davidyyd/alpha-SVRG.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bayesian-Methods-for-Media-Mix-Modelling-with-shape-and-funnel-effects"><a href="#Bayesian-Methods-for-Media-Mix-Modelling-with-shape-and-funnel-effects" class="headerlink" title="Bayesian Methods for Media Mix Modelling with shape and funnel effects"></a>Bayesian Methods for Media Mix Modelling with shape and funnel effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05587">http://arxiv.org/abs/2311.05587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Marin</li>
<li>for: 这项研究旨在探讨Maxwell-Boltzmann方程和Michaelis-Menten模型在广告推广中的应用 potential.</li>
<li>methods: 该研究提议将这些方程 incorporated into Hierarchical Bayesian模型来分析消费者行为。</li>
<li>results: 这些方程集 excell in accurately describing complex systems like social interactions and consumer-advertising interactions的随机动态。<details>
<summary>Abstract</summary>
In recent years, significant progress in generative AI has highlighted the important role of physics-inspired models that utilize advanced mathematical concepts based on fundamental physics principles to enhance artificial intelligence capabilities. Among these models, those based on diffusion equations have greatly improved image quality. This study aims to explore the potential uses of Maxwell-Boltzmann equation, which forms the basis of the kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix Modelling (MMM) applications. We propose incorporating these equations into Hierarchical Bayesian models to analyse consumer behaviour in the context of advertising. These equation sets excel in accurately describing the random dynamics in complex systems like social interactions and consumer-advertising interactions.
</details>
<details>
<summary>摘要</summary>
近年来，生成AI的进步强调了基于物理原理的模型的重要性，以提高人工智能能力。这些模型中，基于卷积Equation的模型在图像质量方面做出了大量改进。本研究旨在探讨使用Maxwell-Boltzmann方程和Michaelis-Menten模型在市场混合模型（MMM）应用中的潜在用途。我们建议将这些方程集成到层次 bayesian模型中，以分析消费者行为在广告投放的 контексте。这些方程集在描述复杂系统中的随机动态方面具有卓越的表现。
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Wasserstein-DRO"><a href="#Outlier-Robust-Wasserstein-DRO" class="headerlink" title="Outlier-Robust Wasserstein DRO"></a>Outlier-Robust Wasserstein DRO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05573">http://arxiv.org/abs/2311.05573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sbnietert/outlier-robust-wdro">https://github.com/sbnietert/outlier-robust-wdro</a></li>
<li>paper_authors: Sloan Nietert, Ziv Goldfeld, Soroosh Shafiee</li>
<li>for: 该论文是为了解决数据驱动决策中受到不确定性的问题而设计的。</li>
<li>methods: 该论文使用了 Wasserstein DRO（WDRO）方法，WDRO 方法是一种可以在数据分布不确定性下进行决策的方法。</li>
<li>results: 该论文提出了一种robust WDRO框架，该框架可以快速计算并且可以抵御各种不同类型的干扰（包括水平干扰和总变分干扰）。<details>
<summary>Abstract</summary>
Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.
</details>
<details>
<summary>摘要</summary>
distributionally robust optimization (DRO) 是一种有效的方法 для决策数据驱动下的不确定性。水平不确定性，如抽样或本地扰动数据点，可以通过水stein DRO (WDRO) 捕捉，WDRO 目的是学习一个在水stein圈心 around observed data distribution 上 uniformly well 的模型。然而，WDRO 不能考虑非水平扰动，如反对攻击异常点，这些扰动可以很大地扭曲水stein距离测量，阻碍学习的模型。我们Fill this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (waterstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Neural-Network-Statistics-for-Low-Power-DNN-Inference"><a href="#Exploiting-Neural-Network-Statistics-for-Low-Power-DNN-Inference" class="headerlink" title="Exploiting Neural-Network Statistics for Low-Power DNN Inference"></a>Exploiting Neural-Network Statistics for Low-Power DNN Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05557">http://arxiv.org/abs/2311.05557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Bamberg, Ardalan Najafi, Alberto Garcia-Ortiz</li>
<li>for: 这个研究目的是为了提高边缘人工智能推断引擎的能效性，减少处理器和内存的能力浪费。</li>
<li>methods: 这个研究使用了无预量处理和统计分析方法来减少处理器和内存的能力浪费，并且不会对精度产生影响。</li>
<li>results: 这个研究获得了最多80%的处理器和内存能力浪费减少，并且可以降低compute block的能源消耗量最多39%，而且不会对精度产生影响。<details>
<summary>Abstract</summary>
Specialized compute blocks have been developed for efficient DNN execution. However, due to the vast amount of data and parameter movements, the interconnects and on-chip memories form another bottleneck, impairing power and performance. This work addresses this bottleneck by contributing a low-power technique for edge-AI inference engines that combines overhead-free coding with a statistical analysis of the data and parameters of neural networks. Our approach reduces the interconnect and memory power consumption by up to 80% for state-of-the-art benchmarks while providing additional power savings for the compute blocks by up to 39%. These power improvements are achieved with no loss of accuracy and negligible hardware cost.
</details>
<details>
<summary>摘要</summary>
专用计算块已经为深度学习模型的高效执行而开发。然而，由于巨量数据和参数的移动，总线和镜像内存又成为了另一个瓶颈，影响了能效性。这项工作通过将数据和参数统计分析，并使用无开销编程技术，解决了这个瓶颈，从而实现了减少总线和内存的功率消耗，最高可达80%。此外，我们的方法还可以为计算块提供更多的功率改善，最高可达39%。这些功率改善不会导致精度损失和硬件成本的增加。
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-generalization-bounds-for-learning-from-quantum-data"><a href="#Information-theoretic-generalization-bounds-for-learning-from-quantum-data" class="headerlink" title="Information-theoretic generalization bounds for learning from quantum data"></a>Information-theoretic generalization bounds for learning from quantum data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05529">http://arxiv.org/abs/2311.05529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Caro, Tom Gur, Cambyse Rouzé, Daniel Stilck França, Sathyawageeswar Subramanian</li>
<li>for: 量子学习理论的普遍性和综合性</li>
<li>methods: 使用量子优化运输和量子凝固不等式来建立非凡量子抽象凝固，以获得量子学习场景中的普遍性和综合性</li>
<li>results: 提出一种通用的量子学习框架，可以描述量子学习器在训练量子-классической数据后，如何在新数据上进行推断，并且提供了一系列基于量子信息论和量子概率论的普遍性和综合性 bounds。<details>
<summary>Abstract</summary>
Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We propose a general mathematical formalism for describing quantum learning by training on classical-quantum data and then testing how well the learned hypothesis generalizes to new data. In this framework, we prove bounds on the expected generalization error of a quantum learner in terms of classical and quantum information-theoretic quantities measuring how strongly the learner's hypothesis depends on the specific data seen during training.   To achieve this, we use tools from quantum optimal transport and quantum concentration inequalities to establish non-commutative versions of decoupling lemmas that underlie recent information-theoretic generalization bounds for classical machine learning.   Our framework encompasses and gives intuitively accessible generalization bounds for a variety of quantum learning scenarios such as quantum state discrimination, PAC learning quantum states, quantum parameter estimation, and quantumly PAC learning classical functions. Thereby, our work lays a foundation for a unifying quantum information-theoretic perspective on quantum learning.
</details>
<details>
<summary>摘要</summary>
学习任务在量子信息和计算中发挥越来越重要的作用。它们包括从基本问题如状态识别和测量到量子可能错误学习（PAC）学习框架，以及最近提出的阴影变体状态测量。然而，量子学习理论的多种方向ntil now已经发展独立。我们提出一种通用的数学ormalism для描述量子学习，通过在类型-量子数据上训练量子学习模型，然后测试该模型是否能够在新数据上适应。在这种框架下，我们证明在类型-量子信息学术和量子集中度量的基础上，预期量子学习器的总体适应误差的下界。为 достичь这个目标，我们使用量子优化运输和量子集中度量的工具来建立量子不 commutative 版本的解 coupling 证明，这些证明在类型-量子信息学术和量子集中度量的基础上，提供了类型-量子信息学术的总体适应误差 bounds。我们的框架包括并为各种量子学习场景提供直观可 accessible的总体适应误差 bounds，如量子状态识别、PAC学习量子状态、量子参数估计和量子PAC学习类型函数。因此，我们的工作为量子信息学- computation 领域提供了一个统一的视角， laying a foundation for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="Dirichlet-Active-Learning"><a href="#Dirichlet-Active-Learning" class="headerlink" title="Dirichlet Active Learning"></a>Dirichlet Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05501">http://arxiv.org/abs/2311.05501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Miller, Ryan Murray</li>
<li>for: 这篇论文是为了提出 Dirichlet Active Learning（DiAL），一种基于 bayesian 的活动学习算法设计方法。</li>
<li>methods: 这种方法使用 Dirichlet 随机场模型，模型特征受到相似性的观察力，以便在学习任务中进行准确的分类和活动学习。</li>
<li>results: 该方法在具有低标签率的图像学习中进行了成功应用，并与当前最佳实践相比赢得了竞争力。此外，该方法还提供了一系列正式保证，确保其能够同时实现探索和利用。<details>
<summary>Abstract</summary>
This work introduces Dirichlet Active Learning (DiAL), a Bayesian-inspired approach to the design of active learning algorithms. Our framework models feature-conditional class probabilities as a Dirichlet random field and lends observational strength between similar features in order to calibrate the random field. This random field can then be utilized in learning tasks: in particular, we can use current estimates of mean and variance to conduct classification and active learning in the context where labeled data is scarce. We demonstrate the applicability of this model to low-label rate graph learning by constructing ``propagation operators'' based upon the graph Laplacian, and offer computational studies demonstrating the method's competitiveness with the state of the art. Finally, we provide rigorous guarantees regarding the ability of this approach to ensure both exploration and exploitation, expressed respectively in terms of cluster exploration and increased attention to decision boundaries.
</details>
<details>
<summary>摘要</summary>
（这个研究引入了 Dirichlet Active Learning（DiAL），一种基于 Bayesian 的活动学习算法设计方法。我们的框架将特征决策概率模型为 Dirichlet 随机场，并将相似特征之间的观测力共享，以便准确地调整随机场。这个随机场可以在学习任务中使用，特别是在标注数据稀缺的情况下进行分类和活动学习。我们通过基于图 Laplacian 的“传播算子”来实现这种模型，并提供了 computation 研究，证明这种方法与当前的状态艺术水平竞争。最后，我们提供了一系列具有充分的保证，表明这种方法能够保证 both exploration 和 exploitation，即尝试新的cluster和增强决策边界的注意力。）
</details></li>
</ul>
<hr>
<h2 id="Disease-Gene-Prioritization-With-Quantum-Walks"><a href="#Disease-Gene-Prioritization-With-Quantum-Walks" class="headerlink" title="Disease Gene Prioritization With Quantum Walks"></a>Disease Gene Prioritization With Quantum Walks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05486">http://arxiv.org/abs/2311.05486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harto Saarinen, Mark Goldsmith, Rui-Sheng Wang, Joseph Loscalzo, Sabrina Maniscalco</li>
<li>For: 本研究开发了一种基于连续时间量子漫步的疾病基因优先级化算法，用于预测疾病基因。* Methods: 该算法使用蛋白质-蛋白质交互网络中的邻居矩阵，并可以编码种子节点自回路到下面的哈密顿函数中，提高性能。* Results: 对三种疾病集合和七个蛋白质-蛋白质交互网络进行比较，该算法表现出较高的预测疾病基因性能，并进行了拥抱分析和自Loop的影响研究。<details>
<summary>Abstract</summary>
Disease gene prioritization assigns scores to genes or proteins according to their likely relevance for a given disease based on a provided set of seed genes. Here, we describe a new algorithm for disease gene prioritization based on continuous-time quantum walks using the adjacency matrix of a protein-protein interaction (PPI) network. Our algorithm can be seen as a quantum version of a previous method known as the diffusion kernel, but, importantly, has higher performance in predicting disease genes, and also permits the encoding of seed node self-loops into the underlying Hamiltonian, which offers yet another boost in performance. We demonstrate the success of our proposed method by comparing it to several well-known gene prioritization methods on three disease sets, across seven different PPI networks. In order to compare these methods, we use cross-validation and examine the mean reciprocal ranks and recall values. We further validate our method by performing an enrichment analysis of the predicted genes for coronary artery disease. We also investigate the impact of adding self-loops to the seeds, and argue that they allow the quantum walker to remain more local to low-degree seed nodes.
</details>
<details>
<summary>摘要</summary>
疾病基因优先顺序 assigns 分数到基因或蛋白质根据它们可能对某种疾病的可能性，基于提供的种子基因。在这里，我们描述了一种新的疾病基因优先顺序算法，基于连续时间量子游走使用蛋白质-蛋白质互作（PPI）网络的相互作用矩阵。我们的算法可以看作量子版的既有方法，即扩散核，但具有更高的疾病基因预测性能，同时允许种子节点自闭Loop编码到下面的哈密顿中，又提供了另一个提升性能的优势。我们通过对多个疾病集和七个不同的PPI网络进行跨验证，并通过评估平均反转排名和回归值来比较这些方法。我们还进行了预测基因扩充分析，以验证我们的方法的有效性。此外，我们还研究了将自闭Loop添加到种子中的影响，并论证它们使得量子游走者更加偏爱低度种子节点。
</details></li>
</ul>
<hr>
<h2 id="Do-Ensembling-and-Meta-Learning-Improve-Outlier-Detection-in-Randomized-Controlled-Trials"><a href="#Do-Ensembling-and-Meta-Learning-Improve-Outlier-Detection-in-Randomized-Controlled-Trials" class="headerlink" title="Do Ensembling and Meta-Learning Improve Outlier Detection in Randomized Controlled Trials?"></a>Do Ensembling and Meta-Learning Improve Outlier Detection in Randomized Controlled Trials?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05473">http://arxiv.org/abs/2311.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hamilton-health-sciences/ml4h-traq">https://github.com/hamilton-health-sciences/ml4h-traq</a></li>
<li>paper_authors: Walter Nelson, Jonathan Ranisau, Jeremy Petch</li>
<li>for: 这些论文主要目标是为了评估现代多中心随机化控制试验中收集的大量表格数据中的异常点。</li>
<li>methods: 这篇论文使用了6种现代机器学习基于异常检测算法对738个实际数据集和77,001名患者从44个国家的7个真实多中心随机控制试验中的数据进行了实验评估。</li>
<li>results: 这篇论文的结果证明了先前的研究中的一些结论，即现有算法可以无监督地识别数据异常，至少有一种算法在70.6%的时间内表现出正确的结果。然而，数据集之间的性能差异非常大，无一个算法在所有数据集中表现一致，因此需要新的方法来选择无监督模型或其他方式将可能冲突的预测结果集成。这篇论文提出了元学习概率ensemble（MePE）算法，可以将多个无监督模型的预测结果集成，并证明了它在 Meta-learning 方法中表现良好。然而，小集合在 average 上表现更好，这可能是一个负面结果，可能导向当前异常检测方法的应用。<details>
<summary>Abstract</summary>
Modern multi-centre randomized controlled trials (MCRCTs) collect massive amounts of tabular data, and are monitored intensively for irregularities by humans. We began by empirically evaluating 6 modern machine learning-based outlier detection algorithms on the task of identifying irregular data in 838 datasets from 7 real-world MCRCTs with a total of 77,001 patients from over 44 countries. Our results reinforce key findings from prior work in the outlier detection literature on data from other domains. Existing algorithms often succeed at identifying irregularities without any supervision, with at least one algorithm exhibiting positive performance 70.6% of the time. However, performance across datasets varies substantially with no single algorithm performing consistently well, motivating new techniques for unsupervised model selection or other means of aggregating potentially discordant predictions from multiple candidate models. We propose the Meta-learned Probabilistic Ensemble (MePE), a simple algorithm for aggregating the predictions of multiple unsupervised models, and show that it performs favourably compared to recent meta-learning approaches for outlier detection model selection. While meta-learning shows promise, small ensembles outperform all forms of meta-learning on average, a negative result that may guide the application of current outlier detection approaches in healthcare and other real-world domains.
</details>
<details>
<summary>摘要</summary>
现代多中心随机控制试验 (MCRCTs) 收集了大量的表格数据，并且被人类紧张监测以寻找异常。我们开始由 empirically 评估 6 种现代机器学习基于算法在异常检测任务中表现的可行性。我们的结果证实了先前Literature中关于数据 других 领域的发现，现有算法可以无监控下检测异常，至少有一个算法在 70.6% 的时间表现出正确的结果。然而，数据集之间的表现差异很大，没有单一的算法能够在所有数据集中表现良好，因此推动了新的无监控模型选择技术或其他方法来聚合可能存在冲突的预测。我们提出了 Meta-learned Probabilistic Ensemble (MePE)，一种简单的算法来聚合多个无监控模型的预测，并证明它与当前的出生学习方法相比表现良好。虽然 meta-learning 有推动之处，但小集合在 average 上表现较佳，这可能导向当前异常检测方法在医疗和其他实际领域的应用中的限制。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Approach-to-Novel-Class-Discovery-in-Tabular-Data"><a href="#A-Practical-Approach-to-Novel-Class-Discovery-in-Tabular-Data" class="headerlink" title="A Practical Approach to Novel Class Discovery in Tabular Data"></a>A Practical Approach to Novel Class Discovery in Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05440">http://arxiv.org/abs/2311.05440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin Troisemaine, Alexandre Reiffers-Masson, Stéphane Gosselin, Vincent Lemaire, Sandrine Vaton</li>
<li>for: solving the novel class discovery (NCD) problem in tabular data without prior knowledge of the novel classes.</li>
<li>methods: proposing a simple deep NCD model with only essential elements, adapting $k$-fold cross-validation with hidden classes, and leveraging knowledge of known classes with unsupervised clustering algorithms ($k$-means and Spectral Clustering).</li>
<li>results: impressive performance under realistic conditions, reliable estimation of the number of novel classes, and effective solution to the NCD problem without relying on knowledge from the novel classes.<details>
<summary>Abstract</summary>
The problem of Novel Class Discovery (NCD) consists in extracting knowledge from a labeled set of known classes to accurately partition an unlabeled set of novel classes. While NCD has recently received a lot of attention from the community, it is often solved on computer vision problems and under unrealistic conditions. In particular, the number of novel classes is usually assumed to be known in advance, and their labels are sometimes used to tune hyperparameters. Methods that rely on these assumptions are not applicable in real-world scenarios. In this work, we focus on solving NCD in tabular data when no prior knowledge of the novel classes is available. To this end, we propose to tune the hyperparameters of NCD methods by adapting the $k$-fold cross-validation process and hiding some of the known classes in each fold. Since we have found that methods with too many hyperparameters are likely to overfit these hidden classes, we define a simple deep NCD model. This method is composed of only the essential elements necessary for the NCD problem and performs impressively well under realistic conditions. Furthermore, we find that the latent space of this method can be used to reliably estimate the number of novel classes. Additionally, we adapt two unsupervised clustering algorithms ($k$-means and Spectral Clustering) to leverage the knowledge of the known classes. Extensive experiments are conducted on 7 tabular datasets and demonstrate the effectiveness of the proposed method and hyperparameter tuning process, and show that the NCD problem can be solved without relying on knowledge from the novel classes.
</details>
<details>
<summary>摘要</summary>
文本问题（Novel Class Discovery，NCD）的问题是从已知类别集中提取知识，以便将未知类别集分割为精确的类别。尽管NCD在计算机视觉问题上已经受到了社区的一些关注，但是它们通常是在不实际的假设下解决的，例如，未知类别的数量通常是先知的，而且有时候用于调整超参数。这些假设不适用于实际场景。在这种情况下，我们将关注解决NCD在表格数据上的问题，而无需先知未知类别的数量。为此，我们提议通过适应$k$-fold横排验证过程和隐藏已知类别来调整NCD方法的超参数。由于我们发现了过多的超参数可能会过拟合这些隐藏的类别，所以我们定义了一个简单的深度NCD模型。这种方法由只有NCD问题所需的基本元素组成，并在实际条件下表现出色。此外，我们发现了这种方法的隐藏空间可以用于可靠地估计未知类别的数量。此外，我们将$k$-means和特征分布 clustering两种无监督归类算法调整，以利用已知类别的知识。我们对7个表格数据集进行了广泛的实验，并证明了我们提出的方法和超参数调整过程的效果，以及NCD问题可以无需已知未知类别的知识解决。
</details></li>
</ul>
<hr>
<h2 id="Fair-Wasserstein-Coresets"><a href="#Fair-Wasserstein-Coresets" class="headerlink" title="Fair Wasserstein Coresets"></a>Fair Wasserstein Coresets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05436">http://arxiv.org/abs/2311.05436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Xiong, Niccolò Dalmasso, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</li>
<li>For: The paper aims to generate fair synthetic representative samples for downstream learning tasks, addressing biases in the data concerning subgroups defined by factors like race, gender, or other sensitive attributes.* Methods: The paper proposes Fair Wasserstein Coresets (FWC), a novel coreset approach that generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing demographic parity, a prominent criterion for algorithmic fairness.* Results: The paper shows that FWC can be thought of as a constrained version of Lloyd’s algorithm for k-medians or k-means clustering, and demonstrates the scalability of the approach through experiments conducted on both synthetic and real datasets. The results highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.Here is the same information in Simplified Chinese text:* For: 本研究旨在生成具有宽泛代表性的公平synthetic representative samples，以便在下游学习任务中应用。* Methods: 本研究提出了公平 Wasserstein coresets（FWC），一种新的 coreset方法，该方法生成了公平synthetic representative samples并将其权重为用于下游学习任务。 FWC寻求将原始数据集和重量化synthetic samples之间的普通 Wasserstein distance降为最低，同时保证demographic parity，一种公平性指标。* Results: 本研究表明，FWC可以视为Lloyd的算法 дляk-medians或k-means clustering的受限版本。我们的实验表明，FWC可扩展到大规模数据集，并且在比较公平的情况下表现更加竞争力。<details>
<summary>Abstract</summary>
Recent technological advancements have given rise to the ability of collecting vast amounts of data, that often exceed the capacity of commonly used machine learning algorithms. Approaches such as coresets and synthetic data distillation have emerged as frameworks to generate a smaller, yet representative, set of samples for downstream training. As machine learning is increasingly applied to decision-making processes, it becomes imperative for modelers to consider and address biases in the data concerning subgroups defined by factors like race, gender, or other sensitive attributes. Current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples. These methods, however, are not guaranteed to positively affect the performance or fairness of downstream learning processes. In this work, we present Fair Wasserstein Coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC aims to minimize the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing (an empirical version of) demographic parity, a prominent criterion for algorithmic fairness, via a linear constraint. We show that FWC can be thought of as a constrained version of Lloyd's algorithm for k-medians or k-means clustering. Our experiments, conducted on both synthetic and real datasets, demonstrate the scalability of our approach and highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.
</details>
<details>
<summary>摘要</summary>
Current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples. However, these methods are not guaranteed to positively affect the performance or fairness of downstream learning processes. In this work, we present Fair Wasserstein Coresets (FWC), a novel coreset approach that generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks.FWC aims to minimize the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing (an empirical version of) demographic parity, a prominent criterion for algorithmic fairness, via a linear constraint. We show that FWC can be thought of as a constrained version of Lloyd's algorithm for k-medians or k-means clustering. Our experiments, conducted on both synthetic and real datasets, demonstrate the scalability of our approach and highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.
</details></li>
</ul>
<hr>
<h2 id="Parkinson’s-Disease-Detection-through-Vocal-Biomarkers-and-Advanced-Machine-Learning-Algorithms-A-Comprehensive-Study"><a href="#Parkinson’s-Disease-Detection-through-Vocal-Biomarkers-and-Advanced-Machine-Learning-Algorithms-A-Comprehensive-Study" class="headerlink" title="Parkinson’s Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms: A Comprehensive Study"></a>Parkinson’s Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms: A Comprehensive Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05435">http://arxiv.org/abs/2311.05435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abu Sayed, Sabbir Ahamed, Duc M Cao, Md Eyasin Ul Islam Pavel, Malay Sarkar, Md Tuhin Mia<br>for: 预测parkinson病的发病概率methods: 使用多种先进机器学习算法，包括XGBoost、LightGBM、Bagging、AdaBoost和支持向量机等，评估这些模型的预测性能，并计算准确率、曲线面积、敏感度和特异度等指标。results: 研究发现LightGBM模型表现最佳，其准确率达96%，圆曲线面积也是96%，敏感度为100%，特异度为94.43%，在准确率和曲线面积方面与其他机器学习算法相比较出色。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) is a prevalent neurodegenerative disorder known for its impact on motor neurons, causing symptoms like tremors, stiffness, and gait difficulties. This study explores the potential of vocal feature alterations in PD patients as a means of early disease prediction. This research aims to predict the onset of Parkinson's disease. Utilizing a variety of advanced machine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost, and Support Vector Machine, among others, the study evaluates the predictive performance of these models using metrics such as accuracy, area under the curve (AUC), sensitivity, and specificity. The findings of this comprehensive analysis highlight LightGBM as the most effective model, achieving an impressive accuracy rate of 96%, alongside a matching AUC of 96%. LightGBM exhibited a remarkable sensitivity of 100% and specificity of 94.43%, surpassing other machine learning algorithms in accuracy and AUC scores. Given the complexities of Parkinson's disease and its challenges in early diagnosis, this study underscores the significance of leveraging vocal biomarkers coupled with advanced machine-learning techniques for precise and timely PD detection.
</details>
<details>
<summary>摘要</summary>
帕金森病 (PD) 是一种常见的神经退化疾病，知名于对运动神经元的影响，导致如颤抖、硬直和步态困难等症状。这项研究探讨了在PD患者中 voz feature 的变化作为早期疾病预测的可能性。这项研究的目的是预测帕金森病的病起。使用多种高级机器学习算法，包括 XGBoost、LightGBM、Bagging、AdaBoost 和 Support Vector Machine 等，这项研究评估了这些模型在精度和准确性方面的预测性能。研究发现 LightGBM 是最有效的模型，其精度率达 96%，并与报数曲线 (AUC) 均达 96%。LightGBM 表现出了惊人的敏感性（100%）和特异性（94.43%），在精度和报数曲线方面超过了其他机器学习算法。由于帕金森病的复杂性和早期诊断的挑战，这项研究highlights the significance of combining vocal biomarkers with advanced machine-learning techniques for precise and timely PD detection.
</details></li>
</ul>
<hr>
<h2 id="Taxonomy-for-Resident-Space-Objects-in-LEO-A-Deep-Learning-Approach"><a href="#Taxonomy-for-Resident-Space-Objects-in-LEO-A-Deep-Learning-Approach" class="headerlink" title="Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach"></a>Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05430">http://arxiv.org/abs/2311.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>For: 本研究提出了一个新的taxonomy来加强空间交通管理，以应对随着数量的增加的空间垃圾物体（RSO）的风险。* Methods: 本研究使用了深度学习模型，包括自适应网络和uniform manifold approximation and projection（UMAP），来分类RSOs并提取其特征。* Results: 本研究提出了一个新的taxonomy，并使用了深度学习模型来分类RSOs，并成功地捕捉了RSOs的复杂和非线性关系。这些成果可以帮助改善空间交通管理和降低风险。<details>
<summary>Abstract</summary>
The increasing number of RSOs has raised concerns about the risk of collisions and catastrophic incidents for all direct and indirect users of space. To mitigate this issue, it is essential to have a good understanding of the various RSOs in orbit and their behaviour. A well-established taxonomy defining several classes of RSOs is a critical step in achieving this understanding. This taxonomy helps assign objects to specific categories based on their main characteristics, leading to better tracking services. Furthermore, a well-established taxonomy can facilitate research and analysis processes by providing a common language and framework for better understanding the factors that influence RSO behaviour in space. These factors, in turn, help design more efficient and effective strategies for space traffic management. Our work proposes a new taxonomy for RSOs focusing on the low Earth orbit regime to enhance space traffic management. In addition, we present a deep learning-based model that uses an autoencoder architecture to reduce the features representing the characteristics of the RSOs. The autoencoder generates a lower-dimensional space representation that is then explored using techniques such as Uniform Manifold Approximation and Projection to identify fundamental clusters of RSOs based on their unique characteristics. This approach captures the complex and non-linear relationships between the features and the RSOs' classes identified. Our proposed taxonomy and model offer a significant contribution to the ongoing efforts to mitigate the overall risks posed by the increasing number of RSOs in orbit.
</details>
<details>
<summary>摘要</summary>
随着各种人造卫星的数量不断增加，这引发了对各种卫星的碰撞和灾难性事件的风险的担忧。为了解决这个问题，需要有一个良好的理解各种在轨道上运行的人造卫星的行为。一个well-established taxonomy可以将 объек们分类到特定类别基于其主要特征，从而实现更好的跟踪服务。此外，一个well-established taxonomy还可以促进研究和分析过程，提供一个共同语言和框架，以更好地理解各种卫星行为的因素，并设计更有效率的空间交通管理策略。我们的工作提出了一个专注于低地球轨道域的新分类法，并提出了一种基于自适应神经网络的模型，使用自适应网络架构减少表示卫星特征的特征。这种方法可以捕捉各种卫星特征之间的复杂和非线性关系，并基于这些特征分类出各种卫星的基本类别。我们的提议的分类法和模型对当前各种卫星的增加数量而言，具有积极的贡献。
</details></li>
</ul>
<hr>
<h2 id="Statistical-Learning-of-Conjunction-Data-Messages-Through-a-Bayesian-Non-Homogeneous-Poisson-Process"><a href="#Statistical-Learning-of-Conjunction-Data-Messages-Through-a-Bayesian-Non-Homogeneous-Poisson-Process" class="headerlink" title="Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process"></a>Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05426">http://arxiv.org/abs/2311.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>for: 避免卫星碰撞和空间交通管理中的挑战，特别是随着卫星数量的不断增加和自动化解决方案的缺乏。</li>
<li>methods: 使用统计学学习模型，以解决卫星碰撞风险评估和避免碰撞机动的问题。</li>
<li>results: 提出了一种 Bayesian 非Homogeneous Poisson 过程，可以更好地描述卫星碰撞风险评估和避免碰撞机动的问题，并与基准模型进行比较，表明该方法可以更准确地解决这些问题。<details>
<summary>Abstract</summary>
Current approaches for collision avoidance and space traffic management face many challenges, mainly due to the continuous increase in the number of objects in orbit and the lack of scalable and automated solutions. To avoid catastrophic incidents, satellite owners/operators must be aware of their assets' collision risk to decide whether a collision avoidance manoeuvre needs to be performed. This process is typically executed through the use of warnings issued in the form of CDMs which contain information about the event, such as the expected TCA and the probability of collision. Our previous work presented a statistical learning model that allowed us to answer two important questions: (1) Will any new conjunctions be issued in the next specified time interval? (2) When and with what uncertainty will the next CDM arrive? However, the model was based on an empirical Bayes homogeneous Poisson process, which assumes that the arrival rates of CDMs are constant over time. In fact, the rate at which the CDMs are issued depends on the behaviour of the objects as well as on the screening process performed by third parties. Thus, in this work, we extend the previous study and propose a Bayesian non-homogeneous Poisson process implemented with high precision using a Probabilistic Programming Language to fully describe the underlying phenomena. We compare the proposed solution with a baseline model to demonstrate the added value of our approach. The results show that this problem can be successfully modelled by our Bayesian non-homogeneous Poisson Process with greater accuracy, contributing to the development of automated collision avoidance systems and helping operators react timely but sparingly with satellite manoeuvres.
</details>
<details>
<summary>摘要</summary>
当前的协调碰撞避免和空间交通管理技术面临着许多挑战，主要是由于遥感器量在轨道上的不断增加和缺乏可扩展和自动化的解决方案。为了避免慢速碰撞，卫星所有者/运营商必须了解自己的资产碰撞风险，以确定是否需要执行碰撞避免操作。这个过程通常通过使用CDM（Conjunction Data Message）中包含的信息进行执行，如预计的TCA（Time of Closest Approach）和碰撞的可能性。我们的前一项研究提出了一种统计学学习模型，可以回答以下两个重要问题：（1）将来有新的相对应吗？（2）预计下一个CDM会在什么时间到达，以及具有多少不确定性？然而，该模型基于empirical Bayes homogeneous Poisson process，即CDM的到达速率是时间不变的。事实上，CDM的发送速率取决于对象的行为以及第三方creening过程。因此，在这项工作中，我们延续前一项研究，并提出一种 Bayesian non-homogeneous Poisson process，使用高精度的可probabilistic Programming Language来完全描述下面现象。我们与基准模型进行比较，以展示我们的方法的added value。结果表明，我们的方法可以更高度准确地模型这个问题，为自动化碰撞避免系统的发展和操作人员在时间上作出合适的决策做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Causal-Representation-Learning"><a href="#Diffusion-Based-Causal-Representation-Learning" class="headerlink" title="Diffusion Based Causal Representation Learning"></a>Diffusion Based Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05421">http://arxiv.org/abs/2311.05421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Mohammad Karimi Mamaghan, Andrea Dittadi, Stefan Bauer, Karl Henrik Johansson, Francesco Quinzan</li>
<li>for:  This paper is written for learning causal representations and discovering causal structures in complex systems.</li>
<li>methods:  The paper proposes a new Diffusion-based Causal Representation Learning (DCRL) algorithm, which uses diffusion-based representations for causal discovery and offers access to infinite dimensional latent codes.</li>
<li>results:  The paper demonstrates the effectiveness of DCRL in identifying the causal structure and causal variables, compared to previous methods such as Variational Auto-Encoders (VAE).<details>
<summary>Abstract</summary>
Causal reasoning can be considered a cornerstone of intelligent systems. Having access to an underlying causal graph comes with the promise of cause-effect estimation and the identification of efficient and safe interventions. However, learning causal representations remains a major challenge, due to the complexity of many real-world systems. Previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). These methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. To overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. This algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. In a first proof of principle, we investigate the use of DCRL for causal representation learning. We further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.
</details>
<details>
<summary>摘要</summary>
causal reasoning 可以被视为智能系统的基础之一。  possession of an underlying causal graph 提供了 cause-effect estimation 和 identification of efficient and safe interventions 的机会。 However, learning causal representations remains a major challenge, due to the complexity of many real-world systems. Previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). These methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. To overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. This algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. In a first proof of principle, we investigate the use of DCRL for causal representation learning. We further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.Here's the word-for-word translation of the text into Simplified Chinese: causal reasoning 可以被视为智能系统的基础之一。  possession of an underlying causal graph 提供了 cause-effect estimation 和 identification of efficient and safe interventions 的机会。 however, learning causal representations remains a major challenge, due to the complexity of many real-world systems. previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). these methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. to overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. this algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. in a first proof of principle, we investigate the use of DCRL for causal representation learning. we further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.
</details></li>
</ul>
<hr>
<h2 id="Counterfactually-Fair-Representation"><a href="#Counterfactually-Fair-Representation" class="headerlink" title="Counterfactually Fair Representation"></a>Counterfactually Fair Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05420">http://arxiv.org/abs/2311.05420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-srml/cf_representation_learning">https://github.com/osu-srml/cf_representation_learning</a></li>
<li>paper_authors: Zhiqun Zuo, Mohammad Mahdi Khalili, Xueru Zhang</li>
<li>for: 本研究旨在探讨机器学习模型在高风险应用（如医疗、贷款、大学推荐）中的可能存在的保护群体偏见问题，以及如何通过不同的公平性观念和方法来缓解这些偏见。</li>
<li>methods: 本研究使用了Counterfactual Fairness（CF）作为公平性观念，CF需要实际世界中个体所经历的结果与假设的counterfactual世界中个体所经历的结果相同。 learns CF models only using non-descendants of sensitive attributes while eliminating all descendants，这是一种可以满足CF的简单方法。  however, this work proposes a new algorithm that trains models using all the available features，这种方法可以 theoretically and empirically show that models trained with this method can satisfy CF。</li>
<li>results: 本研究通过 teorically and empirically show that models trained with the proposed method can satisfy CF，提供了一种可以在高风险应用中使用机器学习模型的公平性方法。<details>
<summary>Abstract</summary>
The use of machine learning models in high-stake applications (e.g., healthcare, lending, college admission) has raised growing concerns due to potential biases against protected social groups. Various fairness notions and methods have been proposed to mitigate such biases. In this work, we focus on Counterfactual Fairness (CF), a fairness notion that is dependent on an underlying causal graph and first proposed by Kusner \textit{et al.}~\cite{kusner2017counterfactual}; it requires that the outcome an individual perceives is the same in the real world as it would be in a "counterfactual" world, in which the individual belongs to another social group. Learning fair models satisfying CF can be challenging. It was shown in \cite{kusner2017counterfactual} that a sufficient condition for satisfying CF is to \textbf{not} use features that are descendants of sensitive attributes in the causal graph. This implies a simple method that learns CF models only using non-descendants of sensitive attributes while eliminating all descendants. Although several subsequent works proposed methods that use all features for training CF models, there is no theoretical guarantee that they can satisfy CF. In contrast, this work proposes a new algorithm that trains models using all the available features. We theoretically and empirically show that models trained with this method can satisfy CF\footnote{The code repository for this work can be found in \url{https://github.com/osu-srml/CF_Representation_Learning}.
</details>
<details>
<summary>摘要</summary>
高度的应用（如医疗、贷款、大学招生）中机器学习模型的使用已经引起了增长的关注，因为它们可能会对保护的社会群体产生偏见。不同的公平性概念和方法已经被提出来 mitigate这些偏见。在这项工作中，我们将关注Counterfactual Fairness（CF），这是一种受到下游隐藏 Variable 的依赖关系的公平性概念，由Kusner等人在 \cite{kusner2017counterfactual} 首次提出。它要求个体在真实世界中所感受到的结果与在一个“假设世界”中所感受到的结果一样。学习满足CF的公平模型可以是困难的。在 \cite{kusner2017counterfactual} 中显示了一种 suficient condition，即不使用敏感属性的后代feature。这意味着可以通过不使用敏感属性的后代feature来学习满足CF的模型。然而，后续的一些工作提出了使用所有特征来训练CF模型的方法，但没有理论保证它们可以满足CF。相反，这项工作提出了一种新的算法，该算法使用所有可用特征来训练模型。我们 theoretically和empirically表明，使用该算法可以满足CF。Note: The code repository for this work can be found in \url{https://github.com/osu-srml/CF_Representation_Learning}.
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Position-Uncertainty-at-the-Time-of-Closest-Approach-with-Diffusion-Models"><a href="#Predicting-the-Position-Uncertainty-at-the-Time-of-Closest-Approach-with-Diffusion-Models" class="headerlink" title="Predicting the Position Uncertainty at the Time of Closest Approach with Diffusion Models"></a>Predicting the Position Uncertainty at the Time of Closest Approach with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05417">http://arxiv.org/abs/2311.05417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>for: 避免航天器相撞，提高航天活动的安全性和效率。</li>
<li>methods: 使用机器学习模型，基于扩散模型来预测objects involved in close encounter的位置不确定性的进化。</li>
<li>results: 与其他状态艺术解决方案和na&quot;ive baseline方法相比，提议的解决方案具有提高航天器操作的安全性和效率的潜在优势。<details>
<summary>Abstract</summary>
The risk of collision between resident space objects has significantly increased in recent years. As a result, spacecraft collision avoidance procedures have become an essential part of satellite operations. To ensure safe and effective space activities, satellite owners and operators rely on constantly updated estimates of encounters. These estimates include the uncertainty associated with the position of each object at the expected TCA. These estimates are crucial in planning risk mitigation measures, such as collision avoidance manoeuvres. As the TCA approaches, the accuracy of these estimates improves, as both objects' orbit determination and propagation procedures are made for increasingly shorter time intervals. However, this improvement comes at the cost of taking place close to the critical decision moment. This means that safe avoidance manoeuvres might not be possible or could incur significant costs. Therefore, knowing the evolution of this variable in advance can be crucial for operators. This work proposes a machine learning model based on diffusion models to forecast the position uncertainty of objects involved in a close encounter, particularly for the secondary object (usually debris), which tends to be more unpredictable. We compare the performance of our model with other state-of-the-art solutions and a na\"ive baseline approach, showing that the proposed solution has the potential to significantly improve the safety and effectiveness of spacecraft operations.
</details>
<details>
<summary>摘要</summary>
随着近年航天空间物体的飞行轨道数量的增加，航天器碰撞避免程序已成为卫星运营中不可或缺的一部分。为确保安全有效的空间活动，卫星所有者和运营商依靠不断更新的遇对总体评估。这些评估包括对每个 объек的位置 uncertainty 的评估，这些评估是规划避免碰撞措施的关键。随着预计的 TCA 接近，对象的轨道决定和推算过程的精度得到改善，但这也意味着在决策时间点附近进行避免碰撞措施可能无法进行或者需要投入大量资源。因此，预测遇对变量的进化可以对操作人员带来很大的帮助。本文提出了基于扩散模型的机器学习模型，用于预测遇对变量的位置不确定性，特别是次要 объек（通常是废弃物）的位置不确定性。我们对本模型与其他当前状态的解决方案和na\"ive基线方法进行比较，显示了本模型在安全性和效果性方面的潜在提升。
</details></li>
</ul>
<hr>
<h2 id="Data-Distillation-for-Neural-Network-Potentials-toward-Foundational-Dataset"><a href="#Data-Distillation-for-Neural-Network-Potentials-toward-Foundational-Dataset" class="headerlink" title="Data Distillation for Neural Network Potentials toward Foundational Dataset"></a>Data Distillation for Neural Network Potentials toward Foundational Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05407">http://arxiv.org/abs/2311.05407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Seob Jung, Sangkeun Lee, Jong Youl Choi</li>
<li>for: 本研究旨在快速提高材料设计和发现的效率，通过生成模型提出有潜力的材料。</li>
<li>methods: 本研究使用机器学习技术和原子尺度模拟，并使用扩展ensemble分子动力学来获得广泛的液相和固相配置。然后通过活动学习减少数据，无需失去准确性。</li>
<li>results: 研究发现，通过使用数据浓缩法，可以从初始数据中提取出能量最小化的闭包晶体结构，而这些结构并未直接出现在初始数据中。此外，这些数据还可以翻译到其他金属系统（铝和锰），无需重复样本和浓缩过程。这种数据获取和浓缩方法可以加速NNP的发展，并提高材料设计和发现的效率。<details>
<summary>Abstract</summary>
Machine learning (ML) techniques and atomistic modeling have rapidly transformed materials design and discovery. Specifically, generative models can swiftly propose promising materials for targeted applications. However, the predicted properties of materials through the generative models often do not match with calculated properties through ab initio calculations. This discrepancy can arise because the generated coordinates are not fully relaxed, whereas the many properties are derived from relaxed structures. Neural network-based potentials (NNPs) can expedite the process by providing relaxed structures from the initially generated ones. Nevertheless, acquiring data to train NNPs for this purpose can be extremely challenging as it needs to encompass previously unknown structures. This study utilized extended ensemble molecular dynamics (MD) to secure a broad range of liquid- and solid-phase configurations in one of the metallic systems, nickel. Then, we could significantly reduce them through active learning without losing much accuracy. We found that the NNP trained from the distilled data could predict different energy-minimized closed-pack crystal structures even though those structures were not explicitly part of the initial data. Furthermore, the data can be translated to other metallic systems (aluminum and niobium), without repeating the sampling and distillation processes. Our approach to data acquisition and distillation has demonstrated the potential to expedite NNP development and enhance materials design and discovery by integrating generative models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Sample-Complexity-Of-ERMs-In-Stochastic-Convex-Optimization"><a href="#The-Sample-Complexity-Of-ERMs-In-Stochastic-Convex-Optimization" class="headerlink" title="The Sample Complexity Of ERMs In Stochastic Convex Optimization"></a>The Sample Complexity Of ERMs In Stochastic Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05398">http://arxiv.org/abs/2311.05398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Carmon, Roi Livni, Amir Yehudayoff</li>
<li>for: 这个论文的目的是解决在现代机器学习中最常研究的随机凸优化问题中的一个基本问题，即如何确定训练数据集中的数据点数，使得任何Empirical Risk Minimizer（ERM）在真实人口中表现良好？</li>
<li>methods: 这篇论文使用了随机凸优化的方法，并提出了一种新的分离结果，即$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$数据点数足够使ERM表现良好。</li>
<li>results: 这篇论文的结果表明，$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$数据点数是必要的和 suficient condition，以确保ERM在真实人口中表现良好。此外，论文还扩展了结果，并证明这种上限 bound 适用于所有对称凸体上的学习问题。<details>
<summary>Abstract</summary>
Stochastic convex optimization is one of the most well-studied models for learning in modern machine learning. Nevertheless, a central fundamental question in this setup remained unresolved: "How many data points must be observed so that any empirical risk minimizer (ERM) shows good performance on the true population?" This question was proposed by Feldman (2016), who proved that $\Omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ data points are necessary (where $d$ is the dimension and $\epsilon>0$ is the accuracy parameter). Proving an $\omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ lower bound was left as an open problem. In this work we show that in fact $\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ data points are also sufficient. This settles the question and yields a new separation between ERMs and uniform convergence. This sample complexity holds for the classical setup of learning bounded convex Lipschitz functions over the Euclidean unit ball. We further generalize the result and show that a similar upper bound holds for all symmetric convex bodies. The general bound is composed of two terms: (i) a term of the form $\tilde{O}(\frac{d}{\epsilon})$ with an inverse-linear dependence on the accuracy parameter, and (ii) a term that depends on the statistical complexity of the class of $\textit{linear}$ functions (captured by the Rademacher complexity). The proof builds a mechanism for controlling the behavior of stochastic convex optimization problems.
</details>
<details>
<summary>摘要</summary>
Stochastic convex optimization 是现代机器学习中最受研究的模型之一。然而，一个中心问题在这种设置下仍然未得到解答：“如何确定训练数据点的数量，以便任何empirical risk minimizer（ERM）在真实人口中表现良好？”这个问题由Feldman（2016）提出，并证明了 $\Omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 数据点是必要的（其中 $d$ 是维度， $\epsilon>0$ 是准确度参数）。但是证明 $\omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 下界的问题仍然未得到解决。在这个工作中，我们证明了事实上，$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 数据点也是充分的。这个样本复杂度适用于经典的学习凸函数于欧几何Unit ball中的设置。我们进一步推广结论，并证明对所有对称凸体都成立的相似上界。总的来说，这个 bound 由两个项组成：（i）一个类似于 $\tilde{O}(\frac{d}{\epsilon})$ 的项，具有反 proportion 到准确度参数的倒整数关系；（ii）一个依赖于类型 linear 函数的统计复杂度（捕捉在 Rademacher 复杂度中）。证明基于控制随机凸优化问题的机制。
</details></li>
</ul>
<hr>
<h2 id="Beyond-the-training-set-an-intuitive-method-for-detecting-distribution-shift-in-model-based-optimization"><a href="#Beyond-the-training-set-an-intuitive-method-for-detecting-distribution-shift-in-model-based-optimization" class="headerlink" title="Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization"></a>Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05363">http://arxiv.org/abs/2311.05363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farhan Damani, David H Brookes, Theodore Sternlieb, Cameron Webster, Stephen Malina, Rishi Jajoo, Kathy Lin, Sam Sinai</li>
<li>for: 本文的目的是解决在科学和工程设计问题中广泛存在的分布转移问题，即使用模型优化时，模型的预测结果与设计样本的分布不同。</li>
<li>methods: 本文提出了一种简单的方法，通过在设计样本中搜索适合的区域，以避免分布转移对设计质量的影响。该方法通过训练一个二分类器，使其在知道设计样本分布时，将训练数据与设计数据分开。</li>
<li>results: 本文在实际应用中运行了离线模型优化，并评估了分布转移对设计质量的影响。结果表明，分布转移的严重程度与优化算法步数相关，并且该简单的方法可以识别这些转移。这使得用户可以将搜索局限在模型预测结果可靠的区域内，从而提高设计质量。<details>
<summary>Abstract</summary>
Model-based optimization (MBO) is increasingly applied to design problems in science and engineering. A common scenario involves using a fixed training set to train models, with the goal of designing new samples that outperform those present in the training data. A major challenge in this setting is distribution shift, where the distributions of training and design samples are different. While some shift is expected, as the goal is to create better designs, this change can negatively affect model accuracy and subsequently, design quality. Despite the widespread nature of this problem, addressing it demands deep domain knowledge and artful application. To tackle this issue, we propose a straightforward method for design practitioners that detects distribution shifts. This method trains a binary classifier using knowledge of the unlabeled design distribution to separate the training data from the design data. The classifier's logit scores are then used as a proxy measure of distribution shift. We validate our method in a real-world application by running offline MBO and evaluate the effect of distribution shift on design quality. We find that the intensity of the shift in the design distribution varies based on the number of steps taken by the optimization algorithm, and our simple approach can identify these shifts. This enables users to constrain their search to regions where the model's predictions are reliable, thereby increasing the quality of designs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Basis-functions-nonlinear-data-enabled-predictive-control-Consistent-and-computationally-efficient-formulations"><a href="#Basis-functions-nonlinear-data-enabled-predictive-control-Consistent-and-computationally-efficient-formulations" class="headerlink" title="Basis functions nonlinear data-enabled predictive control: Consistent and computationally efficient formulations"></a>Basis functions nonlinear data-enabled predictive control: Consistent and computationally efficient formulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05360">http://arxiv.org/abs/2311.05360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mircea Lazar</li>
<li>for: 这个论文探讨了数据启发预测控制（DeePC）在非线性系统上的扩展，通过一般基函数。</li>
<li>methods: 论文使用了基函数DeePC行为预测器，并确定了相应的必要和充分条件，以确保基函数DeePC的有效性。</li>
<li>results: 论文通过对一个示例非线性抛钩系统进行测试，证明了基函数DeePC的有效性，并提出了一种更高效的计算方法。<details>
<summary>Abstract</summary>
This paper considers the extension of data-enabled predictive control (DeePC) to nonlinear systems via general basis functions. Firstly, we formulate a basis functions DeePC behavioral predictor and we identify necessary and sufficient conditions for equivalence with a corresponding basis functions multi-step identified predictor. The derived conditions yield a dynamic regularization cost function that enables a well-posed (i.e., consistent) basis functions formulation of nonlinear DeePC. To optimize computational efficiency of basis functions DeePC we further develop two alternative formulations that use a simpler, sparse regularization cost function and ridge regression, respectively. Consistency implications for Koopman DeePC as well as several methods for constructing the basis functions representation are also indicated. The effectiveness of the developed consistent basis functions DeePC formulations is illustrated on a benchmark nonlinear pendulum state-space model, for both noise free and noisy data.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了基于数据预测控制（DeePC）的扩展到非线性系统，使用通用基函数。我们首先构造了基函数DeePC行为预测器，并确定了相应的必要和 suficient conditions for equivalence with a corresponding basis functions multi-step identified predictor。这些条件导出了一个动态正则化成本函数，使得基函数DeePC的形式ulation well-posed（即一致）。为了提高基函数DeePC的计算效率，我们还开发了两种替代形式，使用简单的稀疏正则化成本函数和ridge regression。我们还探讨了 Koopman DeePC 的一致性含义，以及构造基函数表示的方法。这些方法在一个标准的非线性摆车状态空间模型上进行了效果的证明，包括无噪和噪声数据。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Shapley-Value-Approximation-for-Data-Evaluation"><a href="#Accelerated-Shapley-Value-Approximation-for-Data-Evaluation" class="headerlink" title="Accelerated Shapley Value Approximation for Data Evaluation"></a>Accelerated Shapley Value Approximation for Data Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05346">http://arxiv.org/abs/2311.05346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Watson, Zeno Kujawa, Rayna Andreeva, Hao-Tsung Yang, Tariq Elahi, Rik Sarkar</li>
<li>for: 这个论文主要是为了提出更高效的数据估价方法，以便在机器学习中实现数据筛选、有效学习和数据分享的奖励。</li>
<li>methods: 论文使用了 SHAPLEY 值来进行数据估价，但 SHAPLEY 值 computationally expensive，因此提出了一种基于机器学习问题的结构性质的更高效的approximation方法。</li>
<li>results:  experiments 表明，使用 $\delta$-Shapley 策略可以快速地计算出数据的估价值和排名，而且可以保持数据的准确性和有效性。在预训练网络中，该方法可以更好地实现准确的评估使用小 subsets。<details>
<summary>Abstract</summary>
Data valuation has found various applications in machine learning, such as data filtering, efficient learning and incentives for data sharing. The most popular current approach to data valuation is the Shapley value. While popular for its various applications, Shapley value is computationally expensive even to approximate, as it requires repeated iterations of training models on different subsets of data. In this paper we show that the Shapley value of data points can be approximated more efficiently by leveraging the structural properties of machine learning problems. We derive convergence guarantees on the accuracy of the approximate Shapley value for different learning settings including Stochastic Gradient Descent with convex and non-convex loss functions. Our analysis suggests that in fact models trained on small subsets are more important in the context of data valuation. Based on this idea, we describe $\delta$-Shapley -- a strategy of only using small subsets for the approximation. Experiments show that this approach preserves approximate value and rank of data, while achieving speedup of up to 9.9x. In pre-trained networks the approach is found to bring more efficiency in terms of accurate evaluation using small subsets.
</details>
<details>
<summary>摘要</summary>
“数据评估在机器学习中找到了多种应用，如数据筛选、高效学习和数据共享的激励。目前最受欢迎的数据评估方法是希布利值。虽然具有多种应用，但希布利值计算成本较高，因为需要重复训练模型在不同的数据 subsets上。在这篇论文中，我们表明希布利值可以更有效地被 aproximated，通过利用机器学习问题的结构性质。我们 deriv 出不同学习设置下的准确性拟合保证，包括杂函数梯度下降和非杂函数梯度下降。我们的分析表明，在数据评估中，使用小subset更重要。基于这个想法，我们描述了 $\delta$-希布利（delta-Shapley）策略，即只使用小subset进行拟合。实验表明，这种方法可以保持数据的approximate值和排名，同时实现速度增加达9.9倍。在预训练网络中，该方法被发现更有效，可以更准确地评估数据使用小 subsets。”
</details></li>
</ul>
<hr>
<h2 id="Real-time-Addressee-Estimation-Deployment-of-a-Deep-Learning-Model-on-the-iCub-Robot"><a href="#Real-time-Addressee-Estimation-Deployment-of-a-Deep-Learning-Model-on-the-iCub-Robot" class="headerlink" title="Real-time Addressee Estimation: Deployment of a Deep-Learning Model on the iCub Robot"></a>Real-time Addressee Estimation: Deployment of a Deep-Learning Model on the iCub Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05334">http://arxiv.org/abs/2311.05334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo Mazzola, Francesco Rea, Alessandra Sciutti</li>
<li>for: 本研究旨在开发有效的对话代理人，以便人机交互更加畅通。</li>
<li>methods: 该研究使用深度学习模型，利用说话者的非语言行为，如眼光和姿势，进行地址者估计。</li>
<li>results: 实验结果表明，基于非语言行为的地址者估计模型在实时人机交互中表现出色，比前一次使用同一 dataset 进行训练的模型更高效。<details>
<summary>Abstract</summary>
Addressee Estimation is the ability to understand to whom a person is talking, a skill essential for social robots to interact smoothly with humans. In this sense, it is one of the problems that must be tackled to develop effective conversational agents in multi-party and unstructured scenarios. As humans, one of the channels that mainly lead us to such estimation is the non-verbal behavior of speakers: first of all, their gaze and body pose. Inspired by human perceptual skills, in the present work, a deep-learning model for Addressee Estimation relying on these two non-verbal features is designed, trained, and deployed on an iCub robot. The study presents the procedure of such implementation and the performance of the model deployed in real-time human-robot interaction compared to previous tests on the dataset used for the training.
</details>
<details>
<summary>摘要</summary>
接受人 judgement 是一种关键的技能，用于社交机器人与人类交互。在这种情况下，它是开发有效的对话代理人的问题之一。人类中一个主要的渠道带我们到此估计是说话人的非语言行为：首先是他的视线和姿势。受人类感知技能的欣慰，在当前工作中，一种基于这两种非语言特征的深度学习模型 для Addressee Estimation 被设计、训练和部署到iCub机器人上。该研究介绍了该实施的过程和在真实的人机交互中模型的性能与之前在用于训练的数据集进行比较。
</details></li>
</ul>
<hr>
<h2 id="RepQ-Generalizing-Quantization-Aware-Training-for-Re-Parametrized-Architectures"><a href="#RepQ-Generalizing-Quantization-Aware-Training-for-Re-Parametrized-Architectures" class="headerlink" title="RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures"></a>RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05317">http://arxiv.org/abs/2311.05317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiia Prutianova, Alexey Zaytsev, Chung-Kuei Lee, Fengyu Sun, Ivan Koryakovskiy</li>
<li>for: 提高神经网络的效率和可扩展性，使其在资源有限的环境中部署更加容易。</li>
<li>methods: 使用量化和重 parametrization 两种方法进行神经网络的提升。量化可以压缩神经网络的大小，而重 parametrization 可以提高神经网络的性能。</li>
<li>results: 提出了一种名为 RepQ 的新方法，通过对重 parametrized 网络进行量化，实现了神经网络的效率提升。 RepQ 在不同的重 parametrized 模型中都能够达到比基eline方法LSQ量化方案更好的性能。<details>
<summary>Abstract</summary>
Existing neural networks are memory-consuming and computationally intensive, making deploying them challenging in resource-constrained environments. However, there are various methods to improve their efficiency. Two such methods are quantization, a well-known approach for network compression, and re-parametrization, an emerging technique designed to improve model performance. Although both techniques have been studied individually, there has been limited research on their simultaneous application. To address this gap, we propose a novel approach called RepQ, which applies quantization to re-parametrized networks. Our method is based on the insight that the test stage weights of an arbitrary re-parametrized layer can be presented as a differentiable function of trainable parameters. We enable quantization-aware training by applying quantization on top of this function. RepQ generalizes well to various re-parametrized models and outperforms the baseline method LSQ quantization scheme in all experiments.
</details>
<details>
<summary>摘要</summary>
现有的神经网络具有较大的内存占用和计算复杂度，因此在有限资源环境中部署困难。然而，有多种方法可以改善其效率。其中两种方法是量化和重 parametrization。虽然这两种方法已经受到了研究，但它们之前的同时应用却很少。为了解决这个空白，我们提出了一种新的方法 called RepQ，该方法将量化应用于重 parametrized 网络。我们的方法基于对任意重 parametrized 层的测试阶段权重可以表示为可导的函数的假设。我们通过应用量化来实现量化执行。RepQ 可以通过多种重 parametrized 模型，并在所有实验中超过基准方法 LSQ 量化方案。
</details></li>
</ul>
<hr>
<h2 id="Reliable-and-Efficient-Data-Collection-in-UAV-based-IoT-Networks"><a href="#Reliable-and-Efficient-Data-Collection-in-UAV-based-IoT-Networks" class="headerlink" title="Reliable and Efficient Data Collection in UAV-based IoT Networks"></a>Reliable and Efficient Data Collection in UAV-based IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05303">http://arxiv.org/abs/2311.05303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Poorvi Joshi, Alakesh Kalita, Mohan Gurusamy</li>
<li>for: 本研究旨在探讨无人机（UAV）在物联网（IoT）中的数据采集问题，以提高IoT网络的可靠性和效率。</li>
<li>methods: 本研究涉及了多种UAV-based数据采集方法，包括通信和网络方面的研究，以及数据采集方法的优缺点分析。此外，本研究还考虑了数据准确性和一致性、网络连接稳定性和数据安全隐私等因素。</li>
<li>results: 本研究结果表明，UAV-assisted IoT networks可以通过轨迹规划、碰撞避免、传感器网络划分、数据聚合和人工智能优化等方法来提高数据采集的可靠性和效率。此外，本研究还提出了在UAV-assisted IoT networks中使用无人机为服务的方法，以提高数据采集的可靠性和效率。<details>
<summary>Abstract</summary>
Internet of Things (IoT) involves sensors for monitoring and wireless networks for efficient communication. However, resource-constrained IoT devices and limitations in existing wireless technologies hinder its full potential. Integrating Unmanned Aerial Vehicles (UAVs) into IoT networks can address some challenges by expanding its' coverage, providing security, and bringing computing closer to IoT devices. Nevertheless, effective data collection in UAV-assisted IoT networks is hampered by factors, including dynamic UAV behavior, environmental variables, connectivity instability, and security considerations. In this survey, we first explore UAV-based IoT networks, focusing on communication and networking aspects. Next, we cover various UAV-based data collection methods their advantages and disadvantages, followed by a discussion on performance metrics for data collection. As this article primarily emphasizes reliable and efficient data collection in UAV-assisted IoT networks, we briefly discuss existing research on data accuracy and consistency, network connectivity, and data security and privacy to provide insights into reliable data collection. Additionally, we discuss efficient data collection strategies in UAV-based IoT networks, covering trajectory and path planning, collision avoidance, sensor network clustering, data aggregation, UAV swarm formations, and artificial intelligence for optimization. We also present two use cases of UAVs as a service for enhancing data collection reliability and efficiency. Finally, we discuss future challenges in data collection for UAV-assisted IoT networks.
</details>
<details>
<summary>摘要</summary>
互联网物件 (IoT) 包括侦测器和无线网络，但是资源有限的 IoT 设备和现有的无线技术限制了它的全面性。将无人航空车 (UAV)  integrate into IoT 网络可以解决一些挑战，例如扩展覆盖范围、提供安全性和让计算更近运行 IoT 设备。然而，UAV 协助 IoT 网络的数据收集过程受到多重因素影响，包括 UAV 动态行为、环境变量、连接不稳定和安全考虑。在这篇文章中，我们首先探讨 UAV 基本的 IoT 网络，专注于通信和网络方面。接着，我们详细介绍了不同的 UAV 数据收集方法，包括优点和缺点。接着，我们讨论了数据收集表现指标，以提供可靠数据收集的深入了解。此外，我们还讨论了现有的数据准确和一致性、网络连接和安全性等问题，以及如何使用 UAV 实现可靠的数据收集。最后，我们介绍了两个 UAV 作为服务的使用案例，以增强数据收集可靠性和效率。最后，我们讨论了未来数据收集的挑战，以及如何通过 UAV 实现更好的数据收集。
</details></li>
</ul>
<hr>
<h2 id="Latent-Task-Specific-Graph-Network-Simulators"><a href="#Latent-Task-Specific-Graph-Network-Simulators" class="headerlink" title="Latent Task-Specific Graph Network Simulators"></a>Latent Task-Specific Graph Network Simulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05256">http://arxiv.org/abs/2311.05256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philippdahlinger/ltsgns_ai4science">https://github.com/philippdahlinger/ltsgns_ai4science</a></li>
<li>paper_authors: Philipp Dahlinger, Niklas Freymuth, Michael Volpp, Tai Hoang, Gerhard Neumann</li>
<li>for:  mesh-based simulation 的适应性和可重用性问题</li>
<li>methods: 使用 Bayesian meta-learning 方法，通过非积累任务 posterior 近似来采样 latent 描述未知系统特性，并利用运动 primitives 进行高效的全 trajectory 预测</li>
<li>results: 通过多种实验 validate 了方法的效果，与或超过了现有基eline方法的性能，并且可以处理不同类型的上下文数据，如使用点云进行推理。<details>
<summary>Abstract</summary>
Simulating dynamic physical interactions is a critical challenge across multiple scientific domains, with applications ranging from robotics to material science. For mesh-based simulations, Graph Network Simulators (GNSs) pose an efficient alternative to traditional physics-based simulators. Their inherent differentiability and speed make them particularly well-suited for inverse design problems. Yet, adapting to new tasks from limited available data is an important aspect for real-world applications that current methods struggle with. We frame mesh-based simulation as a meta-learning problem and use a recent Bayesian meta-learning method to improve GNSs adaptability to new scenarios by leveraging context data and handling uncertainties. Our approach, latent task-specific graph network simulator, uses non-amortized task posterior approximations to sample latent descriptions of unknown system properties. Additionally, we leverage movement primitives for efficient full trajectory prediction, effectively addressing the issue of accumulating errors encountered by previous auto-regressive methods. We validate the effectiveness of our approach through various experiments, performing on par with or better than established baseline methods. Movement primitives further allow us to accommodate various types of context data, as demonstrated through the utilization of point clouds during inference. By combining GNSs with meta-learning, we bring them closer to real-world applicability, particularly in scenarios with smaller datasets.
</details>
<details>
<summary>摘要</summary>
模拟动态物理交互是多个科学领域的关键挑战，其应用范围从 робо扮到物理科学。为 mesh-based  simulate，图像网络 simulate（GNS）成为效率备受关注的代替方案。它们的自然差分和速度使其特别适合反向设计问题。然而，适应新任务从有限的数据中学习是现实应用中的重要问题，现有方法困难以处理。我们将 mesh-based  simulate 作为一个元学习问题，使用最近的 Bayesian 元学习方法来提高 GNS 的适应新enario 能力，利用上下文数据和处理不确定性。我们的方法，缺失任务特定图像网络 simulate（LT-GNS），使用非折衔任务 posterior  aproximations 采样缺失系统属性的latent描述。此外，我们利用运动 primitives  для高效地预测全 trajectory，有效地解决过去的 auto-regressive 方法所遇到的积累错误问题。我们通过多个实验 validate 了我们的方法的有效性，与或更好于现有基eline方法。运动 primitives 还允许我们根据不同的上下文数据进行可扩展的应用，如通过使用点云进行推理。通过将 GNS 与元学习结合，我们使其更适合实际应用，特别是在小数据量的情况下。
</details></li>
</ul>
<hr>
<h2 id="When-Meta-Learning-Meets-Online-and-Continual-Learning-A-Survey"><a href="#When-Meta-Learning-Meets-Online-and-Continual-Learning-A-Survey" class="headerlink" title="When Meta-Learning Meets Online and Continual Learning: A Survey"></a>When Meta-Learning Meets Online and Continual Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05241">http://arxiv.org/abs/2311.05241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehyeon Son, Soochan Lee, Gunhee Kim</li>
<li>for: This paper aims to provide a comprehensive survey of various learning frameworks, including meta-learning, continual learning, and online learning, and their applications in deep neural networks.</li>
<li>methods: The paper uses a consistent terminology and formal descriptions to organize the problem settings and learning algorithms, and offers an overview of the current state of research in these areas.</li>
<li>results: The paper aims to facilitate a clear understanding of the differences between the learning frameworks and foster further advancements in this promising area of research.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提供深度神经网络中不同学习框架的总结，包括元学习、继续学习和在线学习，以及它们的应用。</li>
<li>methods: 这篇论文使用一致的术语和正式描述来组织问题设定和学习算法，并提供深度神经网络中这些领域的现状报告。</li>
<li>results: 这篇论文目的是为这些学习框架的研究做出清晰的认知，并促进这个领域的进一步发展。<details>
<summary>Abstract</summary>
Over the past decade, deep neural networks have demonstrated significant success using the training scheme that involves mini-batch stochastic gradient descent on extensive datasets. Expanding upon this accomplishment, there has been a surge in research exploring the application of neural networks in other learning scenarios. One notable framework that has garnered significant attention is meta-learning. Often described as "learning to learn," meta-learning is a data-driven approach to optimize the learning algorithm. Other branches of interest are continual learning and online learning, both of which involve incrementally updating a model with streaming data. While these frameworks were initially developed independently, recent works have started investigating their combinations, proposing novel problem settings and learning algorithms. However, due to the elevated complexity and lack of unified terminology, discerning differences between the learning frameworks can be challenging even for experienced researchers. To facilitate a clear understanding, this paper provides a comprehensive survey that organizes various problem settings using consistent terminology and formal descriptions. By offering an overview of these learning paradigms, our work aims to foster further advancements in this promising area of research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whisper-in-Focus-Enhancing-Stuttered-Speech-Classification-with-Encoder-Layer-Optimization"><a href="#Whisper-in-Focus-Enhancing-Stuttered-Speech-Classification-with-Encoder-Layer-Optimization" class="headerlink" title="Whisper in Focus: Enhancing Stuttered Speech Classification with Encoder Layer Optimization"></a>Whisper in Focus: Enhancing Stuttered Speech Classification with Encoder Layer Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05203">http://arxiv.org/abs/2311.05203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huma Ameer, Seemab Latif, Rabia Latif, Sana Mukhtar</li>
<li>for: 本研究旨在探讨自动识别吞声的可能性，利用深度学习技术来分类吞声中的不一致类型。</li>
<li>methods: 本研究使用了Whisper模型进行分类，并对SEP28-kbenchmark dataset进行改进，以及引入有效的Encoder层冻结策略。</li>
<li>results: 优化后的Whisper模型在分类中获得了0.81的平均F1分数，表明其能力。研究还发现，深度Encoder层在识别不一致类型中具有更大的贡献，相比于初始层。<details>
<summary>Abstract</summary>
In recent years, advancements in the field of speech processing have led to cutting-edge deep learning algorithms with immense potential for real-world applications. The automated identification of stuttered speech is one of such applications that the researchers are addressing by employing deep learning techniques. Recently, researchers have utilized Wav2vec2.0, a speech recognition model to classify disfluency types in stuttered speech. Although Wav2vec2.0 has shown commendable results, its ability to generalize across all disfluency types is limited. In addition, since its base model uses 12 encoder layers, it is considered a resource-intensive model. Our study unravels the capabilities of Whisper for the classification of disfluency types in stuttered speech. We have made notable contributions in three pivotal areas: enhancing the quality of SEP28-k benchmark dataset, exploration of Whisper for classification, and introducing an efficient encoder layer freezing strategy. The optimized Whisper model has achieved the average F1-score of 0.81, which proffers its abilities. This study also unwinds the significance of deeper encoder layers in the identification of disfluency types, as the results demonstrate their greater contribution compared to initial layers. This research represents substantial contributions, shifting the emphasis towards an efficient solution, thereby thriving towards prospective innovation.
</details>
<details>
<summary>摘要</summary>
Recently, advancements in speech processing technology have led to the development of cutting-edge deep learning algorithms with numerous practical applications. One such application is the automated identification of stuttered speech, which researchers are addressing using deep learning techniques. Previously, researchers have utilized Wav2vec2.0, a speech recognition model, to classify disfluency types in stuttered speech. Although Wav2vec2.0 has shown promising results, its ability to generalize across all disfluency types is limited. Additionally, its base model uses 12 encoder layers, making it a resource-intensive model. Our study explores the capabilities of Whisper for classifying disfluency types in stuttered speech. Our contributions include enhancing the quality of the SEP28-k benchmark dataset, exploring the use of Whisper for classification, and introducing an efficient encoder layer freezing strategy. The optimized Whisper model achieved an average F1-score of 0.81, demonstrating its effectiveness. Our findings also highlight the significance of deeper encoder layers in identifying disfluency types, as the results show that they contribute more than the initial layers. This research represents significant contributions and paves the way for more efficient solutions, driving innovation forward.Here's the word-for-word translation of the text into Simplified Chinese:近年来，语音处理技术的进步导致了深度学习算法的出现，这些算法在实际应用中具有广泛的潜力。一种如此应用是自动识别吞声的语音，研究人员通过深度学习技术来实现这一目标。之前，研究人员已经使用Wav2vec2.0，一种语音识别模型，来分类吞声语音中的不顺拍类型。虽然Wav2vec2.0表现了良好，但它的总体化能力有限。此外，它的基础模型使用12层编码层，因此被视为资源占用的模型。我们的研究探讨了适用于吞声语音中的不顺拍类型分类的Whisper模型。我们的贡献包括提高SEP28-k测试数据集的质量、使用Whisper模型进行分类和引入高效的编码层冻结策略。优化后的Whisper模型实现了0.81的平均F1分数，这表明它的能力。我们的发现还表明了深度编码层在识别不顺拍类型时的重要性，结果显示它们比初始层更大的贡献。这种研究代表了重要的贡献，推动了可能的创新。
</details></li>
</ul>
<hr>
<h2 id="Perfecting-Liquid-State-Theories-with-Machine-Intelligence"><a href="#Perfecting-Liquid-State-Theories-with-Machine-Intelligence" class="headerlink" title="Perfecting Liquid-State Theories with Machine Intelligence"></a>Perfecting Liquid-State Theories with Machine Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05167">http://arxiv.org/abs/2311.05167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhong Wu, Mengyang Gu</li>
<li>for: 预测电子结构、分子力场和各种固体系统的物理化学性质</li>
<li>methods: Functional machine learning技术，包括代理模型、维度减少和不确定性评估</li>
<li>results: 提高精度、扩展应用范围和计算效率等<details>
<summary>Abstract</summary>
Recent years have seen a significant increase in the use of machine intelligence for predicting electronic structure, molecular force fields, and the physicochemical properties of various condensed systems. However, substantial challenges remain in developing a comprehensive framework capable of handling a wide range of atomic compositions and thermodynamic conditions. This perspective discusses potential future developments in liquid-state theories leveraging on recent advancements of functional machine learning. By harnessing the strengths of theoretical analysis and machine learning techniques including surrogate models, dimension reduction and uncertainty quantification, we envision that liquid-state theories will gain significant improvements in accuracy, scalability and computational efficiency, enabling their broader applications across diverse materials and chemical systems.
</details>
<details>
<summary>摘要</summary>
近年来，机器智能的应用在预测电子结构、分子力场和各种固体系统的物理化学性质方面有了显著增长。然而，构建涵盖各种原子组成和热力学条件的全面框架仍存在巨大的挑战。本着观点认为，利用最新的函数机器学习技术，包括代表模型、维度减少和不确定性评估，将来的液体理论将在准确性、可扩展性和计算效率方面做出显著改进，使其在多种材料和化学系统中得到更广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Counter-Empirical-Attacking-based-on-Adversarial-Reinforcement-Learning-for-Time-Relevant-Scoring-System"><a href="#Counter-Empirical-Attacking-based-on-Adversarial-Reinforcement-Learning-for-Time-Relevant-Scoring-System" class="headerlink" title="Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System"></a>Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05144">http://arxiv.org/abs/2311.05144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin</li>
<li>for: This paper aims to establish a novel framework to improve scoring systems in the era of big data, without relying on ground truth or prior experience.</li>
<li>methods: The proposed framework uses a “counter-empirical attacking” mechanism to generate attacking behavior traces and evaluate the scoring system’s robustness. An adversarial “enhancer” is then applied to find improvement strategies and learn a proper scoring function.</li>
<li>results: Extensive experiments conducted on two scoring systems demonstrate the effectiveness of the proposed framework in improving the scoring system’s robustness to attacking activity traces.<details>
<summary>Abstract</summary>
Scoring systems are commonly seen for platforms in the era of big data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and more efficiently thereby. To establish such scoring systems, several "empirical criteria" are firstly determined, followed by dedicated top-down design for each factor of the score, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What's worse, many fresh projects usually have no ground-truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth, and propose a novel framework to improve the system from scratch. In this paper, we propose a "counter-empirical attacking" mechanism that can generate "attacking" behavior traces and try to break the empirical rules of the scoring system. Then an adversarial "enhancer" is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework.
</details>
<details>
<summary>摘要</summary>
大数据时代中，普遍出现了评分系统。从金融服务中的信用评分系统到电商平台上的会员评分系统，平台管理者通过这些系统来引导用户遵循推荐的活动模式，更好地管理资源，提高效率。为建立这些评分系统，需要首先确定一些“实证标准”，然后针对每个因素进行专门的顶部设计，这通常需要巨大的努力来调整和调整评分函数在新应用场景中。尤其是许多新项目缺乏实践经验或者实证数据，使得设计更加困难。为了减少每个新评分系统的手动调整努力，我们创新地研究了评分系统，不基于实证标准，而是基于预设的empirical criterion。在这篇论文中，我们提出了一种“反实证攻击”机制，可以生成“攻击”行为轨迹，并尝试破坏empirical规则。然后，我们应用了一种“增强器”来评估评分系统，并找到改进策略。通过训练对抗学习问题，我们可以学习一个robust的评分函数，抵抗攻击行为轨迹的尝试。我们在两个评分系统上进行了广泛的实验，包括共享计算资源平台和金融信用系统。实验结果证明了我们提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="On-neural-and-dimensional-collapse-in-supervised-and-unsupervised-contrastive-learning-with-hard-negative-sampling"><a href="#On-neural-and-dimensional-collapse-in-supervised-and-unsupervised-contrastive-learning-with-hard-negative-sampling" class="headerlink" title="On neural and dimensional collapse in supervised and unsupervised contrastive learning with hard negative sampling"></a>On neural and dimensional collapse in supervised and unsupervised contrastive learning with hard negative sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05139">http://arxiv.org/abs/2311.05139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijie Jiang, Thuan Nguyen, Shuchin Aeron, Prakash Ishwar</li>
<li>for: 证明Supervised Contrastive Learning（SCL）、Hard-SCL（HSCL）和Unsupervised Contrastive Learning（UCL）的风险可以通过表现出神经坍缩（NC）来最小化，即类均匀的紧张框（ETF）和数据相同类别的映射到同一个表示。</li>
<li>methods: 使用了 widely-studied data model和通用的损失函数和坚化函数。</li>
<li>results: 证明了对于任何表示映射，HSCL和Hard-UCL的风险都是SCL和UCL风险的下界，而且这种下界是通过NC来实现的。此外，我们的证明比之前的证明更加简洁、紧凑和透明。<details>
<summary>Abstract</summary>
For a widely-studied data model and general loss and sample-hardening functions we prove that the Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) risks are minimized by representations that exhibit Neural Collapse (NC), i.e., the class means form an Equianglular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) risks are lower bounded by the corresponding SCL and UCL risks. Although the optimality of ETF is known for SCL, albeit only for InfoNCE loss, its optimality for HSCL and UCL under general loss and hardening functions is novel. Moreover, our proofs are much simpler, compact, and transparent. We empirically demonstrate, for the first time, that ADAM optimization of HSCL and HUCL risks with random initialization and suitable hardness levels can indeed converge to the NC geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard negatives or feature normalization, however, the representations learned via ADAM suffer from dimensional collapse (DC) and fail to attain the NC geometry.
</details>
<details>
<summary>摘要</summary>
For a widely-studied data model and general loss and sample-hardening functions, we prove that the Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) risks are minimized by representations that exhibit Neural Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) risks are lower bounded by the corresponding SCL and UCL risks. Although the optimality of ETF is known for SCL, albeit only for InfoNCE loss, its optimality for HSCL and UCL under general loss and hardening functions is novel. Moreover, our proofs are much simpler, compact, and transparent. We empirically demonstrate, for the first time, that ADAM optimization of HSCL and HUCL risks with random initialization and suitable hardness levels can indeed converge to the NC geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard negatives or feature normalization, however, the representations learned via ADAM suffer from dimensional collapse (DC) and fail to attain the NC geometry.
</details></li>
</ul>
<hr>
<h2 id="Improving-Computational-Efficiency-for-Powered-Descent-Guidance-via-Transformer-based-Tight-Constraint-Prediction"><a href="#Improving-Computational-Efficiency-for-Powered-Descent-Guidance-via-Transformer-based-Tight-Constraint-Prediction" class="headerlink" title="Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction"></a>Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05135">http://arxiv.org/abs/2311.05135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Briden, Trey Gurga, Breanna Johnson, Abhishek Cauligi, Richard Linares</li>
<li>for: 这篇论文的目的是提出一种可扩展的算法，以减少直接优化形式中的计算复杂性。</li>
<li>methods: 该算法使用Transformer神经网络，通过在前一次的轨迹优化算法中提取数据，准确预测问题参数和最优解的关系。</li>
<li>results: 在应用于火星推进 descent 问题中，T-PDG 可以在计算3个自由度燃料优化 trajectory 时，比lossless convexification 快得多，从1-8秒钟减少到了500毫秒以下。并且保证了安全和优化的解。<details>
<summary>Abstract</summary>
In this work, we present Transformer-based Powered Descent Guidance (T-PDG), a scalable algorithm for reducing the computational complexity of the direct optimization formulation of the spacecraft powered descent guidance problem. T-PDG uses data from prior runs of trajectory optimization algorithms to train a transformer neural network, which accurately predicts the relationship between problem parameters and the globally optimal solution for the powered descent guidance problem. The solution is encoded as the set of tight constraints corresponding to the constrained minimum-cost trajectory and the optimal final time of landing. By leveraging the attention mechanism of transformer neural networks, large sequences of time series data can be accurately predicted when given only the spacecraft state and landing site parameters. When applied to the real problem of Mars powered descent guidance, T-PDG reduces the time for computing the 3 degree of freedom fuel-optimal trajectory, when compared to lossless convexification, from an order of 1-8 seconds to less than 500 milliseconds. A safe and optimal solution is guaranteed by including a feasibility check in T-PDG before returning the final trajectory.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了Transformer基于Powered Descent Guidance（T-PDG）算法，用于降低直接优化问题的计算复杂性。T-PDG使用之前的轨迹优化算法的数据来训练Transformer神经网络，以准确预测问题参数和 globally optimal solution的关系。解决方案被编码为最佳化的约束集，包括约束最小成本轨迹和着陆时刻。通过转换器神经网络的注意力机制，可以准确预测大量时间序列数据，只要提供空间craft状态和着陆地点参数。在应用于 Mars 着陆引导问题时，T-PDG 比lossless convexification减少了计算3个自由度燃料优化轨迹的时间，从大约1-8秒钟降低到 fewer than 500毫秒。保证安全且优化解决方案的可靠性，T-PDG 中包括了可靠性检查，以确保返回最终轨迹。
</details></li>
</ul>
<hr>
<h2 id="Exploring-and-Analyzing-Wildland-Fire-Data-Via-Machine-Learning-Techniques"><a href="#Exploring-and-Analyzing-Wildland-Fire-Data-Via-Machine-Learning-Techniques" class="headerlink" title="Exploring and Analyzing Wildland Fire Data Via Machine Learning Techniques"></a>Exploring and Analyzing Wildland Fire Data Via Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05128">http://arxiv.org/abs/2311.05128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipak Dulal, Joseph J. Charney, Michael Gallagher, Carmeliza Navasca, Nicholas Skowronski<br>for: 这项研究旨在探讨10Hz时间序列的热电差温度和风速测得的动力动量能量（TKE）之间的相关性，以及利用热电差温度作为预测TKE的可能性。methods: 这项研究使用了机器学习模型，包括深度神经网络、Random Forest Regressor、Gradient Boosting和Gaussian Process Regressor，来评估热电差温度干扰的可能性来预测TKE值。results: 研究发现，使用不同的机器学习模型可以高度准确地预测TKE，尤其是使用回归模型。数据视觉和相关分析显示了热电差温度和TKE之间的各种模式和关系，为火Behavior和烟雾科学提供了重要的新视角。<details>
<summary>Abstract</summary>
This research project investigated the correlation between a 10 Hz time series of thermocouple temperatures and turbulent kinetic energy (TKE) computed from wind speeds collected from a small experimental prescribed burn at the Silas Little Experimental Forest in New Jersey, USA. The primary objective of this project was to explore the potential for using thermocouple temperatures as predictors for estimating the TKE produced by a wildland fire. Machine learning models, including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor, are employed to assess the potential for thermocouple temperature perturbations to predict TKE values. Data visualization and correlation analyses reveal patterns and relationships between thermocouple temperatures and TKE, providing insight into the underlying dynamics. The project achieves high accuracy in predicting TKE by employing various machine learning models despite a weak correlation between the predictors and the target variable. The results demonstrate significant success, particularly from regression models, in accurately estimating the TKE. The research findings contribute to fire behavior and smoke modeling science, emphasizing the importance of incorporating machine learning approaches and identifying complex relationships between fine-scale fire behavior and turbulence. Accurate TKE estimation using thermocouple temperatures allows for the refinement of models that can inform decision-making in fire management strategies, facilitate effective risk mitigation, and optimize fire management efforts. This project highlights the valuable role of machine learning techniques in analyzing wildland fire data, showcasing their potential to advance fire research and management practices.
</details>
<details>
<summary>摘要</summary>
Machine learning models, including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor, were applied to assess the potential for thermocouple temperature perturbations to predict TKE values. Data visualization and correlation analyses revealed patterns and relationships between thermocouple temperatures and TKE, providing insights into the underlying dynamics.Despite a weak correlation between the predictors and the target variable, the project achieved high accuracy in predicting TKE using various machine learning models. The results demonstrated significant success, particularly from regression models, in accurately estimating TKE. The findings contribute to fire behavior and smoke modeling science, emphasizing the importance of incorporating machine learning approaches and identifying complex relationships between fine-scale fire behavior and turbulence.Accurate TKE estimation using thermocouple temperatures allows for the refinement of models that can inform decision-making in fire management strategies, facilitate effective risk mitigation, and optimize fire management efforts. This project highlights the valuable role of machine learning techniques in analyzing wildland fire data, showcasing their potential to advance fire research and management practices.
</details></li>
</ul>
<hr>
<h2 id="Covering-Number-of-Real-Algebraic-Varieties-Improved-Bound-and-Applications"><a href="#Covering-Number-of-Real-Algebraic-Varieties-Improved-Bound-and-Applications" class="headerlink" title="Covering Number of Real Algebraic Varieties: Improved Bound and Applications"></a>Covering Number of Real Algebraic Varieties: Improved Bound and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05116">http://arxiv.org/abs/2311.05116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Joe Kileel</li>
<li>for: 提供了一个上界bounds on the covering number of real algebraic varieties, 映射 polynomials, and semialgebraic sets.</li>
<li>methods: 使用了新的方法, which improves the best known bound by Yomdin-Comte and has a much simpler proof.</li>
<li>results: 得到了一个更好的 bound on the volume of the tubular neighborhood of a real variety, 并应用到了三个主要应用领域：low rank CP tensors, (general) polynomial optimization problems, 和 deep neural networks with rational or ReLU activations.<details>
<summary>Abstract</summary>
We prove an upper bound on the covering number of real algebraic varieties, images of polynomial maps and semialgebraic sets. The bound remarkably improves the best known bound by Yomdin-Comte, and its proof is much more straightforward. As a consequence, our result gives a bound on volume of the tubular neighborhood of a real variety, improving the results by Lotz and Basu-Lerario. We apply our theory to three main application domains. Firstly, we derive a near-optimal bound on the covering number of low rank CP tensors. Secondly, we prove a bound on the sketching dimension for (general) polynomial optimization problems. Lastly, we deduce generalization error bounds for deep neural networks with rational or ReLU activations, improving or matching the best known results in the literature.
</details>
<details>
<summary>摘要</summary>
我们证明了实数型变量的覆盖数目的上限，包括映射函数图像和 semi-代数集。这个上限意外地改进了最佳知的 bound by Yomdin-Comte，并且证明的过程非常直观。因此，我们的结果可以提供实体变量的卷积附近体积的上限，超越了 Lotz 和 Basu-Lerario 的结果。我们在三个主要应用领域中运用了我们的理论：第一个，我们从低级CP张量中获得了near-优化的覆盖数目上限。第二个，我们证明了普通多项式优化问题的绘制维度上限。第三个，我们从深度神经网络中获得了 rational 或 ReLU 激活函数的泛化误差上限，超越了文献中最佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Online-Federated-Learning-with-Multiple-Kernels"><a href="#Personalized-Online-Federated-Learning-with-Multiple-Kernels" class="headerlink" title="Personalized Online Federated Learning with Multiple Kernels"></a>Personalized Online Federated Learning with Multiple Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05108">http://arxiv.org/abs/2311.05108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pouyamghari/pof-mkl">https://github.com/pouyamghari/pof-mkl</a></li>
<li>paper_authors: Pouya M. Ghari, Yanning Shen</li>
<li>for: 在线非齐次函数掌 approximation 中表现出色的多核学习 (MKL) 技术，以实现在多客户端上进行线上非齐次函数掌 approximation。</li>
<li>methods: 这篇 paper 提出了一个算法框架，让客户端与服务器进行通信，实现在多客户端上进行线上非齐次函数掌 approximation，并且使用随机特征（RF）近似来实现扩展性。</li>
<li>results: paper 证明了，使用提出的线上联边MKL算法，每个客户端在它的最佳核函数方面具有子线性 regret，这表明了提出的算法能够有效地处理客户端上的数据不均匀性。实验结果显示，提出的算法与其他线上联边核学学算法相比，具有明显的优势。<details>
<summary>Abstract</summary>
Multi-kernel learning (MKL) exhibits well-documented performance in online non-linear function approximation. Federated learning enables a group of learners (called clients) to train an MKL model on the data distributed among clients to perform online non-linear function approximation. There are some challenges in online federated MKL that need to be addressed: i) Communication efficiency especially when a large number of kernels are considered ii) Heterogeneous data distribution among clients. The present paper develops an algorithmic framework to enable clients to communicate with the server to send their updates with affordable communication cost while clients employ a large dictionary of kernels. Utilizing random feature (RF) approximation, the present paper proposes scalable online federated MKL algorithm. We prove that using the proposed online federated MKL algorithm, each client enjoys sub-linear regret with respect to the RF approximation of its best kernel in hindsight, which indicates that the proposed algorithm can effectively deal with heterogeneity of the data distributed among clients. Experimental results on real datasets showcase the advantages of the proposed algorithm compared with other online federated kernel learning ones.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GeoFormer-Predicting-Human-Mobility-using-Generative-Pre-trained-Transformer-GPT"><a href="#GeoFormer-Predicting-Human-Mobility-using-Generative-Pre-trained-Transformer-GPT" class="headerlink" title="GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)"></a>GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05092">http://arxiv.org/abs/2311.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aivin V. Solatorio</li>
<li>for: 预测人类移动（human mobility prediction）</li>
<li>methods: 使用了一种基于 transformer 架构的 decoder-only 模型（GeoFormer），并在 HuMob Challenge 2023 竞赛中进行了实验和评测。</li>
<li>results: 在 HuMob Challenge 2023 中，GeoFormer 表现出色，在两个数据集上都达到了高水平的预测性能，并在两个性能指标（GEO-BLEU 和 Dynamic Time Warping 指标）上都表现良好。<details>
<summary>Abstract</summary>
Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility. Our proposed model is rigorously tested in the context of the HuMob Challenge 2023 -- a competition designed to evaluate the performance of prediction models on standardized datasets to predict human mobility. The challenge leverages two datasets encompassing urban-scale data of 25,000 and 100,000 individuals over a longitudinal period of 75 days. GeoFormer stands out as a top performer in the competition, securing a place in the top-3 ranking. Its success is underscored by performing well on both performance metrics chosen for the competition -- the GEO-BLEU and the Dynamic Time Warping (DTW) measures. The performance of the GeoFormer on the HuMob Challenge 2023 underscores its potential to make substantial contributions to the field of human mobility prediction, with far-reaching implications for disaster preparedness, epidemic control, and beyond.
</details>
<details>
<summary>摘要</summary>
预测人员流动具有重要的实用价值，其应用范围从增进灾害风险规划到模拟疫情传播。在这篇论文中，我们提出了GeoFormer模型，这是基于GPT架构的解码器只模型，用于预测人员流动。我们的提议的模型在 HuMob Challenge 2023 中得到了证明，这是一项 Competition ，用于评估预测模型在标准化数据集上的性能。这个挑战使用了两个数据集，每个数据集包含25,000和100,000个人的城市规模数据，并且持续75天。GeoFormer在这个竞赛中表现出色，得到了排名前三名的成绩。它在选择竞赛中使用的两个性能指标中表现出色，即GEO-BLEU和动态时间戳滑块度量。GeoFormer在 HuMob Challenge 2023 中的表现表明它在人员流动预测方面具有很大的潜力，这对于灾害准备、疫情控制和其他领域都有广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Generalized-test-utilities-for-long-tail-performance-in-extreme-multi-label-classification"><a href="#Generalized-test-utilities-for-long-tail-performance-in-extreme-multi-label-classification" class="headerlink" title="Generalized test utilities for long-tail performance in extreme multi-label classification"></a>Generalized test utilities for long-tail performance in extreme multi-label classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05081">http://arxiv.org/abs/2311.05081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Schultheis, Marek Wydmuch, Wojciech Kotłowski, Rohit Babbar, Krzysztof Dembczyński</li>
<li>for: 这篇论文关注于EXTREME多个标签分类任务中，选择一小subset of relevant标签，并且解决长尾标签问题。</li>
<li>methods: 该论文使用了budgeted “at k” metrics作为代替方案，并在预测性能方面做出了优化。</li>
<li>results: 该论文通过使用块协调赋值法和提供证明的 regret保证和模型误差robustness，实现了对XMLC问题的有效解决。<details>
<summary>Abstract</summary>
Extreme multi-label classification (XMLC) is the task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more interesting or rewarding, but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted "at k" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the expected test utility (ETU) framework, which aims at optimizing the expected performance on a fixed test set. We derive optimal prediction rules and construct computationally efficient approximations with provable regret guarantees and robustness against model misspecification. Our algorithm, based on block coordinate ascent, scales effortlessly to XMLC problems and obtains promising results in terms of long-tail performance.
</details>
<details>
<summary>摘要</summary>
极端多标签分类（XMLC）是选择一小集合的相关标签从一个非常大的可能的标签集中的任务。因此，它具有长尾标签，即大多数标签有很少的正例。使用标准的表现度量如精度@k，一个分类器可以忽略尾标签并仍然报告良好的性能。然而，有时被认为正确预测在尾标签上更有趣或奖励，但社区没有尚未定制一个度量捕捉这个直觉概念。现有的可能性分数度量混乱长尾和缺失标签的问题。在这篇论文中，我们分析通用度量预算"at k"作为一个代替解决方案。为了解决这些度量的优化问题，我们在预测测试用户（ETU）框架中对它们进行优化。我们 deriv出了最佳预测规则，并构建了计算效率高并且对模型误差robust的计算方法。我们的算法基于块协调架，可以轻松扩展到XMLC问题，并取得了良好的长尾性能。
</details></li>
</ul>
<hr>
<h2 id="Social-Media-Bot-Detection-using-Dropout-GAN"><a href="#Social-Media-Bot-Detection-using-Dropout-GAN" class="headerlink" title="Social Media Bot Detection using Dropout-GAN"></a>Social Media Bot Detection using Dropout-GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05079">http://arxiv.org/abs/2311.05079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Shukla, Martin Jurecek, Mark Stamp</li>
<li>for: 寻找社交媒体平台上的机器人活动，以维护在线讨论的威信和避免网络犯罪。</li>
<li>methods: 使用生成对抗网络（GAN）进行机器人检测，并解决模式塌陷问题，通过多个检测器对一个生成器进行训练，同时将检测器与生成器分离，以便在社交媒体上进行检测和数据增强。</li>
<li>results: 相比之前的状态艺技术，我们的方法在分类精度方面表现出色，并且示出了使用生成器来逃脱这种分类技术的可能性。<details>
<summary>Abstract</summary>
Bot activity on social media platforms is a pervasive problem, undermining the credibility of online discourse and potentially leading to cybercrime. We propose an approach to bot detection using Generative Adversarial Networks (GAN). We discuss how we overcome the issue of mode collapse by utilizing multiple discriminators to train against one generator, while decoupling the discriminator to perform social media bot detection and utilizing the generator for data augmentation. In terms of classification accuracy, our approach outperforms the state-of-the-art techniques in this field. We also show how the generator in the GAN can be used to evade such a classification technique.
</details>
<details>
<summary>摘要</summary>
社交媒体平台上的机器人活动是一个普遍的问题，对在线讨论的准确性产生负面影响，并可能导致网络犯罪。我们提出了基于生成对抗网络（GAN）的机器人检测方法。我们解决了模式塌陷的问题，通过多个检测器来训练一个生成器，并将检测器与生成器分离，以进行社交媒体机器人检测和数据增强。在分类精度方面，我们的方法超越了当前领域的状态艺术技术。我们还展示了如何使用生成器在GAN中逃脱这种分类技术。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.LG_2023_11_09/" data-id="clot2mhg700ssx788gtdecc5j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/eess.SP_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T08:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/09/eess.SP_2023_11_09/">eess.SP - 2023-11-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Uncertainty-Aware-Bayes’-Rule-and-Its-Applications"><a href="#Uncertainty-Aware-Bayes’-Rule-and-Its-Applications" class="headerlink" title="Uncertainty-Aware Bayes’ Rule and Its Applications"></a>Uncertainty-Aware Bayes’ Rule and Its Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05532">http://arxiv.org/abs/2311.05532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uvhw/Bitcoin-Foundation">https://github.com/uvhw/Bitcoin-Foundation</a></li>
<li>paper_authors: Shixiong Wang</li>
<li>For: 本研究旨在探讨bayes规则在含模型偏差和&#x2F;或数据分布偏差时的应用问题。* Methods: 本文提出了一种通用的bayes规则改进方法，即不确定性感知bayes规则，以技术实现哲学上的平衡，即在计算后验分布时，如果先验分布和&#x2F;或数据分布过于保守，就需要增加先验信仰（或数据证据）；如果先验分布和&#x2F;或数据分布过于机会主义，就需要减少先验信仰（或数据证据）。* Results: 实验结果表明，对于含模型偏差和&#x2F;或数据分布偏差的情况，使用不确定性感知bayes规则可以超过传统bayes规则的性能。具体来说，提出了不确定性感知kalman滤波器、不确定性感知 particle滤波器和不确定性感知互动多模型滤波器，并进行了验证。<details>
<summary>Abstract</summary>
Bayes' rule has enabled innumerable powerful algorithms of statistical signal processing and statistical machine learning. However, when there exist model misspecifications in prior distributions and/or data distributions, the direct application of Bayes' rule is questionable. Philosophically, the key is to balance the relative importance of prior and data distributions when calculating posterior distributions: if prior (resp. data) distributions are overly conservative, we should upweight the prior belief (resp. data evidence); if prior (resp. data) distributions are overly opportunistic, we should downweight the prior belief (resp. data evidence). This paper derives a generalized Bayes' rule, called uncertainty-aware Bayes' rule, to technically realize the above philosophy, i.e., to combat the model uncertainties in prior distributions and/or data distributions. Simulated and real-world experiments showcase the superiority of the presented uncertainty-aware Bayes' rule over the conventional Bayes' rule: In particular, the uncertainty-aware Kalman filter, the uncertainty-aware particle filter, and the uncertainty-aware interactive multiple model filter are suggested and validated.
</details>
<details>
<summary>摘要</summary>
bayes的规则已经带来无数个强大的统计信号处理和统计机器学习算法。然而，当存在模型偏差在先后分布和/或数据分布时，直接采用bayes的规则是有很大的问题。哲学上，关键是在计算后期分布时平衡先后分布和数据分布的重要性：如果先前分布（resp.数据分布）太保守，我们应该增加先前信念（resp.数据证据）的重要性；如果先前分布（resp.数据分布）太机会主义，我们应该减少先前信念（resp.数据证据）的重要性。这篇论文提出了一种通用的bayes规则，called不确定性意识的bayes规则，以实现以上哲学。具体来说，这种规则可以在模型不确定性和/或数据不确定性情况下减少模型不确定性和数据不确定性的影响。在模拟和实际实验中，提出的不确定性意识bayes规则比普通的bayes规则更加有利，特别是uncertainty-aware kalman滤波器、uncertainty-aware particle滤波器和uncertainty-aware交互多模型滤波器都得到了验证。
</details></li>
</ul>
<hr>
<h2 id="EEG-DG-A-Multi-Source-Domain-Generalization-Framework-for-Motor-Imagery-EEG-Classification"><a href="#EEG-DG-A-Multi-Source-Domain-Generalization-Framework-for-Motor-Imagery-EEG-Classification" class="headerlink" title="EEG-DG: A Multi-Source Domain Generalization Framework for Motor Imagery EEG Classification"></a>EEG-DG: A Multi-Source Domain Generalization Framework for Motor Imagery EEG Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05415">http://arxiv.org/abs/2311.05415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao-Cong Zhong, Qisong Wang, Dan Liu, Zhihuang Chen, Jing-Xiao Liao, Jinwei Sun, Yudong Zhang, Feng-Lei Fan</li>
<li>for: 这个研究旨在提高非侵入式脑-电脑交互（BCI）中的电子脑讯号分类，并解决非站域性和个人差异对电子脑讯号分类造成的影响。</li>
<li>methods: 这篇研究提出了一个新的多源预分布框架（EEG-DG），利用多个来源预分布来建立一个普遍可行的分类模型，并且将多个来源预分布调整为一个内在共声分布，以实现预分布共声的稳定性和可重建性。</li>
<li>results: 系统实验和BCI竞赛数据集IV-2a和IV-2b的结果显示，EEG-DG方法比前一代方法更高，具体的准确率&#x2F;κ值为81.79%&#x2F;0.7572和87.12%&#x2F;0.7424。此外，EEG-DG方法甚至超过了一些领域对应方法。<details>
<summary>Abstract</summary>
Motor imagery EEG classification plays a crucial role in non-invasive Brain-Computer Interface (BCI) research. However, the classification is affected by the non-stationarity and individual variations of EEG signals. Simply pooling EEG data with different statistical distributions to train a classification model can severely degrade the generalization performance. To address this issue, the existing methods primarily focus on domain adaptation, which requires access to the target data during training. This is unrealistic in many EEG application scenarios. In this paper, we propose a novel multi-source domain generalization framework called EEG-DG, which leverages multiple source domains with different statistical distributions to build generalizable models on unseen target EEG data. We optimize both the marginal and conditional distributions to ensure the stability of the joint distribution across source domains and extend it to a multi-source domain generalization framework to achieve domain-invariant feature representation, thereby alleviating calibration efforts. Systematic experiments on a simulative dataset and BCI competition datasets IV-2a and IV-2b demonstrate the superiority of our proposed EEG-DG over state-of-the-art methods. Specifically, EEG-DG achieves an average classification accuracy/kappa value of 81.79%/0.7572 and 87.12%/0.7424 on datasets IV-2a and IV-2b, respectively, which even outperforms some domain adaptation methods. Our code is available at https://github.com/XC-ZhongHIT/EEG-DG for free download and evaluation.
</details>
<details>
<summary>摘要</summary>
motor imagery EEG 分类在非侵入式大脑-计算机界面（BCI）研究中扮演着关键角色。然而，分类被影响了EEG信号的不站立性和个体差异。简单地将EEG数据Pooling with different statistical distributions to train a classification model can severely degrade the generalization performance. To address this issue, existing methods primarily focus on domain adaptation, which requires access to the target data during training. This is unrealistic in many EEG application scenarios. In this paper, we propose a novel multi-source domain generalization framework called EEG-DG, which leverages multiple source domains with different statistical distributions to build generalizable models on unseen target EEG data. We optimize both the marginal and conditional distributions to ensure the stability of the joint distribution across source domains and extend it to a multi-source domain generalization framework to achieve domain-invariant feature representation, thereby alleviating calibration efforts. Systematic experiments on a simulative dataset and BCI competition datasets IV-2a and IV-2b demonstrate the superiority of our proposed EEG-DG over state-of-the-art methods. Specifically, EEG-DG achieves an average classification accuracy/κ value of 81.79%/0.7572 and 87.12%/0.7424 on datasets IV-2a and IV-2b, respectively, which even outperforms some domain adaptation methods. Our code is available at https://github.com/XC-ZhongHIT/EEG-DG for free download and evaluation.
</details></li>
</ul>
<hr>
<h2 id="Joint-Angle-and-Delay-Cramer-Rao-Bound-Optimization-for-Integrated-Sensing-and-Communications"><a href="#Joint-Angle-and-Delay-Cramer-Rao-Bound-Optimization-for-Integrated-Sensing-and-Communications" class="headerlink" title="Joint Angle and Delay Cramér-Rao Bound Optimization for Integrated Sensing and Communications"></a>Joint Angle and Delay Cramér-Rao Bound Optimization for Integrated Sensing and Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05372">http://arxiv.org/abs/2311.05372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Hu, Yuan Fang, Ling Qiu</li>
<li>for: 这个论文研究了一种多输入多输出（MIMO）扩展 beamforming 设计，用于一个集成感知通信（ISAC）系统中，该系统中的 ISAC 基站（BS）可以同时与多个下降用户通信，同时 reuse 通信信号进行多个目标的感知。</li>
<li>methods: 作者首先 derive 了目标角和延迟的 Cramér-Rao bound（CRB），然后使用 transmit beamforming 在 BS 中最小化 CRB，Subject to 通信率和功率约束。在特定情况下，作者获得了closed-form的优化解决方案，并证明了多目标场景中的优化解决方案具有简洁性，从而降低了优化的计算复杂性。</li>
<li>results: 数值结果表明，优化后的 beamforming 可以提供出色的定位性能，并有效减少了基站antenna的数量需求。<details>
<summary>Abstract</summary>
In this paper, we study a multi-input multi-output (MIMO) beamforming design in an integrated sensing and communication (ISAC) system, in which an ISAC base station (BS) is used to communicate with multiple downlink users and simultaneously the communication signals are reused for sensing multiple targets. Our interested sensing parameters are the angle and delay information of the targets, which can be used to locate these targets. Under this consideration, we first derive the Cram\'{e}r-Rao bound (CRB) for angle and delay estimation. Then, we optimize the transmit beamforming at the BS to minimize the CRB, subject to communication rate and power constraints. In particular, we obtain the optimal solution in closed-form in the case of single-target and single-user, and in the case of multi-target and multi-user scenario, the sparsity of the optimal solution is proven, leading to a reduction in computational complexity during optimization. The numerical results demonstrate that the optimized beamforming yields excellent positioning performance and effectively reduces the requirement for a large number of antennas at the BS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一个多输入多输出（MIMO）扩 beamforming 设计在一个集成感知通信（ISAC）系统中，其中一个 ISAC 基站（BS）用于与多个下降用户进行通信，同时通信信号被重复用于感知多个目标。我们的关注的感知参数是目标的角度和延迟信息，可以用于确定这些目标的位置。在这种情况下，我们首先 deriv 了 Cramér-Rao  bound（CRB） для角度和延迟估计。然后，我们优化了 bs 发射的扩 beamforming，以最小化 CRB，保持通信率和功率限制。具体来说，我们在单Target和单用户情况下获得了封闭式的优化解，而在多Target和多用户情况下，我们证明了优化解的稀疏性，从而降低了优化过程中的计算复杂性。数值结果表明，优化后的扩 beamforming 具有出色的定位性能，并有效地减少了 bs 需要的antenna数量。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Analog-Beamforming-for-RF-WET-with-Charging-Time-Constraint"><a href="#Energy-Efficient-Analog-Beamforming-for-RF-WET-with-Charging-Time-Constraint" class="headerlink" title="Energy-Efficient Analog Beamforming for RF-WET with Charging Time Constraint"></a>Energy-Efficient Analog Beamforming for RF-WET with Charging Time Constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05325">http://arxiv.org/abs/2311.05325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osmel Martínez Rosabal, Onel L. Alcaraz López, Hirley Alves</li>
<li>for: 这研究旨在提出一种时分多播方案，以实现低功率设备的高效充电，以推动互联网 Things（IoT）可持续发展。</li>
<li>methods: 该研究使用了多天线能量扫描器（PB），通过能量扫描来驱动设备的能量回收电路，达到最高能量转换效率点，从而实现最小的能量消耗。研究采用了分析多天线架构，因为它具有低复杂度、成本和能量消耗。</li>
<li>results: 研究结果表明，与参考方案相比，时分多播策略可以更高效地充电低功率设备。此外，随着PB天线数量的增加，性能也随之提高。<details>
<summary>Abstract</summary>
Internet of Things (IoT) sustainability may hinge on radio frequency wireless energy transfer (RF-WET). However, energy-efficient charging strategies are still needed, motivating our work. Specifically, this letter proposes a time division scheme to efficiently charge low-power devices in an IoT network. For this, a multi-antenna power beacon (PB) drives the devices' energy harvesting circuit to the highest power conversion efficiency point via energy beamforming, thus achieving minimum energy consumption. Herein, we adopt the analog multi-antenna architecture due to its low complexity, cost, and energy consumption. The proposal includes a simple yet accurate model for the transfer characteristic of the energy harvesting circuit, enabling the optimization framework. The results evince the effectiveness of our RF-WET strategy over a benchmark scheme where the PB charges all the IoT devices simultaneously. Furthermore, the performance increases with the number of PB antennas.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT）可持续性可能会受到无线频率无线能量传输（RF-WET）的限制。然而，仍需要能效的充电策略，这么我们的工作就有了动机。本信函中提出了一个时分多标的方案，以高效地充电低功率设备在IoT网络中。在这个方案中，一个多天线能量扩散器（PB）驱动设备的能量回收芯片至最高的能量转换效率点，以取得最少的能源消耗。我们采用了分析多天线架构，因为它具有低的复杂度、成本和能源消耗。我们还提出了一个简单又精准的转换特性模型，以便优化框架。结果显示了我们的RF-WET策略比对 benchmark 方案（PB同时充电所有IoT设备）更有效果。此外，性能随着PB天线数量增加。
</details></li>
</ul>
<hr>
<h2 id="Empowering-high-dimensional-optical-fiber-communications-with-integrated-photonic-processors"><a href="#Empowering-high-dimensional-optical-fiber-communications-with-integrated-photonic-processors" class="headerlink" title="Empowering high-dimensional optical fiber communications with integrated photonic processors"></a>Empowering high-dimensional optical fiber communications with integrated photonic processors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05282">http://arxiv.org/abs/2311.05282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaihang Lu, Zengqi Chen, Hao Chen, Wu Zhou, Zunyue Zhang, Hon Ki Tsang, Yeyu Tong</li>
<li>for: 这篇论文旨在探讨一种高维光纤通信系统，使用可重新配置的光子学处理器实现多通道通信。</li>
<li>methods: 该系统使用了光纤中的多模杂化，并使用激光器和光学元件实现多模式传输和解scrambling。</li>
<li>results: 实验结果表明，该系统可以实现高速和跨板通信，并且能够高效地处理Random Mode Scrambling和极化旋转。<details>
<summary>Abstract</summary>
Mode division multiplexing (MDM) in optical fibers enables multichannel capabilities for various applications, including data transmission, quantum networks, imaging, and sensing. However, MDM optical fiber systems, usually necessities bulk-optics approaches for launching different orthogonal fiber modes into the multimode optical fiber, and multiple-input multiple-output digital electronic signal processing at the receiver side to undo the arbitrary mode scrambling in a circular-core optical fiber. Here we show that a high-dimensional optical fiber communication system can be entirely implemented by a reconfigurable integrated photonic processor, featuring kernels of multichannel mode multiplexing transmitter and all-optical descrambling receiver. High-speed and inter-chip communications involving six spatial- and polarization modes have been experimentally demonstrated with high efficiency and high-quality eye diagrams, despite the presence of random mode scrambling and polarization rotation in a circular-core few-mode fiber. The proposed photonic integration approach holds promising prospects for future space-division multiplexing applications.
</details>
<details>
<summary>摘要</summary>
Mode division multiplexing (MDM) in optical fibers enables multichannel capabilities for various applications, including data transmission, quantum networks, imaging, and sensing. However, MDM optical fiber systems usually require bulk-optics approaches for launching different orthogonal fiber modes into the multimode optical fiber, and multiple-input multiple-output digital electronic signal processing at the receiver side to undo the arbitrary mode scrambling in a circular-core optical fiber. Here we show that a high-dimensional optical fiber communication system can be entirely implemented by a reconfigurable integrated photonic processor, featuring kernels of multichannel mode multiplexing transmitter and all-optical descrambling receiver. High-speed and inter-chip communications involving six spatial- and polarization modes have been experimentally demonstrated with high efficiency and high-quality eye diagrams, despite the presence of random mode scrambling and polarization rotation in a circular-core few-mode fiber. The proposed photonic integration approach holds promising prospects for future space-division multiplexing applications.Translation notes:* "mode division multiplexing" (MDM) is translated as "光纤多谱多路复用" (guāngsuān duōshì duōlù fùròng)* "multichannel" is translated as "多频道" (duō fèngdào)* "bulk-optics" is translated as "干涉光学" (gānchāng guāngxì)* "arbitrary mode scrambling" is translated as "随机模式扰乱" (suījī móxìiàng)* "circular-core optical fiber" is translated as "圆核光纤" (kuànghép guāngsuān)* "high-dimensional" is translated as "高维" (gāo wéi)* "photonic integration" is translated as "光学集成" (guāngxì jíshēng)* "high-speed" is translated as "高速" (gāosù)* "inter-chip" is translated as " между芯" (jiān shēn)* "polarization rotation" is translated as "旋转 polarization" (xuān zhòu bō liáng)
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Recognition-and-Classification-of-Jamming-Signal-via-CGAN-Based-Fusion-CNN-Algorithm"><a href="#Few-Shot-Recognition-and-Classification-of-Jamming-Signal-via-CGAN-Based-Fusion-CNN-Algorithm" class="headerlink" title="Few-Shot Recognition and Classification of Jamming Signal via CGAN-Based Fusion CNN Algorithm"></a>Few-Shot Recognition and Classification of Jamming Signal via CGAN-Based Fusion CNN Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05273">http://arxiv.org/abs/2311.05273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhui Ding, Yue Zhang, Gaoyang Li, Neng Ye, Yuting Guo, Takuya Mabuchi, Hitomi Anzai, Kai Yang</li>
<li>for: 这篇论文的目的是提出一种基于条件生成对抗网络（CGAN）和卷积神经网络（CNN）的融合算法，以解决实际通信系统中对深度学习（DL）算法的应用困难，尤其是对于快速变化的干扰信号。</li>
<li>methods: 这篇论文使用了一种基于CGAN和CNN的融合算法，该算法能够对干扰信号进行精确的分类。</li>
<li>results: 该论文的实验结果显示，该算法能够在实际通信系统中获得8%的改善率，并且在资料量有限的情况下表现更好。此外，该论文还使用了实际的卫星通信实验设备来模拟实际的通信enario，并且验证了其算法的可靠性。<details>
<summary>Abstract</summary>
The precise classification of jamming signals holds paramount significance in the effective implementation of anti-jamming strategies within communication systems subject to intricate environmental variables. In light of this imperative, we propose an innovative fusion algorithm based on conditional generative adversarial network (CGAN) and convolutional neural network (CNN) to solve the problem of difficulty in applying deep learning (DL) algorithms due to the instantaneous nature of jamming signals in practical communication systems. Compared with previous methods, our algorithm achieved an 8% improvement in accuracy even when working with a limited dataset. Unlike previous research, we have simulated real-world satellite communication scenarios using a hardware platform and validated our algorithm using the resulting time-domain waveform data. The experimental results indicate that our algorithm still performs extremely well, which demonstrates significant potential for practical application in real-world communication scenarios.
</details>
<details>
<summary>摘要</summary>
“精确的干扰信号分类持续至关重要在实现有效反干扰策略中，特别是在面临复杂环境变量的情况下。为此，我们提出了一个新的融合算法，基于条件生成问题网络（CGAN）和卷积神经网络（CNN），以解决实际通信系统中深度学习（DL）算法对干扰信号的应用困难。与前一代方法相比，我们的算法在有限数据集上获得了8%的提升精度。不同于前一些研究，我们使用实验室硬件平台模拟了实际卫星通信场景，并使用时域波形数据进行验证。实验结果显示，我们的算法在实际场景中仍然表现出色，这表明它具有实际应用潜力。”
</details></li>
</ul>
<hr>
<h2 id="Delay-Doppler-Transform"><a href="#Delay-Doppler-Transform" class="headerlink" title="Delay Doppler Transform"></a>Delay Doppler Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05236">http://arxiv.org/abs/2311.05236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang-Gen Xia</li>
<li>for: 这篇论文是为了介绍延迟Doppler变换（DDT）在时域信号中的应用。</li>
<li>methods: 论文使用了DDT来描述时域信号中的延迟和Doppler扩散。</li>
<li>results: DDT可以提供关于延迟Doppler通道的新的理解和研究方向。I hope this helps!<details>
<summary>Abstract</summary>
This letter is to introduce delay Doppler transform (DDT) for a time domain signal. It is motivated by the recent studies in wireless communications over delay Doppler channels that have both time and Doppler spreads, such as, satellite communication channels. We present some simple properties of DDT as well. The DDT study may provide insights of delay Doppler channels.
</details>
<details>
<summary>摘要</summary>
这封信是为引入延迟Doppler变换（DDT），用于时域信号。这是因为最近的无线通信研究表明，在具有时间和Doppler扩散的延迟Doppler通道上进行通信，具有重要的意义。我们还介绍了一些简单的DDT性质。研究DDT可能会为延迟Doppler通道提供新的意义。Here's the breakdown of the translation:* 这封信 (zhè fāng xìn) - This letter* 是 (shì) - Is* 为 (wèi) - For* 引入 (yǐn rù) - Introduce* 延迟Doppler变换 (diān zhì Doppler biàn huà) - Delay Doppler transform* 用于 (yòng yòu) - Used for* 时域信号 (shí jiāo xìn hao) - Time domain signal* 最近 (zuì jìn) - Recent* 无线通信研究 (wù xiào tōng xìn yán jí) - Wireless communication research* 表明 (biǎo míng) - Show* 在 (zài) - In* 具有 (gù yǒu) - Have* 时间和Doppler扩散 (shí jì yǔ Doppler kuò chǎng) - Time and Doppler spreads* 延迟Doppler通道 (diān zhì Doppler tōng dào) - Delay Doppler channels* 上 (shàng) - On* 进行 (jìn háng) - To perform* 通信 (tōng xìn) - Communication* 具有重要的意义 (gù yǒu zhòng yào de yì yì) - Have important significance* 研究 (yán jí) - Research* DDT (DDT) - DDT* 可能 (kě néng) - May* 为延迟Doppler通道提供新的意义 (wèi diān zhì Doppler tōng dào tī fang xīn de yì yì) - Provide new significance for delay Doppler channels.
</details></li>
</ul>
<hr>
<h2 id="Coverage-and-Rate-Analysis-for-Cell-Free-LEO-Satellite-Networks"><a href="#Coverage-and-Rate-Analysis-for-Cell-Free-LEO-Satellite-Networks" class="headerlink" title="Coverage and Rate Analysis for Cell-Free LEO Satellite Networks"></a>Coverage and Rate Analysis for Cell-Free LEO Satellite Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05189">http://arxiv.org/abs/2311.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Li, Bodong Shang, Na Deng, Shanzhi Chen</li>
<li>for: 本研究旨在 investigate 6G 下的 Low-earth orbit (LEO) 卫星通信技术，以提高用户的质量服务 (QoS)。</li>
<li>methods: 本研究采用了系统水平的 architecture 方法，并使用了多卫星支持的 cell-free (CF) LEO 卫星网络。</li>
<li>results: 对于一个典型用户，CFLS 网络可以获得更高的覆盖率，而且通过选择合适的服务卫星，可以最大化用户的均衡速率。<details>
<summary>Abstract</summary>
Low-earth orbit (LEO) satellite communication is one of the enabling key technologies in next-generation (6G) networks. However, single satellite-supported downlink communication may not meet user's needs due to limited signal strength, especially in emergent scenarios. In this letter, we investigate an architecture of cell-free (CF) LEO satellite (CFLS) networks from a system-level perspective, where a user can be served by multiple satellites to improve its quality-of-service (QoS). Furthermore, we analyze the coverage and rate of a typical user in the CFLS network. Simulation and numerical results show that the CFLS network achieves a higher coverage probability than the traditional single satellite-supported network. Moreover, user's ergodic rate is maximized by selecting an appropriate number of serving satellites.
</details>
<details>
<summary>摘要</summary>
低地轨道卫星通信是6G网络中一种关键技术。然而，单个卫星支持下链路通信可能无法满足用户需求，尤其是在紧急情况下。在这封信中，我们从系统水平出发，研究了无终端（CF）低地轨道卫星网络的架构，以提高用户的质量服务（QoS）。此外，我们分析了CFLS网络中用户的覆盖率和速率。数据 simulate 和数值结果表明，CFLS网络可以在覆盖率和速率方面提供更高的性能，而且可以通过选择合适的服务卫星来最大化用户的均衡速率。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communication-for-Network-Assisted-Full-Duplex-Cell-Free-Distributed-Massive-MIMO-Systems"><a href="#Integrated-Sensing-and-Communication-for-Network-Assisted-Full-Duplex-Cell-Free-Distributed-Massive-MIMO-Systems" class="headerlink" title="Integrated Sensing and Communication for Network-Assisted Full-Duplex Cell-Free Distributed Massive MIMO Systems"></a>Integrated Sensing and Communication for Network-Assisted Full-Duplex Cell-Free Distributed Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05101">http://arxiv.org/abs/2311.05101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zeng, Jingxuan Yu, Jiamin Li, Feiyang Liu, Dongming Wang, Xiaohu You</li>
<li>for: 本研究旨在实现 integrate sensing and communication (ISAC) 系统，combine network-assisted full-duplex (NAFD) 技术和分布式雷达探测。</li>
<li>methods: 本研究使用 uplink 和 downlink 远程广播单元 (RRUs) 具有通信和探测能力，评估系统的通信和探测性能。</li>
<li>results: 比较结果表明，提出的方案可以提供更稳定的探测和更好的通信性能，并提出了基于 deep Q-网络 (DQN) 和非统治遗传遗传算法 II (NSGA-II) 的两种优化算法来同时优化通信和探测性能。<details>
<summary>Abstract</summary>
In this paper, we combine the network-assisted full-duplex (NAFD) technology and distributed radar sensing to implement integrated sensing and communication (ISAC). The ISAC system features both uplink and downlink remote radio units (RRUs) equipped with communication and sensing capabilities. We evaluate the communication and sensing performance of the system using the sum communication rates and the Cramer-Rao lower bound (CRLB), respectively. We compare the performance of the proposed scheme with other ISAC schemes, the result shows that the proposed scheme can provide more stable sensing and better communication performance. Furthermore, we propose two power allocation algorithms to optimize the communication and sensing performance jointly. One algorithm is based on the deep Q-network (DQN) and the other one is based on the non-dominated sorting genetic algorithm II (NSGA-II). The proposed algorithms provide more feasible solutions and achieve better system performance than the equal power allocation algorithm.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们将网络协助全双工（NAFD）技术和分布式雷达探测结合，实现集成探测通信（ISAC）系统。该系统具有上行和下行远程无线单元（RRU），具有通信和探测能力。我们使用总通信速率和克劳德-罗素下界（CRLB）来评估系统的通信和探测性能。与其他ISAC方案相比，我们的方案可以提供更稳定的探测和更好的通信性能。此外，我们提出了两种功率分配算法，以便同时优化通信和探测性能。一种是基于深度Q网络（DQN），另一种是基于非占据 sorting genetic algorithm II（NSGA-II）。这两种算法可以提供更实际的解决方案，并且可以更好地调整系统性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/eess.SP_2023_11_09/" data-id="clot2mho801dux788gy9c4mm2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/eess.AS_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T14:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/eess.AS_2023_11_08/">eess.AS - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="1-step-Speech-Processing-and-Understanding-Using-CTC-Loss"><a href="#1-step-Speech-Processing-and-Understanding-Using-CTC-Loss" class="headerlink" title="1-step Speech Processing and Understanding Using CTC Loss"></a>1-step Speech Processing and Understanding Using CTC Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04753">http://arxiv.org/abs/2311.04753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Singla, Shahab Jalavand, Yeon-Jun Kim, Antonio Moreno Daniel, Srinivas Bangalore, Andrej Ljolje, Ben Stern</li>
<li>for: 提高自然语言处理系统的命名实体识别和意图识别能力</li>
<li>methods: 使用Connectionist Temporal Classification（CTC）损失进行端到端语音识别编码器的优化，并添加了一组未使用的占位符号来扩展自动语音识别系统的词汇</li>
<li>results: 在SLUE benchmark上实现了明显的命名实体标记、意图识别和译文准确率提高，并且与SLURP数据集的结果相当Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the ability of natural language processing systems to recognize named entities and intent in speech.</li>
<li>methods: The authors propose a solution that extends the vocabulary of the end-to-end automatic speech recognition (ASR) system by adding a set of unused placeholder symbols, which are then assigned to represent semantic tags. These placeholders are integrated into the transcription process as distinct tokens.</li>
<li>results: The proposed solution achieves notable improvements in entity tagging, intent discernment, and transcription accuracy on the SLUE benchmark, and the results are on par with those for the SLURP dataset. Additionally, the authors provide a visual analysis of the system’s proficiency in accurately pinpointing meaningful tokens over time, illustrating the enhancement in transcription quality through the utilization of supplementary semantic tags.<details>
<summary>Abstract</summary>
Recent studies have made some progress in refining end-to-end (E2E) speech recognition encoders by applying Connectionist Temporal Classification (CTC) loss to enhance named entity recognition within transcriptions. However, these methods have been constrained by their exclusive use of the ASCII character set, allowing only a limited array of semantic labels. Our proposed solution extends the E2E automatic speech recognition (ASR) system's vocabulary by adding a set of unused placeholder symbols, conceptually akin to the <pad> tokens used in sequence modeling. These placeholders are then assigned to represent semantic tags and are integrated into the transcription process as distinct tokens. We demonstrate notable improvements in entity tagging, intent discernment, and transcription accuracy on the SLUE benchmark and yields results that are on par with those for the SLURP dataset. Additionally, we provide a visual analysis of the system's proficiency in accurately pinpointing meaningful tokens over time, illustrating the enhancement in transcription quality through the utilization of supplementary semantic tags.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:最近的研究已经做出了一些进步，用Connectionist Temporal Classification（CTC）损失来提高名称识别 within 转录。然而，这些方法受限于它们仅使用 ASCII 字符集，只能处理有限数量的 semantic label。我们的提议的解决方案是将 E2E 自动语音识别（ASR）系统的词汇表扩展到添加一组未使用的 placeholder symbol，类似于 sequence modeling 中的 <pad> token。这些 placeholder 然后被分配到表示 semantic tag 的各种符号，并被 integrating 到转录过程中作为特定的 tokens。我们在 SLUE 标准集上示出了明显的提高，包括实体标记、意图识别和转录精度。此外，我们还提供了一种可视化分析，表明系统在时间上准确地标记了意义上的 token， thereby illustrating the enhancement in transcription quality through the use of supplementary semantic tags。
</details></li>
</ul>
<hr>
<h2 id="Selective-HuBERT-Self-Supervised-Pre-Training-for-Target-Speaker-in-Clean-and-Mixture-Speech"><a href="#Selective-HuBERT-Self-Supervised-Pre-Training-for-Target-Speaker-in-Clean-and-Mixture-Speech" class="headerlink" title="Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech"></a>Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04526">http://arxiv.org/abs/2311.04526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingru Lin, Meng Ge, Wupeng Wang, Haizhou Li, Mengling Feng</li>
<li>for: 这篇论文的目的是提出一种新的自我超vision演示模型，以实现选择性的对话声音抽象，并且能够在各种声音处理任务中提供高性能。</li>
<li>methods: 这篇论文使用了一种新的预训练方法，称为Selective-HuBERT（SHuBERT），它通过预测目标说话者的pseudo标签，并且使用了双路训练策略和跨相关约束，以实现选择性地对声音进行抽象。</li>
<li>results: 实验结果显示，SHuBERT可以在SUPERB评量标准和LibriMix数据集上达到高性能，并且能够在实际应用中提供高质量的声音抽象，甚至在具有极低量的标签资料下进行优化。<details>
<summary>Abstract</summary>
Self-supervised pre-trained speech models were shown effective for various downstream speech processing tasks. Since they are mainly pre-trained to map input speech to pseudo-labels, the resulting representations are only effective for the type of pre-train data used, either clean or mixture speech. With the idea of selective auditory attention, we propose a novel pre-training solution called Selective-HuBERT, or SHuBERT, which learns the selective extraction of target speech representations from either clean or mixture speech. Specifically, SHuBERT is trained to predict pseudo labels of a target speaker, conditioned on an enrolled speech from the target speaker. By doing so, SHuBERT is expected to selectively attend to the target speaker in a complex acoustic environment, thus benefiting various downstream tasks. We further introduce a dual-path training strategy and use the cross-correlation constraint between the two branches to encourage the model to generate noise-invariant representation. Experiments on SUPERB benchmark and LibriMix dataset demonstrate the universality and noise-robustness of SHuBERT. Furthermore, we find that our high-quality representation can be easily integrated with conventional supervised learning methods to achieve significant performance, even under extremely low-resource labeled data.
</details>
<details>
<summary>摘要</summary>
自适应预训练语音模型在各种下游语音处理任务中显示出效iveness。由于它们主要预训练为将输入语音映射到 Pseudo-labels，因此生成的表示只有效果于使用的预训练数据类型，可能是干净的语音或混合语音。我们提出了一种新的预训练解决方案 called Selective-HuBERT（SHuBERT），它学习选择提取目标语音表示。特别是，SHuBERT 在预训练时预测目标说话人的 Pseudo-labels，条件在报名说话人的语音上。通过这样做，SHuBERT 可以选择性地听到目标说话人在复杂的声学环境中，从而利于各种下游任务。我们还提出了一种 dual-path 训练策略，并使用两个分支之间的协方差约束来鼓励模型生成难以干扰的表示。在 SUPERB benchmark 和 LibriMix 数据集上进行了实验， demonstrably 表明 SHuBERT 的 universality 和 noise-robustness。此外，我们发现我们的高质量表示可以轻松地与传统的监督学习方法结合使用，即使受到极低资源的标注数据。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/eess.AS_2023_11_08/" data-id="clot2mhjw0134x788h6bj7nn5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.CV_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T13:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.CV_2023_11_08/">cs.CV - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Transfer-Learning-for-Efficient-Video-Specific-Human-Pose-Estimation"><a href="#Active-Transfer-Learning-for-Efficient-Video-Specific-Human-Pose-Estimation" class="headerlink" title="Active Transfer Learning for Efficient Video-Specific Human Pose Estimation"></a>Active Transfer Learning for Efficient Video-Specific Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05041">http://arxiv.org/abs/2311.05041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iminthemiddle/vatl4pose-wacv2024">https://github.com/iminthemiddle/vatl4pose-wacv2024</a></li>
<li>paper_authors: Hiromu Taketsugu, Norimichi Ukita</li>
<li>For: 这个论文的目的是提出一种基于活动学习和传输学习的方法，以有效地适应人体 pose 估计器到个人视频频谱中。* Methods: 该方法使用了热图的变化来衡量估计结果的不确定性，以及全身人体 pose 的不自然性来选择不同和不确定的样本进行有效的估计器学习。此外，该方法还重新评估了现有的活动转移学习方法，并提出了新的重新训练方法和停止 criterion。* Results: 实验结果表明，该方法可以提高学习效率，并在比较方法中得到更好的性能。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/ImIntheMiddle/VATL4Pose-WACV2024%E3%80%82">https://github.com/ImIntheMiddle/VATL4Pose-WACV2024。</a><details>
<summary>Abstract</summary>
Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024
</details>
<details>
<summary>摘要</summary>
人体姿势（HP）估计是正在活跃研究的领域，因为它有很多应用场景。然而，即使使用大量数据进行预训练，HP估计器也可能不能达到预期的性能，这是因为训练和测试数据之间的领域差异。为了解决这个问题，我们提出了一种结合活动学习（AL）和转移学习（TL）的方法，以高效地适应视频域中的HP估计器。为了高效学习，我们的方法量化了（i）估计热图中的时间变化引入的估计不确定性，以及（ii）全身HP估计中的不自然性。这些量化的 критери价然与当前最佳表达性 критериion（SC）有效地结合，以选择不确定和多样的样本，以便高效地学习HP估计器。此外，我们重新考虑了现有的活动转移学习（ATL）方法，并引入了新的重新训练方法和停止标准（SC）。实验结果表明，我们的方法可以提高学习效率，并比相对方法高效。我们的代码可以在 GitHub 上获取：https://github.com/ImIntheMiddle/VATL4Pose-WACV2024
</details></li>
</ul>
<hr>
<h2 id="S-3-AD-Semi-supervised-Small-Apple-Detection-in-Orchard-Environments"><a href="#S-3-AD-Semi-supervised-Small-Apple-Detection-in-Orchard-Environments" class="headerlink" title="S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments"></a>S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05029">http://arxiv.org/abs/2311.05029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Johanson, Christian Wilms, Ole Johannsen, Simone Frintrop</li>
<li>for: 本文 targets 苹果干部检测问题，为精准农业应用，如自动化受产量测量或水果摘取提供了解决方案。</li>
<li>methods: 本文提出了一种半监督的苹果检测方法，使用上下文注意力和选择平铺来改进小苹果的检测，同时减少计算成本。</li>
<li>results: 对于MAD数据集和MSU数据集的广泛评估表明，S$^3$AD系统与多个强大的完全监督基线系统进行比较，达到了最高的$14.9%$的提升。此外，通过利用数据集中苹果属性的详细注释，分析了不同系统对尺寸和干扰度的影响，量化了当前的挑战。<details>
<summary>Abstract</summary>
Crop detection is integral for precision agriculture applications such as automated yield estimation or fruit picking. However, crop detection, e.g., apple detection in orchard environments remains challenging due to a lack of large-scale datasets and the small relative size of the crops in the image. In this work, we address these challenges by reformulating the apple detection task in a semi-supervised manner. To this end, we provide the large, high-resolution dataset MAD comprising 105 labeled images with 14,667 annotated apple instances and 4,440 unlabeled images. Utilizing this dataset, we also propose a novel Semi-Supervised Small Apple Detection system S$^3$AD based on contextual attention and selective tiling to improve the challenging detection of small apples, while limiting the computational overhead. We conduct an extensive evaluation on MAD and the MSU dataset, showing that S$^3$AD substantially outperforms strong fully-supervised baselines, including several small object detection systems, by up to $14.9\%$. Additionally, we exploit the detailed annotations of our dataset w.r.t. apple properties to analyze the influence of relative size or level of occlusion on the results of various systems, quantifying current challenges.
</details>
<details>
<summary>摘要</summary>
减损检测是精准农业应用程序的关键组成部分，如自动化产量估计或果实摘取。然而，减损检测，例如苹果检测在园区环境中仍然是一个挑战，因为缺乏大规模数据集和图像中小型减损的相对比例。在这种情况下，我们通过半upervised的方式改进苹果检测任务。为此，我们提供了大型、高分辨率的数据集MAD，包含105个标注图像和14,667个注解apple实例，以及4,440个未标注图像。基于这个数据集，我们还提出了一种新的半upervised小 apple检测系统S$^3$AD，使用contextual attention和选择分割来改进小apple的检测，同时限制计算过程的开销。我们对MAD和MSU数据集进行了广泛的评估，显示S$^3$AD在比较减损的情况下，与强大的完全supervised基eline相比，提高了14.9%。此外，我们利用我们数据集中 apple 属性的详细注解，分析不同系统在不同的减损和 occlusion 情况下的结果，并对现有挑战进行了质量评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-a-realistic-synthetic-database-to-learn-Shape-from-Shading-for-estimating-the-colon-depth-in-colonoscopy-images"><a href="#Leveraging-a-realistic-synthetic-database-to-learn-Shape-from-Shading-for-estimating-the-colon-depth-in-colonoscopy-images" class="headerlink" title="Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images"></a>Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05021">http://arxiv.org/abs/2311.05021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josué Ruano, Martín Gómez, Eduardo Romero, Antoine Manzanera</li>
<li>for: 这个研究旨在提高单摄colonoscopy影像中的肠道深度估计，以帮助诊断肠及肛部癌症。</li>
<li>methods: 这个研究使用了一个新的方法来从单摄colonoscopy影像中估计肠道深度图。这个方法基于肠道墙的阴影变化，并由一个专门训练的单元神经网络估计。</li>
<li>results: 这个研究的结果显示了这个方法可以实现高精度的肠道深度估计，具有95.65%的阈值准确性和0.451cm的平均误差。此外，该方法在一些真实的影像中也显示了良好的成果。<details>
<summary>Abstract</summary>
Colonoscopy is the choice procedure to diagnose colon and rectum cancer, from early detection of small precancerous lesions (polyps), to confirmation of malign masses. However, the high variability of the organ appearance and the complex shape of both the colon wall and structures of interest make this exploration difficult. Learned visuospatial and perceptual abilities mitigate technical limitations in clinical practice by proper estimation of the intestinal depth. This work introduces a novel methodology to estimate colon depth maps in single frames from monocular colonoscopy videos. The generated depth map is inferred from the shading variation of the colon wall with respect to the light source, as learned from a realistic synthetic database. Briefly, a classic convolutional neural network architecture is trained from scratch to estimate the depth map, improving sharp depth estimations in haustral folds and polyps by a custom loss function that minimizes the estimation error in edges and curvatures. The network was trained by a custom synthetic colonoscopy database herein constructed and released, composed of 248,400 frames (47 videos), with depth annotations at the level of pixels. This collection comprehends 5 subsets of videos with progressively higher levels of visual complexity. Evaluation of the depth estimation with the synthetic database reached a threshold accuracy of 95.65%, and a mean-RMSE of 0.451 cm, while a qualitative assessment with a real database showed consistent depth estimations, visually evaluated by the expert gastroenterologist coauthoring this paper. Finally, the method achieved competitive performance with respect to another state-of-the-art method using a public synthetic database and comparable results in a set of images with other five state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
殖colonoscopy是诊断大肠和直肠癌的首选方法，从早期发现小前癌肿瘤（肿块）到确认肿瘤。然而，肠部的高变化性和肠壁的复杂形状使这种探测困难。通过了解视觉和感知能力，技术上的限制可以得到应用。这项工作介绍了一种新的方法，用单摄影探测肠部深度图。该图由肠壁阴影变化与光源的相对位置学习得到，并通过一个自定义的损失函数来改进锐利的深度估计。这种方法使用了自定义的 sintetic colonoscopy 数据库，包括 248,400 帧（47 个视频），其中每帧有像素级别的深度注解。这个收集包括 5 个视频subset，每个 subset 的视觉复杂度逐渐增加。对于这个数据库的评估，depth estimation 达到了95.65%的阈值精度和0.451 cm的平均欧姆误差，而且与真实数据库的评估显示了一致的深度估计。最终，该方法在与另一个状态略方法进行比较时达到了竞争性性能，并在一组图像中与其他五个状态略方法的结果相当。
</details></li>
</ul>
<hr>
<h2 id="Familiarity-Based-Open-Set-Recognition-Under-Adversarial-Attacks"><a href="#Familiarity-Based-Open-Set-Recognition-Under-Adversarial-Attacks" class="headerlink" title="Familiarity-Based Open-Set Recognition Under Adversarial Attacks"></a>Familiarity-Based Open-Set Recognition Under Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05006">http://arxiv.org/abs/2311.05006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel</li>
<li>for: 本研究旨在探讨 familiarscore-based open-set recognition 中的攻击问题，以及这些攻击的效果。</li>
<li>methods: 本研究使用了梯度导向的 adversarial 攻击方法，包括 False Familiarity 和 False Novelty 两种类型的攻击。</li>
<li>results: 研究发现，在 informed 和 uninformed 设置下，这些攻击都能够减少 familiarscore-based open-set recognition 的准确率。<details>
<summary>Abstract</summary>
Open-set recognition (OSR), the identification of novel categories, can be a critical component when deploying classification models in real-world applications. Recent work has shown that familiarity-based scoring rules such as the Maximum Softmax Probability (MSP) or the Maximum Logit Score (MLS) are strong baselines when the closed-set accuracy is high. However, one of the potential weaknesses of familiarity-based OSR are adversarial attacks. Here, we present gradient-based adversarial attacks on familiarity scores for both types of attacks, False Familiarity and False Novelty attacks, and evaluate their effectiveness in informed and uninformed settings on TinyImageNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effective-Restoration-of-Source-Knowledge-in-Continual-Test-Time-Adaptation"><a href="#Effective-Restoration-of-Source-Knowledge-in-Continual-Test-Time-Adaptation" class="headerlink" title="Effective Restoration of Source Knowledge in Continual Test Time Adaptation"></a>Effective Restoration of Source Knowledge in Continual Test Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04991">http://arxiv.org/abs/2311.04991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahim Faisal Niloy, Sk Miraj Ahmed, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury</li>
<li>for: 本文旨在解决测试时适应（TTA）方法在动态环境中的挑战，包括累累忘记之前学习的有价值源知识和逐渐增加的错误。</li>
<li>methods: 本文提出了一种无监督领域变换检测方法，可以在动态环境中检测领域变换并将模型参数还原到原始源预训练值。这种方法通过监测领域变换触发的统计变化来重新启用原始源的知识，从而 correction 模型参数的负面影响。</li>
<li>results: 对比州前方法，本文的方法在动态环境中表现出色，可以减少模型参数的负面影响和累累忘记。我们通过对多个 benchmark 数据集进行广泛的实验来证明本文的方法的优秀性。<details>
<summary>Abstract</summary>
Traditional test-time adaptation (TTA) methods face significant challenges in adapting to dynamic environments characterized by continuously changing long-term target distributions. These challenges primarily stem from two factors: catastrophic forgetting of previously learned valuable source knowledge and gradual error accumulation caused by miscalibrated pseudo labels. To address these issues, this paper introduces an unsupervised domain change detection method that is capable of identifying domain shifts in dynamic environments and subsequently resets the model parameters to the original source pre-trained values. By restoring the knowledge from the source, it effectively corrects the negative consequences arising from the gradual deterioration of model parameters caused by ongoing shifts in the domain. Our method involves progressive estimation of global batch-norm statistics specific to each domain, while keeping track of changes in the statistics triggered by domain shifts. Importantly, our method is agnostic to the specific adaptation technique employed and thus, can be incorporated to existing TTA methods to enhance their performance in dynamic environments. We perform extensive experiments on benchmark datasets to demonstrate the superior performance of our method compared to state-of-the-art adaptation methods.
</details>
<details>
<summary>摘要</summary>
传统的测试时适应（TTA）方法在面临不断变化的长期目标分布环境中遇到重大挑战。这些挑战主要来自两个因素： catastrophic forgetting 已经学习的源知识的价值，以及由于 pseudo 标签的误偏而导致的慢滑块误差的堆积。为解决这些问题，本文提出了一种无监督领域变化检测方法，能够在动态环境中检测领域的变化，并将模型参数重置回源预训练值。通过恢复源知识，它有效地纠正由持续变化的领域所导致的模型参数的负面影响。我们的方法包括逐步估计每个领域的全局批处理 нор 特征统计，并记录每个领域的变化触发的统计变化。与此同时，我们的方法是对现有 TTA 方法进行增强，不需要改变现有的适应技术。我们在标准的测试数据集上进行了广泛的实验，并证明了我们的方法在动态环境中的超过当今适应方法的表现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Inductive-Biases-in-Video-Modeling-through-Neural-CDEs"><a href="#Exploiting-Inductive-Biases-in-Video-Modeling-through-Neural-CDEs" class="headerlink" title="Exploiting Inductive Biases in Video Modeling through Neural CDEs"></a>Exploiting Inductive Biases in Video Modeling through Neural CDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04986">http://arxiv.org/abs/2311.04986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johnathan Chiu, Samuel Duffield, Max Hunter-Gordon, Kaelan Donatella, Max Aifer, Andi Gu</li>
<li>for: 这篇论文是用于Video Task中的Video interpolating和Mask Propagation问题。</li>
<li>methods: 本文提出了一种使用Controlled Differential Equations（CDEs）来解决Video Task中的Key challenges，包括Video interpolating和Mask Propagation。它将CDEs应用于不同的分辨率，实现了一个连续时间的U-Net架构。不同于传统方法，本文的方法不需要明确的Optical Flow学习，而是利用CDEs的自然连续时间特性来生成高度表达力的Video模型。</li>
<li>results: 本文展示了与State-of-the-art模型相比，本文的方法在Video interpolating和Mask Propagation任务中具有竞争性的性能。<details>
<summary>Abstract</summary>
We introduce a novel approach to video modeling that leverages controlled differential equations (CDEs) to address key challenges in video tasks, notably video interpolation and mask propagation. We apply CDEs at varying resolutions leading to a continuous-time U-Net architecture. Unlike traditional methods, our approach does not require explicit optical flow learning, and instead makes use of the inherent continuous-time features of CDEs to produce a highly expressive video model. We demonstrate competitive performance against state-of-the-art models for video interpolation and mask propagation tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的视频模型方法，利用控制的微分方程（CDE）解决视频任务中的关键挑战，包括视频 interpolate 和mask propagation。我们在不同的分辨率上应用CDE，导致一种连续时间的U-Net架构。与传统方法不同，我们的方法不需要显式学习流体力学，而是利用CDE的内在连续时间特征来生成一个非常表达力的视频模型。我们在视频 interpolate 和mask propagation任务中示出了与状态艺术模型的竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs"><a href="#GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs" class="headerlink" title="GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"></a>GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04901">http://arxiv.org/abs/2311.04901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan</li>
<li>for: 本研究目的是提出一种基于生长和重用模块的生成型神经符号逻辑视觉理解方法，以提高现有神经符号逻辑模型的效率和可重用性。</li>
<li>methods: 本方法包括三个独特的阶段：模块初始化、模块生成和模块执行。首先，给定一个视力语言任务，我们采用大语言模型来检查是否可以重用和增长已有的模块来解决这个新任务。如果不能，我们将创建一个新模块，并指定这个模块的输入和输出。然后，我们使用大语言模型来生成匹配要求的代码块，并将其添加到模块库中。为了更好地评估新模块的能力，我们将几个例子作为测试用例，并评估它们是否可以通过这些测试。如果可以，我们将新模块添加到模块库中，并在其他任务上进行重用。最后，我们使用执行生成的程序来评估模型的性能。</li>
<li>results: 我们的模型在标准任务如视觉问答和参考表达理解中表现竞争力强，同时模块学习自一个任务可以很好地转移到新任务上。此外，我们的模型可以通过几个例子的几个测试来适应新的视觉理解任务，而不需要大量的训练数据。<details>
<summary>Abstract</summary>
Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate language into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. We propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module's ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.
</details>
<details>
<summary>摘要</summary>
First, we use LLMs to determine if we can reuse or grow existing modules to handle a new task. If not, we initialize a new module with the task's inputs and outputs. Then, we use LLMs to generate code snippets that match the module's requirements. We treat a few training examples as test cases to evaluate the new module's ability. If the module passes the test cases, it is added to the module library for future reuse. Finally, we evaluate the model's performance on a testing set by executing the parsed programs with the newly made visual modules.Our proposed model has several advantages. First, it performs well on standard visual reasoning tasks like visual question answering and referring expression comprehension. Second, the modules learned from one task can be easily transferred to new tasks. Lastly, the model can adapt to new visual reasoning tasks by observing a few training examples and reusing modules.
</details></li>
</ul>
<hr>
<h2 id="Are-foundation-models-efficient-for-medical-image-segmentation"><a href="#Are-foundation-models-efficient-for-medical-image-segmentation" class="headerlink" title="Are foundation models efficient for medical image segmentation?"></a>Are foundation models efficient for medical image segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04847">http://arxiv.org/abs/2311.04847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danielle Ferreira, Rima Arnaout</li>
<li>for: 这个论文是为了评估Segment Anything模型（SAM）在各种物体分割任务中的表现，以及相比之下一种特有的模式自适应学习（SSL）方法在25种量化报告中的性能。</li>
<li>methods: 这个论文使用了Supervised Training方法，并对SAM和SSL方法进行了比较，以评估它们在100次心脏超声报告中的性能和资源占用情况。</li>
<li>results: 研究发现，SAM在评估中表现不佳，需要更多的标注和计算资源，而SSL方法则表现更好，具有更高的效率。<details>
<summary>Abstract</summary>
Foundation models are experiencing a surge in popularity. The Segment Anything model (SAM) asserts an ability to segment a wide spectrum of objects but required supervised training at unprecedented scale. We compared SAM's performance (against clinical ground truth) and resources (labeling time, compute) to a modality-specific, label-free self-supervised learning (SSL) method on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and required significantly more labeling and computing resources, demonstrating worse efficiency than SSL.
</details>
<details>
<summary>摘要</summary>
基础模型目前正在流行。射频段化模型（SAM）声称可以分类广泛的物体，但需要前所未有的指导式训练。我们比较了SAM的性能（与临床真实值）和资源（标签时间、计算）与一种特定Modalities自适应学习（SSL）方法在25个测量上100个心脏超声图像。SAM表现不佳，需要更多的标签和计算资源，示出了与SSL的更差的效率。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction"><a href="#Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction" class="headerlink" title="Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction"></a>Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04834">http://arxiv.org/abs/2311.04834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplab-ai/selfsupervisedvrd">https://github.com/deeplab-ai/selfsupervisedvrd</a></li>
<li>paper_authors: Zacharias Anastasakis, Dimitrios Mallis, Markos Diomataris, George Alexandridis, Stefanos Kollias, Vassilis Pitsikalis</li>
<li>for: 本研究旨在提出一种自助学习方法，用于视觉关系检测任务（VRD）。</li>
<li>methods: 该方法基于Masked Image Modeling（MIM）的想法，提出Masked Bounding Box Reconstruction（MBBR），即在场景中随机mask一部分实体&#x2F;物体，然后通过不masked对象进行重建。这种方法通过对象级别的masked模型学习，使网络学习场景中对象之间的交互，并因此具有高度预测视觉对象关系的表示。</li>
<li>results: 对于Visual Relationship Detection（VRD）任务，该方法在几个少量示例的情况下，能够超过现有的状态对方法，并且 Qualitative和Quantitative评估都表明了该方法学习的图像表示能够具有高度的Robustness和可预测性。<details>
<summary>Abstract</summary>
We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplab-ai/SelfSupervisedVRD.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的自主学习方法，具体是用于视觉关系检测（VRD）任务。我们受到Masked Image Modeling（MIM）的成功所 inspirited，我们提议Masked Bounding Box Reconstruction（MBBR），这是MIM的一种变体，在场景中部分对象被遮盖，然后根据未遮盖的对象进行重建。核心想法是通过对象水平的遮盖模型，使网络学习场景中对象之间的交互，从而学习出高度预测视觉对象关系的上下文意识的表示。我们在几个shot设置下进行了详细评估learned表示，并证明MBBR可以学习出高效的视觉表示，特别适用于VRD任务。我们使用了只有几个标注样本，但我们的方法仍然可以超越当前VRD方法在Predicate Detection（PredDet）评估设置中的性能。我们在https://github.com/deeplab-ai/SelfSupervisedVRD中提供了代码。
</details></li>
</ul>
<hr>
<h2 id="Anonymizing-medical-case-based-explanations-through-disentanglement"><a href="#Anonymizing-medical-case-based-explanations-through-disentanglement" class="headerlink" title="Anonymizing medical case-based explanations through disentanglement"></a>Anonymizing medical case-based explanations through disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04833">http://arxiv.org/abs/2311.04833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Montenegro, Jaime S. Cardoso</li>
<li>for: 本研究旨在Addressing the problem of privacy concerns in deep learning models for medical image analysis, by proposing a novel method for disentangling identity and medical characteristics of images and anonymizing them.</li>
<li>methods: 本研究使用了一种novel方法， named 嵌入式推理（Embedding-based reasoning）, which disentangles the identity and medical characteristics of images by replacing some feature vectors while preserving the remaining features. The researchers also proposed a model to manufacture synthetic privacy-preserving identities to replace the original image’s identity and achieve anonymization.</li>
<li>results: 实验表明，这种方法可以生成真实的、适用于医疗和生物特征的synthetic privacy-preserving identities，并且可以通过替换医学特征来生成counterfactual images。 The results demonstrate the capacity of the proposed method to generate realistic-looking anonymized images that preserve their original medical content, and the network’s inherent capacity to generate counterfactual images through the replacement of medical features.<details>
<summary>Abstract</summary>
Case-based explanations are an intuitive method to gain insight into the decision-making process of deep learning models in clinical contexts. However, medical images cannot be shared as explanations due to privacy concerns. To address this problem, we propose a novel method for disentangling identity and medical characteristics of images and apply it to anonymize medical images. The disentanglement mechanism replaces some feature vectors in an image while ensuring that the remaining features are preserved, obtaining independent feature vectors that encode the images' identity and medical characteristics. We also propose a model to manufacture synthetic privacy-preserving identities to replace the original image's identity and achieve anonymization. The models are applied to medical and biometric datasets, demonstrating their capacity to generate realistic-looking anonymized images that preserve their original medical content. Additionally, the experiments show the network's inherent capacity to generate counterfactual images through the replacement of medical features.
</details>
<details>
<summary>摘要</summary>
情况基本的解释是一种直观的方法，用于掌握深度学习模型在医疗场景中做出决策的过程。然而，医疗图像不能被用作解释，因为隐私问题。为解决这个问题，我们提出了一种新的方法，用于分离图像的标识特征和医疗特征。这种分离机制将某些图像特征替换，以保证保留图像的其他特征，从而获得独立的特征向量，这些特征向量都是图像的标识特征和医疗特征。我们还提出了一种模型，用于生成隐私保护的人工标识，以替换原始图像的标识。这些模型在医疗和生物метрических数据集上应用， demonstarting their capacity to generate realistic-looking anonymized images that preserve their original medical content. In addition, the experiments show the network's inherent capacity to generate counterfactual images through the replacement of medical features.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training"><a href="#SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training" class="headerlink" title="SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training"></a>SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04828">http://arxiv.org/abs/2311.04828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VimsLab/SODAWideNet">https://github.com/VimsLab/SODAWideNet</a></li>
<li>paper_authors: Rohit Venkata Sai Dulam, Chandra Kambhamettu</li>
<li>for: 这个论文的目的是开发一个新的突出对象检测（SOD）模型，而不需要在ImageNet dataset上重新训练整个网络。</li>
<li>methods: 该模型使用了一个encoder-decoder风格的网络，并提出了多种新的特征反ernerModule来使用backbone特征。其中包括了多重接收场（MRFFAM）和多scale注意（MSA）等模块，以提高网络的表达能力和灵活性。</li>
<li>results: 模型在五个 dataset上达到了竞争性的性能，并且 Parameters efficient。<details>
<summary>Abstract</summary>
Developing a new Salient Object Detection (SOD) model involves selecting an ImageNet pre-trained backbone and creating novel feature refinement modules to use backbone features. However, adding new components to a pre-trained backbone needs retraining the whole network on the ImageNet dataset, which requires significant time. Hence, we explore developing a neural network from scratch directly trained on SOD without ImageNet pre-training. Such a formulation offers full autonomy to design task-specific components. To that end, we propose SODAWideNet, an encoder-decoder-style network for Salient Object Detection. We deviate from the commonly practiced paradigm of narrow and deep convolutional models to a wide and shallow architecture, resulting in a parameter-efficient deep neural network. To achieve a shallower network, we increase the receptive field from the beginning of the network using a combination of dilated convolutions and self-attention. Therefore, we propose Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently obtains discriminative features from farther regions at higher resolutions using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which creates a feature pyramid and efficiently computes attention across multiple resolutions to extract global features from larger feature maps. Finally, we propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that achieve competitive performance against state-of-the-art models on five datasets.
</details>
<details>
<summary>摘要</summary>
开发新的突出对象检测（SOD）模型需要选择一个ImageNet预训练后缘和创造新的特征级进程来使用后缘特征。然而，将新组件添加到预训练后缘需要重新训练整个网络在ImageNet dataset上，这需要很长时间。因此，我们探索直接从 scratch 开发一个神经网络，不需要 ImageNet 预训练。这种方法允许我们完全自主地设计任务特定的组件。为此，我们提出了SODAWideNet，一个encoder-decoder风格的神经网络用于突出对象检测。我们与通常实践的窄深 convolutional 模型不同，使用宽浅的架构，从而实现参数效率的深度神经网络。为了实现更 shallow 的网络，我们从网络的开始点增加了扩展卷积和自注意力。因此，我们提出了多个感知场Feature Aggregation Module（MRFFAM），可以有效地从更远的区域获取高分辨率的特征。接着，我们提出了多Scale Attention（MSA），可以快速计算多个分辨率之间的注意力，以提取更大的特征图。最后，我们提出了SODAWideNet-S（3.03M）和SODAWideNet（9.03M）两种变体，在五个 dataset 上实现了与当前模型竞争的性能。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment"><a href="#Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment" class="headerlink" title="Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment"></a>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04818">http://arxiv.org/abs/2311.04818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattgorb/iterative_parameter_alignment">https://github.com/mattgorb/iterative_parameter_alignment</a></li>
<li>paper_authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray</li>
<li>for: 这 paper 的目的是提出一种基于 peer-to-peer topology的 Federated Learning 方法，以便在各自的数据集上训练模型，并在模型之间进行参数的对齐，以提高模型的泛化能力。</li>
<li>methods: 这 paper 使用了一种Weighted Distance Minimization 方法来对模型参数进行对齐，并在每个参与者的模型中寻找一个唯一的解。</li>
<li>results: 这 paper 的实验结果表明，这种方法可以在不同的数据集上达到竞争力的结果，并且在不同的领域中进行模型的对齐。此外，这种方法还能够在各个参与者的模型中寻找唯一的解，从而实现了在模型之间的对齐。<details>
<summary>Abstract</summary>
Learning from the collective knowledge of data dispersed across private sources can provide neural networks with enhanced generalization capabilities. Federated learning, a method for collaboratively training a machine learning model across remote clients, achieves this by combining client models via the orchestration of a central server. However, current approaches face two critical limitations: i) they struggle to converge when client domains are sufficiently different, and ii) current aggregation techniques produce an identical global model for each client. In this work, we address these issues by reformulating the typical federated learning setup: rather than learning a single global model, we learn N models each optimized for a common objective. To achieve this, we apply a weighted distance minimization to model parameters shared in a peer-to-peer topology. The resulting framework, Iterative Parameter Alignment, applies naturally to the cross-silo setting, and has the following properties: (i) a unique solution for each participant, with the option to globally converge each model in the federation, and (ii) an optional early-stopping mechanism to elicit fairness among peers in collaborative learning settings. These characteristics jointly provide a flexible new framework for iteratively learning from peer models trained on disparate datasets. We find that the technique achieves competitive results on a variety of data partitions compared to state-of-the-art approaches. Further, we show that the method is robust to divergent domains (i.e. disjoint classes across peers) where existing approaches struggle.
</details>
<details>
<summary>摘要</summary>
学习从分散在私人源中的数据收集的总知识可以提高神经网络的泛化能力。联邦学习方法可以在远程客户端上协同训练机器学习模型，并将客户端模型通过中央服务器的协调合并到一起。然而，现有方法面临两个重要的限制：一是在客户端领域差异足够大时困难于收敛，二是现有的集成技术会生成每个客户端都相同的全球模型。在这项工作中，我们解决这些问题 by 重新定义联邦学习设置：而不是学习单一的全球模型，我们学习 N 个优化为共同目标的模型。为 достичь这一点，我们使用分数据归一化来对参数共享在层次结构中进行补做。该框架被称为迭代参数对齐，可以自然地应用于跨积 Sylos 的设置。具有以下特点：1. 每个参与者都有唯一的解决方案，可以在联邦中每个模型都进行全球收敛。2. 可选的早期停止机制，以便在合作学习设置中约束参与者之间的公平。这些特点结合起来，为我们提供了一种灵活的新框架，可以逐步学习来自各个模型在不同数据集上的训练。我们发现，该技术可以与现有方法相比，在多个数据分区上实现竞争性的结果。此外，我们还证明该方法在不同领域（即客户端领域中的分离类）where existing approaches struggle。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning"><a href="#Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning" class="headerlink" title="Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning"></a>Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04815">http://arxiv.org/abs/2311.04815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, Mohsen Ali</li>
<li>for: 这个研究旨在提高深度学习基于物体探测器的适应能力，尤其是在面对新目标领域时，该领域具有明显的物体和背景变化。</li>
<li>methods: 本研究使用模型的预测不确定性来实现内部平衡和类别平衡。具体来说，我们发展了一种量化预测不确定性的技术，并使用高信任率预测生成 pseudo-label，以便进行自我训练。</li>
<li>results: 我们的方法在五个不同和具有挑战性的适应情况下表现出优秀的成绩，与现有的州��cially-of-the-art方法之间存在明显的差异。<details>
<summary>Abstract</summary>
Deep learning based object detectors struggle generalizing to a new target domain bearing significant variations in object and background. Most current methods align domains by using image or instance-level adversarial feature alignment. This often suffers due to unwanted background and lacks class-specific alignment. A straightforward approach to promote class-level alignment is to use high confidence predictions on unlabeled domain as pseudo-labels. These predictions are often noisy since model is poorly calibrated under domain shift. In this paper, we propose to leverage model's predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. We develop a technique to quantify predictive uncertainty on class assignments and bounding-box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-training, whereas the ones with higher uncertainty are used to generate tiles for adversarial feature alignment. This synergy between tiling around uncertain object regions and generating pseudo-labels from highly certain object regions allows capturing both image and instance-level context during the model adaptation. We report thorough ablation study to reveal the impact of different components in our approach. Results on five diverse and challenging adaptation scenarios show that our approach outperforms existing state-of-the-art methods with noticeable margins.
</details>
<details>
<summary>摘要</summary>
深度学习基于对象检测器在新目标领域中一般化是一个问题。大多数当前方法使用图像或实例水平的对抗特征偏移来对领域进行Alignment。这经常受到背景的不良影响并且缺乏类别偏移。在本文中，我们提出使用高确定性预测作为 pseudo-labels来促进类别偏移。我们开发了一种技术来评估预测的不确定性，并使用低不确定性预测生成 pseudo-labels，而高不确定性预测则用于生成对抗特征偏移。这种同时使用瓷砾在不确定性范围内和生成高确定性预测作为 pseudo-labels的方法，允许在模型适应过程中捕捉图像和实例级上下文。我们进行了完整的ablation研究，以便了解不同组件在我们的方法中的影响。我们在五种多样化和挑战性的适应场景中进行了实验，并发现我们的方法可以与现有状态的方法相比，表现出明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth"><a href="#Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth" class="headerlink" title="Be Careful When Evaluating Explanations Regarding Ground Truth"></a>Be Careful When Evaluating Explanations Regarding Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04813">http://arxiv.org/abs/2311.04813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mi2datalab/be-careful-evaluating-explanations">https://github.com/mi2datalab/be-careful-evaluating-explanations</a></li>
<li>paper_authors: Hubert Baniecki, Maciej Chrabaszcz, Andreas Holzinger, Bastian Pfeifer, Anna Saranti, Przemyslaw Biecek</li>
<li>for: 本研究旨在评估深度学习模型的安全性，尤其是在医疗影像分析和机器人应用中，通过评估模型与人类理解之间的匹配程度。</li>
<li>methods: 本研究提出了一种框架，用于同时评估深度学习模型和解释方法的稳定性，并使用了精度调整程序来（不）对模型与真实情况之间的匹配进行调整。</li>
<li>results: 实验结果表明，视transformer模型和相关的解释方法在不同的模型架构和post-hoc本地解释方法下的Robustness具有一定的潜在攻击风险。<details>
<summary>Abstract</summary>
Evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. Driven by this observation, we propose a framework for $\textit{jointly}$ evaluating the robustness of safety-critical systems that $\textit{combine}$ a deep neural network with an explanation method. These are increasingly used in real-world applications like medical image analysis or robotics. We introduce a fine-tuning procedure to (mis)align model$\unicode{x2013}$explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. Experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.
</details>
<details>
<summary>摘要</summary>
evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. driven by this observation, we propose a framework for $\textit{jointly}$ evaluating the robustness of safety-critical systems that $\textit{combine}$ a deep neural network with an explanation method. these are increasingly used in real-world applications like medical image analysis or robotics. we introduce a fine-tuning procedure to (mis)align model$\unicode{x2013}$explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.Here's the text with some notes on the translation:* "ground truth" is translated as "人类理解的标准" (rénxìng lǐjiě de biānwù)* "segmentation masks" is translated as "分割面积" (fēnzhì miànjì)* "deep neural network" is translated as "深度神经网络" (shēngrán jiānxīngwǎng)* "explanation method" is translated as "解释方法" (jiějie fāngfa)* "safety-critical systems" is translated as "安全关键系统" (ānquè guānjī systems)* "jointly" is translated as "共同" (gòngdòng)* "worst-case scenarios" is translated as "最坏情况" (zuì huò qíngkē)* "best-case scenarios" is translated as "最好情况" (zuì hǎo qíngkē)* "human alignment" is translated as "人类对齐" (rénxìng duìqí)Note that the translation of "jointly" as "共同" is a bit more formal than the original English text, but it accurately conveys the meaning of the phrase. Additionally, the translation of "worst-case scenarios" and "best-case scenarios" as "最坏情况" and "最好情况" is a bit more idiomatic than the original English text, but it accurately conveys the meaning of the phrases in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Image-Based-Virtual-Try-On-A-Survey"><a href="#Image-Based-Virtual-Try-On-A-Survey" class="headerlink" title="Image-Based Virtual Try-On: A Survey"></a>Image-Based Virtual Try-On: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04811">http://arxiv.org/abs/2311.04811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/little-misfit/survey-of-virtual-try-on">https://github.com/little-misfit/survey-of-virtual-try-on</a></li>
<li>paper_authors: Dan Song, Xuanpu Zhang, Juan Zhou, Weizhi Nie, Ruofeng Tong, An-An Liu</li>
<li>For: This paper aims to provide a comprehensive analysis of state-of-the-art techniques and methodologies in image-based virtual try-on, and to identify key trends and future research directions in this field.* Methods: The paper uses a pipeline architecture that includes person representation, try-on indication, clothing warping, and try-on stage. The authors also propose a new semantic criteria using CLIP and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset.* Results: The paper provides a comprehensive overview of the current state of image-based virtual try-on research, including both quantitative and qualitative evaluations of current open-source methods. The authors also demonstrate the potential of large-scale models on this task by fine-tuning a recent image generation model (PBE) using ControlNet.Here is the text in Simplified Chinese:* For: 这篇论文目的是提供图像基于虚拟试穿的全面分析，并预测这个领域的未来研究趋势。* Methods: 该论文使用管道式架构，包括人体表示、试穿指示、衣服折叠和试穿阶段。作者还提出了基于CLIP的新的Semantic criteria。* Results: 论文提供了图像基于虚拟试穿的现状报告，包括现有开源方法的量化和质量评估。作者还示出了大规模模型在这个任务上的潜在潜力。<details>
<summary>Abstract</summary>
Image-based virtual try-on aims to synthesize a naturally dressed person image with a clothing image, which revolutionizes online shopping and inspires related topics within image generation, showing both research significance and commercial potentials. However, there is a great gap between current research progress and commercial applications and an absence of comprehensive overview towards this field to accelerate the development. In this survey, we provide a comprehensive analysis of the state-of-the-art techniques and methodologies in aspects of pipeline architecture, person representation and key modules such as try-on indication, clothing warping and try-on stage. We propose a new semantic criteria with CLIP, and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset. In addition to quantitative and qualitative evaluation of current open-source methods, we also utilize ControlNet to fine-tune a recent large image generation model (PBE) to show future potentials of large-scale models on image-based virtual try-on task. Finally, unresolved issues are revealed and future research directions are prospected to identify key trends and inspire further exploration. The uniformly implemented evaluation metrics, dataset and collected methods will be made public available at https://github.com/little-misfit/Survey-Of-Virtual-Try-On.
</details>
<details>
<summary>摘要</summary>
图像基于虚拟试穿涉及合成一个自然地穿着人像与衣服图像，这种技术革新了在线购物和相关领域的图像生成，具有研究重要性和商业潜力。然而，目前研究进步和商业应用之间存在巨大的差距，而且对这一领域的全面概述缺乏，以便加速发展。在这份调查中，我们提供了图像基于虚拟试穿领域的全面分析，包括管道架构、人体表示和关键模块such as 试穿指示、衣服扭曲和试穿阶段。我们还提出了一种新的semantic标准，并使用CLIP进行评估代表方法。此外，我们还使用ControlNet来精度调整最近一个大型图像生成模型（PBE），以示未来大型模型在图像基于虚拟试穿任务中的潜力。最后，我们揭示了未解的问题和未来研究方向，以便识别关键趋势和激发更多的探索。我们 uniformly 实施的评估指标、数据集和收集的方法将在https://github.com/little-misfit/Survey-Of-Virtual-Try-On 上公开。
</details></li>
</ul>
<hr>
<h2 id="VioLA-Aligning-Videos-to-2D-LiDAR-Scans"><a href="#VioLA-Aligning-Videos-to-2D-LiDAR-Scans" class="headerlink" title="VioLA: Aligning Videos to 2D LiDAR Scans"></a>VioLA: Aligning Videos to 2D LiDAR Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04783">http://arxiv.org/abs/2311.04783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun-Jee Chao, Selim Engin, Nikhil Chavan-Dafle, Bhoram Lee, Volkan Isler</li>
<li>for: 将视频Sequence align到2D LiDAR扫描图中的环境</li>
<li>methods: 引入VioLA方法，首先从图像序列中建立本地场景的semantic map，然后从图像序列中提取高度为固定值的点进行注册</li>
<li>results: 通过使用 pré-trained text-to-image填充模型和深度完成模型来填充缺失的场景内容，提高了pose注册性能，最多提高20%<details>
<summary>Abstract</summary>
We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed scene completion module improves the pose registration performance by up to 20%.
</details>
<details>
<summary>摘要</summary>
我们研究将视频Capture的本地环境与2D LiDAR扫描的全景环境进行对应的问题。我们提出了一种方法（VioLA），它首先从图像序列中建立了本地场景的semantic map，然后提取了一个固定高度的点进行与LiDAR地图进行对应。由于恢复错误或相机扫描的部分覆盖，可能导致重建的semantic maplack sufficient information for registration。为解决这个问题，VioLA利用了一个预训练的文本-图像填充模型和一个depth completion模型来填充缺失的场景内容，以保持准确的姿态注册。我们在两个实际的RGB-D标准benchmark上以及一个自拍摄的大办公室场景中进行了评估，并观察到我们提议的场景完成模块可以提高姿态注册性能达20%。
</details></li>
</ul>
<hr>
<h2 id="Lidar-Annotation-Is-All-You-Need"><a href="#Lidar-Annotation-Is-All-You-Need" class="headerlink" title="Lidar Annotation Is All You Need"></a>Lidar Annotation Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04777">http://arxiv.org/abs/2311.04777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evocargo/lidar-annotation-is-all-you-need">https://github.com/evocargo/lidar-annotation-is-all-you-need</a></li>
<li>paper_authors: Dinar Sharafutdinov, Stanislav Kuskov, Saian Protasov, Alexey Voropaev<br>for: 这 paper 的目的是提高图像分割的效率，使用 convolutional neural network 在多感器设置下进行图像分割。methods: 该方法使用 lidar 精度测量点云，并将其直接用于图像分割模型的训练。该方法还使用 masked loss 来处理稀疏的地面数据。results: 实验表明，该方法可以在多个数据集上实现相似的性能，而不需要大量的注释数据。该方法可以减少注释负担，并允许在图像分割模型的训练中混合不同类型的地面数据。<details>
<summary>Abstract</summary>
In recent years, computer vision has transformed fields such as medical imaging, object recognition, and geospatial analytics. One of the fundamental tasks in computer vision is semantic image segmentation, which is vital for precise object delineation. Autonomous driving represents one of the key areas where computer vision algorithms are applied. The task of road surface segmentation is crucial in self-driving systems, but it requires a labor-intensive annotation process in several data domains. The work described in this paper aims to improve the efficiency of image segmentation using a convolutional neural network in a multi-sensor setup. This approach leverages lidar (Light Detection and Ranging) annotations to directly train image segmentation models on RGB images. Lidar supplements the images by emitting laser pulses and measuring reflections to provide depth information. However, lidar's sparse point clouds often create difficulties for accurate object segmentation. Segmentation of point clouds requires time-consuming preliminary data preparation and a large amount of computational resources. The key innovation of our approach is the masked loss, addressing sparse ground-truth masks from point clouds. By calculating loss exclusively where lidar points exist, the model learns road segmentation on images by using lidar points as ground truth. This approach allows for blending of different ground-truth data types during model training. Experimental validation of the approach on benchmark datasets shows comparable performance to a high-quality image segmentation model. Incorporating lidar reduces the load on annotations and enables training of image-segmentation models without loss of segmentation quality. The methodology is tested on diverse datasets, both publicly available and proprietary. The strengths and weaknesses of the proposed method are also discussed in the paper.
</details>
<details>
<summary>摘要</summary>
Recently, computer vision has revolutionized fields such as medical imaging, object recognition, and geospatial analytics. One of the fundamental tasks in computer vision is semantic image segmentation, which is crucial for precise object delineation. Autonomous driving is one of the key areas where computer vision algorithms are applied, and the task of road surface segmentation is crucial in self-driving systems. However, this task requires a labor-intensive annotation process in several data domains.The work described in this paper aims to improve the efficiency of image segmentation using a convolutional neural network in a multi-sensor setup. This approach leverages lidar (Light Detection and Ranging) annotations to directly train image segmentation models on RGB images. Lidar supplements the images by emitting laser pulses and measuring reflections to provide depth information. However, lidar's sparse point clouds often create difficulties for accurate object segmentation.The key innovation of our approach is the masked loss, which addresses sparse ground-truth masks from point clouds. By calculating loss exclusively where lidar points exist, the model learns road segmentation on images by using lidar points as ground truth. This approach allows for blending of different ground-truth data types during model training. Experimental validation of the approach on benchmark datasets shows comparable performance to a high-quality image segmentation model. Incorporating lidar reduces the load on annotations and enables training of image-segmentation models without loss of segmentation quality. The methodology is tested on diverse datasets, both publicly available and proprietary. The strengths and weaknesses of the proposed method are also discussed in the paper.
</details></li>
</ul>
<hr>
<h2 id="GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration"><a href="#GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration" class="headerlink" title="GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration"></a>GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04772">http://arxiv.org/abs/2311.04772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage">https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage</a></li>
<li>paper_authors: Xuhao Shan, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qingying Xiao, Xiang Wan, Changmiao Wang</li>
<li>for: 预测急性脑出血（ICH）的诊断和治疗结果，提高患者生存率。</li>
<li>methods: 利用多Modal脑CT图像数据和格拉斯哥昏迷分数（GCS）来提高ICH诊断，使用trasnformer基本的融合模块进行评估。</li>
<li>results: GCS-ICHNet实现了81.03%的敏感性和91.59%的特异性，超过了平均临床医生和其他当前状态的方法。<details>
<summary>Abstract</summary>
Intracerebral Hemorrhage (ICH) is a severe condition resulting from damaged brain blood vessel ruptures, often leading to complications and fatalities. Timely and accurate prognosis and management are essential due to its high mortality rate. However, conventional methods heavily rely on subjective clinician expertise, which can lead to inaccurate diagnoses and delays in treatment. Artificial intelligence (AI) models have been explored to assist clinicians, but many prior studies focused on model modification without considering domain knowledge. This paper introduces a novel deep learning algorithm, GCS-ICHNet, which integrates multimodal brain CT image data and the Glasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizes a transformer-based fusion module for assessment. GCS-ICHNet demonstrates high sensitivity 81.03% and specificity 91.59%, outperforming average clinicians and other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Intracerebral Hemorrhage (ICH) 是一种严重的疾病，由于脑血管受损而导致血液泄露，可能会导致严重的后果和死亡。因此，有效和准确的诊断和治疗是非常重要的，因为其mortality rate 很高。然而，传统的方法很多都是基于专业知识的，可能会导致不准确的诊断和治疗延迟。人工智能（AI）模型已经被探讨以帮助临床医生，但许多先前的研究都是对模型进行修改而不考虑域知识。本文介绍了一种新的深度学习算法，GCS-ICHNet，它通过结合多Modal脑CT图像数据和格拉斯哥昏迷scale（GCS）分数来改善ICH的诊断。该算法使用 transformer-based 混合模块进行评估。GCS-ICHNet 在敏感性和特点上都有出色的表现，高达 81.03% 和 91.59%，超过了平均的临床医生和其他现有的方法。
</details></li>
</ul>
<hr>
<h2 id="An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer"><a href="#An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer" class="headerlink" title="An attention-based deep learning network for predicting Platinum resistance in ovarian cancer"></a>An attention-based deep learning network for predicting Platinum resistance in ovarian cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04769">http://arxiv.org/abs/2311.04769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoming Zhuang, Beibei Li, Jingtong Ma, Patrice Monkam, Shouliang Qi, Wei Qian, Dianning He<br>for: 这项研究的目的是提出一种基于深度学习的方法，用于判断患有高等级胸膜癌的患者是否 platinum 抵抗性。methods: 该研究使用了289名高等级胸膜癌患者的数据，并建立了一个结合了压缩块（SE Block）和空间 пирамид Pooling层（SPPLayer）的Dense Convolutional Network（DenseNet）模型。利用多Modal PET&#x2F;CT 图像数据进行预测患者 platinum 抵抗性。results: 经五次交叠验证，SE-SPP-DenseNet 模型在预测患者 platinum 抵抗性方面 achieved a high accuracy rate和抛物线曲线（AUC）值，分别为92.6%和0.93。通过进行剥离实验和单Modal 数据验证，证明了将 SE Block 和 SPPLayer 添加到深度学习模型中，并考虑多Modal 数据的重要性。<details>
<summary>Abstract</summary>
Background: Ovarian cancer is among the three most frequent gynecologic cancers globally. High-grade serous ovarian cancer (HGSOC) is the most common and aggressive histological type. Guided treatment for HGSOC typically involves platinum-based combination chemotherapy, necessitating an assessment of whether the patient is platinum-resistant. The purpose of this study is to propose a deep learning-based method to determine whether a patient is platinum-resistant using multimodal positron emission tomography/computed tomography (PET/CT) images. Methods: 289 patients with HGSOC were included in this study. An end-to-end SE-SPP-DenseNet model was built by adding Squeeze-Excitation Block (SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) to Dense Convolutional Network (DenseNet). Multimodal data from PET/CT images of the regions of interest (ROI) were used to predict platinum resistance in patients. Results: Through five-fold cross-validation, SE-SPP-DenseNet achieved a high accuracy rate and an area under the curve (AUC) in predicting platinum resistance in patients, which were 92.6% and 0.93, respectively. The importance of incorporating SE Block and SPPLayer into the deep learning model, and considering multimodal data was substantiated by carrying out ablation studies and experiments with single modality data. Conclusions: The obtained classification results indicate that our proposed deep learning framework performs better in predicting platinum resistance in patients, which can help gynecologists make better treatment decisions. Keywords: PET/CT, CNN, SE Block, SPP Layer, Platinum resistance, Ovarian cancer
</details>
<details>
<summary>摘要</summary>
背景：子宫癌是全球三大妇科癌种之一，高等级膜蛋白癌（HGSOC）是最常见并且最严重的 histological 型。对 HGSOC 患者的治疗通常包括钍基化学疗法，需要评估患者是否为钍耐 resistance。本研究的目的是提出一种基于深度学习的方法，使用多Modal positron emission tomography/computed tomography（PET/CT）图像来判断患者是否为钍耐 resistance。方法：本研究包括289名 HGSOC 患者。我们建立了一个结合 Squeeze-Excitation Block（SE Block）和 Spatial Pyramid Pooling Layer（SPPLayer）的 Dense Convolutional Network（DenseNet）模型。使用 ROI 的多Modal PET/CT 图像来预测患者是否为钍耐 resistance。结果：通过五次交叉验证，SE-SPP-DenseNet 模型在预测患者是否为钍耐 resistance 中达到了高精度率和报春分（AUC）的好Result，分别为92.6%和0.93。我们通过进行剖除研究和单模态数据实验来证明，将 SE Block 和 SPPLayer 添加到深度学习模型中，以及考虑多Modal数据的重要性。结论：我们的提出的深度学习框架在预测患者是否为钍耐 resistance 中表现更好，可以帮助妇科医生作出更好的治疗决策。关键词：PET/CT, CNN, SE Block, SPP Layer, 钍耐 resistance, 子宫癌
</details></li>
</ul>
<hr>
<h2 id="DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation"><a href="#DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation"></a>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04766">http://arxiv.org/abs/2311.04766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guinan Su, Yanwu Yang, Zhifeng Li</li>
<li>for: 这个研究旨在提高音频驱动3D面部动画的精度和效率，特别是在虚拟现实、游戏和视频会议等应用中。</li>
<li>methods: 该研究提出了一种交叉模式双学习框架，称为DualTalker，以提高数据使用效率并关联跨模态关系。该框架共同受过主任务（音频驱动面部动画）和其双任务（详细说话）的联合训练，并共享音频&#x2F;运动编码器组件。</li>
<li>results: 经过广泛的实验和一项感知用户研究，我们展示了我们的方法在VOCA和BIWI数据集上的较好的表现，both qualitatively和quantitatively。我们的代码和视频示例已经在<a target="_blank" rel="noopener" href="https://github.com/sabrina-su/iadf.git%E4%B8%AD%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/sabrina-su/iadf.git中提供。</a><details>
<summary>Abstract</summary>
In recent years, audio-driven 3D facial animation has gained significant attention, particularly in applications such as virtual reality, gaming, and video conferencing. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies approach the facial animation task as a single regression problem, which often fail to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation and overlook their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability that decreases the performance. To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency as well as relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework facilitates more efficient data usage by leveraging information from both tasks and explicitly capitalizing on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics. Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. We have made our code and video demonstrations available at https://github.com/sabrina-su/iadf.git.
</details>
<details>
<summary>摘要</summary>
Recently, audio-driven 3D facial animation has gained significant attention, especially in virtual reality, gaming, and video conferencing applications. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies treat the facial animation task as a single regression problem, which often fails to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation, and overlooks their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability, which decreases performance.To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency and relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework leverages information from both tasks and explicitly capitalizes on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics.Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. Our code and video demonstrations are available at <https://github.com/sabrina-su/iadf.git>.
</details></li>
</ul>
<hr>
<h2 id="Social-Motion-Prediction-with-Cognitive-Hierarchies"><a href="#Social-Motion-Prediction-with-Cognitive-Hierarchies" class="headerlink" title="Social Motion Prediction with Cognitive Hierarchies"></a>Social Motion Prediction with Cognitive Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04726">http://arxiv.org/abs/2311.04726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Walter0807/Social-CH">https://github.com/Walter0807/Social-CH</a></li>
<li>paper_authors: Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</li>
<li>for: 这个研究的目的是复制人类在预测他人行为方面的能力，通过解决社交动作预测问题。</li>
<li>methods: 这个研究使用了一个新的比较器、一种新的形式化、以及基于认知的框架。他们还使用了行为做副本和生成对抗学习来提高学习效率和通用性。</li>
<li>results: 研究人员通过实施了一种新的3D多人动作数据集，并通过对比较器和生成对抗学习来验证数据集和方法的有效性。<details>
<summary>Abstract</summary>
Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach. Code and data are available at https://walter0807.github.io/Social-CH/.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的其他人行为预测能力和自己行为规划能力。在这项研究中，我们努力复制这种能力，解决社会动作预测问题。我们引入了新的标准集，一种新的形ulation，以及一个基于认知的框架。我们提供了一个3D多人动作数据集，名为Wusi，该数据集在体育赛事中展示了人类之间的激烈和策略性交互，以及多种姿态分布。我们将问题转换为多智能 reinforcement learning 视角，并应用行为做样和生成对抗学习来提高学习效率和泛化能力。此外，我们考虑了人类社会行为规划过程中的认知方面，并开发了认知层次框架来预测人类社会互动的策略。我们进行了全面的实验来验证我们的提posed dataset和方法的有效性。代码和数据可以在https://walter0807.github.io/Social-CH/获取。
</details></li>
</ul>
<hr>
<h2 id="Training-CLIP-models-on-Data-from-Scientific-Papers"><a href="#Training-CLIP-models-on-Data-from-Scientific-Papers" class="headerlink" title="Training CLIP models on Data from Scientific Papers"></a>Training CLIP models on Data from Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04711">http://arxiv.org/abs/2311.04711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nopperl/clip_arxiv_pmc">https://github.com/nopperl/clip_arxiv_pmc</a></li>
<li>paper_authors: Calvin Metzger</li>
<li>for: 这篇论文旨在检验 CLIP 模型是否通过使用高质量数据来提高总体性能。</li>
<li>methods: 该论文使用 arXiv 和 PubMed Central 搜索引擎提取文本图像数据，并在小规模 CLIP 模型（ViT B&#x2F;32）上进行实验。</li>
<li>results: 实验结果表明，使用高质量数据源可以提高 CLIP 模型的性能，但提高的程度只是有 moderate。这表明使用这些数据源来训练大规模 CLIP 模型是一个值得进行研究的方向。<details>
<summary>Abstract</summary>
Contrastive Language-Image Pretraining (CLIP) models are able to capture the semantic relationship of images and texts and have enabled a wide range of applications, from image retrieval to classification. These models are trained with datasets extracted from web crawls, which are of large quantity but limited quality. This paper explores whether limited amounts higher quality data in a specific domain improve the general performance of CLIP models. To this purpose, we extract text-image data from scientific papers hosted in the arXiv and PubMed Central repositories. Experiments on small-scale CLIP models (ViT B/32) show that model performance increases on average, but only moderately. This result indicates that using the data sources considered in the paper to train large-scale CLIP models is a worthwile research direction.
</details>
<details>
<summary>摘要</summary>
对比语言-图像预训（CLIP）模型可以捕捉图像和文本之间的 semantic 关系，并实现了许多应用，从图像检索到分类。这些模型通常通过互联网爬虫获取数据进行训练，这些数据的量很大，但质量有限。这篇文章探讨了是否有限量但高质量数据在特定领域提高CLIP模型的总性能。为此，我们从arXiv和PubMed Central数据库中提取了文本-图像数据。实验结果表明，使用这些数据来训练小规模CLIP模型（ViT B/32）时，模型的性能平均提高，但只有moderate。这结果表明，使用文章中所考虑的数据来训练大规模CLIP模型是一个有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud"><a href="#3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud" class="headerlink" title="3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud"></a>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04699">http://arxiv.org/abs/2311.04699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchao Ci, Xin Wang, David Rapado-Rincón, Akshay K. Burusa, Gert Kootstra</li>
<li>For: 本研究旨在提供一种基于关键点检测的RGB-D相机数据的方法，用于自动探测护叶节点，以便在绿色家庭中自动探测 Tomatoes。* Methods: 本方法使用了RGB-D相机数据，通过检测颜色图像中的四个骨干特征点，并将其与3D点云信息集成，以确定护叶节点的3D姿态。* Results: 研究结果表明，该方法具有高精度的物体检测能力（<a href="mailto:&#65;&#x50;&#x40;&#48;&#x2e;&#53;">&#65;&#x50;&#x40;&#48;&#x2e;&#53;</a>&#x3D;0.96）、高精度的关键点检测率（<a href="mailto:&#x50;&#68;&#x4a;&#64;&#x30;&#46;&#50;">&#x50;&#68;&#x4a;&#64;&#x30;&#46;&#50;</a>&#x3D;94.31%）和3D姿态估计精度（MAE&#x3D;11.38o和9.93o）。此外，该方法还可以快速响应视点变化。<details>
<summary>Abstract</summary>
Greenhouse production of fruits and vegetables in developed countries is challenged by labor 12 scarcity and high labor costs. Robots offer a good solution for sustainable and cost-effective 13 production. Acquiring accurate spatial information about relevant plant parts is vital for 14 successful robot operation. Robot perception in greenhouses is challenging due to variations in 15 plant appearance, viewpoints, and illumination. This paper proposes a keypoint-detection-based 16 method using data from an RGB-D camera to estimate the 3D pose of peduncle nodes, which 17 provides essential information to harvest the tomato bunches. 18 19 Specifically, this paper proposes a method that detects four anatomical landmarks in the color 20 image and then integrates 3D point-cloud information to determine the 3D pose. A 21 comprehensive evaluation was conducted in a commercial greenhouse to gain insight into the 22 performance of different parts of the method. The results showed: (1) high accuracy in object 23 detection, achieving an Average Precision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints (PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25 accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relative upper and lower 26 angles between the peduncle and main stem, respectively. Furthermore, the capability to handle 27 variations in viewpoint was investigated, demonstrating the method was robust to view changes. 28 However, canonical and higher views resulted in slightly higher performance compared to other 29 views. Although tomato was selected as a use case, the proposed method is also applicable to 30 other greenhouse crops like pepper.
</details>
<details>
<summary>摘要</summary>
developed countries的绿色房production of fruits and vegetables面临劳动力短缺和高劳动成本的挑战。Robots可以提供可持续和成本效果的解决方案。获取有关相关植物部分的准确空间信息是成功机器人运行的关键。在绿色房中机器人识别是由于植物外观、视角和照明变化而困难。这篇论文提出了基于特征点探测的方法，使用RGB-D摄像头数据来估算 Tomatoes的3D姿态。 Specifically, this paper proposes a method that detects four anatomical landmarks in the color image and then integrates 3D point-cloud information to determine the 3D pose. A comprehensive evaluation was conducted in a commercial greenhouse to gain insight into the performance of different parts of the method. The results showed: (1) high accuracy in object detection, achieving an Average Precision (AP) of AP@0.5=0.96; (2) an average Percentage of Detected Joints (PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation accuracy with mean absolute errors (MAE) of 11.38° and 9.93° for the relative upper and lower angles between the peduncle and main stem, respectively. Furthermore, the capability to handle variations in viewpoint was investigated, demonstrating the method was robust to view changes. However, canonical and higher views resulted in slightly higher performance compared to other views. Although tomato was selected as a use case, the proposed method is also applicable to other greenhouse crops like pepper.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-cross-model-learning-in-high-content-screening"><a href="#Weakly-supervised-cross-model-learning-in-high-content-screening" class="headerlink" title="Weakly supervised cross-model learning in high-content screening"></a>Weakly supervised cross-model learning in high-content screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04678">http://arxiv.org/abs/2311.04678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Watkinson Gabriel, Cohen Ethan, Bourriez Nicolas, Bendidi Ihab, Bollot Guillaume, Genovesio Auguste</li>
<li>for: 本研究旨在探索如何在药物搜索中连接不同数据类型的数据。</li>
<li>methods: 我们提出了一种新的方法，利用弱监督和跨站复制在高内容检测中使用CLIP建立跨模态表示。</li>
<li>results: 我们的方法可以学习更好的表示，减轻批处理效应，并且对JUMP-CP数据集进行了有效的预处理，从85TB减少到7TB，保留了所有干扰和大多数信息内容。<details>
<summary>Abstract</summary>
With the surge in available data from various modalities, there is a growing need to bridge the gap between different data types. In this work, we introduce a novel approach to learn cross-modal representations between image data and molecular representations for drug discovery. We propose EMM and IMM, two innovative loss functions built on top of CLIP that leverage weak supervision and cross sites replicates in High-Content Screening. Evaluating our model against known baseline on cross-modal retrieval, we show that our proposed approach allows to learn better representations and mitigate batch effect. In addition, we also present a preprocessing method for the JUMP-CP dataset that effectively reduce the required space from 85Tb to a mere usable 7Tb size, still retaining all perturbations and most of the information content.
</details>
<details>
<summary>摘要</summary>
随着不同数据模式之间的数据量的增加，需要桥接这些数据模式之间的 gap 变得更加重要。在这种工作中，我们介绍了一种新的方法，用于从图像数据和分子表示之间学习 crossed-modal 表示。我们提出了两种创新的损失函数EMM和IMM，基于 CLIP 的上下文，利用弱监督和跨站复制在高内容检测中。我们对知道的基准进行跨Modal 检索，并显示了我们提议的方法可以学习更好的表示，并减轻批处理效应。此外，我们还提出了对 JUMP-CP 数据集的预处理方法，可以有效地将数据减少到可用 7Tb 大小，保留所有干扰和大多数信息内容。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Diffusion-Models-with-Distillation-Based-Block-Neural-Architecture-Search"><a href="#Lightweight-Diffusion-Models-with-Distillation-Based-Block-Neural-Architecture-Search" class="headerlink" title="Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search"></a>Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04950">http://arxiv.org/abs/2311.04950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Yansong Tang, Wenwu zhu</li>
<li>for: 提高 diffusion models 的计算效率，使其在多种任务中实现 state-of-the-art 性能。</li>
<li>methods: 提出了一种基于 Diffusion Distillation 的 Block-wise Neural Architecture Search (DiffNAS) 方法，通过自动去除 diffusion models 中的结构冗余来减少计算成本。</li>
<li>results: 实验表明，DiffNAS 可以实现约 50% MACs 和参数减少，并且可以在 latent diffusion models 上实现比 teacher 更好的性能。<details>
<summary>Abstract</summary>
Diffusion models have recently shown remarkable generation ability, achieving state-of-the-art performance in many tasks. However, the high computational cost is still a troubling problem for diffusion models. To tackle this problem, we propose to automatically remove the structural redundancy in diffusion models with our proposed Diffusion Distillation-based Block-wise Neural Architecture Search (DiffNAS). Specifically, given a larger pretrained teacher, we leverage DiffNAS to search for the smallest architecture which achieves on-par or even better performance than the teacher. Considering current diffusion models are based on UNet which naturally has a block-wise structure, we perform neural architecture search independently in each block, which largely reduces the search space. Different from previous block-wise NAS methods, DiffNAS contains a block-wise local search strategy and a retraining strategy with a joint dynamic loss. Concretely, during the search process, we block-wisely select the best subnet to avoid the unfairness brought by the global search strategy used in previous works. When retraining the searched architecture, we adopt a dynamic joint loss to maintain the consistency between supernet training and subnet retraining, which also provides informative objectives for each block and shortens the paths of gradient propagation. We demonstrate this joint loss can effectively improve model performance. We also prove the necessity of the dynamic adjustment of this loss. The experiments show that our method can achieve significant computational reduction, especially on latent diffusion models with about 50% MACs and Parameter reduction.
</details>
<details>
<summary>摘要</summary>
Diffusion 模型最近显示出了很好的生成能力，在许多任务中达到了状态的核心性能。然而，高计算成本仍然是 diffusion 模型的一个困扰问题。为解决这个问题，我们提出了自动Remove diffusion 模型中的结构冗余的方法：Diffusion Distillation-based Block-wise Neural Architecture Search (DiffNAS)。具体来说，我们使用 DiffNAS 在一个更大的预训练老师模型基础上进行搜索，找到与老师模型具有相同或更好的性能的最小架构。由于现有的 diffusion 模型基于 UNet 结构，我们在搜索过程中独立地进行每个块的 neural architecture search，从而大大减少搜索空间。与前一些块基本 NAS 方法不同，DiffNAS 包含块基本选择策略和重新训练策略，并且采用了一种动态共同损失。在搜索过程中，我们会在每个块中选择最佳子网，以避免由全局搜索策略所带来的不公平性。在重新训练搜索出的架构时，我们采用了一种动态共同损失，以保持超网训练和子网重新训练之间的一致性，同时提供了每个块的有用目标。我们证明了这种动态调整的损失是必要的。实验表明，我们的方法可以实现显著的计算减少，特别是在含有约50% MACs和参数的含括 diffusion 模型上。
</details></li>
</ul>
<hr>
<h2 id="VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering"><a href="#VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering" class="headerlink" title="VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering"></a>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04634">http://arxiv.org/abs/2311.04634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lfranke/vet">https://github.com/lfranke/vet</a></li>
<li>paper_authors: Linus Franke, Darius Rückert, Laura Fink, Matthias Innmann, Marc Stamminger</li>
<li>for: 这个论文的目的是提高点云图像的新视图合成质量。</li>
<li>methods: 该论文使用了一种基于神经网络的方法，使用点云代理geometry来检测和修复新视图合成中的缺失或损害。</li>
<li>results: 论文的实验结果表明，该方法可以显著提高点云图像的新视图合成质量，并且可以有效地修复大规模的缺失和细腻结构。同时，该方法的实时渲染速度也得到了改进。<details>
<summary>Abstract</summary>
In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.
</details>
<details>
<summary>摘要</summary>
Recently, deep neural networks have led to significant advances in novel view synthesis. Many of these methods rely on a coarse proxy geometry obtained through structure from motion algorithms. While small defects in the proxy can be corrected by neural rendering, larger holes or missing parts can still result in distracting artifacts and temporal instability. In this paper, we propose a novel neural-rendering-based approach to detect and fix such deficiencies. We use a point cloud as our proxy, which allows us to easily remove outlier geometry and fill in missing geometry without complicated topological operations. The key components of our approach are:1. A differentiable, blending point-based renderer that can blend out redundant points.2. The concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D regions lacking geometry and spawn novel points accordingly.3. The addition of points as nested environment maps, which allows us to generate high-quality renderings of the surroundings in the same pipeline.Our results show that our approach can significantly improve the quality of a point cloud obtained by structure from motion and increase novel view synthesis quality. In contrast to point growing techniques, our approach can effectively fix large-scale holes and missing thin structures. The rendering quality outperforms state-of-the-art methods, and temporal stability is significantly improved, all while rendering is possible at real-time frame rates.
</details></li>
</ul>
<hr>
<h2 id="General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems"><a href="#General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems" class="headerlink" title="General Framework to Evaluate Unlinkability in Biometric Template Protection Systems"></a>General Framework to Evaluate Unlinkability in Biometric Template Protection Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04633">http://arxiv.org/abs/2311.04633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Gomez-Barrero, Javier Galbally, Christian Rathgeb, Christoph Busch</li>
<li>for: 保护生物特征数据的隐私问题</li>
<li>methods: 提出了一个新的普适框架来评估生物特征模板的不可识别性</li>
<li>results: 应用于四种现有的生物特征模板保护技术中的一种，并与其他现有的指标进行比较，以显示其优势<details>
<summary>Abstract</summary>
The wide deployment of biometric recognition systems in the last two decades has raised privacy concerns regarding the storage and use of biometric data. As a consequence, the ISO/IEC 24745 international standard on biometric information protection has established two main requirements for protecting biometric templates: irreversibility and unlinkability. Numerous efforts have been directed to the development and analysis of irreversible templates. However, there is still no systematic quantitative manner to analyse the unlinkability of such templates. In this paper we address this shortcoming by proposing a new general framework for the evaluation of biometric templates' unlinkability. To illustrate the potential of the approach, it is applied to assess the unlinkability of four state-of-the-art techniques for biometric template protection: biometric salting, Bloom filters, Homomorphic Encryption and block re-mapping. For the last technique, the proposed framework is compared with other existing metrics to show its advantages.
</details>
<details>
<summary>摘要</summary>
在过去二十年中，生物认证系统的广泛应用已引发了隐私问题，特别是 relate to the storage and use of biometric data。为了解决这问题，国际标准ISO/IEC 24745要求保护生物特征模板的两个主要要求是不可逆和不可关联。虽然有很多努力在发展和分析不可逆模板，但是还没有一个系统性的量化方法来分析不可关联性。在这篇论文中，我们正在解决这一缺点，并提出了一个新的通用框架来评估生物特征模板的不可关联性。为了证明我们的方法的潜力，我们应用它来评估四种现状的生物模板保护技术：生物盐、Bloom filter、Homomorphic Encryption和块重映射。对于最后一种技术，我们的框架与其他现有的指标进行比较，以显示它的优势。
</details></li>
</ul>
<hr>
<h2 id="Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes"><a href="#Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes" class="headerlink" title="Image Patch-Matching with Graph-Based Learning in Street Scenes"></a>Image Patch-Matching with Graph-Based Learning in Street Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04617">http://arxiv.org/abs/2311.04617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Wee Peng Tay, Yong Liang Guan, Diego Navarro Navarro, Andreas Hartmannsgruber</li>
<li>for: 这篇论文主要针对自动驾驶中的计算视觉任务，即将实时捕捉的车辆摄像头中的图像与图像库中的特征区域匹配。</li>
<li>methods: 该论文提出了一种基于图像图的空间关系学习模型，其中图像patches的edge表示图像区域之间的空间关系。</li>
<li>results: 该模型在多个街景数据集上进行评估，并取得了领先的匹配结果。<details>
<summary>Abstract</summary>
Matching landmark patches from a real-time image captured by an on-vehicle camera with landmark patches in an image database plays an important role in various computer perception tasks for autonomous driving. Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Matching landmark patches from a real-time image captured by an on-vehicle camera with landmark patches in an image database plays an important role in various computer perception tasks for autonomous driving. Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.")Here's the translation:<<SYS>>匹配从在车载摄像头捕捉的实时图像中提取的标志性补丁与图像库中的标志性补丁之间的匹配对计算机视觉任务中扮演着重要角色。当前方法主要集中于区域关注点的本地匹配，而不考虑图像补丁之间的空间相邻关系，通常对环境中的物体相应。在本文中，我们构建了一个空间图，其顶点对应于补丁，而边则捕捉了图像补丁之间的空间相邻关系。我们提出了一种联合特征和度量学习模型，并基于图形学习。我们提供了对图形学习损失的理论基础，并证明了我们的框架下，匹配后的分布conditioned on matched和unmatched对的信息距离最大。我们使用了多个街景数据集来评估我们的方法，并证明了我们的方法可以达到状态级匹配结果。
</details></li>
</ul>
<hr>
<h2 id="On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology"><a href="#On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology" class="headerlink" title="On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology"></a>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04592">http://arxiv.org/abs/2311.04592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross-caps/dnntopology">https://github.com/cross-caps/dnntopology</a></li>
<li>paper_authors: Suryaka Suresh, Bishshoy Das, Vinayak Abrol, Sumantra Dutta Roy</li>
<li>For: 本研究使用深度学习神经网络（DNN）的层次结构来研究特征表示空间的topologic变化。* Methods: 本研究使用Cubical homology来分析深度神经网络的特征表示空间，并对多种流行的深度架构和实际图像数据进行了扩展分析。* Results: 研究发现，随着深度层数的增加，特征表示空间的topologic复杂度逐渐减少，最终达到最低的Betti数。此外，研究还发现了一些对准变换和数据采样等因素的不变性，这些不变性有助于提高神经网络的泛化能力。<details>
<summary>Abstract</summary>
We study how the topology of feature embedding space changes as it passes through the layers of a well-trained deep neural network (DNN) through Betti numbers. Motivated by existing studies using simplicial complexes on shallow fully connected networks (FCN), we present an extended analysis using Cubical homology instead, with a variety of popular deep architectures and real image datasets. We demonstrate that as depth increases, a topologically complicated dataset is transformed into a simple one, resulting in Betti numbers attaining their lowest possible value. The rate of decay in topological complexity (as a metric) helps quantify the impact of architectural choices on the generalization ability. Interestingly from a representation learning perspective, we highlight several invariances such as topological invariance of (1) an architecture on similar datasets; (2) embedding space of a dataset for architectures of variable depth; (3) embedding space to input resolution/size, and (4) data sub-sampling. In order to further demonstrate the link between expressivity \& the generalization capability of a network, we consider the task of ranking pre-trained models for downstream classification task (transfer learning). Compared to existing approaches, the proposed metric has a better correlation to the actually achievable accuracy via fine-tuning the pre-trained model.
</details>
<details>
<summary>摘要</summary>
我们研究深度神经网络（DNN）中层次结构的变化，通过瓶颈复合体（Cubical homology）进行扩展分析，使用多种流行的深度架构和实际图像数据集。我们发现，随着深度增加，复杂的图像数据集变换成简单的一个，导致Betti数达到最低可能的值。 decay 率可以量化架构选择对通用能力的影响。从表示学学习的视角来看，我们发现了一些对称性，包括：(1) 架构在相似的数据集上的 topological invariance; (2) 数据集的嵌入空间的嵌入空间的 topological invariance; (3) 嵌入空间与输入分辨率/大小的 topological invariance; (4) 数据采样的 topological invariance。为了进一步证明拓扑表达能力和通用能力之间的关系，我们考虑了预训练模型的排名任务（transfer learning）。与现有方法相比，我们的指标具有更好的与实际可 achievable 精度的相关性。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Human-Pose-Estimation-for-Autonomous-Driving-with-3D-Event-Representations"><a href="#Rethinking-Human-Pose-Estimation-for-Autonomous-Driving-with-3D-Event-Representations" class="headerlink" title="Rethinking Human Pose Estimation for Autonomous Driving with 3D Event Representations"></a>Rethinking Human Pose Estimation for Autonomous Driving with 3D Event Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04591">http://arxiv.org/abs/2311.04591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masterhow/eventpointpose">https://github.com/masterhow/eventpointpose</a></li>
<li>paper_authors: Xiaoting Yin, Hao Shi, Jiaan Chen, Ze Wang, Yaozu Ye, Huajian Ni, Kailun Yang, Kaiwei Wang</li>
<li>for: 提高自动驾驶和停车安全性，通过预测人类行为。</li>
<li>methods: 使用事件摄像机，创建3D事件表示，并开发EV-3DPW数据集。</li>
<li>results: 在公共实际世界DHP19数据集上，事件点云技术实现了实时移动预测，而解除事件 voxel方法达到了最高准确性。实验表明我们的提posed 3D表示方法在 traditional RGB图像和事件帧技术的比较中具有更高的总体化能力。<details>
<summary>Abstract</summary>
Human pose estimation is a critical component in autonomous driving and parking, enhancing safety by predicting human actions. Traditional frame-based cameras and videos are commonly applied, yet, they become less reliable in scenarios under high dynamic range or heavy motion blur. In contrast, event cameras offer a robust solution for navigating these challenging contexts. Predominant methodologies incorporate event cameras into learning frameworks by accumulating events into event frames. However, such methods tend to marginalize the intrinsic asynchronous and high temporal resolution characteristics of events. This disregard leads to a loss in essential temporal dimension data, crucial for safety-critical tasks associated with dynamic human activities. To address this issue and to unlock the 3D potential of event information, we introduce two 3D event representations: the Rasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates events within concise temporal slices at identical positions, preserving 3D attributes with statistical cues and markedly mitigating memory and computational demands. Meanwhile, the DEV representation discretizes events into voxels and projects them across three orthogonal planes, utilizing decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore, we develop and release EV-3DPW, a synthetic event-based dataset crafted to facilitate training and quantitative analysis in outdoor scenes. On the public real-world DHP19 dataset, our event point cloud technique excels in real-time mobile predictions, while the decoupled event voxel method achieves the highest accuracy. Experiments reveal our proposed 3D representation methods' superior generalization capacities against traditional RGB images and event frame techniques. Our code and dataset are available at https://github.com/MasterHow/EventPointPose.
</details>
<details>
<summary>摘要</summary>
人体姿态估计是自动驾驶和停车中的关键组件，提高安全性 by 预测人类行为。传统的帧基摄像头和视频通常被应用，但在高动态范围或重重运动模糊的场景下变得不可靠。相比之下，事件摄像头提供了一种可靠的解决方案。大多数方法是将事件摄像头集成到学习框架中，但这些方法通常会忽略事件的本质异步和高时间分辨率特性。这种忽略会导致数据中丢失重要的时间维度信息，这些信息对于安全关键任务相对至关重要。为了解决这个问题并激活事件信息的3D潜力，我们介绍了两种3D事件表示方法：矩阵化事件点云（RasEPC）和解除事件VOXEL（DEV）。 RasEPC将事件按照时间片的方式归并在同一个位置，保留3D特征并减少内存和计算负担。 DEV表示法将事件分解成立方体，并将其投影到三个orthogonal平面上，通过独立事件注意力来捕捉3D准确信息。此外，我们还开发了EV-3DPW Synthetic Event-based Dataset，用于训练和量化分析户外场景。在公共的real-world DHP19数据集上，我们的事件点云技术在实时移动预测中表现出色，而DEV表示法在精度方面达到最高水平。实验表明我们提出的3D表示方法具有传统RGB图像和事件帧技术的更好的总体化能力。我们的代码和数据可以在https://github.com/MasterHow/EventPointPose上获取。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-deepfake-localization-in-diffusion-generated-images"><a href="#Weakly-supervised-deepfake-localization-in-diffusion-generated-images" class="headerlink" title="Weakly-supervised deepfake localization in diffusion-generated images"></a>Weakly-supervised deepfake localization in diffusion-generated images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04584">http://arxiv.org/abs/2311.04584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dragos Tantaru, Elisabeta Oneata, Dan Oneata</li>
<li>for: 这 paper 的目的是提出一种weakly-supervised的 Deepfake detection方法，以便提供更多的信息，包括哪些区域被修改。</li>
<li>methods: 这 paper 使用了 three main categories of methods，包括explanations, local scores 和 attention。这些方法都基于 Xception 网络作为共同背景 Architecure。</li>
<li>results: 这 paper 的结果表明，weakly-supervised localization 是可能的，并且使用 local scores 方法可以更加敏感于缺乏超级vision。<details>
<summary>Abstract</summary>
The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a "fake" or "real" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator.
</details>
<details>
<summary>摘要</summary>
“denoising diffusion模型的卓越生成能力已经引起了互联网上每天看到的图像的真实性的新问题。然而，现有的深伪检测模型都是基于前一代生成方法（如GAN）进行测试，通常只提供每个图像的“伪”或“真”标签。我们认为，更有用的输出将是在每个图像上添加一个涉及到输入图像的涉及级别的地图，以便更好地了解图像中哪些部分被修改。为了实现这一目标，我们将这个任务视为一个弱有监督的地方化任务，并将涉及到的方法分为三大类（基于解释、本地分数或注意力）。我们使用Xception网络作为共同背景架构，并进行了仔细的分析，包括方法选择、监督类型、数据集和生成器在内的所有主要因素。我们的研究表明，弱有监督的地方化是可行的，最高性能的检测方法（基于本地分数）在监督不充分的情况下比 DATASET或生成器的不一致性更加敏感。”
</details></li>
</ul>
<hr>
<h2 id="A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations"><a href="#A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations" class="headerlink" title="A 3D generative model of pathological multi-modal MR images and segmentations"></a>A 3D generative model of pathological multi-modal MR images and segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04552">http://arxiv.org/abs/2311.04552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/virginiafdez/brainspade3d_rel">https://github.com/virginiafdez/brainspade3d_rel</a></li>
<li>paper_authors: Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Mark S. Graham, Tom Vercauteren, M. Jorge Cardoso</li>
<li>for: 本研究旨在提供一种用于脑MRI和相关分割的三维生成模型，以便 condition on 特定的疾病现象和对比。</li>
<li>methods: 本研究使用了生成对抗网络（GANs）和扩散模型（DMs）来生成高质量的Synthetic MRI和相关分割数据，并允许用户根据特定的疾病现象和对比来控制生成的图像和分割结果。</li>
<li>results: 研究表明，brainSPADE3D可以生成高度具有一致性的Synthetic MRI和相关分割数据，并且可以结合不同的疾病现象来生成混合的图像和分割结果。此外，研究还发现，使用brainSPADE3D可以改善预测模型在不期望的疾病存在时的性能。<details>
<summary>Abstract</summary>
Generative modelling and synthetic data can be a surrogate for real medical imaging datasets, whose scarcity and difficulty to share can be a nuisance when delivering accurate deep learning models for healthcare applications. In recent years, there has been an increased interest in using these models for data augmentation and synthetic data sharing, using architectures such as generative adversarial networks (GANs) or diffusion models (DMs). Nonetheless, the application of synthetic data to tasks such as 3D magnetic resonance imaging (MRI) segmentation remains limited due to the lack of labels associated with the generated images. Moreover, many of the proposed generative MRI models lack the ability to generate arbitrary modalities due to the absence of explicit contrast conditioning. These limitations prevent the user from adjusting the contrast and content of the images and obtaining more generalisable data for training task-specific models. In this work, we propose brainSPADE3D, a 3D generative model for brain MRI and associated segmentations, where the user can condition on specific pathological phenotypes and contrasts. The proposed joint imaging-segmentation generative model is shown to generate high-fidelity synthetic images and associated segmentations, with the ability to combine pathologies. We demonstrate how the model can alleviate issues with segmentation model performance when unexpected pathologies are present in the data.
</details>
<details>
<summary>摘要</summary>
生成模型和人工数据可以作为真实医学影像数据的代理，解决医疗应用中深度学习模型的准确性问题。在最近几年里，有越来越多的人关注使用这些模型进行数据增强和人工数据共享，使用生成敌方网络（GANs）或扩散模型（DMs）。然而，使用生成数据进行3D磁共振成像（MRI）分割 task 仍然受到生成图像无标签的限制，以及生成模型无法生成多种模式的缺乏能力。这些限制使得用户无法调整图像的对比度和内容，从而获得更普适的数据用于训练任务特定模型。在这项工作中，我们提出了brainSPADE3D，一种3D生成模型用于脑MRI和相关的分割。用户可以根据特定的疾病现象和对比来conditioning这些模型。我们展示了该模型可以生成高 fideli ty的人工图像和相关的分割，并能够组合疾病。我们还示出了该模型可以解决预期疾病存在于数据中时，分割模型性能下降的问题。
</details></li>
</ul>
<hr>
<h2 id="Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images"><a href="#Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images" class="headerlink" title="Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images"></a>Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04521">http://arxiv.org/abs/2311.04521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Jain, Suryansh Kumar, Luc Van Gool</li>
<li>for: 这 paper 是用于解决计算机视觉中的神经图像基于渲染问题，即在给定一组由自由移动相机拍摄的图像时，在测试时使用神经网络synthesize场景图像。</li>
<li>methods: 该 paper 使用了以下方法：（i）通过一个可靠的渠道来重建高精度相机参数，以便在神经新视角synthesize过程中更加准确地模拟场景图像。（ii）在day-to-day不 pose的图像中，模型对象内容的多resolution采用，以适应高速运动的相机。</li>
<li>results: 该 paper 通过实验表明，在不考虑 Camera pose 估计精度的情况下，模型多scale neural scene representation可以是Counterproductive。而在具有准确的相机pose估计的场景表示框架中，可以准确地synthesize图像。<details>
<summary>Abstract</summary>
We introduce an improved solution to the neural image-based rendering problem in computer vision. Given a set of images taken from a freely moving camera at train time, the proposed approach could synthesize a realistic image of the scene from a novel viewpoint at test time. The key ideas presented in this paper are (i) Recovering accurate camera parameters via a robust pipeline from unposed day-to-day images is equally crucial in neural novel view synthesis problem; (ii) It is rather more practical to model object's content at different resolutions since dramatic camera motion is highly likely in day-to-day unposed images. To incorporate the key ideas, we leverage the fundamentals of scene rigidity, multi-scale neural scene representation, and single-image depth prediction. Concretely, the proposed approach makes the camera parameters as learnable in a neural fields-based modeling framework. By assuming per view depth prediction is given up to scale, we constrain the relative pose between successive frames. From the relative poses, absolute camera pose estimation is modeled via a graph-neural network-based multiple motion averaging within the multi-scale neural-fields network, leading to a single loss function. Optimizing the introduced loss function provides camera intrinsic, extrinsic, and image rendering from unposed images. We demonstrate, with examples, that for a unified framework to accurately model multiscale neural scene representation from day-to-day acquired unposed multi-view images, it is equally essential to have precise camera-pose estimates within the scene representation framework. Without considering robustness measures in the camera pose estimation pipeline, modeling for multi-scale aliasing artifacts can be counterproductive. We present extensive experiments on several benchmark datasets to demonstrate the suitability of our approach.
</details>
<details>
<summary>摘要</summary>
我们介绍一个改进了的解决方案，用于计算机视觉中的神经图像基于测试项目。假设我们有一组由自由移动摄像机拍摄的图像，我们的方法可以将这些图像转换为具有真实感的图像，并且在测试时间点上实现不同的观察角度。我们的关键想法包括：1. 从日常生活中的不条理图像中获取精确的摄像机参数，这是神经novel view synthesis问题的重要前提。2. 因为日常生活中的图像可能会受到剧烈的摄像机运动，因此需要在不同的解析率上模型物体内容。为了实现这些想法，我们利用了场景的刚性、多尺度神经场景表示和单图像深度预测的基础知识。具体来说，我们将摄像机参数设置为神经场中的学习型态。通过假设每个视角深度预测是固定的，我们将相关的视角之间的相对位置组成一个多尺度神经网络中的多动作平均，从而得到一个单一的损失函数。通过优化这个损失函数，我们可以获得摄像机参数、摄像机内部和图像输出等。我们在多个 benchmark 数据集上进行了广泛的实验，证明了我们的方法适用于实现多尺度神经场景表示。而不具备稳定性测量的摄像机参数估计在神经场景表示框架中是Equally essential。如果不考虑稳定性测量，则模型多尺度杂质噪压可能会是Counterproductive。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discriminative-Features-for-Crowd-Counting"><a href="#Learning-Discriminative-Features-for-Crowd-Counting" class="headerlink" title="Learning Discriminative Features for Crowd Counting"></a>Learning Discriminative Features for Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04509">http://arxiv.org/abs/2311.04509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuehai Chen</li>
<li>For: 提高人群计数模型在高度拥挤的区域中的准确性，特别是在人群中的小对象和背景之间的区分。* Methods: 提出了一种学习权重特征框架，包括遮盖特征预测模块（MPM）和监督像素级异常学习模块（CLM），以提高模型在高度拥挤区域中的局部化和对比背景的能力。* Results: 模型在各种计算机视觉任务中，如人群计数和物体检测，在拥挤环境下提高了地面的准确性。<details>
<summary>Abstract</summary>
Crowd counting models in highly congested areas confront two main challenges: weak localization ability and difficulty in differentiating between foreground and background, leading to inaccurate estimations. The reason is that objects in highly congested areas are normally small and high-level features extracted by convolutional neural networks are less discriminative to represent small objects. To address these problems, we propose a learning discriminative features framework for crowd counting, which is composed of a masked feature prediction module (MPM) and a supervised pixel-level contrastive learning module (CLM). The MPM randomly masks feature vectors in the feature map and then reconstructs them, allowing the model to learn about what is present in the masked regions and improving the model's ability to localize objects in high-density regions. The CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background. Additionally, the proposed modules can be beneficial in various computer vision tasks, such as crowd counting and object detection, where dense scenes or cluttered environments pose challenges to accurate localization. The proposed two modules are plug-and-play, incorporating the proposed modules into existing models can potentially boost their performance in these scenarios.
</details>
<details>
<summary>摘要</summary>
群体计数模型在高度拥堵的区域面临两大挑战：一是地方化能力弱和Difficulty in differentiating between foreground and background，导致估计不准确。这是因为在高度拥堵的区域中的对象通常是小型，高级特征提取网络的特征更难以区分小对象。为解决这些问题，我们提议一种学习特征分类框架 для群体计数，该框架包括带mask的特征预测模块（MPM）和supervised像素级冲突学习模块（CLM）。MPM randomly masks特征向量在特征图中，然后重建它们，使模型能够学习masked regions中的内容，提高对象的定位能力。CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background。此外，提议的两个模块可以在多种计算机视觉任务中有效，如人群计数和物体检测， где高度拥堵的环境或拥堵的环境会对准确的定位 pose challenges。提议的两个模块可以plug-and-play，将其 integrate into existing models可能会提高其性能在这些场景中。
</details></li>
</ul>
<hr>
<h2 id="NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction"><a href="#NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction" class="headerlink" title="NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction"></a>NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04505">http://arxiv.org/abs/2311.04505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thorsten Hempel, Magnus Jung, Ahmed A. Abdelrahman, Ayoub Al-Hamadi</li>
<li>for: The paper is written for advancing ego-vision-based eye contact research, specifically in the fields of computer vision, human-computer interaction, and social robotics.</li>
<li>methods: The paper presents a hand-annotated eye contact dataset called NITEC, which exceeds existing datasets in size and variety of demographics, social contexts, and lighting conditions.</li>
<li>results: The paper demonstrates strong cross-dataset performance of NITEC, emphasizing its effectiveness and adaptability in various scenarios, and makes the dataset publicly available for further exploration and reproducibility.<details>
<summary>Abstract</summary>
Eye contact is a crucial non-verbal interaction modality and plays an important role in our everyday social life. While humans are very sensitive to eye contact, the capabilities of machines to capture a person's gaze are still mediocre. We tackle this challenge and present NITEC, a hand-annotated eye contact dataset for ego-vision interaction. NITEC exceeds existing datasets for ego-vision eye contact in size and variety of demographics, social contexts, and lighting conditions, making it a valuable resource for advancing ego-vision-based eye contact research. Our extensive evaluations on NITEC demonstrate strong cross-dataset performance, emphasizing its effectiveness and adaptability in various scenarios, that allows seamless utilization to the fields of computer vision, human-computer interaction, and social robotics. We make our NITEC dataset publicly available to foster reproducibility and further exploration in the field of ego-vision interaction. https://github.com/thohemp/nitec
</details>
<details>
<summary>摘要</summary>
眼接触是非常重要的非语言交互方式，在我们每天的社交生活中扮演着重要的角色。然而，机器人的眼接触捕捉能力仍然很差。我们解决这个挑战，并提供了 NITEC，一个手动标注的眼接触数据集 для egovision交互。NITEC 的大小和多样性都超过了现有的 egovision 眼接触数据集，包括不同的人种、社会背景和照明条件，这使得它成为了 egovision 研究中的一个非常有价值的资源。我们对 NITEC 进行了广泛的评估，并证明了它在多个场景中的强大横跨数据集表现，表明它在计算机视觉、人机交互和社交机器人等领域可以无缝应用。我们将 NITEC 数据集公开提供，以便重现和进一步探索 egovision 交互领域。更多信息请参考 https://github.com/thohemp/nitec。
</details></li>
</ul>
<hr>
<h2 id="PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds"><a href="#PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds" class="headerlink" title="PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds"></a>PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04501">http://arxiv.org/abs/2311.04501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yang, Haiyang Wang, Di Dai, Liwei Wang</li>
<li>for: The paper is written for outdoor point cloud pre-training, addressing the issue of incompleteness in point clouds and incorporating images for improved performance.</li>
<li>methods: The paper proposes a novel image-assisted pre-training framework called PRED, which uses a Birds-Eye-View feature map conditioned semantic rendering and point-wise masking with a high mask ratio (95%) to enhance the model’s performance.</li>
<li>results: The paper demonstrates the superiority of PRED over prior point cloud pre-training methods, achieving significant improvements on various large-scale datasets for 3D perception tasks.<details>
<summary>Abstract</summary>
Pre-training is crucial in 3D-related fields such as autonomous driving where point cloud annotation is costly and challenging. Many recent studies on point cloud pre-training, however, have overlooked the issue of incompleteness, where only a fraction of the points are captured by LiDAR, leading to ambiguity during the training phase. On the other hand, images offer more comprehensive information and richer semantics that can bolster point cloud encoders in addressing the incompleteness issue inherent in point clouds. Yet, incorporating images into point cloud pre-training presents its own challenges due to occlusions, potentially causing misalignments between points and pixels. In this work, we propose PRED, a novel image-assisted pre-training framework for outdoor point clouds in an occlusion-aware manner. The main ingredient of our framework is a Birds-Eye-View (BEV) feature map conditioned semantic rendering, leveraging the semantics of images for supervision through neural rendering. We further enhance our model's performance by incorporating point-wise masking with a high mask ratio (95%). Extensive experiments demonstrate PRED's superiority over prior point cloud pre-training methods, providing significant improvements on various large-scale datasets for 3D perception tasks. Codes will be available at https://github.com/PRED4pc/PRED.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>在3D相关领域，如自动驾驶，前期训练是非常重要的。然而，许多最近的点云预训练研究却忽视了点云的不完整性问题，导致训练阶段的模糊性。相比之下，图像具有更广泛的信息和更丰富的 semantics，可以增强点云编码器对点云不完整性的应对。然而，将图像与点云进行结合存在其自身的挑战，因为可能存在遮挡，导致点云和像素之间的不一致。在这个工作中，我们提出了一种新的图像助け预训练框架，称为PRED（图像协助预训练）。我们的框架的主要组成部分是基于 bird's eye view（BEV）的Semantic Feature Map Conditioned Neural Rendering，利用图像semantics来为预训练提供超vision。此外，我们还增强了我们的模型性能，通过点wise掩蔽（mask ratio为95%）。广泛的实验证明PRED的优越性，在多个大规模数据集上提供了3D感知任务的显著改进。代码将在https://github.com/PRED4pc/PRED上提供。
</details></li>
</ul>
<hr>
<h2 id="PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders"><a href="#PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders" class="headerlink" title="PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders"></a>PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04496">http://arxiv.org/abs/2311.04496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hezhen Hu, Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Lu Yuan, Dong Chen, Houqiang Li</li>
<li>for: This paper is written for the task of Person Re-identification (ReID), specifically to learn generic feature representation for this task.</li>
<li>methods: The paper proposes a simple yet effective pre-training framework called PersonMAE, which involves two core designs in masked autoencoders to better serve the task of Person Re-ID. The framework generates two regions from the given image, corrupts one region with block-wise masking to mimic common occlusion in ReID, and then predicts the whole other region at both pixel level and semantic feature level.</li>
<li>results: The paper achieves state-of-the-art performance on four downstream ReID tasks, including supervised (holistic and occluded setting), and unsupervised (UDA and USL setting). Specifically, with the ViT-B backbone, the paper achieves 79.8% and 69.5% mAP on the MSMT17 and OccDuke datasets, respectively, surpassing the previous state-of-the-art by a large margin of +8.0 mAP and +5.3 mAP, respectively.<details>
<summary>Abstract</summary>
Pre-training is playing an increasingly important role in learning generic feature representation for Person Re-identification (ReID). We argue that a high-quality ReID representation should have three properties, namely, multi-level awareness, occlusion robustness, and cross-region invariance. To this end, we propose a simple yet effective pre-training framework, namely PersonMAE, which involves two core designs into masked autoencoders to better serve the task of Person Re-ID. 1) PersonMAE generates two regions from the given image with RegionA as the input and \textit{RegionB} as the prediction target. RegionA is corrupted with block-wise masking to mimic common occlusion in ReID and its remaining visible parts are fed into the encoder. 2) Then PersonMAE aims to predict the whole RegionB at both pixel level and semantic feature level. It encourages its pre-trained feature representations with the three properties mentioned above. These properties make PersonMAE compatible with downstream Person ReID tasks, leading to state-of-the-art performance on four downstream ReID tasks, i.e., supervised (holistic and occluded setting), and unsupervised (UDA and USL setting). Notably, on the commonly adopted supervised setting, PersonMAE with ViT-B backbone achieves 79.8% and 69.5% mAP on the MSMT17 and OccDuke datasets, surpassing the previous state-of-the-art by a large margin of +8.0 mAP, and +5.3 mAP, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT预训练在人识别（ReID）中扮演着日益重要的角色，我们认为高质量的ReID表示应具有三种性能，即多级意识、遮挡Robustness和跨地域一致性。为此，我们提出了一个简单 yet有效的预训练框架，即PersonMAE，该框架包括两个核心设计：1. PersonMAE将给定图像分成两个区域， RegionA 作为输入，RegionB 作为预测目标。 RegionA 会被块性遮盖，以模拟ReID中常见的遮挡，其可见部分会被编码器处理。2. PersonMAE会 endeavour 预测 RegionB 的整个像素级和 semantic feature 级。这种设计使得 PersonMAE 的预训练特征表示具有上述三种性能，这些性能使得 PersonMAE 与下游 ReID 任务相匹配，导致 PersonMAE 在四个下游 ReID 任务中 achieved  state-of-the-art 性能，包括超级vised（整体和 occluded 设置）和无监督（UDA 和 USL 设置）。特别是，在通常采用的超级vised 设置下，PersonMAE  WITH ViT-B 基础模型 achieved 79.8% 和 69.5% mAP 在 MSMT17 和 OccDuke 数据集上，比前一个 state-of-the-art 的margin 加大了 +8.0 mAP，+5.3 mAP，分别。>>
</details></li>
</ul>
<hr>
<h2 id="Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior"><a href="#Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior" class="headerlink" title="Non-Rigid Shape Registration via Deep Functional Maps Prior"></a>Non-Rigid Shape Registration via Deep Functional Maps Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04494">http://arxiv.org/abs/2311.04494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rqhuang88/DFR">https://github.com/rqhuang88/DFR</a></li>
<li>paper_authors: Puhua Jiang, Mingze Sun, Ruqi Huang</li>
<li>for: 非RIGID shape registration without correspondence supervision</li>
<li>methods: 使用学习基于的框架，通过高维空间映射学习得到非RIGID shape registration</li>
<li>results: 可以处理大幅度内在变换和外在变换的shape registration，并且可以提供高质量的对应关系 between 不同形状对Here’s a more detailed explanation of each point:1. for: The paper proposes a learning-based framework for non-rigid shape registration without correspondence supervision. This means that the framework does not rely on manually specified correspondences between shapes, but instead uses learning-based methods to establish these correspondences.2. methods: The framework uses a combination of high-dimensional embedding and deep functional maps (DFM) to establish correspondences between shapes. The high-dimensional embedding maps shapes into a high-dimensional space where they are easier to align, and the DFM learns a non-linear mapping between the shapes. The correspondences are dynamically updated based on the intermediate registrations and filtered by a consistency prior, which makes the pipeline more robust.3. results: The paper demonstrates that the proposed framework achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching, and can handle both significant extrinsic and intrinsic deformations. The framework is also able to provide high-quality correspondences between unseen challenging shape pairs, which is not possible with traditional registration methods or intrinsic methods.<details>
<summary>Abstract</summary>
In this paper, we propose a learning-based framework for non-rigid shape registration without correspondence supervision. Traditional shape registration techniques typically rely on correspondences induced by extrinsic proximity, therefore can fail in the presence of large intrinsic deformations. Spectral mapping methods overcome this challenge by embedding shapes into, geometric or learned, high-dimensional spaces, where shapes are easier to align. However, due to the dependency on abstract, non-linear embedding schemes, the latter can be vulnerable with respect to perturbed or alien input. In light of this, our framework takes the best of both worlds. Namely, we deform source mesh towards the target point cloud, guided by correspondences induced by high-dimensional embeddings learned from deep functional maps (DFM). In particular, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Moreover, in order to alleviate the requirement of extrinsically aligned input, we train an orientation regressor on a set of aligned synthetic shapes independent of the training shapes for DFM. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work. The code is available at https://github.com/rqhuang88/DFR.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种学习基于的非定制形状匹配框架，不需要对匹配得到超vision。传统的形状匹配技术通常靠 extrinsic  proximity 引起的匹配，因此在大规模内部扭变的情况下失败。spectral mapping 方法可以将形状嵌入高维空间中，使形状更容易匹配。然而，由于依赖于抽象的非线性嵌入方案，后者可能对输入数据进行敏感操作。为了解决这个问题，我们的框架结合了两者的优点。具体来说，我们将源网格弯曲到目标点云，以高维空间中学习的深度函数映射（DFM）中的匹配为导向。特别是，匹配在中间registrations 更新和consistency prior 的筛选下进行动态更新，以提高整体的稳定性。此外，为了避免外部对齐的需求，我们在独立于训练形状的synthetic shapes 上训练了一个旋转回归器。实验结果表明，只需几十个有限的训练形状，我们的管道可以在多个非定制点云匹配 benchmark 上达到领先的Result，并且能够在未看到的挑战性形状对应中提供高质量的匹配。代码可以在 https://github.com/rqhuang88/DFR 上获取。
</details></li>
</ul>
<hr>
<h2 id="All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing"><a href="#All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing" class="headerlink" title="All-Optical Phase Conjugation Using Diffractive Wavefront Processing"></a>All-Optical Phase Conjugation Using Diffractive Wavefront Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04473">http://arxiv.org/abs/2311.04473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che-Yung Shen, Jingxi Li, Tianyi Gan, Mona Jarrahi, Aydogan Ozcan</li>
<li>for: 用于Counteracting wavefront distortions，包括Imaging和Beam focusing。</li>
<li>methods: 使用Deep learning优化Passive diffractive layers，实现All-optical phase conjugation操作。</li>
<li>results: 通过实验验证，Diffractive wavefront processor可以成功地对phas aberrations进行OPC操作，并且可以在不同的电磁波谱中实现Cost-effective wavefront engineering解决方案。<details>
<summary>Abstract</summary>
Optical phase conjugation (OPC) is a nonlinear technique used for counteracting wavefront distortions, with various applications ranging from imaging to beam focusing. Here, we present the design of a diffractive wavefront processor to approximate all-optical phase conjugation operation for input fields with phase aberrations. Leveraging deep learning, a set of passive diffractive layers was optimized to all-optically process an arbitrary phase-aberrated coherent field from an input aperture, producing an output field with a phase distribution that is the conjugate of the input wave. We experimentally validated the efficacy of this wavefront processor by 3D fabricating diffractive layers trained using deep learning and performing OPC on phase distortions never seen by the diffractive processor during its training. Employing terahertz radiation, our physical diffractive processor successfully performed the OPC task through a shallow spatially-engineered volume that axially spans tens of wavelengths. In addition to this transmissive OPC configuration, we also created a diffractive phase-conjugate mirror by combining deep learning-optimized diffractive layers with a standard mirror. Given its compact, passive and scalable nature, our diffractive wavefront processor can be used for diverse OPC-related applications, e.g., turbidity suppression and aberration correction, and is also adaptable to different parts of the electromagnetic spectrum, especially those where cost-effective wavefront engineering solutions do not exist.
</details>
<details>
<summary>摘要</summary>
光学相 conjugation（OPC）是一种非线性技术，用于对波front distortions 进行矫正，具有各种应用，从图像到波front фокусировки。在这里，我们提出了一种使用 diffractive wavefront processor 来近似 all-optical phase conjugation 操作，用于处理具有相位偏移的输入场。通过深度学习，我们分配了一组 passive diffractive layers 来对输入场进行 all-optical 处理，以生成一个输出场的相位分布，与输入场的相位分布是相似的。我们通过实验验证了这种 wavefront processor 的有效性，使用了 deep learning 进行优化的 diffractive layers 和标准镜的组合，以实现 OPC 任务。使用 terahertz 辐射，我们的物理 diffractive processor 成功完成了 OPC 任务，并在一个 axially 延伸多达数十波长的束缚空间中完成了 shallow 的 SPATIALLY-ENGINEERED 体。此外，我们还创造了一个 diffractive phase-conjugate mirror，通过将 deep learning 优化的 diffractive layers 与标准镜相结合。由于它的 компакт、被动和可扩展性，我们的 diffractive wavefront processor 可以用于多种 OPC-相关的应用，例如浊度补做和偏移补做，并可以适应不同的电磁波谱спектrum，特别是那些没有cost-effective wavefront engineering 解决方案。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning"><a href="#Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning" class="headerlink" title="Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning"></a>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04464">http://arxiv.org/abs/2311.04464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Zhu, Yuefeng Chen, Wei Wang, Xiaofeng Mao, Xiu Yan, Yue Wang, Zhigang Li, Wang lu, Jindong Wang, Xiangyang Ji</li>
<li>for: 本研究的目的是提高深度神经网络在具有限制样本数的低资源场景中表现，通过修改CLIP预训练模型的特定部分来适应不同的少shot任务。</li>
<li>methods: 我们修改了CLIP预训练模型的视觉编码器中的特征权重卷积层，使其在不同的少shot任务中适应不同的 semantics。在训练过程中，我们根据任务特点调整了这些权重，以便模型能够更好地适应具体的任务。在测试阶段，我们使用了差分融合来结合原始的权重卷积层和调整后的权重卷积层，以便将它们两者的知识融合在一起。</li>
<li>results: 我们的方法可以增强传统的少shot CLIP，并且与现有的adapter方法（SAFE-A）兼容。我们的方法可以更好地适应不同的少shot任务，并且在测试阶段的性能得到了提升。<details>
<summary>Abstract</summary>
Learning generalized representations from limited training samples is crucial for applying deep neural networks in low-resource scenarios. Recently, methods based on Contrastive Language-Image Pre-training (CLIP) have exhibited promising performance in few-shot adaptation tasks. To avoid catastrophic forgetting and overfitting caused by few-shot fine-tuning, existing works usually freeze the parameters of CLIP pre-trained on large-scale datasets, overlooking the possibility that some parameters might not be suitable for downstream tasks. To this end, we revisit CLIP's visual encoder with a specific focus on its distinctive attention pooling layer, which performs a spatial weighted-sum of the dense feature maps. Given that dense feature maps contain meaningful semantic information, and different semantics hold varying importance for diverse downstream tasks (such as prioritizing semantics like ears and eyes in pet classification tasks rather than side mirrors), using the same weighted-sum operation for dense features across different few-shot tasks might not be appropriate. Hence, we propose fine-tuning the parameters of the attention pooling layer during the training process to encourage the model to focus on task-specific semantics. In the inference process, we perform residual blending between the features pooled by the fine-tuned and the original attention pooling layers to incorporate both the few-shot knowledge and the pre-trained CLIP's prior knowledge. We term this method as Semantic-Aware FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot CLIP and is compatible with the existing adapter approach (termed SAFE-A).
</details>
<details>
<summary>摘要</summary>
学习通用表示法从有限的训练样本中学习是深度神经网络在低资源场景中应用的关键。最近，基于对比语言图像预训练（CLIP）的方法在几架适应任务中表现出色。为了避免几架适应过拟合和忘记，现有的工作通常将CLIP预训练在大规模数据集上的参数冻结，忽略了可能一些参数不适合下游任务。为此，我们重新审视CLIP的视觉Encoder，尤其是其独特的注意力池化层，该层通过在权重总和中进行空间权重的积分来实现。由于稠密特征图包含有意义的语义信息，而不同语义在不同下游任务中具有不同的重要性（例如在宠物分类任务中更重视耳朵和眼睛而非侧镜），因此在不同几架适应任务中使用同样的权重积分操作可能不合适。因此，我们提议在训练过程中细化注意力池化层的参数，以便让模型强调任务特有的语义。在推理过程中，我们通过将细化后的注意力池化层和原始注意力池化层的特征进行差分融合来 incorporate 两者的知识。我们称这种方法为 Semantic-Aware FinE-tuning（SAFE）。SAFE 有效地提高了传统的几架 CLIP，并且与现有的适配器方法（称为 SAFE-A）兼容。
</details></li>
</ul>
<hr>
<h2 id="Retargeting-video-with-an-end-to-end-framework"><a href="#Retargeting-video-with-an-end-to-end-framework" class="headerlink" title="Retargeting video with an end-to-end framework"></a>Retargeting video with an end-to-end framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04458">http://arxiv.org/abs/2311.04458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi-Ngoc-Hanh Le, HuiGuang Huang, Yi-Ru Chen, Tong-Yee Lee</li>
<li>For: 这个研究旨在为 Computer Graphics 应用程序提供影片重定向功能，以增强用户观赏体验。* Methods: 本研究使用了一个终端到终端的 RETVI 方法，具有两个模组：内容特征分析器 (CFA) 和适应型扩展估计器 (ADE)，以解决旧有方法的计算瓶颈和限制。* Results: 实验和评估结果显示，我们的系统在质量和运行时间上具有明显的优势，超越了先前的工作。更多结果可以在 <a target="_blank" rel="noopener" href="http://graphics.csie.ncku.edu.tw/RETVI">http://graphics.csie.ncku.edu.tw/RETVI</a> 网站上获取。<details>
<summary>Abstract</summary>
Video holds significance in computer graphics applications. Because of the heterogeneous of digital devices, retargeting videos becomes an essential function to enhance user viewing experience in such applications. In the research of video retargeting, preserving the relevant visual content in videos, avoiding flicking, and processing time are the vital challenges. Extending image retargeting techniques to the video domain is challenging due to the high running time. Prior work of video retargeting mainly utilizes time-consuming preprocessing to analyze frames. Plus, being tolerant of different video content, avoiding important objects from shrinking, and the ability to play with arbitrary ratios are the limitations that need to be resolved in these systems requiring investigation. In this paper, we present an end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We eliminate the computational bottleneck in the conventional approaches by designing RETVI with two modules, content feature analyzer (CFA) and adaptive deforming estimator (ADE). The extensive experiments and evaluations show that our system outperforms previous work in quality and running time. Visit our project website for more results at http://graphics.csie.ncku.edu.tw/RETVI.
</details>
<details>
<summary>摘要</summary>
视频具有计算机图形应用中的重要意义。由于数字设备的多样性，对视频进行重定向变得非常重要，以提高用户视觉体验。在研究视频重定向方面，保持视频中相关的视觉内容，避免抖动、处理时间和视频内容的多样性是核心挑战。由于视频重定向技术的运行时间较长，将图像重定向技术应用于视频领域是挑战。现有的视频重定向方法主要通过时间consuming的预处理分析帧来解决这些挑战。此外，保持重要对象不减小、避免抖动和处理时间也是需要解决的问题。在这篇论文中，我们提出了一种终端到终端的视频重定向方法（RETVI），以解决以上问题。我们通过设计内容特征分析器（CFA）和自适应扭转估计器（ADE）两个模块来消除传统方法的计算瓶颈。我们的系统在质量和运行时间方面胜过先前的工作。更多结果可以在我们项目网站上找到：http://graphics.csie.ncku.edu.tw/RETVI。
</details></li>
</ul>
<hr>
<h2 id="SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification"><a href="#SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification" class="headerlink" title="SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification"></a>SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04442">http://arxiv.org/abs/2311.04442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Lin, Feng Gao, Xiaocheng Shi, Junyu Dong, Qian Du</li>
<li>for: 这个研究旨在提出一种基于自动编码器的隐藏特征模型（SS-MAE），用于混合多源数据的类别 tasks。</li>
<li>methods: 该模型包括空间对应分支和спектраль对应分支，具体来说，空间对应分支随机填充patches，并从原始数据中恢复缺失的像素；而 спектраль对应分支随机填充频道，并从原始数据中恢复缺失的频道。</li>
<li>results: 实验结果显示，Compared with多个基于state-of-the-art的基eline，SS-MAE在三个公开的数据集上表现出色，并且可以充分利用输入数据的空间和спектраль特征。<details>
<summary>Abstract</summary>
Masked image modeling (MIM) is a highly popular and effective self-supervised learning method for image understanding. Existing MIM-based methods mostly focus on spatial feature modeling, neglecting spectral feature modeling. Meanwhile, existing MIM-based methods use Transformer for feature extraction, some local or high-frequency information may get lost. To this end, we propose a spatial-spectral masked auto-encoder (SS-MAE) for HSI and LiDAR/SAR data joint classification. Specifically, SS-MAE consists of a spatial-wise branch and a spectral-wise branch. The spatial-wise branch masks random patches and reconstructs missing pixels, while the spectral-wise branch masks random spectral channels and reconstructs missing channels. Our SS-MAE fully exploits the spatial and spectral representations of the input data. Furthermore, to complement local features in the training stage, we add two lightweight CNNs for feature extraction. Both global and local features are taken into account for feature modeling. To demonstrate the effectiveness of the proposed SS-MAE, we conduct extensive experiments on three publicly available datasets. Extensive experiments on three multi-source datasets verify the superiority of our SS-MAE compared with several state-of-the-art baselines. The source codes are available at \url{https://github.com/summitgao/SS-MAE}.
</details>
<details>
<summary>摘要</summary>
高清晰自适应模型（MIM）是一种非常受欢迎且有效的无监督学习方法，用于图像理解。现有的MIM基于方法主要关注空间特征模型化，忽略spectral特征模型化。另外，现有的MIM基于方法使用Transformer来EXTRACT特征，可能会导致一些本地或高频信息丢失。为了解决这个问题，我们提议一种具有空间特征和spectral特征的掩码自适应编码器（SS-MAE），用于混合高光谱和LiDAR/SAR数据的分类。具体来说，SS-MAE包括一个空间特征分支和一个spectral特征分支。空间特征分支掩码随机质点并重建缺失像素，而spectral特征分支掩码随机spectral通道并重建缺失通道。我们的SS-MAE完全利用输入数据的空间和spectral表示。此外，为了补偿本地特征在训练阶段的不足，我们添加了两个轻量级CNN来EXTRACT特征。我们的方法同时利用全球特征和本地特征来模型特征。为了证明我们提议的SS-MAE的效果，我们在三个公共可用的数据集上进行了广泛的实验。我们的实验结果表明，SS-MAE在多源数据集上的表现明显超过了一些状态的基eline。我们的代码可以在github上找到：https://github.com/summitgao/SS-MAE。
</details></li>
</ul>
<hr>
<h2 id="Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression"><a href="#Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression" class="headerlink" title="Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression"></a>Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04430">http://arxiv.org/abs/2311.04430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawit Mureja Argaw, Junsik Kim, In So Kweon</li>
<li>for: 本研究旨在提高视频压缩（VC）方法的 universality，使其在不同的时间前提下能够维持视频质量。</li>
<li>methods: 本研究使用了一种基于可让渡的最小化最大化优化方法，通过利用视频压缩和图像增强之间的自然质量补做，提高视频质量。</li>
<li>results: 对多个标准数据集进行了广泛的实验，证明了我们的方法在比较于现有的VC方法之上具有更高的效果。<details>
<summary>Abstract</summary>
Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches.
</details>
<details>
<summary>摘要</summary>
现有的视频压缩（VC）方法主要目标是减少视频中的空间和时间重复性，以保持视频质量。在这种情况下，先前的工作已经实现了在特定的曝光时间和闭合速度下拍摄的视频中获得了出色的结果。然而，当这些方法应用于不同的时间优先顺序下拍摄的视频时，它们无法保持视频内容的质量。在这种情况下，我们解决了视频压缩问题，充分利用了视频增强和数据压缩之间的自然负荷关系，并提出了一种有效的框架和训练策略。对多个标准数据集进行了广泛的实验，证明了我们的方法与许多现有的VC方法相比，有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="CSAM-A-2-5D-Cross-Slice-Attention-Module-for-Anisotropic-Volumetric-Medical-Image-Segmentation"><a href="#CSAM-A-2-5D-Cross-Slice-Attention-Module-for-Anisotropic-Volumetric-Medical-Image-Segmentation" class="headerlink" title="CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation"></a>CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04942">http://arxiv.org/abs/2311.04942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/al3x-o-o-hung/csam">https://github.com/al3x-o-o-hung/csam</a></li>
<li>paper_authors: Alex Ling Yu Hung, Haoxin Zheng, Kai Zhao, Xiaoxi Du, Kaifeng Pang, Qi Miao, Steven S. Raman, Demetri Terzopoulos, Kyunghyun Sung</li>
<li>for: This paper aims to address the problem of anisotropic volumetric medical data in deep learning-based segmentation, specifically in magnetic resonance imaging (MRI) data.</li>
<li>methods: The proposed method is a 2.5D approach that combines 2D convolution with volumetric information, using a Cross-Slice Attention Module (CSAM) to capture information across all slices in the volume. The CSAM module applies semantic, positional, and slice attention on deep feature maps at different scales.</li>
<li>results: The proposed method was extensively tested using different network architectures and tasks, and the results demonstrate the usefulness and generalizability of CSAM. The code for the proposed method is available at <a target="_blank" rel="noopener" href="https://github.com/aL3x-O-o-Hung/CSAM">https://github.com/aL3x-O-o-Hung/CSAM</a>.<details>
<summary>Abstract</summary>
A large portion of volumetric medical data, especially magnetic resonance imaging (MRI) data, is anisotropic, as the through-plane resolution is typically much lower than the in-plane resolution. Both 3D and purely 2D deep learning-based segmentation methods are deficient in dealing with such volumetric data since the performance of 3D methods suffers when confronting anisotropic data, and 2D methods disregard crucial volumetric information. Insufficient work has been done on 2.5D methods, in which 2D convolution is mainly used in concert with volumetric information. These models focus on learning the relationship across slices, but typically have many parameters to train. We offer a Cross-Slice Attention Module (CSAM) with minimal trainable parameters, which captures information across all the slices in the volume by applying semantic, positional, and slice attention on deep feature maps at different scales. Our extensive experiments using different network architectures and tasks demonstrate the usefulness and generalizability of CSAM. Associated code is available at https://github.com/aL3x-O-o-Hung/CSAM.
</details>
<details>
<summary>摘要</summary>
“一大部分的体积医学数据，特别是磁共振成像（MRI）数据，具有不对称性，通常在水平方向的分辨率比垂直方向的分辨率低得多。三维和仅二维的深度学习基于的分类方法都不适合处理这种体积数据，因为三维方法在遇到不对称的数据时表现不佳，而二维方法则忽略了体积数据中的重要信息。对二点五维方法的研究相对较少，这些模型通常在标本之间学习关系，但通常有许多受训的参数。我们提出了跨条件注意模块（CSAM），具有最少受训参数，可以在不同标本之间学习关系，并且可以在不同的标本中捕捉到重要信息。我们的广泛实验显示了 CSAM 的有用性和普遍性。相关的代码可以在 GitHub 上找到：https://github.com/aL3x-O-o-Hung/CSAM。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation"><a href="#Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation" class="headerlink" title="Learning the What and How of Annotation in Video Object Segmentation"></a>Learning the What and How of Annotation in Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04414">http://arxiv.org/abs/2311.04414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</li>
<li>for: 提高视频对象分割（VOS）模型训练效率，减少人工标注成本。</li>
<li>methods: 提出了一种人工 Loop（HITL）注意力机制，通过预测哪些帧（”What”）和哪种注意力类型（”How”）进行标注，以提高标注效率。</li>
<li>results: 对MOSE和DAVIS数据集进行实验，比较了EVA-VOS和标准 annotating 方法，结果表明：EVA-VOS可以在3.5倍快的速度达到与人类一致的准确率；选择帧performanced状态的方法得到了状元性的表现；EVA-VOS在标注时间方面具有显著的提升。<details>
<summary>Abstract</summary>
Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame ("What") to annotate and which annotation type ("How") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5x faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines.
</details>
<details>
<summary>摘要</summary>
视频对象分割（VOS）是许多应用程序中的关键技术，从视频编辑到视频数据生成。训练VOS模型需要大量的手动标注视频。传统的Annotation Way是要求人类 manually draw detailed segmentation masks on the target objects at each video frame。然而，这个Annotation process是费时的和费力的。在这篇论文中，我们提出了EVA-VOS，一个人类在Loop的标注框架 для视频对象分割。与传统方法不同，我们引入了一个代理人，该代理人预测iteratively Which frame ("What") to annotate和Which annotation type ("How") to use。然后，标注者仅标注选择的帧，并将其用于更新VOS模块，从而实现了显著的标注时间缩短。我们在MOSE和DAVIS datasets上进行了实验，并显示了以下结果：（a）EVA-VOS可以在3.5倍 faster than the standard way of annotating videos 实现masks with accuracy close to human agreement。（b）我们的帧选择性能在State-of-the-art level。（c）EVA-VOS比所有方法和基准值具有显著的标注时间缩短。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.CV_2023_11_08/" data-id="clot2mhdw00ldx7888ldzh09y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.AI_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T12:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.AI_2023_11_08/">cs.AI - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Geometry-Calibrated-DRO-Combating-Over-Pessimism-with-Free-Energy-Implications"><a href="#Geometry-Calibrated-DRO-Combating-Over-Pessimism-with-Free-Energy-Implications" class="headerlink" title="Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications"></a>Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05054">http://arxiv.org/abs/2311.05054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashuo Liu, Jiayun Wu, Tianyu Wang, Hao Zou, Bo Li, Peng Cui</li>
<li>for: 提高机器学习算法对 distribuitional shift 的鲁棒性，Addressing the issue of distributional shifts in machine learning algorithms.</li>
<li>methods: 使用 Distributionally Robust Optimization (DRO) 方法，Optimizing the worst-case risk within an uncertainty set.</li>
<li>results: 提出 Geometry-Calibrated DRO (GCDRO) 方法，which incorporates data geometry into calibration terms to alleviate the impact of noise, and establishes a connection between the risk objective and the Helmholtz free energy in statistical physics. Comprehensive experiments confirm GCDRO’s superiority over conventional DRO methods.<details>
<summary>Abstract</summary>
Machine learning algorithms minimizing average risk are susceptible to distributional shifts. Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case risk within an uncertainty set. However, DRO suffers from over-pessimism, leading to low-confidence predictions, poor parameter estimations as well as poor generalization. In this work, we conduct a theoretical analysis of a probable root cause of over-pessimism: excessive focus on noisy samples. To alleviate the impact of noise, we incorporate data geometry into calibration terms in DRO, resulting in our novel Geometry-Calibrated DRO (GCDRO) for regression. We establish the connection between our risk objective and the Helmholtz free energy in statistical physics, and this free-energy-based risk can extend to standard DRO methods. Leveraging gradient flow in Wasserstein space, we develop an approximate minimax optimization algorithm with a bounded error ratio and elucidate how our approach mitigates noisy sample effects. Comprehensive experiments confirm GCDRO's superiority over conventional DRO methods.
</details>
<details>
<summary>摘要</summary>
Neste trabalho, realizamos uma análise teórica de uma causa provável do pessimismo excessivo: o foco excessivo em amostras ruidosas. Para aliviar o impacto do ruído, incorporamos informações de geometria de dados na calibração de DRO, resultando em nossa técnica novativa de Geometry-Calibrated DRO (GCDRO) para regressão. Estabelecemos a conexão entre nossa meta de risco e a energia livre de Helmholtz na física estatística, e essa medida de risco baseada em energia livre pode ser aplicada a métodos de DRO padrão.Leverando o fluxo de gradiente na espaço de Wasserstein, desenvolvemos um algoritmo de otimização aproximada com um erro de ratio limitado e elucidamos como nossa abordagem mitiga os efeitos das amostras ruidosas. Experimentos compreensivos confirmam a superioridade do GCDRO em relação aos métodos de DRO conventioneis.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Translation-of-Attention-Patterns-in-VQA-Models-to-Natural-Language"><a href="#Zero-shot-Translation-of-Attention-Patterns-in-VQA-Models-to-Natural-Language" class="headerlink" title="Zero-shot Translation of Attention Patterns in VQA Models to Natural Language"></a>Zero-shot Translation of Attention Patterns in VQA Models to Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05043">http://arxiv.org/abs/2311.05043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/zs-a2t">https://github.com/explainableml/zs-a2t</a></li>
<li>paper_authors: Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata</li>
<li>for: 这 paper 的目的是提出一种可以在零批处理下将 transformer 注意力转换成自然语言的框架，以便更好地理解模型内部的含义。</li>
<li>methods: 该框架基于一个预训练的大型语言模型（LLM），该模型接受任务提示、问题和预测答案作为输入，并根据这些输入选择tokен来描述输入图像中 VQA 模型的注意力区域。</li>
<li>results: 该框架在 textual explanation 数据集上达到了零批处理情况下的州OF-the-art表现，在 GQA-REX 和 VQA-X 上得到了优秀的结果。<details>
<summary>Abstract</summary>
Converting a model's internals to text can yield human-understandable insights about the model. Inspired by the recent success of training-free approaches for image captioning, we propose ZS-A2T, a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer, as inputs. The LLM is guided to select tokens which describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g. attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, giving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T.
</details>
<details>
<summary>摘要</summary>
可以将模型内部转换为文本来提供人类可理解的启示。 drawing inspiration from recent successful training-free approaches for image captioning, we propose ZS-A2T，a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer as inputs. The LLM is guided to select tokens that describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g., attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, achieving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T.
</details></li>
</ul>
<hr>
<h2 id="Automated-Annotation-of-Scientific-Texts-for-ML-based-Keyphrase-Extraction-and-Validation"><a href="#Automated-Annotation-of-Scientific-Texts-for-ML-based-Keyphrase-Extraction-and-Validation" class="headerlink" title="Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation"></a>Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05042">http://arxiv.org/abs/2311.05042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwamayowa O. Amusat, Harshad Hegde, Christopher J. Mungall, Anna Giannakou, Neil P. Byers, Dan Gunter, Kjiersten Fagnan, Lavanya Ramakrishnan</li>
<li>for: 本研究旨在提高机器学习生成的 metadata 的可信度，以便更好地搜索和利用生物科学领域中的大数据。</li>
<li>methods: 本研究提出了两种新的自动文本标签方法，即使用不同类型的数据源关联和使用领域专用的控制词汇或 Ontology。</li>
<li>results: 实验结果表明，提posed的标签分配方法可以生成高度特定的文本标签，与机器学习生成的关键字列表匹配度高达44%。<details>
<summary>Abstract</summary>
Advanced omics technologies and facilities generate a wealth of valuable data daily; however, the data often lacks the essential metadata required for researchers to find and search them effectively. The lack of metadata poses a significant challenge in the utilization of these datasets. Machine learning-based metadata extraction techniques have emerged as a potentially viable approach to automatically annotating scientific datasets with the metadata necessary for enabling effective search. Text labeling, usually performed manually, plays a crucial role in validating machine-extracted metadata. However, manual labeling is time-consuming; thus, there is an need to develop automated text labeling techniques in order to accelerate the process of scientific innovation. This need is particularly urgent in fields such as environmental genomics and microbiome science, which have historically received less attention in terms of metadata curation and creation of gold-standard text mining datasets.   In this paper, we present two novel automated text labeling approaches for the validation of ML-generated metadata for unlabeled texts, with specific applications in environmental genomics. Our techniques show the potential of two new ways to leverage existing information about the unlabeled texts and the scientific domain. The first technique exploits relationships between different types of data sources related to the same research study, such as publications and proposals. The second technique takes advantage of domain-specific controlled vocabularies or ontologies. In this paper, we detail applying these approaches for ML-generated metadata validation. Our results show that the proposed label assignment approaches can generate both generic and highly-specific text labels for the unlabeled texts, with up to 44% of the labels matching with those suggested by a ML keyword extraction algorithm.
</details>
<details>
<summary>摘要</summary>
高级数据技术和设施每天生成大量有价值的数据，但这些数据经常缺乏必要的元数据，使研究人员困难地找到和搜索这些数据。缺乏元数据对于使用这些数据而言是一个重要的挑战。基于机器学习的元数据抽取技术已经出现为可能的解决方案，可以自动将科学数据集中添加元数据。文本标签，通常是手动完成的，在验证机器提取的元数据中扮演着关键的角色。然而，手动标签是时间消耗的，因此有一个需要开发自动文本标签技术，以加速科学创新的过程。这个需求特别是在环境遗传学和微生物学等领域 particularly urgent，这些领域在元数据管理和创建高品质的文本挖掘数据方面 historically received less attention。在这篇论文中，我们提出了两种新的自动文本标签方法，用于验证机器生成的元数据的有效性。这两种方法都利用了不同的数据来源之间的关系，以及领域专门的控制词汇或 ontology。我们在这篇论文中详细介绍了这些方法的应用。我们的结果表明，我们的标签分配方法可以为无标签文本生成both generic和高度特定的文本标签，并且最高达44%的标签与机器学习 keyword extraction 算法提出的标签相匹配。
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-from-a-sparsely-annotated-dataset-of-3D-medical-images"><a href="#Transfer-learning-from-a-sparsely-annotated-dataset-of-3D-medical-images" class="headerlink" title="Transfer learning from a sparsely annotated dataset of 3D medical images"></a>Transfer learning from a sparsely annotated dataset of 3D medical images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05032">http://arxiv.org/abs/2311.05032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/medicaltransferlearning3d-unet">https://github.com/diagnijmegen/medicaltransferlearning3d-unet</a></li>
<li>paper_authors: Gabriel Efrain Humpire-Mamani, Colin Jacobs, Mathias Prokop, Bram van Ginneken, Nikolas Lessmann<br>for: This study aims to improve the efficiency of annotation and increase the accessibility of accurate organ segmentation in medical imaging using transfer learning.methods: The authors use transfer learning to leverage pre-trained model features from a large dataset to improve the performance of deep convolutional neural networks for organ segmentation in medical imaging. They use a base segmentation model (3D U-Net) trained on a large and sparsely annotated dataset and fine-tune it for four new down-stream segmentation tasks with fully annotated datasets.results: The results show that transfer learning from the base model is beneficial when small datasets are available, providing significant performance improvements. Fine-tuning the base model is more beneficial than updating all the network weights with vanilla transfer learning. The study also shows that cross-modality transfer learning using CT scans is beneficial. The performance of the fine-tuned models increased by up to 0.129 (+28%) Dice score and on average 23 experiments increased the performance by 0.029 Dice score in the new segmentation tasks.<details>
<summary>Abstract</summary>
Transfer learning leverages pre-trained model features from a large dataset to save time and resources when training new models for various tasks, potentially enhancing performance. Due to the lack of large datasets in the medical imaging domain, transfer learning from one medical imaging model to other medical imaging models has not been widely explored. This study explores the use of transfer learning to improve the performance of deep convolutional neural networks for organ segmentation in medical imaging. A base segmentation model (3D U-Net) was trained on a large and sparsely annotated dataset; its weights were used for transfer learning on four new down-stream segmentation tasks for which a fully annotated dataset was available. We analyzed the training set size's influence to simulate scarce data. The results showed that transfer learning from the base model was beneficial when small datasets were available, providing significant performance improvements; where fine-tuning the base model is more beneficial than updating all the network weights with vanilla transfer learning. Transfer learning with fine-tuning increased the performance by up to 0.129 (+28\%) Dice score than experiments trained from scratch, and on average 23 experiments increased the performance by 0.029 Dice score in the new segmentation tasks. The study also showed that cross-modality transfer learning using CT scans was beneficial. The findings of this study demonstrate the potential of transfer learning to improve the efficiency of annotation and increase the accessibility of accurate organ segmentation in medical imaging, ultimately leading to improved patient care. We made the network definition and weights publicly available to benefit other users and researchers.
</details>
<details>
<summary>摘要</summary>
通过使用已经训练过的模型特征，转移学习可以为训练新的模型 saves time和资源，并且可能提高性能。由于医疗影像领域的大型数据集罕见，医疗影像中的转移学习还没有广泛探索。本研究探讨了使用转移学习提高医疗影像中深度卷积神经网络的组织分 segmentation性能。基本分 segmentation模型（3D U-Net）在一个大型但罕见地注解的数据集上训练，其 weights 用于转移学习四个新的下游分 segmentation任务上。我们分析了训练集大小的影响，以模拟缺乏数据的情况。结果表明，从基本模型进行转移学习在小数据集时是有利的，提供了显著性能提升（+28%）；而在所有网络权重更新为vanilla transfer learning的情况下，进行精度调整（fine-tuning）更是有利。转移学习与精度调整共同提高了新分 segmentation任务的性能，平均提高0.029 Dice分数。研究还发现了在CT扫描图像上进行转移学习的利好。本研究发现，转移学习可以提高医疗影像中的注解效率和准确率，从而提高患者治疗的质量。我们将网络定义和权重公开，以便其他用户和研究人员使用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Paraphrasing-for-Information-Disguise"><a href="#Towards-Effective-Paraphrasing-for-Information-Disguise" class="headerlink" title="Towards Effective Paraphrasing for Information Disguise"></a>Towards Effective Paraphrasing for Information Disguise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05018">http://arxiv.org/abs/2311.05018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idecir/idecir-towards-effective-paraphrasing-for-information-disguise">https://github.com/idecir/idecir-towards-effective-paraphrasing-for-information-disguise</a></li>
<li>paper_authors: Anmol Agarwal, Shrey Gupta, Vamshi Bonagiri, Manas Gaur, Joseph Reagle, Ponnurangam Kumaraguru<br>for: 本研究的目的是提出一种基于幂等词汇替换的信息隐蔽技术，以防止互联网上作者的文字媒体宣传被非法利用。methods: 本研究使用了自然语言处理技术中的人工智能自动词汇替换工具（如SpinRewriter、WordAI），并通过对 sentence 进行迭代 perturbation 来混淆搜索机制。results: 本研究表明，使用多级词汇替换和幂等词汇替换可以成功隐藏 sentences 82% 的时间。这种方法可以帮助作者隐藏敏感信息，从而减少不良用户利用这些信息的风险。<details>
<summary>Abstract</summary>
Information Disguise (ID), a part of computational ethics in Natural Language Processing (NLP), is concerned with best practices of textual paraphrasing to prevent the non-consensual use of authors' posts on the Internet. Research on ID becomes important when authors' written online communication pertains to sensitive domains, e.g., mental health. Over time, researchers have utilized AI-based automated word spinners (e.g., SpinRewriter, WordAI) for paraphrasing content. However, these tools fail to satisfy the purpose of ID as their paraphrased content still leads to the source when queried on search engines. There is limited prior work on judging the effectiveness of paraphrasing methods for ID on search engines or their proxies, neural retriever (NeurIR) models. We propose a framework where, for a given sentence from an author's post, we perform iterative perturbation on the sentence in the direction of paraphrasing with an attempt to confuse the search mechanism of a NeurIR system when the sentence is queried on it. Our experiments involve the subreddit 'r/AmItheAsshole' as the source of public content and Dense Passage Retriever as a NeurIR system-based proxy for search engines. Our work introduces a novel method of phrase-importance rankings using perplexity scores and involves multi-level phrase substitutions via beam search. Our multi-phrase substitution scheme succeeds in disguising sentences 82% of the time and hence takes an essential step towards enabling researchers to disguise sensitive content effectively before making it public. We also release the code of our approach.
</details>
<details>
<summary>摘要</summary>
信息掩蔽（ID），一部分的计算伦理在自然语言处理（NLP）中，关注在文本重新排序方法的最佳实践中，以防止互联网上作者的帖子不经授权使用。对于研究人员来说，研究ID变得重要，特别是作者在互联网上发表的文本与敏感领域相关，例如心理健康。在过去，研究人员使用基于AI的自动词汇替换工具（如SpinRewriter、WordAI）进行重新排序内容。然而，这些工具并不满足ID的目的，因为它们重新排序后的内容仍然可以跟踪回原始来源。有限的先前研究探讨了重新排序方法的效果对于ID在搜索引擎或其代理Neural Retriever（NeurIR）模型。我们提出了一个框架，其中，对于作者的一句话，我们在重新排序方向下进行迭代 perturbation，以混淆搜索机制中查询该句话的NeurIR系统。我们的实验使用了Reddit上的“r/AmItheAsshole”社区为公共内容的来源，并使用基于Neural Retriever的代理来模拟搜索引擎。我们的方法包括phrase重要性排名使用混淆分数和多级词汇替换via扫描搜索。我们的多词汇替换方案成功地隐藏了句子82%的时间，因此为研究人员隐藏敏感内容的有效方法。我们还发布了我们的方法的代码。
</details></li>
</ul>
<hr>
<h2 id="Joint-Sensing-and-Semantic-Communications-with-Multi-Task-Deep-Learning"><a href="#Joint-Sensing-and-Semantic-Communications-with-Multi-Task-Deep-Learning" class="headerlink" title="Joint Sensing and Semantic Communications with Multi-Task Deep Learning"></a>Joint Sensing and Semantic Communications with Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05017">http://arxiv.org/abs/2311.05017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 这篇论文探讨了深度学习技术的紧密 integrate 感知通信，包括延伸到semantic communications。</li>
<li>methods:  transmitter 使用深度神经网络（encoder）进行源编码、频率编码和模ulation，而 receiver 使用另一个深度神经网络（decoder）进行模ulation、频率解码和源解码来重construct 数据样本。</li>
<li>results: 该实验使用 CIFAR-10 作为输入数据，并考虑了通道效应如添加白噪和衰减抑制。结果表明多任务深度学习可以实现高精度的紧密感知通信和semantic communications。<details>
<summary>Abstract</summary>
This paper explores the integration of deep learning techniques for joint sensing and communications, with an extension to semantic communications. The integrated system comprises a transmitter and receiver operating over a wireless channel, subject to noise and fading effects. The transmitter employs a deep neural network, namely an encoder, for joint operations of source coding, channel coding, and modulation, while the receiver utilizes another deep neural network, namely a decoder, for joint operations of demodulation, channel decoding, and source decoding to reconstruct the data samples. The transmitted signal serves a dual purpose, supporting communication with the receiver and enabling sensing. When a target is present, the reflected signal is received, and another deep neural network decoder is utilized for sensing. This decoder is responsible for detecting the target's presence and determining its range. All these deep neural networks, including one encoder and two decoders, undergo joint training through multi-task learning, considering data and channel characteristics. This paper extends to incorporate semantic communications by introducing an additional deep neural network, another decoder at the receiver, operating as a task classifier. This decoder evaluates the fidelity of label classification for received signals, enhancing the integration of semantics within the communication process. The study presents results based on using the CIFAR-10 as the input data and accounting for channel effects like Additive White Gaussian Noise (AWGN) and Rayleigh fading. The results underscore the effectiveness of multi-task deep learning in achieving high-fidelity joint sensing and semantic communications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interpreting-Pretrained-Language-Models-via-Concept-Bottlenecks"><a href="#Interpreting-Pretrained-Language-Models-via-Concept-Bottlenecks" class="headerlink" title="Interpreting Pretrained Language Models via Concept Bottlenecks"></a>Interpreting Pretrained Language Models via Concept Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05014">http://arxiv.org/abs/2311.05014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Tan, Lu Cheng, Song Wang, Yuan Bo, Jundong Li, Huan Liu</li>
<li>for: 本研究旨在解释PLMs的黑obox性，提高PLMs在自然语言处理任务中的可读性和可理解性。</li>
<li>methods: 我们提出了一种新的方法，利用高级别的意义ful的概念来解释PLMs。我们使用人工标注和机器生成的概念相结合，提取隐藏神经元，以捕捉semantically meaningful和任务特定的概念。</li>
<li>results: 我们通过对实际数据集进行实证研究，发现我们的方法可以提供有价值的解释PLMs的行为，帮助诊断模型失败和提高模型的Robustness。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) have made significant strides in various natural language processing tasks. However, the lack of interpretability due to their ``black-box'' nature poses challenges for responsible implementation. Although previous studies have attempted to improve interpretability by using, e.g., attention weights in self-attention layers, these weights often lack clarity, readability, and intuitiveness. In this research, we propose a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans. For example, we learn the concept of ``Food'' and investigate how it influences the prediction of a model's sentiment towards a restaurant review. We introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. Through empirical evaluations on real-world datasets, we manifest that our approach offers valuable insights to interpret PLM behavior, helps diagnose model failures, and enhances model robustness amidst noisy concept labels.
</details>
<details>
<summary>摘要</summary>
Pretrained language models (PLMs) have made significant progress in various natural language processing tasks. However, their "black-box" nature poses challenges for responsible implementation. Previous studies have attempted to improve interpretability by using, for example, attention weights in self-attention layers, but these weights often lack clarity, readability, and intuitiveness. In this research, we propose a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans. For example, we learn the concept of "Food" and investigate how it influences the prediction of a model's sentiment towards a restaurant review. We introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. Through empirical evaluations on real-world datasets, we demonstrate that our approach offers valuable insights to interpret PLM behavior, helps diagnose model failures, and enhances model robustness amidst noisy concept labels.
</details></li>
</ul>
<hr>
<h2 id="Expressibility-induced-Concentration-of-Quantum-Neural-Tangent-Kernels"><a href="#Expressibility-induced-Concentration-of-Quantum-Neural-Tangent-Kernels" class="headerlink" title="Expressibility-induced Concentration of Quantum Neural Tangent Kernels"></a>Expressibility-induced Concentration of Quantum Neural Tangent Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04965">http://arxiv.org/abs/2311.04965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li-Wei Yu, Weikang Li, Qi Ye, Zhide Lu, Zizhao Han, Dong-Ling Deng</li>
<li>for: 这篇论文主要研究了量子机器学习模型的性能分析方法，以及这些方法如何应用于实际应用中的宽量子变量电路设计。</li>
<li>methods: 这篇论文使用了量子触 neighboorhood kernel 方法，用于分析量子机器学习模型在无限宽限制下的性能。这些方法还被应用于描述量子神经网络训练误差的整数化方式。</li>
<li>results: 研究发现，在全球损失函数下，高表达能量的全球和本地量子编码可以导致量子触 neighboorhood kernel 值快速减少到零。而在本地损失函数下，虽然高表达能量可以导致量子触 neighboorhood kernel 值快速减少，但是不能完全消除。此外，通过广泛的数值实验， authors 验证了这些分析理论。这些发现对量子机器学习模型的设计提供了重要的指导意见。<details>
<summary>Abstract</summary>
Quantum tangent kernel methods provide an efficient approach to analyzing the performance of quantum machine learning models in the infinite-width limit, which is of crucial importance in designing appropriate circuit architectures for certain learning tasks. Recently, they have been adapted to describe the convergence rate of training errors in quantum neural networks in an analytical manner. Here, we study the connections between the trainability and expressibility of quantum tangent kernel models. In particular, for global loss functions, we rigorously prove that high expressibility of both the global and local quantum encodings can lead to exponential concentration of quantum tangent kernel values to zero. Whereas for local loss functions, such issue of exponential concentration persists owing to the high expressibility, but can be partially mitigated. We further carry out extensive numerical simulations to support our analytical theories. Our discoveries unveil a pivotal characteristic of quantum neural tangent kernels, offering valuable insights for the design of wide quantum variational circuit models in practical applications.
</details>
<details>
<summary>摘要</summary>
量子触感керnel方法提供了一种有效的方式来分析量子机器学习模型在无穷宽限下的性能，这对于设计适当的电路体系结构非常重要。最近，它们已经被适应来描述量子神经网络训练错误的减少速率。在这里，我们研究了量子触感керnel模型的可教化和表达能力之间的连接。特别是，对于全局损失函数，我们严格地证明了高表达能力的全局和本地量子编码的情况下，可以导致量子触感керnel值快速减少到零。而对于本地损失函数，这种问题仍然存在，但可以通过高表达能力来部分缓解。我们还进行了广泛的数值仿真，以支持我们的分析理论。我们的发现揭示了量子神经触感kernek的一个重要特点，为实际应用中的宽量子变量电路模型设计提供了价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Size-How-Gradients-Shape-Pruning-Decisions-in-Large-Language-Models"><a href="#Beyond-Size-How-Gradients-Shape-Pruning-Decisions-in-Large-Language-Models" class="headerlink" title="Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"></a>Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04902">http://arxiv.org/abs/2311.04902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rocktimjyotidas/gblm-pruner">https://github.com/rocktimjyotidas/gblm-pruner</a></li>
<li>paper_authors: Rocktim Jyoti Das, Liqun Ma, Zhiqiang Shen</li>
<li>for: 这个研究是为了提出一种基于梯度的语言模型剔除方法（GBLM-Pruner），以提高这些模型的优化和简化。</li>
<li>methods: 这个方法使用了大型语言模型的预训梯度，通过计算梯度的第一项泰勒展开来决定剔除重要性分数，并且不需要任何训练或重新调整。</li>
<li>results: 试验结果显示，GBLM-Pruner比其他两种方法（SparseGPT和Wanda）在多个测试 benchmark 上表现更好，并且不需要任何额外的训练或重新调整。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the importance pruning score, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (weights+activations) and SparseGPT (weights+activations+weight update) by significant margins. Our code and models are available at https://github.com/RocktimJyotiDas/GBLM-Pruner.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） WITH 10亿或更多参数是适用于网络剪裁的目标，寻求减少一部分网络权重而不影响性能。先前的方法，如权重强度、SparseGPT和Wanda，ether solely focused on weights or integrated weights with activations for sparsity，但它们忽略了预训练大语言模型中的有用梯度。在这篇论文中，我们提出了一种新的简洁中心的剪裁方法 для预训练LLM，称为Gradient-based Language Model Pruner（GBLM-Pruner）。GBLM-Pruner利用预训练大语言模型中的梯度进行一ORDER Taylor扩展，在无需训练的情况下，通过正确规范梯度来确定剪裁重要性分数，并显著超越了竞争对手SparseGPT和Wanda在多个benchmark上。有趣的是，在执行剪裁后，不结构化的剪裁方法往往会揭示一些结构性 Patterns，这与LLMs中参数结构的几何相互关系有关。此外，GBLM-Pruner不需要任何后续重新训练或权重更新，以简化其他对手。我们在LLaMA-1和LLaMA-2上进行了多种语言benchmark和折衔度评估，发现GBLM-Pruner在比例剪裁、Wanda（权重+活动）和SparseGPT（权重+活动+权重更新）的情况下具有显著优势。我们的代码和模型可以在https://github.com/RocktimJyotiDas/GBLM-Pruner中找到。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Sketching-for-Large-Language-Models"><a href="#Prompt-Sketching-for-Large-Language-Models" class="headerlink" title="Prompt Sketching for Large Language Models"></a>Prompt Sketching for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04954">http://arxiv.org/abs/2311.04954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Beurer-Kellner, Mark Niklas Müller, Marc Fischer, Martin Vechev</li>
<li>For: The paper aims to address the issue of disconnected and wordy intermediate responses in recent prompting strategies for large language models (LLMs).* Methods: The proposed method, called prompt sketching, involves predicting values for multiple variables in a template, allowing users to have more control over the generation process and provide a reasoning framework via intermediate instructions. The key idea is to adapt the decoding procedure to also score follow-up instructions during text generation, optimizing overall template likelihood in inference.* Results: The paper shows that prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. The paper also releases a number of generic, yet effective sketches applicable to many tasks and an open source library called dclib, powering the sketch-aware decoders.<details>
<summary>Abstract</summary>
Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.
</details>
<details>
<summary>摘要</summary>
很多现代提示策略 для大型自然语言模型（LLM）会在 sequential 方式下询问模型多次——首先生成中间结果，然后生成最终答案。然而，使用这些方法时，decoder和模型都不知道可能的后续提示，导致中间响应不连贯和不 DESIRED  wordy。在这项工作中，我们解决这个问题，提出了提示绘制（prompt sketching），一种新的提示方式，在 котором一个 LLM 不仅通过完成提示来回答，而且可以预测多个变量的值在模板中。这样，绘制可以让用户更有控制力量，例如提供了一个reasoning框架 via intermediate instructions，导致更好的总体结果。我们的关键想法是使用现有的、自然进行推断的模型，将解码过程修改为在文本生成过程中也评分后续指令，以便在推断过程中优化总体模板概率。我们的实验表明，在零配置情况下，提示绘制在 7 个 LLMBenchmark 任务上比直接询问或链式思维方法表现出色，包括状态跟踪、数学逻辑和通用问答。为便于未来使用，我们发布了一些通用 yet 有效的绘制，以及一个名为 dclib 的开源库，该库将power我们的绘制执行器。
</details></li>
</ul>
<hr>
<h2 id="Two-Complementary-Perspectives-to-Continual-Learning-Ask-Not-Only-What-to-Optimize-But-Also-How"><a href="#Two-Complementary-Perspectives-to-Continual-Learning-Ask-Not-Only-What-to-Optimize-But-Also-How" class="headerlink" title="Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How"></a>Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04898">http://arxiv.org/abs/2311.04898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timm Hess, Tinne Tuytelaars, Gido M. van de Ven</li>
<li>for: 这篇论文主要关注于如何解决深度神经网络的持续学习问题，特别是当开始训练新任务时会发生快速忘记的问题。</li>
<li>methods: 这篇论文提出了一种基于加入回放或调整项的方法来 aproximate 缩寸类损失函数，并且还使用了梯度对映技术来调整优化路径。</li>
<li>results: 这篇论文预计通过结合回放-approximated joint 损失函数和梯度对映-based 优化路径，以测试加入后者是否能够提供以下优点：(1) 缓和稳定差距，(2) 增加学习效率，(3) 提高最终学习成果。<details>
<summary>Abstract</summary>
Recent years have seen considerable progress in the continual training of deep neural networks, predominantly thanks to approaches that add replay or regularization terms to the loss function to approximate the joint loss over all tasks so far. However, we show that even with a perfect approximation to the joint loss, these approaches still suffer from temporary but substantial forgetting when starting to train on a new task. Motivated by this 'stability gap', we propose that continual learning strategies should focus not only on the optimization objective, but also on the way this objective is optimized. While there is some continual learning work that alters the optimization trajectory (e.g., using gradient projection techniques), this line of research is positioned as alternative to improving the optimization objective, while we argue it should be complementary. To evaluate the merits of our proposition, we plan to combine replay-approximated joint objectives with gradient projection-based optimization routines to test whether the addition of the latter provides benefits in terms of (1) alleviating the stability gap, (2) increasing the learning efficiency and (3) improving the final learning outcome.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络的不断训练方法得到了很大的进步，主要是通过添加回放或规则化项来 aproximate 所有任务的共同损失函数。然而，我们发现，即使有完美的共同损失函数近似，这些方法仍会在开始训练新任务时出现临时且显著的忘记。我们被这个"稳定差距"所驱使，我们提议，不断学习策略应该不仅关注优化目标，还应该关注优化目标的优化方式。虽然有一些不断学习研究使用梯度投影技术来修改优化轨迹，但我们认为这种研究应该是补充优化目标的，而不是替代。为了评估我们的提议的价值，我们计划将回放近似的共同损失函数与梯度投影技术相结合，以测试这种组合是否可以提供以下 beneficial effects：1. 减轻稳定差距2. 提高学习效率3. 提高最终学习成果
</details></li>
</ul>
<hr>
<h2 id="DAMEX-Dataset-aware-Mixture-of-Experts-for-visual-understanding-of-mixture-of-datasets"><a href="#DAMEX-Dataset-aware-Mixture-of-Experts-for-visual-understanding-of-mixture-of-datasets" class="headerlink" title="DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets"></a>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04894">http://arxiv.org/abs/2311.04894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinga-lala/damex">https://github.com/jinga-lala/damex</a></li>
<li>paper_authors: Yash Jain, Harkirat Behl, Zsolt Kira, Vibhav Vineet</li>
<li>for: 本文旨在提出一种universal detector的建构方法，如何在大量混合数据集上训练一个模型？</li>
<li>methods: 作者提出了一种名为Dataset-Aware Mixture-of-Experts（DAMEX）的解决方案，通过训练专家来 Route每个数据集的令牌到它的映射专家，以提高模型的性能。</li>
<li>results: 在Universal Object-Detection Benchmark上进行了实验，比较了与现有状态的前一代和非MoE基线方法，达到了平均提升10.2个AP分数，并且在不同的数据集混合情况下（1）有限的可用性、（2）不同的领域和（3）不同的标签集）都达到了稳定的提升。此外，作者还质量地表明了DAMEX的专家表示 collapse问题的Robustness。<details>
<summary>Abstract</summary>
Construction of a universal detector poses a crucial question: How can we most effectively train a model on a large mixture of datasets? The answer lies in learning dataset-specific features and ensembling their knowledge but do all this in a single model. Previous methods achieve this by having separate detection heads on a common backbone but that results in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose Dataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an `expert' of a dataset by learning to route each dataset tokens to its mapped expert. Experiments on Universal Object-Detection Benchmark show that we outperform the existing state-of-the-art by average +10.2 AP score and improve over our non-MoE baseline by average +2.0 AP score. We also observe consistent gains while mixing datasets with (1) limited availability, (2) disparate domains and (3) divergent label sets. Further, we qualitatively show that DAMEX is robust against expert representation collapse.
</details>
<details>
<summary>摘要</summary>
建构一个通用探测器存在一个关键问题：如何有效地训练一个模型在大量的混合数据集上？答案在于学习数据集特有的特征并将其拼接在单一模型中。先前的方法通过在共同脊梁上添加分开的探测头来实现这一点，但这会导致参数的增加。在这项工作中，我们提出了混合专家（MoE）作为解决方案，并证明MoE不仅是一种扩展性工具。我们提出了数据集特性混合专家（DAMEX），其中我们在训练专家时将数据集的各个元素分配给它们所对应的专家。实验表明，我们在通用物体探测benchmark上的平均AP分数高于现有状态的拟合性工具，提高了非MoE基线的平均AP分数 by 10.2个点。我们还发现在混合不同数据集、不同领域和不同标签集时，DAMEX具有一定的稳定性和可靠性。此外，我们还证明DAMEX不容易受到专家表示塌陷的影响。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Annotation-Learning-in-Computer-Vision-Application-to-Image-Classification-and-Object-Detection-tasks"><a href="#Towards-Few-Annotation-Learning-in-Computer-Vision-Application-to-Image-Classification-and-Object-Detection-tasks" class="headerlink" title="Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks"></a>Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04888">http://arxiv.org/abs/2311.04888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bouniot</li>
<li>for: 这个论文的目的是提出一些用于机器学习的有限标签问题的理论、算法和实验贡献，特别是在计算机视觉中进行图像分类和对象检测。</li>
<li>methods: 这个论文使用了许多现有的Meta-学习算法，以及多任务学习理论基础，以针对少量标签问题进行更有效的meta-学习。此外，它还提出了一种不使用标签的对象检测器预训练方法，以及一种使用部分标签的semi-supervised学习方法。</li>
<li>results: 这个论文的实验结果表明，通过将多任务学习理论与Meta-学习算法相结合，可以更好地适应少量标签问题，并且可以在对象检测器中使用不使用标签的预训练方法来提高对象检测的准确率。<details>
<summary>Abstract</summary>
In this thesis, we develop theoretical, algorithmic and experimental contributions for Machine Learning with limited labels, and more specifically for the tasks of Image Classification and Object Detection in Computer Vision. In a first contribution, we are interested in bridging the gap between theory and practice for popular Meta-Learning algorithms used in Few-Shot Classification. We make connections to Multi-Task Representation Learning, which benefits from solid theoretical foundations, to verify the best conditions for a more efficient meta-learning. Then, to leverage unlabeled data when training object detectors based on the Transformer architecture, we propose both an unsupervised pretraining and a semi-supervised learning method in two other separate contributions. For pretraining, we improve Contrastive Learning for object detectors by introducing the localization information. Finally, our semi-supervised method is the first tailored to transformer-based detectors.
</details>
<details>
<summary>摘要</summary>
在这个论文中，我们提出了理论、算法和实验贡献，用于机器学习具有有限标签数据的任务，特别是计算机视觉中的图像分类和物体检测。在第一个贡献中，我们尝试 bridge 理论和实践中的各种Meta-Learning算法，用于少量样本分类。我们与多任务学习理论建立连接，以验证最佳的meta-learning条件。然后，我们提出了一种不使用标签数据的对象检测器培训方法，基于Transformer架构。在这两个分布中，我们分别提出了一种无监督预训练方法和一种半监督学习方法。在预训练中，我们提出了一种基于本地化信息的对比学习方法，以提高对象检测器的性能。最后，我们的半监督学习方法是首次应用于基于Transformer架构的对象检测器。
</details></li>
</ul>
<hr>
<h2 id="SEMQA-Semi-Extractive-Multi-Source-Question-Answering"><a href="#SEMQA-Semi-Extractive-Multi-Source-Question-Answering" class="headerlink" title="SEMQA: Semi-Extractive Multi-Source Question Answering"></a>SEMQA: Semi-Extractive Multi-Source Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04886">http://arxiv.org/abs/2311.04886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/quotesum">https://github.com/google-research-datasets/quotesum</a></li>
<li>paper_authors: Tal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William W. Cohen, Donald Metzler</li>
<li>for: 这篇论文旨在提出一种新的多选问答任务，即将多个多样性的源文摘要成一篇全面的答案，以便更好地评估语言模型的能力。</li>
<li>methods: 这篇论文使用了 semi-extractive 方法，即将factual quoted spans（直接从输入源文中摘取的 span）和非factual free-text connectors（将这些 span 连接成一个完整的 passage）相结合，以生成一个全面的答案。</li>
<li>results: 经过对多个语言模型的实验， authors 发现这个任务 surprisingly 具有挑战性，表明 QuoteSum 可以用于开发和研究这种混合摘要能力。<details>
<summary>Abstract</summary>
Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.   In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans -- copied verbatim from given input sources -- and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate.   To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.
</details>
<details>
<summary>摘要</summary>
最近提出的长形问答系统（QA），支持大型自然语言模型（LLM），已经显示了有前途的能力。然而，归功和验证生成的抽象答案是一个Difficult Challenge。在这种工作中，我们引入了一个新的问答任务，即摘要多个多样的源文，并生成一个包含多个事实引用 span 和自由文本连接器的全面答案。这种设定可以在EXTRACTIVE QA系统的输出和具有更高级别的自由文本生成能力的语言模型之间形成一个桥接。特别是，它允许语言模型利用其高级语言生成能力，同时生成易于验证、解释和评估的精准引用。为研究这个任务，我们创建了第一个这种类型的数据集，即QuoteSum，其中包含人类编写的 semi-extractive 答案，以及自然和生成的问题。我们定义了文本基于的评估指标。在不同的设定下，我们使用多种语言模型进行实验，发现这个任务实际上非常困难，这表明QuoteSum 对开发和研究这种整合能力的研究具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="LongQLoRA-Efficient-and-Effective-Method-to-Extend-Context-Length-of-Large-Language-Models"><a href="#LongQLoRA-Efficient-and-Effective-Method-to-Extend-Context-Length-of-Large-Language-Models" class="headerlink" title="LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models"></a>LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04879">http://arxiv.org/abs/2311.04879</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangjianxin1/longqlora">https://github.com/yangjianxin1/longqlora</a></li>
<li>paper_authors: Jianxin Yang</li>
<li>for: 提高大语言模型的上下文长度，并采用少量训练资源。</li>
<li>methods:  combining Position Interpolation, QLoRA和Shift Short Attention of LongLoRA，并在单个32GB V100 GPU上进行训练。</li>
<li>results: 可以将LLaMA2 7B和13B的上下文长度从4096延长到8192和更长，并在PG19和Proof-pile数据集上实现竞争性的抗抑制性表现，并且与MPT-7B-8K在评估上下文长度为8192的情况下几乎相同。<details>
<summary>Abstract</summary>
We present LongQLoRA, an efficient and effective method to extend context length of large language models with less training resources. LongQLoRA combines the advantages of Position Interpolation, QLoRA and Shift Short Attention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the context length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within 1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on PG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close to MPT-7B-8K within the evaluation context length of 8192. We collect and build 39k long instruction data to extend context length of Vicuna-13B from 4096 to 8192 and achieve good performance both in long and short context generation task. We also do some ablation experiments to study the effect of LoRA rank, finetuning steps and attention patterns in inference.The model weights, training data and code are avaliable at https://github.com/yangjianxin1/LongQLoRA.
</details>
<details>
<summary>摘要</summary>
我们提出了LongQLoRA，一种高效和有效的方法，可以将大语言模型的上下文长度延长，使用更少的训练资源。LongQLoRA结合了Position Interpolation、QLoRA和Shift Short Attention的优点。使用单个32GB V100 GPU，LongQLoRA可以将LLaMA2 7B和13B的上下文长度从4096提高至8192以及12k，在1000个finetuning步骤内完成。LongQLoRA在PG19和Proof-pile数据集上达到了竞争力的折射性表现，我们的模型比LongLoRA更高效，与MPT-7B-8K在评估Context length为8192的情况下几乎相当。我们收集了39k个长 instrucion数据，以延长Vicuna-13B的上下文长度从4096提高至8192，并在长和短上下文生成任务中达到了良好的表现。我们还进行了一些剥夺实验，以研究LoRA排名、finetuning步骤和注意模式在推理中的效果。模型权重、训练数据和代码可以在https://github.com/yangjianxin1/LongQLoRA中下载。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Benchmark-and-Contamination-for-Language-Models-with-Rephrased-Samples"><a href="#Rethinking-Benchmark-and-Contamination-for-Language-Models-with-Rephrased-Samples" class="headerlink" title="Rethinking Benchmark and Contamination for Language Models with Rephrased Samples"></a>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04850">http://arxiv.org/abs/2311.04850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lm-sys/llm-decontaminator">https://github.com/lm-sys/llm-decontaminator</a></li>
<li>paper_authors: Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica</li>
<li>for: 这篇论文的目的是探讨大型自然语言模型（LLM）在训练时可能会受到污染的问题，以及如何使用更强大的检测方法来解决这个问题。</li>
<li>methods: 本文使用了多种检测方法来检查LLM的训练数据是否受到污染，包括字串匹配（n-gram overlap）和简译等方法。另外，本文还提出了一个更强大的LLM-based检测方法，并将其应用于广泛使用的预训练和终端训练数据中。</li>
<li>results: 本文的实验结果显示，使用现有的检测方法可能无法彻底检测LLM的污染问题，而且简译等方法可以轻松地绕过检测方法。另外，本文还发现了一些实验数据中的污染问题，包括GPT-3.5&#x2F;4生成的 sintetic数据。总之，本文强调了需要更强大的检测方法来确保LLM的训练数据是clean的，并且呼吁了社区对于公共benchmark的使用进行更多的检测和监控。<details>
<summary>Abstract</summary>
Large language models are increasingly trained on all the data ever produced by humans. Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets. While most data decontamination efforts apply string matching (e.g., n-gram overlap) to remove benchmark data, we show that these methods are insufficient, and simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures. Furthermore, we demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4. We validate such observations in widely used benchmarks such as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a stronger LLM-based decontamination method and apply it to widely used pre-training and fine-tuning datasets, revealing significant previously unknown test overlap. For example, in pre-training sets such as RedPajama-Data-1T and StarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps. Interestingly, we also find such contamination in synthetic dataset generated by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We urge the community to adopt stronger decontamination approaches when using public benchmarks. Moreover, we call for the community to actively develop fresh one-time exams to evaluate models accurately. Our decontamination tool is publicly available at https://github.com/lm-sys/llm-decontaminator.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练时使用了所有人类生产的数据。许多人提出了公共参考数据的可靠性问题，因为可能有污染在预训或精革 dataset 中。大多数数据净化努力使用字串匹配（例如 n-gram 重叠）移除参考数据，但我们表明这些方法是不充分的，并且简单的变体（例如重写、翻译）可以轻松地绕过这些净化措施。此外，我们显示了如果这种变体数据不被消除，则一个 13B 模型可以轻松地适应参考数据，并取得极高的性能，与 GPT-4 相似。我们在广泛使用的参考数据中进行验证这些观察，包括 MMLU、GSK8k 和 HumanEval。为了解决这个增长的风险，我们提出了一个更强的 LLM-based 净化方法，并将其应用到广泛使用的预训和精革 dataset 中，发现了 significannot 前所未知的参考数据重合。例如，在 RedPajama-Data-1T 和 StarCoder-Data 预训集中，我们发现了 8-18% 的 HumanEval 参考数据重合。 interestingly，我们也发现了这种污染在 GPT-3.5/4 生成的 sintetic 数据中，建议社区将更强的净化方法应用到公共参考数据中，以确保模型的测试性能是可靠的。此外，我们呼吁社区积极开发新的一次验证问题，以确保模型的性能是正确的。我们的净化工具已经在 GitHub 上公开，可以在 https://github.com/lm-sys/llm-decontaminator 中找到。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Semantic-Component-for-Robust-Molecular-Property-Prediction"><a href="#Identifying-Semantic-Component-for-Robust-Molecular-Property-Prediction" class="headerlink" title="Identifying Semantic Component for Robust Molecular Property Prediction"></a>Identifying Semantic Component for Robust Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04837">http://arxiv.org/abs/2311.04837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmirlab-group/sci">https://github.com/dmirlab-group/sci</a></li>
<li>paper_authors: Zijian Li, Zunhong Xu, Ruichu Cai, Zhenhui Yang, Yuguang Yan, Zhifeng Hao, Guangyi Chen, Kun Zhang</li>
<li>for: 本研究旨在提高Graph Neural Networks（GNN）在不同数据集下的泛化能力。</li>
<li>methods: 我们提出了一种名为Semantic-Components Identifiability（SCI）的生成模型，可以将latent variable分解成semantic-relevant（SR）和semantic-irrelevant（SI）组成部分。</li>
<li>results: 我们的实验研究表明，SCI方法可以在21个数据集上 achieve state-of-the-art performance，并且可以提供更多的泛化性能。此外，我们的Visualization结果也提供了具有启发性的案例研究和预测结果的解释。<details>
<summary>Abstract</summary>
Although graph neural networks have achieved great success in the task of molecular property prediction in recent years, their generalization ability under out-of-distribution (OOD) settings is still under-explored. Different from existing methods that learn discriminative representations for prediction, we propose a generative model with semantic-components identifiability, named SCI. We demonstrate that the latent variables in this generative model can be explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI) components, which contributes to better OOD generalization by involving minimal change properties of causal mechanisms. Specifically, we first formulate the data generation process from the atom level to the molecular level, where the latent space is split into SI substructures, SR substructures, and SR atom variables. Sequentially, to reduce misidentification, we restrict the minimal changes of the SR atom variables and add a semantic latent substructure regularization to mitigate the variance of the SR substructure under augmented domain changes. Under mild assumptions, we prove the block-wise identifiability of the SR substructure and the comment-wise identifiability of SR atom variables. Experimental studies achieve state-of-the-art performance and show general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the visualization results of the proposed SCI method provide insightful case studies and explanations for the prediction results. The code is available at: https://github.com/DMIRLAB-Group/SCI.
</details>
<details>
<summary>摘要</summary>
虽然 Graf Neural Networks 在 recent 年取得了大量成功的质量预测task，但它们的Out-of-Distribution（OOD）环境下的普遍性仍然受探索。不同于现有的方法将学习探测表示，我们提出了具有semantic-components可识别的生成模型，称为SCI。我们证明了隐藏变量在这个生成模型中可以明确地分解为semantic-相关（SR）和semantic-不相关（SI）组成部分，这对于OOD普遍性做出了贡献，因为它们涉及到最小改变的causal mechanism。具体来说，我们首先将质量生成从原子层次到分子层次，其隐藏空间被拆分为SI子结构、SR子结构和SR原子变量。接着，为了降低错误识别，我们限制SR原子变量的最小改变，并将SR子结构加入semantic latent substructure regularization，以减少对于增强领域变化的变异。根据严谨的假设，我们证明了SR子结构的对����分解和SR原子变量的对����分解。实验研究获得了3大主流benchmark中的state-of-the-art表现，并在21个数据集上显示了一般提高。此外，我们的SCI方法的视觉化结果将提供了有用的案例研究和预测结果解释。SCI代码可以在以下地址获取：https://github.com/DMIRLAB-Group/SCI。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Personalized-Online-Federated-Learning"><a href="#Decentralized-Personalized-Online-Federated-Learning" class="headerlink" title="Decentralized Personalized Online Federated Learning"></a>Decentralized Personalized Online Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04817">http://arxiv.org/abs/2311.04817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renzhi Wu, Saayan Mitra, Xiang Chen, Anup Rao</li>
<li>for: 这个论文旨在提出一种新的学习设定，即分布式个性化在线学习（Decentralized Personalized Online Federated Learning，DPOEL），以满足企业端服务器（enterprise edge servers）上一些重要应用程序的需求。</li>
<li>methods: 该论文提出了两种技术挑战：首先，如何将来自邻居客户端的共享模型参数集成到本地模型中，以获得良好的本地模型性能。其次，如何选择客户端与其他客户端进行交互的邻居。该论文提出了一种基于学习权重的对等选择方法。</li>
<li>results: 该论文在三个实际项赋予预测数据集和一个空气质量预测数据集上进行了实验，并证明了其效果和可靠性。<details>
<summary>Abstract</summary>
Vanilla federated learning does not support learning in an online environment, learning a personalized model on each client, and learning in a decentralized setting. There are existing methods extending federated learning in each of the three aspects. However, some important applications on enterprise edge servers (e.g. online item recommendation at global scale) involve the three aspects at the same time. Therefore, we propose a new learning setting \textit{Decentralized Personalized Online Federated Learning} that considers all the three aspects at the same time.   In this new setting for learning, the first technical challenge is how to aggregate the shared model parameters from neighboring clients to obtain a personalized local model with good performance on each client. We propose to directly learn an aggregation by optimizing the performance of the local model with respect to the aggregation weights. This not only improves personalization of each local model but also helps the local model adapting to potential data shift by intelligently incorporating the right amount of information from its neighbors. The second challenge is how to select the neighbors for each client. We propose a peer selection method based on the learned aggregation weights enabling each client to select the most helpful neighbors and reduce communication cost at the same time. We verify the effectiveness and robustness of our proposed method on three real-world item recommendation datasets and one air quality prediction dataset.
</details>
<details>
<summary>摘要</summary>
vanilla federated learning 不支持在线学习、学习每个客户端上的个性化模型，以及分布式设置下学习。现有的方法可以在每个方面进行扩展。然而，一些重要的企业端服务器应用（例如，全球范围内的在线项目推荐）需要同时考虑这三个方面。因此，我们提出了一种新的学习设定——分布式个性化在线 federated learning，它同时考虑了这三个方面。在这种新的学习设定中，技术挑战之一是如何将来自邻居客户端的共享模型参数集成到每个客户端上，以获得个性化的本地模型。我们提议直接通过优化本地模型的性能来学习权重。这不仅提高了每个本地模型的个性化程度，还帮助本地模型适应数据变化，通过智能地包含邻居客户端的信息来适应可能出现的数据变化。另一个挑战是如何选择每个客户端的邻居。我们提议基于学习的权重来选择邻居，使每个客户端可以选择最有助的邻居，同时降低通信成本。我们对三个实际的 item recommendation 数据集和一个空气质量预测数据集进行了验证和robustness测试，结果表明我们的方法是有效和可靠的。
</details></li>
</ul>
<hr>
<h2 id="MTGER-Multi-view-Temporal-Graph-Enhanced-Temporal-Reasoning-over-Time-Involved-Document"><a href="#MTGER-Multi-view-Temporal-Graph-Enhanced-Temporal-Reasoning-over-Time-Involved-Document" class="headerlink" title="MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document"></a>MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04816">http://arxiv.org/abs/2311.04816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, Bing Qin</li>
<li>for: 这个论文是用于解决文档中的时间关系和推理问题的。</li>
<li>methods: 该论文提出了一种多视图时间图加强的时间推理框架（MTGER），该框架可以Explicitly模型文档中的时间关系，并通过多视图机制和自动融合来提高模型的隐式推理能力。</li>
<li>results: 实验结果表明，MTGER可以在TimeQA和SituatedQA datasets上达到显著的效果，并且在问题变化时能够给出更一致的答案。<details>
<summary>Abstract</summary>
The facts and time in the document are intricately intertwined, making temporal reasoning over documents challenging. Previous work models time implicitly, making it difficult to handle such complex relationships. To address this issue, we propose MTGER, a novel Multi-view Temporal Graph Enhanced Temporal Reasoning framework for temporal reasoning over time-involved documents. Concretely, MTGER explicitly models the temporal relationships among facts by multi-view temporal graphs. On the one hand, the heterogeneous temporal graphs explicitly model the temporal and discourse relationships among facts; on the other hand, the multi-view mechanism captures both time-focused and fact-focused information, allowing the two views to complement each other through adaptive fusion. To further improve the implicit reasoning capability of the model, we design a self-supervised time-comparing objective. Extensive experimental results demonstrate the effectiveness of our method on the TimeQA and SituatedQA datasets. Furthermore, MTGER gives more consistent answers under question perturbations.
</details>
<details>
<summary>摘要</summary>
文档中的事实和时间关系紧密相连，使得文档中的时间逻辑推理变得困难。前期工作中的模型对时间进行了隐式表示，导致处理复杂的时间关系变得困难。为解决这个问题，我们提议MTGER，一种基于多视图时间图的新型多视图时间图增强的时间逻辑推理框架。具体来说，MTGER使用多视图时间图来明确事实之间的时间关系。一方面，不同视图中的时间图表示了事实之间的时间和论述关系；另一方面，多视图机制使得时间和事实信息相互补充，通过适应融合来增强模型的隐式逻辑能力。此外，我们还设计了一个自动supervised时间比较目标，以提高模型的隐式逻辑能力。实验结果表明，MTGER在TimeQA和SituatedQA datasets上具有显著的效果，并且在问题扰动下的答案更加一致。
</details></li>
</ul>
<hr>
<h2 id="DACBERT-Leveraging-Dependency-Agreement-for-Cost-Efficient-Bert-Pretraining"><a href="#DACBERT-Leveraging-Dependency-Agreement-for-Cost-Efficient-Bert-Pretraining" class="headerlink" title="DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining"></a>DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04799">http://arxiv.org/abs/2311.04799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/fa101e30ca4ffd6a0479993b0e1c7299d2311c0416c0b68e2551534430e1e8fe">https://github.com/sw-packages/fa101e30ca4ffd6a0479993b0e1c7299d2311c0416c0b68e2551534430e1e8fe</a></li>
<li>paper_authors: Martin Kuo, Jianyi Zhang, Yiran Chen</li>
<li>for: 提高预训练模型的性能和可解性，以及增强自然语言理解任务中预训练模型的表现。</li>
<li>methods: 提出了一种新的预训练模型——依赖协议拟合BERT（DACBERT），并开发了一种两阶段预训练方框架——依赖协议预训练。这个方框架基于语言理论，将语法和 semantics信息灵活地纳入预训练过程中。第一阶段使用四个专门的子模型来捕捉chunk级别的代表性依赖关系，并将这些依赖关系转化为嵌入。第二阶段使用这些精细化的嵌入，与传统的BERT嵌入相结合，导向预训练 осталь部分的模型。</li>
<li>results: 在GLUE测试 benchmark上，我们的DACBERT表现出色，在不同任务中的表现都有所提高，比Crammed BERT提高3.13%的RTE任务和2.26%的MRPC任务。此外，我们的方法可以在单个GPU上，在24小时内完成预训练过程，不需要额外的计算资源或延长预训练时间。广泛的研究还证明了我们的方法在自然语言理解任务中的表现是可靠的。<details>
<summary>Abstract</summary>
Building on the cost-efficient pretraining advancements brought about by Crammed BERT, we enhance its performance and interpretability further by introducing a novel pretrained model Dependency Agreement Crammed BERT (DACBERT) and its two-stage pretraining framework - Dependency Agreement Pretraining. This framework, grounded by linguistic theories, seamlessly weaves syntax and semantic information into the pretraining process. The first stage employs four dedicated submodels to capture representative dependency agreements at the chunk level, effectively converting these agreements into embeddings. The second stage uses these refined embeddings, in tandem with conventional BERT embeddings, to guide the pretraining of the rest of the model. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable improvement across various tasks, surpassing Crammed BERT by 3.13% in the RTE task and by 2.26% in the MRPC task. Furthermore, our method boosts the average GLUE score by 0.83%, underscoring its significant potential. The pretraining process can be efficiently executed on a single GPU within a 24-hour cycle, necessitating no supplementary computational resources or extending the pretraining duration compared with the Crammed BERT. Extensive studies further illuminate our approach's instrumental role in bolstering the interpretability of pretrained language models for natural language understanding tasks.
</details>
<details>
<summary>摘要</summary>
基于Cost-efficient pre-training的进步，我们又提出了一种新的预训练模型——Dependency Agreement Crammed BERT（DACBERT）和其两个阶段预训练框架——Dependency Agreement Pretraining。这个框架，基于语言理论，通过将 sintax和semantic信息灵活地整合到预训练过程中来提高模型的性能和可解性。第一个阶段使用四个专门的子模型来捕捉chunk级别的代名词协议，并将其转化为嵌入。第二个阶段使用这些精炼的嵌入，与普通的BERT嵌入一起，导引预训练其余部分的模型。在GLUE标准测试 benchmark上，我们的DACBERT表现出色，在RTE任务上超过Crammed BERT by 3.13%，在MRPC任务上超过by 2.26%。此外，我们的方法提高了GLUE平均分数 by 0.83%，强调其显著的潜力。预训练过程可以在单个GPU上完成 within a 24-hour cycle，不需要补充的计算资源或延长预训练时间与Crammed BERT相比。广泛的研究也证明了我们的方法在自然语言理解任务中增强预训练语言模型的可解性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Multiple-Roles-of-Ontologies-in-Explainable-AI"><a href="#On-the-Multiple-Roles-of-Ontologies-in-Explainable-AI" class="headerlink" title="On the Multiple Roles of Ontologies in Explainable AI"></a>On the Multiple Roles of Ontologies in Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04778">http://arxiv.org/abs/2311.04778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roberto Confalonieri, Giancarlo Guizzardi</li>
<li>for: 这篇论文探讨了ontology在可解释AI和人类中心的解释系统中的不同角色。</li>
<li>methods: 论文考虑了三个主要的ontology应用角度，包括参考模型、通用常识逻辑和知识精简和复杂性管理。</li>
<li>results: 论文结论提出了 Ontology-based 方法可以帮助解释AI 的人类理解和效果，但还需要解决一些挑战。<details>
<summary>Abstract</summary>
This paper discusses the different roles that explicit knowledge, in particular ontologies, can play in Explainable AI and in the development of human-centric explainable systems and intelligible explanations. We consider three main perspectives in which ontologies can contribute significantly, namely reference modelling, common-sense reasoning, and knowledge refinement and complexity management. We overview some of the existing approaches in the literature, and we position them according to these three proposed perspectives. The paper concludes by discussing what challenges still need to be addressed to enable ontology-based approaches to explanation and to evaluate their human-understandability and effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vital-Sign-Forecasting-for-Sepsis-Patients-in-ICUs"><a href="#Vital-Sign-Forecasting-for-Sepsis-Patients-in-ICUs" class="headerlink" title="Vital Sign Forecasting for Sepsis Patients in ICUs"></a>Vital Sign Forecasting for Sepsis Patients in ICUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04770">http://arxiv.org/abs/2311.04770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anubhav Bhatti, Yuwei Liu, Chen Dan, Bingjie Shen, San Lee, Yonghwan Kim, Jang Yong Kim</li>
<li>for: 预测Intensive Care Units（ICU）中病人的生命体征指标，帮助医疗工作者早发现生命体征不稳定的迹象并预测 septic shock 的发展</li>
<li>methods: 使用现代深度学习（DL）架构，开发了一种多步预测系统，利用历史生命体征数据预测未来的生命体征状况</li>
<li>results: 比较了三种DL模型（N-BEATS、N-HiTS、Temporal Fusion Transformer）在 eICU Collaborative Research Database 上的预测能力，发现 TFT 模型能够更好地捕捉生命体征趋势，而 N-HiTS 模型能够更好地保持生命体征短期波动在预定范围内<details>
<summary>Abstract</summary>
Sepsis and septic shock are a critical medical condition affecting millions globally, with a substantial mortality rate. This paper uses state-of-the-art deep learning (DL) architectures to introduce a multi-step forecasting system to predict vital signs indicative of septic shock progression in Intensive Care Units (ICUs). Our approach utilizes a short window of historical vital sign data to forecast future physiological conditions. We introduce a DL-based vital sign forecasting system that predicts up to 3 hours of future vital signs from 6 hours of past data. We further adopt the DILATE loss function to capture better the shape and temporal dynamics of vital signs, which are critical for clinical decision-making. We compare three DL models, N-BEATS, N-HiTS, and Temporal Fusion Transformer (TFT), using the publicly available eICU Collaborative Research Database (eICU-CRD), highlighting their forecasting capabilities in a critical care setting. We evaluate the performance of our models using mean squared error (MSE) and dynamic time warping (DTW) metrics. Our findings show that while TFT excels in capturing overall trends, N-HiTS is superior in retaining short-term fluctuations within a predefined range. This paper demonstrates the potential of deep learning in transforming the monitoring systems in ICUs, potentially leading to significant improvements in patient care and outcomes by accurately forecasting vital signs to assist healthcare providers in detecting early signs of physiological instability and anticipating septic shock.
</details>
<details>
<summary>摘要</summary>
septic shock 和 septic shock 是一种严重的医疗情况，影响全球数百万人，mortality rate 较高。这篇论文使用当前最先进的深度学习（DL）建筑，引入一种多步预测系统，以预测 ICU 中重要的生物参数，以便评估 septic shock 的进程。我们的方法使用一个短时间的历史生物参数数据，预测未来的生物参数。我们引入了 DL 基于的生物参数预测系统，可以预测未来 3 小时的生物参数，从 6 小时的历史数据中。我们进一步采用 DILATE 损失函数，以更好地捕捉生物参数的形态和时间动态，这些是临床决策中的关键。我们使用公共可用的 eICU 合作研究数据库（eICU-CRD），比较三种 DL 模型（N-BEATS、N-HiTS 和 Temporal Fusion Transformer ）的预测能力。我们使用 mean squared error（MSE）和 dynamic time warping（DTW） метри来评估我们的模型。我们的发现显示，虽然 TFT 能够捕捉总趋势，但 N-HiTS 在保持短期波动内的范围内表现更优。这篇论文示出了深度学习在 ICU 监测系统中的潜在潜力，可能导致患者护理和结果的显著改善，通过准确预测生物参数，帮助医疗提供者早期检测生物参数的不稳定，预测 septic shock。
</details></li>
</ul>
<hr>
<h2 id="The-voraus-AD-Dataset-for-Anomaly-Detection-in-Robot-Applications"><a href="#The-voraus-AD-Dataset-for-Anomaly-Detection-in-Robot-Applications" class="headerlink" title="The voraus-AD Dataset for Anomaly Detection in Robot Applications"></a>The voraus-AD Dataset for Anomaly Detection in Robot Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04765">http://arxiv.org/abs/2311.04765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Thieß Brockmann, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt</li>
<li>For: This paper aims to provide a dataset for anomaly detection (AD) in robotic applications, and to introduce a new baseline method called MVT-Flow that outperforms previous baselines by a large margin.* Methods: The paper uses machine data from a pick-and-place application to create a dataset for AD, and introduces MVT-Flow, a deep-learning-based density estimation method that takes the structure of the data domain into account.* Results: The paper shows that MVT-Flow outperforms previous baselines by a large margin of 6.2% in area under ROC.Here is the text in Simplified Chinese:* For: 这篇论文目的是为了提供机器人应用中的异常检测（AD）数据集，并引入一种新的基线方法called MVT-Flow，该方法在ROC领域的表现明显超过了之前的基线方法。* Methods: 论文使用机器人执行pick-and-place任务的机器数据创建了AD数据集，并引入MVT-Flow方法，该方法是基于深度学习的概率分布预测方法，它采用了数据领域的结构来 tailor its architecture。* Results: 论文表明，MVT-Flow方法在ROC领域的表现比之前的基线方法高出6.2%的差。<details>
<summary>Abstract</summary>
During the operation of industrial robots, unusual events may endanger the safety of humans and the quality of production. When collecting data to detect such cases, it is not ensured that data from all potentially occurring errors is included as unforeseeable events may happen over time. Therefore, anomaly detection (AD) delivers a practical solution, using only normal data to learn to detect unusual events. We introduce a dataset that allows training and benchmarking of anomaly detection methods for robotic applications based on machine data which will be made publicly available to the research community. As a typical robot task the dataset includes a pick-and-place application which involves movement, actions of the end effector and interactions with the objects of the environment. Since several of the contained anomalies are not task-specific but general, evaluations on our dataset are transferable to other robotics applications as well. Additionally, we present MVT-Flow (multivariate time-series flow) as a new baseline method for anomaly detection: It relies on deep-learning-based density estimation with normalizing flows, tailored to the data domain by taking its structure into account for the architecture. Our evaluation shows that MVT-Flow outperforms baselines from previous work by a large margin of 6.2% in area under ROC.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Euclidean-Projective-Conformal-Choosing-a-Geometric-Algebra-for-Equivariant-Transformers"><a href="#Euclidean-Projective-Conformal-Choosing-a-Geometric-Algebra-for-Equivariant-Transformers" class="headerlink" title="Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers"></a>Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04744">http://arxiv.org/abs/2311.04744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pim de Haan, Taco Cohen, Johann Brehmer</li>
<li>for: 该论文旨在开发一种基于几何深度学习的灵活架构，即几何深度学习变换器（GATr）。</li>
<li>methods: 该论文使用几何代数来扩展GATr架构，使其适用于任何几何（或CLIFFORD）代数。作者还研究了使用不同几何代数的版本，包括几何代数、 проектив代数和对称代数，来表示3D数据。</li>
<li>results: 作者在理论和实践中评估了这些不同版本，发现 simplest几何版本 computationally cheap，但 symmetry group smaller，不够表达能力，而 projective model 表达能力不够。对称代数和改进的 проектив代数定义出了强大、高性能的架构。<details>
<summary>Abstract</summary>
The Geometric Algebra Transformer (GATr) is a versatile architecture for geometric deep learning based on projective geometric algebra. We generalize this architecture into a blueprint that allows one to construct a scalable transformer architecture given any geometric (or Clifford) algebra. We study versions of this architecture for Euclidean, projective, and conformal algebras, all of which are suited to represent 3D data, and evaluate them in theory and practice. The simplest Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient, while the projective model is not sufficiently expressive. Both the conformal algebra and an improved version of the projective algebra define powerful, performant architectures.
</details>
<details>
<summary>摘要</summary>
“几何深度学习构件（GATr）是一种多功能的构件，基于射影几何代数。我们将这个构件转换为可扩展的构件，让你可以根据任何几何（或Clifford）代数建立扩展性强的 transformer 架构。我们研究了这些架构的几何、 проектив和对称 algebra 版本，这些版本都适合表示3D数据，并进行了理论和实践评估。最简单的几何架构较便宜计算，但它的同调集小，不够sample-efficient，而 проектив模型不够表达力。对称 algebra 和改进的 projetive algebra 定义了强大、高性能的架构。”Note that Simplified Chinese is a simplified version of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world where traditional Chinese is prevalent.
</details></li>
</ul>
<hr>
<h2 id="The-Quest-for-Content-A-Survey-of-Search-Based-Procedural-Content-Generation-for-Video-Games"><a href="#The-Quest-for-Content-A-Survey-of-Search-Based-Procedural-Content-Generation-for-Video-Games" class="headerlink" title="The Quest for Content: A Survey of Search-Based Procedural Content Generation for Video Games"></a>The Quest for Content: A Survey of Search-Based Procedural Content Generation for Video Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04710">http://arxiv.org/abs/2311.04710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mar Zamorano, Carlos Cetina, Federica Sarro</li>
<li>for: 游戏内容的大量生成，以满足日益增长的游戏需求。</li>
<li>methods: 使用搜索算法实现自动化内容生成。</li>
<li>results: 对SBPCG领域的现状和未来研究方向的报告，以及一些实践者可采取的建议。<details>
<summary>Abstract</summary>
Video games demand is constantly increasing, which requires the costly production of large amounts of content. Towards this challenge, researchers have developed Search-Based Procedural Content Generation (SBPCG), that is, the (semi-)automated creation of content through search algorithms. We survey the current state of SBPCG, reporting work appeared in the field between 2011-2022 and identifying open research challenges. The results lead to recommendations for practitioners and to the identification of several potential future research avenues for SBPCG.
</details>
<details>
<summary>摘要</summary>
电子游戏的需求不断增长，需要大量的内容生成，而这也导致了高昂的生产成本。为了应对这个挑战，研究人员开发了搜索基于生成内容的技术（Search-Based Procedural Content Generation，SBPCG），即通过搜索算法（semi-)自动生成内容。我们对SBPCG领域的当前状况进行了报告，涵盖2011-2022年间出版的研究成果，并确定了一些未解决的研究挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Challenging-Common-Assumptions-in-Multi-task-Learning"><a href="#Challenging-Common-Assumptions-in-Multi-task-Learning" class="headerlink" title="Challenging Common Assumptions in Multi-task Learning"></a>Challenging Common Assumptions in Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04698">http://arxiv.org/abs/2311.04698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott</li>
<li>for: 本研究探讨多任务学习（MTL）下的下降搜索方法，尤其是在单任务学习（STL）基础上的情况下。</li>
<li>methods: 本研究使用了常用的STL工具，如Adam优化器，并证明Adam优化器在MTL中的效iveness归功于部分损失度量的兼容性。此外，本研究还研究了梯度冲突的角色在MTL和STL中，并发现梯度强度作为主要 отли异点。</li>
<li>results: 对于常见的图像损害，本研究未发现MTL对特征传递性的明显优势。总的来说，本研究发现MTL和STL在一些方面存在相似之处，建议在更广泛的上下文中考虑这两种方法。<details>
<summary>Abstract</summary>
While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge common assumptions in MTL in the context of STL: First, the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial loss-scale invariance. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no evidence that this is a unique problem in MTL. We emphasize differences in gradient magnitude as the main distinguishing factor. Lastly, we compare the transferability of features learned through MTL and STL on common image corruptions, and find no conclusive evidence that MTL leads to superior transferability. Overall, we find surprising similarities between STL and MTL suggesting to consider methods from both fields in a broader context.
</details>
<details>
<summary>摘要</summary>
MTL（多任务学习）在最近几年内得到了广泛关注，但它的内在机制仍未得到了充分理解。现有方法不能够在单任务学习（STL）基础上实现一致性的性能提升，这重申了对MTL的深入理解的重要性。在我们的研究中，我们挑战了MTL中通常被假设的一些假设：首先，MTL中选择优化器的研究只是轻度地进行了调查。我们表明了通用STL工具such as Adam优化器在MTL中的重要作用。我们发现Adam的有效性归功于它的部分损失尺度不变性。其次，在MTL中 gradient conflicts 的概念经常被宣称为特定的问题。我们探讨了MTL中 gradient conflicts 的角色，并与 STL 进行比较。对于 angular gradient alignment，我们未能发现这是MTL中独特的问题。我们强调了 gradient magnitude 的差异作为主要 отлича点。最后，我们比较了通过 MTL 和 STL 学习的特征的传输性，并未发现MTL leads to superior transferability。总的来说，我们发现了MTL 和 STL 之间的意外相似之处，建议在更广泛的上下文中考虑这两个领域的方法。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Speculative-Sampling-and-KV-Cache-Optimizations-Together-for-Generative-AI-using-OpenVINO"><a href="#Leveraging-Speculative-Sampling-and-KV-Cache-Optimizations-Together-for-Generative-AI-using-OpenVINO" class="headerlink" title="Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO"></a>Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04951">http://arxiv.org/abs/2311.04951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/openvino_notebooks">https://github.com/openvinotoolkit/openvino_notebooks</a></li>
<li>paper_authors: Haim Barad, Ekaterina Aidova, Yury Gorbachev</li>
<li>for: 提高用户体验和减少基础设施成本和能耗</li>
<li>methods: 使用幻数据批处理和量化优化</li>
<li>results: 提高文本生成响应时间，并与标准抽样相比较Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the user experience and reduce infrastructure costs and power consumption by optimizing text generation using inference optimizations.</li>
<li>methods: The paper proposes using speculative sampling, a form of dynamic execution, to reduce the overall latency of text generation. The authors also use model-based optimizations such as quantization and KV caching.</li>
<li>results: The paper compares the performance of speculative sampling with standard autoregressive sampling and shows that speculative sampling can improve the response time of text generation.<details>
<summary>Abstract</summary>
Inference optimizations are critical for improving user experience and reducing infrastructure costs and power consumption. In this article, we illustrate a form of dynamic execution known as speculative sampling to reduce the overall latency of text generation and compare it with standard autoregressive sampling. This can be used together with model-based optimizations (e.g. quantization) to provide an optimized solution. Both sampling methods make use of KV caching. A Jupyter notebook and some sample executions are provided.
</details>
<details>
<summary>摘要</summary>
推理优化是提高用户体验和减少基础设施成本和电力消耗的关键。在这篇文章中，我们介绍了一种动态执行技术known as speculative sampling，用于减少文本生成总延迟。我们还与标准排取样本相比较。这两种抽样方法都使用KV缓存。我们提供了一个Jupyter笔记和一些示例执行。
</details></li>
</ul>
<hr>
<h2 id="Diff-HierVC-Diffusion-based-Hierarchical-Voice-Conversion-with-Robust-Pitch-Generation-and-Masked-Prior-for-Zero-shot-Speaker-Adaptation"><a href="#Diff-HierVC-Diffusion-based-Hierarchical-Voice-Conversion-with-Robust-Pitch-Generation-and-Masked-Prior-for-Zero-shot-Speaker-Adaptation" class="headerlink" title="Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation"></a>Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04693">http://arxiv.org/abs/2311.04693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hayeong0/Diff-HierVC">https://github.com/hayeong0/Diff-HierVC</a></li>
<li>paper_authors: Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee</li>
<li>for: 提高voice conversion（VC）系统的精度和声音适应质量</li>
<li>methods: 基于两个扩散模型的层次VC系统（Diff-HierVC），包括DiffPitch和DiffVoice两部分</li>
<li>results: 实验结果表明，our模型在抽象F0生成和声音风格转换方面具有优异表现，并在零基elineVC场景中达到CER&#x3D;0.83%和EER&#x3D;3.29%的性能。<details>
<summary>Abstract</summary>
Although voice conversion (VC) systems have shown a remarkable ability to transfer voice style, existing methods still have an inaccurate pitch and low speaker adaptation quality. To address these challenges, we introduce Diff-HierVC, a hierarchical VC system based on two diffusion models. We first introduce DiffPitch, which can effectively generate F0 with the target voice style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech with a target voice style. Furthermore, using the source-filter encoder, we disentangle the speech and use the converted Mel-spectrogram as a data-driven prior in DiffVoice to improve the voice style transfer capacity. Finally, by using the masked prior in diffusion models, our model can improve the speaker adaptation quality. Experimental results verify the superiority of our model in pitch generation and voice style transfer performance, and our model also achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios.
</details>
<details>
<summary>摘要</summary>
尽管voice conversion（VC）系统已经表现出了很好的语音风格传递能力，现有的方法仍然具有不准确的抑音和低端应用质量。为解决这些挑战，我们提出了Diff-HierVC，一种基于两个扩散模型的层次VC系统。我们首先引入DiffPitch，它可以有效地生成F0目标声音风格。然后，生成的F0被 fed到DiffVoice中，用来转换语音为目标声音风格。此外，通过源-滤波器编码器，我们分离出语音，并使用转换后的Mel-spectrogram作为数据驱动的先验知识来提高voice style转换能力。最后，通过在扩散模型中使用假标记先验，我们的模型可以提高 speaker adaptation质量。实验结果证明我们的模型在抑音和voice style转换性能方面具有优势，并且在零shot VC场景下 achieved CER of 0.83%和EER of 3.29%。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-LLMs-using-human-like-development-data-corpus"><a href="#Pre-training-LLMs-using-human-like-development-data-corpus" class="headerlink" title="Pre-training LLMs using human-like development data corpus"></a>Pre-training LLMs using human-like development data corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04666">http://arxiv.org/abs/2311.04666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushi Bhardwaj, Raj Sanjay Shah, Sashank Varma</li>
<li>for: 这个论文的目的是测试大型自然语言模型（LLMs）在语言理解和推理任务中的表现，以及模型在不同的训练环境下的可重复性和稳定性。</li>
<li>methods: 这个论文使用了大量的raw文本数据进行预训练，并对LLMs进行了评估，以评估模型在不同的训练环境下的表现。</li>
<li>results: 论文提出了一系列的基准值，包括不同架构、评估 epochs 的变化和报告的预训练 metric，以及对RoBERTa 基线值的评估。<details>
<summary>Abstract</summary>
Pre-trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre-training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre-training to human language acquisition, where the number of tokens seen by 13-year-old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures, evaluation of changes in performance across epochs, and reported pre-training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict-small tracks in this report.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在多种语言推理和理解任务中表现出色。LLMs的预训阶段将关注大量的原始文本数据。在这个工作中，我们将LLMs预训和评估其能够学习上下文 word 表现。我们使用相似数量的字元与13岁儿童所看到的字元进行比较。我们提供了强大的基准点，包括不同架构、评估改变过程中的表现、和预训中的 metric。我们还尝试复制RoBERTa基eline，以观察对于数据选择和可重现性的训练稳定性。我们在这份报告中提供了预训和紧缩小 tracks 的提交细节。
</details></li>
</ul>
<hr>
<h2 id="Pragmatic-Reasoning-Unlocks-Quantifier-Semantics-for-Foundation-Models"><a href="#Pragmatic-Reasoning-Unlocks-Quantifier-Semantics-for-Foundation-Models" class="headerlink" title="Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models"></a>Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04659">http://arxiv.org/abs/2311.04659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyuan Li, Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava</li>
<li>for: This paper aims to explore the ability of recent foundation models to understand generalized quantifiers in natural language, specifically in sentences featuring percentage-equipped predicates.</li>
<li>methods: The paper uses a crowd-sourced dataset of human-annotated generalized quantifiers in Wikipedia sentences, called QuRe, and a framework called PRESQUE, which combines natural language inference and the Rational Speech Acts framework, to test the ability of language models to understand quantifier percentage scopes.</li>
<li>results: The experimental results on the HVD dataset and QuRe show that PRESQUE, which uses pragmatic reasoning, performs 20% better than a literal reasoning baseline when predicting quantifier percentage scopes, with no additional training required.<details>
<summary>Abstract</summary>
Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates are satisfied (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be helpful for tasks like logic formalization and surface-form quantitative reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains unclear if recent foundation models possess this ability, as they lack direct training signals. To explore this, we introduce QuRe, a crowd-sourced dataset of human-annotated generalized quantifiers in Wikipedia sentences featuring percentage-equipped predicates. We explore quantifier comprehension in language models using PRESQUE, a framework that combines natural language inference and the Rational Speech Acts framework. Experimental results on the HVD dataset and QuRe illustrate that PRESQUE, employing pragmatic reasoning, performs 20% better than a literal reasoning baseline when predicting quantifier percentage scopes, with no additional training required.
</details>
<details>
<summary>摘要</summary>
通用量词（例如，很少、大多数）用于指示逻辑 predicate 满足的порпорzioni（例如，一些苹果是红色的）。一种方法可以理解量词 semantics 是通过显式绑定这些满足情况的百分比范围（例如，30%-40% 的苹果是红色的）。这种方法可以对逻辑ormalization和表面形量化理性（Gordon 和Schubert，2010；Roy 等，2015）进行帮助。然而，是否现代基础模型拥有这种能力仍然未知，因为它们缺乏直接训练信号。为了探索这一点，我们引入 QuRe，一个由人工标注的通用量词在Wikipedia句子中出现的百分比范围的数据集。我们使用 PRESQUE，一个结合自然语言推理和理性演讲框架的框架，来探索语言模型中量词理解的能力。实验结果表明，PRESQUE，通过使用 Pragmatic Reasoning，在 HVD 数据集和 QuRe 上预测量词百分比范围时，与 Literal Reasoning 基准相比，提高了20%的性能，无需额外训练。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Focal-and-Full-Range-Attention-Based-Graph-Transformers"><a href="#Hybrid-Focal-and-Full-Range-Attention-Based-Graph-Transformers" class="headerlink" title="Hybrid Focal and Full-Range Attention Based Graph Transformers"></a>Hybrid Focal and Full-Range Attention Based Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04653">http://arxiv.org/abs/2311.04653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhong Zhu, Zhenhao Zhao, Weiran Cai</li>
<li>for: 这篇论文旨在提高图structured数据学习中Graph Transformer的性能，通过增强对本地信息的捕捉和全范围相关性的学习。</li>
<li>methods: 该论文提出了一种新的具有复合注意力机制的强化图Transformer模型，named Focal and Full-Range Graph Transformer (FFGT)，通过结合全范围注意力和K-hop焦点注意力来捕捉全范围和本地信息。</li>
<li>results: 该论文在多个开放数据集上提高了现有的Graph Transformer性能，同时在一些Long-Range Graph Benchmark (LRGB)数据集上达到了与普通Transformer相同的SOTA性能，而无需任何特殊的参数调整或特定的数据预处理。<details>
<summary>Abstract</summary>
The paradigm of Transformers using the self-attention mechanism has manifested its advantage in learning graph-structured data. Yet, Graph Transformers are capable of modeling full range dependencies but are often deficient in extracting information from locality. A common practice is to utilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture local information, which however are still inadequate for comprehending substructures. In this paper, we present a purely attention-based architecture, namely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the loss of local information in learning global correlations. The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information. Beyond the scope of canonical Transformers, the FFGT has the merit of being more substructure-aware. Our approach enhances the performance of existing Graph Transformers on various open datasets, while achieves compatible SOTA performance on several Long-Range Graph Benchmark (LRGB) datasets even with a vanilla transformer. We further examine influential factors on the optimal focal length of attention via introducing a novel synthetic dataset based on SBM-PATTERN.
</details>
<details>
<summary>摘要</summary>
“对于使用自我注意机制的Transformers模型，它在学习图像数据中表现出了优势。然而，图像Transformers可以模型全范围的相互关系，但通常缺乏从本地获取信息的能力。为了解决这个问题，常用Message Passing Neural Networks（MPNNs）作为辅助，以便捕捉本地信息，但这些MPNNs仍然无法彻底理解子结构。在这篇论文中，我们提出了一个纯注意 mechanism的架构，即全范围注意和K-hop焦点注意的复合注意机制（FFGT），以便储存全范围和本地信息。与传统Transformers不同的是，FFGT更加注重到子结构。我们的方法可以提高现有的图像Transformers的性能，并在多个Long-Range Graph Benchmark（LRGB）dataset上实现了Compatible SOTA的性能，甚至使用普通的Transformer。我们进一步研究了注意力的最佳焦点因素，并通过引入一个基于SBM-PATTERN的新 sintetic dataset。”
</details></li>
</ul>
<hr>
<h2 id="SKU-Patch-Towards-Efficient-Instance-Segmentation-for-Unseen-Objects-in-Auto-Store"><a href="#SKU-Patch-Towards-Efficient-Instance-Segmentation-for-Unseen-Objects-in-Auto-Store" class="headerlink" title="SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store"></a>SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04645">http://arxiv.org/abs/2311.04645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biqi Yang, Weiliang Tang, Xiaojie Gao, Xianzhi Li, Yun-Hui Liu, Chi-Wing Fu, Pheng-Ann Heng</li>
<li>for: 这篇论文主要针对大规模库存中的自动采矿领域问题，旨在提供一个新的patch-guided实例分割方案，以减少人工干预和模型重训。</li>
<li>methods: 本文提出了一个 novel transformer-based网络，包括(i)一个patch-image相联 corrle encoder，用于捕捉多个层次的图像特征，并(ii)一个patch-aware transformer decoder，用于生成实例 mask。</li>
<li>results: 实验结果显示，SKU-Patch可以在四个库存测试 benchmark上取得最佳性能，并在一个真实的自动存储运输管线中，实现了逾90%的抓捕成功率，证明其实用性和可行性。<details>
<summary>Abstract</summary>
In large-scale storehouses, precise instance masks are crucial for robotic bin picking but are challenging to obtain. Existing instance segmentation methods typically rely on a tedious process of scene collection, mask annotation, and network fine-tuning for every single Stock Keeping Unit (SKU). This paper presents SKU-Patch, a new patch-guided instance segmentation solution, leveraging only a few image patches for each incoming new SKU to predict accurate and robust masks, without tedious manual effort and model re-training. Technical-wise, we design a novel transformer-based network with (i) a patch-image correlation encoder to capture multi-level image features calibrated by patch information and (ii) a patch-aware transformer decoder with parallel task heads to generate instance masks. Extensive experiments on four storehouse benchmarks manifest that SKU-Patch is able to achieve the best performance over the state-of-the-art methods. Also, SKU-Patch yields an average of nearly 100% grasping success rate on more than 50 unseen SKUs in a robot-aided auto-store logistic pipeline, showing its effectiveness and practicality.
</details>
<details>
<summary>摘要</summary>
大规模仓库中，精准实例掩模是机器人抓取物品的关键，但是它们很难以获得。现有的实例分割方法通常需要 tedious scene collection、标注和网络微调，对每个 Stock Keeping Unit (SKU) 进行一个一一的处理。本文介绍了 SKU-Patch，一种新的 patch-guided 实例分割解决方案，只需要对每个新的 SKU 输入一些图像块来预测准确和可靠的掩模，不需要手动劳累和模型重新训练。技术上，我们设计了一种基于 transformer 网络的新网络，包括：(i) 一个 patch-image correlation encoder，用于捕捉多级图像特征，并将其与块信息相协调。(ii) 一个 patch-aware transformer decoder，包括多个并行任务头，用于生成实例掩模。经验表明，SKU-Patch 能够在四个仓库测试准则上取得最好的性能，并且在 robot-aided auto-store 物流管线中，SKU-Patch 可以实现97%的抓取成功率，验证了其实用性和实际性。
</details></li>
</ul>
<hr>
<h2 id="Object-Centric-Learning-with-Slot-Mixture-Module"><a href="#Object-Centric-Learning-with-Slot-Mixture-Module" class="headerlink" title="Object-Centric Learning with Slot Mixture Module"></a>Object-Centric Learning with Slot Mixture Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04640">http://arxiv.org/abs/2311.04640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniil Kirilenko, Vitaliy Vorobyov, Alexey K. Kovalev, Aleksandr I. Panov</li>
<li>for: 这篇论文是为了提出一种基于 Gaussian Mixture Model 的学习式划分方法，用于改进 object-centric 架构中的 slot 表示方法。</li>
<li>methods: 该方法使用学习式划分方法来分解特征图像，并将分配给 slot 的信息包含在 slot 表示中，从而得到更表示力的 slot 表示。</li>
<li>results: 对于 object-centric enario，使用该方法而不使用 Slot Attention 可以提高性能，达到目前最佳Result 在 set property prediction 任务中。<details>
<summary>Abstract</summary>
Object-centric architectures usually apply a differentiable module to the entire feature map to decompose it into sets of entity representations called slots. Some of these methods structurally resemble clustering algorithms, where the cluster's center in latent space serves as a slot representation. Slot Attention is an example of such a method, acting as a learnable analog of the soft k-means algorithm. Our work employs a learnable clustering method based on the Gaussian Mixture Model. Unlike other approaches, we represent slots not only as centers of clusters but also incorporate information about the distance between clusters and assigned vectors, leading to more expressive slot representations. Our experiments demonstrate that using this approach instead of Slot Attention improves performance in object-centric scenarios, achieving state-of-the-art results in the set property prediction task.
</details>
<details>
<summary>摘要</summary>
通常，对象中心的架构会应用一个可导模块到整个特征地图，将其分解成Entity表示集合的集合。一些这些方法结构上类似于聚类算法，其中聚类中心在隐藏空间中服务为槽表示。槽注意力是一 such方法， behaving as a learnable soft k-means algorithm。我们的工作使用一种学习聚类方法基于 Gaussian Mixture Model。与其他方法不同，我们在表示槽不仅包括聚类中心，还包括聚类与分配向量之间的距离信息，导致更具表达力的槽表示。我们的实验表明，使用这种方法而不是槽注意力可以在对象中心的情况下提高性能，实现了属性集 Prediction任务的国际最佳 результа。
</details></li>
</ul>
<hr>
<h2 id="Explained-anomaly-detection-in-text-reviews-Can-subjective-scenarios-be-correctly-evaluated"><a href="#Explained-anomaly-detection-in-text-reviews-Can-subjective-scenarios-be-correctly-evaluated" class="headerlink" title="Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?"></a>Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04948">http://arxiv.org/abs/2311.04948</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Novoa-Paradela, Oscar Fontenla-Romero, Bertha Guijarro-Berdiñas</li>
<li>for: 这个研究的目的是检测和解释在线平台上的异常评论。</li>
<li>methods: 这个ipeline包括三个模块，用于检测不会为用户提供价值的评论，包括无用和黑客组成的评论。每个分类都有一个正常性分数和一个解释，以 justify 的决定。</li>
<li>results: 这个ipeline在不同的数据集上进行了评测，并进行了一项解释技术的比较研究，以评估解释模块的效果。这项研究可以帮助自动化在线平台上的评论审核任务，例如电子商务平台上的评论审核，并为相关领域的异常检测任务提供灵感。此外，这项研究还表明了对不同解释技术的人类评估，并探讨了是否可以解释人类化的任务，如异常评论检测。<details>
<summary>Abstract</summary>
This paper presents a pipeline to detect and explain anomalous reviews in online platforms. The pipeline is made up of three modules and allows the detection of reviews that do not generate value for users due to either worthless or malicious composition. The classifications are accompanied by a normality score and an explanation that justifies the decision made. The pipeline's ability to solve the anomaly detection task was evaluated using different datasets created from a large Amazon database. Additionally, a study comparing three explainability techniques involving 241 participants was conducted to assess the explainability module. The study aimed to measure the impact of explanations on the respondents' ability to reproduce the classification model and their perceived usefulness. This work can be useful to automate tasks in review online platforms, such as those for electronic commerce, and offers inspiration for addressing similar problems in the field of anomaly detection in textual data. We also consider it interesting to have carried out a human evaluation of the capacity of different explainability techniques in a real and infrequent scenario such as the detection of anomalous reviews, as well as to reflect on whether it is possible to explain tasks as humanly subjective as this one.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "anomalous reviews" 翻译为 "不常见的评论"* "online platforms" 翻译为 "在线平台"* "worthless or malicious composition" 翻译为 "无用或有恶意组合"* "normality score" 翻译为 "常见度分数"* "explanation" 翻译为 "解释"* "classification model" 翻译为 "分类模型"* "electronic commerce" 翻译为 "电子商务"* "humanly subjective" 翻译为 "有人性的"
</details></li>
</ul>
<hr>
<h2 id="LuminanceL1Loss-A-loss-function-which-measures-percieved-brightness-and-colour-differences"><a href="#LuminanceL1Loss-A-loss-function-which-measures-percieved-brightness-and-colour-differences" class="headerlink" title="LuminanceL1Loss: A loss function which measures percieved brightness and colour differences"></a>LuminanceL1Loss: A loss function which measures percieved brightness and colour differences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04614">http://arxiv.org/abs/2311.04614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominic De Jonge</li>
<li>for: 提高图像修复任务的性能</li>
<li>methods: 使用新的损失函数LuminanceL1Loss，将图像转换为灰度图并计算MSE损失两个频道</li>
<li>results: 对Retinexformer、BUIFD和DnCNN arquitectures进行了评测，并表明LuminanceL1Loss可以超越传统方法，提高图像修复任务的性能，最高提高4.7dB。<details>
<summary>Abstract</summary>
We introduce LuminanceL1Loss, a novel loss function designed to enhance the performance of image restoration tasks. We demonstrate its superiority over MSE when applied to the Retinexformer, BUIFD and DnCNN architectures. Our proposed LuminanceL1Loss leverages a unique approach by transforming images into grayscale and subsequently computing the MSE loss for both grayscale and color channels. Experimental results demonstrate that this innovative loss function consistently outperforms traditional methods, showcasing its potential in image denoising and other related tasks in image reconstruction. It demonstrates gains up to 4.7dB. The results presented in this study highlight the efficacy of LuminanceL1Loss for various image restoration tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的损失函数，即LuminanceL1Loss，用于提高图像恢复任务的性能。我们在Retinexformer、BUIFD和DnCNN架构上进行了实验，并证明了LuminanceL1Loss在这些架构上的优越性。我们的提案的LuminanceL1Loss采用了一种独特的方法，即将图像转换成灰度图像，然后计算灰度和色彩通道之间的MSE损失。实验结果表明，这种创新的损失函数在图像压缩和其他相关的图像重建任务中具有优越的表现，提高了4.7dB。这些研究结果表明LuminanceL1Loss在各种图像恢复任务中的可靠性和普适性。
</details></li>
</ul>
<hr>
<h2 id="TEAL-Tokenize-and-Embed-ALL-for-Multi-modal-Large-Language-Models"><a href="#TEAL-Tokenize-and-Embed-ALL-for-Multi-modal-Large-Language-Models" class="headerlink" title="TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models"></a>TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04589">http://arxiv.org/abs/2311.04589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou</li>
<li>for: 本研究想要帮助多modal语言模型（MM-LLMs）更好地处理多modal输入和生成非文本模式。</li>
<li>methods: 本方法使用了Tokenize和Embed ALl（TEAL）方法，将输入从任何模式转化为token序列，并学习一个共同的嵌入空间 для所有模式。</li>
<li>results: 实验表明，TEAL可以获得显著的多modal理解提升，并实现了一个简单的多modal生成方案。<details>
<summary>Abstract</summary>
Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such as image and audio. Thus, the textual LLM can just work as an interface and maintain its high performance in textual understanding and generation. Experiments show that TEAL achieves substantial improvements in multi-modal understanding, and implements a simple scheme for multi-modal generations.
</details>
<details>
<summary>摘要</summary>
尽管多模态大型语言模型（MM-LLMs）在最近几年内做出了吸人的进步，但它们仍然努力地模型多模态输入的交互和非文本modalities中的生成。在这项工作中，我们提出了TEAL（Tokenize and Embed ALl），一种方法，其中输入从任何模式都会被视为一个token序列，并在一个共享的embedding空间中学习一个共同的embedding矩阵。具体来说，对于输入从任何模式，TEAL首先将它拆分成一个token序列，使用可用的tokenizer进行拆分，然后将token序列embedding到一个共同的embedding空间中，使用一个学习的embedding矩阵。MM-LLMs只需要预测多modal tokens的autoregressive预测，就像文本LLMs一样。最后，对于每个模式，使用预测的token序列生成输出。与共同的embedding空间相比，TEAL使得冻结的LLMs可以在多modal任务中进行理解和生成任务，如图像和音频。因此，文本LLM可以作为界面，维持高效的文本理解和生成能力。实验结果表明，TEAL在多modal理解方面取得了显著的提升，并实现了简单的多modal生成方案。
</details></li>
</ul>
<hr>
<h2 id="Army-of-Thieves-Enhancing-Black-Box-Model-Extraction-via-Ensemble-based-sample-selection"><a href="#Army-of-Thieves-Enhancing-Black-Box-Model-Extraction-via-Ensemble-based-sample-selection" class="headerlink" title="Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection"></a>Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04588">http://arxiv.org/abs/2311.04588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akshitjindal1/aot_wacv">https://github.com/akshitjindal1/aot_wacv</a></li>
<li>paper_authors: Akshit Jindal, Vikram Goyal, Saket Anand, Chetan Arora</li>
<li>for: 防止机器学习模型被盗用（Model Stealing Attacks），当机器学习模型被部署为服务时。</li>
<li>methods: 使用 ensemble of deep learning models 作为盗取模型，以便选择最有用的数据点子集。</li>
<li>results: 比基于单个模型的方法高效，可以提高盗取模型的质量和攻击成功率。在 CIFAR-10 数据集上，我们的方法可以提高对模型的攻击性能，比基于单个模型的方法高效。<details>
<summary>Abstract</summary>
Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model's capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd's wisdom. Based on the ensemble's collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GResilience-Trading-Off-Between-the-Greenness-and-the-Resilience-of-Collaborative-AI-Systems"><a href="#GResilience-Trading-Off-Between-the-Greenness-and-the-Resilience-of-Collaborative-AI-Systems" class="headerlink" title="GResilience: Trading Off Between the Greenness and the Resilience of Collaborative AI Systems"></a>GResilience: Trading Off Between the Greenness and the Resilience of Collaborative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04569">http://arxiv.org/abs/2311.04569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diaeddin Rimawi, Antonio Liotta, Marco Todescato, Barbara Russo</li>
<li>for: 本研究旨在提供一种自动评估Collaborative Artificial Intelligence System（CAIS）恢复行动的能力来衡量系统的可恢复性和绿色性。</li>
<li>methods: 本研究提出了一种以优化和游戏理论为基础的方法来评估CAIS恢复行动的可恢复性和绿色性。</li>
<li>results: 研究人员通过设计了一种实验协议和应用于一个真实的CAIS示例器，并通过优化和游戏理论来评估CAIS恢复行动的可恢复性和绿色性。<details>
<summary>Abstract</summary>
A Collaborative Artificial Intelligence System (CAIS) works with humans in a shared environment to achieve a common goal. To recover from a disruptive event that degrades its performance and ensures its resilience, a CAIS may then need to perform a set of actions either by the system, by the humans, or collaboratively together. As for any other system, recovery actions may cause energy adverse effects due to the additional required energy. Therefore, it is of paramount importance to understand which of the above actions can better trade-off between resilience and greenness. In this in-progress work, we propose an approach to automatically evaluate CAIS recovery actions for their ability to trade-off between the resilience and greenness of the system. We have also designed an experiment protocol and its application to a real CAIS demonstrator. Our approach aims to attack the problem from two perspectives: as a one-agent decision problem through optimization, which takes the decision based on the score of resilience and greenness, and as a two-agent decision problem through game theory, which takes the decision based on the payoff computed for resilience and greenness as two players of a cooperative game.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:一个协同人工智能系统（CAIS）与人类在共享环境中工作以实现共同目标。在一个破坏性事件导致系统性能下降后，CAIS可能需要执行一系列动作，这些动作可以由系统、人类或者共同执行。由于这些恢复动作可能会带来更多的能源消耗，因此非常重要理解这些动作可以如何让恢复和绿色之间进行更好的负担平衡。在这个进行中的工作中，我们提出了一种方法，可以自动评估CAIS恢复动作的能量和绿色之间的负担平衡。我们还设计了一个实验协议和应用于一个真实的CAIS示范器。我们的方法尝试从两个角度解决问题：作为一个一个代理决策问题，通过优化来做出决策，以及作为两个玩家的合作游戏问题，通过游戏理论来做出决策。
</details></li>
</ul>
<hr>
<h2 id="CAIS-DMA-A-Decision-Making-Assistant-for-Collaborative-AI-Systems"><a href="#CAIS-DMA-A-Decision-Making-Assistant-for-Collaborative-AI-Systems" class="headerlink" title="CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems"></a>CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04562">http://arxiv.org/abs/2311.04562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmrimawi/cais-dma">https://github.com/dmrimawi/cais-dma</a></li>
<li>paper_authors: Diaeddin Rimawi, Antonio Lotta, Marco Todescato, Barbara Russo<br>for:This paper aims to develop a methodology to automatically support the decision-making process in a Collaborative Artificial Intelligence System (CAIS) when the system experiences performance degradation after a disruptive event.methods:The proposed framework consists of three components: one manages or simulates CAIS’s environment and disruptive events, the second automates the decision-making process, and the third provides a visual analysis of CAIS behavior.results:The framework can automatically monitor the decision-making process, intervene whenever a performance degradation occurs, and recommend the next action that balances between minimizing the recovery time (i.e., resilience) and minimizing the energy adverse effects (i.e., greenness).<details>
<summary>Abstract</summary>
A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical system that learns actions in collaboration with humans in a shared environment to achieve a common goal. In particular, a CAIS is equipped with an AI model to support the decision-making process of this collaboration. When an event degrades the performance of CAIS (i.e., a disruptive event), this decision-making process may be hampered or even stopped. Thus, it is of paramount importance to monitor the learning of the AI model, and eventually support its decision-making process in such circumstances. This paper introduces a new methodology to automatically support the decision-making process in CAIS when the system experiences performance degradation after a disruptive event. To this aim, we develop a framework that consists of three components: one manages or simulates CAIS's environment and disruptive events, the second automates the decision-making process, and the third provides a visual analysis of CAIS behavior. Overall, our framework automatically monitors the decision-making process, intervenes whenever a performance degradation occurs, and recommends the next action. We demonstrate our framework by implementing an example with a real-world collaborative robot, where the framework recommends the next action that balances between minimizing the recovery time (i.e., resilience), and minimizing the energy adverse effects (i.e., greenness).
</details>
<details>
<summary>摘要</summary>
一个协同人工智能系统（CAIS）是一个融合物理系统，通过与人类在共同环境中学习行动来实现共同目标。特别是，CAIS具有一个人工智能模型，用于支持协同决策过程。当系统经历破坏性事件（例如，突发事件）时，这个决策过程可能受到影响或者even stop。因此，监测人工智能模型的学习是极其重要的。这篇论文提出了一种新的方法，用于自动支持CAIS协同决策过程在系统经历破坏性事件后。为此，我们开发了一个框架，该框架包括三个组件：一个管理或模拟CAIS的环境和破坏性事件，第二个自动化决策过程，第三个提供CAIS行为的可视分析。总之，我们的框架可以自动监测决策过程，在破坏性事件发生时进行交互，并 recommends the next action，以保持系统的可靠性和绿色性。我们通过实施一个实际的协同 робоット示例来证明我们的框架。在这个示例中，我们的框架建议下一个行动，以均衡系统的恢复时间（即可靠性）和能源不良影响（即绿色性）。
</details></li>
</ul>
<hr>
<h2 id="Local-Differential-Privacy-for-Smart-Meter-Data-Sharing"><a href="#Local-Differential-Privacy-for-Smart-Meter-Data-Sharing" class="headerlink" title="Local Differential Privacy for Smart Meter Data Sharing"></a>Local Differential Privacy for Smart Meter Data Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04544">http://arxiv.org/abs/2311.04544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashothara Shanmugarasa, M. A. P. Chamikara, Hye-young Paik, Salil S. Kanhere, Liming Zhu</li>
<li>for: 提供消费者和能源公司 valuabe insights into energy management, while protecting privacy.</li>
<li>methods: 使用Local Differential Privacy (LDP) methods with randomized response techniques and sliding windows to protect appliance-level energy consumption data.</li>
<li>results: Efficient and effective privacy protection, balancing privacy and data utility for analysis.<details>
<summary>Abstract</summary>
Energy disaggregation techniques, which use smart meter data to infer appliance energy usage, can provide consumers and energy companies valuable insights into energy management. However, these techniques also present privacy risks, such as the potential for behavioral profiling. Local differential privacy (LDP) methods provide strong privacy guarantees with high efficiency in addressing privacy concerns. However, existing LDP methods focus on protecting aggregated energy consumption data rather than individual appliances. Furthermore, these methods do not consider the fact that smart meter data are a form of streaming data, and its processing methods should account for time windows. In this paper, we propose a novel LDP approach (named LDP-SmartEnergy) that utilizes randomized response techniques with sliding windows to facilitate the sharing of appliance-level energy consumption data over time while not revealing individual users' appliance usage patterns. Our evaluations show that LDP-SmartEnergy runs efficiently compared to baseline methods. The results also demonstrate that our solution strikes a balance between protecting privacy and maintaining the utility of data for effective analysis.
</details>
<details>
<summary>摘要</summary>
智能计量数据分解技术可以为消费者和能源公司提供有价值的能源管理信息，但这些技术也存在隐私风险，如行为 Profiling 的可能性。本地均衡隐私（LDP）方法可以提供强隐私保证，但现有的 LDP 方法主要关注保护归并的能源消耗数据而不是个体设备。此外，这些方法没有考虑智能计量数据是流动数据，其处理方法应该考虑时间窗口。在本文中，我们提出了一种新的 LDP 方法（名为 LDP-SmartEnergy），它利用随机响应技术和滑块窗口来帮助在时间上分享设备级能源消耗数据，而不抛露个体用户的设备使用模式。我们的评估结果显示，LDP-SmartEnergy 能够高效运行，与基eline方法相比。结果还表明，我们的解决方案能够平衡保护隐私和维护数据的有用性。
</details></li>
</ul>
<hr>
<h2 id="RankAug-Augmented-data-ranking-for-text-classification"><a href="#RankAug-Augmented-data-ranking-for-text-classification" class="headerlink" title="RankAug: Augmented data ranking for text classification"></a>RankAug: Augmented data ranking for text classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04535">http://arxiv.org/abs/2311.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiasa Singha Roy, Priyam Basu</li>
<li>for: 这个论文主要是为了提高生成模型的评估方法。</li>
<li>methods: 这篇论文提出了一种文本排序方法，用于检测和过滤生成文本中最相似的文本，以提高NLU任务的准确率。</li>
<li>results: 实验结果显示，通过judicious选择筛选技术可以提高准确率，最高提高35%。<details>
<summary>Abstract</summary>
Research on data generation and augmentation has been focused majorly on enhancing generation models, leaving a notable gap in the exploration and refinement of methods for evaluating synthetic data. There are several text similarity metrics within the context of generated data filtering which can impact the performance of specific Natural Language Understanding (NLU) tasks, specifically focusing on intent and sentiment classification. In this study, we propose RankAug, a text-ranking approach that detects and filters out the top augmented texts in terms of being most similar in meaning with lexical and syntactical diversity. Through experiments conducted on multiple datasets, we demonstrate that the judicious selection of filtering techniques can yield a substantial improvement of up to 35% in classification accuracy for under-represented classes.
</details>
<details>
<summary>摘要</summary>
研究数据生成和增强主要集中在提高生成模型，留下了许多评估合成数据的方法的空白。在生成数据筛选中，文本相似度指标在NLU任务中的意图和情感分类方面具有重要作用。本研究提出了RankAug方法，它通过词语和语法多样性来推荐最相似的文本，从而提高分类精度。通过在多个数据集上进行实验，我们证明了选择合适的筛选技术可以提高受 represeted类准确率达35%。
</details></li>
</ul>
<hr>
<h2 id="Validating-ChatGPT-Facts-through-RDF-Knowledge-Graphs-and-Sentence-Similarity"><a href="#Validating-ChatGPT-Facts-through-RDF-Knowledge-Graphs-and-Sentence-Similarity" class="headerlink" title="Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity"></a>Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04524">http://arxiv.org/abs/2311.04524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michalis Mountantonakis, Yannis Tzitzikas</li>
<li>for: 这个论文目的是 validate ChatGPT 的答案和补充它们的证明和来源。</li>
<li>methods: 这个论文使用 RDF 知识Graph（KG）和短句嵌入来实现 ChatGPT 答案的验证和补充。特别是使用 DBpedia 和 LODsyndesis（一个 aggregate KG，包含 2000 亿 triple 从 400 RDF KGs 多个领域），并引入一种算法，可以返回更加相关的 triple（ accompaniment 和 confidence score）。</li>
<li>results: 在评估这种服务（以及类似服务）时，作者创建了一个评估标准套件，包括 2000 个 ChatGPT 答案（其中 1000 个是希腊名人、500 个是希腊地点、500 个是关于希腊的事件）。手动标注后，发现 ChatGPT 的答案中约 73% 是正确的，27% 是错误的。结果很有 promise，例如，对整个benchmark来说，我们成功验证了 ChatGPT 的 85.3% 正确答案，并找到了错误答案中 62.6% 的正确答案。<details>
<summary>Abstract</summary>
Since ChatGPT offers detailed responses without justifications, and erroneous facts even for popular persons, events and places, in this paper we present a novel pipeline that retrieves the response of ChatGPT in RDF and tries to validate the ChatGPT facts using one or more RDF Knowledge Graphs (KGs). To this end we leverage DBpedia and LODsyndesis (an aggregated Knowledge Graph that contains 2 billion triples from 400 RDF KGs of many domains) and short sentence embeddings, and introduce an algorithm that returns the more relevant triple(s) accompanied by their provenance and a confidence score. This enables the validation of ChatGPT responses and their enrichment with justifications and provenance. To evaluate this service (such services in general), we create an evaluation benchmark that includes 2,000 ChatGPT facts; specifically 1,000 facts for famous Greek Persons, 500 facts for popular Greek Places, and 500 facts for Events related to Greece. The facts were manually labelled (approximately 73% of ChatGPT facts were correct and 27% of facts were erroneous). The results are promising; indicatively for the whole benchmark, we managed to verify the 85.3% of the correct facts of ChatGPT and to find the correct answer for the 62.6% of the erroneous ChatGPT facts.
</details>
<details>
<summary>摘要</summary>
自从ChatGPT提供了详细的回答无需证明，而且甚至包含错误的信息关于知名人物、事件和地点，因此在这篇论文中，我们提出了一个新的管道，它将ChatGPT的回答转换为RDF格式，并使用一个或多个RDF知识 graphs（KGs）来验证ChatGPT的信息是否正确。为此，我们利用了DBpedia和LODsyndesis（一个包含400个RDF KGs的多个领域的知识Graph，总共包含200亿个三元组），并使用短句嵌入，并引入一种算法，它可以返回更加相关的 triple（或多个 triple），以及它们的来源和信任分数。这使得可以验证ChatGPT的回答，并为其添加证明和来源。为了评估这种服务（以及类似服务），我们创建了一个评估标准，包括2,000个ChatGPT的信息，其中包括1,000个著名希腊人物、500个希腊地点和500个与希腊相关的事件。这些信息都是手动标注的（约73%的ChatGPT信息正确，27%的信息错误）。结果很有 promise，例如，对整个benchmark，我们成功验证了85.3%的正确ChatGPT信息，并为错误的ChatGPT信息找到了正确的答案的62.6%。
</details></li>
</ul>
<hr>
<h2 id="FFINet-Future-Feedback-Interaction-Network-for-Motion-Forecasting"><a href="#FFINet-Future-Feedback-Interaction-Network-for-Motion-Forecasting" class="headerlink" title="FFINet: Future Feedback Interaction Network for Motion Forecasting"></a>FFINet: Future Feedback Interaction Network for Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04512">http://arxiv.org/abs/2311.04512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miao Kang, Shengqi Wang, Sanping Zhou, Ke Ye, Jingjing Jiang, Nanning Zheng</li>
<li>for: 预测交通代理人的未来合理行为，以提高自动驾驶系统的安全性和效率。</li>
<li>methods: 提出了一种新的未来反馈互动网络（FFINet），通过将当前观察和未来互动的特征进行聚合，以提高多模态轨迹预测的准确性。</li>
<li>results: 在 Argoverse 1 和 Argoverse 2 动态预测测试数据集上，FFINet 实现了状态领先的性能。<details>
<summary>Abstract</summary>
Motion forecasting plays a crucial role in autonomous driving, with the aim of predicting the future reasonable motions of traffic agents. Most existing methods mainly model the historical interactions between agents and the environment, and predict multi-modal trajectories in a feedforward process, ignoring potential trajectory changes caused by future interactions between agents. In this paper, we propose a novel Future Feedback Interaction Network (FFINet) to aggregate features the current observations and potential future interactions for trajectory prediction. Firstly, we employ different spatial-temporal encoders to embed the decomposed position vectors and the current position of each scene, providing rich features for the subsequent cross-temporal aggregation. Secondly, the relative interaction and cross-temporal aggregation strategies are sequentially adopted to integrate features in the current fusion module, observation interaction module, future feedback module and global fusion module, in which the future feedback module can enable the understanding of pre-action by feeding the influence of preview information to feedforward prediction. Thirdly, the comprehensive interaction features are further fed into final predictor to generate the joint predicted trajectories of multiple agents. Extensive experimental results show that our FFINet achieves the state-of-the-art performance on Argoverse 1 and Argoverse 2 motion forecasting benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>自动驾驶中，预测行为的预测具有重要的作用，旨在预测未来的合理行为。现有的方法主要是基于历史交互和环境的模型，预测多模态轨迹，忽略了未来交互所引起的轨迹变化。在这篇论文中，我们提出了一种新的未来反馈互动网络（FFINet），用于聚合特征。首先，我们采用不同的空间-时间编码器，将分解的位坐标和当前场景的位置进行嵌入，提供丰富的特征 для后续的跨时间汇集。其次，我们采用相对交互和跨时间汇集策略，先后采用交互模块、观察交互模块、未来反馈模块和全局汇集模块，其中未来反馈模块可以帮助理解预测的预先行为。最后，我们将全面交互特征传递给最终预测器，生成多个交互的联合预测轨迹。广泛的实验结果表明，我们的 FFINet 在 Argoverse 1 和 Argoverse 2 运动预测Benchmark上达到了状态 искусственный智能的表现。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-on-Investment-Constraints-and-Non-stationarity-in-Dynamic-Portfolio-Optimization-through-Reinforcement-Learning"><a href="#Causal-Inference-on-Investment-Constraints-and-Non-stationarity-in-Dynamic-Portfolio-Optimization-through-Reinforcement-Learning" class="headerlink" title="Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning"></a>Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04946">http://arxiv.org/abs/2311.04946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasuhiro Nakayama, Tomochika Sawaki</li>
<li>for: 本研究开发了一种动态资产配置投资策略，使用了回归学习技术。</li>
<li>methods: 我们解决了金融时间序列数据不stationarity问题的问题，并引入了一些变量，如 режим变化，以提高预测精度。</li>
<li>results: 我们的研究发现，通过在投资策略中应用回归学习技术，可以实现高精度的预测，并且可以考虑实际面临的投资者实际约束，从而实现有效的优化。<details>
<summary>Abstract</summary>
In this study, we have developed a dynamic asset allocation investment strategy using reinforcement learning techniques. To begin with, we have addressed the crucial issue of incorporating non-stationarity of financial time series data into reinforcement learning algorithms, which is a significant implementation in the application of reinforcement learning in investment strategies. Our findings highlight the significance of introducing certain variables such as regime change in the environment setting to enhance the prediction accuracy. Furthermore, the application of reinforcement learning in investment strategies provides a remarkable advantage of setting the optimization problem flexibly. This enables the integration of practical constraints faced by investors into the algorithm, resulting in efficient optimization. Our study has categorized the investment strategy formulation conditions into three main categories, including performance measurement indicators, portfolio management rules, and other constraints. We have evaluated the impact of incorporating these conditions into the environment and rewards in a reinforcement learning framework and examined how they influence investment behavior.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们开发了一种动态资产分配投资策略使用强化学习技术。首先，我们解决了金融时间序列数据不Stationarity问题的应用在强化学习算法中的问题，这是投资策略应用强化学习中的一个重要实现。我们的发现表明，在环境设置中引入 certain 变量，如状态转换，可以提高预测精度。此外，强化学习在投资策略中提供了一个remarkable的优势，即可以自由地设置优化问题。这使得可以将实际面临的投资者的限制 integrate into the algorithm，从而实现高效的优化。我们对投资策略的形式化条件进行分类，包括表现指标、股票管理规则和其他限制。我们在强化学习框架中包含这些条件的环境和奖励，并研究了它们如何影响投资行为。
</details></li>
</ul>
<hr>
<h2 id="Auto-deep-learning-for-bioacoustic-signals"><a href="#Auto-deep-learning-for-bioacoustic-signals" class="headerlink" title="Auto deep learning for bioacoustic signals"></a>Auto deep learning for bioacoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04945">http://arxiv.org/abs/2311.04945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giuliotosato/autokeras-bioacustic">https://github.com/giuliotosato/autokeras-bioacustic</a></li>
<li>paper_authors: Giulio Tosato, Abdelrahman Shehata, Joshua Janssen, Kees Kamp, Pramatya Jati, Dan Stowell</li>
<li>for: 这个研究旨在探讨自动深度学习是否可以提高多类bird vocalization分类的准确率和效率，并与传统的手动设计的深度学习模型进行比较。</li>
<li>methods: 这个研究使用了AutoKeras自动机器学习框架，自动化了神经网络搜索和超参数优化。</li>
<li>results: 结果表明，AutoKeras-derived模型在Western Mediterranean Wetland Birds dataset上 consistently outperform了传统模型如MobileNet、ResNet50和VGG16。这种方法和结论推动了生物声学研究的进步，并提供了一种新的自动化深度学习方法。<details>
<summary>Abstract</summary>
This study investigates the potential of automated deep learning to enhance the accuracy and efficiency of multi-class classification of bird vocalizations, compared against traditional manually-designed deep learning models. Using the Western Mediterranean Wetland Birds dataset, we investigated the use of AutoKeras, an automated machine learning framework, to automate neural architecture search and hyperparameter tuning. Comparative analysis validates our hypothesis that the AutoKeras-derived model consistently outperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach and findings underscore the transformative potential of automated deep learning for advancing bioacoustics research and models. In fact, the automated techniques eliminate the need for manual feature engineering and model design while improving performance. This study illuminates best practices in sampling, evaluation and reporting to enhance reproducibility in this nascent field. All the code used is available at https: //github.com/giuliotosato/AutoKeras-bioacustic   Keywords: AutoKeras; automated deep learning; audio classification; Wetlands Bird dataset; comparative analysis; bioacoustics; validation dataset; multi-class classification; spectrograms.
</details>
<details>
<summary>摘要</summary>
Keywords: AutoKeras; automated deep learning; audio classification; Wetlands Bird dataset; comparative analysis; bioacoustics; validation dataset; multi-class classification; spectrograms.中文翻译：本研究探索使用自动化深度学习提高多类分类鸟叫声的精度和效率，与传统手动设计的深度学习模型进行比较。我们使用西地中海湿地鸟类 dataset 和 AutoKeras 框架自动化神经网络搜索和超参调整。我们的结果表明，AutoKeras  derive 模型在 MobileNet、ResNet50 和 VGG16 等传统模型的比较中 consistently 表现出色。本研究强调自动化深度学习在生物声学研究中的潜在价值，因为它消除了手动特征工程和模型设计的需求，同时提高性能。我们还提供了采样、评估和报告的最佳实践，以增强这个领域的可重复性。所有代码使用的可以在 GitHub 上找到（https://github.com/giuliotosato/AutoKeras-bioacustic）。键语：AutoKeras; 自动化深度学习; 音频分类; 湿地鸟类 dataset; 比较分析; 生物声学; 验证集; 多类分类; spectrograms.
</details></li>
</ul>
<hr>
<h2 id="NExT-Chat-An-LMM-for-Chat-Detection-and-Segmentation"><a href="#NExT-Chat-An-LMM-for-Chat-Detection-and-Segmentation" class="headerlink" title="NExT-Chat: An LMM for Chat, Detection and Segmentation"></a>NExT-Chat: An LMM for Chat, Detection and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04498">http://arxiv.org/abs/2311.04498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NExT-ChatV/NExT-Chat">https://github.com/NExT-ChatV/NExT-Chat</a></li>
<li>paper_authors: Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, Tat-Seng Chua</li>
<li>for: 本研究旨在提高大型语言模型（LLM）在多Modal理解方面的水平，通过增强视觉理解能力，使LMM能够更好地理解和回答多Modal问题。</li>
<li>methods: 本研究提出了一种新的对象位置模型方法 called pixel2emb，该方法让LMM输出位置嵌入，然后通过不同的解码器进行解码。这种嵌入基于的位置模型方法允许使用不同的位置格式（如 bounding box 和 mask）在多Modal会话中。</li>
<li>results: 在有限资源的情况下，我们的 pixel2emb 方法在位置输入和输出任务中表现出色，与现有SOTA方法相比，具有更高的性能。基于提出的 pixel2emb 方法，我们训练了一个名为 NExT-Chat 的 LMM，并证明其能够处理多种任务，如视觉固定、区域描述和基于物理的理解。<details>
<summary>Abstract</summary>
The development of large language models (LLMs) has greatly advanced the field of multimodal understanding, leading to the emergence of large multimodal models (LMMs). In order to enhance the level of visual comprehension, recent studies have equipped LMMs with region-level understanding capabilities by representing object bounding box coordinates as a series of text sequences (pixel2seq). In this paper, we introduce a novel paradigm for object location modeling called pixel2emb method, where we ask the LMM to output the location embeddings and then decoded by different decoders. This paradigm allows for different location formats (such as bounding boxes and masks) to be used in multimodal conversations Furthermore, this kind of embedding based location modeling enables the utilization of existing practices in localization tasks, such as detection and segmentation. In scenarios with limited resources, our pixel2emb demonstrates superior performance compared to existing state-of-the-art (SOTA) approaches in both the location input and output tasks under fair comparison. Leveraging the proposed pixel2emb method, we train an LMM named NExT-Chat and demonstrate its capability of handling multiple tasks like visual grounding, region caption, and grounded reasoning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的发展对多Modal理解领域带来了巨大的进步，导致大多Modal模型（LMM）的出现。为了提高视觉理解水平， latest studies have equipped LMMs with regional understanding capabilities by representing object bounding box coordinates as a series of text sequences (pixel2seq). 在这篇论文中，我们介绍了一种新的对象位置模型方法，即像素2Embedding（pixel2emb）方法，其中我们问LMM输出位置嵌入，然后通过不同的解码器进行解码。这种嵌入基于位置模型方法允许使用不同的位置格式（如 bounding box 和 mask）在多Modal conversation中，并且可以利用现有的Localization任务的实践，如检测和分割。在有限的资源情况下，我们的像素2emb在位置输入和输出任务中表现出了较好的性能，与现有SOTA方法相比。基于提出的像素2emb方法，我们训练了一个名为NExT-Chat的LMM，并证明其能处理多个任务，如视觉定位、区域描述和基于位置的理解。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-Earth-Observation-Current-Methods-Open-Challenges-and-Opportunities"><a href="#Explainable-AI-for-Earth-Observation-Current-Methods-Open-Challenges-and-Opportunities" class="headerlink" title="Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities"></a>Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04491">http://arxiv.org/abs/2311.04491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulsen Taskin, Erchan Aptoula, Alp Ertürk</li>
<li>for: This paper provides a panorama of the state-of-the-art in explainable remote sensing image analysis, organized by prominent Earth observation application fields.</li>
<li>methods: The paper explores a wide spectrum of Explainable Artificial Intelligence techniques to address the lack of explainability and interpretability in deep learning methods for remote sensing.</li>
<li>results: The paper presents the state-of-the-art in explainable remote sensing image analysis, covering a range of Earth observation application fields.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文提供了遍及主要地球观测应用领域的现状的Remote Sensing图像分析的状况报告，使用了Explainable Artificial Intelligence技术来解决深度学习方法的解释性和可解释性问题。</li>
<li>methods: 论文探讨了各种Explainable Artificial Intelligence技术，以解决深度学习方法在Remote Sensing图像分析中的解释性和可解释性问题。</li>
<li>results: 论文提供了Remote Sensing图像分析领域的状态对应的现状报告，涵盖了主要的地球观测应用领域。<details>
<summary>Abstract</summary>
Deep learning has taken by storm all fields involved in data analysis, including remote sensing for Earth observation. However, despite significant advances in terms of performance, its lack of explainability and interpretability, inherent to neural networks in general since their inception, remains a major source of criticism. Hence it comes as no surprise that the expansion of deep learning methods in remote sensing is being accompanied by increasingly intensive efforts oriented towards addressing this drawback through the exploration of a wide spectrum of Explainable Artificial Intelligence techniques. This chapter, organized according to prominent Earth observation application fields, presents a panorama of the state-of-the-art in explainable remote sensing image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已经在数据分析领域的所有领域中掀尘，包括远程感知。然而，尽管表现得非常出色，但深度学习的不可解性和解释性问题，从神经网络的出现以来一直存在的问题，仍然是对其进行批判的主要来源。因此，深度学习方法在远程感知领域的扩张被附加了解释人工智能技术的探索。这章，按照主要的地球观测应用领域分类，介绍了现代 explainable 远程感知图像分析的状况。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-for-Rules-Reasoning"><a href="#Emergent-Communication-for-Rules-Reasoning" class="headerlink" title="Emergent Communication for Rules Reasoning"></a>Emergent Communication for Rules Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04474">http://arxiv.org/abs/2311.04474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Guo, Yifan Hao, Rui Zhang, Enshuai Zhou, Zidong Du, Xishan Zhang, Xinkai Song, Yuanbo Wen, Yongwei Zhao, Xuehai Zhou, Jiaming Guo, Qi Yi, Shaohui Peng, Di Huang, Ruizhi Chen, Qi Guo, Yunji Chen</li>
<li>for: 这个论文主要研究了深度学习基于代理的emergent通信，它们在语言和人工智能方面提供了灵感。但是，之前的尝试都是在感知 Orientated的环境下进行emerging通信， forcing agents to describe low-level perceptual features within image or symbol contexts.</li>
<li>methods: 在这篇论文中，我们提出了一种新的认知游戏（namely Reasoning Game），这个游戏鼓励代理通过思维和通信来解释高级规则，而不是仅仅描述低级感知上下文。我们还提出了一个不偏的数据集（namely rule-RAVEN）作为一个基准，以避免过拟合。此外，我们还提出了一种两阶段训练方法，用于在Reasoning Game中更稳定地 converge。</li>
<li>results: 实验结果表明，在Reasoning Game中，代理们能够解释高级规则，并将其应用到未看过的上下文特性中。此外，emerged语言还帮助代理们在不同上下文特性或任务之间进行泛化和传输。<details>
<summary>Abstract</summary>
Research on emergent communication between deep-learning-based agents has received extensive attention due to its inspiration for linguistics and artificial intelligence. However, previous attempts have hovered around emerging communication under perception-oriented environmental settings, that forces agents to describe low-level perceptual features intra image or symbol contexts. In this work, inspired by the classic human reasoning test (namely Raven's Progressive Matrix), we propose the Reasoning Game, a cognition-oriented environment that encourages agents to reason and communicate high-level rules, rather than perceived low-level contexts. Moreover, we propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid overfitting, 2) and a two-stage curriculum agent training method as a baseline for more stable convergence in the Reasoning Game, where contexts and semantics are bilaterally drifting. Experimental results show that, in the Reasoning Game, a semantically stable and compositional language emerges to solve reasoning problems. The emerged language helps agents apply the extracted rules to the generalization of unseen context attributes, and to the transfer between different context attributes or even tasks.
</details>
<details>
<summary>摘要</summary>
研究深度学习代理之间的emergentcommunication已经受到了人工智能和语言科学的广泛关注，因为它们可以提供人工智能和语言科学的灵感。然而，之前的尝试都集中在感知 oriented 环境下的 emerging communication， forcing agents to describe low-level perceptual features within image or symbol contexts。在这项工作中， Drawing inspiration from the classic human reasoning test (namely Raven's Progressive Matrix), we propose the Reasoning Game, a cognition-oriented environment that encourages agents to reason and communicate high-level rules, rather than perceived low-level contexts。 In addition, we propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid overfitting, 2) and a two-stage curriculum agent training method as a baseline for more stable convergence in the Reasoning Game, where contexts and semantics are bilaterally drifting。实验结果表明，在 Reasoning Game 中，semantically stable and compositional language emerges to solve reasoning problems。这种emerged language helps agents apply the extracted rules to the generalization of unseen context attributes, and to the transfer between different context attributes or even tasks。
</details></li>
</ul>
<hr>
<h2 id="RDGCN-Reinforced-Dependency-Graph-Convolutional-Network-for-Aspect-based-Sentiment-Analysis"><a href="#RDGCN-Reinforced-Dependency-Graph-Convolutional-Network-for-Aspect-based-Sentiment-Analysis" class="headerlink" title="RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis"></a>RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04467">http://arxiv.org/abs/2311.04467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rdgcn/rdgcn">https://github.com/rdgcn/rdgcn</a></li>
<li>paper_authors: Xusheng Zhao, Hao Peng, Qiong Dai, Xu Bai, Huailiang Peng, Yanbing Liu, Qinglang Guo, Philip S. Yu</li>
<li>for: 这 paper 的目的是提高 aspect-based sentiment analysis (ABSA) 的精度，使其能够更好地预测句子中的 sentiment polarity。</li>
<li>methods: 这 paper 使用 graph neural networks (GNN) 来捕捉句子中的结构 Patterns，并通过 reinforcement learning 来改进 dependency graph 中的重要性计算。</li>
<li>results:  compare to state-of-the-art GNN-based baselines, RDGCN 在三个 популяр的 dataset 上的全面实验中表现出色，提高了 ABSA 的精度。<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the sentiment polarity of aspect terms within sentences. Employing graph neural networks to capture structural patterns from syntactic dependency parsing has been confirmed as an effective approach for boosting ABSA. In most works, the topology of dependency trees or dependency-based attention coefficients is often loosely regarded as edges between aspects and opinions, which can result in insufficient and ambiguous syntactic utilization. To address these problems, we propose a new reinforced dependency graph convolutional network (RDGCN) that improves the importance calculation of dependencies in both distance and type views. Initially, we propose an importance calculation criterion for the minimum distances over dependency trees. Under the criterion, we design a distance-importance function that leverages reinforcement learning for weight distribution search and dissimilarity control. Since dependency types often do not have explicit syntax like tree distances, we use global attention and mask mechanisms to design type-importance functions. Finally, we merge these weights and implement feature aggregation and classification. Comprehensive experiments on three popular datasets demonstrate the effectiveness of the criterion and importance functions. RDGCN outperforms state-of-the-art GNN-based baselines in all validations.
</details>
<details>
<summary>摘要</summary>
Initially, we propose an importance calculation criterion for the minimum distances over dependency trees. Under the criterion, we design a distance-importance function that leverages reinforcement learning for weight distribution search and dissimilarity control. Since dependency types often do not have explicit syntax like tree distances, we use global attention and mask mechanisms to design type-importance functions. Finally, we merge these weights and implement feature aggregation and classification.Comprehensive experiments on three popular datasets demonstrate the effectiveness of the criterion and importance functions. RDGCN outperforms state-of-the-art GNN-based baselines in all validations.
</details></li>
</ul>
<hr>
<h2 id="Edge-assisted-U-Shaped-Split-Federated-Learning-with-Privacy-preserving-for-Internet-of-Things"><a href="#Edge-assisted-U-Shaped-Split-Federated-Learning-with-Privacy-preserving-for-Internet-of-Things" class="headerlink" title="Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving for Internet of Things"></a>Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving for Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04944">http://arxiv.org/abs/2311.04944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengliang Tang, Zihang Zhao, Detian Liu, Yang Cao, Shiqiang Zhang, Siqing You</li>
<li>for: 这个研究旨在解决互联网领域内的物联网（IoT）设备上的深度学习模型部署问题。这些设备通常没有计算和通信能力，直接传输数据会导致网络拥堵和不合理的执行。中央化数据处理在数据中心也不再可行，因为关于数据隐私和安全的 Concerns。</li>
<li>methods: 我们提出了一个创新的 Edge-assisted U-Shaped Split Federated Learning（EUSFL）框架，利用边缘服务器的高性能能力协助IoT设备进行模型训练和优化过程。在这个框架中，我们运用了 Federated Learning（FL），让数据持有者共同训练模型，而不需要分享数据，从而提高隐私保护。此外，我们将神经网络分为三部分使用U-型分割，让IoT设备进行本地训练。这样可以利用边缘服务器的更高计算能力，实现全面训练时间的缩短，并让IoT设备 avec varying capabilities 进行训练任务得以高效。</li>
<li>results: 我们的理论分析和实验结果显示，EUSFL可以与不同的聚合算法结合使用，在不同的IoT设备计算能力下保持良好的性能，并对训练时间和本地计算负载进行了明显的缩短。此外，我们还提出了一种新的杂音机制 called LabelDP，以保护数据特征和标签免受重建攻击，排除隐私泄露的风险。<details>
<summary>Abstract</summary>
In the realm of the Internet of Things (IoT), deploying deep learning models to process data generated or collected by IoT devices is a critical challenge. However, direct data transmission can cause network congestion and inefficient execution, given that IoT devices typically lack computation and communication capabilities. Centralized data processing in data centers is also no longer feasible due to concerns over data privacy and security. To address these challenges, we present an innovative Edge-assisted U-Shaped Split Federated Learning (EUSFL) framework, which harnesses the high-performance capabilities of edge servers to assist IoT devices in model training and optimization process. In this framework, we leverage Federated Learning (FL) to enable data holders to collaboratively train models without sharing their data, thereby enhancing data privacy protection by transmitting only model parameters. Additionally, inspired by Split Learning (SL), we split the neural network into three parts using U-shaped splitting for local training on IoT devices. By exploiting the greater computation capability of edge servers, our framework effectively reduces overall training time and allows IoT devices with varying capabilities to perform training tasks efficiently. Furthermore, we proposed a novel noise mechanism called LabelDP to ensure that data features and labels can securely resist reconstruction attacks, eliminating the risk of privacy leakage. Our theoretical analysis and experimental results demonstrate that EUSFL can be integrated with various aggregation algorithms, maintaining good performance across different computing capabilities of IoT devices, and significantly reducing training time and local computation overhead.
</details>
<details>
<summary>摘要</summary>
在互联网物联网（IoT）领域，部署深度学习模型来处理由IoT设备生成或收集的数据是一项关键挑战。然而，直接数据传输会导致网络拥堵和不efficient执行，因为IoT设备通常缺乏计算和通信能力。中央化数据处理在数据中心也不再可行，因为数据隐私和安全问题。为解决这些挑战，我们提出了一种创新的Edge助けU型分布式学习（EUSFL）框架，利用边缘服务器的高性能特性来帮助IoT设备进行模型训练和优化过程。在这个框架中，我们运用分布式学习（FL），使得数据持有者可以共同训练模型，而不需要将数据共享，从而提高数据隐私保护。此外，受到分learn（SL）的启发，我们将神经网络分成三部分，在IoT设备上进行本地训练。通过利用边缘服务器的更高计算能力，我们的框架可以有效减少总训练时间，让IoT设备按照不同的能力进行训练任务，并且可以保持不同计算能力的IoT设备之间的兼容性。此外，我们还提出了一种新的噪声机制called LabelDP，以保护数据特征和标签免受重建攻击，从而消除隐私泄露的风险。我们的理论分析和实验结果表明，EUSFL可以与不同的聚合算法结合使用，保持不同计算能力的IoT设备之间的兼容性，同时减少训练时间和本地计算负担。
</details></li>
</ul>
<hr>
<h2 id="Improving-Pacing-in-Long-Form-Story-Planning"><a href="#Improving-Pacing-in-Long-Form-Story-Planning" class="headerlink" title="Improving Pacing in Long-Form Story Planning"></a>Improving Pacing in Long-Form Story Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04459">http://arxiv.org/abs/2311.04459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yichenzw/pacing">https://github.com/yichenzw/pacing</a></li>
<li>paper_authors: Yichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein</li>
<li>for: 提高自动生成故事大纲的自然整体感 (improve the natural pacing of automatically generated story outlines)</li>
<li>methods: 使用 coneCreteness 评价器控制层次大纲生成 (use a concreteness evaluator to control hierarchical outline generation)，并使用 predicted concreteness 筛选新的大纲项 (filter new outline items based on predicted concreteness)</li>
<li>results: 与基线对比，人类评价 CONCOCT 的均衡性高于 57% 多个大纲长度 (compared to a baseline, humans judge CONCOCT’s pacing to be more consistent over 57% of the time across multiple outline lengths)<details>
<summary>Abstract</summary>
Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to improve pacing when automatically generating story outlines. We first train a concreteness evaluator to judge which of two events is more concrete (low-level-detailed). This evaluator can then be used to control pacing in hierarchical outline generation; in this work, we explore a vaguest-first expansion procedure that aims for uniform pacing. We further use the evaluator to filter new outline items based on predicted concreteness. Compared to a baseline hierarchical outline generator, humans judge CONCOCT's pacing to be more consistent over 57% of the time across multiple outline lengths; the gains also translate to downstream stories. All code, data, and models are open-sourced.
</details>
<details>
<summary>摘要</summary>
We first train a concreteness evaluator to determine which of two events is more concrete (low-level-detailed). This evaluator is then used to control pacing in hierarchical outline generation. Specifically, we use a vaguest-first expansion procedure that aims for uniform pacing. Additionally, we use the evaluator to filter new outline items based on their predicted concreteness.Compared to a baseline hierarchical outline generator, humans judge CONCOCT's pacing to be more consistent over 57% of the time across multiple outline lengths. Furthermore, the gains translate to downstream stories. All code, data, and models are open-sourced.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Uncertainty-Quantification-approaches-for-Neural-PDEs-in-scientific-applications"><a href="#Evaluating-Uncertainty-Quantification-approaches-for-Neural-PDEs-in-scientific-applications" class="headerlink" title="Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications"></a>Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04457">http://arxiv.org/abs/2311.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vardhan Dongre, Gurpreet Singh Hora<br>for: This paper focuses on the development of Uncertainty Quantification (UQ) methods for Neural Partial Differential Equations (Neural PDEs) in scientific applications, specifically for forward and inverse problems.methods: The paper evaluates various UQ approaches, including Bayesian methods such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), as well as a conventional approach using Deep Ensembles (DE).results: The results show that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters, but the Bayesian methods tend to display a higher degree of certainty in their predictions compared to the DE approach. This suggests that Bayesian techniques may underestimate the true underlying uncertainty, appearing more confident in their predictions than the DE approach.Here’s the Chinese version of the three key points:for: 这篇论文关注使用神经partial differential equations (Neural PDEs)在科学应用中的uncertainty量化 (UQ)方法的开发，特别是对于前向和反向问题。methods: 论文评估了多种UQ方法，包括 bayesian方法such as Hamiltonian Monte Carlo (HMC)和Monte-Carlo Dropout (MCD)，以及一种传统的 Deep Ensembles (DE) 方法。results: 结果显示 Neural PDEs 可以有效地重construct流体系统和相关的未知参数，但 bayesian方法在其预测中显示了更高的自信度，与 DE 方法相比。这表明 bayesian 技术可能会下降 true 的下层不确定性，因此在其预测中显得更自信。<details>
<summary>Abstract</summary>
The accessibility of spatially distributed data, enabled by affordable sensors, field, and numerical experiments, has facilitated the development of data-driven solutions for scientific problems, including climate change, weather prediction, and urban planning. Neural Partial Differential Equations (Neural PDEs), which combine deep learning (DL) techniques with domain expertise (e.g., governing equations) for parameterization, have proven to be effective in capturing valuable correlations within spatiotemporal datasets. However, sparse and noisy measurements coupled with modeling approximation introduce aleatoric and epistemic uncertainties. Therefore, quantifying uncertainties propagated from model inputs to outputs remains a challenge and an essential goal for establishing the trustworthiness of Neural PDEs. This work evaluates various Uncertainty Quantification (UQ) approaches for both Forward and Inverse Problems in scientific applications. Specifically, we investigate the effectiveness of Bayesian methods, such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach, Deep Ensembles (DE). To illustrate their performance, we take two canonical PDEs: Burger's equation and the Navier-Stokes equation. Our results indicate that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters. However, it is noteworthy that the results derived from Bayesian methods, based on our observations, tend to display a higher degree of certainty in their predictions as compared to those obtained using the DE. This elevated certainty in predictions suggests that Bayesian techniques might underestimate the true underlying uncertainty, thereby appearing more confident in their predictions than the DE approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用可得到的散点数据，由于可靠且有效的感知器和数值实验，解决科学问题，包括气候变化、天气预测和城市规划等。神经partial differential equations（神经PDEs），结合深度学习（DL）技术和领域专业知识（例如，管理方程）进行参数化，能够很好地捕捉空间时间数据中的有价值相关性。然而，稀缺和噪声的测量数据，加之模型简化，导致 aleatoric和epistemicuncertainty。因此，将模型输入到输出中的不确定性进行评估是一项重要的任务，以确保神经PDEs的可靠性。本工作评估了多种uncertainty quantification（UQ）方法，包括forward和 inverse问题在科学应用中。 Specifically, we investigate the effectiveness of Bayesian methods, such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach, Deep Ensembles (DE). To illustrate their performance, we take two canonical PDEs: Burger's equation and the Navier-Stokes equation. Our results indicate that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters. However, it is noteworthy that the results derived from Bayesian methods, based on our observations, tend to display a higher degree of certainty in their predictions as compared to those obtained using the DE. This elevated certainty in predictions suggests that Bayesian techniques might underestimate the true underlying uncertainty, thereby appearing more confident in their predictions than the DE approach.Translated by Google Translate.
</details></li>
</ul>
<hr>
<h2 id="MathNAS-If-Blocks-Have-a-Role-in-Mathematical-Architecture-Design"><a href="#MathNAS-If-Blocks-Have-a-Role-in-Mathematical-Architecture-Design" class="headerlink" title="MathNAS: If Blocks Have a Role in Mathematical Architecture Design"></a>MathNAS: If Blocks Have a Role in Mathematical Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04943">http://arxiv.org/abs/2311.04943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangqinsi1/mathnas">https://github.com/wangqinsi1/mathnas</a></li>
<li>paper_authors: Wang Qinsi, Ke Jinhan, Liang Zhi, Zhang Sihai</li>
<li>For: 这个研究是要解决Neural Architecture Search（NAS）中的大型模型设计问题，因为现有的方法在搜寻和评估候选网络时需要庞大的 computation cost。* Methods: 这篇研究提出了一个新的分治策略，利用搜寻空间的弹性特性，将候选网络的表现计算分解为各个专案中的表现，并使用数学程式来预测网络表现。* Results: 这篇研究显示，这个新的分治策略可以实现快速的网络表现评估，并且可以实现更高的准确性和更快的搜寻速度。<details>
<summary>Abstract</summary>
Neural Architecture Search (NAS) has emerged as a favoured method for unearthing effective neural architectures. Recent development of large models has intensified the demand for faster search speeds and more accurate search results. However, designing large models by NAS is challenging due to the dramatical increase of search space and the associated huge performance evaluation cost. Consider a typical modular search space widely used in NAS, in which a neural architecture consists of $m$ block nodes and a block node has $n$ alternative blocks. Facing the space containing $n^m$ candidate networks, existing NAS methods attempt to find the best one by searching and evaluating candidate networks directly.Different from the general strategy that takes architecture search as a whole problem, we propose a novel divide-and-conquer strategy by making use of the modular nature of the search space.Here, we introduce MathNAS, a general NAS framework based on mathematical programming.In MathNAS, the performances of the $m*n$ possible building blocks in the search space are calculated first, and then the performance of a network is directly predicted based on the performances of its building blocks. Although estimating block performances involves network training, just as what happens for network performance evaluation in existing NAS methods, predicting network performance is completely training-free and thus extremely fast. In contrast to the $n^m$ candidate networks to evaluate in existing NAS methods, which require training and a formidable computational burden, there are only $m*n$ possible blocks to handle in MathNAS. Therefore, our approach effectively reduces the complexity of network performance evaluation.Our code is available at https://github.com/wangqinsi1/MathNAS.
</details>
<details>
<summary>摘要</summary>
neural Architecture Search (NAS) 已经成为发掘有效神经建筑的首选方法。 recent development of large models 使得寻找更快的搜索速度和更准确的搜索结果变得更加紧迫。然而，通过 NAS 设计大型模型是挑战，因为搜索空间的增加会导致搜索速度的增加和评估成本的增加。 faces a typical modular search space widely used in NAS, in which a neural architecture consists of $m$ block nodes and a block node has $n$ alternative blocks. existing NAS methods attempt to find the best one by searching and evaluating candidate networks directly. unlike the general strategy that takes architecture search as a whole problem, we propose a novel divide-and-conquer strategy by making use of the modular nature of the search space. here, we introduce MathNAS, a general NAS framework based on mathematical programming. in MathNAS, the performances of the $m*n$ possible building blocks in the search space are calculated first, and then the performance of a network is directly predicted based on the performances of its building blocks. although estimating block performances involves network training, just as what happens for network performance evaluation in existing NAS methods, predicting network performance is completely training-free and thus extremely fast. in contrast to the $n^m$ candidate networks to evaluate in existing NAS methods, which require training and a formidable computational burden, there are only $m*n$ possible blocks to handle in MathNAS. therefore, our approach effectively reduces the complexity of network performance evaluation. our code is available at https://github.com/wangqinsi1/MathNAS.
</details></li>
</ul>
<hr>
<h2 id="MixTEA-Semi-supervised-Entity-Alignment-with-Mixture-Teaching"><a href="#MixTEA-Semi-supervised-Entity-Alignment-with-Mixture-Teaching" class="headerlink" title="MixTEA: Semi-supervised Entity Alignment with Mixture Teaching"></a>MixTEA: Semi-supervised Entity Alignment with Mixture Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04441">http://arxiv.org/abs/2311.04441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiefeng69/mixtea">https://github.com/xiefeng69/mixtea</a></li>
<li>paper_authors: Feng Xie, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan</li>
<li>for: 本文提出了一种新的半监督实体对应（EA）方法，以解决由于缺乏充分的标注数据而带来的实体对应问题。</li>
<li>methods: 本文使用了一种独特的结合人工标注和 probabilistic pseudo 对应的混合教学方法，并在 pseudo 对应学习中提出了bi-directional voting（BDV）策略和匹配多样性基于修正（MDR）模块，以降低噪声对 pseudo 对应学习的负面影响。</li>
<li>results: 对于多个 benchmark 数据集，以及进一步的分析，表明了我们提出的方法的优越性和实用性。<details>
<summary>Abstract</summary>
Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings or largely ignore the uncertainty of pseudo mappings. In this paper, we propose a novel semi-supervised EA method, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. We firstly train a student model using few labeled mappings as standard. More importantly, in pseudo mapping learning, we propose a bi-directional voting (BDV) strategy that fuses the alignment decisions in different directions to estimate the uncertainty via the joint matching confidence score. Meanwhile, we also design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning, thus reducing the negative influence of noisy mappings. Extensive results on benchmark datasets as well as further analyses demonstrate the superiority and the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
semi-supervised entity alignment（EA）是一个实际和挑战性的任务，因为缺乏充足的标注映射作为训练数据。大多数工作是通过生成 Pseudo 映射来 Address 这个问题，但它们都会受到 Pseudo 映射的错误（噪声）的影响或者忽略 Pseudo 映射的不确定性。在这篇论文中，我们提出了一种新的 semi-supervised EA 方法，名为 MixTEA，它使用端到端混合教学法和概率 Pseudo 映射来导引模型学习。我们首先在几个标注映射上训练一个学生模型。更重要的是，在 Pseudo 映射学习中，我们提出了两个方向投票（BDV）策略，它将在不同的方向投票结果中进行折衔，以便估计 uncertainty via 联合匹配信息指数。同时，我们还设计了一个匹配多样性基于修正（MDR）模块，以降低 Pseudo 映射学习中的负面影响。我们在标准 benchmark 数据集以及进一步的分析中表明了我们的提出的方法的优越性和效果。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Geoscience-Artificial-Intelligence-XGeoS-AI-Application-to-Demystify-Image-Recognition"><a href="#Interpretable-Geoscience-Artificial-Intelligence-XGeoS-AI-Application-to-Demystify-Image-Recognition" class="headerlink" title="Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition"></a>Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04940">http://arxiv.org/abs/2311.04940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin-Jian Xu, Hao Zhang, Chao-Sheng Tang, Lin Li, Bin Shi</li>
<li>for: 这个研究的目的是解释地球科学中的图像识别问题，并提出一种可解释的地球科学人工智能（XGeoS-AI）框架来实现这一目标。</li>
<li>methods: 该框架采用了人类视觉机制的思想，通过在整个图像中选择一个地方生成一个阈值，以完成图像识别任务。此外，可以采用不同的人工智能方法，如支持向量回归（SVR）、多层感知神经网络（MLP）和卷积神经网络（CNN）等，来快速完成地球科学图像识别任务。</li>
<li>results: 实验结果表明，提出的XGeoS-AI框架具有高效、多样化和可解释的优点，有很大的潜力应用于地球科学图像识别问题。此外，该框架还可以推动地球科学领域内的技术创新。<details>
<summary>Abstract</summary>
As Earth science enters the era of big data, artificial intelligence (AI) not only offers great potential for solving geoscience problems, but also plays a critical role in accelerating the understanding of the complex, interactive, and multiscale processes of Earth's behavior. As geoscience AI models are progressively utilized for significant predictions in crucial situations, geoscience researchers are increasingly demanding their interpretability and versatility. This study proposes an interpretable geoscience artificial intelligence (XGeoS-AI) framework to unravel the mystery of image recognition in the Earth sciences, and its effectiveness and versatility is demonstrated by taking computed tomography (CT) image recognition as an example. Inspired by the mechanism of human vision, the proposed XGeoS-AI framework generates a threshold value from a local region within the whole image to complete the recognition. Different kinds of artificial intelligence (AI) methods, such as Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI framework to efficiently complete geoscience image recognition tasks. Experimental results demonstrate that the effectiveness, versatility, and heuristics of the proposed framework have great potential in solving geoscience image recognition problems. Interpretable AI should receive more and more attention in the field of the Earth sciences, which is the key to promoting more rational and wider applications of AI in the field of Earth sciences. In addition, the proposed interpretable framework may be the forerunner of technological innovation in the Earth sciences.
</details>
<details>
<summary>摘要</summary>
如今地球科学进入大数据时代，人工智能（AI）不仅提供了解决地球科学问题的极大潜力，还扮演着促进地球行为复杂、互动和多尺度过程的加速器。随着地球科学AI模型在重要情况下的广泛应用，地球科学研究人员更加需要其解释性和多样性。本研究提出了一种可解释的地球科学人工智能（XGeoS-AI）框架，以解决地球科学图像识别问题的谜题。灵感自人类视觉机制，提议的XGeoS-AI框架在全图像中 locates 一个区域，并生成该区域的阈值，以完成图像识别。不同的人工智能方法，如支持向量回归（SVR）、多层感知网络（MLP）和卷积神经网络（CNN），可以作为XGeoS-AI框架中的人工智能引擎，高效完成地球科学图像识别任务。实验结果表明，提议的框架具有效果、多样性和启发性，在地球科学图像识别问题上具有很大的潜力。可解释AI在地球科学领域应该收到更多的关注，这是推动AI在地球科学领域的更广泛应用的关键。此外，提议的可解释框架可能成为地球科学技术创新的先驱者。
</details></li>
</ul>
<hr>
<h2 id="LooGLE-Can-Long-Context-Language-Models-Understand-Long-Contexts"><a href="#LooGLE-Can-Long-Context-Language-Models-Understand-Long-Contexts" class="headerlink" title="LooGLE: Can Long-Context Language Models Understand Long Contexts?"></a>LooGLE: Can Long-Context Language Models Understand Long Contexts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04939">http://arxiv.org/abs/2311.04939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/loogle">https://github.com/bigai-nlco/loogle</a></li>
<li>paper_authors: Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang</li>
<li>for: 评估大语言模型（LLMs）在长文本理解方面的能力。</li>
<li>methods: 使用新的文档（post-2022），且文档长度超过24,000个字符，同时采用人工生成的6,000个问题和多个领域的问题集来评估LLMs的长dependency能力。</li>
<li>results: 研究发现：（i）商业模型在LLMs中表现更好；（ii）LLMs在短dependency任务中表现出色，但在更复杂的长dependency任务中表现不佳；（iii）在文本上下文中学习和连接思维只提供了有限的改进；（iv）检索基本技术在短问题 answering中表现出色，而扩展文本窗口长度的技术对长文本理解有限的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards "true long-context understanding".
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），尽管在不同语言任务中表现出色，但通常只能处理文本内容窗口大小。这一限制促进了大量研究，以提高LLM的长期文本理解能力。然而，先前的数据集受到一些缺点的影响，如文本长度较短，与现代LLM的上下文窗口大小相比; 带有数据泄露问题的旧文档; 和更重视短语相互作用而非长语相互作用。本文提出了LooGLE，一个长期语言评估准则，用于评估LLM的长期文本理解能力。LooGLE的特点包括：使用最新的文档（2022年后），文档长度超过24,000个字符，并且新生成了6,000个问题，覆盖多个领域。人工标注员仔细制作了1,100个高质量问题对，以满足长期依赖要求。这些对经过了严格的交叉验证，以便获得LLM的长期依赖能力的最准确评估。八种当前最佳LLM在LooGLE上进行评估后，得到了以下发现：（i）商业模型比开源模型表现更佳；（ii）LLM在短语相互作用任务中表现出色，但在更复杂的长语相互作用任务中受到限制；（iii）嵌入式学习和串联思维只提供了有限的改进；（iv）基于检索的技术在短问题回答任务中具有显著的优势，而扩展上下文窗口长度的策略对长期文本理解的改进具有有限的影响。因此，LooGLE不仅提供了LLM的长期文本理解能力的系统和全面的评估方案，还探讨了未来 LLM 的发展，以实现“真正的长期文本理解”。
</details></li>
</ul>
<hr>
<h2 id="Data-Factors-for-Better-Compositional-Generalization"><a href="#Data-Factors-for-Better-Compositional-Generalization" class="headerlink" title="Data Factors for Better Compositional Generalization"></a>Data Factors for Better Compositional Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04420">http://arxiv.org/abs/2311.04420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/owenzx/data4comp">https://github.com/owenzx/data4comp</a></li>
<li>paper_authors: Xiang Zhou, Yichen Jiang, Mohit Bansal</li>
<li>for: 本文旨在探讨模型在不同数据集上的泛化能力，以及如何通过不同的数据因素来改善模型的泛化能力。</li>
<li>methods: 本文使用Transformer模型在不同数据集上进行训练，并通过分析不同数据因素的影响来解释模型的泛化能力。</li>
<li>results: 研究发现，增加数据复杂度可以提高模型的泛化能力，并且这种改善来自于数据集中提供更多的多样化示例和降低示例重复频率的效果。此外，训练例子的Difficulty Level也对泛化能力产生不同的影响，在 sintetic datasets上，简单的例子更能够 invoke  compose understanding，而在大规模的实际语言 datasets上，一个平衡的混合mixture of simple和hard例子可以induce最强的泛化能力。<details>
<summary>Abstract</summary>
Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and more general datasets show better generalization ability. In this work, to reconcile this inconsistency, we conduct an empirical analysis by training Transformer models on a variety of training sets with different data factors, including dataset scale, pattern complexity, example difficulty, etc. First, we show that increased dataset complexity can lead to better generalization behavior on multiple different generalization challenges. To further understand this improvement, we show two axes of the benefit from more complex datasets: they provide more diverse examples so compositional understanding becomes more effective, and they also prevent ungeneralizable memorization of the examples due to reduced example repetition frequency. Finally, we explore how training examples of different difficulty levels influence generalization differently. On synthetic datasets, simple examples invoke stronger compositionality than hard examples do. On larger-scale real language datasets, while hard examples become more important potentially to ensure decent data coverage, a balanced mixture of simple and hard examples manages to induce the strongest generalizability. The code and data for this work are available at https://github.com/owenzx/data4comp
</details>
<details>
<summary>摘要</summary>
Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), have revealed severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and more general datasets have shown better generalization ability. In this work, we aim to reconcile this inconsistency by conducting an empirical analysis of Transformer models trained on various training sets with different data factors, including dataset scale, pattern complexity, example difficulty, etc.First, we find that increased dataset complexity leads to better generalization behavior on multiple different generalization challenges. To further understand this improvement, we identify two axes of benefit from more complex datasets: they provide more diverse examples that enhance compositional understanding, and they also reduce the likelihood of ungeneralizable memorization due to reduced example repetition frequency.Finally, we explore how training examples of different difficulty levels influence generalization differently. On synthetic datasets, simple examples tend to invoke stronger compositionality than hard examples do. On larger-scale real language datasets, while hard examples become more important to ensure decent data coverage, a balanced mixture of simple and hard examples is found to induce the strongest generalizability. The code and data for this work are available at <https://github.com/owenzx/data4comp>.
</details></li>
</ul>
<hr>
<h2 id="PepLand-a-large-scale-pre-trained-peptide-representation-model-for-a-comprehensive-landscape-of-both-canonical-and-non-canonical-amino-acids"><a href="#PepLand-a-large-scale-pre-trained-peptide-representation-model-for-a-comprehensive-landscape-of-both-canonical-and-non-canonical-amino-acids" class="headerlink" title="PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids"></a>PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04419">http://arxiv.org/abs/2311.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochi Zhang, Haoran Wu, Yuting Xiu, Kewei Li, Ningning Chen, Yu Wang, Yan Wang, Xin Gao, Fengfeng Zhou</li>
<li>For: PepLand is a pre-training architecture for representation and property analysis of peptides spanning both canonical and non-canonical amino acids.* Methods: PepLand leverages a comprehensive multi-view heterogeneous graph neural network to unveil the subtle structural representations of peptides.* Results: PepLand effectively captures salient synthetic peptide features, laying a robust foundation for transformative advances in peptide-centric research domains.Here’s the Chinese translation of the three points:* For: 这篇研究旨在提出一个专门针对包含非标准氨基酸的肽的预训架构，以探索这些肽的积极特征和性能。* Methods: 这个预训架构使用了一个全面的多观察异源Graph Neural Network（GNN），以捕捉这些肽的细微结构特征。* Results: PepLand 能够有效地捕捉这些肽的积极特征，实现了在肽中心研究领域中的创新进步。<details>
<summary>Abstract</summary>
In recent years, the scientific community has become increasingly interested on peptides with non-canonical amino acids due to their superior stability and resistance to proteolytic degradation. These peptides present promising modifications to biological, pharmacological, and physiochemical attributes in both endogenous and engineered peptides. Notwithstanding their considerable advantages, the scientific community exhibits a conspicuous absence of an effective pre-trained model adept at distilling feature representations from such complex peptide sequences. We herein propose PepLand, a novel pre-training architecture for representation and property analysis of peptides spanning both canonical and non-canonical amino acids. In essence, PepLand leverages a comprehensive multi-view heterogeneous graph neural network tailored to unveil the subtle structural representations of peptides. Empirical validations underscore PepLand's effectiveness across an array of peptide property predictions, encompassing protein-protein interactions, permeability, solubility, and synthesizability. The rigorous evaluation confirms PepLand's unparalleled capability in capturing salient synthetic peptide features, thereby laying a robust foundation for transformative advances in peptide-centric research domains. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/pepland
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AI-accelerated-Discovery-of-Altermagnetic-Materials"><a href="#AI-accelerated-Discovery-of-Altermagnetic-Materials" class="headerlink" title="AI-accelerated Discovery of Altermagnetic Materials"></a>AI-accelerated Discovery of Altermagnetic Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04418">http://arxiv.org/abs/2311.04418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfgao66/mataltmag">https://github.com/zfgao66/mataltmag</a></li>
<li>paper_authors: Ze-Feng Gao, Shuai Qu, Bocheng Zeng, Ji-Rong Wen, Hao Sun, Pengjie Guo, Zhong-Yi Lu<br>for: 这篇论文旨在探索新型磁性阶段—— alternate magnetism，以及发现更多的磁性材料。methods: 这篇论文使用了人工智能搜索引擎，融合了对组态分析、图像神经网络预训、最佳运输理论和基本电子结构计算，发现25种新的磁性材料，包括金属、半导体和对磁体。results: 这篇论文发现了25种新的磁性材料，其中8种是$i$-波磁性材料，这些材料具有独特的物理性能，例如非常性 Hall 效应、非常性 Kerr 效应和トポロジック 性。<details>
<summary>Abstract</summary>
Altermagnetism, a new magnetic phase, has been theoretically proposed and experimentally verified to be distinct from ferromagnetism and antiferromagnetism. Although altermagnets have been found to possess many exotic physical properties, the very limited availability of known altermagnetic materials~(e.g., 14 confirmed materials) hinders the study of such properties. Hence, discovering more types of altermagnetic materials is crucial for a comprehensive understanding of altermagnetism and thus facilitating new applications in the next generation information technologies, e.g., storage devices and high-sensitivity sensors. Here, we report 25 new altermagnetic materials that cover metals, semiconductors, and insulators, discovered by an AI search engine unifying symmetry analysis, graph neural network pre-training, optimal transport theory, and first-principles electronic structure calculation. The wide range of electronic structural characteristics reveals that various innovative physical properties manifest in these newly discovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr effect, and topological property. Noteworthy, we discovered 8 $i$-wave altermagnetic materials for the first time. Overall, the AI search engine performs much better than human experts and suggests a set of new altermagnetic materials with unique properties, outlining its potential for accelerated discovery of altermagnetic materials.
</details>
<details>
<summary>摘要</summary>
新型磁相���, 名为“alternromagnetism”, 已经理论上提出并实验验证, 与��磁和反磁异有区别。 although altermagnets possess many exotic physical properties, the limited availability of known altermagnetic materials (e.g., 14 confirmed materials) hinders the study of such properties. Therefore, discovering more types of altermagnetic materials is crucial for a comprehensive understanding of altermagnetism and will facilitate new applications in the next generation information technologies, such as storage devices and high-sensitivity sensors.我们报告了25种新的� alternate magnetic materials，包括� metal、半导体和半导体，通过� unity symmetry analysis、graph neural network pre-training、optimal transport theory和� first-principles electronic structure calculation发现。 these newly discovered altermagnetic materials exhibit a wide range of electronic structural characteristics, resulting in various innovative physical properties, such as anomalous Hall effect, anomalous Kerr effect, and topological property. noteworthy, we discovered 8 $i$-wave altermagnetic materials for the first time.相比� human experts, AI search engine表现更好，提供了一些新的� alternate magnetic materials with unique properties, highlighting its potential for accelerated discovery of altermagnetic materials.
</details></li>
</ul>
<hr>
<h2 id="Human-Conditional-Reasoning-in-Answer-Set-Programming"><a href="#Human-Conditional-Reasoning-in-Answer-Set-Programming" class="headerlink" title="Human Conditional Reasoning in Answer Set Programming"></a>Human Conditional Reasoning in Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04412">http://arxiv.org/abs/2311.04412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Chiaki Sakama</li>
<li>for: 本研究探讨了人类思维中的四种推理类型，包括假设前提 (AA)、假设结果 (AC)、否定前提 (DA) 和否定结果 (DC)。</li>
<li>methods: 本文使用 answer set programming 实现了 AC、DA 和 DC 推理类型，并研究了这些推理类型的正式性和人类思维任务的相关性。</li>
<li>results: 本研究发现，AC 和 DA 推理类型在日常生活中很常见，而 DC 推理类型则是逻辑有效的。此外，本文还应用了这些完成方法于通用智能 reasoning 领域。<details>
<summary>Abstract</summary>
Given a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.
</details>
<details>
<summary>摘要</summary>
Given a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA, and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.Here's the translation in Traditional Chinese: givent a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA, and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.
</details></li>
</ul>
<hr>
<h2 id="Improved-DDIM-Sampling-with-Moment-Matching-Gaussian-Mixtures"><a href="#Improved-DDIM-Sampling-with-Moment-Matching-Gaussian-Mixtures" class="headerlink" title="Improved DDIM Sampling with Moment Matching Gaussian Mixtures"></a>Improved DDIM Sampling with Moment Matching Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04938">http://arxiv.org/abs/2311.04938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prasad Gabbur</li>
<li>for: 用于加速采样从预训练的推 Sobolev 模型（DDPM）中。</li>
<li>methods: 使用 Gaussian Mixture Model（GMM）作为反转过程操作（核），具体是匹配 DDPM 前向积分的首两个中心差异。</li>
<li>results: 通过对 CelebAHQ 和 FFHQ 的不条件模型以及 ImageNet 的类条件模型进行实验，并证明使用 GMM 核可以在少量采样步骤下提高生成样本的质量，并且在 FID 和 IS 指标中具有显著改善。例如在 ImageNet 256x256 上，使用 10 步采样，我们可以达到 FID 6.94 和 IS 207.85，与 Gaussian 核相比，这些指标的值分别为 10.15 和 196.73。<details>
<summary>Abstract</summary>
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73 respectively with a Gaussian kernel.
</details>
<details>
<summary>摘要</summary>
我们提议使用 Gaussian Mixture Model（GMM）作为反转过程操作器（核函数）在 Denoising Diffusion Implicit Models（DDIM）框架中，这是一种广泛使用的方法来加速从预训练的 Denoising Diffusion Probabilistic Models（DDPM）中采样。我们匹配了 DDPM 前向分布的首和第二个中心均值，通过限制 GMM 参数来实现这一点。我们发现， momemt 匹配是 suficient 来获得与原始 DDIM  Gaussian 核函数相同或更好的质量的采样。我们在 CelebAHQ 和 FFHQ 上训练了无条件模型，并在 ImageNet 数据集上训练了类别 condition 模型。我们的实验结果表明，使用 GMM 核函数可以在少量采样步骤下获得较好的采样质量， как measured by FID 和 IS 度量。例如，在 ImageNet 256x256 上，使用 10 步骤，我们达到了 FID 6.94 和 IS 207.85 ，与 Gaussian 核函数相比，分别降低了 3.21 和 139.88。
</details></li>
</ul>
<hr>
<h2 id="Human-Centered-Planning"><a href="#Human-Centered-Planning" class="headerlink" title="Human-Centered Planning"></a>Human-Centered Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04403">http://arxiv.org/abs/2311.04403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Yuliang Li, Nitin Kamra, Ruta Desai, Alon Halevy</li>
<li>for: 本研究旨在开发一个基于深度学习模型（LLM）的计划程序，以便在用户提供的自然语言约束下，生成一个合理的计划。</li>
<li>methods: 本研究使用了LLM和符号逻辑计划程序（SymPlan），并将两者结合在一起以提高计划的可靠性和用户满意度。</li>
<li>results: 实验结果显示，LLMPlan与传统的符号逻辑计划程序相比，在不具有正式约束的情况下，平均表现相当，而且能够更好地满足用户的隐式需求。在交互评估中，LLM-基于的计划程序的用户满意度高于符号逻辑计划程序的用户满意度（70.5% vs. 40.4%）。<details>
<summary>Abstract</summary>
LLMs have recently made impressive inroads on tasks whose output is structured, such as coding, robotic planning and querying databases. The vision of creating AI-powered personal assistants also involves creating structured outputs, such as a plan for one's day, or for an overseas trip. Here, since the plan is executed by a human, the output doesn't have to satisfy strict syntactic constraints. A useful assistant should also be able to incorporate vague constraints specified by the user in natural language. This makes LLMs an attractive option for planning.   We consider the problem of planning one's day. We develop an LLM-based planner (LLMPlan) extended with the ability to self-reflect on its output and a symbolic planner (SymPlan) with the ability to translate text constraints into a symbolic representation. Despite no formal specification of constraints, we find that LLMPlan performs explicit constraint satisfaction akin to the traditional symbolic planners on average (2% performance difference), while retaining the reasoning of implicit requirements. Consequently, LLM-based planners outperform their symbolic counterparts in user satisfaction (70.5% vs. 40.4%) during interactive evaluation with 40 users.
</details>
<details>
<summary>摘要</summary>
LLMs 近期在输出结构化任务上做出了印象深刻的进展，如编程、机器人规划和查询数据库等。创建基于 AI 的人工智能助手也涉及到创建结构化输出，如一天的计划或国外旅行计划。在这些情况下，由人类执行计划，输出不需要严格的语法约束。一个有用的助手应该能够根据用户提供的自然语言笔记中的抽象约束进行计划。这使得 LLMs 成为规划的有力选择。我们考虑一天的规划问题。我们开发了一个基于 LLM 的规划器（LLMPlan），并增加了对自己输出的自适应能力以及一个基于符号的规划器（SymPlan），可以将自然语言约束转换为符号表示。虽无正式约束规则，但我们发现 LLMPlan 在平均上与传统的符号规划器相当于满足约束（2%性能差异），同时保留了逻辑推理的隐式要求。因此，基于 LLM 的规划器在用户满意度方面（70.5% vs. 40.4%）在交互评估中超过符号规划器。
</details></li>
</ul>
<hr>
<h2 id="LRM-Large-Reconstruction-Model-for-Single-Image-to-3D"><a href="#LRM-Large-Reconstruction-Model-for-Single-Image-to-3D" class="headerlink" title="LRM: Large Reconstruction Model for Single Image to 3D"></a>LRM: Large Reconstruction Model for Single Image to 3D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04400">http://arxiv.org/abs/2311.04400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, Hao Tan</li>
<li>for: 预测3D模型从单个输入图像中</li>
<li>methods: 使用高可Scalable transformer-based architecture和500万可学习参数直接预测神经辐射场（NeRF）</li>
<li>results: 可以高效地预测3D模型，包括从真实捕捉和生成模型中的图像中Here is the same information in Simplified Chinese:</li>
<li>for: 预测3D模型从单个输入图像中</li>
<li>methods: 使用高可Scalable transformer-based architecture和500万可学习参数直接预测神经辐射场（NeRF）</li>
<li>results: 可以高效地预测3D模型，包括从真实捕捉和生成模型中的图像中<details>
<summary>Abstract</summary>
We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs including real-world in-the-wild captures and images from generative models. Video demos and interactable 3D meshes can be found on this website: https://yiconghong.me/LRM/.
</details>
<details>
<summary>摘要</summary>
我们提出了首个大型重建模型（LRM），可以从单个输入图像中预测对象的3D模型，只需5秒钟。与以往的方法不同，LRM采用了可扩展的变换器基 architecture，并有5亿个学习参数来直接预测神经辐射场（NeRF）。我们在终端到终 Point manner进行了大规模的训练，使用了包括Objaverse中的 sintetic renderings和MVImgNet中的真实捕捉的大量多视图数据，共约1000万个对象。这种高容量模型和大规模的训练数据使得我们的模型具有高度泛化性和可以高质量地从多种测试输入，包括实际世界中的野外捕捉和生成模型中的图像，生成高质量的3D重建。网站上有视频 demo和可交互的3D mesh：https://yiconghong.me/LRM/。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.AI_2023_11_08/" data-id="clot2mh9b006zx7888m5u1rwx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/90/">90</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
