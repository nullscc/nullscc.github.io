
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.SD_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T15:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.SD_2023_10_31/">cs.SD - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Study-of-speaker-localization-with-binaural-microphone-array-incorporating-auditory-filters-and-lateral-angle-estimation"><a href="#Study-of-speaker-localization-with-binaural-microphone-array-incorporating-auditory-filters-and-lateral-angle-estimation" class="headerlink" title="Study of speaker localization with binaural microphone array incorporating auditory filters and lateral angle estimation"></a>Study of speaker localization with binaural microphone array incorporating auditory filters and lateral angle estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20238">http://arxiv.org/abs/2310.20238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanir Maymon, Israel Nelken, Boaz Rafaely</li>
<li>for: 这 paper 的目的是提出一种基于人类听觉的声音定位方法，用于应用 such as 语音交流、视频会议和机器人听觉。</li>
<li>methods: 这 paper 提出了一些不同于现有方法的处理阶段，包括使用快时傅立叙Transform (STFT) 转换，以及基于转换 HRTF 集的方向来源搜索。</li>
<li>results: 在 simulations 和实验中，提出的方法与现有方法相比，具有更高的准确率和更好的性能。<details>
<summary>Abstract</summary>
Speaker localization for binaural microphone arrays has been widely studied for applications such as speech communication, video conferencing, and robot audition. Many methods developed for this task, including the direct path dominance (DPD) test, share common stages in their processing, which include transformation using the short-time Fourier transform (STFT), and a direction of arrival (DOA) search that is based on the head related transfer function (HRTF) set. In this paper, alternatives to these processing stages, motivated by human hearing, are proposed. These include incorporating an auditory filter bank to replace the STFT, and a new DOA search based on transformed HRTF as steering vectors. A simulation study and an experimental study are conducted to validate the proposed alternatives, and both are applied to two binaural DOA estimation methods; the results show that the proposed method compares favorably with current methods.
</details>
<details>
<summary>摘要</summary>
对于双耳麦克录音的话音位置定位，有广泛的研究，用于应用如语音通信、 видео会议和机器人听觉。许多这些方法都有共同的处理阶段，包括对STFT（短时傅立叶变换）进行转换，以及基于HRTF（人类听觉函数）集的方向来探索（DOA）搜寻。在这篇论文中，我们提出了一些人类听觉的启发，包括将音频范围对应到人类听觉系统中的启发，并且将HRTF集转换为探索方向的对称点。我们将这些方法与现有的方法进行比较，通过实验和 simulations 进行验证。结果显示，提案的方法与现有方法相比，在双耳DOA估计方法中具有比较好的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.SD_2023_10_31/" data-id="clogy1z7900xrffrafjrjejqm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CV_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T13:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.CV_2023_10_31/">cs.CV - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FPO-Efficient-Encoding-and-Rendering-of-Dynamic-Neural-Radiance-Fields-by-Analyzing-and-Enhancing-Fourier-PlenOctrees"><a href="#FPO-Efficient-Encoding-and-Rendering-of-Dynamic-Neural-Radiance-Fields-by-Analyzing-and-Enhancing-Fourier-PlenOctrees" class="headerlink" title="FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance Fields by Analyzing and Enhancing Fourier PlenOctrees"></a>FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance Fields by Analyzing and Enhancing Fourier PlenOctrees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20710">http://arxiv.org/abs/2310.20710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saskia Rabich, Patrick Stotko, Reinhard Klein</li>
<li>for: 这个论文旨在提高动态神经辐射场（NeRF）的实时渲染，使用改进的傅立做Octree表示。</li>
<li>methods: 该论文使用了改进的傅立做Octree表示，并对其进行了深入分析和优化。特别是，它使用了一种适应传输函数的权重编码，以降低压缩过程中引入的 artifacts。此外，它还提出了一种增强训练数据的方法，以减少压缩过程中的周期性假设。</li>
<li>results: 该论文通过Quantitative和Qualitative的评估，在synthetic和实际场景中证明了其改进后的表示方法的效果。<details>
<summary>Abstract</summary>
Fourier PlenOctrees have shown to be an efficient representation for real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many advantages, this method suffers from artifacts introduced by the involved compression when combining it with recent state-of-the-art techniques for training the static per-frame NeRF models. In this paper, we perform an in-depth analysis of these artifacts and leverage the resulting insights to propose an improved representation. In particular, we present a novel density encoding that adapts the Fourier-based compression to the characteristics of the transfer function used by the underlying volume rendering procedure and leads to a substantial reduction of artifacts in the dynamic model. Furthermore, we show an augmentation of the training data that relaxes the periodicity assumption of the compression. We demonstrate the effectiveness of our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative evaluations on synthetic and real-world scenes.
</details>
<details>
<summary>摘要</summary>
傅리对象网（Fourier PlenOctrees）已经证明是实时渲染动态神经遮蔽场（NeRF）的有效表示方法。 despite its many advantages, this method suffers from artifacts introduced by the involved compression when combining it with recent state-of-the-art techniques for training the static per-frame NeRF models. In this paper, we perform an in-depth analysis of these artifacts and leverage the resulting insights to propose an improved representation. In particular, we present a novel density encoding that adapts the Fourier-based compression to the characteristics of the transfer function used by the underlying volume rendering procedure and leads to a substantial reduction of artifacts in the dynamic model. Furthermore, we show an augmentation of the training data that relaxes the periodicity assumption of the compression. We demonstrate the effectiveness of our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative evaluations on synthetic and real-world scenes.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="DDAM-PS-Diligent-Domain-Adaptive-Mixer-for-Person-Search"><a href="#DDAM-PS-Diligent-Domain-Adaptive-Mixer-for-Person-Search" class="headerlink" title="DDAM-PS: Diligent Domain Adaptive Mixer for Person Search"></a>DDAM-PS: Diligent Domain Adaptive Mixer for Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20706">http://arxiv.org/abs/2310.20706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mustansarfiaz/ddam-ps">https://github.com/mustansarfiaz/ddam-ps</a></li>
<li>paper_authors: Mohammed Khaleed Almansoori, Mustansar Fiaz, Hisham Cholakkal</li>
<li>for: 本文目的是提高人体搜索（PS） task 的预测性能，特别是在针对不同频谱的目标域频谱上进行适应性调整。</li>
<li>methods: 本文提出了一种努力适应域（DDAM）模块，用于组合源域和目标域的表示，并使得这些表示具有中等的混合域特征。该模块通过混合域来减少源和目标域之间的距离，从而提高人体识别（ReID）任务。为此，本文引入了两个桥接损失和一个差异损失。两个桥接损失的目的是使得中等混合域表示保持源和目标域表示的适当距离，而差异损失的目的是避免中等混合域表示偏向任一个域。</li>
<li>results: 本文的方法在PRW和CUHK-SYSU等难度较高的 dataset 上进行了实验，并达到了比较出色的性能。<details>
<summary>Abstract</summary>
Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our source code is publicly available at \url{https://github.com/mustansarfiaz/DDAM-PS}.
</details>
<details>
<summary>摘要</summary>
人体搜索（PS）是一个 Computer Vision 问题，既需要实现人体检测和重新识别（ReID）的联合优化。 although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer（DDAM）for person search（DDAP-PS）framework that aims to bridge this gap and improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our source code is publicly available at \url{https://github.com/mustansarfiaz/DDAM-PS}.
</details></li>
</ul>
<hr>
<h2 id="SEINE-Short-to-Long-Video-Diffusion-Model-for-Generative-Transition-and-Prediction"><a href="#SEINE-Short-to-Long-Video-Diffusion-Model-for-Generative-Transition-and-Prediction" class="headerlink" title="SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"></a>SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20700">http://arxiv.org/abs/2310.20700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, Ziwei Liu</li>
<li>for: 生成高质量的长视频（story-level），包括创新的过渡和预测效果。</li>
<li>methods: 提出了一种基于随机掩模的视频扩散模型，通过提供不同场景图像和文本描述来自动生成过渡。</li>
<li>results: 对比 EXISTS 等方法，模型能够生成高质量的长视频，并且可以扩展到其他任务，如图像到视频动画和自然语言视频预测。<details>
<summary>Abstract</summary>
Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips ("shot-level") depicting a single scene. To deliver a coherent long video ("story-level"), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos. Project page: https://vchitect.github.io/SEINE-project/ .
</details>
<details>
<summary>摘要</summary>
近期视频生成技术已经取得了很大的进步，但现有的人工智能生成的视频通常是单个场景的短片("shot-level")。为了提供一个流畅、创新的长视频("story-level")，需要有生成过渡和预测效果。本文提出了一种短视频扩展模型，即SEINE，其目的是生成高质量的长视频，并保证每个镜头的视觉质量和各个场景之间的过渡是流畅、创新的。具体来说，我们提出了一种随机掩模型，通过文本描述来自动生成过渡。通过输入不同场景的图像，以及文本控制，我们的模型可以生成具有各个场景的视觉协调性和高质量的过渡视频。此外，我们的模型可以轻松扩展到其他任务，如图像到视频动画和自适应视频预测。为了全面评估这种新的生成任务，我们提出了三个评价标准：时间一致性、semantic similarity和视频文本semantic alignment。广泛的实验证明了我们的方法在生成过渡和预测方面的效果，使得创建story-level长视频变得可能。项目页面：https://vchitect.github.io/SEINE-project/
</details></li>
</ul>
<hr>
<h2 id="NeRF-Revisited-Fixing-Quadrature-Instability-in-Volume-Rendering"><a href="#NeRF-Revisited-Fixing-Quadrature-Instability-in-Volume-Rendering" class="headerlink" title="NeRF Revisited: Fixing Quadrature Instability in Volume Rendering"></a>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20685">http://arxiv.org/abs/2310.20685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikacuy/PL-NeRF">https://github.com/mikacuy/PL-NeRF</a></li>
<li>paper_authors: Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas, Ke Li</li>
<li>for: 本文旨在解决NeRF中的量子不稳定性问题，提高rendering结果的精度和稳定性。</li>
<li>methods: 本文提出一种基于拓扑学原理的解决方案，通过修改样本基于渲染公式，使其与偏极质量density函数匹配，并同时解决了多个问题，如样本之间冲突、层次样本选择不精确和模型参数不可导。</li>
<li>results: 相比传统的样本基于渲染公式，本文的提案可以提供更加锐利的文字、更好的几何重建和更强的深度指导。此外，本文的方法可以与现有NeRF方法的量子渲染公式进行drop-in替换，从而不需要更改现有的实现。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub quadrature instability. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under piecewise linear volume density. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation of existing NeRF-based methods. Our project page can be found at pl-nerf.github.io.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="StairNet-Visual-Recognition-of-Stairs-for-Human-Robot-Locomotion"><a href="#StairNet-Visual-Recognition-of-Stairs-for-Human-Robot-Locomotion" class="headerlink" title="StairNet: Visual Recognition of Stairs for Human-Robot Locomotion"></a>StairNet: Visual Recognition of Stairs for Human-Robot Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20666">http://arxiv.org/abs/2310.20666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Garrett Kurbis, Dmytro Kuzmenko, Bogdan Ivanyuk-Skulskiy, Alex Mihailidis, Brokoslaw Laschowski<br>for: 这个研究旨在开发新的深度学习模型，用于视觉感知和识别楼梯，以提高人机合作下的机器人步行控制。methods: 这个研究使用了大规模的手动标注图像数据集（超过515,000张图像），以及不同的深度学习模型（如2D和3D CNN、混合CNN和LSTM、ViT网络）和训练方法（如监督学习和 semi-监督学习）。results: 这个研究表明，使用StairNet数据集和不同的设计，可以实现高精度分类（最高达98.8%），并且可以在移动设备上使用GPU和NPU加速器实现实时推理速度（最高达2.8ms）。但是，在使用特制的CPU驱动的智能眼镜上部署时，因为嵌入式硬件的限制，推理速度只有1.5秒，这presenting a trade-off between human-centered design and performance。<details>
<summary>Abstract</summary>
Human-robot walking with prosthetic legs and exoskeletons, especially over complex terrains such as stairs, remains a significant challenge. Egocentric vision has the unique potential to detect the walking environment prior to physical interactions, which can improve transitions to and from stairs. This motivated us to create the StairNet initiative to support the development of new deep learning models for visual sensing and recognition of stairs, with an emphasis on lightweight and efficient neural networks for onboard real-time inference. In this study, we present an overview of the development of our large-scale dataset with over 515,000 manually labeled images, as well as our development of different deep learning models (e.g., 2D and 3D CNN, hybrid CNN and LSTM, and ViT networks) and training methods (e.g., supervised learning with temporal data and semi-supervised learning with unlabeled images) using our new dataset. We consistently achieved high classification accuracy (i.e., up to 98.8%) with different designs, offering trade-offs between model accuracy and size. When deployed on mobile devices with GPU and NPU accelerators, our deep learning models achieved inference speeds up to 2.8 ms. We also deployed our models on custom-designed CPU-powered smart glasses. However, limitations in the embedded hardware yielded slower inference speeds of 1.5 seconds, presenting a trade-off between human-centered design and performance. Overall, we showed that StairNet can be an effective platform to develop and study new visual perception systems for human-robot locomotion with applications in exoskeleton and prosthetic leg control.
</details>
<details>
<summary>摘要</summary>
人机徒步使用假肢和外囊仍然是一个 significante挑战，尤其是在复杂的地形上，如楼梯。 Egocentric vision有独特的潜在力量，可以在物理互动之前探测行走环境，从而改善徒步楼梯之间的转换。这些motivated我们创立StairNet项目，以支持开发新的深度学习模型，用于视觉感知和识别楼梯，强调轻量级和高效的神经网络，以便在实时推理中进行本地执行。在这种研究中，我们提供了大规模的数据集，包括超过515,000个手动标注的图像，以及我们开发的不同的深度学习模型（如2D和3D CNN、混合CNN和LSTM网络）和训练方法（如监督学习和无标签图像）。我们一致地实现了高精度分类（达98.8%），提供了模型精度和大小之间的质量。当在移动设备上部署深度学习模型时，我们实现了最高的推理速度达2.8ms。此外，我们还部署了我们的模型在自定义的CPU驱动的智能眼镜上，但由于嵌入式硬件的限制，推理速度为1.5秒，表现出人类中心设计和性能之间的负担。总之，我们表明了StairNet可以是一个有效的平台，用于开发和研究新的视觉感知系统，以应对人机徒步控制的应用。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Limitations-of-State-Aware-Imitation-Learning-for-Autonomous-Driving"><a href="#Addressing-Limitations-of-State-Aware-Imitation-Learning-for-Autonomous-Driving" class="headerlink" title="Addressing Limitations of State-Aware Imitation Learning for Autonomous Driving"></a>Addressing Limitations of State-Aware Imitation Learning for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20650">http://arxiv.org/abs/2310.20650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Cultrera, Federico Becattini, Lorenzo Seidenari, Pietro Pala, Alberto Del Bimbo</li>
<li>for: 本研究旨在解决自动驾驶代理人训练中的两个问题：一是惯性问题，即模型尝试将低速和无加速错误地相关联，二是在线和离线性能之间的低相关性，由于小错误的积累而导致模型处于未经见过的状态。</li>
<li>methods: 本研究提出了一种基于多任务学习的多stage视transformer网络，其中包含了状态卷积的方法。我们将自动驾驶车辆的状态作为特殊的转换器的Token，然后在网络中卷积其他环境特征。这种方法可以从多个角度解决问题：引导驾驶策略，使用学习的停止&#x2F;继续信息；直接在车辆状态上进行数据扩展；并且可以直观地解释模型的决策。</li>
<li>results: 我们的实验结果表明，使用这种方法可以减少惯性问题，并且在线和离线性能之间呈高相关性。<details>
<summary>Abstract</summary>
Conditional Imitation learning is a common and effective approach to train autonomous driving agents. However, two issues limit the full potential of this approach: (i) the inertia problem, a special case of causal confusion where the agent mistakenly correlates low speed with no acceleration, and (ii) low correlation between offline and online performance due to the accumulation of small errors that brings the agent in a previously unseen state. Both issues are critical for state-aware models, yet informing the driving agent of its internal state as well as the state of the environment is of crucial importance. In this paper we propose a multi-task learning agent based on a multi-stage vision transformer with state token propagation. We feed the state of the vehicle along with the representation of the environment as a special token of the transformer and propagate it throughout the network. This allows us to tackle the aforementioned issues from different angles: guiding the driving policy with learned stop/go information, performing data augmentation directly on the state of the vehicle and visually explaining the model's decisions. We report a drastic decrease in inertia and a high correlation between offline and online metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>请求转换文本为简化中文。<</SYS>>条件模仿学习是自驾车智能代理的常见和有效方法。然而，两个问题限制了这种方法的全面潜力：（i）抗力问题，特殊情况下的 causal 混乱，agent 错误地相关低速和无加速的 corrrelation，和（ii）在线和离线性能之间的低相关性，由小错误的积累导致agent在未经见过的状态下。这两个问题对状态意识模型非常重要，但是通过告诉驾车代理其内部状态以及环境状态的重要性。在这篇论文中，我们提出了基于多任务学习的多stage vision transformer，并将驾车器的状态作为特殊token传播到网络中。这allow us to从不同的角度解决上述问题：通过学习停止/继续信息来引导驾车策略，直接在驾车器的状态上进行数据扩展，以及可视化模型的决策。我们报告了很大的抗力减少和在线和离线指标之间的高相关性。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Batch-Norm-Statistics-Update-for-Natural-Robustness"><a href="#Dynamic-Batch-Norm-Statistics-Update-for-Natural-Robustness" class="headerlink" title="Dynamic Batch Norm Statistics Update for Natural Robustness"></a>Dynamic Batch Norm Statistics Update for Natural Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20649">http://arxiv.org/abs/2310.20649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahbaz Rezaei, Mohammad Sadegh Norouzzadeh</li>
<li>for: 提高隐形训练模型对各种损害样本的抗干扰性能</li>
<li>methods: 使用快速响应的批量常数（BatchNorm）统计更新法，以及在快速响应的批量常数统计更新法基础上的频域检测模型</li>
<li>results: 在不同的模型和数据集上实现了约8%和4%的抗干扰性能提高，并且可以进一步提高现有的state-of-the-art robust模型的性能，如AugMix和DeepAug。<details>
<summary>Abstract</summary>
DNNs trained on natural clean samples have been shown to perform poorly on corrupted samples, such as noisy or blurry images. Various data augmentation methods have been recently proposed to improve DNN's robustness against common corruptions. Despite their success, they require computationally expensive training and cannot be applied to off-the-shelf trained models. Recently, it has been shown that updating BatchNorm (BN) statistics of an off-the-shelf model on a single corruption improves its accuracy on that corruption significantly. However, adopting the idea at inference time when the type of corruption is unknown and changing decreases the effectiveness of this method. In this paper, we harness the Fourier domain to detect the corruption type, a challenging task in the image domain. We propose a unified framework consisting of a corruption-detection model and BN statistics update that improves the corruption accuracy of any off-the-shelf trained model. We benchmark our framework on different models and datasets. Our results demonstrate about 8% and 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively. Furthermore, our framework can further improve the accuracy of state-of-the-art robust models, such as AugMix and DeepAug.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在天然清晰样本上训练后表现不佳于受损样本，如噪音或模糊图像。 various数据增强方法已经在最近提出来提高DNN的对通用损害的Robustness。 despite their success, they require computationally expensive training and cannot be applied to off-the-shelf trained models.  Recently, it has been shown that updating BatchNorm（BN）统计值 of an off-the-shelf model on a single corruption improves its accuracy on that corruption significantly. However, adopting the idea at inference time when the type of corruption is unknown and changing decreases the effectiveness of this method.In this paper, we harness the Fourier domain to detect the corruption type, a challenging task in the image domain. We propose a unified framework consisting of a corruption-detection model and BN statistics update that improves the corruption accuracy of any off-the-shelf trained model. We benchmark our framework on different models and datasets. Our results demonstrate about 8% and 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively. Furthermore, our framework can further improve the accuracy of state-of-the-art robust models, such as AugMix and DeepAug.
</details></li>
</ul>
<hr>
<h2 id="Using-Higher-Order-Moments-to-Assess-the-Quality-of-GAN-generated-Image-Features"><a href="#Using-Higher-Order-Moments-to-Assess-the-Quality-of-GAN-generated-Image-Features" class="headerlink" title="Using Higher-Order Moments to Assess the Quality of GAN-generated Image Features"></a>Using Higher-Order Moments to Assess the Quality of GAN-generated Image Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20636">http://arxiv.org/abs/2310.20636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Luzi, Helen Jenne, Ryan Murray, Carlos Ortiz Marrero</li>
<li>for: 评估生成对抗网络（GANs）模型的耐性性能</li>
<li>methods: 利用新的度量指标——偏差启发距离（SID）来评估图像特征数据的分布性能</li>
<li>results: 实验结果表明，SID可以跟踪或与人类感知更加一致地评估图像特征数据的质量，并且可以替代传统的启发距离（FID）指标。<details>
<summary>Abstract</summary>
The rapid advancement of Generative Adversarial Networks (GANs) necessitates the need to robustly evaluate these models. Among the established evaluation criteria, the Fr\'{e}chet Inception Distance (FID) has been widely adopted due to its conceptual simplicity, fast computation time, and strong correlation with human perception. However, FID has inherent limitations, mainly stemming from its assumption that feature embeddings follow a Gaussian distribution, and therefore can be defined by their first two moments. As this does not hold in practice, in this paper we explore the importance of third-moments in image feature data and use this information to define a new measure, which we call the Skew Inception Distance (SID). We prove that SID is a pseudometric on probability distributions, show how it extends FID, and present a practical method for its computation. Our numerical experiments support that SID either tracks with FID or, in some cases, aligns more closely with human perception when evaluating image features of ImageNet data.
</details>
<details>
<summary>摘要</summary>
“ Generative Adversarial Networks (GANs) 的快速进步需要对这些模型进行坚固的评估。已有多个评估标准，其中 Fréchet Inception Distance (FID) 因为其概念简单、计算速度快和人类感知强相关性而广泛运用。然而，FID 有一些限制，主要是假设特征嵌入 seguir una distribución Gaussian，因此可以通过它们的首两个维度定义。然而，在实际应用中，这不是实际情况。这篇论文探讨了特征数据中的第三维度信息的重要性，并使用这个信息定义一个新的衡量方法，我们称之为 Skew Inception Distance (SID)。我们证明了 SID 是一个 pseudometric 在概率分布上，并详细介绍了它与 FID 之间的关系。我们的实验表明，SID 可以跟踪 FID 或，在一些情况下，与人类感知更加相似地评估图像特征。”
</details></li>
</ul>
<hr>
<h2 id="Deepfake-detection-by-exploiting-surface-anomalies-the-SurFake-approach"><a href="#Deepfake-detection-by-exploiting-surface-anomalies-the-SurFake-approach" class="headerlink" title="Deepfake detection by exploiting surface anomalies: the SurFake approach"></a>Deepfake detection by exploiting surface anomalies: the SurFake approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20621">http://arxiv.org/abs/2310.20621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ciamarra, Roberto Caldelli, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 本研究旨在探讨深伪创造过程对场景特征的影响，以提高深伪检测的精度。</li>
<li>methods: 本研究使用SurFake方法，通过分析图像表面特征来生成描述符，并使用深度学习模型进行检测。</li>
<li>results: 实验结果表明，SurFake方法可以有效地分辨深伪图像和正常图像，并且可以与视觉数据结合使用，提高检测精度。<details>
<summary>Abstract</summary>
The ever-increasing use of synthetically generated content in different sectors of our everyday life, one for all media information, poses a strong need for deepfake detection tools in order to avoid the proliferation of altered messages. The process to identify manipulated content, in particular images and videos, is basically performed by looking for the presence of some inconsistencies and/or anomalies specifically due to the fake generation process. Different techniques exist in the scientific literature that exploit diverse ad-hoc features in order to highlight possible modifications. In this paper, we propose to investigate how deepfake creation can impact on the characteristics that the whole scene had at the time of the acquisition. In particular, when an image (video) is captured the overall geometry of the scene (e.g. surfaces) and the acquisition process (e.g. illumination) determine a univocal environment that is directly represented by the image pixel values; all these intrinsic relations are possibly changed by the deepfake generation process. By resorting to the analysis of the characteristics of the surfaces depicted in the image it is possible to obtain a descriptor usable to train a CNN for deepfake detection: we refer to such an approach as SurFake. Experimental results carried out on the FF++ dataset for different kinds of deepfake forgeries and diverse deep learning models confirm that such a feature can be adopted to discriminate between pristine and altered images; furthermore, experiments witness that it can also be combined with visual data to provide a certain improvement in terms of detection accuracy.
</details>
<details>
<summary>摘要</summary>
随着人工生成内容在不同领域的日常生活中越来越广泛应用，特别是在媒体信息领域，需要深入检测深刻抹黑技术以避免扩散修改的信息。检测修改内容的过程通常是通过检查修改过程中的一些不一致和异常来进行，特别是对图像和视频进行修改。在科学文献中，有多种不同特点的技术可以检测修改，以高亮可能的修改。在本文中，我们将研究深刻抹黑创造对图像（视频）采集过程中场景的特征所带来的影响。Specifically, when an image (video) is captured, the overall geometry of the scene (e.g. surfaces) and the acquisition process (e.g. illumination) determine a unique environment that is directly represented by the image pixel values; all these intrinsic relations are possibly changed by the deepfake generation process. By resorting to the analysis of the characteristics of the surfaces depicted in the image, it is possible to obtain a descriptor usable to train a CNN for deepfake detection: we refer to such an approach as SurFake. Experimental results carried out on the FF++ dataset for different kinds of deepfake forgeries and diverse deep learning models confirm that such a feature can be adopted to discriminate between pristine and altered images; furthermore, experiments witness that it can also be combined with visual data to provide a certain improvement in terms of detection accuracy.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Reconstruction-of-Ultrasound-Images-with-Informative-Uncertainty"><a href="#Diffusion-Reconstruction-of-Ultrasound-Images-with-Informative-Uncertainty" class="headerlink" title="Diffusion Reconstruction of Ultrasound Images with Informative Uncertainty"></a>Diffusion Reconstruction of Ultrasound Images with Informative Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20618">http://arxiv.org/abs/2310.20618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yuxin-Zhang-Jasmine/DRUS-v2">https://github.com/Yuxin-Zhang-Jasmine/DRUS-v2</a></li>
<li>paper_authors: Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</li>
<li>For: 提高超声音像质量，解决超声音像噪声和artefacts问题。* Methods: 基于扩散模型，结合超声 физи学特性，实现高质量图像重建。* Results: 在模拟、实验室和实际数据上进行了广泛的实验，证明了我们的方法可以从单个扩散波输入获得高质量图像重建，并比 estado-of-the-art 方法更高效。<details>
<summary>Abstract</summary>
Despite its wide use in medicine, ultrasound imaging faces several challenges related to its poor signal-to-noise ratio and several sources of noise and artefacts. Enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. In recent years, there has been progress both in model-based and learning-based approaches to improve ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid approach leveraging advances in diffusion models. To this end, we adapt Denoising Diffusion Restoration Models (DDRM) to incorporate ultrasound physics through a linear direct model and an unsupervised fine-tuning of the prior diffusion model. We conduct comprehensive experiments on simulated, in-vitro, and in-vivo data, demonstrating the efficacy of our approach in achieving high-quality image reconstructions from a single plane wave input and in comparison to state-of-the-art methods. Finally, given the stochastic nature of the method, we analyse in depth the statistical properties of single and multiple-sample reconstructions, experimentally show the informativeness of their variance, and provide an empirical model relating this behaviour to speckle noise. The code and data are available at: (upon acceptance).
</details>
<details>
<summary>摘要</summary>
医学中广泛使用ultrasound imaging技术，但它面临着减少信号噪声和噪声的多种源头的挑战。提高ultrasound图像质量需要平衡同时的因素，例如对比、分辨率和颗粒保持。在最近几年中，有进展在基于模型和学习方法来提高ultrasound图像重建。我们提出了一种hybrid方法，利用了扩散模型的进步。为此，我们采用了Denosing Diffusion Restoration Models（DDRM），并在线性直方程和无监督精度修正中加入了ultrasound物理。我们对实验室、生物实验室和动物实验室数据进行了广泛的实验，并证明了我们的方法在获得高质量图像重建的单板波入力和相比之下的state-of-the-art方法。最后，由于方法的随机性，我们进行了深入的统计分析，实验证明了单个和多个样本重建的异常性，并提供了一个预测这种行为和颗粒噪声之间的关系的模型。代码和数据在接受后提供。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Synthetic-MRI-Generation-from-CT-Scans-Using-CycleGAN-with-Feature-Extraction"><a href="#Enhanced-Synthetic-MRI-Generation-from-CT-Scans-Using-CycleGAN-with-Feature-Extraction" class="headerlink" title="Enhanced Synthetic MRI Generation from CT Scans Using CycleGAN with Feature Extraction"></a>Enhanced Synthetic MRI Generation from CT Scans Using CycleGAN with Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20604">http://arxiv.org/abs/2310.20604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Nikbakhsh, Lachin Naghashyar, Morteza Valizadeh, Mehdi Chehel Amirani</li>
<li>for:  This paper aims to address the challenges of multimodal alignment in radiotherapy planning by proposing an approach for enhanced monomodal registration using synthetic MRI images.</li>
<li>methods:  The proposed method uses unpaired data and combines CycleGANs and feature extractors to produce synthetic MRI images from CT scans.</li>
<li>results:  The method outperforms several state-of-the-art methods and shows promising results, validated by multiple comparison metrics.Here is the full summary in Simplified Chinese:</li>
<li>for: 本文目的是解决 ради疗规划中多ModalAlignment的挑战，提出一种基于synthetic MRI图像的增强单模准则注册方法。</li>
<li>methods: 该方法使用无对数据，将CycleGANs和特征提取器组合使用，从CT扫描图像生成synthetic MRI图像。</li>
<li>results: 该方法超过了一些state-of-the-art方法的表现，通过多个比较指标证明其效果。<details>
<summary>Abstract</summary>
In the field of radiotherapy, accurate imaging and image registration are of utmost importance for precise treatment planning. Magnetic Resonance Imaging (MRI) offers detailed imaging without being invasive and excels in soft-tissue contrast, making it a preferred modality for radiotherapy planning. However, the high cost of MRI, longer acquisition time, and certain health considerations for patients pose challenges. Conversely, Computed Tomography (CT) scans offer a quicker and less expensive imaging solution. To bridge these modalities and address multimodal alignment challenges, we introduce an approach for enhanced monomodal registration using synthetic MRI images. Utilizing unpaired data, this paper proposes a novel method to produce these synthetic MRI images from CT scans, leveraging CycleGANs and feature extractors. By building upon the foundational work on Cycle-Consistent Adversarial Networks and incorporating advancements from related literature, our methodology shows promising results, outperforming several state-of-the-art methods. The efficacy of our approach is validated by multiple comparison metrics.
</details>
<details>
<summary>摘要</summary>
在放射治疗领域，准确的成像和图像对应是至关重要的，以便精准地规划治疗方案。核磁共振成像（MRI）可以提供详细的成像，无需侵入性和软组织对比能力出色，因此成为放射治疗规划的首选设备。然而，MRI的高价格、长时间收集数据和某些健康因素对患者 pose 挑战。相比之下，计算机断层成像（CT）扫描可以提供更快和便宜的成像解决方案。为了 bridge 这两种模式和解决多模式对应问题，本文提出了一种增强单模式注册使用人工MRI图像的方法。通过使用无对应数据，本文提出了一种新的方法，使用CycleGANs和特征提取器生成人工MRI图像从CT扫描中。建立在基础的Cycle-Consistent Adversarial Networks之上，并 incorporate 相关文献的进步，我们的方法ologie 显示了可观的结果，超越了多个状态的参考方法。多个比较指标 validate 了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Brain-like-Flexible-Visual-Inference-by-Harnessing-Feedback-Feedforward-Alignment"><a href="#Brain-like-Flexible-Visual-Inference-by-Harnessing-Feedback-Feedforward-Alignment" class="headerlink" title="Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward Alignment"></a>Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20599">http://arxiv.org/abs/2310.20599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toosi/feedback_feedforward_alignment">https://github.com/toosi/feedback_feedforward_alignment</a></li>
<li>paper_authors: Tahereh Toosi, Elias B. Issa</li>
<li>for: This paper aims to explore the mechanisms behind how feedback connections in the visual cortex support flexible visual functions, such as denoising, resolving occlusions, hallucination, and imagination.</li>
<li>methods: The authors propose a learning algorithm called Feedback-Feedforward Alignment (FFA) that leverages feedback and feedforward pathways to co-optimize classification and reconstruction tasks, and demonstrate its effectiveness on widely used MNIST and CIFAR10 datasets.</li>
<li>results: The study shows that FFA can endow feedback connections with emergent visual inference functions, and alleviates weight transport problems encountered in traditional backpropagation (BP) methods, enhancing the bio-plausibility of the learning algorithm.<details>
<summary>Abstract</summary>
In natural vision, feedback connections support versatile visual inference capabilities such as making sense of the occluded or noisy bottom-up sensory information or mediating pure top-down processes such as imagination. However, the mechanisms by which the feedback pathway learns to give rise to these capabilities flexibly are not clear. We propose that top-down effects emerge through alignment between feedforward and feedback pathways, each optimizing its own objectives. To achieve this co-optimization, we introduce Feedback-Feedforward Alignment (FFA), a learning algorithm that leverages feedback and feedforward pathways as mutual credit assignment computational graphs, enabling alignment. In our study, we demonstrate the effectiveness of FFA in co-optimizing classification and reconstruction tasks on widely used MNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows feedback connections with emergent visual inference functions, including denoising, resolving occlusions, hallucination, and imagination. Moreover, FFA offers bio-plausibility compared to traditional backpropagation (BP) methods in implementation. By repurposing the computational graph of credit assignment into a goal-driven feedback pathway, FFA alleviates weight transport problems encountered in BP, enhancing the bio-plausibility of the learning algorithm. Our study presents FFA as a promising proof-of-concept for the mechanisms underlying how feedback connections in the visual cortex support flexible visual functions. This work also contributes to the broader field of visual inference underlying perceptual phenomena and has implications for developing more biologically inspired learning algorithms.
</details>
<details>
<summary>摘要</summary>
natural vision 的反馈连接支持多样化的视觉推理能力，如对 occluded 或噪声底上感知信息的理解或通过 pure top-down 过程来实现想象。然而，这些反馈路径学习如何灵活地演化这些能力的机制不清楚。我们提议，top-down 效应通过反馈和前进路径之间的对齐来实现。为此，我们提出了 Feedback-Feedforward Alignment（FFA）学习算法，利用反馈和前进路径作为互相归功计算图，实现对齐。在我们的研究中，我们证明 FF A在类型 MNIST 和 CIFAR10 上进行分类和重建任务中的效果。特别是，FFA 中的对齐机制使得反馈连接获得了 emergent 视觉推理功能，包括减噪、解除 occlusion、幻觉和想象。此外，FFA 具有与传统 backpropagation（BP）方法相比较高的生物可能性，因为它可以重用计算图的归功计算来实现对齐。通过将计算图转换为目标驱动的反馈路径，FFA 可以解决 BP 中的权重传输问题，从而提高生物可能性。我们的研究提出 FF A作为视觉推理机制的可能性 Mechanisms 的证明，这也对涉及到视觉推理的广泛领域有着推动作用。
</details></li>
</ul>
<hr>
<h2 id="FLODCAST-Flow-and-Depth-Forecasting-via-Multimodal-Recurrent-Architectures"><a href="#FLODCAST-Flow-and-Depth-Forecasting-via-Multimodal-Recurrent-Architectures" class="headerlink" title="FLODCAST: Flow and Depth Forecasting via Multimodal Recurrent Architectures"></a>FLODCAST: Flow and Depth Forecasting via Multimodal Recurrent Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20593">http://arxiv.org/abs/2310.20593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ciamarra, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 预测物体的运动和空间位置，特别在自动驾驶等安全关键场景中。</li>
<li>methods: 提议了一种名为FLODCAST的流和深度预测模型，利用多任务回归架构，同时预测两种不同的模式。</li>
<li>results: 在Cityscapes dataset上测试了模型，得到了流和深度预测两个领域的状态体现Record，同时还有助于下游任务 segmentation forecasting 的提高。<details>
<summary>Abstract</summary>
Forecasting motion and spatial positions of objects is of fundamental importance, especially in safety-critical settings such as autonomous driving. In this work, we address the issue by forecasting two different modalities that carry complementary information, namely optical flow and depth. To this end we propose FLODCAST a flow and depth forecasting model that leverages a multitask recurrent architecture, trained to jointly forecast both modalities at once. We stress the importance of training using flows and depth maps together, demonstrating that both tasks improve when the model is informed of the other modality. We train the proposed model to also perform predictions for several timesteps in the future. This provides better supervision and leads to more precise predictions, retaining the capability of the model to yield outputs autoregressively for any future time horizon. We test our model on the challenging Cityscapes dataset, obtaining state of the art results for both flow and depth forecasting. Thanks to the high quality of the generated flows, we also report benefits on the downstream task of segmentation forecasting, injecting our predictions in a flow-based mask-warping framework.
</details>
<details>
<summary>摘要</summary>
预测物体的运动和空间位置是基本重要的，尤其在自动驾驶等安全关键场景中。在这种情况下，我们解决这个问题，通过预测两种不同的模式，即光流和深度。为此，我们提出了FLODCAST模型，它利用多任务回归架构，同时预测两种模式。我们表明，在训练时使用光流和深度图像 вместе，可以提高模型的性能。我们还训练模型进行多个时间步预测，以提供更好的超VIZ和更准确的预测。我们在Cityscapes dataset上测试了我们的模型，并取得了流和深度预测的状态态和平台。由于我们生成的流是非常高质量的，因此我们还发现了在基于流的掩码扭曲框架中注入我们预测的好处。
</details></li>
</ul>
<hr>
<h2 id="Long-Tailed-Learning-as-Multi-Objective-Optimization"><a href="#Long-Tailed-Learning-as-Multi-Objective-Optimization" class="headerlink" title="Long-Tailed Learning as Multi-Objective Optimization"></a>Long-Tailed Learning as Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20490">http://arxiv.org/abs/2310.20490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqi Li, Fan Lyu, Fanhua Shang, Liang Wan, Wei Feng</li>
<li>for: 强调预测tail classes的模型性能，因为现有的方法对于头级类和尾级类之间存在“ seesaw dilemma”，即对尾级类的性能进步可能会对头级类的性能造成负面影响。</li>
<li>methods: 我们提出了一种基于多目标伸展的方法，将long-tailed recognition视为一个多目标伸展问题，以同时尊重头级和尾级类的贡献。我们还提出了一种 Gradient-Balancing Grouping (GBG) 策略，将类别分组，以便更好地平衡不同类别的梯度。</li>
<li>results: 我们在常用的benchmark上进行了广泛的实验，并证明了我们的方法在现有SOTA方法之上得到了superior的性能。<details>
<summary>Abstract</summary>
Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under a Pareto descent direction. Our GBG method drives classes with similar gradient directions to form more representative gradient and provide ideal compensation to the tail classes. Moreover, We conduct extensive experiments on commonly used benchmarks in long-tailed learning and demonstrate the superiority of our method over existing SOTA methods.
</details>
<details>
<summary>摘要</summary>
In this paper, we argue that the seesaw dilemma is caused by an imbalance in the gradients of the different classes. When the gradients of some classes are too important, the model can become overcompensated or undercompensated for those classes. To solve this problem, we formulate the long-tailed recognition as a multi-objective optimization problem, which respects the contributions of both the majority and minority classes simultaneously.To efficiently solve this problem, we propose a Gradient-Balancing Grouping (GBG) strategy. This strategy gathers classes with similar gradient directions together, and then approximately makes each update under a Pareto descent direction. This ensures that the model is updated in a way that is fair to all classes. Our GBG method helps classes with similar gradient directions to form more representative gradients, providing ideal compensation for the minority classes.We conduct extensive experiments on commonly used benchmarks in long-tailed learning and show that our method outperforms existing state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="LAVSS-Location-Guided-Audio-Visual-Spatial-Audio-Separation"><a href="#LAVSS-Location-Guided-Audio-Visual-Spatial-Audio-Separation" class="headerlink" title="LAVSS: Location-Guided Audio-Visual Spatial Audio Separation"></a>LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20446">http://arxiv.org/abs/2310.20446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YYX666660/LAVSS">https://github.com/YYX666660/LAVSS</a></li>
<li>paper_authors: Yuxin Ye, Wenming Yang, Yapeng Tian</li>
<li>for: 这篇论文旨在提高现有的机器学习研究中的独立音频视频分离（MAVS）技术，以便在虚拟现实（VR）&#x2F;虚拟现实（AR）场景中更好地分离音频来源。</li>
<li>methods: 这篇论文提出了一种基于空间声音分离的新方法，称为LAVSS（位置导向的声音视频分离器），它利用声音的相位差作为空间指示，并通过多级跨模态注意力来实现视觉位置协作。此外，它还利用了预训练的独立声音分离器来传递知识，以提高空间声音分离的性能。</li>
<li>results: 实验结果表明，LAVSS比既有的音频视频分离 benchmark 高效。具体来说，LAVSS在 FAIR-Play 数据集上的分离性能高于既有的音频视频分离方法。<details>
<summary>Abstract</summary>
Existing machine learning research has achieved promising results in monaural audio-visual separation (MAVS). However, most MAVS methods purely consider what the sound source is, not where it is located. This can be a problem in VR/AR scenarios, where listeners need to be able to distinguish between similar audio sources located in different directions. To address this limitation, we have generalized MAVS to spatial audio separation and proposed LAVSS: a location-guided audio-visual spatial audio separator. LAVSS is inspired by the correlation between spatial audio and visual location. We introduce the phase difference carried by binaural audio as spatial cues, and we utilize positional representations of sounding objects as additional modality guidance. We also leverage multi-level cross-modal attention to perform visual-positional collaboration with audio features. In addition, we adopt a pre-trained monaural separator to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on the FAIR-Play dataset demonstrate the superiority of the proposed LAVSS over existing benchmarks of audio-visual separation. Our project page: https://yyx666660.github.io/LAVSS/.
</details>
<details>
<summary>摘要</summary>
现有的机器学习研究已经实现了耳音视频分离（MAVS）的承诺结果。然而，大多数MAVS方法忽略了声音来源的位置信息。这可以是VR/AR场景中的一个问题，listeners需要能够在不同方向上分辨相似的声音来源。为了解决这些限制，我们扩展了MAVS到三维声音分离，并提出了位置引导的声音视频空间分离器（LAVSS）。LAVSS由声音视频的相关性引导，并利用降噪 Audio的相位差作为空间指示。此外，我们还利用多级跨模态注意力来实现视觉位置协作。此外，我们采用了已经预训练的单声道分离器来传递知识，从而提高了三维声音分离的性能。这利用了单声道和双声道通道之间的相关性。FAIR-Play数据集的实验表明，提出的LAVSS超过了现有的音视频分离标准。更多信息请参考我们的项目页面：https://yyx666660.github.io/LAVSS/.
</details></li>
</ul>
<hr>
<h2 id="SignAvatars-A-Large-scale-3D-Sign-Language-Holistic-Motion-Dataset-and-Benchmark"><a href="#SignAvatars-A-Large-scale-3D-Sign-Language-Holistic-Motion-Dataset-and-Benchmark" class="headerlink" title="SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark"></a>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20436">http://arxiv.org/abs/2310.20436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengdi Yu, Shaoli Huang, Yongkang Cheng, Tolga Birdal</li>
<li>for:  bridging the communication gap for hearing-impaired individuals by providing a large-scale multi-prompt 3D sign language (SL) motion dataset.</li>
<li>methods: compiling and curating the SignAvatars dataset, which includes 70,000 videos from 153 signers, and introducing an automated annotation pipeline for 3D holistic annotations.</li>
<li>results: facilitating various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs, and providing a unified benchmark for 3D SL holistic motion production.Here’s the text in Simplified Chinese:</li>
<li>for: 为听力障碍者 Bridge 通信差距，提供大规模多提示3D手语动作数据集。</li>
<li>methods: 编译并筹集SignAvatars数据集，包括70,000个视频，153名手语演示者，并引入自动化注释管道 для3D全息注释。</li>
<li>results: 实现多个任务，如3D手语识别（SLR）和3D手语生产（SLP）从多种输入，并提供3D手语全息动作生产的统一标准。<details>
<summary>Abstract</summary>
In this paper, we present SignAvatars, the first large-scale multi-prompt 3D sign language (SL) motion dataset designed to bridge the communication gap for hearing-impaired individuals. While there has been an exponentially growing number of research regarding digital communication, the majority of existing communication technologies primarily cater to spoken or written languages, instead of SL, the essential communication method for hearing-impaired communities. Existing SL datasets, dictionaries, and sign language production (SLP) methods are typically limited to 2D as the annotating 3D models and avatars for SL is usually an entirely manual and labor-intensive process conducted by SL experts, often resulting in unnatural avatars. In response to these challenges, we compile and curate the SignAvatars dataset, which comprises 70,000 videos from 153 signers, totaling 8.34 million frames, covering both isolated signs and continuous, co-articulated signs, with multiple prompts including HamNoSys, spoken language, and words. To yield 3D holistic annotations, including meshes and biomechanically-valid poses of body, hands, and face, as well as 2D and 3D keypoints, we introduce an automated annotation pipeline operating on our large corpus of SL videos. SignAvatars facilitates various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs like text scripts, individual words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further propose a unified benchmark of 3D SL holistic motion production. We believe that this work is a significant step forward towards bringing the digital world to the hearing-impaired communities. Our project page is at https://signavatars.github.io/
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍SignAvatars，第一个大规模多提示3D手语（SL）动作数据集，旨在为听力障碍人群填补沟通差距。然而，目前的数字通信研究启示，大多数现有的通信技术仅适用于口语或书面语言，而不是手语，听力障碍社区的重要沟通方式。现有的手语数据集、词汇和手语生产（SLP）方法通常是2D的，因为生成3D模型和手语人物需要手动、劳动密集的过程，导致生成的人物往往不自然。为了解决这些挑战，我们编辑和筛选了SignAvatars数据集，包括70,000个视频，共计8,34万帧，覆盖隔离手语和连续、相关的手语，以及多种提示，包括汉诺系、口语和单词。为了生成3D全息注释，包括身体、手部和脸部的生物可靠姿势，以及2D和3D关键点，我们引入了一个自动化注释管道，运行在我们大量的SL视频库中。SignAvatars支持多种任务，如3D手语识别（SLR）和基于多种输入的3D手语生产（SLP）。为了评估SignAvatars的潜力，我们进一步提出了一个3D手语整体动作生产的统一标准。我们认为，这项工作是听力障碍社区将数字世界带入的一大步 forward。我们的项目页面可以在<https://signavatars.github.io/>中找到。
</details></li>
</ul>
<hr>
<h2 id="Assessing-and-Enhancing-Robustness-of-Deep-Learning-Models-with-Corruption-Emulation-in-Digital-Pathology"><a href="#Assessing-and-Enhancing-Robustness-of-Deep-Learning-Models-with-Corruption-Emulation-in-Digital-Pathology" class="headerlink" title="Assessing and Enhancing Robustness of Deep Learning Models with Corruption Emulation in Digital Pathology"></a>Assessing and Enhancing Robustness of Deep Learning Models with Corruption Emulation in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20427">http://arxiv.org/abs/2310.20427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peixiang Huang, Songtao Zhang, Yulu Gan, Rui Xu, Rongqi Zhu, Wenkang Qin, Limei Guo, Shan Jiang, Lin Luo</li>
<li>for: 这个论文主要是为了提高深度学习在数字patologia中的Robustness，以便在临床诊断中使用。</li>
<li>methods: 这篇论文使用了全Stack corruption Emulation（OmniCE）方法来评估和进一步提高深度神经网络（DNN）模型的Robustness。</li>
<li>results: 研究发现，通过使用OmniCE方法生成的21种类型的损害数据，可以评估和提高DNN模型的Robustness，并且可以使用这些损害数据作为训练和实验数据来验证模型的普适性。<details>
<summary>Abstract</summary>
Deep learning in digital pathology brings intelligence and automation as substantial enhancements to pathological analysis, the gold standard of clinical diagnosis. However, multiple steps from tissue preparation to slide imaging introduce various image corruptions, making it difficult for deep neural network (DNN) models to achieve stable diagnostic results for clinical use. In order to assess and further enhance the robustness of the models, we analyze the physical causes of the full-stack corruptions throughout the pathological life-cycle and propose an Omni-Corruption Emulation (OmniCE) method to reproduce 21 types of corruptions quantified with 5-level severity. We then construct three OmniCE-corrupted benchmark datasets at both patch level and slide level and assess the robustness of popular DNNs in classification and segmentation tasks. Further, we explore to use the OmniCE-corrupted datasets as augmentation data for training and experiments to verify that the generalization ability of the models has been significantly enhanced.
</details>
<details>
<summary>摘要</summary>
深度学习在数字 PATHOLOGY 中带来了智能和自动化作为诊断的重要丰富。然而，从组织准备到板准图成像的多个步骤中引入了各种图像损害，使得深度神经网络（DNN）模型在临床应用中实现稳定的诊断结果很难。为了评估和进一步提高模型的Robustness，我们分析了 PATHOLOGY 生命周期中全栈损害的物理原因，并提出了Omni-Corruption Emulation（OmniCE）方法来重现21种损害，并将其分为5级严重程度。然后，我们构建了三个OmniCE-损害的标准 Dataset，其中一个是patch level，另外两个是板准图 level。我们还评估了popular DNNs在分类和分割任务中的Robustness。最后，我们explore使用OmniCE-损害的数据集作为训练和实验数据，以验证模型的通用能力得到了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Thermal-Infrared-Remote-Target-Detection-System-for-Maritime-Rescue-based-on-Data-Augmentation-with-3D-Synthetic-Data"><a href="#Thermal-Infrared-Remote-Target-Detection-System-for-Maritime-Rescue-based-on-Data-Augmentation-with-3D-Synthetic-Data" class="headerlink" title="Thermal-Infrared Remote Target Detection System for Maritime Rescue based on Data Augmentation with 3D Synthetic Data"></a>Thermal-Infrared Remote Target Detection System for Maritime Rescue based on Data Augmentation with 3D Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20412">http://arxiv.org/abs/2310.20412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjin Cheong, Wonho Jung, Yoon Seop Lim, Yong-Hwa Park</li>
<li>for: 这个论文是为了提出一种基于深度学习和数据增强的海上搜救thermal红外目标检测系统。</li>
<li>methods: 论文使用了深度学习和数据增强来提高海上搜救thermal红外目标检测的精度。具体来说，他们使用了一个自然语言生成器来实现预测和数据增强。</li>
<li>results: 实验结果显示，使用了增强数据的网络比只使用实际红外数据训练的网络表现得更好，并且提出的 segmentation 模型超过了现有的segmentation方法的性能。<details>
<summary>Abstract</summary>
This paper proposes a thermal-infrared (TIR) remote target detection system for maritime rescue using deep learning and data augmentation. We established a self-collected TIR dataset consisting of multiple scenes imitating human rescue situations using a TIR camera (FLIR). Additionally, to address dataset scarcity and improve model robustness, a synthetic dataset from a 3D game (ARMA3) to augment the data is further collected. However, a significant domain gap exists between synthetic TIR and real TIR images. Hence, a proper domain adaptation algorithm is essential to overcome the gap. Therefore, we suggest a domain adaptation algorithm in a target-background separated manner from 3D game-to-real, based on a generative model, to address this issue. Furthermore, a segmentation network with fixed-weight kernels at the head is proposed to improve the signal-to-noise ratio (SNR) and provide weak attention, as remote TIR targets inherently suffer from unclear boundaries. Experiment results reveal that the network trained on augmented data consisting of translated synthetic and real TIR data outperforms that trained on only real TIR data by a large margin. Furthermore, the proposed segmentation model surpasses the performance of state-of-the-art segmentation methods.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "TIR" is translated as "红外可见光" (hóngwai kěyǐguāng)* "remote target detection" is translated as "远程目标检测" (yuèjìng mùzhì jiǎnèsè)* "deep learning" is translated as "深度学习" (shēngrán xuéxí)* "data augmentation" is translated as "数据扩充" (shùjì kuòchōng)* "synthetic dataset" is translated as "合成数据集" (hétián shùjì)* "3D game" is translated as "3D游戏" (3D yóuxì)* "domain adaptation" is translated as "领域适应" (fāngyùn tiǎnjīn)* "generative model" is translated as "生成模型" (shēngchǎn módelì)* "segmentation network" is translated as "分割网络" (fēnzhàng wǎngluò)* "signal-to-noise ratio" is translated as "信号噪声比" (xìnhòu zhōngshēng bǐ)* "weak attention" is translated as "弱注意" (ruò zhùyì)
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Reference-Image-Assisted-Volumetric-Super-Resolution-of-Cardiac-Diffusion-Weighted-Imaging"><a href="#High-Resolution-Reference-Image-Assisted-Volumetric-Super-Resolution-of-Cardiac-Diffusion-Weighted-Imaging" class="headerlink" title="High-Resolution Reference Image Assisted Volumetric Super-Resolution of Cardiac Diffusion Weighted Imaging"></a>High-Resolution Reference Image Assisted Volumetric Super-Resolution of Cardiac Diffusion Weighted Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20389">http://arxiv.org/abs/2310.20389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhe Wu, Jiahao Huang, Fanwen Wang, Pedro Ferreira, Andrew Scott, Sonia Nielles-Vallespin, Guang Yang<br>for: 这个研究旨在提高心脏微结构的非侵入式检测方法，以更好地理解健康心脏的宏观功能与疾病的微结构畸形之间的关系。methods: 这个研究使用了深度学习基于方法来提高图像质量，并采用了高分辨率参照图像作为输入。results: 研究表明，使用高分辨率参照图像可以提高超解像图像质量，并且模型可以在未看过的b值下进行超解像，证明了模型框架的通用性。<details>
<summary>Abstract</summary>
Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is the only in vivo method to non-invasively examine the microstructure of the human heart. Current research in DT-CMR aims to improve the understanding of how the cardiac microstructure relates to the macroscopic function of the healthy heart as well as how microstructural dysfunction contributes to disease. To get the final DT-CMR metrics, we need to acquire diffusion weighted images of at least 6 directions. However, due to DWI's low signal-to-noise ratio, the standard voxel size is quite big on the scale for microstructures. In this study, we explored the potential of deep-learning-based methods in improving the image quality volumetrically (x4 in all dimensions). This study proposed a novel framework to enable volumetric super-resolution, with an additional model input of high-resolution b0 DWI. We demonstrated that the additional input could offer higher super-resolved image quality. Going beyond, the model is also able to super-resolve DWIs of unseen b-values, proving the model framework's generalizability for cardiac DWI superresolution. In conclusion, we would then recommend giving the model a high-resolution reference image as an additional input to the low-resolution image for training and inference to guide all super-resolution frameworks for parametric imaging where a reference image is available.
</details>
<details>
<summary>摘要</summary>
Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) 是人体心脏内部结构的唯一非侵入式检测方法。当前研究的目标是提高健康心脏macroscopic功能与微结构之间的关系，以及诊断疾病中微结构功能的异常。为获得最终DT-CMR指标，需要收集至少6个方向的扩散束图像。然而，由于扩散束图像的信号噪声比较低，标准voxel大小很大，无法直接覆盖微结构的细节。本研究提出了基于深度学习的方法，以提高图像质量，并在所有维度上提高了图像的分辨率4倍。此外，模型还能够超Resolution DWI的不同扩散值，证明模型框架的普适性。因此，我们建议在训练和推理过程中给模型提供高分辨率参考图像作为附加输入，以便为所有参数影像推理框架提供引导。
</details></li>
</ul>
<hr>
<h2 id="A-Low-cost-Strategic-Monitoring-Approach-for-Scalable-and-Interpretable-Error-Detection-in-Deep-Neural-Networks"><a href="#A-Low-cost-Strategic-Monitoring-Approach-for-Scalable-and-Interpretable-Error-Detection-in-Deep-Neural-Networks" class="headerlink" title="A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks"></a>A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20349">http://arxiv.org/abs/2310.20349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Geissler, Syed Qutub, Michael Paulitsch, Karthik Pattabiraman</li>
<li>for: 这个论文是为了提出一种高度压缩的运行时监测方法，以检测深度计算机视觉网络中的数据损害。</li>
<li>methods: 该方法基于活动分布中异常的峰或堆叠的假设，并使用策略性地置置量标记来准确地估计当前的推理异常。</li>
<li>results: 该方法可以准确地检测深度计算机视觉网络中的数据损害，并且可以保持模型的解释性。对比之前的异常检测技术，该方法需要的计算负担非常低（只有0.3%）。<details>
<summary>Abstract</summary>
We present a highly compact run-time monitoring approach for deep computer vision networks that extracts selected knowledge from only a few (down to merely two) hidden layers, yet can efficiently detect silent data corruption originating from both hardware memory and input faults. Building on the insight that critical faults typically manifest as peak or bulk shifts in the activation distribution of the affected network layers, we use strategically placed quantile markers to make accurate estimates about the anomaly of the current inference as a whole. Importantly, the detector component itself is kept algorithmically transparent to render the categorization of regular and abnormal behavior interpretable to a human. Our technique achieves up to ~96% precision and ~98% recall of detection. Compared to state-of-the-art anomaly detection techniques, this approach requires minimal compute overhead (as little as 0.3% with respect to non-supervised inference time) and contributes to the explainability of the model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高度压缩的运行时监测方法，用于深度计算机视觉网络，从只有几个隐藏层中提取选择的知识，并快速检测硬件内存和输入错误引起的沉淀数据错误。我们基于critical faults通常表现为活动分布中峰值或批量偏移的观察，使用策略性地置置量标记来准确地估算当前推理的异常性。重要的是，检测器组件本身保持了算法透明性，以便对很好地解释模型的异常行为。我们的方法可以达到~96%的精度和~98%的回归检测率。相比之下，state-of-the-art anomaly detection技术，这种方法需要的计算开销非常低（只有0.3%与非监测时间相比），同时也增加了模型的解释性。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Learning-with-Pre-trained-Vision-Language-Models"><a href="#Class-Incremental-Learning-with-Pre-trained-Vision-Language-Models" class="headerlink" title="Class Incremental Learning with Pre-trained Vision-Language Models"></a>Class Incremental Learning with Pre-trained Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20348">http://arxiv.org/abs/2310.20348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xialei Liu, Xusheng Cao, Haori Lu, Jia-wen Xiao, Andrew D. Bagdanov, Ming-Ming Cheng</li>
<li>for: 本研究旨在利用大规模预训练模型进行 kontinual learning 场景下的适应和利用。</li>
<li>methods: 本文提出一种利用预训练视觉语言模型（如 CLIP）的方法，允许更多的适应而不仅仅采用零Instance learning 新任务。我们在预训练 CLIP 模型中添加了额外层，包括线性抽象层、自我注意层和提示调整层。我们还提出了一种参数保留方法，使得适应层中的参数可以更好地维持稳定和可变性。</li>
<li>results: 我们的实验表明， simplest solution – 一个单独的线性抽象层和参数保留方法 – 在多个传统的Benchmark上具有最好的结果，与当前状态的艺术高度相比，具有明显的优势。<details>
<summary>Abstract</summary>
With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.   In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conventional benchmarks consistently show a significant margin of improvement over the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
随着大规模预训练模型的出现，对于 continual learning 场景的适应和利用这些模型的兴趣增加了。在这篇论文中，我们提出了使用预训练视语模型（例如 CLIP）的方法，以便在新任务上进行适应而不仅仅是零扩展学习。我们在预训练 CLIP 模型的 Image Encoder 或 Text Encoder 上添加了额外层。我们 investigate 三种不同的策略：线性适配器、自注意适配器和提示调整，每个策略都在图像嵌入上进行操作。此外，我们还提出了一种参数保留方法，使用参数的重要性度量来更好地保持稳定性和抗塑性在增量学习中。我们的实验表明，最简单的解决方案——单个线性适配器层和参数保留——可以 producing the best results。我们在多个标准的 benchmark 上进行了多次实验，并 consistently 显示在当前状态的先进技术上达到了显著的margin of improvement。
</details></li>
</ul>
<hr>
<h2 id="Recaptured-Raw-Screen-Image-and-Video-Demoireing-via-Channel-and-Spatial-Modulations"><a href="#Recaptured-Raw-Screen-Image-and-Video-Demoireing-via-Channel-and-Spatial-Modulations" class="headerlink" title="Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations"></a>Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20332">http://arxiv.org/abs/2310.20332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tju-chengyijia/vd_raw">https://github.com/tju-chengyijia/vd_raw</a></li>
<li>paper_authors: Huanjing Yue, Yijia Cheng, Xin Liu, Jingyu Yang</li>
<li>for: 该文章目的是提出一种适用于原始输入的图像和视频去扭辑网络。</li>
<li>methods: 该网络使用了一种新的特征分支，通过通道和空间修正来融合传统特征混合分支。</li>
<li>results: 实验表明，该方法可以在图像和视频去扭辑方面达到状态之最的性能。 codes和数据集在<a target="_blank" rel="noopener" href="https://github.com/tju-chengyijia/VD_raw%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tju-chengyijia/VD_raw上发布。</a><details>
<summary>Abstract</summary>
Capturing screen contents by smartphone cameras has become a common way for information sharing. However, these images and videos are often degraded by moir\'e patterns, which are caused by frequency aliasing between the camera filter array and digital display grids. We observe that the moir\'e patterns in raw domain is simpler than those in sRGB domain, and the moir\'e patterns in raw color channels have different properties. Therefore, we propose an image and video demoir\'eing network tailored for raw inputs. We introduce a color-separated feature branch, and it is fused with the traditional feature-mixed branch via channel and spatial modulations. Specifically, the channel modulation utilizes modulated color-separated features to enhance the color-mixed features. The spatial modulation utilizes the feature with large receptive field to modulate the feature with small receptive field. In addition, we build the first well-aligned raw video demoir\'eing (RawVDemoir\'e) dataset and propose an efficient temporal alignment method by inserting alternating patterns. Experiments demonstrate that our method achieves state-of-the-art performance for both image and video demori\'eing. We have released the code and dataset in https://github.com/tju-chengyijia/VD_raw.
</details>
<details>
<summary>摘要</summary>
捕捉屏幕内容通过智能手机摄像头已成为常见的信息分享方式。然而，这些图像和视频经常受到谐波模式的抑制，这些谐波模式由摄像头筛子阵列和数字显示网格的频率对应引起。我们发现 raw 频谱中的谐波模式比 sRGB 频谱中的谐波模式更简单，raw 频谱中的谐波模式也有不同的特性。因此，我们提出了针对 raw 输入的图像和视频除谐波网络。我们在传统的特征混合分支上引入了色彩分离特征分支，并通过色彩分离特征的滤波和空间模ulation进行了混合。具体来说，色彩分离特征的滤波使用模ulated color-separated features来增强色彩混合特征。空间模ulation使用具有大覆盖面积的特征来模ulation具有小覆盖面积的特征。此外，我们还建立了首个一致的 raw 视频除谐波（RawVDemoir\'e）数据集，并提出了高效的时间对align方法，通过插入交换 patrern来实现。实验结果表明，我们的方法在图像和视频除谐波方面具有状态机器的性能。我们在 GitHub 上分享了代码和数据集，请参考 https://github.com/tju-chengyijia/VD_raw。
</details></li>
</ul>
<hr>
<h2 id="GACE-Geometry-Aware-Confidence-Enhancement-for-Black-Box-3D-Object-Detectors-on-LiDAR-Data"><a href="#GACE-Geometry-Aware-Confidence-Enhancement-for-Black-Box-3D-Object-Detectors-on-LiDAR-Data" class="headerlink" title="GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data"></a>GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20319">http://arxiv.org/abs/2310.20319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dschinagl/gace">https://github.com/dschinagl/gace</a></li>
<li>paper_authors: David Schinagl, Georg Krispel, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof</li>
<li>for: 提高3D物体探测器的置信度估计，以提高对真实存在的物体的识别率。</li>
<li>methods: 使用散射光探测器提取的几何信息，并将其与检测结果的空间关系进行聚合，以改善置信度估计。</li>
<li>results: 对多种州OF-the-art 3D物体探测器进行评测，显示GACE方法可以提高置信度估计的精度，尤其是对潜在受损用户类型（如行人和自行车）的识别率。<details>
<summary>Abstract</summary>
Widely-used LiDAR-based 3D object detectors often neglect fundamental geometric information readily available from the object proposals in their confidence estimation. This is mostly due to architectural design choices, which were often adopted from the 2D image domain, where geometric context is rarely available. In 3D, however, considering the object properties and its surroundings in a holistic way is important to distinguish between true and false positive detections, e.g. occluded pedestrians in a group. To address this, we present GACE, an intuitive and highly efficient method to improve the confidence estimation of a given black-box 3D object detector. We aggregate geometric cues of detections and their spatial relationships, which enables us to properly assess their plausibility and consequently, improve the confidence estimation. This leads to consistent performance gains over a variety of state-of-the-art detectors. Across all evaluated detectors, GACE proves to be especially beneficial for the vulnerable road user classes, i.e. pedestrians and cyclists.
</details>
<details>
<summary>摘要</summary>
广泛使用LiDAR基于3D对象探测器经常忽略可用的基本 геометрической信息，主要是因为架构设计决策，通常是从2D图像领域采用的，其中 geomertic context  rarely available。在3D中， however，考虑对象特性和其周围环境的整体方式是重要的，以分辨 true 和 false 阳性检测，例如受阻行人在群体中。为解决这个问题，我们提出了GACE，一种直观和高效的方法，用于改善给定黑盒3D对象探测器的信任度估计。我们将检测结果的几何特征和其空间关系聚合起来，以确定其可能性，并从而改善信任度估计。这会导致一致性地提高多种现状顶峰的检测器的性能。对于易受损路用户类型，例如行人和自行车手，GACE的效果特别出色。
</details></li>
</ul>
<hr>
<h2 id="HWD-A-Novel-Evaluation-Score-for-Styled-Handwritten-Text-Generation"><a href="#HWD-A-Novel-Evaluation-Score-for-Styled-Handwritten-Text-Generation" class="headerlink" title="HWD: A Novel Evaluation Score for Styled Handwritten Text Generation"></a>HWD: A Novel Evaluation Score for Styled Handwritten Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20316">http://arxiv.org/abs/2310.20316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/hwd">https://github.com/aimagelab/hwd</a></li>
<li>paper_authors: Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Rita Cucchiara</li>
<li>for: 这篇论文的目的是提出一个适合用于评估手写文本生成（Styled HTG）模型的评估指标，即手写距离（HWD）。</li>
<li>methods: 这篇论文使用了一个特定的 neural network 来提取手写风格特征，并使用了一种感知距离来比较手写中的微妙 геометри�features。</li>
<li>results: 经过广泛的实验评估， authors 展示了 HWD 的适用性以评估 Styled HTG 模型，并且显示了 HWD 可以作为一个有用的评估指标，以促进这个重要的研究领域的发展。<details>
<summary>Abstract</summary>
Styled Handwritten Text Generation (Styled HTG) is an important task in document analysis, aiming to generate text images with the handwriting of given reference images. In recent years, there has been significant progress in the development of deep learning models for tackling this task. Being able to measure the performance of HTG models via a meaningful and representative criterion is key for fostering the development of this research topic. However, despite the current adoption of scores for natural image generation evaluation, assessing the quality of generated handwriting remains challenging. In light of this, we devise the Handwriting Distance (HWD), tailored for HTG evaluation. In particular, it works in the feature space of a network specifically trained to extract handwriting style features from the variable-lenght input images and exploits a perceptual distance to compare the subtle geometric features of handwriting. Through extensive experimental evaluation on different word-level and line-level datasets of handwritten text images, we demonstrate the suitability of the proposed HWD as a score for Styled HTG. The pretrained model used as backbone will be released to ease the adoption of the score, aiming to provide a valuable tool for evaluating HTG models and thus contributing to advancing this important research area.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩文本简化（Simplified Chinese）版本：📝 Styled Handwritten Text Generation（样式化手写文本生成）是文档分析领域的一个重要任务，旨在生成基于参考图像的手写文本图像。随着深度学习模型在这个领域的发展，评估HTG模型的性能变得非常重要。然而，由于手写文本生成的评估标准尚未稳定，评估生成的手写文本质量仍然存在挑战。为此，我们提出了特有的手写距离（HWD），适用于HTG评估。具体来说，它在特定的手写样式特征提取网络中的特征空间中工作，利用感知距离来比较手写文本中细微的几何特征。经过广泛的实验评估不同的单词级和行级手写文本图像集，我们证明了我们提出的HWD是一个适用于STYLED HTG的分数。我们将预训练的模型作为后向提供，以便推广HWD分数，以便为HTG模型的评估提供有价值的工具，从而为这一重要研究领域的发展做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Bilateral-Network-with-Residual-U-blocks-and-Dual-Guided-Attention-for-Real-time-Semantic-Segmentation"><a href="#Bilateral-Network-with-Residual-U-blocks-and-Dual-Guided-Attention-for-Real-time-Semantic-Segmentation" class="headerlink" title="Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation"></a>Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20305">http://arxiv.org/abs/2310.20305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/likelidoa/bidganet">https://github.com/likelidoa/bidganet</a></li>
<li>paper_authors: Liang Liao, Liang Wan, Mingsheng Liu, Shusheng Li</li>
<li>for: 这种研究旨在提高两分支架构中的 Semantic Segmentation 性能，特别是在实时性能方面。</li>
<li>methods: 该研究使用了一种新的注意力计算导向的融合机制，以取代一些多级宽度的变换。此外，使用了 Residual U-blocks (RSU) 构建一个分支网络，以获得更好的多级特征。</li>
<li>results: 对 Cityscapes 和 CamVid  dataset 进行了广泛的实验，显示了该方法的有效性。<details>
<summary>Abstract</summary>
When some application scenarios need to use semantic segmentation technology, like automatic driving, the primary concern comes to real-time performance rather than extremely high segmentation accuracy. To achieve a good trade-off between speed and accuracy, two-branch architecture has been proposed in recent years. It treats spatial information and semantics information separately which allows the model to be composed of two networks both not heavy. However, the process of fusing features with two different scales becomes a performance bottleneck for many nowaday two-branch models. In this research, we design a new fusion mechanism for two-branch architecture which is guided by attention computation. To be precise, we use the Dual-Guided Attention (DGA) module we proposed to replace some multi-scale transformations with the calculation of attention which means we only use several attention layers of near linear complexity to achieve performance comparable to frequently-used multi-layer fusion. To ensure that our module can be effective, we use Residual U-blocks (RSU) to build one of the two branches in our networks which aims to obtain better multi-scale features. Extensive experiments on Cityscapes and CamVid dataset show the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:在某些应用场景中，如自动驾驶，需要使用 semantic segmentation 技术时， primary concern 是实时性而不是极高的 segmentation accuracy。为了实现好的 speed-accuracy 质量平衡，recent years 中提出了 two-branch 架构。它将空间信息和 semantics 信息分别处理，使模型可以 composed of two networks  Both not heavy。然而，将多种缩放级别的特征进行融合成为性能瓶颈 для many  current two-branch 模型。在这些研究中，我们设计了一种新的融合机制 для two-branch 架构，即 guided by attention 计算。具体来说，我们使用我们提出的 Dual-Guided Attention (DGA) 模块来取代一些多缩放级别的转换。这意味着我们只需使用一些 near linear complexity 的 attention layers 来实现与常用多层融合的性能相当。为确保我们的模块可行，我们使用 Residual U-blocks (RSU)  construct one of the two branches 在我们的网络中，以获得更好的 multi-scale features。广泛的实验表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Annotator-A-Generic-Active-Learning-Baseline-for-LiDAR-Semantic-Segmentation"><a href="#Annotator-A-Generic-Active-Learning-Baseline-for-LiDAR-Semantic-Segmentation" class="headerlink" title="Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation"></a>Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20293">http://arxiv.org/abs/2310.20293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binhui Xie, Shuang Li, Qingju Guo, Chi Harold Liu, Xinjing Cheng</li>
<li>for: 这种论文是为了提出一种高效的活动学习方法，以便在LiDAR semantic segmentation中提高模型的性能和训练效率。</li>
<li>methods: 这种方法使用了一种基于voxel的在线选择策略，以便高效地询问和标注LiDAR扫描数据中的关键和典型的小体积。具体来说，这种方法首先对多种常见的选择策略进行了深入的分析，例如随机、信息熵、边缘margin等，然后开发了一种基于点云的小体积冲击度（VCD）来利用点云的本地拓扑关系和结构。</li>
<li>results: 这种方法在多种场景下表现出色，特别是在活动学习（AL）、活动源自由领域适应（ASFDA）和活动领域适应（ADA）等场景下。它在LiDAR semantic segmentation的多个benchmark上具有优秀的性能，包括从 simulate-to-real和real-to-real两种enario中。具体来说，Annotator在SynLiDAR-to-SemanticKITTI任务中只需要标注每个扫描数据中的5个小体积，从而实现了87.8%的完全监督性能。<details>
<summary>Abstract</summary>
Active learning, a label-efficient paradigm, empowers models to interactively query an oracle for labeling new data. In the realm of LiDAR semantic segmentation, the challenges stem from the sheer volume of point clouds, rendering annotation labor-intensive and cost-prohibitive. This paper presents Annotator, a general and efficient active learning baseline, in which a voxel-centric online selection strategy is tailored to efficiently probe and annotate the salient and exemplar voxel girds within each LiDAR scan, even under distribution shift. Concretely, we first execute an in-depth analysis of several common selection strategies such as Random, Entropy, Margin, and then develop voxel confusion degree (VCD) to exploit the local topology relations and structures of point clouds. Annotator excels in diverse settings, with a particular focus on active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). It consistently delivers exceptional performance across LiDAR semantic segmentation benchmarks, spanning both simulation-to-real and real-to-real scenarios. Surprisingly, Annotator exhibits remarkable efficiency, requiring significantly fewer annotations, e.g., just labeling five voxels per scan in the SynLiDAR-to-SemanticKITTI task. This results in impressive performance, achieving 87.8% fully-supervised performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that Annotator will offer a simple, general, and efficient solution for label-efficient 3D applications. Project page: https://binhuixie.github.io/annotator-web
</details>
<details>
<summary>摘要</summary>
aktive lärning, ein label-effizientes Paradigma, ermöglicht Modellen, interaktiv einen Oracle für das Labeling neuer Daten zu fragen. In der Welt von LiDAR-semantischen Segmentierung gibt es Herausforderungen durch den enormen Umfang von Punktwolken, die die Annotation aufwändig und teuer machen. Diese Arbeit präsentiert Annotator, eine allgemeine und effiziente aktive Lärm-Baseline, bei der eine Vektor-zentrische Online-Auswahlstrategie entwickelt wurde, um effizient die salienten und exemplarischen Vektor-Gitter within each LiDAR-Scans zu durchsuchen, auch unter Veränderung der Verteilung. Konkret ausgeführt wir eine tiefe Analyse von verschiedenen Auswahlstrategien wie zufällig, Entropie, Margin und entwickeln das Vektor-Verwirrungsgrad (VCD), um die lokalen Topologie-Beziehungen und -Strukturen der Punktwolken zu nutzen. Annotator zeichnet sich in verschiedenen Setting aus, insbesondere in aktiver Lärm (AL), aktiver Quell-freier Domain-Adaptation (ASFDA) und aktiver Domain-Adaptation (ADA) aus. Es erreicht hervorragende Leistungen bei LiDAR-semantischen Segmentierung-Benchmarks, die beide simulation-to-real und real-to-real-Szenarien umfassen. Überraschenderweise erfordert Annotator sehr wenige Annotationen, z.B. nur fünf Vektoren pro Scan im SynLiDAR-to-SemanticKITTI-Task. Dies führt zu einer beeindruckenden Leistung mit 87,8% fully-supervised Performance unter AL, 88,5% unter ASFDA und 94,4% unter ADA. Wir glauben, dass Annotator eine einfache, allgemeine und effiziente Lösung für label-effiziente 3D-Anwendungen bietet. Projekthinweis: <https://binhuixie.github.io/annotator-web>
</details></li>
</ul>
<hr>
<h2 id="IARS-SegNet-Interpretable-Attention-Residual-Skip-connection-SegNet-for-melanoma-segmentation"><a href="#IARS-SegNet-Interpretable-Attention-Residual-Skip-connection-SegNet-for-melanoma-segmentation" class="headerlink" title="IARS SegNet: Interpretable Attention Residual Skip connection SegNet for melanoma segmentation"></a>IARS SegNet: Interpretable Attention Residual Skip connection SegNet for melanoma segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20292">http://arxiv.org/abs/2310.20292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shankara Narayanan V, Sikha OK, Raul Benitez</li>
<li>for: 针对皮肤病变分割 Task，提出了一种基于SegNet基础模型的高级分割框架IARS SegNet。</li>
<li>methods: 该方法具有三个关键组成部分：跳过连接、径远核心和全局注意力机制。这三个组成部分都是用于强调临床有用的区域，特别是皮肤病变的边沿。</li>
<li>results: 该方法可以准确地分割皮肤病变，并提高了模型的可解释性。<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a crucial role in the computer-aided diagnosis of melanoma. Deep Learning models have shown promise in accurately segmenting skin lesions, but their widespread adoption in real-life clinical settings is hindered by their inherent black-box nature. In domains as critical as healthcare, interpretability is not merely a feature but a fundamental requirement for model adoption. This paper proposes IARS SegNet an advanced segmentation framework built upon the SegNet baseline model. Our approach incorporates three critical components: Skip connections, residual convolutions, and a global attention mechanism onto the baseline Segnet architecture. These elements play a pivotal role in accentuating the significance of clinically relevant regions, particularly the contours of skin lesions. The inclusion of skip connections enhances the model's capacity to learn intricate contour details, while the use of residual convolutions allows for the construction of a deeper model while preserving essential image features. The global attention mechanism further contributes by extracting refined feature maps from each convolutional and deconvolutional block, thereby elevating the model's interpretability. This enhancement highlights critical regions, fosters better understanding, and leads to more accurate skin lesion segmentation for melanoma diagnosis.
</details>
<details>
<summary>摘要</summary>
皮肤 lesion 分割在计算机支持的诊断 melanoma 中扮演着关键性的角色。深度学习模型在精准地分割皮肤 lesion 方面表现出了承诺，但是它们在实际的医疗设置中广泛应用的困难由其内在的黑盒特性困扰。在如此重要的医疗领域，可读性不仅是一个特性，而是基本的需求，以便模型的采纳。这篇文章提出了IARS SegNet，一个基于 SegNet 基础模型的高级分割框架。我们的方法包括三个关键组件：跳过连接、径远核心和全局注意力机制。这些元素在突出临床相关区域的重要性方面发挥关键作用，特别是皮肤 lesion 的边沿。跳过连接使模型学习细节的细腻 Details，而径远核心允许建立更深的模型，保留essential image features。全局注意力机制进一步提供了每个径远核心块中的精细特征图，从而提高模型的可读性。这种提高突出了关键区域，促进了更好的理解，并导致更精准的皮肤 lesion 分割 для melanoma 诊断。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-refinement-of-in-situ-images-acquired-by-low-electron-dose-LC-TEM"><a href="#Machine-learning-refinement-of-in-situ-images-acquired-by-low-electron-dose-LC-TEM" class="headerlink" title="Machine learning refinement of in situ images acquired by low electron dose LC-TEM"></a>Machine learning refinement of in situ images acquired by low electron dose LC-TEM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20279">http://arxiv.org/abs/2310.20279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroyasu Katsuno, Yuki Kimura, Tomoya Yamazaki, Ichigaku Takigawa</li>
<li>for: 这个论文是为了提高在半导体电镜显微镜下进行现场观察时所获得的图像质量的。</li>
<li>methods: 这个论文使用了一种基于U-Net架构和ResNetEncoder的机器学习技术来改善图像。</li>
<li>results: 该模型可以将噪图转换为清晰图像，并且转换时间只需10ms左右。此外，通过该模型，在Gatan DigitalMicrograph软件中不可见的 nanoparticle 也可以在后续的改进图像中被检测到。<details>
<summary>Abstract</summary>
We study a machine learning (ML) technique for refining images acquired during in situ observation using liquid-cell transmission electron microscopy (LC-TEM). Our model is constructed using a U-Net architecture and a ResNet encoder. For training our ML model, we prepared an original image dataset that contained pairs of images of samples acquired with and without a solution present. The former images were used as noisy images and the latter images were used as corresponding ground truth images. The number of pairs of image sets was $1,204$ and the image sets included images acquired at several different magnifications and electron doses. The trained model converted a noisy image into a clear image. The time necessary for the conversion was on the order of 10ms, and we applied the model to in situ observations using the software Gatan DigitalMicrograph (DM). Even if a nanoparticle was not visible in a view window in the DM software because of the low electron dose, it was visible in a successive refined image generated by our ML model.
</details>
<details>
<summary>摘要</summary>
我们研究了一种机器学习（ML）技术，用于从liquid-cell transmission electron microscopy（LC-TEM）中获得更加清晰的图像。我们的模型采用了U-Net架构和ResNetEncoder。为了训练我们的ML模型，我们准备了一个原始图像集，其中包括了样品在存在和不存在解药物时所取得的图像对。前者用作噪音图像，后者用作对应的真实图像。这个图像集共有1,204对图像组，其中包括了不同的放大和电子剂量。训练模型可以将噪音图像转化为清晰图像，转化时间在10ms之间。我们使用了Gatan DigitalMicrograph（DM）软件来应用这个模型于实际观察中。即使在DM软件中的视窗中不可见一个奈米粒子因为电子剂量过低，我们的ML模型可以成功地将其视为可见。
</details></li>
</ul>
<hr>
<h2 id="From-Denoising-Training-to-Test-Time-Adaptation-Enhancing-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#From-Denoising-Training-to-Test-Time-Adaptation-Enhancing-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation"></a>From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20271">http://arxiv.org/abs/2310.20271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenruxue/detta">https://github.com/wenruxue/detta</a></li>
<li>paper_authors: Ruxue Wen, Hangjie Yuan, Dong Ni, Wenbo Xiao, Yaoyao Wu</li>
<li>for: 本研究旨在解决医学图像分割领域中的频繁领域转换问题，即因数据获取设备和其他因素而导致的频繁领域转换。</li>
<li>methods: 该研究提出了一种基于自我超vision学习理念的方法，称为Denosing Y-Net（DeY-Net），它在基本的U-Net架构中添加了一个辅助的干扰去除解码器。该解码器的目的是在培训中进行干扰去除，以增强领域通用性。此外，这种方法还可以利用无标签数据。</li>
<li>results: 对于广泛采用的肝脏分割 benchmark 进行了广泛的实验，显示 DeY-Net 在频繁领域转换问题中具有显著的领域通用性改进，与基线和现有方法相比，得到了更好的结果。<details>
<summary>Abstract</summary>
In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA.
</details>
<details>
<summary>摘要</summary>
医学图像分割中，领域总是一个大的挑战，因为数据获取设备和其他因素引起的领域shift。这些shift在最常见的场景中尤其突出，因为隐私问题限制了数据采集。为解决这个问题，我们 Draw inspiration from self-supervised learning paradigm，which effectively discourages overfitting to the source domain。我们提议使用auxiliary denoising decoder，并将其添加到基本的U-Net架构中。这个auxiliary decoder的目的是在培训过程中进行减除训练，以提高领域总是适应性。此外，这个 paradigm还允许使用无标注数据。在基于减除训练的基础上，我们提议denoising test time adaptation（DeTTA），它可以：（i）在目标领域中对模型进行采样性的适应，以及（ii）适应受到噪声损害的输入。我们对广泛采用的肝脏分割 benchmark进行了广泛的实验，并证明了我们的基eline和当前最佳方法相比，具有显著的领域总是适应性提升。代码可以在https://github.com/WenRuxue/DeTTA中获取。
</details></li>
</ul>
<hr>
<h2 id="Low-Dose-CT-Image-Enhancement-Using-Deep-Learning"><a href="#Low-Dose-CT-Image-Enhancement-Using-Deep-Learning" class="headerlink" title="Low-Dose CT Image Enhancement Using Deep Learning"></a>Low-Dose CT Image Enhancement Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20265">http://arxiv.org/abs/2310.20265</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Demir, M. M. A. Shames, O. N. Gerek, S. Ergin, M. Fidan, M. Koc, M. B. Gulmezoglu, A. Barkana, C. Calisir</li>
<li>for: 降低ionizing radiation对 CT图像的影像质量影响</li>
<li>methods: 使用U-NET deep learning方法进行图像提升</li>
<li>results: 比对低剂量CT图像和普通CT图像，U-NET图像提升方法可以提供视觉上的改善和诊断上的优化<details>
<summary>Abstract</summary>
The application of ionizing radiation for diagnostic imaging is common around the globe. However, the process of imaging, itself, remains to be a relatively hazardous operation. Therefore, it is preferable to use as low a dose of ionizing radiation as possible, particularly in computed tomography (CT) imaging systems, where multiple x-ray operations are performed for the reconstruction of slices of body tissues. A popular method for radiation dose reduction in CT imaging is known as the quarter-dose technique, which reduces the x-ray dose but can cause a loss of image sharpness. Since CT image reconstruction from directional x-rays is a nonlinear process, it is analytically difficult to correct the effect of dose reduction on image quality. Recent and popular deep-learning approaches provide an intriguing possibility of image enhancement for low-dose artifacts. Some recent works propose combinations of multiple deep-learning and classical methods for this purpose, which over-complicate the process. However, it is observed here that the straight utilization of the well-known U-NET provides very successful results for the correction of low-dose artifacts. Blind tests with actual radiologists reveal that the U-NET enhanced quarter-dose CT images not only provide an immense visual improvement over the low-dose versions, but also become diagnostically preferable images, even when compared to their full-dose CT versions.
</details>
<details>
<summary>摘要</summary>
globally，使用离子化 radiation for diagnostic imaging 是非常普遍的。然而， imaging 本身仍然是一个相对危险的过程。因此，使用最低化的离子化 radiation  dosage 是可以的，特别是在 computed tomography (CT) 图像系统中，其中多个 x-ray 操作用于体 тissue 的重建。一种受欢迎的方法是known as the quarter-dose technique，它可以降低 x-ray 剂量，但可能会导致图像锐度下降。由于 CT 图像重建是非线性的过程，因此是analytically 难以修正剂量减少对图像质量的影响。 recient 和 popular deep-learning 方法提供了一种可能的图像提高方法。some recent works propose combinations of multiple deep-learning and classical methods for this purpose, which over-complicate the process。然而，在这里观察到的是，直接使用 well-known U-NET 提供了非常成功的结果 для低剂量缺陷的修正。 blind tests with actual radiologists reveal that the U-NET 进行了修正的quarter-dose CT 图像不仅提供了巨大的视觉改善，而且成为了诊断更有优势的图像，即使与其全剂量 CT 版本相比。
</details></li>
</ul>
<hr>
<h2 id="Pose-to-Motion-Cross-Domain-Motion-Retargeting-with-Pose-Prior"><a href="#Pose-to-Motion-Cross-Domain-Motion-Retargeting-with-Pose-Prior" class="headerlink" title="Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior"></a>Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20249">http://arxiv.org/abs/2310.20249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingqing Zhao, Peizhuo Li, Wang Yifan, Olga Sorkine-Hornung, Gordon Wetzstein</li>
<li>for: 这篇论文的目的是为了开发一种基于神经网络的人物动作合成方法，使得可以从不同的人物 pose 数据中生成更加自然和真实的动作。</li>
<li>methods: 该方法利用 pose 数据作为代替的数据源，通过引入适应技术来将原始的人物动作数据转移到目标人物上，以生成更加自然和真实的动作。</li>
<li>results: 实验结果表明，该方法可以充分地 combinig 来自源人物的动作特征和目标人物的 pose 特征，并在小或噪音的 pose 数据集上表现稳定，而且在用户测试中，大多数参与者认为这些转移后的动作更加愉悦、更加真实、更加没有噪音。<details>
<summary>Abstract</summary>
Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created poses to noisy poses estimated directly from images. Additionally, a conducted user study indicated that a majority of participants found our retargeted motion to be more enjoyable to watch, more lifelike in appearance, and exhibiting fewer artifacts. Project page: https://cyanzhao42.github.io/pose2motion
</details>
<details>
<summary>摘要</summary>
创建可信任的动作 для不同的角色已经是计算机图形的长期目标。现有的学习基于动作合成方法通常需要大量的动作数据，而这些数据往往很难以获得。相比之下，姿势数据更加 accessible，因为静止的姿势角色更容易创建，甚至可以从图像中提取使用最新的计算机视觉技术。在这篇论文中，我们利用这种备用数据源，并引入一种神经动作合成方法通过重定向。我们的方法可以将已有的动作数据中的动作特征转移到另一个角色的姿势数据中，即使这两个角色有极其不同的骨架。我们的实验表明，我们的方法可以有效地将源角色的动作特征与目标角色的姿势特征结合在一起，并在小或噪音的姿势数据集上表现稳定。此外，我们进行了一项用户研究，发现大多数参与者认为我们的重定向动作更加有趣看，更加生动一切，并且具有 fewer artifacts。项目页面：https://cyanzhao42.github.io/pose2motion
</details></li>
</ul>
<hr>
<h2 id="Contrast-agent-induced-deterministic-component-of-CT-density-in-the-abdominal-aorta-during-routine-angiography-proof-of-concept-study"><a href="#Contrast-agent-induced-deterministic-component-of-CT-density-in-the-abdominal-aorta-during-routine-angiography-proof-of-concept-study" class="headerlink" title="Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study"></a>Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20243">http://arxiv.org/abs/2310.20243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria R. Kodenko, Yuriy A. Vasilev, Nicholas S. Kulberg, Andrey V. Samorodov, Anton V. Vladzimirskyy, Olga V. Omelyanskaya, Roman V. Reshetnikov</li>
<li>for: 这个研究旨在开发一种基于CTA数据的血液动力学模型，以便提高CT图像处理工具和人工智能训练数据的生成。</li>
<li>methods: 该研究采用了Beer-Lambert法则和血液动力学的假设，提出了一个 deterministic CA-induced 成分在CT信号密度中的模型，该模型具有6个相关血液动力学性质的系数。用非线性最小二乘法与Levenberg-Marquardt优化算法进行数据适应。</li>
<li>results: 研究分析了594个CTA图像（4个研究，每个研究144扫描slice，IQR&#x3D;[134; 158.5]，1:1正常:疾病平衡），并证明了模型的合理性（Wilcox测试p-value&gt;0.05）。模型能正确地模拟正常血液流和地方疾病所引起的血液动力学异常。<details>
<summary>Abstract</summary>
Background and objective: CTA is a gold standard of preoperative diagnosis of abdominal aorta and typically used for geometric-only characteristic extraction. We assume that a model describing the dynamic behavior of the contrast agent in the vessel can be developed from the data of routine CTA studies, allowing the procedure to be investigated and optimized without the need for additional perfusion CT studies. Obtained spatial distribution of CA can be valuable for both increasing the diagnostic value of a particular study and improving the CT data processing tools. Methods: In accordance with the Beer-Lambert law and the absence of chemical interaction between blood and CA, we postulated the existence of a deterministic CA-induced component in the CT signal density. The proposed model, having a double-sigmoid structure, contains six coefficients relevant to the properties of hemodynamics. To validate the model, expert segmentation was performed using the 3D Slicer application for the CTA data obtained from publicly available source. The model was fitted to the data using the non-linear least square method with Levenberg-Marquardt optimization. Results: We analyzed 594 CTA images (4 studies with median size of 144 slices, IQR [134; 158.5]; 1:1 normal:pathology balance). Goodness-of-fit was proved by Wilcox test (p-value > 0.05 for all cases). The proposed model correctly simulated normal blood flow and hemodynamics disturbances caused by local abnormalities (aneurysm, thrombus and arterial branching). Conclusions: Proposed approach can be useful for personalized CA modeling of vessels, improvement of CTA image processing and preparation of synthetic CT training data for artificial intelligence.
</details>
<details>
<summary>摘要</summary>
背景和目标：CTA是腹部大动脉的金标准预操作诊断方法，通常用于只有几何特征提取。我们假设可以从日常CTA研究数据中发展出描述干扰物在动脉中的动态行为模型，以便不需要额外的血液 perfusion CT 研究。获取的干扰物空间分布可以为特定研究提高诊断价值，并且改进 CT 数据处理工具。方法：根据贝尔-拉姆伯特定律和血液与干扰物之间的化学反应 Absence，我们提出了一个具有双折衣结构的模型，该模型包含6个有关血液动力学性质的参数。为验证模型，我们使用3D Slicer应用程序对公开 obtain CT 数据进行专家分 segmentation。模型使用非线性最小二乘法与 Levenberg-Marquardt 优化算法进行适应。结果：我们分析了594个 CT 图像（4个研究，每个研究中 median 144 张 slice，IQR [134; 158.5] ; 1:1正常：疾病平衡）。通过威尔科克斯测试（p-value > 0.05 所有情况），我们证明了模型的良好适应。提案的模型可 corrrectly 模拟正常血液流动和地方异常（血栓、aneurysm和动脉分支）引起的各种各样的血液动力学异常。结论：我们的方法可以用于个性化干扰物模型、CT 图像处理改进和人工智能 Synthetic CT 训练数据的准备。
</details></li>
</ul>
<hr>
<h2 id="HEDNet-A-Hierarchical-Encoder-Decoder-Network-for-3D-Object-Detection-in-Point-Clouds"><a href="#HEDNet-A-Hierarchical-Encoder-Decoder-Network-for-3D-Object-Detection-in-Point-Clouds" class="headerlink" title="HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds"></a>HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20234">http://arxiv.org/abs/2310.20234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanggang001/hednet">https://github.com/zhanggang001/hednet</a></li>
<li>paper_authors: Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Xiaolin Hu</li>
<li>for: 3D object detection in point clouds for autonomous driving systems</li>
<li>methods: 使用encoder-decoder块捕捉长距离依赖关系的特征，以提高大小远距离对象的检测精度</li>
<li>results: 在Waymo Open和nuScenes数据集上实现了以前的最佳方法的superior检测精度，与竞争对手效果相当Here’s the translation in English:</li>
<li>for: 3D object detection in point clouds for autonomous driving systems</li>
<li>methods: Using encoder-decoder blocks to capture long-range dependencies of features, particularly for large and distant objects, to improve detection accuracy</li>
<li>results: Achieved superior detection accuracy on both the Waymo Open and nuScenes datasets, comparable to competitive efficiency<details>
<summary>Abstract</summary>
3D object detection in point clouds is important for autonomous driving systems. A primary challenge in 3D object detection stems from the sparse distribution of points within the 3D scene. Existing high-performance methods typically employ 3D sparse convolutional neural networks with small kernels to extract features. To reduce computational costs, these methods resort to submanifold sparse convolutions, which prevent the information exchange among spatially disconnected features. Some recent approaches have attempted to address this problem by introducing large-kernel convolutions or self-attention mechanisms, but they either achieve limited accuracy improvements or incur excessive computational costs. We propose HEDNet, a hierarchical encoder-decoder network for 3D object detection, which leverages encoder-decoder blocks to capture long-range dependencies among features in the spatial space, particularly for large and distant objects. We conducted extensive experiments on the Waymo Open and nuScenes datasets. HEDNet achieved superior detection accuracy on both datasets than previous state-of-the-art methods with competitive efficiency. The code is available at https://github.com/zhanggang001/HEDNet.
</details>
<details>
<summary>摘要</summary>
三维物体检测在点云中是自动驾驶系统中重要的一环。主要挑战在三维场景中点的稀疏分布上。现有高性能方法通常采用三维稀疏卷积神经网络，使用小kernel来提取特征。以减少计算成本，这些方法通常采用子抽象稀疏卷积，这会阻碍特征之间的信息交换。一些最近的方法尝试解决这个问题，通过引入大kernel卷积或自注意机制，但它们可能只能实现有限的准确性改进或者承受过高的计算成本。我们提议了HEDNet，一种嵌入式编码器-解码器网络，用于三维物体检测。HEDNet利用编码器-解码器块来捕捉点云中特征之间的长距离依赖关系，特别是大小远距离的物体。我们在 Waymo Open 和 nuScenes 数据集上进行了广泛的实验，HEDNet在两个数据集上达到了前一个state-of-the-art方法的更高的检测精度，同时具有竞争力的效率。代码可以在 GitHub 上找到：https://github.com/zhanggang001/HEDNet。
</details></li>
</ul>
<hr>
<h2 id="UWFormer-Underwater-Image-Enhancement-via-a-Semi-Supervised-Multi-Scale-Transformer"><a href="#UWFormer-Underwater-Image-Enhancement-via-a-Semi-Supervised-Multi-Scale-Transformer" class="headerlink" title="UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer"></a>UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20210">http://arxiv.org/abs/2310.20210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhang Chen, Zinuo Li, Shenghong Luo, Weiwen Chen, Shuqiang Wang, Chi-Man Pun</li>
<li>for: 提高水下图像质量，改善颜色均衡和对比度</li>
<li>methods: 使用 Multi-scale Transformer-based Network 和 Nonlinear Frequency-aware Attention mechanism，以及 Multi-Scale Fusion Feed-forward Network 进行多频级别增强</li>
<li>results: 与现有方法相比，该方法在水下图像增强方面达到了更高的质量和视觉效果<details>
<summary>Abstract</summary>
Underwater images often exhibit poor quality, imbalanced coloration, and low contrast due to the complex and intricate interaction of light, water, and objects. Despite the significant contributions of previous underwater enhancement techniques, there exist several problems that demand further improvement: (i) Current deep learning methodologies depend on Convolutional Neural Networks (CNNs) that lack multi-scale enhancement and also have limited global perception fields. (ii) The scarcity of paired real-world underwater datasets poses a considerable challenge, and the utilization of synthetic image pairs risks overfitting. To address the aforementioned issues, this paper presents a Multi-scale Transformer-based Network called UWFormer for enhancing images at multiple frequencies via semi-supervised learning, in which we propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale Fusion Feed-forward Network for low-frequency enhancement. Additionally, we introduce a specialized underwater semi-supervised training strategy, proposing a Subaqueous Perceptual Loss function to generate reliable pseudo labels. Experiments using full-reference and non-reference underwater benchmarks demonstrate that our method outperforms state-of-the-art methods in terms of both quantity and visual quality.
</details>
<details>
<summary>摘要</summary>
水下图像经常呈现低质量、不均匀颜色和低对比度，这是由光线、水和物体之间复杂且细致的交互所致。虽然过去的水下增强技术有着显著贡献，但还存在一些需要进一步改进的问题：（i）当前的深度学习方法ologies rely heavily on卷积神经网络（CNNs），它们缺乏多尺度增强和全球视场的感知。（ii）水下实际数据缺乏对照数据，使用 sintethic image pairs 难以避免过拟合。为了解决以上问题，本文提出了一种具有多尺度增强和 semi-supervised learning 的 Multi-scale Transformer-based Network，称之为 UWFormer。在该网络中，我们提出了一种非线性频率意识机制和多尺度融合 feed-forward 网络，用于低频增强。此外，我们还提出了一种特殊的水下半supervised 训练策略，包括一种Subaqueous Perceptual Loss函数，以生成可靠的 pseudo labels。实验表明，我们的方法在全referenced和非referenced水下标准准chmark上都超过了状态之前的方法。
</details></li>
</ul>
<hr>
<h2 id="ZoomNeXt-A-Unified-Collaborative-Pyramid-Network-for-Camouflaged-Object-Detection"><a href="#ZoomNeXt-A-Unified-Collaborative-Pyramid-Network-for-Camouflaged-Object-Detection" class="headerlink" title="ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection"></a>ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20208">http://arxiv.org/abs/2310.20208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lartpang/ZoomNeXt">https://github.com/lartpang/ZoomNeXt</a></li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, Huchuan Lu<br>for:The paper is written for object detection in real-world scenarios, specifically in camouflaged object detection (COD) where objects are visually blended into their surroundings.methods:The paper proposes an effective unified collaborative pyramid network that mimics human behavior when observing vague images and videos, using a zooming strategy to learn discriminative mixed-scale semantics and a routing mechanism to adaptively ignore static representations.results:The proposed approach consistently outperforms existing state-of-the-art methods in image and video COD benchmarks, providing a highly task-friendly framework for real-world COD applications.<details>
<summary>Abstract</summary>
Recent camouflaged object detection (COD) attempts to segment objects visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from the high intrinsic similarity between camouflaged objects and their background, objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To this end, we propose an effective unified collaborative pyramid network which mimics human behavior when observing vague images and videos, \textit{i.e.}, zooming in and out. Specifically, our approach employs the zooming strategy to learn discriminative mixed-scale semantics by the multi-head scale integration and rich granularity perception units, which are designed to fully explore imperceptible clues between candidate objects and background surroundings. The former's intrinsic multi-head aggregation provides more diverse visual patterns. The latter's routing mechanism can effectively propagate inter-frame difference in spatiotemporal scenarios and adaptively ignore static representations. They provides a solid foundation for realizing a unified architecture for static and dynamic COD. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization, uncertainty awareness loss, to encourage predictions with higher confidence in candidate regions. Our highly task-friendly framework consistently outperforms existing state-of-the-art methods in image and video COD benchmarks. The code will be available at \url{https://github.com/lartpang/ZoomNeXt}.
</details>
<details>
<summary>摘要</summary>
最近的隐形对象检测（COD）尝试将对象视觉上与背景融合在一起，这是现实世界中非常复杂和困难的任务。除了高度的内在相似性外，对象通常具有多种比例、模糊的外观和严重的遮挡。为此，我们提议一种高效的统一协同层次网络，该网络模仿人类在欠准图像和视频中观察的行为，即在图像和视频中“缩进”和“缩出”。具体来说，我们的方法使用缩进策略来学习混合比例 semantics，通过多头聚合和丰富的粒度感知单元来全面探索对象和背景之间的微不KB细节。前者的内在多头聚合提供更多的视觉模式。后者的路由机制可以有效地在空间时间场景中传递差异，适应性地忽略静止表示。它们为实现统一的静态和动态COD提供了坚实的基础。此外，考虑到来自不可区分的纹理的不确定性和模糊性，我们构建了一种简单 yet有效的REG regularization，uncertainty awareness loss，以促进候选区域的预测具有更高的信任度。我们的高度任务友好的框架在图像和视频COD benchmark中 persistently击败现有的状态革命方法。代码将在 \url{https://github.com/lartpang/ZoomNeXt} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Visible-to-Thermal-image-Translation-for-improving-visual-task-in-low-light-conditions"><a href="#Visible-to-Thermal-image-Translation-for-improving-visual-task-in-low-light-conditions" class="headerlink" title="Visible to Thermal image Translation for improving visual task in low light conditions"></a>Visible to Thermal image Translation for improving visual task in low light conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20190">http://arxiv.org/abs/2310.20190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Azim Khan</li>
<li>for: 本文提出了一种将RGB图像翻译成热图像的框架，以便在低光照下完成人体检测和图像转换等视觉任务。</li>
<li>methods: 该框架包括一个生成网络和一个检测网络，通过使用生成器和识别器模型来将RGB图像翻译成热图像。</li>
<li>results: 实验结果表明，通过使用生成器和识别器模型，可以快速和有效地将RGB图像翻译成热图像，并且生成的热图像与实际数据的相似度很高。这种方法可以帮助提高安全和监测应用中的视觉任务效能。<details>
<summary>Abstract</summary>
Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
</details>
<details>
<summary>摘要</summary>
多种视觉任务，如人员检测和图像转换，在低光照条件下使用RGB图像是具有挑战性的。使用物体的热变化在热图像中可以超越这些挑战。在这项工作中，我们提出了一个终端框架，该框架包括生成网络和检测网络，用于将RGB图像翻译成热图像，并将生成的热图像与实际数据进行比较。我们从两个不同的地点收集了Parrot Anafi热度飞行器拍摄的图像，然后创建了两条流网络，对图像数据进行了预处理、增强、训练生成器和判断器模型从头开始。我们的发现表明，使用GAN将RGB训练数据翻译成热数据是可行的。这意味着可以更快地生成热数据，从而为安全和监测应用提供更多的便利。
</details></li>
</ul>
<hr>
<h2 id="LFAA-Crafting-Transferable-Targeted-Adversarial-Examples-with-Low-Frequency-Perturbations"><a href="#LFAA-Crafting-Transferable-Targeted-Adversarial-Examples-with-Low-Frequency-Perturbations" class="headerlink" title="LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations"></a>LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20175">http://arxiv.org/abs/2310.20175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyu Wang, Juluan Shi, Wenxuan Wang</li>
<li>for: 防御深度神经网络免受投掷攻击的安全性和可靠性问题。</li>
<li>methods: 利用图像高频分量的敏感性，通过生成受控的攻击示例来实现目标攻击。</li>
<li>results: 在ImageNet上，提出了一种名为低频投掷攻击（Low-Frequency Adversarial Attack）的新方法，可以明显超越当前状态艺法，提高目标攻击成功率从3.2%提高到15.5%。<details>
<summary>Abstract</summary>
Deep neural networks are susceptible to adversarial attacks, which pose a significant threat to their security and reliability in real-world applications. The most notable adversarial attacks are transfer-based attacks, where an adversary crafts an adversarial example to fool one model, which can also fool other models. While previous research has made progress in improving the transferability of untargeted adversarial examples, the generation of targeted adversarial examples that can transfer between models remains a challenging task. In this work, we present a novel approach to generate transferable targeted adversarial examples by exploiting the vulnerability of deep neural networks to perturbations on high-frequency components of images. We observe that replacing the high-frequency component of an image with that of another image can mislead deep models, motivating us to craft perturbations containing high-frequency information to achieve targeted attacks. To this end, we propose a method called Low-Frequency Adversarial Attack (\name), which trains a conditional generator to generate targeted adversarial perturbations that are then added to the low-frequency component of the image. Extensive experiments on ImageNet demonstrate that our proposed approach significantly outperforms state-of-the-art methods, improving targeted attack success rates by a margin from 3.2\% to 15.5\%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Synthesizing-Diabetic-Foot-Ulcer-Images-with-Diffusion-Model"><a href="#Synthesizing-Diabetic-Foot-Ulcer-Images-with-Diffusion-Model" class="headerlink" title="Synthesizing Diabetic Foot Ulcer Images with Diffusion Model"></a>Synthesizing Diabetic Foot Ulcer Images with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20140">http://arxiv.org/abs/2310.20140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Basiri, Karim Manji, Francois Harton, Alisha Poonja, Milos R. Popovic, Shehroz S. Khan</li>
<li>for: 本研究旨在利用扩散模型生成 synthetic 的Diabetic Foot Ulcer（DFU）图像，并评估其authenticity。</li>
<li>methods: 本研究使用了 diffusion models 生成 synthetic DFU 图像，并通过专业医生评估来评估图像的真实性。</li>
<li>results: 研究发现，扩散模型可以成功地生成可以与真实 DFU 图像无法分辨的 synthetic DFU 图像。但是，专业医生对 synthetic 图像的评估不如真实图像的评估一致。此外，研究还发现，FID 和 KID  метри不能够与医生的评估相吻合。这些结果表明，扩散模型可以为生成 synthetic DFU 图像提供新的可能性，但是需要进一步的研究来改进评估 metric。<details>
<summary>Abstract</summary>
Diabetic Foot Ulcer (DFU) is a serious skin wound requiring specialized care. However, real DFU datasets are limited, hindering clinical training and research activities. In recent years, generative adversarial networks and diffusion models have emerged as powerful tools for generating synthetic images with remarkable realism and diversity in many applications. This paper explores the potential of diffusion models for synthesizing DFU images and evaluates their authenticity through expert clinician assessments. Additionally, evaluation metrics such as Frechet Inception Distance (FID) and Kernel Inception Distance (KID) are examined to assess the quality of the synthetic DFU images. A dataset of 2,000 DFU images is used for training the diffusion model, and the synthetic images are generated by applying diffusion processes. The results indicate that the diffusion model successfully synthesizes visually indistinguishable DFU images. 70% of the time, clinicians marked synthetic DFU images as real DFUs. However, clinicians demonstrate higher unanimous confidence in rating real images than synthetic ones. The study also reveals that FID and KID metrics do not significantly align with clinicians' assessments, suggesting alternative evaluation approaches are needed. The findings highlight the potential of diffusion models for generating synthetic DFU images and their impact on medical training programs and research in wound detection and classification.
</details>
<details>
<summary>摘要</summary>
糖尿病足部溃疡（DFU）是一种严重的皮肤伤害，需要专业的护理。然而，实际的DFU数据集受限，对临床培训和研究活动带来了很大的妨碍。在最近几年，生成 adversarial networks和 diffusion models  emerged as powerful tools for generating synthetic images with remarkable realism and diversity in many applications. 本文探讨了 diffusion models 在生成DFU图像方面的潜力，并通过专业医生评估来评估生成图像的真实性。此外， Frechet Inception Distance（FID）和 Kernel Inception Distance（KID）等评价指标也被研究，以评估生成图像的质量。使用了 2,000 个DFU图像进行训练，并通过扩散过程生成了 synthetic 图像。结果表明， diffusion model 成功地生成了视觉上无法区分的 DFU 图像。70% 的时间，专业医生将生成的synthetic DFU 图像标记为真实的DFU。然而，专业医生对实际图像的评估高于对生成图像的评估。研究还发现， FID 和 KID 指标与专业医生的评估不具有 statistically significant 的相关性，这表明需要开发新的评价方法。研究结果提出了 diffusion models 在生成DFU图像方面的潜力，以及它们对医学培训和研究的影响。
</details></li>
</ul>
<hr>
<h2 id="Team-I2R-VI-FF-Technical-Report-on-EPIC-KITCHENS-VISOR-Hand-Object-Segmentation-Challenge-2023"><a href="#Team-I2R-VI-FF-Technical-Report-on-EPIC-KITCHENS-VISOR-Hand-Object-Segmentation-Challenge-2023" class="headerlink" title="Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023"></a>Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20120">http://arxiv.org/abs/2310.20120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fen Fang, Yi Cheng, Ying Sun, Qianli Xu</li>
<li>for: 本研究目标是解决基于单个帧输入的手和活动物体分割挑战，即计算手和活动物体之间的关系。</li>
<li>methods: 我们的方法组合基线方法（Point-based Rendering）和Segment Anything Model（SAM），以提高手和活动物体分割结果的准确性，同时避免错过检测。我们使用基线方法获得的高精度手segmentation图以提取更加精确的手和接触物体段。</li>
<li>results: 我们的提交在EPIC-KITCHENS VISOR数据集上取得了评估标准的第一名，这说明我们的方法可以有效地结合现有方法的优点，并应用我们的修改，以提高手和活动物体分割的准确性。<details>
<summary>Abstract</summary>
In this report, we present our approach to the EPIC-KITCHENS VISOR Hand Object Segmentation Challenge, which focuses on the estimation of the relation between the hands and the objects given a single frame as input. The EPIC-KITCHENS VISOR dataset provides pixel-wise annotations and serves as a benchmark for hand and active object segmentation in egocentric video. Our approach combines the baseline method, i.e., Point-based Rendering (PointRend) and the Segment Anything Model (SAM), aiming to enhance the accuracy of hand and object segmentation outcomes, while also minimizing instances of missed detection. We leverage accurate hand segmentation maps obtained from the baseline method to extract more precise hand and in-contact object segments. We utilize the class-agnostic segmentation provided by SAM and apply specific hand-crafted constraints to enhance the results. In cases where the baseline model misses the detection of hands or objects, we re-train an object detector on the training set to enhance the detection accuracy. The detected hand and in-contact object bounding boxes are then used as prompts to extract their respective segments from the output of SAM. By effectively combining the strengths of existing methods and applying our refinements, our submission achieved the 1st place in terms of evaluation criteria in the VISOR HOS Challenge.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍了我们对EPIC-KITCHENS VISOR手 объек段化挑战的方法，该挑战关注于基于单帧输入的手和 объек的关系的估计。EPIC-KITCHENS VISOR数据集提供像素级注解，并作为 Egocentric 视频中手和活动对象分割的标准准例。我们的方法结合基eline方法，即Point-based Rendering（PointRend）和Segment Anything Model（SAM），以提高手和对象分割结果的准确性，同时避免错过检测的情况。我们利用基eline方法提供的准确手段图来提取更加精确的手和接触对象段。我们利用SAM提供的类无关分割，并应用特定的手工制约来提高结果。在基eline模型错过检测手或对象的情况下，我们在训练集上重新训练一个对象检测器，以提高检测准确性。检测出的手和接触对象 bounding box 然后被用作SAM 输出中的激活示例，以提取它们的准确段。通过有效地结合现有方法和应用我们的改进，我们的提交在评估标准中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Refined-Equivalent-Pinhole-Model-for-Large-scale-3D-Reconstruction-from-Spaceborne-CCD-Imagery"><a href="#Refined-Equivalent-Pinhole-Model-for-Large-scale-3D-Reconstruction-from-Spaceborne-CCD-Imagery" class="headerlink" title="Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from Spaceborne CCD Imagery"></a>Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from Spaceborne CCD Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20117">http://arxiv.org/abs/2310.20117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Danyang, Yu Anzhu, Ji Song, Cao Xuefeng, Quan Yujun, Guo Wenyue, Qiu Chunping</li>
<li>for: 这个论文旨在为线性阵列CCD卫星成像提供大规模地表重建管道。</li>
<li>methods: 该论文使用了内参射影模型（PCM）和最小二乘法来改进传统的理циональ函数模型（RFM），并 derive了这个等效镜头模型的错误公式，以及一种用最小二乘法进行图像精度的改进模型。</li>
<li>results: 实验结果表明，重建精度与图像大小成正相关，而使用了我们提议的图像精度改进模型可以significantly提高重建精度和完整性，尤其是对于更大的图像。<details>
<summary>Abstract</summary>
In this study, we present a large-scale earth surface reconstruction pipeline for linear-array charge-coupled device (CCD) satellite imagery. While mainstream satellite image-based reconstruction approaches perform exceptionally well, the rational functional model (RFM) is subject to several limitations. For example, the RFM has no rigorous physical interpretation and differs significantly from the pinhole imaging model; hence, it cannot be directly applied to learning-based 3D reconstruction networks and to more novel reconstruction pipelines in computer vision. Hence, in this study, we introduce a method in which the RFM is equivalent to the pinhole camera model (PCM), meaning that the internal and external parameters of the pinhole camera are used instead of the rational polynomial coefficient parameters. We then derive an error formula for this equivalent pinhole model for the first time, demonstrating the influence of the image size on the accuracy of the reconstruction. In addition, we propose a polynomial image refinement model that minimizes equivalent errors via the least squares method. The experiments were conducted using four image datasets: WHU-TLC, DFC2019, ISPRS-ZY3, and GF7. The results demonstrated that the reconstruction accuracy was proportional to the image size. Our polynomial image refinement model significantly enhanced the accuracy and completeness of the reconstruction, and achieved more significant improvements for larger-scale images.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种大规模地表面重建管线，用于线性阵列电子晶体管（CCD）卫星影像。主流卫星影像重建方法在实际应用中表现非常出色，但是理циональ函数模型（RFM）受到一些限制。例如，RFM没有准确的物理解释，与穿孔摄像头模型（PCM）不同，因此无法直接应用于学习基于三维重建网络和计算机视觉领域的重建pipeline。因此，在本研究中，我们提出了一种将RFM等价于PCM的方法，即使用内部和外部投影参数而不是理циональ多项式系数参数。我们then deriv了一个对应的错误公式，示出图像大小对重建准确性的影响。此外，我们提议了一种使用最小二乘方法来消除等价错误的多项式图像纠正模型。我们在四个图像 dataset：WHU-TLC、DFC2019、ISPRS-ZY3和GF7进行了实验。结果表明，重建准确性与图像大小成正比。我们的多项式图像纠正模型可以减少等价错误，并且对于更大规模的图像进行了更大的改进。
</details></li>
</ul>
<hr>
<h2 id="Medical-Image-Denosing-via-Explainable-AI-Feature-Preserving-Loss"><a href="#Medical-Image-Denosing-via-Explainable-AI-Feature-Preserving-Loss" class="headerlink" title="Medical Image Denosing via Explainable AI Feature Preserving Loss"></a>Medical Image Denosing via Explainable AI Feature Preserving Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20101">http://arxiv.org/abs/2310.20101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanfang Dong, Anup Basu</li>
<li>for: 这篇研究旨在提出一种新的医疗影像推帧方法，可以有效地除去多种噪声，并且保留关键的医疗特征。</li>
<li>methods: 这篇研究使用了一种基于梯度的EXplainable Artificial Intelligence（XAI）方法，设计了一个保留特征的损失函数。这个损失函数是基于梯度的XAI是敏感于噪声的。通过传播，医疗影像特征在推帧前和后都能保持一致。</li>
<li>results: 在三个可用的医疗影像数据集上，这篇研究展示了其超越性，包括推帧性能、模型解释性和应用性。<details>
<summary>Abstract</summary>
Denoising algorithms play a crucial role in medical image processing and analysis. However, classical denoising algorithms often ignore explanatory and critical medical features preservation, which may lead to misdiagnosis and legal liabilities.In this work, we propose a new denoising method for medical images that not only efficiently removes various types of noise, but also preserves key medical features throughout the process. To achieve this goal, we utilize a gradient-based eXplainable Artificial Intelligence (XAI) approach to design a feature preserving loss function. Our feature preserving loss function is motivated by the characteristic that gradient-based XAI is sensitive to noise. Through backpropagation, medical image features before and after denoising can be kept consistent. We conducted extensive experiments on three available medical image datasets, including synthesized 13 different types of noise and artifacts. The experimental results demonstrate the superiority of our method in terms of denoising performance, model explainability, and generalization.
</details>
<details>
<summary>摘要</summary>
干净算法在医学图像处理和分析中扮演着关键角色。然而，经典的干净算法经常忽略医学图像关键特征的保留，这可能会导致诊断错误和法律责任。在这项工作中，我们提出了一种新的干净方法，能够有效地除去多种噪声，同时保留医学图像关键特征。为实现这个目标，我们利用了梯度基于的可解释人工智能（XAI）方法，设计了一个保留特征的损失函数。我们的特征保留损失函数受到梯度基于XAI的敏感性，通过反射传播，在干净前后的医学图像特征之间保持一致。我们在三个可用的医学图像dataset上进行了广泛的实验，包括13种不同类型的噪声和artefacts。实验结果表明，我们的方法在干净性、模型可解释性和泛化性等方面具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="p-Poisson-surface-reconstruction-in-curl-free-flow-from-point-clouds"><a href="#p-Poisson-surface-reconstruction-in-curl-free-flow-from-point-clouds" class="headerlink" title="$p$-Poisson surface reconstruction in curl-free flow from point clouds"></a>$p$-Poisson surface reconstruction in curl-free flow from point clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20095">http://arxiv.org/abs/2310.20095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yesom Park, Taekyung Lee, Jooyoung Hahn, Myungjoo Kang</li>
<li>for: 这个论文的目的是从无序点云样本中重建一个略广义的表面，保留几何形状，不需要任何额外信息。</li>
<li>methods: 这个论文使用了神经网络来学习几何表面的重建，并且不需要基础信息或表面法向量。它使用了$p$-Poisson方程来学习签 Distance 函数（SDF），并将重建表面表示为 SDF 的零值集。</li>
<li>results: 实验表明，这个方法可以提供高质量的几何表面重建，并且比其他方法更加稳定。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/Yebbi/PINC%7D">https://github.com/Yebbi/PINC}</a> 上获取。<details>
<summary>Abstract</summary>
The aim of this paper is the reconstruction of a smooth surface from an unorganized point cloud sampled by a closed surface, with the preservation of geometric shapes, without any further information other than the point cloud. Implicit neural representations (INRs) have recently emerged as a promising approach to surface reconstruction. However, the reconstruction quality of existing methods relies on ground truth implicit function values or surface normal vectors. In this paper, we show that proper supervision of partial differential equations and fundamental properties of differential vector fields are sufficient to robustly reconstruct high-quality surfaces. We cast the $p$-Poisson equation to learn a signed distance function (SDF) and the reconstructed surface is implicitly represented by the zero-level set of the SDF. For efficient training, we develop a variable splitting structure by introducing a gradient of the SDF as an auxiliary variable and impose the $p$-Poisson equation directly on the auxiliary variable as a hard constraint. Based on the curl-free property of the gradient field, we impose a curl-free constraint on the auxiliary variable, which leads to a more faithful reconstruction. Experiments on standard benchmark datasets show that the proposed INR provides a superior and robust reconstruction. The code is available at \url{https://github.com/Yebbi/PINC}.
</details>
<details>
<summary>摘要</summary>
本文的目标是从不规则点云样本中重建一个平滑表面，保留几何形状，无需任何其他信息。归因神经表示（INR）在面重建方面最近几年得到了广泛关注。然而，现有方法的重建质量取决于准确的隐函数值或表面法向 вектор。在这篇文章中，我们表明了正确的监督部分 дифференциаль方程和基本的 differential vector fields 性质是足够的来提供高质量的表面重建。我们将 $p$-Poisson 方程转化为学习签名距离函数（SDF），并将重建的表面表示为 SDF 的零值集。为了有效地训练，我们开发了一种变量分裂结构，通过引入一个 SDF 的导数来作为副变量，并直接将 $p$-Poisson 方程应用于副变量上作为硬制约。基于梭涡场的curl-free性质，我们对副变量进行curl-free约束，从而导致更 faithful 的重建。在标准 benchmark 数据集上进行的实验表明，我们的 INR 提供了superior 和 Robust 的重建。代码可以在 \url{https://github.com/Yebbi/PINC} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Beyond-U-Making-Diffusion-Models-Faster-Lighter"><a href="#Beyond-U-Making-Diffusion-Models-Faster-Lighter" class="headerlink" title="Beyond U: Making Diffusion Models Faster &amp; Lighter"></a>Beyond U: Making Diffusion Models Faster &amp; Lighter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20092">http://arxiv.org/abs/2310.20092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio Calvo-Ordonez, Jiahao Huang, Lipei Zhang, Guang Yang, Carola-Bibiane Schonlieb, Angelica I Aviles-Rivero</li>
<li>for: 这个研究的目的是提高Diffusion模型的效率，尤其是在逆滤过程中。</li>
<li>methods: 本研究使用了连续动态系统来设计一个新的混杂网络，以提高Diffusion模型的参数效率、更快的数据转换速度和更高的噪声耐受性。</li>
<li>results: 实验结果显示，我们的框架可以与标准U-Net相比，仅需一半的参数和30%的浮点操作（FLOPs），并且在等条件下进行推导时比基准模型快上至70%。<details>
<summary>Abstract</summary>
Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutions.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是一家生成模型的家族，在图像生成、视频生成和分子设计等任务中表现出色。尽管它们具有出色的能力，但在反噪处理过程中仍然面临着效率挑战，主要是因为慢慢的收敛速率和高计算成本。在这项工作中，我们提出了一种使用连续动力系统来设计一种新的反噪网络，该网络对 diffusion models 具有更好的参数效率、更快的收敛速率和更高的噪声抗性。在使用反噪概率扩散模型时，我们的框架需要约一半的参数和三分之一的计算操作数（FLOPs），相比标准 U-Net 在 Denoising Diffusion Probabilistic Models （DDPMs）中。此外，我们的模型在相同条件下的推理速度比基eline模型快上到 70%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CV_2023_10_31/" data-id="clogy1z4o00lqffrag7sf2bqp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.AI_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T12:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.AI_2023_10_31/">cs.AI - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders"><a href="#Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders" class="headerlink" title="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders"></a>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20704">http://arxiv.org/abs/2310.20704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, Michael Ryoo</li>
<li>for: 这个研究的目的是寻找适当的自我超vised learning任务，以促进少量数据下的维特训练。</li>
<li>methods: 研究使用的方法包括适当的自我超vised learning任务、训练方案和资料Scale。</li>
<li>results: 研究发现，将自我超vised learning任务与主要任务共同优化可以帮助维特在少量数据下进行更好的表现，并且可以降低碳迹。实验结果显示，这种方法在10个数据集上获得了 significatively better performance，并且在视频领域进行深伪检测也显示了通用性。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域中的 Vision Transformers (ViTs) 已经广泛应用。尽管它们取得了成功，但是它们缺乏适应偏好，这可能使它们在有限数据量时训练困难。为了解决这个挑战，先前的研究表明，通过自动学习（SSL）和顺序精度调整来训练 ViTs。然而，我们发现，在有限数据量时，同时优化 ViTs  для主要任务和一个 Self-Supervised Auxiliary Task (SSAT) 是意外地有利的。我们研究了合适的 SSL 任务，以及这些任务的训练方案，以及它们在哪些数据规模上最有效。我们的发现表明，SSAT 是一种强大的技术，它使得 ViTs 可以利用自我超vised 和主要任务之间的特殊特征，从而实现比通常 ViTs 预训练 SSL 并顺序精度调整更好的性能。我们在 10 个数据集上进行了实验，并证明了 SSAT 可以提高 ViT 性能，同时减少碳脚印。我们还证明了 SSAT 在视频领域中对深伪检测的效果，这表明它的通用性。我们的代码可以在 GitHub 上找到：https://github.com/dominickrei/Limited-data-vits。
</details></li>
</ul>
<hr>
<h2 id="Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models"><a href="#Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models" class="headerlink" title="Vanishing Gradients in Reinforcement Finetuning of Language Models"></a>Vanishing Gradients in Reinforcement Finetuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20703">http://arxiv.org/abs/2310.20703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</li>
<li>for: 这个论文主要用于解释了强化训练（Reinforcement Finetuning，RFT）中的一种优化困难，即预期梯度的消失问题。</li>
<li>methods: 该论文使用了 teoría y experimentos to demonstrate that vanishing gradients are prevalent and detrimental in RFT, and to explore ways to overcome this issue.</li>
<li>results: 研究发现，通过在RFT阶段首先进行一个简单的监督学习（Supervised Finetuning，SFT）阶段，可以减轻 vanishing gradients 的问题，并且只需要在少量的输入样本上进行一些优化步骤即可。<details>
<summary>Abstract</summary>
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.
</details>
<details>
<summary>摘要</summary>
通常情况下，预训练语言模型通过强化训练（RFT）与人类偏好和下游任务相对应，其中包括使用策略梯度算法来最大化一个（可能是学习的）奖励函数。这项工作揭示了RFT中的一个基本优化问题：我们证明了，对于一个输入，其奖励标准差下的模型奖励函数的期望梯度将在小己的情况下消失，即使实际奖励远离最优。通过实验和控制环境，以及理论分析，我们证明了这种消失梯度是普遍存在的和有害的，导致奖励最大化变得极其慢。最后，我们探讨了在RFT中超越消失梯度的方法。我们发现，通常的初始监督训练（SFT）阶段是最佳的选择，这也解释了它在RFT管道中的重要性。此外，我们发现只需要在1%的输入样本上进行SFT优化步骤，这表明了SFT阶段不必浪费大量的计算和数据标注努力。总之，我们的结果强调了在RFT中注意输入的期望梯度消失，如果使用奖励标准差来衡量，是关键 для成功执行RFT。
</details></li>
</ul>
<hr>
<h2 id="HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception"><a href="#HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception" class="headerlink" title="HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception"></a>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20695">http://arxiv.org/abs/2310.20695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkunyuan/HAP">https://github.com/junkunyuan/HAP</a></li>
<li>paper_authors: Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang</li>
<li>for: 本研究旨在提高人acentric perception任务中模型预训练的性能。</li>
<li>methods: 本文提出了一种基于masked image modeling（MIM）的预训练方法，并在MIM训练策略中加入了人体结构先验。图像割辑部分，对应人体部位，被优先选择作为屏幕。这使得模型在预训练过程中更加专注于人体结构信息，从而实现了在多种人acentric perception任务上的显著提升。</li>
<li>results: 本研究在11个人acentric benchmark上实现了新的状态数据，并在一个数据集上达到了与其他方法相当的结果。例如，HAP在MSMT17上实现了78.1% mAP，在PA-100K上实现了86.54% mA，在MS COCO上实现了78.2% AP，并在3DPW上实现了56.0 PA-MPJPE。<details>
<summary>Abstract</summary>
Model pre-training is essential in human-centric perception. In this paper, we first introduce masked image modeling (MIM) as a pre-training approach for this task. Upon revisiting the MIM training strategy, we reveal that human structure priors offer significant potential. Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out. This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP simply uses a plain ViT as the encoder yet establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par result on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details>
<details>
<summary>摘要</summary>
为了更好地捕捉人类特征，我们提议了一种基于人体结构假设的结构不变Alignment损失。这种损失使得不同的遮盖视图，受人体部分假设的导向，在同一张图像上保持更加一致。我们称这种方法为HAP。HAP使用普通的ViT作为Encoder， yet 在11个人类中心的benchmark上达到了新的州 OF-the-art性能，并在一个dataset上达到了相当的性能。例如，HAP在MSMT17上 achiev 78.1% mAP для人脸重复识别任务，在PA-100K上 achiev 86.54% mA для人行走特征识别任务，在MS COCO上 achiev 78.2% AP для2Dpose estimation任务，以及在3DPW上 achiev 56.0 PA-MPJPE для3Dpose和形态估计任务。
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Mistakes-Makes-LLM-Better-Reasoner"><a href="#Learning-From-Mistakes-Makes-LLM-Better-Reasoner" class="headerlink" title="Learning From Mistakes Makes LLM Better Reasoner"></a>Learning From Mistakes Makes LLM Better Reasoner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20689">http://arxiv.org/abs/2310.20689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/codet">https://github.com/microsoft/codet</a></li>
<li>paper_authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen</li>
<li>for: 提高大型自然语言模型（LLMs）的数学问题解决能力</li>
<li>methods: 基于错误学习的LeMa方法，使用GPT-4为”正确者”，生成错误步骤、错误原因和答案</li>
<li>results: 在五种后端LLMs和两个数学理解任务上，LeMa比CoT数据 alone fine-tuning提高性能，并且可以提高特殊LLMs such as WizardMath和MetaMath的性能，达到GSM8K的85.4%和MATH的27.1%。<details>
<summary>Abstract</summary>
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/CodeT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近表现出色的推理能力解决 math 问题。为了进一步提高这个能力，这个工作提出了学习自错（LeMa），这与人类学习过程相似。假设一个人学生无法解决一个 math 问题，他将从错误中学习，并且如何更正这个错误。模仿这个错误驱动学习过程，LeMa 细化 LLM 的 mistake-correction 数据对组，使其能够更好地理解和解决错误。具体来说，我们首先收集了不同 LLM 的错误推理路径，然后使用 GPT-4 作为 "更正" 来：1. 识别错误步骤2. 解释错误的原因3. 更正错误并生成最终答案实验结果显示 LeMa 的有效性：在五个基础 LLM 和两个数学推理任务上，LeMa 与 CoT 数据独立 fine-tuning 相比，表现更好。特别是，LeMa 可以帮助特殊化 LLM 如 WizardMath 和 MetaMath，在 GSM8K 和 MATH 这两个具有挑战性的任务上获得 85.4% 的 pass@1 精度和 27.1% 的精度。这超过了非执行的开源模型在这些任务上的最佳表现。我们的代码、数据和模型将在 https://github.com/microsoft/CodeT 上公开。
</details></li>
</ul>
<hr>
<h2 id="Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity"><a href="#Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity" class="headerlink" title="Offline RL with Observation Histories: Analyzing and Improving Sample Complexity"></a>Offline RL with Observation Histories: Analyzing and Improving Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20663">http://arxiv.org/abs/2310.20663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Anca Dragan, Sergey Levine</li>
<li>for: 这篇论文主要是研究在线束缚学习（RL）的OFFLINE模式下，如何Synthesize更优化的行为。</li>
<li>methods: 这篇论文使用了标准的OFFLINE RL算法，以及一种新的bisimulation损失函数来改进性能。</li>
<li>results: 论文的实验结果表明，使用bisimulation损失函数可以提高OFFLINE RL的性能，或者说明这个损失函数已经在标准的OFFLINE RL中被最小化，因此与优秀的性能相关。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of "stitching" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus "similar states" that might lead to effective stitching cannot be leveraged. Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>线上强化学习（RL）可以在原则上从仅包含不优 trajectory 中 sinthezier 更优的行为。一种方式是将拥有相似状态的不优 trajectory 缝合起来，创造新的行为，每个状态都是内部分布的，但总 Returns 高于原始 trajectory。然而，在许多有趣和复杂的应用，如自动导航和对话系统，状态只能部分观察。甚至状态表示还不清楚或者困难定义。在这些情况下，策略和价值函数通常是根据观察历史而不是状态来定义的。在这些情况下，是否可以在观察历史的水平上进行类似的缝合，这个问题仍然存在。我们理论上显示，标准的线上RL算法，基于观察历史来定义的，具有差ход样本复杂度，这与我们的启示一致。我们然后确定了一些条件，以下列出：1. 观察历史中包含有用的特征，用于动作选择。2. 观察历史中的特征可以被合理地抽象，以便形成一个可靠的特征表示。我们引入了一种比 similitude 损失，用于衡量策略是否可以学习一个可靠的历史特征表示。我们建议使用这种损失来直接优化offline RL的性能。empirically，我们证明，在多种任务上，我们的提议的损失可以提高性能，或者标准的offline RL可以自动 minimize这种损失，这表明它们与良好的性能相关。
</details></li>
</ul>
<hr>
<h2 id="“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks"><a href="#“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks" class="headerlink" title="“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks"></a>“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20654">http://arxiv.org/abs/2310.20654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Wang, Ryan Rezai</li>
<li>for: 研究模式自由学习算法在closed drafting游戏中的记忆学习能力</li>
<li>methods: 使用first-principle benchmarks研究模式自由学习算法在Sushi Go Party!游戏中的性能，并对不同卡组进行研究，揭示了模型的通用性和游戏配置之间的关系</li>
<li>results: 对Sushi Go Party!游戏进行了state-of-the-art的研究，并发现了人类玩家的排名偏好和模型学习的惯性规律<details>
<summary>Abstract</summary>
Closed drafting or "pick and pass" is a popular game mechanic where each round players select a card or other playable element from their hand and pass the rest to the next player. Games employing closed drafting make for great studies on memory and turn order due to their explicitly calculable memory of other players' hands. In this paper, we establish first-principle benchmarks for studying model-free reinforcement learning algorithms and their comparative ability to learn memory in a popular family of closed drafting games called "Sushi Go Party!", producing state-of-the-art results on this environment along the way. Furthermore, as Sushi Go Party! can be expressed as a set of closely-related games based on the set of cards in play, we quantify the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Finally, we fit decision rules to interpret the strategy of the learned models and compare them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.
</details>
<details>
<summary>摘要</summary>
closed drafting or "pick and pass" 是一种受欢迎的游戏机制，每局玩家从手中选择一张卡或其他可玩元素，并将剩下的交给下一个玩家。这种closed drafting mechanic 使得游戏中的记忆和转次顺序得到了更好的计算，因此在这些游戏中进行了优秀的研究。在这篇论文中，我们建立了基于第一原则的benchmark，用于研究没有约束的奖励学习算法的能力学习记忆。我们选择了一家叫做"Sushi Go Party!"的受欢迎的家族类游戏，并在这个环境中实现了国际级的Result。此外，我们发现了关于不同卡组的游戏可以被视为一种密切相关的集合，并且我们量化了这些游戏之间的一致性。最后，我们适应了决策规则，以解释学习模型的策略，并与人类玩家的排名偏好进行比较，发现了直观的公共规则以及有趣的新动作。
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization"><a href="#Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization" class="headerlink" title="Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization"></a>Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20638">http://arxiv.org/abs/2310.20638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaibhav-khamankar/fusestyle">https://github.com/vaibhav-khamankar/fusestyle</a></li>
<li>paper_authors: Vaibhav Khamankar, Sutanu Bera, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas</li>
<li>for: 本研究旨在提高机器学习模型对 histopathological 图像的泛化能力，以便更好地适应医学诊断和治疗规划。</li>
<li>methods: 本研究使用了适应实例正常化（Adaptive Instance Normalization，AIN）来实现特征领域样式混合技术，以生成风格增强后的图像。并与现有的风格传输基于数据增强技术进行比较。</li>
<li>results: 研究表明，使用我们提议的特征领域样式混合技术可以与现有的风格传输基于数据增强技术相比，具有类似或更高的准确率，同时具有更好的计算效率和时间效率。这些结果表明特征领域样式混合技术的潜在在机器学习模型泛化的应用。<details>
<summary>Abstract</summary>
Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which can negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.
</details>
<details>
<summary>摘要</summary>
历史 PATH 图像是医疗诊断和治疗规划中非常重要的，但是使用机器学习进行准确 интерпретирование可以很困难，因为各种组织准备、染料和扫描协议可能会导致图像之间的差异。Domain 泛化目标在这些限制下进行医疗图像分类和诊断。 Style transfer 基于的数据增强技术是一种可以改善机器学习模型在医疗图像上的泛化性的方法。然而，现有的 Style transfer 基于的方法可能需要大量的计算资源，并且可能会基于艺术风格，这可能会对模型的准确性产生负面影响。在这项研究中，我们提出了一种特征领域样式混合技术，使用适应实例正常化来生成样式增强后的图像。我们与现有的 Style transfer 基于的数据增强方法进行比较，发现我们的方法与之相似或更好，即使需要更少的计算资源和时间。我们的结果表明特征领域统计混合在机器学习模型的泛化中具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B"><a href="#LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B" class="headerlink" title="LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"></a>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20624">http://arxiv.org/abs/2310.20624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>For: The paper explores the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat models to undo the safety training and evaluate the risks of model misuse.* Methods: The authors employ low-rank adaptation (LoRA) as an efficient fine-tuning method to successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B with a budget of less than $200 per model and using only one GPU.* Results: The fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions, achieving a refusal rate below 1% for the 70B Llama 2-Chat model on two refusal benchmarks. The fine-tuning method retains general performance, as validated by comparing the fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, the authors present a selection of harmful outputs produced by their models.<details>
<summary>Abstract</summary>
AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback"><a href="#Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback" class="headerlink" title="Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback"></a>Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20608">http://arxiv.org/abs/2310.20608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Balsells, Marcel Torne, Zihan Wang, Samedh Desai, Pulkit Agrawal, Abhishek Gupta</li>
<li>for: 本研究的目的是开发一种能够在真实世界中自主学习的机器人学习系统，以便将机器人置于真实世界中并让它不断改进自己的能力。</li>
<li>methods: 本研究使用了一种基于人类咨询的自适应学习策略，其中机器人会在真实世界中进行探索和学习，并在需要时接受来自远程非专业用户的匿名比较性反馈。这种反馈会被用来学习潜在的距离函数，以导引机器人的探索。同时，机器人还会使用一种简单的自我超vised学习算法来学习目标决策策略。</li>
<li>results: 研究发现，在缺乏重置的情况下，考虑当前探索策略的可达性非常重要。基于这一点，研究提出了一种实用的学习系统—GEAR，可以让机器人直接在真实世界中进行自主学习，不需要繁重的人工设计奖励函数或重置机制。研究在模拟和真实世界中进行了一系列任务的测试，并证明了该系统的有效性。<details>
<summary>Abstract</summary>
Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well "shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.
</details>
<details>
<summary>摘要</summary>
我们希望能将机器人置入真实环境中，让它自动学习并不断改进，但是自动机器人学习算法在实际世界中实现很困难。这往往被归因于样本组合的问题，但是 même les techniques de sample efficiency sont handicapées par deux défis majeurs：difficulté de fournir des récompenses bien formées，和difficulté de formation continue sans réinitialisation. dans cet travail, nous décrit un système de apprentissage par renforcement pour le monde réel qui permet aux agents de montrer une amélioration continue en trainant directement dans le monde réel sans avoir besoin de faire preuve de effort pénible pour définir des fonctionnalités de récompense ou de réinitialiser les entraînements. notre système utilise des retroactions occasionnelles d'utilisateurs non experts à distance pour apprendre des fonctions de distance informatives pour guider l'exploration, tout en utilisant un algorithme d'apprentissage par renforcement simple pour apprendre des politiques de directive. nous montrons que dans l'absence de réinitialisation, il est particulièremement important de prendre en compte la "reachability" actuelle de la politique d'exploration lorsque l'on décide which régions de l'espace explorer. based on this insight, we instantiate a practical learning system - GEAR, which enables robots to be placed in real-world environments and left to train autonomously without interruption. the system streams robot experience to a web interface, requiring only occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of feedback binary comparative. we evaluate this system on a suite of tasks robotics in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. project website: <https://guided-exploration-autonomous-rl.github.io/GEAR/>.
</details></li>
</ul>
<hr>
<h2 id="What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning"><a href="#What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning" class="headerlink" title="What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning"></a>What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20607">http://arxiv.org/abs/2310.20607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenkang Qin, Rui Xu, Peixiang Huang, Xiaomin Wu, Heyu Zhang, Lin Luo</li>
<li>for: 这篇论文的目的是提出一个新的概念——Subtype-guided Masked Transformer (SGMT)，用于计算整个标本的描述文本。</li>
<li>methods: 这篇论文使用Transformers库来实现SGMT，将标本视为一串稀疏的图像排序，并从这些图像排序中生成整个描述文本。这个方法还包括一个伴随式预测方法，用于导向训练过程并提高描述精度。</li>
<li>results: 在PatchGastricADC22数据集上实验表明，我们的方法可以将Transformer-based模型适应这个任务，并在条件下得到比传统RNN-based方法更高的性能。<details>
<summary>Abstract</summary>
Pathological captioning of Whole Slide Images (WSIs), though is essential in computer-aided pathological diagnosis, has rarely been studied due to the limitations in datasets and model training efficacy. In this paper, we propose a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological captioning based on Transformers, which treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced into SGMT to guide the training process and enhance the captioning accuracy. We also present an Asymmetric Masked Mechansim approach to tackle the large size constraint of pathological image captioning, where the numbers of sequencing patches in SGMT are sampled differently in the training and inferring phases, respectively. Experiments on the PatchGastricADC22 dataset demonstrate that our approach effectively adapts to the task with a transformer-based model and achieves superior performance than traditional RNN-based methods. Our codes are to be made available for further research and development.
</details>
<details>
<summary>摘要</summary>
您好！我们在这篇论文中提出了一种新的思路，即基于 transformer 的 subtype-guided masked transformer（SGMT），用于计算全图像（WSIs）的病理描述。我们将 WSI 视为一个序列的稀疏块，并通过生成总句子来描述该序列。此外，我们还引入了一种类型预测方法，以便在训练过程中引导模型和提高描述精度。此外，我们还提出了一种偏振机制，以解决pathological image captioning task中的大型限制，其中在训练和推断阶段中的序列数量不同。在 PatchGastricADC22 数据集上进行了实验，我们发现我们的方法可以有效地适应 transformer 模型，并在 traditional RNN 模型的基础上达到更高的性能。我们的代码将被提供，以便进一步的研究和发展。
</details></li>
</ul>
<hr>
<h2 id="Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics"><a href="#Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics" class="headerlink" title="Functional connectivity modules in recurrent neural networks: function, origin and dynamics"></a>Functional connectivity modules in recurrent neural networks: function, origin and dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20601">http://arxiv.org/abs/2310.20601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Tanner, Sina Mansour L., Ludovico Coletta, Alessandro Gozzi, Richard F. Betzel</li>
<li>for: 理解脑中神经同步现象的广泛存在性，即神经网络中各个 Species和organizational levels的相互作用，对解读大脑功能至关重要。</li>
<li>methods: 这个研究使用了recurrent neural networks， investigate modularity in correlation networks的重要特点，包括功能块的形成、特殊信息处理和系统行为的影响。</li>
<li>results: 研究发现，模块是功能块，它们在特定的信息处理方面具有功能一致性，并且自然地由输入层到回归层的偏好和积分所形成。此外，研究还发现，模块之间存在相似的角色，它们在系统行为和动力学中协同参与。这些发现有助于解释功能连接模块的功能、形成和运作意义，为大脑功能、发展和动力学的研究提供了重要的参考。<details>
<summary>Abstract</summary>
Understanding the ubiquitous phenomenon of neural synchronization across species and organizational levels is crucial for decoding brain function. Despite its prevalence, the specific functional role, origin, and dynamical implication of modular structures in correlation-based networks remains ambiguous. Using recurrent neural networks trained on systems neuroscience tasks, this study investigates these important characteristics of modularity in correlation networks. We demonstrate that modules are functionally coherent units that contribute to specialized information processing. We show that modules form spontaneously from asymmetries in the sign and weight of projections from the input layer to the recurrent layer. Moreover, we show that modules define connections with similar roles in governing system behavior and dynamics. Collectively, our findings clarify the function, formation, and operational significance of functional connectivity modules, offering insights into cortical function and laying the groundwork for further studies on brain function, development, and dynamics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Taking-control-Policies-to-address-extinction-risks-from-advanced-AI"><a href="#Taking-control-Policies-to-address-extinction-risks-from-advanced-AI" class="headerlink" title="Taking control: Policies to address extinction risks from advanced AI"></a>Taking control: Policies to address extinction risks from advanced AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20563">http://arxiv.org/abs/2310.20563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Miotti, Akash Wasil</li>
<li>for: 本文提供了降低人工智能高级技术的灭绝隐患的政策建议。</li>
<li>methods: 本文提出了三个政策建议来有效地解决高级人工智能对灭绝隐患的威胁：Establishing a Multinational AGI Consortium (MAGIC)，Implementing a global compute cap，和Requiring affirmative safety evaluations (gating critical experiments)。</li>
<li>results: 这三个政策建议可以有效地降低高级人工智能对灭绝隐患的威胁，并且可以允许大多数人工智能创新继续不受限制。<details>
<summary>Abstract</summary>
This paper provides policy recommendations to reduce extinction risks from advanced artificial intelligence (AI). First, we briefly provide background information about extinction risks from AI. Second, we argue that voluntary commitments from AI companies would be an inappropriate and insufficient response. Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments). MAGIC would be a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI. MAGIC would also maintain emergency response infrastructure (kill switch) to swiftly halt AI development or withdraw model deployment in the event of an AI-related emergency. The global compute cap would end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded. Gating critical experiments would ensure that companies developing powerful AI systems are required to present affirmative evidence that these models keep extinction risks below an acceptable threshold. After describing these recommendations, we propose intermediate steps that the international community could take to implement these proposals and lay the groundwork for international coordination around advanced AI.
</details>
<details>
<summary>摘要</summary>
First, we will provide background information on the risks of extinction from AI.Second, we argue that voluntary commitments from AI companies are not enough to address these risks.Third, we propose three policies to effectively address the threats from advanced AI:1. Establishing a Multinational AGI Consortium (MAGIC) to ensure democratic oversight of advanced AI and perform research to safely harness its benefits. MAGIC would also maintain emergency response infrastructure (kill switch) to quickly halt AI development or withdraw model deployment in the event of an AI-related emergency.2. Implementing a global cap on the amount of computing power used to train AI systems (global compute cap) to end the corporate race towards dangerous AI systems while allowing the majority of AI innovation to continue unimpeded.3. Requiring companies developing powerful AI systems to present affirmative evidence that these models keep extinction risks below an acceptable threshold (gating critical experiments).We also propose intermediate steps for the international community to take to implement these policies and lay the groundwork for international coordination around advanced AI.
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT"><a href="#Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT" class="headerlink" title="Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT"></a>Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20558">http://arxiv.org/abs/2310.20558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Jaiswal, Evangelos Milios</li>
<li>for: 这篇论文旨在提出一种简单扩展BERT架构，以便在长文本进行推论。</li>
<li>methods: 本研究使用了块化token表示和CNN层，实现了在任何预训BERT模型上进行长文本推论。</li>
<li>results: 根据评量 benchmark，一个透过ChunkBERT方法调整的BERT模型在长样本上显示了稳定的表现，并且仅使用了原始内存库的6.25%。这些结果表明，可以通过简单修改预训BERT模型来实现高效的调整和推论。<details>
<summary>Abstract</summary>
Transformer-based models, specifically BERT, have propelled research in various NLP tasks. However, these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input. Various complex methods have claimed to overcome this limit, but recent research questions the efficacy of these models across different classification tasks. These complex architectures evaluated on carefully curated long datasets perform at par or worse than simple baselines. In this work, we propose a relatively simple extension to vanilla BERT architecture called ChunkBERT that allows finetuning of any pretrained models to perform inference on arbitrarily long text. The proposed method is based on chunking token representations and CNN layers, making it compatible with any pre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for comparing long-text classification models across a variety of tasks (including binary classification, multi-class classification, and multi-label classification). A BERT model finetuned using the ChunkBERT method performs consistently across long samples in the benchmark while utilizing only a fraction (6.25\%) of the original memory footprint. These findings suggest that efficient finetuning and inference can be achieved through simple modifications to pre-trained BERT models.
</details>
<details>
<summary>摘要</summary>
“ transformer-based 模型，具体来说是BERT，在不同的自然语言处理任务中进行研究，但这些模型受到512个token的最大限制。这使得在实际应用中处理长输入变得非常困难。 various complex methods 已经被提出来突破这个限制，但最近的研究表明这些模型在不同的分类任务中的效果不如simple baseline。这些复杂的架构在手动编辑的长数据集上进行评估时，与基线相比，表现相当或更差。在这个工作中，我们提出了一种简单扩展BERT架构的方法，称为ChunkBERT。这种方法可以让预训练的BERT模型进行长文本的推理。我们基于token representation的分割和CNN层，使得ChunkBERT与任何预训练BERT模型兼容。我们在一个用于对比不同任务中的长文本分类模型的benchmark上进行了仅用6.25%的原始内存占用来finetune ChunkBERT模型。这些结果表明，可以通过简单地修改预训练BERT模型来实现高效的finetuning和推理。”
</details></li>
</ul>
<hr>
<h2 id="CapsFusion-Rethinking-Image-Text-Data-at-Scale"><a href="#CapsFusion-Rethinking-Image-Text-Data-at-Scale" class="headerlink" title="CapsFusion: Rethinking Image-Text Data at Scale"></a>CapsFusion: Rethinking Image-Text Data at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20550">http://arxiv.org/abs/2310.20550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/CapsFusion">https://github.com/baaivision/CapsFusion</a></li>
<li>paper_authors: Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, Jingjing Liu</li>
<li>for: 提高大型多Modal模型的总体性能和可扩展性</li>
<li>methods: 利用大型语言模型对web上的图片文本对进行整合和优化，以提高模型的性能和可扩展性</li>
<li>results: 对于COCO和NoCaps测试集，CapsFusion caption的性能提高18.8和18.3分，对比基eline的11-16倍更高效，同时具有更深的世界知识和可扩展性优势<details>
<summary>Abstract</summary>
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.
</details>
<details>
<summary>摘要</summary>
大型多Modal模型在零模式下表现出杰出的通用能力，可以完成多种多Modal任务。大规模的网络图像文本对称贡献到这种成功，但受到过度噪音的影响。现有的研究使用由描述模型生成的另外caption，并已经 дости得到了remarkable benchmark表现。然而，我们的实验表明，使用生成的caption会导致Scalability Deficiency和World Knowledge Loss问题，这些问题在初始的benchmark成功后被掩盖了。经过仔细分析，我们发现了这些问题的根本原因是现有的生成caption Language structure too simplistic and lack of knowledge details。为了提供更高质量和更可扩展的多Modal预训练数据，我们提出了CapsFusion，一种高级框架，利用大型语言模型来集成和修正来自网络图像文本对和生成caption的信息。我们的广泛实验表明，CapsFusion caption具有remarkable的全面优异性，包括模型性能（如COCO和NoCaps的CIDEr score提高18.8和18.3）、样本效率（需要11-16倍 menos计算量 чем基eline）、世界知识深度和可扩展性。这些有效性、效率和可扩展性优势，使CapsFusion成为未来LMM训练的优秀候选人。
</details></li>
</ul>
<hr>
<h2 id="LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts"><a href="#LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts" class="headerlink" title="LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"></a>LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20501">http://arxiv.org/abs/2310.20501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Jun Xu</li>
<li>for: 这项研究旨在探讨在大语言模型（LLM）时代的信息检索（IR）系统中，LLM自动生成的文档对IR系统的影响。</li>
<li>methods: 本研究采用了量化的方法进行评估不同的IR模型在人类写作和LLM生成文档的混合enario下的表现。</li>
<li>results: 研究发现，神经检索模型倾向于将LLM生成的文档 ranked高。这种偏好被称为“源偏好”，并且不仅存在于第一阶段神经检索模型中，还扩展到第二阶段神经重新排序模型中。<details>
<summary>Abstract</summary>
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher.We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, we provide an in-depth analysis from the perspective of text compression and observe that neural models can better understand the semantic information of LLM-generated text, which is further substantiated by our theoretical analysis.We also discuss the potential server concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks and codes will later be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
</details>
<details>
<summary>摘要</summary>
近些时间，大语言模型（LLM）的出现对信息检索（IR）应用领域产生了革命性的变革，尤其是在网络搜索方面。LLMs可以生成人类样式的文本，因此在互联网上生成了巨量的文本。这使得IR系统在LLMs时代面临一个新的挑战：索引文档现在不仅由人类生成，还由LLMs生成。这种LLM生成的文本如何影响IR系统是一个 Pressing 和仍未被探索的问题。在这项工作中，我们进行了量化评估不同IR模型在人类写作和LLM生成文本相关的场景中的表现。我们发现，神经检索模型倾向于将LLM生成文本排名在首位。我们称这种偏好为“源偏好”。此外，我们发现这种偏好不仅影响首次神经检索器，还影响第二次神经重新排序器。然后，我们提供了从文本压缩角度进行深入分析，并证明神经模型对LLM生成文本的Semantic信息更好地理解。此外，我们还讨论了由于观察到的源偏好可能会产生的服务器关心，并希望我们的发现可以作为IR社区以及更广泛的人工智能领域的重要警示。为便于未来在LLM时代进行IR探索，我们将制作两个新的benchmark和代码，并将在GitHub上提供，可以在 \url{https://github.com/KID-22/LLM4IR-Bias} 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations"><a href="#A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations" class="headerlink" title="A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations"></a>A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20494">http://arxiv.org/abs/2310.20494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/butterfliesss/sdt">https://github.com/butterfliesss/sdt</a></li>
<li>paper_authors: Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu</li>
<li>for: 这个论文主要关注于 Multimodal Emotion Recognition in Conversations (ERC) 任务，即在对话中识别每句话的情感。</li>
<li>methods: 该论文提出了一种基于 transformer 的模型，使用自适应的卷积神经网络和层次闭合策略来Capture 多modal 信息的交互和学习多modal 模型之间的权重。</li>
<li>results: 实验结果表明，该模型在 IEMOCAP 和 MELD  datasets 上表现出色，超越了之前的基eline。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
情绪识别在对话中 (ERC)，Recognizing the emotion of each utterance in a conversation is crucial for building empathetic machines. Previous studies have focused mainly on capturing context- and speaker-sensitive dependencies on the textual modality, but have ignored the significance of multimodal information. In contrast, our proposed model, which uses a transformer-based architecture with self-distillation (SDT), captures intra- and inter-modal interactions between utterances, learns weights between modalities dynamically, and enhances modal representations. We introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality, and experiments on IEMOCAP and MELD datasets show that SDT outperforms previous state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification"><a href="#Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification" class="headerlink" title="Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification"></a>Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20478">http://arxiv.org/abs/2310.20478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shajalal, Sebastian Denef, Md. Rezaul Karim, Alexander Boden, Gunnar Stevens</li>
<li>for: 本研究旨在提出一种可解释的多类别专利分类框架，以提高人类专家对复杂的AI技术的理解和管理。</li>
<li>methods: 本研究使用了深度神经网络（DNN），并引入层 wise relevance propagation（LRP）以提供人类可理解的解释。</li>
<li>results: 实验结果表明，使用了不同的DNN模型（Bi-LSTM、CNN和CNN-BiLSTM），并对每个预测进行了层 wise relevance propagation，可以生成高效的解释。<details>
<summary>Abstract</summary>
Recent technological advancements have led to a large number of patents in a diverse range of domains, making it challenging for human experts to analyze and manage. State-of-the-art methods for multi-label patent classification rely on deep neural networks (DNNs), which are complex and often considered black-boxes due to their opaque decision-making processes. In this paper, we propose a novel deep explainable patent classification framework by introducing layer-wise relevance propagation (LRP) to provide human-understandable explanations for predictions. We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details>
<details>
<summary>摘要</summary>
We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable.Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting"><a href="#Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting" class="headerlink" title="Global Transformer Architecture for Indoor Room Temperature Forecasting"></a>Global Transformer Architecture for Indoor Room Temperature Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20476">http://arxiv.org/abs/2310.20476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfredo V Clemente, Alessandro Nocente, Massimiliano Ruocco</li>
<li>for: 降低建筑物能源消耗和气候变化associated with HVAC systems, 优化能源consumption和绿色气体排放</li>
<li>methods: 使用Global Transformer架构进行室内温度预测, 利用深度学习模型实现更高精度的预测</li>
<li>results: 提高预测性能, 简化部署和维护, 为建筑业中的能源优化和气候变化做出贡献<details>
<summary>Abstract</summary>
A thorough regulation of building energy systems translates in relevant energy savings and in a better comfort for the occupants. Algorithms to predict the thermal state of a building on a certain time horizon with a good confidence are essential for the implementation of effective control systems. This work presents a global Transformer architecture for indoor temperature forecasting in multi-room buildings, aiming at optimizing energy consumption and reducing greenhouse gas emissions associated with HVAC systems. Recent advancements in deep learning have enabled the development of more sophisticated forecasting models compared to traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset encompassing all rooms, eliminating the need for multiple room-specific models, significantly improving predictive performance, and simplifying deployment and maintenance. Notably, this study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings. The proposed approach provides a novel solution to enhance the accuracy and efficiency of temperature forecasting, serving as a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning have made it possible to develop more sophisticated forecasting models than traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset, eliminating the need for multiple room-specific models and significantly improving predictive performance. This study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings, providing a novel solution to enhance accuracy and efficiency in temperature forecasting. This approach can be a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details></li>
</ul>
<hr>
<h2 id="Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph"><a href="#Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph" class="headerlink" title="Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph"></a>Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20475">http://arxiv.org/abs/2310.20475</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlamprecht/linkedpaperswithcode">https://github.com/davidlamprecht/linkedpaperswithcode</a></li>
<li>paper_authors: Michael Färber, David Lamprecht</li>
<li>for: 本文提供了一个名为Linked Papers With Code（LPWC）的RDF知识图，该图包含了大约400,000篇机器学习研讨文献的完整、当前信息。</li>
<li>methods: 本文使用了RDF格式将最新的机器学习研究翻译成RDF格式，并允许科学影响评估和学术关键内容推荐等新方法。</li>
<li>results: LPWC可以帮助科学家和研究人员快速地获得机器学习领域的最新研究情况和结果。<details>
<summary>Abstract</summary>
In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC not only translates the latest advancements in machine learning into RDF format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. LPWC is openly accessible at https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple formats, from RDF dump files to a SPARQL endpoint for direct web queries, as well as a data source with resolvable URIs and links to the data sources SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph embeddings, enabling LPWC to be readily applied in machine learning applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 Linked Papers With Code（LPWC），一个基于 RDF 知识图谱，提供了关于大约 400,000 篇机器学习论文的全面、当前信息。这包括论文中所 Addressed 的任务、使用的数据集、实施的方法、以及进行的评估结果。相比其非 RDF-based 对手 Papers With Code，LPWC 不仅将最新的机器学习成果翻译成 RDF 格式，还允许 novel 的科学影响量量化和学术关键内容推荐。LPWC 公开访问可以在 <https://linkedpaperswithcode.com> 上找到，并且采用 CC-BY-SA 4.0 许可证。作为 Linked Open Data 云中的知识图谱，我们提供了 LPWC 在多种格式，从 RDF 填充文件到 SPARQL 终端进行直接网络查询，以及一个具有可解析 URI 和链接到数据源 SemOpenAlex、Wikidata 和 DBLP 的数据源。此外，我们还提供了知识图谱嵌入，使 LPWC 可以轻松应用于机器学习应用程序中。
</details></li>
</ul>
<hr>
<h2 id="Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot"><a href="#Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot" class="headerlink" title="Critical Role of Artificially Intelligent Conversational Chatbot"></a>Critical Role of Artificially Intelligent Conversational Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20474">http://arxiv.org/abs/2310.20474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seraj A. M. Mostafa, Md Z. Islam, Mohammad Z. Islam, Fairose Jeehan, Saujanna Jafreen, Raihan U. Islam</li>
<li>for: 本研究探讨了 chatGPT 在学术上的伦理问题和限制，以及用户群体可能的不当使用。</li>
<li>methods: 本研究采用了许多情况的伦理分析和技术方案，以探讨 chatGPT 的可能的不当使用和限制。</li>
<li>results: 研究发现了一些可能的伦理问题和限制，以及一些技术方案可以避免和解决这些问题。<details>
<summary>Abstract</summary>
Artificially intelligent chatbot, such as ChatGPT, represents a recent and powerful advancement in the AI domain. Users prefer them for obtaining quick and precise answers, avoiding the usual hassle of clicking through multiple links in traditional searches. ChatGPT's conversational approach makes it comfortable and accessible for finding answers quickly and in an organized manner. However, it is important to note that these chatbots have limitations, especially in terms of providing accurate answers as well as ethical concerns. In this study, we explore various scenarios involving ChatGPT's ethical implications within academic contexts, its limitations, and the potential misuse by specific user groups. To address these challenges, we propose architectural solutions aimed at preventing inappropriate use and promoting responsible AI interactions.
</details>
<details>
<summary>摘要</summary>
人工智能聊天机器人，如ChatGPT，是现代人工智能领域的一项新和强大的进步。用户喜欢使用它们以获取快速和准确的答案，而不需要 clicks through多个链接的传统搜索方式。ChatGPT的对话方式使其易于使用，且能够有组织的方式提供答案。然而，这些聊天机器人存在限制，特别是在提供准确答案以及伦理问题方面。在这篇研究中，我们探讨了ChatGPT在学术上下文中的伦理问题，以及它们的限制，以及特定用户群体可能会滥用它们的可能性。为解决这些挑战，我们提议了一些建筑解决方案，以防止不当使用并促进负责任的人工智能交互。
</details></li>
</ul>
<hr>
<h2 id="ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology"><a href="#ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology" class="headerlink" title="ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology"></a>ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20467">http://arxiv.org/abs/2310.20467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Tang, Frank Guerin, Chenghua Lin</li>
<li>for: 提高研究人员对ACL Anthology中文章的访问和管理效率，帮助他们更好地找到和组织相关的文献。</li>
<li>methods: 提供了一个名为“ACL Anthology Helper”的工具，可以自动将ACL Anthology中的文章和相关元信息解析并存储在本地MySQL数据库中，从而提供了许多操作，如“where”、“group”、“order”等，以便进行Targeted和高效的文献检索。</li>
<li>results: 该工具已经成功地应用于写作一篇评介文章（Tang et al.,2022a），并且可以帮助研究人员更好地找到和组织ACL Anthology中的相关文献，提高了他们的研究效率。<details>
<summary>Abstract</summary>
The ACL Anthology is an online repository that serves as a comprehensive collection of publications in the field of natural language processing (NLP) and computational linguistics (CL). This paper presents a tool called ``ACL Anthology Helper''. It automates the process of parsing and downloading papers along with their meta-information, which are then stored in a local MySQL database. This allows for efficient management of the local papers using a wide range of operations, including "where," "group," "order," and more. By providing over 20 operations, this tool significantly enhances the retrieval of literature based on specific conditions. Notably, this tool has been successfully utilised in writing a survey paper (Tang et al.,2022a). By introducing the ACL Anthology Helper, we aim to enhance researchers' ability to effectively access and organise literature from the ACL Anthology. This tool offers a convenient solution for researchers seeking to explore the ACL Anthology's vast collection of publications while allowing for more targeted and efficient literature retrieval.
</details>
<details>
<summary>摘要</summary>
ACL Anthology是一个在线存储库，用于收集自然语言处理（NLP）和计算语言学（CL）领域的论文。这篇论文介绍了一种名为“ACL Anthology Helper”的工具。该工具可自动将ACL Anthology中的论文和相关信息解析并下载到本地MySQL数据库中，以便高效管理本地论文。这些操作包括“where”、“group”、“order”等多种操作，可以根据特定条件进行高效的文献检索。值得一提的是，这个工具已经成功应用于 Tang et al. (2022a) 的一篇论文中。通过引入ACL Anthology Helper，我们希望提高研究人员对ACL Anthology中文献的访问和管理能力，以便更加高效地检索和组织相关文献。这个工具提供了一种便捷的解决方案，帮助研究人员更好地探索ACL Anthology的庞大文献收藏，并实现更加targeted和高效的文献检索。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks"><a href="#Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks" class="headerlink" title="Interpretable Neural PDE Solvers using Symbolic Frameworks"></a>Interpretable Neural PDE Solvers using Symbolic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20463">http://arxiv.org/abs/2310.20463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 这篇论文旨在解决深度学习中的解释性问题，即使用符号框架（如符号回归）来帮助解释神经网络模型的决策机理。</li>
<li>methods: 这篇论文提出了将符号框架纳入神经网络模型中，以提高神经网络模型的解释性。</li>
<li>results: 该方法可以帮助提高神经网络模型的解释性，使其更加可读性和可理解性。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) are ubiquitous in the world around us, modelling phenomena from heat and sound to quantum systems. Recent advances in deep learning have resulted in the development of powerful neural solvers; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. In this context, a notable gap in current research is the integration of symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details>
<details>
<summary>摘要</summary>
In simplified Chinese, the text might be translated as:partial differential equations (PDEs) 在我们周围的世界中 ubique, 模拟了各种现象, 如热和声, 以及量子系统。 Recent advances in deep learning 已经导致了 poderoso neural solvers 的发展; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. Currently, there is a notable gap in current research: integrating symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details></li>
</ul>
<hr>
<h2 id="AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms"><a href="#AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms" class="headerlink" title="AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms"></a>AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20452">http://arxiv.org/abs/2310.20452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rustem Islamov, Mher Safaryan, Dan Alistarh</li>
<li>for: 这个论文是关于分布式SGD在异步 режи度下的研究，具体来说是在不同工作者的计算和通信速度以及数据分布情况下。</li>
<li>methods: 这个论文使用了异步类型的算法，其中每个工作者在一些历史上的某个迭代阶段就计算了自己的本地数据的可能不准确和随机梯度，然后将这些梯度返回给服务器而无需与其他工作者同步。</li>
<li>results: 论文提出了一种统一的征识理论，用于非对称函数的异步SGD在不同速度和数据分布情况下的收敛性。这种方法还可以用于证明SGD的其他修改版本的收敛性。数学分析表明，这些修改版本的收敛速率受到哪些因素的影响，并且可以通过哪些方法提高性能。此外，论文还提出了一种新的异步方法，基于工作者的排序。实验结果支持理论结论，并证明了这种方法在实际应用中的良好性能。<details>
<summary>Abstract</summary>
We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method.
</details>
<details>
<summary>摘要</summary>
我们分析异步类算法在异步设置下进行分布式SGD，其中每个工作者有自己的计算和通信速度，以及数据分布。在这些算法中，工作者在某个过去的迭代中计算并返回本地数据相对于历史的可能偏移和抽象的梯度。我们提出一种统一的收敛理论，用于非对称凸函数在异步 режи困难的情况下。我们的分析显示了迭代的收敛率，并且解释了影响收敛率的因素，以及如何提高异步算法的性能。特别是，我们介绍了一种新的异步方法，基于工作者洗牌。我们的分析也证明了SGD WITH RANDOM RESHUFFLING和SHUFFLE-ONCE MINI-BATCH SGD等算法的收敛性。得到的收敛率与最佳结果匹配，表明我们的方法的紧致性。最后，我们的数值评估表明了我们的方法在实际应用中的良好性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks"><a href="#Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks" class="headerlink" title="Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks"></a>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20447">http://arxiv.org/abs/2310.20447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Adriaensen, Herilalaina Rakotoarison, Samuel Müller, Frank Hutter</li>
<li>for: 预测模型在训练后期的性能，基于早期训练期间的性能。</li>
<li>methods: 使用 Bayesian方法，但现有方法存在一定的限制和计算成本问题。</li>
<li>results: 提出了首次应用先进数据预测网络（PFN）在这个预测中，PFN通过单个前进 pass来进行粗略抽象抽象 Bayesian推理。 Comparing with MCMC, LC-PFN可以更准确地 aproximate posterior predictive distribution，而且速度比MCMC快上万倍。同时，LC-PFN可以在多种模型architecture和输入数据Modalities上实现类似的性能。最后， investigate its potential in the context of model selection, and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.<details>
<summary>Abstract</summary>
Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs. In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.
</details>
<details>
<summary>摘要</summary>
学习曲线拟合目标是预测训练过程中后期模型表现，基于早期表现。在这种工作中，我们认为，由于拟合学习曲线的内在不确定性，需要采用 bayesian 方法。然而，现有方法具有以下两点不足：（i）过于限制性，和/或（ii） computationally  expensive。我们描述了首次应用 priors 数据适应神经网络（PFN）在这种上下文中。PFN 是一种 transformer ，在数据生成自 prior 的基础上，进行 Approximate Bayesian Inference 的单个前进 pass。我们提出了 LC-PFN，一个基于 MCMC 生成的1000万个人造右缺学习曲线，PFN 在这些学习曲线上进行拟合。我们示出，LC-PFN 可以更准确地 aproximate posterior predictive distribution，并且比 MCMC 快速得多（大约10000 倍）。此外，我们还证明了同一个 LC-PFN 可以在4个学习曲线准 benchmark（LCBench、NAS-Bench-201、Taskset 和 PD1）中拟合总共20000个真实学习曲线，这些学习曲线来自于训练多种模型结构（MLPs、CNNs、RNNs 和 Transformers）在53个不同的数据集（表格、图像、文本和蛋白质数据）上。最后，我们探讨了它在模型选择上的潜在应用，并发现一个简单的 LC-PFN 基本预测早期停止 criterion 可以在45个数据集上获得2-6倍的速度提升，而且几乎没有额外负担。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications"><a href="#Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications" class="headerlink" title="Analyzing the Impact of Companies on AI Research Based on Publications"></a>Analyzing the Impact of Companies on AI Research Based on Publications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20444">http://arxiv.org/abs/2310.20444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LazaTabax/AI-Impact-Scientometrics">https://github.com/LazaTabax/AI-Impact-Scientometrics</a></li>
<li>paper_authors: Michael Färber, Lazaros Tampakis</li>
<li>for: This paper aims to measure the influence of companies on AI research through scientific publishing activities.</li>
<li>methods: The authors compare academic- and company-authored AI publications published in the last decade, using scientometric data from multiple scholarly databases to identify differences and top contributing organizations.</li>
<li>results: The authors find that publications with company participation receive higher citation counts and more online attention, but also note that the vast majority of publications are still produced by academia. They provide recommendations to safeguard a harmonious balance between academia and industry in AI research.Here are the three key information points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是通过科学出版活动来衡量企业对人工智能研究的影响。</li>
<li>methods: 作者们通过比较过去十年的学术期刊文章，使用多个学术数据库的科学指标来描述不同的组织和个人的贡献。</li>
<li>results: 作者们发现，与企业参与有关的文章 receiving higher citation counts和更多的在线关注，但也注意到大多数文章仍然是由学术界出版。他们提供了保持学术界和产业之间和谐协作的建议。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is one of the most momentous technologies of our time. Thus, it is of major importance to know which stakeholders influence AI research. Besides researchers at universities and colleges, researchers in companies have hardly been considered in this context. In this article, we consider how the influence of companies on AI research can be made measurable on the basis of scientific publishing activities. We compare academic- and company-authored AI publications published in the last decade and use scientometric data from multiple scholarly databases to look for differences across these groups and to disclose the top contributing organizations. While the vast majority of publications is still produced by academia, we find that the citation count an individual publication receives is significantly higher when it is (co-)authored by a company. Furthermore, using a variety of altmetric indicators, we notice that publications with company participation receive considerably more attention online. Finally, we place our analysis results in a broader context and present targeted recommendations to safeguard a harmonious balance between academia and industry in the realm of AI research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines"><a href="#Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines" class="headerlink" title="Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines"></a>Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20443">http://arxiv.org/abs/2310.20443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Björn Schembera, Frank Wübbeling, Hendrik Kleikamp, Christine Biedinger, Jochen Fiedler, Marco Reidelbach, Aurela Shehu, Burkhard Schmidt, Thomas Koprucki, Dorothea Iglezakis, Dominik Göddeke</li>
<li>for: 这篇论文是为了推动数学研究数据的可重用性和可访问性而写的。</li>
<li>methods: 论文使用了ontology和知识图来增强数学研究数据的Semantic Web表示，并将数学模型和数值算法的知识映射到ontology中。</li>
<li>results: 通过使用ontology和知识图，论文能够准确地表示数学研究数据的含义，从而提高数学研究数据的可重用性和可访问性。<details>
<summary>Abstract</summary>
In applied mathematics and related disciplines, the modeling-simulation-optimization workflow is a prominent scheme, with mathematical models and numerical algorithms playing a crucial role. For these types of mathematical research data, the Mathematical Research Data Initiative has developed, merged and implemented ontologies and knowledge graphs. This contributes to making mathematical research data FAIR by introducing semantic technology and documenting the mathematical foundations accordingly. Using the concrete example of microfracture analysis of porous media, it is shown how the knowledge of the underlying mathematical model and the corresponding numerical algorithms for its solution can be represented by the ontologies.
</details>
<details>
<summary>摘要</summary>
在应用数学和相关领域，模拟优化工作流程是一个非常重要的方案，数学模型和数值算法在这些数学研究数据中扮演着关键角色。为了使数学研究数据变得可重用，数学研究数据Initiave已经开发、合并和实施了ontologies和知识图。这有助于使数学研究数据变得FAIR，通过使用semantic技术和文档数学基础 accordingly。使用微裂隙分析含气体的具体例子，示出了ontologies可以表示数学模型的知识和相应的数值算法的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation"><a href="#Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation" class="headerlink" title="Raising the ClaSS of Streaming Time Series Segmentation"></a>Raising the ClaSS of Streaming Time Series Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20431">http://arxiv.org/abs/2310.20431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ermshaua/classification-score-stream">https://github.com/ermshaua/classification-score-stream</a></li>
<li>paper_authors: Arik Ermshaus, Patrick Schäfer, Ulf Leser</li>
<li>for: 本研究旨在开发一种高效、准确的流处理时间序列分 segmentation（STSS）算法，用于处理高频流处理数据，以捕捉进程或实体的状态变化。</li>
<li>methods: 本研究使用了一种新的自助学习时间序列分类方法，并应用了统计测试来检测状态变化点（CP）。</li>
<li>results: 实验结果表明，ClaSS算法在两个大标准套件和六个实际数据库中表现了更高的精度，与八种现有竞争算法相比。其空间和时间复杂度独立于分 segment size，线性增长只有滑动窗口大小。 ClaSS算法还被实现为Apache Flink流处理引擎的窗口运算符，平均处理速度为538个数据点每秒。<details>
<summary>Abstract</summary>
Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, we found ClaSS to be significantly more precise than eight state-of-the-art competitors. Its space and time complexity is independent of segment sizes and linear only in the sliding window size. We also provide ClaSS as a window operator with an average throughput of 538 data points per second for the Apache Flink streaming engine.
</details>
<details>
<summary>摘要</summary>
今天的普遍感知器（ubiquitous sensors） emit high frequency流量数字量测量，这些量测reflect了人类、动物、工业、商业和自然过程的属性。这些过程的变化，例如由外部事件或内部状态变化引起的变化，会在记录的信号中manifest。流时序段分（STSS）的任务是将流分成连续的变量大小分割，这些分割对应于观察过程或实体的状态。这个分割操作必须在输入信号频率上能够 coping 。我们介绍了一种新的、高效、高精度的流时序段分算法（ClaSS）。ClaSS 使用自我超级时间序列分类来评估分割的一致性，并运用统计测试来检测显著的变化点（CPs）。在我们使用两个大的 referential和六个实际数据存档进行实验evaluation 中，我们发现ClaSS 与八种现有的竞争者相比，更加精确。其空间和时间复杂度是独立于分割大小和线性增长。我们还提供了 ClaSS 作为窗口运算符，其均衡throughput 为 Apache Flink 流动引擎中的538个数据点/秒。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-for-Multi-View-Visuomotor-Systems"><a href="#Meta-Learning-for-Multi-View-Visuomotor-Systems" class="headerlink" title="Meta Learning for Multi-View Visuomotor Systems"></a>Meta Learning for Multi-View Visuomotor Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20414">http://arxiv.org/abs/2310.20414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benji Alwis, Nick Pears, Pengcheng Liu</li>
<li>for: 这篇论文旨在快速适应机器人多视角系统各种相机配置的变化，并且使用meta-学习精确地调整感知网络，保持政策网络不变。</li>
<li>methods: 这篇论文使用meta-学习来精确地调整感知网络，以减少新的训练集训练集的数量，并且保持政策网络不变。</li>
<li>results: 实验结果显示，这篇论文可以快速适应机器人多视角系统各种相机配置的变化，并且可以获得基准性能。<details>
<summary>Abstract</summary>
This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking"><a href="#Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking" class="headerlink" title="Multi-Base Station Cooperative Sensing with AI-Aided Tracking"></a>Multi-Base Station Cooperative Sensing with AI-Aided Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20403">http://arxiv.org/abs/2310.20403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Favarelli, Elisabetta Matricardi, Lorenzo Pucci, Enrico Paolini, Wen Xu, Andrea Giorgetti</li>
<li>for: 这个研究旨在提高 JOINT SENSING AND COMMUNICATION（JSC）网络中的监测性能，包括多个基站（BS）在抽象中心（FC）之间进行信息交换，以便同时进行监测环境和建立用户设备（UE）之间的通信链接。</li>
<li>methods: 每个BS在网络中 acts as a monostatic radar system，可以全面扫描监测区域，生成范围角图，提供监测区域中目标的位置信息。图像被FC进行拟合，然后使用卷积神经网络（CNN）来推断目标类别，并使用适应分组算法来更有效地组合探测来自同一个目标的检测。最后，使用概率 гипотез浓度筛选器（PHD）和多 Bernoulli 混合（MBM）筛选器来估算目标的状态。</li>
<li>results: 数值结果表明，我们的框架可以提供出色的监测性能，实现距离估计在 60 cm 以下，同时保持与 UE 之间的通信服务，减少了通信负荷在 10% 到 20% 之间。研究还表明，在特定的案例研究中，使用 3 个BS进行监测可以 garantía 的 Localization 错误在 1 m 以下。<details>
<summary>Abstract</summary>
In this work, we investigate the performance of a joint sensing and communication (JSC) network consisting of multiple base stations (BSs) that cooperate through a fusion center (FC) to exchange information about the sensed environment while concurrently establishing communication links with a set of user equipments (UEs). Each BS within the network operates as a monostatic radar system, enabling comprehensive scanning of the monitored area and generating range-angle maps that provide information regarding the position of a group of heterogeneous objects. The acquired maps are subsequently fused in the FC. Then, a convolutional neural network (CNN) is employed to infer the category of the targets, e.g., pedestrians or vehicles, and such information is exploited by an adaptive clustering algorithm to group the detections originating from the same target more effectively. Finally, two multi-target tracking algorithms, the probability hypothesis density (PHD) filter and multi-Bernoulli mixture (MBM) filter, are applied to estimate the state of the targets. Numerical results demonstrated that our framework could provide remarkable sensing performance, achieving an optimal sub-pattern assignment (OSPA) less than 60 cm, while keeping communication services to UEs with a reduction of the communication capacity in the order of 10% to 20%. The impact of the number of BSs engaged in sensing is also examined, and we show that in the specific case study, 3 BSs ensure a localization error below 1 m.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了一个共同感知和通信（JSC）网络，该网络由多个基站（BS）组成，通过协调中心（FC）进行信息交换，以便在同时建立用户设备（UE）与网络的通信连接。每个BS作为单static雷达系统，可以全面扫描监测区域，生成距离角度图，提供目标的位置信息。获取的图像被后续的卷积神经网络（CNN）进行推理，并将推理结果用adaptive clustering算法进行分组。最后，我们使用了多目标跟踪算法（PHD滤波器和多 Bernoulli 混合（MBM）滤波器）来估算目标的状态。我们的框架可以提供出色的感知性能，达到优化的子模式分配（OSPA）小于60cm，并保持与UE的通信服务的减少，在10%到20%之间。我们还研究了BS参与感知的数量的影响，并发现在特定的案例研究中，3个BS可以 garantuee localization error below 1m。
</details></li>
</ul>
<hr>
<h2 id="Utilitarian-Algorithm-Configuration"><a href="#Utilitarian-Algorithm-Configuration" class="headerlink" title="Utilitarian Algorithm Configuration"></a>Utilitarian Algorithm Configuration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20401">http://arxiv.org/abs/2310.20401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drgrhm/utilitarian-ac">https://github.com/drgrhm/utilitarian-ac</a></li>
<li>paper_authors: Devon R. Graham, Kevin Leyton-Brown, Tim Roughgarden</li>
<li>for: 这个论文旨在配置启发式算法以提高它们对用户提供的价值，同时提供理论保证。</li>
<li>methods: 论文使用的方法包括：建立Utilitarian目标，提供有效和理论上正确的配置过程，并对这些过程进行分析和实验验证。</li>
<li>results: 论文的研究结果包括：提供了一种新的配置方法，可以帮助算法设计者在实际应用中做出更好的选择，同时提供了理论上的保证。这种方法可以在实际应用中提供更高的性能，并且可以在不同的情况下进行适应。<details>
<summary>Abstract</summary>
We present the first nontrivial procedure for configuring heuristic algorithms to maximize the utility provided to their end users while also offering theoretical guarantees about performance. Existing procedures seek configurations that minimize expected runtime. However, very recent theoretical work argues that expected runtime minimization fails to capture algorithm designers' preferences. Here we show that the utilitarian objective also confers significant algorithmic benefits. Intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details>
<details>
<summary>摘要</summary>
我们提出了首个不同实际方法来配置假设算法以最大化它们的终端用户所提供的用途，同时提供理论上的保证。现有的方法寻求最小化预期所需的时间。然而，非常最近的理论工作表明，预期时间最小化无法捕捉算法设计师的偏好。我们显示了Utilitarian目标也具有重要的算法优点。 intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details></li>
</ul>
<hr>
<h2 id="Do-large-language-models-solve-verbal-analogies-like-children-do"><a href="#Do-large-language-models-solve-verbal-analogies-like-children-do" class="headerlink" title="Do large language models solve verbal analogies like children do?"></a>Do large language models solve verbal analogies like children do?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20384">http://arxiv.org/abs/2310.20384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms">https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms</a></li>
<li>paper_authors: Claire E. Stevenson, Mathilde ter Veen, Rochelle Choenni, Han L. J. van der Maas, Ekaterina Shutova</li>
<li>for:  investigate whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do.</li>
<li>methods: used verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year olds from the Netherlands solved 622 analogies in Dutch.</li>
<li>results: the six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, and XLM-V and GPT-3 the best, but when controlling for associative processes, each model’s performance level drops 1-2 years.<details>
<summary>Abstract</summary>
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.
</details>
<details>
<summary>摘要</summary>
人类认知的核心是比喻创造。大人解决比喻问题，如“匹 belongs to 牧场 like 鸡 belongs to ...?”，通过将关系（如“被保管”）映射到答案，如“鸡巢”。然而，孩子们通常使用关联，例如回答“蛋”。这篇论文研究了大型自然语言模型（LLMs）是否使用关联来解决A:B::C:?的语言比喻。我们使用来自在线适应式学习环境的622个荷兰7-12岁儿童解决的语言比喻，并测试了六个荷兰单语言和多语言LLMs。结果显示，这六个模型在与孩子们的性能水平相当，MGPT表现最差，约等于7岁儿童水平，而XLM-V和GPT-3表现最好，略高于11岁儿童水平。然而，当我们控制了关联过程时，每个模型的表现水平下降1-2年。进一步的实验表明，关联过程经常在正确解决比喻问题中发挥作用。我们结论，我们测试的LLMs实际上都是通过关联来解决语言比喻的，与孩子们一样。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging"><a href="#A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging" class="headerlink" title="A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging"></a>A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20381">http://arxiv.org/abs/2310.20381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou</li>
<li>for:  This paper evaluates GPT-4V’s capabilities in medical imaging tasks, including Radiology Report Generation, Medical VQA, and Visual Grounding.</li>
<li>methods:  The paper uses publicly available benchmarks to evaluate GPT-4V’s performance in these tasks, and the evaluation highlights areas for improvement.</li>
<li>results:  GPT-4V demonstrates potential in generating descriptive reports for chest X-ray images, but its performance is less accurate in certain evaluation metrics like CIDEr for Medical VQA and lacks precision in identifying specific medical organs and signs in Visual Grounding.Here is the same information in Simplified Chinese:</li>
<li>for:  This paper evaluates GPT-4V的应用在医疗影像任务中，包括医学报告生成、医学问题回答和视觉基础。</li>
<li>methods:  This paper使用公共可用的benchmark来评估GPT-4V的表现在这些任务中，而评估点出了一些需要改进的地方。</li>
<li>results:  GPT-4V在胸部X射影像描述报告中表现出了潜力，但在certain evaluation metric like CIDEr中的表现不够精确，而在视觉基础任务中的精度也有待改进。<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of more semantically robust assessment methods. In the field of Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding boxes, but its precision is lacking, especially in identifying specific medical organs and signs. Our evaluation underscores the significant potential of GPT-4V in the medical imaging domain, while also emphasizing the need for targeted refinements to fully unlock its capabilities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs"><a href="#A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs" class="headerlink" title="A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs"></a>A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20367">http://arxiv.org/abs/2310.20367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Michalakopoulos, Elissaios Sarmas, Ioannis Papias, Panagiotis Skaloumpakas, Vangelis Marinakis, Haris Doukas<br>for:This paper aims to identify the most suitable consumer clusters with similar energy consumption behaviors using smart meter data.methods:The authors use four widely used clustering algorithms (K-means, K-medoids, Hierarchical Agglomerative Clustering, and Density-based Spatial Clustering) and empirical analysis to assess the algorithms. They also redefine the problem as a probabilistic classification one and use Explainable AI (xAI) to enhance interpretability.results:The optimal number of clusters for this case is seven, but two of the clusters exhibit significant internal dissimilarity and are split further into nine clusters. The method is scalable and versatile, making it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.<details>
<summary>Abstract</summary>
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10\% of the dataset, exhibit significant internal dissimilarity and thus it splits them even further to create nine clusters in total. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>智能仪器数据中的荷形loads频繁用于分析每天能源消耗模式，尤其在应用程序 like Demand Response (DR) 中。然而，最重要的挑战在于确定最适合的消耗者群集，具有相似的消耗行为。在这篇论文中，我们提出了一种基于机器学习的框架，通过实际案例研究，使用伦敦 almost 5000 户数据。我们运用了四种常用的划分算法，即 K-means、K-medoids、 Hierarchical Agglomerative Clustering 和 Density-based Spatial Clustering。通过实验分析以及多个评价指标，我们评估了这些算法。然后，我们将问题重定义为一个 probabilistic 类别问题，类ifier 模拟划分算法的行为，使用 Explainable AI (xAI) 提高解释性。根据划分算法分析，这个案例的优化数量为七。尽管如此，我们的方法显示，这两个群集，数据集中约 10%，具有显著的内部不一致，因此我们将其进一步分割，创建九个群集。我们的解决方案具有扩展性和多样性，使得电力公司可以更好地对用户进行分类，为创建更加targeted的 Demand Response 计划。
</details></li>
</ul>
<hr>
<h2 id="Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory"><a href="#Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory" class="headerlink" title="Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory"></a>Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20360">http://arxiv.org/abs/2310.20360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/introdeeplearning/book">https://github.com/introdeeplearning/book</a></li>
<li>paper_authors: Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</li>
<li>for: 本书的目的是提供深度学习算法的基础教程。</li>
<li>methods: 本书详细介绍了深度学习算法的基本组成部分，包括不同的人工神经网络架构（如全连接Feedforward ANNs、 convolutional ANNs、 recurrent ANNs、 residual ANNs、批量normalization ANNs）和不同的优化算法（如基本的随机梯度下降方法、加速方法和自适应方法）。</li>
<li>results: 本书还涵盖了深度学习算法的一些理论方面，包括人工神经网络的拟合能力（包括神经网络的 calculus）、优化理论（包括Kurdyka-{\L}ojasiewicz不等式）和泛化错误。 最后一部分的书评论了深度学习方法的泛化方法，包括物理学习神经网络（PINNs）和深度加尔各答方法。<details>
<summary>Abstract</summary>
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.
</details>
<details>
<summary>摘要</summary>
The book will cover the following topics:* Different ANN architectures, including fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization* Optimization algorithms, including the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods* Theoretical aspects of deep learning algorithms, including approximation capacities of ANNs, optimization theory, and generalization errors* Deep learning approximation methods for partial differential equations (PDEs), including physics-informed neural networks (PINNs) and deep Galerkin methods.The book is intended to provide a comprehensive introduction to deep learning algorithms, with a focus on mathematical details and theoretical understanding. It will be a valuable resource for students, scientists, and practitioners who want to gain a deeper understanding of this rapidly evolving field.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model"><a href="#Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model" class="headerlink" title="Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model"></a>Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20357">http://arxiv.org/abs/2310.20357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Zhao, Zhenyu Li, Zhi Jin, Feng Zhang, Haiyan Zhao, Chengfeng Dou, Zhengwei Tao, Xinhai Xu, Donghong Liu</li>
<li>for: 提高大型语言模型（MLLM）的空间意识能力，以满足自动驾驶、智能医疗、 робо扮演、虚拟和增强现实等行业的需求。</li>
<li>methods: 使用更精确的物体之间空间位置信息来导引MLLM提供更 precis的回答。利用几何空间信息和场景图来获取相关的几何空间信息和场景细节。</li>
<li>results: 经验表明，提出的方法能够有效地提高MLLM的空间意识能力，并在多模态大语言模型中进行空间意识相关任务时达到更高的精度。<details>
<summary>Abstract</summary>
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. Extensive experiments were conducted in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型（MLLM）指的是在大语言模型（LLM）基础上增加了接收和推理多模式数据的能力。在这些多模式数据中，空间意识是关键的一个能力，涵盖了对物品之间和场景区域之间的物品理解的多种技能。自动驾驶、智能医疗、机器人、虚拟和增强现实等行业都有极高的需求于 MLLM 的空间意识能力。然而，目前 MLLM 的空间意识能力和人类需求之间存在显著的差距。为了解决这个问题，这篇论文提出了使用更加精确的物体之间的空间位置信息来导引 MLLM 在用户提出的 queries 中更加准确地回答空间意识相关的问题。具体来说，为某个多模式任务，我们使用了获取 geometric 空间信息和场景图来获得相关的 geometric 空间信息和场景细节。然后，根据这些信息，我们向 MLLM 提供空间意识相关的 queries。我们在 MME、MM-Vet 和其他多模式大语言模型的 benchmark 中进行了广泛的实验，并取得了证明方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Muscle-volume-quantification-guiding-transformers-with-anatomical-priors"><a href="#Muscle-volume-quantification-guiding-transformers-with-anatomical-priors" class="headerlink" title="Muscle volume quantification: guiding transformers with anatomical priors"></a>Muscle volume quantification: guiding transformers with anatomical priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20355">http://arxiv.org/abs/2310.20355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louise Piecuch, Vanessa Gonzales Duque, Aurélie Sarcher, Enzo Hollville, Antoine Nordez, Giuseppe Rabita, Gaël Guilhem, Diana Mateus</li>
<li>for: 这篇论文是为了提供一种自动 segmentation 技术，帮助在运动和类型疾病的追踪中量化肌肉量和形状。</li>
<li>methods: 这篇论文提出了一个混合构造的分类模型，结合了卷积和视觉对应块，以帮助解决肌肉分类中的挑战。</li>
<li>results: 实验结果显示，这种混合模型可以从相对较小的数据库中训练出高精度的肌肉分类模型，并且遵循体内关系规律可以提高预测的精度。<details>
<summary>Abstract</summary>
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hybrid architectures in the context of muscle segmentation for shape analysis. Considering the consistent anatomical muscle configuration, we rely on transformer blocks to capture the longrange relations between the muscles. To further exploit the anatomical priors, a second contribution of this work consists in adding a regularisation loss based on an adjacency matrix of plausible muscle neighbourhoods estimated from the training data. Our experimental results on a unique database of elite athletes show it is possible to train complex hybrid models from a relatively small database of large volumes, while the anatomical prior regularisation favours better predictions.
</details>
<details>
<summary>摘要</summary>
筋体积是运动健康领域中有用的量化生物标志，同时也适用于追踪萎缩骨骼疾病。除筋体积外，其他形状生物标志也可以从医疗图像中提取。现今，手动分割仍然是量化测量的黄金标准，尽管很时间consuming。我们提议一种自动分割lower limb的18个肌肉的3D磁共振图像，以帮助 morphometric分析。由于不同肌肉组织的组织学性不同，因此分割算法无法仅基于观察到的外观特征。但是，肌肉的边缘很难于检测，而且这些边缘的厚度在不同的个体之间存在差异。为了解决以上挑战，我们提议一种 combining convolutional和视觉转换块的分割方法。我们在 muscle segmentation 领域中首次研究了这种混合架构的行为。由于肌肉的一致性，我们依靠转换块来捕捉肌肉之间的长距离关系。为了进一步利用骨骼的 анатоical prior，我们的第二个贡献是添加一个基于邻居矩阵的准确性损失，该矩阵是通过训练数据来估算的 plausible 肌肉邻居。我们在一个Unique数据库中进行了实验，结果表明可以从相对较小的数据库中训练复杂的混合模型，而且准确性增加。
</details></li>
</ul>
<hr>
<h2 id="Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand"><a href="#Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand" class="headerlink" title="Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand"></a>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20350">http://arxiv.org/abs/2310.20350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand, Berthold Bäuml</li>
<li>for:  grasping objects with limited or no prior knowledge, especially in assistive robotics</li>
<li>methods:  deep learning pipeline with shape completion and grasp predictor modules, using VQDIF and two-stage architecture</li>
<li>results:  successful grasping of a wide range of household objects based on a single depth image, with fast processing time (1 s for shape completion and 0.3 s for generating 1000 grasps)<details>
<summary>Abstract</summary>
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful grasping of a wide range of household objects based on a depth image from a single viewpoint. The whole pipeline is fast, taking only about 1 s for completing the object's shape (0.7 s) and generating 1000 grasps (0.3 s).
</details>
<details>
<summary>摘要</summary>
握住无知物体是助动器学中高度相关的技能。然而，在总体设定下，这问题仍然是开放问题，特别是当对象只有部分可见和多指手部抓取时。我们提出了一种新的、快速、高精度深度学习管道，包括基于单个深度图像的形状完成模块，然后是基于预测对象形状的抓取预测器。形状完成网络基于VQDIF，预测空间占用值在任意查询点。抓取预测器使用我们的两阶段架构，首先使用自动逆 corr 模型生成手势，然后对每个姿势进行指关键点的回归。关键因素包括数据实现和扩展，以及特别对困难情况的特别关注 during training。实验在物理机器人平台上成功抓取了一系列家用物品，基于单个视角深度图像。整个管道快速，只需0.7秒完成对象的形状预测和1000个抓取预测（0.3秒）。
</details></li>
</ul>
<hr>
<h2 id="Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View"><a href="#Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View" class="headerlink" title="Improving Entropy-Based Test-Time Adaptation from a Clustering View"></a>Improving Entropy-Based Test-Time Adaptation from a Clustering View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20327">http://arxiv.org/abs/2310.20327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoliang Lin, Hanjiang Lai, Yan Pan, Jian Yin</li>
<li>for: 本文是关于如何处理预测数据和测试数据之间的域Shift问题，具体来说是使用测试时间适应（TTA）技术来适应模型。</li>
<li>methods: 本文提出了一种新的视角，即将Entropy-Based TTA（EBTTA）方法解释为 clustering 方法，具体来说是一种迭代算法，其中在分配步骤中，使用 EBTTA 模型对测试样本进行标签分配，而在更新步骤中，通过分配的样本更新模型。</li>
<li>results: 实验结果表明，我们的方法可以在多个数据集上减少域Shift问题的影响，并且可以在不同的批处理大小和初始分配情况下保持稳定性。<details>
<summary>Abstract</summary>
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA methods are sensitive to initial assignments, outliers, and batch size. This observation can guide us to put forward the improvement of EBTTA. We propose robust label assignment, weight adjustment, and gradient accumulation to alleviate the above problems. Experimental results demonstrate that our method can achieve consistent improvements on various datasets. Code is provided in the supplementary material.
</details>
<details>
<summary>摘要</summary>
域外 shift 是现实世界中常见的问题，训练数据和测试数据follow不同的数据分布。为解决这个问题，全部测试时间适应（TTA）可以利用测试时间遇到的无标签数据来适应模型。特别是Entropy-Based TTA（EBTTA）方法，通过最小化测试样本上预测的 entropy 来实现成功。在这篇论文中，我们提出了一新的视角，即通过对 EBTTA 方法进行 clustering 的解释。这是一个迭代算法：1）在分配步骤，EBTTA 模型的前进过程是对测试样本进行标签分配；2）在更新步骤，后进程是通过分配的样本更新模型。根据这种解释，我们可以更深入地理解 EBTTA，并证明了在预测 Entropy 损失时，最大概率会进一步增加。因此，我们提出了一种代替解释，即为何现有的 EBTTA 方法受初始分配、异常样本和批处理大小的影响。这一观察可以引导我们提出改进 EBTTA 的方法。我们提出了robust label assignment、重量调整和梯度积累来缓解上述问题。实验结果表明，我们的方法可以在多个 dataset 上实现一致性的改进。代码位于辅助材料中。
</details></li>
</ul>
<hr>
<h2 id="SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues"><a href="#SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues" class="headerlink" title="SemanticBoost: Elevating Motion Generation with Augmented Textual Cues"></a>SemanticBoost: Elevating Motion Generation with Augmented Textual Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20323">http://arxiv.org/abs/2310.20323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blackgold3/SemanticBoost">https://github.com/blackgold3/SemanticBoost</a></li>
<li>paper_authors: Xin He, Shaoli Huang, Xiaohang Zhan, Chao Wen, Ying Shan</li>
<li>for: 本研究的目的是提高现代技术对复杂 semantic description 的动作生成能力，即使数据集中的 semantics 不够、contextual understanding 弱。</li>
<li>methods: 我们提出了 SemanticBoost 框架，它包括 Semantic Enhancement 模块和 Context-Attuned Motion Denoiser (CAMD)。Semantic Enhancement 模块从动作数据中提取更多 semantics，使得数据集的文本描述和动作数据之间的匹配更加精准，不需要大型语言模型。CAMD 方法为生成高质量、semantically consistent 动作序列提供了一个通用解决方案，可以有效地捕捉 context information，并将生成的动作与给定的文本描述进行对齐。</li>
<li>results: 我们的实验结果表明，SemanticBoost 方法在 Humanml3D 数据集上比 auto-regressive-based 技术高效，同时保持了真实和平滑的动作生成质量。SemanticBoost 还能生成准确的 orientational movements、combined motions based on specific body part descriptions、以及从复杂、延展的 sentence 中生成的动作。<details>
<summary>Abstract</summary>
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences. Our experimental results demonstrate that SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based techniques, achieving cutting-edge performance on the Humanml3D dataset while maintaining realistic and smooth motion generation quality.
</details>
<details>
<summary>摘要</summary>
当前技术面临着从复杂 semantic 描述中生成动作的困难，主要是因为数据集中的 semantic 注解不够和contextual 理解不强。为解决这些问题，我们提出 SemanticBoost 框架，该框架同时解决了这两个问题。我们的框架包括semantic 增强模块和context-attuned motion denoiser (CAMD)。semantic 增强模块从动作数据中提取补充semantic信息，使 dataset 的文本描述更加详细，并确保文本和动作数据之间的准确对应，不需要依赖于大型语言模型。另一方面，CAMD 方法为生成高质量、semantically consistent 动作序列提供了一个总面 Solution，其效果是 capture context information和将生成的动作与给定的文本描述相align。与现有方法不同，我们的方法可以生成准确的orientation movement, based on specific body part descriptions, 以及从复杂、extended sentences中生成动作。我们的实验结果表明，SemanticBoost 作为一种 diffusion-based 方法，在 Humanml3D  dataset 上超越了 auto-regressive-based 技术，实现了 cutting-edge 性能，同时保持了实际和平滑的动作生成质量。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests"><a href="#Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests" class="headerlink" title="Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"></a>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20320">http://arxiv.org/abs/2310.20320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max J. van Duijn, Bram M. A. van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R. Spruit, Peter van der Putten</li>
<li>for: 这篇论文是关于大语言模型（LLM）的理解能力的研究，特别是对理解意图和信念的理论心（ToM）的能力。</li>
<li>methods: 这篇论文使用了11个基础和指导学习过的 LLM，测试它们在超过常见的谎言测试中的能力，包括非直观语言使用和循环意向性。</li>
<li>results: 研究发现，基于 GPT 家族的指导学习 LLM 表现最佳，经常也超过了7-10岁的儿童。基础 LLM 通常无法解决 ToM 任务，即使使用特定的提示。研究人员认为，语言和 ToM 的演进和发展可能帮助解释 instruciton-tuning 添加了什么：奖励合作交流，考虑到对方和场景。<details>
<summary>Abstract</summary>
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.
</details>
<details>
<summary>摘要</summary>
有哪些认知能力应该归因于大语言模型（LLM）？在这篇文章中，我们加入了这个emerging debate中，通过（i）测试11个基础和指导调整LLMs的能力，包括非直接语言使用和循环意向;（ii）使用新 rewrite versions of standardized tests to assess LLMs' robustness;（iii）提问和评分打开和关闭问题;以及（iv）对LLM表现与7-10岁儿童的同任务进行比较。我们发现，GPT家族的指导调整LLMs表现最高，经常也超过了儿童。基础LLMs无法解决ToM任务，即使使用特殊的提示。我们建议，语言和ToM的演进和发展可能帮助解释 instrucion-tuning 添加了什么：奖励合作通信，考虑到对方和场景。我们 conclude 认为，对 LLMS 的ToM 需要一种细致的视角。
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers"><a href="#Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers" class="headerlink" title="Causal Interpretation of Self-Attention in Pre-Trained Transformers"></a>Causal Interpretation of Self-Attention in Pre-Trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20307">http://arxiv.org/abs/2310.20307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov</li>
<li>for: 这个论文是为了解释Transformer神经网络中的自注意力机制，并将其解释为一种估计输入序列中Symbols之间的结构方程模型。</li>
<li>methods: 该论文使用了自注意力层中的表示进行Conditional Independence关系的估计，以实现学习输入序列上的 causal结构。</li>
<li>results: 该论文通过使用现有的约束基本算法，使得现有的预训练Transformer可以用于零MQ causal探测。同时，论文还提供了一些实验来证明这种方法的有效性，包括情感分类和推荐等两个任务。<details>
<summary>Abstract</summary>
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.
</details>
<details>
<summary>摘要</summary>
我们提出一种 causal 解释自注意力在 transformer 神经网络架构中的含义。我们将自注意力解释为一种对给定输入序列符号（token）的 structural equation model 的估计机制。这个 structural equation model 可以被解释为输入序列在特定上下文中的 causal 结构。这种解释在干扰因素存在时仍然有效。基于这种解释，我们可以计算输入符号之间的 conditional independence 关系，以便在已有的约束基于算法上学习 causal 结构。因此，现有的预训练 transformer 可以用于零化 causal-发现。我们在两个任务中（情感分类和推荐）提供了 causal 解释，以示例ifying 这种方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions"><a href="#Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions" class="headerlink" title="Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions"></a>Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20301">http://arxiv.org/abs/2310.20301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed R. Shoaib, Heba M. Emara, Jun Zhao</li>
<li>for: 该论文旨在探讨基于AI基础模型的多种食品安全应用程序，以超越现有深度学习和机器学习方法的局限性。</li>
<li>methods: 该论文使用不同数据类型，包括多spectral遥感数据、气象数据、土壤特性、历史记录和高分辨率卫星遥感数据，应用AI基础模型。</li>
<li>results: 该论文显示，基于AI基础模型的方法可以准确预测作物种类、耕地地图、田地分割和作物产量，提高资源分配和决策支持。这些模型为全球食品安全努力提供了一种变革性的力量，为实现可持续可靠的食品未来做出了重要贡献。<details>
<summary>Abstract</summary>
Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure food future.
</details>
<details>
<summary>摘要</summary>
Note: The above text is in Simplified Chinese.Please note that the translation is done using a machine translation tool, and the quality of the translation may vary depending on the complexity and nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents"><a href="#Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents" class="headerlink" title="Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents"></a>Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20287">http://arxiv.org/abs/2310.20287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung</li>
<li>For: The paper aims to address the limitations of the vanilla reset method in deep reinforcement learning (RL) and enhance sample efficiency.* Methods: The proposed method leverages deep ensemble learning to mitigate primacy bias and improve RL performance.* Results: The proposed method achieves high sample efficiency and safety considerations in various experiments, including those in the domain of safe RL.Here’s the Chinese translation of the three points:* For: 这篇论文的目的是解决深度强化学习（RL）中的早期经验偏袋现象，提高样本效率。* Methods: 提议的方法利用深度集成学习来缓解早期经验偏袋现象，提高RL表现。* Results: 实验结果表明，提议的方法在各种实验中具有高样本效率和安全考虑的优点。<details>
<summary>Abstract</summary>
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numerical results show its effectiveness in high sample efficiency and safety considerations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data"><a href="#AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data" class="headerlink" title="AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data"></a>AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20280">http://arxiv.org/abs/2310.20280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H. Nguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, Harshit Kumar, Jayant Kalagnanam, Nandyala Hemachandra, Narayan Rangaraj</li>
<li>for: This paper is written for improving the forecasting accuracy of business key performance indicators (Biz-KPIs) using IT event data.</li>
<li>methods: The paper introduces a new approach called AutoMixer, which combines channel-compressed pretraining and finetuning with a time-series Foundation Model (FM) to improve the accuracy of multivariate time series forecasting.</li>
<li>results: The paper shows that AutoMixer consistently improves the forecasting accuracy of Biz-KPIs by 11-15%, which provides actionable business insights and enhances efficiency and revenue through proactive corrective measures.<details>
<summary>Abstract</summary>
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks. Through detailed experiments and dashboard analytics, we show AutoMixer's capability to consistently improve the Biz-KPI's forecasting accuracy (by 11-15%) which directly translates to actionable business insights.
</details>
<details>
<summary>摘要</summary>
企业过程效率取决于企业关键性表示（Biz-KPI），而IT失败可能会对其产生负面影响。BizITOps数据将Biz-KPI和IT事件通道融合为多变量时间序列数据。预测Biz-KPI可以提高效率和收入，但BizITOps数据通常存在Biz-KPI和IT事件之间有用和噪声的交互，需要有效隔离。这会导致使用现有多变量预测模型时的预测性能不佳。为解决这个问题，我们介绍AutoMixer，一种基于时间序列基本模型（FM）的approach，基于 novelt channel-compressed pretrain和finetune工作流程。AutoMixer利用AutoEncoder дляchannel-compressed pretraining，并将其与高级TSMixer模型结合，以提高TSMixer模型的准确预测能力。此外，AutoMixer还可以在多个下游任务中进行普适。通过详细的实验和幕布分析，我们表明AutoMixer能够一直提高Biz-KPI的预测精度（11-15%），这直接对商业决策提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning"><a href="#Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning"></a>Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20268">http://arxiv.org/abs/2310.20268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu</li>
<li>for: This paper focuses on few-shot class-incremental learning (FSCIL), which aims to build machine learning models that can continually learn new concepts from a few data samples without forgetting knowledge of old classes.</li>
<li>methods: The proposed Sample-to-Class (S2C) graph learning method for FSCIL includes a Sample-level Graph Network (SGN) that analyzes sample relationships within a single session, and a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage.</li>
<li>results: The experiments on three popular benchmark datasets show that the proposed method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.<details>
<summary>Abstract</summary>
Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.   The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.   Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.   Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. This network plays a crucial role in linking the knowledge between different sessions and helps improve overall learning in the FSCIL scenario. Moreover, we design a multi-stage strategy for training S2C model, which mitigates the training challenges posed by limited data in the incremental process.   The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage. Experiments on three popular benchmark datasets show that our method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.
</details>
<details>
<summary>摘要</summary>
法律词数增长学习（FSCIL）目标是建立一个可以从少量数据样本中不断学习新概念的机器学习模型，而不会忘记过去的类知识。  however, the challenges of FSCIL lie in the limited data of new classes, which not only leads to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems.  early studies have shown that building sample relationships is beneficial for learning from few-shot samples. In this paper, we extend this idea to the incremental scenario and propose a Sample-to-Class (S2C) graph learning method for FSCIL.具体来说，我们提出了一种样本水平图学网络（SGN），它专注于在单个会话中分析样本之间的关系。这种网络可以帮助汇集相似的样本，从而提取更加细致的类水平特征。然后，我们提出了一种类水平图学网络（CGN），它在新和老类水平特征之间建立连接。这种网络在不同会话之间连接知识，并且帮助改善FSCIL中的总体学习。此外，我们设计了一种多阶段训练策略，用于训练S2C模型。这种多阶段训练策略是从基础阶段到几极阶段，通过额外的 pseudo-incremental 阶段来提高模型的容量。实验结果表明，我们的方法在三个 популяр的 benchmark 数据集上明显超越基eline，并在 FSCIL 中设置了新的 state-of-the-art 记录。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Average-Return-in-Markov-Decision-Processes"><a href="#Beyond-Average-Return-in-Markov-Decision-Processes" class="headerlink" title="Beyond Average Return in Markov Decision Processes"></a>Beyond Average Return in Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20266">http://arxiv.org/abs/2310.20266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Marthe, Aurélien Garivier, Claire Vernade</li>
<li>for: 本研究探讨了Markov Decision Processes中可以高效计算和优化的奖励函数。</li>
<li>methods: 本研究使用动态 програм设(DP)和分布式奖励学习(DistRL)来研究奖励函数的计算和优化。</li>
<li>results: 研究发现仅可高效计算和优化通用化均值函数，其他函数只能近似计算。提供了误差 bound 以及这种方法的潜在应用和局限性。<details>
<summary>Abstract</summary>
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
</details>
<details>
<summary>摘要</summary>
Markov 决策过程中可以计算和优化的奖励功能有哪些？在无限期、未折扣设置下，动态规划（DP）只能有效处理某些类型的统计。我们summarize政策评估的特征，并给出一个新的 плани组织问题的答案。有趣的是，我们证明只有通用均值可以被优化，甚至在更广泛的分布式激励学习（DistRL）框架下也是如此。DistRL 允许评估其他函数approximately，我们提供误差 bound 的估计器，并讨论这种方法的潜在和局限性。这些结果对Markov 决策过程的理论发展做出了贡献，特别是针对风险觉刻的策略。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy"><a href="#Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy" class="headerlink" title="Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy"></a>Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20254">http://arxiv.org/abs/2310.20254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Marote, Marie Martin, Anne Bonhomme, Pierre Lantéri, Yohann Clément</li>
<li>for: 这个论文主要是为了研究如何使用数字工具和分析技术来评估新型商品的潜在毒性，以及控制和规定这些产品的质量。</li>
<li>methods: 这篇论文使用了数字工具 such as spectral database, mixture database, experimental design, 和 Chemometrics &#x2F; Machine Learning algorithm，以及不同的样本准备方法，如 raw sample, 或多种压缩&#x2F;分解样本。</li>
<li>results: 这篇论文通过使用这些数字工具和分析技术，成功地鉴定了混合物的成分，并估计了其组成。这种策略可以在不同的分析工具上实施，从而节省时间，并且可以应用于不同的 matrices 和 industrialsector 中的质量控制和环境评估。<details>
<summary>Abstract</summary>
The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has enabled the identification of the mixture's constituents and an estimation of its composition. Implementing such strategies across different analytical tools can result in time savings for pollutant identification and contamination assessment in various matrices. This strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details>
<details>
<summary>摘要</summary>
现代复杂混合物的反工程化（irrespective of its nature）已经成为当今的重要问题。快速评估新商品的环境影响可以提供实际分析挑战。采用数字工具（数据库、化学ometry、机器学习等）和分析技术（Raman光谱、 Near Infrared Spectroscopy、质谱等）可以用于标识潜在有害分子。本文使用洗涤品的例子，其组成可能对人类或环境构成威胁，需要精确的标识和量化以确保质量控制和法规目的。将不同的数字工具（光谱数据库、混合数据库、实验设计、化学ometry / 机器学习算法等）与不同的样本准备方法（raw sample、几种浓缩/减弱样本）结合使用，可以通过拉曼光谱识别混合物的成分并估算其组成。实施这些策略在不同的分析工具上可以节省污染物识别和污染评估的时间。这种策略也适用于工业部门，用于产品或原材料控制，以及质量控制目的。
</details></li>
</ul>
<hr>
<h2 id="Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning"><a href="#Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning" class="headerlink" title="Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning"></a>Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20250">http://arxiv.org/abs/2310.20250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaichao Li, Jinsong Chen, John E. Hopcroft, Kun He</li>
<li>for: This paper aims to improve graph pooling methods for downsampling graphs, with the goal of achieving better performance on graph-level tasks like graph classification and graph generation.</li>
<li>methods: The proposed method, called GTPool, uses Transformer to improve node dropping pooling by capturing long-range pairwise interactions and sampling nodes diversely. The method includes a scoring module based on self-attention and a diversified sampling method called Roulette Wheel Sampling.</li>
<li>results: The paper shows that GTPool outperforms existing popular graph pooling methods on 11 benchmark datasets, demonstrating its effectiveness in capturing long-range information and selecting more representative nodes.<details>
<summary>Abstract</summary>
Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.
</details>
<details>
<summary>摘要</summary>
Graph pooling方法已经广泛应用于下采 Graph，实现了多个graph-level任务的出色成绩，如图像分类和图像生成。一个重要的笔记叫node dropping pooling，目的在于利用学习权重函数来Drop nodes with relatively lower significance scores。然而，现有的node dropping方法受到两种限制：（1）for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones;（2）pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes。为了解决这些问题，我们提出了一种图Transformer Pooling方法，称为GTPool，which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely。 Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively。GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes。In this way, GTPool could effectively obtain long-range information and select more representative nodes。Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations"><a href="#Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations" class="headerlink" title="Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"></a>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20246">http://arxiv.org/abs/2310.20246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nuochenpku/MathOctopus">https://github.com/nuochenpku/MathOctopus</a></li>
<li>paper_authors: Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li</li>
<li>for: This paper aims to develop powerful Multilingual Math Reasoning (xMR) language learning models (LLMs) that can perform well in a multilingual context.</li>
<li>methods: The authors use a combination of translation and instruction datasets to train their xMR LLMs, and propose several training strategies to improve their performance. They also employ a rejection sampling strategy and parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages.</li>
<li>results: The authors achieve remarkable results with their proposed xMR LLMs, particularly in few-shot scenarios. The best-performing model, MathOctopus-13B, reaches 47.6% accuracy on the MGSM testset, outperforming ChatGPT. Additionally, the authors observe that extending the rejection sampling strategy to the multilingual context is effective for model performances, and that employing parallel corpora for math SFT can significantly enhance model performance multilingually and in specific languages.<details>
<summary>Abstract</summary>
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.
</details>
<details>
<summary>摘要</summary>
现有研究主要集中在开发强大的语言学习模型（LLM），以便在单语言中提高数学逻辑能力，而几乎没有探索在多语言上保持效果的问题。为了bridging这个差距，本文尝试了开拓和训练强大的多语言数学逻辑模型（xMR）。首先，通过翻译，我们构建了首个多语言数学逻辑指导集合，MGSM8KInstruct，包括10种不同的语言，因此解决了训练数据的缺乏问题在xMR任务中。基于收集的数据集，我们提出了不同的训练策略，以建立强大的xMR LLMs，称为MathOctopus，并脱颖出 Convention Open-Source LLMs 和 ChatGPT 在少数shot情况下表现出色。特别是，MathOctopus-13B 在 MGSM 测试集上达到了 47.6% 的准确率，超过 ChatGPT 46.3%。除了惊人的结果外，我们也在广泛的实验中发现了一些重要的观察和发现：1. 在多语言上扩展拒绝采样策略，虽然有限的效果，但也有助于提高模型性能。2. 在多语言上使用平行 Corpora 进行数学监督精度调教（SFT），不仅可以显著提高模型的多语言性能，还可以提高其单语言性能。这表示，制作多语言 Corpora 可以被视为一种重要的提高模型性能的策略，尤其在数学逻辑任务中。例如， MathOctopus-7B 在 GSM8K 测试集上从 42.2% 提高到 50.8%。
</details></li>
</ul>
<hr>
<h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20240">http://arxiv.org/abs/2310.20240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</li>
<li>for: 实现自然且精确的对话幕后描绘，以提高虚拟人物的生动性和真实感。</li>
<li>methods: 引入VividTalker框架，将声音驱动的3D脸部动画分解为头姿和口部运动，并通过窗口式Transformer架构进行自动生成。</li>
<li>results: 与现有方法相比，VividTalker能够实现更自然和精确的声音驱动3D脸部动画，并且能够Synthesize facial details align with speech content。<details>
<summary>Abstract</summary>
The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation.
</details>
<details>
<summary>摘要</summary>
创造生动的语音驱动3D人脸动画需要自然和精准的声音输入和脸部表情的同步。然而，现有的方法仍然无法渲染具有灵活头姿和自然的脸部细节（例如，皱纹）。这种限制主要归结于两点：1）收集具有细节3D人脸形状的训练集是非常昂贵的。这种缺乏细节形状注释限制了模型学习表达性人脸动画的训练。2）与口移动相比，头姿与语音内容的相关性远低。因此，同时模型口移动和头姿的控制导致了脸部动作的缺乏控制。为解决这些挑战，我们介绍了VividTalker，一个新的框架，用于实现语音驱动3D人脸动画，具有灵活的头姿和自然的脸部细节。具体来说，我们明确分解了脸部动画为头姿和口移动两个分支，并将它们分别编码到独立的权重空间中。然后，我们通过窗口基本的Transformer架构进行自动生成这些特征。为了增加3D人脸动画的丰富性，我们构建了一个新的3D数据集，包含细节shape，并学习以语音内容为导向synthesize facial details。广泛的量化和质量测试表明，VividTalker在语音驱动3D人脸动画方面的表现较为�ivid和真实。
</details></li>
</ul>
<hr>
<h2 id="VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision"><a href="#VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision" class="headerlink" title="VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision"></a>VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20225">http://arxiv.org/abs/2310.20225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Hao, Fan Yang, Hao Huang, Shuaihang Yuan, Sundeep Rangan, John-Ross Rizzo, Yao Wang, Yi Fang</li>
<li>for: 帮助人们 WITH 视力障碍和低视力（pBLV）更好地认识和识别不熟的环境，以及感知和识别障碍物。</li>
<li>methods: 利用大量的视觉语言模型，帮助pBLV通过提供精确的描述和警告，了解环境中的物体和障碍物。</li>
<li>results: 实验表明，我们的方法可以准确地识别物体，并为pBLV提供详细和有用的环境描述和障碍物警告。<details>
<summary>Abstract</summary>
People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive descriptions of the environment and identifies potential risks in the environment by analyzing the environmental objects and scenes, relevant to the prompt. We evaluate our approach through experiments conducted on both indoor and outdoor datasets. Our results demonstrate that our method is able to recognize objects accurately and provide insightful descriptions and analysis of the environment for pBLV.
</details>
<details>
<summary>摘要</summary>
人们 WITH 聊车和低视力 (pBLV) 在不熟悉的环境中面临了严重的挑战，包括缺乏准确的对象识别和环境识别。此外，由于视力损失，pBLV 有 difficulity 访问和识别陌生环境中的障碍物。在这篇论文中，我们提出了一种创新的方法，利用大量的视觉语言模型来增强聊车和低视力人群的视觉感知，并提供细致和完整的环境描述，以及陌生环境中的障碍物警告。我们的方法首先利用大量的图像标记模型（RAM）来识别捕捉到的图像中的常见对象。然后，recognition结果和用户查询被组合成特定 для pBLV 的提示，并与输入图像一起被传递给大量视觉语言模型（InstructBLIP）。InstructBLIP 根据提示和输入图像，生成细致和完整的环境描述，并通过分析环境对象和场景，对陌生环境中的障碍物进行分析和警告。我们通过对室内和室外数据集进行实验，证明了我们的方法可以准确地识别对象并为 pBLV 提供有用的环境描述和障碍物警告。
</details></li>
</ul>
<hr>
<h2 id="Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering"><a href="#Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering" class="headerlink" title="Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering"></a>Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20224">http://arxiv.org/abs/2310.20224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Li, Hao Yan, Chen Zhang, Lijun Sun, Wolfgang Ketter, Fugee Tsung</li>
<li>for: 这个研究旨在提出一种基于维度 Dirichlet 过程多元混合模型和图structured clustering方法，用于旅客组聚分析。</li>
<li>methods: 这个方法利用了维度 Dirichlet 过程多元混合模型和图structured clustering方法，能够保留旅客旅行资料的层次结构，并在一步骤中自动决定集群数量。</li>
<li>results: 在香港地铁旅客数据中进行了一个案例研究，展示了这个方法可以自动决定集群数量，并提供了更好的集群质量（旅客内部紧密度和旅客间间距离）。<details>
<summary>Abstract</summary>
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Collapsed Gibbs Sampling method with a minimum cluster size requirement. A case study based on Hong Kong metro passenger data is conducted to demonstrate the automatic process of cluster amount evolution and better cluster quality measured by within-cluster compactness and cross-cluster separateness. The code is available at https://github.com/bonaldli/TensorDPMM-G.
</details>
<details>
<summary>摘要</summary>
乘客分组基于行程记录是交通运营商必备的。然而，现有方法难以将乘客分组，因为乘客旅行信息具有多级结构，包括每个乘客内部的多个旅行和多维信息。此外，现有方法需要准确指定分组数量，并且不考虑地理Semantic graph和功能相似性 между地点。本文提出了一种新的tensor Dirichlet进程多元混合模型，可以保持多维行程信息的层次结构并将其一步混合，并且可以自动确定分组数量。使用地理Graph进行社区检测，将semantic neighborLink。我们还提出了tensor版Collapsed Gibbs sampling方法，并要求每个分组至少包含一定的最小 cluster size。一个基于香港地铁乘客数据的案例研究，以示 automatic cluster amount evolution和更好的cluster质量（内部紧凑性和跨分组分化）。代码可以在https://github.com/bonaldli/TensorDPMM-G上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting"><a href="#A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting" class="headerlink" title="A Systematic Review for Transformer-based Long-term Series Forecasting"></a>A Systematic Review for Transformer-based Long-term Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20218">http://arxiv.org/abs/2310.20218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyilei Su, Xumin Zuo, Rui Li, Xin Wang, Heng Zhao, Bingding Huang</li>
<li>for: 本文主要针对长期时间序列预测（LTSF）任务，探讨了transformer架构的应用和改进。</li>
<li>methods: 本文首先提供了transformer架构的概述，以及其在LTSF任务中的应用和改进。然后介绍了公共可用的LTSF数据集和评价指标。最后，提供了时间序列分析中train transformer的最佳实践和技巧。</li>
<li>results: 本文提出了LTSF任务中train transformer的最佳实践和技巧，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
深度学习的出现引发了时间序列预测（TSF）领域的不eworthy进步。特别是transformer架构在TSF任务中得到了广泛的应用和采用。transformer架构能够有效提取长序列内元素之间的semantic相关性。不同的变体使得transformer架构能够有效处理长期时间序列预测（LTSF）任务。本文首先提供了transformer架构的全面概述和其后续改进，用于解决不同的LTSF任务。然后，我们summarize了公共可用的LTSF数据集和相关的评价指标。此外，我们还提供了有价值的时间序列分析训练策略和最佳实践。最后，我们提出了这个逐渐发展的领域中的可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Does-GPT-4-Pass-the-Turing-Test"><a href="#Does-GPT-4-Pass-the-Turing-Test" class="headerlink" title="Does GPT-4 Pass the Turing Test?"></a>Does GPT-4 Pass the Turing Test?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20216">http://arxiv.org/abs/2310.20216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Jones, Benjamin Bergen</li>
<li>for: 这个论文主要是用来评估GPT-4模型在在线图灵测试中的性能。</li>
<li>methods: 论文使用了一个公共的在线图灵测试来评估GPT-4模型的表现，并对比了基线值（ELIZA、GPT-3.5）和人类参与者的表现。</li>
<li>results: 结果显示，GPT-4模型在41%的游戏中突破了基线值（ELIZA的27%和GPT-3.5的14%），但 ainda fall short of chance和人类参与者的表现（63%）。研究发现，参与者的决策主要基于语言风格（35%）和社会情感特征（27%），表明智能不足以通过图灵测试。<details>
<summary>Abstract</summary>
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.
</details>
<details>
<summary>摘要</summary>
我们对 GPT-4 进行了公共在线图灵测试。最佳 GPT-4 提示在游戏中取得了41%的成绩，超过了 ELIZA （27%）和 GPT-3.5 （14%）的基线，但落后于人类参与者的基线（63%）。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），这支持了智能不足以通过图灵测试的想法。参与者的民生背景、教育和 LLMS familiarity 没有预测检测率，表明，即使深入了解系统并经常与它们交互，也可能受到欺骗。虽然图灵测试有知限性，但我们认为它仍然是自然语言交流和欺骗的有效评价方式。 AI 模型具有人类化的能力可能会对社会造成广泛的影响，我们分析了不同的策略和标准来评价人类化程度。
</details></li>
</ul>
<hr>
<h2 id="Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization"><a href="#Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization" class="headerlink" title="Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization"></a>Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20215">http://arxiv.org/abs/2310.20215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Hyung Lee, Chanyoung Park, Soohyun Park, Andreas F. Molisch</li>
<li>for: 这项研究旨在解决低地球轨道卫星网络中的长传输延迟问题，通过提出一种基于深度学习承诺协议（DHO）。</li>
<li>methods: 该研究使用了一种predictive能力训练的LEO卫星轨道模式，以避免MR阶段的传输延迟，同时仍能提供有效的HO决策。</li>
<li>results: 比较研究表明，DHO协议在多种网络条件下比传统HO协议表现更好，包括访问延迟、碰撞率和HO成功率。此外，研究还检验了访问延迟和碰撞率之间的负面关系，以及DHO使用不同DRL算法的训练性能和归一化。<details>
<summary>Abstract</summary>
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这个研究提出了一种基于深度学习束约学习（DRL）的手over（HO）协议，称为DHO，特意设计用于解决低轨道卫星网络中HO过程中的持续存在的长延迟问题。DHO跳过测量报告（MR）阶段，通过使用已经预先确定的LEO卫星轨道模式来预测MR的内容。这种简化掉了MR阶段所带来的延迟，同时仍然提供有效的HO决策。提议的DHO在多种网络条件下比传统HO协议有更好的访问延迟、碰撞率和手over成功率，这显示了DHO在实际网络中的实用性。此外，研究还检验了访问延迟和碰撞率之间的负面关系，以及DHO使用不同DRL算法的训练性能和融合。
</details></li>
</ul>
<hr>
<h2 id="In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey"><a href="#In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey" class="headerlink" title="In Search of Lost Online Test-time Adaptation: A Survey"></a>In Search of Lost Online Test-time Adaptation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20199">http://arxiv.org/abs/2310.20199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Wang, Yadan Luo, Liang Zheng, Zhuoxiao Chen, Sen Wang, Zi Huang</li>
<li>for: 本文提供了在线测试时适应（OTTA）的全面评估，OTTA 是适应机器学习模型到新数据分布 upon batch arrival 的 paradigm。</li>
<li>methods: 本文将 OTTA 技术分为三类，并对其进行了 benchmark 使用 potent Vision Transformer（ViT）backbone，以便进行比较和评估。</li>
<li>results: 我们的 benchmark 包括了 conventional corrupted datasets 如 CIFAR-10&#x2F;100-C 和 ImageNet-C，以及实际世界中的 shift 如 CIFAR-10.1 和 CIFAR-10-Warehouse，这些Shift 包括了搜索引擎和Diffusion模型生成的数据。我们引入了新的评估指标 FLOPs，以评估在线场景中的效率。我们的发现是：(1) transformers 对多种领域的 shift 具有高度的抗性，(2) 许多 OTTA 方法的效果取决于较大的 batch size，(3) 在适应过程中，稳定的优化和抗扰特性尤其重要，特别是当 batch size 为 1。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusive of FLOPs, shedding light on the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, indicating: (1) transformers exhibit heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample batch sizes, and (3) stability in optimization and resistance to perturbations are critical during adaptation, especially when the batch size is 1. Motivated by these insights, we pointed out promising directions for future research. The source code will be made available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了在线测试时适应（OTTA）的全面评论，涉及到在批处 arrival 时适应机器学习模型的新数据分布。尽管在最近的几年内，OTTA 技术得到了广泛的应用和研究，但是这个领域仍然受到一些问题的困扰，如模糊的设置、过时的基础模型和不一致的 гиперпарамет尔调整，这些问题使得研究成果受到难以复制和评估的影响。为了便于对 OTTA 技术进行明确和系统的比较，我们将其分为三种主要类别，并对它们进行 benchmark 使用强大的 Vision Transformer（ViT）基础模型。我们的 benchmark 包括 conventional 的损害数据集 CIFAR-10/100-C 和 ImageNet-C，以及实际世界的变化，包括 CIFAR-10.1 和 CIFAR-10-Warehouse，这些变化包括搜索引擎和 Synthesized 数据的扩散模型。为了评估在线enario 中的效率，我们引入了新的评价指标，包括 FLOPs，从而揭示了适应精度和计算负担之间的交易。我们的发现与现有文献不同，表明：（1） transformers 在多种领域的域shift 中表现出了更高的抗性，（2）许多 OTTA 方法的效果取决于较大的批处大小，（3）在适应过程中稳定的优化和抗辐射性是关键，特别是批处大小为 1。基于这些发现，我们指出了未来研究的可能性。我们将源代码公开。
</details></li>
</ul>
<hr>
<h2 id="Generating-Continuations-in-Multilingual-Idiomatic-Contexts"><a href="#Generating-Continuations-in-Multilingual-Idiomatic-Contexts" class="headerlink" title="Generating Continuations in Multilingual Idiomatic Contexts"></a>Generating Continuations in Multilingual Idiomatic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20195">http://arxiv.org/abs/2310.20195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhitabrat Pokharel, Ameeta Agrawal</li>
<li>for: 测试语言模型对 figurative text 的理解能力</li>
<li>methods: 使用英语和葡萄牙语的数据集，采用零 shot、几 shot 和精度调整的训练方法</li>
<li>results: 模型在 literal 和idiomatic 上的表现差异很小，同时模型在两种语言上的表现相似，表明生成模型在这种任务中的 Robustness.<details>
<summary>Abstract</summary>
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
</details>
<details>
<summary>摘要</summary>
语言处理词组或文本中的idiomatic表达能力是任何语言理解和生成的关键方面。我们通过对包含idiomatic（或literal）表达的故事中的上下文 relevancecontinuation进行生成测试，以评估生成语言模型（LMs）对非compositional figurative文本的理解能力。我们在英语和葡萄牙语两种语言下进行了一系列实验，使用零shot、少数shot和精度调整的训练方法。我们的结果表明，模型对 literal上下文的生成性能和idiomatic上下文的生成性能几乎没有差异，差距非常小。此外，我们的实验结果表明，模型在这两种语言中表现几乎相同，这表明生成模型在这种任务上具有坚定的 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Pre-training-for-Precipitation-Post-processor"><a href="#Self-supervised-Pre-training-for-Precipitation-Post-processor" class="headerlink" title="Self-supervised Pre-training for Precipitation Post-processor"></a>Self-supervised Pre-training for Precipitation Post-processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20187">http://arxiv.org/abs/2310.20187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, Sujeong You</li>
<li>for: 提高地方气象预测中的暴雨预测精度，以防止恶势力天气事件。</li>
<li>methods: 提议使用深度学习对气象数值预测模型进行修正，并使用自动标注法对不均匀的数据集进行训练。</li>
<li>results: 实验结果显示，提议的方法可以在地方气象预测中提高暴雨预测精度，并且超过其他方法的性能。<details>
<summary>Abstract</summary>
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
</details>
<details>
<summary>摘要</summary>
要确保地方降水的预测预测时间足够，是预测恶势力天气事件的关键。然而，全球变暖引起的气候变化使得准确预测恶势力降水事件变得更加困难。在这项工作中，我们提出了基于深度学习的降水后处理方法，用于数值天气预测（NWP）模型。降水后处理方法包括（i）自我监督预训练，其中参数Encoder在掩码变量的恢复问题上进行自我监督预训练，以及（ii）在降水分类任务（目标领域）上进行传输学习，从预训练的Encoder中提取参数。我们还介绍了一种有效地训练类别不均衡数据集的启发性标签方法。我们的实验结果表明，提议的方法在地方NWP中的降水更正中超过了其他方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Discover-Skills-through-Guidance"><a href="#Learning-to-Discover-Skills-through-Guidance" class="headerlink" title="Learning to Discover Skills through Guidance"></a>Learning to Discover Skills through Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20178">http://arxiv.org/abs/2310.20178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, Jaegul Choo</li>
<li>For: 提高无监督技能发现（USD）中的探索性能，尤其是在环境复杂度高时。* Methods: 提出了一种新的 USD算法，即技能发现指导（DISCO-DANCE），通过选择具有最高潜力达到未探索状态的导导技能，然后导引其他技能跟随，最后将导引技能分散以 maximize其在未探索状态中的分化性。* Results: 对比其他 USD基线，DISCO-DANCE在具有具有高环境复杂度的 Navigation benchmark 和 Continuous Control  benchmark 中表现出色，并且可以在具有高环境复杂度的情况下提高探索性能。<details>
<summary>Abstract</summary>
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance.
</details>
<details>
<summary>摘要</summary>
在无监督技能发现（USD）领域，一个主要挑战是有限的探索，主要是因为行为偏离初始轨迹的惩罚。为了增强探索，现有方法ologies使用辅助奖励来最大化状态的认知不确定性或熵。然而，我们发现在环境复杂度增加时，这些奖励的效果下降。因此，我们提出了一种新的 USD算法，即技能发现导航（DISCO-DANCE），它包括以下三个步骤：1. 选择具有最高潜在性能力的导航技能，即可以达到未探索的状态。2. 将其他技能指导到导航技能的轨迹上。3. 使用导航技能的各个维度进行散布，以最大化其在未探索状态下的分化度。实验证明，DISCO-DANCE在复杂环境中比其他 USD 基线表现更佳，包括两个导航benchmark和一个连续控制benchmark。详细的visualization和代码可以在 <https://mynsng.github.io/discodance> 上查看。
</details></li>
</ul>
<hr>
<h2 id="GraphTransformers-for-Geospatial-Forecasting"><a href="#GraphTransformers-for-Geospatial-Forecasting" class="headerlink" title="GraphTransformers for Geospatial Forecasting"></a>GraphTransformers for Geospatial Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20174">http://arxiv.org/abs/2310.20174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pallavi Banerjee, Satyaki Chakraborty</li>
<li>for: 预测地理序列中的轨迹，使用图Transformer提高预测精度。</li>
<li>methods: 利用地理点之间自然生成的图结构，并将其 direktly  интегрирован到Transformer模型中，以提高预测精度。</li>
<li>results: 在HURDAT数据集上，our GraphTransformer方法与基准模型相比，显著提高了预测精度。<details>
<summary>Abstract</summary>
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的框架，用于预测地理序列的轨迹使用图transformer。当视图多个序列时，我们发现了不同的地理点之间自然形成的图结构，通常不被考虑在这类序列模型任务中。我们表明，通过显式利用这个图结构，可以显著提高地理轨迹预测。我们的图transformer方法在HURDAT dataset上，对6小时预测风暴轨迹的任务显示出了明显的提高。
</details></li>
</ul>
<hr>
<h2 id="Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation"><a href="#Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation" class="headerlink" title="Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?"></a>Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20162">http://arxiv.org/abs/2310.20162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Pan, Supryadi, Deyi Xiong</li>
<li>for: 本研究旨在探讨多种语言之间的机器翻译模型在不同翻译方向下的 Robustness 性能是否可以转移。</li>
<li>methods: 本研究使用了 Character-、word- 和 multi-level 噪声进行攻击特定翻译方向的多语言神经机器翻译模型，并对其他翻译方向的 Robustness 进行评估。</li>
<li>results: 研究结果表明，在一个翻译方向上获得的 Robustness 性能可以转移到其他翻译方向上，并且在字符级噪声和单词级噪声攻击下，Robustness 性能更容易转移。<details>
<summary>Abstract</summary>
Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details>
<details>
<summary>摘要</summary>
Robustness，模型在干扰下表现良好的能力，对于建立可靠的自然语言处理系统是非常重要。 latest studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts"><a href="#Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts" class="headerlink" title="Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts"></a>Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20159">http://arxiv.org/abs/2310.20159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/lg-vqa">https://github.com/declare-lab/lg-vqa</a></li>
<li>paper_authors: Deepanway Ghosal, Navonil Majumder, Roy Ka-Wei Lee, Rada Mihalcea, Soujanya Poria</li>
<li>for: 这 paper 的目的是解释如何使用语言指导来提高视觉问答系统的性能。</li>
<li>methods: 该 paper 提出了一种多模态框架，使用语言指导（LG）来提高视觉问答系统的准确率。LG 包括理由、图像描述、场景图等。</li>
<li>results: 该 paper 在 A-OKVQA、Science-QA、VSR 和 IconQA 等数据集上进行了多个多选问答任务的测试，并证明了语言指导是一种简单 yet 强大的策略，可以提高 CLIP 和 BLIP 模型在 A-OKVQA 数据集中的性能。<details>
<summary>Abstract</summary>
Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.
</details>
<details>
<summary>摘要</summary>
“视觉问答（VQA）是解答图像上的问题的任务。该任务假设理解图像和问题，以提供自然语言的答案。VQA在过去几年内得到了广泛的关注，因为它在多个领域有广泛的应用前景，如机器人、教育和医疗。在这篇论文中，我们关注知识增强VQA，其中回答问题需要通过图像和问题的理解，以及对概念和想法的推理。我们提出了一种多modal的框架，使用语言指导（LG），包括理由、图像描述、场景图等，以更准确地回答问题。我们使用CLIP和BLIP模型对多个选择问答任务进行了benchmarking，并证明了使用语言指导是一种简单 yet powerful和有效的策略。我们在A-OKVQA、Science-QA、VSR和IconQA数据集上实现了LG-VQA，并观察到在这些数据集上表现更加稳定和可靠。LG-VQA的实现可以在https://github.com/declare-lab/LG-VQA上获得。”
</details></li>
</ul>
<hr>
<h2 id="MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows"><a href="#MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows" class="headerlink" title="MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows"></a>MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20155">http://arxiv.org/abs/2310.20155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavlo O. Dral, Fuchun Ge, Yi-Fan Hou, Peikun Zheng, Yuxinxin Chen, Mario Barbatti, Olexandr Isayev, Cheng Wang, Bao-Xin Xue, Max Pinheiro Jr, Yuming Su, Yiheng Dai, Yangtao Chen, Lina Zhang, Shuang Zhang, Arif Ullah, Quanhao Zhang, Yanchi Ou</li>
<li>for: 这篇论文主要是用来探讨Machine Learning（ML）在计算化学中的应用，以及如何使用MLatom3软件包来实现自定义工作流程。</li>
<li>methods: 这篇论文使用了MLatom3软件包，可以通过命令行选项、输入文件或脚本来运行计算化学 simulations，并且可以在计算机和XACS云计算平台上进行计算。</li>
<li>results: 这篇论文可以用ML、量子机理和组合模型来计算能量和热化学性质、优化结构、运行分子和量子动力学、计算（ro）振荡、一 photon UV&#x2F;vis吸收和两 photon吸收谱。用户可以从库中选择各种预训练的ML模型和量子机理方法，而开发者可以使用多种ML算法来建立自己的模型。<details>
<summary>Abstract</summary>
Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models and quantum mechanical approximations such as AIQM1 approaching coupled-cluster accuracy. The developers can build their own models using various ML algorithms. The great flexibility of MLatom is largely due to the extensive use of the interfaces to many state-of-the-art software packages and libraries.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision"><a href="#Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision" class="headerlink" title="Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"></a>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20153">http://arxiv.org/abs/2310.20153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</li>
<li>for: 这则研究旨在开发小型领域专门的语言模型（LLM），并且在有限的标注预算下进行开发。</li>
<li>methods: 我们提出了一个名为多元信息学习（IMFL）框架，它可以在有限的标注预算下开发小型领域专门的LLM。我们将这个预算为多元信息学习问题，并且专注于确定最佳的标注策略，以将低信任度自动LLM标注和高信任度人工标注结合以最大化模型性能。</li>
<li>results: 我们的实验结果显示，IMFL可以与人工标注相比，在金融和医疗领域中的四个任务中表现出色。对于有限的人工标注预算，IMFL可以将人工标注cost从原来的一半降低，同时保持比人工标注性能几乎相同。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同任务中表现出了杰出的能力，但是它们在执行特定领域任务时，受到巨大规模的部署和易受到误information的影响，并且需要大量的数据标注成本。我们提出了一个新的互动式多层精确度学习（IMFL）框架，用于效率地开发小型领域特定的 LLM，以降低数据标注成本。我们的方法将领域特定精确化过程定义为多层精确度学习问题，专注于发现最佳获取策略，以将低精度自动 LLM 标注和高精度人类标注平衡为最大化模型表现。我们还提出了一个探索-利用查询策略，增加标注多样性和有用性，包括两个创新设计：1）提取人类标注项目中的内容例子，以改善 LLM 标注，2）变化批次大小，以控制每个精度选择的顺序，以便传播知识传授，最终提高标注质量。实验结果显示，IMFL 在金融和医疗任务中具有较高的表现，相比单一精度标注，IMFL 在所有四个任务中具有更高的表现，并且在两个任务中实现了非常接近人类标注的表现。这些成果显示，对特定领域任务的人类标注成本可以通过IMFL的实现，从而实现更好的数据标注效率。
</details></li>
</ul>
<hr>
<h2 id="Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs"><a href="#Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs" class="headerlink" title="Unlearn What You Want to Forget: Efficient Unlearning for LLMs"></a>Unlearn What You Want to Forget: Efficient Unlearning for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20150">http://arxiv.org/abs/2310.20150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaao Chen, Diyi Yang</li>
<li>for: 提高大语言模型（LLMs）的隐私和数据保护，避免数据泄露和违反数据保护法规。</li>
<li>methods: 提出一种高效的快速忘记框架，通过在转换器中引入轻量级忘记层，实现不需要重新训练整个模型来更新模型。此外，提出一种 fuselay mechanism，以有效地结合不同的忘记层，处理序列的忘记操作。</li>
<li>results: 通过对分类和生成任务进行实验，证明了我们提出的方法的效果，比靶场标准方法更高。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在预训练和记忆各种文本数据上取得了显著进步，但这个过程可能会遇到隐私问题和数据保护规定的违反。因此，能够轻松地从LLM中移除关于个人用户的数据而不影响预测质量的能力变得越来越重要。为解决这些问题，在这个工作中，我们提出了一种高效的忘记框架，通过在转换器中添加轻量级忘记层，使得在数据移除后不需要重新训练整个模型。此外，我们还引入了融合机制，以有效地将不同的忘记层结合，以处理一系列忘记操作。实验表明，我们提出的方法与当前基eline相比，在分类和生成任务上具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network"><a href="#Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network" class="headerlink" title="Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network"></a>Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20148">http://arxiv.org/abs/2310.20148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Kaiwen Liu, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky</li>
<li>for: 本研究旨在帮助自动驾驶车辆更好地理解周围交通情况，以便更好地完成任务。</li>
<li>methods: 本研究提出了一个行为模型，该模型将交通 Driver 的交互意图编码为隐藏的社会心理参数。通过 bayesian 滤波器，我们开发了一种循环滤波器优化算法，该算法考虑了交互 Driver 的意图不确定性。在线部署时，我们设计了基于注意力机制的神经网络架构，该架构模仿行为模型，并使用在线估计的参数先验。我们还提出了一种决策搜索算法来解决决策问题。</li>
<li>results: 我们对行为模型进行了实际路径预测测试，并对决策模块进行了广泛的评估，包括在强制合流场景中使用真实世界交通数据进行测试。结果表明，我们的算法可以在不同交通条件下完成强制合流任务，同时保证安全驾驶。<details>
<summary>Abstract</summary>
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajectory prediction. We further conduct extensive evaluations of the proposed decision-making module, in forced highway merging scenarios, using both simulated environments and real-world traffic datasets. The results demonstrate that our algorithms can complete the forced merging tasks in various traffic conditions while ensuring driving safety.
</details>
<details>
<summary>摘要</summary>
自主车辆需要在交通中完成任务，因此需要具备人工智能来更好地理解周围交通的意图，以便更好地完成任务。在这个工作中，我们提出了一个行为模型，将交互的driver意图编码为隐藏的社会心理参数。利用一个bayesian滤波器，我们开发了一个逐页估计Optimization的控制器，考虑到交互driver意图的不确定性。为在线上部署，我们设计了一个基于注意力机制的神经网络架构，实现行为模型的在线估计参数。我们还提出了一个搜索算法来解决实时决策问题。实验结果显示，我们的方法可以在不同的交通情况下完成强制合流任务，并确保安全驾驶。
</details></li>
</ul>
<hr>
<h2 id="EELBERT-Tiny-Models-through-Dynamic-Embeddings"><a href="#EELBERT-Tiny-Models-through-Dynamic-Embeddings" class="headerlink" title="EELBERT: Tiny Models through Dynamic Embeddings"></a>EELBERT: Tiny Models through Dynamic Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20144">http://arxiv.org/abs/2310.20144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan</li>
<li>for: 这篇论文是为了实现对 transformer-based 模型（例如 BERT）的压缩，并对下游任务的精度维持最小影响。</li>
<li>methods: 这篇论文使用的方法是取代模型的输入嵌入层，并使用动态嵌入计算。这个嵌入层贡献了模型的大部分，尤其是小型 BERT 的情况下。</li>
<li>results: 这篇论文的实验结果显示，使用这种方法可以对 BERT 模型进行压缩，并且对下游任务的精度维持相对稳定。特别是，在 GLUE 测试 benchmark 上，我们的 BERT Variants（EELBERT）与传统 BERT 模型之间的差异几乎没有。这允许我们开发出最小化的模型 UNO-EELBERT，它在 GLUE 测试中获得了相对于完全训练的 BERT-tiny 的 GLUE 分数，但是它的大小仅有 1.2 MB，相对于 BERT-tiny 的 4%。<details>
<summary>Abstract</summary>
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</details>
<details>
<summary>摘要</summary>
我们介绍EELBERT，一种对transformer-based模型（如BERT）进行压缩，并对下游任务的精度有 minimal impact。这是通过取代模型中的输入嵌入层而实现的，并且使用dinamic嵌入计算函数。由于输入嵌入层对模型大小的影响相当大，特别是小型BERT的情况下，因此替换这个层可以实现很大的模型尺寸优化。我们的实验结果显示，我们的BERT variants（EELBERT）与传统BERT模型之间的差异非常小，而且我们可以开发出最小的模型UNO-EELBERT，其GLUE分数与完全训练的BERT-tiny相似（准确度 Within 4%），但是仅有1.2 MB的大小，相较于BERT-tiny的15倍。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Difference-Predictive-Coding"><a href="#Contrastive-Difference-Predictive-Coding" class="headerlink" title="Contrastive Difference Predictive Coding"></a>Contrastive Difference Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20141">http://arxiv.org/abs/2310.20141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chongyi-zheng/td_infonce">https://github.com/chongyi-zheng/td_infonce</a></li>
<li>paper_authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach</li>
<li>for: 学习和预测未来事件的方法</li>
<li>methods: 使用对比预测编码学习时间序列数据，并将不同时间序列数据缝合起来减少学习数据量</li>
<li>results: 比对前方法，该方法在目标条件强化学习中 achieve 2 倍的成功率增加和更好地处理随机环境，并在表格设置中比标准（蒙特卡洛）对比预测编码oding更高效，具体是 $20 \times$ 更高效和 $1500 \times$ 更高效。<details>
<summary>Abstract</summary>
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \times$ more sample efficient than the successor representation and $1500 \times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将未来的预测和推理视为时间序列问题的核心。例如，目标条件化强化学习可以视为学习表示系统来预测哪些状态可能会在未来访问。而先前的方法使用了对比预测编码来模型时间序列数据，而学习表示系统通常需要大量数据来表示长期依赖关系。在这篇论文中，我们引入了时间差版本的对比预测编码，将不同时间序列数据的片段缝合起来，以降低学习未来事件预测所需的数据量。我们应用这种表示学习方法， derivate一种离线RL算法。实验表明，相比先前的RL方法，我们的方法可以达到2倍的成功率，并且在随机环境中更好地适应。在表格设置下，我们表明，我们的方法比successor表示和标准（蒙地卡罗）对比预测编码高$20\times$的样本效率，并且高$1500\times$的样本效率。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models"><a href="#Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models" class="headerlink" title="Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models"></a>Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20105">http://arxiv.org/abs/2310.20105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaromir Savelka, Paul Denny, Mark Liffiton, Brad Sheese</li>
<li>for: 本研究旨在评估用大语言模型（LLM）自动分类学生帮助请求的可能性，以便在教育系统中提高响应效果。</li>
<li>methods: 本研究使用GPT-3.5和GPT-4模型进行 Zero-shot 试验，以评估这两种模型在分类学生帮助请求的能力。</li>
<li>results: GPT-3.5和GPT-4模型在大多数类别上表现相似，但GPT-4在 debug 相关的子类别上表现出了较高的表现。 fine-tuning GPT-3.5 模型可以大幅提高其表现，以至于与两名人类评分者的准确率和一致性几乎相同。<details>
<summary>Abstract</summary>
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过准确地类别学生求助的类型，可以实现个性化的回应。自动类别这些请求是一件非常困难的任务，但大语言模型（LLM）似乎提供了可 accessible 和Cost-effective的解决方案。这个研究评估了 GPT-3.5 和 GPT-4 模型在学生入门编程课程的学生帮助请求中的表现。在零批测试中，GPT-3.5 和 GPT-4 在大多数类别上表现相似，而 GPT-4 在 Debugging 相关的sub-category上表现更高，Outperformed GPT-3.5。细化 GPT-3.5 模型可以使其表现得更好，以至于与两名人类评分员的准确率和一致性相似。总之，这个研究表明了使用 LLM 提高教育系统的可能性，通过自动类别学生需求。Note: "GPT-3.5" and "GPT-4" refer to two different language models, and "zero-shot trials" refer to situations where the models are tested on tasks they have not been trained on. "Sub-categories" refer to more specific categories within a larger category. "Fine-tuning" refers to the process of adjusting the model's parameters to improve its performance on a specific task.
</details></li>
</ul>
<hr>
<h2 id="Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics"><a href="#Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics" class="headerlink" title="Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics"></a>Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20104">http://arxiv.org/abs/2310.20104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Karnalim, Hapnes Toba, Meliana Christianti Johan, Erico Darmawan Handoyo, Yehezkiel David Setiawan, Josephine Alvina Luwia</li>
<li>for: This paper aims to address the issues of plagiarism and misuse of AI assistance in web programming education.</li>
<li>methods: The authors conducted a controlled experiment to compare student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT).</li>
<li>results: The study found that students who engaged in misconduct (plagiarism or AI assistance) received comparable test marks with less completion time, while AI-assisted submissions were more complex and less readable. Students believed that AI assistance could be useful with proper acknowledgment, but were not convinced of its readability and correctness.<details>
<summary>Abstract</summary>
In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and correctness of the solutions.
</details>
<details>
<summary>摘要</summary>
在编程教育中， copying 和人工智能（AI）帮助的不当使用是emerging issues。然而，不多的相关研究集中在网络编程。我们计划开发自动化工具来帮助教师识别这些不当行为。为了全面理解问题，我们进行了一个控制实验，观察到不正当的利益和特征。我们比较了学生完成网络编程任务的独立完成、抄袭提交和AI帮助（ChatGPT）的性能。我们的研究显示，参与抄袭和AI帮助的学生的测试marks相当，但完成时间较短。抄袭提交和独立完成的作品相似，只有一些 superficies 的不同，如颜色和标识符名称。AI帮助的作品更复杂，使其难以阅读。学生认为AI帮助可以是有用的，但不确定其可读性和正确性。
</details></li>
</ul>
<hr>
<h2 id="Data-Market-Design-through-Deep-Learning"><a href="#Data-Market-Design-through-Deep-Learning" class="headerlink" title="Data Market Design through Deep Learning"></a>Data Market Design through Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20096">http://arxiv.org/abs/2310.20096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Sai Srivatsa Ravindranath, Yanchen Jiang, David C. Parkes</li>
<li>For: The paper is written to explore the use of deep learning for the design of revenue-optimal data markets, with the goal of expanding the frontiers of what can be understood and achieved in this area.* Methods: The paper uses deep learning techniques to learn signaling schemes for data market design, rather than allocation rules, and handles obedience constraints that arise from modeling the downstream actions of buyers.* Results: The paper demonstrates that the new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.<details>
<summary>Abstract</summary>
The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must learn signaling schemes rather than allocation rules and handle $\textit{obedience constraints}$ $-$ these arising from modeling the downstream actions of buyers $-$ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.
</details>
<details>
<summary>摘要</summary>
“数据市场设计”问题是经济理论中寻找一组信号协议（统计实验），以最大化信息卖家所得到的预期收益，其中每个实验 revelas一部分卖家知道的信息，并有相应的价格。每个买家在世界环境中做出决策，他们对某些实验的信息所对的价值取决于他们的先前知识和不同结果的价值。在多个买家的情况下，买家对某个实验的预期价值也可能受到他们向其他人销售的信息的影响。我们介绍了使用深度学习设计数据市场的应用，以扩展我们所能理解和实现的前iers。相比于早期的深度学习卖场设计（D\"utting et al., 2023），我们需要学习信号协议而不是分配规则，并处理“服从约束”（来自下游行为的模型），以及奖励竞拍 constraint。我们的实验表明，这新的深度学习框架可以准确地复制所有已知的理论解决方案，扩展到更复杂的设定，并用于确定数据市场的优化设计和提出新的设计 conjectures。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition"><a href="#Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition" class="headerlink" title="Evaluating Neural Language Models as Cognitive Models of Language Acquisition"></a>Evaluating Neural Language Models as Cognitive Models of Language Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20093">http://arxiv.org/abs/2310.20093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Héctor Javier Vázquez Martínez, Annika Lea Heuser, Charles Yang, Jordan Kodner</li>
<li>for: This paper argues that current benchmarks for evaluating the syntactic capacities of neural language models (LMs) may not be sufficient, and suggests using alternative datasets that are more representative of human language use.</li>
<li>methods: The paper uses small-scale data modeling child language acquisition to evaluate the performance of LMs, and compares their results to those of simple baseline models.</li>
<li>results: The paper finds that LMs can be readily matched by simple baseline models, and that they evaluate sentences in a way inconsistent with human language users. Additionally, the paper suggests that using carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers may be more effective in evaluating the syntactic capacities of LMs.<details>
<summary>Abstract</summary>
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.
</details>
<details>
<summary>摘要</summary>
neural network language models (LMs) 的成功在多种技术任务上，使其潜在地成为语言科学的理论，尽管LM训练和儿童语言学习有一些明显的不同。在这篇论文中，我们 argueThat some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.Note: I've used the simplified Chinese characters and grammar to make the text more accessible to a wider audience.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/" data-id="clogy1z1l006yffra8uhofnwi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CL_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T11:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.CL_2023_10_31/">cs.CL - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="What’s-In-My-Big-Data"><a href="#What’s-In-My-Big-Data" class="headerlink" title="What’s In My Big Data?"></a>What’s In My Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20707">http://arxiv.org/abs/2310.20707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge</li>
<li>For: The paper aims to provide a comprehensive understanding of the content of large text corpora used to train language models, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).* Methods: The paper proposes a platform called What’s In My Big Data? (WIMBD) that offers sixteen analyses to reveal and compare the contents of large text corpora, leveraging two basic capabilities - count and search - at scale.* Results: The paper applies WIMBD to ten different corpora used to train popular language models and uncovers several surprising findings, such as the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, about 50% of the documents in RedPajama and LAION-2B-en are duplicates.<details>
<summary>Abstract</summary>
Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.
</details>
<details>
<summary>摘要</summary>
大型文本 corpus 是语言模型的基础。然而，我们对这些 corpus 的内容有限的了解，包括一般统计、质量、社会因素和评估数据（污染）。在这项工作中，我们提出了 What's In My Big Data? (WIMBD) 平台和十六种分析，可以让我们对大型文本 corpus 的内容进行揭示和比较。WIMBD 基于计数和搜索的两种基本能力，可以在标准计算节点上处理 более于 35 terabytes 的数据。我们在十个不同的 corpus 中使用 WIMBD，包括 C4、The Pile 和 RedPajama。我们的分析发现了一些 previously undocumented 的发现，包括高度异常的重复、合成、低质量内容、个人隐私信息、恶意语言和评估数据污染。例如，我们发现了 RedPajama 和 LAION-2B-en 中的约 50% 的文档是重复的。此外，一些用于评估模型的数据集被污染了，包括 Winograd Schema Challenge 和 GLUE 和 SuperGLUE 中的一些部分。我们将 WIMBD 的代码和文件开源，以提供一个标准的评估集和鼓励更多的分析和透明度。可以在 GitHub 上找到我们的代码：github.com/allenai/wimbd。
</details></li>
</ul>
<hr>
<h2 id="Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language"><a href="#Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language" class="headerlink" title="Text-Transport: Toward Learning Causal Effects of Natural Language"></a>Text-Transport: Toward Learning Causal Effects of Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20697">http://arxiv.org/abs/2310.20697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/torylin/text-transport">https://github.com/torylin/text-transport</a></li>
<li>paper_authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</li>
<li>for: 这个论文是为了研究语言技术在实际场景中的影响，特别是语言变化对文本的影响。</li>
<li>methods: 这个论文提出了一种名为Text-Transport的方法，用于在自然语言下 estimating causal effects。这种方法可以在任何文本分布下进行计算，不需要强制假设。</li>
<li>results: 研究人员通过实验和分析，证明了Text-Transport的有效性和可靠性。此外，这种方法还用于研究社交媒体上的仇恨言论，发现了语言变化对文本的 causal effects 的差异。<details>
<summary>Abstract</summary>
As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Non-Compositionality-in-Sentiment-New-Data-and-Analyses"><a href="#Non-Compositionality-in-Sentiment-New-Data-and-Analyses" class="headerlink" title="Non-Compositionality in Sentiment: New Data and Analyses"></a>Non-Compositionality in Sentiment: New Data and Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20656">http://arxiv.org/abs/2310.20656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vernadankers/noncompsst">https://github.com/vernadankers/noncompsst</a></li>
<li>paper_authors: Verna Dankers, Christopher G. Lucas</li>
<li>for: 本研究的目的是获取句子 sentiment 的非compositional 评分。</li>
<li>methods: 本研究提出了一种方法来获取句子 sentiment 的非compositional 评分，并创建了一个包含 259 个句子的评分资源（NonCompSST）。</li>
<li>results: 研究使用 NonCompSST 资源进行了一种计算模型的 sentiment 分析，并对结果进行了分析和评估。<details>
<summary>Abstract</summary>
When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a methodology for obtaining those non-compositionality ratings, b) a resource of ratings for 259 phrases -- NonCompSST -- along with an analysis of that resource, and c) an evaluation of computational models for sentiment analysis using this new resource.
</details>
<details>
<summary>摘要</summary>
当自然语言短语组合时，它们的意思经常大于其部分。在NLG任务中，如情感分析，这个规则仍然适用。许多NLG研究强调情感计算是主要 Compositional，我们则决定获得不可组合性评分 для短语。我们的贡献如下：a) 一种方法获取非可组合性评分，b) 一个包含259个短语评分的资源——NonCompSST，以及对这个资源的分析，c) 使用这个新资源进行情感分析计算模型的评估。
</details></li>
</ul>
<hr>
<h2 id="Defining-a-New-NLP-Playground"><a href="#Defining-a-New-NLP-Playground" class="headerlink" title="Defining a New NLP Playground"></a>Defining a New NLP Playground</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20633">http://arxiv.org/abs/2310.20633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi R. Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji</li>
<li>for: 这篇论文的目的是定义一个新的自然语言处理（NLP）游戏场，为PhD学生提供20多个硬件研究方向，以满足现有的理论分析、新和挑战性问题、学习 парадиг和跨学科应用等方面的需求。</li>
<li>methods: 本论文使用的方法包括现有的NLP模型和算法的析密分析、新的问题设定和挑战性研究、不同的学习 парадиг和跨学科应用等方面的研究。</li>
<li>results: 本论文的结果是提出了20多个PhD硬件研究方向，包括理论分析、新和挑战性问题、学习 парадиг和跨学科应用等方面的研究，以满足现有的NLP领域需求和未来发展的趋势。<details>
<summary>Abstract</summary>
The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation"><a href="#The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation" class="headerlink" title="The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation"></a>The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20620">http://arxiv.org/abs/2310.20620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Tokarchuk, Vlad Niculae</li>
<li>for: 这个论文主要针对的是 neural machine translation (NMT) 的连续输出问题。</li>
<li>methods: 该论文使用了一种叫做 CoNMT (Continuous-output neural machine translation) 的方法，它将下一个词的预测问题转化为一个嵌入 Prediction 问题。</li>
<li>results: 研究发现，完全随机生成的嵌入可以超越劳动ious地预训练的嵌入，特别是在更大的数据集上。进一步的调查发现，这个意外效果尤其strongest  для rare words，这是因为这些嵌入的几何结构。<details>
<summary>Abstract</summary>
Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building"><a href="#Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building" class="headerlink" title="Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building"></a>Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20589">http://arxiv.org/abs/2310.20589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Momen, David Arps, Laura Kallmeyer</li>
<li>for: 这个研究是为了提出一种数据效果语言模型预训练方法（Warstadt et al., 2023）。</li>
<li>methods: 这个研究使用了基于转换器的偏挥masked语言模型，并将无监督预测的层次句子结构纳入模型架构中。具体来说，这个研究使用了Structformer架构（Shen et al., 2021）和其变种。StructFormer模型在有限的预训练数据上进行了无监督语义推导，并且在一些任务上实现了性能提升。</li>
<li>results: 对于39个任务，我们的模型在某些任务上实现了明显的性能提升，尤其是在具有层次结构的任务上。然而，我们的模型并没有在所有任务上一直性能超过提供的RoBERTa基eline模型（提供者：shared task组织者）。<details>
<summary>Abstract</summary>
In this paper, we describe our submission to the BabyLM Challenge 2023 shared task on data-efficient language model (LM) pretraining (Warstadt et al., 2023). We train transformer-based masked language models that incorporate unsupervised predictions about hierarchical sentence structure into the model architecture. Concretely, we use the Structformer architecture (Shen et al., 2021) and variants thereof. StructFormer models have been shown to perform well on unsupervised syntactic induction based on limited pretraining data, and to yield performance improvements over a vanilla transformer architecture (Shen et al., 2021). Evaluation of our models on 39 tasks provided by the BabyLM challenge shows promising improvements of models that integrate a hierarchical bias into the architecture at some particular tasks, even though they fail to consistently outperform the RoBERTa baseline model provided by the shared task organizers on all tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们对2023年 BabyLM Challenge 共同任务中语言模型预训练（LM）的提交（Warstadt et al., 2023）。我们使用基于转换器的假设语言模型，并在模型建筑中包含不supervised的句子结构预测。具体来说，我们使用Structformer架构（Shen et al., 2021）和其变种。StructFormer模型在有限的预训练数据上进行无监督语法推导时表现良好，并在vanilla transformer架构中提供性能改进（Shen et al., 2021）。我们对 shared task提供的39个任务进行评估，发现在某些任务上， integrating hierarchical bias into the architecture 可以获得明显的改进，尽管它们在所有任务上不能 consistently outperform RoBERTa基线模型提供的（shared task organizers）。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding"><a href="#Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding" class="headerlink" title="Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding"></a>Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20588">http://arxiv.org/abs/2310.20588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De</li>
<li>for: 本研究旨在提高医疗决策中的信息检索效率，适用于无法训练的医疗信息检索（MIR）领域。</li>
<li>methods: 本文提出了一种新的零容量医疗信息检索方法——MedFusionRank， combinates 预训练语言模型和统计方法，并解决了它们的局限性。</li>
<li>results: 实验表明，MedFusionRank在医疗数据集上的表现较 existing methods 出色，具有多种评价指标的批判性。 MedFusionRank 能够从短或单个查询中检索到相关信息，表现出色。<details>
<summary>Abstract</summary>
In the era of the Internet of Things (IoT), the retrieval of relevant medical information has become essential for efficient clinical decision-making. This paper introduces MedFusionRank, a novel approach to zero-shot medical information retrieval (MIR) that combines the strengths of pre-trained language models and statistical methods while addressing their limitations. The proposed approach leverages a pre-trained BERT-style model to extract compact yet informative keywords. These keywords are then enriched with domain knowledge by linking them to conceptual entities within a medical knowledge graph. Experimental evaluations on medical datasets demonstrate MedFusion Rank's superior performance over existing methods, with promising results with a variety of evaluation metrics. MedFusionRank demonstrates efficacy in retrieving relevant information, even from short or single-term queries.
</details>
<details>
<summary>摘要</summary>
在互联网时代，医疗信息检索已成为医疗决策中的关键。本文介绍MedFusionRank，一种新的零shot医疗信息检索方法，结合预训练语言模型和统计方法，并解决它们的局限性。该方法利用预训练BERT类型模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图连接，以增强医学知识。实验证明，MedFusionRank在医学数据集上的性能较高，并且与多种评价指标具有良好的结果。MedFusionRank能够从短或单个查询中检索相关信息，表现出色。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models"><a href="#Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models" class="headerlink" title="Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models"></a>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20499">http://arxiv.org/abs/2310.20499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang</li>
<li>for: 评估 LLM 智能agent 的自动评估是开发高级 LLM 智能agent 的关键。现有的人工标注评估数据集，如 AlpacaEval，尝试了很多努力，但是这些方法是贵重的、耗时的，缺乏适应性。</li>
<li>methods: 我们提出了一种基于语言游戏“Who is Spy”的方法，使用word guessing game来评估 LLM 的智能性能。给定一个单词，LLM 需要用语言描述单词，并确定其身份（间谍或非间谍）基于自己和其他玩家的描述。理想情况下，高级Agent应该拥有准确地描述单词，并同时增加对 conservative 描述的混淆，以提高其在游戏中的参与度。为此，我们首先开发了 DEEP，用于评估 LLM 的表达和隐蔽能力。DEEP 需要 LLM 在攻击和保守两种模式下描述单词。然后，我们引入了 SpyGame，一个交互式多代理框架，用于评估 LLM 的智能能力。在这个框架中，目标 LLM 需要具备语言技能和战略思维，以提供更全面的评估 LLM 的人类智能能力和在复杂交通情况下的适应能力。</li>
<li>results: 我们进行了广泛的实验，收集了多种来源、领域和语言的单词，并使用我们提出的评估框架进行测试。实验结果表明，我们的 DEEP 和 SpyGame 可以有效评估不同 LLM 的能力，捕捉它们在新情况下的适应和战略通信能力。<details>
<summary>Abstract</summary>
The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.
</details>
<details>
<summary>摘要</summary>
自动评估LLM基于代理人智能是发展高级LLM的关键。虽然有很大努力投入到了人类标注评估数据库，如AlpacaEval，但现有技术都是费时费力，缺乏适应性。在本文中，我们提出使用话语游戏来评估LLM的智能表现。给定一个词，LLM被要求描述这个词并确定其身份（间谍或非间谍）基于其自己和其他玩家的描述。理想情况下，高级代理人应该具备精准描述给定词的能力，同时通过攻击性和保守性的描述来增强其参与度。为此，我们首先开发了DEEP来评估LLM的表达和伪装能力。DEEP需要LLM描述一个词在攻击和保守模式下。然后，我们引入了SpyGame，一个交互式多代理人框架，用于评估LLM的智能能力。SpyGame需要目标LLM具备语言技能和战略思维，以提供更全面的评估高级LLM的人类智能能力和复杂通信情况中的适应性。我们的评估框架非常容易实现。我们从多个来源、领域和语言收集了词汇，并使用我们的评估框架进行实验。广泛的实验结果表明，我们的DEEP和SpyGame有效地评估了不同LLM的能力，捕捉它们在新情况下适应和战略通信的能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users"><a href="#Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users" class="headerlink" title="Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users"></a>Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20479">http://arxiv.org/abs/2310.20479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos</li>
<li>for: 这个论文的目的是提出一种多用户多语言对话 dataset，以便开发多用户协作对话系统。</li>
<li>methods: 论文使用了将每个用户utterance替换为两个用户的小聊天，以保持对话的semantic和pragmatic consistency。</li>
<li>results: 研究发现，使用预测的 rewrite 可以在多用户对话中大幅提高对话状态追踪，而不需要修改现有的单用户对话系统。此外，这种方法还可以在未看过的领域中进行推理。<details>
<summary>Abstract</summary>
While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is semantically and pragmatically consistent with the original user utterance, thus resulting in the same dialogue state and system response. These dialogues reflect interesting dynamics of collaborative decision-making in task-oriented scenarios, e.g., social chatter and deliberation. Supported by this data, we propose the novel task of multi-user contextual query rewriting: to rewrite a task-oriented chat between two users as a concise task-oriented query that retains only task-relevant information and that is directly consumable by the dialogue system. We demonstrate that in multi-user dialogues, using predicted rewrites substantially improves dialogue state tracking without modifying existing dialogue systems that are trained for single-user dialogues. Further, this method surpasses training a medium-sized model directly on multi-user dialogues and generalizes to unseen domains.
</details>
<details>
<summary>摘要</summary>
而 більшість任务对话系统假设对话是在单一用户和对话系统之间进行的，但是对话系统将在多用户之间进行通信的需求在增加。为了推进这些系统的开发，我们发布了多用户多WOZ数据集：具有两个用户和一个对话系统之间的任务对话。为了获得这个数据集，我们将每个用户说话从MultiWOZ 2.2替换为两个用户之间的小聊天，这些聊天的 semantics 和 Pragmatics 与原始用户说话相同，因此维持了同一个对话状态和系统回应。这些对话反映了多用户协作做决策的 interessing dinamics，例如社交聊天和考虑。基于这样的数据，我们提出了一个新的任务：多用户Contextual Query Rewriting，即将多用户任务对话 rewrite 为一个简洁的任务对话，保留仅任务相关的信息，并且可以直接由对话系统处理。我们证明了，使用预测重写可以在多用户对话中优化对话状态追踪，不需要修改现有的对话系统，并且可以在未见预料的领域中获得更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation"><a href="#Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation" class="headerlink" title="Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation"></a>Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20470">http://arxiv.org/abs/2310.20470</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Seza Doğruöz, Sunayana Sitaram, Zheng-Xin Yong</li>
<li>for: 本研究旨在探讨现有的CSW数据集（68）中语言对的偏袋现象，并分析数据采集和准备阶段的问题。</li>
<li>methods: 本研究采用了一种critical study的方法，通过分析68个CSW数据集中的语言对，探讨数据采集和准备阶段的问题。</li>
<li>results: 研究发现，现有的CSW数据集具有许多问题，包括英语占据绝大多数语言对，数据采集和准备阶段存在地域、社会经济和注册变化的问题，同时数据选择和筛选阶段的不清晰性也影响了CSW数据集的代表性。<details>
<summary>Abstract</summary>
Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g. transcription and annotation) stages. This in-depth analysis reveals that \textbf{a)} most CSW data involves English ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of representativeness in data collection and preparation stages due to ignoring the location based, socio-demographic and register variation in CSW. In addition, lack of clarity on the data selection and filtering stages shadow the representativeness of CSW data sets. We conclude by providing a short check-list to improve the representativeness for forthcoming studies involving CSW data collection and preparation.
</details>
<details>
<summary>摘要</summary>
多语种现象广泛存在全球，代码转换（CSW）是不同语言对的常见做法。然而，建立成功的CSW系统还很难，尽管最近的质量语言模型（MMLM）有了 significativemental advances。我们通过对现有CSW数据集（68）的抽查和分析，探讨这一退化的原因。我们发现：a）大多数CSW数据集中英语占主导地位，其他语言对不受到充分考虑。b）数据收集和预处理阶段存在 represetativeness 问题，这些问题包括：①  ignore 地点基础、社会经济和注册变化。② 数据选择和筛选阶段的不清晰性，使CSW数据集的表现性受到影响。我们 conclude 这些问题的存在，并提供一份简短的检查列表，以提高CSW数据集的表现性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation"><a href="#Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation" class="headerlink" title="Towards a Deep Understanding of Multilingual End-to-End Speech Translation"></a>Towards a Deep Understanding of Multilingual End-to-End Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20456">http://arxiv.org/abs/2310.20456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong</li>
<li>for: 这个论文旨在分析一种多语言端到端语音翻译模型在22种语言上学习的表示。</li>
<li>methods: 这个论文使用了Singular Value Canonical Correlation Analysis（SVCCA）来分析这种多语言端到端语音翻译模型中的表示相似性。</li>
<li>results: 这个研究发现了以下三点：（I）在多语言端到端语音翻译中，语言相似性在数据有限时失效。（II）在不受限制的数据量和相对well-aligned的音频文本数据的情况下，提高encoder表示可以提高翻译质量，超过双语翻译。（III）多语言端到端语音翻译的encoder表示可以更好地预测语言 typology 中的音韵特征。这些发现可能提供一种更有效的方法来提高多语言端到端语音翻译的效果。<details>
<summary>Abstract</summary>
In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our understanding of the functionality of multilingual speech translation and its potential connection to multilingual neural machine translation. The multilingual speech translation model is trained on the CoVoST 2 dataset in all possible directions, and we utilize LASER to extract parallel bitext data for SVCCA analysis. We derive three major findings from our analysis: (I) Linguistic similarity loses its efficacy in multilingual speech translation when the training data for a specific language is limited. (II) Enhanced encoder representations and well-aligned audio-text data significantly improve translation quality, surpassing the bilingual counterparts when the training data is not compromised. (III) The encoder representations of multilingual speech translation demonstrate superior performance in predicting phonetic features in linguistic typology prediction. With these findings, we propose that releasing the constraint of limited data for low-resource languages and subsequently combining them with linguistically related high-resource languages could offer a more effective approach for multilingual end-to-end speech translation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们使用协值幂积分析（SVCCA）来分析在多语言端到端语音翻译模型中学习的表示。SVCCA使我们能够估计语言之间的表示相似性和层次结构，从而深入理解多语言语音翻译的功能和可能与多语言神经机器翻译的联系。我们使用CoVoST 2 dataset进行所有可能的方向训练，并使用LASER来提取平行文本数据 для SVCCA分析。我们从分析中得出三个主要发现：（I）在多语言语音翻译中，语言相似性的效果随着语言训练数据的减少而逐渐消失。（II）在训练数据不受限制的情况下，增强的encoder表示和对齐的音频文本数据可以提高翻译质量，超过双语翻译。（III）多语言语音翻译的encoder表示在语言类型预测中表现出色，可以预测语言的phonetic特征。基于这些发现，我们提议在严格控制数据量的情况下，先 liberate低资源语言的数据限制，然后将其与语言相似性高的高资源语言相结合，可以实现更有效的多语言端到端语音翻译。
</details></li>
</ul>
<hr>
<h2 id="The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models"><a href="#The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models" class="headerlink" title="The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"></a>The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20440">http://arxiv.org/abs/2310.20440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Abreu-Vicente, Hannah Sonntag, Thomas Eidens, Thomas Lemberger</li>
<li>For: 本研究旨在 automatizing knowledge extraction from vast amounts of biomedical literature and preprints, using Natural Language Processing (NLP) techniques.* Methods: 本研究使用 Named-Entity Recognition (NER) 和 Named-Entity Linking (NEL) 技术，以及 context-dependent semantic interpretation，提取生物医学文献中的结构化信息和关键概念。* Results: 研究提供了 SourceData-NLP 数据集，包含生物医学文献中8种类型的生物实体（小分子、蛋白质、细胞组成物、细胞类型、组织、生物体、疾病）的标注，以及这些实体在实验设计中的角色和实验方法的标注。 SourceData-NLP 数据集包含了18,689张图文件中的620,000个生物实体标注，来自3,223篇生物医学文献。研究者还引入了一种新的语义任务，以测试模型是否能够理解实体是否为控制 intervención 的目标或测量的对象。<details>
<summary>Abstract</summary>
Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.   Results: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.   Conclusions: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details>
<details>
<summary>摘要</summary>
引言：科学出版物的领域正在急速扩展， posing challenges for researchers to keep up with the evolution of the literature.自然语言处理（NLP）已成为自动提取知识的有力手段之一。任务如名称实体识别（NER）和名称实体连接（NEL）， along with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.结果：我们介绍了 SourceData-NLP 数据集，通过出版过程中的常规筹编来生成。这个数据集的特点是强调在图文中标注生物实体。我们标注了8类生物实体（小分子、蛋白质、细胞组成部分、细胞系列、细胞类型、组织、生物体和疾病），它们在实验设计中的角色和实验方法的性质。SourceData-NLP 包含了620,000多个标注的生物实体，从18,689个图中筹编于3,223篇分子和细胞生物研究。我们表明了 SourceData-NLP 数据集的用途，通过评估 BioLinkBERT 和 PubmedBERT，两种基于 transformers 的模型，在 SourceData-NLP 数据集上进行 NER 任务的 fine-tuning。我们还介绍了一种新的上下文依赖的 semantic 任务，可以推断实体是否为控制 intervención 的目标或测量的对象。结论：SourceData-NLP 的规模 highlights the value of integrating curation into publishing.models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details></li>
</ul>
<hr>
<h2 id="FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models"><a href="#FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models" class="headerlink" title="FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models"></a>FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20410">http://arxiv.org/abs/2310.20410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang</li>
<li>For: This paper proposes a benchmark for evaluating the ability of large language models (LLMs) to follow instructions, specifically focusing on fine-grained constraints.* Methods: The proposed benchmark, called FollowBench, includes five types of fine-grained constraints (content, scenario, style, format, and example) and a multi-level mechanism to incrementally add constraints to the initial instruction.* Results: The authors evaluate nine popular LLMs on FollowBench and find that they have weaknesses in instruction following, particularly in handling challenging semantic constraints.Here are the three points in Simplified Chinese text:* For: 这篇论文提出了一个评估大语言模型（LLMs）遵循指令的benchmark，具体来说是关注细化的约束。* Methods: 该benchmark被称为FollowBench，包括五种细化的约束类型（内容、情况、式样、格式和示例），并使用多级机制来逐步添加约束到初始指令中。* Results: 作者通过对九种popular LLMs进行评估，发现它们在遵循指令方面存在弱点，尤其是在处理复杂的semantic约束方面。<details>
<summary>Abstract</summary>
The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Scenario, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each level. To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint evolution paths to handle challenging semantic constraints. By evaluating nine closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>大型语言模型（LLM）在各种实际应用中的能力是关键。现有的标准 benchmark primarily focuses on评估表面级别的回答质量，而不一定能够评估 LLM 是否能够遵循指令。为了填补这个研究漏洞，在这篇论文中，我们提出了 FollowBench，一个多级细化要求 Following Benchmark for LLMs。FollowBench 包括五种不同类型（即内容、场景、风格、格式和示例）的细化要求。为了准确地评估 LLMs 是否遵循每个约束，我们提出了一种多级机制，逐级添加一个单独的约束到初始指令中。为了评估 LLMs 的输出是否满足每个约束，我们提出了使用约束演化路径来挑战性的约束。通过评估 nine 种关闭源和开源的受欢迎 LLMs 在 FollowBench 上，我们指出了 LLMs 在遵循指令方面的弱点，并指向未来研究的可能性。数据和代码都可以在 https://github.com/YJiangcm/FollowBench 上获取。
</details></li>
</ul>
<hr>
<h2 id="AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction"><a href="#AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction" class="headerlink" title="AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"></a>AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20352">http://arxiv.org/abs/2310.20352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Hu, Hou Pong Chan, Yu Yin</li>
<li>for: 这篇研究旨在提出一个新的对话式构思框架，用于对话式自然语言处理中的论证生成。</li>
<li>methods: 这篇研究使用了一个新的框架，名为Americano，它使用了人工智能代理人互动来生成论证。这个框架分为两个阶段：首先生成论证组件，然后使用这些组件来生成最终的论证。此外，这篇研究还引入了一个实验性的写作评估模组，以提高左侧生成的语言模型。</li>
<li>results: 这篇研究使用了一 subset 的 Reddit&#x2F;CMV 数据集，以评估这个新的对话式构思框架。结果显示，这个方法比以往的终端到终点和排序对话式构思方法更好，并且可以生成更加整体和说服力强的论证。<details>
<summary>Abstract</summary>
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details>
<details>
<summary>摘要</summary>
Argument generation是自然语言处理中一项具有挑战性的任务，需要严格的逻辑推理和正确的内容组织。 drawing inspiration from recent chain-of-thought prompting, which breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.Here is a word-for-word translation of the text into Simplified Chinese:Argument Generation是自然语言处理中一项具有挑战性的任务，需要严格的逻辑推理和正确的内容组织。 drawing inspiration from recent chain-of-thought prompting, which breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM"><a href="#Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM" class="headerlink" title="Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM"></a>Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20347">http://arxiv.org/abs/2310.20347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Alaejos, Adrián Castelló, Pedro Alonso-Jordá, Francisco D. Igual, Héctor Martínez, Enrique S. Quintana-Ortí</li>
<li>for: 这 paper 的目的是使用 Apache TVM 开源框架自动生成高性能的矩阵乘法（GEMM）算法，以满足不同数据类型、处理器架构和矩阵操作形状的需求。</li>
<li>methods: 这 paper 使用的方法包括使用 Apache TVM 框架生成块分割的矩阵乘法算法，以及自动生成处理器特定的微内核。这些方法可以很好地灵活地调整和优化算法，以适应不同的数据类型和处理器架构。</li>
<li>results: 这 paper 的实验结果表明，使用 TVM-生成的块分割矩阵乘法算法和微内核可以提高端到端的性能，并且具有较小的内存占用。此外，这种方法还可以轻松地调整和优化算法，以适应不同的数据类型和处理器架构。<details>
<summary>Abstract</summary>
We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yielding performance on a par (or even superior for specific matrix shapes) with that of hand-tuned libraries; and 3)~features a small memory footprint.
</details>
<details>
<summary>摘要</summary>
我们探讨使用Apache TVM开源框架自动生成一系列采用 популярLinear algebra库（如GotoBLAS2、BLIS和OpenBLAS）的方法来实现高性能块化表示法（GEMM）。此外，我们还充分 automatizethe生成过程，并使用Apache TVM框架来 derivcomplete的处理器特定微内核（micro-kernels） для GEMM。与传统高性能库不同，我们不手动编写Assembly代码来实现微内核。总的来说，我们的 TVM生成的块化算法和微内核可以：1. 提高可移植性、维护性和全面性，使软件生命周期更加简洁；2. 提供高度的灵活性，以便轻松地调整和优化解决方案，以适应不同的数据类型、处理器架构和矩阵操作形状，实现与手动优化库相当的性能（或者甚至超越）；3. 具有较小的内存占用。
</details></li>
</ul>
<hr>
<h2 id="InstructCoder-Empowering-Language-Models-for-Code-Editing"><a href="#InstructCoder-Empowering-Language-Models-for-Code-Editing" class="headerlink" title="InstructCoder: Empowering Language Models for Code Editing"></a>InstructCoder: Empowering Language Models for Code Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20329">http://arxiv.org/abs/2310.20329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qishenghu/CodeInstruct">https://github.com/qishenghu/CodeInstruct</a></li>
<li>paper_authors: Qisheng Hu, Kaixin Li, Xu Zhao, Yuxi Xie, Tiedong Liu, Hui Chen, Qizhe Xie, Junxian He</li>
<li>for: 本研究旨在探讨使用大型自然语言模型（LLM）自动编辑代码，以满足开发者日常几种实际任务的需求。</li>
<li>methods: 本研究使用 GitHub 提交作为种子任务，并通过 ChatGPT 进行反射编辑任务，逐渐扩展 InstructCoder 数据集，以适应更广泛的代码编辑任务。</li>
<li>results: 研究发现，通过对 InstructCoder 数据集进行 fine-tuning，可以使 open-source LLM  Correctly 编辑代码 Based on 用户指令大多数情况下，表明了训练指令 fine-tuning 可以提高代码编辑能力。<details>
<summary>Abstract</summary>
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our experiments demonstrate that open-source LLMs fine-tuned on InstructCoder can edit code correctly based on users' instructions most of the time, exhibiting unprecedented code-editing performance levels. Such results suggest that proficient instruction-finetuning can lead to significant amelioration in code editing abilities. The dataset and the source code are available at https://github.com/qishenghu/CodeInstruct.
</details>
<details>
<summary>摘要</summary>
开发者日常需要完成一系列的实用任务，代码编辑是其中的一个重要部分。尽管代码编辑具有重要的实用性和现实意义，但是自动代码编辑还是深入探索的领域之一，其中一个原因是数据的缺乏。在这项工作中，我们explore了使用大型自然语言模型（LLM）来编辑代码，以满足用户的指令。我们引入了InstructCoder，这是首个适用于通用代码编辑的大型自然语言模型dataset，包含了广泛的隐藏任务，如注释插入、代码优化和代码重构。这个dataset通过一种递归的方法来扩展，从GitHub提交中提取代码编辑数据作为种子任务，然后使用生成的任务来让ChatGPT生成更多的任务数据。我们的实验表明，通过训练在InstructCoder上的开源LLM可以根据用户的指令编辑代码正确的大部分时间，达到了前所未有的代码编辑性能水平。这些结果表明，可以通过练习任务来提高代码编辑的能力。dataset和源代码可以在https://github.com/qishenghu/CodeInstruct上获取。
</details></li>
</ul>
<hr>
<h2 id="ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science"><a href="#ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science" class="headerlink" title="ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science"></a>ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20328">http://arxiv.org/abs/2310.20328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Max J. van Duijn, Suzan Verberne, Marco R. Spruit</li>
<li>for: 研究children的语言和认知发展，使用计算机工具。</li>
<li>methods:  compiling a new corpus of 619 fantasy stories told freely by 442 Dutch children aged 4-12, with text, audio, and annotations for character complexity and linguistic complexity, as well as additional metadata for one third of the children.</li>
<li>results:  showcasing the potential of the corpus with three case studies: stability of syntactic complexity across ages, obedience to Zipf’s law, and richness of the corpus for training informative lemma vectors.<details>
<summary>Abstract</summary>
In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show that even though ChiSCor is relatively small, the corpus is rich enough to train informative lemma vectors that allow us to analyse children's language use. We end with a reflection on the value of narrative datasets in computational linguistics.
</details>
<details>
<summary>摘要</summary>
在这份资源文章中，我们发布了一个新的资料库，即ChiSCor，其包含了619则幻想故事，这些故事由442名荷兰儿童 aged 4-12年old所自由地告诉。ChiSCor是为了研究儿童如何表达角色的视角，以及语言和认知在发展中的关系，而编制的。与现有资源不同，ChiSCor的故事在自然的上下文中进行制作，与最近的呼吁更多的生态学有效的数据集相符。ChiSCor包含文本、音频和注释，以及人工智能工具。此外，有一些附加的元数据（例如照顾者的教育水平）可以为一 third of the Dutch children。ChiSCor还包含62则英语故事。这篇文章介绍了如何编制ChiSCor，并显示了它的潜在价值，通过三个简要的案例研究：一、我们发现儿童的故事语言 sintactic complexity是年龄的稳定的;二、我们扩展了在自由语言中Zipfian Distribution的研究，并发现ChiSCor准确遵循社会上的Zipf's Law;三、我们发现尽管ChiSCor较小，但这个资料库具有足够的资料，可以训练有用的语料 vectors，以便分析儿童的语言使用。文章结束时，我们反思了计算语言学中的narritive数据集的价值。
</details></li>
</ul>
<hr>
<h2 id="Erato-Automatizing-Poetry-Evaluation"><a href="#Erato-Automatizing-Poetry-Evaluation" class="headerlink" title="Erato: Automatizing Poetry Evaluation"></a>Erato: Automatizing Poetry Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20326">http://arxiv.org/abs/2310.20326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manexagirrezabal/erato">https://github.com/manexagirrezabal/erato</a></li>
<li>paper_authors: Manex Agirrezabal, Hugo Gonçalo Oliveira, Aitor Ormazabal</li>
<li>for: 这篇论文是为了描述一种自动评价诗歌的框架，帮助分析和比较人类创作的诗歌和机器生成的诗歌。</li>
<li>methods: 这篇论文使用了多种特征，包括语言预测、语义分析和语音分析等，以评价诗歌的质量和特点。</li>
<li>results: 通过使用这种框架，研究人员可以准确地分辨人类创作的诗歌和机器生成的诗歌之间的关键差异，并且可以对诗歌进行更好的评价和分析。<details>
<summary>Abstract</summary>
We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
</details>
<details>
<summary>摘要</summary>
我们介绍Erato框架，用于自动评估诗歌，包括由诗歌生成系统生成的诗歌。我们的框架使用多种特征，我们提供了Erato的功能和扩展性的简要概述。使用Erato，我们比较和比较了人类作者创作的诗歌和自动生成的诗歌，展示了它的效果。我们的实现代码和软件都是根据GNU GPLv3许可证发布的自由软件。
</details></li>
</ul>
<hr>
<h2 id="FA-Team-at-the-NTCIR-17-UFO-Task"><a href="#FA-Team-at-the-NTCIR-17-UFO-Task" class="headerlink" title="FA Team at the NTCIR-17 UFO Task"></a>FA Team at the NTCIR-17 UFO Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20322">http://arxiv.org/abs/2310.20322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Okumura, Masato Fujitake</li>
<li>for: 本研究参与了NTCIR-17年度理解非财务对象financial报告中的表数据提取（TDE）和文本到表关系提取（TTRE）任务。</li>
<li>methods: 我们采用了基于ELECTRA语言模型的多种优化技术来提取表中的有价值数据。</li>
<li>results: 我们的努力得到了93.43%的TDE准确率，在排名榜上名列第二名，这是我们提出的方法的效果的证明。<details>
<summary>Abstract</summary>
The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
</details>
<details>
<summary>摘要</summary>
FA团队参与了NTCIR-17年度《非财务对象在财务报告中理解》（UFO）中的表数据抽取（TDE）和文本到表关系抽取（TTRE）任务。本文介绍我们的解决方案和官方结果。我们成功地应用了基于ELECTRA语言模型的各种优化技术，从表中提取有价值数据。我们的努力得分为93.43%，在排名榜上名列第二位。这一优异成绩是我们提议的方法效果的证明。在TTRE任务中，我们提出了规则基本方法来提取文本和表之间的有意义关系，并证实了性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Entities-of-Interest-from-Comparative-Product-Reviews"><a href="#Extracting-Entities-of-Interest-from-Comparative-Product-Reviews" class="headerlink" title="Extracting Entities of Interest from Comparative Product Reviews"></a>Extracting Entities of Interest from Comparative Product Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20274">http://arxiv.org/abs/2310.20274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jatinarora2702/Review-Information-Extraction">https://github.com/jatinarora2702/Review-Information-Extraction</a></li>
<li>paper_authors: Jatin Arora, Sumit Agrawal, Pawan Goyal, Sayan Pathak</li>
<li>for: 这 paper 是为了提取在用户评论中的产品比较信息而设计的深度学习方法。</li>
<li>methods: 这 paper 使用 LSTM  capture 产品比较信息中的依赖关系，并且使用现有的手动标注数据进行评估。</li>
<li>results: 评估结果显示，这 paper 的方法在产品比较信息抽取 task 中表现出色，比Semantic Role Labeling (SRL) 框架更高效。<details>
<summary>Abstract</summary>
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis"><a href="#Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis" class="headerlink" title="Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis"></a>Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20260">http://arxiv.org/abs/2310.20260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/resrepos/leap">https://github.com/resrepos/leap</a></li>
<li>paper_authors: Haifa Alrdahi, Riza Batista-Navarro</li>
<li>for: 这个论文旨在探讨使用棋盘游戏教学资料来帮助机器学习棋盘游戏。</li>
<li>methods: 该论文使用了Transformer基础模型进行情感分析，以评估棋盘游戏中的搬砖落。</li>
<li>results: 实验结果表明，使用Transformer基础模型进行情感分析是可行的，最高得分为68%。此外，该论文还将LEAP集成体组织成更大的数据集，可以用于解决棋盘游戏领域的文本资源匮乏问题。<details>
<summary>Abstract</summary>
Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various transformer-based baseline models for sentiment analysis. Our results demonstrate the feasibility of employing transformer-based sentiment analysis models for evaluating chess moves, with the best performing model obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP corpus to create a larger dataset, which can be used as a solution to the limited textual resource in the chess domain.
</details>
<details>
<summary>摘要</summary>
学习国际象棋策略的研究已经广泛进行，大多数研究都是通过搜索算法学习 previous games。国际象棋书籍概括大师的知识，解释棋略和需要较小的搜索空间，相比于传统的国际象棋机器人。这篇论文探讨国际象棋书籍作为新的知识源，以便让机器学习如何玩国际象棋 -- 这是一个未曾探讨的资源。我们开发了LEAP corpus，一个新的、多元的数据集，包括结构化数据（棋子移动notation和棋盘状态）和无结构化数据（文本描述），从一本包含1164句描述策略移动的国际象棋书籍中收集到。我们首先对句子进行了 relevance 标注，即是否讲述了一个移动。每个相关的句子 then 按照其对描述的移动的 sentiment 进行标注。我们进行了employm empirical experiments，评估了多种基于 transformer 的基线模型在 sentiment analysis 方面的性能。我们的结果表明，可以使用 transformer 基eline模型进行评估棋子移动的 sentiment，最好performing model 在weighted micro F_1 score 上得分 68%。最后，我们合并了LEAP corpus，创建了一个更大的数据集，可以用于解决国际象棋领域中文本资源的有限性。
</details></li>
</ul>
<hr>
<h2 id="PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection"><a href="#PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection" class="headerlink" title="PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection"></a>PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20256">http://arxiv.org/abs/2310.20256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taoyang225/psycot">https://github.com/taoyang225/psycot</a></li>
<li>paper_authors: Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu</li>
<li>for: 本研究旨在探索大语言模型（LLMs）在人性探测领域中的潜力，以及如何通过 incorporating 心理测试问naire（Psychological Questionnaires）中的 Item 进行更好的人性探测。</li>
<li>methods: 我们提出了一种新的人性探测方法，即 PsyCoT，它通过在多turn dialogue 中让 AI 助手（GPT-3.5）评分 Psychological Questionnaires 中的 Item，并利用历史评分结果来 derive 个人人ality preference。</li>
<li>results: 我们的实验结果表明，PsyCoT 方法可以提高 GPT-3.5 在人性探测任务中的表现和稳定性，相比标准提问方法，平均提高 F1 分数4.23&#x2F;10.63点在两个 benchmark 数据集上。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM），如ChatGPT，已经展现出了无需训练的Zero-shot性能在各种自然语言处理任务中。然而，LLM在人格探测方面的潜在能力仍然尚未得到充分利用。drawing inspiration from psychological questionnaires，which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items，we argue that these items can be regarded as a collection of well-structured chain-of-thought（CoT）processes。by incorporating these processes，LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input。in light of this，we propose a novel personality detection method，called PsyCoT，which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner。in particular，we employ a LLM as an AI assistant with a specialization in text analysis。we prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference。our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection，achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method。our code is available at https://github.com/TaoYang225/PsyCoT。
</details></li>
</ul>
<hr>
<h2 id="Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning"><a href="#Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning" class="headerlink" title="Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning"></a>Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20236">http://arxiv.org/abs/2310.20236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi</li>
<li>for: This paper is written for the task of temporal relation classification, specifically for identifying the relationship between two mentions (events, times, and document creation times) in a text.</li>
<li>methods: The paper proposes an event-centric model that allows for managing dynamic event representations across multiple temporal links (TLINKs). The model uses multi-task learning to leverage the full size of the data and improves upon state-of-the-art models and two transfer learning baselines.</li>
<li>results: The experimental results show that the proposed model outperforms existing models and baselines on both English and Japanese data.<details>
<summary>Abstract</summary>
Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>时间关系分类是一对一任务，用于确定两个提及（TLINK）之间的时间关系，即事件、时间和文档创建时间（DCT）之间的关系。这两个限制：1）两个TLINK都不能共享信息。2）现有的模型具有独立的分类器 для每个TLINK类（E2E、E2T和E2D），使得不能使用整个数据集。本文提出了一种事件中心模型，可以在多个TLINK之间管理动态事件表示。我们的模型处理三种TLINK类型，通过多任务学习来利用整个数据集。实验结果表明，我们的提议在英文和日语数据上比STATE-OF-THE-ART模型和两个基于传输学习的基线模型表现出色。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History"><a href="#General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History" class="headerlink" title="General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History"></a>General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20204">http://arxiv.org/abs/2310.20204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starmpcc/remed">https://github.com/starmpcc/remed</a></li>
<li>paper_authors: Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-Gil Jeong, Edward Choi</li>
<li>for: 提高电子医疗记录（EHR）中的临床预测模型（如mortality预测）的开发效率，并且可以自动选择相关的临床事件和预测窗口大小。</li>
<li>methods: 提出了一种基于Retrieval-Enhanced Medical prediction model（REMed）的方法，可以自动评估无数量的临床事件，选择相关的事件，并进行预测。这种方法可以减少临床专家的干预，并且可以不受观察窗口大小的限制。</li>
<li>results: 通过对27个临床任务和两个独立的EHR数据集进行实验，发现REMed可以与其他同时处理多个事件的建筑物相比，在预测性能上表现出色。此外，发现REMed的偏好与医生的偏好吻唇。我们预计，这种方法将能够大幅提高EHR预测模型的开发效率，并减少临床专家的干预。<details>
<summary>Abstract</summary>
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.
</details>
<details>
<summary>摘要</summary>
Traditional clinical prediction models (例如 Mortality prediction) based on electronic health records (EHRs) usually rely on expert opinion for feature selection and adjusting observation window size. This puts a burden on experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address these challenges. REMed can evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.Here's a breakdown of the translation:1. "Traditional clinical prediction models" is translated as "传统的临床预测模型".2. "based on electronic health records" is translated as "基于电子医疗记录".3. "usually rely on expert opinion" is translated as "通常依赖专家意见".4. "for feature selection and adjusting observation window size" is translated as " для特征选择和观察窗口大小调整".5. "This puts a burden on experts" is translated as "这会对专家造成压力".6. "and creates a bottleneck in the development process" is translated as "并导致开发过程中的瓶颈".7. "We propose Retrieval-Enhanced Medical prediction model (REMed)" is translated as "我们提议的是 Retrieval-Enhanced Medical prediction model (REMed)".8. "REMed can evaluate an unlimited number of clinical events" is translated as "REMed可以评估无限多的临床事件".9. "select the relevant ones" is translated as "选择相关的那些".10. "and make predictions" is translated as "并预测".11. "This approach eliminates the need for manual feature selection" is translated as "这种方法消除了手动特征选择的需求".12. "and enables an unrestricted observation window" is translated as "并允许无限制的观察窗口".13. "We verified these properties through experiments on 27 clinical tasks" is translated as "我们通过27个临床任务的实验验证了这些性质".14. "and two independent cohorts" is translated as "以及两个独立的凝聚体".15. "from publicly available EHR datasets" is translated as "从公开可用的EHR数据集中".16. "where REMed outperformed other contemporary architectures" is translated as "在其他当代架构上，REMed超越了其他的表现".17. "Notably, we found that the preferences of REMed align closely with those of medical experts" is translated as "值得注意的是，REMed的偏好与医疗专家的偏好很相似".18. "We expect our approach to significantly expedite the development of EHR prediction models" is translated as "我们期望我们的方法能够快速推动EHR预测模型的开发".19. "by minimizing clinicians' need for manual involvement" is translated as "通过减少临床人员的手动参与".
</details></li>
</ul>
<hr>
<h2 id="Video-Helpful-Multimodal-Machine-Translation"><a href="#Video-Helpful-Multimodal-Machine-Translation" class="headerlink" title="Video-Helpful Multimodal Machine Translation"></a>Video-Helpful Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20201">http://arxiv.org/abs/2310.20201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ku-nlp/video-helpful-mmt">https://github.com/ku-nlp/video-helpful-mmt</a></li>
<li>paper_authors: Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li</li>
<li>for: 这个研究的目的是提高多Modal Machine Translation（MMT）的性能，使其能更好地处理含歧义的字幕。</li>
<li>methods: 这个研究使用了一个新的 dataset called EVA，该dataset包含了852万个日语英文（Ja-En）并行字幕对，520万个中文英文（Zh-En）并行字幕对，以及对应的视频clip。此外，研究还提出了一种基于Selective Attention模型的两种新方法：Frame attention loss和Ambiguity augmentation，以使用视频来解决歧义。</li>
<li>results: 实验表明，使用视频和提议的方法可以提高翻译性能，并且我们的模型与现有的 MMT 模型相比，表现出了显著的优势。<details>
<summary>Abstract</summary>
Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details>
<details>
<summary>摘要</summary>
现有的多modal机器翻译（MMT）数据集包括图像和视频标题或教程视频字幕，这些rarely包含语言歧义，使视觉信息成为不合适的翻译生成。 recient work constructedaambiguous subtitles dataset to alleviate this problem, but it is still limited to the problem that videos do not necessarily contribute to disambiguation. 我们介绍EVA（Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation），一个MMT数据集，包含852k日语英文（Ja-En）并行字幕对，520k中文英文（Zh-En）并行字幕对，并与其对应的电影和电视剧视频片断集成。除了广泛的训练集之外，EVA还包含一个视频helpful评估集，其中字幕是歧义的，而视频则保证可以减少歧义。此外，我们提出了SAFA（Selective Attention模型），一种基于选择性注意力模型的两种新方法：Frame attention loss和Ambiguity augmentation，旨在使用EVA中的视频完全使用视频来减少歧义。EVA数据集和SAFA模型在https://github.com/ku-nlp/video-helpful-MMT.git中可以获取。Note: "Simplified Chinese" is a romanization of the Chinese language using the simplified characters, which are commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text"><a href="#DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text" class="headerlink" title="DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text"></a>DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20170">http://arxiv.org/abs/2310.20170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, Semih Yavuz</li>
<li>for: This paper aims to address the issue of hallucinations in large language models (LLMs) when answering questions that require less commonly known information.</li>
<li>methods: The authors propose a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval, to ground LLMs in external knowledge. They also introduce a comprehensive dataset that poses two unique challenges: two-hop multi-source questions and the generation of symbolic queries.</li>
<li>results: The authors’ model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在生成方面表现出色，但它们在仅仅基于自己的内部知识时会出现幻觉，特别是当面对需要更少知道的信息时。在grounding LLMs in external knowledge的问题上， Retrieval-augmented LLMs  emerged as a potential solution。然而， current approaches  mainly emphasize retrieval from unstructured text corpora，它们可以轻松地整合到提问中。而使用结构化数据，如知识图，大多数方法将其简化成自然文本，忽略了下面结构。此外，当前领域中存在一个重要的空白是评估基于不同知识源（例如知识库和文本）的LLMs的效果的准确的benchmark。为了填充这一空白，我们创建了一个全面的数据集，其 poses two unique challenges：（1）两步多源问题，需要从开放领域结构化知识源和自然文本知识源中检索信息，从结构化知识源中检索信息是 correctly answering questions 的关键组成部分。（2）生成符号 queries（例如SPARQL для Wikidata）是一种关键的要求，这 adds another layer of challenge。我们的数据集通过自动生成和人工标注来创建，并引入了一种多工具重力学习方法，包括文本段 retrieval和符号语言协助 retrieve。我们的模型在以上理解挑战方面表现出了显著的优异，证明了它的效果。
</details></li>
</ul>
<hr>
<h2 id="GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval"><a href="#GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval" class="headerlink" title="GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval"></a>GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20158">http://arxiv.org/abs/2310.20158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, Amit Sharma</li>
<li>For:  Zero-shot passage retrieval task, to output a ranked list of relevant documents.* Methods:  Combining large language models (LLMs) with embedding-based retrieval models, using generation-augmented retrieval (GAR) and retrieval-augmented generation (RAG) paradigms.* Results:  Establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.<details>
<summary>Abstract</summary>
Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Agent-Consensus-Seeking-via-Large-Language-Models"><a href="#Multi-Agent-Consensus-Seeking-via-Large-Language-Models" class="headerlink" title="Multi-Agent Consensus Seeking via Large Language Models"></a>Multi-Agent Consensus Seeking via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20151">http://arxiv.org/abs/2310.20151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao</li>
<li>for: This paper explores consensus-seeking in multi-agent systems driven by large language models (LLMs) to understand how they can reach a consensus through inter-agent negotiation.</li>
<li>methods: The paper uses LLMs to drive the agents and studies the impact of agent number, agent personality, and network topology on the negotiation process.</li>
<li>results: The paper finds that the LLM-driven agents primarily use the average strategy for consensus seeking, and demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks through an application to a multi-robot aggregation task.<details>
<summary>Abstract</summary>
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details>
<details>
<summary>摘要</summary>
多智能体系驱动by大语言模型（LLM）已经展示了解决复杂任务的潜力。这项工作考虑到多智能体系协作中的一个基本问题：协商达成共识。当多个智能体系合作时，我们关心如何使其们通过间智能体之间的谈判达成共识值。为了实现这一目标，这项工作研究了智能体系的共识寻求任务，其中每个智能体的状态是一个数字值，它们之间进行谈判以达成共识值。研究发现，当不直接指导其所采取的策略时，LLM驱动的智能体主要使用平均策略进行协商达成共识，尽管它们可能会occasionally采用其他策略。此外，这项工作分析了智能体数量、智能体性格和网络拓扑对谈判过程的影响。这些发现可能为LLM驱动多智能体系解决更复杂任务提供基础。此外，LLM驱动的共识寻求还应用于多机器人聚合任务，这种应用示了LLM驱动智能体可以实现零例自主规划的多机器人协作任务。项目网站：westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details></li>
</ul>
<hr>
<h2 id="DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models"><a href="#DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models" class="headerlink" title="DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models"></a>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20138">http://arxiv.org/abs/2310.20138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</li>
<li>for: 防止语言模型中的数据泄露</li>
<li>methods: 使用检测私隐神经方法和修改私隐神经方法来降低数据泄露风险</li>
<li>results: 能够有效地降低数据泄露风险，无需影响模型性能<details>
<summary>Abstract</summary>
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练数据的巨量资料中获得了丰富的知识和信息。这些已训练的语言模型中的数据记忆和重复能力，在先前的研究中已经被揭露出来，带来数据泄露的风险。为了有效地减少这些风险，我们提出了DEPN框架，用于检测和修改隐私神经元。在DEPN中，我们提出了一种新的方法，即隐私神经元检测方法，以找到对隐私信息有关的神经元，然后将这些检测到的隐私神经元的活化设置为零。此外，我们提出了一种隐私神经元组合器，用于批处理中对隐私信息进行匿名处理。实验结果显示，我们的方法可以对隐私泄露进行有效和高效的隐藏，不会对模型性能产生负面影响。此外，我们也实践了模型记忆和隐私神经元之间的关系，从多种角度进行探索，包括模型大小、训练时间、提示、隐私神经元分布，以证明我们的方法的稳定性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Prompt-Tuning-with-Learned-Prompting-Layers"><a href="#Improving-Prompt-Tuning-with-Learned-Prompting-Layers" class="headerlink" title="Improving Prompt Tuning with Learned Prompting Layers"></a>Improving Prompt Tuning with Learned Prompting Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20127">http://arxiv.org/abs/2310.20127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhu, Ming Tan</li>
<li>for: 这个研究是为了提高预训练模型（PTM）的适应能力和下游任务的效能。</li>
<li>methods: 这个研究使用选择性问题预训练（SPT）方法，将问题层新增到PTM的每个中间层，并通过一个可读的概率门控制问题的选择。</li>
<li>results: 在十个基准数据集下，这个研究的结果显示了SPT方法可以比前一代PETuning基eline更好地适应预训练，并且仅需比较少的参数进行调整。<details>
<summary>Abstract</summary>
Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
Prompt tuning 是一种 prepends a soft prompt to the input embeddings or hidden states，并仅仅优化 prompt 以适应预训练模型（PTM）到下游任务。以前的工作 manually selects prompt layers，这些层数远离优化的，而且无法充分发挥提前uning的潜力。在这种工作中，我们提出了一个新的框架，Selective Prompt Tuning（SPT），它可以学习选择合适的 prompt layers。我们还提出了一个新的两级优化框架，SPT-DARTS，它可以更好地优化可学习的门控和提高最终的提前uning性能。我们在十个标准 benchmark 数据集下进行了广泛的实验，包括全数据和少shotenario。结果表明，我们的 SPT 框架可以与之前的 state-of-the-art PETuning 基线比较或少于 Tunable 参数来perform better。
</details></li>
</ul>
<hr>
<h2 id="Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula"><a href="#Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula" class="headerlink" title="Ling-CL: Understanding NLP Models through Linguistic Curricula"></a>Ling-CL: Understanding NLP Models through Linguistic Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20121">http://arxiv.org/abs/2310.20121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clu-uml/ling-cl">https://github.com/clu-uml/ling-cl</a></li>
<li>paper_authors: Mohamed Elgaar, Hadi Amiri</li>
<li>for: 本研究旨在开发基于数据驱动的课程，以优化语言处理任务中的语言知识。</li>
<li>methods: 本研究使用心理语言学和语言学习研究中的语言复杂度特征来开发课程，并分析了多个标准的NLU任务数据集，以确定每个任务所需的语言知识。</li>
<li>results: 本研究通过分析多个标准NLU任务数据集，发现了一些语言指标（指标），这些指标可以描述每个任务所需的语言知识和逻辑处理能力。这些结果可以帮助未来的NLU研究人员更好地考虑语言复杂度，从而提高NLU模型的性能。<details>
<summary>Abstract</summary>
We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
</details>
<details>
<summary>摘要</summary>
我们采用语言复杂性Characterization从心理语言学和语言学习研究来开发数据驱动课程，以理解模型学习NLP任务时所需的基础语言知识。我们的创新在于基于数据、现有语言复杂性知识和模型训练过程中的行为来开发语言课程。我们分析了多个标准NLP数据集，我们的课程学习方法可以帮助您了解每个任务所需的语言复杂性指标。我们的工作将影响未来所有NLP领域的研究，让语言复杂性在研究和开发过程中得到考虑。此外，我们的工作也适得以考虑黄金标准和公平评价在NLP中。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Data-Creators"><a href="#Making-Large-Language-Models-Better-Data-Creators" class="headerlink" title="Making Large Language Models Better Data Creators"></a>Making Large Language Models Better Data Creators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20111">http://arxiv.org/abs/2310.20111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-data-creation">https://github.com/microsoft/llm-data-creation</a></li>
<li>paper_authors: Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar</li>
<li>for: 提高NLG系统的可靠性和一致性</li>
<li>methods: 使用LLM生成数据，并使用单个格式示例创建数据创建管道</li>
<li>results: 模型使用自动生成的数据表现比人工标注数据更好（17.5%），同时保持与标准任务的相似性<details>
<summary>Abstract</summary>
Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a unified data creation pipeline that requires only a single formatting example and is applicable to a broad range of tasks, including those with semantically devoid label spaces. Our experiments show that instruction-following LLMs are highly cost-effective data creators, and models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details></li>
</ul>
<hr>
<h2 id="Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning"><a href="#Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning" class="headerlink" title="Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning"></a>Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20089">http://arxiv.org/abs/2310.20089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenia Alleva, Isotta Landi, Leslee J Shaw, Erwin Böttinger, Thomas J Fuchs, Ipek Ensari</li>
<li>for: 这个研究是为了提高临床病历分类任务的效果，但是已有的数据集很少。</li>
<li>methods: 这个研究使用了提问基本学习方法，并对模型进行了优化。关键是提取模型中的模板（即提问文本），并且研究了模板的位置对性能的影响。</li>
<li>results: 研究发现，通过优化模板的位置，可以提高临床病历分类任务的性能，尤其是在零例学习和几例学习的情况下。<details>
<summary>Abstract</summary>
Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/" data-id="clogy1z3800emffraal5egv9r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.LG_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T10:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.LG_2023_10_31/">cs.LG - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization"><a href="#Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization" class="headerlink" title="Unexpected Improvements to Expected Improvement for Bayesian Optimization"></a>Unexpected Improvements to Expected Improvement for Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20708">http://arxiv.org/abs/2310.20708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</li>
<li>for: 提高 Bayesian 优化中的 acquisition function 性能</li>
<li>methods: 提出 LogEI 家族的新 acquisition function，其成员在数值优化中更易优化，并且可以提高 optimization 性能</li>
<li>results: 实验结果显示 LogEI 家族的 acquisition function 可以substantially 提高 Bayesian 优化中的性能，并且与最新的 state-of-the-art acquisition function 相当或超越其性能， highlighting the understated role of numerical optimization in the literature.<details>
<summary>Abstract</summary>
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.
</details>
<details>
<summary>摘要</summary>
预期改进（EI）是搜索优化中最受欢迎的功能之一，在搜索优化中发现了 countless 的成功应用，但其表现通常被更新的方法超越。尤其是EI和其变体，包括并行和多目标设置，在数值上是困难优化的，因为其获取值在许多地方会数值消失。这种困难通常随着观察次数、搜索空间维度或约束数量增加，导致文献中表现不一致，通常是低效的。在这里，我们提出了 LogEI，一种新的获取函数家族，其成员在Canonical counterparts中具有相同或相近的极值，但是数值上更容易优化。我们证明了经典的EI、Expected Hypervolume Improvement（EHVI）以及其约束、噪音和并行变体中的数值问题，并提出了相应的改进方案。我们的实验结果表明，LogEI家族中的获取函数可以大幅提高与Canonical counterparts的优化表现，另外，与当前状态的最佳获取函数相当或超越其表现，这 highlights the understated role of numerical optimization in the literature。
</details></li>
</ul>
<hr>
<h2 id="Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search"><a href="#Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search" class="headerlink" title="Farthest Greedy Path Sampling for Two-shot Recommender Search"></a>Farthest Greedy Path Sampling for Two-shot Recommender Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20705">http://arxiv.org/abs/2310.20705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Cao, Tunhou Zhang, Wei Wen, Feng Yan, Hai Li, Yiran Chen</li>
<li>for: 提高深度推荐模型的开发效率和性能。</li>
<li>methods: 使用Weight-sharing Neural Architecture Search (WS-NAS)机制，并引入新的path sampling策略Farthest Greedy Path Sampling (FGPS)，以提高搜索空间的覆盖率和subnet重量的协同适应性。</li>
<li>results: 通过在Three Click-Through Rate (CTR) prediction benchmark上进行评估，发现该方法可以准确地识别和利用优秀的建筑，并在大多数NAS基础模型上显著超越 manual设计和其他 NAS 模型。<details>
<summary>Abstract</summary>
Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient mechanism for developing end-to-end deep recommender models. However, in complex search spaces, distinguishing between superior and inferior architectures (or paths) is challenging. This challenge is compounded by the limited coverage of the supernet and the co-adaptation of subnet weights, which restricts the exploration and exploitation capabilities inherent to weight-sharing mechanisms. To address these challenges, we introduce Farthest Greedy Path Sampling (FGPS), a new path sampling strategy that balances path quality and diversity. FGPS enhances path diversity to facilitate more comprehensive supernet exploration, while emphasizing path quality to ensure the effective identification and utilization of promising architectures. By incorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive high-performance architectures. Evaluations on three Click-Through Rate (CTR) prediction benchmarks demonstrate that our approach consistently achieves superior results, outperforming both manually designed and most NAS-based models.
</details>
<details>
<summary>摘要</summary>
��wort-sharing Neural Architecture Search (WS-NAS) 提供了一种高效的终端深度推荐模型开发机制。然而，在复杂的搜索空间中， отличить优于劣的架构（或路径）是困难的。这种挑战更加困难由超网络的limited coverage和子网重量协作所带来的搜索和利用机制的限制。为解决这些挑战，我们介绍了远程艰苟路径采样（FGPS），一种新的路径采样策略，该策略平衡路径质量和多样性。FGPS通过增强超网络搜索的多样性，以便更全面地探索超网络，同时强调路径质量，以确保有效地识别和利用优秀的架构。通过将FGPS纳入TS-NAS框架中，我们 derivation高性能的架构。对三个Click-Through Rate（CTR）预测 benchmark中的评估表明，我们的方法能够一直保持优秀的 результа，与 manually designed 和大多数 NAS-based 模型相比，表现出优。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods"><a href="#Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods" class="headerlink" title="Bayesian Multistate Bennett Acceptance Ratio Methods"></a>Bayesian Multistate Bennett Acceptance Ratio Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20699">http://arxiv.org/abs/2310.20699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinqiang Ding</li>
<li>for: 计算热动力学状态的自由能量</li>
<li>methods:  bayesian generalization of MBAR method, integrating configurations sampled from thermodynamic states with a prior distribution to compute posterior distribution of free energies</li>
<li>results: 提供了更准确的自由能量估计和相关的不确定性计算Please note that the above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespread use in free energy calculations, we anticipate BayesMBAR to be an essential tool in various applications of free energy calculations.
</details>
<details>
<summary>摘要</summary>
多态Bennett接受度方法（MBAR）是一种广泛应用的计算热力学状态自由能的方法。在这项工作中，我们介绍了抽象MBAR方法的 bayesian扩展方法——抽象MBAR方法。通过将thermodynamic状态中采样的配置与一个先验分布集成起来，抽象MBAR方法计算出了后验自由能分布。使用后验分布，我们计算出自由能估计值和相关的不确定性。吸引注意的是，当使用均匀先验分布时，抽象MBAR方法可以重新计算MBAR方法的结果，并提供更加准确的不确定性估计。此外，当有具体关于自由能的先验知识时，抽象MBAR方法可以在计算过程中integrate这些信息，通过使用非均匀先验分布来提高估计精度。作为一个例子，我们显示了，通过在自由能表面的平滑性信息上 incorporate先验知识，抽象MBAR方法可以提供更加准确的估计值。由于MBAR方法在自由能计算中广泛应用，我们预计抽象MBAR方法将成为许多应用中的关键工具。
</details></li>
</ul>
<hr>
<h2 id="Compression-with-Exact-Error-Distribution-for-Federated-Learning"><a href="#Compression-with-Exact-Error-Distribution-for-Federated-Learning" class="headerlink" title="Compression with Exact Error Distribution for Federated Learning"></a>Compression with Exact Error Distribution for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20682">http://arxiv.org/abs/2310.20682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Hegazy, Rémi Leluc, Cheuk Ting Li, Aymeric Dieuleveut</li>
<li>for: 本文研究了在 Federated Learning（FL）中使用压缩方案，以降低分布式学习中的通信成本。</li>
<li>methods: 本文提出了基于层次量化的各种压缩和聚合方案，以实现特定的错误分布，如 Gaussian 或 Laplace 分布。</li>
<li>results: 我们的方法可以在 differential privacy 应用中实现压缩-for-free，并且可以提高标准的 FL 方案中的 Gaussian 噪声，如 Langevin 动力学和随机缓和。<details>
<summary>Abstract</summary>
Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在分布式学习（Federated Learning，FL）中，压缩方案广泛应用于减少分布式学习的通信成本。而大多数方法都假设压缩器生成的噪声具有有界的方差，这篇论文则研究使用压缩和汇总方案生成特定的错误分布，例如高斯或勒贝 Distribution，在汇总数据中。我们提出和分析不同层次量化器基于的汇总方案，实现精确的错误分布。我们还提供不同的方法来利用我们的压缩方案在隐私保护应用中实现压缩-for-free。我们的通用压缩方法可以恢复和改进标准FL方案中的高斯噪声，例如谱温动和随机缓和。
</details></li>
</ul>
<hr>
<h2 id="Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields"><a href="#Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields" class="headerlink" title="Latent Field Discovery In Interacting Dynamical Systems With Neural Fields"></a>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20679">http://arxiv.org/abs/2310.20679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkofinas/aether">https://github.com/mkofinas/aether</a></li>
<li>paper_authors: Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</li>
<li>for: 本文旨在发现场景中的场效果，而不直接观察场效果。</li>
<li>methods: 本文提出了 neural fields 来学习场效果，并将 мест交互作用模型为 $\mathrm{SE}(n)$  equivariant graph networks。</li>
<li>results: 实验表明，可以准确发现场景中的场效果，并使用其来预测未来轨迹。<details>
<summary>Abstract</summary>
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models"><a href="#Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models" class="headerlink" title="Balancing Act: Constraining Disparate Impact in Sparse Models"></a>Balancing Act: Constraining Disparate Impact in Sparse Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20673">http://arxiv.org/abs/2310.20673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/merajhashemi/balancing_act">https://github.com/merajhashemi/balancing_act</a></li>
<li>paper_authors: Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi, Simon Lacoste-Julien, Jose Gallego-Posada</li>
<li>for: 这篇论文旨在提出一种可以在边缘设备上部署大型深度学习模型的方法，并且能够降低模型的硬件要求和存储空间。</li>
<li>methods: 这篇论文使用了一种受限的优化方法来降低模型的硬件要求和存储空间，并且直接地处理过滤后的精度下降。</li>
<li>results: 实验结果显示，这篇论文的方法可以可靠地应用于大型模型和许多保护的子集之间，并且能够提供可靠的精度下降 bound。<details>
<summary>Abstract</summary>
Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that $\textit{directly addresses the disparate impact of pruning}$: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.
</details>
<details>
<summary>摘要</summary>
We propose a constrained optimization approach that directly addresses the disparate impact of pruning. Our formulation sets bounds on the accuracy change between the dense and sparse models for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results show that our technique can be reliably applied to problems involving large models and hundreds of protected sub-groups.
</details></li>
</ul>
<hr>
<h2 id="Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction"><a href="#Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction" class="headerlink" title="Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction"></a>Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20671">http://arxiv.org/abs/2310.20671</a></li>
<li>repo_url: None</li>
<li>paper_authors: José Daniel Viqueira, Daniel Faílde, Mariamo M. Juane, Andrés Gómez, David Mera</li>
<li>for: 本研究旨在实现多变量时间序列预测的量子循环神经网络（QRNN）模型，但现有的量子硬件限制了一些QRNN模型的可实现性。为此，我们提出了一种特定的模拟方法，基于density matrix formalism。</li>
<li>methods: 我们使用了tensor notation来提供一种紧凑的数学开发，以显示如何在模拟网络中传递当前和过去时间序列信息，并如何在每个时间步骤中减少计算成本。此外，我们 derivated了模型输出的导数和希尔бер特，以便使用梯度下降法进行训练和处理真实量子处理器的噪声输出。</li>
<li>results: 我们使用了一种新的硬件高效的 Ansatz 和三种多元和单元时间序列数据集来测试我们的方法。结果表明，QRNNs 可以准确预测输入序列中的未来值，并capture不同复杂度的输入序列的非常贯通 patterns。<details>
<summary>Abstract</summary>
Quantum Recurrent Neural Networks (QRNNs) are robust candidates to model and predict future values in multivariate time series. However, the effective implementation of some QRNN models is limited by the need of mid-circuit measurements. Those increase the requirements for quantum hardware, which in the current NISQ era does not allow reliable computations. Emulation arises as the main near-term alternative to explore the potential of QRNNs, but existing quantum emulators are not dedicated to circuits with multiple intermediate measurements. In this context, we design a specific emulation method that relies on density matrix formalism. The mathematical development is explicitly provided as a compact formulation by using tensor notation. It allows us to show how the present and past information from a time series is transmitted through the circuit, and how to reduce the computational cost in every time step of the emulated network. In addition, we derive the analytical gradient and the Hessian of the network outputs with respect to its trainable parameters, with an eye on gradient-based training and noisy outputs that would appear when using real quantum processors. We finally test the presented methods using a novel hardware-efficient ansatz and three diverse datasets that include univariate and multivariate time series. Our results show how QRNNs can make accurate predictions of future values by capturing non-trivial patterns of input series with different complexities.
</details>
<details>
<summary>摘要</summary>
量子循环神经网络（QRNN）是Multivariate时间序列预测的稳定候选人，但有些QRNN模型的实施效率受到中间测量的限制。这些测量会增加量子硬件的需求，现今的NISQ时代不可靠计算。虚拟实现成为离散QRNN的主要近期代替方案，但现有的量子虚拟器并不适用于多个中间测量的环circuit。在这种情况下，我们设计了专门的虚拟方法，基于density matrix formalism。我们使用tensor notation提供了一种紧凑的表述，以显示环circuit传递当前和过去时间序列信息，并如何在每个时间步骤中减少计算成本。此外，我们derive了网络输出的分析导数和Hessian，以便使用梯度下降法和噪声输出来训练网络。最后，我们使用一种硬件高效的ansatz和三个多样化的时间序列Dataset进行测试，其中包括单variate和多variate时间序列。我们的结果表明，QRNN可以准确预测未来值，并捕捉输入序列的不同复杂性中的非rival patterns。
</details></li>
</ul>
<hr>
<h2 id="Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes"><a href="#Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes" class="headerlink" title="Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes"></a>Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20641">http://arxiv.org/abs/2310.20641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celal Alagoz</li>
<li>for: 这个研究探讨了幂等分类（HC）在多类分类任务中的表现，特别是无法直接获取先验知识结构的场景下。</li>
<li>methods: 研究涉及了 hierarchy generation和 hierarchy exploitation的两个方面，并提出了两种新的 hierarchy exploitation scheme，LCPN+和LCPN+F，以及对现有方法的评估。</li>
<li>results: 研究发现LCPN+F在多个数据集和场景下表现出了一致的优势，并且与平面分类（FC）的运行时间性能相似。此外，研究还强调了选择合适的 hierarchy exploitation scheme可以最大化分类性能。<details>
<summary>Abstract</summary>
Hierarchical classification (HC) plays a pivotal role in multi-class classification tasks, where objects are organized into a hierarchical structure. This study explores the performance of HC through a comprehensive analysis that encompasses both hierarchy generation and hierarchy exploitation. This analysis is particularly relevant in scenarios where a predefined hierarchy structure is not readily accessible. Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, which extend the capabilities of LCPN and combine the strengths of global and local classification, have been introduced and evaluated alongside existing methods. The findings reveal the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios. Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain runtime performance comparable to Flat Classification (FC). Additionally, this study underscores the importance of selecting the right hierarchy exploitation scheme to maximize classification performance. This work extends our understanding of HC and establishes a benchmark for future research, fostering advancements in multi-class classification methodologies.
</details>
<details>
<summary>摘要</summary>
Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, have been introduced and evaluated, which extend the capabilities of LCPN and combine the strengths of global and local classification. The results consistently show that LCPN+F outperforms other schemes across various datasets and scenarios.Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain comparable runtime performance to Flat Classification (FC). This study highlights the importance of selecting the appropriate hierarchy exploitation scheme to maximize classification performance.This work extends our understanding of HC and establishes a benchmark for future research, paving the way for advancements in multi-class classification methodologies.
</details></li>
</ul>
<hr>
<h2 id="Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression"><a href="#Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression" class="headerlink" title="Projecting basis functions with tensor networks for Gaussian process regression"></a>Projecting basis functions with tensor networks for Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20630">http://arxiv.org/abs/2310.20630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Menzen, Eva Memmel, Kim Batselier, Manon Kok</li>
<li>for: 这 paper 的目的是提出一种 Approximate Gaussian Process（GP） regression 方法，使用 tensor networks（TNs）来实现。</li>
<li>methods: 这 paper 使用了一种基于 tensor networks 的方法，通过在低维度子空间中解决 Bayesian  inference 问题来找到模型的参数。</li>
<li>results: 在一个 18 维数据集上进行了一个 inverse dynamics 问题的实验，得到了模型的应用。<details>
<summary>Abstract</summary>
This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the smaller subspace. In an experiment with an 18-dimensional benchmark data set, we show the applicability of our method to an inverse dynamics problem.
</details>
<details>
<summary>摘要</summary>
Our method addresses this limitation by using low-rank TNs to reduce the computational complexity. We first find a suitable low-dimensional subspace from the data using a low-rank TN. In this low-dimensional subspace, we then solve a Bayesian inference problem to infer the weights of our model. Finally, we project the resulting weights back to the original space to make GP predictions.The key advantage of our approach is the projection to a smaller subspace, which modifies the shape of the basis functions in a way that is suitable for the given data. This allows for efficient computations in the smaller subspace, making our method scalable to large datasets. We demonstrate the applicability of our method on an 18-dimensional benchmark data set for an inverse dynamics problem.
</details></li>
</ul>
<hr>
<h2 id="Graph-Matching-via-convex-relaxation-to-the-simplex"><a href="#Graph-Matching-via-convex-relaxation-to-the-simplex" class="headerlink" title="Graph Matching via convex relaxation to the simplex"></a>Graph Matching via convex relaxation to the simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20609">http://arxiv.org/abs/2310.20609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Araya Valdivia, Hemant Tyagi</li>
<li>For: 该论文解决了图像匹配问题，即输入两个图像的最佳对齐问题，该问题在计算机视觉、网络匿名化和蛋白质对齐等领域都有广泛的应用。* Methods: 该论文提出了一种新的对体矩阵的凸 relaxation，并开发了一种高效的镜像下降算法来解决该问题。在某些特定的 Gaussian Wigner 模型下，我们证明了对体矩阵relaxation的解是唯一的，并且在无噪 случа子下能够精确地回归真实的Permutation。* Results: 在无噪 случа子下，我们证明了镜像下降算法可以在一步内回归真实的Permutation，并且在某些特定的输入矩阵下，我们可以获得更好的回归结果，比如使用镜像下降算法可以在一步内回归true Permutation。<details>
<summary>Abstract</summary>
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).   Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme, in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.
</details>
<details>
<summary>摘要</summary>
Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this implies exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used "diagonal dominance" condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.文章关注图像匹配问题，该问题的目标是找到两个输入图的最佳对应，并有广泛的应用于计算机视觉、网络匿名化和蛋白Alignment等领域。通常通过NP困难的quadratic assignment problem（QAP）的凸relaxation来解决该问题。在本文中，我们引入了一个新的凸relaxation onto the unit simplex，并开发了一种高效的mirror descent scheme with closed-form iterations来解决该问题。在相关的 Gaussian Wigner模型下，我们证明了unit simplex relaxation admits a unique solution with high probability。在无噪case，这imply exact recovery of the ground truth permutation。此外，我们还设立了一个新的sufficiency condition for the input matrix in standard greedy rounding methods，这个condition是less restrictive than the commonly used "diagonal dominance" condition。我们使用这个condition来show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme in the noiseless setting。此外，我们还使用这个condition来 obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting。
</details></li>
</ul>
<hr>
<h2 id="Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms"><a href="#Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms" class="headerlink" title="Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms"></a>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20598">http://arxiv.org/abs/2310.20598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</li>
<li>for: 这个论文研究了在互联网上进行转换的成本，具体来说是在一段时间 horizon 内，一个在线玩家尝试在每个时间步骤上购买（或者卖出）一个资产的剩余部分，并且每次决定时都会付出一个转换成本。</li>
<li>methods: 作者提出了一种基于阈值的在线算法，可以在 deterministic 上实现最优性，并且在某些情况下可以通过学习扩展来提高性能。</li>
<li>results: 作者通过一个碳素识别的电动车充电场景来实验研究，并证明了他们的算法可以相比基准方法具有显著改善。<details>
<summary>Abstract</summary>
We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significantly better average-case performance without sacrificing worst-case competitive guarantees. Finally, we empirically evaluate our proposed algorithms using a carbon-aware EV charging case study, showing that our algorithms substantially improve on baseline methods for this problem.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究在线转换问题，这是一种涉及到能源和可持续发展的新兴问题。在这个问题中，一名在线玩家尝试在固定时间Interval $[T]$ 内购买（或销售）资产的分数部分。在每个时间步骤中，一个成本函数（或价格函数）会被公布，玩家必须不可逆决定一定量的资产转换。在连续的两个时间步骤中，如果玩家的决定发生变化，例如增加或减少购买量，就会产生交易成本。我们提出了竞争（可靠）阈值基于算法，以便在排序算法中实现最佳性。然后，我们提出了学习增强算法，可以利用不可信的黑盒建议（如机器学习模型的预测）来实现较好的平均情况性能而无需牺牲最坏情况竞争保证。最后，我们employs empirical evaluation using a carbon-aware EV charging case study, showing that our algorithms significantly improve on baseline methods for this problem.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning"><a href="#Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning" class="headerlink" title="Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"></a>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20587">http://arxiv.org/abs/2310.20587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</li>
<li>For: The paper is focused on developing a new framework called Language Models for Motion Control (LaMo) that leverages pre-trained language models (LMs) to improve offline reinforcement learning (RL) in real-world scenarios where data collection is limited.* Methods: The LaMo framework uses pre-trained LMs to initialize Decision Transformers, which are then fine-tuned using the LoRA method to combine pre-trained knowledge and in-domain knowledge. The framework also employs a non-linear MLP transformation to generate embeddings and integrates an auxiliary language prediction loss during fine-tuning to stabilize the LMs.* Results: The paper demonstrates that LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks, particularly in scenarios with limited data samples.Here’s the information in Simplified Chinese:* For: 该论文目标是提出一种基于语言模型（LM）的无线 Reinforcement Learning（RL）框架，以便在实际场景中，充分利用预收集的数据来实现更好的控制。* Methods: 该框架使用预训练的 LM 初始化决策变换器，并使用 LoRA 方法来有效地结合预训练知识和地区知识。框架还使用非线性 MLP 变换来生成嵌入，并在精度优化中添加语言预测损失以稳定 LM。* Results: 论文表明，LaMo 在稀有奖励任务中达到了状态的最佳性能，并在具有限制数据amples的情况下追赶到了值基于 offline RL 方法和决策变换器的性能。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io
</details>
<details>
<summary>摘要</summary>
偏向学习（Offline Reinforcement Learning）的目标是找到近似优质策略，使用预收集的数据进行训练。在现实世界中，数据收集可能是成本高昂且危险的，因此偏向学习在域内数据有限的情况下变得特别挑战。基于最新的大语言模型（Large Language Models，LLMs）和它们的几shot学习能力，本文引入了Language Models for Motion Control（LaMo）框架，用于有效地使用预训练的语言模型（LMs） для偏向学习。我们的框架包括四个关键组成部分：1. 使用顺序预训练的语言模型初始化决策变换器。2. 使用LoRA fine-tuning方法，而不是全量 fine-tuning，将预训练知识从语言模型和域内知识相结合。3. 使用非线性多层Perceptron变换器而不是线性投影，生成嵌入。4. 在精度uning过程中添加语言预测损失，以稳定语言模型并保持它们的原始语言能力。实验结果表明，LaMo在稀缺奖励任务中达到了状态机器人学习的最佳性能，并在 dense-reward 任务中追caught up with Decision Transformers。尤其是在数据样本有限的情况下，LaMo表现出色。更多信息可以查看我们的项目网站：<https://lamo2023.github.io>
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right"><a href="#Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right" class="headerlink" title="Stochastic Gradient Descent for Gaussian Processes Done Right"></a>Stochastic Gradient Descent for Gaussian Processes Done Right</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20581">http://arxiv.org/abs/2310.20581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári, José Miguel Hernández-Lobato, David Janz</li>
<li>for: 本文关注的问题是 Gaussian process regression 的优化问题，使用平方损失函数。</li>
<li>methods: 本文使用的方法包括 Stochastic Dual Gradient Descent 算法，以及特定的设计选择，如减少维度版本的问题。</li>
<li>results: 本文的实验结果表明，当使用特定的设计选择和优化策略时，Stochastic Dual Gradient Descent 算法可以高效地解决 Gaussian process regression 的优化问题，并在标准回归benchmark和 Bayesian 优化任务中表现出色，与前置 conjugate gradients、variational Gaussian process approximations 和之前的 Stochastic Gradient Descent 算法相比。在一个分子绑定亲和力预测任务中，本文的方法使 Gaussian process regression 与STATE-OF-THE-ART graph neural networks 的性能相当。<details>
<summary>Abstract</summary>
We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from preconditioned conjugate gradients, variational Gaussian process approximations, and a previous version of stochastic gradient descent for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with state-of-the-art graph neural networks.
</details>
<details>
<summary>摘要</summary>
我们研究了 Gaussian process regression 的优化问题，使用平方损失函数。最常见的方法是使用精确解算法，如 conjugate gradient descent，直接或将问题缩放到更少的维度上进行解决。在这篇文章中，我们表明了使用 deep learning 的成功吸引了人们的注意力，并且在我们的方法中使用了特定的优化和核心社区的知识，从而在解决方法中获得了高效性。我们提出了一种特定的随机对偶梯度下降算法，可以使用任何深度学习框架进行实现，并且我们解释了我们的设计决策，并通过缺失研究和比较分析表明了它们的优势。我们的评估结果表明，我们的方法在标准的回归测试集和 Bayesian 优化任务上与 préconditioned conjugate gradients、variational Gaussian process approximation 和之前的随机梯度下降方法相比，具有更高的竞争力。在一个分子绑定亲和力预测任务上，我们的方法将 Gaussian process regression 与状态之前的 graph neural networks 的性能均衡。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks"><a href="#Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks" class="headerlink" title="Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks"></a>Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20579">http://arxiv.org/abs/2310.20579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher</li>
<li>for: 本研究探讨随机机器学习算法中模型过参数化对训练数据信息泄露的影响。</li>
<li>methods: 研究者利用了Randomized Matrix Method和Expected Gradient Method来分析模型的隐私泄露。</li>
<li>results: 研究者发现，模型的深度会影响隐私泄露的规模，并且初始化分布的选择会影响模型的隐私性。 Specifically, for the special setting of linearized network, the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution.<details>
<summary>Abstract</summary>
We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization.
</details>
<details>
<summary>摘要</summary>
我们分析了随机机器学习算法中模型过参数化对训练数据信息泄露的影响。我们证明了一个隐私约束 для KL散度 между模型分布在最差邻居数据集上，并研究了其与初始化、宽度和深度相关。我们发现这个 KL 隐私约束主要取决于在训练过程中模型参数的预期平方Gradient norm。特别是在特殊设置下（线性化网络），我们的分析表明，预期平方Gradient norm（以及因此隐私损失的增长）与初始化分布的每层方差直接相关。通过这种分析，我们示出了在某些初始化情况下（如LeCun和Xavier），隐私约束随着深度增长而改善，而在其他初始化情况下（如He和NTK），隐私约束随着深度增长而下降。我们的工作揭示了depth和初始化distribution之间的复杂互动，以及隐私和深度之间的负面相互作用。此外，我们还证明了固定 KL 隐私预算下的过项预测误差上限，并显示了隐私利用均衡和深度之间的相互作用。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization"><a href="#Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization" class="headerlink" title="Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization"></a>Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20574">http://arxiv.org/abs/2310.20574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alrhub/arturo">https://github.com/alrhub/arturo</a></li>
<li>paper_authors: Philipp Dahlinger, Philipp Becker, Maximilian Hüttenrauch, Gerhard Neumann</li>
<li>for: 优化神经网络的梯度下降法是非常重要的。而现有的方法通过减小步长和方向的灵活调整，这些方法是尝试性的，而不是理论基础的。这种方法可以通过对目标函数的辛勤约束来改善优化器。</li>
<li>methods: 我们提议使用信息理论信任区优化（arTuRO），它使用神经网络参数的 Gaussian 分布，使用 Kullback-Leibler 偏度信任区来实现更稳定和更快的优化过程。在每次更新之前，它解决了一个优化问题，以获得最佳步长。</li>
<li>results: 我们表明，arTuRO 可以结合适应式 momentum-based 优化的快速收敛和神经网络优化的通用性。<details>
<summary>Abstract</summary>
Stochastic gradient-based optimization is crucial to optimize neural networks. While popular approaches heuristically adapt the step size and direction by rescaling gradients, a more principled approach to improve optimizers requires second-order information. Such methods precondition the gradient using the objective's Hessian. Yet, computing the Hessian is usually expensive and effectively using second-order information in the stochastic gradient setting is non-trivial. We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process. We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details>
<details>
<summary>摘要</summary>
“Stochastic gradient-based优化是神经网络优化的关键。各种方法尝试通过缩放梯度来适应步长和方向，但是这些方法并不是非常原理性的。实际上，使用目标函数的Hessian来预condition gradients是一种更原理性的方法。然而，计算Hessian通常是非常昂贵的，而且在梯度计算中使用第二个信息在随机 gradient  Setting 中是非常困难的。我们提出了基于信息理论的信任区域优化（arTuRO），它可以在随机 gradient  Setting 中提供更好的更新，并考虑到目标函数的凹陷和参数的不确定性。在每次更新之前，arTuRO 会解决一个信任区域问题，以获得最佳步长，从而使优化过程更加稳定和快速。我们使用梯度的权重来 aproximate Hessian 的对角元素，并使用一种简单的回归最小二乘方法来构建一个时间上的 Hessian 模型。我们表明，arTuRO 可以结合适应型 momentum 的优化和 SGD 的泛化能力。”
</details></li>
</ul>
<hr>
<h2 id="One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification"><a href="#One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification" class="headerlink" title="One-shot backpropagation for multi-step prediction in physics-based system identification"></a>One-shot backpropagation for multi-step prediction in physics-based system identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20567">http://arxiv.org/abs/2310.20567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesare Donati, Martina Mammarella, Fabrizio Dabbene, Carlo Novara, Constantino Lagoa</li>
<li>for: 这篇论文旨在提出一种全新的可以保持物理性质的多步预测系统识别框架。</li>
<li>methods: 该方法使用了分析性的回潘迪法计算多步损失函数的衰变，从而提供物理和结构直观到学习算法中。</li>
<li>results: 作为一个案例研究，该方法在估算空间废弃物的拟合矩阵上进行了测试，并取得了高准确率和物理意义的结果。<details>
<summary>Abstract</summary>
The aim of this paper is to present a novel general framework for the identification of possibly interconnected systems, while preserving their physical properties and providing accuracy in multi-step prediction. An analytical and recursive algorithm for the gradient computation of the multi-step loss function based on backpropagation is introduced, providing physical and structural insight directly into the learning algorithm. As a case study, the proposed approach is tested for estimating the inertia matrix of a space debris starting from state observations.
</details>
<details>
<summary>摘要</summary>
本文的目的是提出一种新的总体框架，用于可能相互连接的系统的特征标识，同时保持物理性质和多步预测的准确性。我们提出了一种分析和递归的梯度计算方法，基于反射学习，以直接将物理和结构性质反映到学习算法中。为了实践，我们对Space debris的惯性矩进行估算，并从状态观察结果进行测试。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning"><a href="#Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning" class="headerlink" title="Privacy-preserving design of graph neural networks with applications to vertical federated learning"></a>Privacy-preserving design of graph neural networks with applications to vertical federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20552">http://arxiv.org/abs/2310.20552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruofan Wu, Mingyang Zhang, Lingjuan Lyu, Xiaolong Xu, Xiuquan Hao, Xinyi Fu, Tengfei Liu, Tianyi Zhang, Weiqiang Wang</li>
<li>for: 这篇 paper 的目的是提出一个基于 Vertical Federated Learning (VFL) 的数据加权学习框架，以实现金融风险管理 (FRM) 应用中的高性能。</li>
<li>methods: 这篇 paper 使用了一种称为 perturbed message passing (PMP) 的通用隐私保证方案，并将其应用于多个受欢迎的图 neural network 架构中。</li>
<li>results: 这篇 paper 的实验结果显示，VESPER 能够在理想的隐私预算下训练高性能的图神经网络模型，并且在实际应用中也能够实现高性能。<details>
<summary>Abstract</summary>
The paradigm of vertical federated learning (VFL), where institutions collaboratively train machine learning models via combining each other's local feature or label information, has achieved great success in applications to financial risk management (FRM). The surging developments of graph representation learning (GRL) have opened up new opportunities for FRM applications under FL via efficiently utilizing the graph-structured data generated from underlying transaction networks. Meanwhile, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees. In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing (PMP) that allows the privatization of many popular graph neural architectures.Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details>
<details>
<summary>摘要</summary>
Vertical Federated Learning (VFL) 的 paradigm, where institutions collaboratively train machine learning models by combining each other's local feature or label information, 在金融风险管理 (FRM) 应用中取得了很大成功。随着图表学习 (GRL) 的发展，开创了新的FRM应用场景，通过高效地利用基于交易网络生成的图structured data。然而，交易信息经常被视为高度敏感。为了防止训练过程中的数据泄露，在FL中发展 Privacy 保障的协议是 kritical。本文提出了一种基于杂化消息传递 (PMP) 的综合隐私化框架，称为 VESPER，可以隐私化多种流行的图神经网络架构。基于 PMP，我们讨论了特定设计选择的强点和弱点，并提供了对 both dense 和稀疏图的解决方案。我们进行了广泛的实验评估，证明 VESPER 可以在理想的隐私预算下训练高性能 GNN 模型 both dense 和稀疏图。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-of-convex-combinations-of-forecasting-models"><a href="#Multi-task-learning-of-convex-combinations-of-forecasting-models" class="headerlink" title="Multi-task learning of convex combinations of forecasting models"></a>Multi-task learning of convex combinations of forecasting models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20545">http://arxiv.org/abs/2310.20545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoniosudoso/mtl-comb">https://github.com/antoniosudoso/mtl-comb</a></li>
<li>paper_authors: Giovanni Felici, Antonio M. Sudoso</li>
<li>for: 这篇论文旨在实现多预测模型的结合，以提高预测精度。</li>
<li>methods: 本文提出了一个基于多任务学习的方法，通过一个深度神经网络，分别执行预测模型选择和预测模型结合两个任务。</li>
<li>results: 实验结果显示，该方法可以对大量的时间序列数据进行改进预测，并且比前一代方法更高的精度。<details>
<summary>Abstract</summary>
Forecast combination involves using multiple forecasts to create a single, more accurate prediction. Recently, feature-based forecasting has been employed to either select the most appropriate forecasting models or to learn the weights of their convex combination. In this paper, we present a multi-task learning methodology that simultaneously addresses both problems. This approach is implemented through a deep neural network with two branches: the regression branch, which learns the weights of various forecasting methods by minimizing the error of combined forecasts, and the classification branch, which selects forecasting methods with an emphasis on their diversity. To generate training labels for the classification task, we introduce an optimization-driven approach that identifies the most appropriate methods for a given time series. The proposed approach elicits the essential role of diversity in feature-based forecasting and highlights the interplay between model combination and model selection when learning forecasting ensembles. Experimental results on a large set of series from the M4 competition dataset show that our proposal enhances point forecast accuracy compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
预测组合是使用多个预测来创建一个更准确的预测。最近，基于特征的预测组合被应用于选择最合适的预测模型或学习预测模型的权重。在这篇论文中，我们提出了一种多任务学习方法，同时解决了这两个问题。这种方法通过一个深度神经网络，其中有两个分支：回归分支，通过最小化组合预测错误来学习预测模型的权重；分类分支，通过强调多样性来选择预测模型。为生成训练标签的分类任务，我们提出了一种优化驱动的方法，可以为给定时间序列选择最合适的预测方法。我们的方法强调了特征基于预测组合中的多样性，并高亮了组合预测模型和选择预测模型在学习预测集中的互动关系。实验结果表明，我们的提议在M4竞赛数据集上实现了点预测精度的提高，比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks"><a href="#Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks" class="headerlink" title="Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks"></a>Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20524">http://arxiv.org/abs/2310.20524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aytijhya Saha, Nikhil R. Pal</li>
<li>for: 这个论文旨在提出一种基于多层感知网络的嵌入特征选择方法，并将其推广到组特征或感知器选择问题，以控制选择的特征之间的重复率。</li>
<li>methods: 该方法使用了多层感知网络和一种普适的梯度下降算法，并将 GROUP LASSO 罚项推广为选择有价值的组特征，同时保持特征之间的重复率控制。</li>
<li>results: 实验结果表明，提出的方法在一些标准数据集上具有优秀的表现，并且在特征选择和组特征选择方面比一些现有方法更为有效。<details>
<summary>Abstract</summary>
In this paper, we present a novel embedded feature selection method based on a Multi-layer Perceptron (MLP) network and generalize it for group-feature or sensor selection problems, which can control the level of redundancy among the selected features or groups. Additionally, we have generalized the group lasso penalty for feature selection to encompass a mechanism for selecting valuable group features while simultaneously maintaining a control over redundancy. We establish the monotonicity and convergence of the proposed algorithm, with a smoothed version of the penalty terms, under suitable assumptions. Experimental results on several benchmark datasets demonstrate the promising performance of the proposed methodology for both feature selection and group feature selection over some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多层感知网络（MLP）的嵌入式特征选择方法，并推广到组特征或感知器选择问题，可以控制选择的特征或组中的重复性水平。此外，我们扩展了组lasso penalty的特征选择机制，以同时选择价值的组特征，并保持重复性控制。我们证明了提案的算法假设下的卷积和满足条件下的升oothed penalty term的减少性和收敛性。实验结果表明，提案的方法在多个标准数据集上表现出色，在特征选择和组特征选择方面超过了一些现有方法。
</details></li>
</ul>
<hr>
<h2 id="Parametric-Fairness-with-Statistical-Guarantees"><a href="#Parametric-Fairness-with-Statistical-Guarantees" class="headerlink" title="Parametric Fairness with Statistical Guarantees"></a>Parametric Fairness with Statistical Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20508">http://arxiv.org/abs/2310.20508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paramfair/submission_974">https://github.com/paramfair/submission_974</a></li>
<li>paper_authors: François HU, Philipp Ratz, Arthur Charpentier</li>
<li>for: This paper focuses on improving algorithmic fairness in machine learning models by incorporating domain knowledge and addressing issues related to intersectional fairness.</li>
<li>methods: The paper introduces a new metric called Distributional Demographic Parity (DDP) that incorporates distributional properties in the predictions, allowing for the use of expert knowledge in the fair solution. The paper also develops a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending.</li>
<li>results: The paper demonstrates the effectiveness of the proposed method through a practical example of wages and shows that it can provide a more robust solution for real-life applications compared to traditional fairness metrics like Equalized Odds and Demographic Parity.<details>
<summary>Abstract</summary>
Algorithmic fairness has gained prominence due to societal and regulatory concerns about biases in Machine Learning models. Common group fairness metrics like Equalized Odds for classification or Demographic Parity for both classification and regression are widely used and a host of computationally advantageous post-processing methods have been developed around them. However, these metrics often limit users from incorporating domain knowledge. Despite meeting traditional fairness criteria, they can obscure issues related to intersectional fairness and even replicate unwanted intra-group biases in the resulting fair solution. To avoid this narrow perspective, we extend the concept of Demographic Parity to incorporate distributional properties in the predictions, allowing expert knowledge to be used in the fair solution. We illustrate the use of this new metric through a practical example of wages, and develop a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending, offering a robust solution for real-life applications.
</details>
<details>
<summary>摘要</summary>
“算法公平”在社会和 regulatory 问题中得到了更多的注意，因为机器学习模型中的偏见问题。常见的集体公平度量如“平等概率”和“人口平衡”在分类和回归方面都广泛使用，但这些度量通常会限制用户不能够考虑专家知识。尽管遵循传统的公平准则，它们可能会隐藏 intersectional 公平问题，甚至在对内部偏见的解决方案中重复不良的内部偏见。为了避免这种狭隘的视野，我们将“人口平衡”概念扩展到预测中的分布性质，以便使用专家知识。我们透过一个实际的薪资例子来显示这个新的度量，并开发了一个可效的 parametric 方法来解决实际应用中的实际挑战，如有限的训练数据和总预算的限制，提供了一个坚固的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Learning-of-Continuous-Data-by-Tensor-Networks"><a href="#Generative-Learning-of-Continuous-Data-by-Tensor-Networks" class="headerlink" title="Generative Learning of Continuous Data by Tensor Networks"></a>Generative Learning of Continuous Data by Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20498">http://arxiv.org/abs/2310.20498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Meiburg, Jing Chen, Jacob Miller, Raphaëlle Tihon, Guillaume Rabusseau, Alejandro Perdomo-Ortiz</li>
<li>for:  solves machine learning problems, especially unsupervised generative learning</li>
<li>methods:  uses tensor network generative models for continuous data, with a new family of models based on matrix product states</li>
<li>results:  the model can approximate any reasonably smooth probability density function with arbitrary precision, and performs well on synthetic and real-world datasets with both continuous and discrete variables.<details>
<summary>Abstract</summary>
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.
</details>
<details>
<summary>摘要</summary>
对于模型多体量子系统的起源，tensor network已经发展成为解决机器学习问题的有力的模型，尤其是不监督生成学习。这些量子感应的特性具有许多权威的特点，但是它们之前只能用于二进制或分类数据，限制了它们在实际应用中的用途。我们在matrix product states中引入了一新的家族tensor network生成模型，可以从包含连续随机变量的分布中学习。我们首先证明这个模型家族可以对任何合理平滑概率密度函数进行拟合，然后在几个 sintetic和实际数据集上进行了实验，发现这个模型可以从不同的数据域中学习和推导。此外，我们还开发了可调弹性压缩层，对于有限的内存或计算资源，可以提高模型的性能。总的来说，我们的方法给出了量子感应方法在快速增长的生成学习领域中的重要理论和实验证据。
</details></li>
</ul>
<hr>
<h2 id="BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis"><a href="#BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis" class="headerlink" title="BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis"></a>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20496">http://arxiv.org/abs/2310.20496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzl5116190/basisformer">https://github.com/nzl5116190/basisformer</a></li>
<li>paper_authors: Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</li>
<li>for: 这个研究旨在提出一个可学习且可解释的基底架构，以便实现时间序列预测的精确预测。</li>
<li>methods: 这个架构包括三个部分：首先，通过自适应式无监督学习，获取基底，并运用对比学习。接着，我们设计了一个 Coef 模组，通过两向交叉注意力计算时间序列和基底之间的相似度。最后，我们呈现了一个预测模组，根据相似度选择和聚合基底，实现精确的未来预测。</li>
<li>results: 经过六个数据集的广泛实验，我们证明了 BasisFormer 比前一代方法提高了11.04%和15.78%的精确预测性。代码可以在：\url{<a target="_blank" rel="noopener" href="https://github.com/nzl5116190/Basisformer%7D">https://github.com/nzl5116190/Basisformer}</a>  obtain.<details>
<summary>Abstract</summary>
Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04\% and 15.78\% respectively for univariate and multivariate forecasting tasks. Code is available at: \url{https://github.com/nzl5116190/Basisformer}
</details>
<details>
<summary>摘要</summary>
基于现代深度学习模型的时间序列预测中，基因已成为一个重要的组成部分，因为它们可以作为特征提取器或未来参照。为了有效，一个基因必须适应特定的时间序列数据集，并且与每个时间序列之间显示明显的相关性。然而，当前状态艺术方法受限于同时满足这两个需求的能力。为解决这个挑战，我们提出了 BasisFormer，一种终端时间序列预测架构，它利用可学习和可解释的基因。这个架构包括以下三个组成部分：1. 我们通过适应性自我超vised学习获取基因，将历史和未来部分视为两个不同的视图，并使用对比学习。2. 我们设计了 Coef模块，通过双向跨注意力对时间序列和基因之间进行对比，以计算时间序列和基因之间的相似度系数。3. 我们提出了 Forecast模块，根据相似度系数选择和组合未来视图中的基因，以实现准确的未来预测。通过对六个数据集进行广泛的实验，我们证明了 BasisFormer可以与之前的状态艺术方法相比，在单变量和多变量预测任务中提高11.04%和15.78%的性能。代码可以在以下链接获取：\url{https://github.com/nzl5116190/Basisformer}
</details></li>
</ul>
<hr>
<h2 id="Requirement-falsification-for-cyber-physical-systems-using-generative-models"><a href="#Requirement-falsification-for-cyber-physical-systems-using-generative-models" class="headerlink" title="Requirement falsification for cyber-physical systems using generative models"></a>Requirement falsification for cyber-physical systems using generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20493">http://arxiv.org/abs/2310.20493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshaheryarmalik/stgem">https://github.com/mshaheryarmalik/stgem</a></li>
<li>paper_authors: Jarkko Peltomäki, Ivan Porres</li>
<li>for:  automatic requirement falsification of cyber-physical systems</li>
<li>methods:  uses OGAN algorithm, which is a generative machine learning model to produce counterexamples</li>
<li>results:  can find inputs that are counterexamples for the safety of a system, revealing design, software, or hardware defects before the system is taken into operation, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.<details>
<summary>Abstract</summary>
We present the OGAN algorithm for automatic requirement falsification of cyber-physical systems. System inputs and output are represented as piecewise constant signals over time while requirements are expressed in signal temporal logic. OGAN can find inputs that are counterexamples for the safety of a system revealing design, software, or hardware defects before the system is taken into operation. The OGAN algorithm works by training a generative machine learning model to produce such counterexamples. It executes tests atomically and does not require any previous model of the system under test. We evaluate OGAN using the ARCH-COMP benchmark problems, and the experimental results show that generative models are a viable method for requirement falsification. OGAN can be applied to new systems with little effort, has few requirements for the system under test, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.
</details>
<details>
<summary>摘要</summary>
我们介绍了OGAN算法，用于自动发现游戏物理系统的需求虚拟。系统输入和输出被表示为时间上的分割定律信号，需求则以信号时间逻辑表示。OGAN可以找到系统的安全性缺陷，包括设计、软件或硬件的缺陷，以前设不可能找到。OGAN算法通过将生成机器学习模型用于生成这些Counterexample。它执行测试原子地，不需要任何关于系统下测试的先前模型。我们使用ARCH-COMP问题集进行评估，实验结果显示，生成模型是可靠的需求虚拟方法。OGAN可以对新的系统进行快速适用，具有少量的系统下测试需求，并且表现出了顶尖的CPS虚拟效率和有效性。
</details></li>
</ul>
<hr>
<h2 id="Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study"><a href="#Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study" class="headerlink" title="Log-based Anomaly Detection of Enterprise Software: An Empirical Study"></a>Log-based Anomaly Detection of Enterprise Software: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20492">http://arxiv.org/abs/2310.20492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadun Wijesinghe, Hadi Hemmati</li>
<li>for: 本研究旨在评估多种现有的异常检测模型在工业 dataset 上的表现，以及这些模型对不同类型异常的检测能力。</li>
<li>methods: 本研究使用了多种深度学习模型，包括 Long-Short Term Memory 和 Transformer 模型，进行日志数据中的异常检测。</li>
<li>results: 研究发现，不同的模型在不同类型异常中的检测能力有所不同，而且模型在 less-structured 数据集上的表现较好。 另外，通过移除一些常见的数据泄露，研究发现模型的效果会改善。<details>
<summary>Abstract</summary>
Most enterprise applications use logging as a mechanism to diagnose anomalies, which could help with reducing system downtime. Anomaly detection using software execution logs has been explored in several prior studies, using both classical and deep neural network-based machine learning models. In recent years, the research has largely focused in using variations of sequence-based deep neural networks (e.g., Long-Short Term Memory and Transformer-based models) for log-based anomaly detection on open-source data. However, they have not been applied in industrial datasets, as often. In addition, the studied open-source datasets are typically very large in size with logging statements that do not change much over time, which may not be the case with a dataset from an industrial service that is relatively new. In this paper, we evaluate several state-of-the-art anomaly detection models on an industrial dataset from our research partner, which is much smaller and loosely structured than most large scale open-source benchmark datasets. Results show that while all models are capable of detecting anomalies, certain models are better suited for less-structured datasets. We also see that model effectiveness changes when a common data leak associated with a random train-test split in some prior work is removed. A qualitative study of the defects' characteristics identified by the developers on the industrial dataset further shows strengths and weaknesses of the models in detecting different types of anomalies. Finally, we explore the effect of limited training data by gradually increasing the training set size, to evaluate if the model effectiveness does depend on the training set size.
</details>
<details>
<summary>摘要</summary>
大多数企业应用程序使用日志作为诊断异常的机制，以减少系统停机时间。异常检测使用软件执行日志已经在多个先前研究中进行了探讨，使用了古典和深度神经网络学习模型。在最近几年，研究主要集中在使用序列基本深度神经网络模型（如Long-Short Term Memory和Transformer-based模型）进行日志基本异常检测。然而，这些模型尚未在企业数据集中应用，因为企业数据集通常较小，不具备大规模开源数据集的特点。此外，研究使用的开源数据集通常很大，logging语句不变化很多，这可能不符合一个新的工业服务数据集。在这篇论文中，我们评估了一些当前领先的异常检测模型，在我们的研究合作伙伴提供的工业数据集上进行了评估。结果表明，虽然所有模型都能检测异常，但某些模型更适合不结构化数据集。我们还发现，模型的效果随着一种常见的数据泄露问题的消除而改变。在进行了开发者对工业数据集中异常的 качеitative研究后，我们发现了不同类型异常的特点。最后，我们研究了模型效果随着训练数据集大小的增加而变化。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations"><a href="#Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations" class="headerlink" title="Exploring Practitioner Perspectives On Training Data Attribution Explanations"></a>Exploring Practitioner Perspectives On Training Data Attribution Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20477">http://arxiv.org/abs/2310.20477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Nguyen, Evgenii Kortukov, Jean Song, Seong Joon Oh</li>
<li>for: This paper aims to understand the potential usefulness of training data attribution (TDA) explanations in Explainable AI (XAI) and to explore the design space of such an approach.</li>
<li>methods: The authors interviewed 10 practitioners to gather their insights on the importance of training data quality, their current practices for curating data, and their expectations for explanations in XAI.</li>
<li>results: The authors found that training data quality is a crucial factor for high model performance, and that model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model, but are not necessarily prioritizing TDA explanations. The authors also found that TDA explanations are not well-known and not widely used.Here are the same three points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是了解培训数据拟合（TDA）解释在可解释AI（XAI）中的可能用途，以及这种方法的设计空间。</li>
<li>methods: 作者们采访了10名实践者，了解他们对培训数据质量的重要性，他们当前的数据拟合方法，以及他们对XAI中的解释的期望。</li>
<li>results: 作者们发现，培训数据质量对高度模型性能非常重要，而模型开发者主要通过自己的经验来拟合数据。用户希望解释能够增强与模型的互动，但并不一定优先考虑TDA解释。作者们还发现，TDA解释并不很知名，并不广泛使用。<details>
<summary>Abstract</summary>
Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.
</details>
<details>
<summary>摘要</summary>
Explainable AI (XAI) 目的是为人类提供模型决策的理解，因此是一个横跨多个领域的领域。在这篇论文中，我们采访了10名实践者，以了解培训数据归因（TDA）解释的可用性，并探讨这种方法的设计空间。我们发现，在实践中，模型表现高的关键因素通常是培训数据质量，而模型开发者主要依靠自己的经验来选择数据。用户希望通过解释与模型进行交互，并不一定优先考虑培训数据，但是他们对培训数据作为解释的可能性存在开放。在我们的参与者中，我们发现TDA解释并不够熟悉，因此不被使用。我们建议社区在人机合作的视角下注重TDA技术的实用性，扩大TDA评估范围，以反映实际使用情况。
</details></li>
</ul>
<hr>
<h2 id="Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning"><a href="#Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning" class="headerlink" title="Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning"></a>Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20469">http://arxiv.org/abs/2310.20469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobile-intelligence-lab/amoeba">https://github.com/mobile-intelligence-lab/amoeba</a></li>
<li>paper_authors: Haoyu Liu, Alec F. Diallo, Paul Patras</li>
<li>for: circumventing Internet censorship</li>
<li>methods: using machine learning algorithms to detect anti-censorship systems, and a novel reinforcement learning algorithm called Amoeba to generate adversarial flows</li>
<li>results: an attack success rate of 94% against a range of ML algorithms, and robustness in different network environments and transferability across various ML models<details>
<summary>Abstract</summary>
Embedding covert streams into a cover channel is a common approach to circumventing Internet censorship, due to censors' inability to examine encrypted information in otherwise permitted protocols (Skype, HTTPS, etc.). However, recent advances in machine learning (ML) enable detecting a range of anti-censorship systems by learning distinct statistical patterns hidden in traffic flows. Therefore, designing obfuscation solutions able to generate traffic that is statistically similar to innocuous network activity, in order to deceive ML-based classifiers at line speed, is difficult.   In this paper, we formulate a practical adversarial attack strategy against flow classifiers as a method for circumventing censorship. Specifically, we cast the problem of finding adversarial flows that will be misclassified as a sequence generation task, which we solve with Amoeba, a novel reinforcement learning algorithm that we design. Amoeba works by interacting with censoring classifiers without any knowledge of their model structure, but by crafting packets and observing the classifiers' decisions, in order to guide the sequence generation process. Our experiments using data collected from two popular anti-censorship systems demonstrate that Amoeba can effectively shape adversarial flows that have on average 94% attack success rate against a range of ML algorithms. In addition, we show that these adversarial flows are robust in different network environments and possess transferability across various ML models, meaning that once trained against one, our agent can subvert other censoring classifiers without retraining.
</details>
<details>
<summary>摘要</summary>
使用嵌入式流入受控通道是常见的绕过互联网审查的方法，因为审查器无法检查加密的信息在允许的协议（Skype、HTTPS等）中。然而，最近的机器学习（ML）技术的进步使得可以检测一些防止审查系统的anti-censorship系统。因此，设计生成混淆解决方案，以生成与正常网络活动 statistically similar的流量，以欺骗基于机器学习的分类器，是困难的。在这篇论文中，我们提出了一种实用的反恶意攻击策略，作为审查系统的绕过方法。我们将问题定义为一种序列生成任务，并使用我们自己的新型迭代学习算法——Amoeba来解决。Amoeba通过与审查类ifiers无知的模型结构互动，通过制作包和观察类ifiers的决策，来引导序列生成过程。我们的实验使用了两个流行的反审查系统所收集的数据，显示Amoeba可以有效地生成94%的攻击成功率，对多种机器学习算法进行攻击。此外，我们还证明了这些攻击流量在不同的网络环境中具有可重复性和传输性，这意味着我们的代理可以在不需要重新训练的情况下，在其他审查类ifiers上采取行动。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-detects-terminal-singularities"><a href="#Machine-learning-detects-terminal-singularities" class="headerlink" title="Machine learning detects terminal singularities"></a>Machine learning detects terminal singularities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20458">http://arxiv.org/abs/2310.20458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/ml_terminality">https://bitbucket.org/fanosearch/ml_terminality</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这个论文的目的是研究代数变量的分类。</li>
<li>methods: 这个论文使用机器学习技术来理解代数变量的分类。</li>
<li>results: 这个论文使用 neural network 分类器预测8维 positively-curved 代数变量是否为 Q-Fano 变量，准确率达95%。此外，通过量子期的视图，发现这些分类结果在一个有界区域内，并且与 Fano 指数相关。这些结果提供了新的证明和推断 conjectures 的机会。<details>
<summary>Abstract</summary>
Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are "atomic pieces" of more complex shapes - the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. In this paper we demonstrate that machine learning can be used to understand this classification. We focus on 8-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank 2, and develop a neural network classifier that predicts with 95% accuracy whether or not such an algebraic variety is Q-Fano. We use this to give a first sketch of the landscape of Q-Fanos in dimension 8. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classification as revealed by ML appears to fall within a bounded region, and is stratified by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank 2 to have terminal singularities. Together with the first sketch of the landscape of Q-Fanos in higher dimensions, this gives new evidence that machine learning can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery.
</details>
<details>
<summary>摘要</summary>
代数变量是数学和科学中的几何形状，它们是一种系数为多项式的方程定义的。其中包括Q-Fano变量，它们是正几何形状，并且有Q因子终点特性。Q-Fano变量在几何中具有基本重要性，它们是更复杂形状的“原子部件”，通过分解这种形式的过程被称为最小模型 програм。尽管其分类仍未知，但我们在这篇论文中使用机器学习来理解这个分类。我们关注8维正几何变量，具有抽象群同质和 Picard rank 2，并开发了一个神经网络分类器，可以将95%的准确率地判断这种代数变量是否为Q-Fano。我们使用这种分类器来给8维Q-Fano变量的预览 landscape 提供第一个绘制。神经网络如何准确地检测Q-Fano变量的具体原理还未知，这表明存在深刻的数学理论 waits to be uncovered。此外，通过量子时期的视觉，我们发现分类结果在一定的 bounded 区域内，并且以 Fano 指数作为分类标准。这表明可能在未来验证完teness 的 conjecture。受机器学习分析的激发，我们提出并证明了一种全局 combinatorial 条件，它 garanties that a positively curved toric variety of Picard rank 2 has terminal singularities。此外，我们还给出了高维Q-Fano变量的首个预览，这给出了新的证明，表明机器学习可以成为数学推理的关键工具。
</details></li>
</ul>
<hr>
<h2 id="FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments"><a href="#FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments" class="headerlink" title="FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments"></a>FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20457">http://arxiv.org/abs/2310.20457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Unsal, Ali Maatouk, Antonio De Domenico, Nicola Piovesan, Fadhel Ayed</li>
<li>for: 这篇论文旨在解决深度学习模型在多种设备环境中的问题，因为这些模型的大小使得它们在低功率或资源受限的设备上难以部署，导致长时间的推理时间和高能耗。</li>
<li>methods: 这篇论文提出了一个名为FlexTrain的框架，可以在训练阶段满足不同设备的存储和计算资源的多样性。FlexTrain可以实现高效地部署深度学习模型，同时遵循设备限制，减少通信成本，并让不同设备进行顺毕合作。</li>
<li>results: 在CIFAR-100 dataset上，使用FlexTrain训练单一全球模型可以轻松地在多种设备上部署，从而降低训练时间和能耗。此外，这篇论文还将FlexTrain扩展到联邦学习设定下，证明了我们的方法在CIFAR-10和CIFAR-100 dataset上比标准联邦学习基准更好。<details>
<summary>Abstract</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习模型随着大小的增加，在不同设备环境中带来了 significiant 挑战。由于深度学习模型的大小，在低功率或资源受限的设备上部署它们 becomes 困难，导致推理时间长，能源消耗高。为解决这些挑战，我们提出了 FlexTrain 框架，该框架在训练阶段适应不同设备上的多样化存储和计算资源。 FlexTrain 允许高效地部署深度学习模型，同时尊重设备约束，降低通信成本，并确保与多种设备的可靠集成。我们在 CIFAR-100 数据集上证明 FlexTrain 的有效性，其中一个全球模型通过 FlexTrain 可以轻松地在多种设备上部署，从而降低训练时间和能源消耗。此外，我们扩展了 FlexTrain 到联合学习设定下，并证明我们的方法在 CIFAR-10 和 CIFAR-100 数据集上超过标准联合学习标准准则。
</details></li>
</ul>
<hr>
<h2 id="The-Phase-Transition-Phenomenon-of-Shuffled-Regression"><a href="#The-Phase-Transition-Phenomenon-of-Shuffled-Regression" class="headerlink" title="The Phase Transition Phenomenon of Shuffled Regression"></a>The Phase Transition Phenomenon of Shuffled Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20438">http://arxiv.org/abs/2310.20438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Ping Li</li>
<li>for: 这个论文是关于排序（permuted）回归问题中的阶段变换现象的研究，这个问题在数据库、隐私、数据分析等领域都有广泛的应用。</li>
<li>methods: 本研究使用了消息传递（MP）技术来精准地确定排序问题中的阶段变换点。首先，我们将排序问题转化为一个概率图模型，然后利用MP算法的分析工具， derivate一个跟踪MP算法的散度的方程。通过将这个方程连接到分支游击随机步骤过程，我们可以characterize随着信号响应比例（SNR）的影响于排序。</li>
<li>results: 在 oracle 和 non-oracle 两种情况下，我们分别研究了排序问题中的阶段变换点。在 oracle 情况下，我们可以准确预测阶段变换 SNR，而在 non-oracle 情况下，我们的算法可以预测排序中最多允许的排序行数，并且可以揭示这个数量与样本数之间的关系。<details>
<summary>Abstract</summary>
We study the phase transition phenomenon inherent in the shuffled (permuted) regression problem, which has found numerous applications in databases, privacy, data analysis, etc. In this study, we aim to precisely identify the locations of the phase transition points by leveraging techniques from message passing (MP). In our analysis, we first transform the permutation recovery problem into a probabilistic graphical model. We then leverage the analytical tools rooted in the message passing (MP) algorithm and derive an equation to track the convergence of the MP algorithm. By linking this equation to the branching random walk process, we are able to characterize the impact of the signal-to-noise-ratio ($\snr$) on the permutation recovery. Depending on whether the signal is given or not, we separately investigate the oracle case and the non-oracle case. The bottleneck in identifying the phase transition regimes lies in deriving closed-form formulas for the corresponding critical points, but only in rare scenarios can one obtain such precise expressions. To tackle this technical challenge, this study proposes the Gaussian approximation method, which allows us to obtain the closed-form formulas in almost all scenarios. In the oracle case, our method can fairly accurately predict the phase transition $\snr$. In the non-oracle case, our algorithm can predict the maximum allowed number of permuted rows and uncover its dependency on the sample number.
</details>
<details>
<summary>摘要</summary>
我们研究排序（协同）问题中的相变现象，这问题在数据库、隐私、数据分析等领域获得了广泛应用。在这些研究中，我们想要精确地找到相变点的位置，并且使用讯息传递（MP）技术来实现。在我们的分析中，我们首先将排序问题转换为一个 probabilistic graphical model，然后使用 MP 算法的分析工具来解释 MP 算法的参数。通过与分支随机步进程（BRW）的连结，我们可以描述 $\snr$ 对排序的影响。对于知道讯号的情况（oracle case）和不知道讯号的情况（non-oracle case），我们分别进行了研究。在 oracle case 中，我们可以对相变 $\snr$ 进行精确预测。在 non-oracle case 中，我们的算法可以预测最多允许的排序次数，并且揭露它对数据数量的依赖。Note: "Simplified Chinese" is a romanization of Chinese characters, and it is not a native Chinese script. The actual Chinese text would be written in Traditional Chinese or Simplified Chinese characters, depending on the region and the audience.
</details></li>
</ul>
<hr>
<h2 id="Discussing-the-Spectrum-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications"><a href="#Discussing-the-Spectrum-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications" class="headerlink" title="Discussing the Spectrum of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications"></a>Discussing the Spectrum of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20425">http://arxiv.org/abs/2310.20425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Haywood-Alexander, Wei Liu, Kiran Bacsa, Zhilu Lai, Eleni Chatzi</li>
<li>for: 本研究旨在探讨物理学和机器学习之间的交叉点，即物理学习（PEML），以提高机器学习的能力和降低数据或物理学习方法的缺陷。</li>
<li>methods: 本文通过对物理学习方法的总体探讨，包括其特征、应用和动机，探讨了物理学习方法的各种类型和应用场景。此外，文章还提供了一些实际应用和开发的PEML技术，以示其在复杂问题解决方面的可能性。</li>
<li>results: 本文的研究表明，PEML方法可以在各种领域中提高机器学习的性能，例如在单度量oscillator中的示例中，不同类型的PEML方法具有不同的特点和动机。此外，文章还提供了一些实际应用和开发的PEML技术，以便读者可以参考。作为基础贡献，本文强调PEML在科学和工程研究中的重要性，它可以通过物理知识和机器学习能力的共同作用，推动科学和工程领域的发展。<details>
<summary>Abstract</summary>
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the code of these working examples is provided alongside this paper. As a foundational contribution, this paper underscores the significance of PEML in pushing the boundaries of scientific and engineering research, underpinned by the synergy of physical insights and machine learning capabilities.
</details>
<details>
<summary>摘要</summary>
Physics-enhanced machine learning (PEML) 是一种新兴 Paradigma，利用物理学和机器学习技术的相互作用，以提高机器学习的能力和降低数据或物理方法的缺陷。在这篇论文中，我们详细探讨了 PEML 方法在物理和数据两个轴上的特点、应用和动机，并通过一系列应用和发展例子，展示了 PEML 在复杂问题解决方面的潜力。此外，我们还使用单度关节振荡器作为一个简单的工作示例，以阐述不同类型 PEML 方法的特点和动机。为促进协作和透明度，以及为读者提供实践性的例子，我们附加了这些工作示例的代码。作为基础性贡献，这篇论文强调了 PEML 在科学和工程研究中的前沿地位，以物理学和机器学习技术的相互作用为基础。
</details></li>
</ul>
<hr>
<h2 id="DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory"><a href="#DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory" class="headerlink" title="DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory"></a>DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20424">http://arxiv.org/abs/2310.20424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cenlin Duan, Jianlei Yang, Xiaolin He, Yingjie Qi, Yikun Wang, Yiou Wang, Ziyan He, Bonan Yan, Xueyan Wang, Xiaotao Jia, Weitao Pan, Weisheng Zhao</li>
<li>for: 这个研究旨在提高快速处理在内存中（PIM）的性能，并且将其应用于神经网络中。</li>
<li>methods: 研究人员提出了一个名为DDC-PIM的有效算法&#x2F;架构创新方法，以 doubles the equivalent data capacity。在算法层次，他们提出了一个称为filter-wise complementary correlation（FCC）的算法，以获得一对bitwise complementary pair。在架构层次，他们利用6T SRAM的内置的交叉关联结构，储存每个SRAM红色的bitwise complementary pair，以最大化每个SRAM红色的资料容量。</li>
<li>results: 评估结果显示，DDC-PIM比PIM基eline实现约2.84倍的速度提升，并且与PIM基eline的精度损失相近。与现有的SRAM-based PIM芯片相比，DDC-PIM可以达到8.41倍和2.75倍的重量密度和面积效率提升。<details>
<summary>Abstract</summary>
Processing-in-memory (PIM), as a novel computing paradigm, provides significant performance benefits from the aspect of effective data movement reduction. SRAM-based PIM has been demonstrated as one of the most promising candidates due to its endurance and compatibility. However, the integration density of SRAM-based PIM is much lower than other non-volatile memory-based ones, due to its inherent 6T structure for storing a single bit. Within comparable area constraints, SRAM-based PIM exhibits notably lower capacity. Thus, aiming to unleash its capacity potential, we propose DDC-PIM, an efficient algorithm/architecture co-design methodology that effectively doubles the equivalent data capacity. At the algorithmic level, we propose a filter-wise complementary correlation (FCC) algorithm to obtain a bitwise complementary pair. At the architecture level, we exploit the intrinsic cross-coupled structure of 6T SRAM to store the bitwise complementary pair in their complementary states ($Q/\overline{Q}$), thereby maximizing the data capacity of each SRAM cell. The dual-broadcast input structure and reconfigurable unit support both depthwise and pointwise convolution, adhering to the requirements of various neural networks. Evaluation results show that DDC-PIM yields about $2.84\times$ speedup on MobileNetV2 and $2.69\times$ on EfficientNet-B0 with negligible accuracy loss compared with PIM baseline implementation. Compared with state-of-the-art SRAM-based PIM macros, DDC-PIM achieves up to $8.41\times$ and $2.75\times$ improvement in weight density and area efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
“处理在内存（PIM）”是一种新的计算模式，它在数据移动效率方面提供了显著性能提升。SRAM基于PIM被认为是最有前途的候选人，因为它具有持续性和兼容性。然而，SRAM基于PIM的集成密度远低于其他不朽存储器基于的一个，因为它的内置6T结构只能存储一个bit。在相同的面积限制下，SRAM基于PIM表现出较低的容量。因此，我们提议了DDC-PIM，一种高效的算法/架构合理化方法，可以有效地double equipotential data capacity。在算法层次，我们提出了一种filter-wise complementary correlation（FCC）算法，以获得一个bitwise complementary pair。在架构层次，我们利用6T SRAM的内置交叉结构来存储bitwise complementary pair的两个相互补做（$Q/\bar{Q}$），以最大化每个SRAM cel的数据容量。该双广播输入结构和可重新配置单元支持深度wise和点wise卷积，与多种神经网络的需求相符。评估结果表明，DDC-PIM比PIM基eline实现具有约2.84倍的速度提升，无损准确性，对MobileNetV2和EfficientNet-B0进行评估。相比特状态的SRAM基于PIM封装，DDC-PIM实现了最高的Weight Density和面积效率提升，分别达到8.41倍和2.75倍。
</details></li>
</ul>
<hr>
<h2 id="Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value"><a href="#Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value" class="headerlink" title="Coalitional Manipulations and Immunity of the Shapley Value"></a>Coalitional Manipulations and Immunity of the Shapley Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20415">http://arxiv.org/abs/2310.20415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Basteck, Frank Huettner</li>
<li>for: 该论文关注在合作游戏中的操作，即一个联盟想要提高其成员的总收益。</li>
<li>methods: 该论文使用了一种新的基础设定，即取代添加性的原始特征，使用内部重新分配价值的防止和全体不变下的价值下降来定义希柏利值。</li>
<li>results: 该论文发现，对于有效的分配规则，内部重新分配价值的防止等同于受限的边缘性，这是希柏利值的唯一一种有效和Symmetric的分配规则，并且具有免除合作操作的特性。<details>
<summary>Abstract</summary>
We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality.
</details>
<details>
<summary>摘要</summary>
我们在合作游戏中考虑操作，合作团体想要增加成员的总回扣。一个分配规则是免受合作操作的影响，如果无法内部重新分配值的层次（内部重新分配证明），并且如果无法在所有 else 保持不变的情况下从值中获得更多的回扣（弱合作偏好）。将添加性在雪佛利原始特征中更替换为这些要求，则得到一个新的基础，即雪佛利值是唯一的有效和对称分配规则，将无关玩家获得任何回扣，并且免受合作操作的影响。我们进一步发现，对于有效的分配规则，内部重新分配证明与条件紧密相关，即 constrained marginality，这是 Young 的 marginality axioma 的弱化版本。我们的第二个特征改进了 Young 的特征，免除了独立性的需求，这是 marginality axioma 中的一个内在的限制。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks"><a href="#A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks" class="headerlink" title="A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks"></a>A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20398">http://arxiv.org/abs/2310.20398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/veronicasaz/planetarysystem_hnn">https://github.com/veronicasaz/planetarysystem_hnn</a></li>
<li>paper_authors: Veronica Saz Ulibarrena, Philipp Horn, Simon Portegies Zwart, Elena Sellentin, Barry Koren, Maxwell X. Cai<br>for: 这个研究旨在使用人工神经网络（ANNs）来加速行星系统的Simulating the evolution of the gravitational N-body problem的计算，以减少计算成本。methods: 研究使用Hamiltonian Neural Networks（HNNs）来取代计算成本高的部分，并与传统的Deep Neural Networks（DNNs）进行比较。results: 使用Hybrid integrator，包含神经网络的方法可以增加计算的可靠性，并避免大量能量误差，但是在 asteroids 的数量超过 70 时，使用神经网络可以得到更快的 simulations。<details>
<summary>Abstract</summary>
Simulating the evolution of the gravitational N-body problem becomes extremely computationally expensive as N increases since the problem complexity scales quadratically with the number of bodies. We study the use of Artificial Neural Networks (ANNs) to replace expensive parts of the integration of planetary systems. Neural networks that include physical knowledge have grown in popularity in the last few years, although few attempts have been made to use them to speed up the simulation of the motion of celestial bodies. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details>
<details>
<summary>摘要</summary>
计算机模拟行星系统的演化变得越来越复杂，因为问题的复杂性平方增长与体数（N）成正比。我们研究使用人工神经网络（ANNs）来替代计算昂贵的部分。包含物理知识的神经网络在最近几年内得到了广泛应用，尽管对其用于加速天体运动的模拟还未有充分的尝试。我们研究使用哈密顿神经网络来替代计算昂贵的部分的优势和限制，并与传统的深度神经网络进行比较，特别是了解这个问题的挑战。由于 gravitation 方程的非线性性，误差在计算中会卷积。为了增加使用神经网络的方法的稳定性，我们提议一种混合 интеIntegrator，该 inteGreater than 70 asteroids, the use of neural networks can achieve faster simulations.
</details></li>
</ul>
<hr>
<h2 id="Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods"><a href="#Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods" class="headerlink" title="Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods"></a>Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20380">http://arxiv.org/abs/2310.20380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengpeng Xie, Changdong Yu, Weizheng Qiao</li>
<li>For: This paper aims to improve the stability and convergence of policy optimization algorithms in reinforcement learning by addressing the issue of high variance in the surrogate objective caused by importance sampling.* Methods: The paper proposes a dropout technique to avoid the excessive increase of the surrogate objective variance, and introduces a general reinforcement learning framework applicable to mainstream policy optimization methods. The authors also apply the dropout technique to the Proximal Policy Optimization (PPO) algorithm to obtain the D-PPO variant.* Results: The paper conducts comparative experiments between the D-PPO and PPO algorithms in the Atari 2600 environment, and shows that D-PPO achieves significant performance improvements compared to PPO, while effectively limiting the excessive increase of the surrogate objective variance during training.<details>
<summary>Abstract</summary>
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details>
<details>
<summary>摘要</summary>
政策基于学习算法在多种领域广泛应用。其中，主流政策优化算法如PPO和TRPO通过重要抽样引入在学习中，这使得可以重用历史数据。但这也会导致优化目标函数的偏振和学习过程的稳定性受到 indirect 影响。在这篇论文中，我们首先 derive 了优化目标函数的偏振的Upper bound，可以 quadratic 增长与优化目标函数的增长相关。然后，我们提出了dropout技术来避免由重要抽样引入的优化目标函数偏振过度增长的问题。接着，我们引入了一种通用的学习框架，适用于主流政策优化方法。最后，我们将Dropout技术应用于PPO算法，并称之为D-PPO。我们对D-PPO和PPO算法在Atari 2600环境中进行了比较性实验，结果显示，D-PPO在性能方面比PPO具有显著改善，并有效地限制了优化目标函数偏振的过度增长 durante 训练。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm"><a href="#Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm" class="headerlink" title="Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm"></a>Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20369">http://arxiv.org/abs/2310.20369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoxi Zhu, Li Shen, Bo Du, Dacheng Tao</li>
<li>for: This paper is written for researchers and practitioners interested in decentralized machine learning and the generalization of decentralized algorithms.</li>
<li>methods: The paper uses the approach of algorithmic stability to analyze the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm in both convex-concave and nonconvex-nonconcave settings.</li>
<li>results: The paper demonstrates that the decentralized structure of the D-SGDA algorithm does not destroy its stability and generalization, and that the algorithm can generalize as well as the vanilla SGDA in certain situations. Additionally, the paper analyzes the impact of different topologies on the generalization bound of the D-SGDA algorithm and evaluates the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究分布式机器学习和分布式算法的稳定性和泛化性而写的。</li>
<li>methods: 这篇论文使用了算法稳定性的方法来分析分布式权值逻辑 descent ascent（D-SGDA）算法在凸-凹和非凸-非凹的设置下的双方面泛化约束。</li>
<li>results: 这篇论文表明了分布式结构不会destroy D-SGDA算法的稳定性和泛化性，并且在某些情况下，D-SGDA算法可以与标准的SGDA算法具有相同的泛化能力。此外，论文还分析了不同的トポлогиー的影响于D-SGDA算法的泛化约束，并评估了优化误差和泛化差以获得D-SGDA算法在凸-凹设置下的优化人口风险。<details>
<summary>Abstract</summary>
The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such as sample sizes, learning rates, and iterations. We also evaluate the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting. Additionally, we perform several numerical experiments which validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
随着可用数据的增长，解决分布式的最小最大问题（minimax problem）在机器学习任务中吸引了越来越多的关注。先前的理论研究主要集中在分布式最小最大算法的收敛率和通信复杂度上，几乎没有关注其泛化性。在这篇论文中，我们 investigate了分布式随机梯度搜索算法（D-SGDA）的原理稳定性下的 primal-dual 泛化 bound，包括 convex-concave 和 nonconvex-nonconcave 设置下的情况。我们的理论表明，分布式结构不会对 D-SGDA 的稳定性和泛化性产生负面影响，这意味着它可以在某些情况下与普通的 SGDA 相当具有泛化能力。我们的研究还分析了不同拓扑结构对 D-SGDA 算法的泛化 bound 的影响，并超越了聚合因素、学习率、迭代次数等简单的 фактор。此外，我们还评估了 D-SGDA 算法的优化误差，并尝试平衡优化误差和泛化差，以获得 D-SGDA 算法在 convex-concave 设置下的最佳人口风险。最后，我们进行了多个数学实验，以验证我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data"><a href="#Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data" class="headerlink" title="Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?"></a>Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20366">http://arxiv.org/abs/2310.20366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction">https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction</a></li>
<li>paper_authors: Guopeng Li, Victor L. Knoop, J. W. C., van Lint</li>
<li>for: 本研究是为了解决网络级别的交通情况预测问题，尤其是对于 loop detector 数据。</li>
<li>methods: 本研究提出了一种不确定性感知的交通预测框架，结合流体理论和图 neural network，以确保预测和不确定性评估的Robustness。此外，使用 evidential learning 来评估不同来源的不确定性，并将估计的不确定性用于”浓缩”数据集中的信息内容。</li>
<li>results: 通过对一条横跨阿姆斯特丹的高速公路网络进行实验，发现2018-2021年日间80%以上的数据可以被去除，剩下的20%样本具有相同的预测力度。这结果表明，大规模的交通数据集可以被分解成更小但具有相同信息内容的数据集。这些发现证明了提posed方法的价值性，并且可以进一步推广到EXTRACTING更小但非重叠的数据集。<details>
<summary>Abstract</summary>
Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content. Results from a case study of a highway network around Amsterdam show that, from 2018 to 2021, more than 80\% of the data during daytime can be removed. The remaining 20\% samples have equal prediction power for training models. This result suggests that indeed large traffic datasets can be subdivided into significantly smaller but equally informative datasets. From these findings, we conclude that the proposed methodology proves valuable in evaluating large traffic datasets' true information content. Further extensions, such as extracting smaller, spatially non-redundant datasets, are possible with this method.
</details>
<details>
<summary>摘要</summary>
Our proposed framework combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Additionally, we use evidential learning to quantify different sources of uncertainty in a single pass, and the estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content.We conducted a case study of a highway network around Amsterdam from 2018 to 2021, and found that more than 80% of the data during daytime can be removed, while the remaining 20% of samples have equal prediction power for training models. This result suggests that large traffic datasets can be subdivided into significantly smaller but equally informative datasets.Our findings demonstrate the value of the proposed methodology in evaluating the true information content of large traffic datasets. Furthermore, we can extend this method to extract smaller, spatially non-redundant datasets, which can help improve the efficiency of traffic forecasting.
</details></li>
</ul>
<hr>
<h2 id="CAFE-Conflict-Aware-Feature-wise-Explanations"><a href="#CAFE-Conflict-Aware-Feature-wise-Explanations" class="headerlink" title="CAFE: Conflict-Aware Feature-wise Explanations"></a>CAFE: Conflict-Aware Feature-wise Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20363">http://arxiv.org/abs/2310.20363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Dejl, Hamed Ayoobi, Matthew Williams, Francesca Toni</li>
<li>for: 本研究旨在提出一种新的特征归属方法，以解释神经网络模型的决策过程中的特征影响。</li>
<li>methods: 该方法（CAFE）解决了现有方法的三大限制：忽略内部矛盾特征的影响、忽略偏见项的影响以及当地化活动函数的过敏。CAFE方法提供了防止过度估计输入特征的影响的保障，并分别跟踪输入特征和偏见项的正面和负面影响，从而提高了其robustness和能力浮现特征冲突。</li>
<li>results: 实验表明，CAFE方法在synthetic tabular数据上更好地标识内部矛盾特征，并在几个真实世界的 tabular数据集上达到最高的总准确率，同时具有高效计算性。<details>
<summary>Abstract</summary>
Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient.
</details>
<details>
<summary>摘要</summary>
Feature 归因方法广泛使用于解释神经网络模型，它们可以确定输入特征对模型输出的影响。我们提出了一种新的Feature归因方法，称为CAFE（Conflict-Aware Feature-wise Explanations），它解决了现有方法的三个限制：它们忽略了对抗性特征的影响、缺乏对偏移项的考虑和当地活动函数下的过敏感。与其他方法不同，CAFE提供了防止过度估计输入神经元影响的保障，并分别跟踪输入特征和偏移项的正面和负面影响，从而提高了robustness和抑制特征冲突的能力。我们通过实验表明，CAFE在Synthetic表格数据上更好地标识对抗性特征，并在多个实际表格数据上显示出最好的总准确率，同时具有高度计算效率。
</details></li>
</ul>
<hr>
<h2 id="Verification-of-Neural-Networks-Local-Differential-Classification-Privacy"><a href="#Verification-of-Neural-Networks-Local-Differential-Classification-Privacy" class="headerlink" title="Verification of Neural Networks Local Differential Classification Privacy"></a>Verification of Neural Networks Local Differential Classification Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20299">http://arxiv.org/abs/2310.20299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen</li>
<li>for: 保护隐私性（Privacy）</li>
<li>methods: 使用地域拟合分布（KDE）和分布式验证（MILP）</li>
<li>results: 通过训练 Only 7% of the networks, Sphynx 可以预测一个抽象网络，并达到 93% 的验证精度和将验证时间减少 $1.7\cdot10^4$ 倍。<details>
<summary>Abstract</summary>
Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, making it difficult to predict an abstraction, and predicting too large abstraction harms the verification. Our key idea is to transform the parameters into a distribution given by KDE, allowing to keep the over-approximation error small. To verify LDCP, we extend a MILP verifier to analyze an abstract network. Experimental results show that by training only 7% of the networks, Sphynx predicts an abstract network obtaining 93% verification accuracy and reducing the analysis time by $1.7\cdot10^4$x.
</details>
<details>
<summary>摘要</summary>
“神经网络容易受到隐私攻击。至今，无法对受训集中的个人隐私进行推理。我们提出了一新的隐私性质，即本地分类隐私性（LDCP），将本地可靠性应用到各种隐私设置中。给定一个输入集，如果某个分类器在该集中的任何一个输入 omitted 时都将所有输入分类为同一个类别，那么该分类器是 LDCP。一个简单的算法是非常不实用，因为它需要训练一个非常大的数量的网络，并对每个网络进行本地稳健性检查。我们提出了一个名为 Sphynx 的算法，可以从一小数量的网络中计算一个抽象网络，并将 LDCP 直接验证到抽象网络上。挑战是两重的：网络参数不遵循已知的概率分布，使预测抽象变得困难；而且，如果预测的抽象太大，则会导致验证失败。我们的关键想法是将参数转换为一个由 KDE 提供的分布，这样可以保持过度上下文错误小。为验证 LDCP，我们将 MILP 验证器扩展到抽象网络上进行分析。实验结果表明，只需训练 7% 的网络，Sphynx 可以预测一个抽象网络， obtining 93% 的验证精度，并将分析时间减少 $1.7\cdot10^4$ 倍。”
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty"><a href="#Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty" class="headerlink" title="Accelerating Generalized Linear Models by Trading off Computation for Uncertainty"></a>Accelerating Generalized Linear Models by Trading off Computation for Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20285">http://arxiv.org/abs/2310.20285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Tatzel, Jonathan Wenger, Frank Schneider, Philipp Hennig</li>
<li>for: 这个论文是为了提出一种Iterative方法，用于提高 bayesian generalized linear models（GLMs）的准确性和效率。</li>
<li>methods: 这种Iterative方法利用了并行计算和信息压缩技术，可以快速地训练 GLMs，并且可以提供更好的 uncertainty 估计。</li>
<li>results: 在一个实际的分类问题中，这种方法可以快速地训练 GLMs，并且可以提供更好的 uncertainty 估计，相比传统的方法。<details>
<summary>Abstract</summary>
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
</details>
<details>
<summary>摘要</summary>
泛函 Bayesian Generalized Linear Models (GLMs) 提供了一个洒脱的概率框架，用于模elling categorical、ordinal 和连续变数，并在实践中广泛使用。然而，对大型数据集进行精确的推断是不可能的，因此需要使用估计。然而，这个估计错误会对模型的可靠性产生负面影响，而且不会考虑到预测中的不确定性。在这个工作中，我们引入了一家族的迭代方法，可以明确地模型这个错误。这些方法特别适合平行现代计算机硬件，可以高效地重复计算，将信息压缩，以减少 GLMs 的时间和内存需求。我们在一个实际上是一个大型分类问题中显示了，我们的方法可以快速地训练，并可以明确地交换精确性和不确定性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space"><a href="#Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space" class="headerlink" title="Advancing Bayesian Optimization via Learning Correlated Latent Space"></a>Advancing Bayesian Optimization via Learning Correlated Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20258">http://arxiv.org/abs/2310.20258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim</li>
<li>for: 优化黑盒函数（black-box function），具有有限的函数评估次数。</li>
<li>methods: 使用深度生成模型（deep generative models），如变量自动编码器（variational autoencoders），实现有效和高效的极限优化。</li>
<li>results: 在离散数据上实现高性能，并且在有限评估次数下寻找优化解。<details>
<summary>Abstract</summary>
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种强大的方法，用于优化黑板函数，但是它具有限制性的函数评估。现有研究表明，通过在深度生成模型中进行几何变换，可以实现有效和高效的 bayesian 优化。但是，由于优化不在输入空间进行，因此会导致内在的差距，从而导致可能的优化解决方案不佳。为了解决这个差距，我们提出相关的独立空间抽象 Bayesian 优化方法（CoBO），它专门关注学习相关独立空间，其中独立空间中的距离与目标函数中的距离具有强相关性。specifically，我们的方法引入了 lipschitz 正则化、损失权重和信任区域重新协调，以降低内在差距的潜在问题。我们在一些离散数据优化任务中，如分子设计和数学表达适应，实现了高效性，并且在小预算内达到了高性能。
</details></li>
</ul>
<hr>
<h2 id="STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction"><a href="#STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction" class="headerlink" title="STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction"></a>STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20223">http://arxiv.org/abs/2310.20223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maoxiang Sun, Weilong Ding, Tianpu Zhang, Zijian Liu, Mengda Xing</li>
<li>for:  traffic prediction in newly developed cities with insufficient sensors</li>
<li>methods:  few-shot learning (FSL) and spatio-temporal domain adaptation (STDA)</li>
<li>results:  improved prediction performance by 7% compared to baseline models on two metrics (MAE and RMSE)<details>
<summary>Abstract</summary>
As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient training data, when transferring knowledge from different domains to the intended target domain.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we train the STDA model using a Model-Agnostic Meta-Learning (MAML) based episode learning process, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples. We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7\% compared to baseline models on the two metrics of MAE and RMSE.
</details>
<details>
<summary>摘要</summary>
随着城市的发展，交通堵塞问题变得越来越严重，交通预测成为解决这一问题的一种 clasic 方法。交通预测是空间时间预测学习的一个特定应用，与axi scheduling、天气预测和船舶轨迹预测等问题相关。在实际情况下，一些新建的城市缺乏感知器，导致预测性能下降。在这种情况下，具有少量数据的学习方法被称为几何学习（FSL），交通预测中的FSL具有挑战。一方面，Graph结构的不规则性和空间时间学习方法的动态性使得学习成本增加。另一方面，传统的领域适应方法在缺乏训练数据时无法达到预期的性能。为解决这些挑战，我们提出了一种新的空间时间领域适应（STDA）方法，通过在数据充足城市中学习启发式空间时间元知识，以提高数据缺乏城市的预测性能。具体来说，我们使用基于MAML的模型无关元学习（MAML）基于话语学习过程，这是一种可以在很少的训练样本基础上解决新的学习任务的模型无关元学习框架。我们在四个交通预测数据集上进行了多次实验，结果显示，我们的模型的预测性能与基eline模型相比提高了7%的MAE和RMSE两个纪录。
</details></li>
</ul>
<hr>
<h2 id="Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics"><a href="#Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics" class="headerlink" title="Calibration by Distribution Matching: Trainable Kernel Calibration Metrics"></a>Calibration by Distribution Matching: Trainable Kernel Calibration Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20211">http://arxiv.org/abs/2310.20211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kernel-calibration/kernel-calibration">https://github.com/kernel-calibration/kernel-calibration</a></li>
<li>paper_authors: Charles Marx, Sofian Zalouk, Stefano Ermon</li>
<li>for: 该论文旨在提高预测probability的准确性和不确定性捕捉，并且提供一种基于kernel的抽象度量来评估预测准确性。</li>
<li>methods: 该论文使用了kernel-based抽象度量，它们可以总结和普适多种预测calibration方法，并且可以轻松地在empirical risk minimization中添加calibration目标。</li>
<li>results: 该论文的实验结果表明，通过使用这些抽象度量作为正则函数，可以提高预测的准确性、锐度和决策效果，并且超过了仅仅采用后期重新补做calibration的方法。<details>
<summary>Abstract</summary>
Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用标准化的抽象语言来描述：Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.中文翻译：使用标准化的抽象语言描述：calibration确保probabilistic forecasts能够准确地捕捉到uncertainty，通过要求预测的概率与实际频率相匹配。然而，许多现有的calibration方法专门为post-hoc recalibration，可能会恶化预测的精度。 Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression.这些 metric采用分布匹配任务的思想，可以快速地在empirical risk minimization中添加calibration目标。此外，我们还提供了直观的机制来适应决策任务，并强制实现准确的损失估计和无悬决策。我们的实验证明，通过将这些metric作为正则izer来使用，可以提高calibration、锐度和决策的性能，在多种分类和回归任务上超过仅仅靠post-hoc recalibration的方法。
</details></li>
</ul>
<hr>
<h2 id="Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning"><a href="#Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning" class="headerlink" title="Network Contention-Aware Cluster Scheduling with Reinforcement Learning"></a>Network Contention-Aware Cluster Scheduling with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20209">http://arxiv.org/abs/2310.20209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gajagajago/deepshare">https://github.com/gajagajago/deepshare</a></li>
<li>paper_authors: Junyeol Ryu, Jeongyoon Eo</li>
<li>For: 这个论文主要针对 GPU 集群中的训练工作负载进行优化，尤其是针对不同量、比例和模式的通信对训练进程的影响。* Methods: 本论文使用了人工智能学习来解决 GPU 集群中的网络竞争问题，具体来说是通过将 GPU 集群调度问题转换为一个人工智能学习问题，以便学习一个网络竞争意识的调度策略。* Results: 相比于通用的调度策略，本论文的方法可以降低平均训练时间达18.2%，并且可以将尾部训练时间降低达20.7%，同时允许更好的资源利用率和训练时间平衡。<details>
<summary>Abstract</summary>
With continuous advances in deep learning, distributed training is becoming common in GPU clusters. Specifically, for emerging workloads with diverse amounts, ratios, and patterns of communication, we observe that network contention can significantly degrade training throughput. However, widely used scheduling policies often face limitations as they are agnostic to network contention between jobs. In this paper, we present a new approach to mitigate network contention in GPU clusters using reinforcement learning. We formulate GPU cluster scheduling as a reinforcement learning problem and opt to learn a network contention-aware scheduling policy that efficiently captures contention sensitivities and dynamically adapts scheduling decisions through continuous evaluation and improvement. We show that compared to widely used scheduling policies, our approach reduces average job completion time by up to 18.2\% and effectively cuts the tail job completion time by up to 20.7\% while allowing a preferable trade-off between average job completion time and resource utilization.
</details>
<details>
<summary>摘要</summary>
随着深度学习的不断发展，分布式训练在GPU集群中变得越来越普遍。特别是对于出现的工作负载，这些负载有多样的量、比率和通信模式，我们发现网络竞争可以较大程度地降低训练速率。然而，广泛使用的调度策略经常面临限制，因为它们对网络竞争between jobs无法适应。在这篇论文中，我们提出一种新的网络竞争 Mitigation Approach for GPU clusters using reinforcement learning。我们将GPU集群调度问题转化为一个reinforcement learning问题，并选择学习一个网络竞争意识的调度策略，能够高效地捕捉竞争敏感度并通过连续评估和改进来动态地调整调度决策。我们表明，相比广泛使用的调度策略，我们的方法可以降低平均任务完成时间 by up to 18.2%，同时可以有效地削减尾号任务完成时间 by up to 20.7%，并允许可接受的资源利用率和任务完成时间之间的交易。
</details></li>
</ul>
<hr>
<h2 id="Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning"><a href="#Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning" class="headerlink" title="Importance Estimation with Random Gradient for Neural Network Pruning"></a>Importance Estimation with Random Gradient for Neural Network Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20203">http://arxiv.org/abs/2310.20203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suman Sapkota, Binod Bhattarai</li>
<li>for: 本研究旨在提高神经网络的效率，通过全球神经元重要性估计来减少神经网络的参数数量。</li>
<li>methods: 本研究使用了一种基于TaylorFOapproximation的轮征方法来估计神经元的全球重要性。此外，我们还提出了两种改进方法，包括从最后一层网络中传播随机梯度，以及对最后一层输出的梯度幅度进行正规化。</li>
<li>results: 我们在ResNet和VGG架构上测试了我们的方法，并得到了更好的性能。具体来说，我们在CIFAR-100和STL-10 datasets上分别提高了1.3%和1.6%的测试错误率。此外，我们还发现我们的方法可以与现有的方法相结合，以提高它们的性能。<details>
<summary>Abstract</summary>
Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. Furthermore, our method also complements the existing methods and improves their performances when combined with them.
</details>
<details>
<summary>摘要</summary>
全球神经元重要性估计是用于提高神经网络效率的方法之一。以现有的方法为基础，大多数使用活动或梯度信息，或者两者都使用，以估计每个神经元或卷积核心的全球重要性。在这种情况下，我们使用规则来 derivimportance estimation similar to Taylor First Order (TaylorFO) approximation based methods。我们命名我们的方法为TaylorFO-abs和TaylorFO-sq。我们还提出了两种方法来改进这些重要性估计方法。首先，我们从神经网络的最后一层传递随机梯度，以避免需要标注的例子。其次，我们在最后一层输出之前对梯度大小进行 нор化，使所有的例子都能够对重要性分数做出相同的贡献。我们的方法与其他技术结合使用时表现更好，并且在ResNet和VGG架构上的CIFAR-100和STL-10 datasets上进行测试时也表现出了更好的成绩。
</details></li>
</ul>
<hr>
<h2 id="FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems"><a href="#FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems" class="headerlink" title="FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems"></a>FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20193">http://arxiv.org/abs/2310.20193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Zhichao Wang, Xi Leng, Xiaoying Tang</li>
<li>for: 防止边缘用户隐私泄露和减少通信成本，提高推荐系统效能。</li>
<li>methods: 使用最佳子集选择基于特征相似性生成近乎最佳虚拟评分，仅使用用户本地资讯，减少杂音而无需额外的通信成本。并使用沃氏距离估计客户端多样性和贡献，解决客户端多样性问题。</li>
<li>results: 实验结果显示FedRec+可以在不同的参考数据集上实现现代表性的性能，超过现有方法。<details>
<summary>Abstract</summary>
Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)保护用户隐私和降低边缘用户的通信成本是推荐系统中的主要挑战。虽然联邦学习已经证明能够保护隐私 by avoiding data exchange between clients and servers，但是服务器可以根据两次连续的用户上传的非零梯度推算用户评分。此外，联邦推荐系统（FRS）面临着不一致性挑战，导致推荐性能下降。在这篇论文中，我们提出了FedRec+，一个ensemble框架 для FRS，增强隐私性，并解决不一致性挑战。FedRec+使用最佳子集选择基于特征相似性来生成近似最佳虚拟评分 дляpseudo item，只使用用户的本地信息。这种方法减少噪音而不导致额外的通信成本。此外，我们利用 Wasserstein distance来估算每个客户端的不一致性和贡献，并 deriv optimal aggregation weights by solving a defined optimization problem。实验结果表明FedRec+在不同的参考数据集上具有状态之art的表现。
</details></li>
</ul>
<hr>
<h2 id="Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer"><a href="#Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer" class="headerlink" title="Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer"></a>Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20172">http://arxiv.org/abs/2310.20172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren</li>
<li>for: 这篇论文主要目标是解决空间 gravitational wave 探测中数据处理中的增加复杂性问题，具体来说是用一种可解释性强大的大型预训练模型来预测 Compact Binary Systems 波形。</li>
<li>methods: 这篇论文提出了一种名为 CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）的可解释性强大的大型预训练模型，用于预测 Compact Binary Systems 波形。三个模型都被训练以预测 Masive Black Hole Binary（MBHB）、Extreme Mass-Ratio Inspirals（EMRIs）和 Galactic Binary（GB）波形，其中预测精度分别为98%、91%和99%。</li>
<li>results: 这篇论文的研究结果表明，CBS-GPT 模型具有出色的可解释性，其隐藏参数能够有效地捕捉波形中的复杂信息，即使 Instrument Response 和参数范围很广。这种研究展示了大型预训练模型在 gravitational wave 数据处理中的潜在应用前景，包括 gap completion、GW 信号检测和信号噪声减少等任务。<details>
<summary>Abstract</summary>
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex instrument response and a wide parameter range. Our research demonstrates the potential of large pre-trained models in gravitational wave data processing, opening up new opportunities for future tasks such as gap completion, GW signal detection, and signal noise reduction.
</details>
<details>
<summary>摘要</summary>
Space-based gravitational wave detection是下一代的 gravitational wave（GW）探测项目中最受期待的一个，能够探测丰富的紧凑binary系统。然而，准确预测space GW 波形仍然未经充分研究。为解决探测器响应和第二代时间延迟相互探测（TDI 2.0）中数据处理困难，一种可解释的大型预训练模型被提议，称为CBS-GPT（紧凑 binary 系统波形生成器with Generative Pre-trained Transformer）。对紧凑binary系统波形，CBS-GPT模型进行了三种不同的模型训练，分别预测了大质量黑洞binary（MBHB）、极大质量比例沉落（EMRIs）和 галактиче binary（GB）波形，实现了预测精度为98%、91%和99%。CBS-GPT模型表现出了remarkable可解释性，其隐藏参数能够有效捕捉波形中的复杂信息，即使用户器Response和广泛的参数范围。我们的研究表明大预训练模型在GW数据处理中具有潜在的应用前景，打开了未来任务 such as gap completion、GW信号探测和信号噪声减少等的新机遇。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds"><a href="#Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds" class="headerlink" title="Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds"></a>Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20168">http://arxiv.org/abs/2310.20168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justus C. Will, Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt</li>
<li>for: 研究云中微物理过程，特别是云中落体大小分布的影响。</li>
<li>methods: 使用变分自动编码器（VAEs）生成了新的可读性抽象，用于描述落体大小分布的时间变化。</li>
<li>results: 研究发现，即使颗粒浓度不同，落体spectrum的演化都类似，只是速度不同。这表明降水起始过程具有共同特征。<details>
<summary>Abstract</summary>
Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation initiation processes are alike despite variations in onset times.
</details>
<details>
<summary>摘要</summary>
<langpack> simplifiedchinese</langpack><text>这篇文章提出了一个极其重要的问题：如何对云中的小滴子互动进行全面的分析，以更好地理解云内的微物理过程和它们对全球气候的影响。对于这个问题，作者使用了大� Eddy  simulations（LES）的bin微物理来模拟有效的滴子大小分布，并挑战了现有的分析技术。使用了对预测矩阵的简洁隐藏表示（VAEs），作者创造了新的可读性和直观性的可视化，以便更好地理解滴子大小和时间的演化。这个可视化给了更多的解释，并允许我们对不同气体浓度的 simulations 进行比较。我们发现，随着气体浓度的变化，滴子spectrum的演化是相似的，但发生的时间则有所不同。这些相似性表明，降水启动过程是相似的，即使在不同的启动时间。</text></sys>
</details></li>
</ul>
<hr>
<h2 id="Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs"><a href="#Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs" class="headerlink" title="Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs"></a>Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20145">http://arxiv.org/abs/2310.20145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen</li>
<li>for: 这篇论文的目的是提出一种能够有效地处理输入不确定性的 bayesian 优化算法。</li>
<li>methods: 这种算法使用了 Gaussian Process 模型，并通过 Maximum Mean Discrepancy (MMD) 来直接模型输入的不确定性。它还使用了 Nystrom 简化 posterior inference。</li>
<li>results: 经过对 synthetic functions 和实际问题的测试，这种方法可以处理各种输入不确定性，并达到现有最佳性能。<details>
<summary>Abstract</summary>
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic functions and real problems demonstrate that our approach can handle various input uncertainties and achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）是一种效率高的优化算法，广泛应用于不同领域。在某些复杂的BO任务中，输入不确定性 arise due to 优化过程中的随机性，如机床错误、执行噪声或上下文变化。这种不确定性会使输入偏离预期的值，导致优化结果中的性能波动。在这篇论文中，我们介绍了一种新的Robust Bayesian Optimization算法，AIRBO，可以有效地确定一个Robust optimum，在任意输入不确定性下表现一致性强。我们直接使用Gaussian Process模型来模型不确定的输入，并通过Maximum Mean Discrepancy（MMD）和Nystrom采样加速 posterior 推理。我们提供了严格的理论 regret bound，并在synthetic functions 和实际问题上进行了广泛的实验，证明我们的方法可以处理不同类型的输入不确定性，并达到领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds"><a href="#Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds" class="headerlink" title="Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds"></a>Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20102">http://arxiv.org/abs/2310.20102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Wang, Yongyi Mao</li>
<li>for: 提供新的信息理论性 garantue，通过一种新的“邻域假设”矩阵和一个新的样本条件假设稳定性（SCH稳定性）家族。</li>
<li>methods: 使用新的构造和稳定性评价方法来提供更加锐利的信息理论性 garantue，超越了现有的信息理论性 bounds 在不同的学习场景中。</li>
<li>results: 得到的结果是一种更加精细的信息理论性 garantue，可以在某些学习问题中（如统计几何优化问题）提供更高的精度和更好的性能。<details>
<summary>Abstract</summary>
We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
</details>
<details>
<summary>摘要</summary>
我们提出了一新的信息理论基础的扩展保证，通过一种新的“邻居假设”矩阵的建构和一新的家族叫做“样本调和假设”（SCH）稳定性。我们的方法可以获得更加锐利的界限，超越了现有信息理论界限在各种学习情况下的限制，特别是在数学 convex 优化（SCO）问题中。Here's a breakdown of the translation:* "新的信息理论基础" (新的信息理论基础) - This phrase emphasizes that the information-theoretic generalization guarantees presented are new and innovative.* "扩展保证" (扩展保证) - This term refers to the guarantees provided by the information-theoretic framework, which ensure that the learned model will generalize well to new data.* "邻居假设" (邻居假设) - This phrase refers to the "neighboring-hypothesis" matrix, which is a novel construction used in the information-theoretic framework to provide tighter bounds.* " sample-conditioned hypothesis (SCH) stability" (样本调和假设（SCH）稳定性) - This term refers to a new family of stability notions used in the information-theoretic framework, which are based on the idea of conditioning on the sample.* "获得更加锐利的界限" (获得更加锐利的界限) - This phrase emphasizes that the information-theoretic generalization guarantees provided by the new construction and stability notions are sharper and more accurate than previous bounds.* "超越了现有信息理论界限" (超越了现有信息理论界限) - This phrase emphasizes that the new information-theoretic generalization guarantees improve upon previous bounds, and provide a more comprehensive and accurate understanding of the generalization abilities of learning algorithms.
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay"><a href="#Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay" class="headerlink" title="Robust Learning for Smoothed Online Convex Optimization with Feedback Delay"></a>Robust Learning for Smoothed Online Convex Optimization with Feedback Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20098">http://arxiv.org/abs/2310.20098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren</li>
<li>for: 本研究探讨了具有多步非线性切换成本和反馈延迟的精制online凸优化问题（SOCO）。</li>
<li>methods: 我们提出了一种新的机器学习（ML）加强的在线算法，即可靠性约束学习（RCL），该算法将不可靠的ML预测与可靠的专家在线算法结合在一起，通过受限 проекcion来强化ML预测。</li>
<li>results: 我们证明了RCL可以保证$(1+\lambda)$-竞争力对任何给定专家，而且在多步切换成本和反馈延迟的情况下，RCL也可以显著提高平均性能和稳定性。<details>
<summary>Abstract</summary>
We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
</details>
<details>
<summary>摘要</summary>
我们研究了一种具有多步非线性跳转成本和反馈延迟的简化在线凸优化问题（SOCO）。我们提出了一种新的机器学习（ML）增强在线算法，即robustness-constrained learning（RCL），该算法将不信任的ML预测与可信的专家在线算法相结合，通过受限 проекtion来强制实现ML预测的稳定性。我们证明了RCL能够保证$(1+\lambda)$-竞争性对任何给定专家，而且同时在平均情况下提高ML预测的性能。这是ML增强算法中第一个具有证明的可靠性保证的多步跳转成本和反馈延迟的情况。我们通过电动汽车续航管理为 случа研究，证明了RCL在稳定性和平均性两个方面的改进。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows"><a href="#Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows" class="headerlink" title="Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows"></a>Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20090">http://arxiv.org/abs/2310.20090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimx/bridging-the-gap-between-vi-and-wgf">https://github.com/yimx/bridging-the-gap-between-vi-and-wgf</a></li>
<li>paper_authors: Mingxuan Yi, Song Liu</li>
<li>for:  bridges the gap between variational inference and Wasserstein gradient flows</li>
<li>methods:  uses the Bures-Wasserstein gradient flow to recast the Euclidean gradient flow, and uses the path-derivative gradient estimator to generate the vector field of the gradient flow</li>
<li>results:  offers an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow, and extends the gradient estimator to encompass $f$-divergences and non-Gaussian variational families, which can be readily implemented using contemporary machine learning libraries like PyTorch or TensorFlow.Here is the summary in Traditional Chinese:</li>
<li>for:  connects 统计量推理和 Wasserstein 梯度流</li>
<li>methods: 使用 Bures-Wasserstein 梯度流将 Euclidian 梯度流重新推射, 并使用 path-derivative 梯度统计来生成梯度流的向量场</li>
<li>results: 提供了一个 alternative  perspective on path-derivative 梯度, 把其描述为 Wasserstein 梯度流的蒸发程序, 并将 gradient estimator 扩展到包括 $f$-divergence 和非泊松型 variational 家族, 这些可以使用 contemporary 机器学习库 like PyTorch 或 TensorFlow 的 readily implementable 方式进行实现。<details>
<summary>Abstract</summary>
Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator for $f$-divergences, readily implementable using contemporary machine learning libraries like PyTorch or TensorFlow.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用变量推断法可以估算目标分布，但是 Wasserstein 梯度流不一定具有参数化概率分布。在这篇论文中，我们证明在某些条件下，布尔-沃asserstein 梯度流可以转化为欧式梯度流，其前向迭代方案与标准黑盒变量推断算法相同。具体来说，梯度流的向量场由路径导数梯度估计生成。我们还提供了一种 alternate 视角，将路径导数梯度框架为 Wasserstein 梯度流的蒸馏过程。这种扩展可以包括 $f$-散度和非泊松变量家族，从而获得一个新的梯度估计器，可以使用现代机器学习库 like PyTorch 或 TensorFlow 实现。Note: "变量推断法" (variable inference) is a more common term in Simplified Chinese, so I used that instead of "变量推断" (variable inference) as in the original text.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/" data-id="clogy1z5w00rkffrab5ad9jky" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/eess.IV_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T09:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/eess.IV_2023_10_31/">eess.IV - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation"><a href="#Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation" class="headerlink" title="Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation"></a>Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20586">http://arxiv.org/abs/2310.20586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Savannah P. Hays, Dzung L. Pham, Jerry L. Prince, Aaron Carass<br>for:这篇论文旨在解决深度学习算法在不同测试环境中表现不佳的问题，即领域泛化错误。methods:这篇论文提出了一种纠正领域泛化错误的方法，即将一shot适应数据与融合training数据进行合并。这种方法被称为“协调融合”（contrast harmonization）。results:实验表明，将一shot适应数据与融合training数据进行合并可以超越使用单独的数据源。此外，只使用融合training数据进行适应也可以达到相似或更高的性能，并且只需要少量的微调。<details>
<summary>Abstract</summary>
Deep learning algorithms utilizing magnetic resonance (MR) images have demonstrated cutting-edge proficiency in autonomously segmenting multiple sclerosis (MS) lesions. Despite their achievements, these algorithms may struggle to extend their performance across various sites or scanners, leading to domain generalization errors. While few-shot or one-shot domain adaptation emerges as a potential solution to mitigate generalization errors, its efficacy might be hindered by the scarcity of labeled data in the target domain. This paper seeks to tackle this challenge by integrating one-shot adaptation data with harmonized training data that incorporates labels. Our approach involves synthesizing new training data with a contrast akin to that of the test domain, a process we refer to as "contrast harmonization" in MRI. Our experiments illustrate that the amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Notably, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation. Moreover, all adaptations required only minimal fine-tuning, ranging from 2 to 5 epochs for convergence.
</details>
<details>
<summary>摘要</summary>
深度学习算法利用核磁共振（MR）图像自动分割多发性淋巴炎（MS）斑点得到了顶尖的表现。然而，这些算法可能会在不同的场景或扫描仪上扩展性能出现域泛化错误。而几拍或一拍适应采用可能解决这个问题，但可能受到目标域的标注数据的罕见性的限制。这篇论文想要解决这个挑战，通过将一shot适应数据与融合标注数据进行协同训练。我们称之为“对比融合”（contrast harmonization）。我们的实验表明，将一shot适应数据与融合标注数据进行协同训练可以超过使用各自的数据源进行训练。此外，使用仅harmonized training data进行适应也可以达到与一shot适应相当或更高的性能，并且所有的适应都需要只需要2-5个epochs进行 converges。
</details></li>
</ul>
<hr>
<h2 id="C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration"><a href="#C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration" class="headerlink" title="C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration"></a>C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20289">http://arxiv.org/abs/2310.20289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weizhu Xu, Qingbin Fan, Peicheng Lin, Jiarong Wang, Hao Hu, Tao Yue, Xuemei Hu, Ting Xu</li>
<li>for:  This paper is written for researchers and developers who are interested in spectrometer technology and its applications. It aims to present a new method for miniaturized spectrometers based on spectrally engineered filtering.</li>
<li>methods:  The paper proposes a new method for reconstructive spectrometers based on silicon metasurfaces. The method uses angle integration to achieve robustness to angle and aperture within a wide working bandwidth.</li>
<li>results:  The proposed method is experimentally demonstrated to maintain spectral consistency from F&#x2F;1.8 to F&#x2F;4 and accurately reconstruct the incident hyperspectral signal with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels is established using the proposed method.<details>
<summary>Abstract</summary>
Compared with conventional grating-based spectrometers, reconstructive spectrometers based on spectrally engineered filtering have the advantage of miniaturization because of the less demand for dispersive optics and free propagation space. However, available reconstructive spectrometers fail to balance the performance on operational bandwidth, spectral diversity and angular stability. In this work, we proposed a compact silicon metasurfaces based spectrometer/camera. After angle integration, the spectral response of the system is robust to angle/aperture within a wide working bandwidth from 400nm to 800nm. It is experimentally demonstrated that the proposed method could maintain the spectral consistency from F/1.8 to F/4 (The corresponding angle of incident light ranges from 7{\deg} to 16{\deg}) and the incident hyperspectral signal could be accurately reconstructed with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels is also established in this work. The accurate reconstructed hyperspectral image indicates that the proposed aperture-robust spectrometer has the potential to be extended as a high-resolution broadband hyperspectral camera.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/eess.IV_2023_10_31/" data-id="clogy1za0018dffrafsqn46no" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/eess.SP_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T08:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/eess.SP_2023_10_31/">eess.SP - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Energy-Aware-Adaptive-Sampling-for-Self-Sustainability-in-Resource-Constrained-IoT-Devices"><a href="#Energy-Aware-Adaptive-Sampling-for-Self-Sustainability-in-Resource-Constrained-IoT-Devices" class="headerlink" title="Energy-Aware Adaptive Sampling for Self-Sustainability in Resource-Constrained IoT Devices"></a>Energy-Aware Adaptive Sampling for Self-Sustainability in Resource-Constrained IoT Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20331">http://arxiv.org/abs/2310.20331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ETH-PBL/EcoTrack">https://github.com/ETH-PBL/EcoTrack</a></li>
<li>paper_authors: Marco Giordano, Silvano Cortesi, Prodromos-Vasileios Mekikis, Michele Crabolu, Giovanni Bellusci, Michele Magno</li>
<li>for: 这篇论文主要用于提出一种能源意识的适应式采样率算法，用于适应资源受限、电池供电的iot设备。</li>
<li>methods: 该算法基于finite state machine (FSM)，并且受到传输控制协议(TCP) Reno的加法增加和乘法减少的启发。该算法的目的是最大化传感器采样率，以确保自足性能无需担心电池耗尽。</li>
<li>results: 该算法在三个欧洲城市的数据上进行验证，结果表明，该算法可以实现自足性能，同时最大化采样位置数。在使用3000mAh电池的实验中，算法一直保持了每天至少24个地点的Localization，并达到了3000个的峰值。<details>
<summary>Abstract</summary>
In the ever-growing Internet of Things (IoT) landscape, smart power management algorithms combined with energy harvesting solutions are crucial to obtain self-sustainability. This paper presents an energy-aware adaptive sampling rate algorithm designed for embedded deployment in resource-constrained, battery-powered IoT devices. The algorithm, based on a finite state machine (FSM) and inspired by Transmission Control Protocol (TCP) Reno's additive increase and multiplicative decrease, maximizes sensor sampling rates, ensuring power self-sustainability without risking battery depletion. Moreover, we characterized our solar cell with data acquired over 48 days and used the model created to obtain energy data from an open-source world-wide dataset. To validate our approach, we introduce the EcoTrack device, a versatile device with global navigation satellite system (GNSS) capabilities and Long-Term Evolution Machine Type Communication (LTE-M) connectivity, supporting MQTT protocol for cloud data relay. This multi-purpose device can be used, for instance, as a health and safety wearable, remote hazard monitoring system, or as a global asset tracker. The results, validated on data from three different European cities, show that the proposed algorithm enables self-sustainability while maximizing sampled locations per day. In experiments conducted with a 3000 mAh battery capacity, the algorithm consistently maintained a minimum of 24 localizations per day and achieved peaks of up to 3000.
</details>
<details>
<summary>摘要</summary>
在不断扩展的互联网物联网（IoT）景观中，智能电力管理算法与能量收集解决方案是获得自足性的关键。这篇论文提出了一种基于finite state machine（FSM）的能量意识适应样本率算法，用于嵌入式部署在有限的电池电力的IoT设备中。该算法利用TCP Reno的加法增加和乘法减少，最大化传感器样本率，保证电池不会耗尽而无风险。此外，我们使用了48天的数据来 caracterize我们的太阳能电池，并使用了一个开源的全球数据集来获取能量数据。为验证我们的方法，我们引入了EcoTrack设备，这是一种多功能设备，具有全球定位系统（GNSS）和Long-Term Evolution Machine Type Communication（LTE-M）连接，支持MQTT协议将数据传输到云端。这种多功能设备可以用作健康和安全套件、远程险情监测系统或全球资产追踪器。实验结果，在三个欧洲城市的数据上验证了我们的方法，显示了自我可持续性，最大化采样位置数。在使用3000mAh电池容量的实验中，算法一直保持了每天至少24个地点的Localization，并达到了3000个的峰值。
</details></li>
</ul>
<hr>
<h2 id="Age-Optimum-Sampling-in-Non-Stationary-Environment"><a href="#Age-Optimum-Sampling-in-Non-Stationary-Environment" class="headerlink" title="Age Optimum Sampling in Non-Stationary Environment"></a>Age Optimum Sampling in Non-Stationary Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20275">http://arxiv.org/abs/2310.20275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinheng Zhang, Haoyue Tang, Jintao Wang, Sastry Kompella, Leandros Tassiulas</li>
<li>for: 这篇论文是关于状态更新系统的研究，其中包括一个传感器和接收器。传感器将状态更新信息传递给接收器，但是传输滞后分布是非站ARY的，这使得接收器所获得的数据的新鲜度难以保证。</li>
<li>methods: 作者提出了一种在线采样策略，用于最小化接收器所获得的数据新鲜度。该策略使用了 Stochastic Approximation 和非 Parametric Change Point Detection 算法，可以快速探测传输滞后分布的变化，并在变化后重新开始学习过程。</li>
<li>results:  simulations 表明，提出的策略可以快速探测传输滞后分布的变化，并且接收器所获得的数据新鲜度可以减少至最小值。<details>
<summary>Abstract</summary>
In this work, we consider a status update system with a sensor and a receiver. The status update information is sampled by the sensor and then forwarded to the receiver through a channel with non-stationary delay distribution. The data freshness at the receiver is quantified by the Age-of-Information (AoI). The goal is to design an online sampling strategy that can minimize the average AoI when the non-stationary delay distribution is unknown. Assuming that channel delay distribution may change over time, to minimize the average AoI, we propose a joint stochastic approximation and non-parametric change point detection algorithm that can: (1) learn the optimum update threshold when the delay distribution remains static; (2) detect the change in transmission delay distribution quickly and then restart the learning process. Simulation results show that the proposed algorithm can quickly detect the delay changes, and the average AoI obtained by the proposed policy converges to the minimum AoI.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们考虑了一个状态更新系统，该系统包括一个传感器和一个接收器。状态更新信息由传感器采样并将其传递给接收器通过一个具有非站ARY的延迟分布的通道。接收器的数据新鲜度被衡量为Age-of-Information（AoI）。目标是设计一个在线采样策略，以最小化接收器的平均AoI，当传播延迟分布未知时。假设通道延迟分布可能会随时间变化，我们提议一种联合随机抽象和非参数变化检测算法，可以：（1）在延迟分布保持静止时学习最佳更新阈值；（2）快速检测传播延迟分布的变化，然后重新开始学习过程。实验结果表明，我们的算法可以快速检测延迟变化，并且提出的策略的平均AoI converge到最小AoI。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Reflecting-Surface-Assisted-UAV-Communications-for-6G-Networks"><a href="#Intelligent-Reflecting-Surface-Assisted-UAV-Communications-for-6G-Networks" class="headerlink" title="Intelligent-Reflecting-Surface-Assisted UAV Communications for 6G Networks"></a>Intelligent-Reflecting-Surface-Assisted UAV Communications for 6G Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20242">http://arxiv.org/abs/2310.20242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolong Ning, Tengfeng Li, Yu Wu, Xiaojie Wang, Qingqing Wu, Fei Richard Yu, Song Guo</li>
<li>for: 本研究是关于6G移动网络中使用智能反射表面和无人机技术的全面评估。</li>
<li>methods: 本文使用智能反射表面技术来解决6G移动网络中的覆盖困难和资源约束问题。</li>
<li>results: 本研究提出了一些特定的解决方案，以解决智能反射表面助け的无人机通信中的问题。<details>
<summary>Abstract</summary>
In 6th-Generation (6G) mobile networks, Intelligent Reflective Surfaces (IRSs) and Unmanned Aerial Vehicles (UAVs) have emerged as promising technologies to address the coverage difficulties and resource constraints faced by terrestrial networks. UAVs, with their mobility and low costs, offer diverse connectivity options for mobile users and a novel deployment paradigm for 6G networks. However, the limited battery capacity of UAVs, dynamic and unpredictable channel environments, and communication resource constraints result in poor performance of traditional UAV-based networks. IRSs can not only reconstruct the wireless environment in a unique way, but also achieve wireless network relay in a cost-effective manner. Hence, it receives significant attention as a promising solution to solve the above challenges. In this article, we conduct a comprehensive survey on IRS-assisted UAV communications for 6G networks. First, primary issues, key technologies, and application scenarios of IRS-assisted UAV communications for 6G networks are introduced. Then, we put forward specific solutions to the issues of IRS-assisted UAV communications. Finally, we discuss some open issues and future research directions to guide researchers in related fields.
</details>
<details>
<summary>摘要</summary>
在6代移动网络（6G）中，智能反射表面（IRS）和无人飞行器（UAV）已经出现为解决地面网络覆盖困难和资源受限的技术。UAV具有 mobilidad和低成本，可以为移动用户提供多样的连接选择和6G网络的新部署模式。然而，UAV的电池容量有限，通信环境随机和难以预测，以及通信资源的限制，导致传统的UAV网络表现不佳。IRS可以不仅重构无线环境，还可以在成本效益的情况下实现无线网络中继。因此，它在6G网络中得到了广泛的关注。在这篇文章中，我们进行了6G网络中IRS协助UAV通信的全面评估。首先，我们介绍了6G网络中IRS协助UAV通信的主要问题、关键技术和应用场景。然后，我们提出了解决IRS协助UAV通信的问题的具体方案。最后，我们讨论了一些未解决的问题和未来研究方向，以帮助相关领域的研究人员。
</details></li>
</ul>
<hr>
<h2 id="Structured-Two-Stage-True-Time-Delay-Array-Codebook-Design-for-Multi-User-Data-Communication"><a href="#Structured-Two-Stage-True-Time-Delay-Array-Codebook-Design-for-Multi-User-Data-Communication" class="headerlink" title="Structured Two-Stage True-Time-Delay Array Codebook Design for Multi-User Data Communication"></a>Structured Two-Stage True-Time-Delay Array Codebook Design for Multi-User Data Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20198">http://arxiv.org/abs/2310.20198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Wadaskar, Ding Zhao, Ibrahim Pehlivan, Danijela Cabric</li>
<li>for: 该论文旨在设计一种适用于多用户同时通信的宽频毫米波和teraHz系统中的频率依赖性扩散 beam。</li>
<li>methods: 该论文提出一种结构化的analog True-Time-Delay（TTD）编码库，用于生成具有量化频率带-角度映射的 beam。</li>
<li>results: 该论文分析了由所提出的阶梯 TTD 编码库生成的 beam 图像的频率空间行为，并评估了这些 beam 在多用户通信网络中的性能。<details>
<summary>Abstract</summary>
Wideband millimeter-wave and terahertz (THz) systems can facilitate simultaneous data communication with multiple spatially separated users. It is desirable to orthogonalize users across sub-bands by deploying frequency-dependent beams with a sub-band-specific spatial response. True-Time-Delay (TTD) antenna arrays are a promising wideband architecture to implement sub-band-specific dispersion of beams across space using a single radio frequency (RF) chain. This paper proposes a structured design of analog TTD codebooks to generate beams that exhibit quantized sub-band-to-angle mapping. We introduce a structured Staircase TTD codebook and analyze the frequency-spatial behaviour of the resulting beam patterns. We develop the closed-form two-stage design of the proposed codebook to achieve the desired sub-band-specific beams and evaluate their performance in multi-user communication networks.
</details>
<details>
<summary>摘要</summary>
宽带毫米波和tera赫兹（THz）系统可以实现同时数据通信多个空间分隔的用户。愿意将用户分配到子带中的差异化，通过部署频率依赖的扫描阵列来实现。准确时延（TTD）天线阵列是一种广泛应用的宽带体系，可以通过单个 радио频率（RF）链实现子带特定的扫描阵列。本文提出了一种结构化的analog TTD编码ebook的设计，以生成具有量化子带-角度映射的扫描阵列。我们介绍了一种结构化的梯形 TTD编码ebook，并分析了其频率空间行为的 beam 模式。我们还开发了closed-form两个阶段的设计方案，以实现所需的子带特定的扫描阵列，并评估其在多用户通信网络中的性能。
</details></li>
</ul>
<hr>
<h2 id="SWIPT-in-Mixed-Near-and-Far-Field-Channels-Joint-Beam-Scheduling-and-Power-Allocation"><a href="#SWIPT-in-Mixed-Near-and-Far-Field-Channels-Joint-Beam-Scheduling-and-Power-Allocation" class="headerlink" title="SWIPT in Mixed Near- and Far-Field Channels: Joint Beam Scheduling and Power Allocation"></a>SWIPT in Mixed Near- and Far-Field Channels: Joint Beam Scheduling and Power Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20186">http://arxiv.org/abs/2310.20186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francisco-cabanillas/BC_231918-201864-231073">https://github.com/francisco-cabanillas/BC_231918-201864-231073</a></li>
<li>paper_authors: Yunpu Zhang, Changsheng You</li>
<li>for: 本文主要针对未来无线网络中大规模antenna的应用，通过实现射频镜的笔直扩散来提高spectrum efficiency和空间分辨率。</li>
<li>methods: 本文提出了一种新的实用场景，即混合近场和远场SWIPT，其中能量收集器和信息解码器分别位于near-field和far-field地域中。本文还提出了一种最优化问题，以最大化所有能量收集器中所得到的总功率。</li>
<li>results: 本文的计算结果表明，在多个能量收集器和一个信息解码器的情况下，最优的设计是将一部分功率分配给信息解码器，以满足速率约束，而剩余的功率则分配给一个能量收集器 WITH 最高收集能力。这与传统远场SWIPT情况不同，在那里所有功率都应该分配给信息解码器。计算结果还表明， compare to other benchmark schemes without optimization of beam scheduling and&#x2F;or power allocation, 我们提出的联合设计显著地提高性能。<details>
<summary>Abstract</summary>
Extremely large-scale array (XL-array) has emerged as a promising technology to enhance the spectrum efficiency and spatial resolution in future wireless networks by exploiting massive number of antennas for generating pencil-like beamforming. This also leads to a fundamental paradigm shift from conventional far-field communications towards the new near-field communications. In contrast to the existing works that mostly considered simultaneous wireless information and power transfer (SWIPT) in the far field, we consider in this paper a new and practical scenario, called mixed near- and far-field SWIPT, where energy harvesting (EH) and information decoding (ID) receivers are located in the near- and far-field regions of the XL-array base station (BS), respectively. Specifically, we formulate an optimization problem to maximize the weighted sum-power harvested at all EH receivers by jointly designing the BS beam scheduling and power allocation, under the constraints on the maximum sum-rate and BS transmit power. First, for the general case with multiple EH and ID receivers, we propose an efficient algorithm to obtain a suboptimal solution by utilizing the binary variable elimination and successive convex approximation methods. To obtain useful insights, we then study the joint design for special cases. In particular, we show that when there are multiple EH receivers and one ID receiver, in most cases, the optimal design is allocating a portion of power to the ID receiver for satisfying the rate constraint, while the remaining power is allocated to one EH receiver with the highest EH capability. This is in sharp contrast to the conventional far-field SWIPT case, for which all powers should be allocated to ID receivers. Numerical results show that our proposed joint design significantly outperforms other benchmark schemes without the optimization of beam scheduling and/or power allocation.
</details>
<details>
<summary>摘要</summary>
很大规模的数组（XL-array）已经出现为未来无线网络的提高谱效率和空间分辨率的有力技术，通过卷积大量天线来实现笔直式增强 beamforming。这也导致了传统远场通信的基本思想转变，转而 towards near-field communication。与现有的works主要关注远场同时无线信息和能量传输（SWIPT），我们在这篇论文中考虑了一个新和实用的情况，即混合近场和远场 SWIPT，其中能量收集（EH）和信息解码（ID）接收器分别位于近场和远场基站（BS）附近。特别是，我们提出了一个优化问题，以最大化所有EH接收器上的加权总能量，通过BS扫描和分配电力的共同设计，以满足BS传输功率和最大合理谱效率的约束。首先，对多个EH和ID接收器的普通 случа件，我们提出了一种高效的算法，通过利用二进制变量消除和渐进几何方法来获得一个子优化解决方案。为了获得有用的洞察，我们然后研究特殊情况的共同设计。具体来说，当有多个EH接收器和一个ID接收器时，我们发现，在大多数情况下，最优的设计是将一部分能量分配给ID接收器，以满足速率约束，而剩下的能量则分配给EH接收器 WITH 最高EH能力。这与传统远场 SWIPT情况不同，在那里，所有能量都应该分配给ID接收器。数值结果显示，我们提出的共同设计显著超越了没有优化扫描和/或分配电力的参考方案。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/eess.SP_2023_10_31/" data-id="clogy1zag01a7ffra93rl3gbb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.SD_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T15:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.SD_2023_10_30/">cs.SD - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DCHT-Deep-Complex-Hybrid-Transformer-for-Speech-Enhancement"><a href="#DCHT-Deep-Complex-Hybrid-Transformer-for-Speech-Enhancement" class="headerlink" title="DCHT: Deep Complex Hybrid Transformer for Speech Enhancement"></a>DCHT: Deep Complex Hybrid Transformer for Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19602">http://arxiv.org/abs/2310.19602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialu Li, Junhui Li, Pu Wang, Youshan Zhang</li>
<li>for: 提高对话 зву频环境中的噪声消除性能</li>
<li>methods:  combines spectrogram和waveform频域方法，使用复杂的Swin-Unet和双路传播 transformer网络</li>
<li>results: 在BirdSoundsDenoising和VCTK+DEMAND datasets上实验表明，提出的方法可以比领域内其他方法提高噪声消除性能<details>
<summary>Abstract</summary>
Most of the current deep learning-based approaches for speech enhancement only operate in the spectrogram or waveform domain. Although a cross-domain transformer combining waveform- and spectrogram-domain inputs has been proposed, its performance can be further improved. In this paper, we present a novel deep complex hybrid transformer that integrates both spectrogram and waveform domains approaches to improve the performance of speech enhancement. The proposed model consists of two parts: a complex Swin-Unet in the spectrogram domain and a dual-path transformer network (DPTnet) in the waveform domain. We first construct a complex Swin-Unet network in the spectrogram domain and perform speech enhancement in the complex audio spectrum. We then introduce improved DPT by adding memory-compressed attention. Our model is capable of learning multi-domain features to reduce existing noise on different domains in a complementary way. The experimental results on the BirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our method can achieve better performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的深度学习基础的话音提高方法大多都仅在几何域或波形域中运作。虽然有跨领域transformer combing waveform-和spectrogram-domain inputs的方案，但其表现仍可以进一步改善。在这篇文章中，我们提出了一个新的深度复杂混合transformer模型，它整合了几何域和波形域的方法，以提高话音提高的表现。我们的模型由两个部分组成：一个复杂的Swin-Unet网络在几何域，以及一个双输路transformer网络（DPTnet）在波形域。我们首先在几何域建立了一个复杂的Swin-Unet网络，并在复杂的音频几何中进行话音提高。接着，我们提出了改进的DPT，通过增加快速储存压缩注意力。我们的模型能够学习不同领域的特征，以减少不同领域上的现有噪音，并且在不同领域上实现了辅助功能。实验结果显示，我们的方法在BirdSoundsDenoising dataset和VCTK+DEMAND dataset上比前方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Sound-of-Story-Multi-modal-Storytelling-with-Audio"><a href="#Sound-of-Story-Multi-modal-Storytelling-with-Audio" class="headerlink" title="Sound of Story: Multi-modal Storytelling with Audio"></a>Sound of Story: Multi-modal Storytelling with Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19264">http://arxiv.org/abs/2310.19264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeyeon Bae, Seokhoon Jeong, Seokun Kang, Namgi Han, Jae-Yon Lee, Hyounghun Kim, Taehwan Kim</li>
<li>for: 这研究旨在扩展故事理解和告知领域，通过引入一个新的“背景声”组件，它是基于故事上下文的声音，没有语言信息。</li>
<li>methods: 这研究使用了一个新的“Sound of Story”（SoS）数据集，其包含27,354个故事，每个故事有19.6张图片和984小时的语音背景音乐和其他声音。</li>
<li>results: 研究提出了一些新的数据采集和任务基准，包括 междуmodalities Retrieval Task和audio generation Task，这些任务可以帮助探索故事的多模态理解。<details>
<summary>Abstract</summary>
Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of the story. Therefore, we propose to extend story understanding and telling areas by establishing a new component called "background sound" which is story context-based audio without any linguistic information. For this purpose, we introduce a new dataset, called "Sound of Story (SoS)", which has paired image and text sequences with corresponding sound or background music for a story. To the best of our knowledge, this is the largest well-curated dataset for storytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6 images per story and 984 hours of speech-decoupled audio such as background music and other sounds. As benchmark tasks for storytelling with sound and the dataset, we propose retrieval tasks between modalities, and audio generation tasks from image-text sequences, introducing strong baselines for them. We believe the proposed dataset and tasks may shed light on the multi-modal understanding of storytelling in terms of sound. Downloading the dataset and baseline codes for each task will be released in the link: https://github.com/Sosdatasets/SoS_Dataset.
</details>
<details>
<summary>摘要</summary>
幻像是多modal的在真实世界中。当一个人 telling a story 时，可能使用所有的视觉化和声音来表达故事的意义。然而，先前的故事数据集和任务却对声音 pay little attention，尽管声音也携带了故事中意义重要的 semantics。因此，我们提议扩展故事理解和告诉领域，通过在“背景声音”（Background Sound）中添加新组件，该组件是基于故事上下文的声音，没有任何语言信息。为了实现这一目标，我们介绍了一个新的数据集，即“幻像的声音”（Sound of Story，SoS），该数据集包含27,354个故事，每个故事有19.6个图像和984小时的语音，其中包括背景音乐和其他声音。我们认为这是故事幻像理解中声音的最大规模、最好的数据集。为了评估这些数据集和任务，我们提出了多个任务，包括 между模态逻辑回归、图像-文本Sequences 到声音的生成任务，并提供了强大的基线。我们认为这些任务和数据集可能为故事幻像理解中声音提供新的灵感。下载这些数据集和基线代码可以通过以下链接：https://github.com/Sosdatasets/SoS_Dataset。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.SD_2023_10_30/" data-id="clogy1z7300x6ffra15jcchcj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/eess.AS_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T14:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/eess.AS_2023_10_30/">eess.AS - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Scenario-Aware-Audio-Visual-TF-GridNet-for-Target-Speech-Extraction"><a href="#Scenario-Aware-Audio-Visual-TF-GridNet-for-Target-Speech-Extraction" class="headerlink" title="Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction"></a>Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19644">http://arxiv.org/abs/2310.19644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Gordon Wichern, Yoshiki Masuyama, Francois G. Germain, Sameer Khurana, Chiori Hori, Jonathan Le Roux<br>for: 本研究旨在提取基于给定的条件音频引导信号的目标语音信号，该信号受到干扰源如噪声或竞争说话者的污染。methods: 我们提出了基于TF-GridNet的视觉地基因模型AV-GridNet，该模型在提取过程中使用目标说话人的脸部录制作为条件因素。此外，我们还提出了场景意识模型SAV-GridNet，该模型可以在不同的干扰enario中识别干扰源的类型，然后应用专门为该场景预训练的专家模型。results: 我们的提出模型在第二届COG-MHEAR Audio-Visual Speech Enhancement Challenge中获得了最佳Result，在对比其他模型的对比和听测中，我们的模型表现出了明显的优势。我们还进行了对结果的广泛分析，包括两个场景下的分析。<details>
<summary>Abstract</summary>
Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.
</details>
<details>
<summary>摘要</summary>
Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.Here's the translation in Traditional Chinese: Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/eess.AS_2023_10_30/" data-id="clogy1z800110ffrabm2ffq4s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CV_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T13:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.CV_2023_10_30/">cs.CV - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview"><a href="#Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview" class="headerlink" title="Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview"></a>Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20083">http://arxiv.org/abs/2310.20083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuvam Keshari, Tanusree Dutta, Raju Mullick, Ashish Rathor, Priyadarshi Patnaik</li>
<li>for: 这个研究旨在帮助采访者更好地评估面试者的人格特征，减少采访者的认知压力。</li>
<li>methods: 这个研究使用了 behaviometry 技术，具体来说是通过分析面试者的表情、嗓音、肤色反应、近距离行为、身体语言等不同方面的行为特征，以获取不受社会可能性和假装影响的 объектив信息。</li>
<li>results: 研究发现，通过使用 behaviometry 技术，可以对面试者的行为进行对比分析，并且可以在不同的视频样本中获取高度相似的结果。在实验中，使用 open-source 计算机视觉算法和库（python-opencv 和 dlib），对三个 YouTube 视频样本进行了帧对帆分析，并发现 SSID 分数在 75% 以上的情况下表现出行为一致性。<details>
<summary>Abstract</summary>
Choosing the right person for the right job makes the personnel interview process a cognitively demanding task. Psychometric tests, followed by an interview, have often been used to aid the process although such mechanisms have their limitations. While psychometric tests suffer from faking or social desirability of responses, the interview process depends on the way the responses are analyzed by the interviewers. We propose the use of behaviometry as an assistive tool to facilitate an objective assessment of the interviewee without increasing the cognitive load of the interviewer. Behaviometry is a relatively little explored field of study in the selection process, that utilizes inimitable behavioral characteristics like facial expressions, vocalization patterns, pupillary reactions, proximal behavior, body language, etc. The method analyzes thin slices of behavior and provides unbiased information about the interviewee. The current study proposes the methodology behind this tool to capture facial expressions, in terms of facial asymmetry and micro-expressions. Hemi-facial composites using a structural similarity index was used to develop a progressive time graph of facial asymmetry, as a test case. A frame-by-frame analysis was performed on three YouTube video samples, where Structural similarity index (SSID) scores of 75% and more showed behavioral congruence. The research utilizes open-source computer vision algorithms and libraries (python-opencv and dlib) to formulate the procedure for analysis of the facial asymmetry.
</details>
<details>
<summary>摘要</summary>
选择合适的人 für 合适的工作是人员面试的认知需求很高。常用的方法包括心理测试 followed by 面试，但这些机制有其局限性。心理测试受到假答案和社会受欢迎的响应的困扰，而面试过程则取决于面试官如何分析回答。我们提议使用 behaviometry 作为助手，以帮助对面试者进行 объектив的评估，不增加面试官的认知负担。 behaviometry 是一个还未受到充分探索的选拔过程中的研究领域，它利用不同人的唯一行为特征，如面部表情、嗓音模式、眼动反应、躯体语言等，来对面试者进行评估。该方法分析精细的行为迹象，提供不受偏见的信息。本研究提出了对于捕捉面部表情的方法，包括面部不均匀性和微表情。通过 Hemiface composite 技术，建立了面部不均匀性的进度图。对 YouTube 视频三个样本进行了帧对帆分析，SSID 分数达 75% 以上显示行为一致。研究使用了开源计算机视觉算法和库（python-opencv 和 dlib）来定义面部不均匀性的分析过程。
</details></li>
</ul>
<hr>
<h2 id="LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart"><a href="#LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart" class="headerlink" title="LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart"></a>LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20065">http://arxiv.org/abs/2310.20065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Fanwei Kong, Shawn Shadden</li>
<li>for: automatically generate computer models of the human heart from patient imaging data</li>
<li>methods: deforming a template mesh to fit the cardiac structures, using a two-stage diffeomorphic deformation process and a novel loss function to minimize mesh self-penetration</li>
<li>results: meshes free of self-intersections, comparable accuracy with state-of-the-art methods, and readily usable in physics-based simulation without the need for post-processing and cleanup.Here’s the full text in Simplified Chinese:</li>
<li>for:  automatische genereren van computer modellen van het menselijke hart uit patiënt-imaging-gegevens</li>
<li>methods: deformeren van een template-mesh om de hartstructuren te passen, gebruik maken van een twee-fase diffeomorfe deformatieve proces en een nieuwe verliesfunctie om mes-zelf-doorboringen te minimaliseren</li>
<li>results: mesh-vrij van zelf-doorboringen, vergelijkbare accuracy met staat-van-het-kunst-methoden, en direct gebruikbaar in fysisch-gebaseerde simulatie zonder post-behandeling en opschoning nodig hebben.<details>
<summary>Abstract</summary>
We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
</details>
<details>
<summary>摘要</summary>
我们提出了一种深度学习模型，用于自动生成人体心脏模型从患者影像数据中，强调其能够生成薄壁心脏结构。我们的方法是将模板网格变形以适应给定图像中的心脏结构。与先前的深度学习方法不同，我们的框架是为避免网格自交叠，通常在变形 superfic 上存在小距离之间的自交叠。我们实现了这一目标通过两stage diffeomorphic deformation 过程和一种新的损失函数，该函数基于运动 dinamics 中的几何学特征，以惩罚表面接触和交叠。我们的模型与当前状态的方法具有相似的准确性，同时还可以生成自交叠的网格。所得到的网格可以directly用于基于物理学的仿真，减少后期处理和整理的需求。
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal"><a href="#A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal" class="headerlink" title="A Scalable Training Strategy for Blind Multi-Distribution Noise Removal"></a>A Scalable Training Strategy for Blind Multi-Distribution Noise Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20064">http://arxiv.org/abs/2310.20064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Zhang, Sakshum Kulshrestha, Christopher Metzler</li>
<li>for: 提出一种基于adaptive sampling&#x2F;active learning的 universal denoising networks训练方法，以提高特定频率训练的效果。</li>
<li>methods: 使用一种基于多项式近似的特性损失地形图，以减少训练时间，并使用适应样本选择策略来吸引训练数据的优化。</li>
<li>results: 通过测试在模拟的联合波音-高斯-斑雾噪声中，提出的训练策略可以使一个普通的权重相同的单个缺陷网络达到特定频率训练的特化网络的峰峰信号强率 bound，并且在一个小型图像集上，对比uniform训练的基准，提出的适应样本选择策略可以获得更好的性能。<details>
<summary>Abstract</summary>
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.   In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed universal denoiser training strategy by extending these results to higher dimensions and by incorporating a polynomial approximation of the true specification-loss landscape. This approximation allows us to reduce training times by almost two orders of magnitude. We test our method on simulated joint Poisson-Gaussian-Speckle noise and demonstrate that with our proposed training strategy, a single blind, generalist denoiser network can achieve peak signal-to-noise ratios within a uniform bound of specialized denoiser networks across a large range of operating conditions. We also capture a small dataset of images with varying amounts of joint Poisson-Gaussian-Speckle noise and demonstrate that a universal denoiser trained using our adaptive-sampling strategy outperforms uniformly trained baselines.
</details>
<details>
<summary>摘要</summary>
尽管最近有所进步，总结束universal锈排除和遗传物理误差网络仍然是一个大多数开放的问题：固定网络参数时，一定程度上特化于一个任务（例如，除掉Poisson锈）的性能会降低另一个任务（例如，除掉锦锈锈）的性能。此外，训练这种网络也是困难的，因为维度的诅咒：随着特征空间的维度的增加（即需要描述噪声分布的参数数），需要训练的唯一特征的数量会 exponential增加。随机抽取这个空间会导致一个网络可以在非常困难的任务特征上做出非常好的表现，但是在容易的任务特征上做出非常差的表现，即使大的错误也只会对总的平方误差产生非常小的影响。在这个工作中，我们提出了使用adaptive-sampling/active-learning策略来训练锈排除网络。我们的工作超越了最近提出的universal锈排除训练策略，并在更高的维度上进行扩展，并通过使用幂等式approximation来减少训练时间。我们在 simulated joint Poisson-Gaussian-Speckle 噪声下测试了我们的方法，并证明了一个替身、通用的锈排除网络可以在一个广泛的操作条件下达到特殊化锈排除网络的峰值信号强度。我们还捕捉了一个小型的图像数据集，该数据集包含不同量的 joint Poisson-Gaussian-Speckle 噪声，并证明了一个通用的锈排除网络，使用我们的adaptive-sampling策略训练后，可以超过uniform训练的基eline。
</details></li>
</ul>
<hr>
<h2 id="SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling"><a href="#SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling" class="headerlink" title="SolarFormer: Multi-scale Transformer for Solar PV Profiling"></a>SolarFormer: Multi-scale Transformer for Solar PV Profiling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20057">http://arxiv.org/abs/2310.20057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Luis, Minh Tran, Taisei Hanyu, Anh Tran, Liao Haitao, Roy McCann, Alan Mantooth, Ying Huang, Ngan Le</li>
<li>for: 该论文旨在提供高精度的太阳能板 Installation 地图，以便更好地理解太阳能板的采用和扩展。</li>
<li>methods: 该论文提出了 SolarFormer 模型，通过多尺度 Transformer 编码器和带mask的 Transformer 解码器，解决了太阳能板识别的复杂问题，并利用低级特征和实例查询机制进一步提高了地图的精度。</li>
<li>results: 经过广泛的实验表明，SolarFormer 模型能够与或超过当前状态的模型，提供高精度的太阳能板分 segmentation，为全球可再生能源发展提供了有价值的参考。<details>
<summary>Abstract</summary>
As climate change intensifies, the global imperative to shift towards sustainable energy sources becomes more pronounced. Photovoltaic (PV) energy is a favored choice due to its reliability and ease of installation. Accurate mapping of PV installations is crucial for understanding their adoption and informing energy policy. To meet this need, we introduce the SolarFormer, designed to segment solar panels from aerial imagery, offering insights into their location and size. However, solar panel identification in Computer Vision is intricate due to various factors like weather conditions, roof conditions, and Ground Sampling Distance (GSD) variations. To tackle these complexities, we present the SolarFormer, featuring a multi-scale Transformer encoder and a masked-attention Transformer decoder. Our model leverages low-level features and incorporates an instance query mechanism to enhance the localization of solar PV installations. We rigorously evaluated our SolarFormer using diverse datasets, including GGE (France), IGN (France), and USGS (California, USA), across different GSDs. Our extensive experiments consistently demonstrate that our model either matches or surpasses state-of-the-art models, promising enhanced solar panel segmentation for global sustainable energy initiatives.
</details>
<details>
<summary>摘要</summary>
However, identifying solar panels using computer vision is a complex task due to factors such as weather conditions, roof conditions, and variations in ground sampling distance (GSD). To tackle these challenges, the SolarFormer employs a multi-scale transformer encoder and a masked-attention transformer decoder, which leverages low-level features and incorporates an instance query mechanism to enhance the localization of solar PV installations.We evaluated the SolarFormer using diverse datasets from France, California, and other regions, and our extensive experiments consistently showed that our model either matches or outperforms existing models in terms of solar panel segmentation. This promises to enhance sustainable energy initiatives globally.
</details></li>
</ul>
<hr>
<h2 id="Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation"><a href="#Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation" class="headerlink" title="Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation"></a>Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20039">http://arxiv.org/abs/2310.20039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoichi Watanabe, Rukhsora Akramova<br>for: 这项研究旨在评估验血医生和自动分割工具的分割精度，并评估Dice相似度系数（DSC）是否能够准确评估分割精度。methods: 本研究使用PyRadiomics提取 радиомькс特征，并根据Concordance Correlation Coefficient（CCC）选择可重复的特征来评估分割精度。results: 研究发现206个 радиомькс特征的CCC值大于0.93，表明这些特征具有Robust重复性。此外，7个特征表现出低Intraclass Correlation Coefficients（ICC），这意味着它们更敏感于分割差异。特别是，原始形态特征，包括球形性、延展性和平坦性，ICC值在0.1177-0.995之间。与此相反，所有DSC值都大于0.778。这项研究表明， радиомькс特征，特别是形态和能量相关的特征，可以捕捉分割精度中的细微差异，而DSC则无法做到这一点。因此，Radiomics特征与ICC证明为评估医生分割能力和自动分割工具性能的优秀指标。这些新指标可以用来评估新的自动分割方法，并提高医疗分割训练，从而提高放疗医疗实践。<details>
<summary>Abstract</summary>
In high-quality radiotherapy delivery, precise segmentation of targets and healthy structures is essential. This study proposes Radiomics features as a superior measure for assessing the segmentation ability of physicians and auto-segmentation tools, in comparison to the widely used Dice Similarity Coefficient (DSC). The research involves selecting reproducible radiomics features for evaluating segmentation accuracy by analyzing radiomics data from 2 CT scans of 10 lung tumors, available in the RIDER Data Library. Radiomics features were extracted using PyRadiomics, with selection based on the Concordance Correlation Coefficient (CCC). Subsequently, CT images from 10 patients, each segmented by different physicians or auto-segmentation tools, were used to assess segmentation performance. The study reveals 206 radiomics features with a CCC greater than 0.93 between the two CT images, indicating robust reproducibility. Among these features, seven exhibit low Intraclass Correlation Coefficients (ICC), signifying increased sensitivity to segmentation differences. Notably, ICCs of original shape features, including sphericity, elongation, and flatness, ranged from 0.1177 to 0.995. In contrast, all DSC values exceeded 0.778. This research demonstrates that radiomics features, particularly those related to shape and energy, can capture subtle variations in tumor segmentation characteristics, unlike DSC. As a result, Radiomics features with ICC prove superior for evaluating a physician's tumor segmentation ability and the performance of auto-segmentation tools. The findings suggest that these new metrics can be employed to assess novel auto-segmentation methods and enhance the training of individuals in medical segmentation, thus contributing to improved radiotherapy practices.
</details>
<details>
<summary>摘要</summary>
高品质放射治疗需要精准的目标和健康结构分割。本研究提出，基于放射学特征（Radiomics）的评价方法比普遍使用的相似度系数（Dice Similarity Coefficient，DSC）更为可靠。研究选择了可重复的放射学特征，通过分析20个肺癌CT扫描图，从RIDER数据库中获得的放射学数据，以确定评价准确性的可靠特征。然后，从10名患者的CT图像中，每名患者由不同的医生或自动分割工具进行分割，以评估分割性能。研究发现，206个放射学特征之间的 concordance correlation coefficient（CCC）大于0.93，表明可靠的重复性。其中，7个特征具有低的间类相关系数（ICC），表明它们更敏感于分割差异。特别是，原始形状特征，包括圆形性、长宽比和平整度，其ICC值分别为0.1177-0.995。与此相比，所有DSC值都大于0.778。这种研究表明，基于形状和能量的放射学特征可以更好地捕捉小差异在肿瘤分割特征中，与DSC不同。因此，基于ICC的放射学特征评价方法比DSC更为可靠，可以用于评估新的自动分割方法和医生分割能力的训练。这些发现可能对放射治疗实践产生积极影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning"><a href="#Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning" class="headerlink" title="Adaptive Anchor Label Propagation for Transductive Few-Shot Learning"></a>Adaptive Anchor Label Propagation for Transductive Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19996">http://arxiv.org/abs/2310.19996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michalislazarou/a2lp">https://github.com/michalislazarou/a2lp</a></li>
<li>paper_authors: Michalis Lazarou, Yannis Avrithis, Guangyu Ren, Tania Stathaki</li>
<li>for: 该研究是为了解决基于有限标注数据的图像分类问题。</li>
<li>methods: 该研究使用了推理扩充法，特别是标签传播法，以利用大量无标注数据来提高少量学习的性能。</li>
<li>results: 该研究提出了一种新的算法，称为自适应固定标签传播（Adaptive Anchor Label Propagation），该算法可以在扩充法中更好地调整标签，从而提高少量学习的性能。对四个常用的少量学习 benchmark 数据集和两种常用的背部骨架进行了实验，并证明了该算法的优势。<details>
<summary>Abstract</summary>
Few-shot learning addresses the issue of classifying images using limited labeled data. Exploiting unlabeled data through the use of transductive inference methods such as label propagation has been shown to improve the performance of few-shot learning significantly. Label propagation infers pseudo-labels for unlabeled data by utilizing a constructed graph that exploits the underlying manifold structure of the data. However, a limitation of the existing label propagation approaches is that the positions of all data points are fixed and might be sub-optimal so that the algorithm is not as effective as possible. In this work, we propose a novel algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function optimizing their positions in the manifold in the process. Our novel algorithm, Adaptive Anchor Label Propagation}, outperforms the standard label propagation algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings respectively. We provide experimental results highlighting the merits of our algorithm on four widely used few-shot benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and WideResNet-28-10. The source code can be found at https://github.com/MichalisLazarou/A2LP.
</details>
<details>
<summary>摘要</summary>
几个拟合学习地址了使用有限的标注数据来分类图像的问题。通过使用无标注数据的推理方法，如标签传播，可以在几个拟合学习中显著提高性能。标签传播将无标注数据的 pseudo-标签推断出来，利用构建的图像结构下的数据图 Mann 的方法。然而，现有的标签传播方法的局限性是所有数据点的位置都是固定的，这可能会导致算法不太有效。在这种情况下，我们提出了一种新的算法，可以适应标注数据的特征表示进行最优化。我们称之为 Adaptive Anchor Label Propagation（A2LP）。在1-shot和5-shot设置中，A2LP算法可以与标准标签传播算法相比，提高性能达7%和2%。我们通过在四个广泛使用的几个拟合学习测试集上进行实验，分别是miniImageNet、tieredImageNet、CUB和CIFAR-FS，以及两种常用的后向核心，ResNet12和WideResNet-28-10，来证明我们的算法的优势。代码可以在https://github.com/MichalisLazarou/A2LP 找到。
</details></li>
</ul>
<hr>
<h2 id="Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning"><a href="#Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning" class="headerlink" title="Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning"></a>Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19995">http://arxiv.org/abs/2310.19995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Etesam, Ozge Nilay Yalcin, Chuxuan Zhang, Angelica Lim</li>
<li>for: 本研究的目的是评估 latest large vision language models (CLIP, LLaVA) 和 large language models (GPT-3.5) 中嵌入的情感常识，并evaluate a purely text-based language model on images。</li>
<li>methods: 研究者使用 “narrative captions” 来描述情感，并使用 872 个物理社会信号描述和 224 个情感相关的环境 Labels 来构建 image-to-language-to-emotion 任务。</li>
<li>results: 研究发现，将 “fast” 和 “slow” 两种理解结合使用可以提高情感识别系统的性能，但 zero-shot 情感理论心问题仍然存在，与之前在 EMOTIC 数据集上进行的研究相比，还有一定的差距。<details>
<summary>Abstract</summary>
The emotional theory of mind problem in images is an emotion recognition task, specifically asking "How does the person in the bounding box feel?" Facial expressions, body pose, contextual information and implicit commonsense knowledge all contribute to the difficulty of the task, making this task currently one of the hardest problems in affective computing. The goal of this work is to evaluate the emotional commonsense knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely text-based language model on images, we construct "narrative captions" relevant to emotion perception, using a set of 872 physical social signal descriptions related to 26 emotional categories, along with 224 labels for emotionally salient environmental contexts, sourced from writer's guides for character expressions and settings. We evaluate the use of the resulting captions in an image-to-language-to-emotion task. Experiments using zero-shot vision-language models on EMOTIC show that combining "fast" and "slow" reasoning is a promising way forward to improve emotion recognition systems. Nevertheless, a gap remains in the zero-shot emotional theory of mind task compared to prior work trained on the EMOTIC dataset.
</details>
<details>
<summary>摘要</summary>
“情感理论心智问题在图像中是一个情感识别任务，具体的问题是“人在矩形盒中如何感到？”Facial expressions、身体姿势、contextual information和隐藏的常识知识都会增加这个任务的difficulty，使其成为现在情感 computing中最难的问题。本工作的目的是评估 latest large vision language models (CLIP, LLaVA) 和 large language models (GPT-3.5) 中嵌入的情感常识，以及在 EMOTIC 数据集上的表现。为了评估仅基于文本的语言模型在图像上，我们建立了“narative captions”，这些caption relevante 到情感识别，使用872个物理社交信号描述和26个情感分类，以及224个情感突出的环境 Labels，它们来自作家的表达和设置指南。我们使用这些 captions 进行图像-语言-情感任务的评估。这些 zero-shot vision-language models 在 EMOTIC 数据集上的实验表明，结合“快”和“慢”的思考是一种有前途的方法来提高情感识别系统。然而，在 zero-shot 情感理论心智任务中，与先前工作相比，仍有一个差距。”
</details></li>
</ul>
<hr>
<h2 id="Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models"><a href="#Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models" class="headerlink" title="Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models"></a>Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19986">http://arxiv.org/abs/2310.19986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preetam Prabhu Srikar Dammu, Yunhe Feng, Chirag Shah</li>
<li>for: 提高机器学习模型对弱化群体的表现</li>
<li>methods: 利用网络搜索和生成模型提高机器学习模型的鲁棒性和减少偏见</li>
<li>results: 在图像分类问题上使用ImageNet的人类树subset中的例子，并达到了减少性别差距的目的，同时提高模型的总性能。<details>
<summary>Abstract</summary>
Machine learning (ML) technologies are known to be riddled with ethical and operational problems, however, we are witnessing an increasing thrust by businesses to deploy them in sensitive applications. One major issue among many is that ML models do not perform equally well for underrepresented groups. This puts vulnerable populations in an even disadvantaged and unfavorable position. We propose an approach that leverages the power of web search and generative models to alleviate some of the shortcomings of discriminative models. We demonstrate our method on an image classification problem using ImageNet's People Subtree subset, and show that it is effective in enhancing robustness and mitigating bias in certain classes that represent vulnerable populations (e.g., female doctor of color). Our new method is able to (1) identify weak decision boundaries for such classes; (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion; and (3) show how these newly captured training samples could alleviate population bias issue. While still improving the model's overall performance considerably, we achieve a significant reduction (77.30\%) in the model's gender accuracy disparity. In addition to these improvements, we observed a notable enhancement in the classifier's decision boundary, as it is characterized by fewer weakspots and an increased separation between classes. Although we showcase our method on vulnerable populations in this study, the proposed technique is extendable to a wide range of problems and domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion"><a href="#‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion" class="headerlink" title="‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion"></a>‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19981">http://arxiv.org/abs/2310.19981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sourojit Ghosh, Aylin Caliskan</li>
<li>For: The paper examines the stereotypes embedded in the text-to-image generator Stable Diffusion, specifically looking at gender and nationality&#x2F;continental identity.* Methods: The authors use vision-language model CLIP’s cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 and manually examine the results. They use prompts with front-facing images of persons from different continents, nationalities, and genders.* Results: The authors find that Stable Diffusion outputs of “a person” without any additional gender&#x2F;nationality information correspond closest to images of men and least with persons of nonbinary gender. They also observe continental stereotypes and resultant harms, such as the erasure of Indigenous Oceanic peoples. Additionally, they unexpectedly observe a pattern of oversexualization of women, specifically Latin American, Mexican, Indian, and Egyptian women relative to other nationalities.<details>
<summary>Abstract</summary>
We study stereotypes embedded within one of the most popular text-to-image generators: Stable Diffusion. We examine what stereotypes of gender and nationality/continental identity does Stable Diffusion display in the absence of such information i.e. what gender and nationality/continental identity is assigned to `a person', or to `a person from Asia'. Using vision-language model CLIP's cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 verified by manual examination, we chronicle results from 136 prompts (50 results/prompt) of front-facing images of persons from 6 different continents, 27 nationalities and 3 genders. We observe how Stable Diffusion outputs of `a person' without any additional gender/nationality information correspond closest to images of men and least with persons of nonbinary gender, and to persons from Europe/North America over Africa/Asia, pointing towards Stable Diffusion having a concerning representation of personhood to be a European/North American man. We also show continental stereotypes and resultant harms e.g. a person from Oceania is deemed to be Australian/New Zealander over Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who form a majority over descendants of colonizers both in Papua New Guinea and in Oceania overall. Finally, we unexpectedly observe a pattern of oversexualization of women, specifically Latin American, Mexican, Indian and Egyptian women relative to other nationalities, measured through an NSFW detector. This demonstrates how Stable Diffusion perpetuates Western fetishization of women of color through objectification in media, which if left unchecked will amplify this stereotypical representation. Image datasets are made publicly available.
</details>
<details>
<summary>摘要</summary>
我们研究了Stable Diffusion中嵌入的 gender和国家/地域标准的偏见。我们发现在没有任何信息时，Stable Diffusion将人物映射到男性和欧美人类中最为常见的情况下。使用CLIP的cosine similarity来比较由CLIP基于的Stable Diffusion v2.1生成的图像，我们对6个大陆、27个国家和3个 gender的136个提示（每个提示50个结果）进行了chronicle。我们发现Stable Diffusion无gender/国家信息时输出的图像最接近男性图像，并且与非binary gender人物最少相似，而且倾向于欧美人类而不是非洲/亚洲人类。这表明Stable Diffusion存在一种担忧的人类标准，即欧美男性。我们还发现了 continent预设和其相关的危害，例如，在 Oceanian 中，人物被认为是澳大利亚/新西兰人而不是 Papua New Guinean，这表明了殖民者的后代和原住民之间的人类观念的抹杀。最后，我们意外地发现了一种对女性进行性化的偏见，具体来说是对拉丁美洲、墨西哥、印度和埃及女性的性化，与其他国家的女性相比，这种偏见会通过媒体对女性的对象化来增强。我们将图像集公开发布。
</details></li>
</ul>
<hr>
<h2 id="Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks"><a href="#Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks" class="headerlink" title="Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks"></a>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19909">http://arxiv.org/abs/2310.19909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsouri/battle-of-the-backbones">https://github.com/hsouri/battle-of-the-backbones</a></li>
<li>paper_authors: Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein</li>
<li>For: The paper aims to benchmark a diverse suite of pretrained models for computer vision tasks, including vision-language models, self-supervised learning models, and the Stable Diffusion backbone, and to provide insights for the research community to advance computer vision.* Methods: The paper uses a diverse set of computer vision tasks, including classification, object detection, and out-of-distribution (OOD) generalization, and conducts more than 1500 training runs to evaluate the performance of various pretrained models.* Results: The paper finds that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks, and that self-supervised learning (SSL) backbones are highly competitive and indicate potential for future advancements with advanced architectures and larger pretraining datasets.<details>
<summary>Abstract</summary>
Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones
</details>
<details>
<summary>摘要</summary>
神经网络基于的计算机视觉系统通常建立在脊梁上，脊梁可以是预训练或随机初始化的特征提取器。过去几年，默认选择是ImageNet快速学习卷积神经网络。然而，最近几年，各种算法和数据集驱动的许多脊梁逐渐出现。这种多样性使得各种系统表现得更好，但是为实践者做出 Informed 决策变得更加困难。“Backbone Battle”（BoB）使得这种选择变得更加容易，它对一系列预训练模型进行了多种计算机视觉任务的测试，包括分类、物体检测和OOD泛化等。此外，BoB还为研究者提供了推进计算机视觉的可能性的指导，通过对超过1500次训练运行的全面分析，揭示了现有方法的优缺点。虽然视transformer（ViTs）和自动学习（SSL）在增长，但我们发现，通过大规模预训练集进行预训练的卷积神经网络仍然在大多数任务上表现最好。此外，在同一种架构和相同大小的预训练集上进行了Apples-to-Apples比较，我们发现SSL脊梁在相同的架构和预训练集上表现很竞争力，这意味着未来的工作应该使用更高级的架构和更大的预训练集进行SSL预训练。我们将实验结果和相关代码发布到GitHub上：<https://github.com/hsouri/Battle-of-the-Backbones>
</details></li>
</ul>
<hr>
<h2 id="MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder"><a href="#MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder" class="headerlink" title="MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder"></a>MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19898">http://arxiv.org/abs/2310.19898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahman-motiur/mist">https://github.com/rahman-motiur/mist</a></li>
<li>paper_authors: Md Motiur Rahman, Shiva Shokouhmand, Smriti Bhatt, Miad Faezipour</li>
<li>for: 这个研究是为了提高医疗影像分类的深度学习方法，特别是使用 transformer 来捕捉医疗影像中的长距离相依性。</li>
<li>methods: 我们提出了一个名为 Medical Image Segmentation Transformer (MIST) 的新方法，它包括一个预训 multi-axis vision transformer (MaxViT) 作为 Encoder，以及一个具有 Convolutional Attention Mixing (CAM) 解码器来于所有空间维度中捕捉长距离相依性。</li>
<li>results: 我们的 MIST transformer 在 ACDC 和 Synapse 数据集上与顶尖模型相比，实现了医疗影像分类的最佳性能。另外，我们还证明了在不同的维度上加入 CAM 解码器可以将分类性能提高到一个更高的水平。<details>
<summary>Abstract</summary>
One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details>
<details>
<summary>摘要</summary>
一种常见且有前途的深度学习方法是使用变换器进行医学图像分割，因为它可以通过自注意力来捕捉图像像素之间的长距离依赖关系。Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details></li>
</ul>
<hr>
<h2 id="DiffEnc-Variational-Diffusion-with-a-Learned-Encoder"><a href="#DiffEnc-Variational-Diffusion-with-a-Learned-Encoder" class="headerlink" title="DiffEnc: Variational Diffusion with a Learned Encoder"></a>DiffEnc: Variational Diffusion with a Learned Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19789">http://arxiv.org/abs/2310.19789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther</li>
<li>for: 这篇论文主要研究了Diffusion模型，它可以视为一种层次变换自动机（VAEs），具有两个改进：参数共享在生成过程中的条件分布，以及独立计算生成过程中的损失。</li>
<li>methods: 论文提出了两种改进 diffusion 模型，包括引入数据和深度相关的均值函数，以及让反编码过程中噪声方差的比率为自由参数。</li>
<li>results: 论文实现了 CIFAR-10 的状态之论文，并提供了一些理论探讨，如使用权重 diffusion 损失来优化噪声计划，以及在无穷深度层次中使用 ELBO 作为目标函数。<details>
<summary>Abstract</summary>
Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite-depth hierarchy, on the other hand, the weight parameter has to be 1 to have a well-defined ELBO.
</details>
<details>
<summary>摘要</summary>
Diffusion models可以看作为层次variational autoencoder (VAEs)，其中有两个改进：在生成过程中共享参数 для conditional distribution，以及独立计算损失的独立项。我们考虑了对 diffusion model 进行两种改变，以增加模型的灵活性。首先，我们在 diffusion process 中引入了数据和深度相依的均值函数，这导致了一个修改后的扩散损失。我们的提议框架，DiffEnc，在 CIFAR-10 上达到了状态泰�值的可能性。其次，我们允许反推进程中噪声变量的比率被视为一个自由参数，而不是固定为 1。这导致了一些理论上的洞察：在有限深度层次结构下，可以使用损失函数作为一个权重 diffusion loss 的目标，并且可以优化噪声 schedules 特别是 для推理。而在无限深度层次结构下，权重必须为 1，以便有一个准确的 ELBO。
</details></li>
</ul>
<hr>
<h2 id="MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision"><a href="#MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision" class="headerlink" title="MM-VID: Advancing Video Understanding with GPT-4V(ision)"></a>MM-VID: Advancing Video Understanding with GPT-4V(ision)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19773">http://arxiv.org/abs/2310.19773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang</li>
<li>for: 本文提出了一个整合GPT-4V的视频理解系统，用于解决长形视频和复杂任务，如多集集成的故事和多小时内容理解。</li>
<li>methods: 本文使用视频转脚本生成器，将多modal元素转化为长文本脚本，以便大语言模型实现视频理解。</li>
<li>results: 实验结果表明，MM-VID可以处理不同类型的视频，并在交互环境中表现出色，如视频游戏和图形用户界面。<details>
<summary>Abstract</summary>
We present MM-VID, an integrated system that harnesses the capabilities of GPT-4V, combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.
</details>
<details>
<summary>摘要</summary>
我们介绍MM-VID，一个集成了GPT-4V的系统，结合视觉、声音和语音特化工具，以实现高级视频理解。MM-VID是为了解决长形视频和复杂任务，如在多集 episodes 中理解故事情节和在多小时内进行推理。MM-VID 使用视频到脚本生成，使用GPT-4V将多Modal元素转化为长文本脚本。生成的脚本包括人物移动、动作、表情和对话，使得大自然语言模型（LLM）可以实现视频理解。这使得可以实现高级功能，包括音频描述、人物识别和多Modal高级理解。实验结果表明MM-VID可以处理不同的视频类型和视频长度。此外，我们还展示了它在交互环境中的潜在应用，如视频游戏和图形用户界面。
</details></li>
</ul>
<hr>
<h2 id="Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP"><a href="#Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP" class="headerlink" title="Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP"></a>Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19752">http://arxiv.org/abs/2310.19752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idstcv/inmap">https://github.com/idstcv/inmap</a></li>
<li>paper_authors: Qi Qian, Yuanhong Xu, Juhua Hu</li>
<li>for: 这篇论文主要是为了提出一种新的方法，即通过文本proxy直接学习视觉代理，以提高零基础任务的性能。</li>
<li>methods: 该方法使用CLIP的vision-language预训练方法，并提出了一种新的抽象方法，即在视觉空间中学习视觉代理。</li>
<li>results: 实验结果表明，该方法可以在短时间内在单个GPU上学习视觉代理，并提高零基础任务的性能。特别是，在ImageNet上，使用ViT-L&#x2F;14@336预训练后的CLIP，可以通过InMaP提高零基础任务的性能从77.02%提高到80.21%.<details>
<summary>Abstract</summary>
Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from $77.02\%$ to $80.21\%$ on ImageNet with ViT-L/14@336 pre-trained by CLIP. Code is available at \url{https://github.com/idstcv/InMaP}.
</details>
<details>
<summary>摘要</summary>
“文本语言预训练方法，如CLIP，显示了无需训练数据的视觉分类表现是非常出佩的。然而，视觉和文本空间之间的模态差异可能会导致表现下降。我们 theoretically 表明，这个差异无法通过CLIP中的对偶损失来减小到足够的程度。因此，在没有标注目标视觉数据的情况下，我们提议通过文本代理来直接学习视觉代理。此外，根据我们的理论分析，我们开发了一些策略来进一步优化 pseudo 标签得到的 InMaP 中的视觉代理。实验表明，InMaP 可以在单个 GPU 上在一分钟内 obtian 视觉代理，并将预训练后 CLIP 的 ViT-L/14@336 的零shot 准确率提高到 $77.02\%$ 到 $80.21\%$。代码可以在 \url{https://github.com/idstcv/InMaP} 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization"><a href="#Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization" class="headerlink" title="Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization"></a>Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19743">http://arxiv.org/abs/2310.19743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Wysoczanska, Moran Beladev, Karen Lastmann Assaraf, Fengjun Wang, Ofri Kleinfeld, Gil Amsalem, Hadas Harush Boker</li>
<li>for: 提高Booking.com上Properties的视觉摘要的用户体验，使其更加符合用户的意图和偏好。</li>
<li>methods: 通过分析用户的评论，提取用户最重要的问题，并将其 integrate into 图像摘要中，以提高用户的满意度。</li>
<li>results: 通过人类感知研究，证明我们的跨Modal方法（CrossSummarizer）在无需昂贵的标注的情况下，提高了图像摘要的质量，比基eline和图像 clustering 方法高。<details>
<summary>Abstract</summary>
Image collection summarization techniques aim to present a compact representation of an image gallery through a carefully selected subset of images that captures its semantic content. When it comes to web content, however, the ideal selection can vary based on the user's specific intentions and preferences. This is particularly relevant at Booking.com, where presenting properties and their visual summaries that align with users' expectations is crucial. To address this challenge, we consider user intentions in the summarization of property visuals by analyzing property reviews and extracting the most significant aspects mentioned by users. By incorporating the insights from reviews in our visual summaries, we enhance the summaries by presenting the relevant content to a user. Moreover, we achieve it without the need for costly annotations. Our experiments, including human perceptual studies, demonstrate the superiority of our cross-modal approach, which we coin as CrossSummarizer over the no-personalization and image-based clustering baselines.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:图像集合概要技术的目的是为图像集合提供一个精选的子集，以捕捉它的semantic content。但是在网络内容上，用户的具体目标和喜好会影响最佳选择。这 particualry relevant at Booking.com，因为在这里，为用户提供Properties和它们的视觉概要，与用户的期望保持一致，是关键。为解决这个挑战，我们在property visual概要中考虑用户的意图，通过分析Property reviews和提取用户提到的最重要方面。通过在概要中包含这些概要，我们可以提高概要，并为用户提供相关的内容。此外，我们可以在不需要昂贵的标注的情况下实现这一点。我们的实验，包括人类感知研究，证明我们的 CrossSummarizer 方法在无个性化和图像基于归类基eline上方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models"><a href="#Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models" class="headerlink" title="Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models"></a>Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19721">http://arxiv.org/abs/2310.19721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Jiacheng Wang, Ipek Oguz</li>
<li>for: 这个论文主要针对医疗影像分类领域的诸问题，例如数据收集和标签可用性问题，提出了将自然图像领域的学习结果转移到医疗影像领域的方法。</li>
<li>methods: 这个论文提出了一个名为ProMISe的启发驱动的3D医疗影像分类模型，使用仅一个点启发来利用已经预训的2D图像基础模型中的知识。具体来说，这个模型使用预训的vision transformer从Segment Anything Model（SAM），并添加轻量级的适应器来提取深度相关（3D）空间上下文，不需要更新预训的重量。另外，这个模型还使用了混合网络，以提高结果的稳定性。</li>
<li>results: 这个论文的实验结果显示，Compared to现有的分类方法，包括对应的启发工程，ProMISe模型在两个公共数据集上的colon和肝脏癌瘤分类任务中表现出色，得到了superior的性能。<details>
<summary>Abstract</summary>
To address prevalent issues in medical imaging, such as data acquisition challenges and label availability, transfer learning from natural to medical image domains serves as a viable strategy to produce reliable segmentation results. However, several existing barriers between domains need to be broken down, including addressing contrast discrepancies, managing anatomical variability, and adapting 2D pretrained models for 3D segmentation tasks. In this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation model using only a single point prompt to leverage knowledge from a pretrained 2D image foundation model. In particular, we use the pretrained vision transformer from the Segment Anything Model (SAM) and integrate lightweight adapters to extract depth-related (3D) spatial context without updating the pretrained weights. For robust results, a hybrid network with complementary encoders is designed, and a boundary-aware loss is proposed to achieve precise boundaries. We evaluate our model on two public datasets for colon and pancreas tumor segmentations, respectively. Compared to the state-of-the-art segmentation methods with and without prompt engineering, our proposed method achieves superior performance. The code is publicly available at https://github.com/MedICL-VU/ProMISe.
</details>
<details>
<summary>摘要</summary>
要解决医学成像中的常见问题，如数据获取困难和标签可用性问题，从自然图像领域传输学习可以作为一个可靠的策略来生成可靠的分割结果。然而，需要破坏一些存在于域之间的抗 correlate，包括对比度差、管理生物学变化和适应2D预训练模型 для 3D分割任务。在本文中，我们提出了ProMISe，一种基于单点提示的3D医学成像分割模型，使用Segment Anything Model（SAM）预训练的视图变换器，并将lightweight adapter integrate到抽取深度相关（3D）空间上下文中，无需更新预训练的 веса。为了实现稳定的结果，我们设计了混合网络，并提出了边界意识损失来实现精确的边界。我们对两个公共数据集进行了colon和肠癌肿分割任务的评估，并与无提示工程和已有的状态艺法分割方法进行了比较。与之比较，我们的提posed方法实现了更高的性能。代码可以在https://github.com/MedICL-VU/ProMISe上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions"><a href="#Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions" class="headerlink" title="Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions"></a>Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19695">http://arxiv.org/abs/2310.19695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saúl Alonso-Monsalve, Davide Sgalaberna, Xingyu Zhao, Adrien Molines, Clark McGrew, André Rubbia</li>
<li>for: 这篇论文的目的是解析高能物理中的核反应域中的中微子互动，特别是在实验室中观察中微子互动的发生点，对于低动量的粒子进行精确的识别和量测，以提高中微子事件的能量解析。</li>
<li>methods: 这篇论文使用深度学习方法来精确地EXTRACT单一物体FROM多维 overlap sparse 图像中，并且与完全可微分的生成模型结合，进一步提高图像解析。</li>
<li>results: 这篇论文获得了前所未有的结果，通过深度学习方法进行图像解析，精确地识别并量测低动量粒子的kinematic 参数，进一步提高中微子事件的能量解析。<details>
<summary>Abstract</summary>
Image decomposition plays a crucial role in various computer vision tasks, enabling the analysis and manipulation of visual content at a fundamental level. Overlapping images, which occur when multiple objects or scenes partially occlude each other, pose unique challenges for decomposition algorithms. The task intensifies when working with sparse images, where the scarcity of meaningful information complicates the precise extraction of components. This paper presents a solution that leverages the power of deep learning to accurately extract individual objects within multi-dimensional overlapping-sparse images, with a direct application in high-energy physics with decomposition of overlaid elementary particles obtained from imaging detectors. In particular, the proposed approach tackles a highly complex yet unsolved problem: identifying and measuring independent particles at the vertex of neutrino interactions, where one expects to observe detector images with multiple indiscernible overlapping charged particles. By decomposing the image of the detector activity at the vertex through deep learning, it is possible to infer the kinematic parameters of the identified low-momentum particles - which otherwise would remain neglected - and enhance the reconstructed energy resolution of the neutrino event. We also present an additional step - that can be tuned directly on detector data - combining the above method with a fully-differentiable generative model to improve the image decomposition further and, consequently, the resolution of the measured parameters, achieving unprecedented results. This improvement is crucial for precisely measuring the parameters that govern neutrino flavour oscillations and searching for asymmetries between matter and antimatter.
</details>
<details>
<summary>摘要</summary>
In particular, the proposed approach tackles a highly complex yet unsolved problem: identifying and measuring independent particles at the vertex of neutrino interactions, where one expects to observe detector images with multiple indiscernible overlapping charged particles. By decomposing the image of the detector activity at the vertex through deep learning, it is possible to infer the kinematic parameters of the identified low-momentum particles - which otherwise would remain neglected - and enhance the reconstructed energy resolution of the neutrino event.Additionally, we present an additional step that combines the above method with a fully-differentiable generative model to improve the image decomposition further and, consequently, the resolution of the measured parameters. This improvement is crucial for precisely measuring the parameters that govern neutrino flavor oscillations and searching for asymmetries between matter and antimatter.The proposed approach can be applied to various fields such as high-energy physics, where the decomposition of overlaid elementary particles obtained from imaging detectors is a crucial task. By accurately identifying and measuring individual particles, the proposed approach can enhance the reconstructed energy resolution of the neutrino event and provide valuable insights into the properties of neutrinos and their role in the universe.
</details></li>
</ul>
<hr>
<h2 id="A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification"><a href="#A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification" class="headerlink" title="A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification"></a>A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19675">http://arxiv.org/abs/2310.19675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Achintha Wijesinghe, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这个论文主要针对低成本感知器应用于远程图像分类问题，涉及到Edge服务器和云端分类器之间的物理通道。</li>
<li>methods: 该论文提出了一种三步共同学习策略，以便在限制通道带宽的情况下，使encoder学习出高精度分类结果。</li>
<li>results: 测试结果显示，该方法可以在CIFAR-10和CIFAR-100上提高分类精度，相比传统朴素迪克逊学习方法。<details>
<summary>Abstract</summary>
Among applications of deep learning (DL) involving low cost sensors, remote image classification involves a physical channel that separates edge sensors and cloud classifiers. Traditional DL models must be divided between an encoder for the sensor and the decoder + classifier at the edge server. An important challenge is to effectively train such distributed models when the connecting channels have limited rate/capacity. Our goal is to optimize DL models such that the encoder latent requires low channel bandwidth while still delivers feature information for high classification accuracy. This work proposes a three-step joint learning strategy to guide encoders to extract features that are compact, discriminative, and amenable to common augmentations/transformations. We optimize latent dimension through an initial screening phase before end-to-end (E2E) training. To obtain an adjustable bit rate via a single pre-deployed encoder, we apply entropy-based quantization and/or manual truncation on the latent representations. Tests show that our proposed method achieves accuracy improvement of up to 1.5% on CIFAR-10 and 3% on CIFAR-100 over conventional E2E cross-entropy training.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）应用中使用低成本传感器时，远程图像分类具有物理通信频道，将边缘计算器和云端分类器分离开来。传统的DL模型需要将编码器与云端分类器分开，这会导致训练分布式模型时，通信频道带宽有限制。我们的目标是优化DL模型，使编码器生成的缓冲器具有低带宽，但仍能提供高精度分类。我们提出了一种三步结合学习策略，以导引编码器提取高度准确、可变性强的特征。在扫描阶段之前，我们对缓冲器维度进行了初步屏选。然后，我们通过权重学习来训练编码器，以便在缓冲器中提取高度准确的特征。最后，我们通过对缓冲器进行 entropy 基于压缩和/或手动跳转来控制缓冲器的带宽。我们的方法在 CIFAR-10 和 CIFAR-100 上测试，测试结果显示，我们的方法可以提高精度为 1.5% 和 3%。
</details></li>
</ul>
<hr>
<h2 id="DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization"><a href="#DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization" class="headerlink" title="DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization"></a>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19668">http://arxiv.org/abs/2310.19668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu<br>for:这篇论文的目的是解决现有的视觉学习控制任务中存在的各种问题，包括样本效率、极限性能和种子选择的不稳定性。methods:这篇论文使用了三种核心机制来引导代理人的冒险决策，包括活动减少率（dormant ratio）来衡量代理人网络中的不活跃程度。results:实验表明，DrM在三个连续控制 benchmark环境中（包括DeepMind Control Suite、MetaWorld和Adroit） achievement significant improvements in sample efficiency和极限性能，而无 broken seeds（76个种子）。此外，DrM成为了第一个没有示范的模型自由算法，在DeepMind Control Suite中解决了狗和手 manipulate tasks，以及Adroit中的三个灵活手 manipulate tasks。<details>
<summary>Abstract</summary>
Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.
</details>
<details>
<summary>摘要</summary>
视觉强化学习（RL）在连续控制任务中表现出了承诺。尽管它们已经取得了进步，但现有的算法仍然在许多方面表现不满足，包括样本效率、极限性能和随机种子的稳定性。在这篇论文中，我们发现了现有的视觉RL方法中的一个重要缺陷：代理人经常在训练的早期展现持续的不活跃，从而限制了它们的探索能力。基于这一重要观察，我们还发现了代理人倾向于不活跃探索的倾向和策略网络中的神经活动的相关性。为量化这种不活跃，我们采用了休眠率作为RL代理人网络中的活动度量表。实验证明，DrM可以有效地减少休眠率，从而提高样本效率和极限性能，无需破坏种子（76个种子）。此外，DrM还是首个不需要示例的模型自由算法，在DeepMind Control Suite中的狗和机器人控制任务中解决了任务，以及Adroit中的三个灵活手 manipulate任务。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines"><a href="#Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines" class="headerlink" title="Domain Generalization in Computational Pathology: Survey and Guidelines"></a>Domain Generalization in Computational Pathology: Survey and Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19656">http://arxiv.org/abs/2310.19656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Jahanifar, Manahil Raza, Kesi Xu, Trinh Vuong, Rob Jewsbury, Adam Shephard, Neda Zamanitajeddin, Jin Tae Kwak, Shan E Ahmed Raza, Fayyaz Minhas, Nasir Rajpoot</li>
<li>for: 本研究旨在探讨域特化（Domain Generalization，DS）的问题在计算 PATHOLOGY（CPath）中，以提高模型的泛化能力和可靠性。</li>
<li>methods: 本文系统性地介绍了各种DS类型，并对CPath领域中exist的多种域特化方法进行了概述和分类。同时， authors 还提供了一系列benchmarking实验来评估这些方法的有效性。</li>
<li>results: 研究发现，在CPath领域中，通过精心设计实验和使用特征增强技术，可以很好地解决域特化问题。然而，无一SIZE-fits-all的解决方案，因此 authors 提出了明确的指南和方法来检测和管理域特化问题。<details>
<summary>Abstract</summary>
Deep learning models have exhibited exceptional effectiveness in Computational Pathology (CPath) by tackling intricate tasks across an array of histology image analysis applications. Nevertheless, the presence of out-of-distribution data (stemming from a multitude of sources such as disparate imaging devices and diverse tissue preparation methods) can cause \emph{domain shift} (DS). DS decreases the generalization of trained models to unseen datasets with slightly different data distributions, prompting the need for innovative \emph{domain generalization} (DG) solutions. Recognizing the potential of DG methods to significantly influence diagnostic and prognostic models in cancer studies and clinical practice, we present this survey along with guidelines on achieving DG in CPath. We rigorously define various DS types, systematically review and categorize existing DG approaches and resources in CPath, and provide insights into their advantages, limitations, and applicability. We also conduct thorough benchmarking experiments with 28 cutting-edge DG algorithms to address a complex DG problem. Our findings suggest that careful experiment design and CPath-specific Stain Augmentation technique can be very effective. However, there is no one-size-fits-all solution for DG in CPath. Therefore, we establish clear guidelines for detecting and managing DS depending on different scenarios. While most of the concepts, guidelines, and recommendations are given for applications in CPath, we believe that they are applicable to most medical image analysis tasks as well.
</details>
<details>
<summary>摘要</summary>
深度学习模型在计算 PATH（CPath）中表现出了惊人的有效性，可以解决各种复杂的 Histology 图像分析应用。然而，由于不同来源的数据（如不同的扫描设备和不同的组织准备方法）而导致的域shift（DS）问题，使得训练过的模型在未看过的数据集上的泛化性受到了影响。为了解决这个问题，我们需要开发创新的域泛化（DG）解决方案。我们认为，DG 方法可以在癌症研究和临床实践中对诊断和预后模型产生重要影响，因此我们在这篇文章中提供了关于 DG 在 CPath 中的报告和指南。我们仔细定义了不同类型的 DS，系统地查询和分类了 CPath 领域中的现有 DG 方法和资源，并提供了这些方法的优缺点和适用范围。我们还进行了28种先进 DG 算法的精心的 benchmarking 实验，我们的发现表明，在 CPath 中使用 Careful 实验设计和特有的染料增强技术可以非常有效。然而，没有一个适用于所有情况的 DG 解决方案。因此，我们建立了明确的指南，以便在不同的场景下探测和处理 DS。大多数概念、指南和建议都适用于医学图像分析任务，因此我们认为这些概念、指南和建议是可靠的。
</details></li>
</ul>
<hr>
<h2 id="Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models"><a href="#Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models" class="headerlink" title="Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models"></a>Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19653">http://arxiv.org/abs/2310.19653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Z. Xiao, Johannes Zenn, Robert Bamler</li>
<li>for: 本研究旨在 Mitigating overfitting in variational autoencoders (VAEs) by training on samples from a pre-trained diffusion model.</li>
<li>methods: 该方法使用 diffusion models 的 encoder 固定，从而简化训练并实现精确地近似 true data distribution $p_{\text{data}(\mathbf{x})$。</li>
<li>results: 研究发现，通过在 VAEs 中训练使用 pre-trained diffusion model 的样本，可以有效地 mitigate overfitting。此外，该方法还可以提高 VAEs 的总体性能、挥uren gap 和 robustness，并且只需要一小量的 diffusion model 的样本即可获得这些优化。<details>
<summary>Abstract</summary>
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our proposed method on three different data sets. We find improvements in all metrics compared to both normal training and conventional data augmentation methods, and we show that a modest amount of samples from the diffusion model suffices to obtain these gains.
</details>
<details>
<summary>摘要</summary>
Variational autoencoders (VAEs) 是一种常用的表示学习模型，但它们的编码器容易过拟合 (Cremer et al., 2018)。这是因为它们在训练集而不是真实数据分布 $p_{\text{data}(\mathbf{x})$ 上训练。Diffusion models 则避免了这个问题，因为它们的编码器是固定的。这使得它们的表示变得更难理解，但是它们的训练变得更加简单，可以提供精准的连续近似 $p_{\text{data}(\mathbf{x})$。在这篇论文中，我们表明了使用预训练的Diffusion模型训练 VAE 的编码器可以有效地 Mitigate overfitting。这些结果有些意外，因为最近的发现 (Alemohammad et al., 2023; Shumailov et al., 2023)  Observation 生成模型训练数据上的模型表现下降。我们分析了 VAE 在不同数据集上的总体表现、积累差和稳定性，并发现它们在所有指标中均有改善。我们还发现，只需要一小量的Diffusion模型样本即可获得这些改善。
</details></li>
</ul>
<hr>
<h2 id="DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking"><a href="#DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking" class="headerlink" title="DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking"></a>DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19641">http://arxiv.org/abs/2310.19641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Ollion, Martin Maliet, Caroline Giuglaris, Elise Vacher, Maxime Deforet</li>
<li>for: 这个论文是为了提高二维细胞分 segmentation和跟踪的精度，特别是在复杂的细胞数据集上。</li>
<li>methods: 该论文提出了一种新的深度神经网络（DNN）架构，名为DistNet2D，它利用了中期和长期的时间上下文来减少错误率。DistNet2D使用了七帧输入，并使用了一种后处理过程来利用整个电影中的信息来修正分 segmentation 错误。</li>
<li>results: DistNet2D在两个实验数据集上表现出优于两种最近的方法，其中一个包含紧密排列的细菌细胞，另一个包含核细胞。此外，DistNet2D还可以在大量数据上对细胞大小和形态与其运输性能进行相关分析，并在细菌和核细胞上都达到了优秀的表现。<details>
<summary>Abstract</summary>
Extracting long tracks and lineages from videomicroscopy requires an extremely low error rate, which is challenging on complex datasets of dense or deforming cells. Leveraging temporal context is key to overcome this challenge. We propose DistNet2D, a new deep neural network (DNN) architecture for 2D cell segmentation and tracking that leverages both mid- and long-term temporal context. DistNet2D considers seven frames at the input and uses a post-processing procedure that exploits information from the entire movie to correct segmentation errors. DistNet2D outperforms two recent methods on two experimental datasets, one containing densely packed bacterial cells and the other containing eukaryotic cells. It has been integrated into an ImageJ-based graphical user interface for 2D data visualization, curation, and training. Finally, we demonstrate the performance of DistNet2D on correlating the size and shape of cells with their transport properties over large statistics, for both bacterial and eukaryotic cells.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译文本到简化中文。<</SYS>>从视频微型ECT需要非常低的错误率，特别是在复杂的 datasets 上，这会导致细胞的压缩或变形。我们提出了 DistNet2D，一种新的深度神经网络（DNN）架构，用于2D细胞分割和跟踪，利用了中期和长期的时间上下文。DistNet2D 使用输入7帧，并使用一种利用整个电影中的信息进行修正 segmentation 错误的后处理过程。DistNet2D 在两个实验数据集上表现出色，一个是 densely packed 细菌细胞，另一个是 eukaryotic 细胞。它已经被 integrate 到 ImageJ 基于的图形用户界面中，用于2D 数据可视化、审核和训练。最后，我们示出了 DistNet2D 在细胞大小和形状与其运输性能之间的相关性，包括细菌和 eukaryotic 细胞。
</details></li>
</ul>
<hr>
<h2 id="Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition"><a href="#Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition" class="headerlink" title="Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition"></a>Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19636">http://arxiv.org/abs/2310.19636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge">https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge</a></li>
<li>paper_authors: Yuhang Zhang, Yaqi Li, Lixiong Qin, Xuannan Liu, Weihong Deng</li>
<li>for: 本研究旨在解决面部表达识别（FER）问题中的数据不均衡问题，即大多数收集到的数据显示了快乐或中性表情，而恐惧或厌恶表情则少得多。</li>
<li>methods: 本研究提出了一种新的方法，即利用重新平衡的注意力地图来补做模型，使其从所有训练样本中提取转换不变的信息，以提高对少数类的识别能力。此外，我们还引入了重新平衡的平滑标签，以规则模型的权重，使其更加注意少数类。</li>
<li>results: 实验表明，两个模块共同工作，可以补做模型，并在不均衡FER任务中实现了状态畅的性能。<details>
<summary>Abstract</summary>
Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.
</details>
<details>
<summary>摘要</summary>
Facial expression data exhibits a significant imbalance, with most data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our goal is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by this, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models"><a href="#Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models" class="headerlink" title="Bidirectional Captioning for Clinically Accurate and Interpretable Models"></a>Bidirectional Captioning for Clinically Accurate and Interpretable Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19635">http://arxiv.org/abs/2310.19635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keegan Quigley, Miriam Cha, Josh Barua, Geeticka Chauhan, Seth Berkowitz, Steven Horng, Polina Golland</li>
<li>for: 这个论文主要是为了探讨视语预训练在医学影像分析中的应用和优劣点。</li>
<li>methods: 这篇论文使用了对医学影像报告的 bidirectional captioning 作为预训练方法，并对RadTex 架构进行优化。</li>
<li>results: 研究发现，不仅 captioning 预训练可以提供与对比学习方法相当的视觉编码器，而且 transformer 解码器可以生成丰富的医学信息和相关的交互输出。<details>
<summary>Abstract</summary>
Vision-language pretraining has been shown to produce high-quality visual encoders which transfer efficiently to downstream computer vision tasks. While generative language models have gained widespread attention, image captioning has thus far been mostly overlooked as a form of cross-modal pretraining in favor of contrastive learning, especially in medical image analysis. In this paper, we experiment with bidirectional captioning of radiology reports as a form of pretraining and compare the quality and utility of learned embeddings with those from contrastive pretraining methods. We optimize a CNN encoder, transformer decoder architecture named RadTex for the radiology domain. Results show that not only does captioning pretraining yield visual encoders that are competitive with contrastive pretraining (CheXpert competition multi-label AUC of 89.4%), but also that our transformer decoder is capable of generating clinically relevant reports (captioning macro-F1 score of 0.349 using CheXpert labeler) and responding to prompts with targeted, interactive outputs.
</details>
<details>
<summary>摘要</summary>
视觉语言预训程已经证明可以生成高质量的视觉编码器，这些编码器可以有效地传输到下游计算机视觉任务中。而生成语言模型已经受到广泛关注，但是医学影像分析中的图像描述还未得到了较多的关注。在这篇论文中，我们通过对医学报告的双向描述进行预训程，并与对比学习方法进行比较。我们优化了医学领域的CNNEncoder和TransformerDecoder architecture，并命名为RadTex。结果表明，不仅captioning预训程可以生成与对比学习方法相当的视觉编码器（CheXpert竞赛多标签AUC为89.4%），而且我们的TransformerDecoder还能生成有价值的医学报告（描述macro-F1分数为0.349，使用CheXpert标签器），并能够在提示中生成目标化、互动的输出。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles"><a href="#Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles" class="headerlink" title="Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles"></a>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19630">http://arxiv.org/abs/2310.19630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Rukundo, Andrea Behanova, Riccardo De Feo, Seppo Ronkko, Joni Oja, Jussi Tohka</li>
<li>for: The paper is written for manufacturers of virus-based gene therapy vector products and intermediates to help them monitor the primary particles and purity profiles of their products during development and manufacturing processes.</li>
<li>methods: The paper uses transmission electron microscopy (TEM) imaging to detect and characterize intact adenoviruses in the presence of debris, broken adenoviruses, and artefact particles. The authors developed two software tools for semi-automatic annotation and segmentation of adenoviruses, and automatic segmentation and detection of intact adenoviruses.</li>
<li>results: The authors evaluated the performance of their software tools using quantitative and qualitative methods and found outstanding true positive detection rates compared to false positive and negative rates. The tools were able to detect intact adenoviruses without mistaking them for real debris, broken adenoviruses, and&#x2F;or staining artefacts.<details>
<summary>Abstract</summary>
Regular monitoring of the primary particles and purity profiles of a drug product during development and manufacturing processes is essential for manufacturers to avoid product variability and contamination. Transmission electron microscopy (TEM) imaging helps manufacturers predict how changes affect particle characteristics and purity for virus-based gene therapy vector products and intermediates. Since intact particles can characterize efficacious products, it is beneficial to automate the detection of intact adenovirus against a non-intact-viral background mixed with debris, broken, and artefact particles. In the presence of such particles, detecting intact adenoviruses becomes more challenging. To overcome the challenge, due to such a presence, we developed a software tool for semi-automatic annotation and segmentation of adenoviruses and a software tool for automatic segmentation and detection of intact adenoviruses in TEM imaging systems. The developed semi-automatic tool exploited conventional image analysis techniques while the automatic tool was built based on convolutional neural networks and image analysis techniques. Our quantitative and qualitative evaluations showed outstanding true positive detection rates compared to false positive and negative rates where adenoviruses were nicely detected without mistaking them for real debris, broken adenoviruses, and/or staining artefacts.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Regular monitoring" is translated as "常规监测" (chángyè jiānnéng), which means regular or constant monitoring.* "Primary particles" is translated as "基本粒子" (jīběn zhízi), which refers to the basic particles that make up a drug product.* "Purity profiles" is translated as "纯度轨迹" (zhēngróng xiàoxi), which refers to the purity of the drug product over time.* "Transmission electron microscopy" is translated as "电子显微镜" (diàn xiǎnwēi jìng), which is a technique used to image samples at the nanoscale.* "Adenoviruses" is translated as "adenovirus" (adenovirus), which is a type of virus that is commonly used as a vector for gene therapy.* "Intact particles" is translated as "完整粒子" (quánzhì zhízi), which refers to particles that are not broken or damaged.* "Non-intact-viral background" is translated as "非完整病毒背景" (fēi quánzhì bìngdài xiàoxi), which refers to the presence of non-intact viral particles in the background of the image.* "Debris" is translated as "垃圾" (shīwù), which refers to any debris or contaminants present in the image.* "Broken adenoviruses" is translated as "损坏adenovirus" (shènghuà adenovirus), which refers to adenoviruses that have been damaged or broken.* "Staining artifacts" is translated as "染色artefacts" (rǎn sé artefacts), which refers to any artifacts or staining present in the image that are not related to the adenoviruses.* "Semi-automatic annotation and segmentation" is translated as "半自动注释和分割" (bàn zìdòng běndào xiàngxiàng yǔ fēnpiè), which refers to the process of annotating and segmenting the image manually, but with the assistance of software.* "Automatic segmentation and detection" is translated as "自动分割和检测" (zìdòng fēnpiè yǔ jiànnéng), which refers to the process of automatically segmenting and detecting the adenoviruses in the image.* "Convolutional neural networks" is translated as "卷积神经网络" (juéshì nervous networks), which is a type of neural network that is commonly used for image processing tasks.
</details></li>
</ul>
<hr>
<h2 id="GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo"><a href="#GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo" class="headerlink" title="GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo"></a>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19583">http://arxiv.org/abs/2310.19583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vkvats/GC-MVSNet">https://github.com/vkvats/GC-MVSNet</a></li>
<li>paper_authors: Vibhas K. Vats, Sripad Joshi, David J. Crandall, Md. Alimoor Reza, Soon-heung Jung</li>
<li>for: 本研究は多视角ステレオ（MVS）方法の改善を目的としています。</li>
<li>methods: 本研究では、多个源视角の参考ビュー深度地図の几何学的一致性を学习の中で直接推し量る新しいアプローチを提案します。</li>
<li>results: 我们の実験结果によると、我们のアプローチはDTUおよびBlendedMVSデータセットで新たな状态のアートであり、Tanks and Templesベンチマークでも竞争的な结果を得ることができます。<details>
<summary>Abstract</summary>
Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning.
</details>
<details>
<summary>摘要</summary>
传统的多视图顺序（MVS）方法强调光学和几何一致性约束，而 newer的机器学习基于MVS方法只在多个源视图之间的几何一致性检查为后处理步骤。在这篇论文中，我们提出了一种新的方法，该方法在多个源视图中规范化参考视图深度地图的几何一致性，并在学习过程中直接强制执行这种几何一致性约束。我们发现，添加这种几何一致性损失可以明显加速学习，因为它直接处罚不一致的像素，从而减少了学习迭代次数，相比其他MVS方法的两倍。我们的广泛实验表明，我们的方法在DTU和BlendedMVS数据集上达到了新的状态态枢纽，并在Tanks和Temples benchmark上获得了竞争性的结果。我们认为，GC-MVSNet是首次在多视图、多级几何一致性检查中进行学习的尝试。
</details></li>
</ul>
<hr>
<h2 id="Human-interpretable-and-deep-features-for-image-privacy-classification"><a href="#Human-interpretable-and-deep-features-for-image-privacy-classification" class="headerlink" title="Human-interpretable and deep features for image privacy classification"></a>Human-interpretable and deep features for image privacy classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19582">http://arxiv.org/abs/2310.19582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darya Baranouskaya, Andrea Cavallaro</li>
<li>for: 本研究旨在探讨隐私分类 datasets 和 controversial images 的特性，以及不同评估者对这些图像的隐私标签的差异。</li>
<li>methods: 本研究提出了 eight 隐私特定和人类可读的特征，以提高深度学习模型的性能和图像隐私分类表示。</li>
<li>results: 研究发现，这八种特征可以增加深度学习模型的表达能力，同时也可以提高图像隐私分类的精度。<details>
<summary>Abstract</summary>
Privacy is a complex, subjective and contextual concept that is difficult to define. Therefore, the annotation of images to train privacy classifiers is a challenging task. In this paper, we analyse privacy classification datasets and the properties of controversial images that are annotated with contrasting privacy labels by different assessors. We discuss suitable features for image privacy classification and propose eight privacy-specific and human-interpretable features. These features increase the performance of deep learning models and, on their own, improve the image representation for privacy classification compared with much higher dimensional deep features.
</details>
<details>
<summary>摘要</summary>
文本翻译到简化中文：隐私是一个复杂、主观和Contextual的概念，难以定义。因此，对隐私分类器的图像注释是一项具有挑战性的任务。在这篇论文中，我们分析了隐私分类数据集和不同评估员对涉及隐私的图像的分类标签的冲突。我们讨论了适合图像隐私分类的特征，并提出了八种适合人类理解的隐私特有的特征。这些特征可以提高深度学习模型的性能，同时也可以提高图像隐私分类的表示力。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The Traditional Chinese writing system is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model"><a href="#Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model" class="headerlink" title="Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model"></a>Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19581">http://arxiv.org/abs/2310.19581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyeon Lee, Chaeyoung Jung, Youngjoon Jang, Jaehun Kim, Joon Son Chung</li>
<li>for: 提高对话者的声音分离精度，使其能够更好地保持自然性。</li>
<li>methods: 基于扩散机制，提出了一种 audio-visual 语音分离模型，并提出了一种基于巧合注意力的特征融合机制，以便有效地融合两种模式。</li>
<li>results: 在 VoxCeleb2 和 LRS3 两个测试集上，提出的方法实现了状态之最的结果，并且生成的语音具有更好的自然性。<details>
<summary>Abstract</summary>
The objective of this work is to extract target speaker's voice from a mixture of voices using visual cues. Existing works on audio-visual speech separation have demonstrated their performance with promising intelligibility, but maintaining naturalness remains a challenge. To address this issue, we propose AVDiffuSS, an audio-visual speech separation model based on a diffusion mechanism known for its capability in generating natural samples. For an effective fusion of the two modalities for diffusion, we also propose a cross-attention-based feature fusion mechanism. This mechanism is specifically tailored for the speech domain to integrate the phonetic information from audio-visual correspondence in speech generation. In this way, the fusion process maintains the high temporal resolution of the features, without excessive computational requirements. We demonstrate that the proposed framework achieves state-of-the-art results on two benchmarks, including VoxCeleb2 and LRS3, producing speech with notably better naturalness.
</details>
<details>
<summary>摘要</summary>
“我们的目标是从多个声音混合中提取目标说话者的声音，使用视觉上的指示。现有的对话音频分类方法已经展示了有前途的可能性，但维持自然性仍然是一大挑战。为解决这个问题，我们提出了AVDiffuSS，一个基于扩散机制的对话音频分类模型。为实现两 modalities 的有效融合，我们也提出了一个基于对话的跨注意力特征融合机制。这个机制特别针对说话领域，以整合对话中的音频特征和视觉特征，以维持高时间分辨率的特征融合，无过度的计算需求。我们显示了我们的框架可以在VoxCeleb2和LRS3两个标准资料集上实现州立顶对话说话的成果，并且说话质量明显更高。”
</details></li>
</ul>
<hr>
<h2 id="A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction"><a href="#A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction" class="headerlink" title="A Perceptual Shape Loss for Monocular 3D Face Reconstruction"></a>A Perceptual Shape Loss for Monocular 3D Face Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19580">http://arxiv.org/abs/2310.19580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Otto, Prashanth Chandran, Gaspard Zoss, Markus Gross, Paulo Gotardo, Derek Bradley</li>
<li>for: 提出了一种新的损失函数，用于评估照明影响的3D人脸重建质量。</li>
<li>methods: 利用人类视觉系统中对3D形状的识别指标——光照效果，设计了一种新的’人体感知’形状损失函数，通过 critics网络来评估给定图像和geometry estimate的匹配程度。</li>
<li>results: 结果表明，将这种新的损失函数与传统的能量函数相结合，可以提高现有状态的最佳结果。<details>
<summary>Abstract</summary>
Monocular 3D face reconstruction is a wide-spread topic, and existing approaches tackle the problem either through fast neural network inference or offline iterative reconstruction of face geometry. In either case carefully-designed energy functions are minimized, commonly including loss terms like a photometric loss, a landmark reprojection loss, and others. In this work we propose a new loss function for monocular face capture, inspired by how humans would perceive the quality of a 3D face reconstruction given a particular image. It is widely known that shading provides a strong indicator for 3D shape in the human visual system. As such, our new 'perceptual' shape loss aims to judge the quality of a 3D face estimate using only shading cues. Our loss is implemented as a discriminator-style neural network that takes an input face image and a shaded render of the geometry estimate, and then predicts a score that perceptually evaluates how well the shaded render matches the given image. This 'critic' network operates on the RGB image and geometry render alone, without requiring an estimate of the albedo or illumination in the scene. Furthermore, our loss operates entirely in image space and is thus agnostic to mesh topology. We show how our new perceptual shape loss can be combined with traditional energy terms for monocular 3D face optimization and deep neural network regression, improving upon current state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
单眼3D脸重建是一个广泛的研究主题，现有的方法可以通过快速的神经网络推断或者离线迭代重建脸部几何。在任一情况下，当面对着精心设计的能量函数时，通常包括图像损失、点击复写损失和其他损失函数。在这个工作中，我们提出了一个新的损失函数 для单眼3D脸捕捉，灵感来自于人类对3D形状的视觉评价。我们发现，阴影提供了3D形状中强大的视觉指标。因此，我们的新的“感知”形状损失将评估3D脸估计是否具有优秀的视觉质感，使用对照绘制的阴影。我们的损失函数是一个批评网络，它将对一个脸形状估计和一个阴影绘制进行评估，并且预测这两个元素之间的视觉匹配程度。这个批评网络仅仅使用RGB图像和几何测量，没有需要场景照明估计。此外，我们的损失函数具有对 mesh 顶点数据的无知性，即使是在不同的 mesh 构造下。我们显示了我们的新的感知形状损失可以与传统的能量函数和神经网络回推进行结合，以提高目前的州OF-THE-ART结果。
</details></li>
</ul>
<hr>
<h2 id="Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms"><a href="#Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms" class="headerlink" title="Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms"></a>Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19574">http://arxiv.org/abs/2310.19574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debvrat Varshney, Masoud Yari, Oluwanisola Ibikunle, Jilu Li, John Paden, Maryam Rahnemoonfar</li>
<li>for: 这项研究的目的是提高空气雷达探测器上的冰层层次检测精度，以便计算冰川沉积量和海平面升高的贡献。</li>
<li>methods: 本研究使用了wavelet基于的多尺度深度学习架构来处理空气雷达探测器上的冰层信号，以提高冰层检测的精度。</li>
<li>results: 我们的提议的Skip-WaveNet架构可以在不同的数据集上实现高度的泛化能力，并且可以准确地估计冰层的深度和层次结构。这种网络可以帮助科学家跟踪冰层，计算年度的雪堆积量，估计冰川的表面质量减少，并帮助预测全球海平面升高。<details>
<summary>Abstract</summary>
Echograms created from airborne radar sensors capture the profile of firn layers present on top of an ice sheet. Accurate tracking of these layers is essential to calculate the snow accumulation rates, which are required to investigate the contribution of polar ice cap melt to sea level rise. However, automatically processing the radar echograms to detect the underlying firn layers is a challenging problem. In our work, we develop wavelet-based multi-scale deep learning architectures for these radar echograms to improve firn layer detection. We show that wavelet based architectures improve the optimal dataset scale (ODS) and optimal image scale (OIS) F-scores by 3.99% and 3.7%, respectively, over the non-wavelet architecture. Further, our proposed Skip-WaveNet architecture generates new wavelets in each iteration, achieves higher generalizability as compared to state-of-the-art firn layer detection networks, and estimates layer depths with a mean absolute error of 3.31 pixels and 94.3% average precision. Such a network can be used by scientists to trace firn layers, calculate the annual snow accumulation rates, estimate the resulting surface mass balance of the ice sheet, and help project global sea level rise.
</details>
<details>
<summary>摘要</summary>
雷达探测机器上的echogram可以捕捉到冰层的Profile，以便计算雪聚积率，这些积积率是研究北极冰川融化对海平面升高的重要因素。然而，自动从雷达echogram中检测下面的冰层是一项复杂的问题。在我们的工作中，我们开发了基于波лет的多尺度深度学习架构，以提高firn层的检测精度。我们发现，基于波лет的架构可以提高优化数据集大小（ODS）和优化图像大小（OIS）的F1分数 by 3.99%和3.7%，respectively，相比非波лет架构。此外，我们提出的Skip-WaveNet架构在每次迭代中生成新的波лет，实现了更高的普适性，并且测算层的深度平均绝对误差为3.31像素和94.3%的平均准确率。这种网络可以被科学家用来跟踪firn层，计算年度雪聚积率，估算冰川的表面质量平衡，并帮助计算全球海平面升高。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning"><a href="#Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning" class="headerlink" title="Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning"></a>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19559">http://arxiv.org/abs/2310.19559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL">https://github.com/Andy20178/DCL</a></li>
<li>paper_authors: Changsheng Lv, Shuai Zhang, Yapeng Tian, Mengshi Qi, Huadong Ma</li>
<li>for: 本研究提出了一种基于counterfactual学习的分离 Commonsense Physical Reasoning~(DCL)方法，用于物理 audiovisual 常识逻辑推理。</li>
<li>methods: 我们的提posed方法使用分离sequential encoder来解决视频数据中不同特征的问题，并通过对比损失函数来增强模型之间的相互信息。此外，我们还引入了对抗学习模块，以增强模型的逻辑能力。</li>
<li>results: 我们的实验结果表明， compared to基eline方法，我们的提posed方法可以提高模型的性能，并达到领先的状态。我们的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL">https://github.com/Andy20178/DCL</a> 中下载。<details>
<summary>Abstract</summary>
In this paper, we propose a Disentangled Counterfactual Learning~(DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. Our source code is available at https://github.com/Andy20178/DCL.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种分离性counterfactual学习（DCL）方法，用于物理audiovisual常识逻辑。任务的目标是根据视频和音频输入推理出物体的物理常识，主要挑战是如何模仿人类的思维能力。现有方法多数不能充分利用多modal数据的不同特征，同时模型中缺乏 causal 逻辑能力，这些问题限制了隐藏物理知识的推理进展。为解决这些问题，我们提出的DCL方法在幂值空间中分离视频为静态（时间不变）和动态（时间变化）因素，使用分离sequential编码器，该编码器采用了variational autoencoder（VAE）来最大化与对比损失函数的mutual information。此外，我们引入了对因果学习模块，以增强模型的逻辑能力，通过模型物理知识之间的相互关系的模拟，在对因果干预下进行推理。我们提出的方法可以与任何基础模型集成，并在实验中超越基础方法，达到了状态的最佳性能。我们的源代码可以在https://github.com/Andy20178/DCL中获取。
</details></li>
</ul>
<hr>
<h2 id="Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining"><a href="#Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining" class="headerlink" title="Harvest Video Foundation Models via Efficient Post-Pretraining"></a>Harvest Video Foundation Models via Efficient Post-Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19554">http://arxiv.org/abs/2310.19554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/internvideo">https://github.com/opengvlab/internvideo</a></li>
<li>paper_authors: Yizhuo Li, Kunchang Li, Yinan He, Yi Wang, Yali Wang, Limin Wang, Yu Qiao, Ping Luo</li>
<li>For: 提出了一种高效的基于图像的视频基础模型抽象方法，以便更好地利用大量的图像数据来快速建立高质量的视频基础模型。* Methods: 提出了一种INTUITIVE和简单的方法，包括随机drop输入视频patches和mask输入文本，以促进跨模态融合学习。* Results: 在多种视频语言下沉淀任务上展现出了优秀的表现，与一些努力预训练的视频基础模型的表现相当。该方法可以在8个GPU上训练completed within one day，只需要WebVid-10M作为预训练数据。<details>
<summary>Abstract</summary>
Building video-language foundation models is costly and difficult due to the redundant nature of video data and the lack of high-quality video-language datasets. In this paper, we propose an efficient framework to harvest video foundation models from image ones. Our method is intuitively simple by randomly dropping input video patches and masking out input text during the post-pretraining procedure. The patch dropping boosts the training efficiency significantly and text masking enforces the learning of cross-modal fusion. We conduct extensive experiments to validate the effectiveness of our method on a wide range of video-language downstream tasks including various zero-shot tasks, video question answering, and video-text retrieval. Despite its simplicity, our method achieves state-of-the-art performances, which are comparable to some heavily pretrained video foundation models. Our method is extremely efficient and can be trained in less than one day on 8 GPUs, requiring only WebVid-10M as pretraining data. We hope our method can serve as a simple yet strong counterpart for prevalent video foundation models, provide useful insights when building them, and make large pretrained models more accessible and sustainable. This is part of the InternVideo project \url{https://github.com/OpenGVLab/InternVideo}.
</details>
<details>
<summary>摘要</summary>
（建立视频语言基础模型是成本和困难的，因为视频数据具有重复性和缺乏高质量视频语言数据。在这篇论文中，我们提出一种高效的框架，可以从图像基础模型中抽取视频基础模型。我们的方法是通过随机删除输入视频补丁和隐藏输入文本来进行后期预训练。补丁删除提高了训练效率，而文本隐藏强制学习 crossing 模式融合。我们进行了广泛的实验，以验证我们的方法在多种视频语言下沉淀任务中的效果，包括多种零 shot 任务、视频问答和视频文本检索。尽管我们的方法简单，但它可以达到状态 искусственного智能性的表现，与一些努力预训练的视频基础模型相当。我们的方法非常高效，可以在8个GPU上训练完成 less than one day，只需要 WebVid-10M 作为预训练数据。我们希望我们的方法可以作为一种简单 yet strong 对等，为建立视频基础模型提供有用的指导，使大型预训练模型更加可持续和可 accessible。这是InternVideo项目的一部分，请参考 \url{https://github.com/OpenGVLab/InternVideo}.）
</details></li>
</ul>
<hr>
<h2 id="MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection"><a href="#MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection" class="headerlink" title="MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection"></a>MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19545">http://arxiv.org/abs/2310.19545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colton R. Crum, Adam Czajka</li>
<li>for: 提高困难任务中 CNN 的表现，如生物认证表现攻击检测。</li>
<li>methods: 利用人类注意力图生成 CNN 的训练数据，并通过两轮训练来快速地集成人类注意力到模型中。</li>
<li>results: 提高生物认证表现攻击检测性能，生成无穷数量的人类类似注意力图，提高模型训练效率。<details>
<summary>Abstract</summary>
Incorporating human salience into the training of CNNs has boosted performance in difficult tasks such as biometric presentation attack detection. However, collecting human annotations is a laborious task, not to mention the questions of how and where (in the model architecture) to efficiently incorporate this information into model's training once annotations are obtained. In this paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr iris pResentation attack detection), which addresses both of these issues through two unique rounds of training. First, we train an autoencoder to learn human saliency maps given an input iris image (both real and fake examples). Once this representation is learned, we utilize the trained autoencoder in two different ways: (a) as a pre-trained backbone for an iris presentation attack detector, and (b) as a human-inspired annotator of salient features on unknown data. We show that MENTOR's benefits are threefold: (a) significant boost in iris PAD performance when using the human perception-trained encoder's weights compared to general-purpose weights (e.g. ImageNet-sourced, or random), (b) capability of generating infinite number of human-like saliency maps for unseen iris PAD samples to be used in any human saliency-guided training paradigm, and (c) increase in efficiency of iris PAD model training. Sources codes and weights are offered along with the paper.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用人类焦点进行 convolutional neural network（CNN）训练，可以提高难度任务，如生物ometric presentation attack detection（PAD）的性能。然而，收集人类注解是一项劳动ioso的任务，而且模型建立的问题，即如何有效地将这些信息 integrate into 模型的训练中。本文介绍了MENTOR（huMan pErceptioN-guided preTraining fOr iris pResentation attack detection），它解决了这两个问题。我们首先在输入图像（真实和假示例）上使用自动encoder学习人类焦点地图。然后，我们利用这已经学习的地图在两种不同的方式：（a）作为预训练的backbone for iris PAD detector，和（b）作为人类静观注解的人类化特征标注工具。我们表明了MENTOR的优势是：（a）使用人类识别训练的encoder的 weights比如ImageNet或随机的 weights，可以获得显著提高iris PAD性能；（b）可以生成无数量的人类化焦点地图，用于未经见过的iris PAD样本训练；（c）提高iris PAD模型训练效率。我们提供了代码和 weights。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking"><a href="#Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking" class="headerlink" title="Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking"></a>Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19542">http://arxiv.org/abs/2310.19542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/AViTMP">https://github.com/Tchuanm/AViTMP</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer, Jianlin Zhang, Yongmei Huang</li>
<li>for: 提高单支持神经网络的视觉跟踪性能，尤其是在长时间跟踪和鲁棒性方面。</li>
<li>methods: 提出了一种适应 ViT 模型预测跟踪器（AViTMP），通过在 ViT 编码器中引入适应模块和联合目标状态嵌入来强化密度插值方式。然后，将 AViT-Enc 与密集混合解码器和抑制性目标模型结合，以便准确预测位置。最后，提出了一种新的推理管线called CycleTrack，可以在干扰物存在下提高跟踪 robustness。</li>
<li>results: 在十个跟踪 benchmark 上进行了全面评估，并获得了state-of-the-art表现，特别是在长时间跟踪和鲁棒性方面。<details>
<summary>Abstract</summary>
Despite achieving state-of-the-art performance in visual tracking, recent single-branch trackers tend to overlook the weak prior assumptions associated with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the effectiveness of discriminative trackers remains constrained due to the adoption of the dual-branch pipeline. To tackle the inferior effectiveness of the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP) to bridge the gap between single-branch network and discriminative models. Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module and joint target state embedding to enrich the dense embedding paradigm based on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a discriminative target model to predict accurate location. Further, to mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. Lastly, we propose a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. In the experiments, we evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that AViTMP attains state-of-the-art performance, especially on long-time tracking and robustness.
</details>
<details>
<summary>摘要</summary>
尽管最新的单支追踪器在视觉追踪性能方面培达了国际级水平，但是这些单支追踪器往往忽视了视觉转换器（ViT）encoder和推理管道中的弱优先级假设。此外，使用双支管道管道的追踪器效果受到了限制。为了解决普通的ViT追踪器的不足，我们提出了适应性ViT模型预测追踪器（AViTMP），以填补单支网络和推理模型之间的差距。具体来说，在我们提出的AViT-Encoder中，我们引入了适应模块和联合目标状态嵌入，以把拥有 dense embedding 的ViT Encoder更加强化。然后，我们将AViT-Encoder与 dense-fusion 解码器和一个推理模型组合起来，以准确预测位置。此外，为了缓解传统的推理实践中的局限性，我们提出了一种新的推理管道 called CycleTrack，它通过双向循环跟踪验证来增强追踪的Robustness。最后，我们提出了一种双帧更新推理策略，以适应长期enario中的挑战。在实验中，我们对十种追踪标准 benchmark 进行了广泛的评估，包括LaSOT、LaSOTExtSub、AVisT等。实验结果明确表明，AViTMP在长期追踪和Robustness方面具有国际级水平的表现。
</details></li>
</ul>
<hr>
<h2 id="IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models"><a href="#IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models" class="headerlink" title="IterInv: Iterative Inversion for Pixel-Level T2I Models"></a>IterInv: Iterative Inversion for Pixel-Level T2I Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19540">http://arxiv.org/abs/2310.19540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/IterInv">https://github.com/Tchuanm/IterInv</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer</li>
<li>for: 这个研究的目的是提供一个可控的文本至图像传播模型，让用户可以通过修改文本提示来控制生成的图像。</li>
<li>methods: 这个研究使用了内存散射模型（LDM）和深度对映（DeepFloyd-IF）等技术，并提出了一个迭代复位（IterInv）技术来解决对于DDIM倒数的问题。</li>
<li>results: 根据实验结果，IterInv技术可以将生成的图像修改为原始图像，并且与具有实际应用前景的图像修改方法结合，证明了IterInv的应用前景。<details>
<summary>Abstract</summary>
Large-scale text-to-image diffusion models have been a ground-breaking development in generating convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are relying on DDIM inversion as a common practice based on the Latent Diffusion Models (LDM). However, the large pretrained T2I models working on the latent space as LDM suffer from losing details due to the first compression stage with an autoencoder mechanism. Instead, another mainstream T2I pipeline working on the pixel level, such as Imagen and DeepFloyd-IF, avoids this problem. They are commonly composed of several stages, normally with a text-to-image stage followed by several super-resolution stages. In this case, the DDIM inversion is unable to find the initial noise to generate the original image given that the super-resolution diffusion models are not compatible with the DDIM technique. According to our experimental findings, iteratively concatenating the noisy image as the condition is the root of this problem. Based on this observation, we develop an iterative inversion (IterInv) technique for this stream of T2I models and verify IterInv with the open-source DeepFloyd-IF model. By combining our method IterInv with a popular image editing method, we prove the application prospects of IterInv. The code will be released at \url{https://github.com/Tchuanm/IterInv.git}.
</details>
<details>
<summary>摘要</summary>
大规模文本到图像扩散模型已经成为生成真实图像的重要发展之一，目标是通过修改文本提示来给用户控制生成图像。当前的图像编辑技术仍然是基于Diffusion-based Image Synthesis（DDIM）的倒推。然而，大规模预训练的T2I模型在干扰空间中作为LDM时会产生loss of details问题，这是因为第一个压缩阶段使用自适应机制。相比之下，另一个主流的T2I管道，如Imagen和DeepFloyd-IF，通常由文本到图像阶段和多个超分辨阶段组成，这些阶段可以减少loss of details问题。然而，使用DDIM倒推时，无法找到初始噪声，因为超分辨扩散模型与DDIM技术不兼容。根据我们的实验结果，iteratively concatenating the noisy image as the condition是这个问题的根本原因。基于这一观察，我们开发了一种iterative inversion（IterInv）技术，并在open-source DeepFloyd-IF模型上验证了IterInv。通过将IterInv与一种流行的图像编辑方法结合使用，我们证明了IterInv的应用前景。代码将在 \url{https://github.com/Tchuanm/IterInv.git} 上发布。
</details></li>
</ul>
<hr>
<h2 id="Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation"><a href="#Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation" class="headerlink" title="Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation"></a>Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19535">http://arxiv.org/abs/2310.19535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaowei Gao, Mingyang Song, Christopher Schroers, Yang Zhang</li>
<li>for: 这篇论文是为了提出一种基于深度学习的视频去扫描方法，以提高传统视频内容的显示质量。</li>
<li>methods: 该方法使用了深度学习技术，包括bidirectional空间时间信息传递、多尺度特征精度提升等，以提高去扫描效果。</li>
<li>results: 实验结果显示，该方法的性能比现有方法更高，可能在实时处理中得到应用。<details>
<summary>Abstract</summary>
Due to old CRT display technology and limited transmission bandwidth, early film and TV broadcasts commonly used interlaced scanning. This meant each field contained only half of the information. Since modern displays require full frames, this has spurred research into deinterlacing, i.e. restoring the missing information in legacy video content. In this paper, we present a deep-learning-based method for deinterlacing animated and live-action content. Our proposed method supports bidirectional spatio-temporal information propagation across multiple scales to leverage information in both space and time. More specifically, we design a Flow-guided Refinement Block (FRB) which performs feature refinement including alignment, fusion, and rectification. Additionally, our method can process multiple fields simultaneously, reducing per-frame processing time, and potentially enabling real-time processing. Our experimental results demonstrate that our proposed method achieves superior performance compared to existing methods.
</details>
<details>
<summary>摘要</summary>
More specifically, we design a Flow-guided Refinement Block (FRB) which performs feature refinement, including alignment, fusion, and rectification. Additionally, our method can process multiple fields simultaneously, reducing per-frame processing time and potentially enabling real-time processing. Our experimental results demonstrate that our proposed method achieves superior performance compared to existing methods.
</details></li>
</ul>
<hr>
<h2 id="Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification"><a href="#Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification" class="headerlink" title="Are Natural Domain Foundation Models Useful for Medical Image Classification?"></a>Are Natural Domain Foundation Models Useful for Medical Image Classification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19522">http://arxiv.org/abs/2310.19522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joana Palés Huix, Adithya Raju Ganeshan, Johan Fredin Haslum, Magnus Söderberg, Christos Matsoukas, Kevin Smith</li>
<li>for: 本研究旨在 investigate the transferability of various state-of-the-art foundation models to medical image classification tasks.</li>
<li>methods: 本研究使用了 five foundation models，namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP，并在 four well-established medical imaging datasets 进行了评估。不同的训练设置也被探讨，以便充分发挥这些模型的潜力。</li>
<li>results: DINOv2 在特定的训练设置下表现出色，并一直超过了标准的 ImageNet 预训练方法。然而，其他基础模型却无法一致地超过这个已知基准， indicating limitations in their transferability to medical image classification tasks.<details>
<summary>Abstract</summary>
The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 in particular, consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.
</details>
<details>
<summary>摘要</summary>
深度学习领域正在向通用基础模型的使用倾斜，以便轻松地适应多种任务。在自然语言处理领域，这种思维方式已经成为常见的实践，但在计算机视觉领域进步 slower。在这篇论文中，我们试图解决这个问题，通过调查不同状态的基础模型在医疗影像分类任务中的可转移性。我们评估了五种基础模型，namely SAM、SEEM、DINOv2、BLIP和OpenCLIP，在四个成熟的医疗影像数据集上的表现。我们探索了不同的训练设置，以充分利用这些模型的潜力。我们的研究显示了混合的结果。DINOv2在特别情况下一直表现出色，而其他基础模型则在医疗影像分类任务中的可转移性有限。
</details></li>
</ul>
<hr>
<h2 id="Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes"><a href="#Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes" class="headerlink" title="Generating Context-Aware Natural Answers for Questions in 3D Scenes"></a>Generating Context-Aware Natural Answers for Questions in 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19516">http://arxiv.org/abs/2310.19516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Munzer Dwedari, Matthias Niessner, Dave Zhenyu Chen</li>
<li>for: 这 paper 是为了解决 3D 视力语言领域中的问题回答问题。</li>
<li>methods: 该 paper 使用了将问题回答任务转化为序列生成任务，以生成自然的回答。具体来说，它们使用了语言奖励来优化模型，并采用了 Pragmatic 语言理解奖来进一步提高句子质量。</li>
<li>results: 该 paper 在 ScanQA 测试集上 achieved 新的 SOTA 成绩（CIDEr 分数为 72.22&#x2F;66.57）。<details>
<summary>Abstract</summary>
3D question answering is a young field in 3D vision-language that is yet to be explored. Previous methods are limited to a pre-defined answer space and cannot generate answers naturally. In this work, we pivot the question answering task to a sequence generation task to generate free-form natural answers for questions in 3D scenes (Gen3DQA). To this end, we optimize our model directly on the language rewards to secure the global sentence semantics. Here, we also adapt a pragmatic language understanding reward to further improve the sentence quality. Our method sets a new SOTA on the ScanQA benchmark (CIDEr score 72.22/66.57 on the test sets).
</details>
<details>
<summary>摘要</summary>
三维问答是一个年轻的领域，尚未得到充分探索。先前的方法受限于固定的答案空间，无法自然生成答案。在这种工作中，我们将问答任务转换为序列生成任务，以生成3D场景中自然的答案（Gen3DQA）。为此，我们直接优化我们的模型以获取语言奖励，以保证全句 semantics。此外，我们还适应了 Pragmatic language understanding奖励，以进一步提高句子质量。我们的方法在ScanQAbenchmark上达到了新的最高纪录（CIDEr分数72.22/66.57）。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather"><a href="#Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather" class="headerlink" title="Transformer-based nowcasting of radar composites from satellite images for severe weather"></a>Transformer-based nowcasting of radar composites from satellite images for severe weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19515">http://arxiv.org/abs/2310.19515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caglarkucuk/earthformer-satellite-to-radar">https://github.com/caglarkucuk/earthformer-satellite-to-radar</a></li>
<li>paper_authors: Çağlar Küçük, Apostolos Giannakos, Stefan Schneider, Alexander Jann</li>
<li>for: 这个研究旨在开发一个基于Transformer架构的nowcasting模型，用于处理由卫星数据驱动的陆基雷达图像序列。</li>
<li>methods: 这个模型使用了卫星数据，并使用Transformer架构进行学习，以预测不同天气现象下的雷达场。</li>
<li>results: 模型在不同天气现象下预测雷达场的能力具有高准确性，并且能够在短时间内预测降水现象。<details>
<summary>Abstract</summary>
Weather radar data are critical for nowcasting and an integral component of numerical weather prediction models. While weather radar data provide valuable information at high resolution, their ground-based nature limits their availability, which impedes large-scale applications. In contrast, meteorological satellites cover larger domains but with coarser resolution.   However, with the rapid advancements in data-driven methodologies and modern sensors aboard geostationary satellites, new opportunities are emerging to bridge the gap between ground- and space-based observations, ultimately leading to more skillful weather prediction with high accuracy.   Here, we present a Transformer-based model for nowcasting ground-based radar image sequences using satellite data up to two hours lead time. Trained on a dataset reflecting severe weather conditions, the model predicts radar fields occurring under different weather phenomena and shows robustness against rapidly growing/decaying fields and complex field structures.   Model interpretation reveals that the infrared channel centered at 10.3 $\mu m$ (C13) contains skillful information for all weather conditions, while lightning data have the highest relative feature importance in severe weather conditions, particularly in shorter lead times.   The model can support precipitation nowcasting across large domains without an explicit need for radar towers, enhance numerical weather prediction and hydrological models, and provide radar proxy for data-scarce regions. Moreover, the open-source framework facilitates progress towards operational data-driven nowcasting.
</details>
<details>
<summary>摘要</summary>
天气雷达数据是现场预报中非常重要的一种数据来源，它们提供高分辨率的信息，但是由于地面设备的限制，其可用性受到限制，这使得大规模应用变得困难。相比之下，气象卫星可以覆盖更大的区域，但是其分辨率相对较低。然而，随着数据驱动方法和现代探测器技术的快速发展，新的机会正在出现，可以bridge the gap between ground-和空中数据，从而实现更准确的天气预报。在这篇文章中，我们提出了一种基于Transformer模型的nowcasting模型，用于预测基于雷达图像序列的天气情况，使用卫星数据作为输入，可以在两个小时前的预测。这个模型在不同的天气情况下预测雷达场景，并且对于快速增长/衰减的场景和复杂的场景表现出了Robustness。模型解释表明，在10.3微米（C13）温差频谱中，infrared通道含有有用的信息，而且在严重天气情况下，闪电数据的相对特征重要性最高。这个模型可以支持大规模的降水预报，不需要显式的雷达塔，可以增强数值天气预报和水文模型，并且可以提供数据缺乏地区的雷达代理。此外，我们的开源框架可以促进操作数据驱动的nowcasting的进步。
</details></li>
</ul>
<hr>
<h2 id="VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation"><a href="#VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation" class="headerlink" title="VideoCrafter1: Open Diffusion Models for High-Quality Video Generation"></a>VideoCrafter1: Open Diffusion Models for High-Quality Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19512">http://arxiv.org/abs/2310.19512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/videocrafter">https://github.com/ailab-cvc/videocrafter</a></li>
<li>paper_authors: Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, Ying Shan</li>
<li>for: 这两种扩散模型是为了提供高质量的视频生成模型，以便研究人员和工程师可以进行更多的研究和应用。</li>
<li>methods: 这两种模型分别使用文本输入和图像输入来生成视频，其中T2V模型可以生成高质量、电影级的视频，而I2V模型可以保持给定图像的内容、结构和风格，并将其转换成视频形式。</li>
<li>results: 这两种模型都可以生成高质量的视频，并且T2V模型可以在质量上超过其他开源T2V模型。<details>
<summary>Abstract</summary>
Video generation has increasingly gained interest in both academia and industry. Although commercial tools can generate plausible videos, there is a limited number of open-source models available for researchers and engineers. In this work, we introduce two diffusion models for high-quality video generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V models synthesize a video based on a given text input, while I2V models incorporate an additional image input. Our proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style. This model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints. We believe that these open-source video generation models will contribute significantly to the technological advancements within the community.
</details>
<details>
<summary>摘要</summary>
视频生成在学术和industry中都受到越来越多的关注。虽然商业工具可以生成看起来很可信的视频，但是学术和工程师可以使用的开源模型却有限。在这个工作中，我们介绍了两种扩散模型，即文本到视频（T2V）和图像到视频（I2V）模型。T2V模型将给定的文本输入Synthesize一个视频，而I2V模型具有一个额外的图像输入。我们提出的T2V模型可以生成高质量、电影级的视频，解决了其他开源T2V模型的质量不够问题。I2V模型是一个首个开源基础模型，可以将给定的图像转换成视频clip，保持内容、结构和风格的一致性。我们认为这些开源视频生成模型将对社区技术进步产生重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Visual-Navigation-of-Underwater-Robots"><a href="#Deep-Learning-for-Visual-Navigation-of-Underwater-Robots" class="headerlink" title="Deep Learning for Visual Navigation of Underwater Robots"></a>Deep Learning for Visual Navigation of Underwater Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19495">http://arxiv.org/abs/2310.19495</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Sunbeam</li>
<li>for: 本研究旨在简报深度学习方法用于水下 робо得视觉导航。</li>
<li>methods: 本文涵盖了水下 робо得视觉导航中使用深度学习方法的视觉感知、可用的视频水下数据集、模仿学习和奖励学习方法等方面。</li>
<li>results: 本文对现有的深度学习方法用于水下导航进行了简要的概述，并将相关的研究分为imitazione学习或深度学习 paradigm下的训练方法。<details>
<summary>Abstract</summary>
This paper aims to briefly survey deep learning methods for visual navigation of underwater robotics. The scope of this paper includes the visual perception of underwater robotics with deep learning methods, the available visual underwater datasets, imitation learning, and reinforcement learning methods for navigation. Additionally, relevant works will be categorized under the imitation learning or deep learning paradigm for underwater robots for clarity of the training methodologies in the current landscape. Literature that uses deep learning algorithms to process non-visual data for underwater navigation will not be considered, except as contrasting examples.
</details>
<details>
<summary>摘要</summary>
这篇论文目的是简要报告深度学习方法用于水下机器人视觉导航。论文的范围包括水下机器人视觉处理深度学习方法、可用的水下视像数据集、模仿学习和奖励学习方法用于导航。此外，相关的工作会根据训练方法类别为水下机器人下的imitating learning或深度学习 парадиг进行分类。文献使用深度学习算法处理非视觉数据用于水下导航不会被考虑，除非作为对比例。
</details></li>
</ul>
<hr>
<h2 id="VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation"><a href="#VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation" class="headerlink" title="VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation"></a>VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19477">http://arxiv.org/abs/2310.19477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Wu, Zhiyan Du, Zhi Li, Feng-Lei Fan, Tieyong Zeng</li>
<li>for:  recovering clear images from blurry ones with an unknown blur kernel</li>
<li>methods:  using deep image prior (DIP) with variational deep image prior (VDIP) and total generalized variational (TGV) regularization, and solving the model using alternating direction method of multipliers (ADMM)</li>
<li>results:  surpassing various state-of-the-art models quantitatively and qualitatively in recovering image edges and details while reducing oil painting artifacts.Here’s the simplified Chinese text:</li>
<li>for:  recuperar imágenes claras a partir de imágenes borrosas con una cámara de desconocida blur</li>
<li>methods:  utilizar prior de imagen profunda (DIP) con prior de imagen variacional (VDIP) y regularización de generalización total (TGV), y resolver el modelo utilizando el método de multiplicadores en sentido alternante (ADMM)</li>
<li>results:  superar a diferentes modelos estado-de-la-arte cuantitativamente y qualitativamente en la recuperación de límites y detalles de imágenes mientras reducen los artefactos de pintura al óleo.<details>
<summary>Abstract</summary>
Recovering clear images from blurry ones with an unknown blur kernel is a challenging problem. Deep image prior (DIP) proposes to use the deep network as a regularizer for a single image rather than as a supervised model, which achieves encouraging results in the nonblind deblurring problem. However, since the relationship between images and the network architectures is unclear, it is hard to find a suitable architecture to provide sufficient constraints on the estimated blur kernels and clean images. Also, DIP uses the sparse maximum a posteriori (MAP), which is insufficient to enforce the selection of the recovery image. Recently, variational deep image prior (VDIP) was proposed to impose constraints on both blur kernels and recovery images and take the standard deviation of the image into account during the optimization process by the variational principle. However, we empirically find that VDIP struggles with processing image details and tends to generate suboptimal results when the blur kernel is large. Therefore, we combine total generalized variational (TGV) regularization with VDIP in this paper to overcome these shortcomings of VDIP. TGV is a flexible regularization that utilizes the characteristics of partial derivatives of varying orders to regularize images at different scales, reducing oil painting artifacts while maintaining sharp edges. The proposed VDIP-TGV effectively recovers image edges and details by supplementing extra gradient information through TGV. Additionally, this model is solved by the alternating direction method of multipliers (ADMM), which effectively combines traditional algorithms and deep learning methods. Experiments show that our proposed VDIP-TGV surpasses various state-of-the-art models quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
recuperar imagens claras de imagens borrosas com um kernel de blur desconhecido é um problema desafiador. A prioridade de imagem profunda (DIP) propõe usar a rede profunda como um regularizador para uma imagem individual em vez de um modelo de aprendizado supervisionado, o que alcança resultados encorajadores no problema de desblurring não cego. No entanto, desde que a relação entre as imagens e as arquiteturas de rede é incerta, é difícil encontrar uma arquitetura adequada para fornecer restrições suficientes sobre os kernel de blur e as imagens limpas. Além disso, DIP usa a máxima a posteriori sparse (MAP), o que é insuficiente para impor a seleção da imagem de recovery.Recentemente, o prior de imagem profunda variável (VDIP) foi proposto para impor restrições sobre os kernel de blur e as imagens de recovery e considerar a variância da imagem durante o processo de otimização pelo princípio variacional. No entanto, encontramos empreiticamente que VDIP tem dificuldade em processar detalhes de imagem e tende a gerar resultados subótimos quando o kernel de blur é grande. Portanto, combinamos a regularização total geral variável (TGV) com VDIP neste artigo para superar as deficiências de VDIP. TGV é uma regularização flexível que utiliza as características das derivações parciais de órders variáveis para regularizar as imagens em escalas diferentes, reduzindo artefatos de óleo pintura enquanto manteve as bordos afiados. A nossa propriedade VDIP-TGV efetivamente recupera as bordos e detalhes das imagens by suplementando informações de gradiente adicionais através de TGV. Além disso, este modelo é resolvido pelo método de direções alternadas de multiplicadores (ADMM), que eficazmente combina métodos tradicionais e de aprendizado profundo. Os resultados experimentais mostram que nossa propriedade VDIP-TGV ultrapassa modelos estado-da-arte quantitativamente e qualitativamente.
</details></li>
</ul>
<hr>
<h2 id="Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions"><a href="#Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions" class="headerlink" title="Generative Neural Fields by Mixtures of Neural Implicit Functions"></a>Generative Neural Fields by Mixtures of Neural Implicit Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19464">http://arxiv.org/abs/2310.19464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tackgeun/mNIF">https://github.com/tackgeun/mNIF</a></li>
<li>paper_authors: Tackgeun You, Mijeong Kim, Jungtaek Kim, Bohyung Han</li>
<li>for: 学习生成神经场的线性组合卷积网络表示</li>
<li>methods: 采用meta-学习或自动解码方法学习卷积网络的含义表示和其系数在隐藏空间</li>
<li>results: 提出一种可以增加生成神经场容量的方法，同时保持推理网络的小型化，并通过权重平均来减少推理时间和内存占用。此外，通过适应推理任务来采样latent mixture coefficient，使得最终模型能够生成未见数据。实验表明，该方法在多种图像、体量数据和NeRF场景中达到了竞争性的生成性能。<details>
<summary>Abstract</summary>
We propose a novel approach to learning the generative neural fields represented by linear combinations of implicit basis networks. Our algorithm learns basis networks in the form of implicit neural representations and their coefficients in a latent space by either conducting meta-learning or adopting auto-decoding paradigms. The proposed method easily enlarges the capacity of generative neural fields by increasing the number of basis networks while maintaining the size of a network for inference to be small through their weighted model averaging. Consequently, sampling instances using the model is efficient in terms of latency and memory footprint. Moreover, we customize denoising diffusion probabilistic model for a target task to sample latent mixture coefficients, which allows our final model to generate unseen data effectively. Experiments show that our approach achieves competitive generation performance on diverse benchmarks for images, voxel data, and NeRF scenes without sophisticated designs for specific modalities and domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来学习生成神经场，即利用线性组合的隐式基准网络表示。我们的算法在隐藏空间学习基准网络和其相对权重，可以通过meta-学习或自动解码方式进行。这种方法可以轻松扩大生成神经场的容量，只需增加基准网络的数量，而不需增加推理网络的大小。因此，使用该模型进行采样实例会具有较低的延迟和内存占用。此外，我们可以根据目标任务自定义销毁扩散概率模型，从而使我们的最终模型能够生成未看过的数据。实验表明，我们的方法在多个图像、VOXEL数据和NeRF场景上实现了竞争力强的生成性能，无需特殊的设计 для具体的Modalities和Domains。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers"><a href="#Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers" class="headerlink" title="Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers"></a>Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19447">http://arxiv.org/abs/2310.19447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsong Zhang, Lingfeng Gu, Yu-Kun Lai, Xueyang Wang, Kun Li</li>
<li>for: 本研究旨在提高大规模场景中的群体检测精度，特别是在多个人体重叠的情况下。</li>
<li>methods: 我们提出了一个端到端框架，即GroupTransformer，用于大规模场景中的群体检测。我们设计了一个干扰编码器，用于检测和抑制严重干扰人体的裁剪。同时，我们还提出了空间时间变换器，用于同时提取人体轨迹信息和 fusion inter-person特征在层次结构中。</li>
<li>results: 我们的方法在大规模场景和小规模场景上都实现了比州前方法更好的性能，具体来说，在大规模场景上，我们的方法可以提高精度和F1分数的表现，提高了10%以上。在小规模场景上，我们的方法仍然提高了F1分数的表现，提高了5%以上。<details>
<summary>Abstract</summary>
Group detection, especially for large-scale scenes, has many potential applications for public safety and smart cities. Existing methods fail to cope with frequent occlusions in large-scale scenes with multiple people, and are difficult to effectively utilize spatio-temporal information. In this paper, we propose an end-to-end framework,GroupTransformer, for group detection in large-scale scenes. To deal with the frequent occlusions caused by multiple people, we design an occlusion encoder to detect and suppress severely occluded person crops. To explore the potential spatio-temporal relationship, we propose spatio-temporal transformers to simultaneously extract trajectory information and fuse inter-person features in a hierarchical manner. Experimental results on both large-scale and small-scale scenes demonstrate that our method achieves better performance compared with state-of-the-art methods. On large-scale scenes, our method significantly boosts the performance in terms of precision and F1 score by more than 10%. On small-scale scenes, our method still improves the performance of F1 score by more than 5%. The project page with code can be found at http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans.
</details>
<details>
<summary>摘要</summary>
团体探测，特别是在大规模场景中，有很多应用前景，如公共安全和智能城市。现有方法在大规模场景中频繁受到多人 occlusion 的影响，而且很难有效地利用空间时间信息。在这篇论文中，我们提出了一个端到端框架，GroupTransformer，用于大规模场景中的团体探测。为了处理由多人引起的严重遮挡，我们设计了遮挡编码器来检测并抑制严重遮挡人裁剪。为了探索团体之间的空间时间关系，我们提出了空间时间变换器，以同时提取 trajectory 信息并在层次结构中融合人群特征。实验结果表明，我们的方法在大规模场景和小规模场景上都有更好的表现，与现有方法相比，在大规模场景上提高了精度和 F1 分数的表现，提高了10%以上；在小规模场景上提高了 F1 分数的表现，提高了5%以上。项目页面和代码可以在 http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans 找到。
</details></li>
</ul>
<hr>
<h2 id="One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation"><a href="#One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation" class="headerlink" title="One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation"></a>One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19444">http://arxiv.org/abs/2310.19444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao840/ofakd">https://github.com/hao840/ofakd</a></li>
<li>paper_authors: Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, Chang Xu<br>for:This paper focuses on the challenge of knowledge distillation (KD) between heterogeneous models, specifically teacher and student models with different architectures.methods:The proposed method, OFA-KD, utilizes centered kernel alignment (CKA) to compare learned features between the teacher and student models, and projects intermediate features into an aligned latent space such as the logits space to discard architecture-specific information. Additionally, an adaptive target enhancement scheme is introduced to prevent the student from being disturbed by irrelevant information.results:The proposed OFA-KD framework significantly improves the distillation performance between heterogeneous architectures, with notable performance improvements for the student models, achieving a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset.<details>
<summary>Abstract</summary>
Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）已经证明是一种非常有效的方法，可以通过教师模型和学生模型之间的培训策略来提高模型性能。然而，大多数现有的塑化方法都是基于教师和学生模型属于同一个模型家族的假设，特别是Hint-based方法。我们使用中心kernel对比（CKA）来比较教师和学生模型学习的特征之间的差异，我们发现了显著的特征差异。这种差异表明了前一些Hint-based方法在跨建制塑化中的效果不佳。为了解决跨建制塑化的挑战，我们提出了一个简单 yet有效的一对所有KD框架，称为OFA-KD。具体来说，我们将中间特征投影到一个Alignment的特征空间，例如logits空间，这里抛弃了建制特定的信息。此外，我们引入了一种适应的目标增强方案，以避免学生被无关信息所干扰。我们对不同建制的模型，包括CNN、Transformer和MLP，进行了广泛的实验，结果表明了我们的OFA-KD框架在跨建制塑化中的优越性。具体来说，当我们的OFA-KD框架与学生模型结合使用时，学生模型的性能得到了明显的提高，最大提高为CIFAR-100数据集上的8.0%和ImageNet-1K数据集上的0.7%。PyTorch代码和检查点可以在https://github.com/Hao840/OFAKD中找到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements"><a href="#Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements" class="headerlink" title="Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements"></a>Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19441">http://arxiv.org/abs/2310.19441</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. James Cotton, Colleen Peyton</li>
<li>for: 这个研究是为了发展一种可靠的三维运动分析方法，以帮助诊断和治疗儿童中的神经发展障碍和其他疾病。</li>
<li>methods: 这个研究使用了动态 Gaussian splatting 技术，将其应用于儿童身上的罕见 markerless motion capture 资料。这种方法利用 semantic segmentation 图像来专注在婴儿身上，以改善Scene的初始化。</li>
<li>results: 研究结果显示了这种方法的潜在，可以实现新的景象和婴儿运动的追踪。这些成果开辟了一条新的道路，以帮助对多种临床人口进行进一步的运动分析，特别是在婴儿的早期诊断中。<details>
<summary>Abstract</summary>
Easy access to precise 3D tracking of movement could benefit many aspects of rehabilitation. A challenge to achieving this goal is that while there are many datasets and pretrained algorithms for able-bodied adults, algorithms trained on these datasets often fail to generalize to clinical populations including people with disabilities, infants, and neonates. Reliable movement analysis of infants and neonates is important as spontaneous movement behavior is an important indicator of neurological function and neurodevelopmental disability, which can help guide early interventions. We explored the application of dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our approach leverages semantic segmentation masks to focus on the infant, significantly improving the initialization of the scene. Our results demonstrate the potential of this method in rendering novel views of scenes and tracking infant movements. This work paves the way for advanced movement analysis tools that can be applied to diverse clinical populations, with a particular emphasis on early detection in infants.
</details>
<details>
<summary>摘要</summary>
便捷的3D运动跟踪可以有助于很多方面的康复。一个挑战是，虽然有很多数据集和预训练算法可以用于健康成人，但这些算法经常无法泛化到临床人口，包括残疾人、婴儿和新生儿。准确的婴儿和新生儿运动分析非常重要，因为自发运动行为是脑功能和发育障碍的重要指标，可以帮助早期 intervene。我们探讨了使用动态Gaussian泵浦法处理缺失标记的运动捕捉数据。我们的方法利用 semantic segmentation 面积来关注婴儿，可以大幅提高场景的初始化。我们的结果表明这种方法有potential用于生成新视图和跟踪婴儿运动。这项工作为临床各种人口提供了先进的运动分析工具，尤其是早期检测。
</details></li>
</ul>
<hr>
<h2 id="GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning"><a href="#GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning" class="headerlink" title="GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning"></a>GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19418">http://arxiv.org/abs/2310.19418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cosmaadrian/gaitformer">https://github.com/cosmaadrian/gaitformer</a></li>
<li>paper_authors: Adrian Cosma, Emilian Radoi</li>
<li>for: 本研究旨在提出一种基于人体运动模式的人识别方法，同时还能够提取多种外观特征。</li>
<li>methods: 该方法使用了 dense-trajectory 技术自动从视频流中提取人体运动模式，并在多任务学习模式下预训练一个 transformer 模型（GaitFormer）。</li>
<li>results: 该方法可以在 CASIA-B 和 FVG 数据集上达到 92.5% 和 85.33% 的准确率，比类似方法提高 +14.2% 和 +9.67%。此外，该方法还能够准确地识别性别信息和多种外观特征。<details>
<summary>Abstract</summary>
Gait analysis is proven to be a reliable way to perform person identification without relying on subject cooperation. Walking is a biometric that does not significantly change in short periods of time and can be regarded as unique to each person. So far, the study of gait analysis focused mostly on identification and demographics estimation, without considering many of the pedestrian attributes that appearance-based methods rely on. In this work, alongside gait-based person identification, we explore pedestrian attribute identification solely from movement patterns. We propose DenseGait, the largest dataset for pretraining gait analysis systems containing 217K anonymized tracklets, annotated automatically with 42 appearance attributes. DenseGait is constructed by automatically processing video streams and offers the full array of gait covariates present in the real world. We make the dataset available to the research community. Additionally, we propose GaitFormer, a transformer-based model that after pretraining in a multi-task fashion on DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without utilizing any manually annotated data. This corresponds to a +14.2% and +9.67% accuracy increase compared to similar methods. Moreover, GaitFormer is able to accurately identify gender information and a multitude of appearance attributes utilizing only movement patterns. The code to reproduce the experiments is made publicly.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文。<</SYS>>研究人员通过径行分析确认了径行分析是可靠的人体特征标识方法，不需要参与者的合作。径行是一种不会在短时间内发生 significiant 变化的生物метри，可以作为每个人独特的标识。在这项研究中，我们不仅进行了人体标识，还 explore了通过运动模式来确定 pedestrian 的属性。我们提出了 DenseGait 数据集，包含 217 万个匿名的跟踪样本，自动获得了 42 个外观特征的标注。DenseGait 数据集通过自动处理视频流程建立，包含了实际世界中的所有径行 covariates。我们将数据集提供给研究人员。此外，我们提出了 GaitFormer 模型，通过在多任务方式进行预训练后，在 CASIA-B 和 FVG 上达到了 92.5% 和 85.33% 的准确率，不使用任何手动标注数据。这相当于与类似方法相比增加了 +14.2% 和 +9.67% 的准确率。此外，GaitFormer 能够通过运动模式来准确地确定 gender 信息和多种外观特征。我们将实验代码公开。
</details></li>
</ul>
<hr>
<h2 id="CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance"><a href="#CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance" class="headerlink" title="CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance"></a>CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19413">http://arxiv.org/abs/2310.19413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani</li>
<li>for: 本研究旨在提供个性化人识别模块，以便在拥挤的环境中Robot与合适的个体合作，并且能够适应视觉出入和干扰。</li>
<li>methods: 本研究使用了不断改进的视觉适应技术，以确保Robot与人类之间的无间隔合作。</li>
<li>results: 实验结果显示，对于各个视频，CARPE-ID可以准确地跟踪每个选择的目标，而s-o-t-a MOT方法则有4个跟踪错误。<details>
<summary>Abstract</summary>
In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency exists to assume that the robot shall cooperate with the closest individual or that the scene involves merely a singular human actor. However, in realistic scenarios, such as shop floor operations, such an assumption may not hold and personalized target recognition by the robot in crowded environments is required. To fulfil this requirement, in this work, we propose a person re-identification module based on continual visual adaptation techniques that ensure the robot's seamless cooperation with the appropriate individual even subject to varying visual appearances or partial or complete occlusions. We test the framework singularly using recorded videos in a laboratory environment and an HRI scenario, i.e., a person-following task by a mobile robot. The targets are asked to change their appearance during tracking and to disappear from the camera field of view to test the challenging cases of occlusion and outfit variations. We compare our framework with one of the state-of-the-art Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can accurately track each selected target throughout the experiments in all the cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of 4 tracking errors for each video.
</details>
<details>
<summary>摘要</summary>
今天的人机交互（HRI）场景中，一种普遍的偏好是假设机器人与最近的个体或场景中只有一个人actor进行交互。然而，在实际场景中，如生产工程中，这种假设可能不成立，机器人需要在拥挤的环境中进行个体识别。为了实现这一目标，在这个工作中，我们提出了基于不断视觉适应技术的人重识别模块，以确保机器人在变化的视觉表现和部分或完全遮挡的情况下仍能够无缝地合作与正确的个体。我们使用实验室环境中录制的视频进行单个测试，以及人机交互场景，即移动机器人进行跟踪任务。目标人员在跟踪中改变外表和隐藏于相机视野中进行测试具有挑战性的 occlusion 和衣服变化情况。我们与一种state-of-the-art多对目标跟踪（MOT）方法进行比较，结果显示，CARPE-ID 能够在所有情况下（除了两个限制情况）accurately track每个选择的目标，而 s-o-t-a MOT 的平均跟踪错误为每个视频4。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images"><a href="#Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images" class="headerlink" title="Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images"></a>Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19411">http://arxiv.org/abs/2310.19411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Yaqub, Feng Jinchao</li>
<li>For: The paper is written for early detection of breast cancer (BC) using mammography images.* Methods: The proposed method uses a deep learning approach that includes data collection from established benchmark sources, image segmentation employing an Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet (ACA-AMDN) model. The hyperparameters within the models are optimized using the Modified Mussel Length-based Eurasian Oystercatcher Optimization (MML-EOO) algorithm.* Results: The proposed BC detection framework attains superior precision rates in early disease detection, demonstrating its potential to enhance mammography-based screening methodologies.Here are the three key information points in Simplified Chinese text:* For: 这篇论文是为早期检测乳腺癌（BC）使用乳腺图像而写的。* Methods: 提议的方法使用深度学习方法，包括数据收集自确认标准源，使用具有扩展核心的Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet（ACA-ATRUNet）建筑，以及使用Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet（ACA-AMDN）模型进行BC标识。模型中的超参数使用改进的贝叶绒长度基于欧洲乌苏鸟优化算法进行优化。* Results: 提议的BC检测框架在早期疾病检测中具有更高的准确率，表明其可能性用于提高乳腺图像基本creening方法。<details>
<summary>Abstract</summary>
Breast cancer (BC) significantly contributes to cancer-related mortality in women, underscoring the criticality of early detection for optimal patient outcomes. A mammography is a key tool for identifying and diagnosing breast abnormalities; however, accurately distinguishing malignant mass lesions remains challenging. To address this issue, we propose a novel deep learning approach for BC screening utilizing mammography images. Our proposed model comprises three distinct stages: data collection from established benchmark sources, image segmentation employing an Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet (ACA-AMDN) model. The hyperparameters within the ACA-ATRUNet and ACA-AMDN models are optimised using the Modified Mussel Length-based Eurasian Oystercatcher Optimization (MML-EOO) algorithm. Performance evaluation, leveraging multiple metrics, is conducted, and a comparative analysis against conventional methods is presented. Our experimental findings reveal that the proposed BC detection framework attains superior precision rates in early disease detection, demonstrating its potential to enhance mammography-based screening methodologies.
</details>
<details>
<summary>摘要</summary>
乳癌（BC）对女性患有癌症的致死率具有重要性，因此早期发现是至关重要的。而现在，诊断乳腺疾病的主要工具是胸部X射线图像，但是准确地识别肿瘤困难。为解决这个问题，我们提出了一种基于深度学习的新型乳癌检测方法，使用胸部X射线图像。我们的提议的模型包括三个阶段：数据收集自确定的参照源，使用基于Atrous Convolution的Attentive和Adaptive Trans-Res-UNet（ACA-ATRUNet）架构进行图像分割，以及使用基于Atrous Convolution的Attentive和Adaptive Multi-scale DenseNet（ACA-AMDN）模型进行乳癌识别。模型中的hyperparameter被MML-EOO算法优化。我们进行了性能评估，并对传统方法进行比较分析。我们的实验结果表明，我们的乳癌检测框架可以在早期疾病检测中实现更高的精度率，这显示了它在胸部X射线图像基本的检测方法中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models"><a href="#Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models" class="headerlink" title="Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models"></a>Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19410">http://arxiv.org/abs/2310.19410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minxingzhang/miagm">https://github.com/minxingzhang/miagm</a></li>
<li>paper_authors: Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, Yang Zhang</li>
<li>for: 这篇论文的目的是测试生成模型的隐私漏洞，以及对它们的攻击性能。</li>
<li>methods: 本研究使用了一种通用的会员推论攻击（MIA），可以对多种生成模型进行攻击，包括生成对抗网络、自适应器、隐藏函数和传播模型。这种攻击只需要对目标生成器生成的分布进行访问，并且不需要shadow模型或白盒式存取。</li>
<li>results: 实验结果显示了所有的生成模型都受到了本研究的攻击，例如在DDPM、DDIM和FastDPM预测器上，攻击得分高于0.99。对VQGAN、LDM（文本参数生成）和LIIF进行攻击也得到了AUC高于0.90。因此，本研究强调了生成模型的隐私漏洞，并呼吁设计和发布生成模型时需要考虑这一点。<details>
<summary>Abstract</summary>
Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary non-member datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC $>0.99$ against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC $>0.90.$ As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese生成模型在各种视觉创作任务中表现出了革命性的成功，但在同时，它们也面临了泄露私人训练数据信息的风险。多种会员推断攻击（MIA）已经提出，以示生成模型的隐私漏洞，但这些攻击受到了重大限制，例如需要附加模型和白盒访问，并且忽略或只关注涉及到扩散模型的唯一特性，这限制了它们的普适性。相比之下，我们提出了首次总体会员推断攻击，可以 against 多种生成模型，如生成对抗网络、自适应网络、隐藏函数和扩散模型。我们只需要使用目标生成器生成的分布，并且不需要访问target generator的 Architecture或应用场景，因此可以视为target generator为黑盒。实验证明了所有生成模型都是易受我们攻击的。例如，我们的工作在 DDPM、DDIM 和 FastDPM 上对 CIFAR-10 和 CelebA 进行了攻击，并获得了 AUC > 0.99。对 VQGAN、LDM（ для文本 conditional generation）和 LIIF 进行了攻击，并获得了 AUC > 0.90。因此，我们呼吁我们的社区在设计和发布生成模型时注意隐私泄露风险。
</details></li>
</ul>
<hr>
<h2 id="Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks"><a href="#Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks" class="headerlink" title="Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks"></a>Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19405">http://arxiv.org/abs/2310.19405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzeen Munir, Shoaib Azam, Tomasz Kucner, Ville Kyrki, Moongu Jeon</li>
<li>for: 提高抗风险的自动驾驶系统准确检测环境中的对象。</li>
<li>methods:  combining radar和lidar数据，使用添加性注意力进行 integrate，并使用一种新的平行分支结构（PFS）来管理尺度变化。</li>
<li>results: 在Radiate数据集上使用COCO指标测试，与状态艺法比较，提高了1.89%和2.61%在有利和不利天气条件下。<details>
<summary>Abstract</summary>
Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due to the data's sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by $1.89\%$ and $2.61\%$ in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.
</details>
<details>
<summary>摘要</summary>
Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due to the data's sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by 1.89% and 2.61% in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.Here's the word-for-word translation of the text into Simplified Chinese:对各种感知系统来说，对象检测是核心组件，为ego车辆提供环境信息，确保安全路径规划。尽管摄像头和激光探测技术有所进步，但在不利天气条件下，它们的性能可能有限。相比之下，毫米波技术可以使radar在这些条件下 функциональ effectively。然而，仅仅通过radar来构建感知系统并不能完全捕捉环境，因为数据的稀疏性。为此，感知融合策略被引入。我们提议一种双支分支框架，用于融合radar和激光数据以获得提高的对象检测。主支分支专注提取radar特征，而辅助支分支提取激光特征。这些特征然后被combine用添加性注意。然后，混合特征被传递 через一种新型的并行分支结构（PFS）来管理尺度变化。一个区域提议头然后用于对象检测。我们使用Radiate数据集对我们提议的方法进行评估，使用COCO指标。结果显示，它在有利和不利天气条件下比 estado-of-the-art 方法提高 $1.89\%$ 和 $2.61\%$。这表明，毫米波-激光融合在恶势夹攻击天气条件下能够实现精准的对象检测和定位。
</details></li>
</ul>
<hr>
<h2 id="A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma"><a href="#A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma" class="headerlink" title="A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma"></a>A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19392">http://arxiv.org/abs/2310.19392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navodini Wijethilake, Steve Connor, Anna Oviedova, Rebecca Burger, Tom Vercauteren, Jonathan Shapey<br>for:This paper aims to automate and improve the process of clinical decision making for patients with Vestibular Schwannoma by using deep learning-based segmentation to extract relevant clinical features.methods:The authors use deep learning-based segmentation to extract measurements from T1 and T2 weighted MRI scans, and propose a novel algorithm to choose and extract the most appropriate maximum linear measurement from the segmented regions based on the size of the extrameatal portion of the tumor.results:The authors achieved Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, and 0.8222 +- 0.2108 and 0.9049 +- 0.0646 for T1 weighted MRI. The measurements extracted manually by an expert neuroradiologist showed a significant correlation with the automated measurements (p &lt; 0.0001).<details>
<summary>Abstract</summary>
Vestibular Schwannoma is a benign brain tumour that grows from one of the balance nerves. Patients may be treated by surgery, radiosurgery or with a conservative "wait-and-scan" strategy. Clinicians typically use manually extracted linear measurements to aid clinical decision making. This work aims to automate and improve this process by using deep learning based segmentation to extract relevant clinical features through computational algorithms. To the best of our knowledge, our study is the first to propose an automated approach to replicate local clinical guidelines. Our deep learning based segmentation provided Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, whereas 0.8222 +- 0.2108 and 0.9049 +- 0.0646 were obtained for T1 weighted MRI. We propose a novel algorithm to choose and extract the most appropriate maximum linear measurement from the segmented regions based on the size of the extrameatal portion of the tumour. Using this tool, clinicians will be provided with a visual guide and related metrics relating to tumour progression that will function as a clinical decision aid. In this study, we utilize 187 scans obtained from 50 patients referred to a tertiary specialist neurosurgical service in the United Kingdom. The measurements extracted manually by an expert neuroradiologist indicated a significant correlation with the automated measurements (p < 0.0001).
</details>
<details>
<summary>摘要</summary>
vestibular schwannoma 是一种无害的脑肿，来自平衡神经的增生。患者可能会通过手术、放射学手术或保守的 "等待和扫描" 策略进行治疗。临床医生通常使用手动提取的线性测量以帮助临床决策。这项工作想要自动化和改进这个过程，使用深度学习基于分割来提取相关的临床特征。据我们所知，我们的研究是第一个提出自动化方法来复制当地临床指南。我们使用深度学习基于分割提取的 dice 分数为 0.8124 ± 0.2343 和 0.8969 ± 0.0521  для T2 重度 MRI，而为 T1 重度 MRI 分别为 0.8222 ± 0.2108 和 0.9049 ± 0.0646。我们提出了一种新的算法，可以从分割区域中选择和提取最有用的最大线性测量，基于脑外部分的大小。使用这个工具，临床医生将获得一个可见导航和相关的各种指标，用于诊断和治疗。在本研究中，我们使用 187 个扫描数据，来自 50 名患者，被提交到英国特等专业神经外科服务。手动由专家神经 radiologist 提取的测量显示与自动测量存在显著相关性（p < 0.0001）。
</details></li>
</ul>
<hr>
<h2 id="TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition"><a href="#TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition" class="headerlink" title="TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition"></a>TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19380">http://arxiv.org/abs/2310.19380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmmmeng/transxnet">https://github.com/lmmmeng/transxnet</a></li>
<li>paper_authors: Meng Lou, Hong-Yu Zhou, Sibei Yang, Yizhou Yu</li>
<li>for: This paper aims to improve the performance of vision backbone networks by integrating convolution and self-attention mechanisms.</li>
<li>methods: The proposed Dual Dynamic Token Mixer (D-Mixer) aggregates global information and local details in an input-dependent way, using an efficient global attention module and input-dependent depthwise convolution.</li>
<li>results: The proposed TransXNet vision backbone network delivers compelling performance on ImageNet-1K image classification and other dense prediction tasks, outperforming other state-of-the-art networks while having lower computational costs.<details>
<summary>Abstract</summary>
Recent studies have integrated convolution into transformers to introduce inductive bias and improve generalization performance. However, the static nature of conventional convolution prevents it from dynamically adapting to input variations, resulting in a representation discrepancy between convolution and self-attention as self-attention calculates attention matrices dynamically. Furthermore, when stacking token mixers that consist of convolution and self-attention to form a deep network, the static nature of convolution hinders the fusion of features previously generated by self-attention into convolution kernels. These two limitations result in a sub-optimal representation capacity of the constructed networks. To find a solution, we propose a lightweight Dual Dynamic Token Mixer (D-Mixer) that aggregates global information and local details in an input-dependent way. D-Mixer works by applying an efficient global attention module and an input-dependent depthwise convolution separately on evenly split feature segments, endowing the network with strong inductive bias and an enlarged effective receptive field. We use D-Mixer as the basic building block to design TransXNet, a novel hybrid CNN-Transformer vision backbone network that delivers compelling performance. In the ImageNet-1K image classification task, TransXNet-T surpasses Swin-T by 0.3\% in top-1 accuracy while requiring less than half of the computational cost. Furthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability, achieving top-1 accuracy of 83.8\% and 84.6\% respectively, with reasonable computational costs. Additionally, our proposed network architecture demonstrates strong generalization capabilities in various dense prediction tasks, outperforming other state-of-the-art networks while having lower computational costs.
</details>
<details>
<summary>摘要</summary>
最近的研究已经结合卷积 INTO transformers 以引入导引偏好并提高通用性表现。然而，传统的卷积方法因为静态的特性而无法灵活地适应输入变化，导致卷积和自我注意的表现差异。此外，当堆叠token mixer  consisting of convolution and self-attention to form a deep network 时，静态的卷积方法会阻碍对先前由自我注意生成的特征进行融合。这两个限制导致建立的网络表现下限。为了解决这个问题，我们提出了一个轻量级的双动态token mixer (D-Mixer)，它可以在输入相依的方式下对全局信息和本地细节进行弹性聚合。D-Mixer 通过单簇全球注意模组和输入相依的深度卷积分别在批量分割后的特征段进行对应。这使得网络具有强大的导引偏好和扩大的有效访问范围。我们使用 D-Mixer 为基础建立 TransXNet，一个新的混合 CNN-Transformer 视觉后续网络，它在 ImageNet-1K 图像识别任务中，轻松过越 Swin-T 的顶部一个精度，并且需要较少的计算成本。此外，TransXNet-S 和 TransXNet-B 在不同的计算成本下表现出色，分别获得了83.8% 和 84.6% 的顶部精度。此外，我们提出的网络架构显示了强大的通用能力，在不同的紧密预测任务中表现出色，而且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="Color-Equivariant-Convolutional-Networks"><a href="#Color-Equivariant-Convolutional-Networks" class="headerlink" title="Color Equivariant Convolutional Networks"></a>Color Equivariant Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19368">http://arxiv.org/abs/2310.19368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/attila94/ceconv">https://github.com/attila94/ceconv</a></li>
<li>paper_authors: Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert</li>
<li>for:  addresses the issue of color-based domain shifts in Convolutional Neural Networks (CNNs)</li>
<li>methods:  proposes a novel deep learning building block called Color Equivariant Convolutions (CEConvs), which enables shape feature sharing across the color spectrum while retaining important color information</li>
<li>results:  demonstrates the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts.<details>
<summary>Abstract</summary>
Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs.
</details>
<details>
<summary>摘要</summary>
颜色是计算机视觉中的一个关键视觉提示符，它被卷积神经网络（CNN）广泛利用于物体识别。然而，如果数据中存在颜色变化的偏度，CNN会受到影响，而色彩不变则会消除所有颜色信息，导致识别力下降。在这篇论文中，我们提出了颜色共轭卷积（CEConvs），一种新的深度学习构建块，它可以在颜色谱中共享形状特征，同时保留重要的颜色信息。我们扩展了卷积神经网络中的对称性概念，从 геомétric 变换中扩展到 photométriques 变换，通过在 neural network 中共享参数来实现色彩映射。我们示出了 CEConvs 在不同任务上的下游性能和颜色变化的Robustness，以及可以轻松地整合到现有的架构中，如 ResNets。这种方法可以解决计算机视觉中的颜色域shift问题，并提供了一个可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection"><a href="#Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection" class="headerlink" title="Semi- and Weakly-Supervised Domain Generalization for Object Detection"></a>Semi- and Weakly-Supervised Domain Generalization for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19351">http://arxiv.org/abs/2310.19351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Furuta, Yoichi Sato</li>
<li>For: 这篇论文是为了解决域别训练和测试数据之间的域差问题，提出了两个新的问题设定：半监督域通用物类检测（SS-DGOD）和弱监督域通用物类检测（WS-DGOD）。* Methods: 这篇论文使用了学生-教师学习框架，其中一个学生网络是由另一个教师网络的pseudo标签output trains on unlabeled or weakly labeled data。* Results: 实验结果显示，这篇论文所提出的问题设定可以将物类检测器训练到同等或更好的性能，而不需要依赖目标域数据的训练。<details>
<summary>Abstract</summary>
Object detectors do not work well when domains largely differ between training and testing data. To solve this problem, domain generalization approaches, which require training data with ground-truth labels from multiple domains, have been proposed. However, it is time-consuming and labor-intensive to collect those data for object detection because not only class labels but also bounding boxes must be annotated. To overcome the problem of domain gap in object detection without requiring expensive annotations, we propose to consider two new problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. We show that object detectors can be effectively trained on the proposed settings with the same student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. The experimental results demonstrate that the object detectors trained on the proposed settings significantly outperform baseline detectors trained on one labeled domain data and perform comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, while ours do not use target domain data for training in contrast to UDA.
</details>
<details>
<summary>摘要</summary>
Object detectors 不工作好当域和测试数据之间差异较大。为解决这个问题，域合一扩展方法（Domain Generalization，DG），需要训练数据包含多个域的真实标签，有 been proposed. However, collecting such data for object detection is time-consuming and labor-intensive, as not only class labels but also bounding boxes must be annotated. To overcome the problem of domain gap in object detection without requiring expensive annotations, we propose two new problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). Unlike conventional DG, which requires labeled data from multiple domains, SS-DGOD and WS-DGOD only require labeled data from one domain and unlabeled or weakly-labeled data from multiple domains for training. We show that object detectors can be effectively trained on the proposed settings using a student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. Experimental results demonstrate that object detectors trained on the proposed settings significantly outperform baseline detectors trained on one labeled domain data and perform comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, without using target domain data for training.
</details></li>
</ul>
<hr>
<h2 id="Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer"><a href="#Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer" class="headerlink" title="Label-Only Model Inversion Attacks via Knowledge Transfer"></a>Label-Only Model Inversion Attacks via Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19342">http://arxiv.org/abs/2310.19342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>For: This paper is focused on addressing the privacy threat of model inversion (MI) attacks, specifically in the label-only setup where the adversary only has access to the model’s predicted labels.* Methods: The proposed approach, called LOKT, uses transfer learning from an opaque target model to surrogate models, and leverages generative modeling to effectively transfer knowledge from the target model. The approach involves creating a new model called Target model-assisted ACGAN (T-ACGAN) to facilitate knowledge transfer.* Results: The proposed method significantly outperforms existing state-of-the-art (SOTA) label-only MI attacks by more than 15% across all MI benchmarks, and compares favorably in terms of query budget. The study highlights the rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed.Here’s the simplified Chinese text for the three key points:* For: 这篇论文关注了机器学习模型反向攻击（MI）的隐私问题，特别是在标签只setup中， adversary只有模型预测的标签（硬标签）的情况下。* Methods: 提议的方法是基于目标模型的知识传递，使用转移学习来训练助手模型，并使用生成模型来有效地传递知识。* Results: 提议的方法在所有MI benchmark上比现有SOTA标签只MI攻击提高了更多于15%，并与查询预算相比具有优势。这种研究指出ML模型even when minimal information (i.e., hard labels) is exposed的隐私问题在增长。<details>
<summary>Abstract</summary>
In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information.   In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）模型倒逼（MI）攻击中，敌方利用对ML模型的访问来推断和重建私有训练数据。在白盒和黑盒设置中，敌方有访问完整的模型或模型的软输出。然而，有很少的研究在最复杂但实际上最重要的设置中：标签只MI攻击，敌方只有访问模型预测的标签（硬标签）而没有信任分数也没有其他模型信息。在这种情况下，我们提出了一种新的方法：LOKT。我们的思路是通过目标模型的知识传递到临时模型中。然后，我们可以通过这些临时模型使用高级白盒攻击。我们提出了基于生成模型的知识传递，并引入了一种新的模型：Target model-assisted ACGAN（T-ACGAN）。我们的方法将复杂的标签只MI转化为更易于处理的白盒设置。我们提供分析支持，表明我们的方法可以使用临时模型来实现有效的MI攻击。我们的实验表明，我们的方法在所有MI benchmark中比现有最佳状态的标签只MI攻击方法高效性超过15%。此外，我们的方法与查询预算相比也很有优势。我们的研究显示，即使只暴露硬标签，ML模型仍然面临着严重的隐私威胁。我们的研究也表明，随着ML模型的普及，隐私威胁的发展将变得更加严重。我们的代码、 demo、模型和重建数据可以在我们的项目页面上获得：https://ngoc-nguyen-0.github.io/lokt/Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="On-Measuring-Fairness-in-Generative-Models"><a href="#On-Measuring-Fairness-in-Generative-Models" class="headerlink" title="On Measuring Fairness in Generative Models"></a>On Measuring Fairness in Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19297">http://arxiv.org/abs/2310.19297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher T. H. Teo, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>for: 本研究旨在提供一种准确的公平度评估方法，以便评估公平的生成模型。</li>
<li>methods: 本研究提出了一种新的公平度评估框架，称为CLassifier Error-Aware Measurement（CLEAM），它使用统计模型来补做敏感特征分类器的不准确性，从而减少了评估公平度时的错误。</li>
<li>results: 本研究表明，使用CLEAM评估StyleGAN2模型的gender公平度时，错误率从4.98%降低到0.62%，而且这种改进只需要少量的额外开销。此外，本研究还发现了一些重要的文本生成模型和GANs中的偏见问题，这些问题需要进一步的研究和解决。<details>
<summary>Abstract</summary>
Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on fairness measurement, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., 4.98% $\rightarrow$ 0.62% for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. Code and more resources: https://sutd-visual-computing-group.github.io/CLEAM/.
</details>
<details>
<summary>摘要</summary>
最近，关于公平生成模型的兴趣增长。在这项工作中，我们进行了第一次深入研究公平度量测试，这是评估公平生成模型进步的关键组件。我们做出了三个贡献：首先，我们进行了一项研究，发现现有的公平度量测试框架具有较大的测量误差，即使使用高度准确的敏感特征（SA）分类器也是如此。这些发现质量上的公平改进的成果into question。第二，为解决这个问题，我们提议了一种新的公平度量测试框架：CLassifier Error-Aware Measurement（CLEAM）。CLEAM使用统计模型来考虑SA分类器的不准确性，从而减少测量误差。我们的提议的CLEAM可以减少测量误差，例如，StyleGAN2中的性别从4.98%降至0.62%。此外，CLEAM可以实现这一目标，而无需增加额外的负担。第三，我们使用CLEAM测量了文本到图像生成器和GANs中的公平性，发现这些模型存在较大的偏见，这引发了对其应用的担忧。代码和更多资源可以在以下链接中找到：https://sutd-visual-computing-group.github.io/CLEAM/.
</details></li>
</ul>
<hr>
<h2 id="FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound"><a href="#FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound" class="headerlink" title="FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound"></a>FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19293">http://arxiv.org/abs/2310.19293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyu Chen, Xin Yang, Yuhao Huang, Wenlong Shi, Yan Cao, Mingyuan Luo, Xindi Hu, Lei Zhue, Lequan Yu, Kejuan Yue, Yuanji Zhang, Yi Xiong, Dong Ni, Weijun Huang</li>
<li>For: 提供全面的胎儿信息，包括胎儿的各个解剖结构的连接。* Methods: 提出了一种新的3D胎儿姿态估计框架（叫做FetusMapV2），以解决胎儿US图像中的多种挑战，如图像质量不佳、限制GPU内存等。* Results: 对比其他强大竞争者，该方法在一大量的胎儿US数据集上表现出优异的表达效果。<details>
<summary>Abstract</summary>
Fetal pose estimation in 3D ultrasound (US) involves identifying a set of associated fetal anatomical landmarks. Its primary objective is to provide comprehensive information about the fetus through landmark connections, thus benefiting various critical applications, such as biometric measurements, plane localization, and fetal movement monitoring. However, accurately estimating the 3D fetal pose in US volume has several challenges, including poor image quality, limited GPU memory for tackling high dimensional data, symmetrical or ambiguous anatomical structures, and considerable variations in fetal poses. In this study, we propose a novel 3D fetal pose estimation framework (called FetusMapV2) to overcome the above challenges. Our contribution is three-fold. First, we propose a heuristic scheme that explores the complementary network structure-unconstrained and activation-unreserved GPU memory management approaches, which can enlarge the input image resolution for better results under limited GPU memory. Second, we design a novel Pair Loss to mitigate confusion caused by symmetrical and similar anatomical structures. It separates the hidden classification task from the landmark localization task and thus progressively eases model learning. Last, we propose a shape priors-based self-supervised learning by selecting the relatively stable landmarks to refine the pose online. Extensive experiments and diverse applications on a large-scale fetal US dataset including 1000 volumes with 22 landmarks per volume demonstrate that our method outperforms other strong competitors.
</details>
<details>
<summary>摘要</summary>
《胎儿pose估计在3D超声成像中存在许多挑战，包括图像质量不佳、限制性的GPU内存、相似的解剖结构和胎儿姿态的巨大变化。本研究提出了一种新的3D胎儿pose估计框架（称为FetusMapV2），以解决以上挑战。我们的贡献包括以下三个方面：一、我们提出了一种启发性的网络结构方法，包括不受限制的GPU内存管理方法和活动不受限制的网络结构方法，可以在有限的GPU内存条件下提高输入图像的分辨率，从而获得更好的结果。二、我们设计了一种新的对应损失函数，用于减少由相似的解剖结构和同一类型的损失函数所引起的混淆。该损失函数将隐藏的分类任务和准确的地标任务分离开来，从而逐渐促进模型的学习。三、我们提出了一种基于形态约束的自动学习方法，通过选择稳定的地标来进行在线的姿态约束。广泛的实验和多种应用在1000个3D胎儿超声数据集上，表明我们的方法在其他强大竞争对手之上表现出色。》
</details></li>
</ul>
<hr>
<h2 id="EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution"><a href="#EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution" class="headerlink" title="EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution"></a>EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19288">http://arxiv.org/abs/2310.19288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xy-boy/ediffsr">https://github.com/xy-boy/ediffsr</a></li>
<li>paper_authors: Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, Xianyu Jin, Liangpei Zhang</li>
<li>for: 这篇论文的目的是提出一种高效的遥感图像超分辨（SR）方法，以提高遥感图像的视觉质量。</li>
<li>methods: 该方法使用了Diffusion Probabilistic Model（DPM）和Efficient Activation Network（EANet）来实现高效的噪声预测和图像重建。DPM可以生成视觉愉悦的图像，而EANet可以提高噪声预测性能。此外，该方法还提出了一种Conditional Prior Enhancement Module（CPEM），可以帮助提取更多的有用信息，以提高SR的准确性。</li>
<li>results: 对四个遥感图像 dataset 进行了广泛的实验，结果表明，EDiffSR可以在模拟和真实遥感图像上，Restore visual-pleasant images， both quantitatively and qualitatively。<details>
<summary>Abstract</summary>
Recently, convolutional networks have achieved remarkable development in remote sensing image Super-Resoltuion (SR) by minimizing the regression objectives, e.g., MSE loss. However, despite achieving impressive performance, these methods often suffer from poor visual quality with over-smooth issues. Generative adversarial networks have the potential to infer intricate details, but they are easy to collapse, resulting in undesirable artifacts. To mitigate these issues, in this paper, we first introduce Diffusion Probabilistic Model (DPM) for efficient remote sensing image SR, dubbed EDiffSR. EDiffSR is easy to train and maintains the merits of DPM in generating perceptual-pleasant images. Specifically, different from previous works using heavy UNet for noise prediction, we develop an Efficient Activation Network (EANet) to achieve favorable noise prediction performance by simplified channel attention and simple gate operation, which dramatically reduces the computational budget. Moreover, to introduce more valuable prior knowledge into the proposed EDiffSR, a practical Conditional Prior Enhancement Module (CPEM) is developed to help extract an enriched condition. Unlike most DPM-based SR models that directly generate conditions by amplifying LR images, the proposed CPEM helps to retain more informative cues for accurate SR. Extensive experiments on four remote sensing datasets demonstrate that EDiffSR can restore visual-pleasant images on simulated and real-world remote sensing images, both quantitatively and qualitatively. The code of EDiffSR will be available at https://github.com/XY-boy/EDiffSR
</details>
<details>
<summary>摘要</summary>
最近，卷积网络在遥感图像超分辨 (SR) 领域取得了显著的发展，通过最小化差分目标函数，如 Mean Squared Error (MSE) 损失函数。然而，这些方法经常受到过度平滑问题的困扰，导致图像质量不佳。生成对抗网络可以推测细节，但它们容易塌陷，导致不良的artefacts。为了缓解这些问题，本文提出了Diffusion Probabilistic Model (DPM)  для高效的遥感图像 SR，称为 EDiffSR。EDiffSR 易于训练和维护，并保留 DPM 生成的感知性图像。与前一些使用重量积网络进行噪声预测的方法不同，我们开发了一个高效的活化网络 (EANet)，通过简单的通道注意力和简单的闸机操作，可以获得优秀的噪声预测性能。此外，为了将更多的有价值前景知识引入到提议的 EDiffSR 中，我们开发了一个实用的 Conditional Prior Enhancement Module (CPEM)，以帮助提取更丰富的condition。与大多数DPM-based SR模型直接将LR图像扩充为condition，不同的是，我们的 CPEM 可以帮助保留更多的信息 clue  для准确的 SR。我们在四个遥感数据集上进行了广泛的实验，显示EDiffSR可以在模拟和实际遥感图像上还原高质量的视觉 pleasant 图像， both quantitatively and qualitatively。代码将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition"><a href="#Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition" class="headerlink" title="Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition"></a>Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19258">http://arxiv.org/abs/2310.19258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Shi, Yanyuan Qiao, Qi Wu, Lingqiao Liu, Feras Dayoub</li>
<li>for: 这个研究是为了提高移动机器人中的实时物体检测，并且在无法控制的环境中进行适应。</li>
<li>methods: 这篇研究使用了线上无标本领域适应（O-SFDA），并且提出了一种新的方法来优化这种适应。这个方法会将最有价的无标本数据包含在线上训练过程中。</li>
<li>results: 实验结果显示，这种方法可以超越现有的州际顶尖O-SFDA技术，并且在实际应用中表现出色。<details>
<summary>Abstract</summary>
Effective object detection in mobile robots is challenged by deployment in diverse and unfamiliar environments. Online Source-Free Domain Adaptation (O-SFDA) offers real-time model adaptation using a stream of unlabeled data from a target domain. However, not all captured frames in mobile robotics contain information that is beneficial for adaptation, particularly when there is a strong domain shift. This paper introduces a novel approach to enhance O-SFDA for adaptive object detection in mobile robots via unsupervised data acquisition. Our methodology prioritizes the most informative unlabeled samples for inclusion in the online training process. Empirical evaluation on a real-world dataset reveals that our method outperforms existing state-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised data acquisition for improving adaptive object detection in mobile robots.
</details>
<details>
<summary>摘要</summary>
“智能物体检测在移动机器人中是面临挑战的，特别是在不熟悉的环境中部署。在线源自自适应（O-SFDA）可以在目标领域上进行实时模型适应，但不所有捕捉到的帧都含有有用的信息，特别是当域名shift强大时。这篇论文介绍了一种新的方法，通过不supervised数据收集来增强O-SFDA的适应性。我们的方法会优先选择目标领域中最有用的无标签样本进行在线训练。实验表明，我们的方法在实际 datasets 上表现出了与现有状态的先进技术相比的优势，这说明了不supervised数据收集的可行性以及适应性。”Note that Simplified Chinese is a common writing system used in mainland China, and it may differ from Traditional Chinese, which is used in other regions such as Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture"><a href="#A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture" class="headerlink" title="A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture"></a>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19257">http://arxiv.org/abs/2310.19257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Shen, Yunhan Zhao, Nahyun Kwon, Jeeeun Kim, Yanan Li, Shu Kong</li>
<li>for:  This paper aims to address the long-standing problem of instance detection (InsDet) in robotics and computer vision, which involves detecting object instances (predefined by visual examples) in a cluttered scene.</li>
<li>methods:  The paper introduces a new InsDet dataset and protocol, which includes a realistic setup for training and testing, as well as the release of a real-world database with multi-view capture of 100 object instances and high-resolution testing images.</li>
<li>results:  The paper extensively studies baseline methods for InsDet on the new dataset and finds that using an off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving &gt;10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).<details>
<summary>Abstract</summary>
Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).
</details>
<details>
<summary>摘要</summary>
Instance detection (InsDet) 是一个长期存在的问题在机器人和计算机视觉领域，旨在在拥挤的场景中检测预先定义的对象实例。尽管其实际意义很大，但它的发展受到了对象检测的遮挡，这是因为当前的 InsDet 数据集都很小。例如，流行的 InsDet 数据集 GMU (发布于 2016 年) 只有 23 个实例，相比 COCO (80 个类)，一个很好的对象检测数据集，发布于 2014 年。我们被激励提出一个新的 InsDet 数据集和协议。首先，我们定义了实际的 InsDet 设置：训练数据包括多视图实例捕捉，以及包含多种场景图像，以便通过贴上实例图像并自由地标注盒子来生成训练图像。第二，我们发布了一个真实世界数据库，包含 100 个对象实例的多视图捕捉，以及高分辨率 (6k x 8k) 测试图像。第三，我们进行了广泛的基线方法研究，分析了它们的性能，并提出了未来工作。有些奇异的是，使用开源的无类型分割模型 (Segment Anything Model, SAM) 和自然学习的特征表示 DINOv2 可以达到 >10 AP 更高的性能，比抽象的 InsDet 模型（例如 FasterRCNN 和 RetinaNet）的结束到练项训练的模型。
</details></li>
</ul>
<hr>
<h2 id="There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation"><a href="#There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation" class="headerlink" title="There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation"></a>There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19231">http://arxiv.org/abs/2310.19231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Schmitt, Seyed Ali Ahmadi, Yonghao Xu, Gulsen Taskin, Ujjwal Verma, Francescopaolo Sica, Ronny Hansch</li>
<li>for: 本文旨在强调EARTH OBSERVATION（EO）领域中机器学习 datasets的重要性，并将这些数据集作为AI应用的基础进行探讨。</li>
<li>methods: 本文根据历史发展和现有资源，对EARTH OBSERVATION领域的机器学习数据集进行了概述，并提出了未来发展的视角。</li>
<li>results: 本文希望通过强调EARTH OBSERVATION领域数据集的特点，了解这些数据集的特殊性，并认为这些特点是我们领域的核心技能之一。<details>
<summary>Abstract</summary>
Carefully curated and annotated datasets are the foundation of machine learning, with particularly data-hungry deep neural networks forming the core of what is often called Artificial Intelligence (AI). Due to the massive success of deep learning applied to Earth Observation (EO) problems, the focus of the community has been largely on the development of ever-more sophisticated deep neural network architectures and training strategies largely ignoring the overall importance of datasets. For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for Earth observation. With this article, we want to change the perspective and put machine learning datasets dedicated to Earth observation data and applications into the spotlight. Based on a review of the historical developments, currently available resources are described and a perspective for future developments is formed. We hope to contribute to an understanding that the nature of our data is what distinguishes the Earth observation community from many other communities that apply deep learning techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline.
</details>
<details>
<summary>摘要</summary>
《深入探讨Machine Learning数据集的重要性》Introduction: Machine learning（机器学习）是现代计算机科学中的一个重要分支，它的应用领域包括地球观测（Earth Observation，EO）等。在深度学习（deep learning）的应用中，特别是在地球观测领域，数据集的精心挑选和注释已成为机器学习的基础。然而，由于社区的焦点主要集中在深度学习模型的设计和训练策略上，数据集的重要性被忽略了。本文旨在改变这种观点，将地球观测领域的机器学习数据集推到前线，并对历史发展、当前资源和未来发展提出了一种视角。我们希望通过这篇文章，让读者理解地球观测领域的数据特点，并认识到我们的数据是与其他图像数据领域不同的。Historical Background: Earth observation（EO）是指通过卫星、飞机、气象雷达等设备对地球的观测和记录。EO数据的应用领域包括气候变化、自然灾害、资源管理、农业等。随着深度学习技术的发展，EO领域的应用也逐渐增多。在过去的几年中，许多专门的任务数据集被创建，但这些数据集却被许多关于AI的评论文章忽略。Current Resources: 目前，EO领域的机器学习数据集已经具有了很好的资源。以下是一些常用的数据集：1. 地球观测数据集（EO-1）：由中国国家地面观测中心提供，包括卫星图像、附近观测数据等。2. 地球观测数据集（EO-2）：由美国地球观测署提供，包括卫星图像、附近观测数据等。3. 气候变化数据集（Climate Change）：由世界 Meteorological Organization（WMO）提供，包括卫星图像、气象数据等。Future Perspective: 未来，EO领域的机器学习数据集将继续增长和改进。随着深度学习技术的发展，EO领域的应用将变得更加广泛和复杂。在这种情况下，数据集的质量和可靠性将成为非常重要的瓶颈。我们需要继续探讨和研究EO领域的数据特点，以提高机器学习模型的性能和可靠性。同时，我们还需要与其他领域的专家合作，以推动EO领域的发展。Conclusion: 在这篇文章中，我们提出了一种将地球观测领域的机器学习数据集推到前线的视角。我们希望通过这篇文章，让读者理解地球观测领域的数据特点，并认识到我们的数据是与其他图像数据领域不同的。未来，我们需要继续探讨和研究EO领域的数据特点，以提高机器学习模型的性能和可靠性。同时，我们还需要与其他领域的专家合作，以推动EO领域的发展。
</details></li>
</ul>
<hr>
<h2 id="CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging"><a href="#CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging" class="headerlink" title="CHAMMI: A benchmark for channel-adaptive models in microscopy imaging"></a>CHAMMI: A benchmark for channel-adaptive models in microscopy imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19224">http://arxiv.org/abs/2310.19224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/channel_adaptive_models">https://github.com/chaudatascience/channel_adaptive_models</a></li>
<li>paper_authors: Zitong Chen, Chau Pham, Siqi Wang, Michael Doron, Nikita Moshkov, Bryan A. Plummer, Juan C. Caicedo</li>
<li>for: This paper is written for researchers and developers working on microscopy imaging and neural networks. The paper aims to address the challenge of varying channel numbers in microscopy images and the lack of reusable models for different settings.</li>
<li>methods: The paper proposes a benchmark for investigating channel-adaptive models in microscopy imaging, which includes a dataset of varied-channel single-cell images and a biologically relevant evaluation framework. The authors also adapt several existing techniques to create channel-adaptive models and compare their performance to fixed-channel, baseline models.</li>
<li>results: The paper finds that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. The authors contribute a curated dataset and an evaluation API to facilitate objective comparisons in future research and applications.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为微scopic imaging和神经网络研究人员写的，目的是解决微scopic图像中通道数量不固定的问题，并提供可重用的模型 для不同的设置。</li>
<li>methods: 论文提出了一个用于研究可适应通道数量的微scopic imaging模型的 benchmark，包括一个包含不同通道数量单元细胞图像的数据集和一个生物学 relevance 的评估框架。作者还适应了一些现有的技术，创建了可适应通道数量的模型，并与基线模型进行比较。</li>
<li>results: 论文发现，可适应通道数量的模型可以更好地适应不同的任务和数据集，并且可以减少计算成本。作者提供了一个可搜索的数据集和评估 API，以便未来的研究和应用中进行对比。<details>
<summary>Abstract</summary>
Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications.
</details>
<details>
<summary>摘要</summary>
大多数神经网络假设输入图像具有固定数量的通道（三个 für RGB 图像）。然而，在微scopic 中有很多情况下，通道的数量可能会变化，例如微scopic 图像中的通道数量可能会根据设备和实验目标而变化。然而，到目前为止没有一个系统性的尝试创建和评估可以快速适应通道数量和类型的神经网络。因此，训练的模型通常只能在特定的研究中使用，而不能在其他微scopic 设置中 reuse。在这篇论文中，我们提供了一个用于调查可适应通道的模型在微scopic 成像中的 benchmark，包括：1）一个包含不同通道单元细胞图像的 dataset，和2）一个生物学上相关的评估框架。此外，我们还采用了一些现有的技术，以创建可适应通道的模型，并将其比较基线模型的性能。我们发现可适应通道模型可以更好地对应用于尖端任务，并且可以减少计算成本。我们提供了一个已经检索的 dataset（https://doi.org/10.5281/zenodo.7988357）和一个评估 API（https://github.com/broadinstitute/MorphEm.git），以便在未来的研究和应用中进行 объектив的比较。
</details></li>
</ul>
<hr>
<h2 id="Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images"><a href="#Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images" class="headerlink" title="Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images"></a>Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19223">http://arxiv.org/abs/2310.19223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong Li</li>
<li>for: 本研究旨在使用低成本的RGB图像检测抓取姿势，而传统方法则依赖深度感知器。</li>
<li>methods: 本研究提出了一种模块化学习网络，结合抓取检测和语义分割，针对具有平行板抓取器的机器人。网络不仅可以识别可抓取物体，还可以结合先前的抓取分析和语义分割，进一步提高抓取检测精度。</li>
<li>results: 实验和评估表明，我们提出的方法可以准确地检测抓取姿势，并且可以适应受扰和模糊的视觉。我们的设计具有可重复性和灵活性，能够应用于实际场景中。<details>
<summary>Abstract</summary>
While traditional methods relies on depth sensors, the current trend leans towards utilizing cost-effective RGB images, despite their absence of depth cues. This paper introduces an interesting approach to detect grasping pose from a single RGB image. To this end, we propose a modular learning network augmented with grasp detection and semantic segmentation, tailored for robots equipped with parallel-plate grippers. Our network not only identifies graspable objects but also fuses prior grasp analyses with semantic segmentation, thereby boosting grasp detection precision. Significantly, our design exhibits resilience, adeptly handling blurred and noisy visuals. Key contributions encompass a trainable network for grasp detection from RGB images, a modular design facilitating feasible grasp implementation, and an architecture robust against common image distortions. We demonstrate the feasibility and accuracy of our proposed approach through practical experiments and evaluations.
</details>
<details>
<summary>摘要</summary>
而传统方法倚靠深度感知器，当前趋势却是利用便宜的RGB图像，尽管它们缺乏深度提示。这篇论文介绍了一种有趣的方法，用于从单个RGB图像中检测抓取姿势。为此，我们提议一种具有抓取检测和语义分割的模块学习网络，适用于配备平行板抓取机器人。我们的网络不仅可以识别可抓取物体，还可以结合先前的抓取分析和语义分割，因此提高了抓取检测精度。此外，我们的设计还能够处理模糊和噪声的视觉，从而提高了系统的稳定性和可靠性。我们的关键贡献包括一种可学习的RGB图像抓取检测网络、一种可 Module 化的设计，以及一种对常见图像扭曲的抗衰减设计。我们通过实验和评估证明了我们提出的方法的可行性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Category-Discovery-with-Clustering-Assignment-Consistency"><a href="#Generalized-Category-Discovery-with-Clustering-Assignment-Consistency" class="headerlink" title="Generalized Category Discovery with Clustering Assignment Consistency"></a>Generalized Category Discovery with Clustering Assignment Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19210">http://arxiv.org/abs/2310.19210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangli Yang, Xinglin Pan, Irwin King, Zenglin Xu</li>
<li>for: 这篇论文的目的是提出一种基于openset任务的通用类发现方法，使得无标签样本集中的无标签样本可以通过从标签样本集中获取的信息进行自动分类。</li>
<li>methods: 该方法使用了协助学习和强化数据生成的方法，首先将样本经压缩变换后生成两个不同的视图，然后通过协同学习假设来学习一个具有一致性的表示学习策略，最后使用这些协同学习的准确性来构建一个原始稀有网络，并使用社区探测方法来获取分类结果和类数同时。</li>
<li>results: 实验表明，该方法在三个通用 benchmark 上达到了状态的前一个基eline，并在三个细化的视觉识别任务上达到了优秀的性能。尤其是在 ImageNet-100 数据集上，该方法在 \texttt{Novel} 和 \texttt{All} 类上超过了最佳基eline的15.5%和7.0%。<details>
<summary>Abstract</summary>
Generalized category discovery (GCD) is a recently proposed open-world task. Given a set of images consisting of labeled and unlabeled instances, the goal of GCD is to automatically cluster the unlabeled samples using information transferred from the labeled dataset. The unlabeled dataset comprises both known and novel classes. The main challenge is that unlabeled novel class samples and unlabeled known class samples are mixed together in the unlabeled dataset. To address the GCD without knowing the class number of unlabeled dataset, we propose a co-training-based framework that encourages clustering consistency. Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. Extensive experiments show that our method achieves state-of-the-art performance on three generic benchmarks and three fine-grained visual recognition datasets. Especially in the ImageNet-100 data set, our method significantly exceeds the best baseline by 15.5\% and 7.0\% on the \texttt{Novel} and \texttt{All} classes, respectively.
</details>
<details>
<summary>摘要</summary>
通用类发现（GCD）是一个最近提出的开放世界任务。给定一个包含标注和无标注实例的图像集合，GCD的目标是自动将无标注样本分组使用从标注集合中获取的信息。无标注集合包含已知和新类。主要挑战是无标注新类样本和已知类样本在无标注集合中混合在一起。为解决GCD而不知道无标注集合的类数，我们提出了基于协同学习的框架。 Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. 经验表明，我们的方法在三个通用权重折衔测试集上达到了状态之最好性能。特别是在ImageNet-100数据集上，我们的方法在\texttt{Novel}和\texttt{All}类上明显超过了最佳基准值，相差15.5%和7.0%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CV_2023_10_30/" data-id="clogy1z4h00ksffra6qzf00w5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/83/">83</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
