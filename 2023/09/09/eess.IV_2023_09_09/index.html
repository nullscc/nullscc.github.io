
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-09 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Latent Degradation Representation Constraint for Single Image Deraining paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04780 repo_url: None paper_authors: Yuhong He, Long Peng, Lu Wang, Jun Cheng for: 本研究旨在提出一种">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-09">
<meta property="og:url" content="https://nullscc.github.io/2023/09/09/eess.IV_2023_09_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Latent Degradation Representation Constraint for Single Image Deraining paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04780 repo_url: None paper_authors: Yuhong He, Long Peng, Lu Wang, Jun Cheng for: 本研究旨在提出一种">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-09T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:18.639Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/09/eess.IV_2023_09_09/" class="article-date">
  <time datetime="2023-09-09T09:00:00.000Z" itemprop="datePublished">2023-09-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-09
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Latent-Degradation-Representation-Constraint-for-Single-Image-Deraining"><a href="#Latent-Degradation-Representation-Constraint-for-Single-Image-Deraining" class="headerlink" title="Latent Degradation Representation Constraint for Single Image Deraining"></a>Latent Degradation Representation Constraint for Single Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04780">http://arxiv.org/abs/2309.04780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhong He, Long Peng, Lu Wang, Jun Cheng</li>
<li>for: 本研究旨在提出一种能够解决单张图像抹杂问题的新方法，即Latent Degradation Representation Constraint Network (LDRCNet)。</li>
<li>methods: 该方法包括Direction-Aware Encoder (DAEncoder)、UNet Deraining Network和Multi-Scale Interaction Block (MSIBlock)。DAEncoder使用可变权重核函数来适应抹杂方向的变化，从而提取有用的抹杂表示。然后，通过引入约束损失来显式地约束抹杂表示学习。MSIBlock则是用于与抹杂表示和解码器特征进行互动的多尺度交互块。</li>
<li>results: 实验结果表明，LDRCNet可以在 synthetic 和实际 dataset 上达到新的顶峰性能。<details>
<summary>Abstract</summary>
Since rain streaks show a variety of shapes and directions, learning the degradation representation is extremely challenging for single image deraining. Existing methods are mainly targeted at designing complicated modules to implicitly learn latent degradation representation from coupled rainy images. This way, it is hard to decouple the content-independent degradation representation due to the lack of explicit constraint, resulting in over- or under-enhancement problems. To tackle this issue, we propose a novel Latent Degradation Representation Constraint Network (LDRCNet) that consists of Direction-Aware Encoder (DAEncoder), UNet Deraining Network, and Multi-Scale Interaction Block (MSIBlock). Specifically, the DAEncoder is proposed to adaptively extract latent degradation representation by using the deformable convolutions to exploit the direction consistency of rain streaks. Next, a constraint loss is introduced to explicitly constraint the degradation representation learning during training. Last, we propose an MSIBlock to fuse with the learned degradation representation and decoder features of the deraining network for adaptive information interaction, which enables the deraining network to remove various complicated rainy patterns and reconstruct image details. Experimental results on synthetic and real datasets demonstrate that our method achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
因为雨束显示出多种形状和方向，单图像推干涂除非常困难。现有方法主要是通过设计复杂的模块来隐式地学习隐藏的降低表示。这样做导致缺乏明确约束，从而导致过度或者下降问题。为解决这个问题，我们提出了一种新的隐藏降低表示约束网络（LDRCNet），它包括指向意识Encoder（DAEncoder）、UNet推干网络和多Scale交互块（MSIBlock）。具体来说，DAEncoder使用可变核心扩展来适应雨束方向的一致性，以提取适应的降低表示。然后，我们引入了约束损失来Explicitly constrain the learning of degradation representation during training.最后，我们提出了MSIBlock来融合学习的降低表示和推干网络的解码特征，以实现适应性的信息互动，使推干网络可以去除各种复杂的雨束模式，并重建图像细节。实验结果表明，我们的方法在 sintetic和实际 dataset 上达到了新的州OF-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="SSHNN-Semi-Supervised-Hybrid-NAS-Network-for-Echocardiographic-Image-Segmentation"><a href="#SSHNN-Semi-Supervised-Hybrid-NAS-Network-for-Echocardiographic-Image-Segmentation" class="headerlink" title="SSHNN: Semi-Supervised Hybrid NAS Network for Echocardiographic Image Segmentation"></a>SSHNN: Semi-Supervised Hybrid NAS Network for Echocardiographic Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04672">http://arxiv.org/abs/2309.04672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renqi Chen, Jingjing Luo, Fan Nian, Yuhui Cen, Yiheng Peng, Zekuan Yu</li>
<li>for: 这个研究旨在提高医疗影像分类�で，特别是使用echocardiographic images的混乱讯号。</li>
<li>methods: 这个研究使用Neural Architecture Search（NAS）来设计网络，并将对�Feature fusion和Transformers进行创新的应用，以提高分类结果的精度。</li>
<li>results: 实验结果显示，这个 SSHNN 网络可以对echocardiographic images进行高精度的分类，并且超过了现有的方法。<details>
<summary>Abstract</summary>
Accurate medical image segmentation especially for echocardiographic images with unmissable noise requires elaborate network design. Compared with manual design, Neural Architecture Search (NAS) realizes better segmentation results due to larger search space and automatic optimization, but most of the existing methods are weak in layer-wise feature aggregation and adopt a ``strong encoder, weak decoder" structure, insufficient to handle global relationships and local details. To resolve these issues, we propose a novel semi-supervised hybrid NAS network for accurate medical image segmentation termed SSHNN. In SSHNN, we creatively use convolution operation in layer-wise feature fusion instead of normalized scalars to avoid losing details, making NAS a stronger encoder. Moreover, Transformers are introduced for the compensation of global context and U-shaped decoder is designed to efficiently connect global context with local features. Specifically, we implement a semi-supervised algorithm Mean-Teacher to overcome the limited volume problem of labeled medical image dataset. Extensive experiments on CAMUS echocardiography dataset demonstrate that SSHNN outperforms state-of-the-art approaches and realizes accurate segmentation. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
准确的医疗图像分割，特别是echocardiographic图像，需要考虑到不可避免的噪声。在现有的方法中，Neural Architecture Search（NAS）可以实现更好的分割结果，因为它可以搜索更大的搜索空间并自动优化，但大多数现有方法具有弱的层次特征聚合和“强encoder,弱decoder”结构，无法处理全局关系和本地特征。为解决这些问题，我们提出了一种新的半supervised hybrid NAS网络，称之为SSHNN。在SSHNN中，我们创新地在层次特征融合中使用 convolution 操作，而不是正常化的整数来避免丢失细节，使NAS变得更强。此外，我们还引入了Transformers来补偿全局上下文，并设计了U-shaped decoder来有效地连接全局上下文和本地特征。具体来说，我们实现了一种半supervised算法Mean-Teacher，以超越有限的医疗图像标注数据集的问题。我们在CAMUS echo cardiography数据集进行了广泛的实验，并证明了SSHNN可以超越现有的方法，实现准确的分割。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="ConvFormer-Plug-and-Play-CNN-Style-Transformers-for-Improving-Medical-Image-Segmentation"><a href="#ConvFormer-Plug-and-Play-CNN-Style-Transformers-for-Improving-Medical-Image-Segmentation" class="headerlink" title="ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation"></a>ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05674">http://arxiv.org/abs/2309.05674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xian Lin, Zengqiang Yan, Xianbo Deng, Chuansheng Zheng, Li Yu</li>
<li>for: 提高 transformer 的医疗图像分割性能，增强 attention 的均匀分布和多元特征提取。</li>
<li>methods: 建立 CNN-style Transformers（ConvFormer），包括pooling、CNN-style自注意（CSA）和卷积Feedforward Network（CFFN），使得 transformer 可以更好地利用医疗图像数据的多个特征。</li>
<li>results: 在多个 dataset 上实验表明，ConvFormer 可以作为 transformer 框架中的插件模块，提高 transformer 的医疗图像分割性能，并且可以轻松地替换现有的 transformer 框架。<details>
<summary>Abstract</summary>
Transformers have been extensively studied in medical image segmentation to build pairwise long-range dependence. Yet, relatively limited well-annotated medical image data makes transformers struggle to extract diverse global features, resulting in attention collapse where attention maps become similar or even identical. Comparatively, convolutional neural networks (CNNs) have better convergence properties on small-scale training data but suffer from limited receptive fields. Existing works are dedicated to exploring the combinations of CNN and transformers while ignoring attention collapse, leaving the potential of transformers under-explored. In this paper, we propose to build CNN-style Transformers (ConvFormer) to promote better attention convergence and thus better segmentation performance. Specifically, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) corresponding to tokenization, self-attention, and feed-forward network in vanilla vision transformers. In contrast to positional embedding and tokenization, ConvFormer adopts 2D convolution and max-pooling for both position information preservation and feature size reduction. In this way, CSA takes 2D feature maps as inputs and establishes long-range dependency by constructing self-attention matrices as convolution kernels with adaptive sizes. Following CSA, 2D convolution is utilized for feature refinement through CFFN. Experimental results on multiple datasets demonstrate the effectiveness of ConvFormer working as a plug-and-play module for consistent performance improvement of transformer-based frameworks. Code is available at https://github.com/xianlin7/ConvFormer.
</details>
<details>
<summary>摘要</summary>
transformers 在医学像素分割方面进行了广泛的研究，以建立对比较长距离的相互依赖关系。然而，有限的医学像素数据导致 transformers 很难提取多样化的全球特征，从而导致注意力归一化，注意力映射变得相互相同或甚至相同。相比之下，卷积神经网络（CNN）在小规模训练数据上具有更好的收敛性能，但它们具有有限的接受范围。现有的工作主要关注在 CNN 和 transformers 之间的组合，而忽略了注意力归一化问题，这导致 transformers 的潜在能力尚未得到充分发挥。本文提出了一种具有 CNN 特征的 transformers（ConvFormer），以提高注意力归一化并提高分割性能。具体来说，ConvFormer 包括pooling、CNN 样式自注意（CSA）和 convolutional feed-forward network（CFFN），对应于 tokenization、自注意和 feed-forward network 在 vanilla vision transformers 中。与positional embedding和tokenization不同，ConvFormer 采用了2D卷积和最大池化来保持位置信息和特征大小减少。这样，CSA 可以使用2D特征图作为输入，通过构建自注意矩阵作为卷积核来建立长距离相关性。接着，2D卷积用于特征细化通过 CFFN。实验结果在多个 dataset 上表明，ConvFormer 作为一个插件模块，可以为基于 transformer 框架的分割模型提供可靠的性能改进。代码可以在 <https://github.com/xianlin7/ConvFormer> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Video-and-Synthetic-MRI-Pre-training-of-3D-Vision-Architectures-for-Neuroimage-Analysis"><a href="#Video-and-Synthetic-MRI-Pre-training-of-3D-Vision-Architectures-for-Neuroimage-Analysis" class="headerlink" title="Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis"></a>Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04651">http://arxiv.org/abs/2309.04651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil J. Dhinagar, Amit Singh, Saket Ozarkar, Ketaki Buwa, Sophia I. Thomopoulos, Conor Owens-Walton, Emily Laltoo, Yao-Liang Chen, Philip Cook, Corey McMillan, Chih-Chien Tsai, J-J Wang, Yih-Ru Wu, Paul M. Thompson<br>for: 这个论文旨在评估不同的先进Dataset上预训练视觉模型的效果，以提高其适应三维医疗影像任务的能力。methods: 本研究使用了视觉变换器（ViTs）和卷积神经网络（CNNs）作为预训练模型，并对它们进行了不同的初始化方法。results: 研究发现，预训练可以提高所有任务的性能，包括提高了AD类型诊断的性能7.4%和PD类型诊断的性能4.6%，以及减少了大脑年龄预测错误的年龄1.26年。此外，使用大规模的视频或生成的MRI数据进行预训练可以提高ViTs的性能，而CNNs在有限数据情况下表现了稳定性，而且在预训练数据是同一个频谱频率的情况下，其性能会进一步提高。<details>
<summary>Abstract</summary>
Transfer learning represents a recent paradigm shift in the way we build artificial intelligence (AI) systems. In contrast to training task-specific models, transfer learning involves pre-training deep learning models on a large corpus of data and minimally fine-tuning them for adaptation to specific tasks. Even so, for 3D medical imaging tasks, we do not know if it is best to pre-train models on natural images, medical images, or even synthetically generated MRI scans or video data. To evaluate these alternatives, here we benchmarked vision transformers (ViTs) and convolutional neural networks (CNNs), initialized with varied upstream pre-training approaches. These methods were then adapted to three unique downstream neuroimaging tasks with a range of difficulty: Alzheimer's disease (AD) and Parkinson's disease (PD) classification, "brain age" prediction. Experimental tests led to the following key observations: 1. Pre-training improved performance across all tasks including a boost of 7.4% for AD classification and 4.6% for PD classification for the ViT and 19.1% for PD classification and reduction in brain age prediction error by 1.26 years for CNNs, 2. Pre-training on large-scale video or synthetic MRI data boosted performance of ViTs, 3. CNNs were robust in limited-data settings, and in-domain pretraining enhanced their performances, 4. Pre-training improved generalization to out-of-distribution datasets and sites. Overall, we benchmarked different vision architectures, revealing the value of pre-training them with emerging datasets for model initialization. The resulting pre-trained models can be adapted to a range of downstream neuroimaging tasks, even when training data for the target task is limited.
</details>
<details>
<summary>摘要</summary>
“转移学习”是人工智能系统建设的新方案，它与专门为特定任务训练深度学习模型不同，而是在大量数据集上预训练深度学习模型，并将其最小化调整为特定任务适应。然而，对于3D医学影像任务来说，我们不知道是否应该预训练模型在自然图像、医学图像或者 sintetically生成的MRI扫描或视频数据上。为了评估这些选择，我们在这里对vision transformers（ViTs）和卷积神经网络（CNNs）进行了比较。这些方法被 initialized 以不同的上游预训练方法，然后被适应到三个独特的下游神经成像任务，包括阿尔茨海默病（AD）和 паркинсони病（PD）分类、“脑年龄”预测。实验证明了以下关键观察结果：1. 预训练提高了所有任务的性能，包括对AD分类 task 的提高率为7.4%，对PD分类 task 的提高率为4.6%，对CNNs来说，PD分类任务的提高率为19.1%，并且预测脑年龄的错误率下降了1.26年。2. 预训练在大规模的视频或生成的MRI数据上提高了ViTs的性能。3. CNNs在有限数据设置下表现稳定，而在域内预训练下进一步提高了其表现。4. 预训练提高了模型对非标称数据集和站点的通用性。总之，我们对不同的视觉架构进行了比较，发现预训练这些模型使用emerging dataset可以提高其性能，这些预训练后的模型可以适应到一系列的下游神经成像任务，即使训练数据集为target任务的训练数据集有限。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/09/eess.IV_2023_09_09/" data-id="clmjn91qy00hr0j88ed1469fs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/09/cs.LG_2023_09_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-09
        
      </div>
    </a>
  
  
    <a href="/2023/09/08/cs.SD_2023_09_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-08</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
