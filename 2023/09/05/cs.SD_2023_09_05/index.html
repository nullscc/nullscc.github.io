
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Music Source Separation with Band-Split RoPE Transformer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02612 repo_url: None paper_authors: Wei-Tsung Lu, Ju-Chiang Wang, Qiuqiang Kong, Yun-Ning Hung for: 这个研究的目">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-05">
<meta property="og:url" content="https://nullscc.github.io/2023/09/05/cs.SD_2023_09_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Music Source Separation with Band-Split RoPE Transformer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02612 repo_url: None paper_authors: Wei-Tsung Lu, Ju-Chiang Wang, Qiuqiang Kong, Yun-Ning Hung for: 这个研究的目">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-05T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:16.515Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.SD_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T15:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Music-Source-Separation-with-Band-Split-RoPE-Transformer"><a href="#Music-Source-Separation-with-Band-Split-RoPE-Transformer" class="headerlink" title="Music Source Separation with Band-Split RoPE Transformer"></a>Music Source Separation with Band-Split RoPE Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02612">http://arxiv.org/abs/2309.02612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Tsung Lu, Ju-Chiang Wang, Qiuqiang Kong, Yun-Ning Hung</li>
<li>for: 这个研究的目的是提出一种频域方法来实现音乐源分离（MSS），以分离音乐录音为多个音乐元素，如 vocals、bass、鼓等。</li>
<li>methods: 该方法基于一种带划分模块，将输入复杂spectrogram projected into subband-level表示，然后用一个堆栈的层次Transformer来模型内部band和间隔band序列 для多个band掩码估计。</li>
<li>results: 在MUSDB18HQ和500首额外歌曲上训练BS-RoFormer系统后，在Sound Demixing Challenge（SDX23）的MSS轨道上 ranked the first place。使用一个较小的BS-RoFormer版本在MUSDB18HQ上进行训练，可以达到state-of-the-art result，无需额外的训练数据，平均SDR为9.80 dB。<details>
<summary>Abstract</summary>
Music source separation (MSS) aims to separate a music recording into multiple musically distinct stems, such as vocals, bass, drums, and more. Recently, deep learning approaches such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used, but the improvement is still limited. In this paper, we propose a novel frequency-domain approach based on a Band-Split RoPE Transformer (called BS-RoFormer). BS-RoFormer relies on a band-split module to project the input complex spectrogram into subband-level representations, and then arranges a stack of hierarchical Transformers to model the inner-band as well as inter-band sequences for multi-band mask estimation. To facilitate training the model for MSS, we propose to use the Rotary Position Embedding (RoPE). The BS-RoFormer system trained on MUSDB18HQ and 500 extra songs ranked the first place in the MSS track of Sound Demixing Challenge (SDX23). Benchmarking a smaller version of BS-RoFormer on MUSDB18HQ, we achieve state-of-the-art result without extra training data, with 9.80 dB of average SDR.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）目标是将音乐录音分解成多个音乐上的独立元素，如 vocals、bass、鼓等。近年来，深度学习方法如卷积神经网络（CNN）和循环神经网络（RNN）已经被应用，但是改进的空间还是有限。在这篇论文中，我们提出了一种新的频域方法，基于Band-Split RoPE Transformer（BS-RoFormer）。BS-RoFormer使用带分模块将输入复杂spectrogramProjected into subband-level表示，然后排列一个堆栈的层次Transformer来模型内部band以及交叉band的序列 для多个混合预测。为了训练MSS模型，我们提出了Rotary Position Embedding（RoPE）。BS-RoFormer系统在MUSDB18HQ和500首额外歌曲上训练后，在Sound Demixing Challenge（SDX23）中的MSS轨道上排名第一。对MUSDB18HQ进行训练BS-RoFormer系统的一个较小版本，我们实现了无需额外训练数据的状态级Result，即9.80 dB的平均SDR。
</details></li>
</ul>
<hr>
<h2 id="BWSNet-Automatic-Perceptual-Assessment-of-Audio-Signals"><a href="#BWSNet-Automatic-Perceptual-Assessment-of-Audio-Signals" class="headerlink" title="BWSNet: Automatic Perceptual Assessment of Audio Signals"></a>BWSNet: Automatic Perceptual Assessment of Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02592">http://arxiv.org/abs/2309.02592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Le Moine Veillon, Victor Rosi, Pablo Arias Sarah, Léane Salais, Nicolas Obin</li>
<li>for: 这篇论文是用于提出一种基于Best-Worst scaling（BWS）实验获取人类评价的模型。</li>
<li>methods: 这种模型使用了一组成本函数和约束，将声音样本映射到表示 изучаем特性的嵌入空间中。</li>
<li>results: 对两个BWS实验数据集进行测试后，结果表明模型的嵌入空间结构与人类评价具有一致性。<details>
<summary>Abstract</summary>
This paper introduces BWSNet, a model that can be trained from raw human judgements obtained through a Best-Worst scaling (BWS) experiment. It maps sound samples into an embedded space that represents the perception of a studied attribute. To this end, we propose a set of cost functions and constraints, interpreting trial-wise ordinal relations as distance comparisons in a metric learning task. We tested our proposal on data from two BWS studies investigating the perception of speech social attitudes and timbral qualities. For both datasets, our results show that the structure of the latent space is faithful to human judgements.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文介绍了BWSNet模型，该模型可以通过简单的人类评价获得的Best-Worst排序（BWS）实验来训练。它将声音样本映射到一个表示识别的特性的嵌入空间中。为此，我们提议了一组成本函数和约束，将试验性评价转化为度量学习任务中的距离比较。我们在两个BWS研究中测试了我们的建议，分别研究了社会态度和材质质量的声音识别。对于两个数据集，我们的结果显示，模型中的嵌入空间结构与人类评价具有一定的相似性。
</details></li>
</ul>
<hr>
<h2 id="Symbolic-Music-Representations-for-Classification-Tasks-A-Systematic-Evaluation"><a href="#Symbolic-Music-Representations-for-Classification-Tasks-A-Systematic-Evaluation" class="headerlink" title="Symbolic Music Representations for Classification Tasks: A Systematic Evaluation"></a>Symbolic Music Representations for Classification Tasks: A Systematic Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02567">http://arxiv.org/abs/2309.02567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Zhang, Emmanouil Karystinaios, Simon Dixon, Gerhard Widmer, Carlos Eduardo Cancino-Chacón</li>
<li>for: 本研究目的是对深度学习方法在音乐信息检索（MIR）领域的应用进行系统性的检查和评估。</li>
<li>methods: 本研究使用了矩阵表示（piano roll）、序列表示和图表示等不同的符号音乐表示方法，并与符号谱和表演进行比较。同时，我们还提出了一种新的图表示方法，用于表示符号性能。</li>
<li>results: 我们的系统性评估表明，图表示方法在全球分类任务中表现出色，并且训练时间较短。同时，矩阵表示方法在某些任务中表现较好，而序列表示方法则在其他任务中表现较差。<details>
<summary>Abstract</summary>
Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language like fashion. However, symbolic music is neither an image nor a sentence, and research in the symbolic domain lacks a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation. Our results suggest that the graph representation, as the newest and least explored among the three approaches, exhibits promising performance, while being more light-weight in training.
</details>
<details>
<summary>摘要</summary>
音乐信息检索（MIR）最近几年受到深度学习方法的普遍应用，这些方法常常将 симвоlic music（即用字符表示的音乐）转化为图像或语言的形式。然而，symbolic music并不是图像也不是句子，学术研究在这个域的不同表示方式缺乏全面的概述。本文 investigate matrix（钢琴 Roll）、序列和图表表示方法和其相应的神经网络架构，并与symbolic scores和表演中的三个 Piece-level 分类任务进行系统性的评估。我们还介绍了一种新的图表表示方法 для符号性表演，并对全球分类任务中的图表表示的可能性进行了探索。我们的系统性评估显示每种输入表示方法的优势和局限性。结果表明，图表表示方法，作为最新和最少研究的一种方法，具有潜在的表现优势，同时具有轻量级的训练需求。
</details></li>
</ul>
<hr>
<h2 id="Employing-Real-Training-Data-for-Deep-Noise-Suppression"><a href="#Employing-Real-Training-Data-for-Deep-Noise-Suppression" class="headerlink" title="Employing Real Training Data for Deep Noise Suppression"></a>Employing Real Training Data for Deep Noise Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02432">http://arxiv.org/abs/2309.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Xu, Marvin Sach, Jan Pirklbauer, Tim Fingscheidt</li>
<li>for: 提高深度噪音减除（DNS）模型的训练，使其能够更好地适应实际应用环境中的噪音。</li>
<li>methods: 使用实际训练数据，而不是传统的参照基于损失函数。在这种情况下，我们提出了一种基于非侵入式深度神经网络（DNN）的方法，named PESQ-DNN，来估算噪音下的语音质量评价（PESQ）分数。</li>
<li>results: 与参照方法相比，使用实际训练数据和PESQ-DNN的方法在DNS训练中表现出色，在synthetic test data上的PESQ分数提高0.32分，在实际测试数据上也超过了基线值0.05 DNSMOS分。<details>
<summary>Abstract</summary>
Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.
</details>
<details>
<summary>摘要</summary>
多个深度噪音消除（DNS）模型通常通过参考基于的损失来训练，但在实际应用中，加法麦克风模型可能不够。因此，使用实际训练数据来训练DNS模型可以减少训练/推理匹配问题。使用实际数据进行DNS训练需要使用生成方法或无参考损失。在这种工作中，我们提议使用一个端到端非侵入性的深度神经网络（DNN），名为PESQ-DNN，来估算受损数据的语音质量评分（PESQ）分数。PESQ-DNN提供了一种无参考的语音质量损失，可以使用实际数据进行DNS训练，最大化PESQ分数。此外，我们使用一种每个粒子 alternate 训练协议，首先更新DNS模型使用实际数据，然后PESQ-DNN使用生成数据进行更新。使用PESQ-DNN进行训练的DNS模型超越了所有参考方法，使用 толькоSynthetic 训练数据。在Synthetic 测试数据上，我们的提议方法与Interspeech 2021 DNS Challenge 基准值差异为0.32 PESQ分数。在实际测试数据上，我们的方法也超越了基准值，差异为0.05 DNSMOS分数。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Adaptation-with-Pre-trained-Speech-Encoders-for-Continuous-Emotion-Recognition"><a href="#Personalized-Adaptation-with-Pre-trained-Speech-Encoders-for-Continuous-Emotion-Recognition" class="headerlink" title="Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition"></a>Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02418">http://arxiv.org/abs/2309.02418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Tran, Yufeng Yin, Mohammad Soleymani</li>
<li>for: 实现无监督化人性化情感识别</li>
<li>methods: 使用自类监督学习方法，首先预训一个具有可读取 speaker embedding 的 encoder，然后提出一种无监督方法来补偿标签分布差异</li>
<li>results: 实验结果显示，我们的方法可以与强己基eline比较，并实现情感估计的州监督性能Here’s a brief summary of the paper in English:The paper aims to achieve unsupervised personalized emotion recognition. To do this, the authors first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Then, they propose an unsupervised method to compensate for label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Experimental results on the MSP-Podcast corpus show that their method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation.<details>
<summary>Abstract</summary>
There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation.
</details>
<details>
<summary>摘要</summary>
人们之间存在文化规范和个性的表达行为差异，这些差异可能导致情绪识别性能下降。因此，个性化是提高情绪识别系统通用性和鲁棒性的重要步骤。在这篇论文中，我们首先在不监督的情况下预训一个编码器，使其学习不同说话者的Robust语音表示形式。其次，我们提出了一种不监督的方法，通过找到类似的说话者并利用它们在训练集中的标签分布来补偿标签分布的变化。我们对MSP-Podcast集合进行了广泛的实验，结果表明，我们的方法可以一直超过强个性化基eline和实现情绪识别的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Generating-Realistic-Images-from-In-the-wild-Sounds"><a href="#Generating-Realistic-Images-from-In-the-wild-Sounds" class="headerlink" title="Generating Realistic Images from In-the-wild Sounds"></a>Generating Realistic Images from In-the-wild Sounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02405">http://arxiv.org/abs/2309.02405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds">https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds</a></li>
<li>paper_authors: Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</li>
<li>for: 这篇论文的目的是生成自然环境中的声音图像。</li>
<li>methods: 这篇论文使用了音频描述、音频注意力和句子注意力来表示声音的丰富特征，并使用了CLIP分数和AudioCLIP来直接优化声音。</li>
<li>results: 实验表明，这种方法可以生成高质量的声音图像，并在野外音频Dataset上超过基eline的 both quantitative和qualitative评估。Here’s the full translation in Traditional Chinese:</li>
<li>for: 这篇论文的目的是生成自然环境中的声音图像。</li>
<li>methods: 这篇论文使用了音频描述、音频注意力和句子注意力来表示声音的丰富特征，并使用了CLIP分数和AudioCLIP来直接优化声音。</li>
<li>results: 实验表明，这种方法可以生成高质量的声音图像，并在野外音频Dataset上超过基eline的 both quantitative和qualitative评估。<details>
<summary>Abstract</summary>
Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the significant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel approach to generate images from in-the-wild sounds. First, we convert sound into text using audio captioning. Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本为简化中文。</SYS>>寻求将野性声音转换为图像是一项重要但具有挑战性的任务，主要是因为声音和图像之间没有匹配数据集和这两种模式之间存在显著的不同。先前的研究主要集中在限定类别或音乐中生成图像。在这篇论文中，我们提出了一种将野性声音转换为图像的新方法。首先，我们使用音频描述将声音转换为文本。然后，我们提出了音频注意力和句子注意力来表达声音的丰富特征和视觉化声音。最后，我们提出了直接声音优化CLIPscore和AudioCLIP，并使用扩散模型生成图像。在实验中，我们发现我们的模型能够从野性声音中生成高质量图像，并在野性音频数据集上超过基eline在量和质量评价中。
</details></li>
</ul>
<hr>
<h2 id="Voice-Morphing-Two-Identities-in-One-Voice"><a href="#Voice-Morphing-Two-Identities-in-One-Voice" class="headerlink" title="Voice Morphing: Two Identities in One Voice"></a>Voice Morphing: Two Identities in One Voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02404">http://arxiv.org/abs/2309.02404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Sushanta K. Pani, Anurag Chowdhury, Morgan Sandler, Arun Ross</li>
<li>for: 本研究旨在探讨语音识别系统中的 morph 攻击，并提出了一种基于语音特征的 morph 攻击方法，即 Voice Identity Morphing (VIM)。</li>
<li>methods: 本研究使用了 ECAPA-TDNN 和 x-vector 两种常见的 speaker recognition 系统，并通过对 Librispeech 数据集进行实验来评估这两种系统对 VIM 攻击的抵抗力。</li>
<li>results: 实验结果显示，ECAPA-TDNN 和 x-vector 两种系统都具有较高的抵抗力，但是 VIM 攻击还是能够在这两种系统中获得较高的成功率（MMPMR 超过 80%），而且 false match rate 仅为 1%。<details>
<summary>Abstract</summary>
In a biometric system, each biometric sample or template is typically associated with a single identity. However, recent research has demonstrated the possibility of generating "morph" biometric samples that can successfully match more than a single identity. Morph attacks are now recognized as a potential security threat to biometric systems. However, most morph attacks have been studied on biometric modalities operating in the image domain, such as face, fingerprint, and iris. In this preliminary work, we introduce Voice Identity Morphing (VIM) - a voice-based morph attack that can synthesize speech samples that impersonate the voice characteristics of a pair of individuals. Our experiments evaluate the vulnerabilities of two popular speaker recognition systems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over 80% at a false match rate of 1% on the Librispeech dataset.
</details>
<details>
<summary>摘要</summary>
在生物特征识别系统中，每个生物样本或模板通常与单一身份相关联。然而，最近的研究表明可以生成“变形”生物样本，可以成功匹配多个身份。这种“变形攻击”现在被认为是生物特征识别系统的安全性威胁。然而，大多数变形攻击都在生物特征Modalities的图像领域进行研究，如脸、指纹和肉眼。在这项初步工作中，我们介绍了语音身份变换（VIM） - 一种基于语音的变形攻击，可以合成speech样本，模拟两个人的语音特征。我们的实验评估了两种流行的说话识别系统，ECAPA-TDNN和x-vector，对VIM的抵触性，在Librispeech数据集上达到了80%的成功率，false match率为1%。
</details></li>
</ul>
<hr>
<h2 id="The-Batik-plays-Mozart-Corpus-Linking-Performance-to-Score-to-Musicological-Annotations"><a href="#The-Batik-plays-Mozart-Corpus-Linking-Performance-to-Score-to-Musicological-Annotations" class="headerlink" title="The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations"></a>The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02399">http://arxiv.org/abs/2309.02399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patricia Hu, Gerhard Widmer</li>
<li>for: 这个论文旨在创建一个高质量、高精度的钢琴演奏数据集，结合专业钢琴演奏家罗兰多的莫扎特钢琴奏鸣录音和专家标注的Sheet Music。</li>
<li>methods: 这个论文使用了精度准确的注释对照方法，将演奏录音和Sheet Music进行了精度匹配，并将音乐学研究注释（和声、满声、段落）与演奏录音进行了连接。</li>
<li>results: 这个研究创建了一个高质量、高精度的钢琴演奏数据集，可以用于研究表达性演奏和其与结构特征之间的关系。两项探索性实验表明，这个数据集可以用于分析表达性演奏的多种方面。<details>
<summary>Abstract</summary>
We present the Batik-plays-Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\"osendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by Hentschel et al. (2021).   The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects. In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.
</details>
<details>
<summary>摘要</summary>
我们现在提出了《巴提克扮演莫扎特资料集》，这是一个结合专业莫扎特钢琴室内乐演奏和专家标注的谱面级演奏数据集。这些演奏来自维也纳钢琴家罗兰·巴提克在计算机监测的博Senderfer大钢琴上进行的录音，并可以作为MIDI文件和音频录音形式获得。它们已经精准地对应了当今标准版莫扎特谱面（新莫扎特版），以便进一步与musicology注释（和声、结束、段落）相连接。结果是一个高质量、高精度的谱面和音乐结构注释映射到专业演奏信息的高级资料集。这是第一个类型的资料集，可以作为研究表达性演奏和结构方面的多种方面的 valuable resource。在论文中，我们介绍了对Alignment的筛选过程和进行了两项探索性实验，以示其在分析表达性演奏方面的用于。
</details></li>
</ul>
<hr>
<h2 id="PESTO-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective"><a href="#PESTO-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective" class="headerlink" title="PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective"></a>PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02265">http://arxiv.org/abs/2309.02265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alain Riou, Stefan Lattner, Gaëtan Hadjeres, Geoffroy Peeters</li>
<li>for: 本研究使用自主学习（SSL）方法解决抗频率估计问题。</li>
<li>methods: 我们使用具有满足抗频率转换协变性的小型 ($&lt;$ 30k参数) SIAMESE神经网络，该网络使用两个不同的抗频率版本的同一个音频（通过Constant-Q Transform表示）作为输入。我们提议一种新的类型-基于的转换平衡对象来避免encoder-only设置中的模型塌缩。此外，我们设计了网络的架构，使其具有转换保持性，通过引入学习的Toeplitz矩阵。</li>
<li>results: 我们对唱歌声和乐器抗频率估计两个任务进行评估，并显示我们的模型能够在任务和数据集之间进行泛化，同时具有轻量级和实时应用compatibility。具体来说，我们的结果超过了自主学习基eline，并将自主学习和指导学习之间的性能差降到最小。<details>
<summary>Abstract</summary>
In this paper, we address the problem of pitch estimation using Self Supervised Learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset. We use a lightweight ($<$ 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its Constant-Q Transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.   We evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 Addresses the problem of pitch estimation using Self Supervised Learning (SSL). 我们使用的 SSL 模式是声调转换对应性，这使得我们的模型可以在听到的一小量无标签数据上训练后，准确地进行声调估计于单声音音频。我们使用一个轻量级 ($<$ 30k 参数) 的 Siamese 神经网络，它接受两个不同声调版本的同一个音频，用 Constant-Q Transform 表示。为了避免encoder-only设置中的模型崩溃，我们提出了一种新的类型-based transposition-equivariant 目标，捕捉声调信息。此外，我们设计了我们的网络架构，使其保持声调转换对应性，通过引入学习 Toeplitz 矩阵。  我们评估我们的模型在两个任务中： singing voice 和 musical instrument 声调估计，并示出我们的模型可以适应任务和数据集，同时具有轻量级和实时应用 compatibles。具体来说，我们的结果超过了自我监督基elines，并将自我监督和监督方法之间的性能差退到最小。
</details></li>
</ul>
<hr>
<h2 id="FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection"><a href="#FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection" class="headerlink" title="FSD: An Initial Chinese Dataset for Fake Song Detection"></a>FSD: An Initial Chinese Dataset for Fake Song Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02232">http://arxiv.org/abs/2309.02232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyuankun/fsd-dataset">https://github.com/xieyuankun/fsd-dataset</a></li>
<li>paper_authors: Yuankun Xie, Jingjing Zhou, Xiaolin Lu, Zhenghao Jiang, Yuxin Yang, Haonan Cheng, Long Ye</li>
<li>for: 本研究旨在探讨歌曲深刻护照技术的应用和挑战。</li>
<li>methods: 我们首先构建了一个中文假歌曲检测（FSD）数据集，以便研究歌曲深刻护照领域的特点和挑战。然后，我们使用FSD数据集训练Audio DeepFake Detection（ADD）模型，并对其进行了两种enario的评估：一种是使用原始歌曲，另一种是使用分离的声音轨迹。</li>
<li>results: 我们的实验结果表明，使用歌曲训练的ADD模型与使用语音训练的ADD模型相比，在FSD测试集上的平均相同错误率下降了38.58%。<details>
<summary>Abstract</summary>
Singing voice synthesis and singing voice conversion have significantly advanced, revolutionizing musical experiences. However, the rise of "Deepfake Songs" generated by these technologies raises concerns about authenticity. Unlike Audio DeepFake Detection (ADD), the field of song deepfake detection lacks specialized datasets or methods for song authenticity verification. In this paper, we initially construct a Chinese Fake Song Detection (FSD) dataset to investigate the field of song deepfake detection. The fake songs in the FSD dataset are generated by five state-of-the-art singing voice synthesis and singing voice conversion methods. Our initial experiments on FSD revealed the ineffectiveness of existing speech-trained ADD models for the task of song deepFake detection. Thus, we employ the FSD dataset for the training of ADD models. We subsequently evaluate these models under two scenarios: one with the original songs and another with separated vocal tracks. Experiment results show that song-trained ADD models exhibit a 38.58% reduction in average equal error rate compared to speech-trained ADD models on the FSD test set.
</details>
<details>
<summary>摘要</summary>
嗓音合成和嗓音转化技术已经有了很大的进步，对音乐经验产生了革命性的变革。然而，“深伪歌曲”（Deepfake Song）的出现使得真实性的问题引起了关注。与音频深伪检测（ADD）不同的是，歌曲真实性检测领域没有专门的数据集或方法进行真实性验证。在这篇论文中，我们首先构建了中文伪歌曲检测（FSD）数据集，以调查歌曲深伪检测领域的情况。 fake songs在FSD数据集中是由五种当前最佳的嗓音合成和嗓音转化方法生成的。我们对FSD数据集进行了初步的实验，发现现有的音频深伪检测模型对歌曲深伪检测task不具有效果。因此，我们使用FSD数据集来训练ADD模型。我们随后对这些模型进行了两种情况的评估：一种是使用原始的歌曲，另一种是使用分离的嗓音轨道。实验结果表明，使用歌曲训练的ADD模型在FSD测试集上的平均错误率比使用音频训练的ADD模型下降38.58%。
</details></li>
</ul>
<hr>
<h2 id="Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition"><a href="#Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition"></a>Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02145">http://arxiv.org/abs/2309.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Eickhoff, Matthias Möller, Theresa Pekarek Rosin, Johannes Twiefel, Stefan Wermter</li>
<li>for: 提高 Automatic Speech Recognition (ASR) 系统在听风讲语音中的表现，特别是在听风讲语音中受到干扰的情况下。</li>
<li>methods: 提出了一种新的方法，可以将 End-to-End (E2E) 系统中的干扰除能力提取出来，并将其用于任何 encoder-decoder 架构。这种方法包括提取 Conformer ASR 模型中的隐藏动作，并将其传递给一个解码器来预测干扰除后的spectrogram。</li>
<li>results: 研究表明，使用 Cleancoder 预处理器可以有效地除去听风讲语音中的干扰，并且可以提高 downstream 模型在听风讲语音中的总 Word Error Rate (WER)。此外，研究还表明，使用 Cleancoder 预处理器可以训练更小的 Conformer ASR 模型从 scratch。<details>
<summary>Abstract</summary>
In recent research, in the domain of speech processing, large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have reported state-of-the-art performance on various benchmarks. These systems intrinsically learn how to handle and remove noise conditions from speech. Previous research has shown, that it is possible to extract the denoising capabilities of these models into a preprocessor network, which can be used as a frontend for downstream ASR models. However, the proposed methods were limited to specific fully convolutional architectures. In this work, we propose a novel method to extract the denoising capabilities, that can be applied to any encoder-decoder architecture. We propose the Cleancoder preprocessor architecture that extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. We show that the Cleancoder is able to filter noise from speech and that it improves the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details>
<details>
<summary>摘要</summary>
In this study, we propose a novel method to extract the denoising capabilities that can be applied to any encoder-decoder architecture. We introduce the Cleancoder preprocessor architecture, which extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our preprocessor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs.We evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. Our results show that the Cleancoder is able to filter noise from speech and improve the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion"><a href="#Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion" class="headerlink" title="Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion"></a>Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02133">http://arxiv.org/abs/2309.02133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unilight/seq2seq-vc">https://github.com/unilight/seq2seq-vc</a></li>
<li>paper_authors: Wen-Chin Huang, Tomoki Toda</li>
<li>for: 本研究旨在评估三种 latest proposed methods for ground-truth-free foreign accent conversion (FAC), 以便控制发音和 speaker identity。</li>
<li>methods: 这三种方法均基于 sequence-to-sequence (seq2seq) 和 non-parallel voice conversion (VC) 模型，以提取发音和控制发音人物的特征。</li>
<li>results: 我们的实验评估结果显示，这三种方法在不同评估车辆中并没有一个显著的优势，与之前的研究结论相 contradistinction。 我们还对 seq2seq 模型的训练输入和输出进行了解释，并分析了非平行 VC 模型的设计选择，并发现 intelligibility 指标与主观的发音程度之间无法建立直接的相关性。<details>
<summary>Abstract</summary>
Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.
</details>
<details>
<summary>摘要</summary>
外语口音转换（FAC）是一种特殊的voice转换（VC）应用，旨在将非本地语言speaker的带有口音的语音转换成本地语言speaker的Native-sounding语音，同时保持speaker identity。FAC是具有挑战性，因为获得desired non-native speaker的Native speech为训练目标是不可能的。在这项工作中，我们评估了三种最近提出的ground-truth-free FAC方法，它们都是基于sequence-to-sequence（seq2seq）和非平行VC模型来实现口音转换和控制speaker identity。我们的实验评估结果表明，无一个方法在所有评估轴上显著更好，与之前的研究不同。我们还解释了这些方法的效iveness，包括seq2seq模型的训练输入和输出，以及非平行VC模型的设计选择，并证明了对于智能性指标 word error rates不相关于主观的口音度。最后，我们开源了我们的实现，以便促进可重复性的研究，并帮助未来的研究人员改进相关的系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.SD_2023_09_05/" data-id="clmjn91oc00bt0j88dnwfdk04" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/06/eess.IV_2023_09_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-06
        
      </div>
    </a>
  
  
    <a href="/2023/09/05/cs.LG_2023_09_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-05</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
