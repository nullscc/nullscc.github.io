
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Superclustering by finding statistically significant separable groups of optimal gaussian clusters paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02623 repo_url: https:&#x2F;&#x2F;github.com&#x2F;berng&#x2F;GMSDB paper_authors: Ol">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-05">
<meta property="og:url" content="https://nullscc.github.io/2023/09/05/cs.LG_2023_09_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Superclustering by finding statistically significant separable groups of optimal gaussian clusters paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02623 repo_url: https:&#x2F;&#x2F;github.com&#x2F;berng&#x2F;GMSDB paper_authors: Ol">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-05T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:16.511Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.LG_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T10:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters"><a href="#Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters" class="headerlink" title="Superclustering by finding statistically significant separable groups of optimal gaussian clusters"></a>Superclustering by finding statistically significant separable groups of optimal gaussian clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02623">http://arxiv.org/abs/2309.02623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berng/GMSDB">https://github.com/berng/GMSDB</a></li>
<li>paper_authors: Oleg I. Berngardt</li>
<li>for: 这个论文是为了提出一种基于BIC criterion和统计可分离性的超集群算法。</li>
<li>methods: 该算法包括三个阶段：首先，将数据集表示为一个mixture of Gaussian Distributions - 分布；其次，使用Mahalanobis distance和cluster size来估计分布之间的距离和分布大小；最后，使用DBSCAN方法将分布组合成超集群，并通过最大值引入矩阵质量指标来选择最佳超集群数量。</li>
<li>results: 该算法在测试数据集上达到了良好的结果，并且能够预测新数据中的正确超集群。然而，该算法具有低速度和随机性的缺点，需要较大的数据集进行分 clustering，这是许多统计方法的共同特点。<details>
<summary>Abstract</summary>
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.   The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.   The algorithm has only one hyperparameter - statistical significance level, and automatically detects optimal number and shape of superclusters based of statistical hypothesis testing approach. The algorithm demonstrates a good results on test datasets in noise and noiseless situations. An essential advantage of the algorithm is its ability to predict correct supercluster for new data based on already trained clusterer and perform soft (fuzzy) clustering. The disadvantages of the algorithm are: its low speed and stochastic nature of the final clustering. It requires a sufficiently large dataset for clustering, which is typical for many statistical methods.
</details>
<details>
<summary>摘要</summary>
文章提出了一种算法，用于将数据集分为优化的超集群。该算法包括三个阶段：1. 将数据集表示为一种mixture of Gaussian distributions - 集群，集群数量由BIC criterion的最小值确定；2. 使用Mahalanobis distance来估计集群之间的距离和集群大小；3. 使用DBSCAN方法将集群组合成超集群，并通过最大值引入的矩阵质量标准来确定最大值。该算法具有一个超参数 - 统计 significancLevel，可以自动找到最佳数量和形状的超集群，基于统计假设检测方法。它在测试数据集上显示了良好的效果，包括噪声和噪声free的情况。该算法的一个优点是它可以预测新数据中的正确超集群，并实现软（柔） clustering。However, the algorithm has some disadvantages, such as low speed and stochastic nature of the final clustering. It requires a large enough dataset for clustering, which is common for many statistical methods.Here's the translation in Traditional Chinese:文章提出了一个算法，用于将数据集分为优化的超集群。该算法包括三个阶段：1. 将数据集表示为一种mixture of Gaussian distributions - 集群，集群数量由BIC criterion的最小值确定；2. 使用Mahalanobis distance来估计集群之间的距离和集群大小；3. 使用DBSCAN方法将集群组合成超集群，并通过最大值引入的矩阵质量标准来确定最大值。该算法具有一个超参数 - 统计 significancLevel，可以自动找到最佳数量和形状的超集群，基于统计假设检测方法。它在测试数据集上显示了良好的效果，包括噪声和噪声free的情况。该算法的一个优点是它可以预测新数据中的正确超集群，并实现软（柔） clustering。然而，该算法有一些缺点，例如低速和统计性的最终分 clustering。它需要一个够大的数据集来类别，这是许多统计方法的常见需求。
</details></li>
</ul>
<hr>
<h2 id="Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning"><a href="#Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning" class="headerlink" title="Compressing Vision Transformers for Low-Resource Visual Learning"></a>Compressing Vision Transformers for Low-Resource Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02617">http://arxiv.org/abs/2309.02617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chensy7/efficient-vit">https://github.com/chensy7/efficient-vit</a></li>
<li>paper_authors: Eric Youn, Sai Mitheran J, Sanjana Prabhu, Siyuan Chen</li>
<li>For: The paper aims to bring vision transformers to resource-constrained devices like unmanned aerial vehicles (UAVs) for applications such as surveillance and environmental monitoring.* Methods: The paper utilizes popular model compression techniques like distillation, pruning, and quantization to reduce the size of the vision transformer model and improve its deployment on mobile and edge scenarios.* Results: The paper targets to achieve rapid inference of a vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss, enabling the deployment of ViTs on resource-constrained devices.Here’s the Chinese version of the three key points:* 为： paper 目标是将视transformer 部署到有限资源设备中，如无人飞机（UAV），用于应用程序如监视和环境监测等。* 方法： paper 利用流行的模型压缩技术，如精神投射、剪辑和量化，来减少视transformer 模型的大小，提高其在移动和边缘场景中的部署。* 结果： paper 目标是在 NVIDIA Jetson Nano（4GB）上实现视transformer 快速推理，保持最小的准确性损失，以实现resource-constrained devices 上的 ViT 部署。<details>
<summary>Abstract</summary>
Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.   Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires high accuracy close to that of state-of-the-art ViTs to ensure safe object avoidance in autonomous navigation, or correct localization of humans in search-and-rescue. Inference latency should also be minimized given the application requirements. Hence, our target is to enable rapid inference of a vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss. This allows us to deploy ViTs on resource-constrained devices, opening up new possibilities in surveillance, environmental monitoring, etc. Our implementation is made available at https://github.com/chensy7/efficient-vit.
</details>
<details>
<summary>摘要</summary>
《vision transformer（ViT）和其变种在视觉学领域中占据了领先地位，并提供了最佳精度在图像分类、物体检测和semantic segmentation任务中，通过不同部分的视觉输入注意和捕捉长距离 espacial dependent。然而，这些模型很大和计算沉重。例如，最近提出的ViT-B模型有86M参数，使其在资源有限设备上不可 praktical。因此，它们在移动和边缘场景中的部署受限。在我们的工作中，我们决心将视觉 transformer 带到边缘，通过使用流行的模型压缩技术 such as distillation、pruning和quantization。our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM。在这种应用环境中，UAV需要高精度，接近现状的state-of-the-art ViTs，以确保自主导航中的物体避免和人员Localization的正确。同时，推理延迟应该被最小化，因为应用要求。因此，我们的目标是在NVIDIA Jetson Nano (4GB)上快速推理一个vision transformer，并保持最小的精度损失。这使得我们能够在资源有限设备上部署ViTs，开 up new possibilities in surveillance, environmental monitoring, etc。我们的实现可以在https://github.com/chensy7/efficient-vit中找到。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts"><a href="#Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts" class="headerlink" title="Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts"></a>Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02616">http://arxiv.org/abs/2309.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim</li>
<li>for: 降低网络资源消耗，实现通信目标，而不需要 JOINT 训练 semantic encoder 和 decoder。</li>
<li>methods: 利用 Generative Artificial Intelligence (GAI) 模型，实现准确的内容解码。并 introducing covert communications aided by a friendly jammer，以确保成功的传输和安全通信。</li>
<li>results: 提出一种 GAI-aided SemCom 系统，可以准确地重建源消息，并提供了一种安全的传输方式。<details>
<summary>Abstract</summary>
Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decoding. Moreover, in response to security concerns, we introduce the application of covert communications aided by a friendly jammer. The system jointly optimizes the diffusion step, jamming, and transmitting power with the aid of the generative diffusion models, enabling successful and secure transmission of the source messages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts"><a href="#Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts" class="headerlink" title="Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts"></a>Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02615">http://arxiv.org/abs/2309.02615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryan Shaddy, Deep Ray, Angel Farguell, Valentina Calaza, Jan Mandel, James Haley, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Assad Oberai</li>
<li>for: 这项研究的目的是开发一种基于卫星测量数据的高级野火行为模型，以便更好地预测野火的 spreadof.</li>
<li>methods: 这项研究使用了一种named conditional Wasserstein Generative Adversarial Network (cWGAN)，通过训练WRF-SFIRE模型，来INFER野火爆发时间从卫星活动火灾数据中。</li>
<li>results: 研究测试了四起加利福尼亚州的野火，并与高分辨率空中红外测量数据进行比较。结果显示，cWGAN的预测结果具有高度准确性， average Sorensen’s coefficient为0.81， average ignition time error为32分钟。<details>
<summary>Abstract</summary>
Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. Samples produced by the cWGAN are further used to assess the uncertainty of predictions. The cWGAN is tested on four California wildfires occurring between 2020 and 2022, and predictions for fire extent are compared against high resolution airborne infrared measurements. Further, the predicted ignition times are compared with reported ignition times. An average Sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes suggest that the method is highly accurate.
</details>
<details>
<summary>摘要</summary>
due to the increasing frequency and impact of wildfires, there is a need for high-resolution models of wildfire behavior to predict the spread of fires. recent advances in satellite technology allow for the detection of fire locations, which can be used to improve fire spread forecasts through data assimilation. this study develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models in a physics-informed approach. the fire arrival time, which is the time it takes for the fire to reach a specific location, is used as a concise representation of the history of a wildfire. in this study, a conditional Wasserstein generative adversarial network (cWGAN) is used to infer the fire arrival time from satellite active fire data. the cWGAN is trained with WRF-SFIRE simulations and produces samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. the samples produced by the cWGAN are used to assess the uncertainty of predictions. the cWGAN is tested on four california wildfires that occurred between 2020 and 2022, and the predictions for fire extent are compared against high-resolution airborne infrared measurements. the predicted ignition times are also compared with reported ignition times. the results show an average sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes, indicating that the method is highly accurate.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds"><a href="#Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds" class="headerlink" title="Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds"></a>Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02614">http://arxiv.org/abs/2309.02614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds">https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds</a></li>
<li>paper_authors: Frederic Abraham, Matthew Stephenson</li>
<li>for: 这个论文是为了探讨使用生成对抗网络（GANs）来生成适应性的游戏逻辑游戏《愤怒的鸟》稳定结构的适用性。</li>
<li>methods: 这篇论文使用了GANs生成稳定结构，包括一个细致的编码&#x2F;解码过程来将游戏场景描述转换为适合网格表示的格子表示，以及使用当今最佳GAN架构和训练方法来生成新的结构设计。</li>
<li>results: 研究结果表明，GANs可以成功地应用于生成复杂且稳定的游戏结构。<details>
<summary>Abstract</summary>
This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data"><a href="#T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data" class="headerlink" title="T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data"></a>T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02610">http://arxiv.org/abs/2309.02610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijieying Ren, Tianxiang Zhao, Wei Qin, Kunpeng Liu</li>
<li>for: 本研究旨在解决流动数据中的突然分布shift问题，无需提前知道分布boundary。</li>
<li>methods: 提出了一种 Bayesian 框架（T-SaS），通过一个整数分布模型来捕捉流动数据中的突然shift。然后，通过动态网络选择条件来适应不同分布。</li>
<li>results: 对于流动数据中的分布shift问题，本研究的方法能够准确地检测分布boundary，并且在下游预测或分类任务中效果更高。<details>
<summary>Abstract</summary>
In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as T-SaS, with a discrete distribution-modeling variable to capture abrupt shifts of data. Then, we design a model that enable adaptation with dynamic network selection conditioned on that discrete variable. The proposed method learns specific model parameters for each distribution by learning which neurons should be activated in the full network. A dynamic masking strategy is adopted here to support inter-distribution transfer through the overlapping of a set of sparse networks. Extensive experiments show that our proposed method is superior in both accurately detecting shift boundaries to get segments of varying distributions and effectively adapting to downstream forecast or classification tasks.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，流动数据中的分布shift存在时间步骤之间。许多复杂的顺序数据可以有效地分解为不同的领域，这些领域具有持续的动态。了解流动数据中的shift和下沉的 Patterns是理解动态系统的关键。现有方法通常是在流动数据中训练一个Robust模型，以便在不同分布下进行适应。然而，存在两个挑战：（1）数据流中的shift可能会发生急剧和突然，没有预ursor;（2）训练所有领域的共享模型可能无法捕捉不同的模式。这篇论文的目标是解决流动数据中的顺序数据模型化问题，具体来说是在不同分布下快速适应。我们提出了一种 Bayesian框架，名为T-SaS，其中包含一个用于捕捉数据shift的离散分布模型。然后，我们设计了一种基于动态网络选择的模型，可以在不同分布下适应。我们的方法可以学习每个分布的特定参数，并通过在全网络中活化特定神经元来实现这一点。我们采用了一种动态遮盾策略，以支持间隔分布转移。我们的实验表明，我们的方法在准确地检测分布shift并在下游预测或分类任务中适应效果更高。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Variational-Inference-for-Online-Supervised-Learning"><a href="#Distributed-Variational-Inference-for-Online-Supervised-Learning" class="headerlink" title="Distributed Variational Inference for Online Supervised Learning"></a>Distributed Variational Inference for Online Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02606">http://arxiv.org/abs/2309.02606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pptx/distributed-mapping">https://github.com/pptx/distributed-mapping</a></li>
<li>paper_authors: Parth Paritosh, Nikolay Atanasov, Sonia Martinez</li>
<li>for: 这 paper 的目的是开发智能感知网络中的有效解决方案，以提供下一代位置、跟踪和地图服务。</li>
<li>methods: 该 paper 使用分布式概率推理算法，应用于连续变量、不可解 posterior 和大规模实时数据。在中央设置下，变量推理是基本技术，用于approximate Bayesian estimation，其中一个难以解决的 posterior density 被 aproximated 为参数化density。</li>
<li>results: 该 paper 提出了一种分布式逻辑低幂（DELBO），用于在感知网络中进行分布式变量推理。DELBO 是一个 weighted sum of observation likelihood 和偏好函数 divergence，其中 gap 是 due to consensus 和模型误差。该 algorithm 可以解决 binary classification 和回归问题，并处理流动数据。在 Gaussian variational densities 中，我们设计了一种在线分布式算法，用于最大化 DELBO，并对非线性 likelihood 进行特化。最后，我们 derivated 一个 diagonally 的版本，用于在高维模型中进行在线分布式推理。这 paper 的结果表明，DGVI 可以高效地解决 $1$-rank correction 问题，并在多机器人概率地图中应用。<details>
<summary>Abstract</summary>
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary classification and regression problems while handling streaming data, we design an online distributed algorithm that maximizes DELBO, and specialize it to Gaussian variational densities with non-linear likelihoods. The resulting distributed Gaussian variational inference (DGVI) efficiently inverts a $1$-rank correction to the covariance matrix. Finally, we derive a diagonalized version for online distributed inference in high-dimensional models, and apply it to multi-robot probabilistic mapping using indoor LiDAR data.
</details>
<details>
<summary>摘要</summary>
开发高效的推理解决方案对于智能感知网络中的位置、跟踪和地图服务是关键。本文提出了一种可扩展的分布式概率推理算法，可应用于连续变量、不可解 posterior 和大规模实时数据。在中央设置下，变量推理是概率推理的基本技术，用于进行approximate Bayesian estimation，其中一个难以求解 posterior density 被approximated 为 Parametric density。我们的关键贡献在于 derive 一个可分离的下界于中央估计目标，这使得分布式变量推理可以在感知网络中使用一 hop 通信。我们称之为分布式证据下界（DELBO），它包括观察可能性和偏好分布之间的差异，这种差异是由consensus和模型误差引起的。为处理流动数据，我们设计了一种在线分布式算法，该算法可以最大化 DELBO，并特化为 Gaussian variational densities  WITH non-linear likelihoods。这些非线性 likelihoods 可以处理流动数据。最后，我们 derivate 一个 diagonale 版本，用于在高维模型中进行在线分布式推理。我们应用了这种方法于多机器人概率地图使用indoor LiDAR数据。
</details></li>
</ul>
<hr>
<h2 id="Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet"><a href="#Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet" class="headerlink" title="Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet"></a>Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02604">http://arxiv.org/abs/2309.02604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Z. Lu</li>
<li>For: 提高急诊室效率和健康质量， Addressing the issue of overloaded traditional clinical workflows and inaccurate diagnoses in emergency departments.* Methods: 使用机器学习模型自动化排查和诊断确认，TriNet模型在医疗排查数据上培育，可以高度准确地检测患有肺炎和恶性肾炎等常见疾病。* Results:  TriNet模型的正确预测率高于当前临床标准，表明机器学习医疗指导可以提供高特异性的免费非侵入式检测，提高急诊室效率而不增加风险。<details>
<summary>Abstract</summary>
Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high specificity for common conditions, reducing the risk of over-testing while increasing emergency department efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为人口征性和寿命的不断增长，北美洲的急诊室访问量在增长。随着更多的患者来到急诊室，传统的临床工作流程变得过载和不效率，导致排队时间增长和健康保健质量下降。其中一个工作流程是护理医疗指南，受到人工负荷、不准确诊断和侵入性过测的限制。为解决这个问题，我们提出了TriNet：一种基于机器学习的医疗指南，用于在急诊室triage阶段自动进行第一线检测，以确认需要下游测试的疾病。为验证检测潜力，TriNet在医院triage数据上进行训练，在肺炎（0.86）和摄肾炎（0.93）方面达到了高正确预测值。这些模型超越了现有的临床标准，表明机器学习医疗指南可以免费、不侵入性地进行检测，提高急诊室效率，降低风险的过测。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks"><a href="#Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks" class="headerlink" title="Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks"></a>Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02596">http://arxiv.org/abs/2309.02596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Brian Li, Jesse Hoey, Alexander Wong</li>
<li>for: 这个研究旨在检查自我超参数化预训练是否可以生成适用于多个分类任务的脑网络特征提取器，以及这种预训练方法在肺超声分类任务中的性能。</li>
<li>methods: 这个研究使用了自我超参数化预训练方法，并在三个肺超声分类任务上进行了细化。</li>
<li>results: 研究结果表明，使用自我超参数化预训练方法可以提高肺超声分类任务的平均横距下 Receiver Operating Characteristic  Curve（AUC）的值，并且可以降低推理时间。<details>
<summary>Abstract</summary>
In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound classifiers.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了自我超vision学习是否可以生成适用于多个分类任务的神经网络特征提取器。当 fine-tuning 三个肺超声任务时，预训练模型导致了平均 across-task 接收操作曲线下的面积下降值（AUC）的提高，分别为 0.032 和 0.061 在本地和外部测试集上。 compact nonlinear 分类器在一个预训练模型输出的特征上训练后没有提高所有任务的性能，但它们可以将推理时间减少 49%，比对 separte fine-tuned 模型的串行执行更快。当使用 1% 可用标签进行训练时，预训练模型一直 OUTperform 完全supervised 模型，最大观察到的测试 AUC 提高为 0.396  для视图分类任务。总的来说，结果表明自我超vision学习是生成肺超声分类器初始 веса的有用方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning"><a href="#Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning" class="headerlink" title="Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"></a>Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02591">http://arxiv.org/abs/2309.02591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/CM3Leon">https://github.com/kyegomez/CM3Leon</a></li>
<li>paper_authors: Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan</li>
<li>for: 这个论文的目的是提出一种基于Token-based decoder-only多模态语言模型，可以生成和填充文本和图像。</li>
<li>methods: 这个模型使用了CM3多模态架构，并在更多的指令样本上进行了扩大和调参。它还包括一个大规模的检索补做预训练阶段，以及一个第二个多任务监督练练（SFT）阶段。</li>
<li>results: 这个模型可以实现高质量的文本到图像生成和图像到文本生成，并且可以在语言导向图像修改和图像控制生成等任务中示出无 precedent的可控性。在相应的实验中，CM3Leon在文本到图像生成任务中实现了比较方法的5倍减少的训练计算量（零shot MS-COCO FID为4.88）。<details>
<summary>Abstract</summary>
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.
</details>
<details>
<summary>摘要</summary>
我们介绍CM3Leon（发音为“毫毫”），一种具有生成和填充功能的多Modal语言模型，可以生成和填充文本和图像。CM3Leon使用CM3多Modal架构，同时还能够在更多的指令样本数据上扩大和调整，从而实现极高的性能提升。它是首个基于文本Only语言模型的多Modal模型，通过大规模的检索增强预训练阶段和第二个多任务监督精度调整（SFT）阶段进行训练。它同时是一个通用的模型，可以进行文本到图像和图像到文本的生成，使我们能够介绍自包含的对比编码方法，以生成高质量的输出。广泛的实验表明，这种方法对多Modal模型非常有效。CM3Leon在文本到图像生成任务中实现了状态机器的表现（零 shot MS-COCO FID为4.88），并且在语言导向图像修改、图像控制生成和分割等任务中也能够实现无前例的可控性。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-for-Sequential-Volumetric-Design-Tasks"><a href="#Representation-Learning-for-Sequential-Volumetric-Design-Tasks" class="headerlink" title="Representation Learning for Sequential Volumetric Design Tasks"></a>Representation Learning for Sequential Volumetric Design Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02583">http://arxiv.org/abs/2309.02583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ferdous Alam, Yi Wang, Linh Tran, Chin-Yi Cheng, Jieliang Luo</li>
<li>for: 这篇论文是为了提出一种基于变换器模型的自动化建筑设计方法，以便利用学习的设计知识来评估和生成建筑设计。</li>
<li>methods: 该论文使用变换器模型来编码设计知识，并从一群专家或高性能的设计序列中提取有用的表示。然后，它使用这些表示来评估设计和生成sequential设计。</li>
<li>results: 论文通过使用变换器模型来评估设计的预测性和生成sequential设计，并在一个大量的sequentialvolumetric设计数据集上进行了实验。结果显示，论文的方法可以准确地评估设计的预测性和生成sequential设计，并且可以自动完成volumetric设计序列的生成。<details>
<summary>Abstract</summary>
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the preference model by estimating the density of the learned representations whereas we train an autoregressive transformer model for sequential design generation. We demonstrate our ideas by leveraging a novel dataset of thousands of sequential volumetric designs. Our preference model can compare two arbitrarily given design sequences and is almost 90% accurate in evaluation against random design sequences. Our autoregressive model is also capable of autocompleting a volumetric design sequence from a partial design sequence.
</details>
<details>
<summary>摘要</summary>
三维设计，也称为质量设计，是职业建筑设计的第一步，具有顺序的性质。由于三维设计过程复杂，下面的设计过程含有价值信息。许多努力已经被дела以生成合理的三维设计解决方案，但生成的设计解决方案质量不稳定，评估设计解决方案需要 Either a comprehensive set of metrics or expensive human expertise。而前一些方法仅学习了最终的设计而不是顺序的设计任务，我们提议将专家或高性能的设计序列知识编码到 transformer-based 模型中，然后提取有用的表示。后来，我们将学习的表示用于重要的下游应用程序，如设计偏好评估和过程设计生成。我们开发了偏好模型，通过估计学习的表示密度来评估两个任意给定的设计序列。我们还训练了一个自然语言模型，用于生成顺序的设计序列。我们利用了一个 novel dataset of thousands of sequential volumetric designs，并证明了我们的想法。我们的偏好模型可以比较两个任意给定的设计序列，准确率接近 90%。我们的自然语言模型也可以自动完成一个三维设计序列的生成，从一个受限的设计序列开始。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients"><a href="#Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients" class="headerlink" title="Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients"></a>Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02580">http://arxiv.org/abs/2309.02580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bliss Singhal, Fnu Pooja</li>
<li>for: 预测儿童癫痫症发作，提高医疗工作者对癫痫症管理的反应性。</li>
<li>methods: 使用机器学习算法对单模态神经成像数据进行评估，包括电enzephalogram信号。</li>
<li>results: 研究发现，深度学习算法在预测癫痫症发作中表现更好于逻辑回归和k最近邻居算法，特别是Recurrent Neural Network（RNN）和Convolutional Neural Network（CNN）。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. Various machine learning algorithms' performance is evaluated on important metrics such as accuracy, precision, specificity, sensitivity, F1 score and MCC. The results show that the deep learning algorithms are more successful in predicting seizures than logistic Regression, and k nearest neighbors. The recurrent neural network (RNN) gave the highest precision and F1 Score, long short-term memory (LSTM) outperformed RNN in accuracy and convolutional neural network (CNN) resulted in the highest Specificity. This research has significant implications for healthcare providers in proactively managing seizure occurrence in pediatric patients, potentially transforming clinical practices, and improving pediatric care.
</details>
<details>
<summary>摘要</summary>
“偏头痛是一种流行的神经系统疾病，全球病例约5000万人，美国病例约120万人。有数百万名儿童患有难治性偏头痛，这些病例中的病人可能会受到肢体伤害、混乱、失去知觉和其他 симптом，这些病情可能会干扰儿童日常生活。预测偏头痛可以帮助家长和医疗保健专业人员预防危险情况，准确预测可以帮助儿童减少焦虑和不安，这有助于改善儿童健康。本研究提出了一个新的和完整的预测偏头痛框架，通过评估机器学习算法在单一神经内部成像数据上的表现。实验结果显示，深度学习算法在预测偏头痛方面表现更好，比逻辑回传和k最近邻居。RNN和LSTM也获得了较高的精度和F1分数，CNN则获得了最高的特异性。这些研究结果具有重要的实践意义，可以帮助医疗保健专业人员更好地管理儿童偏头痛的发生，将有助于改善儿童健康，并可能改变临床实践。”
</details></li>
</ul>
<hr>
<h2 id="Anatomy-Driven-Pathology-Detection-on-Chest-X-rays"><a href="#Anatomy-Driven-Pathology-Detection-on-Chest-X-rays" class="headerlink" title="Anatomy-Driven Pathology Detection on Chest X-rays"></a>Anatomy-Driven Pathology Detection on Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02578">http://arxiv.org/abs/2309.02578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philip-mueller/adpd">https://github.com/philip-mueller/adpd</a></li>
<li>paper_authors: Philip Müller, Felix Meissen, Johannes Brandt, Georgios Kaissis, Daniel Rueckert</li>
<li>For: 这篇论文的目的是提出一种自动检测医疗扫描图像中的疾病，并提供高度解释性的支持，以帮助医生做出了解的决策。* Methods: 这篇论文使用了弱地监督的物体检测方法，从图像水平的标签中学习疾病的（粗略）本地化。* Results: 这篇论文的结果显示，使用了解析预报的方法可以超过弱地监督的方法和仅有限的训练样本，并且与完全监督的检测方法竞争。<details>
<summary>Abstract</summary>
Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.
</details>
<details>
<summary>摘要</summary>
医学影像检测和定位技术可以自动解读医学影像，如胸部X射线图像，并提供高水平的解释性，以支持 radiologist 做出 informed 决策。但是，标注疾病 bounding box 是一项时间consuming 的任务，因此大型公共数据集 для此目的罕见。现有方法通常使用弱型 supervised object detection 学习疾病的 (粗略) 本地化，但是由于缺乏 bounding box 监督，性能有限。我们因此提议 anatomy-driven pathology detection (ADPD)，使用容易标注的 bounding box 来代表疾病。我们研究了两种训练方法：supervised training 使用 anatomy-level pathology labels 和 multiple instance learning (MIL) 使用 image-level pathology labels。我们的结果显示，我们的 anatomy-level 训练方法高于弱型方法和有限训练样本的完全supervised detection，而我们的 MIL 方法与两个基eline方法竞争，因此证明了我们的方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks"><a href="#Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks" class="headerlink" title="Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks"></a>Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02576">http://arxiv.org/abs/2309.02576</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/bodyct-dram-emph-subtype">https://github.com/diagnijmegen/bodyct-dram-emph-subtype</a></li>
<li>paper_authors: Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Dirk Jan Slebos, Bram van Ginneken</li>
<li>for:  automatizarea clasificării și evaluării severității emfizemului, pentru gestionarea eficientă a BPCO și studiului heterogeneității bolii.</li>
<li>methods:  utilizarea unui algoritm de învățare profundă pentru a simula sistemul de scor visual al Societății Fleischner pentru clasificarea emfizemului și evaluarea severității.</li>
<li>results:  o precizie a predicțiilor de 52%, față de o metodă publicată anterior cu o precizie de 45%. În plus, o bună înțelegere între scorurile predicate de metoda noastră și scorurile visuale.<details>
<summary>Abstract</summary>
Accurate identification of emphysema subtypes and severity is crucial for effective management of COPD and the study of disease heterogeneity. Manual analysis of emphysema subtypes and severity is laborious and subjective. To address this challenge, we present a deep learning-based approach for automating the Fleischner Society's visual score system for emphysema subtyping and severity analysis. We trained and evaluated our algorithm using 9650 subjects from the COPDGene study. Our algorithm achieved the predictive accuracy at 52\%, outperforming a previously published method's accuracy of 45\%. In addition, the agreement between the predicted scores of our method and the visual scores was good, where the previous method obtained only moderate agreement. Our approach employs a regression training strategy to generate categorical labels while simultaneously producing high-resolution localized activation maps for visualizing the network predictions. By leveraging these dense activation maps, our method possesses the capability to compute the percentage of emphysema involvement per lung in addition to categorical severity scores. Furthermore, the proposed method extends its predictive capabilities beyond centrilobular emphysema to include paraseptal emphysema subtypes.
</details>
<details>
<summary>摘要</summary>
正确识别 chronic obstructive pulmonary disease (COPD) 中的肺脂肪病变Subtype和严重程度是管理 COPD 和病理多样性研究的关键。现有的手动分析方法具有劳动密集和主观性问题。为了解决这个挑战，我们提出了一个基于深度学习的方法，可以自动应用 Fleischner Society 的视觉分数系统来分类肺脂肪病变和严重程度的分析。我们在 COPDGene 研究中训练和评估了我们的算法，并取得了52%的预测精度，比之前已出版的方法的45%的精度高。此外，我们的方法可以生成高分辨率的本地启动图表，可以让用户视觉化网络预测结果。此外，我们的方法还可以在分割肺脂肪病变的中心lobular emphysema 之外，还可以分类para septal emphysema 的亚型。
</details></li>
</ul>
<hr>
<h2 id="Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach"><a href="#Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach" class="headerlink" title="Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach"></a>Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02571">http://arxiv.org/abs/2309.02571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mishfad Shaikh Veedu, James Melbourne, Murti V. Salapaka</li>
<li>for: 这种研究探讨了如何从时间序列数据中学习 causal effect，尤其是在存在时间依赖关系时。</li>
<li>methods: 该研究使用了频域逻辑（FD）来表示时间序列数据，并使用了多ivariate Wiener projection来重建系统的 causal 结构。</li>
<li>results: 研究发现，使用 FD 可以有效地实现 causal inference，并且可以使用 do-calculus 机制来实现类型Single-door（with cycles）等经典条件。此外，研究还发现，对于具有交互关系的系统，使用 multivariate Wiener projections 可以实现高效的图重建，并且可以避免时间域方法的缺点。<details>
<summary>Abstract</summary>
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details>
<details>
<summary>摘要</summary>
学习 causal effect from data 是科学中的基本和很受欢迎的问题，特别是当 causal relationship 是静态的时候。然而， causal effect 在动态依赖关系时 menos explored。从时间序列观察中Identifying dynamic causal effects 是计算expensive的，与静态场景相比。我们 demonstarte that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. 因为 FFT 汇聚所有时间依赖项，因此 causal inference 可以efficiently perform by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Partitioning-Around-Medoids"><a href="#Sparse-Partitioning-Around-Medoids" class="headerlink" title="Sparse Partitioning Around Medoids"></a>Sparse Partitioning Around Medoids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02557">http://arxiv.org/abs/2309.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>for: 这篇论文主要关注的是用Partitioning Around Medoids（PAM）分 clustering技术，并且处理非对称的情况，以及大量的问题。</li>
<li>methods: 这篇论文使用的方法包括PAM、快速PAM和组合来提高问题的解决方案。</li>
<li>results: 这篇论文的结果显示，这种方法可以在实际应用中提供更好的解决方案，并且可以适应更大的问题。<details>
<summary>Abstract</summary>
Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the interpretation of facility location, where the possible facility locations are not identical to the consumer locations). Because of sparsity, it may be impossible to cover all points with just k medoids for too small k, which would render the problem unsolvable, and this breaks common heuristics for finding a good starting condition. We, hence, consider determining k as a part of the optimization problem and propose to first construct a greedy initial solution with a larger k, then to optimize the problem by alternating between PAM-style "swap" operations where the result is improved by replacing medoids with better alternatives and "remove" operations to reduce the number of k until neither allows further improving the result quality. We demonstrate the usefulness of this method on a problem from electrical engineering, with the input graph derived from cartographic data.
</details>
<details>
<summary>摘要</summary>
分割附近中点（PAM，k-中点）是一种流行的聚类技术，可以用于任意距离函数或相似性，每个群由其中心对象代表，称为中点或离散中间值。在运维研究中，这家问题也称为设施位置问题（FLP）。Recently，FastPAM引入了一种加速方法，以便在更大的问题上应用，但该方法仍然有线性增长在N上。在这章中，我们讨论了一种稀疏和非对称的变体，用于应用于图数据，如公路网络。通过利用稀疏性，我们可以避免 quadratic runtime和内存需求，并使这种方法可扩展到更大的问题，只要我们能够构建一个具有足够连接度的小 graph。此外，我们考虑非对称情况，其中中点集不同于要覆盖的点集（或在设施位置问题中，可能的设施位置不同于消费者位置）。由于稀疏性，可能无法使用 too small k 覆盖所有点，这会使问题无解，并使常见的启动策略无效。我们因此考虑 determining k 为优化问题的一部分，并提议先构建一个大于 k 的欢迎初始解，然后通过 PAM 风格的 "swap" 操作和 "remove" 操作来优化问题，直到 neither 允许再提高结果质量。我们在电力工程中的一个问题上 demonstrate 了这种方法的有用性，其输入图由地图数据 derivation。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images"><a href="#Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images" class="headerlink" title="Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images"></a>Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02556">http://arxiv.org/abs/2309.02556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teru Nagamori, Sayaka Shiota, Hitoshi Kiya</li>
<li>for: 这个论文应用于保持模型性能的维护和优化，特别是在使用trasformed data时。</li>
<li>methods: 本论文提出了一种基于vision transformer（ViT）的域 adapted fine-tuning方法，不会对模型的性能造成下降。</li>
<li>results: 在实验中，提出的方法能够防止模型的性能下降，甚至在使用加密图像的CIFAR-10和CIFAR-100 datasets上。<details>
<summary>Abstract</summary>
In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络（DNNs）在使用修改后的数据进行训练后应用于了各种应用程序，如隐私保护学习、访问控制和敌意防御。然而，使用修改后的数据会导致模型的性能下降。因此，在这篇论文中，我们提出了一种基于视Transformer（ViT）的新方法，用于精细调整模型使用修改图像。我们的领域适应方法不会导致模型的精度下降，并且基于ViT的嵌入结构进行实现。在实验中，我们证明了该方法可以在使用加密图像的CIFAR-10和CIFAR-100数据集上预防精度下降。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images"><a href="#A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images" class="headerlink" title="A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images"></a>A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02555">http://arxiv.org/abs/2309.02555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Jesse Hoey, Alexander Wong</li>
<li>for: 本研究旨在summarizing recent research on using self-supervised pretraining for X-ray, computed tomography, magnetic resonance, and ultrasound imaging, 以便比较自主监督学习和完全监督学习在诊断任务中的表现。</li>
<li>methods: 本研究使用了自主监督学习方法，并对其与完全监督学习进行比较，以探讨自主监督学习在诊断任务中的表现。</li>
<li>results: 研究发现，自主监督学习通常在下游任务中表现更好于完全监督学习，特别是当无标例数量远大于标记数量时。此外，研究还提出了一些建议和实践方法，如结合临床知识和理论上正确的自主监督学习方法，评估在公共数据集上，扩大ultrasound领域的证据，并Characterizing自主监督学习的泛化性。<details>
<summary>Abstract</summary>
Self-supervised pretraining has been observed to be effective at improving feature representations for transfer learning, leveraging large amounts of unlabelled data. This review summarizes recent research into its usage in X-ray, computed tomography, magnetic resonance, and ultrasound imaging, concentrating on studies that compare self-supervised pretraining to fully supervised learning for diagnostic tasks such as classification and segmentation. The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples. Based on the aggregate evidence, recommendations are provided for practitioners considering using self-supervised learning. Motivated by limitations identified in current research, directions and practices for future study are suggested, such as integrating clinical knowledge with theoretically justified self-supervised learning methods, evaluating on public datasets, growing the modest body of evidence for ultrasound, and characterizing the impact of self-supervised pretraining on generalization.
</details>
<details>
<summary>摘要</summary>
自我超vision学习已被观察到可以提高特征表示，以便在转移学习中提高性能。这篇评论总结了最近关于这一点的研究，涉及到X射线、计算机Tomography、磁共振和ultrasound成像中的自我超vision学习，并对这些研究进行比较，以了解在诊断任务中（如分类和 segmentation）中的表现。最重要的发现是，自我超vision学习通常比全supervision学习提高下游任务性能，特别是当无标例大量出现于标例例子之上。根据总体证据，我们提供了对实践者使用自我超vision学习的建议，以及根据现有研究的局限性，未来研究的方向和实践。
</details></li>
</ul>
<hr>
<h2 id="Data-Aggregation-for-Hierarchical-Clustering"><a href="#Data-Aggregation-for-Hierarchical-Clustering" class="headerlink" title="Data Aggregation for Hierarchical Clustering"></a>Data Aggregation for Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02552">http://arxiv.org/abs/2309.02552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elki-project/elki">https://github.com/elki-project/elki</a></li>
<li>paper_authors: Erich Schubert, Andreas Lang</li>
<li>for: 使用 Hierarchical Agglomerative Clustering (HAC) 进行数据分 clustering，但是由于数据量大，需要使用有效的数据处理方法。</li>
<li>methods: 使用 BETULA 数据集成算法，一种稳定的数据分集方法，可以在具有限制的资源的系统上进行实时处理。</li>
<li>results: 使用 BETULA 数据集成算法可以在具有限制的资源的系统上进行 Hierarchical Agglomerative Clustering (HAC)，并且只需要小量的数据处理loss，从而实现对大规模数据集的探索分析。<details>
<summary>Abstract</summary>
Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continual-Improvement-of-Threshold-Based-Novelty-Detection"><a href="#Continual-Improvement-of-Threshold-Based-Novelty-Detection" class="headerlink" title="Continual Improvement of Threshold-Based Novelty Detection"></a>Continual Improvement of Threshold-Based Novelty Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02551">http://arxiv.org/abs/2309.02551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abe Ejilemele, Jorge Mendez-Mendez</li>
<li>for: 解决 neural network 在 dynamical 和 open-world 环境中探测未知类的问题</li>
<li>methods: 使用 linear search 和 leave-one-out cross-validation 自动选择阈值</li>
<li>results: 在 MNIST、Fashion MNIST 和 CIFAR-10 上提高总准确率<details>
<summary>Abstract</summary>
When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:在动态开放世界中，神经网络对未经见过的类型探测强度不足。这使得持续学习者在真实环境中部署不太可能。一种常见的新类探测技术是基于训练数据点和观察数据点之间的相似性阈值。然而，这些方法通常需要手动指定（在前期）阈值的值，因此无法适应数据的特点。我们提出了一种新的方法，利用线性搜索和留下一个类的批处理来自动选择阈值。我们示示了这种新方法可以在MNIST、Fashion MNIST和CIFAR-10上提高总准确率。
</details></li>
</ul>
<hr>
<h2 id="Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning"><a href="#Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning" class="headerlink" title="Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning"></a>Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02547">http://arxiv.org/abs/2309.02547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manav Kulshrestha, Ahmed H. Qureshi</li>
<li>for: 这种论文旨在提出一种深度学习方法，用于多层物体重新排序规划，以便让机器人在复杂和自由环境中与物体进行互动。</li>
<li>methods: 这种方法使用图注意力网络，以解决多层物体重新排序规划中的结构依赖关系。</li>
<li>results: 这种方法可以在不同的场景中，包括真实世界和虚拟环境中，对多层物体进行重新排序规划，并且可以具有更高的灵活性和效率。<details>
<summary>Abstract</summary>
Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. The dataset, supplementary details, videos, and code implementation are available at: https://manavkulshrestha.github.io/scl
</details>
<details>
<summary>摘要</summary>
robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. existing work focuses primarily on single-level rearrangement planning, and even if multiple levels exist, the dependency relations among substructures are geometrically simpler, like tower stacking. we propose structural concept learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. it is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. we compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. the dataset, supplementary details, videos, and code implementation are available at: https://manavkulshrestha.github.io/sclHere's the word-for-word translation of the text into Simplified Chinese:瑜珈机器人操作任务，如物体重新排序，对机器人与复杂且随机环境进行交互起着关键作用。现有工作主要集中在单级重新排序规划上，即使多级存在，也是更加简单的圆柱堆叠关系。我们提出了结构概念学习（SCL），一种基于深度学习的多级物体重新排序规划方法，通过图注意力网络来解决场景理解。SCL在自生成的 simulate 数据集上进行训练，可以处理未看过的场景，具有更多的对象和更高的结构复杂度，独立抽象出各自的子结构，以便在多个机器人上并发执行任务，并能够通过实际世界进行推广。我们与一系列经典和模型基于的基准进行比较，表明我们的方法能够充分利用场景理解，实现更高的性能、灵活性和效率。数据集、补充细节、视频和代码实现可以在：https://manavkulshrestha.github.io/scl 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation"><a href="#A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation" class="headerlink" title="A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation"></a>A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02539">http://arxiv.org/abs/2309.02539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karn N. Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, Alexander Lerch, William Wolcott</li>
<li>for: 这个论文的目的是提出一种可扩展的频谱分解模型，用于从音频混合中提取对话、音乐和特效的三个分配。</li>
<li>methods: 这个模型使用了 psycho-acoustic 频率缩放，并使用了基于信号噪声比和1-norm的损失函数。它还利用了共同编码器的信息共享特性，以降低训练和推理时的计算复杂度，提高分离性能，并允许在推理时进行轻松的拓展。</li>
<li>results: 该模型在 Divide and Remaster 数据集上达到了最佳性能，对话分配的性能超过了理想的噪声比面积。<details>
<summary>Abstract</summary>
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem.
</details>
<details>
<summary>摘要</summary>
电影音频源分离是一个相对较新的子任务，旨在从它们的混合中提取对话束、音乐束和特效束。在这项工作中，我们开发了一种通用于任何完整或过complete的频谱分解的模型。我们使用了听觉动机驱动的频谱缩放，以便更可靠地提取特征。我们还提出了基于信号噪声比和1- нор的损失函数，以及在训练和推理过程中共享信息的公共编码器设计，以降低计算复杂性，提高分离性能，并在推理时提供灵活性。我们的最佳模型在Divide and Remaster数据集上达到了对话束性能的最佳状态，超过理想的噪声比面积。
</details></li>
</ul>
<hr>
<h2 id="Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test"><a href="#Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test" class="headerlink" title="Experience and Prediction: A Metric of Hardness for a Novel Litmus Test"></a>Experience and Prediction: A Metric of Hardness for a Novel Litmus Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02534">http://arxiv.org/abs/2309.02534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicos Isaak, Loizos Michael<br>for: 这篇论文的目的是提出一种基于机器学习的系统，用于评估Winograd schema的难度水平，并在大规模实验中证明该系统的可靠性和准确性。methods: 该系统采用两种不同的方法，即随机森林和深度学习（LSTM），以评估Winograd schema的难度水平。results: 该研究通过大规模实验表明，人类对Winograd schema的表现异常差异，并且该系统能够快速和准确地评估Winograd schema的难度水平。<details>
<summary>Abstract</summary>
In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.   Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.   Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regarding the number of schemas it could be applied on. This paper adds to previous research by presenting a new system that is based on Machine Learning (ML), able to output the hardness of any Winograd schema faster and more accurately than any other previously used method. Our developed system, which works within two different approaches, namely the random forest and deep learning (LSTM-based), is ready to be used as an extension of any other system that aims to differentiate between Winograd schemas, according to their perceived hardness for humans. At the same time, along with our developed system we extend previous work by presenting the results of a large-scale experiment that shows how human performance varies across Winograd schemas.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie，Winograd Schema Challenge（WSC）已成为研究社区中的一个重要测试方法。因此，WSC 已经激发了大量研究兴趣，因为它可以用来理解人类行为。在这种情况下，开发新技术使得Winograd schema可以在不同领域中应用，如设计新型 CAPTCHAs。根据文献中的基准数据，人类成人在 WSC 中的性能不同，这意味着不同的 Winograd schema 可能会有不同的抵抗度。因此，我们可以使用这个“抵抗度指标”来分类不同的 Winograd schema。我们的先前研究已经证明了可以通过设计自动化系统来输出 Winograd schema 的抵抗度指标，但是这种方法只能处理有限数量的 Winograd schema。在这篇论文中，我们提出了一种基于机器学习（ML）的新系统，可以快速和准确地输出 Winograd schema 的抵抗度指标。我们的系统采用了两种不同的方法，即随机森林和深度学习（LSTM），可以作为任何其他系统的扩展，以便在分类不同 Winograd schema 时使用。同时，我们也将我们开发的系统与先前的研究相结合，并发表了一项大规模的实验结果，以显示人类在不同 Winograd schema 中的性能如何变化。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-on-the-Probability-Simplex"><a href="#Diffusion-on-the-Probability-Simplex" class="headerlink" title="Diffusion on the Probability Simplex"></a>Diffusion on the Probability Simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02530">http://arxiv.org/abs/2309.02530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, Eric Zhengyu Zhu</li>
<li>for: 本文提出了一种基于扩散模型的生成模型，以填充缺失数据。</li>
<li>methods: 该方法使用概率 simplicial 进行扩散处理，并使用 softmax 函数和 Ornstein-Unlenbeck 过程来实现。</li>
<li>results: 该方法可以生成连续的数据分布，并且可以自然地扩展到包括 bounded image generation 等应用。<details>
<summary>Abstract</summary>
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
</details>
<details>
<summary>摘要</summary>
Diffusion模型学习将推进数据分布的进程逆转，创建生成模型。然而，数据的连续性和精度之间存在矛盾，这种矛盾可能会影响模型的性能。为了解决这种矛盾，我们提出了在概率 simpliciter 上进行Diffusion的方法。使用概率 simpliciter 自然地创建了点对应的分类概率分布的解释。我们的方法使用Ornstein-Unlenbeck过程和softmax函数。我们发现我们的方法也自然地扩展到包括Diffusion在单位立方体上，这有应用于 bounded image generation。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs"><a href="#Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs" class="headerlink" title="Adaptive Adversarial Training Does Not Increase Recourse Costs"></a>Adaptive Adversarial Training Does Not Increase Recourse Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02528">http://arxiv.org/abs/2309.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Hardy, Jayanth Yetukuri, Yang Liu</li>
<li>for: 本研究旨在探讨对 adversarial 训练的 adaptive 方法对 algorithmic recourse 成本的影响。</li>
<li>methods: 本研究使用了 adversarial 训练和 adaptive 训练方法，以 investigate 模型在不同攻击半径下的Robustness和 recourse 成本之间的关系。</li>
<li>results: 研究发现，使用 adaptive adversarial training 可以减少模型对攻击的脆弱性，但这并不会增加 algorithmic recourse 成本。这种方法可能为在 recoursability 重要的领域提供可Affordable的Robustness。<details>
<summary>Abstract</summary>
Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.
</details>
<details>
<summary>摘要</summary>
最近的工作已经将敌意攻击方法和算法救济方法联系起来：两者都寻找输入实例中最小的变化，使模型的分类决策发生变化。已经证明，传统的敌意训练，即减少攻击者所用的恶意扰动，会增加生成的救济成本，与攻击训练半径的大小正相关。从算法救济的角度来看， however，合适的攻击训练半径一直未知。另一些最近的工作已经提出了适应式敌意训练，以Addressing the issue of instance-wise variable adversarial vulnerability，并在不同领域中获得了成功。这项工作研究了适应式敌意训练对算法救济成本的影响。我们证明了，通过适应式敌意训练提高模型的鲁棒性，对算法救济成本没有显著影响，这提供了可能的便宜鲁棒性途径，特别在知道攻击训练半径的情况下。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models"><a href="#Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of CPU and GPU Profiling for Deep Learning Models"></a>Comparative Analysis of CPU and GPU Profiling for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02521">http://arxiv.org/abs/2309.02521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipesh Gyawali</li>
<li>for: 这个论文旨在研究用于训练深度神经网络时 CPU 和 GPU 的资源分配和消耗。</li>
<li>methods: 该论文使用 Pytorch 框架来实现深度神经网络的训练，并对 CPU 和 GPU 的运行时间和内存使用进行分析。</li>
<li>results: 研究显示，对于深度神经网络，GPU 的运行时间比 CPU 更低，但对于更简单的网络，GPU 的提升不太明显。<details>
<summary>Abstract</summary>
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）和机器学习（ML）应用在最近几年内逐渐增加。互联网上生成的巨量数据可以通过ML和DL算法提取有意义的结果。硬件资源和开源库的出现使得实现这些算法变得更加容易。TensorFlow和PyTorch是实现ML项目的主要框架之一。通过使用这些框架，我们可以跟踪CPU和GPU上执行的操作，并分析资源分配和消耗。本文介绍了在使用PyTorch训练深度神经网络时，CPU和GPU的时间和内存分配。本文分析显示，在深度神经网络训练中，GPU的运行时间比CPU更低。对于简单的网络，GPU不比CPU具有显著提高。
</details></li>
</ul>
<hr>
<h2 id="Towards-User-Guided-Actionable-Recourse"><a href="#Towards-User-Guided-Actionable-Recourse" class="headerlink" title="Towards User Guided Actionable Recourse"></a>Towards User Guided Actionable Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02517">http://arxiv.org/abs/2309.02517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanth Yetukuri, Ian Hardy, Yang Liu</li>
<li>for: This paper focuses on developing a tool for providing actionable recourse to users who are negatively impacted by machine learning models in critical fields such as healthcare, banking, and criminal justice.</li>
<li>methods: The paper proposes a gradient-based approach to capture user preferences via soft constraints in three simple forms: scoring continuous features, bounding feature values, and ranking categorical features.</li>
<li>results: The proposed approach, called User Preferred Actionable Recourse (UP-AR), is evaluated through extensive experiments to verify its effectiveness.<details>
<summary>Abstract</summary>
Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-RL-via-Disentangled-Environment-and-Agent-Representations"><a href="#Efficient-RL-via-Disentangled-Environment-and-Agent-Representations" class="headerlink" title="Efficient RL via Disentangled Environment and Agent Representations"></a>Efficient RL via Disentangled Environment and Agent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02435">http://arxiv.org/abs/2309.02435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Gmelin, Shikhar Bahl, Russell Mendonca, Deepak Pathak</li>
<li>for: 提高模型自适应RL算法的表示能力，通过 Agent 对环境的视觉知识，如形状或面具，进行权重调整。</li>
<li>methods: 使用简单的 auxillary 损失函数将 Agent 对环境的视觉知识 incorporated 到 RL 目标中，提高模型的表示能力。</li>
<li>results: 在 18 个不同的难度较高的视觉 simulate 环境中，模型表现出色，超过了现有的模型自由方法。Note: “模型自适应RL算法” refers to the fact that the model is able to adapt to the environment, and “视觉知识” refers to the knowledge of the agent’s visual perception of the environment.<details>
<summary>Abstract</summary>
Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots. Website at https://sear-rl.github.io/
</details>
<details>
<summary>摘要</summary>
Agent 可以掌握自己和环境的分离，可以利用这种理解来形成有效的视觉输入表示。我们提出一种学习这种结构化表示的方法，使用视觉知识，如机器人的形状或面具，这些知识通常很便宜获得。我们将这种方法 integrate 到 RL 目标中使用了一个简单的辅助损失。我们显示了我们的方法，结构化环境-代理表示（SEAR），在18个复杂的视觉 simulate 环境中，使用5种不同的机器人，超过了当前无模型的方法。更多信息可以在https://sear-rl.github.io/ 上查看。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach"><a href="#Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach" class="headerlink" title="Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach"></a>Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02429">http://arxiv.org/abs/2309.02429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vimal K B, Saketh Bachu, Tanmay Garg, Niveditha Lakshmi Narasimhan, Raghavan Konuru, Vineeth N Balasubramanian</li>
<li>For: 该研究旨在估计公共可用预训练模型的迁移性，以便在转移学习任务中选择合适的模型。* Methods: 该研究提出了一种基于最优 tranSport 的 suBmOdular tRaNsferability 度量（OSBORN），用于评估多个源模型集的迁移性。OSBORN 考虑了图像频谱差异、任务差异和模型集凝聚性，以提供可靠的迁移性估计。* Results: 该研究在图像分类和semantic segmentation任务上对 OSBORN 进行了性能评估，并与当前状态的metric MS-LEEP 和 E-LEEP 进行了比较。 results 表明，OSBORN 在这些任务中表现出色，并且在模型集迁移中提供了更可靠的估计。<details>
<summary>Abstract</summary>
Estimating the transferability of publicly available pretrained models to a target task has assumed an important place for transfer learning tasks in recent years. Existing efforts propose metrics that allow a user to choose one model from a pool of pre-trained models without having to fine-tune each model individually and identify one explicitly. With the growth in the number of available pre-trained models and the popularity of model ensembles, it also becomes essential to study the transferability of multiple-source models for a given target task. The few existing efforts study transferability in such multi-source ensemble settings using just the outputs of the classification layer and neglect possible domain or task mismatch. Moreover, they overlook the most important factor while selecting the source models, viz., the cohesiveness factor between them, which can impact the performance and confidence in the prediction of the ensemble. To address these gaps, we propose a novel Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the transferability of an ensemble of models to a downstream task. OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability. We gauge the performance of OSBORN on both image classification and semantic segmentation tasks. Our setup includes 28 source datasets, 11 target datasets, 5 model architectures, and 2 pre-training methods. We benchmark our method against current state-of-the-art metrics MS-LEEP and E-LEEP, and outperform them consistently using the proposed approach.
</details>
<details>
<summary>摘要</summary>
公共可用预训练模型的评估对于目标任务中的转移学习任务已经占据了重要的地位。现有的努力提出了可以让用户从一 pool 中选择一个预训练模型，而无需 individually 精度 each model 和确定一个特定的模型。随着可用的预训练模型的数量和模型集的流行，也变得重要对多源模型的转移性进行研究。现有的努力研究了多源模型中的转移性，但是忽视了可能存在的领域或任务差异，以及选择源模型时最重要的因素——模型集成性因素，这可能会影响预测结果和置信度。为解决这些缺陷，我们提出了一种新的 Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN)，用于评估 Ensemble 模型下某个下游任务的转移性。OSBORN 共同考虑图像领域差异、任务差异和模型集成性，以提供可靠的转移性估计。我们在图像分类和 semantic segmentation 任务上测试了我们的方法，我们的设置包括 28 个源数据集、11 个目标数据集、5 个模型架构和 2 种预训练方法。我们对现有的 metric MS-LEEP 和 E-LEEP 进行比较，并在我们的方法中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework"><a href="#Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework" class="headerlink" title="Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework"></a>Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02428">http://arxiv.org/abs/2309.02428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhelal/TensorsPyBook">https://github.com/mhelal/TensorsPyBook</a></li>
<li>paper_authors: Manal Helal</li>
<li>for: 本文旨在介绍tensorization，一种将多维数据转换为简单的二维矩阵的方法，以便在线性代数学习算法中使用。</li>
<li>methods: 本文详细介绍了tensorization的过程，包括多维数据源、多方分析方法和其integration with Deep Neural Networks模型。</li>
<li>results: 本文的实验结果表明，使用多维数据和多方分析方法可以更好地捕捉数据之间的复杂关系，同时减少模型参数数量和加速处理速度。<details>
<summary>Abstract</summary>
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form and applying multiway analysis methods grounded in multilinear algebra reveal a profound capacity to capture intricate interrelationships among various dimensions while, surprisingly, reducing the number of model parameters and accelerating processing. A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains.
</details>
<details>
<summary>摘要</summary>
“公共领域数据的急剧增长和深度学习模型的复杂化，使得更有效的数据表示和分析技术成为了必要。这篇论文受Helal（2023）的研究启发，旨在提供tensorization的全面概述。这种变革性的方法 bridge了数据的自然多维性和常用的两维矩阵在线性 алгебра学习算法之间的 gap。本文探讨tensorization的步骤、多维数据源、多向分析方法和其优点。在Python中，用二维算法和多向算法进行盲源分离（BSS）的小例子被提供，结果表明多向分析更加表达力。相反于维度恩恶的直觉，使用原始形式的多维数据和应用多向分析方法，根据多线性代数的基础，能够捕捉多维维度之间的复杂关系，同时减少模型参数量和加速处理。本文还提供了多向分析方法的报告和与不同领域的深度神经网络模型集成的案例研究。”
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Architectures-for-Language-Agents"><a href="#Cognitive-Architectures-for-Language-Agents" class="headerlink" title="Cognitive Architectures for Language Agents"></a>Cognitive Architectures for Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02427">http://arxiv.org/abs/2309.02427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysymyth/awesome-language-agents">https://github.com/ysymyth/awesome-language-agents</a></li>
<li>paper_authors: Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</li>
<li>for: 本文旨在提出一种新的语言智能代理模型，具有更强的语言理解和决策能力。</li>
<li>methods: 本文使用了大量自然语言处理技术和符号智能技术，以及最新的语言模型，为语言代理模型提供了更多的可能性和可能性。</li>
<li>results: 本文提出了一种名为“CoALA”的概念框架，用于系统化和总结现有的语言代理模型，并提出了未来语言代理模型的发展方向。<details>
<summary>Abstract</summary>
Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.
</details>
<details>
<summary>摘要</summary>
最近努力已经将大型语言模型（LLM）与外部资源（例如互联网）或内部控制流（例如提示链）结合使用，以完成需要固定或理解的任务。然而，这些努力一般都是偶发的，缺乏一个系统化的框架来建立完善的语言代理。为解决这个挑战，我们 drawing on the rich history of agent design in symbolic artificial intelligence，开发了一个蓝图来建立一波新的认知语言代理。我们首先表明LLM具有许多与生产系统相似的性质，而最近努力提高LLM的固定或理解与生产系统的发展相似。然后，我们提出了语言代理认知框架（CoALA），一种概念框架，用于系统化多种LLM基于的理解、固定、学习和决策方法，并将其视为语言代理在框架中的实例。最后，我们使用CoALA框架， highlighting 缺陷并提出了更加强大的语言代理未来的可行方向。
</details></li>
</ul>
<hr>
<h2 id="Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost"><a href="#Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost" class="headerlink" title="Monotone Tree-Based GAMI Models by Adapting XGBoost"></a>Monotone Tree-Based GAMI Models by Adapting XGBoost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02426">http://arxiv.org/abs/2309.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Soroush Aramideh, Jie Chen, Vijayan N. Nair</li>
<li>for: 本研究使用机器学习建模来适应低阶函数ANOVA模型，包括主效应和次阶交互效应。这些GAMI（GAM + 交互）模型可以直接解释为函数主效应和交互效应的抽象图形。但是，现有的GAMI模型不能考虑幂射性要求。</li>
<li>methods: 本研究提出了一种基于搜索树的幂射性GAMI模型，称为幂射GAMI-Tree。该模型使用XGBoost算法进行适应，并使用筛选技术选择重要的交互效应。最后，使用XGBoost算法进行适应，并使用解析和纯化技术来获得幂射GAMI模型。</li>
<li>results: 使用 simulate datasets 进行测试，monotone GAMI-Tree 和 EBM 都可以使用割据函数来实现幂射性。但是，与主效应不同，交互效应通常不幂射。<details>
<summary>Abstract</summary>
Recent papers have used machine learning architecture to fit low-order functional ANOVA models with main effects and second-order interactions. These GAMI (GAM + Interaction) models are directly interpretable as the functional main effects and interactions can be easily plotted and visualized. Unfortunately, it is not easy to incorporate the monotonicity requirement into the existing GAMI models based on boosted trees, such as EBM (Lou et al. 2013) and GAMI-Lin-T (Hu et al. 2022). This paper considers models of the form $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$ and develops monotone tree-based GAMI models, called monotone GAMI-Tree, by adapting the XGBoost algorithm. It is straightforward to fit a monotone model to $f(x)$ using the options in XGBoost. However, the fitted model is still a black box. We take a different approach: i) use a filtering technique to determine the important interactions, ii) fit a monotone XGBoost algorithm with the selected interactions, and finally iii) parse and purify the results to get a monotone GAMI model. Simulated datasets are used to demonstrate the behaviors of mono-GAMI-Tree and EBM, both of which use piecewise constant fits. Note that the monotonicity requirement is for the full model. Under certain situations, the main effects will also be monotone. But, as seen in the examples, the interactions will not be monotone.
</details>
<details>
<summary>摘要</summary>
近期研究使用机器学习建筑来适应低阶函数ANOVA模型，包括主效和二阶交互。这些GAMI（GAM + 交互）模型直接可以解释为函数主效和交互，可以轻松地图示和可见化。然而，不能容易地将幂质要求 incorporate 到现有的GAMI模型中，如EBM（Lou et al. 2013）和GAMI-Lin-T（Hu et al. 2022）。这篇论文考虑模型形式为 $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$，并开发了幂质树基于XGBoost算法的幂质GAMI模型，称为幂质GAMI-Tree。使用XGBoost算法直接适应幂质模型是 straightforward。然而，适应模型仍然是黑盒模型。我们采取了不同的方法：一、使用筛选技术确定重要的交互项，二、使用选择的交互项适应幂质XGBoost算法，并 finally iii）解析和纯化结果，以获得幂质GAMI模型。模拟数据集用于示示幂质GAMI-Tree和EBM两者使用块状常量适应的行为。注意，幂质性要求是全模型的。在某些情况下，主效也可能是幂质的，但交互项通常不是幂质的。
</details></li>
</ul>
<hr>
<h2 id="On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback"><a href="#On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback" class="headerlink" title="On the Minimax Regret in Online Ranking with Top-k Feedback"></a>On the Minimax Regret in Online Ranking with Top-k Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02425">http://arxiv.org/abs/2309.02425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Zhang, Ambuj Tewari</li>
<li>for: 这篇论文关注在线排名算法的实时反馈问题，具体来说是在排名结果中提供反馈，并且只有top-$k$项被反馈。</li>
<li>methods: 这篇论文使用了partial monitoring技术来分析在线排名算法。</li>
<li>results: 论文提供了对top-$k$反馈模型下的全面 caracterization，并给出了一种高效的算法来实现最小最大偏差率。<details>
<summary>Abstract</summary>
In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.
</details>
<details>
<summary>摘要</summary>
在在线排名中，一种学习算法顺序排序一组项目，并接收反馈的形式为相关性分数。由于获取相关性分数通常需要人工标注，因此对于partial feedback Setting而言，很有吸引力。查户和特ва里（2017）提出了在线排名算法的框架，并使用partial monitoring技术。在这篇论文中，我们进一步调查在线排名的top-$k$反馈模型，并解决了查户和特ва里（2017）提出的一些问题。我们为所有的$k$和以下排名性能指标提供了完整的 caracterization of minimax regret rate：Pairwise Loss、Discounted Cumulative Gain和Precision@n。此外，我们还提供了可效的算法，可以实现Precision@n的 minimax regret rate。
</details></li>
</ul>
<hr>
<h2 id="Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test"><a href="#Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test" class="headerlink" title="Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test"></a>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02422">http://arxiv.org/abs/2309.02422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</li>
<li>for: 这个论文是关于最大均值差（MMD）测试的一个报告，MMD 是一种基于 maximizing the mean difference over samples from one distribution $P$ versus another $Q$ 的非 Parametric 两个样本测试。</li>
<li>methods: 这个论文使用了 Radon  bounded variation（RBV）空间中的一个函数 $f$ 来定义 MMD，并且使用了 neural network 来优化这个测试。</li>
<li>results: 论文提出了一种基于 RBV 空间的 MMD 测试，称为 Radon-Kolmogorov-Smirnov（RKS）测试，并证明了这个测试有 asymptotically full power 和 asymptotic null distribution。 Additionally, the paper compares the RKS test with the more traditional kernel MMD test and discusses its strengths and weaknesses through extensive experiments.<details>
<summary>Abstract</summary>
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weakenesses of the RKS test versus the more traditional kernel MMD test.
</details>
<details>
<summary>摘要</summary>
“最大均值差（MMD）是一类非参数性两个样本测试，基于将一个分布 $P$ 与另一个分布 $Q$ 的样本进行比较，并对样本进行数据变换 $f$ 生成的最大均值差。我们 Drawing inspiration from recent work connecting functions of Radon bounded variation (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the Radon-Kolmogorov-Smirnov (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test - the function $f$ achieving the maximum mean difference - is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.”Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Hong Kong, Taiwan, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Computing-SHAP-Efficiently-Using-Model-Structure-Information"><a href="#Computing-SHAP-Efficiently-Using-Model-Structure-Information" class="headerlink" title="Computing SHAP Efficiently Using Model Structure Information"></a>Computing SHAP Efficiently Using Model Structure Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02417">http://arxiv.org/abs/2309.02417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Ke Wang</li>
<li>for: 这个论文的目的是提出一种可以快速计算 SHAP 值的方法，以解决 SHAP 计算的时间复杂度问题。</li>
<li>methods: 这篇论文提出了三种方法来计算 SHAP 值，包括基于函数分解的方法、基于模型结构信息的方法和一种迭代方法。</li>
<li>results: 这些方法可以快速计算 SHAP 值，并且可以适用于不同的模型结构信息。对比 Castor &amp; Gomez (2008) 的采样方法，这些方法显示更高的准确性和效率。<details>
<summary>Abstract</summary>
SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor & Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians"><a href="#First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians" class="headerlink" title="First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians"></a>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02412">http://arxiv.org/abs/2309.02412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Doikov, Geovani Nunes Grapiglia</li>
<li>for: 这个论文旨在解决通用非凸优化问题，提出了一种基于Cubically regularized Newton method的first-order（Hessian-free）和zero-order（derivative-free）实现方法。</li>
<li>methods: 这些方法使用了finite difference approximations of the derivatives，并使用了特殊的适应搜索过程，以适应不同的迭代数。此外，它们还使用了lazys Hessian update，可以重用上一次计算的Hessianapproximation矩阵。</li>
<li>results: 作者证明了这些方法的全局复杂度上限为$\mathcal{O}( n^{1&#x2F;2} \epsilon^{-3&#x2F;2})$函数和梯度评估数，对于Hessian-free方法，以及$\mathcal{O}( n^{3&#x2F;2} \epsilon^{-3&#x2F;2} )$函数评估数，对于derivative-free方法。这些复杂度上限在先前知道的joint dependence on $n$和$\epsilon$上有所改善。<details>
<summary>Abstract</summary>
In this work, we develop first-order (Hessian-free) and zero-order (derivative-free) implementations of the Cubically regularized Newton method for solving general non-convex optimization problems. For that, we employ finite difference approximations of the derivatives. We use a special adaptive search procedure in our algorithms, which simultaneously fits both the regularization constant and the parameters of the finite difference approximations. It makes our schemes free from the need to know the actual Lipschitz constants. Additionally, we equip our algorithms with the lazy Hessian update that reuse a previously computed Hessian approximation matrix for several iterations. Specifically, we prove the global complexity bound of $\mathcal{O}( n^{1/2} \epsilon^{-3/2})$ function and gradient evaluations for our new Hessian-free method, and a bound of $\mathcal{O}( n^{3/2} \epsilon^{-3/2} )$ function evaluations for the derivative-free method, where $n$ is the dimension of the problem and $\epsilon$ is the desired accuracy for the gradient norm. These complexity bounds significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$, for the first-order and zeroth-order non-convex optimization.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们开发了第一顺（无积分梯度法）和零顺（无导数法）实现方法，用于解决通用非凸优化问题。我们使用finite difference方法来估算derivatives。我们使用特殊的适应搜索过程，以同时适应正则化常数和finite difference估算的参数。这使得我们的算法不需要知道实际的Lipschitz常数。此外，我们将我们的算法备备了懒散的Hessian更新，可以重用多个迭代中计算的Hessian近似矩阵。我们证明了新的Hessian-free方法的全球复杂度上限为 $\mathcal{O}(n^{1/2} \epsilon^{-3/2})$ 函数和导数评估数，其中 $n$ 是问题的维度，$\epsilon$ 是求导数评估的精度。这些复杂度上限在之前已知的joint $n$ 和 $\epsilon$ 的依赖关系方面具有显著改善。
</details></li>
</ul>
<hr>
<h2 id="Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices"><a href="#Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices" class="headerlink" title="Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"></a>Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02411">http://arxiv.org/abs/2309.02411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei Zhang</li>
<li>for: 本研究提出了Delta-LoRA，一种能够高效地 Parametric efficient fine-tune大语言模型（LLMs）的新方法。</li>
<li>methods: Delta-LoRA不仅更新了低级矩阵 $\bA$ 和 $\bB$，还将学习传播到预训练 веса $\bW$ 上，通过两低级矩阵的乘积 delta（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来进行更新。这种策略有效地解决了递归更新低级矩阵的限制，使得 delta-LoRA 可以学习出适合下游任务的表示。</li>
<li>results: 对比LoRA和其他低级适应方法，Delta-LoRA在多种任务上表现出色，并且与LoRA相比，Delta-LoRA具有相同的内存需求和计算成本。<details>
<summary>Abstract</summary>
In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\bA$ and $\bB$, but also propagate the learning to the pre-trained weights $\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\bW$ does not need to compute the gradients of $\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Delta-LoRA，这是一种新的精简型 parameter-efficient 方法，用于精细调整大型语言模型（LLM）。与LoRA和其他低级别适应方法，如AdaLoRA，相比，Delta-LoRA不仅更新了低级别矩阵 $\bA$ 和 $\bB$，还通过利用低级别矩阵 $\bA^{(t+1)}$ 和 $\bB^{(t+1)}$ 的乘积的 delta 进行更新，从而有效地解决了适应过程中低级别矩阵的增量更新是不够的问题。此外，因为更新 $\bW$ 不需要计算 $\bW$ 的梯度和存储它们的动量，Delta-LoRA 与 LoRA 的内存需求和计算成本相似。实验结果表明，Delta-LoRA 明显超过了现有的低级别适应方法。我们还提供了详细的分析，以证明 Delta-LoRA 的效果。
</details></li>
</ul>
<hr>
<h2 id="In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms"><a href="#In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms" class="headerlink" title="In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms"></a>In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02393">http://arxiv.org/abs/2309.02393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schilk, Niccolò Polvani, Andrea Ronco, Milos Cernak, Michele Magno</li>
<li>for: 减少耳机中的噪音和扭曲声音，提高耳机中的语音识别率。</li>
<li>methods: 使用新型MEMS骨尖振荡 Microphone，实现个性化语音活动检测和更高精度的声音提升应用。基于骨尖振荡数据和循环神经网络的低功耗个性化语音检测算法，与传统 Microphone 输入的方法进行比较。</li>
<li>results: 使用骨尖振荡 Microphone 实现的个性化语音检测算法，可以在 12.8ms 内准确地检测到语音，准确率为 95%。使用 Ambiq Apollo 4 Blue SoC 实现的最终实现可以在 2.64mW 的平均功耗下达到 43h 的电池寿命，没有循环征逐。<details>
<summary>Abstract</summary>
The recent ubiquitous adoption of remote conferencing has been accompanied by omnipresent frustration with distorted or otherwise unclear voice communication. Audio enhancement can compensate for low-quality input signals from, for example, small true wireless earbuds, by applying noise suppression techniques. Such processing relies on voice activity detection (VAD) with low latency and the added capability of discriminating the wearer's voice from others - a task of significant computational complexity. The tight energy budget of devices as small as modern earphones, however, requires any system attempting to tackle this problem to do so with minimal power and processing overhead, while not relying on speaker-specific voice samples and training due to usability concerns.   This paper presents the design and implementation of a custom research platform for low-power wireless earbuds based on novel, commercial, MEMS bone-conduction microphones. Such microphones can record the wearer's speech with much greater isolation, enabling personalized voice activity detection and further audio enhancement applications. Furthermore, the paper accurately evaluates a proposed low-power personalized speech detection algorithm based on bone conduction data and a recurrent neural network running on the implemented research platform. This algorithm is compared to an approach based on traditional microphone input. The performance of the bone conduction system, achieving detection of speech within 12.8ms at an accuracy of 95\% is evaluated. Different SoC choices are contrasted, with the final implementation based on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW average power consumption at 14uJ per inference, reaching 43h of battery life on a miniature 32mAh li-ion cell and without duty cycling.
</details>
<details>
<summary>摘要</summary>
最近广泛的远程会议普及，伴随着各种不清晰或扭曲的声音通话的感知不满。声音增强可以补做小型真 wireless earbuds 的低质量输入信号，通过应用隐藏噪声技术。这种处理需要快速响应，并能够分辨使用者的声音和其他声音。这是计算机科学的复杂任务。由于现代耳机的能量预算很低，任何系统都不能依赖于使用者的声音样本和训练。这篇论文描述了一个自定义研究平台，基于新型 MEMS 骨尘扬icrophones。这些 microphones 可以记录使用者的说话，并提供个性化声音活动检测和其他声音增强应用。此外，论文还评估了一种低功耗个性化speech检测算法，基于骨尘数据和回归神经网络。这种算法与传统 Mikrophone 输入的方法进行比较。骨尘系统的性能，包括检测speech within 12.8ms 的精度为 95%，被评估。不同的 SoC 选择被比较，最终实现基于cutting-edge Ambiq Apollo 4 Blue SoC 的实现，具有2.64mW 的平均电功耗和14uJ 的推理消耗，可以在32mAh 的锂离子电池上支持43h 的电池寿命，无需循环停止。
</details></li>
</ul>
<hr>
<h2 id="Explaining-grokking-through-circuit-efficiency"><a href="#Explaining-grokking-through-circuit-efficiency" class="headerlink" title="Explaining grokking through circuit efficiency"></a>Explaining grokking through circuit efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02390">http://arxiv.org/abs/2309.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar</li>
<li>for: 本研究旨在解释神经网络通用化的一个意外的谜题：吸收（grokking）。</li>
<li>methods: 本文提出了一种解释吸收的理论，即任务存在一个通用解和一个记忆解，其中通用解需要更长的学习时间但更高效，生成更大的logits具有相同的参数范数。</li>
<li>results: 本文提出了四个新预测，并对吸收进行了证明。其中最引人注目的是两种新的行为：半吸收和半通用。半吸收指的是网络从完美到低测试准确率的回卷，而半通用指的是网络在部分测试数据上显示延迟的通用。<details>
<summary>Abstract</summary>
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.
</details>
<details>
<summary>摘要</summary>
一种非常有趣的神经网络泛化问题是"grokking"：一个网络在完美训练后却表现出低泛化性能。我们提出，grokking发生在任务允许一种泛化解决方案和一种记忆解决方案，其中泛化解决方案需要更长时间学习，但生成更大的logits，同样的参数范数。我们假设记忆ircuits在更大的训练集上变得更不效率，而泛化ircuits不变。这表明存在一个关键的训练集大小，在这个大小上，记忆和泛化是相同效率的。我们提出并证明了四个新预测，提供了重要的证据支持我们的解释。最引人注目的是我们发现了两种新和意外的行为：ungrokking，在完美训练后网络测试准确率下降，以及半grokking，在部分测试集上网络显示延迟的泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation"><a href="#A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation" class="headerlink" title="A Lightweight and Transferable Design for Robust LEGO Manipulation"></a>A Lightweight and Transferable Design for Robust LEGO Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02354">http://arxiv.org/abs/2309.02354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Liu, Yifan Sun, Changliu Liu</li>
<li>for: 这个论文旨在解决机器人LEGO搬运问题，即使用硬件-软件共设以实现安全高效的LEGO搬运。</li>
<li>methods: 该论文使用了进化策略来安全优化机器人运动，并设计了一个终端工具（EOAT），以降低问题维度并使大型工业机器人更容易搬运LEGO brick。</li>
<li>results: 实验表明，EOAT可靠地进行LEGO brick搬运，而学习框架可以efficiently和安全地提高搬运性能，达到100%成功率。该共设在多个机器人（FANUC LR-mate 200id&#x2F;7L和Yaskawa GP4）上进行了多机器人演示，显示其可以准确地搬运不同的LEGO拼接品。<details>
<summary>Abstract</summary>
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.
</details>
<details>
<summary>摘要</summary>
LEGO 是一个很好的平台 для板图物体的原型设计。然而，机器人LEGO 板图处理（即LEGO 积木的操作）具有紧密的连接和精度要求，这使得机器人LEGO 板图处理变得困难。这篇论文探讨了安全和高效的机器人LEGO 板图处理方法。特别是，这篇论文通过硬件软件共设计来降低处理复杂性。一个端部工具（EOAT）被设计出来，可以减少问题维度，使大型工业机器人更容易地操作LEGO 积木。此外，这篇论文使用进化策略来安全地优化机器人的运动路径，以实现100%的成功率。实验表明，EOAT可靠地操作LEGO 积木，并且学习框架可以安全地提高操作性能。此外，我们在多个机器人（即FANUC LR-mate 200id/7L和Yaskawa GP4）上部署了这种共设计，以示其通用性和传输性。最后，我们显示了我们的解决方案可以实现可持续的机器人LEGO 板图处理，在这种情况下，机器人可以不断地组装和解 assemble不同的原型。
</details></li>
</ul>
<hr>
<h2 id="Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics"><a href="#Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics" class="headerlink" title="Exact Inference for Continuous-Time Gaussian Process Dynamics"></a>Exact Inference for Continuous-Time Gaussian Process Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02351">http://arxiv.org/abs/2309.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 本文旨在学习连续时间动力系统。</li>
<li>methods: 本文使用高斯过程（GP）动力模型学习方法，并利用多步和泰勒 интегратор来实现直接推断。</li>
<li>results: 本文提出了一种flexible推断方案，可以准确表示连续时间动力系统。经验和理论分析表明，该方法可以准确表示连续时间动力系统。<details>
<summary>Abstract</summary>
Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators. We demonstrate how to derive flexible inference schemes for these types of integrators. Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.
</details>
<details>
<summary>摘要</summary>
Physical systems can often be described using a continuous-time dynamical system. In practice, the true system is often unknown and must be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties must be conserved. Therefore, we aim to learn a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps, making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. To make direct inference tractable, we propose to leverage multistep and Taylor integrators. We derive how to derive flexible inference schemes for these types of integrators. Furthermore, we derive tailored sampling schemes that allow drawing consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.Translated by Google Translate with some modifications to improve readability.
</details></li>
</ul>
<hr>
<h2 id="PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference"><a href="#PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference" class="headerlink" title="PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference"></a>PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02334">http://arxiv.org/abs/2309.02334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Andronic, George A. Constantinides</li>
<li>for: 这个论文主要用于描述一种基于多变量多项式的神经网络训练方法，以实现在场程序可编程阵列（FPGAs）上进行深度学习推理的快速实现。</li>
<li>methods: 该方法使用多变量多项式作为神经网络的基本构建块，利用FPGA的柔logic隐藏多项式评估所带来的额外开销。</li>
<li>results: 该方法可以在三个任务中实现同等准确性，但使用许多 fewer layers of soft logic，从而实现了明显的响应时间和面积改进。<details>
<summary>Abstract</summary>
Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset.
</details>
<details>
<summary>摘要</summary>
“Field-programmable gate arrays (FPGAs) 广泛应用于深度学习推理。标准深度神经网络推理包括交叠的线性映射和非线性活动函数的计算。现有的工作是将线性映射和非线性活动函数硬编码到 FPGA  lookup tables (LUTs) 中。我们的工作是基于线性映射和非线性活动函数可以在 FPGA 中实现更多的功能的想法。在这篇论文中，我们提出了一种新的方法，用多变量波动函数作为深度神经网络的基本构建块进行训练。我们的方法利用了FPGA 的软逻辑的灵活性，将波动函数评估隐藏在 LUTs 中，无损耗。我们表明，使用波动构建块可以与使用线性函数相比，减少许多层次软逻辑的数量，导致了显著的延迟和面积改善。我们在三个任务中证明了这种方法的有效性：网络侵入检测、CERN Large Hadron Collider 中的截割识别和 MNIST 数据集上的手写数字识别。”
</details></li>
</ul>
<hr>
<h2 id="Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source"><a href="#Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source" class="headerlink" title="Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source"></a>Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02333">http://arxiv.org/abs/2309.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Humble, William Colocho, Finn O’Shea, Daniel Ratner, Eric Darve</li>
<li>for: 这个论文旨在运用深度学习进行异常检测，并且不需要正常训练集（即没有异常数据）或完整的标签训练集。</li>
<li>methods: 这篇论文提出了一种名为Resilient Variational Autoencoder（ResVAE）的深度生成模型，它能够在对于异常数据进行训练时，抵制异常数据的影响，并提供特征层级的异常属性。</li>
<li>results: 在SLAC Linac Coherent Light Source（LCLS）的加速器状态测量系统中应用ResVAE方法，可以实现各种不同类型的异常检测。<details>
<summary>Abstract</summary>
Significant advances in utilizing deep learning for anomaly detection have been made in recent years. However, these methods largely assume the existence of a normal training set (i.e., uncontaminated by anomalies) or even a completely labeled training set. In many complex engineering systems, such as particle accelerators, labels are sparse and expensive; in order to perform anomaly detection in these cases, we must drop these assumptions and utilize a completely unsupervised method. This paper introduces the Resilient Variational Autoencoder (ResVAE), a deep generative model specifically designed for anomaly detection. ResVAE exhibits resilience to anomalies present in the training data and provides feature-level anomaly attribution. During the training process, ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS). By utilizing shot-to-shot data from the beam position monitoring system, we demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习在异常检测方面作出了 significante进步。然而，这些方法假设存在一个正常的训练集（即不受异常影响）或者完全标注的训练集。在许多复杂的工程系统中，如粒子加速器，标签是稀缺的并且昂贵的；因此，在这些情况下，我们必须放弃这些假设，并使用一种完全无监督的方法进行异常检测。这篇文章介绍了一种名为 Resilient Variational Autoencoder（ResVAE）的深度生成模型，特别是设计用于异常检测。ResVAE具有对异常示例的抗性，并提供了每个样本和每个特征的异常投入量。在训练过程中，ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS) by utilizing shot-to-shot data from the beam position monitoring system. We demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details></li>
</ul>
<hr>
<h2 id="Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations"><a href="#Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations" class="headerlink" title="Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations"></a>Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02332">http://arxiv.org/abs/2309.02332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin N. P. Nilsson</li>
<li>for: 这个论文的目的是探索脑中神经元群体的准确编码和运算方法。</li>
<li>methods: 该论文使用了一种现代化的神经元模型，并利用了这个模型的准确性来描述神经元群体的运算方法。</li>
<li>results: 该论文发现，神经元群体可以被视为一种低级编程语言中的操作符，并且可以通过不同的连接来实现多种操作，如特殊化、泛化、新鲜性检测、维度减少、逆模型、预测和关联记忆等。<details>
<summary>Abstract</summary>
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensionality reduction, inverse modeling, prediction, and associative memory. In broader terms, this work illuminates the potential of matrix embeddings in advancing our understanding in fields like cognitive science and AI. These embeddings enhance the capacity for concept processing and hierarchical description over their vector counterparts.
</details>
<details>
<summary>摘要</summary>
在哺乳动物中心神经系统的复杂建筑中，神经细胞形成群体。轴突集合通过快速列表传输信息。然而，这些神经细胞群体的精确编码和操作仍未得到了发现。在我们的分析中，起始点是一种现代机制模型，这种模型拥有пластично性。从这个简单的框架中，出现了一种深刻的数学构造：神经细胞群体可以精确地表示和操纵信息，这可以通过一种Finite convex cone的代数来表示。此外，这些神经细胞群体不仅是被动的传输器。它们作为代数中的操作符，模仿低级编程语言的功能。当这些群体相互连接时，它们实现了简洁却强大的代数表达。这些网络允许它们实现许多操作，如特殊化、通用化、发现新的特征、维度减少、反向模型、预测和相关记忆。在更广泛的意义上，这项工作推照了矩阵嵌入在认知科学和人工智能领域的潜在作用。这些嵌入可以提高概念处理和层次描述的能力，而不是 vector 的对应。
</details></li>
</ul>
<hr>
<h2 id="SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction"><a href="#SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction" class="headerlink" title="SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction"></a>SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02320">http://arxiv.org/abs/2309.02320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sixu0/SeisCLIP">https://github.com/sixu0/SeisCLIP</a></li>
<li>paper_authors: Xu Si, Xinming Wu, Hanlin Sheng, Jun Zhu, Zefeng Li</li>
<li>for: 这篇论文是为了提出一个基础模型，以应对各种地震学任务中的欠缺标签资料和地区对应性问题。</li>
<li>methods: 这篇论文使用了对比学习方法，将多Modal资料集合进行联合预训。具体来说，它使用了一个对应TransformerEncoder来提取时间频率地震谱中的重要特征，以及一个MLPEncoder来联合事件的相位和来源信息。</li>
<li>results: 这篇论文的结果显示，SeisCLIP的性能在不同区域的事件分类、地点定位和聚集机制分析任务中，都大于基准方法的性能。特别是，SeisCLIP可以在不同区域的资料上进行 Transfer Learning，并且可以对应不同的地震谱和事件类型。<details>
<summary>Abstract</summary>
Training specific deep learning models for particular tasks is common across various domains within seismology. However, this approach encounters two limitations: inadequate labeled data for certain tasks and limited generalization across regions. To address these challenges, we develop SeisCLIP, a seismology foundation model trained through contrastive learning from multi-modal data. It consists of a transformer encoder for extracting crucial features from time-frequency seismic spectrum and an MLP encoder for integrating the phase and source information of the same event. These encoders are jointly pre-trained on a vast dataset and the spectrum encoder is subsequently fine-tuned on smaller datasets for various downstream tasks. Notably, SeisCLIP's performance surpasses that of baseline methods in event classification, localization, and focal mechanism analysis tasks, employing distinct datasets from different regions. In conclusion, SeisCLIP holds significant potential as a foundational model in the field of seismology, paving the way for innovative directions in foundation-model-based seismology research.
</details>
<details>
<summary>摘要</summary>
通常在地震学中特定任务上训练特定的深度学习模型是常见的。然而，这种方法有两个限制：不够的标注数据 для某些任务和地区之间的限制。为解决这些挑战，我们开发了SeisCLIP，一个基于对比学习的地震学基础模型。它包括一个转换器编码器，用于提取时频地震谱中的关键特征，以及一个多层感知编码器，用于将相关的阶段和来源信息相结合。这两个编码器在大量数据集上共同培养，并且spectrum编码器在小型数据集上进行精细调整，以便在不同地区的下游任务中进行最佳化。值得注意的是，SeisCLIP的性能超过了基eline方法在事件分类、地点归属和焦点机制分析等任务中的性能，使用不同地区的数据集。总之，SeisCLIP具有广泛的应用前景，可能成为地震学研究中的基础模型，开拓出新的方向。
</details></li>
</ul>
<hr>
<h2 id="A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction"><a href="#A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction" class="headerlink" title="A study on the impact of pre-trained model on Just-In-Time defect prediction"></a>A study on the impact of pre-trained model on Just-In-Time defect prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02317">http://arxiv.org/abs/2309.02317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W. K. Chan, Bo Jiang</li>
<li>for: 本研究主要针对Just-In-Time（JIT）缺陷预测任务中，前期研究者主要关注单个预训模型的性能，而忽略了不同预训模型之间的关系。</li>
<li>methods: 本研究建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都使用了不同的预训模型作为 backing。我们系统地探讨这些模型之间的差异和连接。</li>
<li>results: 我们发现，使用 Commit 代码和 Commit 消息作为输入，每个模型都显示出了改进。此外，我们还发现，当预训模型的类型相似时，需要消耗的训练资源几乎相同。此外，我们还发现，在零shot和几shot情况下，不同的预训模型在缺陷检测中表现出色。这些结果为JIT缺陷预测任务中使用预训模型优化提供新的视角，并 highlight了需要更多注意的因素。<details>
<summary>Abstract</summary>
Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data.
</details>
<details>
<summary>摘要</summary>
previous researchers 在 Just-In-Time（JIT）缺陷预测任务中主要关注各种预训练模型的性能，而未探讨不同预训练模型之间的关系。在这个研究中，我们建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都有不同的预训练模型作为其背景。我们系统地探讨这些模型之间的差异和联系。特别是，我们研究使用 Commit 代码和 Commit 消息作为输入时模型的性能，以及这些模型在不同训练资源的情况下的关系。此外，我们进行了一项排除实验，以探讨每个模型对输入的敏感性。此外，我们还研究了这些模型在零处和几处场景下的性能。我们的发现表明，每个模型基于不同的背景都有改进，而当背景模型的预训练类似时，需要消耗的训练资源很接近。我们还发现， Commit 代码在缺陷检测中发挥了重要作用，不同的预训练模型在均衡数据集下的几处场景下表现出不同的缺陷检测能力。这些发现为优化 JIT 缺陷预测任务中使用预训练模型提供了新的视角，并高亮需要更多注意的因素。此外， CodeGPTJIT 和 GPT2JIT 在两个数据集上分别以 2000 个训练样本的情况下表现出了更好的性能，这些发现强调了基于 transformer 预训练模型的 JIT 缺陷预测任务的效iveness，特别是有限的训练数据 scenarios。
</details></li>
</ul>
<hr>
<h2 id="Graph-Self-Contrast-Representation-Learning"><a href="#Graph-Self-Contrast-Representation-Learning" class="headerlink" title="Graph Self-Contrast Representation Learning"></a>Graph Self-Contrast Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02304">http://arxiv.org/abs/2309.02304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GRAND-Lab/MERIT">https://github.com/GRAND-Lab/MERIT</a></li>
<li>paper_authors: Minjie Chen, Yao Cheng, Ye Wang, Xiang Li, Ming Gao</li>
<li>for: 提高图像表示学习的效果和效率。</li>
<li>methods: 提出一种新的图像自我对比框架GraphSC，只使用一个正例和一个负例，使用 triplet loss 作为目标函数。</li>
<li>results: 通过对 19 种现有方法进行比较，证明 GraphSC 在无监督和传输学习 Setting 中表现出色。<details>
<summary>Abstract</summary>
Graph contrastive learning (GCL) has recently emerged as a promising approach for graph representation learning. Some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. For those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. All these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. In this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. Specifically, self-contrast has two implications. First, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. Second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. Further, Since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. Therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. Finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
graph contrastive learning (GCL) 近期 emerged as a promising approach for graph representation learning. some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. for those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. all these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. in this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. specifically, self-contrast has two implications. first, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. further, since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview"><a href="#Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview" class="headerlink" title="Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview"></a>Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02478">http://arxiv.org/abs/2309.02478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Grassucci, Yuki Mitsufuji, Ping Zhang, Danilo Comminiello</li>
<li>for: 本研究探讨了 semantic communication 在Future AI-driven communication systems 中的潜在作用，以及如何使用 deep generative models  Addressing  semantic communication challenges.</li>
<li>methods: 本研究使用了 deep generative models 来解决 semantic communication 中的 complex data 提取和处理 semantic information 的挑战，并在实际应用中提供了 novel research pathways  для future semantic communication frameworks.</li>
<li>results: 本研究显示了 deep generative models 可以帮助 enhance semantic communication frameworks 在 dealing with real-world complex data 和 channel corruptions 上，并提供了 novel research pathways  для future semantic communication frameworks.<details>
<summary>Abstract</summary>
Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTSemantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.TRANSLATE_TEXTSemantic communication is ready to play a crucial role in shaping the future of AI-driven communication systems. The challenge of extracting semantic information from complex content and generating consistent data at the receiver, while being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper reveals the challenges of semantic communication from a machine learning perspective and shows how deep generative models will significantly improve semantic communication frameworks in dealing with real-world complex data, extracting and utilizing semantic information, and being robust to channel corruptions. The paper also explores new research paths for the next generation of semantic communication frameworks.
</details></li>
</ul>
<hr>
<h2 id="Inferring-effective-couplings-with-Restricted-Boltzmann-Machines"><a href="#Inferring-effective-couplings-with-Restricted-Boltzmann-Machines" class="headerlink" title="Inferring effective couplings with Restricted Boltzmann Machines"></a>Inferring effective couplings with Restricted Boltzmann Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02292">http://arxiv.org/abs/2309.02292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs">https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs</a></li>
<li>paper_authors: Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</li>
<li>for: 该研究旨在提出一种简单的解决方案，以便更好地理解生成模型的物理含义。</li>
<li>methods: 该研究使用了一种直接映射Restricted Boltzmann Machine的能量函数和有效的Isling振荡 Hamiltonian，包括高阶交互between spins。</li>
<li>results: 研究结果表明，该方法能够有效地学习正确的交互网络，并可以应用于模型复杂数据集。同时，研究还评估了不同的训练方法的影响于模型质量。<details>
<summary>Abstract</summary>
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier work attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To validate our method, we perform several controlled numerical experiments where the training samples are equilibrium samples of predefined models containing local external fields, two-body and three-body interactions in various low-dimensional topologies. The results demonstrate the effectiveness of our proposed approach in learning the correct interaction network and pave the way for its application in modeling interesting datasets. We also evaluate the quality of the inferred model based on different training methods.
</details>
<details>
<summary>摘要</summary>
<?xml:namespace prefix = "o" ns = "urn:schemas-microsoft-com:office:office" />生成模型提供了一种直接模型复杂数据的方式。其中，能量基模型为我们提供了一种基于神经网络的模型，该模型目标是在数据中识别所有统计相关性，并在模型级别达到 Boltzmann 权重。然而，一个挑战是理解这些模型的物理含义。在这项研究中，我们提出了一种简单的解决方案，即在能量函数上进行直接映射，以实现一个包含高阶交互的有效牛顿矩阵 Hamiltonian。这种映射包括所有可能的阶数交互，超出了传统的对应 inverse Ising 方法中考虑的对称交互，并允许描述复杂的数据集。过去的工作已经尝试了这种目标，但是提出的映射没有正确地处理问题的复杂性或者没有直接的实践指导。为验证我们的方法，我们在各种控制的数字实验中进行了许多数据训练，其中训练样本是已知模型中的平衡样本，包括局部外场、二体和三体交互在不同的低维度拓扑上。结果表明我们的提posed方法可以有效地学习正确的交互网络，并为模型化有趣的数据集铺平道路。我们还评估了不同的训练方法的影响。
</details></li>
</ul>
<hr>
<h2 id="Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes"><a href="#Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes" class="headerlink" title="Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes"></a>Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02286">http://arxiv.org/abs/2309.02286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Lorenz, Florian Barthel, Daniel Kienzle, Rainer Lienhart</li>
<li>For: 这个论文是为了构建一个新的Scene Graph Dataset和一组测试 metrics，以便测试Scene Graph生成模型在罕见 predicate class 上的预测性能。* Methods: 作者提出了一个模型协助的注意力Annotation管道，以高效地找到图像中的罕见 predicate class。这个管道使用了一些新的技术，如模型预测和人工纠正，以提高注意力Annotation的准确性和效率。* Results: 作者通过对 Haystack Dataset 进行测试，发现这个 dataset 可以帮助提高Scene Graph生成模型在罕见 predicate class 上的预测性能。此外，Haystack  Dataset 还包含了 Explicit Negative Annotations，这些 Annotations 可以帮助改进现有的Scene Graph生成模型。<details>
<summary>Abstract</summary>
Current scene graph datasets suffer from strong long-tail distributions of their predicate classes. Due to a very low number of some predicate classes in the test sets, no reliable metrics can be retrieved for the rarest classes. We construct a new panoptic scene graph dataset and a set of metrics that are designed as a benchmark for the predictive performance especially on rare predicate classes. To construct the new dataset, we propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack.   Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class. Negative annotations are helpful especially in the field of scene graph generation and open up a whole new set of possibilities to improve current scene graph generation models.   Haystack is 100% compatible with existing panoptic scene graph datasets and can easily be integrated with existing evaluation pipelines. Our dataset and code can be found here: https://lorjul.github.io/haystack/. It includes annotation files and simple to use scripts and utilities, to help with integrating our dataset in existing work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt"><a href="#PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt" class="headerlink" title="PromptTTS 2: Describing and Generating Voices with Text Prompt"></a>PromptTTS 2: Describing and Generating Voices with Text Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02285">http://arxiv.org/abs/2309.02285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian</li>
<li>for: 这个论文是用于解决传统文本至语音（TTS）方法中的问题，即使用文本提示（描述）而不是语音提示（参考语音）来提供语音变化的方法。</li>
<li>methods: 这篇论文提出了一种新的TTS方法，即PromptTTS 2，它使用了变化网络来提供不同语音变化的信息，以及一个提取语音特征的模型来生成高质量的文本提示。</li>
<li>results: 实验结果表明，Compared to先前的工作，PromptTTS 2可以更好地根据文本提示生成语音，并且支持采样多种语音变化，从而给用户提供更多的选择。此外，生成提示管道可以生成高质量的提示，从而减少大量标注成本。<details>
<summary>Abstract</summary>
Speech conveys more information than just text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompt for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompt based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available online\footnote{https://speechresearch.github.io/prompttts2}.
</details>
<details>
<summary>摘要</summary>
文本可以传递更多信息than just text, because the same word can be spoken in different voices to convey diverse information. 与传统的文本到语音（TTS）方法相比，使用文本提示（描述）更 user-friendly，因为语音提示可能困难或者不存在。 TTS 方法基于文本提示面临两个挑战：1）一个多对一问题，因为文本提示中不能完全捕捉语音变化的所有细节; 2）限制性的文本提示数据集，需要供应商和大量的数据标注成本来写文本提示 для语音。在这种工作中，我们介绍PromptTTS 2，用以解决这两个挑战。PromptTTS 2 使用变化网络提供不同语音变化的信息，并使用提取大量语言模型（LLM）来组成高质量文本提示。具体来说，变化网络预测基于参考语音（含有全部语音信息）的表示，基于文本提示表示来预测。 для提取大量语言模型，它生成语音提示，并使用语音理解模型来识别语音特征（例如，性别、速度）。实验表明，相比之前的工作，PromptTTS 2 可以更好地根据文本提示生成语音，并支持采样多种语音变化，从而为用户提供更多的选择。此外，提取大量语言模型可以消除大量标注成本。PromptTTS 2 的示例页面可以在线浏览（https://speechresearch.github.io/prompttts2）。
</details></li>
</ul>
<hr>
<h2 id="s-ID-Causal-Effect-Identification-in-a-Sub-Population"><a href="#s-ID-Causal-Effect-Identification-in-a-Sub-Population" class="headerlink" title="s-ID: Causal Effect Identification in a Sub-Population"></a>s-ID: Causal Effect Identification in a Sub-Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02281">http://arxiv.org/abs/2309.02281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Mohammad Abouei, Ehsan Mokhtarian, Negar Kiyavash</li>
<li>for: 本研究旨在解决在特定子population中 causal inference问题，即从观察数据中推断特定子population中 causal effect。</li>
<li>methods: 本研究使用了 necessary and sufficient conditions和一种sound and complete algorithm来解决这个问题。</li>
<li>results: 本研究提供了一个可靠的方法来在特定子population中推断 causal effect，并且可以应用于现有的causal inference方法中。<details>
<summary>Abstract</summary>
Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details>
<details>
<summary>摘要</summary>
causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.Here's the translation in Traditional Chinese:causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Residual-based-Methods-on-Fault-Detection"><a href="#A-Comparison-of-Residual-based-Methods-on-Fault-Detection" class="headerlink" title="A Comparison of Residual-based Methods on Fault Detection"></a>A Comparison of Residual-based Methods on Fault Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02274">http://arxiv.org/abs/2309.02274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-Ching Hsu, Gaetan Frusque, Olga Fink</li>
<li>for: 这篇论文主要针对复杂工业系统中的故障检测进行了一个初步的研究，即获得系统的健康状况的理解。</li>
<li>methods: 这篇论文使用了两种基于差异的方法：自适应神经网络和输入输出模型。</li>
<li>results: 研究发现，两种方法都能够在20个循环后检测到故障，并且具有低的假阳性率。而输入输出模型提供了更好的解释能力，即可能出现的故障类型和可能受损的组件。<details>
<summary>Abstract</summary>
An important initial step in fault detection for complex industrial systems is gaining an understanding of their health condition. Subsequently, continuous monitoring of this health condition becomes crucial to observe its evolution, track changes over time, and isolate faults. As faults are typically rare occurrences, it is essential to perform this monitoring in an unsupervised manner. Various approaches have been proposed not only to detect faults in an unsupervised manner but also to distinguish between different potential fault types. In this study, we perform a comprehensive comparison between two residual-based approaches: autoencoders, and the input-output models that establish a mapping between operating conditions and sensor readings. We explore the sensor-wise residuals and aggregated residuals for the entire system in both methods. The performance evaluation focuses on three tasks: health indicator construction, fault detection, and health indicator interpretation. To perform the comparison, we utilize the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model, specifically a subset of the turbofan engine dataset containing three different fault types. All models are trained exclusively on healthy data. Fault detection is achieved by applying a threshold that is determined based on the healthy condition. The detection results reveal that both models are capable of detecting faults with an average delay of around 20 cycles and maintain a low false positive rate. While the fault detection performance is similar for both models, the input-output model provides better interpretability regarding potential fault types and the possible faulty components.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>复杂的工业系统的故障检测的初步是了解它们的健康状况。因此，对这些健康状况的连续监测成为了非常重要的，以观察其演化，跟踪时间变化，并孤立故障。由于故障通常是罕见的，因此需要在无监督的情况下进行监测。多种方法已经被提议，不仅可以检测故障，还可以将不同的可能的故障类型分类出来。在这种研究中，我们进行了对两种剩余基于方法的比较：自适应神经网络和输入输出模型，它们可以将操作条件和传感器读数之间建立映射。我们分析整个系统的残差和传感器级别的残差，并对三个任务进行评估：健康指标的建构、故障检测和健康指标的解释。为了进行比较，我们使用了商用模块式飞航发动机模型（C-MAPSS）的动力学模型，具体是一个涉及到三种故障类型的发动机数据集。所有模型都是在健康状况下训练的。在检测故障的过程中，我们采用了一个基于健康状况的阈值，以确定是否存在故障。结果显示，两种模型都可以在约20个循环后检测故障，并保持低的假阳性率。虽然两种模型的故障检测性能相似，但输入输出模型提供了更好的可解释性，即可能的故障类型和可能的异常组件。
</details></li>
</ul>
<hr>
<h2 id="Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette"><a href="#Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette" class="headerlink" title="Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette"></a>Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02272">http://arxiv.org/abs/2309.02272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlevinwork/GB-AFS">https://github.com/davidlevinwork/GB-AFS</a></li>
<li>paper_authors: David Levin, Gonen Singer</li>
<li>For: 本研究旨在提出一种基于图的筛选方法，用于自动选择多类分类任务中的特征。* Methods: 该方法使用Jeffries-Matusita距离和t-分布随机邻域抽象（t-SNE）生成一个低维度空间，反映每个特征在每对类之间的分化能力。使用我们提出的新的简单化的抽象指数（MSS）来选择最小的特征数量。* Results: 实验结果表明，提案的GB-AFS方法在公共数据集上表现出优于其他筛选基于技术和自动特征选择方法。此外，GB-AFS方法可以维持使用所有特征时的准确率，只使用7%-30%的特征，从而实现了时间需要进行分类的减少，从15%下降到70%。<details>
<summary>Abstract</summary>
This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks. The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes. It does not require any user-defined parameters such as the number of features to select. The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task. Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\%$ to $30\%$ of the features. Consequently, this resulted in a reduction of the time needed for classifications, from $15\%$ to $70\%$.
</details>
<details>
<summary>摘要</summary>
The methodology employs the Jeffries-Matusita distance and t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using the newly developed Mean Simplified Silhouette (MSS) index, designed to evaluate the clustering results for the feature selection task.Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. The proposed algorithm achieved the same accuracy as using all features, while using only $7\%$ to $30\%$ of the features, resulting in a reduction of the time needed for classifications from $15\%$ to $70\%$.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning"><a href="#Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning" class="headerlink" title="Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning"></a>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02476">http://arxiv.org/abs/2309.02476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Chen Liu, Chenlu Ye, Qing Lian, Yuan Yao, Tong Zhang</li>
<li>for: 这个研究旨在提出一个可靠地选择丛集和活跃学习的方法，以减少深度学习中大量标签的成本。</li>
<li>methods: 我们提出了一个理论上的最佳解决方案，即COPS（uncertainty based Optimal Sub-sampling）方法，可以最小化深度学习模型在抽样后的预期损失。</li>
<li>results: 我们通过实验证明了COPS方法的有效性，与基准方法相比，它在深度学习任务中表现出色。<details>
<summary>Abstract</summary>
Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).   In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampling ratio. This sampling ratio is closely associated with model uncertainty and can be effectively applied to deep learning tasks. Furthermore, we address the challenge of model sensitivity to misspecification by incorporating a down-weighting approach for low-density samples, drawing inspiration from previous works.   To assess the effectiveness of our proposed method, we conducted extensive empirical experiments using deep neural networks on benchmark datasets. The results consistently showcase the superior performance of COPS compared to baseline methods, reaffirming its efficacy.
</details>
<details>
<summary>摘要</summary>
现代深度学习强调大量标注数据，往往带来高的人工标注和计算成本。为了解决这些挑战，研究人员探索了有用的子集选择技术，包括核心集选择和活动学习。特别是，核心集选择是采样数据中的输入($\bx$)和输出($\by$)。在这项研究中，我们提出了在线性软max回归方面的理论优化解决方案，即COPS（不确定性基于最优子集选择）。该方法旨在将模型训练过的子集数据的预期损失最小化。不同于现有方法，我们不使用直接计算 inverse covariance matrix的方法，而是利用模型的归一化值来估算抽样比率。这个抽样比率与模型不确定性密切相关，可以有效应用于深度学习任务。此外，我们还解决了模型偏置低概率样本的挑战，通过引入下降因子方法， drawing inspiration from previous works。为评估我们提出的方法的效果，我们在深度神经网络上进行了广泛的实验。结果一致地显示，COPS比基准方法有更好的表现，证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing"><a href="#MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing" class="headerlink" title="MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing"></a>MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02253">http://arxiv.org/abs/2309.02253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs-crr/ma-vae">https://github.com/lcs-crr/ma-vae</a></li>
<li>paper_authors: Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</li>
<li>for: Automatic anomaly detection in automotive testing, to improve the efficiency of manual evaluation and detect more anomalies.</li>
<li>methods: Variational autoencoder with multi-head attention (MA-VAE) trained on unlabelled data, to avoid false positives and detect the majority of anomalies.</li>
<li>results: The approach achieves a detection rate of 67% and is 9% wrong when flagging an anomaly, with the potential to perform well with only a fraction of the training and validation subset.Here’s the text in Traditional Chinese characters:</li>
<li>for: 自动异常检测在汽车测试中，以提高人工评估的效率和检测更多的异常。</li>
<li>methods: 使用多头注意力的积欠 autoencoder（MA-VAE），通过训练无标的数据，避免false positives和检测异常的大多数。</li>
<li>results: 方法可以在实际工业数据集上取得67%的检测率，并且只有9%的时间错判异常，同时也具有只使用部分训练和验证subset的潜力。<details>
<summary>Abstract</summary>
A clear need for automatic anomaly detection applied to automotive testing has emerged as more and more attention is paid to the data recorded and manual evaluation by humans reaches its capacity. Such real-world data is massive, diverse, multivariate and temporal in nature, therefore requiring modelling of the testee behaviour. We propose a variational autoencoder with multi-head attention (MA-VAE), which, when trained on unlabelled data, not only provides very few false positives but also manages to detect the majority of the anomalies presented. In addition to that, the approach offers a novel way to avoid the bypass phenomenon, an undesirable behaviour investigated in literature. Lastly, the approach also introduces a new method to remap individual windows to a continuous time series. The results are presented in the context of a real-world industrial data set and several experiments are undertaken to further investigate certain aspects of the proposed model. When configured properly, it is 9% of the time wrong when an anomaly is flagged and discovers 67% of the anomalies present. Also, MA-VAE has the potential to perform well with only a fraction of the training and validation subset, however, to extract it, a more sophisticated threshold estimation method is required.
</details>
<details>
<summary>摘要</summary>
有一个明确的需求是自动异常检测应用于汽车测试，因为更多的注意力被 Pays attention to the recorded data and manual evaluation by humans reaches its capacity. 这种真实世界数据是庞大、多样、多变和时间性的，因此需要测试者行为的模型。 我们提议一种多头注意力自适应变换器（MA-VAE），当训练在无标签数据时，不仅几乎没有假阳性结果，而且能够检测大多数异常情况。 此外，该方法还解决了 литературе中所讨论的绕过现象，并 introduce a new method to remap individual windows to a continuous time series。 结果在一个实际工业数据集上展示，并进行了一些实验来深入调查某些方面的提案模型。 当配置正确时，MA-VAE 错误的时间为 9%，并发现 67% 的异常情况。 此外，MA-VAE 还有可能在只有一部分训练和验证集上表现良好，但是要从中提取它，需要更复杂的阈值估算方法。
</details></li>
</ul>
<hr>
<h2 id="RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning"><a href="#RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning" class="headerlink" title="RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning"></a>RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02250">http://arxiv.org/abs/2309.02250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/RoBoSS">https://github.com/mtanveer1/RoBoSS</a></li>
<li>paper_authors: Mushir Akhtar, M. Tanveer, Mohd. Arshad</li>
<li>For: The paper proposes a novel loss function called RoBoSS for supervised learning, which addresses the limitations of traditional loss functions in handling noisy and high-dimensional data, improving model interpretability, and reducing training time.* Methods: The paper introduces a new robust algorithm called $\mathcal{L}<em>{rbss}$-SVM that incorporates the RoBoSS loss function within the framework of support vector machine (SVM). The paper also presents a theoretical analysis of the RoBoSS loss function, including its classification-calibrated property and generalization ability.* Results: The paper evaluates the proposed $\mathcal{L}</em>{rbss}$-SVM model on 88 real-world UCI and KEEL datasets from diverse domains, and demonstrates its superiority in terms of remarkable generalization performance and efficiency in training time, especially in the biomedical realm.<details>
<summary>Abstract</summary>
In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks. It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms. Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training. In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the classification-calibrated property and generalization ability are also presented. These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data. To empirically demonstrate the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains. Additionally, to exemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset. The numerical results substantiate the superiority of the proposed $\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time.
</details>
<details>
<summary>摘要</summary>
在机器学习算法领域，损失函数的重要性特别在超级vised学习任务中。它作为基本柱石，对超级vised学习算法的行为和效果产生深远的影响。传统的损失函数，常见的使用，但容易处理噪音和高维数据、降低模型解释性和训练过程中的 converge 速度。本文提出了一种新的 robust、bounded、sparse和smooth（RoBoSS）损失函数，用于超级vised学习。此外，我们将RoBoSS损失函数 integrate 到支持向量机（SVM）框架中，并提出一种新的Robust算法称为 $\mathcal{L}_{rbss}$-SVM。对于理论分析，我们还提供了分类准备性和泛化能力的 investigate。这些调查对于理解RoBoSS损失函数在分类任务中的表现和其泛化能力具有深入的意义。为了证明提出的 $\mathcal{L}_{rbss}$-SVM的效果，我们对88个来自不同领域的UC Irvine和KEEL数据集进行了empirical评估。此外，为了强调 $\mathcal{L}_{rbss}$-SVM在医学领域的效果，我们对EEG信号数据集和BreaKHis数据集进行了评估。数据显示，提出的 $\mathcal{L}_{rbss}$-SVM模型在泛化性和训练时间效率方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network"><a href="#Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network" class="headerlink" title="Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network"></a>Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02248">http://arxiv.org/abs/2309.02248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Smit Marvaniya, Jitendra Singh, Nicolas Galichet, Fred Ochieng Otieno, Geeth De Mel, Kommy Weldemariam</li>
<li>For: 提高供应链功能的时间序列预测精度，使用短期天气特征作为外生输入。* Methods: 提出了一种新的模型框架，能够效率地编码季节气候预测，并通过模块化神经网络架构学习干扰的秘密表示。* Results: 对多个实际数据集进行了广泛的实验，结果显示，使用该模型框架可以降低约13%~17%的预测错误，相比现有的需求预测方法。<details>
<summary>Abstract</summary>
Current time-series forecasting problems use short-term weather attributes as exogenous inputs. However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience. Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions. The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture. Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\% to 17\% across multiple real-world data sets compared to existing demand forecasting methods.
</details>
<details>
<summary>摘要</summary>
当前的时间序列预测问题使用短期天气特征作为外生输入。然而，在特定的时间序列预测解决方案（例如，供应链中的需求预测）中，季节气候预测是关键的，可以提高其抗难度。表示中期到长期季节气候预测的问题是复杂的，因为季节气候预测具有uncertainty，并且在空间时间上关系气候预测和需求的编码是复杂的。我们提出了一种新的模型框架，可以有效地编码季节气候预测，以提供可靠和可靠的时间序列预测。这种编码框架允许模型学习强大的含义表示，无论是uncertain季节气候预测还是其他时间序列数据（例如，买家模式）。我们的广泛实验表明，通过模型季节气候预测的编码，可以在多个实际数据集中降低错误率约13%到17%，相比之前的需求预测方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis"><a href="#Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis" class="headerlink" title="Self-Similarity-Based and Novelty-based loss for music structure analysis"></a>Self-Similarity-Based and Novelty-based loss for music structure analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02243">http://arxiv.org/abs/2309.02243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffroy Peeters</li>
<li>for: 音乐结构分析（MSA）的目标是确定音乐轨道中的乐段，并可能根据它们的相似性进行标签。本文提出一种监督方法来解决音乐边界检测问题。</li>
<li>methods: 我们同时学习特征和卷积核，并将它们结合使用。我们并同时优化SSM-损失和新鲜度损失。此外，我们还证明了通过自我注意力进行相对特征学习是MSA任务中有利的。</li>
<li>results: 我们对RWC-Pop和SALAMI中的多个子集进行比较，并证明了我们的方法的表现更佳。<details>
<summary>Abstract</summary>
Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.
</details>
<details>
<summary>摘要</summary>
音乐结构分析（MSA）是目标在音乐轨道中标识 Musical Segments 并可能根据它们的相似性进行标签。在这篇论文中，我们提出了一种监督方法来实现音乐边界检测任务。我们同时学习特征和卷积核，并将它们结合使用。为此，我们同时优化 -- 基于学习的自相似矩阵（SSM）获得的特征loss，以及 -- 基于学习的卷积核应用于估计的SSM中的新鲜度loss。我们还证明了通过自我注意力实现相对特征学习对 MSA 任务有利。最后，我们比较了我们的方法与之前提出的方法在标准 RWC-Pop 和 SALAMI 中的表现。
</details></li>
</ul>
<hr>
<h2 id="Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research"><a href="#Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research" class="headerlink" title="Sample Size in Natural Language Processing within Healthcare Research"></a>Sample Size in Natural Language Processing within Healthcare Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02237">http://arxiv.org/abs/2309.02237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaya Chaturvedi, Diana Shamsutdinova, Felix Zimmer, Sumithra Velupillai, Daniel Stahl, Robert Stewart, Angus Roberts<br>for: This paper aims to provide recommendations on sample sizes for text classification tasks in the healthcare domain.methods: The authors use models trained on the MIMIC-III database of critical care records to classify documents as having or not having Unspecified Essential Hypertension or diabetes mellitus without mention of complication. They perform simulations using various classifiers on different sample sizes and class proportions.results: The study finds that a sample size larger than 1000 is sufficient to provide decent performance metrics, and that smaller sample sizes result in better results with K-nearest neighbours classifiers, while larger sample sizes provide better results with support vector machines and BERT models. The simulations provide guidelines for selecting appropriate sample sizes and class proportions, and can be modified for sample size estimates calculations with other datasets.Here is the text in Simplified Chinese:for: 这篇论文目的是为健康领域文本分类任务提供样本大小建议。methods: 作者使用基于MIMIC-III数据库的重症护理记录来分类文档是否有未特定高血压病例。他们使用不同的分类器进行了不同样本大小和类别比例的 simulations。results: 研究发现，样本大于1000是可以提供不错的性能指标的，而小样本更适合K nearest neighbors分类器，大样本更适合支持向量机和BERT模型。这些 simulations 提供了适合样本大小和类别比例的建议，可以用于其他数据集的样本大小估算。<details>
<summary>Abstract</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database. Simulations were performed using various classifiers on different sample sizes and class proportions. This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models. Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data. The methodology used here can be modified for sample size estimates calculations with other datasets.
</details>
<details>
<summary>摘要</summary>
样本大小计算是数据基础学科中的一个重要步骤。大 enough的样本可以保证样本表示性和估计精度的准确性。这是对多个量学研究进行的，包括使用机器学习方法，如自然语言处理，其中自由文本用于生成预测和类型实例。在医疗领域，缺乏足够的先前收集的数据库可以是新研究确定样本大小的限制因素。本文试图解决这个问题，通过对文本分类任务在医疗领域中的样本大小进行建议。使用了基于MIMIC-III数据库的 kritical care记录，从Beth Israel Deaconess医疗中心中的文本进行分类，并将文档分为有无不特定重要高血压的诊断代码。使用不同的类ifier和样本大小进行了多个 simulations。这些 simulations 表明，使用 K-nearest neighbours 类ifier时，小样本大小更好的结果，而使用支持向量机和BERT模型时，更大的样本大小提供了更好的结果。总的来说，样本大于1000是足够的，以提供不错的性能指标。这些 simulations 提供了适用于选择合适的样本大小和类型占比，以及预测预期性能的指南，在建立文本医疗数据中的分类器时。这种方法可以在其他数据集上进行修改，以便为样本大小估算计算提供方法。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces"><a href="#Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces" class="headerlink" title="Distributionally Robust Model-based Reinforcement Learning with Large State Spaces"></a>Distributionally Robust Model-based Reinforcement Learning with Large State Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02236">http://arxiv.org/abs/2309.02236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic</li>
<li>for: 解决权衡学习中的复杂动态系统、高维状态空间和实际世界动态与训练环境不同问题。</li>
<li>methods: 使用分布性Robust Markov决策过程，使用 Gaussian Processes 和最大差异减少算法，fficiently 学习多输出 Nominal 过程动力学。</li>
<li>results: 提出了一种独立数据集获取成本高、不同不同的不确定性集合下的 statistically 可靠的样本复杂度。实验结果表明方法具有分布性不稳定性和高效性。<details>
<summary>Abstract</summary>
Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed.
</details>
<details>
<summary>摘要</summary>
三大挑战在强化学习中是复杂的动力系统和大状态空间，以及实际世界中的动力学与训练环境部署不符。为了解决这些问题，我们研究了分布式Robust Markov决策过程，在Kullback-Leibler、χ²和总变量不确定集中进行研究。我们提出了基于模型的方法，利用 Gaussian Processes 和最大变量减少算法，高效地学习多输出номinal传输动力学，利用 simulator 的访问权限。我们还证明了提posed方法的统计样本复杂性，对不同的不确定集进行证明。这些复杂性下限独立于状态数量，超越了线性动力学，保证了我们的方法在确定分布下采取近似优化的策略。我们的方法可以与其他model-free分布式Robust reinforcement learning方法结合，获得近似优化的强化策略。实验结果表明，我们的算法对分布shift具有鲁棒性，并且在样本数量方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis"><a href="#Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis" class="headerlink" title="Improving equilibrium propagation without weight symmetry through Jacobian homeostasis"></a>Improving equilibrium propagation without weight symmetry through Jacobian homeostasis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02214">http://arxiv.org/abs/2309.02214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laborieux-axel/generalized-holo-ep">https://github.com/laborieux-axel/generalized-holo-ep</a></li>
<li>paper_authors: Axel Laborieux, Friedemann Zenke</li>
<li>for: 这个论文旨在探讨 equilibrio propagation（EP）算法在生物或分析 нейROMorphic substrate上计算神经网络的梯度时的问题。</li>
<li>methods: 这篇论文使用了 generalized EP 算法，可以不需要权重对称，并分离了两种偏误的来源。</li>
<li>results: 研究发现，对于复杂非对称神经网络，finite nudge不会引起问题，但权重不对称会导致低任务性能，因为 EP 的 neuronal error vectors 与 BP 的不同。为解决这个问题，我们提出了一个新的homeostatic objective，可以直接惩罚神经网络的 Jacobian 函数的功能不对称。这种目标可以帮助神经网络更好地解决复杂任务，如 ImageNet 32x32。<details>
<summary>Abstract</summary>
Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to estimate unbiased gradients efficiently. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry affects its applicability is unknown because, in practice, it may be masked by biases introduced through the finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that the finite nudge does not pose a problem, as exact derivatives can still be estimated via a Cauchy integral. In contrast, weight asymmetry introduces bias resulting in low task performance due to poor alignment of EP's neuronal error vectors compared to BP. To mitigate this issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. This homeostatic objective dramatically improves the network's ability to solve complex tasks such as ImageNet 32x32. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate's relaxation dynamics.
</details>
<details>
<summary>摘要</summary>
“均衡传播”（EP）是一种可观的替代方案，用于计算神经网络中的梯度。它需要权重对称和无限小均衡干扰（即推干），以计算不偏的梯度。但是，实际上实现这两个需求是很困难的。特别是，权重的不对称性可能会对EP的适用范围造成影响，但是这个问题仍未得到解释。为了解决这个问题，我们研究了一种不对称的EP，并分析了两种偏差的来源。在复杂的不对称神经网络中，我们发现了一个与推干无关的问题，即权重的不对称性会导致梯度的误差。为了解决这个问题，我们提出了一个新的自适应目标，它直接评估神经网络的Jacobian中的功能不对称性。我们的结果显示，这个自适应目标可以对神经网络的任务性能提高，特别是在ImageNet 32x32类型的复杂任务中。我们的研究提供了实际的理论基础，用于研究和缓解物理网络上学习算法的偏差问题。”
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Machine-Learning-with-Multi-source-Data"><a href="#Distributionally-Robust-Machine-Learning-with-Multi-source-Data" class="headerlink" title="Distributionally Robust Machine Learning with Multi-source Data"></a>Distributionally Robust Machine Learning with Multi-source Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02211">http://arxiv.org/abs/2309.02211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Wang, Peter Bühlmann, Zijian Guo</li>
<li>for: 本研究旨在提高预测性能，解决经典机器学习方法在目标分布与源Population之间的差异问题。</li>
<li>methods: 本文提出了一种基于多个源Population的分布ally robust预测模型，通过最大化对目标分布的劳动价值来优化预测性能。</li>
<li>results: 相比经典采样风险最小化，提案的Robust预测模型可以提高预测性能对于具有分布差异的目标Population。我们还证明了该模型是源Population的conditional outcome模型的加权平均。此外，我们还提出了一种偏向Corrector来改善基本机器学习算法的涨进率。<details>
<summary>Abstract</summary>
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details>
<details>
<summary>摘要</summary>
The proposed model is a weighted average of the source populations' conditional outcome models, and we develop a novel bias-corrected estimator to estimate the optimal aggregation weight. This approach can be seen as a distributionally robust federated learning method that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution.We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details></li>
</ul>
<hr>
<h2 id="Language-Models-for-Novelty-Detection-in-System-Call-Traces"><a href="#Language-Models-for-Novelty-Detection-in-System-Call-Traces" class="headerlink" title="Language Models for Novelty Detection in System Call Traces"></a>Language Models for Novelty Detection in System Call Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02206">http://arxiv.org/abs/2309.02206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Fournier, Daniel Aloise, Leandro R. Costa</li>
<li>for: 本研究旨在检测计算机系统中的新型行为，以便更好地了解系统的行为和性能。</li>
<li>methods: 本研究使用语言模型来检测系统调用序列中的新型行为，并评估了三种不同的神经网络架构：LSTM、Transformer 和 Longformer。</li>
<li>results: 研究发现，使用这些神经网络架构可以达到高于 95% 的 F-score 和 AuROC 值，并且需要 minimal 的专家手工设计。代码和训练模型已经公开发布在 GitHub，而数据集则可以在 Zenodo 上下载。<details>
<summary>Abstract</summary>
Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub while the datasets are available on Zenodo.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于现代计算机系统的复杂性，常见和不常见的行为经常发生。这些异常可能是正常的，如软件更新和新用户活动，或者异常的，如配置问题、延迟问题、入侵和软件错误。无论如何，新的行为对开发者来说非常有趣，并且有一定的实际需求来检测它们。现在，研究人员通常认为系统调用是计算机系统行为的最细grained和准确的源泉。因此，这篇论文提出了一种基于系统调用的新特性检测方法。这种方法基于系统调用的概率分布，可以被视为语言模型。语言模型可以估计序列的可能性，因此新的行为往往会对模型进行异常。随着语义网络在语言模型方面的成功，这篇论文在这三种体系中评估了LSTM、Transformer和Longformer。然而，大型神经网络通常需要很多数据来进行有效地训练，而我们所知道的现代kernel traces数据集没有公开available。这篇论文解决了这个限制，通过介绍一个新的开源数据集，包含超过200万个网络请求，与7种不同的行为相关。提议的方法需要 minimal expert hand-crafting，并在大多数新的行为上达到了F-score和AuROC的95%以上。source code和训练模型都公开可用于GitHub，而数据集则可以在Zenodo上找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence"><a href="#On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence" class="headerlink" title="On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence"></a>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02202">http://arxiv.org/abs/2309.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</li>
<li>for: 这个论文的目的是研究在Fixed Confidence下的Best Arm Identification（BAI）问题，具体来说是在数据敏感应用中实现数据隐私的同时能够有效地确定最佳臂。</li>
<li>methods: 本论文使用了Global Differential Privacy（DP）的概念来保证数据隐私，并提出了一种名为AdaP-TT的变体，这种方法在运行arm-dependent adaptive episodes中添加了Laplace噪声来保证好的隐私利用协议。</li>
<li>results: 本论文提出了一个lower bound的计算复杂度下界，这个下界随着隐私预算($\epsilon$)的变化而改变，并且在高隐私 régime（小$\epsilon）下，这个下界受到一种新的信息理论量——总特征时间的影响。此外，本论文还提出了一种名为AdaP-TT的实验分析， validate our theoretical results.<details>
<summary>Abstract</summary>
Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.
</details>
<details>
<summary>摘要</summary>
Best Arm Identification (BAI) 问题在数据敏感应用中逐渐获得应用，如设计适应性临床试验、调整超参数以及进行用户研究等。由于这些应用所 invoke 的数据隐私问题，我们研究了在 fixed confidence 下的 BAI 问题，以确保数据隐私。首先，为了衡量隐私成本，我们 deriv 了 Any $\delta$-correct BAI 算法满足 $\epsilon$-global Differential Privacy (DP) 的下界。我们的下界表明，存在两种隐私 régime，即高隐私 régime（小 $\epsilon$）和低隐私 régime（大 $\epsilon$）。在高隐私 régime中，难度受到隐私和一种新的信息理论量，called Total Variation Characteristic Time 的coupled effect。在低隐私 régime中，下界降到 classical non-private 的下界。其次，我们提出了 AdaP-TT，一种 $\epsilon$-global DP 版本的 Top Two 算法。AdaP-TT 在 arm-dependent 的适应性集中运行，并添加 Laplace 噪声以保证好的隐私-实用办法。我们 deriv 了 AdaP-TT 的 asymptotic 上界，其与下界几乎相同，只有 multiplicative constants 的差异在高隐私 régime。最后，我们进行了 AdaP-TT 的实验分析，并证明了我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Function-space-Representation-of-Neural-Networks"><a href="#Sparse-Function-space-Representation-of-Neural-Networks" class="headerlink" title="Sparse Function-space Representation of Neural Networks"></a>Sparse Function-space Representation of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02195">http://arxiv.org/abs/2309.02195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</li>
<li>for: 提高深度神经网络（NNs）的不确定性估计和数据更新能力。</li>
<li>methods: 将NNs转换为函数空间，使用双参数化来提供笛卡尔表示， capture数据集中的信息，并提供一种可靠的方法来Quantifying uncertainty in supervised learning tasks。</li>
<li>results: 在UC Irvine（UCI）数据集上进行证明，提出的方法可以快速地 incorporate new data without retraining，保持预测性能。<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known to lack uncertainty estimates and struggle to incorporate new data. We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization. Importantly, the dual parameterization enables us to formulate a sparse representation that captures information from the entire data set. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（NN）因缺乏不确定性估计和新数据 интеграción能力而著称为不稳定。我们提出了一种方法，通过将 NN 从权重空间转换到函数空间，通过双参数化来解决这些问题。这种双参数化允许我们构建一种稀疏表示，捕捉整个数据集中的信息。这种方法可以有效地捕捉不确定性，并在不需要重新训练的情况下将新数据 integrate 到模型中。我们对超vised learning在UCIbenchmark任务上进行了证明性示范。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing"><a href="#Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing" class="headerlink" title="Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing"></a>Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02193">http://arxiv.org/abs/2309.02193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengrong Song, Chuan Ma, Ming Ding, Howard H. Yang, Yuwen Qian, Xiangwei Zhou</li>
<li>for: 提高多架空车 trajectory optimization 和服务质量</li>
<li>methods: 融合 federated learning 和 deep reinforcement learning 技术</li>
<li>results: 提高训练效率和服务质量，比其他 DRL 方法更好<details>
<summary>Abstract</summary>
In the era of 5G mobile communication, there has been a significant surge in research focused on unmanned aerial vehicles (UAVs) and mobile edge computing technology. UAVs can serve as intelligent servers in edge computing environments, optimizing their flight trajectories to maximize communication system throughput. Deep reinforcement learning (DRL)-based trajectory optimization algorithms may suffer from poor training performance due to intricate terrain features and inadequate training data. To overcome this limitation, some studies have proposed leveraging federated learning (FL) to mitigate the data isolation problem and expedite convergence. Nevertheless, the efficacy of global FL models can be negatively impacted by the high heterogeneity of local data, which could potentially impede the training process and even compromise the performance of local agents. This work proposes a novel solution to address these challenges, namely personalized federated deep reinforcement learning (PF-DRL), for multi-UAV trajectory optimization. PF-DRL aims to develop individualized models for each agent to address the data scarcity issue and mitigate the negative impact of data heterogeneity. Simulation results demonstrate that the proposed algorithm achieves superior training performance with faster convergence rates, and improves service quality compared to other DRL-based approaches.
</details>
<details>
<summary>摘要</summary>
在5G移动通信时代，有一场很大的研究集中于无人飞行器（UAV）和边缘计算技术。UAV可以作为边缘计算环境中的智能服务器，优化其飞行轨迹以最大化通信系统吞吐量。基于深度反馈学习（DRL）的轨迹优化算法可能由于复杂的地形特征和不充分的训练数据而表现差。为了解决这些问题，一些研究提议使用联邦学习（FL）来缓解数据隔离问题并加速对称。然而，全球FL模型的效果可能受到本地数据的高多样性的影响，这可能会阻碍训练过程并可能对本地代理器的性能产生负面影响。这项工作提出了一种解决这些挑战的新方法，即个性化联邦深度反馈学习（PF-DRL），用于多个UAV轨迹优化。PF-DRL的目标是为每个代理器开发个性化模型，以解决数据缺乏问题并减少数据多样性的负面影响。实验结果表明，提议的算法可以在训练性能和快速对称上达到更高的水平，并提高服务质量。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification"><a href="#Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification" class="headerlink" title="Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification"></a>Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02189">http://arxiv.org/abs/2309.02189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvys Linhares Pontes, Mohamed Benjannet, Lam Kim Ming</li>
<li>for: 这个研究旨在探讨如何使用BERT语言模型精准地分类新闻文档中的ESG问题标签。</li>
<li>methods: 这个研究使用了多种BERT语言模型，包括RoBERTa和SVM等方法，以实现高精度的新闻文档分类。</li>
<li>results: 研究发现，RoBERTa分类器在英文测试集上获得了第二名，而在法语测试集上与其他方法并列第五名。此外，SVM基于的二分类模型在中文语言上表现出色，在测试集上获得了第二名。<details>
<summary>Abstract</summary>
Environmental, Social, and Governance (ESG) has been used as a metric to measure the negative impacts and enhance positive outcomes of companies in areas such as the environment, society, and governance. Recently, investors have increasingly recognized the significance of ESG criteria in their investment choices, leading businesses to integrate ESG principles into their operations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG) shared task encompasses the classification of news documents into 35 distinct ESG issue labels. In this study, we explored multiple strategies harnessing BERT language models to achieve accurate classification of news documents across these labels. Our analysis revealed that the RoBERTa classifier emerged as one of the most successful approaches, securing the second-place position for the English test dataset, and sharing the fifth-place position for the French test dataset. Furthermore, our SVM-based binary model tailored for the Chinese language exhibited exceptional performance, earning the second-place rank on the test dataset.
</details>
<details>
<summary>摘要</summary>
环境、社会和管理（ESG）已经被用作公司负面影响和改善效果的度量。最近，投资者对ESG标准的重要性日益认识，迫企业将ESG原则纳入其运营和战略中。本研究使用BERT语言模型来实现新闻文档的精确分类，并对英文和法文测试集进行了分析。我们发现，RoBERTa分类器在英文测试集上表现出色，排名第二，而在法文测试集上与其他模型分列第五。此外，我们为中文语言开发的SVM二分类模型也表现出优异，在测试集上排名第二。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges"><a href="#A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges" class="headerlink" title="A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges"></a>A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02473">http://arxiv.org/abs/2309.02473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Zare, Parham M. Kebria, Abbas Khosravi, Saeid Nahavandi</li>
<li>for: This paper provides an introduction to imitation learning (IL) and an overview of its underlying assumptions and approaches, as well as a comprehensive guide to the growing field of IL in robotics and AI.</li>
<li>methods: The paper discusses recent advances and emerging areas of research in IL, including the use of demonstrations, the challenges associated with IL, and potential directions for future research.</li>
<li>results: The paper provides a detailed description of the current state of the field of IL, including recent advances and emerging areas of research, and offers a comprehensive guide to the field.<details>
<summary>Abstract</summary>
In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.   This paper aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of the paper is to provide a comprehensive guide to the growing field of IL in robotics and AI.
</details>
<details>
<summary>摘要</summary>
IL is a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations. This paper aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research.IL has gained increasing attention in recent years due to its ability to learn complex behaviors in a more efficient and effective manner than traditional RL methods. By learning from an expert's behavior, IL can overcome the challenges of specifying an optimal set of rules or reward signals in complex environments. Furthermore, IL can be applied to a wide range of tasks, such as robotic manipulation, human-robot interaction, and autonomous driving.The paper is organized as follows: In section 2, we provide an overview of the underlying assumptions and approaches of IL. In section 3, we discuss recent advances and emerging areas of research in the field. In section 4, we address common challenges associated with IL and provide potential directions for future research. Finally, in section 5, we conclude with a discussion of the potential of IL in robotics and AI.Overall, the goal of this paper is to provide a comprehensive guide to the growing field of IL in robotics and AI, and to highlight the potential of this approach for learning complex behaviors in a more efficient and effective manner.
</details></li>
</ul>
<hr>
<h2 id="Bias-Propagation-in-Federated-Learning"><a href="#Bias-Propagation-in-Federated-Learning" class="headerlink" title="Bias Propagation in Federated Learning"></a>Bias Propagation in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02160">http://arxiv.org/abs/2309.02160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/privacytrustlab/bias_in_FL">https://github.com/privacytrustlab/bias_in_FL</a></li>
<li>paper_authors: Hongyan Chang, Reza Shokri</li>
<li>for: 这篇论文旨在探讨联合学习中的群体公平问题，具体来说是研究如果一些党在联合学习中存在偏见，会如何影响整个网络中的模型。</li>
<li>methods: 作者使用了实际世界数据集来分析和解释联合学习中偏见的传播。他们分析发现，偏见党在训练过程中逐渐增加了对敏感特征的依赖度。</li>
<li>results: 研究发现，在联合学习中，偏见会导致模型受到敏感特征的影响，这种影响比中央训练模型使用整个数据集时更高。这表明，偏见在联合学习中存在，并且需要进行群体公平的审核和设计更加Robust的学习算法。<details>
<summary>Abstract</summary>
We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.
</details>
<details>
<summary>摘要</summary>
我们显示了参与联邦学习可能会对群体公平性造成损害。事实上，一些党征某些受排挤群体（根据敏感特征如性别或种族）的偏见可以透过网络传播到所有网络中的党。我们分析并解释了联邦学习中偏见传播的现象。我们发现，偏见党不知不觉地将偏见传递到小数的模型参数中，并在训练过程中不断增加受排挤群体的依赖。需要注意的是，在联邦学习中体验到的偏见高于各党在集中训练中训练的模型的情况。这表明偏见是由于算法所致。我们的工作呼吁审核联邦学习中的群体公平性，并设计不受偏见传播的学习算法。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Offline-Policy-Optimization-with-Adversarial-Network"><a href="#Model-based-Offline-Policy-Optimization-with-Adversarial-Network" class="headerlink" title="Model-based Offline Policy Optimization with Adversarial Network"></a>Model-based Offline Policy Optimization with Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02157">http://arxiv.org/abs/2309.02157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junming-yang/moan">https://github.com/junming-yang/moan</a></li>
<li>paper_authors: Junming Yang, Xingguo Chen, Shengyuan Wang, Bolei Zhang</li>
<li>for: 这个论文主要针对的是OFFLINE reinforcement learning中的策略优化问题。</li>
<li>methods: 该论文提出了一种基于模型的OFFLINE reinforcement learning方法，使用了对抗学习来建立一个更加通用的转移模型，并通过对抗学习来自动提供模型不确定性的评估。</li>
<li>results:  compare to现有的基于模型的OFFLINE reinforcement learning方法，该方法在广泛的测试集上表现出色，可以更好地优化策略，同时可以更准确地评估模型的不确定性。<details>
<summary>Abstract</summary>
Model-based offline reinforcement learning (RL), which builds a supervised transition model with logging dataset to avoid costly interactions with the online environment, has been a promising approach for offline policy optimization. As the discrepancy between the logging data and online environment may result in a distributional shift problem, many prior works have studied how to build robust transition models conservatively and estimate the model uncertainty accurately. However, the over-conservatism can limit the exploration of the agent, and the uncertainty estimates may be unreliable. In this work, we propose a novel Model-based Offline policy optimization framework with Adversarial Network (MOAN). The key idea is to use adversarial learning to build a transition model with better generalization, where an adversary is introduced to distinguish between in-distribution and out-of-distribution samples. Moreover, the adversary can naturally provide a quantification of the model's uncertainty with theoretical guarantees. Extensive experiments showed that our approach outperforms existing state-of-the-art baselines on widely studied offline RL benchmarks. It can also generate diverse in-distribution samples, and quantify the uncertainty more accurately.
</details>
<details>
<summary>摘要</summary>
模型基于的线上强化学习（RL），建立了一个监督式转移模型，使用日志数据集来避免在线环境中的昂贵交互，已经是无线环境中的减少策略优化的有力的方法。然而，logging数据和线上环境之间的差异可能会导致分布转移问题，许多先前的工作都在如何建立保守的转移模型和准确地估计模型的不确定性方面进行了研究。然而，过于保守的建模可能会限制代理人的探索，并且不确定性估计可能是不可靠的。在这种情况下，我们提出了一种新的模型基于的线上策略优化框架（MOAN）。关键思想是通过对抗学习建立一个更好的泛化模型，其中一个对手用于分辨在数据集中的含义和 OUT-OF-DISTRIBUTION 样本。此外，对手还可以提供有理性保证的模型不确定性量化。我们的方法在广泛的 Offline RL 基准数据集上进行了广泛的实验，并显示了与现有的基准值相比，我们的方法能够更高效地优化策略，同时也能够更准确地量化不确定性。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Reasoners-with-Alignment"><a href="#Making-Large-Language-Models-Better-Reasoners-with-Alignment" class="headerlink" title="Making Large Language Models Better Reasoners with Alignment"></a>Making Large Language Models Better Reasoners with Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02144">http://arxiv.org/abs/2309.02144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui</li>
<li>for: The paper aims to enhance the reasoning capabilities of large language models (LLMs) by addressing the “Assessment Misalignment” problem, which occurs when fine-tuned LLMs assign higher scores to subpar chain of thought (COT) reasoning processes.</li>
<li>methods: The paper introduces an “Alignment Fine-Tuning” (AFT) paradigm that involves fine-tuning LLMs with COT training data, generating multiple COT responses for each question, and calibrating the scores of positive and negative responses with a novel constraint alignment loss.</li>
<li>results: Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT in enhancing the reasoning capabilities of LLMs. The constraint alignment loss is found to be crucial for the performance of recent ranking-based alignment methods, such as DPO, RRHF, and PRO.Here’s the Chinese version of the three key information points:</li>
<li>for: 论文目的是提高大型自然语言模型（LLM）的理解能力，并解决“评价不一致”问题， LLM 通过精心调整可以提高其理解能力。</li>
<li>methods: 论文引入了“对Alignment”（AFT）方法，包括在 COT 训练数据上精心调整 LLM，生成每个问题多个 COT 答案，并对 LLM 的答案进行评分。</li>
<li>results: 实验结果表明， AFT 可以有效地提高 LLM 的理解能力，并且发现 Constraint 对 recient ranking-based alignment方法的性能是关键的。<details>
<summary>Abstract</summary>
Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.
</details>
<details>
<summary>摘要</summary>
理智是一种认知过程，通过证据来达成有sound的结论。理智能力是人工通用智能代理人的关键 Component。latest studies show that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an Assessment Misalignment problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an Alignment Fine-Tuning (AFT) paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection"><a href="#A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection" class="headerlink" title="A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection"></a>A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02140">http://arxiv.org/abs/2309.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dani-capellan/LightTBNet">https://github.com/dani-capellan/LightTBNet</a></li>
<li>paper_authors: Daniel Capellán-Martín, Juan J. Gómez-Valverde, David Bermejo-Peláez, María J. Ledesma-Carbayo<br>for: 这个论文旨在提高肺部X射线图像的诊断精度，特别是用于检测肺结核病。methods: 该论文提出了一种新的轻量级、快速和计算效率低的深度学习模型，称为LightTBNet，用于从肺部X射线图像中检测肺结核病。results: 在使用2个公共数据集的800张前额CXR图像上，该模型实现了0.906、0.907和0.961的准确率、F1分数和ROC曲线的AUC值，并在独立测试集上达到了极高的性能。<details>
<summary>Abstract</summary>
Tuberculosis (TB) is still recognized as one of the leading causes of death worldwide. Recent advances in deep learning (DL) have shown to enhance radiologists' ability to interpret chest X-ray (CXR) images accurately and with fewer errors, leading to a better diagnosis of this disease. However, little work has been done to develop models capable of diagnosing TB that offer good performance while being efficient, fast and computationally inexpensive. In this work, we propose LightTBNet, a novel lightweight, fast and efficient deep convolutional network specially customized to detect TB from CXR images. Using a total of 800 frontal CXR images from two publicly available datasets, our solution yielded an accuracy, F1 and area under the ROC curve (AUC) of 0.906, 0.907 and 0.961, respectively, on an independent test subset. The proposed model demonstrates outstanding performance while delivering a rapid prediction, with minimal computational and memory requirements, making it highly suitable for deployment in handheld devices that can be used in low-resource areas with high TB prevalence. Code publicly available at https://github.com/dani-capellan/LightTBNet.
</details>
<details>
<summary>摘要</summary>
抑阻疾病（TB）仍被认为是全球主要的死亡原因之一。最近的深度学习（DL）突破有助于诊断医生更准确地解读胸部X射线图像（CXR），从而提高TB的诊断精度。然而，对于开发能够诊断TB的模型来说，还很少有工作。在这项工作中，我们提出了LightTBNet，一种特制的轻量级、快速和计算成本低的深度卷积网络，用于从CXR图像中检测TB。使用总计800个前面CXR图像，我们的解决方案在独立测试集上得到了0.906、0.907和0.961的准确率、F1分数和ROC曲线的抑阻率，并且模型具有快速预测和较低的计算和存储需求，因此非常适合在低TB患率地区使用手持式设备进行部署。代码公开可用于https://github.com/dani-capellan/LightTBNet。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Simplicial-Attention-Neural-Networks"><a href="#Generalized-Simplicial-Attention-Neural-Networks" class="headerlink" title="Generalized Simplicial Attention Neural Networks"></a>Generalized Simplicial Attention Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02138">http://arxiv.org/abs/2309.02138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks">https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks</a></li>
<li>paper_authors: Claudio Battiloro, Lucia Testa, Lorenzo Giusti, Stefania Sardellitti, Paolo Di Lorenzo, Sergio Barbarossa</li>
<li>for: 这个论文旨在介绍一种新的神经网络模型，即通用 simplicial attention neural network (GSAN)，用于处理定义在 simplicial 复合体上的数据。</li>
<li>methods: 这种模型使用 masked self-attention 层来处理数据Components，基于 тополоógical signal processing 原理，设计了一系列自注意机制，可以处理不同 simplicial 顺序的数据组成部分，例如节点、边、triangle 和更高阶的 simplicial 结构。</li>
<li>results: 该模型可以在多个 inductive 和 transductive 任务中表现出色，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。此外，模型还 theoretically establishes  permutation equivariant 和 simplicial-aware 性。<details>
<summary>Abstract</summary>
The aim of this work is to introduce Generalized Simplicial Attention Neural Networks (GSANs), i.e., novel neural architectures designed to process data defined on simplicial complexes using masked self-attentional layers. Hinging on topological signal processing principles, we devise a series of self-attention schemes capable of processing data components defined at different simplicial orders, such as nodes, edges, triangles, and beyond. These schemes learn how to weight the neighborhoods of the given topological domain in a task-oriented fashion, leveraging the interplay among simplices of different orders through the Dirac operator and its Dirac decomposition. We also theoretically establish that GSANs are permutation equivariant and simplicial-aware. Finally, we illustrate how our approach compares favorably with other methods when applied to several (inductive and transductive) tasks such as trajectory prediction, missing data imputation, graph classification, and simplex prediction.
</details>
<details>
<summary>摘要</summary>
本工作的目标是介绍通用 simplicial 注意网络（GSAN），即基于 topological signal processing 原理的新型神经网络，用于处理定义在 simplicial 复合体上的数据。我们提出了一系列自我注意机制，可以处理不同 simplicial 顺序的数据组件，如节点、边、triangle 等，并通过DIRAC 算子和其 decompositions 学习权重邻域。我们还证明了 GSAN 具有 permutation 变换对称和 simplicial 意识。最后，我们比较了我们的方法与其他方法在 inductive 和 transductive 任务上的性能，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。Here's the translation in Traditional Chinese:本研究的目的是介绍通用 simplicial 注意网络（GSAN），即基于 topological signal processing 原理的新型神经网络，用于处理定义在 simplicial 复合体上的数据。我们提出了一系列自我注意机制，可以处理不同 simplicial 顺序的数据组件，如节点、边、triangle 等，并通过DIRAC 算子和其 decompositions 学习权重邻域。我们还证明了 GSAN 具有 permutation 变换对称和 simplicial 意识。最后，我们比较了我们的方法与其他方法在 inductive 和 transductive 任务上的性能，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again"><a href="#A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again" class="headerlink" title="A Simple Asymmetric Momentum Make SGD Greatest Again"></a>A Simple Asymmetric Momentum Make SGD Greatest Again</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02130">http://arxiv.org/abs/2309.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongyue Zhang, Dinghuang Zhang, Shuwen Zhao, Donghan Liu, Carrie M. Toptan, Honghai Liu</li>
<li>for: 提高SGD算法的稳定性和精度，帮助避免极值点问题</li>
<li>methods: 使用损控制的非Symmetric摩擦（LCAM）技术，利用Weight conjugation和Traction effect来解释其工作机制</li>
<li>results: 比传统的SGD WITH momentum更高的性能，减少了计算复杂度，并在WRN28-10网络上实现了80.78%的峰值测试精度，比原始WRN paper和CAS更高，且用于 Nearly half convergence time.<details>
<summary>Abstract</summary>
We propose the simplest SGD enhanced method ever, Loss-Controlled Asymmetric Momentum(LCAM), aimed directly at the Saddle Point problem. Compared to the traditional SGD with Momentum, there's no increase in computational demand, yet it outperforms all current optimizers. We use the concepts of weight conjugation and traction effect to explain this phenomenon. We designed experiments to rapidly reduce the learning rate at specified epochs to trap parameters more easily at saddle points. We selected WRN28-10 as the test network and chose cifar10 and cifar100 as test datasets, an identical group to the original paper of WRN and Cosine Annealing Scheduling(CAS). We compared the ability to bypass saddle points of Asymmetric Momentum with different priorities. Finally, using WRN28-10 on Cifar100, we achieved a peak average test accuracy of 80.78\% around 120 epoch. For comparison, the original WRN paper reported 80.75\%, while CAS was at 80.42\%, all at 200 epoch. This means that while potentially increasing accuracy, we use nearly half convergence time. Our demonstration code is available at\\ https://github.com/hakumaicc/Asymmetric-Momentum-LCAM
</details>
<details>
<summary>摘要</summary>
我们提出了最简单的SGD增强方法──损失控制对称动量（LCAM），直接解决点积问题。与传统的SGD增强方法相比，我们的方法不增加计算负载，却能超越现有的增强器。我们利用几何 conjugation 和拖动效应来解释这一现象。我们设计了实验，以迅速降低学习率在指定的epoch中，以更容易地将参数固定在点积点。我们使用 WRN28-10 网络和 Cifar10 和 Cifar100 作为测试集，与原始 WRN 和 Cosine Annealing Scheduling（CAS）的测试集一样。我们比较不同优先顺位的对称动量对点积点的穿透性。最后，使用 WRN28-10 在 Cifar100 上 дости得了约 120 epoch 的峰值平均测试精度为 80.78%。与原始 WRN 报告的 80.75% 和 CAS 的 80.42% 相比，我们的方法可能增加精度，但使用的是近乎半倍的训练时间。我们的示例代码可以在 https://github.com/hakumaicc/Asymmetric-Momentum-LCAM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning"><a href="#Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning" class="headerlink" title="Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning"></a>Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02124">http://arxiv.org/abs/2309.02124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze Liu, Ziming Zhao, Tiehua Zhang, Kang Wang, Xin Chen, Xiaowei Huang, Jun Yin, Zhishu Shen</li>
<li>for: 静脉sleepstage分类是诊断疾病的关键，现有模型主要使用卷积神经网络(CNN)和图 convolutional neural network(GNN)来模型几何数据和非几何数据，但是它们无法同时考虑多Modal数据的异质和交互，以及空间-时间相关性，从而限制了分类性能的提高。</li>
<li>methods: 我们提出了一种动态学习框架STHL，该框架使用幂图来编码空间-时间数据，幂图可以构建多Modal&#x2F;多种类数据而不是使用简单的对两个主体的对比。STHL创建空间和时间幂边分别来建立节点关系，然后进行类型特定幂图学习过程来编码特征到嵌入空间。</li>
<li>results: 我们的提出的STHL在sleep stage分类任务中表现出色，超过了当前状态的模型。<details>
<summary>Abstract</summary>
Sleep stage classification is crucial for detecting patients' health conditions. Existing models, which mainly use Convolutional Neural Networks (CNN) for modelling Euclidean data and Graph Convolution Networks (GNN) for modelling non-Euclidean data, are unable to consider the heterogeneity and interactivity of multimodal data as well as the spatial-temporal correlation simultaneously, which hinders a further improvement of classification performance. In this paper, we propose a dynamic learning framework STHL, which introduces hypergraph to encode spatial-temporal data for sleep stage classification. Hypergraphs can construct multi-modal/multi-type data instead of using simple pairwise between two subjects. STHL creates spatial and temporal hyperedges separately to build node correlations, then it conducts type-specific hypergraph learning process to encode the attributes into the embedding space. Extensive experiments show that our proposed STHL outperforms the state-of-the-art models in sleep stage classification tasks.
</details>
<details>
<summary>摘要</summary>
“睡眠阶段分类是诊断病人健康状况的关键。现有的模型主要使用卷积神经网络（CNN）来模型几何数据，以及图 convolutional neural network（GNN）来模型非几何数据，但这些模型无法同时考虑多modal数据的异质性和交互性以及空间-时间相关性，这限制了分类性能的进一步提高。本文提出了一种动态学习框架 STHL，它通过引入卷积 Graph 来编码空间-时间数据进行睡眠阶段分类。卷积 Graph 可以构造多modal/多种数据而不是使用简单的对两个主体之间的对应。STHL 首先在空间和时间上分别建立 hyperedge，然后进行类型特定的卷积 Graph 学习过程来编码特征到嵌入空间。广泛的实验表明，我们提出的 STHL 在睡眠阶段分类任务中表现出了比前状态艺术模型更高的性能。”Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Label-Information-for-Multimodal-Emotion-Recognition"><a href="#Leveraging-Label-Information-for-Multimodal-Emotion-Recognition" class="headerlink" title="Leveraging Label Information for Multimodal Emotion Recognition"></a>Leveraging Label Information for Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02106">http://arxiv.org/abs/2309.02106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Digimonseeker/LE-MER">https://github.com/Digimonseeker/LE-MER</a></li>
<li>paper_authors: Peiying Wang, Sunlu Zeng, Junqing Chen, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He</li>
<li>for: 本研究旨在提高多Modal Emotion Recognition（MER）的准确率，通过结合语音和文本信息。</li>
<li>methods: 我们提出了一种新的方法，利用标签信息来提高MER。首先，我们获取语音和文本模态的代表标签嵌入，然后通过标签-token和标签-帧交互学习每个语音的标签感知表示。最后，我们设计了一种标签引导的混合模块，将标签意识的语音和文本表示进行情绪分类。</li>
<li>results: 我们在公共的IEMOCAP dataset上进行了广泛的实验，结果表明，我们的提议方法在基础模型的比较下，超越了现有的基线，达到了新的状态域性能。<details>
<summary>Abstract</summary>
Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
多modal情感识别（MER）目的是检测给定表达中的情感状态，通过结合语音和文本信息。Intuitively，标签信息应该能够帮助模型 locate 关键的框架和字符串，从而实现 MER 任务。 inspirited by this，我们提出了一种 novel approach for MER，利用标签信息。specifically，我们首先获得了表达和语音模式的表示性标签嵌入，然后通过标签-Token和标签-框架交互学习label-aware的文本和语音表示。finally，我们设计了一种标签导向的束合模块，将标签 aware的文本和语音表示进行情感分类。我们在公共的IEMOCAP数据集上进行了广泛的实验，实验结果表明，我们提出的方法在exist的基准点上出perform，并实现了新的状态anner-of-the-art performance。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views"><a href="#Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views" class="headerlink" title="Iterative Superquadric Recomposition of 3D Objects from Multiple Views"></a>Iterative Superquadric Recomposition of 3D Objects from Multiple Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02102">http://arxiv.org/abs/2309.02102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/isco">https://github.com/explainableml/isco</a></li>
<li>paper_authors: Stephan Alaniz, Massimiliano Mancini, Zeynep Akata</li>
<li>for: 这个论文旨在提出一种框架，即ISCO，以便从2D视图直接使用3D超quadrics来重建对象，无需训练3D超视觉模型。</li>
<li>methods: 该框架使用3D超quadrics作为semantic part，通过比较rendered 3D视图和2D图像缩影来优化超quadrics参数，进行 iterative 添加新的超quadrics，从粗略区域开始，然后是细节。</li>
<li>results: 实验表明，相比最近的单个实例超quadrics重建方法，ISCO提供了更加准确的3D重建结果，即使是从野生图像中。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ISCO">https://github.com/ExplainableML/ISCO</a> 上获取。<details>
<summary>Abstract</summary>
Humans are good at recomposing novel objects, i.e. they can identify commonalities between unknown objects from general structure to finer detail, an ability difficult to replicate by machines. We propose a framework, ISCO, to recompose an object using 3D superquadrics as semantic parts directly from 2D views without training a model that uses 3D supervision. To achieve this, we optimize the superquadric parameters that compose a specific instance of the object, comparing its rendered 3D view and 2D image silhouette. Our ISCO framework iteratively adds new superquadrics wherever the reconstruction error is high, abstracting first coarse regions and then finer details of the target object. With this simple coarse-to-fine inductive bias, ISCO provides consistent superquadrics for related object parts, despite not having any semantic supervision. Since ISCO does not train any neural network, it is also inherently robust to out-of-distribution objects. Experiments show that, compared to recent single instance superquadrics reconstruction approaches, ISCO provides consistently more accurate 3D reconstructions, even from images in the wild. Code available at https://github.com/ExplainableML/ISCO .
</details>
<details>
<summary>摘要</summary>
人类善于重新组合新的物体，即可以从通用结构到细节上认出未知物体的共同点，这是机器很难复制的能力。我们提出了一个框架，即ISCO，可以使用2D视图直接从3D超quadrics中提取 semantic parts，无需训练一个使用3D超vision的模型。为了实现这一点，我们优化了超quadric参数，以组成特定物体的实例，并比较其渲染后的3D视图和2D图像轮廓。我们的ISCO框架会逐次添加新的超quadrics，以降低重建错误，从抽象到细节的方式进行卷积。通过这种简单的卷积偏好，ISCO提供了相关的对象部分的一致的超quadrics，即使没有任何 semantic 超视。由于ISCO不需要训练任何神经网络，因此也是自然地对 OUT-OF-distribution 对象有效。实验表明，相比最近的单个实例超quadrics重建方法，ISCO可以提供更加准确的3D重建结果，甚至从野外图像中。代码可以在 https://github.com/ExplainableML/ISCO 上找到。
</details></li>
</ul>
<hr>
<h2 id="TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training"><a href="#TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training" class="headerlink" title="TensorBank:Tensor Lakehouse for Foundation Model Training"></a>TensorBank:Tensor Lakehouse for Foundation Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02094">http://arxiv.org/abs/2309.02094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romeo Kienzler, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann</li>
<li>for: 这个论文旨在提出一种基于云存储和GPU内存的高维数据存储和流处理技术，用于训练基础模型。</li>
<li>methods: 这个论文使用了TensorBank，一种可以在云存储中流动地读取和写入高维数据的tensor lakehouse，以及Hierarchical Statistical Indices (HSI)等技术来加速查询。</li>
<li>results: 这个论文的实验结果表明，使用TensorBank和HSI可以在高维数据流处理中实现高速查询和数据转换，并且可以满足不同领域的需求，如计算机视觉、生物物理学、神经科学等。<details>
<summary>Abstract</summary>
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making heavy use of open-source technology. Although, hardened for production use using geospatial-temporal data, this architecture generalizes to other use case like computer vision, computational neuroscience, biological sequence analysis and more.
</details>
<details>
<summary>摘要</summary>
保存和流处高维数据为基础模型训练成为了现代基础模型的关键要求。在这篇论文中，我们介绍了TensorBank，一个 Petabyte 级 tensor 湖屋，可以将 Cloud Object Store（COS）中的tensor流到 GPU 内存的缓存器，并基于复杂的关系查询使用 Hierarchical Statistical Indices（HSI）进行加速。我们的架构允许直接在块级别上地址 tensor，使用 HTTP 范围读取。将数据转换为 PyTorch 变换后，我们提供了一个通用 PyTorch 数据集类型，并将其与相应的数据工厂相关联，以便将关系查询和请求的转换作为实例。通过使用 HSI，我们可以跳过无关块，因为它们的统计信息在不同层次结构级别中具有不同的内容。这是一种基于开源技术的意见架构，并且通过使用 geospatial-temporal 数据进行硬化，可以普遍应用于其他应用场景，如计算机视觉、计算神经科学、生物Sequencing 分析等。
</details></li>
</ul>
<hr>
<h2 id="Natural-Example-Based-Explainability-a-Survey"><a href="#Natural-Example-Based-Explainability-a-Survey" class="headerlink" title="Natural Example-Based Explainability: a Survey"></a>Natural Example-Based Explainability: a Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03234">http://arxiv.org/abs/2309.03234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Antonin Poché, Lucas Hervier, Mohamed-Chafik Bakkay</li>
<li>for: 这篇论文旨在提供natural例子基于XAI的现状报告，描述了每种方法的优缺点，并比较它们的语义定义、认知影响和加值。</li>
<li>methods: 这篇论文涵盖了自然例子基于XAI的多种方法，包括相似例子、counterfactual和semi-factual例子、重要实例、概念范围和prototype等方法。</li>
<li>results: 论文总结了每种方法的优缺点，并比较了它们的语义定义、认知影响和加值，以便促进和促进未来的自然例子基于XAI工作。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.   This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative process. The exclusion of methods that require generating examples is justified by the need for plausibility which is in some regards required to gain a user's trust. Consequently, this paper will explore the following family of methods: similar examples, counterfactual and semi-factual, influential instances, prototypes, and concepts. In particular, it will compare their semantic definition, their cognitive impact, and added values. We hope it will encourage and facilitate future work on natural example-based XAI.
</details>
<details>
<summary>摘要</summary>
《可解释人工智能（XAI）在提高机器学习模型的可解释性和可信worthiness方面变得越来越重要。尽管焦点图在XAI领域中备受推崇，但它们能够反映模型内部过程的能力受到质疑。虽然不太受注意，但示例基于的XAI方法在不断改进。它包括使用示例来解释机器学习模型的预测的方法。这与人类的认知机理相符，使得示例基于的解释自然和直观，让用户更好地理解。实际上，人类学习和理解都是通过形成示例基于的认知来实现的。本文提供了当前自然示例基于XAI的状态艺术概述，描述了每种方法的优缺点。一个“自然”示例指的是直接从训练数据中提取的示例，不包括任何生成过程。这种排除方法是因为需要可靠性，以获得用户的信任。因此，本文将探讨以下家族方法：相似示例、 counterfactual和半实例、原型、概念。尤其是 comparing their semantic definition, cognitive impact, and added value. 我们希望这篇文章能够鼓励和促进未来的自然示例基于XAI的研究。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders"><a href="#An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders" class="headerlink" title="An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders"></a>An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02084">http://arxiv.org/abs/2309.02084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjlab-ammi/vae4ood">https://github.com/zjlab-ammi/vae4ood</a></li>
<li>paper_authors: Zezhen Zeng, Bin Liu</li>
<li>for: 这 paper 关注 deep generative models (DGMs) 的Unsupervised out-of-distribution (OOD) detection。特别是，我们关注 vanilla Variational Autoencoders (VAE) 使用标准正态分布 для隐藏变量。这些模型具有更小的模型大小，使得更快的训练和推断，适用于有限资源应用程序比较复杂的 DGMs。</li>
<li>methods: 我们提出了一个新的 OOD 分数，叫做 Error Reduction (ER)，专门为 vanilla VAE 设计。ER 利用图像输入的损失函数重建图像，并考虑图像的科隆摩擦复杂性。</li>
<li>results: 我们在多个 dataset 上进行了实验，与基线方法相比，我们的方法显示出了更高的超越性。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/VAE4OOD%E3%80%82">https://github.com/ZJLAB-AMMI/VAE4OOD。</a><details>
<summary>Abstract</summary>
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification"><a href="#BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification" class="headerlink" title="BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification"></a>BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02071">http://arxiv.org/abs/2309.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanx749/bcell">https://github.com/yuanx749/bcell</a></li>
<li>paper_authors: Xiao Yuan</li>
<li>for: 这 paper 是为了提出一种新的深度学习基于多任务框架，用于线性 B 细胞 Epitope 预测和抗体类型特异 Epitope 分类。</li>
<li>methods: 该 paper 使用了序列基的神经网络模型，包括回卷层和 Transformer 层，以及一种基于 eigen 分解的 amino acid 编码方法，以帮助模型学习 Epitope 的表示。</li>
<li>results: 实验结果表明，提出的方法有效地预测了 B 细胞 Epitope，并与其他竞争方法相比，表现出色。<details>
<summary>Abstract</summary>
The process of identifying and characterizing B-cell epitopes, which are the portions of antigens recognized by antibodies, is important for our understanding of the immune system, and for many applications including vaccine development, therapeutics, and diagnostics. Computational epitope prediction is challenging yet rewarding as it significantly reduces the time and cost of laboratory work. Most of the existing tools do not have satisfactory performance and only discriminate epitopes from non-epitopes. This paper presents a new deep learning-based multi-task framework for linear B-cell epitope prediction as well as antibody type-specific epitope classification. Specifically, a sequenced-based neural network model using recurrent layers and Transformer blocks is developed. We propose an amino acid encoding method based on eigen decomposition to help the model learn the representations of epitopes. We introduce modifications to standard cross-entropy loss functions by extending a logit adjustment technique to cope with the class imbalance. Experimental results on data curated from the largest public epitope database demonstrate the validity of the proposed methods and the superior performance compared to competing ones.
</details>
<details>
<summary>摘要</summary>
“识别和Characterizing B细胞结构，即抗体认知的部分，是免疫系统理解和许多应用，如疫苗开发、治疗和诊断。 computation epitope prediction 是一个挑战性的任务，但是可以快速地减少实验室工作的时间和成本。 现有的工具中，只有一些具有不 satisfactory 的表现，并只能区分 epitope 和 non-epitope。 本文提出了一个新的深度学习基于多任务框架 Linear B细胞结构预测，以及抗体类型特定的 epitope 分类。 具体来说，我们开发了一个序列化的神经网络模型，使用回归层和 transformer 层。 我们提出了一个使用 eigen decomposition 来编码氨基酸的方法，帮助模型学习 epitope 的表现。 我们还对标准的十字熵损失函数进行修改，以处理类别偏好。 实验结果显示，提出的方法是有效的，并且与竞争方法相比，表现更好。”Note: "Simplified Chinese" is used here to refer to the written form of Chinese that uses simpler characters and grammar, as opposed to "Traditional Chinese" which is the more complex and traditional form of written Chinese.
</details></li>
</ul>
<hr>
<h2 id="Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI"><a href="#Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI" class="headerlink" title="Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI"></a>Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02065">http://arxiv.org/abs/2309.02065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Wright, Christian Igel, Gabrielle Samuel, Raghavendra Selvan</li>
<li>for: 本研究旨在探讨机器学习（ML）技术的环境可持续性问题，尤其是计算机浪费和能源消耗的问题。</li>
<li>methods: 本文使用了多种技术和非技术方法来探讨ML技术的环境可持续性问题，包括计算机浪费和能源消耗的分析、环境影响分析等。</li>
<li>results: 本文表明，提高ML技术的效率并不能够完全解决其环境可持续性问题。作者提出了一种系统思维的方法来解决这些问题，即考虑ML技术与其他变量之间的互动关系，以提高ML技术的环境可持续性。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning (DL) which have accelerated progress on many tasks thought to be out of reach of AI. These ML methods can often be compute hungry, energy intensive, and result in significant carbon emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts including and beyond carbon emissions. The solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the efficiency with which ML systems operate in terms of both compute and energy consumption. In this perspective, we argue that efficiency alone is not enough to make ML as a technology environmentally sustainable. We do so by presenting three high level discrepancies between the effect of efficiency on the environmental sustainability of ML when considering the many variables which it interacts with. In doing so, we comprehensively demonstrate, at multiple levels of granularity both technical and non-technical reasons, why efficiency is not enough to fully remedy the environmental impacts of ML. Based on this, we present and argue for systems thinking as a viable path towards improving the environmental sustainability of ML holistically.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>计算和能源占用的差异：虽然提高 ML 系统的效率可以减少计算和能源占用，但是这些减少的量可能并不足以抵消 ML 系统的总碳排放。2. 数据和模型的差异：ML 系统的数据和模型可能会带来额外的环境影响，例如数据收集和处理的能源占用、模型的训练和测试所需的计算资源等。3. 生产和供应链的差异：ML 系统的生产和供应链可能会带来额外的环境影响，例如硬件生产和运输的能源占用、物资和设备的生产和供应等。因此，我们认为，只是提高 ML 系统的效率不足以全面解决 ML 对环境的影响。我们提出了以系统思维为基础的可持续性解决方案，以确保 ML 技术的发展和应用不会对环境产生负面影响。</details></li>
</ol>
<hr>
<h2 id="MvFS-Multi-view-Feature-Selection-for-Recommender-System"><a href="#MvFS-Multi-view-Feature-Selection-for-Recommender-System" class="headerlink" title="MvFS: Multi-view Feature Selection for Recommender System"></a>MvFS: Multi-view Feature Selection for Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02064">http://arxiv.org/abs/2309.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjune Lee, Yeongjong Jeong, Keunchan Park, SeongKu Kang</li>
<li>for: 提高推荐系统中功能选择的精度和效果，适应不同数据范围内的特征场景。</li>
<li>methods: 基于多视图网络，每个子网络学习不同特征模式下的特征重要性评估方法，以避免主导特征的偏袋问题，提高功能选择的多样性和效果。</li>
<li>results: 对实际数据进行实验，与现状的基准方法相比，提出了更高的精度和效果。<details>
<summary>Abstract</summary>
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Feature selection，即在推荐系统中选择关键特征的技术，在研究中得到了越来越多的注意。最近，Adaptive Feature Selection（AdaFS）表现出了很好的表现，它可以适应选择每个数据实例中的特征，因为特征的重要性可以在数据中差异很大。然而，AdaFS方法仍然存在一些限制，即它的选择过程可能会受到主要特征的偏袋偏见。为解决这些问题，我们提议了 Multi-view Feature Selection（MvFS），它可以更有效地选择每个实例中的信息特征。最重要的是，MvFS使用了多视图网络，每个子网络都学习了不同特征模式下数据中的特征重要性。这样，MvFS可以减少偏袋偏见问题，并且促进了更加平衡的特征选择过程。另外，MvFS采用了一种独立应用于每个字段的重要性分数模型化策略，不会产生特征之间的依赖关系。实验结果表明，MvFS比 estado-of-the-art 基准方法更有效。
</details></li>
</ul>
<hr>
<h2 id="No-Regret-Caching-with-Noisy-Request-Estimates"><a href="#No-Regret-Caching-with-Noisy-Request-Estimates" class="headerlink" title="No-Regret Caching with Noisy Request Estimates"></a>No-Regret Caching with Noisy Request Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02055">http://arxiv.org/abs/2309.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ben Mazziane, Francescomaria Faticanti, Giovanni Neglia, Sara Alouf</li>
<li>for: 本研究旨在设计缓存策略，以满足在高负载和&#x2F;或内存缓存环境下的请求序列预测不准确情况。</li>
<li>methods: 本研究提出了噪声跟踪领导者（NFPL）算法，这是经典跟踪领导者（FPL）算法的一种变种，当请求估计噪声时，它可以提供下线 regret 的保证。</li>
<li>results: 研究人员通过比较传统缓存策略和提出的方案，并在实验中验证了提出的方法在做出缓存决策时的性能。<details>
<summary>Abstract</summary>
Online learning algorithms have been successfully used to design caching policies with regret guarantees. Existing algorithms assume that the cache knows the exact request sequence, but this may not be feasible in high load and/or memory-constrained scenarios, where the cache may have access only to sampled requests or to approximate requests' counters. In this paper, we propose the Noisy-Follow-the-Perturbed-Leader (NFPL) algorithm, a variant of the classic Follow-the-Perturbed-Leader (FPL) when request estimates are noisy, and we show that the proposed solution has sublinear regret under specific conditions on the requests estimator. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under both synthetic and real request traces.
</details>
<details>
<summary>摘要</summary>
在线学习算法已经成功地设计了储存策略，并提供了 regret 保证。现有的算法假设缓存知道精确的请求序列，但在高负载和/或内存受限的情况下，这可能无法实现。在这篇论文中，我们提出了受扰引 perturbed 领袖（NFPL）算法，这是 класи的 Follow-the-Perturbed-Leader（FPL）在请求估计不精确时的变形，并证明了我们的解决方案具有下线性 regret 的特性。实验评估比较了我们的解决方案与传统的储存策略，并在实验中验证了我们的方法在实际请求追踪中的可行性。
</details></li>
</ul>
<hr>
<h2 id="Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning"><a href="#Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning" class="headerlink" title="Model-agnostic network inference enhancement from noisy measurements via curriculum learning"></a>Model-agnostic network inference enhancement from noisy measurements via curriculum learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02050">http://arxiv.org/abs/2309.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Wu, Yuanyuan Li, Jing Liu</li>
<li>For: The paper aims to enhance the performance of network inference models in the presence of noise.* Methods: The proposed framework leverages curriculum learning to mitigate the impact of noisy samples on network inference models. It is model-agnostic and can be integrated into various model-based and model-free network inference methods.* Results: The proposed framework demonstrates substantial performance augmentation under varied noise types, particularly when clean samples are abundant. It outperforms existing methods in various synthetic and real-world networks with diverse nonlinear dynamic processes.Here is the information in Simplified Chinese text:* For: 本文旨在增强网络推理模型在噪声中的性能。* Methods: 提议的框架利用课程学习来减轻噪声样本对网络推理模型的影响。它是模型无关的，可以与多种模型基于和模型自由的网络推理方法集成。* Results: 提议的框架在不同的噪声类型下示出了显著的性能提升，特别在有多个清晰样本的情况下。它比现有方法在多种 sintetic 和实际网络中表现出色，包括不同的非线性动力过程。<details>
<summary>Abstract</summary>
Noise is a pervasive element within real-world measurement data, significantly undermining the performance of network inference models. However, the quest for a comprehensive enhancement framework capable of bolstering noise resistance across a diverse array of network inference models has remained elusive. Here, we present an elegant and efficient framework tailored to amplify the capabilities of network inference models in the presence of noise. Leveraging curriculum learning, we mitigate the deleterious impact of noisy samples on network inference models. Our proposed framework is model-agnostic, seamlessly integrable into a plethora of model-based and model-free network inference methods. Notably, we utilize one model-based and three model-free network inference methods as the foundation. Extensive experimentation across various synthetic and real-world networks, encapsulating diverse nonlinear dynamic processes, showcases substantial performance augmentation under varied noise types, particularly thriving in scenarios enriched with clean samples. This framework's adeptness in fortifying both model-free and model-based network inference methodologies paves the avenue towards a comprehensive and unified enhancement framework, encompassing the entire spectrum of network inference models. Available Code: https://github.com/xiaoyuans/MANIE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>噪声是现实世界测量数据中的一种普遍存在的元素，对网络推理模型的性能产生重要的损害。然而，找到一个全面提升框架，能够在多种网络推理模型中增强噪声抗性，一直是艰难的探索。在这里，我们提出了一种简洁而高效的框架，用于增强网络推理模型在噪声中的性能。通过课程学习，我们将噪声样本的负面影响降至最低。我们的提案的框架是模型无关的，可以顺利地与多种模型基于和模型自由的网络推理方法集成。特别是，我们使用了一个模型基于的和三个模型自由的网络推理方法作为基础。广泛的实验表明，我们的框架在不同的噪声类型下表现出了显著的性能提升，特别是在含有干净样本的场景下卓越。这种框架的强大性在增强模型基于和模型自由的网络推理方法中表现出了一个普遍的和完整的提升框架，开启了涵盖整个网络推理模型谱系的全面提升框架的新可能性。可以在 GitHub 上获取代码：https://github.com/xiaoyuans/MANIE。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization"><a href="#Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization" class="headerlink" title="Probabilistic Self-supervised Learning via Scoring Rules Minimization"></a>Probabilistic Self-supervised Learning via Scoring Rules Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02048">http://arxiv.org/abs/2309.02048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Vahidi, Simon Schoßer, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei<br>for:本文提出了一种新的probabilistic自监学习方法（ProSMIN），用于提高表示质量并避免表示塌积。methods:方法使用了两个神经网络：在线网络和目标网络，它们之间协作学习不同视图下的样本表示的多样性分布。通过知识传播，两个网络学习对方的表示。results:本文通过多种下游任务的实验表明，ProSMIN可以获得superior的准确率和准确性。在ImageNet-O和ImageNet-C大规模 dataset上，ProSMIN超过了自我监督基线，表明其可扩展性和实际应用性。<details>
<summary>Abstract</summary>
In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation quality. We evaluate our probabilistic model on various downstream tasks, such as in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning. Our method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on large-scale datasets like ImageNet-O and ImageNet-C, ProSMIN demonstrates its scalability and real-world applicability.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的概率自助学习方法，即 Scoring Rule Minimization（ProSMIN），该方法利用概率模型来提高表示质量并避免表示塌雷。我们的提议方法包括两个神经网络：在线网络和目标网络，这两个网络共同学习输入样本的多样化分布。我们通过知识传承来让在线网络预测目标网络对同一个样本的不同扩展视图的表示。我们使用我们新的损失函数基于适当的分数规则来训练这两个网络。我们提供了对ProSMIN的优化过程的理论证明，证明其修改后的分数规则的稳定性。这种理论支持ProSMIN的优化过程，并且对其效果和可靠性做出了贡献。我们在各种下游任务上评估了我们的概率模型，包括在样本集中的概率泛化、样本集外的概率检测、数据集损害、低shot学习和传输学习。我们的方法在各种实验中都达到了更高的准确率和准确性，比自助学习基eline在广泛的实验中表现出更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies"><a href="#Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies" class="headerlink" title="Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies"></a>Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02045">http://arxiv.org/abs/2309.02045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yajing Wang, Zongwei Luo</li>
<li>for: 这篇论文旨在提高大型自然语言模型（LLMs）在特定任务中的表现， Specifically, the paper aims to enhance the performance of large language models (LLMs) in sentiment analysis tasks using prompting strategies.</li>
<li>methods: 该论文使用了两种新的提问策略，namely RolePlaying (RP) prompting和Chain-of-thought (CoT) prompting，并提出了RP-CoT提问策略。 These methods include two novel prompting strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting, as well as the RP-CoT prompting strategy.</li>
<li>results: 实验结果表明，采用提出的提问策略可以明显提高 sentiment analysis 精度。 The results demonstrate that the adoption of the proposed prompting strategies leads to a significant enhancement in sentiment analysis accuracy. Additionally, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate that the adoption of the proposed prompting strategies leads to a increasing enhancement in sentiment analysis accuracy. Further, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Generative-Inverse-Design"><a href="#Diffusion-Generative-Inverse-Design" class="headerlink" title="Diffusion Generative Inverse Design"></a>Diffusion Generative Inverse Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02040">http://arxiv.org/abs/2309.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Tatiana López-Guevara, Kelsey Allen, Peter Battaglia, Arnaud Doucet, Kimberley Stachenfeld</li>
<li>for: 这种 inverse design 问题的目的是优化输入参数，以实现目标结果。</li>
<li>methods: 这种方法使用 graph neural networks (GNNs) 来估算 simulator 动态，并使用 gradient-或 sampling-based optimization 进行优化。</li>
<li>results: 这种方法可以高效地解决 inverse design 问题，并且可以减少对 simulator 的调用次数。<details>
<summary>Abstract</summary>
Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems.In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving their efficiency. We perform experiments on a number of fluid dynamics design challenges, and find that our approach substantially reduces the number of calls to the simulator compared to standard techniques.
</details>
<details>
<summary>摘要</summary>
<<SYS>> inverse 设计指的是优化输入对象函数以实现目标结果。许多实际工程问题中，目标函数通常是一个预测系统状态在时间演变的模拟器，而设计挑战是确定初始条件以实现目标结果。现在的学习模拟技术发展已经显示了 Graph Neural Networks (GNNs) 可以用于精度、效率、可导的模拟器动力学Estimation，支持高质量的设计优化过程。然而，从头来设计需要许多昂贵的模拟器调用，这些过程在非凸或高维问题上存在基本错误。在这种情况下，我们表示denoising diffusion models (DDMs) 可以高效地解决 inverse 设计问题，并提出一种粒子抽样算法以进一步提高效率。我们在一些 fluid dynamics 设计挑战中进行了实验，发现我们的方法可以减少对模拟器的调用次数相比标准技术。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models"><a href="#Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models" class="headerlink" title="Data-Juicer: A One-Stop Data Processing System for Large Language Models"></a>Data-Juicer: A One-Stop Data Processing System for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02033">http://arxiv.org/abs/2309.02033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for: 提高大语言模型（LLM）数据处理的效率和可扩展性，以推动LLM应用研究。</li>
<li>methods: 提供一个易用、可扩展的数据处理系统，包括50多种可编程Operator和可插入工具，以满足不同LLM数据处理需求。</li>
<li>results: 对多种预训练和后调整用 caso，通过自动评估和视觉化评估，实现了较大的LLAMA性能提升（最高7.45%），并且在分布式计算环境下实现了大幅提高的处理效率和可扩展性。<details>
<summary>Abstract</summary>
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, diverse, and high-quality data. Despite this, existing open-source tools for LLM data processing remain limited and mostly tailored to specific datasets, with an emphasis on the reproducibility of released data over adaptability and usability, inhibiting potential applications. In response, we propose a one-stop, powerful yet flexible and user-friendly LLM data processing system named Data-Juicer. Our system offers over 50 built-in versatile operators and pluggable tools, which synergize modularity, composability, and extensibility dedicated to diverse LLM data processing needs. By incorporating visualized and automatic evaluation capabilities, Data-Juicer enables a timely feedback loop to accelerate data processing and gain data insights. To enhance usability, Data-Juicer provides out-of-the-box components for users with various backgrounds, and fruitful data recipes for LLM pre-training and post-tuning usages. Further, we employ multi-facet system optimization and seamlessly integrate Data-Juicer with both LLM and distributed computing ecosystems, to enable efficient and scalable data processing. Empirical validation of the generated data recipes reveals considerable improvements in LLaMA performance for various pre-training and post-tuning cases, demonstrating up to 7.45% relative improvement of averaged score across 16 LLM benchmarks and 16.25% higher win rate using pair-wise GPT-4 evaluation. The system's efficiency and scalability are also validated, supported by up to 88.7% reduction in single-machine processing time, 77.1% and 73.1% less memory and CPU usage respectively, and 7.91x processing acceleration when utilizing distributed computing ecosystems. Our system, data recipes, and multiple tutorial demos are released, calling for broader research centered on LLM data.
</details>
<details>
<summary>摘要</summary>
大量的语言模型（LLM）演化带来了数据处理的重要性。然而，现有的开源工具仍然有限，主要是针对特定数据集，强调数据重现性而不是应用和用户性，这限制了其应用前景。为此，我们提出了一个一站式、强大且灵活的 LLM 数据处理系统 named Data-Juicer。我们的系统提供了50多种可选的强大操作和可插入工具，这些工具结合了模块性、可组合性和扩展性，以适应不同的 LLM 数据处理需求。通过添加可视化和自动评估功能，Data-Juicer 可以帮助用户快速获得数据处理和数据 Insight。为了提高可用性，Data-Juicer 提供了为用户们提供了不同背景的各种组件，以及 LLMA 预训练和后处理的数据荚。此外，我们使用多方面的优化和与 LLMA 和分布式计算环境集成，以实现高效和可扩展的数据处理。我们的实验表明，生成的数据荚可以提高 LLMA 的性能，在16个 LLMA benchmark 和16个 GPT-4 评价中得到了7.45%的相对提高，并且在对照竞赛中获得了16.25%更高的胜率。系统的效率和扩展性也得到了验证，包括单机处理时间减少88.7%，内存和CPU使用量减少77.1%和73.1%，并且在使用分布式计算环境时实现了7.91倍的处理加速。我们的系统、数据荚和多个教程示例都已经发布，呼吁更广泛的 LLM 数据研究。
</details></li>
</ul>
<hr>
<h2 id="Non-Parametric-Representation-Learning-with-Kernels"><a href="#Non-Parametric-Representation-Learning-with-Kernels" class="headerlink" title="Non-Parametric Representation Learning with Kernels"></a>Non-Parametric Representation Learning with Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02028">http://arxiv.org/abs/2309.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Pascal Esser, Maximilian Fleissner, Debarghya Ghoshdastidar</li>
<li>for: 本研究旨在探讨无监督学习和自监督学习中的表示学习方法，尤其是使用内核函数来实现表示学习。</li>
<li>methods: 本研究使用了内核自适应学习模型，包括对冲积分别学习模型和自适应编码器模型。</li>
<li>results: 研究人员通过新的表示函数定理和总体抽象误差下界来分析和评估内核表示学习方法的性能。在小数据 regime中和对比神经网络模型的情况下，内核表示学习方法表现良好。<details>
<summary>Abstract</summary>
Unsupervised and self-supervised representation learning has become popular in recent years for learning useful features from unlabelled data. Representation learning has been mostly developed in the neural network literature, and other models for representation learning are surprisingly unexplored. In this work, we introduce and analyze several kernel-based representation learning approaches: Firstly, we define two kernel Self-Supervised Learning (SSL) models using contrastive loss functions and secondly, a Kernel Autoencoder (AE) model based on the idea of embedding and reconstructing data. We argue that the classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices. We further derive generalisation error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with neural network based models.
</details>
<details>
<summary>摘要</summary>
“无监督和自监督表征学学习在最近几年内得到了广泛的推广，它们可以从无标签数据中学习有用的特征。表征学学习主要发展在神经网络文献中，而其他模型的表征学学习则尚未得到充分的探索。在这项工作中，我们引入并分析了一些基于核函数的表征学学习方法：首先，我们定义了两种核Self-Supervised Learning（SSL）模型，使用对比损失函数来学习特征；其次，我们基于数据嵌入和重建的想法，提出了一种核自编码器（AE）模型。我们 argue that classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices。我们还derive generalization error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with神经网络基于模型。”Note that Simplified Chinese is a written language that uses shorter words and simpler grammar than Traditional Chinese. The translation is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length"><a href="#Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length" class="headerlink" title="Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length"></a>Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02027">http://arxiv.org/abs/2309.02027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katerina Hlavackova-Schindler, Anna Melnykova, Irene Tubikanec</li>
<li>for: 本研究使用多变量骨PK进行现实生活现象的模型，如地震、股票市场交易、神经元活动、病毒传播等。</li>
<li>methods: 本文使用抽象衰减函数和估计连接图，以及基于最小消息长度（MML）原理的优化 criterion 和模型选择算法，以估计骨PK中组件之间的Granger causal关系。</li>
<li>results: 比较多种类方法，包括lasso类似 penalty 方法，MML基于方法在短时间尺度下减少过拟合现象，并 achieved highest F1 scores in specific sparse graph settings。此外，通过应用于G7 sovereign bond数据，得到了一致的 causal connections，与专业知识一致。<details>
<summary>Abstract</summary>
Multivariate Hawkes processes (MHPs) are versatile probabilistic tools used to model various real-life phenomena: earthquakes, operations on stock markets, neuronal activity, virus propagation and many others. In this paper, we focus on MHPs with exponential decay kernels and estimate connectivity graphs, which represent the Granger causal relations between their components. We approach this inference problem by proposing an optimization criterion and model selection algorithm based on the minimum message length (MML) principle. MML compares Granger causal models using the Occam's razor principle in the following way: even when models have a comparable goodness-of-fit to the observed data, the one generating the most concise explanation of the data is preferred. While most of the state-of-art methods using lasso-type penalization tend to overfitting in scenarios with short time horizons, the proposed MML-based method achieves high F1 scores in these settings. We conduct a numerical study comparing the proposed algorithm to other related classical and state-of-art methods, where we achieve the highest F1 scores in specific sparse graph settings. We illustrate the proposed method also on G7 sovereign bond data and obtain causal connections, which are in agreement with the expert knowledge available in the literature.
</details>
<details>
<summary>摘要</summary>
多变量骨灰过程（MHP）是一种通用的概率工具，用于模型各种现实生活中的现象：地震、股票市场交易、神经活动、病毒传播等。在这篇论文中，我们关注MHP中的指数衰减kernel，并估计连接图，该图表示MHP中的预测关系。我们通过提出一个优化目标函数和基于最小消息长度（MML）原理的模型选择算法来实现这一目标。MML比较了不同预测模型，并根据奥卡姆的剃刀原理选择最简洁的解释。而大多数当前的方法使用拉asso-类型的约束减少倾向于过拟合，而我们提出的MML基本方法在短时间尺度下具有高F1分数。我们进行了一个数学研究，与其他相关的古典和当前方法进行比较，我们在特定的稀疏图设置中 achievement最高的F1分数。我们还应用了该方法于G7国家债券数据，并获得了一致的 causal 连接，与文献中的专家知识相符。
</details></li>
</ul>
<hr>
<h2 id="RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning"><a href="#RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning" class="headerlink" title="RDGSL: Dynamic Graph Representation Learning with Structure Learning"></a>RDGSL: Dynamic Graph Representation Learning with Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02025">http://arxiv.org/abs/2309.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, Yangyong Zhu</li>
<li>for: 本文主要研究如何在连续时间动态图中学习表示，以提高下游任务的效果。</li>
<li>methods: 本文提出了一种名为RDGSL的表示学习方法，同时也提出了一种名为动态图结构学习的新超级视图信号，以便帮助RDGSL更好地抗击动态图中的噪音。</li>
<li>results: 本文的实验结果表明，RDGSL可以在连续时间动态图中提供更好的表示，并且可以减少噪音的影响，从而提高下游任务的效果。特别是，RDGSL可以在动态图中提供5.1%绝对的AUC提升，比第二个基eline的表现更好。<details>
<summary>Abstract</summary>
Temporal Graph Networks (TGNs) have shown remarkable performance in learning representation for continuous-time dynamic graphs. However, real-world dynamic graphs typically contain diverse and intricate noise. Noise can significantly degrade the quality of representation generation, impeding the effectiveness of TGNs in downstream tasks. Though structure learning is widely applied to mitigate noise in static graphs, its adaptation to dynamic graph settings poses two significant challenges. i) Noise dynamics. Existing structure learning methods are ill-equipped to address the temporal aspect of noise, hampering their effectiveness in such dynamic and ever-changing noise patterns. ii) More severe noise. Noise may be introduced along with multiple interactions between two nodes, leading to the re-pollution of these nodes and consequently causing more severe noise compared to static graphs. In this paper, we present RDGSL, a representation learning method in continuous-time dynamic graphs. Meanwhile, we propose dynamic graph structure learning, a novel supervisory signal that empowers RDGSL with the ability to effectively combat noise in dynamic graphs. To address the noise dynamics issue, we introduce the Dynamic Graph Filter, where we innovatively propose a dynamic noise function that dynamically captures both current and historical noise, enabling us to assess the temporal aspect of noise and generate a denoised graph. We further propose the Temporal Embedding Learner to tackle the challenge of more severe noise, which utilizes an attention mechanism to selectively turn a blind eye to noisy edges and hence focus on normal edges, enhancing the expressiveness for representation generation that remains resilient to noise. Our method demonstrates robustness towards downstream tasks, resulting in up to 5.1% absolute AUC improvement in evolving classification versus the second-best baseline.
</details>
<details>
<summary>摘要</summary>
天时グラフネットワーク（TGN）の表现学习は、连続的时间のグラフで优れた性能を示しています。しかし、実世界の动的グラフでは、多様で复雑なノイズが存在します。このノイズは、表现生成の质を低下させ、TGNの下流タスクでの效果を妨げます。避けるために、构造学习が広く使用されますが、この动的グラフの设定では、2つの大きな挑戦を提起します。i) 时间的なノイズ。既存の构造学习メソッドは、时间的な侧面を考虑していません、これらのノイズを效果的に処理することができません。ii) より强いノイズ。ノイズは、2つのノード间の相互作用によって导入され、それらのノードを污染することで、より强いノイズを生成します。この研究では、RDGSL（时间グラフ表现学习）を提案しています。また、动的グラフ构造学习（DGSL）という新しいスーパーバイザー信号を提案しています。この信号を使用して、RDGSLは、动的グラフの中で效果的にノイズを処理することができます。ノイズの动的な侧面に対処するために、我々は、Dynamic Graph Filter（DGF）を导入しています。DGFは、现在と过去のノイズを考虑して、时间的なノイズを捉えることができます。さらに、Temporal Embedding Learner（TEL）を提案しています。TELは、注意メカニズムを使用して、ノイズのあるエッジに対して、注意を払わないことができます。これにより、正常なエッジに対する表现の生成が増强されます。RDGSLの方法は、下流タスクでのRobustnessを示しています。これにより、evolving classificationのAUCで5.1%の绝対値改善を示しています。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks"><a href="#Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks" class="headerlink" title="Dynamic Early Exiting Predictive Coding Neural Networks"></a>Dynamic Early Exiting Predictive Coding Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02022">http://arxiv.org/abs/2309.02022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alaa Zniber, Ouassim Karrakchou, Mounir Ghogho</li>
<li>for: 这篇论文主要关注于实时应用中的物联网感应器（IoT），如智能装备和健康监控等，它们生成大量数据，并且使用深度学习（DL）模型来进行智能处理。</li>
<li>methods: 这篇论文提出了一个使用预测代码理论和动态早期终止的浅层对称网络，以减少网络的参数和计算复杂度，并且与VGG-16模型在CIFAR-10影像分类任务中实现相似的准确性。</li>
<li>results: 这篇论文的结果显示，使用提案的浅层对称网络可以在CIFAR-10影像分类任务中实现相似的准确性，并且比VGG-16模型使用 fewer 参数和较低的计算复杂度。<details>
<summary>Abstract</summary>
Internet of Things (IoT) sensors are nowadays heavily utilized in various real-world applications ranging from wearables to smart buildings passing by agrotechnology and health monitoring. With the huge amounts of data generated by these tiny devices, Deep Learning (DL) models have been extensively used to enhance them with intelligent processing. However, with the urge for smaller and more accurate devices, DL models became too heavy to deploy. It is thus necessary to incorporate the hardware's limited resources in the design process. Therefore, inspired by the human brain known for its efficiency and low power consumption, we propose a shallow bidirectional network based on predictive coding theory and dynamic early exiting for halting further computations when a performance threshold is surpassed. We achieve comparable accuracy to VGG-16 in image classification on CIFAR-10 with fewer parameters and less computational complexity.
</details>
<details>
<summary>摘要</summary>
互联网物件（IoT）感应器目前在不同的实际应用中广泛使用，从戴式设备到智能建筑，途经农科技和健康监控。这些小型设备生成的巨量数据，深度学习（DL）模型已经广泛地应用以提高他们的智能处理。然而，随着设备的小型化和精确度的提高，DL模型已经成为不可deploy的。因此，为了考虑硬件的限制，我们需要在设计过程中考虑硬件的限制。因此，受人脑的效率和低功耗所 inspirited，我们提出了一个浅层的双向网络，基于预测编码理论和动态早退出，可以在性能门槛超过时间点扩展运算。我们在CIFAR-10上进行图像分类 tasks，与VGG-16模型相比，我们的模型具有较少的参数和较低的计算复杂度，并且具有相似的准确性。
</details></li>
</ul>
<hr>
<h2 id="PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates"><a href="#PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates" class="headerlink" title="PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates"></a>PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02014">http://arxiv.org/abs/2309.02014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell</li>
<li>for: 解决大规模机器学习中的凸优化问题</li>
<li>methods: 使用笔记法实现随机梯度算法，包括SVRG、SAGA和Katyusha等方法，每种方法都有强的理论分析和有效的默认超参数值</li>
<li>results: 比较 tradicional stochastic gradient methods的性能，通过default超参数值，提出的方法在51个ridge和logistic regression问题上表现较好或与通过手动调整超参数值的方法匹配，并在理论上引入了quadratic regularity来确定线性减速的速度，该速度通常比condition number更紧Binding the convergence rate, and explains the fast global linear convergence of the proposed methods.<details>
<summary>Abstract</summary>
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine learning repositories. On the theoretical side, this paper introduces the notion of quadratic regularity in order to establish linear convergence of all proposed methods even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice, and explains the fast global linear convergence of the proposed methods.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了PROMISE（预conditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates），一个基于笔记的预conditioned随机梯度算法集，用于解决大规模凸优化问题。PROMISE包括预conditioned SVRG、SAGA和Katyusha算法，每个算法都有强大的理论分析和有效的默认超参数值。与传统的随机梯度方法相比，PROMISE的算法不需要精心调整超参数，并且在缺conditioning现象存在时表现更好。在实际中，我们通过显示使用默认超参数值时，PROMISE的算法可以超越或与优化的随机梯度优化器相当的性能。从理论角度来看，这篇论文引入了quadratic regularity的概念，以确定预conditioner更新不频繁时的线性准确率。预conditioner更新频繁时的准确率速度是quadratic regularity比率，这经常提供了更紧的 convergence rate bound，并且在理论和实践中都能够解释PROMISE算法的快速全局线性准确率。
</details></li>
</ul>
<hr>
<h2 id="iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation"><a href="#iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation" class="headerlink" title="iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation"></a>iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02012">http://arxiv.org/abs/2309.02012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Xixi Wu, Yiheng Sun, Jiawei Zhang</li>
<li>for: 这个研究旨在提出一个具有快速更新和长期模型的动态图模型方法，以应对实际应用中的金融风险管理和骗案检测等问题。</li>
<li>methods: 这个方法使用了具有适应短期更新和长期模型的Adaptive Short-term Updater和Long-term Updater两部分，以解决现有方法中的三大限制：随机更新、不足的长期模型和缺乏重复性模型。具体来说，这个方法使用了Identity Attention机制，将Transformer-based updater与RNN-dominated设计相组合，以提高实现效果。</li>
<li>results: 这个研究的实验结果显示，iLoRE方法能够具有更高的效果和快速性，在实际应用中的金融风险管理和骗案检测等领域中表现出色。<details>
<summary>Abstract</summary>
Continuous-time dynamic graph modeling is a crucial task for many real-world applications, such as financial risk management and fraud detection. Though existing dynamic graph modeling methods have achieved satisfactory results, they still suffer from three key limitations, hindering their scalability and further applicability. i) Indiscriminate updating. For incoming edges, existing methods would indiscriminately deal with them, which may lead to more time consumption and unexpected noisy information. ii) Ineffective node-wise long-term modeling. They heavily rely on recurrent neural networks (RNNs) as a backbone, which has been demonstrated to be incapable of fully capturing node-wise long-term dependencies in event sequences. iii) Neglect of re-occurrence patterns. Dynamic graphs involve the repeated occurrence of neighbors that indicates their importance, which is disappointedly neglected by existing methods. In this paper, we present iLoRE, a novel dynamic graph modeling method with instant node-wise Long-term modeling and Re-occurrence preservation. To overcome the indiscriminate updating issue, we introduce the Adaptive Short-term Updater module that will automatically discard the useless or noisy edges, ensuring iLoRE's effectiveness and instant ability. We further propose the Long-term Updater to realize more effective node-wise long-term modeling, where we innovatively propose the Identity Attention mechanism to empower a Transformer-based updater, bypassing the limited effectiveness of typical RNN-dominated designs. Finally, the crucial re-occurrence patterns are also encoded into a graph module for informative representation learning, which will further improve the expressiveness of our method. Our experimental results on real-world datasets demonstrate the effectiveness of our iLoRE for dynamic graph modeling.
</details>
<details>
<summary>摘要</summary>
continuous-time dynamic graph modeling是现实世界中许多应用场景中的关键任务，如金融风险管理和欺诈检测。虽然现有的动态图模型方法已经达到了满意的结果，但它们仍然受到三个关键限制，阻碍其扩展性和更多的应用。i) 无分别更新。对于进来的边，现有的方法都会无分别地处理它们，这可能会导致更多的时间消耗和意外的噪音信息。ii) 不够的节点长期模型。它们都是基于循环神经网络（RNN）的后备，已经证明不能够全面捕捉节点长期关系在事件序列中。iii) 忽略重复模式。动态图中的重复 neighbbors 表明其重要性，这一点抑制地被现有的方法忽略。在这篇论文中，我们提出了 iLoRE，一种基于实时节点长期模型和重复保持的动态图模型方法。为了解决无分别更新问题，我们引入了自适应短期更新模块，可以自动抛弃无用或噪音的边，使 iLoRE 具有更高的效iveness 和实时能力。此外，我们还提出了长期更新模块，以实现更有效的节点长期模型，我们创新地提出了标识注意力机制，使用Transformer-based更新器，绕过传统的 RNN-主导的设计的局限性。最后，我们还编码了重复模式到图模块，以便更好地学习表达能力。我们的实验结果表明，iLoRE 可以有效地进行动态图模型。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-Dynamics-of-Self-Supervised-Models"><a href="#Representation-Learning-Dynamics-of-Self-Supervised-Models" class="headerlink" title="Representation Learning Dynamics of Self-Supervised Models"></a>Representation Learning Dynamics of Self-Supervised Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02011">http://arxiv.org/abs/2309.02011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Esser, Satyaki Mukherjee, Debarghya Ghoshdastidar</li>
<li>for: 这 paper 旨在研究自助学习（SSL）模型中的学习动态，具体来说是 SSL 模型中的表示得到的学习过程。</li>
<li>methods: 这 paper 使用了多元回归的扩展来研究 SSL 模型的学习动态，并在这个过程中引入了正交约束来保证模型的学习。</li>
<li>results: 这 paper 发现，使用Gradient Descent在Grassmannian manifold上训练 SSL 模型时，模型会学习简单的标准差表示，并且会导致维度减少问题。此外， paper 还发现，在无穷宽扩展下，SSL 模型与超级vised模型的 neural tangent kernel 不同。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) is an important paradigm for learning representations from unlabelled data, and SSL with neural networks has been highly successful in practice. However current theoretical analysis of SSL is mostly restricted to generalisation error bounds. In contrast, learning dynamics often provide a precise characterisation of the behaviour of neural networks based models but, so far, are mainly known in supervised settings. In this paper, we study the learning dynamics of SSL models, specifically representations obtained by minimising contrastive and non-contrastive losses. We show that a naive extension of the dymanics of multivariate regression to SSL leads to learning trivial scalar representations that demonstrates dimension collapse in SSL. Consequently, we formulate SSL objectives with orthogonality constraints on the weights, and derive the exact (network width independent) learning dynamics of the SSL models trained using gradient descent on the Grassmannian manifold. We also argue that the infinite width approximation of SSL models significantly deviate from the neural tangent kernel approximations of supervised models. We numerically illustrate the validity of our theoretical findings, and discuss how the presented results provide a framework for further theoretical analysis of contrastive and non-contrastive SSL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning"><a href="#Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning" class="headerlink" title="Establishing a real-time traffic alarm in the city of Valencia with Deep Learning"></a>Establishing a real-time traffic alarm in the city of Valencia with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02010">http://arxiv.org/abs/2309.02010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Folgado, Veronica Sanz, Johannes Hirn, Edgar Lorenzo-Saez, Javier Urchueguia</li>
<li>For: 这项研究的目的是分析城市Valencia的交通凝固和污染之间的关系，以及开发一种能够预测下一30分钟内交通异常高峰的警示系统。* Methods: 该研究使用了交通数据更新每10分钟，以及Long Short-Term Memory（LSTM）神经网络来预测下一30分钟内交通异常高峰。* Results: 研究结果表明，交通凝固对某些污染物（特别是$\text{NO}_\text{x}$）水平有显著影响。<details>
<summary>Abstract</summary>
Urban traffic emissions represent a significant concern due to their detrimental impacts on both public health and the environment. Consequently, decision-makers have flagged their reduction as a crucial goal. In this study, we first analyze the correlation between traffic flux and pollution in the city of Valencia, Spain. Our results demonstrate that traffic has a significant impact on the levels of certain pollutants (especially $\text{NO}_\text{x}$). Secondly, we develop an alarm system to predict if a street is likely to experience unusually high traffic in the next 30 minutes, using an independent three-tier level for each street. To make the predictions, we use traffic data updated every 10 minutes and Long Short-Term Memory (LSTM) neural networks. We trained the LSTM using traffic data from 2018, and tested it using traffic data from 2019.
</details>
<details>
<summary>摘要</summary>
城市交通排放对公共健康和环境产生了重要的影响，因此决策者们认为减少它们是一项重要的目标。在本研究中，我们首先分析了城市劳 Valle 的交通流量和污染物之间的相关性。我们的结果表明，交通具有对某些污染物（尤其是 $\text{NO}_\text{x}$）的显著影响。其次，我们开发了一个警示系统，可以预测下一30分钟内某条街道是否会出现不寻常高的交通流量，使用独立的三级水平 для每条街道。为了进行预测，我们使用了每10分钟更新的交通数据和Long Short-Term Memory（LSTM）神经网络。我们使用2018年的交通数据进行训练，并在2019年的交通数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Correlated-Estimations-with-Almost-no-Training"><a href="#Aggregating-Correlated-Estimations-with-Almost-no-Training" class="headerlink" title="Aggregating Correlated Estimations with (Almost) no Training"></a>Aggregating Correlated Estimations with (Almost) no Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02005">http://arxiv.org/abs/2309.02005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Delemazure, François Durand, Fabien Mathieu</li>
<li>for: 解决各种各样的决策问题，因为许多问题无法得到确切的解决方案。</li>
<li>methods: 使用多种估计算法分配不同选项的分数，并考虑估计错误之间的相关性。</li>
<li>results: 比较不同的聚合规则在不同的数据集上的性能，并结论使用最大似然聚合规则在知道估计错误之间的相关性时表现最好，否则推荐使用嵌入式投票法（Embedded Voting，EV）。<details>
<summary>Abstract</summary>
Many decision problems cannot be solved exactly and use several estimation algorithms that assign scores to the different available options. The estimation errors can have various correlations, from low (e.g. between two very different approaches) to high (e.g. when using a given algorithm with different hyperparameters). Most aggregation rules would suffer from this diversity of correlations. In this article, we propose different aggregation rules that take correlations into account, and we compare them to naive rules in various experiments based on synthetic data. Our results show that when sufficient information is known about the correlations between errors, a maximum likelihood aggregation should be preferred. Otherwise, typically with limited training data, we recommend a method that we call Embedded Voting (EV).
</details>
<details>
<summary>摘要</summary>
许多决策问题无法准确解决，通常使用多种估计算法计算不同选项的得分。估计误差之间可能存在各种相关性，从低（例如两种完全不同的方法）到高（例如使用同一算法不同的超参数）。大多数聚合规则会受到这种多样性的影响。在这篇文章中，我们提出了考虑相关性的不同聚合规则，并与简单规则进行了多个实验，基于 синтетиче数据。我们的结果表明，当知道估计误差之间的相关性信息足够时，最大极化聚合应该被首选。否则，通常在有限训练数据的情况下，我们建议一种我们称为嵌入式投票（EV）方法。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge"><a href="#Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge" class="headerlink" title="Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge"></a>Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02001">http://arxiv.org/abs/2309.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Stoica, Mihaela Breaban, Vlad Barbu</li>
<li>for: 提高医疗影像3D分割模型的性能，使其更好地适应不同来源的数据。</li>
<li>methods: 研究如何在训练过程中减少频率域转移，使新收集的数据更好地与原始训练数据结合使用。</li>
<li>results: 比较 histogram matching 和标准化两种方法，发现 histogram matching 能够更好地减少频率域转移，提高模型的性能。<details>
<summary>Abstract</summary>
Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
</details>
<details>
<summary>摘要</summary>
使用更多训练数据可以提高结果，特别是医疗影像三维分割，因为这个领域缺乏训练材料，模型需要将少量可用的数据泛化好。然而，新的数据可能是使用不同的仪器获取的，其分布与原始训练数据有所不同。因此，我们研究如何在训练过程中缓解领域偏移，使得附加数据更好地可用于预处理和训练。我们的结果表明，使用 histogram matching 将附加数据转换可以获得更好的结果，比使用简单的 норmalization。
</details></li>
</ul>
<hr>
<h2 id="sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation"><a href="#sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation" class="headerlink" title="sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation"></a>sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01988">http://arxiv.org/abs/2309.01988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunyang Zhang, Senzhang Wang, Xianzhen Tan, Ruochen Liu, Jian Zhang, Jianxin Wang</li>
<li>for: 填充空缺的空间时间序列数据，以提高智能交通和空气质量监测等实际应用中的性能。</li>
<li>methods: 提出了一种自适应噪声扩大演Diffusion Model named SaSDim，包括新的损失函数和透视模块，以更好地捕捉空间时间序列数据的动态相互关系。</li>
<li>results: 对三个实际数据集进行了广泛的实验，证明SaSDim可以与当前状态艺术基准相比，更高效地完成空间时间序列填充任务。<details>
<summary>Abstract</summary>
Spatial time series imputation is critically important to many real applications such as intelligent transportation and air quality monitoring. Although recent transformer and diffusion model based approaches have achieved significant performance gains compared with conventional statistic based methods, spatial time series imputation still remains as a challenging issue due to the complex spatio-temporal dependencies and the noise uncertainty of the spatial time series data. Especially, recent diffusion process based models may introduce random noise to the imputations, and thus cause negative impact on the model performance. To this end, we propose a self-adaptive noise scaling diffusion model named SaSDim to more effectively perform spatial time series imputation. Specially, we propose a new loss function that can scale the noise to the similar intensity, and propose the across spatial-temporal global convolution module to more effectively capture the dynamic spatial-temporal dependencies. Extensive experiments conducted on three real world datasets verify the effectiveness of SaSDim by comparison with current state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
空间时间序列填充是许多实际应用中的关键问题，如智能交通和空气质量监测。虽然最近的变换器和扩散模型基于方法已经实现了对传统统计学基于方法的显著性能提升，但是空间时间序列填充仍然是一个挑战性的问题，因为空间时间序列数据中存在复杂的空间时间依赖关系和噪声不确定性。尤其是最近的扩散过程基于模型可能会引入随机噪声到填充中，从而对模型性能产生负面影响。为此，我们提议一种自适应噪声扩大扩散模型 named SaSDim，以更有效地进行空间时间序列填充。特别是，我们提出了一个新的损失函数，可以尝试将噪声缩放到相似的强度，并提出了跨空间时间全球 convolution 模块，以更好地捕捉空间时间依赖关系的动态变化。广泛的实验在三个实际数据集上验证了 SaSDim 的有效性，与当前状态艺术基eline 进行比较。
</details></li>
</ul>
<hr>
<h2 id="Retail-store-customer-behavior-analysis-system-Design-and-Implementation"><a href="#Retail-store-customer-behavior-analysis-system-Design-and-Implementation" class="headerlink" title="Retail store customer behavior analysis system: Design and Implementation"></a>Retail store customer behavior analysis system: Design and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03232">http://arxiv.org/abs/2309.03232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dinh Nguyen, Keisuke Hihara, Tung Cao Hoang, Yumeka Utada, Akihiko Torii, Naoki Izumi, Nguyen Thanh Thuy, Long Quoc Tran<br>for: 本研究旨在提高顾客满意度，通过为顾客添加个性化价值。methods: 本研究使用了深度学习技术，包括深度神经网络，对顾客行为进行分析。results: 研究结果表明，使用深度学习技术可以更好地识别顾客行为，并提供更多的数据可视化。<details>
<summary>Abstract</summary>
Understanding customer behavior in retail stores plays a crucial role in improving customer satisfaction by adding personalized value to services. Behavior analysis reveals both general and detailed patterns in the interaction of customers with a store items and other people, providing store managers with insight into customer preferences. Several solutions aim to utilize this data by recognizing specific behaviors through statistical visualization. However, current approaches are limited to the analysis of small customer behavior sets, utilizing conventional methods to detect behaviors. They do not use deep learning techniques such as deep neural networks, which are powerful methods in the field of computer vision. Furthermore, these methods provide limited figures when visualizing the behavioral data acquired by the system. In this study, we propose a framework that includes three primary parts: mathematical modeling of customer behaviors, behavior analysis using an efficient deep learning based system, and individual and group behavior visualization. Each module and the entire system were validated using data from actual situations in a retail store.
</details>
<details>
<summary>摘要</summary>
理解顾客在商场中的行为对于提高顾客满意度非常重要，可以添加个性化的价值至服务。行为分析发现顾客与店内物品和其他人之间的交互，提供店长们顾客偏好的信息。现有的解决方案仅仅是通过传统方法来检测行为，不使用深度学习技术如深度神经网络，这些技术在计算机视觉领域是非常强大的。此外，这些方法只能提供有限的行为数据可视化图表。在本研究中，我们提出了一个框架，包括三个主要部分：顾客行为数学模型、深度学习基于系统Behavior分析和个人和组行为可视化。每个模块和整个系统都被使用实际情况的数据验证。
</details></li>
</ul>
<hr>
<h2 id="An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability"><a href="#An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability" class="headerlink" title="An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability"></a>An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01978">http://arxiv.org/abs/2309.01978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Qiu, Yu Lin, Inez Zwetsloot</li>
<li>for: 本研究旨在透过时间测量变化的资料进行偏差检测，并提出一个基于LSTM预测 интервал的控制图。</li>
<li>methods: 本研究使用了循环神经网络和其变体，并提出了一个基于LSTM预测 интервал的控制图。</li>
<li>results: 运算研究表明，提出的方法在处理时间变化的资料上具有更高的准确率和更好的性能，并且在实际应用中运行时间较短。<details>
<summary>Abstract</summary>
The recurrent neural network and its variants have shown great success in processing sequences in recent years. However, this deep neural network has not aroused much attention in anomaly detection through predictively process monitoring. Furthermore, the traditional statistic models work on assumptions and hypothesis tests, while neural network (NN) models do not need that many assumptions. This flexibility enables NN models to work efficiently on data with time-varying variability, a common inherent aspect of data in practice. This paper explores the ability of the recurrent neural network structure to monitor processes and proposes a control chart based on long short-term memory (LSTM) prediction intervals for data with time-varying variability. The simulation studies provide empirical evidence that the proposed model outperforms other NN-based predictive monitoring methods for mean shift detection. The proposed method is also applied to time series sensor data, which confirms that the proposed method is an effective technique for detecting abnormalities.
</details>
<details>
<summary>摘要</summary>
Recurrent Neural Networks (RNNs) 和其变体在过去几年内表现出了惊人的成功，但它们在异常检测中尚未受到太多关注。此外，传统的统计模型基于假设和 гипотезы测试，而神经网络（NN）模型则不需要这么多假设。这种灵活性使得NN模型能够高效地处理具有时间变化的变化的数据，这是实际数据的常见特性。本文探讨了RNN结构是否能够监控过程，并提出了基于长期记忆（LSTM）预测 интерval的控制图表方法。实验研究表明，提议的模型在mean shift检测方面表现出excel，并且应用于时间序列感知器数据，证明了该方法是异常检测的有效手段。
</details></li>
</ul>
<hr>
<h2 id="Linear-Regression-using-Heterogeneous-Data-Batches"><a href="#Linear-Regression-using-Heterogeneous-Data-Batches" class="headerlink" title="Linear Regression using Heterogeneous Data Batches"></a>Linear Regression using Heterogeneous Data Batches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01973">http://arxiv.org/abs/2309.01973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Jain, Rajat Sen, Weihao Kong, Abhimanyu Das, Alon Orlitsky</li>
<li>for: 本研究的目的是解决多源数据的难题，即每个源数据集都有一个未知的分支，每个分支都有一个未知的输入分布和输入-输出关系。</li>
<li>methods: 本研究使用了一种新的梯度下降算法，该算法可以扩展到不同的分支下，允许输入分布不同、未知和重 tailed ，同时可以在有限的批次数量下 recuperate 所有分支。</li>
<li>results: 本研究得到了较好的结果，可以在有限的批次数量下 recuperate 所有分支，并且可以避免分支分离要求。此外，研究还发现了一些优化的技巧，可以更好地处理各种不同的输入分布。<details>
<summary>Abstract</summary>
In many learning applications, data are collected from multiple sources, each providing a \emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work~\cite{kong2020meta} showed that with abundant small-batches, the regression vectors can be learned with only few, $\tilde\Omega( k^{3/2})$, batches of medium-size with $\tilde\Omega(\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem". We propose a novel gradient-based algorithm that improves on the existing results in several ways. It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite $k$; (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.
</details>
<details>
<summary>摘要</summary>
在许多学习应用中，数据来源从多个来源提供批处理的样本，每个来源的样本本身不够学习其输入输出关系。一种常见的方法假设来源分为多个未知 subgroup，每个 subgroup 有未知的输入分布和输入输出关系。我们考虑这个设置的一个最基本和重要的表现，输出是噪音加权的输入组合，有 $k$ subgroup，每个 subgroup 有自己的 regression vector。先前的研究（\ref{kong2020meta})  показа了，只要有充足的小批处理，可以通过只需要几个 $\tilde\Omega(k^{3/2})$ 中等大小的批处理，每个批处理有 $\tilde\Omega(\sqrt k)$ 个样本，学习 regression vector。然而，这个文章假设输入分布对所有 $k$ subgroup 都是均匀 Gaussian，并且称这是一个“有趣和挑战的问题”。我们提出了一种新的梯度基本算法，在以下方面改进了现有结果：1. 允许 subgroup 的下面输入分布不同、未知、重 tailed;2. 能够恢复所有 subgroup，即使 $k$ 为无穷大;3.  removing separation requirement between regression vectors;4. 减少批处理数量，允许小批处理大小更小。
</details></li>
</ul>
<hr>
<h2 id="AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis"><a href="#AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis" class="headerlink" title="AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis"></a>AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01966">http://arxiv.org/abs/2309.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Guan</li>
<li>for: 这篇论文是为了提出一种高效的优化器，即AdaPlus，该优化器基于AdamW基础上加入了内斯特鲁普振荡和精确步长调整。</li>
<li>methods: 该论文使用的方法包括Nesterov振荡和精确步长调整，并在AdamW基础上 интегрирова。</li>
<li>results: 实验结果表明，AdaPlus在图像分类任务上表现最佳，与SGD振荡相当，并在语言模型任务和GAN训练中表现出最高稳定性。<details>
<summary>Abstract</summary>
This paper proposes an efficient optimizer called AdaPlus which integrates Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does not introduce any extra hyper-parameters. We perform extensive experimental evaluations on three machine learning tasks to validate the effectiveness of AdaPlus. The experiment results validate that AdaPlus (i) is the best adaptive method which performs most comparable with (even slightly better than) SGD with momentum on image classification tasks and (ii) outperforms other state-of-the-art optimizers on language modeling tasks and illustrates the highest stability when training GANs. The experiment code of AdaPlus is available at: https://github.com/guanleics/AdaPlus.
</details>
<details>
<summary>摘要</summary>
这份论文提出了一种高效的优化器called AdaPlus，它在AdamW基础上集成了内斯特鲁夫势和精确步长调整。AdaPlus将AdamW、Nadam和AdaBelief的优点结合在一起，并没有添加任何额外 гипер Parameters。我们在三个机器学习任务上进行了广泛的实验评估，以验证AdaPlus的效果。实验结果表明，AdaPlus（一）在图像分类任务上与SGD势动（even slightly better than）最佳的adaptive方法，（二）在语言模型任务上超越了其他当前最佳优化器，并且在训练GANs时示出了最高的稳定性。AdaPlus的实验代码可以在https://github.com/guanleics/AdaPlus上获取。
</details></li>
</ul>
<hr>
<h2 id="RADIO-Reference-Agnostic-Dubbing-Video-Synthesis"><a href="#RADIO-Reference-Agnostic-Dubbing-Video-Synthesis" class="headerlink" title="RADIO: Reference-Agnostic Dubbing Video Synthesis"></a>RADIO: Reference-Agnostic Dubbing Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01950">http://arxiv.org/abs/2309.01950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyeun Lee, Chaewon Kim, Sangjoon Yu, Jaejun Yoo, Gyeong-Moon Park</li>
<li>for: 提高语音驱动的讲头生成的精度和同步率。</li>
<li>methods: 引入RADIO框架，通过修改decoder层使用latent空间中的音频和参考特征进行调节，并在decoder中添加ViT块以强调高精度 Details。</li>
<li>results: 实验结果显示RADIO可以保持高同步率而不失去精度，尤其在参考帧与实际帆布偏差较大的情况下，方法超越了现有方法，反映其稳定性。<details>
<summary>Abstract</summary>
One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness. Pre-trained model and codes will be made public after the review.
</details>
<details>
<summary>摘要</summary>
一个非常挑战的问题在audio驱动的讲头生成中是实现高精度的详细同步。只有一个参考图像，提取有意义的人脸特征变得更加挑战，常常导致网络夹紧脸和嘴的结构。为解决这些问题，我们介绍了RADIO框架，可以生成高质量的配音视频，不 regard of pose或表情参考图像中的姿势或表情。关键在于在 latent space 中模拟音频和参考特征。此外，我们在decoder中添加了ViT块，以强调高精度的细节，特别是嘴部regions。我们的实验结果表明，RADIO 能够实现高同步无损精度。尤其在参考图像与真实图像偏差较大的情况下，我们的方法超过了当前的状态艺技术， highlighting its robustness。预训练模型和代码将在审核后公开。
</details></li>
</ul>
<hr>
<h2 id="TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models"><a href="#TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models" class="headerlink" title="TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models"></a>TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01947">http://arxiv.org/abs/2309.01947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shangguan, Haichuan Yang, Danni Li, Chunyang Wu, Yassir Fathullah, Dilin Wang, Ayushi Dalmia, Raghuraman Krishnamoorthi, Ozlem Kalinli, Junteng Jia, Jay Mahadeokar, Xin Lei, Mike Seltzer, Vikas Chandra</li>
<li>for: 这个论文是为了提高机器学习模型在专门的硬件上的表现，并且可以将模型训练到多种硬件上。</li>
<li>methods: 这个论文使用了一种新的方法 called TODM（Train Once Deploy Many），它可以将多个硬件友好的模型训练成一个Supernet，并且运用了三种技术来提高结果：适应性 Dropout、实时Alpha-分布知识传授和Scale Adam优化器。</li>
<li>results: 这个论文的结果显示，使用TODM Supernet可以与手动调整的模型相比，在LibriSpeech中的字误率（WER）上提高到3%以上，而且可以实现将训练多个模型的成本降到小数字。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) models need to be optimized for specific hardware before they can be deployed on devices. This can be done by tuning the model's hyperparameters or exploring variations in its architecture. Re-training and re-validating models after making these changes can be a resource-intensive task. This paper presents TODM (Train Once Deploy Many), a new approach to efficiently train many sizes of hardware-friendly on-device ASR models with comparable GPU-hours to that of a single training job. TODM leverages insights from prior work on Supernet, where Recurrent Neural Network Transducer (RNN-T) models share weights within a Supernet. It reduces layer sizes and widths of the Supernet to obtain subnetworks, making them smaller models suitable for all hardware types. We introduce a novel combination of three techniques to improve the outcomes of the TODM Supernet: adaptive dropouts, an in-place Alpha-divergence knowledge distillation, and the use of ScaledAdam optimizer. We validate our approach by comparing Supernet-trained versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using LibriSpeech. Results demonstrate that our TODM Supernet either matches or surpasses the performance of manually tuned models by up to a relative of 3% better in word error rate (WER), while efficiently keeping the cost of training many models at a small constant.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）模型需要对特定硬件进行优化才能在设备上部署。这可以通过调整模型的超参数或探索模型的架构来实现。然而，重新训练和重新验证模型 после进行这些变化可能会占用资源。这篇论文介绍了TODM（Train Once Deploy Many），一种新的方法，可以高效地在多种硬件上训练适合硬件的各种大小的语音识别模型，而无需浪费大量的GPU时间。TODM利用了先前的Supernet工作的经验，其中Recurrent Neural Network Transducer（RNN-T）模型在Supernet中共享权重。它采用了减少Supernet中层数和宽度，以获得适合所有硬件类型的子网络。我们提出了一种新的组合技术，包括适应性Dropout、在位Alpha-分配知识继承和Scale Adam优化器。我们验证了我们的方法，将Supernet训练的模型与手动调整的Multi-Head State Space Model（MH-SSM）RNN-T模型进行比较，使用LibriSpeech数据集。结果表明，我们的TODM Supernet Either Matches或超过了手动调整模型的性能，在单词错误率（WER）方面提高了Relative 3%，而且高效地保持了训练多个模型的成本在小于一个常数。
</details></li>
</ul>
<hr>
<h2 id="OHQ-On-chip-Hardware-aware-Quantization"><a href="#OHQ-On-chip-Hardware-aware-Quantization" class="headerlink" title="OHQ: On-chip Hardware-aware Quantization"></a>OHQ: On-chip Hardware-aware Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01945">http://arxiv.org/abs/2309.01945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yifu Ding, Ying Li, Xianglong Liu</li>
<li>For: This paper focuses on developing an On-chip Hardware-aware Quantization (OHQ) framework for deploying advanced deep models on resource-constrained hardware.* Methods: The proposed OHQ framework uses Mask-guided Quantization Estimation (MQE) technique to efficiently estimate the accuracy metrics of operators under the constraints of on-chip-level computing power, and synthesizes network and hardware insights through linear programming to obtain optimized bit-width configurations.* Results: The proposed OHQ framework achieves accelerated inference after quantization for various architectures and compression ratios, with 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and improves latency by 15~30% compared to INT8 on deployment.Here is the simplified Chinese text:* For: 这篇论文关注开发一种在有限资源硬件上部署高级深度模型的 On-chip Hardware-aware Quantization (OHQ) 框架。* Methods: OHQ 框架使用 Mask-guided Quantization Estimation (MQE) 技术来高效地估算运算数据的精度指标，并通过线性 программирова来整合网络和硬件视角来获得优化的比特宽配置。* Results: OHQ 框架实现了对不同架构和压缩比进行加速的推理，实现了 ResNet-18 和 MobileNetV3 的 70% 和 73% 的精度，并提高了 INT8 的执行时间的 latency 比例。<details>
<summary>Abstract</summary>
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique to efficiently estimate the accuracy metrics of operators under the constraints of on-chip-level computing power.By synthesizing network and hardware insights through linear programming, we obtain optimized bit-width configurations. Notably, the quantization process occurs on-chip entirely without any additional computing devices and data access. We demonstrate accelerated inference after quantization for various architectures and compression ratios, achieving 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively. OHQ improves latency by 15~30% compared to INT8 on deployment.
</details>
<details>
<summary>摘要</summary>
量化技术在部署高级深度模型时 emerges as one of the most promising approaches, 以实现资源受限的硬件上的高效和高精度模型部署。混合精度量化利用多 bit-width 架构，可以激活模型的准确和效率潜力。然而，现有的混合精度量化受搜索空间的约束，导致 immense computational overhead。因此，量化过程通常需要分离的高性能设备，而不是本地进行，这也导致了差距 между 考虑的硬件指标和实际部署。在这篇论文中，我们提出了一个 On-chip Hardware-aware Quantization (OHQ) 框架，可以在本地进行硬件具有资源限制的混合精度量化，不需要访问在线设备。首先，我们构建了 On-chip Quantization Awareness (OQA) 管道，使得量化操作的实际效率指标可以在硬件上被识别。其次，我们提出了 Mask-guided Quantization Estimation (MQE) 技术，可以高效地估计量化操作下的准确指标，在硬件上进行约束。通过线性 программирова，我们将网络和硬件元素相互协同，以获得优化的 bit-width 配置。值得注意的是，量化过程完全发生在本地，不需要任何额外的计算设备和数据访问。我们在不同的架构和压缩比例下进行加速的推理，实现了 ResNet-18 和 MobileNetV3 的 70% 和 73% 的准确率。相比 INT8，OHQ 提高了执行时间，为 15%~30%。
</details></li>
</ul>
<hr>
<h2 id="Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection"><a href="#Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection" class="headerlink" title="Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection"></a>Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03231">http://arxiv.org/abs/2309.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Atif Ali Shah, Nasir Algeelani, Najeeb Al-Sammarraie</li>
<li>for: 提高智能监控系统的准确率和实时性，以满足 densely populated 环境中的监控需求。</li>
<li>methods:  integrate RentinaNet 模型和 Quantum CNN，实现量子人工智能在监控领域的应用。</li>
<li>results: Quantum-RetinaNet 模型能够具有高准确率和实时性，打破传统 CNN 模型的速度和准确率之间的负担，为智能监控系统提供了一个新的选择。<details>
<summary>Abstract</summary>
Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration positions it as a game-changer, addressing the challenges of active monitoring in densely populated scenarios. As demand for efficient surveillance solutions continues to grow, Quantum-RetinaNet offers a compelling alternative to existing CNN models, upholding accuracy standards without sacrificing real-time performance. The unique attributes of Quantum-RetinaNet have far-reaching implications for the future of intelligent surveillance. With its enhanced processing speed, it is poised to revolutionize the field, catering to the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet becomes the new standard, it ensures public safety and security while pushing the boundaries of AI in surveillance.
</details>
<details>
<summary>摘要</summary>
现代世界中维护和平安全的重要元素之一是监测系统。它们在监测可疑活动方面具有重要作用。然而，在高密度环境中，不断的活动监测成为不切实际，因此需要开发智能监测系统。在监测领域中，人工智能（AI）的应用是一个大革命，但速度问题使得其在实际应用中尚未普及。尽管如此，量子人工智能（Quantum AI）的出现带来了很大的突破。量子人工智能基于的监测系统表现出了更高的准确率和在实时场景中的出色表现，这从未被见过。本研究将量子逻辑网络（Quantum-RetinaNet）与量子卷积神经网络（QCNN）结合，并且通过利用量子计算机（QCNN）的量子特性，量子逻辑网络能够平衡准确率和速度。这种创新的结合使得它成为了一个游戏规则，解决了高密度环境中不断监测的挑战。随着需求高效监测解决方案的增长，量子逻辑网络成为了一个有力的替代方案，保持准确标准而不 sacrificing real-time性。量子逻辑网络的独特特点在未来对智能监测的发展有着深远的影响，它的提高处理速度使得它成为了监测领域的新标准，为公众安全和安全提供保障，同时推动人工智能在监测领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis"><a href="#Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis" class="headerlink" title="Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis"></a>Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01941">http://arxiv.org/abs/2309.01941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Kan, Antonio Aodong Chen Gu, Hejie Cui, Ying Guo, Carl Yang</li>
<li>for: 这篇论文的目的是提出一个新的方法来分析大脑功能，以更好地理解大脑组织和神经网络的运作。</li>
<li>methods: 这篇论文使用了一种新的方法，即动态大脑网络分析法（Dynamic Brain Transformer，DART），它结合了静止大脑网络和动态大脑网络，以提高大脑功能分析的精度和可靠性。</li>
<li>results: 这篇论文的结果显示，DRAT方法可以对于血液测量信号的噪音问题进行更好的处理，并且可以提供更多的实用信息，例如哪些神经网络或动态网络在最终预测中做出了贡献。<details>
<summary>Abstract</summary>
Recent neuroimaging studies have highlighted the importance of network-centric brain analysis, particularly with functional magnetic resonance imaging. The emergence of Deep Neural Networks has fostered a substantial interest in predicting clinical outcomes and categorizing individuals based on brain networks. However, the conventional approach involving static brain network analysis offers limited potential in capturing the dynamism of brain function. Although recent studies have attempted to harness dynamic brain networks, their high dimensionality and complexity present substantial challenges. This paper proposes a novel methodology, Dynamic bRAin Transformer (DART), which combines static and dynamic brain networks for more effective and nuanced brain function analysis. Our model uses the static brain network as a baseline, integrating dynamic brain networks to enhance performance against traditional methods. We innovatively employ attention mechanisms, enhancing model explainability and exploiting the dynamic brain network's temporal variations. The proposed approach offers a robust solution to the low signal-to-noise ratio of blood-oxygen-level-dependent signals, a recurring issue in direct DNN modeling. It also provides valuable insights into which brain circuits or dynamic networks contribute more to final predictions. As such, DRAT shows a promising direction in neuroimaging studies, contributing to the comprehensive understanding of brain organization and the role of neural circuits.
</details>
<details>
<summary>摘要</summary>
近期神经成像研究强调了脑网络中心式分析的重要性，尤其是使用功能核磁共振成像。深度神经网络的出现促使了预测临床结果和根据脑网络分类个体的研究。然而，传统方法的静态脑网络分析具有有限的潜在，不能够捕捉脑功能的动态性。虽然最近的研究尝试了利用动态脑网络，但高维度和复杂性带来了巨大的挑战。本文提出了一种新的方法ology，名为动态脑传变（DART），它结合静态脑网络和动态脑网络，以更有效和细腻地分析脑功能。我们的模型使用静态脑网络作为基线，并将动态脑网络与静态脑网络结合，以提高性能。我们创新地使用注意力机制，提高模型解释性，并利用动态脑网络的时间变化。我们的提案方法可以解决血氧含量信号的低信号噪声问题，这是直接DNN模型中常见的问题。此外，我们的方法还提供了精准的哪些脑回路或动态网络在最终预测中做出了更大的贡献。因此，DART表示了神经成像研究中一个有前途的方向，为脑组织的全面理解和神经细胞网络的作用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D"><a href="#Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D" class="headerlink" title="Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)"></a>Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02467">http://arxiv.org/abs/2309.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Jingchuan Guo, William T Donahoo, Zhengkang Fan, Ying Lu, Wei-Han Chen, Huilin Tang, Lori Bilello, Elizabeth A Shenkman, Jiang Bian<br>for: The paper aims to develop an EHR-based machine learning analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with type 2 diabetes (T2D).methods: The paper uses electronic health records (EHR) data from the University of Florida Health Integrated Data Repository, along with contextual and individual-level social determinants of health (SDoH) data, to develop an individualized polysocial risk score (iPsRS) to identify high social risk associated with hospitalizations in T2D patients. The paper also employs explainable AI (XAI) techniques and fairness assessment and optimization.results: The paper finds that the iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk, with the actual 1-year hospitalization rate in the top 5% of iPsRS being ~13 times as high as the bottom decile.Here is the information in Simplified Chinese text:for: 这个论文的目的是开发一个基于电子健康纪录 (EHR) 的机器学习分析管道，以识别患有类型 2  диабеetes (T2D) 患者的社会需求不足，导致的医院风险。methods: 这个论文使用大学OFlorida 健康集成数据存储库中的 EHR 数据，并与社会 Determinants of health (SDoH) 数据相结合，开发了一个个性化多社会风险分数 (iPsRS)，以识别 T2D 患者的高社会风险。论文还使用可解释 AI (XAI) 技术和公平评估和优化。results: 论文发现，iPsRS 在各个种族-民族组中进行公平优化后，对 1 年医院风险预测达到 C 统计值为 0.72。iPsRS 显示了极佳的实用性，可以准确地捕捉高医院风险的个人，Actual 1 年医院风险率在 top 5% 的 iPsRS 是 bottom decile 的 ~13 倍。<details>
<summary>Abstract</summary>
Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) techniques and fairness assessment and optimization. Results: Our iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk; the actual 1-year hospitalization rate in the top 5% of iPsRS was ~13 times as high as the bottom decile. Conclusion: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in T2D patients.
</details>
<details>
<summary>摘要</summary>
背景：种族和民族少数群体和受社会不平等困扰的个人 often 患有型二糖尿病（T2D）和其合并症状。因此，实施有效的社会风险管理策略在点患者护理中是非常重要。目标：开发基于电子健康纪录（EHR）的机器学习（ML）分析管道，以识别患有T2D patients的医疗机器人化风险。方法：我们从2012年至2022年的University of Florida Health Integrated Data Repository中提取了10,192名T2D患者的EHR数据，包括上下文ual SDoH（例如，社区贫困）和个人级SDoH（例如，住房稳定）。我们开发了基于EHR的ML分析管道，即个性化多社会风险分数（iPsRS），以识别患有T2D patients中高社会风险与入院风险的个人，同时使用可解释AI（XAI）技术和公平评估和优化。结果：我们的iPsRS在各种种族-民族组中进行公平优化后，C Statistics 值为0.72，可以准确预测患有T2D patients的1年入院风险。iPsRS表现出了捕捉高入院风险个人的出色Utility，实际1年入院率在top 5% iPsRS的个人为bottom decile的13倍。结论：我们的ML管道iPsRS可以公平、准确地检测患有T2D patients中高社会风险导致入院风险的个人。
</details></li>
</ul>
<hr>
<h2 id="Provably-safe-systems-the-only-path-to-controllable-AGI"><a href="#Provably-safe-systems-the-only-path-to-controllable-AGI" class="headerlink" title="Provably safe systems: the only path to controllable AGI"></a>Provably safe systems: the only path to controllable AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01933">http://arxiv.org/abs/2309.01933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Tegmark, Steve Omohundro</li>
<li>for: 这篇论文写于如何使人类安全地树成功的人工通用智能（AGI）。</li>
<li>methods: 该论文提出了使用高级AI进行正式验证和机械解释来建立AGI满足人类指定的要求。</li>
<li>results: 该论文认为这将很快成为技术可能的，并且只有这一路径可以保证安全控制AGI。<details>
<summary>Abstract</summary>
We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.
</details>
<details>
<summary>摘要</summary>
我们描述了一条路径，使人类安全快乐地与高能力人工通用智能机器人（AGI）共处，通过建立AGI以满足人类指定的要求来实现。我们认为，技术上很快就会实现这一点，使用先进的AI进行正式验证和机械解释。我们还认为，这是 garantuee 安全控制AGI的唯一路径。我们列出了一些挑战问题，解决这些问题会为这一 pozitivo 结果做出贡献，并邀请读者参与这项工作。Note: "guarantee" is translated as " garantuee" in Simplified Chinese, which is a common way to write "guarantee" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes"><a href="#Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes" class="headerlink" title="Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes"></a>Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01922">http://arxiv.org/abs/2309.01922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</li>
<li>for: 这个论文关注的是无穷远 horizon average reward Markov Decision Process（MDP）。与现有的研究不同，我们的方法不假设MDP结构为线性的。</li>
<li>methods: 我们提出了基于政策梯度的算法，并证明其全球协调性。</li>
<li>results: 我们证明了该算法的$\tilde{\mathcal{O}({T}^{3&#x2F;4})$ regret。这是首次在average reward场景中对通用参数化政策梯度算法的 regret-bound计算。<details>
<summary>Abstract</summary>
In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个无穷horizon平均奖励Markov决策过程（MDP）。与现有的研究不同，我们的方法利用了通用的策略梯度基本算法， liberating it from the constraints of assuming a linear MDP structure。我们提出了一种策略梯度基本算法，并证明其全球归一化性。然后，我们证明了我们的算法有$\tilde{\mathcal{O}({T}^{3/4})$的念骨。值得注意的是，这篇论文标志着我们在平均奖励场景中的首次探索regret bound computation的尝试。
</details></li>
</ul>
<hr>
<h2 id="RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking"><a href="#RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking" class="headerlink" title="RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"></a>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01918">http://arxiv.org/abs/2309.01918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar</li>
<li>For:  train a single robot to perform diverse manipulation skills in various settings* Methods:  semantic augmentations and action representations to extract performant policies with small datasets, and reliable task conditioning and expressive policy architecture to exhibit diverse skills in novel situations* Results:  trained a single agent capable of 12 unique skills with 7500 demonstrations, and demonstrated its generalization over 38 tasks in diverse kitchen scenes, outperforming prior methods by over 40% in unseen situations while being more sample efficient and amenable to capability improvements and extensions through fine-tuning.<details>
<summary>Abstract</summary>
The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such an universal agent would require a structured framework capable of wide generalization but trained within a reasonable data budget. In this paper, we develop an efficient system (RoboAgent) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enable our agent to exhibit a diverse repertoire of skills in novel situations specified using language commands. Using merely 7500 demonstrations, we are able to train a single agent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient and being amenable to capability improvements and extensions through fine-tuning. Videos at https://robopen.github.io/
</details>
<details>
<summary>摘要</summary>
文章标题：一种高效的机器人多任务 manipulate 技能训练系统（RoboAgent）摘要：在具有单一机器人可 manipulate 任意物品的宏目标面前，机器人数据集的缺乏是一大问题。获取和扩大这些数据集需要大量的人工劳动、运维成本和安全挑战。为实现这样的通用代理人，我们需要一种结构化的框架，可以广泛适应但是在有限的数据预算内训练。本文提出了一种高效的机器人训练系统（RoboAgent），可以通过（a）semantic 扩充和（b）动作表示来快速增加现有数据集，并提取高性能策略。此外，我们还提出了可靠的任务条件和表达丰富的策略架构，使我们的代理人能够在新的语言命令下展现多样化的技能。使用仅 7500 示例，我们能够训练一个可以完成 12 种任务的单一机器人，并在多个日常活动的厨房场景中展现其普适性。在未看过的情况下，RoboAgent 比 Prior 方法高效了更多的 40%，同时更 sample 有效和可以通过细化进行改进和扩展。视频见 [[1](https://robopen.github.io/)]。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems"><a href="#A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems" class="headerlink" title="A Survey on Physics Informed Reinforcement Learning: Review and Open Problems"></a>A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01909">http://arxiv.org/abs/2309.01909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi</li>
<li>for: 本研究旨在探讨physics-informed reinforcement learning（PIRL）的应用 potential，包括如何在reinforcement learning框架中integrate physical laws和约束，以提高算法的physical plausibility和实用性。</li>
<li>methods: 本研究使用了一种新的分类方法，通过对existings works的分析和比较，derive crucial insights并提出了一种新的taxonomy，用于 классификацияPIRLapproaches。这种分类方法将physics-informed reinforcement learning pipeline分成了三个主要部分：observational bias, inductive bias,和learning bias。</li>
<li>results: 本研究发现PIRLapproaches可以增强reinforcement learning algorithm的physical plausibility和精度，同时提高数据效率和实用性。此外，研究还发现了一些未解决的问题和挑战，例如如何在不同的physics domain中apply PIRLapproaches，以及如何调整PIRLapproaches的parameters和hyperparameters。<details>
<summary>Abstract</summary>
The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
“机器学习框架中包含物理信息的应用已经革命化了许多应用领域。这包括将物理限制和物理法则纳入机器学习过程中，以提高学习效能。在这个研究中，我们将Physics-informed reinforcement learning（PIRL）的 utility 在应用中探讨。我们提出了一个新的分类方案，并将现有的工作分为三大类：代表物理模型的形式、对应的实际应用和与基本的机器学习架构相关的Connection。我们还识别了现有PIRL方法的核心学习架构和物理包含偏见，并将它们用来进一步分类工作，以更好地理解和适应。这个统一的方法提供了一个广泛的见解，探讨PIRL在实际应用中的实现方式，包括这个领域的应用、缺乏和未来研究的机遇。这个新兴的领域具有增加物理可能性、精度、数据效率和实际应用中的应用潜力。”
</details></li>
</ul>
<hr>
<h2 id="Inferring-Actual-Treatment-Pathways-from-Patient-Records"><a href="#Inferring-Actual-Treatment-Pathways-from-Patient-Records" class="headerlink" title="Inferring Actual Treatment Pathways from Patient Records"></a>Inferring Actual Treatment Pathways from Patient Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01897">http://arxiv.org/abs/2309.01897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Wilkins-Caruana, Madhushi Bandara, Katarzyna Musial, Daniel Catchpoole, Paul J. Kennedy</li>
<li>for: 这个研究旨在从管理健康记录 (AHR) 中INFER实际进行的治疗步骤，以提高患者结果。</li>
<li>methods: 该研究使用了一种名为 Defrag 的方法，该方法可以学习健康事件序列的semantic和temporal含义，从而可靠地INFER治疗步骤。 Defrag 使用了人工神经网络 (NN)，这是由一种新的自我超vised学习目标实现的。</li>
<li>results: 研究表明，Defrag 可以有效地INFER breast cancer、lung cancer 和 melanoma 等癌症的治疗路径，并且在 synthetic data  экспериментах中，Defrag 显著超过了一些基准方法。<details>
<summary>Abstract</summary>
Treatment pathways are step-by-step plans outlining the recommended medical care for specific diseases; they get revised when different treatments are found to improve patient outcomes. Examining health records is an important part of this revision process, but inferring patients' actual treatments from health data is challenging due to complex event-coding schemes and the absence of pathway-related annotations. This study aims to infer the actual treatment steps for a particular patient group from administrative health records (AHR) - a common form of tabular healthcare data - and address several technique- and methodology-based gaps in treatment pathway-inference research. We introduce Defrag, a method for examining AHRs to infer the real-world treatment steps for a particular patient group. Defrag learns the semantic and temporal meaning of healthcare event sequences, allowing it to reliably infer treatment steps from complex healthcare data. To our knowledge, Defrag is the first pathway-inference method to utilise a neural network (NN), an approach made possible by a novel, self-supervised learning objective. We also developed a testing and validation framework for pathway inference, which we use to characterise and evaluate Defrag's pathway inference ability and compare against baselines. We demonstrate Defrag's effectiveness by identifying best-practice pathway fragments for breast cancer, lung cancer, and melanoma in public healthcare records. Additionally, we use synthetic data experiments to demonstrate the characteristics of the Defrag method, and to compare Defrag to several baselines where it significantly outperforms non-NN-based methods. Defrag significantly outperforms several existing pathway-inference methods and offers an innovative and effective approach for inferring treatment pathways from AHRs. Open-source code is provided to encourage further research in this area.
</details>
<details>
<summary>摘要</summary>
医疗路径是一系列步骤计划，用于确定特定疾病的推荐医疗方案。这些医疗路径会随着新的治疗方法的发现而进行修订。检查医疗记录是修订过程中的重要一步，但从医疗数据中推导病人实际接受的治疗步骤是具有挑战性的，因为医疗记录中的事件编码系统复杂，而且缺乏路径相关的注释。本研究的目标是从医疗记录中推导特定病人群的实际治疗步骤，并解决了一些技术和方法基础上的差距。我们介绍了一种名为Defrag的方法，可以从医疗记录中推导病人实际接受的治疗步骤。Defrag可以学习医疗事件序列的 semantic 和时间含义，以便可靠地从复杂的医疗数据中推导治疗步骤。我们知道，Defrag 是首个使用神经网络（NN）的医疗路径推导方法，我们还开发了一个用于评估和验证医疗路径推导能力的测试和验证框架。我们在公共医疗记录中identified breast cancer, lung cancer和melanoma的best-practice路径片段。此外，我们使用 sintetic 数据实验来描述Defrag 方法的特点，并与多种基eline进行比较，其中Defrag 显著超过非 NN 基eline。Defrag 提供了一种创新的和有效的医疗路径推导方法，可以从医疗记录中推导病人实际接受的治疗步骤。我们提供了开源代码，以便更多人进行进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis"><a href="#Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis" class="headerlink" title="Extended Symmetry Preserving Attention Networks for LHC Analysis"></a>Extended Symmetry Preserving Attention Networks for LHC Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01886">http://arxiv.org/abs/2309.01886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael James Fenton, Alexander Shmakov, Hideki Okawa, Yuji Li, Ko-Yang Hsiao, Shih-Chieh Hsu, Daniel Whiteson, Pierre Baldi</li>
<li>for: 这个论文的目的是扩展基于通用注意机制的扩展性网络（SPANet），以便在 Large Hadron Collider 上处理涉及多个输入流的潜在重粒子征。</li>
<li>methods: 这篇论文使用了一种基于通用注意机制的扩展性网络（SPANet），以便处理多个输入流，包括电子和全局事件特征。</li>
<li>results: 研究发现，使用扩展的 SPANet 可以在 semi-leptonic 探测中提高探测能力，并且在 top quark pair 生成协同 Higgs  boson 中也能够获得显著提高。<details>
<summary>Abstract</summary>
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (SPANet), has been previously applied to top quark pair decays at the Large Hadron Collider, which produce six hadronic jets. Here we extend the SPANet architecture to consider multiple input streams, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of SPANet in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: search for ttH, measurement of the top quark mass and a search for a heavy Z' decaying to top quark pairs. We present ablation studies to provide insight on what the network has learned in each case.
</details>
<details>
<summary>摘要</summary>
重构不稳定的重子需要使用复杂的技术来对探测器对部分子的分配进行搜索。一种基于普通注意机制的扩展版本（SPANet）在大夸子机器中已经应用于top顺子异常衰变，生成六个坍塌探测器。在这里，我们扩展了SPANet架构，考虑多个输入流，如电子，以及全事件特征，如缺失横向动量。此外，我们还提供了回归和分类输出，以补充部分赋值。我们在semi-leptonic decay of top quark pairs和top quark pairs with Higgs boson production中探索了扩展的SPANet能力的表现。我们发现在三个代表性研究中有显著的改善：搜索ttH、测量top顺子质量和搜索重Z'衰变为top顺子对。我们进行了剥离研究，以了解网络在每个情况中学习的内容。
</details></li>
</ul>
<hr>
<h2 id="QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm"><a href="#QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm" class="headerlink" title="QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm"></a>QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01885">http://arxiv.org/abs/2309.01885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, Rahul Mazumder</li>
<li>for: This paper focuses on the Post-Training Quantization (PTQ) of Large Language Models (LLMs) to achieve efficient deployment.</li>
<li>methods: The paper introduces QuantEase, a layer-wise quantization framework that uses Coordinate Descent (CD) techniques to solve the discrete-structured non-convex optimization problem. The CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems, with straightforward updates that rely solely on matrix and vector operations.</li>
<li>results: The proposed approach achieves state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. The outlier-aware variant of the approach retains significant weights (outliers) with complete precision, allowing for near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy.<details>
<summary>Abstract</summary>
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）的流行，压缩技术的实现已成为一项突出的研究方向。这项研究探讨了 LLM 的 POST-TRAINING QUANTIZATION（PTQ）技术。基于最新的进展，我们提出了 QuantEase，一种层wise 压缩框架，其中每层都进行独立压缩。问题定义为一个离散结构非几何优化问题，因此我们开发了基于 Coordinate Descent（CD）技术的算法。这些 CD 基于的算法可以提供高质量的解决方案，并且具有简单的更新规则，只需要基于矩阵和向量操作，不需要矩阵反转或分解。此外，我们还提出了一种具有对重要权重（异常值）的考虑的变体，可以保留完整的精度。我们的提议在实验中实现了state-of-the-art的表现，包括折衔率和零shot精度，与方法such as GPTQ 相比，有相对提升达 15%。特别值得一提的是我们的异常值考虑变体可以在 LLM 中实现近似或小于 3 位压缩，而无需非几何压缩或分组技术，与方法such as SpQR 相比，可以提高表现的两倍。
</details></li>
</ul>
<hr>
<h2 id="Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies"><a href="#Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies" class="headerlink" title="Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies"></a>Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01884">http://arxiv.org/abs/2309.01884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Li, Nadia Figueroa</li>
<li>for: 学习从示例（Learning from Demonstration）的动力系统（Dynamical System），以稳定性和整合保证从少量轨迹学习激烈动作策略。</li>
<li>methods: 提议使用Gaussian Mixture Model（GMM）基于线性参数变化（LPV）动力系统模型，并将任务参数直接嵌入到GMM中，使其能够� Generic 到新任务实例。</li>
<li>results: 在许多模拟和实际 робоット实验中，Elastic-DS表现出柔顺性和灵活性，并保持了控制理论上的保证。详细视频可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/elastic-ds">https://sites.google.com/view/elastic-ds</a> 找到。<details>
<summary>Abstract</summary>
Dynamical System (DS) based Learning from Demonstration (LfD) allows learning of reactive motion policies with stability and convergence guarantees from a few trajectories. Yet, current DS learning techniques lack the flexibility to generalize to new task instances as they ignore explicit task parameters that inherently change the underlying trajectories. In this work, we propose Elastic-DS, a novel DS learning, and generalization approach that embeds task parameters into the Gaussian Mixture Model (GMM) based Linear Parameter Varying (LPV) DS formulation. Central to our approach is the Elastic-GMM, a GMM constrained to SE(3) task-relevant frames. Given a new task instance/context, the Elastic-GMM is transformed with Laplacian Editing and used to re-estimate the LPV-DS policy. Elastic-DS is compositional in nature and can be used to construct flexible multi-step tasks. We showcase its strength on a myriad of simulated and real-robot experiments while preserving desirable control-theoretic guarantees. Supplementary videos can be found at https://sites.google.com/view/elastic-ds
</details>
<details>
<summary>摘要</summary>
dynamical system (DS) 基于学习从示例 (LfD) 可以从一些轨迹学习响应性动作策略，并提供稳定性和收敛保证。然而，现有的 DS 学习技术缺乏扩展到新任务实例的灵活性，因为它们忽略了明确的任务参数，这些参数直接影响下面轨迹。在这项工作中，我们提出了 Elastic-DS，一种新的 DS 学习和泛化方法，它嵌入了任务参数到 Gaussian Mixture Model (GMM) 基于 Linear Parameter Varying (LPV) DS 形式ulation中。Elastic-DS 的核心思想是使用 Laplacian Editing 将 GMM 约束到 SE(3) 任务相关帧。给定一个新任务实例/上下文，Elastic-GMM 会被转换并用于重新估算 LPV-DS 策略。Elastic-DS 是可组合的，可以用来构建灵活的多步任务。我们在许多模拟和实际 робо臂实验中证明了它的强大，同时保留了控制理论上的可靠保证。补充视频可以在 https://sites.google.com/view/elastic-ds 找到。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Domain-Diffusion-Models-for-Image-Synthesis"><a href="#Gradient-Domain-Diffusion-Models-for-Image-Synthesis" class="headerlink" title="Gradient Domain Diffusion Models for Image Synthesis"></a>Gradient Domain Diffusion Models for Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01875">http://arxiv.org/abs/2309.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 这个论文应用于生成图像和视频Synthesis中的Diffusion模型，以提高它们的效率。</li>
<li>methods: 这个论文提出了在梯度领域进行Diffusion过程，以便更快地读取数据。这是因为梯度领域是原始图像领域的数学等价物，因此每个Diffusion步骤在图像领域有唯一的梯度领域表示。此外，梯度领域比图像领域更为简单，因此Diffusion模型在这个领域中更快地读取数据。</li>
<li>results: 数值实验证明， gradient domainDiffusion模型比原始Diffusion模型更有效率。这个方法可以应用于各种图像处理、计算机视觉和机器学习任务。<details>
<summary>Abstract</summary>
Diffusion models are getting popular in generative image and video synthesis. However, due to the diffusion process, they require a large number of steps to converge. To tackle this issue, in this paper, we propose to perform the diffusion process in the gradient domain, where the convergence becomes faster. There are two reasons. First, thanks to the Poisson equation, the gradient domain is mathematically equivalent to the original image domain. Therefore, each diffusion step in the image domain has a unique corresponding gradient domain representation. Second, the gradient domain is much sparser than the image domain. As a result, gradient domain diffusion models converge faster. Several numerical experiments confirm that the gradient domain diffusion models are more efficient than the original diffusion models. The proposed method can be applied in a wide range of applications such as image processing, computer vision and machine learning tasks.
</details>
<details>
<summary>摘要</summary>
Diffusion模型在生成图像和视频合成中越来越受欢迎。然而，由于扩散过程，它们需要较多的步骤才能够融合。为了解决这个问题，在这篇论文中，我们提议在梯度领域中进行扩散过程。有两个原因。一是，根据波恩Equation，梯度领域与原始图像领域是数学等价的。因此，每一步 diffusion 在图像领域中都有唯一的梯度领域表示。二是，梯度领域比图像领域更加稀疏。因此，梯度领域扩散模型更快 converges。一些数字实验证明，梯度领域扩散模型比原始扩散模型更高效。提议的方法可以应用于各种图像处理、计算机视觉和机器学习任务中。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting"><a href="#Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting" class="headerlink" title="Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting"></a>Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01866">http://arxiv.org/abs/2309.01866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping He, Yifan Xia, Xuhong Zhang, Shouling Ji</li>
<li>for: 本研究旨在提高Android骗器检测方法的安全性，因为黑客可以利用骗器来攻击 Android 操作系统。</li>
<li>methods: 本研究使用机器学习基于的 Android 骗器检测方法，但是这些方法受到逆向工程的威胁。</li>
<li>results: 本研究提出了一种名为 AdvDroidZero 的高效的查询型攻击框架，可以在零知识情况下攻击机器学习基于的 Android 骗器检测方法，并且对多种主流机器学习基于的骗器检测方法和实际应用解决方案进行了广泛的评估。<details>
<summary>Abstract</summary>
The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
</details>
<details>
<summary>摘要</summary>
Android 操作系统的广泛采用使得恶意 Android 应用程序成为了攻击者的吸引点。基于机器学习（ML）的 Android 黑客检测方法（AMD）是解决这个问题的关键，但它们受到了对抗示例的攻击的担忧。现有的对 ML-based AMD 方法的攻击方法表现出色，但它们假设了可能不是实际场景中的假设，例如特征空间、模型参数和训练数据的知识要求。为解决这个限制，我们介绍了 AdvDroidZero，一种基于查询的攻击框架，在零知识设定下运行。我们进行了广泛的评估，显示 AdvDroidZero 对主流 ML-based AMD 方法和实际应用中的抗病毒解决方案都有高效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.LG_2023_09_05/" data-id="clmjn91mw007z0j882evl4e0e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/05/cs.SD_2023_09_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-05
        
      </div>
    </a>
  
  
    <a href="/2023/09/05/eess.IV_2023_09_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-05</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
