
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="An Improved Upper Bound on the Rate-Distortion Function of Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02574 repo_url: https:&#x2F;&#x2F;github.com&#x2F;duanzhiihao&#x2F;lossy-vae paper_authors: Zhihao Duan, Jack Ma, Jia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-05">
<meta property="og:url" content="https://nullscc.github.io/2023/09/05/eess.IV_2023_09_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="An Improved Upper Bound on the Rate-Distortion Function of Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02574 repo_url: https:&#x2F;&#x2F;github.com&#x2F;duanzhiihao&#x2F;lossy-vae paper_authors: Zhihao Duan, Jack Ma, Jia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-05T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:16.545Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.IV_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T09:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images"><a href="#An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images" class="headerlink" title="An Improved Upper Bound on the Rate-Distortion Function of Images"></a>An Improved Upper Bound on the Rate-Distortion Function of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02574">http://arxiv.org/abs/2309.02574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duanzhiihao/lossy-vae">https://github.com/duanzhiihao/lossy-vae</a></li>
<li>paper_authors: Zhihao Duan, Jack Ma, Jiangpeng He, Fengqing Zhu</li>
<li>for: 这 paper 是为了提出一种基于 Variational Autoencoders (VAEs) 的图像lossy compression方法。</li>
<li>methods: 这 paper 使用了一种新的 VAE 模型架构，应用了变量比率压缩技术，并提出了一种新的 \ourfunction{} 来稳定训练。</li>
<li>results: 这 paper 的实验结果表明，使用这种方法可以实现至少 30% BD-rate 减少，相比 VVC codec 的内部预测模式。这表明还有很大的潜在空间来提高图像lossy compression。<details>
<summary>Abstract</summary>
Recent work has shown that Variational Autoencoders (VAEs) can be used to upper-bound the information rate-distortion (R-D) function of images, i.e., the fundamental limit of lossy image compression. In this paper, we report an improved upper bound on the R-D function of images implemented by (1) introducing a new VAE model architecture, (2) applying variable-rate compression techniques, and (3) proposing a novel \ourfunction{} to stabilize training. We demonstrate that at least 30\% BD-rate reduction w.r.t. the intra prediction mode in VVC codec is achievable, suggesting that there is still great potential for improving lossy image compression. Code is made publicly available at https://github.com/duanzhiihao/lossy-vae.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，变分自动编码器（VAEs）可以用来上界图像的信息率-损失（R-D）函数，即图像压缩的基本上限。在这篇论文中，我们报道了一种改进的上界方法，包括（1）介绍新的 VAE 模型建立，（2）应用Variable-rate压缩技术，以及（3）提出一种新的 \ourfunction{} 来稳定训练。我们示示了至少30%的BD-率减少相对于VVC编码器的内部预测模式，这表明还有很大的潜在可能性 для改进图像压缩。代码可以在 GitHub 上获取，https://github.com/duanzhiihao/lossy-vae。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers"><a href="#Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers" class="headerlink" title="Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers"></a>Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02563">http://arxiv.org/abs/2309.02563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhao Liu, Chenyang Qi, Shunxing Bao, Quan Liu, Ruining Deng, Yu Wang, Shilin Zhao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatized image analysis in renal pathology</li>
<li>methods:  deep learning-based approach, including Swin-Unet, Medical-Transformer, TransUNet, U-Net, PSPNet, and DeepLabv3+</li>
<li>results:  Transformer models generally outperform CNN-based models, with a decent Mean Intersection over Union (mIoU) index and promising results for quantitative evaluation of renal cortical structures.Here’s the full Chinese text:</li>
<li>for:  automatized图像分析在肾脏病学中</li>
<li>methods: 使用深度学习方法，包括Swin-Unet、医疗转换器、TransUNet、U-Net、PSPNet和DeepLabv3+</li>
<li>results:  transformer模型通常比CNN模型表现更好，并具有可靠的 Mean Intersection over Union (mIoU) 指标和对肾脏 cortical结构的量化评估。<details>
<summary>Abstract</summary>
The segmentation of kidney layer structures, including cortex, outer stripe, inner stripe, and inner medulla within human kidney whole slide images (WSI) plays an essential role in automated image analysis in renal pathology. However, the current manual segmentation process proves labor-intensive and infeasible for handling the extensive digital pathology images encountered at a large scale. In response, the realm of digital renal pathology has seen the emergence of deep learning-based methodologies. However, very few, if any, deep learning based approaches have been applied to kidney layer structure segmentation. Addressing this gap, this paper assesses the feasibility of performing deep learning based approaches on kidney layer structure segmetnation. This study employs the representative convolutional neural network (CNN) and Transformer segmentation approaches, including Swin-Unet, Medical-Transformer, TransUNet, U-Net, PSPNet, and DeepLabv3+. We quantitatively evaluated six prevalent deep learning models on renal cortex layer segmentation using mice kidney WSIs. The empirical results stemming from our approach exhibit compelling advancements, as evidenced by a decent Mean Intersection over Union (mIoU) index. The results demonstrate that Transformer models generally outperform CNN-based models. By enabling a quantitative evaluation of renal cortical structures, deep learning approaches are promising to empower these medical professionals to make more informed kidney layer segmentation.
</details>
<details>
<summary>摘要</summary>
人类肾脏层结构分割，包括肾脏外带、内带和内 médulla 在人类肾脏整幅图像（WSI）中扮演着重要的作用，对于自动化图像分析在肾脏 Pathology 中起着关键性的作用。然而，现有的手动分割过程具有劳动密集和不可能处理大规模的数字 PATHOLOGY 图像的缺陷。为此，肾脏数字 PATHOLOGY 领域内部，出现了深度学习基本的方法。然而，深度学习基本的方法在肾脏层结构分割方面几乎没有被应用。为了解决这个差距，本研究评估了深度学习基本的方法在肾脏层结构分割方面的可能性。本研究采用了代表性的卷积神经网络（CNN）和转移神经网络（Transformer）分割方法，包括 Swin-Unet、医学转移神经网络（Medical-Transformer）、TransUNet、U-Net、PSPNet 和 DeepLabv3+。我们对六种常见的深度学习模型在mouse肾脏层分割中进行了数据量的评估。研究结果表明，Transformer 模型通常在肾脏外带层分割中表现更好，而 CNN 模型则表现较差。通过启用肾脏 cortical 结构的量化评估，深度学习方法在肾脏 PATHOLOGY 中表现了潜在的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation"><a href="#Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation" class="headerlink" title="Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation"></a>Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02529">http://arxiv.org/abs/2309.02529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haisheng Fu, Feng Liang, Jie Liang, Yongqiang Wang, Guohe Zhang, Jingning Han</li>
<li>for: 这篇论文是为了提高深度学习基于图像压缩的速度和性能而研究的。</li>
<li>methods: 这篇论文使用了四种技术来平衡复杂性和性能的负担：1）引入了可变卷积模块，以便更好地除去输入图像中的纹理重复; 2）设计了检查ер布Context模型，可以在平行解码中保持性能，而无需遵循顺序的上下文适应模型; 3）开发了一种改进的三步知识传递和训练方案，以实现不同的复杂性和性能负担之间的平衡。</li>
<li>results: 对于比较的state-of-the-art学习图像编码方案，我们的方法可以在编码和解码过程中提高速度，并且在PSNR和MS-SSIM指标下测试时，在Kodak和Tecnick-40 datasets上超过了一些领先的学习方案。<details>
<summary>Abstract</summary>
Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the rate-distortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the trade-off between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing the performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between the complexity and the performance of the decoder network, which transfers both the final and intermediate results of the teacher network to the student network to help its training. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩Recently, there have been great advances in deep learning-based image compression. However, many leading schemes use serial context-adaptive entropy models, which are very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we propose four techniques to balance the trade-off between complexity and performance. First, we introduce a deformable convolutional module in the compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between complexity and performance of the decoder network. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details></li>
</ul>
<hr>
<h2 id="An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria"><a href="#An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria" class="headerlink" title="An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria"></a>An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03235">http://arxiv.org/abs/2309.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Upender Kalwa, Yunsoo Park, Michael J. Kimber, Santosh Pandey</li>
<li>For: The paper is written to test potential drug candidates for the treatment of Lymphatic filariasis (LF) caused by Brugia malayi, a thread-like parasitic worm.* Methods: The paper describes a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro, using various parameters such as centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number.* Results: The paper reports on the use of this assay to test the effectiveness of three anthelmintic drugs against adult B. malayi and mf, and provides a visual representation of pose estimates and behavioral attributes in both space and time scales.Here is the information in Simplified Chinese text:* 为: 本文用于测试抑菌药物对brugia malayi引起的淋巴感染的治疗方案。* 方法: 本文描述了一种多参数现象学试验，用于跟踪成人brugia malayi和mf的运动能力，包括中心速度、轨迹弯曲、角速度、弯曲率、轴心距离和欧拉数。* 结果: 本文报告了三种抑菌药物对成人brugia malayi和mf的效果，并提供了在空间和时间尺度上的Visual化pose估计和行为特征。<details>
<summary>Abstract</summary>
Brugia malayi are thread-like parasitic worms and one of the etiological agents of Lymphatic filariasis (LF). Existing anthelmintic drugs to treat LF are effective in reducing the larval microfilaria (mf) counts in human bloodstream but are less effective on adult parasites. To test potential drug candidates, we report a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro. For adult B. malayi, motility is characterized by the centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number. These parameters are evaluated in experiments with three anthelmintic drugs. For B. malayi mf, motility is extracted from the evolving body skeleton to yield positional data and bending angles at 74 key point. We achieved high-fidelity tracking of complex worm postures (self-occlusions, omega turns, body bending, and reversals) while providing a visual representation of pose estimates and behavioral attributes in both space and time scales.
</details>
<details>
<summary>摘要</summary>
布鲁迪亚马LAY是线形寄生虫，是淋巴疟疾（LF）的etiological agent之一。现有的安特藻药可以降低人体血液中的 larval microfilaria（mf）数量，但对成熟寄生虫 menos effective。为测试潜在的药物候选人，我们报道了一种多parameter fenotipic assay，基于成人布鲁迪亚马和mf的运动追踪。对于成人布鲁迪亚马，运动特征包括中心速度、轨迹弯曲、angular velocity、eccentricity、extent和Euler数。这些参数在三种安特藻药实验中被评估。对于布鲁迪亚马mf，运动是从发展的身体骨架中提取的，以获取位势数据和折弯角度。我们实现了高精度的追踪复杂的虫姿（自我 occlusions、omega turns、身体弯曲和反转），同时提供了视觉表示pose estimates和行为特征的空间和时间尺度。
</details></li>
</ul>
<hr>
<h2 id="Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm"><a href="#Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm" class="headerlink" title="Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm"></a>Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02340">http://arxiv.org/abs/2309.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4netzero/infinite_texture_gans">https://github.com/ai4netzero/infinite_texture_gans</a></li>
<li>paper_authors: Alhasan Abdellatif, Ahmed H. Elsheikh</li>
<li>for:  Generating texture images of infinite resolution using GANs with a patch-by-patch paradigm.</li>
<li>methods: Local padding in the generator and spatial stochastic modulation to ensure consistency and diversity of generated textures.</li>
<li>results: Superior scalability compared to existing approaches while maintaining visual coherence of generated textures.Here’s the simplified Chinese version:</li>
<li>for: 使用GANs基于 patch-by-patch 方法生成无限分辨率的текстура图像.</li>
<li>methods: 使用本地补充和空间随机 modify 保证生成图像具有一致性和多样性.</li>
<li>results: 与现有方法相比，实现了更高的可扩展性，同时保持生成图像的视觉一致性.<details>
<summary>Abstract</summary>
In this paper, we introduce a novel approach for generating texture images of infinite resolutions using Generative Adversarial Networks (GANs) based on a patch-by-patch paradigm. Existing texture synthesis techniques often rely on generating a large-scale texture using a one-forward pass to the generating model, this limits the scalability and flexibility of the generated images. In contrast, the proposed approach trains GANs models on a single texture image to generate relatively small patches that are locally correlated and can be seamlessly concatenated to form a larger image while using a constant GPU memory footprint. Our method learns the local texture structure and is able to generate arbitrary-size textures, while also maintaining coherence and diversity. The proposed method relies on local padding in the generator to ensure consistency between patches and utilizes spatial stochastic modulation to allow for local variations and diversity within the large-scale image. Experimental results demonstrate superior scalability compared to existing approaches while maintaining visual coherence of generated textures.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法，使用生成对抗网络（GANs）来生成无穷大分辨率的текxture图像。现有的 texture生成技术 oftentimes rely on generating a large-scale texture using a one-forward pass to the generating model, which limits the scalability and flexibility of the generated images. 在我们的方法中，我们将 GANs 模型训练在单个 texture 图像上，以生成相对较小的 patches，这些 patches 在 мест均匀的方式相互链接，可以形成更大的图像，同时使用常量的 GPU 内存占用。我们的方法学习了地方xture结构，能够生成任意大小的 texture，同时保持了视觉准确性和多样性。我们的方法使用地方padding在生成器中确保 patches 之间的一致性，并使用空间杂化调制器来允许局部变化和多样性在大规模图像中。实验结果表明，我们的方法可以与现有方法相比，在可扩展性和可靠性方面具有更高的可扩展性，同时保持生成的 texture 的视觉准确性。
</details></li>
</ul>
<hr>
<h2 id="DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces"><a href="#DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces" class="headerlink" title="DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces"></a>DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02335">http://arxiv.org/abs/2309.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Williams, João Pedrosa, Muhammad Asad, Laura Cattani, Tom Vercauteren, Jan Deprest, Jan D’hooge</li>
<li>for: 本研究旨在提供一种基于深度学习的自动分割方法，以便在临床应用中提高分割精度和可靠性。</li>
<li>methods: 本研究使用了一种基于Convolutional Neural Network (CNN)的3D分割框架，其中包括一种B-spline explicit active surface (BEAS)，以确保分割结果具有3D空间的缓和性和解剖可能性，同时允许用户精确地编辑3D表面。</li>
<li>results: 实验结果表明，提议的框架可以为用户提供明确的表面控制，相比4D View VOCAL（GE Healthcare，Zipf，奥地利），NASA-TLX指数下降30%，用户时间减少70%（p&lt;0.00001）。<details>
<summary>Abstract</summary>
Deep learning-based automatic segmentation methods have become state-of-the-art. However, they are often not robust enough for direct clinical application, as domain shifts between training and testing data affect their performance. Failure in automatic segmentation can cause sub-optimal results that require correction. To address these problems, we propose a novel 3D extension of an interactive segmentation framework that represents a segmentation from a convolutional neural network (CNN) as a B-spline explicit active surface (BEAS). BEAS ensures segmentations are smooth in 3D space, increasing anatomical plausibility, while allowing the user to precisely edit the 3D surface. We apply this framework to the task of 3D segmentation of the anal sphincter complex (AS) from transperineal ultrasound (TPUS) images, and compare it to the clinical tool used in the pelvic floor disorder clinic (4D View VOCAL, GE Healthcare; Zipf, Austria). Experimental results show that: 1) the proposed framework gives the user explicit control of the surface contour; 2) the perceived workload calculated via the NASA-TLX index was reduced by 30% compared to VOCAL; and 3) it required 7 0% (170 seconds) less user time than VOCAL (p< 0.00001)
</details>
<details>
<summary>摘要</summary>
深度学习自动分割方法已成为状态函数。然而，它们通常不够可靠，因为领域转换影响其性能。自动分割失败可能会导致不优化的结果需要更正。为解决这些问题，我们提出了一种三维扩展的交互式分割框架，它将分割由 convolutional neural network (CNN) 表示为 B-spline 显式活动表面 (BEAS)。BEAS 确保分割是三维空间上的平滑，提高了生物学可能性，同时允许用户精确地编辑三维表面。我们将这种框架应用于transperineal ultrasound (TPUS) 图像中的肛门缺陷复合体 (AS) 的三维分割任务，并与 pelvic floor disorder clinic (4D View VOCAL, GE Healthcare; Zipf, Austria) 使用的临床工具进行比较。实验结果表明：1. 我们的框架给用户显式地控制表面几何;2. 根据 NASA-TLX 指数，用户工作负担降低了30%，与 VOCAL 相比;3. 用户时间需要7% (170秒)  menos than VOCAL (P<0.00001)。
</details></li>
</ul>
<hr>
<h2 id="TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction"><a href="#TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction" class="headerlink" title="TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction"></a>TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02318">http://arxiv.org/abs/2309.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghong Zhou, Huangxuan Zhao, Jiemin Fang, Dongqiao Xiang, Lei Chen, Lingxia Wu, Feihong Wu, Wenyu Liu, Chuansheng Zheng, Xinggang Wang</li>
<li>for: 这篇论文主要适用于四维扫描 Angelography (4D DSA) 的重建，以提高诊断多种医疗疾病的效率，如arteriovenous malformations (AVM) 和 arteriovenous fistulas (AVF)。</li>
<li>methods: 这篇论文提出了一种名为 Time-aware Attenuation Voxel (TiAVox) 的方法，用于精炼iew 4D DSA 重建，以降低高射针 radiation 问题。TiAVox 使用了4D attenuation voxel 网格，反映了空间和时间维度的吸收性质。它通过最小化精炼图像和缺失2D DSA 图像之间的差异来优化。</li>
<li>results: 在临床和 sintetic 数据集上验证了 TiAVox 方法，获得了31.23 Peak Signal-to-Noise Ratio (PSNR) 的新视图合成效果，只使用了30个视图。传统的 Feldkamp-Davis-Kress 方法需要133个视图。此外，只使用了10个视图，TiAVox 还可以获得34.32 PSNR 的新视图合成效果和41.40 PSNR 的3D重建效果。同时，对 TiAVox 的关键组件进行了ablation 研究，以证明其关键性。<details>
<summary>Abstract</summary>
Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical role in the diagnosis of many medical diseases, such as Arteriovenous Malformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant application value, the reconstruction of 4D DSA demands numerous views to effectively model the intricate vessels and radiocontrast flow, thereby implying a significant radiation dose. To address this high radiation issue, we propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA reconstruction, which paves the way for high-quality 4D imaging. Additionally, 2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA images. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation properties from both spatial and temporal dimensions. It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images. Without any neural network involved, TiAVox enjoys specific physical interpretability. The parameters of each learnable voxel represent the attenuation coefficients. We validated the TiAVox approach on both clinical and simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on the clinically sourced dataset, whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly, with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction. We also executed ablation studies to corroborate the essential components of TiAVox. The code will be publically available.
</details>
<details>
<summary>摘要</summary>
四维数字减除成像（4D DSA）在诊断各种医疾方面发挥关键作用，如血管肿瘤（AVM）和血管融合（AVF）。然而，4D DSA重建需要大量视图，以模拟血管复杂的凝固和流动，从而导致高射线暴露。为解决这一问题，我们提出了基于时间的抑减粒子（TiAVox）方法，以实现精度的4D DSA重建，并可以生成2D和3D DSA成像结果。TiAVox引入4D抑减粒子网格，表示空间和时间维度的抑减特性。它通过最小化与精心拍摄的2D DSA图像之间的差异来优化。不含任何神经网络，TiAVox具有特殊的物理解释性。每个学习粒子的参数表示抑减系数。我们在临床和模拟数据集上验证了TiAVox方法，在使用仅30个视图时，对新视图synthesis达到了31.23峰信号响应比（PSNR），而传统的Feldkamp-Davis-Kress方法需要133个视图。同样，只使用10个视图从 sintetic dataset，TiAVox可以达到PSNR为34.32和3D重建为41.40。我们还进行了ablation研究，以证明TiAVox的关键组件。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions"><a href="#Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions" class="headerlink" title="Advanced Underwater Image Restoration in Complex Illumination Conditions"></a>Advanced Underwater Image Restoration in Complex Illumination Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02217">http://arxiv.org/abs/2309.02217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Song, Mengkun She, Kevin Köser</li>
<li>for: 本研究旨在解决深水下摄影中的图像修复问题，具体来说是在200米深度以下的潜水环境中，因为自然光scarce，需要使用人工照明。传统的修复方法不适用于这种情况，因为照明源与摄像头相关，导致场景的变化。</li>
<li>methods: 本研究使用了新的约 Lambertian 表面的约束，通过对摄像头视图几何体中的对象或海底的变化，来估算照明场景。通过每个 voxel 存储一个信号因子和一个反射值，可以非常高效地修复摄像机-照明平台上的图像。</li>
<li>results: 实验结果表明，本方法可以准确地修复图像中的对象真实反射率，同时减少照明和媒体效果的影响。此外，本方法可以轻松扩展到其他场景，如在空中摄影或类似情况下。<details>
<summary>Abstract</summary>
Underwater image restoration has been a challenging problem for decades since the advent of underwater photography. Most solutions focus on shallow water scenarios, where the scene is uniformly illuminated by the sunlight. However, the vast majority of uncharted underwater terrain is located beyond 200 meters depth where natural light is scarce and artificial illumination is needed. In such cases, light sources co-moving with the camera, dynamically change the scene appearance, which make shallow water restoration methods inadequate. In particular for multi-light source systems (composed of dozens of LEDs nowadays), calibrating each light is time-consuming, error-prone and tedious, and we observe that only the integrated illumination within the viewing volume of the camera is critical, rather than the individual light sources. The key idea of this paper is therefore to exploit the appearance changes of objects or the seafloor, when traversing the viewing frustum of the camera. Through new constraints assuming Lambertian surfaces, corresponding image pixels constrain the light field in front of the camera, and for each voxel a signal factor and a backscatter value are stored in a volumetric grid that can be used for very efficient image restoration of camera-light platforms, which facilitates consistently texturing large 3D models and maps that would otherwise be dominated by lighting and medium artifacts. To validate the effectiveness of our approach, we conducted extensive experiments on simulated and real-world datasets. The results of these experiments demonstrate the robustness of our approach in restoring the true albedo of objects, while mitigating the influence of lighting and medium effects. Furthermore, we demonstrate our approach can be readily extended to other scenarios, including in-air imaging with artificial illumination or other similar cases.
</details>
<details>
<summary>摘要</summary>
水下图像修复问题已经是数十年来的挑战，自光学摄影的出现以来。大多数解决方案都专注于浅水enario，其中场景由太阳光均勋照。然而，真正的未探索的水下地形大多集中在200米深度以下，那里的自然光scarce，需要人工照明。在这种情况下，相机上方的灯光 sources co-moving with the camera, dynamically change the scene appearance, 使得浅水修复方法无法满足需求。特别是，现在LEDs组成的多光源系统中，每个灯光的准确性、繁杂性和耗时性都是问题。本文的关键想法是利用相机视图卷积体中对象或海底的变化来恢复图像。通过新的约束，每个像素对应的灯光场在前Camera的位置做出约束，并对每个 voxel 存储一个信号因子和一个反射值，可以用于非常高效地修复相机灯光平台的图像，从而实现了一个大型3D模型和地图的均衡塑造。为验证我们的方法的有效性，我们进行了大量的实验，其中包括模拟数据集和实际数据集。实验结果表明，我们的方法可以准确地恢复物体的真实反射率，同时减轻灯光和媒体效果的影响。此外，我们还证明了我们的方法可以轻松扩展到其他场景，包括在空中拍摄的人工照明情况或类似情况。
</details></li>
</ul>
<hr>
<h2 id="High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network"><a href="#High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network" class="headerlink" title="High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network"></a>High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02179">http://arxiv.org/abs/2309.02179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoforos Galazis, Anil Anthony Bharath, Marta Varela</li>
<li>for: 预测和诊断心血管疾病的诊断和预后。</li>
<li>methods: 使用高分辨率动力磁共振成像（Cine MRI）获得3D左心室运动和扭formation的全面图像，并提出一种自动化左心室运动特征 Extraction 工具。</li>
<li>results: 实验显示该工具可以准确跟踪左心室墙面在心脏周期中的运动， Hausdorff 距离平均值为2.51±1.3mm， Dice 分数平均值为0.96±0.02。<details>
<summary>Abstract</summary>
Functional analysis of the left atrium (LA) plays an increasingly important role in the prognosis and diagnosis of cardiovascular diseases. Echocardiography-based measurements of LA dimensions and strains are useful biomarkers, but they provide an incomplete picture of atrial deformations. High-resolution dynamic magnetic resonance images (Cine MRI) offer the opportunity to examine LA motion and deformation in 3D, at higher spatial resolution and with full LA coverage. However, there are no dedicated tools to automatically characterise LA motion in 3D. Thus, we propose a tool that automatically segments the LA and extracts the displacement fields across the cardiac cycle. The pipeline is able to accurately track the LA wall across the cardiac cycle with an average Hausdorff distance of $2.51 \pm 1.3~mm$ and Dice score of $0.96 \pm 0.02$.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对左心室（LA）的功能分析在心血管疾病诊断和 прогностицировании中日益重要。基于echo射频测量的LA大小和弹性 Parameters 可以作为生物标志物，但它们只提供了 incomplete 的atrial deformation 图像。高解度动力磁共振成像（Cine MRI）可以为LA运动和变形提供更高的空间分辨率和全面的LA覆盖率。然而，没有专门的工具来自动 caracterize LA的运动。因此，我们提议一种工具，可以自动将LA分割成多个部分，并提取cardiac cycle 中的变形场。管道可以准确地跟踪LA墙的运动，average Hausdorff distance 为2.51±1.3毫米，Dice score 为0.96±0.02。
</details></li>
</ul>
<hr>
<h2 id="INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses"><a href="#INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses" class="headerlink" title="INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses"></a>INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02147">http://arxiv.org/abs/2309.02147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AMiiR-S/Inceptnet_cancer_recognition">https://github.com/AMiiR-S/Inceptnet_cancer_recognition</a></li>
<li>paper_authors: Amirhossein Sajedi, Mohammad Javad Fadaeieslam</li>
<li>For: The paper is written for early disease detection and segmentation of medical images, with the goal of enhancing precision and performance.* Methods: The proposed method, called InceptNet, is a deep neural network that uses an Inception module to capture variations in scaled regions of interest, and improves the network’s ability to approximate an optimal local sparse structure.* Results: The proposed method outperformed previous works on four benchmark datasets, with improvements ranging from 0.0021 to 0.0544 in accuracy. Additionally, the method was found to be effective in detecting small scale structures in medical images.<details>
<summary>Abstract</summary>
In view of the recent paradigm shift in deep AI based image processing methods, medical image processing has advanced considerably. In this study, we propose a novel deep neural network (DNN), entitled InceptNet, in the scope of medical image processing, for early disease detection and segmentation of medical images in order to enhance precision and performance. We also investigate the interaction of users with the InceptNet application to present a comprehensive application including the background processes, and foreground interactions with users. Fast InceptNet is shaped by the prominent Unet architecture, and it seizes the power of an Inception module to be fast and cost effective while aiming to approximate an optimal local sparse structure. Adding Inception modules with various parallel kernel sizes can improve the network's ability to capture the variations in the scaled regions of interest. To experiment, the model is tested on four benchmark datasets, including retina blood vessel segmentation, lung nodule segmentation, skin lesion segmentation, and breast cancer cell detection. The improvement was more significant on images with small scale structures. The proposed method improved the accuracy from 0.9531, 0.8900, 0.9872, and 0.9881 to 0.9555, 0.9510, 0.9945, and 0.9945 on the mentioned datasets, respectively, which show outperforming of the proposed method over the previous works. Furthermore, by exploring the procedure from start to end, individuals who have utilized a trial edition of InceptNet, in the form of a complete application, are presented with thirteen multiple choice questions in order to assess the proposed method. The outcomes are evaluated through the means of Human Computer Interaction.
</details>
<details>
<summary>摘要</summary>
因为深度AI技术的最近 парадигShift，医疗图像处理方法得到了显著提高。在这项研究中，我们提出了一种新的深度神经网络（DNN），名为InceptNet，用于医疗图像处理中的疾病早期检测和图像分割，以提高精度和性能。我们还 investigate用户与InceptNet应用程序之间的交互，以提供一个全面的应用程序，包括背景进程和前景交互。快速InceptNet基于显著的Unet架构，并具有一个快速和成本效果的Inception模块，以便在不同的缩放级别上适应地捕捉疾病的变化。通过添加不同的平行核心大小的Inception模块，可以提高网络的能力来捕捉缩放区域的变化。为了实验，我们测试了这个模型在四个标准数据集上，包括血管segmentation、肺肿segmentation、皮肤病变segmentation和乳腺癌细胞检测。结果显示，提出的方法在图像中的小规模结构上的改进更为显著。我们的方法提高了对四个数据集的准确率，从0.9531、0.8900、0.9872和0.9881分别提高到0.9555、0.9510、0.9945和0.9945。此外，通过探索从开始到结束的过程，我们发现了一些人们在使用尝试版InceptNet时的问题，并通过多个选择题询问他们以评估提出的方法。结果被评估通过人计算机交互方式。
</details></li>
</ul>
<hr>
<h2 id="RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image"><a href="#RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image" class="headerlink" title="RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image"></a>RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02020">http://arxiv.org/abs/2309.02020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackzou233/rawhdr">https://github.com/jackzou233/rawhdr</a></li>
<li>paper_authors: Yunhao Zou, Chenggang Yan, Ying Fu</li>
<li>for: 提高高动态范围图像的生成精度</li>
<li>methods: 利用 Raw 感知器数据、分别掌握较为容易和较为困难的区域、两种导航方法（对几个不具备信息的通道使用具备更多信息的通道作为导航）</li>
<li>results: 提高 Raw 图像到高动态范围图像的映射精度，并验证了该方法的优越性以及 newly 收集的 Raw&#x2F;HDR 对应集的有用性。<details>
<summary>Abstract</summary>
High dynamic range (HDR) images capture much more intensity levels than standard ones. Current methods predominantly generate HDR images from 8-bit low dynamic range (LDR) sRGB images that have been degraded by the camera processing pipeline. However, it becomes a formidable task to retrieve extremely high dynamic range scenes from such limited bit-depth data. Unlike existing methods, the core idea of this work is to incorporate more informative Raw sensor data to generate HDR images, aiming to recover scene information in hard regions (the darkest and brightest areas of an HDR scene). To this end, we propose a model tailor-made for Raw images, harnessing the unique features of Raw data to facilitate the Raw-to-HDR mapping. Specifically, we learn exposure masks to separate the hard and easy regions of a high dynamic scene. Then, we introduce two important guidances, dual intensity guidance, which guides less informative channels with more informative ones, and global spatial guidance, which extrapolates scene specifics over an extended spatial domain. To verify our Raw-to-HDR approach, we collect a large Raw/HDR paired dataset for both training and testing. Our empirical evaluations validate the superiority of the proposed Raw-to-HDR reconstruction model, as well as our newly captured dataset in the experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像可以捕捉更多的Intensity级别than标准图像。当前方法主要从8比特低动态范围（LDR）sRGB图像中生成HDR图像，这些图像在摄像头处理管道中受到了压缩。然而，从有限比特数据中恢复极高动态范围场景是一项具有挑战性的任务。与现有方法不同，本文的核心想法是利用更有信息的Raw感知器数据来生成HDR图像，以便恢复场景信息在极高动态范围中的hard区域（极dark和极 bright区域）。为此，我们提出了特有的Raw图像模型，利用Raw数据的独特特点来促进Raw-to-HDR映射。具体来说，我们学习出处理硬区域的曝光面积，然后引入两种重要的导航：对不具有充足信息的通道来说，使用更具有信息的通道进行导航（双极Intensity导航），并在扩展的空间领域内对场景特征进行推导（全局空间导航）。为验证我们的Raw-to-HDR重建模型，我们收集了大量Raw/HDR对应的数据集，用于训练和测试。我们的实验结果证明了我们提出的Raw-to-HDR重建模型的优越性，以及我们新收集的数据集在实验中的有用性。
</details></li>
</ul>
<hr>
<h2 id="Logarithmic-Mathematical-Morphology-theory-and-applications"><a href="#Logarithmic-Mathematical-Morphology-theory-and-applications" class="headerlink" title="Logarithmic Mathematical Morphology: theory and applications"></a>Logarithmic Mathematical Morphology: theory and applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02007">http://arxiv.org/abs/2309.02007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Noyel</li>
<li>for:  Addressing the issue of lighting variations in Mathematical Morphology, a new framework named Logarithmic Mathematical Morphology (LMM) is defined.</li>
<li>methods:  The LMM framework uses an additive law that varies the amplitude of the structuring function according to the image amplitude, and models lighting variations with a physical cause.</li>
<li>results:  In images with uniform lighting variations, the LMM operators perform better than usual morphological operators. In eye-fundus images with non-uniform lighting variations, the LMM method for vessel segmentation shows better robustness to lighting variations compared to three state-of-the-art approaches.<details>
<summary>Abstract</summary>
Classically, in Mathematical Morphology, an image (i.e., a grey-level function) is analysed by another image which is named the structuring element or the structuring function. This structuring function is moved over the image domain and summed to the image. However, in an image presenting lighting variations, the analysis by a structuring function should require that its amplitude varies according to the image intensity. Such a property is not verified in Mathematical Morphology for grey level functions, when the structuring function is summed to the image with the usual additive law. In order to address this issue, a new framework is defined with an additive law for which the amplitude of the structuring function varies according to the image amplitude. This additive law is chosen within the Logarithmic Image Processing framework and models the lighting variations with a physical cause such as a change of light intensity or a change of camera exposure-time. The new framework is named Logarithmic Mathematical Morphology (LMM) and allows the definition of operators which are robust to such lighting variations. In images with uniform lighting variations, those new LMM operators perform better than usual morphological operators. In eye-fundus images with non-uniform lighting variations, a LMM method for vessel segmentation is compared to three state-of-the-art approaches. Results show that the LMM approach has a better robustness to such variations than the three others.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors"><a href="#Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors" class="headerlink" title="Empowering Low-Light Image Enhancer through Customized Learnable Priors"></a>Empowering Low-Light Image Enhancer through Customized Learnable Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01958">http://arxiv.org/abs/2309.01958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zheng980629/cue">https://github.com/zheng980629/cue</a></li>
<li>paper_authors: Naishan Zheng, Man Zhou, Yanmeng Dong, Xiangyu Rui, Jie Huang, Chongyi Li, Feng Zhao</li>
<li>for: 提高低光照图像的品质，增强图像的亮度和降低噪音。</li>
<li>methods: 使用自定义可学习的假设来改善深度 unfolding 架构的透明度，包括通过结构流和优化流两种方法来自定义 Masked Autoencoder（MAE）的特征表示能力。</li>
<li>results: 在多个低光照图像提升数据集上，提出的方法比现有方法更高效，并且可以更好地解释和 интерпретирова模型的输出。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable progress in enhancing low-light images by improving their brightness and eliminating noise. However, most existing methods construct end-to-end mapping networks heuristically, neglecting the intrinsic prior of image enhancement task and lacking transparency and interpretability. Although some unfolding solutions have been proposed to relieve these issues, they rely on proximal operator networks that deliver ambiguous and implicit priors. In this work, we propose a paradigm for low-light image enhancement that explores the potential of customized learnable priors to improve the transparency of the deep unfolding paradigm. Motivated by the powerful feature representation capability of Masked Autoencoder (MAE), we customize MAE-based illumination and noise priors and redevelop them from two perspectives: 1) \textbf{structure flow}: we train the MAE from a normal-light image to its illumination properties and then embed it into the proximal operator design of the unfolding architecture; and m2) \textbf{optimization flow}: we train MAE from a normal-light image to its gradient representation and then employ it as a regularization term to constrain noise in the model output. These designs improve the interpretability and representation capability of the model.Extensive experiments on multiple low-light image enhancement datasets demonstrate the superiority of our proposed paradigm over state-of-the-art methods. Code is available at https://github.com/zheng980629/CUE.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经取得了优化低光照图像的显著进步，提高图像的亮度和消除噪声。然而，大多数现有方法是通过静默地构建终端到终端的映射网络，忽略了图像提升任务的内在先验知识和透明度和可解释性。虽然一些解开解决方案已经被提出，但它们基于距离运算网络，elivery ambiguous和隐藏先验知识。在这个工作中，我们提出了一种低光照图像提升的方法，探索了可定制学习先验知识的潜力，以改善深 unfolding 架构的透明度。我们的方法受到Masked Autoencoder (MAE) 的强大特征表示能力的激发，我们自定义 MAE 基于照明和噪声的特征，并将其重新设计为两种视角：1. 结构流：我们从正常光照图像开始，通过 MAE 进行训练，然后将其 embedding 到 proximal 操作符的设计中。2. 优化流：我们从正常光照图像开始，通过 MAE 进行训练，然后将其作为Regularization term来限制模型输出中的噪声。这些设计提高了模型的透明度和表示能力。我们对多个低光照图像提升数据进行了广泛的实验，结果表明我们的提出的方法在当前的方法之上表现出优异性。代码可以在 <https://github.com/zheng980629/CUE> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network"><a href="#Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network" class="headerlink" title="Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network"></a>Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01944">http://arxiv.org/abs/2309.01944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Deshi Li, Kaitao Meng, Mingliu Liu, Shuya Zhu</li>
<li>for: 提高video播放质量和服务性能在 Vehicular Communication Networks (VCNs) 中</li>
<li>methods: 基于高光 entropy 模型、幂 transform 和 Iterative 高光方向 Trimming 算法</li>
<li>results: 比对assauming 方案具有显著提高高光 entropy 和 jitter 性能<details>
<summary>Abstract</summary>
Video traffic in vehicular communication networks (VCNs) faces exponential growth. However, different segments of most videos reveal various attractiveness for viewers, and the pre-caching decision is greatly affected by the dynamic service duration that edge nodes can provide services for mobile vehicles driving along a road. In this paper, we propose an efficient video highlight pre-caching scheme in the vehicular communication network, adapting to the service duration. Specifically, a highlight entropy model is devised with the consideration of the segments' popularity and continuity between segments within a period of time, based on which, an optimization problem of video highlight pre-caching is formulated. As this problem is non-convex and lacks a closed-form expression of the objective function, we decouple multiple variables by deriving candidate highlight segmentations of videos through wavelet transform, which can significantly reduce the complexity of highlight pre-caching. Then the problem is solved iteratively by a highlight-direction trimming algorithm, which is proven to be locally optimal. Simulation results based on real-world video datasets demonstrate significant improvement in highlight entropy and jitter compared to benchmark schemes.
</details>
<details>
<summary>摘要</summary>
视频流量在交通通信网络（VCN）中呈指数增长趋势。然而，不同的视频段落吸引了不同的观众，并且边节点可以在路面上驱动移动的车辆提供动态服务时间，这大大影响了预缓存决策。在这篇论文中，我们提出了一种高效的视频精彩预缓存方案，适应服务时间。具体来说，我们提出了一种高光积分模型，考虑视频段落的吸引力和时间内 segment之间的连续性，并基于此模型，我们形ulated一个预缓存问题。由于这个问题是非对称的和无法表达目标函数的closed-form，我们使用波лет变换 derivation candidate高光段落，这可以很大减少了预缓存的复杂性。然后，我们通过一种高光方向裁剪算法来解决这个问题，该算法是当地最优的。实验结果基于实际的视频数据集表明，我们的方案可以很大提高高光积分和频率抖动相比 benchmark 方案。
</details></li>
</ul>
<hr>
<h2 id="BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior"><a href="#BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior" class="headerlink" title="BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior"></a>BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01865">http://arxiv.org/abs/2309.01865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Liu, Gesine Muller, Nassir Navab, Carsten Marr, Jan Huisken, Tingying Peng</li>
<li>for: 提高LSFM中样品的高分辨率成像，抵消光散射引起的影像质量下降。</li>
<li>methods: 使用双视图成像技术，通过对样品从不同方向进行扫描，以实现高分辨率成像。并采用全局上下文意识的图像融合方法，基于本地图像质量来确定focus-defocus边界，并考虑光散射对ocus measures的影响。</li>
<li>results: 实验结果表明，BigFUSE可以自动排除结构化 artifacts，高效地融合 dual-view LSFM 中的信息，并且能够稳定地确定ocus-defocus边界，提高LSFM中样品的高分辨率成像质量。<details>
<summary>Abstract</summary>
Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples, experiences defocused image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dualview imaging is helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes Theorem, where (i) the effect of light scattering onto focus measures is included within Likelihood; and (ii) the spatial consistency regarding focus-defocus is imposed in Prior. The expectation-maximum algorithm is then adopted to estimate the focus-defocus boundary. Competitive experimental results show that BigFUSE is the first dual-view LSFM fuser that is able to exclude structured artifacts when fusing information, highlighting its abilities of automatic image fusion.
</details>
<details>
<summary>摘要</summary>
光束照明微scopy (LSFM)，一种平面照明技术，可以实现高分辨率的样品图像。然而，当光子在厚重的样品中传播时，会产生光散射，导致图像失真。为解决这问题，双视图减少是有帮助的。它可以在不同的视角下扫描样品，从而获得不同部分的样品图像。然而，当应用最新的图像融合方法时，由于它们的视野有限，会导致空间不一致的Focus推估。为此，我们提出了BigFUSE，一种全球上下文意识的图像融合器。它可以在LSFM中稳定图像融合，并且考虑样品中光子的全球影响，以确定基于本地图像质量的Focus推估。我们受 dual-view LSFM 图像形成优先顺序的启发，图像融合被视为使用 bayes 定理来定义Focus推估边界，其中（i）光散射对Focus推估的影响被包含在likelihood中，（ii）在Prior中强制实施Focus推估的空间一致性。然后，采用了expectation-maximum算法来估算Focus推估边界。竞争性实验结果表明，BigFUSE 是首个可以排除结构残余的 dual-view LSFM 融合器，highlighting its ability of自动图像融合。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.IV_2023_09_05/" data-id="clmjn91qv00hl0j886gm03gvj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/05/cs.LG_2023_09_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-05
        
      </div>
    </a>
  
  
    <a href="/2023/09/04/cs.SD_2023_09_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-04</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
