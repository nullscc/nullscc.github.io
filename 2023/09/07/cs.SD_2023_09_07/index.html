
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04031 repo_url: None paper_authors: Takuma Udagawa, Masayuki Suzuki, Gakuto K">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-07">
<meta property="og:url" content="https://nullscc.github.io/2023/09/07/cs.SD_2023_09_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04031 repo_url: None paper_authors: Takuma Udagawa, Masayuki Suzuki, Gakuto K">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-07T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.587Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/cs.SD_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T15:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multiple-Representation-Transfer-from-Large-Language-Models-to-End-to-End-ASR-Systems"><a href="#Multiple-Representation-Transfer-from-Large-Language-Models-to-End-to-End-ASR-Systems" class="headerlink" title="Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems"></a>Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04031">http://arxiv.org/abs/2309.04031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuma Udagawa, Masayuki Suzuki, Gakuto Kurata, Masayasu Muraoka, George Saon</li>
<li>for: 实现语言模型知识传递到终端自动语音识别系统中</li>
<li>methods: 使用多种方法获取和传递多个语言模型表示</li>
<li>results: 显示将多个语言模型表示传递到转构器基本自动语音识别系统中可以取得更好的效果<details>
<summary>Abstract</summary>
Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single representation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的知识传递是一种可能性的技术，用于将语言知识结合到端到端自动语音识别系统（ASR）中。然而，现有的工作只将单一的表现传递到 LLM（例如预训BERT的最后一层），而文本表现的非唯一性可以从不同层、上下文和模型中获取。在这个工作中，我们探索了许多方法来获取和传递多个 LLM 的表现到探索式 ASR 系统。尽管概念简单，但我们显示了传递多个 LLM 的表现可以是有效的代替。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Audio-Captioning-via-Audibility-Guidance"><a href="#Zero-Shot-Audio-Captioning-via-Audibility-Guidance" class="headerlink" title="Zero-Shot Audio Captioning via Audibility Guidance"></a>Zero-Shot Audio Captioning via Audibility Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03884">http://arxiv.org/abs/2309.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Shaharabany, Ariel Shaulov, Lior Wolf</li>
<li>for: 这篇论文主要关注的是Audio Captioning зада题，它的目标是生成对应音频文本的描述。</li>
<li>methods: 这篇论文提出了三个希望目标（fluency, faithfulness, audibility），并使用了三个网络来实现这些目标：一个大语言模型GPT-2、一个多媒体匹配网络ImageBind、以及一个文本分类器。</li>
<li>results: 研究人员通过对AudioCap数据集进行测试，发现使用audibility导向 significantly enhances表现，比基线更好。<details>
<summary>Abstract</summary>
The task of audio captioning is similar in essence to tasks such as image and video captioning. However, it has received much less attention. We propose three desiderata for captioning audio -- (i) fluency of the generated text, (ii) faithfulness of the generated text to the input audio, and the somewhat related (iii) audibility, which is the quality of being able to be perceived based only on audio. Our method is a zero-shot method, i.e., we do not learn to perform captioning. Instead, captioning occurs as an inference process that involves three networks that correspond to the three desired qualities: (i) A Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A model that provides a matching score between an audio file and a text, for which we use a multimodal matching network called ImageBind, and (iii) A text classifier, trained using a dataset we collected automatically by instructing GPT-4 with prompts designed to direct the generation of both audible and inaudible sentences. We present our results on the AudioCap dataset, demonstrating that audibility guidance significantly enhances performance compared to the baseline, which lacks this objective.
</details>
<details>
<summary>摘要</summary>
audio captioning的任务类似于图像和视频captioning，但它受到了远 fewer attention。我们提出了三个愿景 для captioning audio：（i）流畅的生成文本，（ii）对输入音频的 faithfulness，以及一定程度相关的（iii）可见度，即基于 purely audio 可以Perceived。我们的方法是一种零极方法，即不需要学习 captioning。而是通过三个网络来完成推理过程：（i）一个大语言模型，我们使用 GPT-2 的框架，（ii）一个对 audio 文件和文本进行匹配的模型，我们使用一个多modal matching network called ImageBind，（iii）一个基于自动生成的文本分类器，我们使用一个自动生成的 dataset 来训练。我们在 AudioCap 数据集上展示了我们的结果，显示了带有可见度指导的性能明显高于基eline，lacking this objective。
</details></li>
</ul>
<hr>
<h2 id="Causal-Signal-Based-DCCRN-with-Overlapped-Frame-Prediction-for-Online-Speech-Enhancement"><a href="#Causal-Signal-Based-DCCRN-with-Overlapped-Frame-Prediction-for-Online-Speech-Enhancement" class="headerlink" title="Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement"></a>Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03684">http://arxiv.org/abs/2309.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julitta Bartolewska, Stanisław Kacprzak, Konrad Kowalczyk</li>
<li>for: 提高单道麦克风信号质量和 inteligibilty</li>
<li>methods: 使用信号基于的 causal DCCRN，减少预测覆盖范围和网络参数数量</li>
<li>results: 实验结果显示，提posed模型在不同的语音提升指标方面具有相似或更好的性能，同时降低缓存延迟和网络参数数量约30%<details>
<summary>Abstract</summary>
The aim of speech enhancement is to improve speech signal quality and intelligibility from a noisy microphone signal. In many applications, it is crucial to enable processing with small computational complexity and minimal requirements regarding access to future signal samples (look-ahead). This paper presents signal-based causal DCCRN that improves online single-channel speech enhancement by reducing the required look-ahead and the number of network parameters. The proposed modifications include complex filtering of the signal, application of overlapped-frame prediction, causal convolutions and deconvolutions, and modification of the loss function. Results of performed experiments indicate that the proposed model with overlapped signal prediction and additional adjustments, achieves similar or better performance than the original DCCRN in terms of various speech enhancement metrics, while it reduces the latency and network parameter number by around 30%.
</details>
<details>
<summary>摘要</summary>
“目的是提高语音信号质量和可识别度，从含噪 microphone 信号中提取出清晰的语音信号。在许多应用中，需要减少计算复杂性和未来信号样本的需求（look-ahead）。这篇论文提出了信号基于的 causal DCCRN，用于在线单通道语音增强，提高语音质量和可识别度，同时减少计算复杂性和网络参数数量。提出的修改包括信号复杂滤波、叠加框预测、 causal 卷积和卷积反卷积，以及损失函数修改。实验结果表明，提出的模型，与原始 DCCRN 相比，在多种语音增强指标上具有类似或更好的性能，同时降低了延迟和网络参数数量约30%。”
</details></li>
</ul>
<hr>
<h2 id="Spiking-Structured-State-Space-Model-for-Monaural-Speech-Enhancement"><a href="#Spiking-Structured-State-Space-Model-for-Monaural-Speech-Enhancement" class="headerlink" title="Spiking Structured State Space Model for Monaural Speech Enhancement"></a>Spiking Structured State Space Model for Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03641">http://arxiv.org/abs/2309.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Du, Xu Liu, Yansong Chua</li>
<li>for: 提高speech干扰除之效率和计算成本，使用深度学习方法。</li>
<li>methods: 使用Spiking Structured State Space Model (Spiking-S4)，结合Spiking Neural Networks (SNN) 的能量效率和Structured State Space Models (S4) 的长距离序列模型能力。</li>
<li>results: 对 DNS Challenge 和 VoiceBank+Demand  datasets 进行评估，发现Spiking-S4 与现有Artificial Neural Network (ANN) 方法相当，但具有更少的计算资源（参数和 Floating Point Operations (FLOPs)）。<details>
<summary>Abstract</summary>
Speech enhancement seeks to extract clean speech from noisy signals. Traditional deep learning methods face two challenges: efficiently using information in long speech sequences and high computational costs. To address these, we introduce the Spiking Structured State Space Model (Spiking-S4). This approach merges the energy efficiency of Spiking Neural Networks (SNN) with the long-range sequence modeling capabilities of Structured State Space Models (S4), offering a compelling solution. Evaluation on the DNS Challenge and VoiceBank+Demand Datasets confirms that Spiking-S4 rivals existing Artificial Neural Network (ANN) methods but with fewer computational resources, as evidenced by reduced parameters and Floating Point Operations (FLOPs).
</details>
<details>
<summary>摘要</summary>
speech enhancement aims to extract clean speech from noisy signals. traditional deep learning methods face two challenges: efficiently using information in long speech sequences and high computational costs. to address these, we introduce the spiking structured state space model (spiking-s4). this approach merges the energy efficiency of spiking neural networks (snn) with the long-range sequence modeling capabilities of structured state space models (s4), offering a compelling solution. evaluation on the dns challenge and voicebank+demand datasets confirms that spiking-s4 rivals existing artificial neural network (ann) methods but with fewer computational resources, as evidenced by reduced parameters and floating point operations (flops).
</details></li>
</ul>
<hr>
<h2 id="Topological-fingerprints-for-audio-identification"><a href="#Topological-fingerprints-for-audio-identification" class="headerlink" title="Topological fingerprints for audio identification"></a>Topological fingerprints for audio identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03516">http://arxiv.org/abs/2309.03516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wreise/top_audio_id">https://github.com/wreise/top_audio_id</a></li>
<li>paper_authors: Wojciech Reise, Ximena Fernández, Maria Dominguez, Heather A. Harrington, Mariano Beguerisse-Díaz</li>
<li>for: 这篇论文是为了开发一种Audio文件重复检测的方法，以便在不同的情况下准确地检测Audio文件的重复性。</li>
<li>methods: 该方法使用 persistente homology 技术，对 audio signal 的 local spectral decomposition 进行计算，并使用 filtered cubical complexes 来编码 audio content。</li>
<li>results: 实验结果表明，该方法可以准确地检测时间对齐的 audio 轨迹，并且在 topological distortions 的情况下表现出excel。<details>
<summary>Abstract</summary>
We present a topological audio fingerprinting approach for robustly identifying duplicate audio tracks. Our method applies persistent homology on local spectral decompositions of audio signals, using filtered cubical complexes computed from mel-spectrograms. By encoding the audio content in terms of local Betti curves, our topological audio fingerprints enable accurate detection of time-aligned audio matchings. Experimental results demonstrate the accuracy of our algorithm in the detection of tracks with the same audio content, even when subjected to various obfuscations. Our approach outperforms existing methods in scenarios involving topological distortions, such as time stretching and pitch shifting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种拓扑音频指纹方法，用于坚定地识别 duplicates 的音频轨迹。我们的方法使用 persistente homology 在本地 spectral decompositions 中应用 filtered cubical complexes，从 mel-spectrograms 中计算出的 audio signals。通过将音频内容编码成本地 Betti 曲线，我们的拓扑音频指纹可以准确地检测时间对齐的音频匹配。实验结果表明我们的算法在包括拓扑扭曲在内的不同场景下具有高精度，比如时间延迟和调高。我们的方法也超过了现有的方法，在拓扑扭曲场景下表现更佳。
</details></li>
</ul>
<hr>
<h2 id="Simulating-room-transfer-functions-between-transducers-mounted-on-audio-devices-using-a-modified-image-source-method"><a href="#Simulating-room-transfer-functions-between-transducers-mounted-on-audio-devices-using-a-modified-image-source-method" class="headerlink" title="Simulating room transfer functions between transducers mounted on audio devices using a modified image source method"></a>Simulating room transfer functions between transducers mounted on audio devices using a modified image source method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03486">http://arxiv.org/abs/2309.03486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audiolabs/DEISM">https://github.com/audiolabs/DEISM</a></li>
<li>paper_authors: Zeyu Xu, Adrian Herzog, Alexander Lodermeyer, Emanuël A. P. Habets, Albert G. Prinn</li>
<li>for: 本研究旨在扩展图像源方法（ISM），以包括听取器和发射器附件的声学折射效应。</li>
<li>methods: 本研究使用圆锥幂直径幂系数来扩展ISM，以包括发射器和听取器附件的声学折射效应。</li>
<li>results: 研究表明，提案方法的准确性与房间内设备的大小、形状、数量和位置有关。一种简化版的提案方法也被提出，可以大幅减少计算成本。<details>
<summary>Abstract</summary>
The image source method (ISM) is often used to simulate room acoustics due to its ease of use and computational efficiency. The standard ISM is limited to simulations of room impulse responses between point sources and omnidirectional receivers. In this work, the ISM is extended using spherical harmonic directivity coefficients to include acoustic diffraction effects due to source and receiver transducers mounted on physical devices, which are typically encountered in practical situations. The proposed method is verified using finite element simulations of various loudspeaker and microphone configurations in a rectangular room. It is shown that the accuracy of the proposed method is related to the sizes, shapes, number, and positions of the devices inside a room. A simplified version of the proposed method, which can significantly reduce computational effort, is also presented. The proposed method and its simplified version can simulate room transfer functions more accurately than currently available image source methods and can aid the development and evaluation of speech and acoustic signal processing algorithms, including speech enhancement, acoustic scene analysis, and acoustic parameter estimation.
</details>
<details>
<summary>摘要</summary>
<SYS><TRANSLATE>图像源方法（ISM）经常用于模拟房间听音，因为它的使用容易和计算效率高。标准的ISM仅能模拟房间冲击响应 между点源和全irectional接收器。在这种工作中，ISM通过使用圆锥幂直径系数来包括听音折射效应，由源和接收器适配器安装在物理设备上，这些设备通常在实际应用中遇到。提议的方法被证明通过finite element simulations of various loudspeaker和microphone配置在rectangular room中。结果表明，提议的方法的准确性与房间内设备的大小、形状、数量和位置有关。一个简化版的提议方法，可以减少计算努力，也被提出。提议的方法和其简化版可以更准确地模拟房间传递函数，并且可以帮助开发和评估speech和听音信号处理算法，包括speech增强、听音场分析和听音参数估计。</TRANSLATE></SYS>Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/cs.SD_2023_09_07/" data-id="clmjn91oe00bx0j88cfdyc00v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/08/eess.IV_2023_09_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-08
        
      </div>
    </a>
  
  
    <a href="/2023/09/07/cs.LG_2023_09_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
