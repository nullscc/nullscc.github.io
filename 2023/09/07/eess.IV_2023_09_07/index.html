
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03906 repo_url: https:&#x2F;&#x2F;github.com&#x2F;uni-medical&#x2F;a-eval paper_authors: Ziyan H">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-07">
<meta property="og:url" content="https://nullscc.github.io/2023/09/07/eess.IV_2023_09_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03906 repo_url: https:&#x2F;&#x2F;github.com&#x2F;uni-medical&#x2F;a-eval paper_authors: Ziyan H">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-07T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.592Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/eess.IV_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T09:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Eval-A-Benchmark-for-Cross-Dataset-Evaluation-of-Abdominal-Multi-Organ-Segmentation"><a href="#A-Eval-A-Benchmark-for-Cross-Dataset-Evaluation-of-Abdominal-Multi-Organ-Segmentation" class="headerlink" title="A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation"></a>A-Eval: A Benchmark for Cross-Dataset Evaluation of Abdominal Multi-Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03906">http://arxiv.org/abs/2309.03906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uni-medical/a-eval">https://github.com/uni-medical/a-eval</a></li>
<li>paper_authors: Ziyan Huang, Zhongying Deng, Jin Ye, Haoyu Wang, Yanzhou Su, Tianbin Li, Hui Sun, Junlong Cheng, Jianpin Chen, Junjun He, Yun Gu, Shaoting Zhang, Lixu Gu, Yu Qiao</li>
<li>for: 本研究的目的是提供一个跨 dataset 的评估指标（A-Eval），用于评估多个腹部多器官分类模型的扩展性。</li>
<li>methods: 本研究使用了四个大规模公共数据集：FLARE22、AMOS、WORD 和 TotalSegmentator，各自提供了广泛的腹部多器官分类标签。实验使用了这些数据集的训练集和验证集，以建立一个多样化的 benchmark。</li>
<li>results: 研究发现，模型可以通过使用不同的数据集和不同的训练策略来提高扩展性。另外，模型的大小也影响了跨 dataset 的扩展性。通过这些分析，研究强调了训练数据集的多样化和模型的设计在提高模型扩展性方面的重要性。<details>
<summary>Abstract</summary>
Although deep learning have revolutionized abdominal multi-organ segmentation, models often struggle with generalization due to training on small, specific datasets. With the recent emergence of large-scale datasets, some important questions arise: \textbf{Can models trained on these datasets generalize well on different ones? If yes/no, how to further improve their generalizability?} To address these questions, we introduce A-Eval, a benchmark for the cross-dataset Evaluation ('Eval') of Abdominal ('A') multi-organ segmentation. We employ training sets from four large-scale public datasets: FLARE22, AMOS, WORD, and TotalSegmentator, each providing extensive labels for abdominal multi-organ segmentation. For evaluation, we incorporate the validation sets from these datasets along with the training set from the BTCV dataset, forming a robust benchmark comprising five distinct datasets. We evaluate the generalizability of various models using the A-Eval benchmark, with a focus on diverse data usage scenarios: training on individual datasets independently, utilizing unlabeled data via pseudo-labeling, mixing different modalities, and joint training across all available datasets. Additionally, we explore the impact of model sizes on cross-dataset generalizability. Through these analyses, we underline the importance of effective data usage in enhancing models' generalization capabilities, offering valuable insights for assembling large-scale datasets and improving training strategies. The code and pre-trained models are available at \href{https://github.com/uni-medical/A-Eval}{https://github.com/uni-medical/A-Eval}.
</details>
<details>
<summary>摘要</summary>
although deep learning have revolutionized 腹部多器官分割，模型经常受到泛化问题的影响，即在不同的数据集上进行训练后，模型是否能够具有良好的泛化能力？如果可以，那么如何进一步提高其泛化能力？为了回答这些问题，我们提出了A-Eval，一个跨数据集评估（Eval）的权威指标集，用于评估腹部多器官分割模型的泛化能力。我们在四个大规模公共数据集上进行了训练：FLARE22、AMOS、WORD和TotalSegmentator，每个数据集都提供了详细的腹部多器官分割标签。为了评估，我们将这些数据集的验证集和BTCV数据集的训练集结合起来，组成一个robust的指标集，包括五个不同的数据集。我们使用这个指标集来评估不同模型的泛化能力，强调多样数据使用方案的影响，包括单独训练每个数据集、使用无标签数据 Pseudo-labeling、混合不同模式和共同训练所有可用数据集。此外，我们还研究模型大小对跨数据集泛化能力的影响。通过这些分析，我们强调了有效地使用数据的重要性，并提供了值得关注的数据准备和训练策略。代码和预训练模型可以在 <https://github.com/uni-medical/A-Eval> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Continuous-Exposure-Value-Representations-for-Single-Image-HDR-Reconstruction"><a href="#Learning-Continuous-Exposure-Value-Representations-for-Single-Image-HDR-Reconstruction" class="headerlink" title="Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction"></a>Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03900">http://arxiv.org/abs/2309.03900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, Yen-Yu Lin</li>
<li>for: 这个论文的目的是为了提高单张图像的HDR重建。</li>
<li>methods: 这个论文使用了深度学习来生成LDR堆栈，并使用了一个隐式函数来生成LDR图像的不同曝光值（EV）。</li>
<li>results: 该方法可以生成包含多个不同EV的连续的LDR堆栈，从而提高HDR重建质量。对比 existed方法，我们的CEVR模型表现更优。<details>
<summary>Abstract</summary>
Deep learning is commonly used to reconstruct HDR images from LDR images. LDR stack-based methods are used for single-image HDR reconstruction, generating an HDR image from a deep learning-generated LDR stack. However, current methods generate the stack with predetermined exposure values (EVs), which may limit the quality of HDR reconstruction. To address this, we propose the continuous exposure value representation (CEVR), which uses an implicit function to generate LDR images with arbitrary EVs, including those unseen during training. Our approach generates a continuous stack with more images containing diverse EVs, significantly improving HDR reconstruction. We use a cycle training strategy to supervise the model in generating continuous EV LDR images without corresponding ground truths. Our CEVR model outperforms existing methods, as demonstrated by experimental results.
</details>
<details>
<summary>摘要</summary>
深度学习通常用于从LDR图像中重建HDR图像。现有的LDR堆栈基本方法用于单个图像HDR重建，通过深度学习生成的LDR堆栈来生成HDR图像。然而，现有方法通常使用预先确定的曝光值（EV）来生成堆栈，这可能会限制HDR重建质量。为解决这个问题，我们提出了连续曝光值表示（CEVR），它使用隐式函数来生成具有任意EV的LDR图像，包括训练过程中未经见到的EV。我们的方法生成了更多包含多样EV的LDR图像，Significantly Improving HDR重建。我们使用循环训练策略来监督模型在生成连续EV LDR图像时，无需对应的真实参考图像。我们的CEVR模型在实验结果中胜过现有方法。
</details></li>
</ul>
<hr>
<h2 id="T2IW-Joint-Text-to-Image-Watermark-Generation"><a href="#T2IW-Joint-Text-to-Image-Watermark-Generation" class="headerlink" title="T2IW: Joint Text to Image &amp; Watermark Generation"></a>T2IW: Joint Text to Image &amp; Watermark Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03815">http://arxiv.org/abs/2309.03815</a></li>
<li>repo_url: None</li>
<li>paper_authors: An-An Liu, Guokai Zhang, Yuting Su, Ning Xu, Yongdong Zhang, Lanjun Wang</li>
<li>for: 本研究旨在提出一种新的文本条件下的图像生成模型，以提高图像生成的真实性。</li>
<li>methods: 本研究使用了一种新的文本和水印（T2IW）任务，以确保图像质量受到最小影响，同时保证水印信息的可读性。</li>
<li>results: 实验结果表明，本方法可以在不同的后处理攻击下保持水印信息的透明度和可靠性，同时图像质量受到最小影响。<details>
<summary>Abstract</summary>
Recent developments in text-conditioned image generative models have revolutionized the production of realistic results. Unfortunately, this has also led to an increase in privacy violations and the spread of false information, which requires the need for traceability, privacy protection, and other security measures. However, existing text-to-image paradigms lack the technical capabilities to link traceable messages with image generation. In this study, we introduce a novel task for the joint generation of text to image and watermark (T2IW). This T2IW scheme ensures minimal damage to image quality when generating a compound image by forcing the semantic feature and the watermark signal to be compatible in pixels. Additionally, by utilizing principles from Shannon information theory and non-cooperative game theory, we are able to separate the revealed image and the revealed watermark from the compound image. Furthermore, we strengthen the watermark robustness of our approach by subjecting the compound image to various post-processing attacks, with minimal pixel distortion observed in the revealed watermark. Extensive experiments have demonstrated remarkable achievements in image quality, watermark invisibility, and watermark robustness, supported by our proposed set of evaluation metrics.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:近期的文本受限画像生成模型的发展，已经对生成真实的结果产生了革命性的变革。然而，这也导致了隐私侵犯和虚假信息的扩散，需要跟踪性、隐私保护和其他安全措施。然而，现有的文本到图像的观念没有技术能力将可追溯的消息与图像生成连接起来。在本研究中，我们提出了一种新的文本到图像和水印（T2IW）任务，以保证在生成复杂图像时，semantic feature和水印信号在像素级别上具有可 compatibles性。此外，我们利用了信息理论和非合作游戏理论，将复杂图像中的 revelaed image 和 revelaed watermark 分离开来。此外，我们通过对复杂图像进行多种后处理攻击，使得 revelaed watermark 中的像素扰动很小。广泛的实验结果表明，我们的方法在图像质量、水印隐身和水印Robustness 等方面具有杰出的成果，支持我们提出的评价指标集。
</details></li>
</ul>
<hr>
<h2 id="Label-efficient-Contrastive-Learning-based-model-for-nuclei-detection-and-classification-in-3D-Cardiovascular-Immunofluorescent-Images"><a href="#Label-efficient-Contrastive-Learning-based-model-for-nuclei-detection-and-classification-in-3D-Cardiovascular-Immunofluorescent-Images" class="headerlink" title="Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images"></a>Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03744">http://arxiv.org/abs/2309.03744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazanin Moradinasab, Rebecca A. Deaton, Laura S. Shankman, Gary K. Owens, Donald E. Brown<br>for:这个论文旨在提出一个 Label-efficient Contrastive learning-based (LECL) 模型，用于检测和类别各种类型的胞况在 3D 免疫染色图像中。methods:我们提出了一个 Extended Maximum Intensity Projection (EMIP) 方法来解决将多个层级萤幕转换为 2D 图像所带来的问题，以及一个 Supervised Contrastive Learning (SCL) 方法来进行弱监督学习设定。results:我们在心血管 dataset 上进行了实验，结果显示我们的提案的框架具有效果和效率地检测和类别各种类型的胞况在 3D 免疫染色图像中。<details>
<summary>Abstract</summary>
Recently, deep learning-based methods achieved promising performance in nuclei detection and classification applications. However, training deep learning-based methods requires a large amount of pixel-wise annotated data, which is time-consuming and labor-intensive, especially in 3D images. An alternative approach is to adapt weak-annotation methods, such as labeling each nucleus with a point, but this method does not extend from 2D histopathology images (for which it was originally developed) to 3D immunofluorescent images. The reason is that 3D images contain multiple channels (z-axis) for nuclei and different markers separately, which makes training using point annotations difficult. To address this challenge, we propose the Label-efficient Contrastive learning-based (LECL) model to detect and classify various types of nuclei in 3D immunofluorescent images. Previous methods use Maximum Intensity Projection (MIP) to convert immunofluorescent images with multiple slices to 2D images, which can cause signals from different z-stacks to falsely appear associated with each other. To overcome this, we devised an Extended Maximum Intensity Projection (EMIP) approach that addresses issues using MIP. Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for weakly supervised settings. We conducted experiments on cardiovascular datasets and found that our proposed framework is effective and efficient in detecting and classifying various types of nuclei in 3D immunofluorescent images.
</details>
<details>
<summary>摘要</summary>
Previous methods use Maximum Intensity Projection (MIP) to convert immunofluorescent images with multiple slices to 2D images, which can cause signals from different z-stacks to falsely appear associated with each other. To overcome this, we devised an Extended Maximum Intensity Projection (EMIP) approach that addresses issues using MIP. Furthermore, we performed a Supervised Contrastive Learning (SCL) approach for weakly supervised settings. We conducted experiments on cardiovascular datasets and found that our proposed framework is effective and efficient in detecting and classifying various types of nuclei in 3D immunofluorescent images.Translation note:* "nuclei" is translated as "核体" (hépán) in Simplified Chinese.* "deep learning-based methods" is translated as "深度学习方法" (shēngrán xuéxí fāngchéng) in Simplified Chinese.* "pixel-wise annotated data" is translated as "像素级标注数据" (xiàngxī jí biāo xiǎngxī) in Simplified Chinese.* "2D histopathology images" is translated as "2D histopathology图像" (2D histopathology túxiàng) in Simplified Chinese.* "3D immunofluorescent images" is translated as "3D免疫染色图像" (3D mǐngyì zhèngsè túxiàng) in Simplified Chinese.* "z-axis" is translated as "z轴" (z jía) in Simplified Chinese.* "Maximum Intensity Projection" is translated as "最大强度投影" (zuìdà qiángdàng tóuè) in Simplified Chinese.* "Extended Maximum Intensity Projection" is translated as "扩展最大强度投影" (kuòxiān zuìdà qiángdàng tóuè) in Simplified Chinese.* "Supervised Contrastive Learning" is translated as "有监督的对比学习" (yǒu jiāndū de duìbǐ xuéxí) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="MS-UNet-v2-Adaptive-Denoising-Method-and-Training-Strategy-for-Medical-Image-Segmentation-with-Small-Training-Data"><a href="#MS-UNet-v2-Adaptive-Denoising-Method-and-Training-Strategy-for-Medical-Image-Segmentation-with-Small-Training-Data" class="headerlink" title="MS-UNet-v2: Adaptive Denoising Method and Training Strategy for Medical Image Segmentation with Small Training Data"></a>MS-UNet-v2: Adaptive Denoising Method and Training Strategy for Medical Image Segmentation with Small Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03686">http://arxiv.org/abs/2309.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Chen, Yufei Han, Pin Xu, Yanyi Li, Kuan Li, Jianping Yin</li>
<li>for: 这个研究是为了提高医疗影像分类 tasks 的表现，并且解决单层 U-Net 构造不够强大的问题，以及当疗病数据量小时的问题。</li>
<li>methods: 我们提出了一个名为 MS-UNet 的新型 U-Net 模型，它使用了多层嵌入式 decoder 结构，具有更好的Semantic feature mapping 能力，从而帮助网络学习更多详细的特征。此外，我们还提出了一个新的边缘损失和一个可插入的精细化 Denoising 模组，这些模组可以对 MS-UNet 进行改进，并且可以个别应用于其他模型中。</li>
<li>results: 实验结果显示，MS-UNet 可以对医疗影像分类 tasks 进行更好的表现，特别是当疗病数据量小时，并且可以对网络进行更高效的特征学习。此外，我们提出的边缘损失和 Denoising 模组可以对 MS-UNet 进行明显改进。<details>
<summary>Abstract</summary>
Models based on U-like structures have improved the performance of medical image segmentation. However, the single-layer decoder structure of U-Net is too "thin" to exploit enough information, resulting in large semantic differences between the encoder and decoder parts. Things get worse if the number of training sets of data is not sufficiently large, which is common in medical image processing tasks where annotated data are more difficult to obtain than other tasks. Based on this observation, we propose a novel U-Net model named MS-UNet for the medical image segmentation task in this study. Instead of the single-layer U-Net decoder structure used in Swin-UNet and TransUnet, we specifically design a multi-scale nested decoder based on the Swin Transformer for U-Net. The proposed multi-scale nested decoder structure allows the feature mapping between the decoder and encoder to be semantically closer, thus enabling the network to learn more detailed features. In addition, we propose a novel edge loss and a plug-and-play fine-tuning Denoising module, which not only effectively improves the segmentation performance of MS-UNet, but could also be applied to other models individually. Experimental results show that MS-UNet could effectively improve the network performance with more efficient feature learning capability and exhibit more advanced performance, especially in the extreme case with a small amount of training data, and the proposed Edge loss and Denoising module could significantly enhance the segmentation performance of MS-UNet.
</details>
<details>
<summary>摘要</summary>
模型基于U字结构已经提高医学影像分割的性能。然而，单层decoder结构的U字网是“薄”到足够利用信息，导致encoder和decoder部分之间的semantic diferencia较大，这会在医学影像处理任务中，where annotated data更加困难获得，使得问题更加严重。为了解决这个问题，我们在本研究中提出了一种名为MS-UNet的新的U字网模型。而不是Swim Transformer和TransUnet中使用的单层U字网decoder结构，我们专门设计了一种基于Swin Transformer的多层嵌套decoder结构。这种多层嵌套decoder结构使得feature mapping междуdecoder和encoder更加接近，因此使得网络能够学习更多的细节特征。此外，我们还提出了一种新的边缘损失和可重复使用的精度调整Denosing模块，这些模块不仅可以有效地提高MS-UNet的分割性能，还可以应用于其他模型。实验结果表明，MS-UNet可以有效地提高网络性能，并且在小量训练数据情况下表现更加出色，而提出的边缘损失和Denosing模块也可以显著提高MS-UNet的分割性能。
</details></li>
</ul>
<hr>
<h2 id="Anatomy-informed-Data-Augmentation-for-Enhanced-Prostate-Cancer-Detection"><a href="#Anatomy-informed-Data-Augmentation-for-Enhanced-Prostate-Cancer-Detection" class="headerlink" title="Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection"></a>Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03652">http://arxiv.org/abs/2309.03652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mic-dkfz/anatomy_informed_da">https://github.com/mic-dkfz/anatomy_informed_da</a></li>
<li>paper_authors: Balint Kovacs, Nils Netzer, Michael Baumgartner, Carolin Eith, Dimitrios Bounias, Clara Meinzer, Paul F. Jaeger, Kevin S. Zhang, Ralf Floca, Adrian Schrader, Fabian Isensee, Regula Gnirs, Magdalena Goertz, Viktoria Schuetz, Albrecht Stenzinger, Markus Hohenfellner, Heinz-Peter Schlemmer, Ivo Wolf, David Bonekamp, Klaus H. Maier-Hein</li>
<li>for: 这篇论文主要针对医疗影像分析中的肿瘤检测，特别是肺癌检测。</li>
<li>methods: 这篇论文提出了一种新的生物学参考变数，它利用邻近器官的信息来模拟典型的生物Physiological deformations of the prostate, and generates unique lesion shapes without altering their label.</li>
<li>results: 这篇论文透过实验证明了这种增强器的有效性，并且显示了它可以轻松地整合到常用的增强框架中。<details>
<summary>Abstract</summary>
Data augmentation (DA) is a key factor in medical image analysis, such as in prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art computer-aided diagnosis systems still rely on simplistic spatial transformations to preserve the pathological label post transformation. However, such augmentations do not substantially increase the organ as well as tumor shape variability in the training set, limiting the model's ability to generalize to unseen cases with more diverse localized soft-tissue deformations. We propose a new anatomy-informed transformation that leverages information from adjacent organs to simulate typical physiological deformations of the prostate and generates unique lesion shapes without altering their label. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. We demonstrate the effectiveness of our augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a state-of-the-art method for PCa detection with different augmentation settings.
</details>
<details>
<summary>摘要</summary>
增强数据 (DA) 是医学图像分析中关键因素，例如肠癌检测在核磁共振图像中。现有的计算机辅助诊断系统仍然仅使用简单的空间变换来保持疾病标签后变换。然而，这些扩展不会显著增加器官以及肿瘤形态多样性在训练集中，限制模型对未经见的案例中更多的地方软组织弯曲的适应能力。我们提议一种新的生物学信息指导的变换，利用邻近器官信息来模拟Typical的生理弯曲，生成唯一的癌症形态，无需改变其标签。由于其轻量级计算需求，它可以轻松地integrated into common DA frameworks。我们在774个采样中证明了我们的扩展的效果，通过评估一种state-of-the-art方法 для PCa检测不同的扩展设置。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-3D-Object-Localization-from-Single-Calibrated-Images-A-Study-of-Basketballs"><a href="#Context-Aware-3D-Object-Localization-from-Single-Calibrated-Images-A-Study-of-Basketballs" class="headerlink" title="Context-Aware 3D Object Localization from Single Calibrated Images: A Study of Basketballs"></a>Context-Aware 3D Object Localization from Single Calibrated Images: A Study of Basketballs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03640">http://arxiv.org/abs/2309.03640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabriel-vanzandycke/deepsport">https://github.com/gabriel-vanzandycke/deepsport</a></li>
<li>paper_authors: Marcello Davide Caio, Gabriel Van Zandycke, Christophe De Vleeschouwer</li>
<li>for: 这个论文的目的是提出一种基于单个滤波图像的三维篮球定位方法。</li>
<li>methods: 该方法使用图像自身和物体位置作为输入，通过计算物体 projection onto the ground plane within the image 来预测物体的高度在图像空间中。然后，通过利用知道的投影矩阵，重建物体的3D坐标。</li>
<li>results: 对于DeepSport dataset的实验，该方法表现出了明显的准确性提升，比前方工作更加有效。这些结果开启了更好的篮球跟踪和理解的可能性，推动计算机视觉在多个领域的进步。<details>
<summary>Abstract</summary>
Accurately localizing objects in three dimensions (3D) is crucial for various computer vision applications, such as robotics, autonomous driving, and augmented reality. This task finds another important application in sports analytics and, in this work, we present a novel method for 3D basketball localization from a single calibrated image. Our approach predicts the object's height in pixels in image space by estimating its projection onto the ground plane within the image, leveraging the image itself and the object's location as inputs. The 3D coordinates of the ball are then reconstructed by exploiting the known projection matrix. Extensive experiments on the public DeepSport dataset, which provides ground truth annotations for 3D ball location alongside camera calibration information for each image, demonstrate the effectiveness of our method, offering substantial accuracy improvements compared to recent work. Our work opens up new possibilities for enhanced ball tracking and understanding, advancing computer vision in diverse domains. The source code of this work is made publicly available at \url{https://github.com/gabriel-vanzandycke/deepsport}.
</details>
<details>
<summary>摘要</summary>
“三维空间中的物件精确位置化（3D）是许多计算机视觉应用中的关键，如 робо械、自动驾驶和增强现实。在这个工作中，我们提出了一种新的方法，用于从单一测量过的图像中精确地推断篮球的3D位置。我们的方法利用图像中的物件位置和图像自身作为输入，预测物件的高度在图像空间中的像素数据，并利用知道的投影矩阵从图像中重建3D坐标。我们在公共的DeepSport dataset上进行了广泛的实验，该dataset提供了这些图像的摄取条件和摄像机协调信息，以及3D篮球位置的真实标注。我们的方法与最近的工作相比，具有了优秀的准确性。我们的工作开启了新的可能性，将计算机视觉应用于多元领域进行进一步发展。我们的源代码可以在 \url{https://github.com/gabriel-vanzandycke/deepsport} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-encoding-of-BOLD-fMRI-time-series-for-categorizing-static-images-across-visual-datasets-A-pilot-study-on-human-vision"><a href="#Spatial-encoding-of-BOLD-fMRI-time-series-for-categorizing-static-images-across-visual-datasets-A-pilot-study-on-human-vision" class="headerlink" title="Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision"></a>Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03590">http://arxiv.org/abs/2309.03590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vamshi K. Kancharala, Debanjali Bhattacharya, Neelam Sinha</li>
<li>for: 这项研究使用功能磁共振成像(fMRI)技术来研究大脑的功能，具体来说是通过检测血液含氧量变化来探测大脑的活动。</li>
<li>methods: 这项研究使用了BOLD5000数据集，这是一个公共可用的fMRI扫描数据集，包含了5254个多种类别的图像，来研究不同类别的图像如何影响大脑的活动。为了更好地理解视觉，研究人员使用了古拉姆angular field（GAF）和马可夫过渡场（MTF）来获得2D BOLDTS，然后使用了分类NN来进行分类。</li>
<li>results: 研究人员发现，并行的CNN模型在多类分类任务中表现出色，比其他网络模型提高了7%的性能。这个结果表明了并行的CNN模型在研究大脑如何处理不同类型的图像时的优势。<details>
<summary>Abstract</summary>
Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. In this study, complexity specific image categorization across different visual datasets is performed using fMRI time series (TS) to understand differences in neuronal activities related to vision. Publicly available BOLD5000 dataset is used for this purpose, containing fMRI scans while viewing 5254 images of diverse categories, drawn from three standard computer vision datasets: COCO, ImageNet and SUN. To understand vision, it is important to study how brain functions while looking at different images. To achieve this, spatial encoding of fMRI BOLD TS has been performed that uses classical Gramian Angular Field (GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing images of COCO, Imagenet and SUN. For classification, individual GAF and MTF features are fed into regular CNN. Subsequently, parallel CNN model is employed that uses combined 2D features for classifying images across COCO, Imagenet and SUN. The result of 2D CNN models is also compared with 1D LSTM and Bi-LSTM that utilizes raw fMRI BOLD signal for classification. It is seen that parallel CNN model outperforms other network models with an improvement of 7% for multi-class classification. Clinical relevance- The obtained result of this analysis establishes a baseline in studying how differently human brain functions while looking at images of diverse complexities.
</details>
<details>
<summary>摘要</summary>
Functional MRI (fMRI) 广泛用于检测脑功能的变化，特别是通过检测脑活动时血液氧化程度的变化。在这个研究中，使用 fMRI 时间序列（TS）来分类不同的视觉数据集，以了解视觉相关的神经活动之间的差异。使用公共可用的 BOLD5000 数据集，包含 fMRI 扫描视看 5254 个多种类别的图像，这些图像来自三个标准计算机视觉数据集：COCO、ImageNet 和 SUN。为了理解视觉，需要研究脑如何在不同的图像上工作。为此，使用类别 Gramian Angular Field (GAF) 和 Markov Transition Field (MTF) 来获得 2D BOLD TS，表示 COCO、ImageNet 和 SUN 三个数据集中的图像。然后，使用单独的 GAF 和 MTF 特征进行分类，并使用组合的 2D 特征来分类图像。结果显示，并行 CNN 模型在多类分类中表现出色，提高了7%的性能。临床 relevance - 这个分析结果建立了研究人类脑如何在不同复杂度的图像上工作的基线。
</details></li>
</ul>
<hr>
<h2 id="Secure-Control-of-Networked-Inverted-Pendulum-Visual-Servo-System-with-Adverse-Effects-of-Image-Computation-Extended-Version"><a href="#Secure-Control-of-Networked-Inverted-Pendulum-Visual-Servo-System-with-Adverse-Effects-of-Image-Computation-Extended-Version" class="headerlink" title="Secure Control of Networked Inverted Pendulum Visual Servo System with Adverse Effects of Image Computation (Extended Version)"></a>Secure Control of Networked Inverted Pendulum Visual Servo System with Adverse Effects of Image Computation (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03556">http://arxiv.org/abs/2309.03556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dajun Du, Changda Zhang, Qianjiang Lu, Minrui Fei, Huiyu Zhou</li>
<li>for: 本研究探讨了在通信网络上传输视频信息时可能会遇到的图像攻击问题，以及这些攻击对系统性能的影响。</li>
<li>methods: 本研究提出了一种基于快速缩放选择性图像加密（F2SIE）算法的新的网络倾斜镜视 servo系统（NIPVSS），该系统不仅可以保证实时性，还可以提高安全性。</li>
<li>results: 实验结果表明，提出的新的NIPVSS方法可以有效地防止图像攻击，并且可以保证系统的稳定性。<details>
<summary>Abstract</summary>
When visual image information is transmitted via communication networks, it easily suffers from image attacks, leading to system performance degradation or even crash. This paper investigates secure control of networked inverted pendulum visual servo system (NIPVSS) with adverse effects of image computation. Firstly, the image security limitation of the traditional NIPVSS is revealed, where its stability will be destroyed by eavesdropping-based image attacks. Then, a new NIPVSS with the fast scaled-selective image encryption (F2SIE) algorithm is proposed, which not only meets the real-time requirement by reducing the computational complexity, but also improve the security by reducing the probability of valuable information being compromised by eavesdropping-based image attacks. Secondly, adverse effects of the F2SIE algorithm and image attacks are analysed, which will produce extra computational delay and errors. Then, a closed-loop uncertain time-delay model of the new NIPVSS is established, and a robust controller is designed to guarantee system asymptotic stability. Finally, experimental results of the new NIPVSS demonstrate the feasibility and effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
当视觉图像信息通过通信网络传输时，容易受到图像攻击，导致系统性能下降或even crash。本文研究了安全控制的网络 inverted pendulum visual servo系统（NIPVSS），并对图像计算所造成的不良影响进行分析。首先，传统NIPVSS的图像安全限制被揭示，其稳定性将被侵犯者发送的图像攻击所 destrucción。然后，一种新的NIPVSS，使用快速缩放选择性图像加密算法（F2SIE），不仅可以实现实时要求，还可以提高安全性，减少侵犯者通过图像攻击获得有价值信息的概率。其次，F2SIE算法和图像攻击的副作用被分析，它们会生成额外的计算延迟和错误。然后，一个closed-loop不确定时延模型的新NIPVSS被建立，并设计了一个Robust控制器，以保证系统的极限稳定性。最后，新NIPVSS的实验结果证明了提案的方法的可行性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Deep-Learning-based-Melanoma-Classification-using-Immunohistochemistry-and-Routine-Histology-A-Three-Center-Study"><a href="#Evaluating-Deep-Learning-based-Melanoma-Classification-using-Immunohistochemistry-and-Routine-Histology-A-Three-Center-Study" class="headerlink" title="Evaluating Deep Learning-based Melanoma Classification using Immunohistochemistry and Routine Histology: A Three Center Study"></a>Evaluating Deep Learning-based Melanoma Classification using Immunohistochemistry and Routine Histology: A Three Center Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03494">http://arxiv.org/abs/2309.03494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Wies, Lucas Schneider, Sarah Haggenmueller, Tabea-Clara Bucher, Sarah Hobelsberger, Markus V. Heppt, Gerardo Ferrara, Eva I. Krieghoff-Henning, Titus J. Brinker</li>
<li>for: 用于提高诊断皮肤癌的准确性</li>
<li>methods: 使用 Deep Learning (DL) 支持系统自动检查组织结构和细胞组成</li>
<li>results: DL 基于 MelanA 的助手系统与标准 H&amp;E 染色 Slides 的性能相同，可能通过多种染色类型的协同分析提高病理医生的诊断效果。<details>
<summary>Abstract</summary>
Pathologists routinely use immunohistochemical (IHC)-stained tissue slides against MelanA in addition to hematoxylin and eosin (H&E)-stained slides to improve their accuracy in diagnosing melanomas. The use of diagnostic Deep Learning (DL)-based support systems for automated examination of tissue morphology and cellular composition has been well studied in standard H&E-stained tissue slides. In contrast, there are few studies that analyze IHC slides using DL. Therefore, we investigated the separate and joint performance of ResNets trained on MelanA and corresponding H&E-stained slides. The MelanA classifier achieved an area under receiver operating characteristics curve (AUROC) of 0.82 and 0.74 on out of distribution (OOD)-datasets, similar to the H&E-based benchmark classification of 0.81 and 0.75, respectively. A combined classifier using MelanA and H&E achieved AUROCs of 0.85 and 0.81 on the OOD datasets. DL MelanA-based assistance systems show the same performance as the benchmark H&E classification and may be improved by multi stain classification to assist pathologists in their clinical routine.
</details>
<details>
<summary>摘要</summary>
PATHOLOGISTS 常用免疫 histochemical（IHC）染色的组织标本与 MelanA 标本进行诊断，以提高诊断皮肤癌的精度。使用基于 Deep Learning（DL）的诊断支持系统来自动检查组织结构和细胞成分已经得到了广泛的研究，但是对 IHC 标本的分析却有少量的研究。因此，我们调查了使用 ResNet 在 MelanA 和相应的 H&E 标本上训练的表现。MelanA 分类器在 OOD 数据集上的地区下Receiver Operating Characteristics Curve（AUROC）为 0.82 和 0.74，与 H&E 基准分类的 0.81 和 0.75 相似。具有 MelanA 和 H&E 的共同分类器在 OOD 数据集上的 AUROC 为 0.85 和 0.81。DL MelanA 基础的辅助系统显示和 H&E 基准分类的相同表现，并且可能会通过多标本分类来帮助病理学家在临床 Routine 中。
</details></li>
</ul>
<hr>
<h2 id="SAM3D-Segment-Anything-Model-in-Volumetric-Medical-Images"><a href="#SAM3D-Segment-Anything-Model-in-Volumetric-Medical-Images" class="headerlink" title="SAM3D: Segment Anything Model in Volumetric Medical Images"></a>SAM3D: Segment Anything Model in Volumetric Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03493">http://arxiv.org/abs/2309.03493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nhat-Tan Bui, Dinh-Hieu Hoang, Minh-Triet Tran, Ngan Le</li>
<li>for: 本研究旨在提出一种基于深度学习的自动图像分割方法，用于医疗图像分析中的图像分割任务。</li>
<li>methods: 该方法基于Segment Anything Model（SAM），并使用SAM encoder提取输入图像的有用表示。与其他现有的SAM-基于三维图像分割方法不同，该方法不需要将输入图像分割成一系列的2D slice，而是直接处理整个3D图像。</li>
<li>results: 经过广泛的实验，该方法在多个医疗图像Dataset上达到了与其他现有方法相当的竞争性result，而且在参数量方面更是有效率。<details>
<summary>Abstract</summary>
Image segmentation is a critical task in medical image analysis, providing valuable information that helps to make an accurate diagnosis. In recent years, deep learning-based automatic image segmentation methods have achieved outstanding results in medical images. In this paper, inspired by the Segment Anything Model (SAM), a foundation model that has received much attention for its impressive accuracy and powerful generalization ability in 2D still image segmentation, we propose a SAM3D that targets at 3D volumetric medical images and utilizes the pre-trained features from the SAM encoder to capture meaningful representations of input images. Different from other existing SAM-based volumetric segmentation methods that perform the segmentation by dividing the volume into a set of 2D slices, our model takes the whole 3D volume image as input and processes it simply and effectively that avoids training a significant number of parameters. Extensive experiments are conducted on multiple medical image datasets to demonstrate that our network attains competitive results compared with other state-of-the-art methods in 3D medical segmentation tasks while being significantly efficient in terms of parameters.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗图像分析中的关键任务，它提供了诊断的有价值信息。在最近的几年中，基于深度学习的自动图像分割方法在医疗图像中取得了出色的结果。在这篇论文中，我们提议了一个基于Segment Anything Model（SAM）的3D医疗图像分割模型（SAM3D），该模型利用SAMEncoder预训练的特征来捕捉输入图像的有意义表示。与其他现有的SAM基于volumetric segmentation方法不同，我们的模型不需要将体volume分割成一系列的2D slice，而是直接处理整个3D图像，从而避免了训练大量参数。我们在多个医疗图像数据集上进行了广泛的实验，以示我们的网络与其他状态之前的方法在3D医疗图像分 segmentation任务中具有竞争力，同时在参数上具有显著的效率优势。
</details></li>
</ul>
<hr>
<h2 id="TSI-Net-A-Timing-Sequence-Image-Segmentation-Network-for-Intracranial-Artery-Segmentation-in-Digital-Subtraction-Angiography"><a href="#TSI-Net-A-Timing-Sequence-Image-Segmentation-Network-for-Intracranial-Artery-Segmentation-in-Digital-Subtraction-Angiography" class="headerlink" title="TSI-Net: A Timing Sequence Image Segmentation Network for Intracranial Artery Segmentation in Digital Subtraction Angiography"></a>TSI-Net: A Timing Sequence Image Segmentation Network for Intracranial Artery Segmentation in Digital Subtraction Angiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03477">http://arxiv.org/abs/2309.03477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lemeng Wang, Wentao Liu, Weijin Xu, Haoyuan Li, Huihua Yang, Feng Gao<br>for:The paper is written for automating the segmentation of intracranial arteries in digital subtraction angiography (DSA) sequences, which is an important step in the diagnosis and treatment of cerebrovascular diseases.methods:The proposed method, called TSI-Net, uses a bi-directional ConvGRU module (BCM) in the encoder to input variable-length DSA sequences and segment them into 2D images, while also incorporating a sensitive detail branch (SDB) to supervise fine vessels.results:The method achieved a Sen evaluation metric of 0.797 on the DSA sequence dataset DIAS, which is a 3% improvement compared to other state-of-the-art methods.<details>
<summary>Abstract</summary>
Cerebrovascular disease is one of the major diseases facing the world today. Automatic segmentation of intracranial artery (IA) in digital subtraction angiography (DSA) sequences is an important step in the diagnosis of vascular related diseases and in guiding neurointerventional procedures. While, a single image can only show part of the IA within the contrast medium according to the imaging principle of DSA technology. Therefore, 2D DSA segmentation methods are unable to capture the complete IA information and treatment of cerebrovascular diseases. We propose A timing sequence image segmentation network with U-shape, called TSI-Net, which incorporates a bi-directional ConvGRU module (BCM) in the encoder. The network incorporates a bi-directional ConvGRU module (BCM) in the encoder, which can input variable-length DSA sequences, retain past and future information, segment them into 2D images. In addition, we introduce a sensitive detail branch (SDB) at the end for supervising fine vessels. Experimented on the DSA sequence dataset DIAS, the method performs significantly better than state-of-the-art networks in recent years. In particular, it achieves a Sen evaluation metric of 0.797, which is a 3% improvement compared to other methods.
</details>
<details>
<summary>摘要</summary>
脑血管疾病是当今世界面临的一大健康问题。自动将脑动脉（IA）分割成数字抵消成像（DSA）序列中的一个重要步骤，对于脑血管相关疾病的诊断和神经内部进行操作是非常重要。然而，单个图像只能显示脑动脉中的一部分，根据DSA技术的假设，因此2D DSA分割方法无法捕捉完整的IA信息。我们提出了一种名为TSI-Net的时序序列图像分割网络，该网络包含一个双向ConvGRU模块（BCM）在编码器中。该网络可以输入变长的DSA序列，同时保留过去和未来信息，将其分割成2D图像。此外，我们还引入了敏感细节分支（SDB），用于监督细血管。在DIAS数据集上进行实验，该方法与过去几年最佳方法相比，表现出了显著的改善，具体来说，它的Sen评价指标达0.797，比其他方法提高3%。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Assessment-of-360-circ-Images-Based-on-Generative-Scanpath-Representation"><a href="#Perceptual-Quality-Assessment-of-360-circ-Images-Based-on-Generative-Scanpath-Representation" class="headerlink" title="Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation"></a>Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03472">http://arxiv.org/abs/2309.03472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangjiesui/gsr">https://github.com/xiangjiesui/gsr</a></li>
<li>paper_authors: Xiangjie Sui, Hanwei Zhu, Xuelin Liu, Yuming Fang, Shiqi Wang, Zhou Wang</li>
<li>for: 这篇论文的目的是提出一种基于生成扫描路径表示（GSR）的高效的全息图像质量评估方法，以解决现有的全息图像质量评估模型忽略了用户观看行为的问题。</li>
<li>methods: 该方法使用了生成扫描路径的技术，通过定义观看条件（包括开始观看点和探索时间），生成了一系列的扫描路径，并将这些扫描路径转换为全息图像的唯一的GSR。然后，通过学习GSR的质量地图，实现高效的全息图像质量评估。</li>
<li>results: 实验结果表明，提出的方法可以快速地和高度一致地评估全息图像的质量，特别是在用户观看条件下存在局部扭曲的情况下。code将会在<a target="_blank" rel="noopener" href="https://github.com/xiangjieSui/GSR%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xiangjieSui/GSR上发布。</a><details>
<summary>Abstract</summary>
Despite substantial efforts dedicated to the design of heuristic models for omnidirectional (i.e., 360$^\circ$) image quality assessment (OIQA), a conspicuous gap remains due to the lack of consideration for the diversity of viewing behaviors that leads to the varying perceptual quality of 360$^\circ$ images. Two critical aspects underline this oversight: the neglect of viewing conditions that significantly sway user gaze patterns and the overreliance on a single viewport sequence from the 360$^\circ$ image for quality inference. To address these issues, we introduce a unique generative scanpath representation (GSR) for effective quality inference of 360$^\circ$ images, which aggregates varied perceptual experiences of multi-hypothesis users under a predefined viewing condition. More specifically, given a viewing condition characterized by the starting point of viewing and exploration time, a set of scanpaths consisting of dynamic visual fixations can be produced using an apt scanpath generator. Following this vein, we use the scanpaths to convert the 360$^\circ$ image into the unique GSR, which provides a global overview of gazed-focused contents derived from scanpaths. As such, the quality inference of the 360$^\circ$ image is swiftly transformed to that of GSR. We then propose an efficient OIQA computational framework by learning the quality maps of GSR. Comprehensive experimental results validate that the predictions of the proposed framework are highly consistent with human perception in the spatiotemporal domain, especially in the challenging context of locally distorted 360$^\circ$ images under varied viewing conditions. The code will be released at https://github.com/xiangjieSui/GSR
</details>
<details>
<summary>摘要</summary>
尽管对权重图像质量评估（OIQA）的设计做出了大量努力，但是存在一个显著的漏洞，即因为忽略了观看行为多样性，导致360度图像的质量强度不均匀。两个关键因素把握这一点：忽略了观看条件的影响，以及对360度图像的质量做出判断只是基于单个视窗序列。为了解决这些问题，我们介绍了一种新的生成扫描路径表示（GSR），用于有效地评估360度图像的质量，该表示方法通过综合考虑多种假设用户的视觉经验来捕捉多样的观看行为。更具体地说，给定一个观看条件，包括开始观看和探索时间，我们可以使用适合的扫描路径生成器生成一系列的扫描路径，这些扫描路径包含动态的视觉固定点。然后，我们使用这些扫描路径将360度图像转换成唯一的GSR，该GSR提供了一个全面的观看关注点，即观看关注点的总和。因此，我们可以快速地将360度图像的质量评估转换成GSR的质量评估。我们then提出了一种高效的OIQA计算框架，通过学习GSR的质量地图来实现。实验结果表明，我们的提议的框架预测结果与人类视觉在空间时间域的吻合程度非常高，特别是在360度图像下的局部扭曲视图下的多种观看条件下。代码将在https://github.com/xiangjieSui/GSR上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/eess.IV_2023_09_07/" data-id="clmjn91qx00hp0j88dsn93ali" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/07/cs.LG_2023_09_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-07
        
      </div>
    </a>
  
  
    <a href="/2023/09/06/cs.SD_2023_09_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
