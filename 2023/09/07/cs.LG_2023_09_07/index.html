
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07080 repo_url: None paper_authors: Abdolmahdi Bagheri, Mohammad Pa">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-07">
<meta property="og:url" content="https://nullscc.github.io/2023/09/07/cs.LG_2023_09_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07080 repo_url: None paper_authors: Abdolmahdi Bagheri, Mohammad Pa">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-07T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.585Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/07/cs.LG_2023_09_07/" class="article-date">
  <time datetime="2023-09-07T10:00:00.000Z" itemprop="datePublished">2023-09-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bayesian-Dynamic-DAG-Learning-Application-in-Discovering-Dynamic-Effective-Connectome-of-Brain"><a href="#Bayesian-Dynamic-DAG-Learning-Application-in-Discovering-Dynamic-Effective-Connectome-of-Brain" class="headerlink" title="Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain"></a>Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07080">http://arxiv.org/abs/2309.07080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdolmahdi Bagheri, Mohammad Pasande, Kevin Bello, Alireza Akhondi-Asl, Babak Nadjar Araabi</li>
<li>for: 提取 Dynamic Effective Connectome (DEC) 可以揭示脑中复杂的机制。</li>
<li>methods: 使用 Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization（BDyMA）方法，可以解决高维动态DAG发现和 fMRI 数据质量低的两大挑战。</li>
<li>results: 比较baseline和现有方法，BDyMA方法可以更准确地检测高维网络，并且可以 incorporate  prior knowledge 进行动态 causal发现，提高结果的准确性。<details>
<summary>Abstract</summary>
Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score function of the BDyMA method allows the incorporation of prior knowledge into the process of dynamic causal discovery which further enhances the accuracy of results. Comprehensive simulations on synthetic data and experiments on Human Connectome Project (HCP) data demonstrate that our method can handle both of the two main challenges, yielding more accurate and reliable DEC compared to state-of-the-art and baseline methods. Additionally, we investigate the trustworthiness of DTI data as prior knowledge for DEC discovery and show the improvements in DEC discovery when the DTI data is incorporated into the process.
</details>
<details>
<summary>摘要</summary>
理解大脑的复杂机制可以通过提取动态有效连ome（DEC）来解开。最近，基于分数的导向无环图（DAG）发现方法在提取 causal 结构和推导有效连接方面表现出了显著的改进。然而，通过这些方法学习 DEC 仍面临两个主要挑战：一是高维动态 DAG 发现方法的基础不足，二是 fMRI 数据质量低下。在这篇文章中，我们介绍了 Bayesian 动态 DAG 学习方法（BDyMA），用于解决这两个挑战。BDyMA 方法可以检测 bidirected 边，并且利用不受限制的框架，以获得更高精度的结果，特别适合提取 DEC。此外，BDyMA 方法的分数函数允许在动态 causal 发现过程中 incorporate 先前知识，进一步提高结果的准确性。我们在 synthetic 数据和 HCP 数据上进行了广泛的 simulate 和实验，结果表明，我们的方法可以解决两个主要挑战，并且与现有基准方法和基准方法相比，提取 DEC 的结果更加准确和可靠。此外，我们还 investigate DTI 数据的可靠性作为 DEC 发现的先前知识，并证明在 incorporate DTI 数据到发现过程中，可以提高 DEC 的准确性。
</details></li>
</ul>
<hr>
<h2 id="SRN-SZ-Deep-Leaning-Based-Scientific-Error-bounded-Lossy-Compression-with-Super-resolution-Neural-Networks"><a href="#SRN-SZ-Deep-Leaning-Based-Scientific-Error-bounded-Lossy-Compression-with-Super-resolution-Neural-Networks" class="headerlink" title="SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks"></a>SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04037">http://arxiv.org/abs/2309.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyang Liu, Sheng Di, Sian Jin, Kai Zhao, Xin Liang, Zizhong Chen, Franck Cappello</li>
<li>for: 科学数据压缩（error-bound lossy compression）技术的应用，以提高现代超级计算机系统中科学数据的管理效率。</li>
<li>methods: 利用深度学习网络（super-resolution neural networks）实现数据格子扩展（hierarchical data grid expansion）方法，并使用最新的超分辨率网络HAT进行压缩。</li>
<li>results: SRN-SZ在比较多种现有压缩器的实验中，可以达到75%的压缩率提升，同时保持与PSNR相同的错误约束。<details>
<summary>Abstract</summary>
The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor leveraging the hierarchical data grid expansion paradigm implemented by super-resolution neural networks. SRN-SZ applies the most advanced super-resolution network HAT for its compression, which is free of time-costing per-data training. In experiments compared with various state-of-the-art compressors, SRN-SZ achieves up to 75% compression ratio improvements under the same error bound and up to 80% compression ratio improvements under the same PSNR than the second-best compressor.
</details>
<details>
<summary>摘要</summary>
现代超级计算系统的快速增长和大规模数据管理问题，带来了科学数据的可用性带来挑战。为维护科学数据的可用性，Error-bound lossy compression被提出和开发为科学数据的大小减少技术。不同的科学仿真数据之间，一些数据无法使用现有的Error-bounded lossy compressor进行有效压缩。人工智能的最近成功，许多研究人员在Error-bounded lossy compressor中 интеGRATE neural networks。然而，这些工作仍然受到压缩率有限和/或非常低效的困扰。为解决这些问题并提高压缩硬件数据，在这篇论文中，我们提出了SRN-SZ，这是一种基于深度学习的科学Error-bounded lossy compressor，利用层次数据格式扩展 paradigm和超分辨率神经网络（HAT）进行压缩。SRN-SZ在压缩中使用了HAT，免除了每个数据需要时间成本的训练。在对比各种现状 compressor 的实验中，SRN-SZ在同等Error bound下实现了75%的压缩率提升，并在同等PSNR下实现了80%的压缩率提升。
</details></li>
</ul>
<hr>
<h2 id="Brief-technical-note-on-linearizing-recurrent-neural-networks-RNNs-before-vs-after-the-pointwise-nonlinearity"><a href="#Brief-technical-note-on-linearizing-recurrent-neural-networks-RNNs-before-vs-after-the-pointwise-nonlinearity" class="headerlink" title="Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity"></a>Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04030">http://arxiv.org/abs/2309.04030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marino Pagan, Adrian Valente, Srdjan Ostojic, Carlos D. Brody</li>
<li>for: 这个论文研究了环境依赖性的神经网络（RNN）动态系统的特性。</li>
<li>methods: 该论文使用了两种不同的线性化方法来研究RNN动态系统：一种是基于活动（输出单元后点wise非线性）的线性化，另一种是基于活动（输入单元前点wise非线性）的线性化。</li>
<li>results: 该论文发现了这两种线性化方法之间的关系，以及它们对动态系统的影响。它还显示了在活动动态系统中存在一些Context-dependent效果，而在活动动态系统中这些效果并不明显。<details>
<summary>Abstract</summary>
Linearization of the dynamics of recurrent neural networks (RNNs) is often used to study their properties. The same RNN dynamics can be written in terms of the ``activations" (the net inputs to each unit, before its pointwise nonlinearity) or in terms of the ``activities" (the output of each unit, after its pointwise nonlinearity); the two corresponding linearizations are different from each other. This brief and informal technical note describes the relationship between the two linearizations, between the left and right eigenvectors of their dynamics matrices, and shows that some context-dependent effects are readily apparent under linearization of activity dynamics but not linearization of activation dynamics.
</details>
<details>
<summary>摘要</summary>
linearization of recurrent neural networks (RNNs) 的动态是经常用来研究其性质。这些 RNN 动态可以表示为“活动”（每个单元的输出， после其点对点非线性）或“启动”（每个单元的输入， перед其点对点非线性）；这两种对应的 linearization 是不同的。这篇简短且不正式的技术笔记描述了这两种 linearization 之间的关系，以及它们动态矩阵的左和右各 eigenvector 之间的关系，并证明了在活动动力化下有些上下文依赖的效果是不太明显的。
</details></li>
</ul>
<hr>
<h2 id="TIDE-Textual-Identity-Detection-for-Evaluating-and-Augmenting-Classification-and-Language-Models"><a href="#TIDE-Textual-Identity-Detection-for-Evaluating-and-Augmenting-Classification-and-Language-Models" class="headerlink" title="TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models"></a>TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04027">http://arxiv.org/abs/2309.04027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/TIDAL">https://github.com/google-research-datasets/TIDAL</a></li>
<li>paper_authors: Emmanuel Klu, Sameer Sethi</li>
<li>for: 本研究旨在提高文本分类和生成模型的公平性，尤其是在文本数据集中检测和修正不公平的数据和模型。</li>
<li>methods: 本研究使用了一个新的标识词典（TIDAL），包含15,123个标识词和其相关的感受上下文，以提高文本中的标识上下文可用性和Machine Learning公平性技术的效果。</li>
<li>results: 研究发现，使用助手注释技术可以提高人工审核过程的可靠性和速度，而我们的数据集和方法在评估和修正期间更能发现不平等现象，并生成更公平的模型。这些方法为实际场景中扩大分类和生成模型公平性提供了一个实用的路线图。<details>
<summary>Abstract</summary>
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Results show our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. Our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. These approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</details>
<details>
<summary>摘要</summary>
We create a new and comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and their associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of machine learning fairness techniques. We evaluate our approaches using human contributors, and conduct experiments focused on dataset and model debiasing. Our results show that our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. Our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. These approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-with-Tempered-Exponential-Measures"><a href="#Optimal-Transport-with-Tempered-Exponential-Measures" class="headerlink" title="Optimal Transport with Tempered Exponential Measures"></a>Optimal Transport with Tempered Exponential Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04015">http://arxiv.org/abs/2309.04015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Amid, Frank Nielsen, Richard Nock, Manfred K. Warmuth</li>
<li>for: 这个论文主要是为了解决优化运输问题中的两个主要子领域之间的冲突：一是不含规则的优化运输（”`a-la-Kantorovich”），它会导致计划非常稀疏，但算法性能不佳；另一个是带有Entropic规则的优化运输（”`a-la-Sinkhorn-Cuturi”），它可以获得近线性approximation算法，但计划会非常稠密。</li>
<li>methods: 这篇论文提出了一种渐进的扩展，即将带有渐进的凝聚排序（tempered exponential measures）应用于优化运输问题中。这种方法可以同时实现非常快的approximation算法和计划的稀疏性。</li>
<li>results: 论文的结果表明，这种渐进的扩展可以在优化运输问题中获得非常好的性能，具有非常快的approximation算法和计划的稀疏性。此外，它还可以自然地应用于不平衡优化运输问题中。<details>
<summary>Abstract</summary>
In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
</details>
<details>
<summary>摘要</summary>
在优化运输领域，有两个显著的子领域面对着：（i）无杂化优化运输，“\`a-la-Kantorovich”，它会导致非常稀疏的计划，但算法执行效率不佳；（ii）Entropic-regulated optimal transport，“\`a-la-Sinkhorn-Cuturi”，它可以得到近线性approximation算法，但计划会变得非常稠密。在这篇论文中，我们展示了一种扩展后者的方法，即使用温和的指数分布，可以达到非常便利的中间位置，具有非常快的approximation算法和控制在某些尺度上的稀疏性。此外，它自然地适应了不均衡优化运输问题的设置。
</details></li>
</ul>
<hr>
<h2 id="An-Element-wise-RSAV-Algorithm-for-Unconstrained-Optimization-Problems"><a href="#An-Element-wise-RSAV-Algorithm-for-Unconstrained-Optimization-Problems" class="headerlink" title="An Element-wise RSAV Algorithm for Unconstrained Optimization Problems"></a>An Element-wise RSAV Algorithm for Unconstrained Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04013">http://arxiv.org/abs/2309.04013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiheng Zhang, Jiahao Zhang, Jie Shen, Guang Lin</li>
<li>for: 这个论文是为了提出一种新的优化算法，以满足不条件能量散失定律并且具有改进的能量对齐性。</li>
<li>methods: 这个算法使用了元素刻relaxed scalar auxiliary variable（E-RSAV）技术，并且提供了对 convex 设置的严格证明，以及在单变量情况下的加速算法，以提高线性增长率。</li>
<li>results: 作者通过大量的数学实验 validate了他们的算法的稳定性和快速增长率。<details>
<summary>Abstract</summary>
We present a novel optimization algorithm, element-wise relaxed scalar auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation law and exhibits improved alignment between the modified and the original energy. Our algorithm features rigorous proofs of linear convergence in the convex setting. Furthermore, we present a simple accelerated algorithm that improves the linear convergence rate to super-linear in the univariate case. We also propose an adaptive version of E-RSAV with Steffensen step size. We validate the robustness and fast convergence of our algorithm through ample numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的优化算法，元素缓和Scalar助变量（E-RSAV），它满足不受条件的能量泄漏定律，并且在修改后和原始能量之间存在改善的对齐。我们的算法具有在凸设定下的严格证明的线性收敛性。此外，我们还提出了在单变量情况下使用加速器，以提高收敛率的超线性收敛。此外，我们还提出了适应E-RSAV的Steffensen步长。我们通过大量的数学实验证明了我们的算法的稳定性和快速收敛性。Here's the word-for-word translation of the text into Simplified Chinese:我们提出了一种新的优化算法，元素缓和Scalar助变量（E-RSAV），它满足不受条件的能量泄漏定律，并且在修改后和原始能量之间存在改善的对齐。我们的算法具有在凸设定下的严格证明的线性收敛性。此外，我们还提出了在单变量情况下使用加速器，以提高收敛率的超线性收敛。此外，我们还提出了适应E-RSAV的Steffensen步长。我们通过大量的数学实验证明了我们的算法的稳定性和快速收敛性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Transformer-for-Material-Segmentation"><a href="#Multimodal-Transformer-for-Material-Segmentation" class="headerlink" title="Multimodal Transformer for Material Segmentation"></a>Multimodal Transformer for Material Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04001">http://arxiv.org/abs/2309.04001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif</li>
<li>for: 这篇论文的目的是提出一种新的多modalities融合策略，以增强多modalities识别材料的性能。</li>
<li>methods: 这篇论文提出了一个名为Multi-Modal Segmentation Transformer（MMSFormer）的新模型，该模型包括一个 novel fusion strategy，用于融合不同的四种模式：RGB、Angel of Linear Polarization（AoLP）、Degree of Linear Polarization（DoLP）和Near-Infrared（NIR）。</li>
<li>results: 根据MCubeS数据集的评估，MMSFormer模型取得了52.05%的mIoU，比前一代的模型高出9.1%和10.4%在检测砾石和人类类别上。<details>
<summary>Abstract</summary>
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials. The code and pretrained models will be made available at https://github.com/csiplab/MMSFormer.
</details>
<details>
<summary>摘要</summary>
利用多Modalities的信息共同 optimize multimodal segmentation任务的性能。然而，将不同Modalities的信息有效融合仍然是一个挑战，因为每种Modalities具有不同的特点。在这篇论文中，我们提出了一种新的融合策略，可以有效地融合不同组合的四种Modalities：RGB、Angle of Linear Polarization（AoLP）、Degree of Linear Polarization（DoLP）和 Near-Infrared（NIR）。我们还提出了一种新的模型，即Multi-Modal Segmentation Transformer（MMSFormer），该模型包含了提出的融合策略，用于进行多模态材料分 segmentation。MMSFormer实现了52.05%的mIoU，超过当前的州度-OF-艺（MCubeS）数据集的现状。例如，我们的方法在检测粗砾 (+10.4%)和人 (+9.1%)类型时提供了显著的改进。归因分析表明，不同的模块在融合块中具有重要的作用，并且不同的输入Modalities可以提高不同类型材料的识别性能。代码和预训练模型将在https://github.com/csiplab/MMSFormer上公开。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Self-Supervised-Representations-to-Multi-Domain-Setups"><a href="#Adapting-Self-Supervised-Representations-to-Multi-Domain-Setups" class="headerlink" title="Adapting Self-Supervised Representations to Multi-Domain Setups"></a>Adapting Self-Supervised Representations to Multi-Domain Setups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03999">http://arxiv.org/abs/2309.03999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Sam Sharpe, Jeremy Goodsitt, Bayan Bruss, Soheil Feizi</li>
<li>for: 提高多 домен数据上自动学习模型的泛化能力</li>
<li>methods: 提出了一种通用、轻量级的领域分离模块（DDM），可以插入任何自我监督编码器，以实现多个多样化领域中的表示学习</li>
<li>results: 对多个多样化领域的数据进行预训练，DDM可以提高自我监督模型的线性探测精度（最多3.5%），并且在无法获得领域标签时，DDM可以使用稳定的聚类方法来找到pseudo领域，从而提高模型对未看过的领域的泛化能力（7.4%）。<details>
<summary>Abstract</summary>
Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.
</details>
<details>
<summary>摘要</summary>
当前最先进的自动编程方法，在各自的领域上有效，但是对未见的领域有限的泛化能力。我们发现这些模型在混合领域上训练时表现差异，使其不适用于实际世界上的多种多样化环境。因此，我们提出一个通用、轻量级的领域分离模块（DDM），可以与任何自动编程Encoder结合使用，以有效地进行多个多样化领域的表示学习，无论有多少共同类。在预训练期间，DDM通过自我指导的方式，在表示空间中强制实施分离，将领域特有的部分与领域不变的部分分离开。当领域标签不可用时，DDM使用了一种鲁棒的聚类方法来找到pseudo领域。我们显示，在DDM的预训练下，可以在多个多样化领域的测试集上显示3.5%的改进，包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins等自动编程模型。此外，DDM训练后的模型在未见领域上的泛化能力提高了7.4%。因此，DDM可以高效地适应自动编程Encoder，以提供高质量、泛化的表示，用于多种多样化的数据。
</details></li>
</ul>
<hr>
<h2 id="Creating-a-Systematic-ESG-Environmental-Social-Governance-Scoring-System-Using-Social-Network-Analysis-and-Machine-Learning-for-More-Sustainable-Company-Practices"><a href="#Creating-a-Systematic-ESG-Environmental-Social-Governance-Scoring-System-Using-Social-Network-Analysis-and-Machine-Learning-for-More-Sustainable-Company-Practices" class="headerlink" title="Creating a Systematic ESG (Environmental Social Governance) Scoring System Using Social Network Analysis and Machine Learning for More Sustainable Company Practices"></a>Creating a Systematic ESG (Environmental Social Governance) Scoring System Using Social Network Analysis and Machine Learning for More Sustainable Company Practices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05607">http://arxiv.org/abs/2309.05607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aarav Patel, Peter Gloor</li>
<li>For: This paper aims to create a data-driven ESG evaluation system that provides more balanced and systemized scores by incorporating social sentiment.* Methods: The authors use Python web scrapers to collect data from Wikipedia, Twitter, LinkedIn, and Google News for S&amp;P 500 companies, and then clean and analyze the data using NLP algorithms to obtain sentiment scores for ESG subcategories. They train and calibrate machine-learning algorithms with S&amp;P Global ESG Ratings to test their predictive capabilities.* Results: The Random-Forest model shows encouraging results with a mean absolute error of 13.4% and a correlation of 26.1% (p-value 0.0372), indicating that measuring ESG social sentiment across sub-categories can help executives focus efforts on areas people care about most.Here’s the simplified Chinese text:* For: 这篇论文旨在创建一种基于数据的 ESG 评估系统，以提供更加均衡和系统化的评估结果，通过包含社交情感。* Methods: 作者使用 Python 网络抓取器收集 Wikipedia、Twitter、LinkedIn 和 Google News 上 S&amp;P 500 公司的数据，然后清洁和分析数据，使用 NLP 算法获得 ESG 子类别的情感分数。他们使用机器学习算法与 S&amp;P 全球 ESG 评级进行训练和校准。* Results: Random Forest 模型显示了鼓动的结果， mean absolute error 为 13.4%，相关度为 26.1% (p-value 0.0372)，表明 mesure ESG 社交情感 across sub-categories 可以帮助行政人员更加专注于人们关心的领域。<details>
<summary>Abstract</summary>
Environmental Social Governance (ESG) is a widely used metric that measures the sustainability of a company practices. Currently, ESG is determined using self-reported corporate filings, which allows companies to portray themselves in an artificially positive light. As a result, ESG evaluation is subjective and inconsistent across raters, giving executives mixed signals on what to improve. This project aims to create a data-driven ESG evaluation system that can provide better guidance and more systemized scores by incorporating social sentiment. Social sentiment allows for more balanced perspectives which directly highlight public opinion, helping companies create more focused and impactful initiatives. To build this, Python web scrapers were developed to collect data from Wikipedia, Twitter, LinkedIn, and Google News for the S&P 500 companies. Data was then cleaned and passed through NLP algorithms to obtain sentiment scores for ESG subcategories. Using these features, machine-learning algorithms were trained and calibrated to S&P Global ESG Ratings to test their predictive capabilities. The Random-Forest model was the strongest model with a mean absolute error of 13.4% and a correlation of 26.1% (p-value 0.0372), showing encouraging results. Overall, measuring ESG social sentiment across sub-categories can help executives focus efforts on areas people care about most. Furthermore, this data-driven methodology can provide ratings for companies without coverage, allowing more socially responsible firms to thrive.
</details>
<details>
<summary>摘要</summary>
环境社会治理（ESG）是一种广泛使用的指标，用于衡量公司的可持续发展实践。目前，ESG是通过自我报告的公司签据来确定的，这使得ESG评估变得主观和不一致，导致公司获得混乱的信息，从而难以决策。本项目的目标是创建一个数据驱动的ESG评估系统，为公司提供更好的指导和更系统化的评分。这个系统通过 incorporating社会情绪来提供更均衡的视角，直接反映公众意见，帮助公司制定更加焦点和有效的措施。为建立这个系统，我们使用Python网络抓取器收集了S&P 500公司的数据，并从Wikipedia、Twitter、LinkedIn和Google News中提取了相关信息。然后，我们清洁了数据并通过自然语言处理（NLP）算法获得了ESG下的情绪分数。使用这些特征，我们使用机器学习算法进行训练和校准，并与S&P全球ESG评级进行比较。Random Forest模型在这些模型中表现最佳，其中 mean absolute error 为13.4%，相关度为26.1%（p-value 0.0372），这表明了这种方法的潜在力量。总之，通过评估ESG社会情绪在不同的子类别中，可以帮助公司更好地focus其努力在人们关心的方面。此外，这种数据驱动的方法ологи也可以为没有评级的公司提供评估， allowing more socially responsible firms to thrive。
</details></li>
</ul>
<hr>
<h2 id="ConDA-Contrastive-Domain-Adaptation-for-AI-generated-Text-Detection"><a href="#ConDA-Contrastive-Domain-Adaptation-for-AI-generated-Text-Detection" class="headerlink" title="ConDA: Contrastive Domain Adaptation for AI-generated Text Detection"></a>ConDA: Contrastive Domain Adaptation for AI-generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03992">http://arxiv.org/abs/2309.03992</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amritabh/conda-gen-text-detection">https://github.com/amritabh/conda-gen-text-detection</a></li>
<li>paper_authors: Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu</li>
<li>for: 这个论文的目的是建立一种可以检测AI生成的新闻文本的检测器。</li>
<li>methods: 该论文使用了域 adaptation 技术和对比学习来学习域不变的表示，以便在无监督情况下进行检测。</li>
<li>results: 该论文的实验结果显示，使用该方法可以获得较好的检测性能，比最佳基eline的表现提高了31.7%，并且与完全监督的检测器的表现相差只有0.8%。<details>
<summary>Abstract</summary>
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at https://github.com/AmritaBh/ConDA-gen-text-detection.
</details>
<details>
<summary>摘要</summary>
We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain-invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best-performing baselines and within 0.8% margin of a fully supervised detector. All our code and data are available at <https://github.com/AmritaBh/ConDA-gen-text-detection>.
</details></li>
</ul>
<hr>
<h2 id="Derivation-of-Coordinate-Descent-Algorithms-from-Optimal-Control-Theory"><a href="#Derivation-of-Coordinate-Descent-Algorithms-from-Optimal-Control-Theory" class="headerlink" title="Derivation of Coordinate Descent Algorithms from Optimal Control Theory"></a>Derivation of Coordinate Descent Algorithms from Optimal Control Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03990">http://arxiv.org/abs/2309.03990</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. M. Ross</li>
<li>for: 这篇论文探讨了一种新的优化原理，即将多种优化算法集成到一个中心来源中，以便更好地解决优化问题。</li>
<li>methods: 该论文使用了一种最大原理和一组”控制” Lyapunov 函数来 derivate 基本的坐标下降算法。</li>
<li>results: 该论文显示了坐标下降算法的转化是与控制的泪函数的控制的耗散相关的。<details>
<summary>Abstract</summary>
Recently, it was posited that disparate optimization algorithms may be coalesced in terms of a central source emanating from optimal control theory. Here we further this proposition by showing how coordinate descent algorithms may be derived from this emerging new principle. In particular, we show that basic coordinate descent algorithms can be derived using a maximum principle and a collection of max functions as "control" Lyapunov functions. The convergence of the resulting coordinate descent algorithms is thus connected to the controlled dissipation of their corresponding Lyapunov functions. The operational metric for the search vector in all cases is given by the Hessian of the convex objective function.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近些时候，有人提出了一种可能将不同优化算法集中到一个中心源上的提议，我们在这里进一步发展这个提议，证明了基本坐标下降算法可以从这种新的原理中得到。具体来说，我们证明了基本坐标下降算法可以通过最大原理和一组"控制" Lyapunov 函数来 derivation。这些 Lyapunov 函数的满足程度的控制则导致坐标下降算法的 converge。搜索向量在所有情况下的操作度量是对对像函数的梯度。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Computing-of-the-mathsf-OR-and-mathsf-MAX-Functions"><a href="#Noisy-Computing-of-the-mathsf-OR-and-mathsf-MAX-Functions" class="headerlink" title="Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions"></a>Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03986">http://arxiv.org/abs/2309.03986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Banghua Zhu, Ziao Wang, Nadim Ghaddar, Jiantao Jiao, Lele Wang</li>
<li>for: 这篇论文是关于计算函数的论文， specifically 是关于使用不准确的查询来计算 $\mathsf{OR}$ 函数和 $\mathsf{MAX}$ 函数。</li>
<li>methods: 这篇论文使用了不准确的查询来计算 $\mathsf{OR}$ 函数和 $\mathsf{MAX}$ 函数。</li>
<li>results: 论文表明了一个预期的查询数量为 $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p | 1-p)}$ 是足够和必要的来计算这两个函数，并且这个结论与之前的研究相比，对 $p$ 的依赖关系有所紧张。<details>
<summary>Abstract</summary>
We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
</details>
<details>
<summary>摘要</summary>
我们考虑一个函数computing的问题，其中每个查询都有一定的错误率$p \in (0,1/2)$。我们考虑了两个函数：一是$n$个数位的$\mathsf{OR}$函数，另一个是$n$个数字的$\mathsf{MAX}$函数。我们显示了，需要一个平均的查询数量为 $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}{D_{\mathsf{KL}(p \| 1-p)}$，以确保函数的误差概率为 $\delta = o(1)$。相比之前的研究，我们的结果对$p$的依赖性提高了在上下限 both bounds for the two functions。
</details></li>
</ul>
<hr>
<h2 id="LanSER-Language-Model-Supported-Speech-Emotion-Recognition"><a href="#LanSER-Language-Model-Supported-Speech-Emotion-Recognition" class="headerlink" title="LanSER: Language-Model Supported Speech Emotion Recognition"></a>LanSER: Language-Model Supported Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03978">http://arxiv.org/abs/2309.03978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taesik Gong, Josh Belanich, Krishna Somandepalli, Arsha Nagrani, Brian Eoff, Brendan Jou</li>
<li>for: 本研究想要提高Speech Emotion Recognition（SER）模型的准确性和效率，使其能够在大量的语音数据上进行训练和测试。</li>
<li>methods: 本研究使用了弱型学习方法，将大型自然语言模型中的弱label推导到语音数据中，以减少人工标注的成本。在弱label推导中，使用文本推理方法选择了最高推理得分的情感label，并将其与自动语音识别得到的语音脚本进行结合。</li>
<li>results: 根据实验结果显示，使用弱型学习方法训练的SER模型在标准SER数据上表现出色，比其他基eline模型更高的准确性和效率。此外，使用弱label推导的表现显示，即使模型仅从文本中推导到情感label，也能够模型语音中的声音内容。<details>
<summary>Abstract</summary>
Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
</details>
<details>
<summary>摘要</summary>
对话情感识别（SER）模型通常需要昂贵的人工标注数据进行训练，使得扩大到大型对话数据和细化情感分类的方法困难。我们介绍了LanSER，一种方法可以使用无标注数据进行训练，通过弱型超级vised学习来推导弱情感标签。为了对约束taxonomy中的情感标签进行推导，我们使用文本包含关系方法，选择一个 speech 稿中的情感标签，通过自动语音识别来提取 speech 稿。我们的实验结果表明，在标注只基于文本的情况下，使用这种弱超级vised学习训练的模型可以在标准SER数据集上超越基eline模型，并显示改进的标签效率。尽管模型只基于文本标签进行训练，但我们发现模型 Apparently 捕捉到了语音中的 просодические内容。
</details></li>
</ul>
<hr>
<h2 id="DBsurf-A-Discrepancy-Based-Method-for-Discrete-Stochastic-Gradient-Estimation"><a href="#DBsurf-A-Discrepancy-Based-Method-for-Discrete-Stochastic-Gradient-Estimation" class="headerlink" title="DBsurf: A Discrepancy Based Method for Discrete Stochastic Gradient Estimation"></a>DBsurf: A Discrepancy Based Method for Discrete Stochastic Gradient Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03974">http://arxiv.org/abs/2309.03974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pau Mulet Arabi, Alec Flowers, Lukas Mauch, Fabien Cardinaux</li>
<li>for: 这 paper 是用来解决计算分布参数预期的 gradient 问题的。</li>
<li>methods: 这 paper 使用了 Reinforce 算法来解决 gradient 估计问题，并 introduce 了一种新的采样方法来减少实际分布和采样之间的差异。</li>
<li>results: 对比 exist 的估计器，DBsurf 在 least squares 问题中具有最低的偏度，并在不同的 dataset 和采样设置下训练 VAE 达到了最佳结果。  finally, 这 paper 使用 DBsurf 构建了一种简单而高效的 neural architecture search 算法，达到了状态革命的性能。<details>
<summary>Abstract</summary>
Computing gradients of an expectation with respect to the distributional parameters of a discrete distribution is a problem arising in many fields of science and engineering. Typically, this problem is tackled using Reinforce, which frames the problem of gradient estimation as a Monte Carlo simulation. Unfortunately, the Reinforce estimator is especially sensitive to discrepancies between the true probability distribution and the drawn samples, a common issue in low sampling regimes that results in inaccurate gradient estimates. In this paper, we introduce DBsurf, a reinforce-based estimator for discrete distributions that uses a novel sampling procedure to reduce the discrepancy between the samples and the actual distribution. To assess the performance of our estimator, we subject it to a diverse set of tasks. Among existing estimators, DBsurf attains the lowest variance in a least squares problem commonly used in the literature for benchmarking. Furthermore, DBsurf achieves the best results for training variational auto-encoders (VAE) across different datasets and sampling setups. Finally, we apply DBsurf to build a simple and efficient Neural Architecture Search (NAS) algorithm with state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
计算对分布参数的期望 gradient 是科学和工程多个领域的问题。通常，这个问题使用 Reinforce 来解决，它将问题定义为 Monte Carlo  simulations。然而，Reinforce 估计器在低抽样 régime 中尤其敏感于真实分布与抽样之间的差异，导致不准确的 gradient 估计。在这篇论文中，我们介绍 DBsurf，一种基于 Reinforce 的分布估计器，使用一种新的抽样过程来减少抽样与实际分布之间的差异。为评估我们的估计器的性能，我们对它进行了多种任务的测试。在现有的估计器中，DBsurf 的方差最低，而且在不同的数据集和抽样设置下，DBsurf 在训练 variational autoencoders (VAE) 中表现最佳。最后，我们使用 DBsurf 建立了一个简单而高效的 neural architecture search (NAS) 算法，并达到了状态空间的前iers Performance。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Concept-Embedding-Model-ACEM-No-train-time-concepts-No-issue"><a href="#Automatic-Concept-Embedding-Model-ACEM-No-train-time-concepts-No-issue" class="headerlink" title="Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!"></a>Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03970">http://arxiv.org/abs/2309.03970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain</li>
<li>for: 本研究旨在提高神经网络的可解释性和可读性，特别是在安全关键领域和提供社会权利的情况下。</li>
<li>methods: 本研究使用自动概念嵌入模型（ACEMs），它们可以自动学习概念标注，而不需要手动标注大量数据。</li>
<li>results: ACEMs可以超越概念嵌入模型（CEMs）的一个重要限制，即需要概念标注的全量训练数据。<details>
<summary>Abstract</summary>
Interpretability and explainability of neural networks is continuously increasing in importance, especially within safety-critical domains and to provide the social right to explanation. Concept based explanations align well with how humans reason, proving to be a good way to explain models. Concept Embedding Models (CEMs) are one such concept based explanation architectures. These have shown to overcome the trade-off between explainability and performance. However, they have a key limitation -- they require concept annotations for all their training data. For large datasets, this can be expensive and infeasible. Motivated by this, we propose Automatic Concept Embedding Models (ACEMs), which learn the concept annotations automatically.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT interpretability 和 explainability of neural networks 在安全关键领域和提供社会的权利Explain 中得到不断增加的重要性，特别是在大规模数据集中。基于概念的解释Architecture 对于 humans 的理解方式非常合理， proven 可以Effective 地解释模型。基于概念的嵌入模型（CEMs）是一种这样的解释Architecture ，它们能够超越性能和解释之间的交易。然而，它们具有一个关键的限制---它们需要概念注释 для所有的训练数据。对于大规模数据集，这可能是非常昂贵和不可能的。 Motivated  by 这一点，我们提出了自动概念嵌入模型（ACEMs），它们可以自动学习概念注释。TRANSLATE_TEXT_END
</details></li>
</ul>
<hr>
<h2 id="Improving-Resnet-9-Generalization-Trained-on-Small-Datasets"><a href="#Improving-Resnet-9-Generalization-Trained-on-Small-Datasets" class="headerlink" title="Improving Resnet-9 Generalization Trained on Small Datasets"></a>Improving Resnet-9 Generalization Trained on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03965">http://arxiv.org/abs/2309.03965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omarawad2/HAET2021_Huawei">https://github.com/omarawad2/HAET2021_Huawei</a></li>
<li>paper_authors: Omar Mohamed Awad, Habib Hajimolahoseini, Michael Lim, Gurpreet Gosal, Walid Ahmed, Yang Liu, Gordon Deng</li>
<li>for: 这个论文是为了参加ICLR竞赛，目标是在 less than 10 minutes 内达到 CIFAR-10 图像分类任务最高准确率。</li>
<li>methods: 该方法包括应用多种技术来提高 ResNet-9 的通用性，包括：锐度感知优化、标签整合、梯度中心化、输入图像白平衡以及基于学习的训练。</li>
<li>results: 实验表明，通过在 less than 10 minutes 内训练只使用 CIFAR-10  dataset 的 10% subset，ResNet-9 可以达到 88% 的准确率。<details>
<summary>Abstract</summary>
This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets
</details>
<details>
<summary>摘要</summary>
本文介绍我们提出的方法，在ICLR竞赛中获得了首奖，挑战是在10分钟内达到最高精度在图像分类任务中。我们使用了一系列技术来提高ResNet-9的通用性，包括：锐度感知优化、标签平滑、梯度中心化、输入裁剪白净化以及基于学习的训练。我们的实验表明，只需在CIFAR-10 dataset中训练10%的子集，ResNet-9可以在10分钟内达到88%的精度。
</details></li>
</ul>
<hr>
<h2 id="REALM-Robust-Entropy-Adaptive-Loss-Minimization-for-Improved-Single-Sample-Test-Time-Adaptation"><a href="#REALM-Robust-Entropy-Adaptive-Loss-Minimization-for-Improved-Single-Sample-Test-Time-Adaptation" class="headerlink" title="REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation"></a>REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03964">http://arxiv.org/abs/2309.03964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Seto, Barry-John Theobald, Federico Danieli, Navdeep Jaitly, Dan Busbridge</li>
<li>for:  Mitigate performance loss due to distribution shifts between train and test data, without access to the training data and without knowledge of the model training procedure.</li>
<li>methods:  Use a pre-trained model adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization, but with a new approach that improves robustness to noisy samples.</li>
<li>results:  Achieve better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating effectiveness.Here’s the full summary in Simplified Chinese:</li>
<li>for:  Mitigate performance loss due to distribution shifts between train and test data, without access to the training data and without knowledge of the model training procedure.</li>
<li>methods:  Use a pre-trained model adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization, but with a new approach that improves robustness to noisy samples.</li>
<li>results:  Achieve better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating effectiveness.<details>
<summary>Abstract</summary>
Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.
</details>
<details>
<summary>摘要</summary>
完全测试时适应（F-TTA）可以减少因为分布变化而导致的性能下降（1）无需访问训练数据，以及（2）无需知道模型训练过程。在线F-TTA中，一个预训练模型通过使用一条流式测试样本来适应，并使用自我指导目标，如Entropy降低，来进行适应。然而，使用在线Entropy降低来适应，尤其在单个样本设置下，容易导致不稳定，Resulting in degenerate solutions, and limiting the adoption of TTA inference strategies.先前的工作认为噪音或不可靠的样本是适应失败的原因。一种解决方案是忽略这些样本，可能会导致更新过程中的偏见，慢速适应，和差异化。在这个工作中，我们提出了一个普遍适用的框架，可以提高F-TTA的异常性，以解决这些噪音样本的问题，得到更好的适应精度。我们提出的方法，即Robust Entropy Adaptive Loss Minimization（REALM），在CIFAR-10和ImageNet-1K上进行了适应过程中的整体性能提高，证明其效果。
</details></li>
</ul>
<hr>
<h2 id="ImageBind-LLM-Multi-modality-Instruction-Tuning"><a href="#ImageBind-LLM-Multi-modality-Instruction-Tuning" class="headerlink" title="ImageBind-LLM: Multi-modality Instruction Tuning"></a>ImageBind-LLM: Multi-modality Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03905">http://arxiv.org/abs/2309.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/llama-adapter">https://github.com/opengvlab/llama-adapter</a></li>
<li>paper_authors: Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao</li>
<li>for: 这个论文的目的是提出一种基于 ImageBind 的多Modalität 语言模型调教方法（LLMs），以便响应不同的感知Modalität，包括语音、3D 点云、视频和它们的Embedding 空间运算。</li>
<li>methods: 该方法使用一个学习可能的绑定网络将 ImageBind 的图像编码器的Embedding 空间与 LLaMA 的语言模型的Embedding 空间绑定在一起。然后，通过一种无注意力和初始化的阀值机制，将图像特征转换为word token的权重，以便在语言模型中进行多Modalität  instrucion 跟踪。</li>
<li>results: 在训练中，ImageBind-LLM 可以响应多Modalität 的 instrucion，并且在语言生成质量方面表现出色。在推理时，通过一种提posed的视觉缓存模型，对多Modalität 输入进行进一步的跨Modalität 嵌入增强。这种缓存模型可以从三百万个由 ImageBind 提取的图像特征中提取出来，从而有效地解决训练-推理Modalität 差距问题。<details>
<summary>Abstract</summary>
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.
</details>
<details>
<summary>摘要</summary>
我们介绍ImageBind-LLM，一种基于ImageBind的多modal性指令调整方法 для大语言模型（LLM）。现有工作主要关注语言和图像指令调整，与之不同，我们的ImageBind-LLM可以响应多modal性条件，包括音频、3D点云、视频和它们的嵌入空间加法。在训练过程中，我们采用学习 bind 网络将 ImageBind 的图像编码器的嵌入空间与 LLaMA 的嵌入空间进行对应。然后，通过 bind 网络将图像特征转换为 word tokens 的所有层，通过无注意力和初始化的阻止机制，逐渐注入视觉指令。帮助joint embedding of ImageBind，简单的图像文本训练可以让我们的模型在多modal性指令跟踪能力方面表现出色。在推理过程中，多modal输入被 feed 到相应的 ImageBind 编码器，并被一种提议的视觉缓存模型进行进一步跨Modal embedding 增强。这种无需训练的缓存模型通过从 ImageBind 提取的三百万个图像特征，有效地解决了训练-推理模态不一致问题。值得注意的是，通过我们的方法，ImageBind-LLM可以响应多modal性指令并达到显著的语言生成质量。代码可以在 https://github.com/OpenGVLab/LLaMA-Adapter 上下载。
</details></li>
</ul>
<hr>
<h2 id="DiffusionEngine-Diffusion-Model-is-Scalable-Data-Engine-for-Object-Detection"><a href="#DiffusionEngine-Diffusion-Model-is-Scalable-Data-Engine-for-Object-Detection" class="headerlink" title="DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection"></a>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03893">http://arxiv.org/abs/2309.03893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manlin Zhang, Jie Wu, Yuxi Ren, Ming Li, Jie Qin, Xuefeng Xiao, Wei Liu, Rui Wang, Min Zheng, Andy J. Ma</li>
<li>for: 这篇论文的目的是推广深度学习中的数据引擎，以提高物体检测的效果。</li>
<li>methods: 这篇论文使用了Diffusion Model，一种可扩展的数据引擎，以生成高质量的物体检测训练对。它还提出了一种名为Detection-Adapter的新方法，可以使得Diffusion Model更好地适应物体检测任务。</li>
<li>results:  experiments show that 使用Diffusion Engine可以在多种场景下提高物体检测的效果，如不同的检测算法、自动预训练、数据稀缺、标签缺乏等。例如，在使用Diffusion Engine和DINO-based adapter进行数据扩大后，COCO上的mAP提高了3.1%，VOC上提高了7.6%，Clipart上提高了11.5%。<details>
<summary>Abstract</summary>
Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing detection benchmarks for facilitating follow-up research. Extensive experiments demonstrate that data scaling-up via DE can achieve significant improvements in diverse scenarios, such as various detection algorithms, self-supervised pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised learning. For example, when using DE with a DINO-based adapter to scale up data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.
</details>
<details>
<summary>摘要</summary>
“数据是深度学习的基础石头。本文揭示了最近发展的扩散模型，它是一种可扩展的数据引擎，用于对象检测。现有的方法通常需要手动收集或生成模型来获取目标图像，然后进行数据扩展和标注，这是成本高、复杂或缺乏多样性的。为解决这些问题，我们提出了扩散引擎（DE），它可以在单个阶段内提供高质量的检测 oriented 训练对。DE 由一个预训练的扩散模型和一个有效的检测适配器组成，可以在插入式的方式下生成可扩展、多样化和普适的检测数据。检测适配器通过将含义扩散模型中的隐式语义和位置知识与检测相关的信号相对而学习，以进一步提高矩形框预测。此外，我们也贡献了 COCO-DE 和 VOC-DE 两个数据集，以扩大现有的检测标准benchmark，便于后续研究。广泛的实验表明，通过 DE 的数据扩展可以在多种enario中实现显著的改善，包括不同的检测算法、自我主义预训练、数据稀缺、标签缺乏、跨频训练和半支持学习。例如，当使用 DE 与 DINO 的适配器扩展数据时，COCO 上的 mAP 提高了3.1%，VOC 上提高了7.6%，Clipart 上提高了11.5%。”
</details></li>
</ul>
<hr>
<h2 id="ArtiGrasp-Physically-Plausible-Synthesis-of-Bi-Manual-Dexterous-Grasping-and-Articulation"><a href="#ArtiGrasp-Physically-Plausible-Synthesis-of-Bi-Manual-Dexterous-Grasping-and-Articulation" class="headerlink" title="ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation"></a>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03891">http://arxiv.org/abs/2309.03891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges</li>
<li>for: 本研究旨在提出一种新的方法，用于synthesize bi-manual手臂物体交互，包括抓取和定位。</li>
<li>methods: 本方法利用了强化学习和物理模拟来训练一个策略，该策略控制了全局和局部手姿。</li>
<li>results: 我们的方法在实验中证明可以高效地synthesize bi-manual手臂交互，并且可以在不同的物体和环境下进行抓取和定位。<details>
<summary>Abstract</summary>
We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping, relocation, and articulation. We show our method's efficacy towards this task. We further demonstrate that our method can generate motions with noisy hand-object pose estimates from an off-the-shelf image-based regressor.
</details>
<details>
<summary>摘要</summary>
我们介绍ArtiGrasp，一种新的方法，用于生成双手手套对象的交互，包括抓取和扭转。由于全球肘部运动的多样性以及需要精准的手指控制来扭转物体，这个任务非常具有挑战性。ArtiGrasp 利用了强化学习和物理模拟来训练一个控制全球和局部手姿的策略。我们的框架将抓取和扭转合并在一个单一的策略中，即单一的手姿参考。此外，为了帮助精准的手指控制，我们提出了一种学习级联，从单手操作静止物体开始，然后与多个代理人进行训练，包括双手和不定形物体。为了评估我们的方法，我们引入了动态物体抓取和扭转任务，这个任务需要抓取、重新定位和扭转。我们示出了我们方法的效果。此外，我们还证明了我们的方法可以生成具有噪音手套对象pose估计的手套动作。
</details></li>
</ul>
<hr>
<h2 id="A-Function-Interpretation-Benchmark-for-Evaluating-Interpretability-Methods"><a href="#A-Function-Interpretation-Benchmark-for-Evaluating-Interpretability-Methods" class="headerlink" title="A Function Interpretation Benchmark for Evaluating Interpretability Methods"></a>A Function Interpretation Benchmark for Evaluating Interpretability Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03886">http://arxiv.org/abs/2309.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimodal-interpretability/find">https://github.com/multimodal-interpretability/find</a></li>
<li>paper_authors: Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba</li>
<li>for: 本研究目的是评估自动化解释方法的效果，以便在真实世界模型上应用。</li>
<li>methods: 本研究使用语言模型（LM）生成代码和语言描述函数行为。</li>
<li>results: 研究发现，使用黑盒访问函数的LM可以偶尔推测函数结构，但是这些描述通常只 capture global函数行为，缺乏地方腐败。这些结果表明，FIND 可以用于评估更复杂的解释方法的性能，以便在真实世界模型上应用。<details>
<summary>Abstract</summary>
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate new and existing methods that use language models (LMs) to produce code-based and language descriptions of function behavior. We find that an off-the-shelf LM augmented with only black-box access to functions can sometimes infer their structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, LM-based descriptions tend to capture global function behavior and miss local corruptions. These results show that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.
</details>
<details>
<summary>摘要</summary>
Labeling neural network submodules with human-readable descriptions 是有用的 для许多下游任务，例如：把这些描述 surface 出来，导引 intervención 和可能 même 解释模型的重要行为。到目前为止，大多数机制性描述已经是小型模型、窄领域和大量的人工劳动。将所有人类可读的子计算机入模型中将 almost certainly 需要自动生成和验证描述的工具。最近，使用学习模型内 Loop 的技术已经开始受到欢迎，但评估这些工具的方法却有限和不一致。本文介绍 FIND（功能 interpretability and Description），一个用于评估自动解释方法的benchmarksuite。 FIND包含类似于训练后神经网络中的组件，以及这些组件的描述。这些函数通过文本和数字领域进行过程构造，并包括噪音、组合、近似、和偏见等实际问题。我们评估新和现有的语言模型（LM）使用语言模型生成代码和语言描述函数行为的方法。我们发现，只有靠black-box访问函数的LM可以偶尔推理出函数结构，行为如一位科学家，提出 гипотезы、建议实验和根据新数据更新描述。然而，LM-based描述通常捕捉全局函数行为，而忽略地方腐化。这些结果表明，FIND将是用于评估更复杂的解释方法的有用工具。
</details></li>
</ul>
<hr>
<h2 id="DoLa-Decoding-by-Contrasting-Layers-Improves-Factuality-in-Large-Language-Models"><a href="#DoLa-Decoding-by-Contrasting-Layers-Improves-Factuality-in-Large-Language-Models" class="headerlink" title="DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"></a>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03883">http://arxiv.org/abs/2309.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/voidism/dola">https://github.com/voidism/dola</a></li>
<li>paper_authors: Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He</li>
<li>for: 降低大语言模型（LLM）中的幻觉，即在预训练后生成的内容与实际信息不匹配。</li>
<li>methods: 提出了一种简单的解码策略，不需要基于已经学习的知识进行条件或进一步调整。该策略通过对不同层的层数进行对比，从 vocabulary 空间中获取下一个token的分布，利用了 LLM 中的知识具有局部特征的事实。</li>
<li>results: 该 Decoding by Contrasting Layers（DoLa）策略能够更好地把握事实，降低 LLM 中的幻觉。DoLa 在多个选择任务和开放式生成任务中表现出色，例如提高了 LLaMA 家族模型在 TruthfulQA 中的表现，提高了真实率12-17%的绝对点数。这表明 DoLa 可以使 LLM 可靠地生成真实的事实。<details>
<summary>Abstract</summary>
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
</details>
<details>
<summary>摘要</summary>
尽管它们具有印象的能力，大语言模型（LLM）仍然容易出现幻觉，即生成与实际见到的预训练数据不符的内容。我们提出了一种简单的解码策略，可以降低 LLM 中的幻觉，不需要conditioning于检索到的外部知识 nor 额外 fine-tuning。我们的方法通过对 later layers 和 earlier layers 的投影到 vocabulary space 中进行对比，利用了 LLM 中的实际知识在特定 transformer layers 中的强制性，以提取 factual knowledge 并减少生成错误的内容。我们称之为 Decoding by Contrasting Layers（DoLa）方法。我们发现 DoLa 方法可以更好地抽出 factual knowledge，并减少 incorrect facts 的生成。DoLa 方法在多个选择任务和开放式生成任务中均有改进 TruthfulQA 模型的表现，例如提高 LLaMA 家族模型的表现 by 12-17% 绝对点数，这表明 DoLa 方法可以使 LLM 可靠地生成真实的事实。
</details></li>
</ul>
<hr>
<h2 id="Better-Practices-for-Domain-Adaptation"><a href="#Better-Practices-for-Domain-Adaptation" class="headerlink" title="Better Practices for Domain Adaptation"></a>Better Practices for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03879">http://arxiv.org/abs/2309.03879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linus Ericsson, Da Li, Timothy M. Hospedales</li>
<li>for: 本文旨在探讨领域适应（DA）在实际应用中遇到的分布shift问题，以及如何通过不使用标签来适应模型。</li>
<li>methods: 本文使用了多种适应算法来解决领域适应问题，包括无监督领域适应（UDA）、源自由领域适应（SFDA）和测试时适应（TTA）。</li>
<li>results: 本文通过使用合适的验证分割和评估指标来评估各种适应算法，并发现了一些挑战。虽然实际性 дости可能更差than预期，但使用正确的验证分割和评估指标可以帮助提高领域适应的性能。<details>
<summary>Abstract</summary>
Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology including Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and Test Time Adaptation (TTA). While the results show that realistically achievable performance is often worse than expected, they also show that using proper validation splits is beneficial, as well as showing that some previously unexplored validation metrics provide the best options to date. Altogether, our improved practices covering data, training, validation and hyperparameter optimisation form a new rigorous pipeline to improve benchmarking, and hence research progress, within this important field going forward.
</details>
<details>
<summary>摘要</summary>
<TRANSLATION>机器学习应用中的分布Shift是非常常见的问题。领域适应（DA）目的是为了适应模型到部署数据，而不需要使用标签。然而，领域Shift场景还存在一个更为细微的挑战：无法在部署数据上进行标签的验证，导致HPO（超参数优化）的过程中使用目标测试标签，这在实际应用中是不可用的。这在DA研究中导致了一些坏习惯，如在目标测试标签上进行HPO，从而导致了对DA研究进展的过度估计。在这篇论文中，我们分析了使用良好评估方法时的DA情况，并对一些候选验证标准进行比较。我们发现，领域适应方法学习中存在三大挑战：无监督领域适应（UDA）、源自领域适应（SFDA）和测试时适应（TTA）。虽然结果表明实际可以达到的性能通常比预期更差，但是也表明使用正确的验证分割是有利的，同时也发现一些以前未探索的验证指标可以提供最佳选择。总的来说，我们提出了一种新的严格的管道，包括数据、训练、验证和超参数优化，以提高研究进度。</TRANSLATION>Note: Please note that the translation is in Simplified Chinese, and the sentence structure and vocabulary may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="OpinionGPT-Modelling-Explicit-Biases-in-Instruction-Tuned-LLMs"><a href="#OpinionGPT-Modelling-Explicit-Biases-in-Instruction-Tuned-LLMs" class="headerlink" title="OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs"></a>OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03876">http://arxiv.org/abs/2309.03876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Haller, Ansar Aynetdinov, Alan Akbik</li>
<li>For: The paper aims to make biases in instruction-tuning explicit and transparent by presenting a web demo called OpinionGPT, which allows users to investigate different biases in the answers provided by the model.* Methods: The authors trained an instruction-tuning model on a corpus of text representing 11 different biases (political, geographic, gender, age) and derived from members of these demographics.* Results: The authors showcased the web application OpinionGPT, which allows side-by-side comparison of answers provided by the model for different biases.<details>
<summary>Abstract</summary>
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
</details>
<details>
<summary>摘要</summary>
现代大型自然语言模型（LLM）在生成适应自然语言指令的能力方面呈现出了惊人的表现。然而，一个开放的研究问题是训练模型内置的偏见。例如，如果用于训练LLM的数据主要由特定政治偏见的人写成，那么生成的答案可能会带有这种偏见。现有研究努力去除这些偏见，或者抑制可能带有偏见的答案。而我们则选择了一个不同的视角：而不是trying to suppress them，我们想要让它们变得显着和透明。为此，我们提出了一个名为OpinionGPT的网页示例，用户可以在这里提问和选择想要调查的偏见。示例将使用根据每种选择的偏见进行微调的模型回答问题，从而实现了侧对比。为了训练基础模型，我们identified 11种偏见（政治、地理、性别、年龄），并生成了一个基于这些人类论述的指令微调集。本文介绍了OpinionGPT，详细介绍了我们如何训练偏见意识的模型，并显示了网页应用程序（可以在https://opiniongpt.informatik.hu-berlin.de/）。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-the-Non-Asymptotic-Theory-of-System-Identification"><a href="#A-Tutorial-on-the-Non-Asymptotic-Theory-of-System-Identification" class="headerlink" title="A Tutorial on the Non-Asymptotic Theory of System Identification"></a>A Tutorial on the Non-Asymptotic Theory of System Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03873">http://arxiv.org/abs/2309.03873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ingvar Ziemann, Anastasios Tsiamis, Bruce Lee, Yassir Jedra, Nikolai Matni, George J. Pappas</li>
<li>for: 本文主要适用于线性系统识别理论中最近发展的非假函数方法。</li>
<li>methods: 本文使用了覆盖技巧、汉森-维特不等式和自正常化Martingale方法，这些工具在系统识别问题中特别有用。</li>
<li>results: 本文使用这些工具给出了不等式基于最小二乘估计器的性能表现的流畅证明，并绘制了扩展到某些非线性识别问题的想法。<details>
<summary>Abstract</summary>
This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.
</details>
<details>
<summary>摘要</summary>
这个教程是非偏极方法理论的引入教程，主要探讨线性系统识别问题。我们强调在这个领域中 particualrly 有用的工具，如覆盖技巧、汉森-温特不等式和自适应马丁加LS。然后，我们使用这些工具来提供直观的证明，证明不同类型的自回归模型参数的最小二乘估计的性能。 finally，我们简要介绍了如何将这些想法扩展到某些非线性识别问题。Note: "非偏极方法" (non-asymptotic methods) in the original text is translated as "非偏极方法理论" (non-asymptotic method theory) in Simplified Chinese, as there is no direct translation for "non-asymptotic" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="CenTime-Event-Conditional-Modelling-of-Censoring-in-Survival-Analysis"><a href="#CenTime-Event-Conditional-Modelling-of-Censoring-in-Survival-Analysis" class="headerlink" title="CenTime: Event-Conditional Modelling of Censoring in Survival Analysis"></a>CenTime: Event-Conditional Modelling of Censoring in Survival Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03851">http://arxiv.org/abs/2309.03851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedhshahin/CenTime">https://github.com/ahmedhshahin/CenTime</a></li>
<li>paper_authors: Ahmed H. Shahin, An Zhao, Alexander C. Whitehead, Daniel C. Alexander, Joseph Jacob, David Barber</li>
<li>for: 预测医疗事件时间（例如死亡或癌症复发）基于初始观察数据，以提高医疗预测的精度。</li>
<li>methods: 提出了一种新的生存分析方法（CenTime），通过直接估计事件时间来提高预测精度。该方法具有一种创新的事件conditional缺失机制，可以在缺失数据情况下表现良好。</li>
<li>results: 比较了CenTime与标准生存分析方法（如Cox准确风险模型和DeepHit），结果显示CenTime可以提供最佳性能，同时保持与其他方法相同的排名性能。<details>
<summary>Abstract</summary>
Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that our approach forms a consistent estimator for the event model parameters, even in the absence of uncensored data. Furthermore, CenTime is easily integrated with deep learning models with no restrictions on batch size or the number of uncensored samples. We compare our approach with standard survival analysis methods, including the Cox proportional-hazard model and DeepHit. Our results indicate that CenTime offers state-of-the-art performance in predicting time-to-death while maintaining comparable ranking performance. Our implementation is publicly available at https://github.com/ahmedhshahin/CenTime.
</details>
<details>
<summary>摘要</summary>
生存分析是一种有用的工具，可以根据基线观察数据来估计特定事件的时间，如死亡或癌症复发。这 particu-larly useful in healthcare，可以预测 based on patient data 的临床重要事件的发生时间。然而，现有的方法往往有限制，一些只是将患者按照生存可能性排名，而忽略了实际事件时间的估计；另一些对事件作为分类任务进行处理，忽略了事件的时间顺序结构。此外，利用 censored 样本的有效使用是关键提高预测模型的准确性。在本文中，我们介绍了 CenTime，一种新的生存分析方法，可以直接估计事件时间。我们的方法具有创新的事件 conditional 封锁机制，可以在缺失完整数据时表现稳定。我们展示了 CenTime 可以在缺失完整数据情况下形成一致的事件模型参数估计器，并且可以与深度学习模型无约束地集成。我们对标准生存分析方法，包括 Cox 相对危险模型和 DeepHit，进行比较。结果表明，CenTime 可以在预测时间到死亡的任务上提供状态机器的性能，同时保持与标准方法相比的比较良好的排名性。我们的实现可以在 https://github.com/ahmedhshahin/CenTime 上获取。
</details></li>
</ul>
<hr>
<h2 id="Mixtures-of-Gaussians-are-Privately-Learnable-with-a-Polynomial-Number-of-Samples"><a href="#Mixtures-of-Gaussians-are-Privately-Learnable-with-a-Polynomial-Number-of-Samples" class="headerlink" title="Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples"></a>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03847">http://arxiv.org/abs/2309.03847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</li>
<li>for: 估计混合 Gaussian 函数数据，即使受到差分隐私（DP）的限制。</li>
<li>methods: 提出了一种新的框架，可能对其他任务有用。该框架基于两个必要条件：(1) 分布集（如 Gaussian）是可列册解码的，(2) 该分布集在总差 variation 距离方面具有 “地方小” 覆盖。</li>
<li>results: 得到了 $\tilde{O}(k^2 d^4 \log(1&#x2F;\delta) &#x2F; \alpha^2 \varepsilon)$ 样本的 suffice 条件，以便在 $\alpha$ 总差 variation 距离内估计混合 Gaussian 函数数据，同时满足 $(\varepsilon, \delta)$-DP。这是现有最佳 finite sample complexity 上限，不假设 GMM 的任何结构。<details>
<summary>Abstract</summary>
We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.   To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
</details>
<details>
<summary>摘要</summary>
我们研究对于数据 privacy 的推估混合 Gaussian 的问题。我们的主要结果是，只需要 $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ 样本来推估 $k$ 个 Gaussian 的混合，并且保证 $\alpha$ 的总差异距离条件，这是第一个不假设 GMM 的结构的最佳时间复杂度上界。我们的解法基于一个新的框架，它可能对其他任务也有用。在高度概括的话，我们证明了如果一个分布类别（如 Gaussian）满足以下两个条件，则其混合的分布也可以私有地学习：1. 分布类别是可识别的（list decodable）。2. 分布类别在总差异距离下是 "局部小" 的覆盖（locally small cover）。我们的证明绕过了已知的一个障碍，即 Unlike Gaussian, GMM 不是 "局部小" 的覆盖。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Feature-Learning-under-Structured-Data"><a href="#Gradient-Based-Feature-Learning-under-Structured-Data" class="headerlink" title="Gradient-Based Feature Learning under Structured Data"></a>Gradient-Based Feature Learning under Structured Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03843">http://arxiv.org/abs/2309.03843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, Murat A. Erdogdu</li>
<li>for: 本文研究了单指数模型在不同结构的输入数据上的学习复杂性。</li>
<li>methods: 本文使用了梯度下降法和权重正规化来学习单指数模型。</li>
<li>results: 研究发现，在带有射频结构的输入数据上，通常使用的球形梯度动力学可能无法正确地回归真向量，而适当的权重正规化可以解决这个问题。此外，通过利用输入卷积环境和目标之间的对称性，本文可以获得改进的样本复杂性和超越下界的 rotationally invariant kernel 方法。<details>
<summary>Abstract</summary>
Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.
</details>
<details>
<summary>摘要</summary>
Recent research has shown that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results only apply to isotropic data, while in practice the input often contains additional structure that can implicitly guide the algorithm. In this study, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Furthermore, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.
</details></li>
</ul>
<hr>
<h2 id="Early-warning-via-transitions-in-latent-stochastic-dynamical-systems"><a href="#Early-warning-via-transitions-in-latent-stochastic-dynamical-systems" class="headerlink" title="Early warning via transitions in latent stochastic dynamical systems"></a>Early warning via transitions in latent stochastic dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03842">http://arxiv.org/abs/2309.03842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingyu Feng, Ting Gao, Wang Xiao, Jinqiao Duan</li>
<li>for: 这篇论文的目的是为了提供一种可以检测复杂系统或高维观测数据的早期警示方法。</li>
<li>methods: 本研究使用一种新的方法：指定方向的不变动对映，来捕捉低维抽象空间中的发展动力学。</li>
<li>results: 通过应用这种方法，我们成功地获得了有效的对应对象，并获得了检测点transition的早期警示信号。<details>
<summary>Abstract</summary>
Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
</details>
<details>
<summary>摘要</summary>
早期预警 для动力学转移在复杂系统或高维观测数据中非常重要，例如基因变化、脑病、自然灾害、金融危机和工程可靠性。为了有效提取早期预警信号，我们开发了一种新方法：导向异otropic扩散地图，这种方法可以捕捉低维抽象空间中的潜在演化动力学。通过应用这种方法，我们成功地找到了适当的有效坐标，并提取了可以检测状态转移的早期预警信号。我们的方法可以将潜在动力学与原始数据集相连接。我们通过数值实验证明了我们的方法是准确和有效的，在density和转移概率方面。结果表明，第二坐标包含了关键的转移点信息。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Adaptive-Human-Machine-Interfaces-with-Offline-Reinforcement-Learning"><a href="#Bootstrapping-Adaptive-Human-Machine-Interfaces-with-Offline-Reinforcement-Learning" class="headerlink" title="Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"></a>Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03839">http://arxiv.org/abs/2309.03839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jensen Gao, Siddharth Reddy, Glen Berseth, Anca D. Dragan, Sergey Levine<br>for: 这种论文旨在帮助用户完成带有噪音、高维度指令信号（例如从大脑计算机接口）的Sequential decision-making任务，例如机器人 теле操作。methods: 本文提出了一种基于人工智能的循环学习算法，用于让界面将原始指令信号映射到行为上，通过线上预训练和线下精度调整。为了解决噪音指令信号和罕见奖励的挑战，我们开发了一种新的方法，用于表示和推理用户在给定路径上的长期意图。results: 我们通过一个用户研究，让12名参与者通过眼睛控制的128个指令信号来完成一个模拟的导航任务，发现我们的方法可以更好地帮助用户完成目标导航，比基eline指令接口。我们还在模拟的Sawyer推push任务和Lunar Lander游戏中测试了我们的方法，并发现它在这些领域中也有所提高。我们还进行了大量的减少实验，以确认每个方法的重要性。<details>
<summary>Abstract</summary>
Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method.
</details>
<details>
<summary>摘要</summary>
可 adaptive 界面可以帮助用户完成Sequential 决策任务，如 робо控制，尤其是从 brain-computer interface 获得的噪音高维度指令信号。现有的人 loop 机器学习技术可以使这些系统进步，但它们通常受到个人用户数据收集的限制。在这篇论文中，我们提出了一种强化学习算法，以训练界面将原始指令信号映射到操作使用混合式的离线预训练和在线精度调整。为了解决指令信号噪音和奖励稀缺的挑战，我们开发了一种新的用户长期意图表示和推理方法。我们主要通过一个用户研究，在其中12名参与者通过眼球跟踪来修改来自 webcam 的 128 维度指令信号来评估我们的方法。结果表明，我们的方法可以更多地帮助用户完成目标 Navigation，而不是只是提供方向性的指导。我们进一步在 simulated Sawyer 推动任务和 eye gaze 控制下测试了我们的方法，以及 Lunar Lander 游戏中的 simulated 用户指令，并发现我们的方法在这些领域中也有所提高。我们还进行了大量的减少实验，以确认每个方法组件的重要性。
</details></li>
</ul>
<hr>
<h2 id="Cross-Task-Attention-Network-Improving-Multi-Task-Learning-for-Medical-Imaging-Applications"><a href="#Cross-Task-Attention-Network-Improving-Multi-Task-Learning-for-Medical-Imaging-Applications" class="headerlink" title="Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications"></a>Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03837">http://arxiv.org/abs/2309.03837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangwook Kim, Thomas G. Purdie, Chris McIntosh<br>for: This paper is written to improve the accuracy of medical imaging tasks using a novel attention-based multi-task learning (MTL) framework.methods: The proposed framework, called Cross-Task Attention Network (CTAN), utilizes cross-task attention mechanisms to incorporate information by interacting across tasks.results: Compared to standard single-task learning (STL) and two widely used MTL baselines (hard parameter sharing and multi-task attention network), CTAN demonstrated a 4.67% improvement in performance and outperformed both baselines across four medical imaging datasets.<details>
<summary>Abstract</summary>
Multi-task learning (MTL) is a powerful approach in deep learning that leverages the information from multiple tasks during training to improve model performance. In medical imaging, MTL has shown great potential to solve various tasks. However, existing MTL architectures in medical imaging are limited in sharing information across tasks, reducing the potential performance improvements of MTL. In this study, we introduce a novel attention-based MTL framework to better leverage inter-task interactions for various tasks from pixel-level to image-level predictions. Specifically, we propose a Cross-Task Attention Network (CTAN) which utilizes cross-task attention mechanisms to incorporate information by interacting across tasks. We validated CTAN on four medical imaging datasets that span different domains and tasks including: radiation treatment planning prediction using planning CT images of two different target cancers (Prostate, OpenKBP); pigmented skin lesion segmentation and diagnosis using dermatoscopic images (HAM10000); and COVID-19 diagnosis and severity prediction using chest CT scans (STOIC). Our study demonstrates the effectiveness of CTAN in improving the accuracy of medical imaging tasks. Compared to standard single-task learning (STL), CTAN demonstrated a 4.67% improvement in performance and outperformed both widely used MTL baselines: hard parameter sharing (HPS) with an average performance improvement of 3.22%; and multi-task attention network (MTAN) with a relative decrease of 5.38%. These findings highlight the significance of our proposed MTL framework in solving medical imaging tasks and its potential to improve their accuracy across domains.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）是深度学习中一种 poderoso 的方法，利用多个任务的信息在训练中来提高模型性能。在医疗影像领域，MTL 表现出了很大的潜力，可以解决多种任务。然而，现有的医疗影像MTL架构受限于任务之间的信息共享，从而减少了MTL 的性能提高 potential。在本研究中，我们提出了一种 novel attention-based MTL 框架，可以更好地利用多任务之间的交互来提高各种任务的性能。具体来说，我们提出了一种 Cross-Task Attention Network（CTAN），通过跨任务注意机制来捕捉多任务之间的信息交互。我们在四个医疗影像 Dataset 上验证了 CTAN，包括：基于规划 CT 图像的两种不同肿瘤抑制（肾癌和开放KBP）；粉刺皮肤抑制和诊断使用 dermatoscopic 图像（HAM10000）；以及COVID-19 诊断和严重程度预测使用胸部 CT 成像（STOIC）。我们的研究表明，CTAN 可以提高医疗影像任务的准确率。相比标准单任务学习（STL），CTAN 表现出4.67%的提高，并在两种广泛使用的 MTL 基线上出perform：硬件参数共享（HPS）的平均提高率为3.22%，以及多任务注意网络（MTAN）的相对下降5.38%。这些结果表明了我们提出的 MTL 框架在解决医疗影像任务方面的重要性和其在不同领域中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Demonstration-via-Probabilistic-Diagrammatic-Teaching"><a href="#Learning-from-Demonstration-via-Probabilistic-Diagrammatic-Teaching" class="headerlink" title="Learning from Demonstration via Probabilistic Diagrammatic Teaching"></a>Learning from Demonstration via Probabilistic Diagrammatic Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03835">http://arxiv.org/abs/2309.03835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson</li>
<li>for: 这篇论文旨在探讨一种新的教学方法，即图文教学（Diagrammatic Teaching），该方法通过让用户在2D图像上绘制示例轨迹，然后通过抽象模型来生成新的运动轨迹。</li>
<li>methods: 该论文提出了一种名为ray-tracing probabilistic trajectory learning（RPTL）的框架，该框架可以从用户绘制的2D示例轨迹中提取时间变化的概率密度，然后通过抽象模型来预测新的运动轨迹。</li>
<li>results: 论文通过在模拟和真实机器人上进行实验，证明了图文教学和RPTL框架的有效性。<details>
<summary>Abstract</summary>
Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions in 3D Cartesian space, and fits a probabilistic model of motion trajectories to these regions. New motion trajectories, which mimic those sketched by the user, can then be generated from the probabilistic model. We empirically validate our framework both in simulation and on real robots, which include a fixed-base manipulator and a quadruped-mounted manipulator.
</details>
<details>
<summary>摘要</summary>
学习示例（LfD）允许机器人学习新技能，通过模仿专家示范，让用户通过直观的方式表达指令。现代LfD的进步 часто靠近身体教学或远程操作作为用户指示示范的媒介。身体教学需要机器人的物理操作，而远程操作需要更多的硬件掌握。本文介绍一种代替LfD的新方法，即图解教学。图解教学的目标是通过让用户在2D场景图上绘制示范轨迹，并将其 sinthez为3D任务空间的生成模型。此外，我们还提出了基于投影图学习的投影概率轨迹学习（RPTL）框架。RPTL从2D图上提取时间变化的概率密度，使用投影图将其映射到3D坐标系中，并适应一个概率动作轨迹模型。通过这个模型，可以生成与用户绘制的示范轨迹类似的新动作轨迹。我们在实验中 validate了我们的框架，包括在模拟环境中和真实的机器人上进行了实验，其中包括一个固定基 manipulator和一个四脚支承 manipulate manipulator。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Drift-in-Textual-Data-An-Unsupervised-Method-for-Detecting-and-Mitigating-Drift-in-Machine-Learning-Models"><a href="#Uncovering-Drift-in-Textual-Data-An-Unsupervised-Method-for-Detecting-and-Mitigating-Drift-in-Machine-Learning-Models" class="headerlink" title="Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models"></a>Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03831">http://arxiv.org/abs/2309.03831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Khaki, Akhouri Abhinav Aditya, Zohar Karnin, Lan Ma, Olivia Pan, Samarth Marudheri Chandrashekar</li>
<li>for: 本研究旨在提出一种无监督式抖散检测方法，以优化机器学习模型的性能和在线客户体验质量指标。</li>
<li>methods: 本方法采用了两步进行抖散检测。第一步是将生产数据编码为目标分布，模型训练数据为参照分布。第二步是使用核函数基本统计测试，通过最大均值差距（MMD）距离度量，比较参照和目标分布的差异，并估计抖散程度。</li>
<li>results: 研究表明，使用该方法可以快速和精准地检测抖散，并且可以特定生产数据带来高抖散的问题。重新训练使用这些标识出的高抖散样本的模型，可以提高在线客户体验质量指标。<details>
<summary>Abstract</summary>
Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production data that is the root cause of the drift. The models retrained using these identified high drift samples show improved performance on online customer experience quality metrics.
</details>
<details>
<summary>摘要</summary>
机器学习中的漂移指的是数据或上下文的统计性质随着时间的变化，导致模型表现下降。因此，为了保证机器学习模型的表现不断改进，需要进行持续监测。然而，supervised drift检测方法需要人工标注，因此可能会延迟检测和缓解漂移的时间。我们提出的无监测漂移检测方法采用两步进程。在第一步中，我们将生产数据编码为目标分布，而模型训练数据则编码为参考分布。在第二步中，我们使用kernel-based统计测试，利用最大均值差距（MMD）距离度量来比较参考和目标分布的差异，并估计任何可能的漂移。此外，我们还可以确定生产数据中具有高漂移样本的子集，并使用这些样本重新训练模型，以改进在线客户体验质量指标。
</details></li>
</ul>
<hr>
<h2 id="ArtHDR-Net-Perceptually-Realistic-and-Accurate-HDR-Content-Creation"><a href="#ArtHDR-Net-Perceptually-Realistic-and-Accurate-HDR-Content-Creation" class="headerlink" title="ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation"></a>ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03827">http://arxiv.org/abs/2309.03827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishav Bakul Barua, Ganesh Krishnasamy, KokSheik Wong, Kalin Stefanov, Abhinav Dhall</li>
<li>for: 本研究旨在填补现有方法不充分考虑人类视觉特点的缺陷，通过基于Convolutional Neural Network的Architecture来实现高动态范围内容创建。</li>
<li>methods: 本研究使用多张曝光低动态范围图像（LDR）特征作为输入，通过提出ArtHDR-Net架构来实现高动态范围内容创建。</li>
<li>results: 实验结果表明，ArtHDR-Net可以在HDR-VDP-2分数（即意见评分指数）中达到状态 искусственный知识领域的前iersperformancedegree，并且在PSNR和SSIM指标中达到竞争性的水平。<details>
<summary>Abstract</summary>
High Dynamic Range (HDR) content creation has become an important topic for modern media and entertainment sectors, gaming and Augmented/Virtual Reality industries. Many methods have been proposed to recreate the HDR counterparts of input Low Dynamic Range (LDR) images/videos given a single exposure or multi-exposure LDRs. The state-of-the-art methods focus primarily on the preservation of the reconstruction's structural similarity and the pixel-wise accuracy. However, these conventional approaches do not emphasize preserving the artistic intent of the images in terms of human visual perception, which is an essential element in media, entertainment and gaming. In this paper, we attempt to study and fill this gap. We propose an architecture called ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR features as input. Experimental results show that ArtHDR-Net can achieve state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean opinion score index) while reaching competitive performance in terms of PSNR and SSIM.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）内容创作已成为现代媒体和娱乐领域的重要话题，游戏和虚拟/增强现实领域。许多方法已经被提出来重创造LDR图像/视频的HDR对应图像，其中大多数方法主要关注保持重建图像的结构相似性和像素精度。然而，这些传统方法不强调保持图像的艺术目标，即人类视觉的感知，这是媒体、娱乐和游戏领域的重要元素。在这篇论文中，我们尝试了研究并填补这个空白。我们提出了一种 Architecture called ArtHDR-Net，基于卷积神经网络，使用多 exposure LDR特征作为输入。实验结果表明，ArtHDR-Net 可以 дости到现状体顶峰性表现（HDR-VDP-2 分数），并且在 PSNR 和 SSIM 方面达到竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="Prime-and-Modulate-Learning-Generation-of-forward-models-with-signed-back-propagation-and-environmental-cues"><a href="#Prime-and-Modulate-Learning-Generation-of-forward-models-with-signed-back-propagation-and-environmental-cues" class="headerlink" title="Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues"></a>Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03825">http://arxiv.org/abs/2309.03825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sama Daryanavard, Bernd Porr</li>
<li>for: 这篇论文是关于深度神经网络学习的一种新方法，它采用 Prime and Modulate 模式，通过利用错误信号的正负指示和环境反馈来优化学习。</li>
<li>methods: 这篇论文使用的方法是 Prime and Modulate 模式，它利用错误信号的正负指示和环境反馈来优化学习。</li>
<li>results: 论文中的实验结果显示，使用 Prime and Modulate 模式可以提高学习速度，比 conventinal back-propagation 更快。<details>
<summary>Abstract</summary>
Deep neural networks employing error back-propagation for learning can suffer from exploding and vanishing gradient problems. Numerous solutions have been proposed such as normalisation techniques or limiting activation functions to linear rectifying units. In this work we follow a different approach which is particularly applicable to closed-loop learning of forward models where back-propagation makes exclusive use of the sign of the error signal to prime the learning, whilst a global relevance signal modulates the rate of learning. This is inspired by the interaction between local plasticity and a global neuromodulation. For example, whilst driving on an empty road, one can allow for slow step-wise optimisation of actions, whereas, at a busy junction, an error must be corrected at once. Hence, the error is the priming signal and the intensity of the experience is a modulating factor in the weight change. The advantages of this Prime and Modulate paradigm is twofold: it is free from normalisation and it makes use of relevant cues from the environment to enrich the learning. We present a mathematical derivation of the learning rule in z-space and demonstrate the real-time performance with a robotic platform. The results show a significant improvement in the speed of convergence compared to that of the conventional back-propagation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Training-Acceleration-of-Low-Rank-Decomposed-Networks-using-Sequential-Freezing-and-Rank-Quantization"><a href="#Training-Acceleration-of-Low-Rank-Decomposed-Networks-using-Sequential-Freezing-and-Rank-Quantization" class="headerlink" title="Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization"></a>Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03824">http://arxiv.org/abs/2309.03824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Habib Hajimolahoseini, Walid Ahmed, Yang Liu</li>
<li>for: 压缩深度学习模型，提高训练和执行速度，不需要采用小rank值</li>
<li>methods: 提议两种加速低级别分解模型的方法：rank优化和顺序冻结分解层</li>
<li>results: 实验结果显示，这两种方法可以在训练和执行过程中提高模型的 Throughput 达到 60% 和 37%，保持模型精度接近原始模型。<details>
<summary>Abstract</summary>
Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the original models
</details>
<details>
<summary>摘要</summary>
低阶减解（LRD）是一种深度学习模型压缩技术，用于减少训练参数和计算复杂性。然而，由于LRD应用后新增的层数较多，可能无法实现高度的训练/推理加速。问题在于，使用小rank增加风险性质下降精度。在本文中，我们提出了两种加速低阶减解模型的技术，不需要使用小rank进行减解。这两种方法包括阶数优化和顺序冻结减解层。我们在卷积和转换器基于模型上进行了实验，实验结果显示，这两种方法可以在训练和推理过程中提高模型 Throughput 达60%和37%，保持精度与原始模型几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Risk-Minimization-for-Losses-without-Variance"><a href="#Empirical-Risk-Minimization-for-Losses-without-Variance" class="headerlink" title="Empirical Risk Minimization for Losses without Variance"></a>Empirical Risk Minimization for Losses without Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03818">http://arxiv.org/abs/2309.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanhua Fang, Ping Li, Gennady Samorodnitsky</li>
<li>for: 本文考虑了一个empirical risk minimization问题，在重 tailed设定下进行研究，数据只有$p$-th moment，$p \in (1,2)$。</li>
<li>methods: 作者使用了Catoni的方法（Catoni, 2012）来 robustly estimate risk values，并使用通用的Generic Chaining方法来提供补偿 bound。</li>
<li>results: 作者通过 theoretically investigating two types of optimization methods，robust gradient descent algorithm和empirical risk-based methods，并进行了广泛的数据研究，发现empirical risk via Catoni-style estimation的优化器实际上表现更好，这表明直接基于剪辑数据进行估计可能会得到不atisfactory的结果。<details>
<summary>Abstract</summary>
This paper considers an empirical risk minimization problem under heavy-tailed settings, where data does not have finite variance, but only has $p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based on truncated observed data, we choose the optimizer by minimizing the risk value. Those risk values can be robustly estimated via using the remarkable Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type influence functions, we are able to establish excess risk upper bounds via using generalized generic chaining methods. Moreover, we take computational issues into consideration. We especially theoretically investigate two types of optimization methods, robust gradient descent algorithm and empirical risk-based methods. With an extensive numerical study, we find that the optimizer based on empirical risks via Catoni-style estimation indeed shows better performance than other baselines. It indicates that estimation directly based on truncated data may lead to unsatisfactory results.
</details>
<details>
<summary>摘要</summary>
这个论文考虑了具有重 tailed 设定的 empirical risk minimization 问题，数据并不具有有限 variance，仅仅具有 $p $-th moment，其中 $p \in (1,2) $。而不是使用 truncated 观察数据的估计过程，我们选择了使用 risk 值来选择优化器。这些 risk 值可以通过 Catoni 方法 (Catoni, 2012) 进行 robust 估计。由于 Catoni-type influence functions 的结构，我们可以通过 generalized generic chaining methods 来确定剩余风险Upper bound。此外，我们还考虑了计算问题。我们特别是使用 robust gradient descent algorithm 和 empirical risk-based methods 进行了理论研究。经过广泛的数值研究，我们发现了基于 empirical risks via Catoni-style estimation 的优化器实际上比其他基准更好的性能。这表明，直接基于 truncated data 进行估计可能会导致不满足的结果。
</details></li>
</ul>
<hr>
<h2 id="AnthroNet-Conditional-Generation-of-Humans-via-Anthropometrics"><a href="#AnthroNet-Conditional-Generation-of-Humans-via-Anthropometrics" class="headerlink" title="AnthroNet: Conditional Generation of Humans via Anthropometrics"></a>AnthroNet: Conditional Generation of Humans via Anthropometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03812">http://arxiv.org/abs/2309.03812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/AnthroNet">https://github.com/Unity-Technologies/AnthroNet</a></li>
<li>paper_authors: Francesco Picetti, Shrinath Deshpande, Jonathan Leban, Soroosh Shahtalebi, Jay Patel, Peifeng Jing, Chunpu Wang, Charles Metze III, Cameron Sun, Cera Laidlaw, James Warren, Kathy Huynh, River Page, Jonathan Hogins, Adam Crespi, Sujoy Ganguly, Salehe Erfanian Ebadi</li>
<li>for: 这个论文是用于开发一种可以生成各种人体形态和姿势的人体模型的。</li>
<li>methods: 这个模型使用了一种深度生成架构，通过直接模拟特定的人体特征来生成人体模型，并且可以在任意的姿势下生成人体。此外，这个模型还使用了一个非常多样化的动画库，以便增加模型训练的学习前提。</li>
<li>results: 这个模型通过使用100000个生成的姿势人体 mesh和其相应的人体测量数据进行训练，可以生成高度准确的人体模型，同时也可以对人体进行精准的测量。此外，这个模型还可以生成数百万个独特的人体形态和姿势，用于非商业学术研究用途。<details>
<summary>Abstract</summary>
We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic research purposes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的人体模型，基于广泛的人体中心的测量，可以生成多种人体形态和姿势。我们的模型可以直接模拟特定的人类标识，通过深度生成架构，可以在任意姿势中生成人类。这是首先使用只有生成的数据进行端到端训练的人体模型，不仅提供了高度准确的人体网格表示，还允许精确的人体人体测量。此外，我们使用了高度多样化的动画库，以 maximizelearnable prior的多样性，并将我们的人体和手部动作拟合到最大化多样性。我们的模型在100,000个生成的姿势人体网格和其相应的人体测量数据集上进行了端到端训练。我们的 sintetic数据生成器可以生成 millions of 独特的人类标识和姿势，供非商业学术研究使用。
</details></li>
</ul>
<hr>
<h2 id="Improved-theoretical-guarantee-for-rank-aggregation-via-spectral-method"><a href="#Improved-theoretical-guarantee-for-rank-aggregation-via-spectral-method" class="headerlink" title="Improved theoretical guarantee for rank aggregation via spectral method"></a>Improved theoretical guarantee for rank aggregation via spectral method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03808">http://arxiv.org/abs/2309.03808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziliang Samuel Zhong, Shuyang Ling</li>
<li>for:  Ranking items based on pairwise comparisons, with applications in sports, recommendation systems, and other web applications.</li>
<li>methods:  Spectral ranking algorithms based on unnormalized and normalized data matrices, with a focus on deriving entry-wise perturbation error bounds and an error bound on the maximum displacement for each item.</li>
<li>results:  Improved sample complexity and theoretical analysis of the eigenvectors and error bounds for the spectral ranking algorithms, with confirmed numerical experiments.<details>
<summary>Abstract</summary>
Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an error bound on the maximum displacement for each item, with only $\Omega(n\log n)$ samples. Our theoretical analysis improves upon the state-of-the-art results in terms of sample complexity, and our numerical experiments confirm these theoretical findings.
</details>
<details>
<summary>摘要</summary>
In the ERO model, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart.Using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an error bound on the maximum displacement for each item. Our theoretical analysis improves upon the state-of-the-art results in terms of sample complexity, and our numerical experiments confirm these theoretical findings.Here is the text in Simplified Chinese:给定多个对比评估中的多个项目，如何将它们排名，以便与观察数据匹配呢？这个问题，称为排名聚合，在体育、推荐系统和其他网络应用中有广泛的应用。然而，找到一个全局排名，以iminimize the mismatch（known as the Kemeny optimization）是NP-hard的，因此我们将关注 Erdős-Rényi outliers（ERO）模型。在这个模型中，每个对比评估是对真实分数差的损害版本。我们 investigate spectral ranking algorithms based on unnormalized and normalized data matrices。关键在于理解它们在 recuperate the underlying scores of each item from the observed data。这将reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart。使用留一个出 techniques，我们提供了一个更加精细的 $\ell_{\infty}$-norm perturbation bound of the eigenvectors，并也 derive an error bound on the maximum displacement for each item。我们的理论分析超过了现状最佳结果，并且我们的数值实验也证明了这些理论发现。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Frontiers-in-Neural-Feature-Learning-Data-Compute-Width-and-Luck"><a href="#Pareto-Frontiers-in-Neural-Feature-Learning-Data-Compute-Width-and-Luck" class="headerlink" title="Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck"></a>Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03800">http://arxiv.org/abs/2309.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang</li>
<li>for: 这项研究探讨了深度学习中计算统计温馈的细则设计选择。</li>
<li>methods: 该文件开始考虑了离线稀疏偏好学习，这是一个supervised分类问题，可以得到Gradient-based Training的多层感知器的统计查询下限。这个下限可以被解释为多种资源贸易前ier：成功学习只能在一个具有足够财富（大型模型）、知识（大量数据）、耐心（多个训练轮）或幸运（多个随机猜测）的情况下进行。</li>
<li>results: 我们经过理论和实验研究发现，离线稀疏初始化和增加网络宽度可以在这种设定下提供显著的样本效率提升。在这种情况下，网络宽度扮演了平行搜索的角色：它增加了找到”彩礼卷”神经元的概率，这些神经元更有效地学习稀疏特征。最后，我们表明了离线稀疏parity任务可以作为实际需要轴对映射学习的问题的代理。我们在表格类型分类 benchmark 上使用了宽度大、稀疏初始化的 MLP 模型，这些网络有时会超越调整后的随机森林。<details>
<summary>Abstract</summary>
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real problems requiring axis-aligned feature learning. We demonstrate improved sample efficiency on tabular classification benchmarks by using wide, sparsely-initialized MLP models; these networks sometimes outperform tuned random forests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Conformal-Autoregressive-Generation-Beam-Search-with-Coverage-Guarantees"><a href="#Conformal-Autoregressive-Generation-Beam-Search-with-Coverage-Guarantees" class="headerlink" title="Conformal Autoregressive Generation: Beam Search with Coverage Guarantees"></a>Conformal Autoregressive Generation: Beam Search with Coverage Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03797">http://arxiv.org/abs/2309.03797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Deutschmann, Marvin Alberts, María Rodríguez Martínez</li>
<li>for: 这 paper 是为了提出两种基于某规遵循性预测（CP）的搜索算法扩展，以生成具有理论保证的序列集。</li>
<li>methods: 这两种方法都基于 CP 算法，第一种方法是使用动态化大小的子集搜索结果，而不同于传统 CP 方法，它们的最高保证度取决于后续调整度量。第二种方法则在解码过程中引入了对准集预测过程，生成一个可变宽度的搜索结果，这个过程可以根据先前设置的保证度来选择。</li>
<li>results: 作者们提供了每种方法的边缘保证 bound，并对其进行了empirical评估，包括自然语言处理和化学等领域的任务。<details>
<summary>Abstract</summary>
We introduce two new extensions to the beam search algorithm based on conformal predictions (CP) to produce sets of sequences with theoretical coverage guarantees. The first method is very simple and proposes dynamically-sized subsets of beam search results but, unlike typical CP procedures, has an upper bound on the achievable guarantee depending on a post-hoc calibration measure. Our second algorithm introduces the conformal set prediction procedure as part of the decoding process, producing a variable beam width which adapts to the current uncertainty. While more complex, this procedure can achieve coverage guarantees selected a priori. We provide marginal coverage bounds for each method, and evaluate them empirically on a selection of tasks drawing from natural language processing and chemistry.
</details>
<details>
<summary>摘要</summary>
我们介绍两种基于对准预测（CP）的粒子搜寻算法扩展，以生成具有理论覆盖保证的序列集。第一种方法非常简单，它在粒子搜寻结果中 dynamically 选择子集，但与通常的 CP 程序不同的是，它具有属于后置检测量的最大保证上限。我们的第二个算法则是在解oding过程中引入对准集预测程序，它可以随着目前的不确定程度而变化粒子幅频率。虽然更加复杂，但这个程序可以根据先前选择的保证 bounds 来生成覆盖保证。我们提供每个方法的margin coverage bounds，并对其进行实验评估，包括自然语言处理和化学领域的任务。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Deep-Learning-with-Optimal-Transport-Regularized-Divergences"><a href="#Adversarially-Robust-Deep-Learning-with-Optimal-Transport-Regularized-Divergences" class="headerlink" title="Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences"></a>Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03791">http://arxiv.org/abs/2309.03791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremiah Birrell, Mohammadreza Ebrahimi</li>
<li>for: 这 paper 的目的是提出一种基于最优运输regularized divergence的方法，以提高深度学习模型的抗击势能力。</li>
<li>methods: 这 paper 使用了一种基于信息差 divergence 和最优运输成本的新类优化方法，以提高 adversarial robustness。</li>
<li>results: 这 paper 的实验结果显示， compared to 先前的方法， $ARMOR_D$ 方法在 MNIST 数据集上的抗击势能力达到了 $98.29%$ 和 $98.18%$  против $FGSM$ 和 $PGD^{40}$ 攻击，减少了误差率超过 $19.7%$ 和 $37.2%$。在恶意软件检测中，$ARMOR_D$ 方法也提高了对 $rFGSM^{50}$ 攻击的抗击势能力，相比先前的最佳 adversarial training 方法，提高了 $37.0%$。同时，它还降低了 false negative 和 false positive 率。<details>
<summary>Abstract</summary>
We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducing the error rate by more than $19.7\%$ and $37.2\%$ respectively compared to prior methods. Similarly, in malware detection, a discrete (binary) data domain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attack compared to the previous best-performing adversarial training methods by $37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and $57.53\%$, respectively.
</details>
<details>
<summary>摘要</summary>
我们引入 $ARMOR_D$ 方法作为深度学习模型中强化敌方攻击性能的新方法。这些方法基于一种新的最佳运输规定化分散度，通过infimal扩展（infimal convolution）between信息分散和最佳运输（OT）成本。我们使用这些工具来强化敌方攻击性能，通过 Maximizing the expected loss over a neighborhood of distributions， known as distributionally robust optimization。 viewed as a tool for constructing adversarial samples，我们的方法可以将样本transported according to the OT cost，并且重新权重 according to the information divergence。我们在这篇文章中证明了 $ARMOR_D$ 的有效性，在这些应用中它可以与现有的方法相比，对于敌方攻击性能来说，提高Robustified accuracy。在 MNIST dataset 上，$ARMOR_D$ 可以实现 Robustified accuracy 的提高， Specifically, it yields a Robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ against $PGD^{40}$. 相比于先前的方法，这些结果表明 $ARMOR_D$ 可以降低错误率超过 $19.7\%$ 和 $37.2\%$。在恶意软件检测中，一个给定（二进制）数据领域的 $ARMOR_D$ 可以在 $rFGSM^{50}$ 攻击下，与先前最佳的对抗式训练方法相比，提高 Robustified accuracy 37.0%，同时降低错误率和假阳性率的值。
</details></li>
</ul>
<hr>
<h2 id="CPU-frequency-scheduling-of-real-time-applications-on-embedded-devices-with-temporal-encoding-based-deep-reinforcement-learning"><a href="#CPU-frequency-scheduling-of-real-time-applications-on-embedded-devices-with-temporal-encoding-based-deep-reinforcement-learning" class="headerlink" title="CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning"></a>CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03779">http://arxiv.org/abs/2309.03779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/coladog/tinyagent">https://github.com/coladog/tinyagent</a></li>
<li>paper_authors: Ti Zhou, Man Lin</li>
<li>for: 这种研究旨在开发高效的电源管理方法，用于 periodic 任务在小型设备上进行执行。</li>
<li>methods: 研究者首先研究了小型设备中现有 Linux 内置的方法的局限性，并描述了三种困难以管理的工作负荷&#x2F;系统模式。然后，他们开发了基于强化学习的技术，使用时间编码， derivate 一个高效的 DVFS govemor。这个govemor只使用了一个性能计数器，与内置的 Linux 机制相同，并不需要明确的任务模型。</li>
<li>results: 研究者实现了一个基于 Nvidia Jetson Nano 板的原型系统，并在六个应用程序上进行了测试，包括两个自定义应用程序和四个标准应用程序。在不同的时间限制下，该方法可以快速 derivate 一个适应性能要求的 DVFS govemor，并在能源节约方面超越内置的 Linux 方法。在 Mibench 工作负荷上，与性能余地从 0.04 s 到 0.4 s 不同，提出的方法可以节省 3% - 11% 更多的能源。AudioReg 和 FaceReg 应用程序测试显示了 5% - 14% 的能源节约改进。研究者已经开源了其在内核中的量化神经网络引擎代码库，可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/coladog/tinyagent%E3%80%82">https://github.com/coladog/tinyagent。</a><details>
<summary>Abstract</summary>
Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our approach can quickly derive a DVFS governor that can adapt to performance requirements and outperform the built-in Linux approach in energy saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4 s, the proposed method can save 3% - 11% more energy compared to Ondemand. AudioReg and FaceReg applications tested have 5%- 14% energy-saving improvement. We have open-sourced the implementation of our in-kernel quantized neural network engine. The codebase can be found at: https://github.com/coladog/tinyagent.
</details>
<details>
<summary>摘要</summary>
小型设备常用于物联网和智能城市应用程序， periodic 执行固定任务， 具有软 Deadline。 这项工作关注于开发高效的电源管理方法，以便在小型设备上进行 periodic 任务。 我们首先研究现有的 Linux 内置方法，以及它们在小型设备中的局限性。 我们描述了三种常见的工作负荷/系统模式，这些模式使得 Linux 的内置解决方案不够有效。 我们开发了基于 reinforcement learning 的技术，使用 temporal 编码，以 derivation 高效的 DVFS GOVERNOR。 我们的方法使用同 Linux 内置机制一样的一个性能计数器，不需要明确的任务模型。 我们实现了一个原型系统，并在 Nvidia Jetson Nano Board 上进行了测试，使用了六个应用程序，包括两个自定义的和四个 referential 应用程序。 在不同的截止时间限制下，我们的方法可以快速 derivation 一个适应性能需求的 DVFS GOVERNOR，并且在能源节约方面超过 Linux 内置方法。 在 Mibench 工作负荷下，在性能拥有弹性范围内（0.04 s - 0.4 s），我们的方法可以节省 3% - 11% 更多的能源。 AudioReg 和 FaceReg 应用程序的能源节约提升为 5% - 14%。 我们已经开源了我们的内核 quantized neural network 引擎的实现代码，代码库可以在以下链接中找到：https://github.com/coladog/tinyagent。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Safety-Concerns-in-Automated-Driving-Perception"><a href="#Deep-Learning-Safety-Concerns-in-Automated-Driving-Perception" class="headerlink" title="Deep Learning Safety Concerns in Automated Driving Perception"></a>Deep Learning Safety Concerns in Automated Driving Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03774">http://arxiv.org/abs/2309.03774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Abrecht, Alexander Hirsch, Shervin Raafatnia, Matthias Woehrle</li>
<li>for: 这篇论文的目的是为了提高自动驾驶系统中的深度学习 Component 的安全性，并且提出了一个系统性和完整的方法来处理安全性 Concerns。</li>
<li>methods: 这篇论文使用了一种称为“安全 Concerns”的概念，它是根据 existing standards 和 academic publications 的反馈，以提高自动驾驶系统中的安全性。</li>
<li>results: 这篇论文发现了一个更加精确和完整的方法来处理安全 Concerns，并且引入了一个新的分类方法来帮助标准化和跨功能团队共同处理这些 Concerns。<details>
<summary>Abstract</summary>
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.   In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.   While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In particular, this paper introduces an additional categorization for a better understanding as well as enabling cross-functional teams to jointly address the concerns.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning and the impressive performance of deep neural networks (DNNs) for perception have led to an increased demand for their use in automated driving (AD) systems. However, the safety of such systems is of utmost importance, and thus it is necessary to consider the unique properties of DNNs. To achieve safety in AD systems with DNN-based perception components, so-called safety concerns have been introduced as a suitable structuring element. This concept is well aligned with existing standards relevant to the safety of AD systems, such as ISO 21448 (SOTIF), and has inspired several academic publications and upcoming standards on AI safety, such as ISO PAS 8800. While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In particular, this paper introduces an additional categorization for a better understanding, as well as enabling cross-functional teams to jointly address the concerns.
</details></li>
</ul>
<hr>
<h2 id="Neural-lasso-a-unifying-approach-of-lasso-and-neural-networks"><a href="#Neural-lasso-a-unifying-approach-of-lasso-and-neural-networks" class="headerlink" title="Neural lasso: a unifying approach of lasso and neural networks"></a>Neural lasso: a unifying approach of lasso and neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03770">http://arxiv.org/abs/2309.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Delgado, Ernesto Curbelo, Danae Carreras</li>
<li>for: 本文旨在演示如何将统计技术与机器学习技术结合，以获得两者之间的优点。</li>
<li>methods: 本文使用隐藏 нейрон网络来表示逻辑法，并发现了这两种方法在优化方面存在差异。特别是，机器学习方法通常在单步使用单个验证集进行优化，而统计方法则使用两步优化方法，基于交叉验证。统计方法的更复杂的优化结果在小训练集时得到更 precis的参数估计。</li>
<li>results: 经过实验，使用新的优化算法可以更好地识别重要变量，并且在synthetic和实际数据集上达到更高的性能。<details>
<summary>Abstract</summary>
In recent years, there is a growing interest in combining techniques attributed to the areas of Statistics and Machine Learning in order to obtain the benefits of both approaches. In this article, the statistical technique lasso for variable selection is represented through a neural network. It is observed that, although both the statistical approach and its neural version have the same objective function, they differ due to their optimization. In particular, the neural version is usually optimized in one-step using a single validation set, while the statistical counterpart uses a two-step optimization based on cross-validation. The more elaborated optimization of the statistical method results in more accurate parameter estimation, especially when the training set is small. For this reason, a modification of the standard approach for training neural networks, that mimics the statistical framework, is proposed. During the development of the above modification, a new optimization algorithm for identifying the significant variables emerged. Experimental results, using synthetic and real data sets, show that this new optimization algorithm achieves better performance than any of the three previous optimization approaches.
</details>
<details>
<summary>摘要</summary>
近年来，人们对将统计学和机器学习技术结合使用的兴趣日益增长。本文通过神经网络表示统计技术lasso的变量选择。研究发现，尽管这两种方法的目标函数都相同，但它们在优化方面有所不同。特别是，神经网络版本通常在一步使用单个验证集进行优化，而统计方法则使用两步优化基于交叉验证。统计方法的更复杂的优化导致更准确的参数估计，特别是当训练集较小时。为此，一种基于统计框架的修改方法 для训练神经网络被提出。在该修改方法的开发过程中，一种新的变量选择优化算法被发现。实验结果，使用 sintetic和实际数据集，显示这种新的优化算法在三种先前的优化方法中表现更好。
</details></li>
</ul>
<hr>
<h2 id="M-otion-mode-Based-Prediction-of-Ejection-Fraction-using-Echocardiograms"><a href="#M-otion-mode-Based-Prediction-of-Ejection-Fraction-using-Echocardiograms" class="headerlink" title="M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms"></a>M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03759">http://arxiv.org/abs/2309.03759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thomassutter/mmodeecho">https://github.com/thomassutter/mmodeecho</a></li>
<li>paper_authors: Ece Ozkan, Thomas M. Sutter, Yurong Hu, Sebastian Balzer, Julia E. Vogt</li>
<li>for: 早期检测心脏功能异常，通过常规检查是诊断心血管疾病的关键。</li>
<li>methods: 使用M模式电子镜图进行心脏功能评估，并利用自适应模型拟合扩展对数据进行学习。</li>
<li>results: 使用M模式电子镜图进行心脏功能评估，可以准确地评估心脏功能，并且在有限数据场景下可以减少人工干预。<details>
<summary>Abstract</summary>
Early detection of cardiac dysfunction through routine screening is vital for diagnosing cardiovascular diseases. An important metric of cardiac function is the left ventricular ejection fraction (EF), where lower EF is associated with cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology, with ultrasound being a low-cost, real-time, and non-ionizing technology. However, human assessment of echocardiograms for calculating EF is time-consuming and expertise-demanding, raising the need for an automated approach. In this work, we propose using the M(otion)-mode of echocardiograms for estimating the EF and classifying cardiomyopathy. We generate multiple artificial M-mode images from a single echocardiogram and combine them using off-the-shelf model architectures. Additionally, we extend contrastive learning (CL) to cardiac imaging to learn meaningful representations from exploiting structures in unlabeled data allowing the model to achieve high accuracy, even with limited annotations. Our experiments show that the supervised setting converges with only ten modes and is comparable to the baseline method while bypassing its cumbersome training process and being computationally much more efficient. Furthermore, CL using M-mode images is helpful for limited data scenarios, such as having labels for only 200 patients, which is common in medical applications.
</details>
<details>
<summary>摘要</summary>
早期发现心脏功能不全通过常规检查是诊断心血管疾病的关键。一个重要的心脏功能指标是左心室泵出率（EF），其下降与心肌疾病有关。电子心室graph是卡диологи中广泛使用的诊断工具，它是一种低成本、实时、非辐射技术。然而，人工评估电子心室гра征用于计算EF是时间consuming和专业需求很高，这有需要自动化方法。在这种工作中，我们提议使用M（运动）模式电子心室гра来估计EF和诊断心肌疾病。我们生成多个人工M模式图像从单个电子心室гра，并将它们组合使用现有模型建筑。此外，我们扩展了对比学习（CL）到心脏成像领域，以学习具有意义的表示从未标注的数据中获得有用信息，使模型可以在有限的标注下达到高精度。我们的实验表明，在监督设置下，只需要使用十个模式即可与基eline方法相当，并且可以快速地 converges。此外，使用M模式图像进行CL可以在医疗应用中有限的数据情况下提供帮助，例如只有200名患者的标注。
</details></li>
</ul>
<hr>
<h2 id="TSGBench-Time-Series-Generation-Benchmark"><a href="#TSGBench-Time-Series-Generation-Benchmark" class="headerlink" title="TSGBench: Time Series Generation Benchmark"></a>TSGBench: Time Series Generation Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03755">http://arxiv.org/abs/2309.03755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Ang, Qiang Huang, Yifan Bao, Anthony K. H. Tung, Zhiyong Huang<br>for:* 这 paper 是为了提供一个统一的评估方法来评估时间序列生成（TSG）方法的性能。methods:* 这 paper 使用了一个名为 \textsf{TSGBench} 的 TSG benchmark，它包括三个模块：一个公共数据集的收集、一个标准化的预处理管道，以及一个涵盖多种评估指标的评估集。results:* 该 paper 通过对十种先进 TSG 方法在十个不同领域的实验来评估 \textsf{TSGBench} 的可靠性和一致性。结果显示，\textsf{TSGBench} 能够准确地评估 TSG 方法的性能，并且能够提供细致的方法评估结果。<details>
<summary>Abstract</summary>
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.   To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measures, new distance-based assessments, and visualization tools; (3) a pioneering generalization test rooted in Domain Adaptation (DA), compatible with all methods. We have conducted extensive experiments across ten real-world datasets from diverse domains, utilizing ten advanced TSG methods and twelve evaluation measures, all gauged through \textsf{TSGBench}. The results highlight its remarkable efficacy and consistency. More importantly, \textsf{TSGBench} delivers a statistical breakdown of method rankings, illuminating performance variations across different datasets and measures, and offering nuanced insights into the effectiveness of each method.
</details>
<details>
<summary>摘要</summary>
“人工时间系列生成（TSG）在许多应用中是重要的，包括数据增强、异常探测和隐私保护。尽管这个领域已经做出了重要的进步，但现有的方法具有三个关键的限制：（1）它们经常对相似的模型类型进行评估，从而阻碍了全面的性能可能性。（2）使用特殊的人工生成和私人数据集 introduces 偏见和降低了普遍化。（3）不确定的评估标准，常常与自定义网络或下游任务相关，导致了不一致和公平的比较。”“为了突破这些限制，我们引入了 \textsf{TSGBench}，TSG Benchmark 的首个版本，旨在提供一个统一和完整的 TSG 方法评估平台。它包括三个模组：（1）一个精心选择的公共可用数据集，配备标准化的预处理管线；（2）一个完整的评估标准组合，包括净量测试、新的距离基准和视觉工具；（3）一个开创性的适应领域（DA）基础，可以与所有方法相容。我们在十个真实世界数据集上进行了广泛的实验，使用了十个进步的 TSG 方法和十二个评估标准，全部通过 \textsf{TSGBench} 进行评估。结果显示了它的很好的效果和一致性。”“更重要的是，\textsf{TSGBench} 提供了一个统计分析的方法排名，透过不同的数据集和评估标准进行分析，将方法的性能差异显示出来，并提供了深入的探究每个方法的效能。”
</details></li>
</ul>
<hr>
<h2 id="Convergence-Analysis-of-Decentralized-ASGD"><a href="#Convergence-Analysis-of-Decentralized-ASGD" class="headerlink" title="Convergence Analysis of Decentralized ASGD"></a>Convergence Analysis of Decentralized ASGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03754">http://arxiv.org/abs/2309.03754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro DL Tosi, Martin Theobald</li>
<li>for: 这种研究旨在提高 Stochastic Gradient Descent (SGD) 的训练效率，以便在多个设备上进行分布式训练。</li>
<li>methods: 该研究使用的方法包括 asynchronous SGD (ASGD) 和 decentralized asynchronous SGD (DASGD)，以及一种新的 convergencerate 分析方法。</li>
<li>results: 研究结果表明，DASGD 的 convergencerate 为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3&#x2F;2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$，当 gradients 不受限制时， convergencerate 为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}<em>{avg}\hat{S}</em>{max}\epsilon^{-1})$。这些结果适用于不变、 homogeneous 和 L-smooth 目标函数。<details>
<summary>Abstract</summary>
Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.   In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$ for the convergence rate of DASGD, where $S_{avg}$ is the average staleness between models, $Q$ is a constant that bounds the norm of the gradients, and $\epsilon$ is a (small) error that is allowed within the bound. Furthermore, when gradients are not bounded, we prove the convergence rate of DASGD to be $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}\epsilon^{-1})$, with $\hat{S}_{max}$ and $\hat{S}_{avg}$ representing a loose version of the average and maximum staleness, respectively. Our convergence proof holds for a fixed stepsize and any non-convex, homogeneous, and L-smooth objective function. We anticipate that our results will be of high relevance for the adoption of DASGD by a broad community of researchers and developers.
</details>
<details>
<summary>摘要</summary>
最近几十年，随机梯度下降（SGD）在机器学习社区中被广泛研究。尽管它具有优秀的灵活性和性能，但是通过SGD来优化大型模型仍然是一个时间consuming的任务。为了减少训练时间，常常将训练过程分布到多个设备上。近些年来，人们发现了异步SGD（ASGD）的速度会比小批量SGD更快。然而，大多数ASGD的速度分析仍然基于中央参数服务器，这会导致训练过程中的瓶颈。在这篇论文中，我们提出了一种新的异步SGD（DASGD）的速度分析，不需要在节点之间进行部分同步，也不需要固定的网络拓扑。我们提供了一个速度 bound的式子： $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$, 其中 $S_{avg}$ 是模型之间的均值偏差， $Q$ 是约束 gradients 的常数，而 $\epsilon$ 是允许的误差。而当 gradients 不受限制时，我们证明 DASGD 的速度为 $\mathcal{O}(\sigma\epsilon^{-2}) + \mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}\epsilon^{-1})$, 其中 $\hat{S}_{max}$ 和 $\hat{S}_{avg}$ 分别表示模型之间的平均和最大偏差。我们的速度证明在任何非 convex、同质、L-smooth 目标函数上都适用，并且适用于固定步长。我们预计这些结果将对广泛的研究人员和开发人员产生很高的 relevance。
</details></li>
</ul>
<hr>
<h2 id="Medoid-Silhouette-clustering-with-automatic-cluster-number-selection"><a href="#Medoid-Silhouette-clustering-with-automatic-cluster-number-selection" class="headerlink" title="Medoid Silhouette clustering with automatic cluster number selection"></a>Medoid Silhouette clustering with automatic cluster number selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03751">http://arxiv.org/abs/2309.03751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>for: 这篇论文主要是关于聚类评估的一种方法和其效率分析。</li>
<li>methods: 该论文使用了一种基于emedoid的Silhouette方法，并提供了两种快速版本，其中一种可以保证与原始方法相同的结果，并且提供了一个$O(k^2)$的运行速度提升。</li>
<li>results: 在实际数据集上，使用这种方法可以获得10464倍的速度提升，并且提供了一种直接选择最佳聚类数量的方法。<details>
<summary>Abstract</summary>
The evaluation of clustering results is difficult, highly dependent on the evaluated data set and the perspective of the beholder. There are many different clustering quality measures, which try to provide a general measure to validate clustering results. A very popular measure is the Silhouette. We discuss the efficient medoid-based variant of the Silhouette, perform a theoretical analysis of its properties, provide two fast versions for the direct optimization, and discuss the use to choose the optimal number of clusters. We combine ideas from the original Silhouette with the well-known PAM algorithm and its latest improvements FasterPAM. One of the versions guarantees equal results to the original variant and provides a run speedup of $O(k^2)$. In experiments on real data with 30000 samples and $k$=100, we observed a 10464$\times$ speedup compared to the original PAMMEDSIL algorithm. Additionally, we provide a variant to choose the optimal number of clusters directly.
</details>
<details>
<summary>摘要</summary>
“评估嵌套结果的评估是很困难的，很多因素会影响，包括评估数据集和观察者的角度。有许多不同的嵌套质量指标，尝试提供一个通用的验证嵌套结果的方法。一个非常受欢迎的指标是Silhouette。我们讨论了内部中心基于Silhouette的快速版本，进行了理论分析其性能，提供了两个快速版本以直接优化，并讨论了选择最佳嵌套数量的方法。我们结合了原始Silhouette与Well-known PAM算法以及其最新改进FasterPAM。其中一个版本保证与原始版本相同的结果，并提供了$O(k^2)$的执行速度提升。在实际数据上，我们观察到30000样本和$k$=100的情况下，实现了10464倍的执行速度提升，相比原始PAMMEDSIL算法。此外，我们还提供了选择最佳嵌套数量的方法。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Pipeline-Based-Conversational-Agents-with-Large-Language-Models"><a href="#Enhancing-Pipeline-Based-Conversational-Agents-with-Large-Language-Models" class="headerlink" title="Enhancing Pipeline-Based Conversational Agents with Large Language Models"></a>Enhancing Pipeline-Based Conversational Agents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03748">http://arxiv.org/abs/2309.03748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mina Foosherian, Hendrik Purwins, Purna Rathnayake, Touhidul Alam, Rui Teimao, Klaus-Dieter Thoben</li>
<li>for: 这篇论文探讨了基于大语言模型（LLM）的自然语言处理（NLP）系统在设计和开发阶段以及运行阶段中的应用可能性。</li>
<li>methods: 论文采用了基于GPT-4的LLM技术，在两个阶段中使用LLM来增强pipeline-based conversational agent的能力。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和人物设计等。在运行阶段，LLM可以帮助 Contextualization、意图类型分类、避免对话崩溃和处理离cope问题、自动修改语句、重新译答案、准确问题解释和关闭问题回答等。</li>
<li>results: 通过在私人银行领域使用GPT-4进行了一些非正式实验，证明了LLM在提高pipeline-based conversational agent的能力方面的潜力。但是，由于隐私问题和现有系统集成的需求，公司可能会拒绝完全取代pipeline-based agents with LLMs。因此， authors提出了一种混合方法，即将LLMs integrate into pipeline-based agents，以保留现有系统集成和隐私保护的优势，同时充分利用LLMs的能力来提高 conversational agents的能力。<details>
<summary>Abstract</summary>
The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs' are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.
</details>
<details>
<summary>摘要</summary>
最新的人工智能和深度学习技术突破了基于大语言模型（LLM）的代理人，如GPT-4。然而，许多商业对话代理人开发工具是管道式的，它们有限制能够保持人类化的对话。这篇论文研究了 LLM 能够在两个阶段增强管道式对话代理人：1）在设计和开发阶段，LLM 可以帮助生成训练数据、提取实体和同义词、本地化和人物设计。2）在运行阶段，LLM 可以帮助 contextualization、意图分类以避免对话崩溃和处理 OUT-OF-SCOPE 问题、自动修正语句、重新表述响应、制定歧义问题、总结和启用关闭问答功能。我们在私人银行领域使用 GPT-4 进行了一些不正式的实验，以示出上述情况。由于隐私问题和现有系统集成的需求，公司可能会尝试保留其管道式代理人并尝试将 LLM  integrate into其中，以节省建立和运行代理人的时间和成本，同时利用 LLM 的能力而无需折衣现有系统的隐私保护和集成。
</details></li>
</ul>
<hr>
<h2 id="Learning-continuous-valued-treatment-effects-through-representation-balancing"><a href="#Learning-continuous-valued-treatment-effects-through-representation-balancing" class="headerlink" title="Learning continuous-valued treatment effects through representation balancing"></a>Learning continuous-valued treatment effects through representation balancing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03731">http://arxiv.org/abs/2309.03731</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/christopher-br/cbrnet">https://github.com/christopher-br/cbrnet</a></li>
<li>paper_authors: Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, Wouter Verbeke</li>
<li>for: 估计对象的治疗效果，即“剂量响应”，在各种领域都具有重要性，从医疗到商业、经济和以上等。</li>
<li>methods: 我们提出了一种名为CBRNet的 causal机器学习方法，用于从观察数据中估计个体剂量响应。CBRNet采用了Neyman-Rubin potential outcome框架，并将权衡表示学习扩展到连续valued treatments中。我们的工作是首次在连续valued treatment Setting中应用表示均衡。</li>
<li>results: 我们的实验表明，CBRNet可以准确地估计受 Selection bias的治疗效果，并与其他现有的方法相比，具有竞争性的性能。<details>
<summary>Abstract</summary>
Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-valued treatment setting. We evaluate our method on a newly proposed benchmark. Our experiments demonstrate CBRNet's ability to accurately learn treatment effects under selection bias and competitive performance with respect to other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose CBRNet, a causal machine learning approach that estimates individual dose responses from observational data. CBRNet is based on the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning to overcome selection bias for continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-valued treatment setting.We evaluate CBRNet on a newly proposed benchmark and demonstrate its ability to accurately learn treatment effects under selection bias, as well as competitive performance compared to other state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="A-Causal-Perspective-on-Loan-Pricing-Investigating-the-Impacts-of-Selection-Bias-on-Identifying-Bid-Response-Functions"><a href="#A-Causal-Perspective-on-Loan-Pricing-Investigating-the-Impacts-of-Selection-Bias-on-Identifying-Bid-Response-Functions" class="headerlink" title="A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions"></a>A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03730">http://arxiv.org/abs/2309.03730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Bockel-Rickermann, Sam Verboven, Tim Verdonck, Wouter Verbeke</li>
<li>for:  This paper aims to address the problem of selection bias in personalized pricing policies, which is not well-studied in the existing pricing literature.</li>
<li>methods:  The authors use causal inference and experimental methods to simulate varying levels of selection bias on a semi-synthetic dataset of mortgage loan applications in Belgium. They compare the performance of conventional methods such as logistic regression and neural networks with state-of-the-art methods from causal machine learning.</li>
<li>results:  The authors show that conventional methods are adversely affected by selection bias, while state-of-the-art methods from causal machine learning are capable of overcoming this bias and providing more accurate estimates of individual bid-response functions.<details>
<summary>Abstract</summary>
In lending, where prices are specific to both customers and products, having a well-functioning personalized pricing policy in place is essential to effective business making. Typically, such a policy must be derived from observational data, which introduces several challenges. While the problem of ``endogeneity'' is prominently studied in the established pricing literature, the problem of selection bias (or, more precisely, bid selection bias) is not. We take a step towards understanding the effects of selection bias by posing pricing as a problem of causal inference. Specifically, we consider the reaction of a customer to price a treatment effect. In our experiments, we simulate varying levels of selection bias on a semi-synthetic dataset on mortgage loan applications in Belgium. We investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions. Our results illustrate how conventional methods such as logistic regression and neural networks suffer adversely from selection bias. In contrast, we implement state-of-the-art methods from causal machine learning and show their capability to overcome selection bias in pricing data.
</details>
<details>
<summary>摘要</summary>
在贷款业务中，因为价格对特定客户和产品都具有特定的价格，有效的个性化价格策略是非常重要。通常，这种策略需要基于观察数据来 derivation，这会引入一些挑战。而“内生性”这个问题在已有的价格理论中已经得到了广泛的研究，但是“拍卖选择偏见”这个问题却没有得到过 suficient 的关注。我们通过视为价格推断问题来解决这个问题，specifically, we consider the reaction of a customer to price as a treatment effect.在我们的实验中，我们使用semi-synthetic dataset on Belgium mortgage loan applications来 simulate varying levels of selection bias。我们 investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions.我们的结果表明， conventional methods such as logistic regression and neural networks will suffer from selection bias.然而，我们实现了当前的 causal machine learning 方法，并证明它们可以在价格数据中超越选择偏见。
</details></li>
</ul>
<hr>
<h2 id="A-Natural-Gas-Consumption-Forecasting-System-for-Continual-Learning-Scenarios-based-on-Hoeffding-Trees-with-Change-Point-Detection-Mechanism"><a href="#A-Natural-Gas-Consumption-Forecasting-System-for-Continual-Learning-Scenarios-based-on-Hoeffding-Trees-with-Change-Point-Detection-Mechanism" class="headerlink" title="A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism"></a>A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03720">http://arxiv.org/abs/2309.03720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rasvob/hoeffding-trees-with-cpd-multistep-forecasing">https://github.com/rasvob/hoeffding-trees-with-cpd-multistep-forecasing</a></li>
<li>paper_authors: Radek Svoboda, Sebastian Basterrech, Jędrzej Kozal, Jan Platoš, Michał Woźniak</li>
<li>for: 预测天然气消耗，考虑季节性和趋势，是精细规划天然气供应和消耗的关键。在供应威胁时，这也是一个关键因素，确保社会能够充足地供应能源。</li>
<li>methods: 本文提出了一种新的多步式预测天然气消耗方法，integrating change point detection，用于数据流处理。该方法可以在实际应用中进行适应性学习。</li>
<li>results: 根据提出的方法，我们在天然气消耗预测中实现了更高的预测精度。此外，我们还发现使用变化点检测可以选择不同的模型集，并且使用预测错误反馈Loop可以更好地适应不同的预测场景。<details>
<summary>Abstract</summary>
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model collection for successive time frames. Thus, three model collection selection procedures (with and without an error feedback loop) are defined and evaluated for forecasting scenarios with various densities of detected change points. These models were compared with change point agnostic baseline approaches. Our experiments show that fewer change points result in a lower forecasting error regardless of the model collection selection procedure employed. Also, simpler model collection selection procedures omitting forecasting error feedback leads to more robust forecasting models suitable for continual learning tasks.
</details>
<details>
<summary>摘要</summary>
预测天然气消耗，考虑季节性和趋势，是至关重要的，以便规划其供应和消耗，同时优化其获取成本，主要由工业实体进行。然而，在供应威胁时，也是一个关键元素，确保提供这种原料，以满足个人消耗需求，保障社会能源安全。本文介绍了一种新的多步预测天然气消耗，并 integrate change point detection 的方法，以便在数据流处理中实现连续学习。我们使用了 Hoeffding 树预测器和 Pruned Exact Linear Time (PELT) 算法来检测变点。通过将不同的模型集选择与时间帧相关联，我们定义了三种模型集选择过程（包括和不包括错误反馈循环），并对各种变点密度的预测场景进行评估。这些模型与无变点背景下的基准方法进行比较。我们的实验表明， fewer change points 对预测错误都有负面影响，无论使用哪种模型集选择过程。此外，简单的模型集选择过程，即不包括预测错误反馈，可以生成更加稳定的预测模型，适用于连续学习任务。
</details></li>
</ul>
<hr>
<h2 id="A-State-Representation-for-Diminishing-Rewards"><a href="#A-State-Representation-for-Diminishing-Rewards" class="headerlink" title="A State Representation for Diminishing Rewards"></a>A State Representation for Diminishing Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03710">http://arxiv.org/abs/2309.03710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Moskovitz, Samo Hromadka, Ahmed Touati, Diana Borsa, Maneesh Sahani</li>
<li>for: 本研究旨在解决多任务学习中agent快速适应不同预期的资源分配问题。</li>
<li>methods: 本文提出了一种新的状态表示法，称为λ表示法（λR），该表示法可以快速评估策略，并且在自然世界中，任务之间的依赖关系更加复杂的情况下具有优势。</li>
<li>results: 本文证明了λ表示法的正式性质，并在机器学习和自然行为研究中展示了其normative优势和实用性。<details>
<summary>Abstract</summary>
A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\lambda$ representation ($\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\lambda$R's formal properties and examine its normative advantages in the context of machine learning, as well as its usefulness for studying natural behaviors, particularly foraging.
</details>
<details>
<summary>摘要</summary>
一般情况下，多任务学习（RL）中的agent需要快速适应不同的静态奖励函数的Random sampling。在这种情况下，Successor representation（SR）是一种广泛使用的框架，它通过分离一个策略的预期折扣、累积状态占用与特定奖励函数解耦来支持快速策略评估。然而，在自然世界中，继任任务很少是独立的，而是受到奖励刺激的可用性和主观感受的变化。基于这种分歧，在这篇论文中，我们研究了减少的偏好效应和引入了一种新的状态表示方法，λ表示（λR），这种表示方法 surprisingly 需要 для策略评估，并且总结了λR的正式性质和在机器学习中的normative优点，以及在研究自然行为，特别是搜寻行为中的使用价值。
</details></li>
</ul>
<hr>
<h2 id="Chat-Failures-and-Troubles-Reasons-and-Solutions"><a href="#Chat-Failures-and-Troubles-Reasons-and-Solutions" class="headerlink" title="Chat Failures and Troubles: Reasons and Solutions"></a>Chat Failures and Troubles: Reasons and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03708">http://arxiv.org/abs/2309.03708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian</li>
<li>for: 本研究探讨了人机交互（HRI）中常见的问题，导致聊天失败和问题。</li>
<li>methods: 本研究使用了适当的机器人和聊天模型，识别常见的问题导致失败，识别可能的解决方案，并规划了不断改进。</li>
<li>results: 研究建议使用封闭控制算法，使用训练过的人工智能（AI）预训练模型，提供词汇筛选、批处理新数据集重新训练、在数据流上学习和&#x2F;或使用奖励学习模型自动更新训练模型，以减少错误。<details>
<summary>Abstract</summary>
This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了人机合作（HRI）在聊天中出现的一些常见问题，导致失败和困难。这些设计决策包括适用的机器人、适用的对话模型、识别常见问题的原因、识别可能的解决方案、并规划持续改进。在结论中，建议使用封闭控制算法，使用已经训练的人工智能（AI）预训练模型，提供词汇筛选、批处理新数据集、在数据流中学习、以及使用强化学习模型自动更新训练模型，以降低错误。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Semi-Supervised-Approach-with-Triplet-Markov-Chains"><a href="#A-Probabilistic-Semi-Supervised-Approach-with-Triplet-Markov-Chains" class="headerlink" title="A Probabilistic Semi-Supervised Approach with Triplet Markov Chains"></a>A Probabilistic Semi-Supervised Approach with Triplet Markov Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03707">http://arxiv.org/abs/2309.03707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katherine Morales, Yohan Petetin</li>
<li>for: 这篇论文旨在提出一个基于条件随机统计的 semi-supervised 泛化模型，用于类别Sequential data。</li>
<li>methods: 本论文提出了一个基于条件随机统计的Variational Bayesian inference方法，用于在半指定情况下训练受条件随机统计模型。</li>
<li>results: 本论文的结果显示，使用该方法可以在半指定情况下训练出高品质的类别模型，并且可以应对不同类型的Sequential data。<details>
<summary>Abstract</summary>
Triplet Markov chains are general generative models for sequential data which take into account three kinds of random variables: (noisy) observations, their associated discrete labels and latent variables which aim at strengthening the distribution of the observations and their associated labels. However, in practice, we do not have at our disposal all the labels associated to the observations to estimate the parameters of such models. In this paper, we propose a general framework based on a variational Bayesian inference to train parameterized triplet Markov chain models in a semi-supervised context. The generality of our approach enables us to derive semi-supervised algorithms for a variety of generative models for sequential Bayesian classification.
</details>
<details>
<summary>摘要</summary>
三重马尔可夫链是一种泛化生成模型，用于处理序列数据，它考虑了三种随机变量：受损的观察值、其关联的整数标签和隐藏变量，这些隐藏变量的目的是强化观察值和其关联的标签的分布。然而，在实践中，我们通常不 possess所有标签与观察值的对应关系，以便 estimate triplet Markov chain 模型的参数。在这篇论文中，我们提出了一种基于变分 bayesian 推理的通用框架，用于在半supervised情况下训练参数化的 triplet Markov chain 模型。我们的方法的通用性允许我们 derivate 半supervised 算法 для多种序列 Bayesian 分类模型。
</details></li>
</ul>
<hr>
<h2 id="DiffDefense-Defending-against-Adversarial-Attacks-via-Diffusion-Models"><a href="#DiffDefense-Defending-against-Adversarial-Attacks-via-Diffusion-Models" class="headerlink" title="DiffDefense: Defending against Adversarial Attacks via Diffusion Models"></a>DiffDefense: Defending against Adversarial Attacks via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03702">http://arxiv.org/abs/2309.03702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hondamunigeprasannasilva/diffdefence">https://github.com/hondamunigeprasannasilva/diffdefence</a></li>
<li>paper_authors: Hondamunige Prasanna Silva, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 这个论文是为了防御机器学习分类器免受敏感攻击而设计的。</li>
<li>methods: 这个论文使用了扩散模型来保护机器学习分类器，而不需要对分类器本身进行任何修改。</li>
<li>results: 论文表明，这种方法可以提供对敏感攻击的鲁棒防御，同时保持清晰精度和快速响应，并且可以与现有的分类器集成使用。Here’s the English version of the paper’s abstract for reference:”This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility.”<details>
<summary>Abstract</summary>
This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility. Code at: https://github.com/HondamunigePrasannaSilva/DiffDefence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-Term-Load-Forecasting-Using-A-Particle-Swarm-Optimized-Multi-Head-Attention-Augmented-CNN-LSTM-Network"><a href="#Short-Term-Load-Forecasting-Using-A-Particle-Swarm-Optimized-Multi-Head-Attention-Augmented-CNN-LSTM-Network" class="headerlink" title="Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network"></a>Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03694">http://arxiv.org/abs/2309.03694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paapa Kwesi Quansah</li>
<li>for: 短期负载预测是电力系统的重要任务，因为它的非线性和动态性导致预测的问题。</li>
<li>methods: 我们的方法使用Particle-Swarm Optimization算法自动探索和优化参数，Multi-Head Attention机制确定关键特征，并实现了 Computational Efficiency。</li>
<li>results: 我们的方法在使用真实的电力需求数据进行评估，其精度、稳定性和计算效率都较前一代方法有明显提升。特别是，我们的 Mean Absolute Percentage Error 为 1.9376，与现有的方法相比，表示了一个新的时代在短期负载预测方面。<details>
<summary>Abstract</summary>
Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 marks a significant advancement over existing state-of-the-art approaches, heralding a new era in short-term load forecasting.
</details>
<details>
<summary>摘要</summary>
短期荷压预测对电力系统的有效运行和规划具有极高的重要性，因为它的自然非线性和动态特性。Recent advances in deep learning have shown promise in addressing this challenge. However, these methods often struggle with hyperparameter sensitivity, lack of interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that overcomes these challenges. Our approach leverages the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to identify the critical features for accurate forecasting, and a streamlined framework for computational efficiency. Our method is rigorously evaluated using a real electricity demand dataset. The results demonstrate its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 represents a significant improvement over existing state-of-the-art approaches, marking a new era in short-term load forecasting.Here's the word-for-word translation of the text into Simplified Chinese:短期荷压预测对电力系统的有效运行和规划具有极高的重要性，因为它的自然非线性和动态特性。Recent advances in deep learning have shown promise in addressing this challenge. However, these methods often struggle with hyperparameter sensitivity, lack of interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that overcomes these challenges. Our approach leverages the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to identify the critical features for accurate forecasting, and a streamlined framework for computational efficiency. Our method is rigorously evaluated using a real electricity demand dataset. The results demonstrate its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error of 1.9376 represents a significant improvement over existing state-of-the-art approaches, marking a new era in short-term load forecasting.
</details></li>
</ul>
<hr>
<h2 id="A-computationally-lightweight-safe-learning-algorithm"><a href="#A-computationally-lightweight-safe-learning-algorithm" class="headerlink" title="A computationally lightweight safe learning algorithm"></a>A computationally lightweight safe learning algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03672">http://arxiv.org/abs/2309.03672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Baumann, Krzysztof Kowalczyk, Koen Tiels, Paweł Wachel</li>
<li>for: 提供一种安全学习算法，可以在训练过程中提供概率性安全保证，而不需要了解系统动态模型。</li>
<li>methods: 使用 Nadaraya-Watson 估计器，而不是 Gaussian processes，以实现概率性安全保证。 Nadaraya-Watson 估计器的扩展级别为对数级别，与数据点数量成正比。</li>
<li>results: 提供了一种 theoretically guaranteed 的安全学习算法，并通过数字七度自由机械手 manipulator 的数据进行了数值实验。<details>
<summary>Abstract</summary>
Safety is an essential asset when learning control policies for physical systems, as violating safety constraints during training can lead to expensive hardware damage. In response to this need, the field of safe learning has emerged with algorithms that can provide probabilistic safety guarantees without knowledge of the underlying system dynamics. Those algorithms often rely on Gaussian process inference. Unfortunately, Gaussian process inference scales cubically with the number of data points, limiting applicability to high-dimensional and embedded systems. In this paper, we propose a safe learning algorithm that provides probabilistic safety guarantees but leverages the Nadaraya-Watson estimator instead of Gaussian processes. For the Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of data points. We provide theoretical guarantees for the estimates, embed them into a safe learning algorithm, and show numerical experiments on a simulated seven-degrees-of-freedom robot manipulator.
</details>
<details>
<summary>摘要</summary>
安全是学习控制策略的重要资产，因为训练时违反安全限制可能会导致设备损坏。为解决这个需求，安全学派在训练时提供了一些可靠的安全保证。这些算法通常基于 Gaussian process 推理，但是 Gaussian process 推理的扩展缩放率是 cubic 的，因此只适用于高维和嵌入式系统。在这篇论文中，我们提出了一种安全学习算法，可以提供可靠的安全保证，但是使用 Nadaraya-Watson 估计器而不是 Gaussian processes。Nadaraya-Watson 估计器的扩展缩放率是 logarithmic 的，我们提供了理论保证，将其包含到安全学习算法中，并在一个模拟的七度自由机械 manipulate 器上进行了数值实验。
</details></li>
</ul>
<hr>
<h2 id="Dataset-Generation-and-Bonobo-Classification-from-Weakly-Labelled-Videos"><a href="#Dataset-Generation-and-Bonobo-Classification-from-Weakly-Labelled-Videos" class="headerlink" title="Dataset Generation and Bonobo Classification from Weakly Labelled Videos"></a>Dataset Generation and Bonobo Classification from Weakly Labelled Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03671">http://arxiv.org/abs/2309.03671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre-Etienne Martin</li>
<li>for: 这个论文旨在提出一个使用常用机器学习方法实现的大猩猩检测和分类管道，以便在没有人工干预的情况下，使用触摸屏设备测试大猩猩。</li>
<li>methods: 这篇论文使用了一种新采集的大猩猩记录数据集，并使用了蛇须探测器来在视频中空间地检测到各个个体。 使用了手工特征和不同的分类算法以及深度学习方法，包括ResNet体系，进行大猩猩识别。</li>
<li>results: 研究表明，经过意义性的数据分离后，使用精心调整的ResNet模型可以达到75%的准确率。<details>
<summary>Abstract</summary>
This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained using a fine-tuned ResNet model and reaches 75% of accuracy.
</details>
<details>
<summary>摘要</summary>
这份论文描述了一个使用常用机器学习方法建立的bonobo检测和分类管道。这种应用是由测试bonobos在围墙中使用触摸屏设备而不需要人类协助的需求所驱动。这个工作介绍了一个新收集的bonobo记录 dataset，该dataset是通过半自动方式生成的。记录是弱Label的，并且通过macaque检测器来在视频中空间检测个体。手工设计的特征与不同的分类算法和深度学习方法使用ResNet架构进行bonobo识别。performance是根据数据库的分辨率进行比较，并且我们示出了数据预处理的重要性，并如何一个错误的数据分离可能导致false好的结果。最后，我们使用精度调整的ResNet模型，达到了75%的准确率。
</details></li>
</ul>
<hr>
<h2 id="How-adversarial-attacks-can-disrupt-seemingly-stable-accurate-classifiers"><a href="#How-adversarial-attacks-can-disrupt-seemingly-stable-accurate-classifiers" class="headerlink" title="How adversarial attacks can disrupt seemingly stable accurate classifiers"></a>How adversarial attacks can disrupt seemingly stable accurate classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03665">http://arxiv.org/abs/2309.03665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver J. Sutton, Qinghua Zhou, Ivan Y. Tyukin, Alexander N. Gorban, Alexander Bastounis, Desmond J. Higham</li>
<li>for: 这篇论文主要针对了遭受敏感攻击的学习系统，以及这种攻击的特点和原因。</li>
<li>methods: 论文使用了一种简单的普适的框架，以解释实际系统中观察到的一些特点，如攻击性能强大且容易构造的攻击方法，以及对于高维输入数据的模型的敏感性。</li>
<li>results: 论文发现了一些重要的结论，包括：对于高维输入数据的模型，小的攻击可以轻松地破坏模型的准确性，而大的随机干扰无法触发模型的攻击性。此外，论文还发现了一种Counterintuitive的现象：即使模型在训练和测试数据上具有小的差异，也可能会隐藏攻击性的潜在问题。<details>
<summary>Abstract</summary>
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fails to trigger the adversarial instability of the network. A surprising takeaway is that even small margins separating a classifier's decision surface from training and testing data can hide adversarial susceptibility from being detected using randomly sampled perturbations. Counterintuitively, using additive noise during training or testing is therefore inefficient for eradicating or detecting adversarial examples, and more demanding adversarial training is required.
</details>
<details>
<summary>摘要</summary>
敌对攻击可以很快地改变一个正常的学习系统的输出，只需要对输入数据进行一些微小的修改。即使这些修改看起来无关紧要，也可以让学习系统失去精度。这种现象可能是因为分类器在处理高维输入数据时的一个基本特性。我们提出了一个简单的普适的框架，可以解释在实际系统中观察到的一些特性，例如易于构建的敌对攻击和对输入数据的随机干扰的抗性。我们证明了这些现象也直接出现在实际的神经网络中，即使大量的随机干扰也无法让神经网络失去精度。一个惊人的发现是，即使分类器的决策面和训练数据之间的差别很小，也可以隐藏敌对攻击的敏感性。因此，使用随机干扰来检测或消除敌对例子可能是不够fficient的，需要更加严格的敌对训练。
</details></li>
</ul>
<hr>
<h2 id="Alzheimer-Disease-Detection-from-Raman-Spectroscopy-of-the-Cerebrospinal-Fluid-via-Topological-Machine-Learning"><a href="#Alzheimer-Disease-Detection-from-Raman-Spectroscopy-of-the-Cerebrospinal-Fluid-via-Topological-Machine-Learning" class="headerlink" title="Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning"></a>Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03664">http://arxiv.org/abs/2309.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Conti, Martina Banchelli, Valentina Bessi, Cristina Cecchi, Fabrizio Chiti, Sara Colantonio, Cristiano D’Andrea, Marella de Angelis, Davide Moroni, Benedetta Nacmias, Maria Antonietta Pascali, Sandro Sorbi, Paolo Matteini</li>
<li>for: 这个研究用于判断阿尔茨海默病（AD）的诊断。</li>
<li>methods: 这个研究使用了拉曼谱（RS）技术和机器学习（ML）方法来分类AD和正常Control组的液体抗体样本。</li>
<li>results: 研究发现，使用Raw Spectra和预处理后的Spectra可以有效地分类AD和Control组，并且使用 topological 分析提高了分类精度（&gt;87%）。<details>
<summary>Abstract</summary>
The cerebrospinal fluid (CSF) of 19 subjects who received a clinical diagnosis of Alzheimer's disease (AD) as well as of 5 pathological controls have been collected and analysed by Raman spectroscopy (RS). We investigated whether the raw and preprocessed Raman spectra could be used to distinguish AD from controls. First, we applied standard Machine Learning (ML) methods obtaining unsatisfactory results. Then, we applied ML to a set of topological descriptors extracted from raw spectra, achieving a very good classification accuracy (>87%). Although our results are preliminary, they indicate that RS and topological analysis together may provide an effective combination to confirm or disprove a clinical diagnosis of AD. The next steps will include enlarging the dataset of CSF samples to validate the proposed method better and, possibly, to understand if topological data analysis could support the characterization of AD subtypes.
</details>
<details>
<summary>摘要</summary>
CSF液（cerebrospinal fluid）19名被诊断为阿尔ц海默病（Alzheimer's disease，AD）的病人和5名病理控制人的样本已经被收集并分析了使用拉曼光谱（Raman spectroscopy，RS）。我们研究了使用Raw和预处理的拉曼谱来分别AD和控制人。首先，我们使用标准机器学习（Machine Learning，ML）方法，但得到的结果并不满足。然后，我们使用ML方法对原始谱的 topological descriptor进行分析，达到了非常好的分类精度（>87%）。虽然我们的结果是初步的，但它们表明RS和 topological数据分析可能是确认或反对临床诊断AD的有效组合。接下来的步骤将包括扩大CSF样本集，以更好地验证我们的方法，并可能理解 topological data analysis是否可以支持AD的分型。
</details></li>
</ul>
<hr>
<h2 id="Towards-Comparable-Knowledge-Distillation-in-Semantic-Image-Segmentation"><a href="#Towards-Comparable-Knowledge-Distillation-in-Semantic-Image-Segmentation" class="headerlink" title="Towards Comparable Knowledge Distillation in Semantic Image Segmentation"></a>Towards Comparable Knowledge Distillation in Semantic Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03659">http://arxiv.org/abs/2309.03659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onno Niemann, Christopher Vox, Thorben Werner</li>
<li>for: 本研究旨在探讨知识塑造（KD）在Semantic Segmentation中的应用，以提高模型大小和推理速度。</li>
<li>methods: 本研究提出25种提出的塑化损失项，从14篇最新4年的论文中提取。然而，由于不同的训练配置，对 published results 进行比较是很困难的。为 illustration，使用相同的模型和数据集，Structural and Statistical Texture Distillation (SSTKD) 报告了学生mIoU提升4.54个和最终性能29.19，而 Adaptive Perspective Distillation (APD) 只提高学生性能2.06个百分点，但实现了最终性能39.25。这种极大的差异的原因通常是模型参数的不佳选择和引用点模型的下性能。</li>
<li>results: 我们发现，塑化过程中的hyperparameter tuning问题导致了塑化提升的消失。为了提高未来研究的比较性，我们建立了三个dataset和两个学生模型的坚实基线，并提供了广泛的hyperparameter tuning信息。我们发现，只有两种技术可以与我们的简单基线在ADE20K dataset上竞争。<details>
<summary>Abstract</summary>
Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter tuning by showing that distillation improvements of two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are optimized sufficiently. To improve comparability of future research in the field, we establish a solid baseline for three datasets and two student models and provide extensive information on hyperparameter tuning. We find that only two out of eight techniques can compete with our simple baseline on the ADE20K dataset.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）是一种提出的解决方案，以减少大型模型的存储空间和 semantic segmentation 的推理速度。在我们的研究中，我们发现了25种提出的distillation损失项，来自14篇过去4年发表的论文。 unfortunately，due to differences in training configurations, a comparison of these terms based on published results is often impossible. a good illustration of this problem is the comparison of two publications from 2022. using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. the reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. in our work, we reveal problems of insufficient hyperparameter tuning by showing that distillation improvements of two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are optimized sufficiently. to improve comparability of future research in the field, we establish a solid baseline for three datasets and two student models and provide extensive information on hyperparameter tuning. we find that only two out of eight techniques can compete with our simple baseline on the ADE20K dataset.
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Automatic-Audiobook-Creation"><a href="#Large-Scale-Automatic-Audiobook-Creation" class="headerlink" title="Large-Scale Automatic Audiobook Creation"></a>Large-Scale Automatic Audiobook Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03926">http://arxiv.org/abs/2309.03926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Walsh, Mark Hamilton, Greg Newby, Xi Wang, Serena Ruan, Sheng Zhao, Lei He, Shaofei Zhang, Eric Dettinger, William T. Freeman, Markus Weimer</li>
<li>for: 这个论文的目的是提供一种自动生成高质量的声音书，以提高文学作品的可访问性和读者参与度。</li>
<li>methods: 该论文使用了现代神经网络文本读取技术，将Project Gutenberg电子书库中的大量ebook内容自动转化为高质量声音书。具有自定义发音速度、式样和情感响应等功能。</li>
<li>results: 该论文通过自动生成大量的开源声音书，提供了一个交互式 demo，让用户快速创建自己的个性化声音书。总共提供了超过五千个开源声音书，可以在 \url{<a target="_blank" rel="noopener" href="https://aka.ms/audiobook%7D">https://aka.ms/audiobook}</a> 上听写。<details>
<summary>Abstract</summary>
An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. To listen to the audiobook collection visit \url{https://aka.ms/audiobook}.
</details>
<details>
<summary>摘要</summary>
Audiobooks可以大幅提高文学作品的可accessibility和读者参与度。然而，制作Audiobooks需要数百个工作小时的人工努力。在这项工作中，我们介绍了一个系统，可以自动生成高质量的Audiobooks从在线电子书。具体来说，我们利用了最新的神经网络文本读取技术，创建并发布了数量之多的人类质量、开源Audiobooks从Project Gutenberg电子书集。我们的方法可以确定电子书内容的合适子集，并可以同时处理数百本书。我们的系统允许用户自定义Audiobook的说速和风格，以及情感倾斜。甚至可以根据一小段示例音频来匹配愿意的声音。这项工作投入了五千个开源Audiobooks以及一个交互式demo，允许用户快速创建自己的个性化Audiobooks。要听取Audiobook集，请访问 \url{https://aka.ms/audiobook}.
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Lipschitz-Stability-of-GNN-for-Fairness"><a href="#Characterizing-Lipschitz-Stability-of-GNN-for-Fairness" class="headerlink" title="Characterizing Lipschitz Stability of GNN for Fairness"></a>Characterizing Lipschitz Stability of GNN for Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03648">http://arxiv.org/abs/2309.03648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaning Jia, Chunhui Zhang, Jundong Li, Chuxu Zhang</li>
<li>for: 这篇论文旨在研究图神经网络（GNNs）中的稳定性和公平性问题。</li>
<li>methods: 本论文使用 lipschitz bound 技术来限制 GNN 模型输出的变化，并对 GNN 模型在不同偏见下的表现进行了分析。</li>
<li>results: 研究发现， lipschitz bound 可以有效地限制 GNN 模型输出的偏见变化，并且可以帮助 GNN 模型更好地平衡准确性和公平性。<details>
<summary>Abstract</summary>
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant remains elusive for non-Euclidean neural networks like GNNs, especially within fairness contexts. To narrow this gap, we begin with the general GNNs operating on an attributed graph, and formulate a Lipschitz bound to limit the changes in the output regarding biases associated with the input. Additionally, we theoretically analyze how the Lipschitz constant of a GNN model could constrain the output perturbations induced by biases learned from data for fairness training. We experimentally validate the Lipschitz bound's effectiveness in limiting biases of the model output. Finally, from a training dynamics perspective, we demonstrate why the theoretical Lipschitz bound can effectively guide the GNN training to better trade-off between accuracy and fairness.
</details>
<details>
<summary>摘要</summary>
《利氏 bound》，一种由稳健统计学派导出的技术，可以限制输出对输入的变化，考虑到相关的无关不平等因素。它是一种有效和可证明的方法，可以在无需更多计算成本的情况下，检查机器学习模型的输出稳定性。最近，图 neural network（GNN），它们在非欧几何数据上运行，已经吸引了广泛的关注。然而，没有前期研究探讨过GNN利氏 bound，以推动模型输出稳定性的研究，特别是在非欧几何数据上，具有自然的偏见。由于常见的图数据在GNN训练中带有自然的偏见，这会对模型输出稳定性提出严重的挑战，以保持训练过程中的公正性。尽管利氏常数在控制欧几何神经网络稳定性方面已经被使用，但计算非欧几何神经网络利氏常数的精确值仍然是一个棘手的问题，特别是在公正性上下文中。为了bridging这个差距，我们开始使用通用的GNN在Attribute graph上运行，并形ulated一个利氏 bound来限制输出对输入偏见的变化。此外，我们还 theoretically analyzed how GNN模型的利氏常数可以控制因输入偏见而导致的模型输出偏见。我们通过实验验证了利氏 bound的有效性，限制模型输出偏见。最后，从训练动态角度来看，我们示出了理论上的利氏 bound可以有效地导向GNN训练，以更好地平衡准确性和公正性。
</details></li>
</ul>
<hr>
<h2 id="Insights-Into-the-Inner-Workings-of-Transformer-Models-for-Protein-Function-Prediction"><a href="#Insights-Into-the-Inner-Workings-of-Transformer-Models-for-Protein-Function-Prediction" class="headerlink" title="Insights Into the Inner Workings of Transformer Models for Protein Function Prediction"></a>Insights Into the Inner Workings of Transformer Models for Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03631">http://arxiv.org/abs/2309.03631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markuswenzel/xai-proteins">https://github.com/markuswenzel/xai-proteins</a></li>
<li>paper_authors: Markus Wenzel, Erik Grüner, Nils Strodthoff</li>
<li>for: 本研究探讨了使用可解释AI（XAI）来探讨神经网络在蛋白质功能预测中的内部工作机制，并将已有的XAI方法扩展以覆盖转换器模型中的隐藏表示。</li>
<li>methods: 本研究使用了已经广泛使用的XAI方法的整合梯度，以便检查转换器模型中的隐藏表示。</li>
<li>results: 该方法允许我们查看序列中特定的氨基酸哪些位置被转换器模型 particualr attention，并证明这些相关序列部分与生物和化学预期相匹配，包括在嵌入层和模型中的转换器头部。<details>
<summary>Abstract</summary>
Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .
</details>
<details>
<summary>摘要</summary>
Motivation: 我们研究了如何使用可解释AI（XAI）来探索神经网络内部工作的方式，通过扩展广泛使用XAI方法的集成梯度的扩展，以便可以 inspectTransformer模型中的隐藏表示。结果：我们能够确定序列中的氨基酸，并证明这些相关的序列部分与生物和化学预期相匹配，包括嵌入层和模型中的转移头。可用性和实现：源代码可以在https://github.com/markuswenzel/xai-proteins上获取。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Self-Supervised-Learning-of-Speech-Representation-via-Invariance-and-Redundancy-Reduction"><a href="#Understanding-Self-Supervised-Learning-of-Speech-Representation-via-Invariance-and-Redundancy-Reduction" class="headerlink" title="Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction"></a>Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03619">http://arxiv.org/abs/2309.03619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann</li>
<li>for:  investigate how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data</li>
<li>methods: propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks</li>
<li>results: MBT improves representation generalization over original BT, especially when fine-tuning with limited target data, highlighting the importance of designing objectives that encourage invariant and transferable representations.Here’s the same information in English:</li>
<li>for: investigate how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data</li>
<li>methods: propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks</li>
<li>results: MBT improves representation generalization over original BT, especially when fine-tuning with limited target data, highlighting the importance of designing objectives that encourage invariant and transferable representations.<details>
<summary>Abstract</summary>
The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.
</details>
<details>
<summary>摘要</summary>
“选择目标函数的选择是自动化学习中非常重要的，以获得高质量的表现。这篇论文探索了不同的巴洛兄弟（BT）目标函数形ulations对下游任务性能的影响，特别是对于语音数据。我们提出了修改后的巴洛兄弟（MBT）目标函数，将潜在值 норmalized，以强制scale-invariance，并评估了语音识别、性别识别和关键字搜寻任务。我们的结果显示MBT改进了表现泛化，特别是当 fine-tuning  WITH limited target data。这表明设计目标函数可以导致不对称和可转移的表现。我们的分析提供了关于如何使用 BT 学习目标函数设计语音表现，以便适应新的下游任务。这篇研究是发展可重用自动化学习语音表现的重要一步。”
</details></li>
</ul>
<hr>
<h2 id="Filtration-Surfaces-for-Dynamic-Graph-Classification"><a href="#Filtration-Surfaces-for-Dynamic-Graph-Classification" class="headerlink" title="Filtration Surfaces for Dynamic Graph Classification"></a>Filtration Surfaces for Dynamic Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03616">http://arxiv.org/abs/2309.03616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aidos-lab/filtration_surfaces">https://github.com/aidos-lab/filtration_surfaces</a></li>
<li>paper_authors: Franz Srambical, Bastian Rieck</li>
<li>for: 本文提出了一种新的方法来分类动态图，以解决现有方法的缺点，包括不可扩展性和缺少EdgeWeight信息。</li>
<li>methods: 本文提出了一种名为”滤波表面”的新方法，它是可扩展的和灵活的，并且可以处理变化的节点集和EdgeWeight信息。</li>
<li>results: 实验表明，filtration surfaces方法可以超过现有的基线，并且具有最低的总标准差。此外，本方法可以在不同的 EdgeWeight 情况下实现最佳性能。<details>
<summary>Abstract</summary>
Existing approaches for classifying dynamic graphs either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate said restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.
</details>
<details>
<summary>摘要</summary>
现有的方法对动态图进行分类 Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate these restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.Here's the breakdown of the translation:* 现有的方法 (existing approaches) -> 现有的方法 (existing methods)* 对动态图进行分类 (classifying dynamic graphs) -> 对动态图进行分类 (classifying dynamic graphs)* Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs) -> Either lift graph kernels to the temporal domain, or use graph neural networks (GNNs)* However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account -> However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account* We propose filtration surfaces, a novel method that is scalable and flexible -> We propose filtration surfaces, a novel method that is scalable and flexible* to alleviate these restrictions -> to alleviate these restrictions* We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information -> We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information* Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation -> Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation
</details></li>
</ul>
<hr>
<h2 id="Your-Battery-Is-a-Blast-Safeguarding-Against-Counterfeit-Batteries-with-Authentication"><a href="#Your-Battery-Is-a-Blast-Safeguarding-Against-Counterfeit-Batteries-with-Authentication" class="headerlink" title="Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication"></a>Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03607">http://arxiv.org/abs/2309.03607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhackiori/eisthentication">https://github.com/mhackiori/eisthentication</a></li>
<li>paper_authors: Francesco Marchiori, Mauro Conti</li>
<li>for: 提高锂离子电池身份验证技术，保证设备使用合法电池，确保设备的运行状况和用户的安全。</li>
<li>methods: 提出了两种新的锂离子电池身份验证方法，包括DCAuth和EISthentication，通过机器学习模型对电池内部特征进行自动验证，不需要外部设备，并能抵抗多种常见伪造技术。</li>
<li>results: 对20个数据集进行了分析，提取了 meaningful 特征，并实现了高精度的锂离子电池身份验证（最高达0.99）和模型识别（最高达0.96），同时也能够保持相同的标识性表现。<details>
<summary>Abstract</summary>
Lithium-ion (Li-ion) batteries are the primary power source in various applications due to their high energy and power density. Their market was estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread adoption of Li-ion batteries has resulted in counterfeit cell production, which can pose safety hazards to users. Counterfeit cells can cause explosions or fires, and their prevalence in the market makes it difficult for users to detect fake cells. Indeed, current battery authentication methods can be susceptible to advanced counterfeiting techniques and are often not adaptable to various cells and systems. In this paper, we improve the state of the art on battery authentication by proposing two novel methodologies, DCAuth and EISthentication, which leverage the internal characteristics of each cell through Machine Learning models. Our methods automatically authenticate lithium-ion battery models and architectures using data from their regular usage without the need for any external device. They are also resilient to the most common and critical counterfeit practices and can scale to several batteries and devices. To evaluate the effectiveness of our proposed methodologies, we analyze time-series data from a total of 20 datasets that we have processed to extract meaningful features for our analysis. Our methods achieve high accuracy in battery authentication for both architectures (up to 0.99) and models (up to 0.96). Moreover, our methods offer comparable identification performances. By using our proposed methodologies, manufacturers can ensure that devices only use legitimate batteries, guaranteeing the operational state of any system and safety measures for the users.
</details>
<details>
<summary>摘要</summary>
锂离子（Li-ion）电池是各种应用的主要电源，其 Market 在 2022 年估算为高达 48 亿美元。然而，广泛采用锂离子电池导致假电池生产，这可能会对用户造成安全隐患。假电池可能会引起爆炸或火灾，而其市场上的存在使用户检测假电池困难。实际上，当前的电池身份验证方法可能会受到先进的假造技术的影响，而且通常不适用于不同的电池和系统。在这篇论文中，我们改进了电池身份验证的状态艺，提出了两种新的方法，即 DCAuth 和 EISthentication，它们通过机器学习模型来利用每个电池的内部特征进行身份验证。我们的方法可以自动Authenticate 锂离子电池模型和体系，不需要任何外部设备，同时也能够抗性高于最常见和最critical的假电池实践。为了评估我们提出的方法的效果，我们分析了 20 个数据集中的时间序列数据，并将其处理以提取有用的特征。我们的方法在锂离子电池模型和体系上达到了高精度的身份验证效果（达到 0.99），同时也能够保持相似的标识性表现。通过使用我们的方法，制造商可以确保设备只使用合法电池，保证系统的操作状态和用户的安全措施。
</details></li>
</ul>
<hr>
<h2 id="Beyond-attention-deriving-biologically-interpretable-insights-from-weakly-supervised-multiple-instance-learning-models"><a href="#Beyond-attention-deriving-biologically-interpretable-insights-from-weakly-supervised-multiple-instance-learning-models" class="headerlink" title="Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models"></a>Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03925">http://arxiv.org/abs/2309.03925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Willem Bonnaffé, CRUK ICGC Prostate Group, Freddie Hamdy, Yang Hu, Ian Mills, Jens Rittscher, Clare Verrill, Dan J. Woodcock</li>
<li>for: 这项研究的目的是提高多例学习（MIL）中的解释性，以便更好地理解模型在数字生理学中的预测方式。</li>
<li>methods: 这项研究使用了一种post-training分析方法，包括生成精制encoder的 tile-level attention和预测分数，以生成预测权重地图（PAW），并通过与核体分割masks集成来提高解释性。</li>
<li>results: 研究发现，在诊断和评估患病程度时，高注意度区域并不一定与肿瘤区域重叠，这表明需要研究非肿瘤细胞，以便更好地评估肿瘤的发展。<details>
<summary>Abstract</summary>
Recent advances in attention-based multiple instance learning (MIL) have improved our insights into the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyse MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features related to the cellular organisation of the tissue and facilitates comparisons with known clinical features. We illustrate the utility of our approach by comparing PAW maps obtained for prostate cancer diagnosis (i.e. samples containing malignant tissue, 381/516 tissue samples) and prognosis (i.e. samples from patients with biochemical recurrence following surgery, 98/663 tissue samples) in a cohort of patients from the international cancer genome consortium (ICGC UK Prostate Group). Our approach reveals that regions that are predictive of adverse prognosis do not tend to co-locate with the tumour regions, indicating that non-cancer cells should also be studied when evaluating prognosis.
</details>
<details>
<summary>摘要</summary>
Recent advances in 注意力基于多个实例学习（MIL）have improved our understanding of the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyze MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features related to the cellular organization of the tissue and facilitates comparisons with known clinical features. We illustrate the utility of our approach by comparing PAW maps obtained for prostate cancer diagnosis (i.e. samples containing malignant tissue, 381/516 tissue samples) and prognosis (i.e. samples from patients with biochemical recurrence following surgery, 98/663 tissue samples) in a cohort of patients from the international cancer genome consortium (ICGC UK Prostate Group). Our approach reveals that regions that are predictive of adverse prognosis do not tend to co-locate with the tumour regions, indicating that non-cancer cells should also be studied when evaluating prognosis.
</details></li>
</ul>
<hr>
<h2 id="Interactive-Hyperparameter-Optimization-in-Multi-Objective-Problems-via-Preference-Learning"><a href="#Interactive-Hyperparameter-Optimization-in-Multi-Objective-Problems-via-Preference-Learning" class="headerlink" title="Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning"></a>Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03581">http://arxiv.org/abs/2309.03581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/automl/interactive-mo-ml">https://github.com/automl/interactive-mo-ml</a></li>
<li>paper_authors: Joseph Giovanelli, Alexander Tornede, Tanja Tornede, Marius Lindauer</li>
<li>for: 这个论文的目的是提出一种人类中心的交互式HPO方法，以优化多目标机器学习（MO-ML）问题。</li>
<li>methods: 该方法使用了参与学习来提取用户的愿景，以便在HPO过程中选择合适的评价指标。</li>
<li>results: 在一个环境影响ml的实验研究中，该方法在基于用户预先选择的指标优化hp中表现出色，并且在高级用户知道选择哪个指标时也能够达到相似的表现。<details>
<summary>Abstract</summary>
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learning to extract desiderata from users that guide the optimization. Instead of relying on the user guessing the most suitable indicator for their needs, our approach automatically learns an appropriate indicator. Concretely, we leverage pairwise comparisons of distinct Pareto fronts to learn such an appropriate quality indicator. Then, we optimize the hyperparameters of the underlying MO-ML algorithm towards this learned indicator using a state-of-the-art HPO approach. In an experimental study targeting the environmental impact of ML, we demonstrate that our approach leads to substantially better Pareto fronts compared to optimizing based on a wrong indicator pre-selected by the user, and performs comparable in the case of an advanced user knowing which indicator to pick.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种人类中心的交互式 HPO 方法，适应多目标 ML 的需求。而不是让用户自己猜测最适合他们需求的指标，我们的方法会自动学习一个合适的指标。具体来说，我们利用 Pareto 前纬度之间的对比来学习这个指标。然后，我们使用现有的 HPO 方法来优化对应的 MO-ML 算法的超参数，以达到这个学习的指标。在针对 Machine Learning 的环境影响的实验研究中，我们展示了我们的方法可以比使用错误的指标进行优化而获得substantially 更好的 Pareto 前纬度，并在用户知道选择哪个指标的情况下表现相对。
</details></li>
</ul>
<hr>
<h2 id="DTW-S-Shape-based-Comparison-of-Time-series-with-Ordered-Local-Trend"><a href="#DTW-S-Shape-based-Comparison-of-Time-series-with-Ordered-Local-Trend" class="headerlink" title="DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend"></a>DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03579">http://arxiv.org/abs/2309.03579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajitesh Srivastava</li>
<li>For: The paper is written for researchers in applied domains who work with time-series data and need to measure the similarity or distance between them, particularly in the context of epidemics.* Methods: The paper proposes a novel measure called DTW+S, which creates an interpretable matrix representation of time-series data and then applies Dynamic Time Warping to compute distances between the matrices.* Results: The paper demonstrates the utility of DTW+S in ensemble building and clustering of epidemic curves, and shows that it results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.<details>
<summary>Abstract</summary>
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in ensemble building and clustering of epidemic curves. We also demonstrate that our approach results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.
</details>
<details>
<summary>摘要</summary>
mesure distance ou similarity entre données de série temporelle est un aspect fondamental de nombreuses applications, notamment la classification et la clustering. Les mesures existantes peuvent ne pas capturer les similitudes en raison des tendances locales (formes) et peuvent même produire des résultats erronés. Notre objectif est de développer une mesure qui cherche des tendances similaires qui ont lieu à des moments similaires et qui est facile à interpréter pour les chercheurs dans les domaines appliqués. Cela est particulièremment utile pour les applications où les séries de temps ont une suite de courbes locales significatives qui sont ordonnées, comme les épidémies (une augmentation suivie d'une pointe suivie d'une baisse). Nous proposons une nouvelle mesure, DTW+S, qui crée une représentation "closeness-preserving" matrix de la série de temps, où chaque colonne représente les tendances locales, et puis applique l'alignement temporel dynamique pour calculer les distances entre ces matrices. Nous présentons une analyse théorique qui soutient le choix de cette représentation. Nous démontrons l'utilité de DTW+S dans la construction d'ensembles et la clustering de courbes d'épidémie. Nous également démontrons que notre approche produit des classements meilleurs que Dynamic Time Warping pour une classe de données, en particulier lorsque les tendances locales plutôt que la taille jouent un rôle décisif.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Federated-Training-of-Object-Detection-in-the-Internet-of-Vehicles"><a href="#Sparse-Federated-Training-of-Object-Detection-in-the-Internet-of-Vehicles" class="headerlink" title="Sparse Federated Training of Object Detection in the Internet of Vehicles"></a>Sparse Federated Training of Object Detection in the Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03569">http://arxiv.org/abs/2309.03569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luping Rao, Chuan Ma, Ming Ding, Yuwen Qian, Lu Zhou, Zhe Liu</li>
<li>for: 提高智能交通系统（ITS）中的物体检测精度，以提供实时有效的交通管理服务。</li>
<li>methods: 基于联邦学习框架，在中央服务器上共享良好地地方模型，并在边缘设备上进行笼统训练。</li>
<li>results: 实验结果表明，提议的方案可以达到需要的物体检测率，同时减少了大量的通信成本。<details>
<summary>Abstract</summary>
As an essential component part of the Intelligent Transportation System (ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic issues. Object detection is one of the key technologies in the IoV, which has been widely used to provide traffic management services by analyzing timely and sensitive vehicle-related information. However, the current object detection methods are mostly based on centralized deep training, that is, the sensitive data obtained by edge devices need to be uploaded to the server, which raises privacy concerns. To mitigate such privacy leakage, we first propose a federated learning-based framework, where well-trained local models are shared in the central server. However, since edge devices usually have limited computing power, plus a strict requirement of low latency in IoVs, we further propose a sparse training process on edge devices, which can effectively lighten the model, and ensure its training efficiency on edge devices, thereby reducing communication overheads. In addition, due to the diverse computing capabilities and dynamic environment, different sparsity rates are applied to edge devices. To further guarantee the performance, we propose, FedWeg, an improved aggregation scheme based on FedAvg, which is designed by the inverse ratio of sparsity rates. Experiments on the real-life dataset using YOLO show that the proposed scheme can achieve the required object detection rate while saving considerable communication costs.
</details>
<details>
<summary>摘要</summary>
作为智能交通系统（ITS）的重要组成部分，互联网器（IoV）在解决交通问题中发挥了关键作用。对象检测是IoV中的关键技术，通过实时和敏感的车辆相关信息的分析来提供交通管理服务。然而，现有的对象检测方法都基于中央深度训练，即从边缘设备获取的敏感数据需要上传到服务器，这会导致隐私泄露。为了缓解这种隐私泄露，我们首先提出了基于联邦学习的框架，其中Well-trained的本地模型在中央服务器中分享。然而，边缘设备通常具有有限的计算能力， plus IoV中的延迟要求很低，我们进一步提议使用缺省训练过程在边缘设备上，可以有效减轻模型，并在边缘设备上提高训练效率，从而减少通信开销。此外，由于边缘设备的多样 computing 能力和动态环境，我们采用不同的缺省率应用于边缘设备。为了进一步保证性能，我们提出了FedWeg，一种基于FedAvg的改进聚合方案，其中借鉴缺省率进行反比分配。实验使用实际数据集和YOLO显示，提出的方案可以实现需要的对象检测率，同时减少了大量的通信成本。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Efficacy-of-Supervised-Learning-vs-Large-Language-Models-for-Identifying-Cognitive-Distortions-and-Suicidal-Risks-in-Chinese-Social-Media"><a href="#Evaluating-the-Efficacy-of-Supervised-Learning-vs-Large-Language-Models-for-Identifying-Cognitive-Distortions-and-Suicidal-Risks-in-Chinese-Social-Media" class="headerlink" title="Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media"></a>Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03564">http://arxiv.org/abs/2309.03564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/chatglm2-6b">https://github.com/thudm/chatglm2-6b</a></li>
<li>paper_authors: Hongzhi Qi, Qing Zhao, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, Jianqiang Li, Guanghui Fu</li>
<li>for: 本研究探讨了使用大语言模型在中国社交媒体平台上进行心理健康风险识别和认知扭曲识别。</li>
<li>methods: 本研究使用了三种不同的策略：零shot、少shot和精度调整，对三个大语言模型（GPT-3.5、GPT-4和RoBERTa）进行了评估。</li>
<li>results: 研究发现，使用大语言模型进行心理健康风险识别和认知扭曲识别时，存在明显的性能差距，主要归结于模型无法完全捕捉微分类。此外，GPT-4在多个场景中表现出色，而GPT-3.5在精度调整后表现出明显的改善。<details>
<summary>Abstract</summary>
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tuning. Our findings revealed a discernible performance gap between the large language models and traditional supervised learning approaches, primarily attributed to the models' inability to fully grasp subtle categories. Notably, while GPT-4 outperforms its counterparts in multiple scenarios, GPT-3.5 shows significant enhancement in suicide risk classification after fine-tuning. To our knowledge, this investigation stands as the maiden attempt at gauging large language models on Chinese social media tasks. This study underscores the forward-looking and transformative implications of using large language models in the field of psychology. It lays the groundwork for future applications in psychological research and practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型，特别是快速进步的GPT系列，在各个领域中受到广泛关注，其影响力渐渐增长。然而，在医学领域，特别是心理学，对这些语言模型的实际应用还很少。同时，社交媒体平台上的用户们正在不断表达自己的情感，一些情感经常转化为负面情感，甚至达到自杀倾向。在时间上采取有效的 intervención 和预防措施是非常重要的。为了办理这一点，我们的研究团队决定进行以下两项实验：识别自杀风险和认知扭曲的任务。我们使用了批处理学习作为基础，并对三种不同的策略进行了考验：零shot、少shot 和精度调整。我们的发现显示，大型语言模型与传统的批处理学习方法相比，存在明显的性能差距，主要归结于模型无法完全理解微妙的类别。值得注意的是，GPT-4在多种场景中表现出色，而GPT-3.5在精度调整后表现出显著的自杀风险分类提升。根据我们所知，这是大型语言模型在中文社交媒体任务上的首次尝试。这项研究标志着大型语言模型在心理学领域的前进，并为未来在心理研究和实践中的应用奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Trinary-Decision-Trees-for-missing-value-handling"><a href="#Trinary-Decision-Trees-for-missing-value-handling" class="headerlink" title="Trinary Decision Trees for missing value handling"></a>Trinary Decision Trees for missing value handling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03561">http://arxiv.org/abs/2309.03561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henning Zakrisson</li>
<li>for: 提高决策树回归和分类器中处理缺失数据的性能</li>
<li>methods: 使用Trinary决策树算法，不假设缺失值包含回归或分类Response的信息</li>
<li>results: 在MCARScene中，Trinary树超过其他算法表现出色，尤其是在只缺失外挂数据时; 在IMScene中，Trinary树落后于其他算法; 混合模型TrinaryMIA树在所有缺失场景中表现稳定。<details>
<summary>Abstract</summary>
This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate method of handling missing data in decision tree algorithms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了三元决策树算法，用于改进决策树回归和分类器处理缺失数据的方法。与其他方法不同，三元决策树不 assumptions 缺失值包含响应的信息。在不同的缺失数据情况（完全随机缺失（MCAR）和有用缺失（IM））下，通过理论计算和实际数据示例，与现有算法进行比较。结果显示，三元树在MCAR情况下表现出色，特别是只缺失外样数据时，而在IM情况下则落后。一种混合模型，三元MIA树，将三元树和缺失在特征（MIA）方法结合，在所有类型的缺失情况下显示了稳定的表现。虽然训练速度可能 slower，但三元树提供了一种更准确和有 Promise 的缺失数据处理方法。
</details></li>
</ul>
<hr>
<h2 id="On-the-dynamics-of-multi-agent-nonlinear-filtering-and-learning"><a href="#On-the-dynamics-of-multi-agent-nonlinear-filtering-and-learning" class="headerlink" title="On the dynamics of multi agent nonlinear filtering and learning"></a>On the dynamics of multi agent nonlinear filtering and learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03557">http://arxiv.org/abs/2309.03557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayed Pouria Talebi, Danilo Mandic</li>
<li>for: 这篇论文旨在研究多智能体系统在分布式学习和联邦学习场景下的行为。</li>
<li>methods: 该文提出了一种通用的多智能体系统行为形式化方法，并给出了实现凝结学习行为的条件。</li>
<li>results: 文章采用这种方法在分布式和联邦学习场景下应用，并得到了一些有趣的结果。<details>
<summary>Abstract</summary>
Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
</details>
<details>
<summary>摘要</summary>
多智能系统目的是通过分散式协同决策动力学完成高度复杂的学习任务，这已经在信号处理和计算智能社区中吸引了很大的关注。这篇文章研究了多智能网络系统中非线性筛选/学习动力学的行为。为此，我们提供了多智能网络系统中代表者行为的一般形式化表述，并给出了实现协调学习行为的条件。特别是，我们在分布式和联邦学习场景中应用了所 derivated框架。
</details></li>
</ul>
<hr>
<h2 id="MVD-A-Novel-Methodology-and-Dataset-for-Acoustic-Vehicle-Type-Classification"><a href="#MVD-A-Novel-Methodology-and-Dataset-for-Acoustic-Vehicle-Type-Classification" class="headerlink" title="MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification"></a>MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03544">http://arxiv.org/abs/2309.03544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohd Ashhad, Omar Ahmed, Sooraj K. Ambat, Zeeshan Ali Haq, Mansaf Alam</li>
<li>for: 这 paper 是为了开发 acoustic traffic monitoring 和 vehicle-type classification 算法而设计的两个开源数据集 MVD 和 MVDA。</li>
<li>methods: 这 paper 使用了 cepstrum 和spectrum 基于本地和全球音频特征，以及多输入神经网络来准确分类 acoustic signals。</li>
<li>results: 实验结果表明，我们的方法可以超越先前的基线值，并在 MVD 和 MVDA 数据集上达到了 91.98% 和 96.66% 的准确率。<details>
<summary>Abstract</summary>
Rising urban populations have led to a surge in vehicle use and made traffic monitoring and management indispensable. Acoustic traffic monitoring (ATM) offers a cost-effective and efficient alternative to more computationally expensive methods of monitoring traffic such as those involving computer vision technologies. In this paper, we present MVD and MVDA: two open datasets for the development of acoustic traffic monitoring and vehicle-type classification algorithms, which contain audio recordings of moving vehicles. The dataset contain four classes- Trucks, Cars, Motorbikes, and a No-vehicle class. Additionally, we propose a novel and efficient way to accurately classify these acoustic signals using cepstrum and spectrum based local and global audio features, and a multi-input neural network. Experimental results show that our methodology improves upon the established baselines of previous works and achieves an accuracy of 91.98% and 96.66% on MVD and MVDA Datasets, respectively. Finally, the proposed model was deployed through an Android application to make it accessible for testing and demonstrate its efficacy.
</details>
<details>
<summary>摘要</summary>
城市人口增长导致交通量增加，使交通监测和管理成为不可或缺的。喷流交通监测（ATM）提供了一种Cost-effective和高效的交通监测方法，而不是基于计算机视觉技术的更复杂和昂贵的方法。在这篇论文中，我们提供了两个开放数据集，用于开发喷流交通监测和车辆类型分类算法：MVD和MVDA数据集。这两个数据集包含了四个类别：卡车、汽车、摩托车和无车类。此外，我们还提出了一种新的和高效的方法，使用cepstrum和spectrum基于本地和全球音频特征，以及多输入神经网络来准确地分类这些喷流信号。实验结果表明，我们的方法超越了先前的基eline，并达到了MVD数据集上的91.98%和MVDA数据集上的96.66%的准确率。最后，我们提出了一种通过Android应用程序进行部署的方法，以便在实验和示范其效果。
</details></li>
</ul>
<hr>
<h2 id="Subgraph-based-Tight-Frames-on-Graphs-with-Compact-Supports-and-Vanishing-Moments"><a href="#Subgraph-based-Tight-Frames-on-Graphs-with-Compact-Supports-and-Vanishing-Moments" class="headerlink" title="Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments"></a>Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03537">http://arxiv.org/abs/2309.03537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruigang Zheng, Xiaosheng Zhuang</li>
<li>for: 本研究提出了一种新的、通用的方法，用于在图上构建紧凑支持的帧，基于一系列的层次分 partitions。</li>
<li>methods: 我们的方法可以跟踪到来自层次分 partitions的抽象构造，并可以flexibly incorporate 子图 Laplacians 到我们的帧设计中。这使得我们可以调整 (子图) 消失 момент的帧lets，以及其他属性，如方向性，以便高效地表示图示Signals with path-like supports。</li>
<li>results: 我们的提议的图帧在非线性近似任务中表现出色。<details>
<summary>Abstract</summary>
In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种新的和通用的方法，用于在图上构建紧凑支持的图帧。从我们的抽象构建起，我们能够通过层次分 partitions来扩展先前基于 partition trees 的方法。因此，我们的通用方法允许我们在图帧中调整（子图）衰减瞬间，以及其他特性，如方向性，以有效地表示图信号。我们还显式定义了多种变体并进行测试。实验结果表明，我们的提议的图帧在非线性近似任务中表现出色。Here's the translation in Traditional Chinese:在这个工作中，我们提出了一种新的和通用的方法，用于在图上建构紧凑支持的图架。从我们的抽象建构起，我们能够通过层次分 partitions 来扩展先前基于 partition trees 的方法。因此，我们的通用方法允许我们在图架中调整（子图）衰减瞬间，以及其他特性，如方向性，以有效地表示图信号。我们还明确定义了多种变体并进行测试。实验结果显示，我们的建议的图架在非线性近似任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Feature-Enhancer-Segmentation-Network-FES-Net-for-Vessel-Segmentation"><a href="#Feature-Enhancer-Segmentation-Network-FES-Net-for-Vessel-Segmentation" class="headerlink" title="Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation"></a>Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03535">http://arxiv.org/abs/2309.03535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq M. Khan, Muhammad Arsalan, Shahzaib Iqbal, Imran Razzak, Erik Meijering</li>
<li>for: 静脉血管精准分割，以跟踪和诊断视力损伤的进程。</li>
<li>methods: 提议一种新的特征增强分割网络（FES-Net），可以准确地分割每个像素，不需要额外的图像增强步骤。</li>
<li>results: FES-Net在四个公共可用的state-of-the-art datasets上表现出色，舒适性比其他竞争方法更高。<details>
<summary>Abstract</summary>
Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.
</details>
<details>
<summary>摘要</summary>
疾病如肥胖症和年龄相关的macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.Here's the translation in Traditional Chinese:疾病如肥胖症和年龄相关的macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Negative-Learning-Approach-to-Partial-Domain-Adaptation-Using-Source-Prototypes"><a href="#A-Robust-Negative-Learning-Approach-to-Partial-Domain-Adaptation-Using-Source-Prototypes" class="headerlink" title="A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes"></a>A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03531">http://arxiv.org/abs/2309.03531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandipan Choudhuri, Suli Adeniye, Arunabha Sen</li>
<li>for: 提高partial domain adaptation的稳定性和泛化能力</li>
<li>methods:  ensemble learning + 多元标签反馈 + 域平衡 optimize intra-class compactness and inter-class separation</li>
<li>results: 比现有state-of-the-art PDA方法更高的稳定性和泛化能力<details>
<summary>Abstract</summary>
This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framework's enhanced robustness and generalization, demonstrating its superiority over existing state-of-the-art PDA approaches.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Partial Domain Adaptation" (PDA) is translated as "半领域适应" (half-domain adaptation)* "negative transfer problem" is translated as "负向传递问题" (negative transfer problem)* "robust target-supervision strategy" is translated as "鲁棒的目标监督策略" (robust target supervision strategy)* "ensemble learning" is translated as "集成学习" (ensemble learning)* "diverse, complementary label feedback" is translated as "多样、补充的标签反馈" (diverse and complementary label feedback)* "first-order moments" is translated as "首领oment" (first-order moment)* "distribution alignment" is translated as "分布对齐" (distribution alignment)* "intra-class compactness" is translated as "类内准确性" (intra-class compactness)* "inter-class separation" is translated as "类间分化" (inter-class separation)* "source data privacy" is translated as "源数据隐私" (source data privacy)* "a priori inference" is translated as "先验知" (a priori inference)
</details></li>
</ul>
<hr>
<h2 id="Efficient-Single-Object-Detection-on-Image-Patches-with-Early-Exit-Enhanced-High-Precision-CNNs"><a href="#Efficient-Single-Object-Detection-on-Image-Patches-with-Early-Exit-Enhanced-High-Precision-CNNs" class="headerlink" title="Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs"></a>Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03530">http://arxiv.org/abs/2309.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Moos</li>
<li>for: 本文提出了一种用于移动机器人检测对象的新方法，主要是检测足球。由于对象在不同光照条件下会有动态变化和模糊图像，因此检测技术具有挑战性。</li>
<li>methods: 本文提出了一种特制 convolutional neural network（CNN） architecture，用于在计算限制的 роботиче平台上进行高精度对象检测。该CNN拟合器通过在图像块中高精度地分类单个对象并确定其准确的空间位置来实现对象检测。此外，文章还 интегрирова了 Early Exits 技术以降低计算成本。</li>
<li>results: 本文的实验结果表明，使用提议方法可以实现对象检测的精度为 100%，并且具有 recall 接近 87%，而且处理时间只需要约 170 $\mu$s per 假设。通过将提议方法与 Early Exit 技术结合使用，可以在平均上提高计算时间的优化率高达 28%。<details>
<summary>Abstract</summary>
This paper proposes a novel approach for detecting objects using mobile robots in the context of the RoboCup Standard Platform League, with a primary focus on detecting the ball. The challenge lies in detecting a dynamic object in varying lighting conditions and blurred images caused by fast movements. To address this challenge, the paper presents a convolutional neural network architecture designed specifically for computationally constrained robotic platforms. The proposed CNN is trained to achieve high precision classification of single objects in image patches and to determine their precise spatial positions. The paper further integrates Early Exits into the existing high-precision CNN architecture to reduce the computational cost of easily rejectable cases in the background class. The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the validation dataset and a recall of almost 87%, while maintaining an execution time of around 170 $\mu$s per hypotheses. By combining the proposed approach with an Early Exit, a runtime optimization of more than 28%, on average, can be achieved compared to the original CNN. Overall, this paper provides an efficient solution for an enhanced detection of objects, especially the ball, in computationally constrained robotic platforms.
</details>
<details>
<summary>摘要</summary>
The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the validation dataset and a recall of almost 87%, while maintaining an execution time of around 170 microseconds per hypotheses. By combining the proposed approach with an Early Exit, a runtime optimization of more than 28%, on average, can be achieved compared to the original CNN. Overall, this paper provides an efficient solution for enhanced object detection, especially the ball, in computationally constrained robotic platforms.
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Continual-Federated-Clustering-via-Adaptive-Resonance-Theory"><a href="#Privacy-preserving-Continual-Federated-Clustering-via-Adaptive-Resonance-Theory" class="headerlink" title="Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory"></a>Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03487">http://arxiv.org/abs/2309.03487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Masuyama-lab/FCAC">https://github.com/Masuyama-lab/FCAC</a></li>
<li>paper_authors: Naoki Masuyama, Yusuke Nojima, Yuichiro Toda, Chu Kiong Loo, Hisao Ishibuchi, Naoyuki Kubota</li>
<li>for: 静态集成学习算法不能处理数据分布未知或持续变化的问题，这篇论文提出了一种隐私保护的连续联合分类算法来解决这个问题。</li>
<li>methods: 该算法使用了适应振荡理论基于的分类算法作为基准分类器，因此具有连续学习能力。</li>
<li>results: 实验结果表明，该算法在 sintetic 和实际数据上具有较高的分类性能，同时实现了数据隐私保护和连续学习能力。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
With the increasing importance of data privacy protection, various privacy-preserving machine learning methods have been proposed. In the clustering domain, various algorithms with a federated learning framework (i.e., federated clustering) have been actively studied and showed high clustering performance while preserving data privacy. However, most of the base clusterers (i.e., clustering algorithms) used in existing federated clustering algorithms need to specify the number of clusters in advance. These algorithms, therefore, are unable to deal with data whose distributions are unknown or continually changing. To tackle this problem, this paper proposes a privacy-preserving continual federated clustering algorithm. In the proposed algorithm, an adaptive resonance theory-based clustering algorithm capable of continual learning is used as a base clusterer. Therefore, the proposed algorithm inherits the ability of continual learning. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to state-of-the-art federated clustering algorithms while realizing data privacy protection and continual learning ability. The source code is available at \url{https://github.com/Masuyama-lab/FCAC}.
</details>
<details>
<summary>摘要</summary>
随着数据隐私保护的重要性日益增加，各种隐私保护机器学习方法已经被提出。在聚类领域，使用联合学习框架（i.e., 联合聚类）的各种算法已经得到了广泛的研究和应用，并表现出高聚类性能而又保护数据隐私。然而，大多数基本聚类算法（i.e., 聚类算法）使用在现有的联合聚类算法中需要先 specify 聚类数量。这些算法因此无法处理数据的分布未知或持续变化。为解决这个问题，本文提出了一种隐私保护的 continual 联合聚类算法。在提出的算法中，使用基于适应振荡理论的聚类算法，该算法具有Continual Learning能力。实验结果表明，提出的算法与现有的联合聚类算法相比，具有更高的聚类性能，同时实现了数据隐私保护和Continual Learning能力。源代码可以在 \url{https://github.com/Masuyama-lab/FCAC} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Fast-FixMatch-Faster-Semi-Supervised-Learning-with-Curriculum-Batch-Size"><a href="#Fast-FixMatch-Faster-Semi-Supervised-Learning-with-Curriculum-Batch-Size" class="headerlink" title="Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size"></a>Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03469">http://arxiv.org/abs/2309.03469</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Chen, Chen Dun, Anastasios Kyrillidis</li>
<li>for: 这个论文的目的是提出一种名为CURRICULUM BATCH SIZE（CBS）的无标签批处理训练策略，以减少SSL训练计算量。</li>
<li>methods: 该论文使用了强化标签扩充、CURRICULUM Pseudo Labeling（CPL）和FixMatch算法来实现这个目标。</li>
<li>results: 该论文的实验结果表明，在CIFAR-10、CIFAR-100、SVHN和STL-10等 dataset上，使用CBS和强化标签扩充&#x2F;CPL可以减少训练计算量，同时保持与传统 FixMatch 算法相同的性能水平。此外，该论文还应用了这些策略在联合学习和流处理学习中，并取得了类似的结果。<details>
<summary>Abstract</summary>
Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training computations, but, in synergy with CBS, they achieve optimal performance. Fast FixMatch also achieves substantially higher data utilization compared to previous state-of-the-art. Fast FixMatch achieves between $2.1\times$ - $3.4\times$ reduced training computations on CIFAR-10 with all but 40, 250 and 4000 labels removed, compared to vanilla FixMatch, while attaining the same cited state-of-the-art error rate \citep{FixMatch}. Similar results are achieved for CIFAR-100, SVHN and STL-10. Finally, Fast MixMatch achieves between $2.6\times$ - $3.3\times$ reduced training computations in federated SSL tasks and online/streaming learning SSL tasks, which further demonstrate the generializbility of Fast MixMatch to different scenarios and tasks.
</details>
<details>
<summary>摘要</summary>
SSL 技术的进步已经几乎完全减少了与超级vised learning（SSL）之间的差距，但是最近的性能改进往往来于提高训练计算量。为了解决这个问题，我们提出了批处理大小学习纲程（CBS），它利用深度神经网络的自然训练动力学来实现。在训练的早期，使用一个小的无标签批处理大小，逐渐增加到训练的末尾。不 matter what dataset, model or number of epochs, we use a fixed curriculum regardless of the dataset, model or number of epochs.我们发现，使用 CBS 可以减少训练计算量。在 FixMatch 基础上，我们采用了强化标签增强、CURRICULUM Pseudo Labeling（CPL）和 Curriculum Batch Size（CBS），并将其称为 Fast FixMatch。我们进行了一个ablation study，发现强化标签增强和/或 CPL 不会减少训练计算量，但是在 CBS 的帮助下，它们可以实现最佳性能。 Fast FixMatch 还实现了对 CIFAR-10 的大量数据使用，相比之前的状态态峰值，提高了数据利用率。Fast MixMatch 在 federated SSL 任务和在线/流动学习 SSL 任务中实现了 $2.6\times$ - $3.3\times$ 减少训练计算量，这些结果再次证明 Fast MixMatch 对不同场景和任务的通用性。
</details></li>
</ul>
<hr>
<h2 id="Cross-Image-Context-Matters-for-Bongard-Problems"><a href="#Cross-Image-Context-Matters-for-Bongard-Problems" class="headerlink" title="Cross-Image Context Matters for Bongard Problems"></a>Cross-Image Context Matters for Bongard Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03468">http://arxiv.org/abs/2309.03468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nraghuraman/bongard-context">https://github.com/nraghuraman/bongard-context</a></li>
<li>paper_authors: Nikhil Raghuraman, Adam W. Harley, Leonidas Guibas</li>
<li>for: 解决自然图像Bongard问题，提高现有方法的性能。</li>
<li>methods: 使用简单的方法将涉及多个支持图像的跨图像上下文integrated into the model,以提高模型的准确率。</li>
<li>results: 实验结果表明，新方法可以 дости得到新的状态数据集Bongard-LOGO（75.3%）和Bongard-HOI（72.45%）的优秀表现，并在原始Bongard问题集（60.84%）上达到了强表现。<details>
<summary>Abstract</summary>
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a variety of simple methods to take this cross-image context into account, and demonstrate substantial gains over prior methods, leading to new state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%) and strong performance on the original Bongard problem set (60.84%).
</details>
<details>
<summary>摘要</summary>
（Current machine learning methods are struggling to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy, where chance is 50%. Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a variety of simple methods to take this cross-image context into account, and demonstrate substantial gains over prior methods, leading to new state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%) and strong performance on the original Bongard problem set (60.84%).）
</details></li>
</ul>
<hr>
<h2 id="Multi-Modality-Guidance-Network-For-Missing-Modality-Inference"><a href="#Multi-Modality-Guidance-Network-For-Missing-Modality-Inference" class="headerlink" title="Multi-Modality Guidance Network For Missing Modality Inference"></a>Multi-Modality Guidance Network For Missing Modality Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03452">http://arxiv.org/abs/2309.03452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuokai Zhao, Harish Palani, Tianyi Liu, Lena Evans, Ruth Toner</li>
<li>for: 解决多模态模型在推理阶段缺失模式时的问题，提高模型在不同模式下的推理性能。</li>
<li>methods: 提出一种新的导航网络，通过在训练阶段优化多模态表示来促进单模态模型的训练，从而提高推理性能。</li>
<li>results: 实际实验表明，提出的框架可以在暴力检测任务中训练单模态模型，其性能明显超过传统训练方法，同时保持同样的推理成本。<details>
<summary>Abstract</summary>
Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference cost.
</details>
<details>
<summary>摘要</summary>
Translate the given text into Simplified Chinese.</SYS>多模态模型在最近几年内取得了 significante 成功。标准的多模态方法frequently假设从训练阶段到推论阶段modalities的不变，但在实践中，许多情况下fail to satisfy这些假设， resulting in missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference cost.(Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. Traditional Chinese is also widely used, especially in Hong Kong, Taiwan, and other countries.)
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-Sound-Recognition-for-Efficient-Underwater-Data-Analysis"><a href="#Cross-domain-Sound-Recognition-for-Efficient-Underwater-Data-Analysis" class="headerlink" title="Cross-domain Sound Recognition for Efficient Underwater Data Analysis"></a>Cross-domain Sound Recognition for Efficient Underwater Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03451">http://arxiv.org/abs/2309.03451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongsoo Park, Dong-Gyun Han, Hyoung Sul La, Sangmin Lee, Yoonchang Han, Eun-Jin Yang</li>
<li>for: 这 paper 是为了分析大量的海洋声学数据而提出的一种深度学习方法，利用一个基于广泛非海洋（天空）声音的模型。</li>
<li>methods: 我们提出了一种两重方法，首先使用 PCA 和 UMAP 可视化海洋数据，然后选择候选标签进行进一步训练。</li>
<li>results: 我们通过对selected海洋数据和非海洋数据进行训练，实现了一个准确率超过 84.3% 的模型，以识别海洋中的气枪声音。<details>
<summary>Abstract</summary>
This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.   The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.   In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common type of underwater sound. The F1 score achieved by our model exceeded 84.3%, demonstrating the effectiveness of our approach in analyzing underwater acoustic data.   The methodology presented in this paper holds significant potential to reduce the amount of labor required in underwater data analysis and opens up new possibilities for further research in the field of cross-domain data analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables clustering of the data in a two-dimensional space and listening to points within these clusters to understand their defining characteristics.2. Training a neural network model using both the selected underwater data and the non-underwater dataset. The model’s precision, recall, and F1 score for recognizing airgun sounds, a common type of underwater sound, were measured and found to exceed 84.3%.The proposed methodology has the potential to significantly reduce the amount of labor required in underwater data analysis and opens up new possibilities for cross-domain data analysis.</details></li>
</ol>
<hr>
<h2 id="XGen-7B-Technical-Report"><a href="#XGen-7B-Technical-Report" class="headerlink" title="XGen-7B Technical Report"></a>XGen-7B Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03450">http://arxiv.org/abs/2309.03450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong</li>
<li>for: 这篇论文目标是为了提高大语言模型（LLMs）的性能，并且开源其模型，以便研究人员可以在此基础上进行进一步的研究和应用。</li>
<li>methods: 这篇论文使用了一系列的7B参数模型，并在8K字节长度上进行了训练。此外，它还对公共领域的教程数据进行了精度调整，创造了专门用于 instruciton-tuned 模型（XGen-Inst）。</li>
<li>results: 论文的评估结果显示，XGen模型在标准测试 benchmark 上达到了相对或更好的结果，而且在长序模型化任务上表现更出色，特别是在8K字节长度上。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence open-source LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Broadband-Ground-Motion-Synthesis-via-Generative-Adversarial-Neural-Operators-Development-and-Validation"><a href="#Broadband-Ground-Motion-Synthesis-via-Generative-Adversarial-Neural-Operators-Development-and-Validation" class="headerlink" title="Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation"></a>Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03447">http://arxiv.org/abs/2309.03447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yzshi5/gm-gano">https://github.com/yzshi5/gm-gano</a></li>
<li>paper_authors: Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zachary E. Ross, Kamyar Azizzadenesheli</li>
<li>For: The paper is written for generating ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) for earthquake engineering applications.* Methods: The paper uses Neural Operators, a resolution-invariant architecture that allows for training the model independently of the data sampling frequency, to generate three-component acceleration time histories conditioned on moment magnitude, rupture distance, time-average shear-wave velocity at the top $30m$, and tectonic environment or style of faulting.* Results: The paper shows that the proposed framework, called cGM-GANO, can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier amplitude and pseudo-spectral accelerations, and produces consistent median scaling with conventional Ground Motion Models (GMMs) for the corresponding tectonic environments. However, the largest misfit is observed at short distances due to the scarcity of training data.<details>
<summary>Abstract</summary>
We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier amplitude and pseudo-spectral accelerations. We evaluate cGM-GANO through residual analysis with the empirical dataset as well as by comparison with conventional Ground Motion Models (GMMs) for selected ground motion scenarios. Results show that cGM-GANO produces consistent median scaling with the GMMs for the corresponding tectonic environments. The largest misfit is observed at short distances due to the scarcity of training data. With the exception of short distances, the aleatory variability of the response spectral ordinates is also well captured, especially for subduction events due to the adequacy of training data. Applications of the presented framework include generation of risk-targeted ground motions for site-specific engineering applications.
</details>
<details>
<summary>摘要</summary>
我们提出了基于数据驱动的震动Synthesize模型，使用Generative Adversarial Neural Operator（GANO），结合了最新的机器学习技术和开放获取的强震数据集，以生成三Component加速度时间历史记录，受力级($M）、破裂距离($R_{rup}$)、顶部30米时间均辐射波速度($V_{S30}$)和地震环境或斜坡类型。我们使用Neural Operators，一种约束独立的建模架构，确保模型训练不受数据采样频率的影响。我们首先介绍了受控震动Synthesize算法（简称为cGM-GANO），并讨论了它与前一个作品的优点。然后，我们验证了cGM-GANO框架使用SCEC Broadband Platform（BBP）生成的模拟震动数据。最后，我们在日本KiK-net数据集上训练了cGM-GANO，并显示了其恢复了震动级、距离和$V_{S30}$的振荡尺度的能力。我们通过预测数据的差异分析和与传统地震模型（GMMs）的比较来评估cGM-GANO。结果表明，cGM-GANO在相应的地震环境中具有一致的中值振荡，但短距离处存在较大的差异。此外，cGM-GANO能够准确捕捉随机变量的响应特征ORD，特别是在沉入事件中，因为训练数据的充足。应用包括生成基于风险的地震动数据，用于特定工程应用。
</details></li>
</ul>
<hr>
<h2 id="Punctate-White-Matter-Lesion-Segmentation-in-Preterm-Infants-Powered-by-Counterfactually-Generative-Learning"><a href="#Punctate-White-Matter-Lesion-Segmentation-in-Preterm-Infants-Powered-by-Counterfactually-Generative-Learning" class="headerlink" title="Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning"></a>Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03440">http://arxiv.org/abs/2309.03440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehua Ren, Yongheng Sun, Miaomiao Wang, Yuying Feng, Xianjun Li, Chao Jin, Jian Yang, Chunfeng Lian, Fan Wang</li>
<li>for: 这个研究旨在提出一种可靠地自动分类潜在白质脑症（PWMLs）的方法，以便在诊断和治疗相关疗病的过程中优化诊断。</li>
<li>methods: 这个研究使用了对抗事实的思维和辅助任务的脑组织分类来学习细部位置信息和形态特征，以提高PWMLs的精确分类和定位。</li>
<li>results: 研究结果显示，这个方法可以在实际临床数据集上达到现有方法的州OF-THE-ART性能，并且提供了一个简单易用的深度学习框架（i.e., DeepPWML），可以轻松地应用于实际应用中。<details>
<summary>Abstract</summary>
Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed. It combines the lesion counterfactual map with the tissue probability map to train a lightweight PWML segmentation network, demonstrating state-of-the-art performance on a real-clinical dataset of infant T1w MR images. The code is available at \href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML}.
</details>
<details>
<summary>摘要</summary>
精准分割脑白 matter斑点病变 (PWMLs) 是诊断和治疗相关developmental disorders的基础。自动从 infant brain MR 图像中提取 PWMLs 的分割是一项挑战，因为这些斑点通常很小，对比度很低，并且每个主体中斑点的数量可能会很大。现有的学习基于方法直接将通用网络架构应用到这项任务上，可能会失去 PWMLs 的细致位坐信息，导致严重的下 segmentation。在这篇论文中，我们提出了利用对立想法 coupled with 脑组织 segmentation 作为 auxillary task，以学习 PWMLs 的细致位坐和形态表示。我们设计了一个简单易用的深度学习框架（i.e., DeepPWML），该框架结合 lesion counterfactual map 和 tissue probability map 来训练一个轻量级 PWML segmentation 网络，并在实际临床数据集上达到了 state-of-the-art 性能。代码可以在 \href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Tucker-Decomposition-Modeling-Commonality-and-Peculiarity-on-Tensor-Data"><a href="#Personalized-Tucker-Decomposition-Modeling-Commonality-and-Peculiarity-on-Tensor-Data" class="headerlink" title="Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data"></a>Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03439">http://arxiv.org/abs/2309.03439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuyun Hu, Naichen Shi, Raed Al Kontar, Hao Yan</li>
<li>for: address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets</li>
<li>methods: personalized Tucker decomposition (perTucker) that decomposes tensor data into shared global components and personalized local components, with a mode orthogonality assumption and a proximal gradient regularized block coordinate descent algorithm</li>
<li>results: effective in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.Here is the summary in Traditional Chinese:</li>
<li>for: Addressing the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets</li>
<li>methods: 个人化Tucker分解（perTucker），将tensor资料分解为共同全部 ком成分和个人化地方成分，并假设模式正交性和 proximal梯度调整Block coordinate descent algorithm</li>
<li>results: 实现异常检测、客户分类和集群，通过一个 simulations study和两个案例研究：太阳风检测和吨位信号分类。<details>
<summary>Abstract</summary>
We propose personalized Tucker decomposition (perTucker) to address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets. perTucker decomposes tensor data into shared global components and personalized local components. We introduce a mode orthogonality assumption and develop a proximal gradient regularized block coordinate descent algorithm that is guaranteed to converge to a stationary point. By learning unique and common representations across datasets, we demonstrate perTucker's effectiveness in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.
</details>
<details>
<summary>摘要</summary>
我们提出个性化图ucker分解（perTucker）来解决传统张量分解方法不能捕捉不同数据集之间的多样性的限制。perTucker将张量数据分解成共享全局组件和个性化地方组件。我们提出了一种方向正交假设，并开发了一种距离正则化块坐标推移算法，这个算法能够保证收敛到站点点。通过学习不同数据集之间的共同和特有表示，我们示出perTucker在异常检测、客户分类和群集分类中的效果。通过一个 simulations 和两个实际案例（太阳闪光检测和吨位信号分类），我们证明了perTucker的有效性。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Federated-Learning-with-Variance-Reduction-and-Differential-Privacy"><a href="#Byzantine-Robust-Federated-Learning-with-Variance-Reduction-and-Differential-Privacy" class="headerlink" title="Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy"></a>Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03437">http://arxiv.org/abs/2309.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Zhang, Rui Hu</li>
<li>for: 保护数据隐私和防止Byzantine攻击 during federated learning (FL) 训练过程，保持数据在客户端（即物联网设备）上，并且只在客户端上共享模型更新。</li>
<li>methods: 我们提出了一种新的 federated learning 方案，通过将采样和势量驱动的差异隐私 Mechanism 引入到客户端级别的隐私保证机制中，以防止Byzantine攻击。我们的安全设计不违反客户端级别的隐私保证机制，因此我们的方法可以保持同样的客户端级别隐私保证。</li>
<li>results: 我们在不同的IID和非IID数据集和不同的任务上进行了广泛的实验，并对不同的Byzantine攻击进行了比较。结果表明我们的框架可以提高系统的Robustness against Byzantine attacks，同时保持强的隐私保证。<details>
<summary>Abstract</summary>
Federated learning (FL) is designed to preserve data privacy during model training, where the data remains on the client side (i.e., IoT devices), and only model updates of clients are shared iteratively for collaborative learning. However, this process is vulnerable to privacy attacks and Byzantine attacks: the local model updates shared throughout the FL network will leak private information about the local training data, and they can also be maliciously crafted by Byzantine attackers to disturb the learning. In this paper, we propose a new FL scheme that guarantees rigorous privacy and simultaneously enhances system robustness against Byzantine attacks. Our approach introduces sparsification- and momentum-driven variance reduction into the client-level differential privacy (DP) mechanism, to defend against Byzantine attackers. The security design does not violate the privacy guarantee of the client-level DP mechanism; hence, our approach achieves the same client-level DP guarantee as the state-of-the-art. We conduct extensive experiments on both IID and non-IID datasets and different tasks and evaluate the performance of our approach against different Byzantine attacks by comparing it with state-of-the-art defense methods. The results of our experiments show the efficacy of our framework and demonstrate its ability to improve system robustness against Byzantine attacks while achieving a strong privacy guarantee.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Equal-Long-term-Benefit-Rate-Adapting-Static-Fairness-Notions-to-Sequential-Decision-Making"><a href="#Equal-Long-term-Benefit-Rate-Adapting-Static-Fairness-Notions-to-Sequential-Decision-Making" class="headerlink" title="Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making"></a>Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03426">http://arxiv.org/abs/2309.03426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuancheng-xu/elbert">https://github.com/yuancheng-xu/elbert</a></li>
<li>paper_authors: Yuancheng Xu, Chenghao Deng, Yanchao Sun, Ruijie Zheng, Xiyao Wang, Jieyu Zhao, Furong Huang</li>
<li>for: 这篇论文关注机器学习模型的决策对长期影响的公平性问题。</li>
<li>methods: 该论文使用Markov Decision Process（MDP）框架来解决长期公平性问题，并定义了长期偏见的概念。</li>
<li>results: 该论文提出了一种名为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑了不同时间步骤的重要性差异，并且可以使用标准政策优化方法来减少偏见。实验结果表明，ELBERT-PO方法可以有效地减少偏见并保持高效性。<details>
<summary>Abstract</summary>
Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefit Rate can be analytically reduced to standard policy gradient. This makes standard policy optimization methods applicable for reducing the bias, leading to our proposed bias mitigation method ELBERT-PO. Experiments on three sequential decision making environments show that ELBERT-PO significantly reduces bias and maintains high utility. Code is available at https://github.com/Yuancheng-Xu/ELBERT.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. We show that the policy gradient of Long-term Benefit Rate can be analytically reduced to standard policy gradient, making standard policy optimization methods applicable for reducing the bias. This leads to our proposed bias mitigation method ELBERT-PO.We evaluate ELBERT-PO on three sequential decision-making environments and show that it significantly reduces bias and maintains high utility. Our code is available at https://github.com/Yuancheng-Xu/ELBERT.Here's the Simplified Chinese translation:机器学习模型的决策可能会有长期影响，因此长期公平是一个非常重要的考虑因素。先前的工作已经证明了，在忽略长期影响的情况下，简单地强制执行公平准则可能会在时间流逝后恶化偏见。为了明确遗传偏见在序列决策中的问题， latest works 在 Markov Decision Process (MDP) 框架下形ulated long-term fairness notions。然而，这些不ions fail to consider the importance difference of different time steps during transitions, leading to a false sense of fairness。在这种情况下，我们引入了 Equal Long-term Benefit Rate (ELBERT)，一种考虑时间重要性的变化的长期公平原则。我们显示了 Long-term Benefit Rate 的政策梯度可以被分析式地减少到标准政策梯度，使得标准政策优化方法可以用来减少偏见。这导致我们的偏见减少方法 ELBERT-PO。我们在三个序列决策环境中评估 ELBERT-PO，并发现它可以减少偏见，同时保持高的用户。我们的代码可以在 https://github.com/Yuancheng-Xu/ELBERT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Optimizers"><a href="#Large-Language-Models-as-Optimizers" class="headerlink" title="Large Language Models as Optimizers"></a>Large Language Models as Optimizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03409">http://arxiv.org/abs/2309.03409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen</li>
<li>for: 用于解决各种实际应用中缺乏导数的优化问题。</li>
<li>methods: 利用大型自然语言模型（LLMs）作为优化器，通过自然语言描述优化任务来生成新的解决方案。</li>
<li>results: 比较human-设计的提示，使用OPRO优化的提示在GSM8K和Big-Bench Hard任务上的性能提高至多达8%和50%。<details>
<summary>Abstract</summary>
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
</details>
<details>
<summary>摘要</summary>
优化是 ubique 存在的。偏导函数基本算法在各种问题上表现出了强大的工具性，但在许多实际应用中，缺乏梯度带来了挑战。在这项工作中，我们提出了一种简单有效的方法，即优化 by PROmpting (OPRO)，它利用大型自然语言模型 (LLMs) 作为优化器，并将优化任务描述为自然语言中的提示。在每次优化步骤中，LLM 生成新的解决方案，这些解决方案基于之前生成的解决方案和其值，然后评估这些新的解决方案，并将其添加到下一个优化步骤中。我们首先在线性回归和旅行商问题上应用 OPRO，然后扩展到提示优化，即找到最佳提示以最大化任务准确率。通过各种 LLMS，我们示示了最佳提示由 OPRO 优化的比人设计提示高达 8% 的 GSM8K 和高达 50% 的 Big-Bench Hard 任务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/07/cs.LG_2023_09_07/" data-id="clmjn91my00850j88fncq1d7h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/07/cs.SD_2023_09_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-07
        
      </div>
    </a>
  
  
    <a href="/2023/09/07/eess.IV_2023_09_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
