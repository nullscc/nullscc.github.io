
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-10 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multimodal Fish Feeding Intensity Assessment in Aquaculture paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05058 repo_url: None paper_authors: Meng Cui, Xubo Liu, Haohe Liu, Zhuangzhuang Du, Tao Chen, Guoping L">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-10">
<meta property="og:url" content="https://nullscc.github.io/2023/09/10/cs.SD_2023_09_10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Multimodal Fish Feeding Intensity Assessment in Aquaculture paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05058 repo_url: None paper_authors: Meng Cui, Xubo Liu, Haohe Liu, Zhuangzhuang Du, Tao Chen, Guoping L">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-10T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:19.127Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/10/cs.SD_2023_09_10/" class="article-date">
  <time datetime="2023-09-10T15:00:00.000Z" itemprop="datePublished">2023-09-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-10
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multimodal-Fish-Feeding-Intensity-Assessment-in-Aquaculture"><a href="#Multimodal-Fish-Feeding-Intensity-Assessment-in-Aquaculture" class="headerlink" title="Multimodal Fish Feeding Intensity Assessment in Aquaculture"></a>Multimodal Fish Feeding Intensity Assessment in Aquaculture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05058">http://arxiv.org/abs/2309.05058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Cui, Xubo Liu, Haohe Liu, Zhuangzhuang Du, Tao Chen, Guoping Lian, Daoliang Li, Wenwu Wang</li>
<li>for: 本研究旨在评估鱼类食欲强度变化 during feeding process, 在工业水生养殖中扮演重要角色。</li>
<li>methods: 本研究使用多Modal方法，包括单模态预训练模型和模态融合方法，并在AV-FFIA数据集上进行了比较研究。</li>
<li>results: 结果表明，多模态方法在噪音环境中表现出色，而单模态方法在清晰环境中表现较差。此外，提出了一种单模型可处理多modal数据的方法，能够实现与专门预训练模型相同或更高的性能，同时减少计算负担。<details>
<summary>Abstract</summary>
Fish feeding intensity assessment (FFIA) aims to evaluate the intensity change of fish appetite during the feeding process, which is vital in industrial aquaculture applications. The main challenges surrounding FFIA are two-fold. 1) robustness: existing work has mainly leveraged single-modality (e.g., vision, audio) methods, which have a high sensitivity to input noise. 2) efficiency: FFIA models are generally expected to be employed on devices. This presents a challenge in terms of computational efficiency. In this work, we first introduce an audio-visual dataset, called AV-FFIA. AV-FFIA consists of 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. To our knowledge, AV-FFIA is the first large-scale multimodal dataset for FFIA research. Then, we introduce a multi-modal approach for FFIA by leveraging single-modality pre-trained models and modality-fusion methods, with benchmark studies on AV-FFIA. Our experimental results indicate that the multi-modal approach substantially outperforms the single-modality based approach, especially in noisy environments. While multimodal approaches provide a performance gain for FFIA, it inherently increase the computational cost. To overcome this issue, we further present a novel unified model, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation from single-modality pre-trained models. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead. Our proposed U-FFIA approach enables a more robust and efficient method for FFIA, with the potential to contribute to improved management practices and sustainability in aquaculture.
</details>
<details>
<summary>摘要</summary>
鱼饵吞吐评估（FFIA）目的是评估鱼饵吞吐过程中鱼饵欲求的变化，这在工业鱼养中非常重要。主要挑战包括：1）稳定性：现有工作主要利用单模态（如视觉、声音）方法，具有高度敏感itivity to input noise。2）效率：FFIA模型通常预期在设备上使用，这会导致计算效率的挑战。在这项工作中，我们首先介绍了一个Audio-Visual（AV）-FFIA数据集，该数据集包含27,000个标注的音频和视频剪辑，每个剪辑都 capture不同水平的鱼饵吞吐Intensity。我们知道，AV-FFIA是首个大规模的多模态FFIA研究数据集。然后，我们介绍了一种多模态方法，通过单模态预训练模型和多模态融合方法，在AV-FFIA上进行了标准 benchmark研究。我们的实验结果表明，多模态方法在噪音环境下表现明显 луч于单模态方法，特别是在噪音环境下。虽然多模态方法提供了FFIA方面的性能提升，但它会自然增加计算成本。为解决这个问题，我们进一步介绍了一种单模型Unified Model（U-FFIA），该模型可以处理音频、视频或Audio-Visual多模态，通过训练时的模式Dropout和知识储存Single-modality预训练模型，实现与单模态FFIA模型的性能相似或更高，同时具有显著更低的计算开销。我们的提出的U-FFIA方法可以实现更加稳定和高效的FFIA，并且具有潜在的提高鱼养管理实践和可持续发展的潜力。
</details></li>
</ul>
<hr>
<h2 id="Gray-Jedi-MVDR-Post-filtering"><a href="#Gray-Jedi-MVDR-Post-filtering" class="headerlink" title="Gray Jedi MVDR Post-filtering"></a>Gray Jedi MVDR Post-filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05057">http://arxiv.org/abs/2309.05057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FrancoisGrondin/mvdrpf">https://github.com/FrancoisGrondin/mvdrpf</a></li>
<li>paper_authors: François Grondin, Caleb Rascón</li>
<li>for: 提高多个说话人enario中语音质量</li>
<li>methods: 使用深度学习基于的抗噪模型和Minimum Variance Distortionless Response（MVDR）进行抗噪和干扰估计</li>
<li>results: 比单输入基eline的干扰估计性能更高，需要 menos computing resources для后处理，可以在多个说话人enario中开发在线语音提高系统<details>
<summary>Abstract</summary>
Spatial filters can exploit deep-learning-based speech enhancement models to increase their reliability in scenarios with multiple speech sources scenarios. To further improve speech quality, it is common to perform postfiltering on the estimated target speech obtained with spatial filtering. In this work, Minimum Variance Distortionless Response (MVDR) is employed to provide the interference estimation, along with the estimation of the target speech, to be later used for postfiltering. This improves the enhancement performance over a single-input baseline in a far more significant way than by increasing the model's complexity. Results suggest that less computing resources are required for postfiltering when provided with both target and interference signals, which is a step forward in developing an online speech enhancement system for multi-speech scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度学习基于的speech enhancement模型可以提高空间滤波器的可靠性，特别在多个语音源场景中。为了进一步提高语音质量，通常会在空间滤波器估计target speech后进行postfiltering。在这种工作中，我们使用Minimum Variance Distortionless Response（MVDR）来提供干扰估计，同时提供target speech的估计，以供后续使用。这会提高增强性能，比单输入基eline的增强性能更高，并且需要更少的计算资源。结果表明，当提供target和干扰信号时，postfiltering需要更少的计算资源，这是在开发多语音场景中的线上speech增强系统的一大进步。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="VoiceFlow-Efficient-Text-to-Speech-with-Rectified-Flow-Matching"><a href="#VoiceFlow-Efficient-Text-to-Speech-with-Rectified-Flow-Matching" class="headerlink" title="VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching"></a>VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05027">http://arxiv.org/abs/2309.05027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, Kai Yu</li>
<li>for: 提高文本到语音的生成质量，降低 diffusion models 的复杂性</li>
<li>methods: 使用矩阵流匹配算法，将文本输入转化为 mel-spectrograms 的生成过程</li>
<li>results: 与 diffusion models 相比，VoiceFlow 实现了更高的生成质量，并且可以采用有限多个抽样步骤进行生成<details>
<summary>Abstract</summary>
Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Diffusion模型在文本至语音转化中变得非常流行，但是它们的内在复杂性使其效率受限。我们提议VoiceFlow，一种使用矫正流匹配算法实现高质量的语音合成，只需少量抽样步骤。VoiceFlow将文本输入转化为mel-spectrogram的生成过程，并且使用了一个条件的ordinary differential equation来估算vector field。矫正流技术然后有效地直线化抽样轨迹，提高了效率。对于单个和多个说话者的corpora进行主观和客观评估，VoiceFlow的合成质量显著高于 diffusion counterpart。ablation studies进一步验证了矫正流技术在VoiceFlow中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Emotional-Adaptation-for-Audio-Driven-Talking-Head-Generation"><a href="#Efficient-Emotional-Adaptation-for-Audio-Driven-Talking-Head-Generation" class="headerlink" title="Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation"></a>Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04946">http://arxiv.org/abs/2309.04946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuangan/eat_code">https://github.com/yuangan/eat_code</a></li>
<li>paper_authors: Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang</li>
<li>for: 这个研究旨在提高现有的音频驱动的人脸合成方法，以提高虚拟人类相关应用的可靠性和生动性。</li>
<li>methods: 我们提出了一种名为Emotional Adaptation for Audio-driven Talking-head（EAT）方法，它可以将无情的人脸模型转化为可控制情感的模型，并且可以在Cost-effective和高效的情况下进行。我们的方法包括三种轻量级的修改（深度情感提示、情感变形网络和情感适应模块），从不同的角度来实现精准和生动的情感控制。</li>
<li>results: 我们的实验结果表明，我们的方法可以在广泛使用的基准数据集上达到当前最佳性能，包括LRW和MEAD。此外，我们的参数效率的修改还能够在情感训练视频罕见或缺失时显示出很好的泛化能力。更多信息请访问<a target="_blank" rel="noopener" href="https://yuangan.github.io/eat/%E3%80%82">https://yuangan.github.io/eat/。</a><details>
<summary>Abstract</summary>
Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</details>
<details>
<summary>摘要</summary>
文本：Audio-driven talking-head synthesis是虚拟人类应用领域中受欢迎的研究主题。然而，现有方法的不灵活和不高效性是重大的限制。在这项工作中，我们提出了情感适应 Audio-driven talking-head（EAT）方法，可以将情感无关的 talking-head 模型转化成可控情感的模型，而不需要昂贵的端到端训练。我们的方法利用了预训练的情感无关 talking-head 变换器，并引入了三种轻量级的适应（深度情感提示、情感变形网络和情感适应模块），从不同的角度实现精准和真实的情感控制。我们的实验表明，我们的方法在广泛使用的标准准则上达到了领先的性能水平，包括 LRW 和 MEAD。此外，我们的参数效率适应表现出了惊人的普适性，即使情感训练视频罕见或缺失。项目网站：https://yuangan.github.io/eat/Translation:文本：Audio-driven talking-head synthesis是虚拟人类应用领域中受欢迎的研究主题。然而，现有方法的不灵活和不高效性是重大的限制。在这项工作中，我们提出了情感适应 Audio-driven talking-head（EAT）方法，可以将情感无关的 talking-head 模型转化成可控情感的模型，而不需要昂贵的端到端训练。我们的方法利用了预训练的情感无关 talking-head 变换器，并引入了三种轻量级的适应（深度情感提示、情感变形网络和情感适应模块），从不同的角度实现精准和真实的情感控制。我们的实验表明，我们的方法在广泛使用的标准准则上达到了领先的性能水平，包括 LRW 和 MEAD。此外，我们的参数效率适应表现出了惊人的普适性，即使情感训练视频罕见或缺失。项目网站：https://yuangan.github.io/eat/Note: The translation is in Simplified Chinese, which is the most widely used Chinese language in mainland China.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/10/cs.SD_2023_09_10/" data-id="clmjn91og00c10j88hr6q42ns" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/11/eess.IV_2023_09_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-11
        
      </div>
    </a>
  
  
    <a href="/2023/09/10/cs.LG_2023_09_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-10</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
