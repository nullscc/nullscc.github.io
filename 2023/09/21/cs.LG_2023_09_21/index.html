
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12283 repo_url: None paper_authors: Ben Maman, Johannes Zeitler, Meinard Müller, Amit">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-21">
<meta property="og:url" content="https://nullscc.github.io/2023/09/21/cs.LG_2023_09_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12283 repo_url: None paper_authors: Ben Maman, Johannes Zeitler, Meinard Müller, Amit">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-21T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:59:17.847Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.LG_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T10:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Performance-Conditioning-for-Diffusion-Based-Multi-Instrument-Music-Synthesis"><a href="#Performance-Conditioning-for-Diffusion-Based-Multi-Instrument-Music-Synthesis" class="headerlink" title="Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis"></a>Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12283">http://arxiv.org/abs/2309.12283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Maman, Johannes Zeitler, Meinard Müller, Amit H. Bermano</li>
<li>for: 本研究的目的是提高多楽器合成的控制精度，使其更能够符合音乐的特性和演奏环境。</li>
<li>methods: 本研究基于当前顶尖的扩散模型，引入表演环境调整，以提高音乐生成的时期和乐器风格控制。</li>
<li>results: 这项研究的评估结果显示，使用表演环境调整可以实现更高的乐器特性和演奏风格控制，并且在不同乐器和演奏环境下实现了类比于人类聆听者的音乐生成。<details>
<summary>Abstract</summary>
Generating multi-instrument music from symbolic music representations is an important task in Music Information Retrieval (MIR). A central but still largely unsolved problem in this context is musically and acoustically informed control in the generation process. As the main contribution of this work, we propose enhancing control of multi-instrument synthesis by conditioning a generative model on a specific performance and recording environment, thus allowing for better guidance of timbre and style. Building on state-of-the-art diffusion-based music generative models, we introduce performance conditioning - a simple tool indicating the generative model to synthesize music with style and timbre of specific instruments taken from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. Our project page, including samples and demonstrations, is available at benadar293.github.io/midipm
</details>
<details>
<summary>摘要</summary>
<<SYS>>将 symbolic music representation 转化为多种乐器音乐是音乐信息检索（MIR）领域中的重要任务。这个问题的中心问题是在生成过程中提供音乐和听众意见。作为本工作的主要贡献，我们提议通过指定特定的表演和录音环境来增强多乐器合成的控制。基于当前的扩散型音乐生成模型，我们引入表演条件 - 一种简单的工具，用于指定生成模型Synthesize music with specific instruments and timbre from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. More information, including samples and demonstrations, can be found at benadar293.github.io/midipm.Note: "FAD" stands for "Flexible Audio Database", which is a standard evaluation metric for music generation systems. A high FAD realism score indicates that the generated music sounds realistic and similar to the original recording.
</details></li>
</ul>
<hr>
<h2 id="The-Broad-Impact-of-Feature-Imitation-Neural-Enhancements-Across-Financial-Speech-and-Physiological-Domains"><a href="#The-Broad-Impact-of-Feature-Imitation-Neural-Enhancements-Across-Financial-Speech-and-Physiological-Domains" class="headerlink" title="The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains"></a>The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12279">http://arxiv.org/abs/2309.12279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Khanmohammadi, Tuka Alhanai, Mohammad M. Ghassemi</li>
<li>for: 这篇论文旨在测试对不同时间序列数据进行初始化神经网络的性能影响。</li>
<li>methods: 本研究使用Feature Imitating Networks（FIN）技术，将神经网络的初始化参数设置为模仿特定的关闭形式统计特征，以提高深度学习架构的性能。</li>
<li>results: 在Bitcoin价格预测任务中，将FIN给神经网络模型中的表现下降了约1000，对比基准模型。在语音情感识别任务中，将FIN与神经网络模型相结合后，提高了分类精度约3%。在chronic neck pain检测任务中，将FIN给神经网络模型中的表现提高了约7%，对比现有的分类器。这些发现证明了FIN在多种应用中的广泛应用和优化性。<details>
<summary>Abstract</summary>
Initialization of neural network weights plays a pivotal role in determining their performance. Feature Imitating Networks (FINs) offer a novel strategy by initializing weights to approximate specific closed-form statistical features, setting a promising foundation for deep learning architectures. While the applicability of FINs has been chiefly tested in biomedical domains, this study extends its exploration into other time series datasets. Three different experiments are conducted in this study to test the applicability of imitating Tsallis entropy for performance enhancement: Bitcoin price prediction, speech emotion recognition, and chronic neck pain detection. For the Bitcoin price prediction, models embedded with FINs reduced the root mean square error by around 1000 compared to the baseline. In the speech emotion recognition task, the FIN-augmented model increased classification accuracy by over 3 percent. Lastly, in the CNP detection experiment, an improvement of about 7 percent was observed compared to established classifiers. These findings validate the broad utility and potency of FINs in diverse applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:初始化神经网络权重的初始化方法对其性能产生决定性的影响。特征模仿网络（FIN）提供了一种新的策略，通过初始化权重来 aproximate 特定的关闭形式统计特征，为深度学习架构提供了一个良好的基础。虽然FIN的可用性主要在生物医学领域进行了证明，但这项研究尝试将其应用到其他时间序列数据集上。这个研究进行了三个不同的实验来测试FIN的可行性和表现：比特币价格预测、语音情感识别和慢性颈部疼痛检测。在比特币价格预测任务中，包含FIN的模型降低了根圆误差约1000比例。在语音情感识别任务中，FIN-加强模型提高了分类精度高达3%。最后，在慢性颈部疼痛检测任务中，FIN-加强模型与现有分类器相比，提高了约7%的性能。这些发现证明了FIN在多样化应用中的广泛适用性和强大性。
</details></li>
</ul>
<hr>
<h2 id="Soft-Merging-A-Flexible-and-Robust-Soft-Model-Merging-Approach-for-Enhanced-Neural-Network-Performance"><a href="#Soft-Merging-A-Flexible-and-Robust-Soft-Model-Merging-Approach-for-Enhanced-Neural-Network-Performance" class="headerlink" title="Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance"></a>Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12259">http://arxiv.org/abs/2309.12259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Yusen Wu, Phuong Nguyen, Chao Liu, Yelena Yesha</li>
<li>for: 提高深度学习模型的性能和稳定性</li>
<li>methods: 使用soft merging方法，通过学习门户参数来融合多个本地最优模型，并使用硬核分布来避免恶性模型的影响</li>
<li>results: 实验表明，使用融合模型可以提高深度学习模型的性能和稳定性，并且可以降低计算成本<details>
<summary>Abstract</summary>
Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and explicit learning process integrated with stochastic gradient descent. Thorough experiments underscore the effectiveness and superior performance of the merged neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Parallelizing-non-linear-sequential-models-over-the-sequence-length"><a href="#Parallelizing-non-linear-sequential-models-over-the-sequence-length" class="headerlink" title="Parallelizing non-linear sequential models over the sequence length"></a>Parallelizing non-linear sequential models over the sequence length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12252">http://arxiv.org/abs/2309.12252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Heng Lim, Qi Zhu, Joshua Selfridge, Muhammad Firmansyah Kasim</li>
<li>for:  acceleration of GPU evaluation of sequential models</li>
<li>methods:  parallel algorithm without special structure in the architecture</li>
<li>results:  training 10 times faster with no compromise on accuracy<details>
<summary>Abstract</summary>
Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work serves as the first step to unlock the potential of non-linear sequential models for long sequence problems.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本，对于具体的翻译请参考下面的详细翻译）Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long been limited by their sequential nature, leading to slow training times. Many believed that these models could not be parallelized, but we challenge this belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. Our method is applicable to a wide range of architectures and can train sequential models up to 10 times faster than traditional methods without any significant difference in training results. By overcoming the training bottleneck, our work paves the way for the potential of non-linear sequential models in long sequence problems.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-Automated-Audio-Captioning-via-text-only-training"><a href="#Weakly-supervised-Automated-Audio-Captioning-via-text-only-training" class="headerlink" title="Weakly-supervised Automated Audio Captioning via text only training"></a>Weakly-supervised Automated Audio Captioning via text only training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12242">http://arxiv.org/abs/2309.12242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zelaki/wsac">https://github.com/zelaki/wsac</a></li>
<li>paper_authors: Theodoros Kouzelis, Vassilis Katsouros</li>
<li>for: automatically generating descriptions for audio clips (AAC)</li>
<li>methods: weakly-supervised approach using text data and pre-trained CLAP model, with strategies to bridge the modality gap</li>
<li>results: relative performance of up to ~$83%$ compared to fully supervised approaches trained with paired target data on Clotho and AudioCaps datasets<details>
<summary>Abstract</summary>
In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to achieve a relative performance of up to ~$83\%$ compared to fully supervised approaches trained with paired target data.
</details>
<details>
<summary>摘要</summary>
近年来，带有音频和caption的数据集已经实现了自动生成音频描述的remarkable成功，即自动语音描述（AAC）。然而，收集到充足数量的带有音频和caption的数据集是时间和劳动密集的。鼓 motivated by recent advances in Contrastive Language-Audio Pretraining（CLAP），我们提出了一种弱监督的方法，通过假设只有文本数据和预训练的CLAP模型，解决了需要对Target数据进行监督的问题。我们的方法利用CLAP模型中的文本和音频嵌入的相似性。在训练过程中，我们学习将文本重建为CLAP文本嵌入，并在推理阶段使用音频嵌入进行解码。为了在模式之间减少差距，我们在训练和推理阶段使用了bridging策略。我们在Clotho和AudioCaps数据集上评估了我们的提议方法，并证明其能够实现相对于完全监督方法训练于带有Target数据的性能的$83\%$。
</details></li>
</ul>
<hr>
<h2 id="t-EER-Parameter-Free-Tandem-Evaluation-of-Countermeasures-and-Biometric-Comparators"><a href="#t-EER-Parameter-Free-Tandem-Evaluation-of-Countermeasures-and-Biometric-Comparators" class="headerlink" title="t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators"></a>t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12237">http://arxiv.org/abs/2309.12237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/takhemlata/t-eer">https://github.com/takhemlata/t-eer</a></li>
<li>paper_authors: Tomi Kinnunen, Kong Aik Lee, Hemlata Tak, Nicholas Evans, Andreas Nautsch</li>
<li>for: The paper is written to propose a new metric for the joint evaluation of presentation attack detection (PAD) solutions operating in tandem with biometric verification.</li>
<li>methods: The paper introduces a new metric called tandem equal error rate (t-EER) for evaluating the performance of PAD solutions in combination with biometric verification systems. The t-EER is a parameter-free metric that measures the equal error rate of both false alarms and misses at a set of operating points.</li>
<li>results: The paper demonstrates the application of the t-EER metric to a wide range of biometric system evaluations under attack, using both simulated and real scores for a voice biometrics application. The proposed approach is shown to be a strong candidate metric for the tandem evaluation of PAD systems and biometric comparators.Here is the simplified Chinese text for the three key information points:</li>
<li>for: 本文是为了提出一个新的 metric 用于 tandem 识别攻击检测 (PAD) 和生物特征验证系统的共同评估。</li>
<li>methods: 本文提出了一个名为 tandem 平等错误率 (t-EER) 的新metric，用于评估 PAD 解决方案和生物特征验证系统之间的共同性能。t-EER 是一个无参数的 metric，可以在多个操作点上测试 false alarm 和 miss 的平等错误率。</li>
<li>results: 本文使用了 simulated 和实际数据，对一个语音生物特征验证应用进行了广泛的评估。结果表明，tandem EER 是一个强andidate metric 用于 tandem 识别攻击检测和生物特征验证系统之间的评估。<details>
<summary>Abstract</summary>
Presentation attack (spoofing) detection (PAD) typically operates alongside biometric verification to improve reliablity in the face of spoofing attacks. Even though the two sub-systems operate in tandem to solve the single task of reliable biometric verification, they address different detection tasks and are hence typically evaluated separately. Evidence shows that this approach is suboptimal. We introduce a new metric for the joint evaluation of PAD solutions operating in situ with biometric verification. In contrast to the tandem detection cost function proposed recently, the new tandem equal error rate (t-EER) is parameter free. The combination of two classifiers nonetheless leads to a \emph{set} of operating points at which false alarm and miss rates are equal and also dependent upon the prevalence of attacks. We therefore introduce the \emph{concurrent} t-EER, a unique operating point which is invariable to the prevalence of attacks. Using both modality (and even application) agnostic simulated scores, as well as real scores for a voice biometrics application, we demonstrate application of the t-EER to a wide range of biometric system evaluations under attack. The proposed approach is a strong candidate metric for the tandem evaluation of PAD systems and biometric comparators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Smooth-ECE-Principled-Reliability-Diagrams-via-Kernel-Smoothing"><a href="#Smooth-ECE-Principled-Reliability-Diagrams-via-Kernel-Smoothing" class="headerlink" title="Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing"></a>Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12236">http://arxiv.org/abs/2309.12236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-calibration">https://github.com/apple/ml-calibration</a></li>
<li>paper_authors: Jarosław Błasiok, Preetum Nakkiran</li>
<li>for: 这篇论文主要研究了如何使用抽象函数来测量和解释抽象预测器的准确性。</li>
<li>methods: 这篇论文使用了Radius Band Function(RBF)核函数来平滑观测值，然后计算Expected Calibration Error(ECE)。</li>
<li>results: 这篇论文提出了一种新的准确性测量方法，即SmoothECE，可以减轻抽象预测器的准确性问题。此外，这篇论文还提供了一个Python包，可以简单地测量和可读地展示抽象预测器的准确性。<details>
<summary>Abstract</summary>
Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures -- binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE.   We also provide a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: `pip install relplot\`.
</details>
<details>
<summary>摘要</summary>
“测量和可靠图是概率预测器的二个基本工具。测量措施量化预测器的误差，并可以视觉化这种误差的结构。然而，通常的构建方法，例如桶化和ECE，都受到了知名的缺陷（例如缺陷）。我们显示，使用RBF核函数平滑观测值后，计算预测ERROR的Expected Calibration Error（ECE），可以得到一个良好的测量方法。对于适当的标Width选择，这种方法具有一定的稳定性（Błasiok等2023a），我们称之为SmoothECE。此外，从这个平滑函数中得到的可靠图可以视觉化SmoothECE，与桶化的可靠图相似。我们还提供了一个Python套件，包含了简单、无参数的方法来量化和Plot calibration：`pip install relplot\`.”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Smooth-Nash-Equilibria-Algorithms-and-Complexity"><a href="#Smooth-Nash-Equilibria-Algorithms-and-Complexity" class="headerlink" title="Smooth Nash Equilibria: Algorithms and Complexity"></a>Smooth Nash Equilibria: Algorithms and Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12226">http://arxiv.org/abs/2309.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantinos Daskalakis, Noah Golowich, Nika Haghtalab, Abhishek Shetty</li>
<li>for: 这篇论文旨在解决 Nash 平衡的计算复杂性问题，提出了一种名为 $\sigma$-粗化 Nash 平衡的变种，并研究了其计算性质。</li>
<li>methods: 该论文使用了简化分析的想法，引入了一种名为 $\sigma$-粗化 Nash 平衡的概念，并提出了两种不同的 $\sigma$-粗化 Nash 平衡变种：强型 $\sigma$-粗化 Nash 平衡和弱型 $\sigma$-粗化 Nash 平衡。</li>
<li>results: 论文表明，在常量 $\sigma$ 和 $\epsilon$ 以及玩家数量都是常数时，可以在常量时间内随机找到一个 $\epsilon$-相近 $\sigma$-粗化 Nash 平衡，而且在同样的参数 régime 下，可以在多项时间内寻找一个强型 $\epsilon$-相近 $\sigma$-粗化 Nash 平衡。这些结果与 Nash 平衡的优化算法不可避免的复杂性不同。<details>
<summary>Abstract</summary>
A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.   We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibria: when $\sigma$ as well as an approximation parameter $\epsilon$ and the number of players are all constants, there is a constant-time randomized algorithm to find a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibrium in normal-form games. In the same parameter regime, there is a polynomial-time deterministic algorithm to find a strong $\epsilon$-approximate $\sigma$-smooth Nash equilibrium in a normal-form game. These results stand in contrast to the optimal algorithm for computing $\epsilon$-approximate Nash equilibria, which cannot run in faster than quasipolynomial-time. We complement our upper bounds by showing that when either $\sigma$ or $\epsilon$ is an inverse polynomial, finding a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibria becomes computationally intractable.
</details>
<details>
<summary>摘要</summary>
《纳什平衡概念的基本缺陷》：纳什平衡的计算复杂性问题，在正常形游戏中，是PPAD困难的。在这篇论文中，我们根据精细分析的想法，引入一种Namedashed variant of Nash equilibrium，即$\sigma$-粗糙纳什平衡，其中$\sigma$是一个精度参数。在一个$\sigma$-粗糙纳什平衡中，玩家只需要实现 Utility 高于或等于它的最佳偏转策，这个策略是一个不太多的质量（如 parametrized by $\sigma$）的分布。我们将这种纳什平衡分为两种变种：强制 $\sigma$-粗糙纳什平衡，在平衡状态下，玩家需要采取 $\sigma$-粗糙策略，以及弱 $\sigma$-粗糙纳什平衡，没有这种要求。我们显示，在常数 $\sigma$ 和 Approximation parameter $\epsilon$ 以及玩家数量都是常数时，可以采取常数时间的随机算法来找到弱 $\epsilon$-近似 $\sigma$-粗糙纳什平衡，并且在同样的参数域内，可以采取多项时间的排序算法来找到强制 $\epsilon$-近似 $\sigma$-粗糙纳什平衡。这些结果与纳什平衡的优化算法不同，后者无法在超过半定时的情况下运行。我们补充了我们的上界，表明当 $\sigma$ 或 $\epsilon$ 是反射函数时，找到弱 $\epsilon$-近似 $\sigma$-粗糙纳什平衡就变得计算困难。
</details></li>
</ul>
<hr>
<h2 id="Regionally-Additive-Models-Explainable-by-design-models-minimizing-feature-interactions"><a href="#Regionally-Additive-Models-Explainable-by-design-models-minimizing-feature-interactions" class="headerlink" title="Regionally Additive Models: Explainable-by-design models minimizing feature interactions"></a>Regionally Additive Models: Explainable-by-design models minimizing feature interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12215">http://arxiv.org/abs/2309.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/givasile/RAM">https://github.com/givasile/RAM</a></li>
<li>paper_authors: Vasilis Gkolemis, Anargiros Tzerefos, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou</li>
<li>for: 本研究旨在提出一种新的可解释模型，即区域添加模型（RAMs），用于解决基于多特征的机器学习问题中，加比模型（GAMs）的缺陷。</li>
<li>methods: 本研究提出了一种三步法则，首先使用黑盒模型进行训练，然后使用区域效果图来确定特征空间中的子区域，最后在每个子区域中采用加比模型来表示输出。</li>
<li>results: 实验结果表明，RAMs 比 GAMs 更高的表达能力，同时保持可解释性。<details>
<summary>Abstract</summary>
Generalized Additive Models (GAMs) are widely used explainable-by-design models in various applications. GAMs assume that the output can be represented as a sum of univariate functions, referred to as components. However, this assumption fails in ML problems where the output depends on multiple features simultaneously. In these cases, GAMs fail to capture the interaction terms of the underlying function, leading to subpar accuracy. To (partially) address this issue, we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models. RAMs identify subregions within the feature space where interactions are minimized. Within these regions, it is more accurate to express the output as a sum of univariate functions (components). Consequently, RAMs fit one component per subregion of each feature instead of one component per feature. This approach yields a more expressive model compared to GAMs while retaining interpretability. The RAM framework consists of three steps. Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we identify subregions where the black-box model exhibits near-local additivity. Lastly, we fit a GAM component for each identified subregion. We validate the effectiveness of RAMs through experiments on both synthetic and real-world datasets. The results confirm that RAMs offer improved expressiveness compared to GAMs while maintaining interpretability.
</details>
<details>
<summary>摘要</summary>
通用加тив模型（GAMs）广泛应用于不同领域的解释性模型中。GAMs假设输出可以表示为一些单变量函数的总和，称为组件。然而，在机器学习问题中，输出受多个特征的同时影响，这个假设失败。在这些情况下，GAMs无法捕捉输出函数的交叉项，导致准确率下降。为解决这个问题，我们提出了区域加тив模型（RAMs），一种新的解释性模型类型。RAMs确定特征空间中的子区域，在这些子区域中，交叉项的影响最小。因此，RAMs采用一个组件来描述每个特征的子区域中的输出。相比GAMs，RAMs采用更加表达力的模型，同时保持可解释性。RAMs的框架包括三个步骤：首先，我们训练黑盒模型；其次，使用地方效果图来确定特征空间中的子区域，这些子区域中黑盒模型的地方效果较低；最后，我们采用GAM组件来描述每个确定的子区域。我们通过对synthetic和实际数据进行实验，证明RAMs可以提高表达力，同时保持可解释性。结果表明，RAMs比GAMs更好地适应机器学习问题。
</details></li>
</ul>
<hr>
<h2 id="SupeRBNN-Randomized-Binary-Neural-Network-Using-Adiabatic-Superconductor-Josephson-Devices"><a href="#SupeRBNN-Randomized-Binary-Neural-Network-Using-Adiabatic-Superconductor-Josephson-Devices" class="headerlink" title="SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices"></a>SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12212">http://arxiv.org/abs/2309.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengang Li, Geng Yuan, Tomoharu Yamauchi, Zabihi Masoud, Yanyue Xie, Peiyan Dong, Xulong Tang, Nobuyuki Yoshikawa, Devesh Tiwari, Yanzhi Wang, Olivia Chen</li>
<li>For: 这 paper 是为了提出一种基于 Adiabatic Quantum-Flux-Parametron (AQFP) 的 randomized Binary Neural Network (BNN) 加速器框架 SupeRBNN，以解决 AQFP 设备在 BNN 计算中的一些关键挑战。* Methods: 这 paper 使用了 AQFP 设备的特殊极性来 denote логические值，并提出了一种基于随机行为的 BNN 加速器框架 SupeRBNN，包括一种随机计算模块和一种时钟调整 Based 电路优化策略。* Results:  compared 于不同技术的实现，包括 CMOS、ReRAM 和超导器 RSFQ&#x2F;ERSFQ，这 paper 的设计在多个数据集和网络架构上进行了验证，并达到了约 7.8 x 10^4 倍于 ReRAM-based BNN 框架的能量效率，同时保持了相似的模型准确率。此外，与超导器基本设备相比，这 paper 的设计在最少两个数量级上高于能量效率。<details>
<summary>Abstract</summary>
Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with extremely high energy efficiency. By employing the distinct polarity of current to denote logic `0' and `1', AQFP devices serve as excellent carriers for binary neural network (BNN) computations. Although recent research has made initial strides toward developing an AQFP-based BNN accelerator, several critical challenges remain, preventing the design from being a comprehensive solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN acceleration framework that leverages software-hardware co-optimization to eventually make the AQFP devices a feasible solution for BNN acceleration. Specifically, we investigate the randomized behavior of the AQFP devices and analyze the impact of crossbar size on current attenuation, subsequently formulating the current amplitude into the values suitable for use in BNN computation. To tackle the accumulation problem and improve overall hardware performance, we propose a stochastic computing-based accumulation module and a clocking scheme adjustment-based circuit optimization method. We validate our SupeRBNN framework across various datasets and network architectures, comparing it with implementations based on different technologies, including CMOS, ReRAM, and superconducting RSFQ/ERSFQ. Experimental results demonstrate that our design achieves an energy efficiency of approximately 7.8x10^4 times higher than that of the ReRAM-based BNN framework while maintaining a similar level of model accuracy. Furthermore, when compared with superconductor-based counterparts, our framework demonstrates at least two orders of magnitude higher energy efficiency.
</details>
<details>
<summary>摘要</summary>
adiabatic量子流 Parametron (AQFP) 是一种超导逻辑，具有极高的能效性。通过使用流动中的极性来表示逻辑“0”和“1”，AQFP设备成为优秀的二进制神经网络（BNN）计算器。虽然最近的研究已经做出了初步的进展，但是还有许多关键的挑战，使得设计无法成为全面的解决方案。在这篇论文中，我们提出了SupeRBNN框架，它是基于AQFP的随机BNN加速器。我们研究了AQFP设备的随机行为，并分析了跨栅大小对流动强度的影响，从而将流动强度转换为适合BNN计算的值。为了解决积累问题并提高硬件性能，我们提出了随机计算模块和时钟调整缓存器优化方法。我们在不同的 datasets 和网络架构上验证了我们的SupeRBNN框架，并与不同技术的实现进行比较，包括CMOS、ReRAM 和超导器RSFQ/ERSFQ。实验结果表明，我们的设计可以达到约7.8×10^4倍高于ReRAM基于BNN框架的能效性，同时保持相同的模型准确性水平。此外，与超导器基于counterparts 相比，我们的框架可以达到至少两个数量级的高效性。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-State-space-Neural-Networks-for-Transport-Phenomena"><a href="#Physics-informed-State-space-Neural-Networks-for-Transport-Phenomena" class="headerlink" title="Physics-informed State-space Neural Networks for Transport Phenomena"></a>Physics-informed State-space Neural Networks for Transport Phenomena</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12211">http://arxiv.org/abs/2309.12211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay J Dave, Richard B. Vilim</li>
<li>for: 本研究开展了一种名为物理数据驱动模型（PSM），用于实时优化、灵活性和故障忍受性在自主系统中，特别是在化学、生物医学和电力等领域的交通主导系统。传统的数据驱动方法缺乏物理限制，PSMs 则通过训练深度神经网络和物理约束使用组件的偏微分方程（PDE），实现物理约束的、端到端可微分前向动力模型。</li>
<li>methods: PSMs 使用感知器数据和物理约束组件的 PDE 训练深度神经网络，以实现物理约束的、端到端可微分前向动力模型。</li>
<li>results: 通过两个在 Silico 实验（一个热通道和一个冷却系统循环），我们证明 PSMs 比传统的数据驱动模型更加准确。此外，PSMs 还有许多优势，例如可以处理常数和时间依赖的约束，并且可以用于系统诊断和故障检测。<details>
<summary>Abstract</summary>
This work introduces Physics-informed State-space neural network Models (PSMs), a novel solution to achieving real-time optimization, flexibility, and fault tolerance in autonomous systems, particularly in transport-dominated systems such as chemical, biomedical, and power plants. Traditional data-driven methods fall short due to a lack of physical constraints like mass conservation; PSMs address this issue by training deep neural networks with sensor data and physics-informing using components' Partial Differential Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable forward dynamics model. Through two in silico experiments - a heated channel and a cooling system loop - we demonstrate that PSMs offer a more accurate approach than purely data-driven models.   Beyond accuracy, there are several compelling use cases for PSMs. In this work, we showcase two: the creation of a nonlinear supervisory controller through a sequentially updated state-space representation and the proposal of a diagnostic algorithm using residuals from each of the PDEs. The former demonstrates the ability of PSMs to handle both constant and time-dependent constraints, while the latter illustrates their value in system diagnostics and fault detection. We further posit that PSMs could serve as a foundation for Digital Twins, constantly updated digital representations of physical systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Creation of a nonlinear supervisory controller through a sequentially updated state-space representation, demonstrating the ability of PSMs to handle both constant and time-dependent constraints.2. Proposal of a diagnostic algorithm using residuals from each of the PDEs, illustrating the value of PSMs in system diagnostics and fault detection.We also suggest that PSMs could serve as a foundation for Digital Twins, constantly updated digital representations of physical systems.</details></li>
</ol>
<hr>
<h2 id="Boolformer-Symbolic-Regression-of-Logic-Functions-with-Transformers"><a href="#Boolformer-Symbolic-Regression-of-Logic-Functions-with-Transformers" class="headerlink" title="Boolformer: Symbolic Regression of Logic Functions with Transformers"></a>Boolformer: Symbolic Regression of Logic Functions with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12207">http://arxiv.org/abs/2309.12207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdascoli/boolformer">https://github.com/sdascoli/boolformer</a></li>
<li>paper_authors: Stéphane d’Ascoli, Samy Bengio, Josh Susskind, Emmanuel Abbé</li>
<li>for: 这个论文旨在介绍一种名为Boolformer的 transformer 架构，用于进行端到端 симвоlic regression 的整数函数预测。</li>
<li>methods: 这种架构使用 clean truth table 预测复杂函数，并在 incomplete 和噪声观察下找到approximate表达。</li>
<li>results: 在各种实际 binary classification 问题上进行了评估，并在模型动态遗传网络中得到了竞争力。 code 和模型都公开 available。<details>
<summary>Abstract</summary>
In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了 Boolformer，首个基于Transformer架构的端到端符号 regression 的布尔函数搜索算法。我们首先表明，它可以预测复杂函数的简洁公式，当提供了干净的真实表格时。然后，我们证明了它在提供不完整和噪音观测时可以找到approximate表达。我们对一组实际的二分类 datasets 进行了评估，表明它可以作为可读性的代替方法。最后，我们将其应用到了模拟生物学网络的动态方面，使用最新的benchmark，我们发现它与当前的遗传算法竞赛得分，但速度快得多个数量级。我们的代码和模型公共可用。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Conditional-Inference-in-Adaptive-Experiments"><a href="#Optimal-Conditional-Inference-in-Adaptive-Experiments" class="headerlink" title="Optimal Conditional Inference in Adaptive Experiments"></a>Optimal Conditional Inference in Adaptive Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12162">http://arxiv.org/abs/2309.12162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiafeng Chen, Isaiah Andrews</li>
<li>for: 研究批处理强化实验，即在实验中采样批处理时，可能会采用不同的批处理策略和目标参数，并且可能会在实验过程中动态地更新这些参数。</li>
<li>methods: 使用了批处理强化实验中的实验设计和数据分析技术，包括使用最后一批数据进行推断和估计。</li>
<li>results: 研究结果表明，在不假设批处理策略和目标参数的情况下，使用最后一批数据进行推断和估计是最佳的。此外，当批处理策略和目标参数是位置不变的（即不受数据的影响）时，可以通过一个额外的线性函数来捕捉更多的信息。<details>
<summary>Abstract</summary>
We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.
</details>
<details>
<summary>摘要</summary>
我们研究批处bandit实验，考虑实验中的批处时间、分配概率和目标参数都可能被选择适应信息，直到最后一批。不受其他限制，我们显示在只使用最后一批结果时进行推断是优化的。当批处方面的可靠性是位置不变的，即批处arm的均值在所有批处时间下都是不变的，我们显示存在一个额外的线性函数，捕捉了批处arm的均值。在更紧张的情况下，停止时间、分配概率和目标参数都知道通过数据来，我们 derivates可运行的和优化的条件推断过程。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-and-Truly-Large-Scale-Audio-Sheet-Music-Retrieval"><a href="#Towards-Robust-and-Truly-Large-Scale-Audio-Sheet-Music-Retrieval" class="headerlink" title="Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval"></a>Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12158">http://arxiv.org/abs/2309.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Carvalho, Gerhard Widmer</li>
<li>for: 本研究的目的是探讨cross-modal music information retrieval的最新方法和技术，特别是将音频和乐谱图像连接到相同的音乐内容上。</li>
<li>methods: 本研究使用深度学习架构来学习joint embedding空间，将音频和乐谱图像modalities相连接。</li>
<li>results: 本研究提出了许多挑战，包括robustness和大规模应用等问题，并 documenated step-by-step improvement along several dimensions。<details>
<summary>Abstract</summary>
A range of applications of multi-modal music information retrieval is centred around the problem of connecting large collections of sheet music (images) to corresponding audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task employs cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities - audio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an insightful examination of the current developments on audio-sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenarios. We then highlight the steps we have taken so far to address some of these challenges, documenting step-by-step improvement along several dimensions. We conclude by analysing the remaining challenges and present ideas for solving these, in order to pave the way to a unified and robust methodology for cross-modal music retrieval.
</details>
<details>
<summary>摘要</summary>
多种多Modal music信息检索的应用集中在将大量的乐谱图像（图像）与对应的音频录音相连接，即 identificador pairs of audio和乐谱摘要段 refer to the same musical content。一种常见的最近的方法是使用交叉模态深度学习建筑来学习联结这两种不同的模态——音频和乐谱图像。随着过去几年的不断改进，但还有一些打开的问题阻碍了大规模应用这种方法。在这篇文章中，我们尝试提供深入的检查当前的深度学习方法在Audio-Sheet music检索方面的发展。我们首先确定了在实际应用中Robust和大规模交叉模态音乐检索的主要挑战。然后，我们高亮了我们已经做出的努力，并记录了一些维度上的改进。 finally，我们分析了剩下的挑战，并提出了解决这些挑战的想法，以便开拓出一种统一和Robust的交叉模态音乐检索方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Contrastive-Learning-for-Robust-Audio-Sheet-Music-Retrieval-Systems"><a href="#Self-Supervised-Contrastive-Learning-for-Robust-Audio-Sheet-Music-Retrieval-Systems" class="headerlink" title="Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems"></a>Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12134">http://arxiv.org/abs/2309.12134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Luis Carvalho, Tobias Washüttl, Gerhard Widmer</li>
<li>for: 提高跨模态音乐检索系统的效果</li>
<li>methods: 使用自我超vision学习法，通过对各种模式的杂音训练网络，从实际音乐内容中提取有用的特征</li>
<li>results: 在多种实验中，预训练模型能够更好地回归 audio 和 sheet 图像中的片断，并在跨模态作品识别任务中提高检索质量<details>
<summary>Abstract</summary>
Linking sheet music images to audio recordings remains a key problem for the development of efficient cross-modal music retrieval systems. One of the fundamental approaches toward this task is to learn a cross-modal embedding space via deep neural networks that is able to connect short snippets of audio and sheet music. However, the scarcity of annotated data from real musical content affects the capability of such methods to generalize to real retrieval scenarios. In this work, we investigate whether we can mitigate this limitation with self-supervised contrastive learning, by exposing a network to a large amount of real music data as a pre-training step, by contrasting randomly augmented views of snippets of both modalities, namely audio and sheet images. Through a number of experiments on synthetic and real piano data, we show that pre-trained models are able to retrieve snippets with better precision in all scenarios and pre-training configurations. Encouraged by these results, we employ the snippet embeddings in the higher-level task of cross-modal piece identification and conduct more experiments on several retrieval configurations. In this task, we observe that the retrieval quality improves from 30% up to 100% when real music data is present. We then conclude by arguing for the potential of self-supervised contrastive learning for alleviating the annotated data scarcity in multi-modal music retrieval models.
</details>
<details>
<summary>摘要</summary>
链接Sheet music图像到音频录音的问题是多媒体音乐检索系统的关键问题。一种基本的方法是通过深度神经网络学习一个跨Modal空间，将 audio和Sheet music之间的连接短暂的音频和Sheet music片段。然而，实际音乐内容的罕见标注数据限制了这些方法的泛化能力。在这项工作中，我们研究了是否可以通过自动学习对比学习来缓解这种限制，通过对 audio和Sheet music之间的随机扩展后的视图进行对比。通过一系列的实验，我们发现在所有场景和预训练配置下，预训练模型都能够更好地检索片段。鼓动了这些结果，我们使用片段嵌入在高级任务中的跨Modal段落识别中，进行更多的实验。在这个任务中，我们发现，当有实际音乐数据时，检索质量从30%提高到100%。最后，我们结论，自动学习对比学习可以减轻多媒体音乐检索模型中的标注数据稀缺。
</details></li>
</ul>
<hr>
<h2 id="Convergence-and-Recovery-Guarantees-of-Unsupervised-Neural-Networks-for-Inverse-Problems"><a href="#Convergence-and-Recovery-Guarantees-of-Unsupervised-Neural-Networks-for-Inverse-Problems" class="headerlink" title="Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems"></a>Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12128">http://arxiv.org/abs/2309.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Buskulic, Jalal Fadili, Yvain Quéau</li>
<li>for: 解决 inverse problems</li>
<li>methods: 使用 unsupervised feedforward multilayer neural networks 和 deterministic convergence and recovery guarantees</li>
<li>results: 提供了对这类网络的 deterministic convergence and recovery guarantees，并 derive overparametrization boundsHere’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文目的是解决 inverse problems</li>
<li>methods: 使用 unsupervised feedforward multilayer neural networks 和 deterministic convergence and recovery guarantees</li>
<li>results: 本文提供了对这类网络的 deterministic convergence and recovery guarantees，并 derive overparametrization bounds<details>
<summary>Abstract</summary>
Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
</details>
<details>
<summary>摘要</summary>
“神经网络在最近几年内已成为解决反问题的主要方法之一。虽然大量的方法被开发来解决反问题，但我们仍然缺乏明确的理论保证。然而，许多研究证明了神经网络在更一般的设置下 converges to optimal solutions，使用过参数化来控制神经 Tangent Kernel。在这个工作中，我们尝试将这两个世界联系起来，并提供反问题的推理和恢复保证 для无监督Feedforward多层神经网络。我们还计算了过参数化的下限，表明在使用滑动函数 activation 时，两层 Deep Inverse Prior 网络会受益于我们的保证。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Passage-Summarization-with-Recurrent-Models-for-Audio-Sheet-Music-Retrieval"><a href="#Passage-Summarization-with-Recurrent-Models-for-Audio-Sheet-Music-Retrieval" class="headerlink" title="Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval"></a>Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12111">http://arxiv.org/abs/2309.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Carvalho, Gerhard Widmer</li>
<li>for: 这篇论文关注的是如何将Sheet Music和音频录音连接起来，以便进行跨modal音乐检索。</li>
<li>methods: 该论文提出了一种使用深度神经网络学习Sheet Music和音频录音之间的共同 embedding空间，并通过适当的相似性结构来连接它们。</li>
<li>results: 该论文通过设计一种循环网络，解决了训练神经网络所需的强相关数据和音乐和音频之间的抽象差异问题，并在实验中表明了该方法可以更高度准确地进行跨modal音乐检索。<details>
<summary>Abstract</summary>
Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.
</details>
<details>
<summary>摘要</summary>
很多跨Modal音乐检索应用都与将乐谱图像与音频记录连接起来。一种常见的方法是通过深度神经网络学习一个共同嵌入空间，使得短时间内的音频和乐谱图像之间存在相似性结构。然而，这种方法存在两个挑战：首先，需要强相关的数据来训练网络，其次，音频和乐谱图像中的音乐内容之间的本地和全局滥讲差异会导致各种非线性。在这篇论文中，我们解决这两个缺陷，通过设计一种跨Modal循环网络，学习联合嵌入空间，可以摘要长passage的相应音频和乐谱图像。我们的方法的优点是：只需弱相关的音频-乐谱图像对，以及循环网络可以处理非线性，带来更高的检索精度。我们在合成和实际钢琴数据和谱面上进行了一系列实验，表明我们的提议的循环方法可以在所有可能的配置下实现更高的检索精度。
</details></li>
</ul>
<hr>
<h2 id="Clustering-based-Domain-Incremental-Learning"><a href="#Clustering-based-Domain-Incremental-Learning" class="headerlink" title="Clustering-based Domain-Incremental Learning"></a>Clustering-based Domain-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12078">http://arxiv.org/abs/2309.12078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SOYJUN/Implement-ODR-protocol">https://github.com/SOYJUN/Implement-ODR-protocol</a></li>
<li>paper_authors: Christiaan Lamers, Rene Vidal, Nabil Belbachir, Niki van Stein, Thomas Baeck, Paris Giampouras</li>
<li>for:  solves the catastrophic forgetting problem in domain-incremental learning, a setting that was previously unsolved.</li>
<li>methods:  uses an online clustering-based approach on a dynamically updated finite pool of samples or gradients to alleviate the need for task information.</li>
<li>results:  experiments on real datasets demonstrate the effectiveness of the proposed strategy and its promising performance compared to state-of-the-art methods.<details>
<summary>Abstract</summary>
We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called "catastrophic forgetting problem", in which the performance of the learner in an "old task" decreases when subsequently trained on a "new task". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the hardest settings, namely: domain-incremental learning, a setting for which the problem was previously unsolved. We showcase the benefits of our approach by applying these ideas to projection-based methods, such as A-GEM and OGD, which lead to task-agnostic versions of them. Experiments on real datasets demonstrate the effectiveness of the proposed strategy and its promising performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们考虑了多个任务在连续学习设定下学习问题，在数据流式方式下提供不同任务的数据给学习者。一个关键问题在这个设定下是所谓的“追忆问题”（catastrophic forgetting），即学习者在学习新任务时，对于之前学习的任务的性能下降。现有的连续学习方法，如Averaged Gradient Episodic Memory（A-GEM）和Orthogonal Gradient Descent（OGD），解决了追忆问题 by 将当前任务的损失降到最低，而不会提高之前任务的损失。但这些方法假设学习者知道任务的变化，这在实践中是不现实的。在这篇论文中，我们使用在线 clustering-based 方法，对于动态更新的有限池的样本或梯度进行处理，从而成功地避免了追忆问题。我们在域逐学习设定下应用这些想法，并将其应用到投影基本方法，如A-GEM 和 OGD，从而实现了任务无关的版本。实验结果表明，提议的策略有效地解决了追忆问题，并在实际数据上达到了比state-of-the-art方法更高的性能。
</details></li>
</ul>
<hr>
<h2 id="S-GBDT-Frugal-Differentially-Private-Gradient-Boosting-Decision-Trees"><a href="#S-GBDT-Frugal-Differentially-Private-Gradient-Boosting-Decision-Trees" class="headerlink" title="S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees"></a>S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12041">http://arxiv.org/abs/2309.12041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Kirsche, Thorsten Peinemann, Joshua Stock, Carlos Cotrini, Esfandiar Mohammadi</li>
<li>For: The paper aims to develop a privacy-preserving learning method for gradient boosting decision trees (GBDT) that provides strong utility-privacy tradeoffs for tabular data.* Methods: The proposed method uses four techniques to improve the utility-privacy tradeoff: (1) an improved noise scaling approach with tighter accounting of privacy leakage, (2) integration of individual R&#39;enyi filters, (3) incorporation of random decision tree splits, and (4) subsampling for privacy amplification.* Results: The proposed method achieves high accuracy on two datasets (Abalone and Adult) with varying levels of privacy parameters (ε). Specifically, on the Abalone dataset, the proposed method achieves a $R^2$-score of 0.39 for ε&#x3D;0.15, which is the closest prior work only achieved for ε&#x3D;10.0. On the Adult dataset, the proposed method achieves a test error of 18.7% for ε&#x3D;0.07, which is the closest prior work only achieved for ε&#x3D;1.0. The proposed method also achieves high accuracy on the Abalone dataset for higher privacy parameters (ε&#x3D;0.54) and is very close to the accuracy of the non-private version of GBDT on the Adult dataset (ε&#x3D;0.54).<details>
<summary>Abstract</summary>
Privacy-preserving learning of gradient boosting decision trees (GBDT) has the potential for strong utility-privacy tradeoffs for tabular data, such as census data or medical meta data: classical GBDT learners can extract non-linear patterns from small sized datasets. The state-of-the-art notion for provable privacy-properties is differential privacy, which requires that the impact of single data points is limited and deniable. We introduce a novel differentially private GBDT learner and utilize four main techniques to improve the utility-privacy tradeoff. (1) We use an improved noise scaling approach with tighter accounting of privacy leakage of a decision tree leaf compared to prior work, resulting in noise that in expectation scales with $O(1/n)$, for $n$ data points. (2) We integrate individual R\'enyi filters to our method to learn from data points that have been underutilized during an iterative training process, which -- potentially of independent interest -- results in a natural yet effective insight to learning streams of non-i.i.d. data. (3) We incorporate the concept of random decision tree splits to concentrate privacy budget on learning leaves. (4) We deploy subsampling for privacy amplification. Our evaluation shows for the Abalone dataset ($<4k$ training data points) a $R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work only achieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training data points) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which the closest prior work only achieved for $\varepsilon=1.0$. For the Abalone dataset for $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to the $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult dataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is very close to the test error $13.7\,\%$ of the nonprivate version of GBDT.
</details>
<details>
<summary>摘要</summary>
privacy-preserving 学习树 boosting 算法（GBDT）在 tabular 数据上有强大的用户-隐私交易，例如人口普查数据或医疗特征数据：经典 GBDT 学习器可以从小型数据集中提取非线性模式。我们引入了一种新的具有证明性隐私性质的 GBDT 学习器，并使用四种主要技术来改善用户-隐私交易。1. 我们使用改进的噪声扩大方法，对决策树叶节点的隐私泄露进行更精细的评估，从而使噪声在预期中呈线性关系，与数据点数 $n$ 成正比。2. 我们将个体 R\'enyi 筛选器 integrate 到我们的方法中，以学习尚未被利用的数据点，这可能是独立有趣的发现，并且自然地带来一种有效的学习流程。3. 我们启用随机决策树分裂的概念，以集中隐私预算在学习叶节点上。4. 我们使用采样来增强隐私压缩。我们的评估显示，在 Abalone 数据集（训练数据点数 fewer than 4k）中，我们在 $\varepsilon=0.15$ 下达到 $R^2$ 分数为 0.39，而最近的相关工作只能在 $\varepsilon=10.0$ 下达到这个分数。在 Adult 数据集（训练数据点数 50k）中，我们在 $\varepsilon=0.07$ 下达到测试错误率为 18.7%，而最近的相关工作只能在 $\varepsilon=1.0$ 下达到这个错误率。在 Abalone 数据集中，在 $\varepsilon=0.54$ 下，我们达到 $R^2$ 分数为 0.47，几乎与非隐私版 GBDT 的 $R^2$ 分数相同。在 Adult 数据集中，在 $\varepsilon=0.54$ 下，我们达到测试错误率为 17.1%，几乎与非隐私版 GBDT 的测试错误率相同。
</details></li>
</ul>
<hr>
<h2 id="Uplift-vs-predictive-modeling-a-theoretical-analysis"><a href="#Uplift-vs-predictive-modeling-a-theoretical-analysis" class="headerlink" title="Uplift vs. predictive modeling: a theoretical analysis"></a>Uplift vs. predictive modeling: a theoretical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12036">http://arxiv.org/abs/2309.12036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theoverhelst/uplift-predictive-paper">https://github.com/theoverhelst/uplift-predictive-paper</a></li>
<li>paper_authors: Théo Verhelst, Robin Petit, Wouter Verbeke, Gianluca Bontempi</li>
<li>for: 本研究旨在探讨机器学习技术在决策中的可加价效应，并对相关的论证和实践提供了一个全面的探讨。</li>
<li>methods: 本研究使用了一种既包含了机器学习技术又具有 causal orientation 的方法，并对这种方法的性能进行了 theoretically 的分析和实践验证。</li>
<li>results: 研究结果显示，在某些情况下，可加价模型会在predictive 模型之上带来更高的效果，但是这些情况下的参数会影响这种效果。研究还发现，mutual information 和 estimator variance 等参数具有重要作用。<details>
<summary>Abstract</summary>
Despite the growing popularity of machine-learning techniques in decision-making, the added value of causal-oriented strategies with respect to pure machine-learning approaches has rarely been quantified in the literature. These strategies are crucial for practitioners in various domains, such as marketing, telecommunications, health care and finance. This paper presents a comprehensive treatment of the subject, starting from firm theoretical foundations and highlighting the parameters that influence the performance of the uplift and predictive approaches. The focus of the paper is on a binary outcome case and a binary action, and the paper presents a theoretical analysis of uplift modeling, comparing it with the classical predictive approach. The main research contributions of the paper include a new formulation of the measure of profit, a formal proof of the convergence of the uplift curve to the measure of profit ,and an illustration, through simulations, of the conditions under which predictive approaches still outperform uplift modeling. We show that the mutual information between the features and the outcome plays a significant role, along with the variance of the estimators, the distribution of the potential outcomes and the underlying costs and benefits of the treatment and the outcome.
</details>
<details>
<summary>摘要</summary>
The main research contributions of the paper include:1. A new formulation of the measure of profit.2. A formal proof of the convergence of the uplift curve to the measure of profit.3. An illustration, through simulations, of the conditions under which predictive approaches still outperform uplift modeling.We show that the mutual information between the features and the outcome plays a significant role, along with the variance of the estimators, the distribution of the potential outcomes, and the underlying costs and benefits of the treatment and the outcome.
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-Loop-Causal-Discovery-under-Latent-Confounding-using-Ancestral-GFlowNets"><a href="#Human-in-the-Loop-Causal-Discovery-under-Latent-Confounding-using-Ancestral-GFlowNets" class="headerlink" title="Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets"></a>Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12032">http://arxiv.org/abs/2309.12032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago da Silva, Eliezer Silva, Adèle Ribeiro, António Góis, Dominik Heider, Samuel Kaski, Diego Mesquita</li>
<li>for: 提高 causal inference 的精度，尤其是在数据稀缺时，避免因为 latent confounders 的影响而导致的不准确的 causal relation 推断。</li>
<li>methods: 提出一种基于 generative flow networks 的方法，通过在 candidate graphs 中采样 proportionally to a belief distribution 以及通过对 experts 的反馈来 iteratively 缩小 uncertainty 。</li>
<li>results: 通过实验表明，该方法可以准确地采样 ancestral graphs  distribution，并且可以通过人类反馈来改进推断质量。<details>
<summary>Abstract</summary>
Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid."中文翻译：<<SYS>> translate "结构学习是 causal inference 的核心。特别是在数据稀缺时， causal discovery（CD）算法容易导致不准确的 causal 关系推断，而且这些关系可能与专家知识相悖。此外，大多数 CD 方法不提供不确定性估计，使得用户不能正确地 интерпретирова结果并改进推断过程。很奇怪的是，CD 是人类中心的事物，但没有任何研究旨在构建可以 outputs 不确定性估计并且与专家交互改进 CD 的方法。为解决这些问题，我们开始由 proposing 使用 generative flow 网络来样本 (causal) 祖先图 proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC)。然后，我们利用候选图的多样性并引入最佳实验设计，以便逐次询问专家关于变量之间的关系，从而减少我们对祖先图的不确定性。最后，我们更新样本以包括人类反馈 via importance sampling。重要的是，我们的方法不需要 causal sufficiency (i.e., unobserved confounders may exist)。我们在 sintetic observational data 上进行了实验，结果表明我们的方法可以准确样本 distribution over ancestral graphs，并且可以通过人类帮助提高推断质量。
</details></li>
</ul>
<hr>
<h2 id="Robust-Approximation-Algorithms-for-Non-monotone-k-Submodular-Maximization-under-a-Knapsack-Constraint"><a href="#Robust-Approximation-Algorithms-for-Non-monotone-k-Submodular-Maximization-under-a-Knapsack-Constraint" class="headerlink" title="Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint"></a>Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12025">http://arxiv.org/abs/2309.12025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tantdhvan/KSE2023">https://github.com/tantdhvan/KSE2023</a></li>
<li>paper_authors: Dung T. K. Ha, Canh V. Pham, Tan D. Tran, Huan X. Hoang</li>
<li>for: 提出了一个非MONOTONE $k$-submodular最大化问题，用于数据概要、信息传递等应用。</li>
<li>methods: 提出了两种杜林约化算法，可以在$O(nk)$查询复杂度下提供竞争性提高的解。</li>
<li>results: 算法可以在$O(nk)$查询复杂度下提供常数约化比率，比现有算法快速返回解。实验结果也证明了算法的理论分析和实际效果。<details>
<summary>Abstract</summary>
The problem of non-monotone $k$-submodular maximization under a knapsack constraint ($\kSMK$) over the ground set size $n$ has been raised in many applications in machine learning, such as data summarization, information propagation, etc. However, existing algorithms for the problem are facing questioning of how to overcome the non-monotone case and how to fast return a good solution in case of the big size of data. This paper introduces two deterministic approximation algorithms for the problem that competitively improve the query complexity of existing algorithms.   Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within $O(nk)$ query complexity. The second one, $\RLA$, improves the approximation ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input parameter.   Our algorithms are the first ones that provide constant approximation ratios within only $O(nk)$ query complexity for the non-monotone objective. They, therefore, need fewer the number of queries than state-of-the-the-art ones by a factor of $\Omega(\log n)$.   Besides the theoretical analysis, we have evaluated our proposed ones with several experiments in some instances: Influence Maximization and Sensor Placement for the problem. The results confirm that our algorithms ensure theoretical quality as the cutting-edge techniques and significantly reduce the number of queries.
</details>
<details>
<summary>摘要</summary>
“非单调 $k$-submodular最大化问题（$\kSMK$) 在机器学习应用中得到了很多关注，例如摘要、信息传递等。然而，现有的算法对这个问题存在两个问题：一是如何解决非单调情况，二是如何快速返回良好的解决方案。本文提出了两个决定性近似算法，它们可以对 $\kSMK$ 问题提供竞争性提高查询量的解决方案。我们的第一个算法（$\LAA$）可以在 $O(nk)$ 查询量下提供一个近似比率为 $1/19$。第二个算法（$\RLA$）可以在 $O(nk)$ 查询量下提供一个近似比率为 $1/5-\epsilon$，其中 $\epsilon$ 是输入参数。我们的算法是第一个可以在非单调情况下提供常数近似比率，并且需要 fewer 查询量 than state-of-the-art 的一个因数为 $\Omega(\log n)$。”Note that the translation is in Simplified Chinese, which is one of the two standardized Chinese writing systems. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-SAEAs-with-Unevaluated-Solutions-A-Case-Study-of-Relation-Model-for-Expensive-Optimization"><a href="#Enhancing-SAEAs-with-Unevaluated-Solutions-A-Case-Study-of-Relation-Model-for-Expensive-Optimization" class="headerlink" title="Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization"></a>Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11994">http://arxiv.org/abs/2309.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Hao, Xiaoqun Zhang, Aimin Zhou</li>
<li>for: 提高优化问题的解决效率（EOPs）中的质量解决方案。</li>
<li>methods: 使用模型帮助选择技术来提高SAEAs的效率。</li>
<li>results: 在两个测试集上，使用关系模型选择未评估解决方案可以更好地提高算法的效率，并且这些未评估解决方案具有高潜力。<details>
<summary>Abstract</summary>
Surrogate-assisted evolutionary algorithms (SAEAs) hold significant importance in resolving expensive optimization problems~(EOPs). Extensive efforts have been devoted to improving the efficacy of SAEAs through the development of proficient model-assisted selection methods. However, generating high-quality solutions is a prerequisite for selection. The fundamental paradigm of evaluating a limited number of solutions in each generation within SAEAs reduces the variance of adjacent populations, thus impacting the quality of offspring solutions. This is a frequently encountered issue, yet it has not gained widespread attention. This paper presents a framework using unevaluated solutions to enhance the efficiency of SAEAs. The surrogate model is employed to identify high-quality solutions for direct generation of new solutions without evaluation. To ensure dependable selection, we have introduced two tailored relation models for the selection of the optimal solution and the unevaluated population. A comprehensive experimental analysis is performed on two test suites, which showcases the superiority of the relation model over regression and classification models in the selection phase. Furthermore, the surrogate-selected unevaluated solutions with high potential have been shown to significantly enhance the efficiency of the algorithm.
</details>
<details>
<summary>摘要</summary>
受助者质量进化算法（SAEA）在解决成本高优化问题（EOP）方面具有重要意义。针对提高 SAEA 的效果，广泛的努力已经投入到了开发高效的模型协助选择方法上。然而，生成高质量解决方案是选择高质量解决方案的先置条件。SAEA 中评估每代限制的解决方案的基本思想会减少邻居 populations 的方差，从而影响下一代解决方案的质量。这是一个 часто遇到的问题，但它尚未受到广泛的关注。本文提出了一种使用未评估解决方案来提高 SAEA 的效率的框架。使用 surrogate 模型来标识高质量解决方案，然后直接生成新的解决方案。为保证可靠的选择，我们引入了两种特制的关系模型，一种用于选择优质解决方案，另一种用于选择未评估 популяции。通过对两个测试集进行了全面的实验分析，我们展示了模型在选择阶段的优越性，以及 surrogate 选择的未评估解决方案具有显著提高 SAEA 效率的作用。
</details></li>
</ul>
<hr>
<h2 id="Variational-Connectionist-Temporal-Classification-for-Order-Preserving-Sequence-Modeling"><a href="#Variational-Connectionist-Temporal-Classification-for-Order-Preserving-Sequence-Modeling" class="headerlink" title="Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling"></a>Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11983">http://arxiv.org/abs/2309.11983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Nan, Ting Dang, Vidhyasaharan Sethu, Beena Ahmed</li>
<li>for: This paper is written for researchers and practitioners working on sequence modeling tasks, particularly in the area of speech recognition, who are interested in using CTC with variational models to improve the generalization of their models and handle data variability.</li>
<li>methods: The paper proposes integrating CTC with a variational model, and derives two versions of a novel variational CTC loss function based on reasonable assumptions about the latent variables. The loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and are computationally tractable.</li>
<li>results: The paper presents the results of training sequence models using the proposed variational CTC loss functions, and shows that they lead to more generalizable models that preserve order in the input and target sequences. The results demonstrate the effectiveness of the proposed approach in handling data variability and improving the performance of sequence models.<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally tractable forms for implementing them.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Connectionist Temporal Classification (CTC) 是一种常用的序列模型化任务，如语音识别，因为它需要保持输入和目标序列之间的顺序关系。然而，CTC 只适用于决定性序列模型，其潜在空间是离散的和稀疏的，这使得它们在面对数据变化时比较不能处理。在这篇论文中，我们将 CT 与变量模型结合，并 derive loss functions 可以用来训练更一般化的序列模型，保持顺序。 Specifically，我们 deriv 两个版本的新的变量 CT 基于两个合理的假设：第一个假设是变量 latent 在每个时间步是独立的；第二个假设是这些 latent 变量是 Markovian。我们显示了这两个损失函数可以直接优化变量下界，并提供了实现的计算 tractable 形式。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation, which is commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Generating-Hierarchical-Structures-for-Improved-Time-Series-Classification-Using-Stochastic-Splitting-Functions"><a href="#Generating-Hierarchical-Structures-for-Improved-Time-Series-Classification-Using-Stochastic-Splitting-Functions" class="headerlink" title="Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions"></a>Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11963">http://arxiv.org/abs/2309.11963</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alagoz/hc4tsc_hdc_ssf">https://github.com/alagoz/hc4tsc_hdc_ssf</a></li>
<li>paper_authors: Celal Alagoz</li>
<li>For: The paper is written for enhancing classification performance in multi-class datasets through hierarchical classification.* Methods: The paper introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to generate hierarchy without requiring explicit information.* Results: The approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC performance.Here is the simplified Chinese text for the three key points:* For: 这篇论文是为了提高多类数据集中的分类性能而写的。* Methods: 论文提出了一种基于杂素分割函数（SSF）的层次分类方法，可以不需要明确的层次信息来生成层次结构。* Results: 实验结果表明，该方法在使用rocket和svm分类器时，在约半数和一third的数据集中能够显著提高分类性能。<details>
<summary>Abstract</summary>
This study introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to enhance classification performance in multi-class datasets through hierarchical classification (HC). The method has the unique capability of generating hierarchy without requiring explicit information, making it suitable for datasets lacking prior knowledge of hierarchy. By systematically dividing classes into two subsets based on their discriminability according to the classifier, the proposed approach constructs a binary tree representation of hierarchical classes. The approach is evaluated on 46 multi-class time series datasets using popular classifiers (svm and rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC performance. While the number of classes and flat classification (FC) score show consistent significance, variations are observed with different splitting functions. Overall, the proposed approach presents a promising strategy for enhancing classification by generating hierarchical structure in multi-class time series datasets. Future research directions involve exploring different splitting functions, classifiers, and hierarchy structures, as well as applying the approach to diverse domains beyond time series data. The source code is made openly available to facilitate reproducibility and further exploration of the method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Probability-of-Immunity"><a href="#On-the-Probability-of-Immunity" class="headerlink" title="On the Probability of Immunity"></a>On the Probability of Immunity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11942">http://arxiv.org/abs/2309.11942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZahirSen/scaling-octo-guide">https://github.com/ZahirSen/scaling-octo-guide</a></li>
<li>paper_authors: Jose M. Peña</li>
<li>for: 本研究探讨了免疫机会的概率，即是否曝露对结果的影响。</li>
<li>methods: 我们 derive了免疫必要和 suficient conditions，以及 $\epsilon$-bounded免疫，即免疫概率为零和 $\epsilon$-bounded的情况。</li>
<li>results: 我们可以从随机控制试验中估计受益的概率（即曝露导致效果），并且可以生成更紧的受益概率 bounds。此外，我们还引入了间接免疫（通过介质）的概念，并重复了我们之前的分析。最后，我们提出了对免疫概率下无量化影响的敏感分析方法。<details>
<summary>Abstract</summary>
This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了免疫概率的可能性，即效果发生或不发生。我们 deriv出了免疫必要和 suficient conditions，即免疫概率为零和ε-bounded免疫概率，分别表示效果发生和ε-bounded的免疫概率。前者允许我们从Randomized controlled trial中估算效果发生的概率，而后者允许我们生成更紧的效果发生的概率上限。我们还介绍了间接免疫（通过介质）的概念，并重复了我们的前一次分析。最后，我们提出了对免疫概率下隐藏偏见的敏感分析方法。Note: "ε-bounded" in the text refers to the probability of immunity being bounded above by a small positive value ε.
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-oriented-Survey-on-Tiny-Machine-Learning"><a href="#A-Machine-Learning-oriented-Survey-on-Tiny-Machine-Learning" class="headerlink" title="A Machine Learning-oriented Survey on Tiny Machine Learning"></a>A Machine Learning-oriented Survey on Tiny Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11932">http://arxiv.org/abs/2309.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Capogrosso, Federico Cunico, Dong Seon Cheng, Franco Fummi, Marco cristani</li>
<li>for: 这篇论文旨在为tiny machine learning（TinyML）领域的研究提供一个权威的综述，尤其是关于TinyML中的学习算法。</li>
<li>methods: 本文采用了PRISMA方法流程进行系统性的文献综述，分为三个工作流程：ML-oriented、HW-oriented和合理设计。</li>
<li>results: 本文提出了TinyML学习领域的分类体系，涵盖了不同家族的模型优化和设计，以及当前领域的最佳实践。<details>
<summary>Abstract</summary>
The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.
</details>
<details>
<summary>摘要</summary>
《tiny machine learning（tinyml）的出现》已经对人工智能（ai）领域产生了积极的革命，推动了资源受限的iot设备和其学习基础架构的共同设计。 tinyml在第四和第五个工业革命中发挥着重要的角色，帮助社会、经济和个人使用有效的ai混合计算技术（例如智能城市、汽车和医疗机器人）。由于tinyml的多学科性，该领域被不同的方向攻击：本综述旨在提供tinyml基础设施中所有学习算法的全面检视。本综述采用《Preferred Reporting Items for Systematic Reviews and Meta-Analyses》（prisma）方法流程，以系统和完整的方式检查文献。特别是，我们首先检查tinyml基础设施实施的三种不同工作流程，即ml oriented、hw oriented和codesign。其次，我们提出了tinyml学习领域的分类，审查ml下的不同家族模型优化和设计，以及当前领域的state-of-the-art学习技术。最后，本综述将展示当前tinyml智能边缘应用中的硬件设备和软件工具的最新状态。 Finally, we discuss the challenges and future directions.
</details></li>
</ul>
<hr>
<h2 id="Activation-Compression-of-Graph-Neural-Networks-using-Block-wise-Quantization-with-Improved-Variance-Minimization"><a href="#Activation-Compression-of-Graph-Neural-Networks-using-Block-wise-Quantization-with-Improved-Variance-Minimization" class="headerlink" title="Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization"></a>Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11856">http://arxiv.org/abs/2309.11856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saintslab/i-exact">https://github.com/saintslab/i-exact</a></li>
<li>paper_authors: Sebastian Eliassen, Raghavendra Selvan</li>
<li>for: 提高大规模图 neural network 的训练效率，尤其是减少内存消耗。</li>
<li>methods: 使用极端活动压缩（EXACT）策略，对中间激活图进行量化，从 INT2 精度下进行压缩，以实现大幅减少 GPU 内存消耗，而无需做出重要性的牺牲。</li>
<li>results: 在 EXACT 策略的基础上，使用块级别的量化策略，可以进一步减少内存消耗（&gt;15%），并且在每个 epoch 中提高运行速度（约 5%），即使在执行极端的量化时也可以保持相似的性能交易。此外，对 EXACT 中间激活图分布的假设（假设为均匀分布）进行了更正，并提供了改进的量化和解量化步骤的方差估计。<details>
<summary>Abstract</summary>
Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (>15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be uniform) and show improved variance estimations of the quantization and dequantization steps.
</details>
<details>
<summary>摘要</summary>
大规模图 neural network (GNN) 的高效训练已经被研究，特别是减少它们的内存消耗。工作 by Liu et al. (2022) 提出了极化活动压缩 (EXACT)，通过对中间活动图进行量化，以 INT2 精度进行压缩。他们发现了很少到无关于性能的下降，同时实现了大量的 GPU 内存消耗减少。在这个工作中，我们提出了对 EXACT 策略的改进，通过分割 activation map 的块式压缩。我们通过不同的块大小进行实验分析，并证明了更大的减少内存消耗（> 15%）和每个轮次的运行速度增加（约 5%），即使在执行极端的量化时，与原始 EXACT 的性能折衔保持相同。此外，我们对 EXACT 中对中间活动图的分布假设（假设为均匀分布）进行了修正，并提供了改进的量化和解量化步骤的方差估计。
</details></li>
</ul>
<hr>
<h2 id="TMac-Temporal-Multi-Modal-Graph-Learning-for-Acoustic-Event-Classification"><a href="#TMac-Temporal-Multi-Modal-Graph-Learning-for-Acoustic-Event-Classification" class="headerlink" title="TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification"></a>TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11845">http://arxiv.org/abs/2309.11845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mgithubl/tmac">https://github.com/mgithubl/tmac</a></li>
<li>paper_authors: Meng Liu, Ke Liang, Dayu Hu, Hao Yu, Yue Liu, Lingyuan Meng, Wenxuan Tu, Sihang Zhou, Xinwang Liu<br>for:This paper proposes a method for acoustic event classification using temporal multi-modal graph learning, which can better handle the information of multi-modal data with temporal attributes.methods:The proposed method, called TMac, constructs a temporal graph for each acoustic event, dividing its audio and video data into multiple segments and modeling the temporal relationships between them using graph learning techniques.results:Experiments show that TMac outperforms other state-of-the-art models in performance, demonstrating its effectiveness in capturing the dynamic information in intra-modal and inter-modal data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MGitHubL/TMac.Here">https://github.com/MGitHubL/TMac.Here</a> is the simplified Chinese text:for:这篇论文提出了一种基于时间多模式图学习的声音事件分类方法，可以更好地处理具有时间属性的多模式数据。methods:该方法，称为TMac，对声音事件进行分解，将其声音数据和视频数据分割成多个segment，然后使用图学习技术来模型这些segment之间的时间关系。results:实验表明，TMac比其他状态对应模型更高效，能够更好地捕捉多模式数据中的内部和间部关系。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MGitHubL/TMac%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MGitHubL/TMac上下载。</a><details>
<summary>Abstract</summary>
Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning techniques. In particular, we construct a temporal graph for each acoustic event, dividing its audio data and video data into multiple segments. Each segment can be considered as a node, and the temporal relationships between nodes can be considered as timestamps on their edges. In this case, we can smoothly capture the dynamic information in intra-modal and inter-modal. Several experiments are conducted to demonstrate TMac outperforms other SOTA models in performance. Our code is available at https://github.com/MGitHubL/TMac.
</details>
<details>
<summary>摘要</summary>
现在的数字时代，audiovisual数据在 everywhere，这 heightened the requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual experience. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning techniques. In particular, we construct a temporal graph for each acoustic event, dividing its audio data and video data into multiple segments. Each segment can be considered as a node, and the temporal relationships between nodes can be considered as timestamps on their edges. In this case, we can smoothly capture the dynamic information in intra-modal and inter-modal. Several experiments are conducted to demonstrate TMac outperforms other SOTA models in performance. Our code is available at https://github.com/MGitHubL/TMac.Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and grammar rules to represent the language in a more phonetic and easier-to-learn format. The translation above is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-of-Community-Detection-in-Graphs"><a href="#A-Comprehensive-Review-of-Community-Detection-in-Graphs" class="headerlink" title="A Comprehensive Review of Community Detection in Graphs"></a>A Comprehensive Review of Community Detection in Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11798">http://arxiv.org/abs/2309.11798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlai Ning, Jiakang Li, Yonggang Lu</li>
<li>for: 本文探讨了图像中的社区结构，强调了图像中的社区结构的探讨，以及社区结构的探讨在不同领域的应用。</li>
<li>methods: 本文介绍了多种社区检测方法，包括一种新的方法，并对这些方法进行了详细的介绍。</li>
<li>results: 本文总结了社区检测方法的发展历程，并对社区检测的应用在不同领域进行了探讨。<details>
<summary>Abstract</summary>
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In conclusion, this comprehensive review provides a deep understanding of community detection in graphs. It serves as a valuable resource for researchers and practitioners in multiple disciplines, offering insights into the challenges, methodologies, and applications of community detection in complex networks.
</details>
<details>
<summary>摘要</summary>
学术研究复杂网络已经有了大量的进展，我们对复杂网络中社区结构的理解得到了重要进步。检测复杂网络中的社区是一个具有挑战性的问题，在社会学、生物学和计算机科学等领域都有着广泛的应用。尽管一群来自不同领域的科学家努力奔走，但是满意的解决方案还没有得到。这篇评论文章将探讨复杂网络中的社区检测问题，这是理解复杂系统的组织和运作的关键部分。我们首先介绍社区结构的概念，即顶点的分布到群集中，群集之间的连接较弱。然后，我们对各种社区检测方法进行了详细的介绍，包括我们新提出的方法。此外，我们还探讨了不同网络中社区检测的实际应用。 conclude，这篇评论文章为研究者和实践者在多种领域提供了深入的理解社区检测在复杂网络中的挑战、方法和应用。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-In-Context-Learning-with-Differentially-Private-Few-Shot-Generation"><a href="#Privacy-Preserving-In-Context-Learning-with-Differentially-Private-Few-Shot-Generation" class="headerlink" title="Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation"></a>Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11765">http://arxiv.org/abs/2309.11765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/dp-few-shot-generation">https://github.com/microsoft/dp-few-shot-generation</a></li>
<li>paper_authors: Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim</li>
<li>for: 实现隐私对称学习（ICL）大语言模型（LLM）上private dataset。</li>
<li>methods: 提出了一个新的算法，将私人集成的几个示例通过正式数据隐私（DP）保证生成为几个实验示例，并证明了它可以实现有效的ICL。</li>
<li>results: 实验结果显示，我们的算法可以与高度隐私水平相当的实现优异的性能，并且与非私人ICL和零例解决方案相比，具有更好的缩减性和更高的可重用性。<details>
<summary>Abstract</summary>
We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
</details>
<details>
<summary>摘要</summary>
我们研究在私有数据集上使用大语言模型（LLM）进行上下文学习（ICL）问题，这种情况可能会导致隐私泄露或模型重复示例。我们提出了一种新的算法，通过从私有数据集生成几拍示例，并提供正式的差分隐私（DP）保证，以实现有效的ICL。我们进行了广泛的实验，并与非私有ICL和零shot解决方案进行比较。我们的结果表明，我们的算法可以实现竞争性的性能，同时保证高度的隐私水平。这些结果开 up了新的可能性，使得ICL中的隐私保护可以推广到许多应用程序。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Optimal-SDG-Pathways-An-Innovative-Approach-Leveraging-Graph-Pruning-and-Intent-Graph-for-Effective-Recommendations"><a href="#Unveiling-Optimal-SDG-Pathways-An-Innovative-Approach-Leveraging-Graph-Pruning-and-Intent-Graph-for-Effective-Recommendations" class="headerlink" title="Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations"></a>Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11741">http://arxiv.org/abs/2309.11741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihang Yu, Shu Wang, Yunqiang Zhu, Wen Yuan, Xiaoliang Dai, Zhiqiang Zou</li>
<li>for: 提出了一种基于用户图 после剪裁和意图图（UGPIG）方法，用于解决现有计算机科学领域推荐算法不能充分考虑地区环境特点和历史互动数据稀缺性的问题，以提高可持续发展模式的推荐效果。</li>
<li>methods: 该方法首先利用了剪裁后用户图的高密度链接能力，解决了推荐算法忽略地区空间不同性的问题。其次，通过建立意图图，capture了目标地区Attributes的偏好，有效地解决了历史互动数据稀缺性问题。</li>
<li>results: 经过广泛的实验，UGPIG方法在可持续发展模式推荐方面比现有的推荐算法（KGCN、KGAT、KGIN）高效，最大提升9.61%。<details>
<summary>Abstract</summary>
The recommendation of appropriate development pathways, also known as ecological civilization patterns for achieving Sustainable Development Goals (namely, sustainable development patterns), are of utmost importance for promoting ecological, economic, social, and resource sustainability in a specific region. To achieve this, the recommendation process must carefully consider the region's natural, environmental, resource, and economic characteristics. However, current recommendation algorithms in the field of computer science fall short in adequately addressing the spatial heterogeneity related to environment and sparsity of regional historical interaction data, which limits their effectiveness in recommending sustainable development patterns. To overcome these challenges, this paper proposes a method called User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the high-density linking capability of the pruned User Graph to address the issue of spatial heterogeneity neglect in recommendation algorithms. Secondly, we construct an Intent Graph by incorporating the intent network, which captures the preferences for attributes including environmental elements of target regions. This approach effectively alleviates the problem of sparse historical interaction data in the region. Through extensive experiments, we demonstrate that UGPIG outperforms state-of-the-art recommendation algorithms like KGCN, KGAT, and KGIN in sustainable development pattern recommendations, with a maximum improvement of 9.61% in Top-3 recommendation performance.
</details>
<details>
<summary>摘要</summary>
“ ecovillage civilization ”模式的建议，即可持续发展目标的实现，对于某地区的生态、经济、社会和资源可持续发展具有极其重要的作用。为了实现这一目标，建议过程应当考虑该地区的自然、环境、资源和经济特点。然而，当前的计算机科学领域的推荐算法尚未能充分考虑地域间的空间不同性和历史互动数据的稀缺性，这限制了它们在可持续发展模式的推荐上的效果。为了解决这些挑战，本文提出了一种方法called User Graph after Pruning and Intent Graph (UGPIG)。首先，我们利用了剪除后的用户图高密度链接能力，解决了推荐算法忽视地域间空间不同性的问题。其次，我们构建了意图图， capture 目标区域的环境元素的偏好。这种方法有效地解决了历史互动数据稀缺性问题。经过广泛的实验，我们证明UGPIG可以比state-of-the-art推荐算法like KGCN、KGAT和KGIN在可持续发展模式的推荐上表现出较高的Top-3推荐性能，最大提升率为9.61%。
</details></li>
</ul>
<hr>
<h2 id="Turaco-Complexity-Guided-Data-Sampling-for-Training-Neural-Surrogates-of-Programs"><a href="#Turaco-Complexity-Guided-Data-Sampling-for-Training-Neural-Surrogates-of-Programs" class="headerlink" title="Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs"></a>Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11726">http://arxiv.org/abs/2309.11726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Renda, Yi Ding, Michael Carbin</li>
<li>for: 本研究旨在提供一种方法ология，用于在训练神经网络模型时，从程序的输入空间中采样数据，以优化模型的准确性。</li>
<li>methods: 本研究使用了一种基于程序执行路径的复杂性分析方法，以确定采样数据的比例，并使用神经网络模型来训练代理模型。</li>
<li>results: 实验结果表明，基于复杂性分析的采样方法可以提高模型的准确性，并且在各种真实世界程序中进行了成功应用。<details>
<summary>Abstract</summary>
Programmers and researchers are increasingly developing surrogates of programs, models of a subset of the observable behavior of a given program, to solve a variety of software development challenges. Programmers train surrogates from measurements of the behavior of a program on a dataset of input examples. A key challenge of surrogate construction is determining what training data to use to train a surrogate of a given program.   We present a methodology for sampling datasets to train neural-network-based surrogates of programs. We first characterize the proportion of data to sample from each region of a program's input space (corresponding to different execution paths of the program) based on the complexity of learning a surrogate of the corresponding execution path. We next provide a program analysis to determine the complexity of different paths in a program. We evaluate these results on a range of real-world programs, demonstrating that complexity-guided sampling results in empirical improvements in accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法来采样培训数据，以训练基于神经网络的代程模型。我们首先量化程序输入空间中每个执行路径的学习复杂性，然后根据复杂性来决定从哪些地方采样数据。接着，我们通过程序分析来确定不同路径的复杂性。我们对多个实际Program进行评估，并证明了复杂性导向采样的实际改进精度。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Core-selecting-Incentive-Mechanism-for-Data-Sharing-in-Federated-Learning"><a href="#Efficient-Core-selecting-Incentive-Mechanism-for-Data-Sharing-in-Federated-Learning" class="headerlink" title="Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning"></a>Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11722">http://arxiv.org/abs/2309.11722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Ji, Genjiu Xu, Jianjun Ge, Mingqiang Li</li>
<li>for: 这个论文的目的是设计一种激励机制，使参与者输入真实数据，并稳定合作。</li>
<li>methods: 这个论文使用游戏理论的核心概念来设计核心选择机制，并使用放弃方法和采样approximation来降低计算开销。</li>
<li>results: 实验表明，这种高效的核心选择机制可以激励参与者输入高质量数据，并且可以降低计算开销相比之前的核心选择机制。<details>
<summary>Abstract</summary>
Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulting in the core-selecting mechanism becoming infeasible. To address this, our core-selecting mechanism employs a relaxation method and simultaneously minimizes the benefits of inputting false data for all participants. However, this mechanism is computationally expensive because it requires aggregating exponential models for all possible coalitions, which is infeasible in federated learning. To address this, we propose an efficient core-selecting mechanism based on sampling approximation that only aggregates models on sampled coalitions to approximate the exact result. Extensive experiments verify that the efficient core-selecting mechanism can incentivize inputting high-quality data and stable cooperation, while it reduces computational overhead compared to the core-selecting mechanism.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种分布式机器学习系统，用 particiants' 数据来训练一个改进的全球模型。在 federated learning 中，particiants 合作训练全球模型，并将收到全球模型和支付。理解 particiants 会尽可能地提高自己的个人利益，并不会真实输入高质量数据，除非他们得到满意的支付基于数据质量。此外，federated learning 受到参与者的合作贡献帮助。因此，如何建立一个奖励机制，使 particiants 尽可能地输入真实数据，同时促进稳定合作成为了一个重要的问题。在这篇论文中，我们介绍了一种数据共享游戏模型，并使用游戏理论方法设计核心选择奖励机制。在 federated learning 中，核心可能是空的，这会使核心选择机制成为不可能的。为解决这个问题，我们的核心选择机制使用了松弛方法，同时减少所有参与者输入假数据的收益。然而，这种机制是计算昂贵的，因为它需要对所有可能的联盟进行汇总，这是 federated learning 中不可能完成的。为解决这个问题，我们提出了一种高效的核心选择机制，基于采样approximation，只需对选择的联盟进行汇总，来近似 exact 结果。广泛的实验证明，高效的核心选择机制可以奖励输入高质量数据和稳定合作，同时减少计算负担，相比核心选择机制。
</details></li>
</ul>
<hr>
<h2 id="Quasi-Monte-Carlo-for-3D-Sliced-Wasserstein"><a href="#Quasi-Monte-Carlo-for-3D-Sliced-Wasserstein" class="headerlink" title="Quasi-Monte Carlo for 3D Sliced Wasserstein"></a>Quasi-Monte Carlo for 3D Sliced Wasserstein</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11713">http://arxiv.org/abs/2309.11713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khai Nguyen, Nicola Bariletto, Nhat Ho</li>
<li>for: 这个论文主要是为了提供一种更好的empirical Sliced Wasserstein（SW）Distance的方法，即Quasi-Sliced Wasserstein（QSW）方法，以及一种基于Randomized Quasi-Sliced Wasserstein（RQSW）的随机化版本，用于3D任务。</li>
<li>methods: 这个论文使用了Quasi-Monte Carlo（QMC）方法，包括Gaussian-based mapping、equal area mapping、generalized spiral points和优化犯 COUNT energies等方法来构建QMC点 clouds。此外，为了减少估计误差， authors还提出了一种基于随机化低犯 COUNT sequence的RQSW方法。</li>
<li>results: 该论文通过实验表明，QSW和RQSW方法在3D任务中表现出色，比如点云比较、点云插值、图像风格传输和深度点云自动编码器的训练等。此外， authors还证明了QSW和RQSW方法的 asymptotic convergence和无偏性。<details>
<summary>Abstract</summary>
Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-discrepancy sequences. For theoretical properties, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants.
</details>
<details>
<summary>摘要</summary>
蒙特卡洛（MC）方法已经被广泛使用作为水星剖分（SW）距离的标准计算方法，但MC方法不是最优的精度下采样方法。为提供更好的empirical SW，我们提议使用 quasi-水星剖分（QSW）方法，该方法基于 quasi-蒙特卡洛（QMC）方法。在更加详细的3D设置下，我们进行了对QMC方法的广泛研究，包括在3D单位球上构建QMC点集的多种方法，如 Gaussian-based mapping、equal area mapping、generalized spiral points和优化误差能量。此外，为了获得不偏向的优化，我们将QSW扩展为Randomized Quasi-Sliced Wasserstein（RQSW），通过引入随机性来讲谱低误差序列。我们证明了QSW的极限收敛性和RQSW的无偏性。最后，我们在多个3D任务上进行了实验，包括点云比较、点云拟合、图像风格传输和深度点云自动编码器的训练，以示提案的QSW和RQSW变体的报道性能。
</details></li>
</ul>
<hr>
<h2 id="Incentivized-Communication-for-Federated-Bandits"><a href="#Incentivized-Communication-for-Federated-Bandits" class="headerlink" title="Incentivized Communication for Federated Bandits"></a>Incentivized Communication for Federated Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11702">http://arxiv.org/abs/2309.11702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhepei Wei, Chuanhao Li, Haifeng Xu, Hongning Wang</li>
<li>for: 鼓励客户端分享数据，以提高联合学习效率和实际可行性。</li>
<li>methods: 提出了一种奖励客户端分享数据的通信问题，并提出了首个奖励通信协议——Inc-FedUCB，可以在Contextual Linear Setting下实现近似优化的停损 regret。</li>
<li>results: 通过Synthetic和实际数据的广泛实验，证明了提案方法在不同环境下的效果。<details>
<summary>Abstract</summary>
Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first incentivized communication protocol, namely, Inc-FedUCB, that achieves near-optimal regret with provable communication and incentive cost guarantees. Extensive empirical experiments on both synthetic and real-world datasets further validate the effectiveness of the proposed method across various environments.
</details>
<details>
<summary>摘要</summary>
现有大多数聚合强投资的研究假设所有客户端都是积极的分享其数据，以便服务器可以实现共同利益。尽管这种假设具有抽象理论保证的性和通信效率，但在实践中，这种假设经常被违背，特别是当算法在自私的客户端上运行时。不正确地忽略这些自私行为可能会对联合强投资学习的学习效率和实际操作性产生很大的影响。为了解决这个下 lista under-explored 的研究领域，我们希望通过正式地引入一种奖励通信问题，使服务器可以鼓励客户端分享数据，以获得奖励。无论总体来说，我们在Contextual linear setting中实例化这个强投资问题，并提出首个奖励通信协议，即Inc-FedUCB，可以实现近似最佳的 regret  guarantee，同时保证通信和奖励成本的 garantate。经验性实验表明，我们的方法在多种环境下都具有很高的实际效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.LG_2023_09_21/" data-id="clmvt7tas00hz26rd9zcm28jg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/21/cs.CL_2023_09_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-09-21
        
      </div>
    </a>
  
  
    <a href="/2023/09/21/eess.IV_2023_09_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">21</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
