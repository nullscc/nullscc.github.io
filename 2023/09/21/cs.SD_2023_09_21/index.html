
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12254 repo_url: None paper_authors: Paulo Vitor">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-21">
<meta property="og:url" content="https://nullscc.github.io/2023/09/21/cs.SD_2023_09_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12254 repo_url: None paper_authors: Paulo Vitor">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-21T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:59:19.269Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.SD_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T15:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Variational-Quantum-Harmonizer-Generating-Chord-Progressions-and-Other-Sonification-Methods-with-the-VQE-Algorithm"><a href="#Variational-Quantum-Harmonizer-Generating-Chord-Progressions-and-Other-Sonification-Methods-with-the-VQE-Algorithm" class="headerlink" title="Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm"></a>Variational Quantum Harmonizer: Generating Chord Progressions and Other Sonification Methods with the VQE Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12254">http://arxiv.org/abs/2309.12254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Vitor Itaboraí, Tim Schwägerl, María Aguado Yáñez, Arianna Crippa, Karl Jansen, Eduardo Reck Miranda, Peter Thomas</li>
<li>for: This paper explores the use of physical-based sonification to visualize and understand the optimization process of Quadratic Unconstrained Binary Optimization (QUBO) problems, which are solved using the Variational Quantum Eigensolver (VQE) algorithm.</li>
<li>methods: The paper uses a musical interface prototype called Variational Quantum Harmonizer (VQH) to sonify the optimization process, which involves using intermediary statevectors to create musical elements such as chords, chord progressions, and arpeggios.</li>
<li>results: The paper demonstrates the potential of using sonification to enhance data visualization and create artistic pieces, and shows how flexible mapping strategies can supply a broad portfolio of sounds for QUBO and quantum-inspired musical compositions. Additionally, the paper highlights the relevance of the methodology for artists to gain intuition towards achieving a desired musical sound by carefully designing QUBO cost functions.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文探讨了使用物理基于的听韵来可见化和理解量子计算机解决的Quadratic Unconstrained Binary Optimization (QUBO) 问题的优化过程。</li>
<li>methods: 这篇论文使用了一个名为Variational Quantum Harmonizer (VQH)的乐interface进行听韵，通过使用中间状态向量来创造音乐元素，如和弦、旋律和arpeggios。</li>
<li>results: 这篇论文显示了使用听韵可以增强数据可视化和创作艺术作品，并表明了可变映射策略可以为QUBO和量子听韵作品提供广泛的音色。此外，论文还 highlights了这种方法的创作意义，可以帮助艺术家更好地理解和实现想要的音乐声色。<details>
<summary>Abstract</summary>
This work investigates a case study of using physical-based sonification of Quadratic Unconstrained Binary Optimization (QUBO) problems, optimized by the Variational Quantum Eigensolver (VQE) algorithm. The VQE approximates the solution of the problem by using an iterative loop between the quantum computer and a classical optimization routine. This work explores the intermediary statevectors found in each VQE iteration as the means of sonifying the optimization process itself. The implementation was realised in the form of a musical interface prototype named Variational Quantum Harmonizer (VQH), providing potential design strategies for musical applications, focusing on chords, chord progressions, and arpeggios. The VQH can be used both to enhance data visualization or to create artistic pieces. The methodology is also relevant in terms of how an artist would gain intuition towards achieving a desired musical sound by carefully designing QUBO cost functions. Flexible mapping strategies could supply a broad portfolio of sounds for QUBO and quantum-inspired musical compositions, as demonstrated in a case study composition, "Dependent Origination" by Peter Thomas and Paulo Itaborai.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这项研究探讨了使用物理基于的SONIFICATION方法来解决Quadratic Unconstrained Binary Optimization (QUBO)问题，使用Variational Quantum Eigensolver (VQE)算法进行优化。VQE算法使用了一个迭代循环来实现解决方案，并使用每个VQE迭代的中间状态向量来SONIFICATIONOptimization过程本身。这种实现形式为音乐界面原型Variational Quantum Harmonizer (VQH)，提供了可能的设计策略 для音乐应用，主要关注于和arpeggios。VQH可以用来增强数据视觉或创作艺术作品。此方法也有用于艺术家如何通过设计QUBO成本函数来获得感知到所需的音乐声色的概念。可以采用灵活的映射策略来供应QUBO和量子启发的各种音色，如在case study作品"Dependent Origination" by Peter Thomas和 Paulo Itaborai中所示。
</details></li>
</ul>
<hr>
<h2 id="A-Multiscale-Autoencoder-MSAE-Framework-for-End-to-End-Neural-Network-Speech-Enhancement"><a href="#A-Multiscale-Autoencoder-MSAE-Framework-for-End-to-End-Neural-Network-Speech-Enhancement" class="headerlink" title="A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement"></a>A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12121">http://arxiv.org/abs/2309.12121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bengt J. Borgstrom, Michael S. Brandstein</li>
<li>for: 提高单通道语音增强的性能</li>
<li>methods: 使用mask-basedArchitecture和多尺度自编码器</li>
<li>results: 相比传统方法，提高了对话质量指标和自动语音识别精度<details>
<summary>Abstract</summary>
Neural network approaches to single-channel speech enhancement have received much recent attention. In particular, mask-based architectures have achieved significant performance improvements over conventional methods. This paper proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural network speech enhancement. The MSAE performs spectral decomposition of an input waveform within separate band-limited branches, each operating with a different rate and scale, to extract a sequence of multiscale embeddings. The proposed framework features intuitive parameterization of the autoencoder, including a flexible spectral band design based on the Constant-Q transform. Additionally, the MSAE is constructed entirely of differentiable operators, allowing it to be implemented within an end-to-end neural network, and be discriminatively trained. The MSAE draws motivation both from recent multiscale network topologies and from traditional multiresolution transforms in speech processing. Experimental results show the MSAE to provide clear performance benefits relative to conventional single-branch autoencoders. Additionally, the proposed framework is shown to outperform a variety of state-of-the-art enhancement systems, both in terms of objective speech quality metrics and automatic speech recognition accuracy.
</details>
<details>
<summary>摘要</summary>
神经网络方法对单通道语音增强 Received much recent attention. In particular, 面积基 architecture has achieved significant performance improvements over conventional methods. This paper proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural network speech enhancement. The MSAE performs spectral decomposition of an input waveform within separate band-limited branches, each operating with a different rate and scale, to extract a sequence of multiscale embeddings. The proposed framework features intuitive parameterization of the autoencoder, including a flexible spectral band design based on the Constant-Q transform. Additionally, the MSAE is constructed entirely of differentiable operators, allowing it to be implemented within an end-to-end neural network, and be discriminatively trained. The MSAE draws motivation both from recent multiscale network topologies and from traditional multiresolution transforms in speech processing. Experimental results show the MSAE to provide clear performance benefits relative to conventional single-branch autoencoders. Additionally, the proposed framework is shown to outperform a variety of state-of-the-art enhancement systems, both in terms of objective speech quality metrics and automatic speech recognition accuracy.
</details></li>
</ul>
<hr>
<h2 id="Is-the-Ideal-Ratio-Mask-Really-the-Best-–-Exploring-the-Best-Extraction-Performance-and-Optimal-Mask-of-Mask-based-Beamformers"><a href="#Is-the-Ideal-Ratio-Mask-Really-the-Best-–-Exploring-the-Best-Extraction-Performance-and-Optimal-Mask-of-Mask-based-Beamformers" class="headerlink" title="Is the Ideal Ratio Mask Really the Best? – Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers"></a>Is the Ideal Ratio Mask Really the Best? – Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12065">http://arxiv.org/abs/2309.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsuo Hiroe, Katsutoshi Itoyama, Kazuhiro Nakadai</li>
<li>for: 这个研究旨在investigate mask-based beamformers (BFs), which estimate filters to extract target speech using time-frequency masks.</li>
<li>methods: 研究使用四种mask-based BFs：最大信号响应率BF、两种其变体，以及多通道wiener filter（MWF）BF。为每个utterance获取最佳mask，使得BF输出与目标语音之间的平均方差最小。</li>
<li>results: 经过实验 validate that the four BFs have the same peak performance as the ideal MWF BF, but the optimal mask depends on the adopted BF and differs from the IRM. 这些结论与传统的想法不同，即最佳mask是共同的forall BFs，并且每个BF的 peak performance不同。<details>
<summary>Abstract</summary>
This study investigates mask-based beamformers (BFs), which estimate filters to extract target speech using time-frequency masks. Although several BF methods have been proposed, the following aspects are yet to be comprehensively investigated. 1) Which BF can provide the best extraction performance in terms of the closeness of the BF output to the target speech? 2) Is the optimal mask for the best performance common for all BFs? 3) Is the ideal ratio mask (IRM) identical to the optimal mask? Accordingly, we investigate these issues considering four mask-based BFs: the maximum signal-to-noise ratio BF, two variants of this, and the multichannel Wiener filter (MWF) BF. To obtain the optimal mask corresponding to the peak performance for each BF, we employ an approach that minimizes the mean square error between the BF output and target speech for each utterance. Via the experiments with the CHiME-3 dataset, we verify that the four BFs have the same peak performance as the upper bound provided by the ideal MWF BF, whereas the optimal mask depends on the adopted BF and differs from the IRM. These observations differ from the conventional idea that the optimal mask is common for all BFs and that peak performance differs for each BF. Hence, this study contributes to the design of mask-based BFs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Which BF can provide the best extraction performance in terms of the closeness of the BF output to the target speech?2. Is the optimal mask for the best performance common for all BFs?3. Is the ideal ratio mask (IRM) identical to the optimal mask?To answer these questions, we consider four mask-based BFs: the maximum signal-to-noise ratio BF, two variants of this, and the multichannel Wiener filter (MWF) BF. We use an approach to obtain the optimal mask corresponding to the peak performance for each BF by minimizing the mean square error between the BF output and target speech for each utterance.Our experiments with the CHiME-3 dataset show that the four BFs have the same peak performance as the upper bound provided by the ideal MWF BF, but the optimal mask depends on the adopted BF and differs from the IRM. These observations differ from the conventional idea that the optimal mask is common for all BFs and that peak performance differs for each BF. Therefore, this study contributes to the design of mask-based BFs.</details></li>
</ol>
<hr>
<h2 id="Improving-Language-Model-Based-Zero-Shot-Text-to-Speech-Synthesis-with-Multi-Scale-Acoustic-Prompts"><a href="#Improving-Language-Model-Based-Zero-Shot-Text-to-Speech-Synthesis-with-Multi-Scale-Acoustic-Prompts" class="headerlink" title="Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts"></a>Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11977">http://arxiv.org/abs/2309.11977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Lei, Yixuan Zhou, Liyang Chen, Dan Luo, Zhiyong Wu, Xixin Wu, Shiyin Kang, Tao Jiang, Yahui Zhou, Yuxing Han, Helen Meng</li>
<li>for: 这个论文旨在提出一个可以复制无见的话者声音的零数据文本读取与Synthesis系统（TTS）。</li>
<li>methods: 这个方法使用了语言模型来模型语音波形的数据几何，并且使用了一个新的话者感知文本编码器来学习个人话语风格。</li>
<li>results: 实验结果显示，该方法可以与基准相比，提高自然性和话者相似性，并且可以通过规模化style prompt来提高表演。<details>
<summary>Abstract</summary>
Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.
</details>
<details>
<summary>摘要</summary>
<<SYS>>zero-shot文本至语音（TTS）合成targets any unseen speaker's voice without adaptation parameters. Byquantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.<</SYS>>Here's the translation in Traditional Chinese:<<SYS>>zero-shot文本至语音（TTS）合成targets any unseen speaker's voice without adaptation parameters. Byquantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Multi-Channel-MOSRA-Mean-Opinion-Score-and-Room-Acoustics-Estimation-Using-Simulated-Data-and-a-Teacher-Model"><a href="#Multi-Channel-MOSRA-Mean-Opinion-Score-and-Room-Acoustics-Estimation-Using-Simulated-Data-and-a-Teacher-Model" class="headerlink" title="Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model"></a>Multi-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11976">http://arxiv.org/abs/2309.11976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jozef Coldenhoff, Andrew Harper, Paul Kendrick, Tijana Stojkovic, Milos Cernak</li>
<li>for: 预测房间听音参数和语音质量指标</li>
<li>methods: 使用多通道模型进行同时预测多个录音设备的房间听音参数和语音质量指标</li>
<li>results: 比单通道模型提高直接听音比率、清晰度和语音传输指标的预测，并且需要约5倍 menos计算资源，但是其他指标的性能减少不大<details>
<summary>Abstract</summary>
Previous methods for predicting room acoustic parameters and speech quality metrics have focused on the single-channel case, where room acoustics and Mean Opinion Score (MOS) are predicted for a single recording device. However, quality-based device selection for rooms with multiple recording devices may benefit from a multi-channel approach where the descriptive metrics are predicted for multiple devices in parallel. Following our hypothesis that a model may benefit from multi-channel training, we develop a multi-channel model for joint MOS and room acoustics prediction (MOSRA) for five channels in parallel. The lack of multi-channel audio data with ground truth labels necessitated the creation of simulated data using an acoustic simulator with room acoustic labels extracted from the generated impulse responses and labels for MOS generated in a student-teacher setup using a wav2vec2-based MOS prediction model. Our experiments show that the multi-channel model improves the prediction of the direct-to-reverberation ratio, clarity, and speech transmission index over the single-channel model with roughly 5$\times$ less computation while suffering minimal losses in the performance of the other metrics.
</details>
<details>
<summary>摘要</summary>
先前的方法只是针对单通道情况进行预测， Room acoustics 和 Mean Opinion Score (MOS) 的预测都是基于单个录音设备。然而，基于质量的设备选择可能会受益于多通道方法，因为模型可能会从多个设备的描述性度量中受益。根据我们的假设，一个模型可能会从多个通道的训练中受益，因此我们开发了一个同时预测 MOS 和 Room acoustics 的多通道模型（MOSRA），对五个通道进行并行预测。由于没有多个渠道的音频数据有ground truth标签，我们使用一个声学模拟器生成了带有房间响应标签的 simulated data，并使用 wav2vec2-based MOS 预测模型生成了 MOS 标签。我们的实验表明，多通道模型在直接响应比、清晰度和语音传输指数方面的预测比单通道模型提高了大约5倍，而且计算量相对减少了大约5倍，而且减少了其他指标的性能。
</details></li>
</ul>
<hr>
<h2 id="Cluster-based-pruning-techniques-for-audio-data"><a href="#Cluster-based-pruning-techniques-for-audio-data" class="headerlink" title="Cluster-based pruning techniques for audio data"></a>Cluster-based pruning techniques for audio data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11922">http://arxiv.org/abs/2309.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Bergsma, Marta Brzezinska, Oleg V. Yazyev, Milos Cernak</li>
<li>for: 针对 audio 频段数据进行减少计算开销，提高深度学习模型的计算效率。</li>
<li>methods: 使用 k-means 聚合算法对数据进行 clustering 分析，以实现数据减少。</li>
<li>results: 对关键词检测（KWS）数据集进行 clustering 分析，发现可以使用 k-means 聚合算法减少 audio 数据集大小，保持分类性能。同时，通过缩放分析对大量样本进行优化采样，提高计算效率。<details>
<summary>Abstract</summary>
Deep learning models have become widely adopted in various domains, but their performance heavily relies on a vast amount of data. Datasets often contain a large number of irrelevant or redundant samples, which can lead to computational inefficiencies during the training. In this work, we introduce, for the first time in the context of the audio domain, the k-means clustering as a method for efficient data pruning. K-means clustering provides a way to group similar samples together, allowing the reduction of the size of the dataset while preserving its representative characteristics. As an example, we perform clustering analysis on the keyword spotting (KWS) dataset. We discuss how k-means clustering can significantly reduce the size of audio datasets while maintaining the classification performance across neural networks (NNs) with different architectures. We further comment on the role of scaling analysis in identifying the optimal pruning strategies for a large number of samples. Our studies serve as a proof-of-principle, demonstrating the potential of data selection with distance-based clustering algorithms for the audio domain and highlighting promising research avenues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Impact-of-Silence-on-Speech-Anti-Spoofing"><a href="#The-Impact-of-Silence-on-Speech-Anti-Spoofing" class="headerlink" title="The Impact of Silence on Speech Anti-Spoofing"></a>The Impact of Silence on Speech Anti-Spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11827">http://arxiv.org/abs/2309.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Zhuo Li, Jingze Lu, Hua Hua, Wenchao Wang, Pengyuan Zhang</li>
<li>for: 这篇论文主要研究了防御诈骗攻击的影响，具体来说是研究了 silence 的影响。</li>
<li>methods: 本论文使用了 Voice Activity Detection (VAD) 技术和 Class Activation Mapping (CAM) Visualization 来分析诈骗攻击的影响。</li>
<li>results: 研究发现，去掉 silence 会使防御诈骗攻击表现下降，并且不同的 waveform generator 生成的 silence 内容和 bonafide speech 内容之间存在差异。此外，通过对 CM 进行重新训练，可以减少诈骗攻击对 CM 的影响。<details>
<summary>Abstract</summary>
The current speech anti-spoofing countermeasures (CMs) show excellent performance on specific datasets. However, removing the silence of test speech through Voice Activity Detection (VAD) can severely degrade performance. In this paper, the impact of silence on speech anti-spoofing is analyzed. First, the reasons for the impact are explored, including the proportion of silence duration and the content of silence. The proportion of silence duration in spoof speech generated by text-to-speech (TTS) algorithms is lower than that in bonafide speech. And the content of silence generated by different waveform generators varies compared to bonafide speech. Then the impact of silence on model prediction is explored. Even after retraining, the spoof speech generated by neural network based end-to-end TTS algorithms suffers a significant rise in error rates when the silence is removed. To demonstrate the reasons for the impact of silence on CMs, the attention distribution of a CM is visualized through class activation mapping (CAM). Furthermore, the implementation and analysis of the experiments masking silence or non-silence demonstrates the significance of the proportion of silence duration for detecting TTS and the importance of silence content for detecting voice conversion (VC). Based on the experimental results, improving the robustness of CMs against unknown spoofing attacks by masking silence is also proposed. Finally, the attacks on anti-spoofing CMs through concatenating silence, and the mitigation of VAD and silence attack through low-pass filtering are introduced.
</details>
<details>
<summary>摘要</summary>
当前的语音反 spoofing 防范措施（CMs）在特定的数据集上表现出非常出色。然而，通过语音活动检测（VAD）移除测试语音中的沉默可能会严重降低性能。在这篇论文中，我们分析了语音反 spoofing 中 silence 的影响。首先，我们研究了 silence 的原因对性能的影响，包括沉默时间的比例和沉默内容。TTS 算法生成的 spoof speech 中的沉默时间比bonafide speech 长，而生成的沉默内容与 bonafide speech 不同。然后，我们研究了 silence 对模型预测的影响。即使重新训练，使用 neural network 基于 end-to-end TTS 算法生成的 spoof speech 在移除沉默后错误率显著增加。为了说明 silence 对 CMs 的影响的原因，我们通过类 activation mapping（CAM）Visualize CM 的注意力分布。此外，我们还实现了在 silence 或非沉默处理下进行实验，以示 silence 的重要性和 non-silence 的重要性。基于实验结果，我们也提出了改进 CMs 对未知 spoofing 攻击的Robustness的方法。最后，我们介绍了 concatenating silence 的攻击和 VAD 和沉默攻击的mitigation 策略。
</details></li>
</ul>
<hr>
<h2 id="Frame-Pairwise-Distance-Loss-for-Weakly-supervised-Sound-Event-Detection"><a href="#Frame-Pairwise-Distance-Loss-for-Weakly-supervised-Sound-Event-Detection" class="headerlink" title="Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection"></a>Frame Pairwise Distance Loss for Weakly-supervised Sound Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11783">http://arxiv.org/abs/2309.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Tao, Yuxing Huang, Xiangdong Wang, Long Yan, Lufeng Zhai, Kazushige Ouchi, Taihao Li</li>
<li>for:  bridging the gap between fully supervised methods and unsupervised techniques in various domains</li>
<li>methods:  introducing a Frame Pairwise Distance (FPD) loss branch and synthesized data to enhance the recognition rate of weakly-supervised sound event detection</li>
<li>results:  validated on the standard DCASE dataset, the proposed approach shows efficacy in improving the recognition rate of weakly-supervised sound event detection.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Weakly-supervised learning has emerged as a promising approach to leverage limited labeled data in various domains by bridging the gap between fully supervised methods and unsupervised techniques. Acquisition of strong annotations for detecting sound events is prohibitively expensive, making weakly supervised learning a more cost-effective and broadly applicable alternative. In order to enhance the recognition rate of the learning of detection of weakly-supervised sound events, we introduce a Frame Pairwise Distance (FPD) loss branch, complemented with a minimal amount of synthesized data. The corresponding sampling and label processing strategies are also proposed. Two distinct distance metrics are employed to evaluate the proposed approach. Finally, the method is validated on the standard DCASE dataset. The obtained experimental results corroborated the efficacy of this approach.
</details>
<details>
<summary>摘要</summary>
微监督学习已经成为各个领域中利用有限的标注数据的可靠方法之一，它在完全监督方法和无监督技术之间填补了空隙。然而，获取听音事件的强制标注是非常昂贵的，使得微监督学习成为更加经济可行的和广泛适用的替代方案。为提高微监督学习检测听音事件的识别率，我们在本文中引入帧对比距离（FPD）损失分支，并采用一小量的合成数据来补充。同时，我们也提出了采样和标签处理策略。两种不同的距离度量被使用来评估该方法。最后，我们在标准的DCASE dataset上验证了该方法的效果。实验结果证明了该方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="CoMFLP-Correlation-Measure-based-Fast-Search-on-ASR-Layer-Pruning"><a href="#CoMFLP-Correlation-Measure-based-Fast-Search-on-ASR-Layer-Pruning" class="headerlink" title="CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning"></a>CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11768">http://arxiv.org/abs/2309.11768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liu, Zhiyuan Peng, Tan Lee</li>
<li>for: 提高资源受限设备上Transformer型声音识别（ASR）模型的性能。</li>
<li>methods: 基于相关度度量的快速搜索层剥离（LP）算法，从多层模型中剥离 redundancy。</li>
<li>results: 比前一代LP方法更高效，仅需常量时间复杂度，并且可以提高ASR模型的性能。<details>
<summary>Abstract</summary>
Transformer-based speech recognition (ASR) model with deep layers exhibited significant performance improvement. However, the model is inefficient for deployment on resource-constrained devices. Layer pruning (LP) is a commonly used compression method to remove redundant layers. Previous studies on LP usually identify the redundant layers according to a task-specific evaluation metric. They are time-consuming for models with a large number of layers, even in a greedy search manner. To address this problem, we propose CoMFLP, a fast search LP algorithm based on correlation measure. The correlation between layers is computed to generate a correlation matrix, which identifies the redundancy among layers. The search process is carried out in two steps: (1) coarse search: to determine top $K$ candidates by pruning the most redundant layers based on the correlation matrix; (2) fine search: to select the best pruning proposal among $K$ candidates using a task-specific evaluation metric. Experiments on an ASR task show that the pruning proposal determined by CoMFLP outperforms existing LP methods while only requiring constant time complexity. The code is publicly available at https://github.com/louislau1129/CoMFLP.
</details>
<details>
<summary>摘要</summary>
“ transformer-based  speech recognition（ASR）模型 WITH deep layers  exhibited significant performance improvement. However, the model is inefficient for deployment on resource-constrained devices. layer pruning（LP）is a commonly used compression method to remove redundant layers. Previous studies on LP usually identify the redundant layers according to a task-specific evaluation metric. They are time-consuming for models with a large number of layers, even in a greedy search manner. To address this problem, we propose CoMFLP, a fast search LP algorithm based on correlation measure. The correlation between layers is computed to generate a correlation matrix, which identifies the redundancy among layers. The search process is carried out in two steps: (1) coarse search: to determine top $K$ candidates by pruning the most redundant layers based on the correlation matrix; (2) fine search: to select the best pruning proposal among $K$ candidates using a task-specific evaluation metric. Experiments on an ASR task show that the pruning proposal determined by CoMFLP outperforms existing LP methods while only requiring constant time complexity. The code is publicly available at https://github.com/louislau1129/CoMFLP.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Sparsely-Shared-LoRA-on-Whisper-for-Child-Speech-Recognition"><a href="#Sparsely-Shared-LoRA-on-Whisper-for-Child-Speech-Recognition" class="headerlink" title="Sparsely Shared LoRA on Whisper for Child Speech Recognition"></a>Sparsely Shared LoRA on Whisper for Child Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11756">http://arxiv.org/abs/2309.11756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liu, Ying Qin, Zhiyuan Peng, Tan Lee</li>
<li>for: 这个论文想要提高 whisper 自动语音识别（ASR）模型在low-resource语音上的零shot性能。</li>
<li>methods: 这个论文使用了 parameter-efficient fine-tuning（PEFT）方法，包括 LoRA 和 AdaLoRA，以及一种新的 Sparsely Shared LoRA（S2-LoRA）方法。</li>
<li>results:  experiments 表明，S2-LoRA 可以在low-resource中国儿童语音上达到与 AdaLoRA 相当的适应性，并且在out-of-domain数据上表现更好，并且自动学习的rank分布与 AdaLoRA 的分布类似。<details>
<summary>Abstract</summary>
Whisper is a powerful automatic speech recognition (ASR) model. Nevertheless, its zero-shot performance on low-resource speech requires further improvement. Child speech, as a representative type of low-resource speech, is leveraged for adaptation. Recently, parameter-efficient fine-tuning (PEFT) in NLP was shown to be comparable and even better than full fine-tuning, while only needing to tune a small set of trainable parameters. However, current PEFT methods have not been well examined for their effectiveness on Whisper. In this paper, only parameter composition types of PEFT approaches such as LoRA and Bitfit are investigated as they do not bring extra inference costs. Different popular PEFT methods are examined. Particularly, we compare LoRA and AdaLoRA and figure out the learnable rank coefficient is a good design. Inspired by the sparse rank distribution allocated by AdaLoRA, a novel PEFT approach Sparsely Shared LoRA (S2-LoRA) is proposed. The two low-rank decomposed matrices are globally shared. Each weight matrix only has to maintain its specific rank coefficients that are constrained to be sparse. Experiments on low-resource Chinese child speech show that with much fewer trainable parameters, S2-LoRA can achieve comparable in-domain adaptation performance to AdaLoRA and exhibit better generalization ability on out-of-domain data. In addition, the rank distribution automatically learned by S2-LoRA is found to have similar patterns to AdaLoRA's allocation.
</details>
<details>
<summary>摘要</summary>
噪音是一个强大的自动语音识别（ASR）模型。然而，它的零实例性表现在低资源语音上仍需进一步改进。儿童语音作为低资源语音的代表类型，被利用于适应。最近， Parametric Efficient Fine-Tuning（PEFT）在自然语言处理（NLP）中显示了相当于或更好的性能，而只需要调整一小部分可变参数。然而，当前PEFT方法尚未对噪音进行了广泛的检验。本文只 investigate parameter composition type PEFTapproaches such as LoRA和Bitfit，因为它们不会增加额外的推理成本。不同的流行PEFT方法被比较。特别是，我们比较LoRA和AdaLoRA，并发现了可学习排名系数是一个好的设计。受AdaLoRA的稀疏排名分布启发，我们提出了一种新的PEFT方法——Sparsely Shared LoRA（S2-LoRA）。两个低级别分解的矩阵在全球共享。每个weight矩阵只需保持它的特定排名系数，这些系数是约束为稀疏的。实验表明，与少量可变参数，S2-LoRA可以达到与AdaLoRA相当的适应性，并且在对外域数据进行推理时表现更好。此外，S2-LoRA自动学习的排名分布与AdaLoRA的分布有相似的模式。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-In-the-Wild-Data-for-Effective-Self-Supervised-Pretraining-in-Speaker-Recognition"><a href="#Leveraging-In-the-Wild-Data-for-Effective-Self-Supervised-Pretraining-in-Speaker-Recognition" class="headerlink" title="Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition"></a>Leveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11730">http://arxiv.org/abs/2309.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Wang, Qibing Bai, Qi Liu, Jianwei Yu, Zhengyang Chen, Bing Han, Yanmin Qian, Haizhou Li</li>
<li>for: 提高 speaker recognition 系统性能</li>
<li>methods: 使用 DINO 自助学习方法和 confidence-based data filtering 算法</li>
<li>results: 在大规模的 WenetSpeech 数据集和 CNCeleb 数据集上提高 speaker recognition 系统性能，并且需要 fewer 训练数据<details>
<summary>Abstract</summary>
Current speaker recognition systems primarily rely on supervised approaches, constrained by the scale of labeled datasets. To boost the system performance, researchers leverage large pretrained models such as WavLM to transfer learned high-level features to the downstream speaker recognition task. However, this approach introduces extra parameters as the pretrained model remains in the inference stage. Another group of researchers directly apply self-supervised methods such as DINO to speaker embedding learning, yet they have not explored its potential on large-scale in-the-wild datasets. In this paper, we present the effectiveness of DINO training on the large-scale WenetSpeech dataset and its transferability in enhancing the supervised system performance on the CNCeleb dataset. Additionally, we introduce a confidence-based data filtering algorithm to remove unreliable data from the pretraining dataset, leading to better performance with less training data. The associated pretrained models, confidence files, pretraining and finetuning scripts will be made available in the Wespeaker toolkit.
</details>
<details>
<summary>摘要</summary>
In this paper, we show the effectiveness of DINO training on the large-scale WenetSpeech dataset and its transferability in enhancing supervised system performance on the CNCeleb dataset. We also introduce a confidence-based data filtering algorithm to remove unreliable data from the pretraining dataset, leading to better performance with less training data. The associated pretrained models, confidence files, pretraining and finetuning scripts will be available in the Wespeaker toolkit.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.SD_2023_09_21/" data-id="clmvt7tbv00mb26rddjjh7ryq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/09/21/cs.CV_2023_09_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-09-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">21</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
