
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-09-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Active Stereo Without Pattern Projector paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12315 repo_url: https:&#x2F;&#x2F;github.com&#x2F;bartn8&#x2F;vppstereo paper_authors: Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-09-21">
<meta property="og:url" content="https://nullscc.github.io/2023/09/21/cs.CV_2023_09_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Active Stereo Without Pattern Projector paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.12315 repo_url: https:&#x2F;&#x2F;github.com&#x2F;bartn8&#x2F;vppstereo paper_authors: Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-21T13:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:59:14.102Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.CV_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T13:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-09-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Stereo-Without-Pattern-Projector"><a href="#Active-Stereo-Without-Pattern-Projector" class="headerlink" title="Active Stereo Without Pattern Projector"></a>Active Stereo Without Pattern Projector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12315">http://arxiv.org/abs/2309.12315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bartn8/vppstereo">https://github.com/bartn8/vppstereo</a></li>
<li>paper_authors: Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti, Stefano Mattoccia</li>
<li>for: 提出了一种基于活动镜头原理的标准静止相机系统中的新框架，无需物理模式投影器。</li>
<li>methods: 通过在左右图像中虚拟投射模式，根据深度感知器获取的稀疏测量。任何设备都可以轻松地插入我们的框架中，在任何环境中实现虚拟活动镜头设置，超越物理模式投影器的限制，如工作范围或环境条件。</li>
<li>results: 对室内&#x2F;室外 dataset进行了实验，包括长距离和近距离场景，实验结果表明我们的方法可以准确地提高镜头算法和深度网络的准确率。<details>
<summary>Abstract</summary>
This paper proposes a novel framework integrating the principles of active stereo in standard passive camera systems without a physical pattern projector. We virtually project a pattern over the left and right images according to the sparse measurements obtained from a depth sensor. Any such devices can be seamlessly plugged into our framework, allowing for the deployment of a virtual active stereo setup in any possible environment, overcoming the limitation of pattern projectors, such as limited working range or environmental conditions. Experiments on indoor/outdoor datasets, featuring both long and close-range, support the seamless effectiveness of our approach, boosting the accuracy of both stereo algorithms and deep networks.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的框架，将活动镜头原理 integrate into标准的普通摄像头系统中，不需要物理Pattern projector。我们在左右图像上虚拟展示了模式，根据深度传感器获得的稀疏测量。任何这种设备都可以轻松插入我们的框架中， allowing for the deployment of a virtual active stereo setup in any possible environment，超越了模式项目器的限制，如工作范围或环境条件。对室内/室外数据集进行了实验，包括长距离和近距离，支持我们的方法的无缝效果，提高了镜头算法和深度网络的准确性。
</details></li>
</ul>
<hr>
<h2 id="TinyCLIP-CLIP-Distillation-via-Affinity-Mimicking-and-Weight-Inheritance"><a href="#TinyCLIP-CLIP-Distillation-via-Affinity-Mimicking-and-Weight-Inheritance" class="headerlink" title="TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance"></a>TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12314">http://arxiv.org/abs/2309.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi, Chen, Xinggang Wang, Hongyang Chao, Han Hu</li>
<li>for: 这个论文提出了一种新的跨模型蒸发法，叫做TinyCLIP，用于大规模的语言-图像预训模型。</li>
<li>methods: 这个方法 introduces two core techniques: affinity mimicking和weight inheritance。affinity mimicking探索了modalities之间的互动，使学生模型能够模仿老师的跨modalities的学习行为，实现视征语Modal Affinity Space中的对应关系。weight inheritance将老师模型的预训过的类 weights传递给学生模型，以提高蒸发效率。</li>
<li>results: 实验结果显示，TinyCLIP可以将预训CLIP ViT-B&#x2F;32的大小增加50%，并维持相同的零配置性性能。而且，将蒸发进行多阶段进度的实现了对应关系的增强。此外，我们的TinyCLIP ViT-8M&#x2F;16，在YFCC-15M上训练，在ImageNet上取得了41.1%的零配置性top-1准确率，比原CLIP ViT-B&#x2F;16高3.5%，并且只使用8.9%的参数。最后，我们显示了TinyCLIP在多个下游任务中的优良传播性。代码和模型将在<a target="_blank" rel="noopener" href="https://aka.ms/tinyclip%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://aka.ms/tinyclip上开源。</a><details>
<summary>Abstract</summary>
In this paper, we propose a novel cross-modal distillation method, called TinyCLIP, for large-scale language-image pre-trained models. The method introduces two core techniques: affinity mimicking and weight inheritance. Affinity mimicking explores the interaction between modalities during distillation, enabling student models to mimic teachers' behavior of learning cross-modal feature alignment in a visual-linguistic affinity space. Weight inheritance transmits the pre-trained weights from the teacher models to their student counterparts to improve distillation efficiency. Moreover, we extend the method into a multi-stage progressive distillation to mitigate the loss of informative weights during extreme compression. Comprehensive experiments demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of the pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot performance. While aiming for comparable performance, distillation with weight inheritance can speed up the training by 1.4 - 7.8 $\times$ compared to training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M, achieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet, surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9% parameters. Finally, we demonstrate the good transferability of TinyCLIP in various downstream tasks. Code and models will be open-sourced at https://aka.ms/tinyclip.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的跨Modal Distillation方法，称为TinyCLIP，用于大规模语言图像预训练模型。该方法 introduce two core techniques：对模式的亲和力模仿和重量继承。对模式的亲和力模仿探索了多Modalities在Distillation过程中的交互，使学生模型能够模仿教师模型在视觉语言对应空间中学习跨Modal featureAlignment的行为。重量继承将预训练的重量从教师模型传递给其学生版本，以提高Distillation的效率。此外，我们扩展了该方法到多Stage Progressive Distillation，以mitigate the loss of informative weights during extreme compression。我们的实验表明，TinyCLIP可以将预训练CLIP ViT-B/32的大小减少50%，保持相同的零shot性能。而在尝试保持相同性能的情况下，与教师模型的Distillation可以加速训练1.4-7.8倍。此外，我们的TinyCLIP ViT-8M/16，在YFCC-15M上训练，在ImageNet上 achieve Zero-shot top-1准确率41.1%，比原CLIP ViT-B/16提高3.5%，使用只有8.9%的参数。最后，我们示出了TinyCLIP在多种下游任务中的好传输性。代码和模型将在https://aka.ms/tinyclip上开源。
</details></li>
</ul>
<hr>
<h2 id="TalkNCE-Improving-Active-Speaker-Detection-with-Talk-Aware-Contrastive-Learning"><a href="#TalkNCE-Improving-Active-Speaker-Detection-with-Talk-Aware-Contrastive-Learning" class="headerlink" title="TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning"></a>TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12306">http://arxiv.org/abs/2309.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaeyoung Jung, Suyeon Lee, Kihyun Nam, Kyeongha Rho, You Jin Kim, Youngjoon Jang, Joon Son Chung</li>
<li>for: 本文旨在提出一种新的对话监视损失函数，以便在视频帧序中确定人员是否正在说话。</li>
<li>methods: 本文提出了一种名为TalkNCE的对话感知损失函数，该损失函数仅在屏幕上显示的人员实际说话的部分应用。这种损失函数鼓励模型通过自然的语音和面部运动的对应学习有效的表示。</li>
<li>results: 实验表明，我们的损失函数可以轻松地与现有的ASD模型一起进行joint优化，提高其性能。我们的方法在AVA-ActiveSpeaker和ASW datasets上达到了状态艺术水平。<details>
<summary>Abstract</summary>
The goal of this work is Active Speaker Detection (ASD), a task to determine whether a person is speaking or not in a series of video frames. Previous works have dealt with the task by exploring network architectures while learning effective representations has been less explored. In this work, we propose TalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part of the full segments where a person on the screen is actually speaking. This encourages the model to learn effective representations through the natural correspondence of speech and facial movements. Our loss can be jointly optimized with the existing objectives for training ASD models without the need for additional supervision or training data. The experiments demonstrate that our loss can be easily integrated into the existing ASD frameworks, improving their performance. Our method achieves state-of-the-art performances on AVA-ActiveSpeaker and ASW datasets.
</details>
<details>
<summary>摘要</summary>
本工作的目标是活动说话人检测（ASD），即在视频帧序中确定人是否说话。先前的工作主要关注网络架构，而学习有效表示的研究相对较少。在这项工作中，我们提议了一种新的对话抑制损失函数，即TalkNCE。这种损失函数只应用于屏幕上的人是否实际说话的部分段落。这会让模型学习有效的表示，通过自然的语音和面部运动之间的相对应。我们的损失函数可以与现有的ASD模型训练目标一起优化，无需额外的监督或训练数据。实验表明，我们的损失函数可以轻松地与现有的ASD框架集成，提高其性能。我们的方法在AVA-ActiveSpeaker和ASW数据集上达到了状态盘点的表现。
</details></li>
</ul>
<hr>
<h2 id="SlowFast-Network-for-Continuous-Sign-Language-Recognition"><a href="#SlowFast-Network-for-Continuous-Sign-Language-Recognition" class="headerlink" title="SlowFast Network for Continuous Sign Language Recognition"></a>SlowFast Network for Continuous Sign Language Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12304">http://arxiv.org/abs/2309.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junseok Ahn, Youngjoon Jang, Joon Son Chung</li>
<li>for: 本研究旨在实现有效的Continuous Sign Language Recognition（CSLR）特征提取。</li>
<li>methods: 我们使用了两条不同的时间分辨率的SlowFast网络，每一条路径独立地捕捉手势（手势）和表情（表情）的空间信息，以及动作（运动）信息。此外，我们还提出了两种特有的特征融合方法：（1）双向特征融合（BFF），使得动态 semantics transfer into spatial semantics和vice versa；和（2）路径特征增强（PFE），通过辅助子网络增强动态和空间表示，而不需Extra的推理时间。</li>
<li>results: 我们的模型在流行的CSLR数据集上（包括PHOENIX14、PHOENIX14-T和CSL-Daily）达到了当前领先的性能。<details>
<summary>Abstract</summary>
The objective of this work is the effective extraction of spatial and dynamic features for Continuous Sign Language Recognition (CSLR). To accomplish this, we utilise a two-pathway SlowFast network, where each pathway operates at distinct temporal resolutions to separately capture spatial (hand shapes, facial expressions) and dynamic (movements) information. In addition, we introduce two distinct feature fusion methods, carefully designed for the characteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which facilitates the transfer of dynamic semantics into spatial semantics and vice versa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and spatial representations through auxiliary subnetworks, while avoiding the need for extra inference time. As a result, our model further strengthens spatial and dynamic representations in parallel. We demonstrate that the proposed framework outperforms the current state-of-the-art performance on popular CSLR datasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.
</details>
<details>
<summary>摘要</summary>
目标是提取CSLR中的空间和动态特征，我们利用了两个不同的时间分辨率的SlowFast网络，每个路径独立捕捉空间（手势、 facial expressions）和动态（运动）信息。此外，我们还提出了两种特有的特征融合方法：（1）双向特征融合（BFF），使动态 semantics transfer into spatial semantics和vice versa；（2）路径特征增强（PFE），通过辅助子网络增强动态和空间表示，而不需要额外的推理时间。这种方法使得我们的模型在平行的情况下进一步强化了空间和动态表示。我们的模型在各种CSLR数据集上达到了当前领先的性能，包括PHOENIX14、PHOENIX14-T和CSL-Daily等。
</details></li>
</ul>
<hr>
<h2 id="PanoVOS-Bridging-Non-panoramic-and-Panoramic-Views-with-Transformer-for-Video-Segmentation"><a href="#PanoVOS-Bridging-Non-panoramic-and-Panoramic-Views-with-Transformer-for-Video-Segmentation" class="headerlink" title="PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation"></a>PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12303">http://arxiv.org/abs/2309.12303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, Wei Zhang</li>
<li>for: 这篇论文旨在提供一个大型投影视频分割 dataset，以满足投影视频中的视频分割问题。</li>
<li>methods: 该论文使用了15种市场上的视频对象分割模型进行评估，并通过错误分析发现这些模型无法处理投影视频中的像素级别内容缺失。因此，该论文提出了一种基于 semantic boundary information的 Panoramic Space Consistency Transformer（PSCFormer），可以有效地利用上一帧的semantic boundary信息来进行像素级别匹配。</li>
<li>results: 对比之前的最佳模型，我们的 PSCFormer 网络在投影视频下表现出了优于其他模型的 segmentation 结果。<details>
<summary>Abstract</summary>
Panoramic videos contain richer spatial information and have attracted tremendous amounts of attention due to their exceptional experience in some fields such as autonomous driving and virtual reality. However, existing datasets for video segmentation only focus on conventional planar images. To address the challenge, in this paper, we present a panoramic video dataset, PanoVOS. The dataset provides 150 videos with high video resolutions and diverse motions. To quantify the domain gap between 2D planar videos and panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS) models on PanoVOS. Through error analysis, we found that all of them fail to tackle pixel-level content discontinues of panoramic videos. Thus, we present a Panoramic Space Consistency Transformer (PSCFormer), which can effectively utilize the semantic boundary information of the previous frame for pixel-level matching with the current frame. Extensive experiments demonstrate that compared with the previous SOTA models, our PSCFormer network exhibits a great advantage in terms of segmentation results under the panoramic setting. Our dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can advance the development of panoramic segmentation/tracking.
</details>
<details>
<summary>摘要</summary>
拼接视频含有更多的空间信息，吸引了很多关注，特别是在自动驾驶和虚拟现实等领域。然而，现有的视频分割数据集只关注传统的平面图像。为了解决这个挑战，在这篇论文中，我们提供了拼接视频数据集（PanoVOS）。该数据集包含150个高分辨率视频和多样化的运动。为了衡量2D平面视频和拼接视频之间的域间差，我们评估了15种市场上的视频对象分割（VOS）模型在PanoVOS上。经过错误分析，我们发现所有模型都无法处理拼接视频中像素级别的内容缺失。因此，我们提出了拼接空间一致变换器（PSCFormer），可以有效利用上一帧的semantic边界信息 для像素级匹配当前帧。广泛的实验表明，与前一代最佳模型相比，我们的PSCFormer网络在拼接设置下展现出了优于其他模型的分割结果。我们的数据集将带来新的拼接VOS挑战，我们希望通过PanoVOS来推动拼接分割/跟踪的发展。
</details></li>
</ul>
<hr>
<h2 id="Text-Guided-Vector-Graphics-Customization"><a href="#Text-Guided-Vector-Graphics-Customization" class="headerlink" title="Text-Guided Vector Graphics Customization"></a>Text-Guided Vector Graphics Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12302">http://arxiv.org/abs/2309.12302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiying Zhang, Nanxuan Zhao, Jing Liao</li>
<li>for: 生成高质量自定义 вектор图形，以满足设计师的创造需求。</li>
<li>methods: 提posed a novel pipeline that leverages large pre-trained text-to-image models and semantic-based path alignment method to generate customized raster images guided by textual prompts, while preserving the properties and layer-wise information of a given exemplar SVG.</li>
<li>results: 经过广泛评估，得到了多 metric 的优秀Result，证明了该管道的效果iveness in generating diverse customizations of vector graphics with exceptional quality.<details>
<summary>Abstract</summary>
Vector graphics are widely used in digital art and valued by designers for their scalability and layer-wise topological properties. However, the creation and editing of vector graphics necessitate creativity and design expertise, leading to a time-consuming process. In this paper, we propose a novel pipeline that generates high-quality customized vector graphics based on textual prompts while preserving the properties and layer-wise information of a given exemplar SVG. Our method harnesses the capabilities of large pre-trained text-to-image models. By fine-tuning the cross-attention layers of the model, we generate customized raster images guided by textual prompts. To initialize the SVG, we introduce a semantic-based path alignment method that preserves and transforms crucial paths from the exemplar SVG. Additionally, we optimize path parameters using both image-level and vector-level losses, ensuring smooth shape deformation while aligning with the customized raster image. We extensively evaluate our method using multiple metrics from vector-level, image-level, and text-level perspectives. The evaluation results demonstrate the effectiveness of our pipeline in generating diverse customizations of vector graphics with exceptional quality. The project page is https://intchous.github.io/SVGCustomization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>Vector graphics 广泛用于数字艺术中，设计师们喜欢它们因为它们可扩展和层次结构的特性。然而，创建和修改vector graphics需要创作和设计技能，这会使得过程变得时间consuming。在这篇论文中，我们提出了一个新的管道，可以根据文本提示生成高质量自定义vector graphics，同时保持原始SVG的特性和层次信息。我们利用大型预训练的文本到图像模型的能力，通过微调模型的交叉注意力层，生成基于文本提示的自定义静止图像。为初始化SVG，我们引入了基于 semantics的路径对齐方法，保持和修改原始SVG中重要的路径。此外，我们使用图像水平和向量水平的损失函数进行路径参数优化，确保图像和向量图像的平滑形变。我们进行了广泛的评估，结果表明我们的管道可以生成多样化的自定义vector graphics，质量极高。项目页面是https://intchous.github.io/SVGCustomization。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Input-image-Normalization-for-Solving-Mode-Collapse-Problem-in-GAN-based-X-ray-Images"><a href="#Adaptive-Input-image-Normalization-for-Solving-Mode-Collapse-Problem-in-GAN-based-X-ray-Images" class="headerlink" title="Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images"></a>Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12245">http://arxiv.org/abs/2309.12245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: 增强医学影像数据集的多样性，提高机器学习分类器的性能。</li>
<li>methods: 使用生成对抗网络（GAN）技术生成 sintetic X-ray 图像，并在 GAN 中加入 adaptive input-image normalization 来解决模式塌collapse问题。</li>
<li>results: DCGAN 和 ACGAN  WITH adaptive input-image normalization 能够提高 classification 性能和多样性 scores，相比于 DCGAN 和 ACGAN  WITH un-normalized X-ray images。<details>
<summary>Abstract</summary>
Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse problems. Synthetically generated images are utilized for data augmentation and training a Vision Transformer model. The classification performance of the model is evaluated using accuracy, recall, and precision scores. Results demonstrate that the DCGAN and the ACGAN with adaptive input-image normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as evidenced by the superior diversity scores and classification scores.
</details>
<details>
<summary>摘要</summary>
生成对抗网络可能会遇到两种不同的模式塌溃问题：内类模式塌溃和间类模式塌溃。这两种问题都会导致生成的 sintetic 图像失去多样化。本文研究了这两种模式塌溃问题，并评估它们对生成的 sintetic X-ray 图像的多样化的影响。这个研究还提供了一种实验室的证明，表明将适应输入图像Normalization与深度卷积GAN和辅助分类器GAN相结合可以解决模式塌溃问题。生成的 sintetic 图像被用于数据增强和训练一个 Vision Transformer 模型。模型的分类性能被评估使用准确率、回归率和精度分数。结果表明，DCGAN 和 ACGAN  WITH 适应输入图像Normalization 比 DCGAN 和 ACGAN  WITH 未normalized X-ray 图像表现更好，根据多样化分数和分类分数。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Reliably-Improve-the-Robustness-to-Image-Acquisition-of-Remote-Sensing-of-PV-Systems"><a href="#Can-We-Reliably-Improve-the-Robustness-to-Image-Acquisition-of-Remote-Sensing-of-PV-Systems" class="headerlink" title="Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?"></a>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12214">http://arxiv.org/abs/2309.12214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</li>
<li>for: 监控区域级单位的拍照电力系统</li>
<li>methods: 利用波浪对数法（WCAM）分解模型预测的空间�atura领域</li>
<li>results: 提高了内部积分模型的可靠性和敏感度，并获得了对于采购条件的变化的深入理解，以增加清洁能源的安全组合。<details>
<summary>Abstract</summary>
Photovoltaic (PV) energy is crucial for the decarbonization of energy systems. Due to the lack of centralized data, remote sensing of rooftop PV installations is the best option to monitor the evolution of the rooftop PV installed fleet at a regional scale. However, current techniques lack reliability and are notably sensitive to shifts in the acquisition conditions. To overcome this, we leverage the wavelet scale attribution method (WCAM), which decomposes a model's prediction in the space-scale domain. The WCAM enables us to assess on which scales the representation of a PV model rests and provides insights to derive methods that improve the robustness to acquisition conditions, thus increasing trust in deep learning systems to encourage their use for the safe integration of clean energy in electric systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Brain-Tumor-Detection-Using-Deep-Learning-Approaches"><a href="#Brain-Tumor-Detection-Using-Deep-Learning-Approaches" class="headerlink" title="Brain Tumor Detection Using Deep Learning Approaches"></a>Brain Tumor Detection Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12193">http://arxiv.org/abs/2309.12193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Razia Sultana Misu</li>
<li>for: 本研究旨在使用深度学习技术自动检测脑肿瘤，以提高脑肿瘤检测和分类精度。</li>
<li>methods: 本研究使用了五种转移学习模型，包括VGG16、VGG19、DenseNet121、ResNet50和YOLO V4，其中ResNet50达到了最高准确率99.54%。</li>
<li>results: 研究表明，使用深度学习技术可以提高脑肿瘤检测和分类精度，并且ResNet50模型达到了最高准确率。<details>
<summary>Abstract</summary>
Brain tumors are collections of abnormal cells that can develop into masses or clusters. Because they have the potential to infiltrate other tissues, they pose a risk to the patient. The main imaging technique used, MRI, may be able to identify a brain tumor with accuracy. The fast development of Deep Learning methods for use in computer vision applications has been facilitated by a vast amount of training data and improvements in model construction that offer better approximations in a supervised setting. The need for these approaches has been the main driver of this expansion. Deep learning methods have shown promise in improving the precision of brain tumor detection and classification using magnetic resonance imaging (MRI). The study on the use of deep learning techniques, especially ResNet50, for brain tumor identification is presented in this abstract. As a result, this study investigates the possibility of automating the detection procedure using deep learning techniques. In this study, I utilized five transfer learning models which are VGG16, VGG19, DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest accuracy 99.54%. The goal of the study is to guide researchers and medical professionals toward powerful brain tumor detecting systems by employing deep learning approaches by way of this evaluation and analysis.
</details>
<details>
<summary>摘要</summary>
脑肿是一种集群畸形细胞的发育，可能形成肿体或集群。由于它们可能会渗透到其他组织，因此对患者存在风险。主要用于识别脑肿的成像技术是MRI，可能准确地识别脑肿。深度学习方法在计算机视觉应用中的快速发展，得益于庞大的训练数据和改进的模型构造，以及更好的超级vised设定。这些方法的需求是扩展的推动者。深度学习方法在MRI中识别和分类脑肿方面表现出了承诺，特别是使用ResNet50模型，其最高准确率为99.54%。本研究旨在通过深度学习方法自动识别脑肿的可能性，并提供一种可靠的脑肿检测系统。本研究使用了五种转移学习模型，包括VGG16、VGG19、DenseNet121、ResNet50和YOLO V4，其中ResNet50提供了最高准确率。本研究的目标是导引研究人员和医疗专业人员通过深度学习方法来实现高效的脑肿检测系统，以便更好地满足医疗需求。
</details></li>
</ul>
<hr>
<h2 id="SG-Bot-Object-Rearrangement-via-Coarse-to-Fine-Robotic-Imagination-on-Scene-Graphs"><a href="#SG-Bot-Object-Rearrangement-via-Coarse-to-Fine-Robotic-Imagination-on-Scene-Graphs" class="headerlink" title="SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs"></a>SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12188">http://arxiv.org/abs/2309.12188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam</li>
<li>for: This paper focuses on developing a novel rearrangement framework for robotic-environment interactions, with the goal of achieving lightweight, real-time, and user-controllable characteristics.</li>
<li>methods: The proposed framework, called SG-Bot, utilizes a coarse-to-fine scheme with a scene graph as the scene representation, and employs a three-fold procedure consisting of observation, imagination, and execution to address the task.</li>
<li>results: Experimental results show that SG-Bot outperforms competitors by a large margin, demonstrating its effectiveness in embodied AI tasks.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文关注开发一种新的重新排序框架，用于机器人环境互动，目的是实现轻量级、实时、用户可控的特点。</li>
<li>methods: 提议的框架被称为SG-Bot，它采用一种粗粒度到细粒度的方案，使用场景图作为场景表示，并采用观察、想象和执行的三重过程来解决问题。</li>
<li>results: 实验结果显示，SG-Bot比竞争对手大幅提高了性能，证明了它在机器人AI任务中的有效性。<details>
<summary>Abstract</summary>
Object rearrangement is pivotal in robotic-environment interactions, representing a significant capability in embodied AI. In this paper, we present SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme with a scene graph as the scene representation. Unlike previous methods that rely on either known goal priors or zero-shot large models, SG-Bot exemplifies lightweight, real-time, and user-controllable characteristics, seamlessly blending the consideration of commonsense knowledge with automatic generation capabilities. SG-Bot employs a three-fold procedure--observation, imagination, and execution--to adeptly address the task. Initially, objects are discerned and extracted from a cluttered scene during the observation. These objects are first coarsely organized and depicted within a scene graph, guided by either commonsense or user-defined criteria. Then, this scene graph subsequently informs a generative model, which forms a fine-grained goal scene considering the shape information from the initial scene and object semantics. Finally, for execution, the initial and envisioned goal scenes are matched to formulate robotic action policies. Experimental results demonstrate that SG-Bot outperforms competitors by a large margin.
</details>
<details>
<summary>摘要</summary>
对象重新排序是人工智能中的一项关键能力，代表了机器人和环境之间的互动。在这篇论文中，我们提出了SG-Bot，一种新的重新排序框架，利用粗略到细化的方案，使用场景图作为场景表示。与前一代方法不同，SG-Bot不依赖于已知目标假设或大型零基础模型，而是具有轻量级、实时和用户可控的特点，可以协调考虑常识知识和自动生成能力。SG-Bot采用三个步骤—观察、想象和执行—以适应任务。首先，从杂乱的场景中提取和识别 объек，并将其粗略地组织和描述于场景图中，以 Commonsense 或用户定义的标准指导。然后，这个场景图将导引一个生成模型，该模型形成基于初始场景和物体 semantics 的细化目标场景。最后，为执行，初始和想象的目标场景相匹配，以形成机器人行为策略。实验结果表明，SG-Bot在竞争者之上大幅提高表现。
</details></li>
</ul>
<hr>
<h2 id="ORTexME-Occlusion-Robust-Human-Shape-and-Pose-via-Temporal-Average-Texture-and-Mesh-Encoding"><a href="#ORTexME-Occlusion-Robust-Human-Shape-and-Pose-via-Temporal-Average-Texture-and-Mesh-Encoding" class="headerlink" title="ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding"></a>ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average Texture and Mesh Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12183">http://arxiv.org/abs/2309.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Cheng, Bo Wang, Robby T. Tan</li>
<li>for:  improve the accuracy of 3D human shape and pose estimation in the presence of occlusion</li>
<li>methods: utilizes temporal information from the input video to better regularize the occluded body parts, and uses a novel average texture learning approach to learn the average appearance of a person and infer a mask based on the average texture</li>
<li>results: achieves significant improvement on the challenging multi-person 3DPW dataset, with 1.8 P-MPJPE error reduction compared to the state-of-the-art rendering-based methods, which enlarge the error up to 5.6 on the same dataset.<details>
<summary>Abstract</summary>
In 3D human shape and pose estimation from a monocular video, models trained with limited labeled data cannot generalize well to videos with occlusion, which is common in the wild videos. The recent human neural rendering approaches focusing on novel view synthesis initialized by the off-the-shelf human shape and pose methods have the potential to correct the initial human shape. However, the existing methods have some drawbacks such as, erroneous in handling occlusion, sensitive to inaccurate human segmentation, and ineffective loss computation due to the non-regularized opacity field. To address these problems, we introduce ORTexME, an occlusion-robust temporal method that utilizes temporal information from the input video to better regularize the occluded body parts. While our ORTexME is based on NeRF, to determine the reliable regions for the NeRF ray sampling, we utilize our novel average texture learning approach to learn the average appearance of a person, and to infer a mask based on the average texture. In addition, to guide the opacity-field updates in NeRF to suppress blur and noise, we propose the use of human body mesh. The quantitative evaluation demonstrates that our method achieves significant improvement on the challenging multi-person 3DPW dataset, where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based methods fail and enlarge the error up to 5.6 on the same dataset.
</details>
<details>
<summary>摘要</summary>
在单一影像视频中的3D人体和姿势估算中，使用有限标签数据训练的模型无法对受遮蔽的影像进行普遍化，这是野外影像中的普遍现象。现有的人类神经渲染方法强调新视角合成，由存在于市场上的人体形状和姿势方法进行初始化，有potential以更正初始人体形状。然而，现有的方法存在一些缺陷，例如错误地处理遮蔽、敏感于不准确的人类分割、以及无法有效地computing条件值场。为了解决这些问题，我们介绍ORTexME，一种防遮蔽时间方法，利用输入影像中的时间信息更好地调节遮蔽的体部部分。我们的ORTexME基于NeRF，以determine可靠的NeRF射线抽样区域，我们运用我们的新的平均文件学习方法学习人类的平均外观，并将其转换为对应的面瘫。此外，为了将NeRF中的透明度场更新更加稳定，我们提议使用人体骨架。我们的量值评估显示，我们的方法在多人3DPW数据集上取得了1.8P-MPJPE误差reduction，而SOTA的渲染基于方法则失败并将误差增加到5.6。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Sign-Language-Production-A-Gloss-Free-Approach-with-Discrete-Representations"><a href="#Autoregressive-Sign-Language-Production-A-Gloss-Free-Approach-with-Discrete-Representations" class="headerlink" title="Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations"></a>Autoregressive Sign Language Production: A Gloss-Free Approach with Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12179">http://arxiv.org/abs/2309.12179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eui Jun Hwang, Huije Lee, Jong C. Park</li>
<li>for: 本研究旨在提供一种直接将口语句子翻译成手语表达，无需中间件gloss。</li>
<li>methods: 本方法基于手势pose序列的vector量化，并支持高级解码方法和latent-levelAlignment。</li>
<li>results: 我们的方法在比较之前的手语生成方法的测试中表现出色，并且通过Back-Translation和Fréchet Gesture Distance的评价指标来证明其可靠性。<details>
<summary>Abstract</summary>
Gloss-free Sign Language Production (SLP) offers a direct translation of spoken language sentences into sign language, bypassing the need for gloss intermediaries. This paper presents the Sign language Vector Quantization Network, a novel approach to SLP that leverages Vector Quantization to derive discrete representations from sign pose sequences. Our method, rooted in both manual and non-manual elements of signing, supports advanced decoding methods and integrates latent-level alignment for enhanced linguistic coherence. Through comprehensive evaluations, we demonstrate superior performance of our method over prior SLP methods and highlight the reliability of Back-Translation and Fr\'echet Gesture Distance as evaluation metrics.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:《无折衣手语生产（SLP）》提供了直接将口语句子翻译成手语，无需中间件。这篇论文介绍了《手语 вектор量化网络》，一种新的SLP方法，利用量化向量来Derive discrete representation from sign pose sequences。我们的方法受到手语的手势和非手势元素的支持，支持高级解码方法并实现了层次匹配。通过全面的评估，我们证明了我们的方法的性能超过了先前的SLP方法，并指出了回传和Fréchet手势距离作为评估指标的可靠性。
</details></li>
</ul>
<hr>
<h2 id="SANPO-A-Scene-Understanding-Accessibility-Navigation-Pathfinding-Obstacle-Avoidance-Dataset"><a href="#SANPO-A-Scene-Understanding-Accessibility-Navigation-Pathfinding-Obstacle-Avoidance-Dataset" class="headerlink" title="SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset"></a>SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12172">http://arxiv.org/abs/2309.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar M. Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko</li>
<li>for: 这个论文是为了提供一个大规模的人центric视频集，用于 dense prediction 在户外环境中。</li>
<li>methods: 这个论文使用了两种视频SESSION：实际视频SESSION和Synthetic视频SESSION，其中Synthetic视频SESSION是由Parallel Domain提供的。所有SESSION都有密集的深度和ODometer标签，而一部分实际SESSION还有时间相关的密集精度分割标签。</li>
<li>results: 这个论文提供了零基eline和SANPObenchmark，以便未来的研究人员可以使用这些数据进行研究。作者希望通过SANPO dataset的挑战性，推动视频分割、深度估计、多任务视模型和synthetic-to-real领域的进步，并为人工导航系统提供更好的支持。<details>
<summary>Abstract</summary>
We introduce SANPO, a large-scale egocentric video dataset focused on dense prediction in outdoor environments. It contains stereo video sessions collected across diverse outdoor environments, as well as rendered synthetic video sessions. (Synthetic data was provided by Parallel Domain.) All sessions have (dense) depth and odometry labels. All synthetic sessions and a subset of real sessions have temporally consistent dense panoptic segmentation labels. To our knowledge, this is the first human egocentric video dataset with both large scale dense panoptic segmentation and depth annotations. In addition to the dataset we also provide zero-shot baselines and SANPO benchmarks for future research. We hope that the challenging nature of SANPO will help advance the state-of-the-art in video segmentation, depth estimation, multi-task visual modeling, and synthetic-to-real domain adaptation, while enabling human navigation systems.   SANPO is available here: https://google-research-datasets.github.io/sanpo_dataset/
</details>
<details>
<summary>摘要</summary>
我们介绍SANPO dataset，一个大规模的自我视角视频集，专注于户外环境中的密集预测。该集包括多个户外环境中的双视频会议，以及由Parallel Domain提供的Synthetic视频会议。所有会议都有密集的深度和运动标签。 Synthetic会议和一部分实际会议都有时间相关的密集精细分割标签。据我们所知，这是人类自我视角视频集中第一个具有大规模密集精细分割和深度标注的 dataset。此外，我们还提供了零基线和SANPO benchmark，以便未来的研究。我们希望SANPO的挑战性能够推动视频分割、深度估计、多任务视觉模型和Synthetic-to-Real领域的进步，同时帮助人类导航系统。SANPO dataset可以在以下链接下下载：https://google-research-datasets.github.io/sanpo_dataset/
</details></li>
</ul>
<hr>
<h2 id="Information-Forensics-and-Security-A-quarter-century-long-journey"><a href="#Information-Forensics-and-Security-A-quarter-century-long-journey" class="headerlink" title="Information Forensics and Security: A quarter-century-long journey"></a>Information Forensics and Security: A quarter-century-long journey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12159">http://arxiv.org/abs/2309.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Barni, Patrizio Campisi, Edward J. Delp, Gwenael Doërr, Jessica Fridrich, Nasir Memon, Fernando Pérez-González, Anderson Rocha, Luisa Verdoliva, Min Wu</li>
<li>for: 本研究领域的目的是确保人们在数位信息时代中使用设备、数据和知识Properties for authorized purposes, 并且将犯罪分子负责任。</li>
<li>methods: 本文发表了过去25年来研究社区对这个领域的重要技术进步，包括选择性领域的主要技术进步。</li>
<li>results: 本文呈现了过去25年来研究社区对这个领域的未来趋势。<details>
<summary>Abstract</summary>
Information Forensics and Security (IFS) is an active R&D area whose goal is to ensure that people use devices, data, and intellectual properties for authorized purposes and to facilitate the gathering of solid evidence to hold perpetrators accountable. For over a quarter century since the 1990s, the IFS research area has grown tremendously to address the societal needs of the digital information era. The IEEE Signal Processing Society (SPS) has emerged as an important hub and leader in this area, and the article below celebrates some landmark technical contributions. In particular, we highlight the major technological advances on some selected focus areas in the field developed in the last 25 years from the research community and present future trends.
</details>
<details>
<summary>摘要</summary>
信息审查安全（IFS）是一个活跃的研发领域，旨在确保人们在授权的目的下使用设备、数据和知识产权。自1990年代以来，IFS研发领域已经不断增长，以应对数字信息时代的社会需求。IEEE信号处理学会（SPS）在这个领域中已经成为重要的中心和领导者，这篇文章将展望过去25年内从研究 сообщества中出现的一些重要技术进步，并预测未来趋势。
</details></li>
</ul>
<hr>
<h2 id="Vulnerability-of-3D-Face-Recognition-Systems-to-Morphing-Attacks"><a href="#Vulnerability-of-3D-Face-Recognition-Systems-to-Morphing-Attacks" class="headerlink" title="Vulnerability of 3D Face Recognition Systems to Morphing Attacks"></a>Vulnerability of 3D Face Recognition Systems to Morphing Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12118">http://arxiv.org/abs/2309.12118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeet Vardam, Luuk Spreeuwers</li>
<li>for: 本研究旨在探讨3DFR系统对3D面部变换攻击的Robustness。</li>
<li>methods: 本文提出了一些方法可以生成高质量的3D面部变换，并对这些变换进行识别。</li>
<li>results: 研究发现，当3DFR系统面临look-a-like变换攻击时，其最高MMPMR约为40%，RMMR约为41.76%。<details>
<summary>Abstract</summary>
In recent years face recognition systems have been brought to the mainstream due to development in hardware and software. Consistent efforts are being made to make them better and more secure. This has also brought developments in 3D face recognition systems at a rapid pace. These 3DFR systems are expected to overcome certain vulnerabilities of 2DFR systems. One such problem that the domain of 2DFR systems face is face image morphing. A substantial amount of research is being done for generation of high quality face morphs along with detection of attacks from these morphs. Comparatively the understanding of vulnerability of 3DFR systems against 3D face morphs is less. But at the same time an expectation is set from 3DFR systems to be more robust against such attacks. This paper attempts to research and gain more information on this matter. The paper describes a couple of methods that can be used to generate 3D face morphs. The face morphs that are generated using this method are then compared to the contributing faces to obtain similarity scores. The highest MMPMR is obtained around 40% with RMMR of 41.76% when 3DFRS are attacked with look-a-like morphs.
</details>
<details>
<summary>摘要</summary>
近年来，人脸识别系统得到了主流的推广，归功于硬件和软件的发展。一直在努力使其更加完善和安全。这也导致了3D人脸识别系统（3DFR）的快速发展，被期望能够超越2D人脸识别系统（2DFR）的一些 limitation。其中一个2DFR系统面临的问题是人脸图像杂化（morphing），目前在这个领域进行了大量的研究，以生成高质量的人脸杂化和攻击检测。然而，对于3DFR系统对3D人脸杂化的抵抗能力的理解仍然较少。但是，预期3DFR系统能够更加强健地对抗这些攻击。本文尝试了对这个问题进行研究，并描述了一些可以用于生成3D人脸杂化的方法。生成的人脸杂化与贡献人脸进行比较，以获得相似度分数。在3DFRS遭受look-a-like杂化攻击时，最高的MMPMR为40%，RMMR为41.76%。
</details></li>
</ul>
<hr>
<h2 id="AutoPET-Challenge-2023-Sliding-Window-based-Optimization-of-U-Net"><a href="#AutoPET-Challenge-2023-Sliding-Window-based-Optimization-of-U-Net" class="headerlink" title="AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net"></a>AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12114">http://arxiv.org/abs/2309.12114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matt3o/autopet2-submission">https://github.com/matt3o/autopet2-submission</a></li>
<li>paper_authors: Matthias Hadlich, Zdravko Marinov, Rainer Stiefelhagen</li>
<li>for: The paper is written for researchers and developers working on tumor segmentation in medical imaging, particularly those using FDG-PET&#x2F;CT scans.</li>
<li>methods: The paper uses a dataset of 1014 FDG-PET&#x2F;CT studies to challenge researchers to develop accurate tumor segmentation methods that can distinguish between tumor-specific uptake and physiological uptake in normal tissues.</li>
<li>results: The paper provides a dataset of FDG-PET&#x2F;CT scans for researchers to use in developing and testing their tumor segmentation methods, with the goal of improving the accuracy of tumor segmentation in clinical practice.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为医学成像中的肿瘤分 segmentation研究人员和开发者写的，特别是使用FDG-PET&#x2F;CT扫描的。</li>
<li>methods: 这篇论文使用1014个FDG-PET&#x2F;CT成像数据集来挑战研究人员开发准确的肿瘤分 segmentation方法，能够将肿瘤吸收与正常组织的吸收区分开。</li>
<li>results: 这篇论文提供了1014个FDG-PET&#x2F;CT成像数据集，用于研究人员开发和测试他们的肿瘤分 segmentation方法，以提高临床中肿瘤分 segmentation的准确性。<details>
<summary>Abstract</summary>
Tumor segmentation in medical imaging is crucial and relies on precise delineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is widely used in clinical practice to detect metabolically active tumors. However, FDG-PET scans may misinterpret irregular glucose consumption in healthy or benign tissues as cancer. Combining PET with Computed Tomography (CT) can enhance tumor segmentation by integrating metabolic and anatomic information. FDG-PET/CT scans are pivotal for cancer staging and reassessment, utilizing radiolabeled fluorodeoxyglucose to highlight metabolically active regions. Accurately distinguishing tumor-specific uptake from physiological uptake in normal tissues is a challenging aspect of precise tumor segmentation. The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT studies, encouraging advancements in accurate tumor segmentation and analysis within the FDG-PET/CT domain. Code: https://github.com/matt3o/AutoPET2-Submission/
</details>
<details>
<summary>摘要</summary>
肿体分割在医学成像中非常重要，需要精准地界定。 fluorodeoxyglucosePositron-Emission Tomography（FDG-PET）在临床实践中广泛应用，用于检测具有异常代谢活性的肿体。然而，FDG-PET扫描可能会误分辨健康或正常组织中的不规则糖分消耗为癌症。将PET与计算机成像（CT）结合可以提高肿体分割，将元素学和解剖信息结合起来。FDG-PET/CT扫描是癌症评估和重新评估中非常重要的，通过使用标记的 fluorodeoxyglucose来高亮具有代谢活性的区域。正确地从正常组织中的代谢吸收中分化出肿体特有的吸收是精准肿体分割的挑战之一。AutoPET挑战提供了1014个FDG-PET/CT研究数据集，鼓励技术创新，以提高FDG-PET/CT频谱中的准确肿体分割和分析。代码：https://github.com/matt3o/AutoPET2-Submission/
</details></li>
</ul>
<hr>
<h2 id="Exploiting-CLIP-based-Multi-modal-Approach-for-Artwork-Classification-and-Retrieval"><a href="#Exploiting-CLIP-based-Multi-modal-Approach-for-Artwork-Classification-and-Retrieval" class="headerlink" title="Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval"></a>Exploiting CLIP-based Multi-modal Approach for Artwork Classification and Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12110">http://arxiv.org/abs/2309.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo</li>
<li>for:  investigate the application of recent CLIP model in artwork domain tasks</li>
<li>methods:  use semantically dense textual supervision to train visual models</li>
<li>results:  impressive zero-shot classification results and promising results in artwork-to-artwork and description-to-artwork domain<details>
<summary>Abstract</summary>
Given the recent advances in multimodal image pretraining where visual models trained with semantically dense textual supervision tend to have better generalization capabilities than those trained using categorical attributes or through unsupervised techniques, in this work we investigate how recent CLIP model can be applied in several tasks in artwork domain. We perform exhaustive experiments on the NoisyArt dataset which is a dataset of artwork images crawled from public resources on the web. On such dataset CLIP achieves impressive results on (zero-shot) classification and promising results in both artwork-to-artwork and description-to-artwork domain.
</details>
<details>
<summary>摘要</summary>
With the recent advances in multimodal image pretraining, visual models trained with semantically dense textual supervision have shown better generalization capabilities compared to those trained using categorical attributes or unsupervised techniques. In this work, we explore the application of the recent CLIP model in various tasks within the artwork domain.We conduct exhaustive experiments on the NoisyArt dataset, a collection of artwork images crawled from public resources on the web. On this dataset, CLIP achieves impressive results in zero-shot classification and promising results in both artwork-to-artwork and description-to-artwork domains.Here's the translation in Simplified Chinese:近期多modal图像预训练的进步，使用语义密集的文本监督训练的视觉模型在泛化能力方面表现出色，比使用分类属性或无监督技术训练的模型更好。在这个工作中，我们探索了最近的CLIP模型在艺术领域中的应用，并在NoisyArt数据集上进行了极限性的实验。在NoisyArt数据集上，CLIP在零shot分类和描述到图像领域中表现出了惊人的成绩，并在描述到图像和艺术作品之间的领域中表现出了可期的成绩。
</details></li>
</ul>
<hr>
<h2 id="FourierLoss-Shape-Aware-Loss-Function-with-Fourier-Descriptors"><a href="#FourierLoss-Shape-Aware-Loss-Function-with-Fourier-Descriptors" class="headerlink" title="FourierLoss: Shape-Aware Loss Function with Fourier Descriptors"></a>FourierLoss: Shape-Aware Loss Function with Fourier Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12106">http://arxiv.org/abs/2309.12106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehmet Bahadir Erden, Selahattin Cansiz, Onur Caki, Haya Khattak, Durmus Etiz, Melek Cosar Yakar, Kerem Duruer, Berke Barut, Cigdem Gunduz-Demir</li>
<li>for: 这个研究是为了提高医疗影像分类 задачі中的网络模型，以提高精确性。</li>
<li>methods: 这个研究使用了Encoder-decoder网络，并导入了一个新的形状意识损失函数（FourierLoss），以强制网络在训练过程中将物体的形状独特性考虑在内。</li>
<li>results: 实验结果显示，这个灵活的形状意识损失函数可以将医疗影像分类精确性提高，比其他方法更好。<details>
<summary>Abstract</summary>
Encoder-decoder networks become a popular choice for various medical image segmentation tasks. When they are trained with a standard loss function, these networks are not explicitly enforced to preserve the shape integrity of an object in an image. However, this ability of the network is important to obtain more accurate results, especially when there is a low-contrast difference between the object and its surroundings. In response to this issue, this work introduces a new shape-aware loss function, which we name FourierLoss. This loss function relies on quantifying the shape dissimilarity between the ground truth and the predicted segmentation maps through the Fourier descriptors calculated on their objects, and penalizing this dissimilarity in network training. Different than the previous studies, FourierLoss offers an adaptive loss function with trainable hyperparameters that control the importance of the level of the shape details that the network is enforced to learn in the training process. This control is achieved by the proposed adaptive loss update mechanism, which end-to-end learns the hyperparameters simultaneously with the network weights by backpropagation. As a result of using this mechanism, the network can dynamically change its attention from learning the general outline of an object to learning the details of its contour points, or vice versa, in different training epochs. Working on 2879 computed tomography images of 93 subjects, our experiments revealed that the proposed adaptive shape-aware loss function led to statistically significantly better results for liver segmentation, compared to its counterparts.
</details>
<details>
<summary>摘要</summary>
现代编码器-解码器网络在医疗图像分割任务中变得越来越受欢迎。当这些网络被标准损失函数训练时，它们不会显式地保持图像中对象的形状完整性。然而，这种网络的能力是获得更加准确的结果的关键，特别是在对象和周围环境之间存在低对比度的情况下。为解决这个问题，本研究提出了一种新的形状意识损失函数，我们称之为FourierLoss。这个损失函数基于计算对象的真实值和预测分割图像中对象的形状差异的福氏描述子，并对这种差异进行惩罚。与前一些研究不同，FourierLoss提供了一个可调参数的损失函数，通过反向传播来动态地更新参数。这种机制使得网络可以在训练过程中动态地变换注意力，从学习对象的大致轮廓到学习对象的细节点，或者vice versa。我们在2879个计算Tomography图像上进行了93个subject的实验，结果显示，提出的适应形状意识损失函数在肝 segmentation  task中比其他方法更为 statistically significantly better。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-sparsification-for-deep-neural-networks-with-Bayesian-model-reduction"><a href="#Bayesian-sparsification-for-deep-neural-networks-with-Bayesian-model-reduction" class="headerlink" title="Bayesian sparsification for deep neural networks with Bayesian model reduction"></a>Bayesian sparsification for deep neural networks with Bayesian model reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12095">http://arxiv.org/abs/2309.12095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrije Marković, Karl J. Friston, Stefan J. Kiebel</li>
<li>for: 本研究的目的是提出一种更高效的权重缩减方法，以优化深度学习模型的计算效率和性能。</li>
<li>methods: 本研究使用了权重缩减技术，包括权重缩减和模型减少。具体来说，研究人员使用了权重缩减的bayesian模型，并采用了黑盒Stochastic Variational Inference（SVI）算法来实现权重缩减。</li>
<li>results: 研究人员通过比较bayesian模型和SVI算法的计算效率和缩减率，发现bayesian模型的计算效率明显高于SVI算法，而且bayesian模型可以更好地缩减模型参数。此外，研究人员还通过应用 bayesian模型和SVI算法于不同的深度学习架构，包括LeNet、Vision Transformers和MLP-Mixers等，发现bayesian模型可以在这些架构上实现更高效的缩减。<details>
<summary>Abstract</summary>
Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimination of redundant model weights based on the posterior estimates under a straightforward (non-hierarchical) generative model. Our comparative study highlights the computational efficiency and the pruning rate of the BMR method relative to the established stochastic variational inference (SVI) scheme, when applied to the full hierarchical generative model. We illustrate the potential of BMR to prune model parameters across various deep learning architectures, from classical networks like LeNet to modern frameworks such as Vision Transformers and MLP-Mixers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Task-Cooperative-Learning-via-Searching-for-Flat-Minima"><a href="#Multi-Task-Cooperative-Learning-via-Searching-for-Flat-Minima" class="headerlink" title="Multi-Task Cooperative Learning via Searching for Flat Minima"></a>Multi-Task Cooperative Learning via Searching for Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12090">http://arxiv.org/abs/2309.12090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuping Wu, Le Zhang, Yang Sun, Yuanhan Mo, Thomas Nichols, Bartlomiej W. Papiez</li>
<li>for: 提高医疗图像分析的通用性和个别任务性能</li>
<li>methods: 提出了一种多任务学习的多级优化问题解决方案，使得特征从不同任务中学习共同逻辑</li>
<li>results: 在三个公开 dataset 上验证了方法的效果，比靶前方法更加有优势，表现出合作学习的优势<details>
<summary>Abstract</summary>
Multi-task learning (MTL) has shown great potential in medical image analysis, improving the generalizability of the learned features and the performance in individual tasks. However, most of the work on MTL focuses on either architecture design or gradient manipulation, while in both scenarios, features are learned in a competitive manner. In this work, we propose to formulate MTL as a multi/bi-level optimization problem, and therefore force features to learn from each task in a cooperative approach. Specifically, we update the sub-model for each task alternatively taking advantage of the learned sub-models of the other tasks. To alleviate the negative transfer problem during the optimization, we search for flat minima for the current objective function with regard to features from other tasks. To demonstrate the effectiveness of the proposed approach, we validate our method on three publicly available datasets. The proposed method shows the advantage of cooperative learning, and yields promising results when compared with the state-of-the-art MTL approaches. The code will be available online.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）在医疗图像分析中表现出了很大的潜力，提高了学习到的特征的通用性和个别任务的性能。然而，大多数MTL工作都集中在架构设计或梯度修正方面，在这两种情况下，特征是在竞争性下学习的。在这个工作中，我们提议将MTL形式为多/双级优化问题，因此让特征从每个任务中学习到的方式是协力的。specifically，我们在每个任务中更新子模型，利用其他任务的学习到的子模型。为了避免优化过程中的负转移问题，我们通过搜索当前目标函数中特征的平坦顶点来缓解负转移问题。为了证明提议的效果，我们在三个公共可用的数据集上验证了我们的方法。提议的方法表现出了协力学习的优势，并与状态的MTL方法进行比较而显示了承诺的结果。代码将在线上公开。
</details></li>
</ul>
<hr>
<h2 id="Self-Calibrating-Fully-Differentiable-NLOS-Inverse-Rendering"><a href="#Self-Calibrating-Fully-Differentiable-NLOS-Inverse-Rendering" class="headerlink" title="Self-Calibrating, Fully Differentiable NLOS Inverse Rendering"></a>Self-Calibrating, Fully Differentiable NLOS Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12047">http://arxiv.org/abs/2309.12047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiseok Choi, Inchul Kim, Dongyoung Choi, Julio Marco, Diego Gutierrez, Min H. Kim</li>
<li>For: The paper aims to improve the reconstruction of hidden scenes in non-line-of-sight (NLOS) imaging by introducing a fully-differentiable end-to-end pipeline that self-calibrates imaging parameters during the reconstruction process.* Methods: The paper uses a combination of diffraction-based volumetric NLOS reconstruction, path-space light transport, and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes. The pipeline is fully differentiable, allowing for gradient descent optimization of imaging parameters.* Results: The paper demonstrates the robustness of the method to consistently reconstruct geometry and albedo, even under significant noise levels. The end-to-end pipeline is able to self-calibrate imaging parameters and produce high-quality reconstructions without the need for manual selection of filtering functions or parameters.<details>
<summary>Abstract</summary>
Existing time-resolved non-line-of-sight (NLOS) imaging methods reconstruct hidden scenes by inverting the optical paths of indirect illumination measured at visible relay surfaces. These methods are prone to reconstruction artifacts due to inversion ambiguities and capture noise, which are typically mitigated through the manual selection of filtering functions and parameters. We introduce a fully-differentiable end-to-end NLOS inverse rendering pipeline that self-calibrates the imaging parameters during the reconstruction of hidden scenes, using as input only the measured illumination while working both in the time and frequency domains. Our pipeline extracts a geometric representation of the hidden scene from NLOS volumetric intensities and estimates the time-resolved illumination at the relay wall produced by such geometric information using differentiable transient rendering. We then use gradient descent to optimize imaging parameters by minimizing the error between our simulated time-resolved illumination and the measured illumination. Our end-to-end differentiable pipeline couples diffraction-based volumetric NLOS reconstruction with path-space light transport and a simple ray marching technique to extract detailed, dense sets of surface points and normals of hidden scenes. We demonstrate the robustness of our method to consistently reconstruct geometry and albedo, even under significant noise levels.
</details>
<details>
<summary>摘要</summary>
现有的非直视（NLOS）成像方法通过推算光路的媒体映射来重建隐藏的场景。这些方法容易受到重建残像的影响，这些残像通常通过手动选择筛选函数和参数来减轻。我们介绍了一个完全可导的终端到终点NLOS反向渲染管道，该管道在重建隐藏场景时自动调整成像参数，使用直接推算光路来估算隐藏场景中的光学信息，并使用梯度下降来优化成像参数。我们的管道使用干扰基于Diffraction的NLOS成像，并结合路径空间光传输和简单的RAY marching技术来提取隐藏场景的详细、稠密的表面点和法向量。我们示示了我们方法在噪音水平较高时仍能顺利重建场景的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Image-Borders-Learning-Feature-Extrapolation-for-Unbounded-Image-Composition"><a href="#Beyond-Image-Borders-Learning-Feature-Extrapolation-for-Unbounded-Image-Composition" class="headerlink" title="Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition"></a>Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12042">http://arxiv.org/abs/2309.12042</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuxiaoyu1104/unic">https://github.com/liuxiaoyu1104/unic</a></li>
<li>paper_authors: Xiaoyu Liu, Ming Liu, Junyi Li, Shuai Liu, Xiaotao Wang, Lei Lei, Wangmeng Zuo</li>
<li>for: 提高图像组合和美观品质，避免图像剪辑方法的局限性。</li>
<li>methods: 提出一种联合框架，包括图像预览帧为输入，并提供不受图像边界限制的视图调整建议，以及通过特征拟合扩展视场来提高视图调整预测精度。</li>
<li>results: 通过对 existed image cropping datasets 进行实验，证明了 UNIC 在无限制的视图调整和图像组合中的效果。源代码、数据集和预训练模型可以在 <a target="_blank" rel="noopener" href="https://github.com/liuxiaoyu1104/UNIC">https://github.com/liuxiaoyu1104/UNIC</a> 上获取。<details>
<summary>Abstract</summary>
For improving image composition and aesthetic quality, most existing methods modulate the captured images by striking out redundant content near the image borders. However, such image cropping methods are limited in the range of image views. Some methods have been suggested to extrapolate the images and predict cropping boxes from the extrapolated image. Nonetheless, the synthesized extrapolated regions may be included in the cropped image, making the image composition result not real and potentially with degraded image quality. In this paper, we circumvent this issue by presenting a joint framework for both unbounded recommendation of camera view and image composition (i.e., UNIC). In this way, the cropped image is a sub-image of the image acquired by the predicted camera view, and thus can be guaranteed to be real and consistent in image quality. Specifically, our framework takes the current camera preview frame as input and provides a recommendation for view adjustment, which contains operations unlimited by the image borders, such as zooming in or out and camera movement. To improve the prediction accuracy of view adjustment prediction, we further extend the field of view by feature extrapolation. After one or several times of view adjustments, our method converges and results in both a camera view and a bounding box showing the image composition recommendation. Extensive experiments are conducted on the datasets constructed upon existing image cropping datasets, showing the effectiveness of our UNIC in unbounded recommendation of camera view and image composition. The source code, dataset, and pretrained models is available at https://github.com/liuxiaoyu1104/UNIC.
</details>
<details>
<summary>摘要</summary>
For improving image composition and aesthetic quality, most existing methods delete unnecessary content near the image borders. However, such image cropping methods are limited in the range of image views. Some methods have been suggested to predict cropping boxes from the extrapolated image. However, the synthesized extrapolated regions may be included in the cropped image, making the image composition result not real and potentially with degraded image quality. In this paper, we overcome this issue by presenting a joint framework for both unbounded recommendation of camera view and image composition (i.e., UNIC). In this way, the cropped image is a sub-image of the image acquired by the predicted camera view, and thus can be guaranteed to be real and consistent in image quality. Specifically, our framework takes the current camera preview frame as input and provides a recommendation for view adjustment, which contains operations unlimited by the image borders, such as zooming in or out and camera movement. To improve the prediction accuracy of view adjustment prediction, we further extend the field of view by feature extrapolation. After one or several times of view adjustments, our method converges and results in both a camera view and a bounding box showing the image composition recommendation. Extensive experiments are conducted on the datasets constructed upon existing image cropping datasets, showing the effectiveness of our UNIC in unbounded recommendation of camera view and image composition. The source code, dataset, and pretrained models are available at https://github.com/liuxiaoyu1104/UNIC.
</details></li>
</ul>
<hr>
<h2 id="BASE-Probably-a-Better-Approach-to-Multi-Object-Tracking"><a href="#BASE-Probably-a-Better-Approach-to-Multi-Object-Tracking" class="headerlink" title="BASE: Probably a Better Approach to Multi-Object Tracking"></a>BASE: Probably a Better Approach to Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12035">http://arxiv.org/abs/2309.12035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Vonheim Larsen, Sigmund Rolfsjord, Daniel Gusland, Jörgen Ahlberg, Kim Mathiassen</li>
<li>for: The paper is written for the field of visual object tracking, specifically to address the lack of probabilistic methods in the leaderboards and to propose a set of pragmatic models to improve the performance of probabilistic trackers.</li>
<li>methods: The paper proposes a new probabilistic tracking algorithm called BASE (Bayesian Approximation Single-hypothesis Estimator), which addresses the challenges of distance in target kinematics, detector confidence, and non-uniform clutter characteristics.</li>
<li>results: The paper achieves state-of-the-art (SOTA) performance on MOT17 and MOT20 without using Re-Id, demonstrating the effectiveness of the proposed approach.Here is the information in Simplified Chinese text:</li>
<li>for: 本文为视觉对象跟踪领域的研究，旨在解决现有的概率方法在领导人员中的缺失，并提出一些实用的模型来提高概率跟踪器的性能。</li>
<li>methods: 本文提出了一种新的概率跟踪算法 called BASE ( bayesian Approximation Single-hypothesis Estimator)，该算法 Addresses 目标动态距离、检测器信任度和非uniform的干扰特征等挑战。</li>
<li>results: 本文在 MOT17 和 MOT20 上达到了 state-of-the-art 性能，不使用 Re-Id，demonstrating 提出的方法的有效性。<details>
<summary>Abstract</summary>
The field of visual object tracking is dominated by methods that combine simple tracking algorithms and ad hoc schemes. Probabilistic tracking algorithms, which are leading in other fields, are surprisingly absent from the leaderboards. We found that accounting for distance in target kinematics, exploiting detector confidence and modelling non-uniform clutter characteristics is critical for a probabilistic tracker to work in visual tracking. Previous probabilistic methods fail to address most or all these aspects, which we believe is why they fall so far behind current state-of-the-art (SOTA) methods (there are no probabilistic trackers in the MOT17 top 100). To rekindle progress among probabilistic approaches, we propose a set of pragmatic models addressing these challenges, and demonstrate how they can be incorporated into a probabilistic framework. We present BASE (Bayesian Approximation Single-hypothesis Estimator), a simple, performant and easily extendible visual tracker, achieving state-of-the-art (SOTA) on MOT17 and MOT20, without using Re-Id. Code will be made available at https://github.com/ffi-no
</details>
<details>
<summary>摘要</summary>
“Visual object tracking 领域由简单追踪算法和对应措施组合所控制。 probabilistic 追踪算法，在其他领域中是领先的，在 visual tracking 中却缺乏表现。我们发现，在目标运动中考虑距离、利用探测器信任度和非均匀杂质特征是critical的。 previous probabilistic methods 无法解决这些问题，我们认为这就是为什么它们落后现有的state-of-the-art（SOTA）方法（MOT17 top 100 中没有 probabilistic 追踪器）。为了推动 probabilistic 方法的进步，我们提出了一些实用的模型，并说明如何将它们集成到 probabilistic 框架中。我们提出了 BASE（Bayesian Approximation Single-hypothesis Estimator），一个简单、高效和易扩展的visual 追踪器，在 MOT17 和 MOT20 中获得了state-of-the-art 成绩，无需使用 Re-Id。我们将在 GitHub 上公开代码。”
</details></li>
</ul>
<hr>
<h2 id="Face-Identity-Aware-Disentanglement-in-StyleGAN"><a href="#Face-Identity-Aware-Disentanglement-in-StyleGAN" class="headerlink" title="Face Identity-Aware Disentanglement in StyleGAN"></a>Face Identity-Aware Disentanglement in StyleGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12033">http://arxiv.org/abs/2309.12033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Suwała, Bartosz Wójcik, Magdalena Proszewska, Jacek Tabor, Przemysław Spurek, Marek Śmieja</li>
<li>for: 本文主要用于解决现有 Conditional GANs 模型中的一个问题，即同时修改图像中的一些特征，而不是只修改请求的特征。</li>
<li>methods: 本文提出了一种名为 PluGeN4Faces 的插件，用于修改 face 图像中的特征，同时保持图像的人脸特征不变。该方法通过在 Movie Frames 中提取图像，并使用一种类型的对比损失函数，来让模型将同一个人的图像分组在 latent 空间中相似的地方。</li>
<li>results: 实验表明，PluGeN4Faces 比现有状态的 искус智模型更能减少修改 face 特征所带来的影响。<details>
<summary>Abstract</summary>
Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> conditional GANs frequently used manipulating face image attributes, such as expression, hairstyle, pose, or age. although state-of-the-art models successfully modify requested attributes, simultaneously modify important image characteristics, such as person's identity. in this paper, focus on solving problem by introducing PluGeN4Faces, StyleGAN plugin, explicitly disentangles face attributes from person's identity. our key idea perform training images retrieved movie frames, given person appears various poses different attributes. applying type contrastive loss, encourage model group images same person similar regions latent space. our experiments demonstrate modifications face attributes performed PluGeN4Faces significantly less invasive remaining image characteristics than existing state-of-the-art models.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Hidden-Realm-Self-supervised-Skeleton-based-Action-Recognition-in-Occluded-Environments"><a href="#Unveiling-the-Hidden-Realm-Self-supervised-Skeleton-based-Action-Recognition-in-Occluded-Environments" class="headerlink" title="Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments"></a>Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12029">http://arxiv.org/abs/2309.12029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyfml/opstl">https://github.com/cyfml/opstl</a></li>
<li>paper_authors: Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 提高自主 robotic 系统中的动作识别率，考虑到目标 occlusion 的情况。</li>
<li>methods: 提议使用 occluded skeleton 序列 pré-train，然后使用 k-means 聚类（KMeans）对序列嵌入进行分组，并使用 K-nearest-neighbor（KNN）填充 missing skeleton 数据。</li>
<li>results: 对 NTURGB+D 60 和 NTURGB+D 120 的 occluded 版本进行验证，证明了我们的填充方法的效iveness。<details>
<summary>Abstract</summary>
To integrate action recognition methods into autonomous robotic systems, it is crucial to consider adverse situations involving target occlusions. Such a scenario, despite its practical relevance, is rarely addressed in existing self-supervised skeleton-based action recognition methods. To empower robots with the capacity to address occlusion, we propose a simple and effective method. We first pre-train using occluded skeleton sequences, then use k-means clustering (KMeans) on sequence embeddings to group semantically similar samples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeleton data based on the closest sample neighbors. Imputing incomplete skeleton sequences to create relatively complete sequences as input provides significant benefits to existing skeleton-based self-supervised models. Meanwhile, building on the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introduce an Occluded Partial Spatio-Temporal Learning (OPSTL) framework. This enhancement utilizes Adaptive Spatial Masking (ASM) for better use of high-quality, intact skeletons. The effectiveness of our imputation methods is verified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D 120. The source code will be made publicly available at https://github.com/cyfml/OPSTL.
</details>
<details>
<summary>摘要</summary>
要将动作识别方法 integrate 到自主 роботи系统中，需要考虑目标 occlusion 的情况。这种情况尚未在现有的自助学习骨架基于动作识别方法中得到充分考虑。为了赋给机器人更多的能力，我们提出了一种简单有效的方法。我们首先使用 occluded 骨架序列进行预训练，然后使用 K-means 聚类（KMeans）对序列嵌入进行分组。接着，我们使用 K-nearest-neighbor（KNN）来填充 missing 骨架数据，基于最近的样本 neighors。填充不完整的骨架序列，以创建相对完整的输入，对现有骨架基于自助学习模型具有重要的优化。此外，我们在 Partial Spatio-Temporal Learning（PSTL）的基础之上，引入 Occluded Partial Spatio-Temporal Learning（OPSTL）框架。这种改进使用 Adaptive Spatial Masking（ASM）来更好地利用高质量、完整的骨架。我们的填充方法的效果被证明在NTURGB+D 60 和 NTURGB+D 120 的 occluded 版本上。源代码将在 GitHub 上公开，可以通过 https://github.com/cyfml/OPSTL 获取。
</details></li>
</ul>
<hr>
<h2 id="Precision-in-Building-Extraction-Comparing-Shallow-and-Deep-Models-using-LiDAR-Data"><a href="#Precision-in-Building-Extraction-Comparing-Shallow-and-Deep-Models-using-LiDAR-Data" class="headerlink" title="Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data"></a>Precision in Building Extraction: Comparing Shallow and Deep Models using LiDAR Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12027">http://arxiv.org/abs/2309.12027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sulaiman, Mina Farmanbar, Ahmed Nabil Belbachir, Chunming Rong</li>
<li>For: 本文使用 LiDAR 数据进行检测建筑物的深度学习模型，以提高建筑物的分割精度。* Methods: 本文使用了 shallow models，并使用了边界面掩模来提高 BIoU 分数。* Results:  shallow models 在 IoU 分数上出perform deep learning models 8%，但是 deep learning models 在 BIoU 分数上表现更好。边界面掩模可以提高 BIoU 分数4%。 LightGBM 表现比 RF 和 XGBoost 更好。<details>
<summary>Abstract</summary>
Building segmentation is essential in infrastructure development, population management, and geological observations. This article targets shallow models due to their interpretable nature to assess the presence of LiDAR data for supervised segmentation. The benchmark data used in this article are published in NORA MapAI competition for deep learning model. Shallow models are compared with deep learning models based on Intersection over Union (IoU) and Boundary Intersection over Union (BIoU). In the proposed work, boundary masks from the original mask are generated to improve the BIoU score, which relates to building shapes' borderline. The influence of LiDAR data is tested by training the model with only aerial images in task 1 and a combination of aerial and LiDAR data in task 2 and then compared. shallow models outperform deep learning models in IoU by 8% using aerial images (task 1) only and 2% in combined aerial images and LiDAR data (task 2). In contrast, deep learning models show better performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and Extreme Gradient Boosting (XGBoost).
</details>
<details>
<summary>摘要</summary>
In the proposed work, boundary masks are generated from the original mask to improve the BIoU score, which relates to building shapes' borderlines. The influence of LiDAR data is tested by training the model with only aerial images in Task 1 and a combination of aerial and LiDAR data in Task 2, and then comparing the results.Shallow models outperform deep learning models in IoU by 8% using aerial images (Task 1) only and 2% in combined aerial images and LiDAR data (Task 2). In contrast, deep learning models show better performance on BIoU scores. Boundary masks improve BIoU scores by 4% in both tasks. Light Gradient-Boosting Machine (LightGBM) performs better than RF and Extreme Gradient Boosting (XGBoost).
</details></li>
</ul>
<hr>
<h2 id="Convolution-and-Attention-Mixer-for-Synthetic-Aperture-Radar-Image-Change-Detection"><a href="#Convolution-and-Attention-Mixer-for-Synthetic-Aperture-Radar-Image-Change-Detection" class="headerlink" title="Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection"></a>Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12010">http://arxiv.org/abs/2309.12010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/summitgao/camixer">https://github.com/summitgao/camixer</a></li>
<li>paper_authors: Haopeng Zhang, Zijing Lin, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li</li>
<li>for:  This paper focuses on improving the performance of synthetic aperture radar (SAR) change detection by incorporating global attention mechanism into Transformer-like architecture.</li>
<li>methods:  The proposed method, called Convolution and Attention Mixer (CAMixer), combines self-attention with shift convolution in a parallel way, and adopts a gating mechanism in the feed-forward network to enhance the non-linear feature transformation.</li>
<li>results:  The proposed CAMixer achieves superior performance in SAR change detection compared to existing CNN-based methods, as demonstrated by extensive experiments conducted on three SAR datasets.<details>
<summary>Abstract</summary>
Synthetic aperture radar (SAR) image change detection is a critical task and has received increasing attentions in the remote sensing community. However, existing SAR change detection methods are mainly based on convolutional neural networks (CNNs), with limited consideration of global attention mechanism. In this letter, we explore Transformer-like architecture for SAR change detection to incorporate global attention. To this end, we propose a convolution and attention mixer (CAMixer). First, to compensate the inductive bias for Transformer, we combine self-attention with shift convolution in a parallel way. The parallel design effectively captures the global semantic information via the self-attention and performs local feature extraction through shift convolution simultaneously. Second, we adopt a gating mechanism in the feed-forward network to enhance the non-linear feature transformation. The gating mechanism is formulated as the element-wise multiplication of two parallel linear layers. Important features can be highlighted, leading to high-quality representations against speckle noise. Extensive experiments conducted on three SAR datasets verify the superior performance of the proposed CAMixer. The source codes will be publicly available at https://github.com/summitgao/CAMixer .
</details>
<details>
<summary>摘要</summary>
“干扰天线射频图像变化检测（SAR）是远感社区中的一个重要任务，但现有的SAR变化检测方法主要基于卷积神经网络（CNN），对于全球注意机制的考虑有限。在本封信中，我们探索了Transformer-like架构来进行SAR变化检测，以内置全球注意机制。为此，我们提出了一个混合卷积和注意混合器（CAMixer）。首先，为了补偿对Transformer的传播偏见，我们在平行的方式结合了自我注意和偏移核函数。这样的平行设计可以同时捕捉全球semantic信息和本地特征特性，从而实现高质量的特征抽象。其次，我们在对待网络中引入了阈值机制，以增强非线性特征转换。这个阈值机制是通过两个平行的线性层进行元素ごとの多项式乘法。重要的特征可以得到高质量的表现，抵制杂音。实验结果显示，我们的CAMixer具有较高的检测性和稳定性，并且可以实现高质量的特征抽象。我们将代码公开于https://github.com/summitgao/CAMixer。”
</details></li>
</ul>
<hr>
<h2 id="Elevating-Skeleton-Based-Action-Recognition-with-Efficient-Multi-Modality-Self-Supervision"><a href="#Elevating-Skeleton-Based-Action-Recognition-with-Efficient-Multi-Modality-Self-Supervision" class="headerlink" title="Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision"></a>Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12009">http://arxiv.org/abs/2309.12009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 本研究旨在提高人体动作识别的自助学习表示。</li>
<li>methods: 我们首先提出了一种偏知交换模块（IKEM），以避免低性能Modalities之间的偏知传递。然后，我们提出了三种新的Modalities，以增强多modalities之间的补充信息。最后，我们提出了一种新的教师生框架，以在引入新Modalities时保持效率，并将第二 modalities中的知识透传到必要modalities中，基于约束anchors, positives和negatives的关系。</li>
<li>results: 实验结果表明，我们的方法有效地提高了skeleton基于多modalities数据的人体动作识别性能。<details>
<summary>Abstract</summary>
Self-supervised representation learning for human action recognition has developed rapidly in recent years. Most of the existing works are based on skeleton data while using a multi-modality setup. These works overlooked the differences in performance among modalities, which led to the propagation of erroneous knowledge between modalities while only three fundamental modalities, i.e., joints, bones, and motions are used, hence no additional modalities are explored.   In this work, we first propose an Implicit Knowledge Exchange Module (IKEM) which alleviates the propagation of erroneous knowledge between low-performance modalities. Then, we further propose three new modalities to enrich the complementary information between modalities. Finally, to maintain efficiency when introducing new modalities, we propose a novel teacher-student framework to distill the knowledge from the secondary modalities into the mandatory modalities considering the relationship constrained by anchors, positives, and negatives, named relational cross-modality knowledge distillation. The experimental results demonstrate the effectiveness of our approach, unlocking the efficient use of skeleton-based multi-modality data. Source code will be made publicly available at https://github.com/desehuileng0o0/IKEM.
</details>
<details>
<summary>摘要</summary>
自我监睹表示学习人体动作识别在最近几年内得到了迅速发展。大多数现有工作基于骨骼数据，使用多模态设置。这些工作忽视了不同模态之间的性能差异，导致错误知识的传播 между模态，只有三种基本模态，即关节、骨骼和运动，因此没有探索其他模态。  在这项工作中，我们首先提出了隐式知识交换模块（IKEM），以消除低性能模态之间的错误知识传播。然后，我们进一步提出了三种新的模态，以增加多模态之间的补充信息。最后，为保持效率而不是引入新模态，我们提出了一种新的教师-学生框架，通过约束anchors、正例和负例之间的关系，将次要模态中的知识透传到必要模态中，称为关系跨模态知识采样。实验结果表明我们的方法的效果，使得骨骼基于多模态数据的高效使用成为可能。源代码将在https://github.com/desehuileng0o0/IKEM公开。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-pneumonia-on-chest-x-ray-images-through-machine-learning"><a href="#Identification-of-pneumonia-on-chest-x-ray-images-through-machine-learning" class="headerlink" title="Identification of pneumonia on chest x-ray images through machine learning"></a>Identification of pneumonia on chest x-ray images through machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11995">http://arxiv.org/abs/2309.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Nabeel-105/Covid-19-and-Pneumonia-Detection-Using-Chest-Xray-Images-Full-Desktop-Application-">https://github.com/Nabeel-105/Covid-19-and-Pneumonia-Detection-Using-Chest-Xray-Images-Full-Desktop-Application-</a></li>
<li>paper_authors: Eduardo Augusto Roeder</li>
<li>for: 这个研究的目的是开发一种用于识别胸部X光图像中的肺炎病例的软件。</li>
<li>methods: 这个研究使用了机器学习技术，特别是传输学习技术，并使用了一个计算模型来训练。</li>
<li>results: 经过训练后，模型可以准确地识别胸部X光图像中的肺炎病例，达到了98%的敏感性和97.3%的特异性。<details>
<summary>Abstract</summary>
Pneumonia is the leading infectious cause of infant death in the world. When identified early, it is possible to alter the prognosis of the patient, one could use imaging exams to help in the diagnostic confirmation. Performing and interpreting the exams as soon as possible is vital for a good treatment, with the most common exam for this pathology being chest X-ray. The objective of this study was to develop a software that identify the presence or absence of pneumonia in chest radiographs. The software was developed as a computational model based on machine learning using transfer learning technique. For the training process, images were collected from a database available online with children's chest X-rays images taken at a hospital in China. After training, the model was then exposed to new images, achieving relevant results on identifying such pathology, reaching 98% sensitivity and 97.3% specificity for the sample used for testing. It can be concluded that it is possible to develop a software that identifies pneumonia in chest X-ray images.
</details>
<details>
<summary>摘要</summary>
全球最主要的感染性新生儿死亡原因是肺炎，早期诊断可以改善病人的结局。使用影像检查可以帮助诊断，其中最常用的检查是胸部X射线。本研究的目标是开发一种可以在胸部X射线图像中识别肺炎的软件。该软件是基于机器学习技术的计算模型，使用了传输学习技术进行训练。训练过程中，图像来自中国医院的儿童胸部X射线图像库。经训练后，模型被推出到新图像上，实现了识别肺炎的相关结果，具有98%的敏感度和97.3%的特异性。可以确定，可以开发一种识别肺炎在胸部X射线图像中的软件。
</details></li>
</ul>
<hr>
<h2 id="Neural-Stochastic-Screened-Poisson-Reconstruction"><a href="#Neural-Stochastic-Screened-Poisson-Reconstruction" class="headerlink" title="Neural Stochastic Screened Poisson Reconstruction"></a>Neural Stochastic Screened Poisson Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11993">http://arxiv.org/abs/2309.11993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silvia Sellán, Alec Jacobson</li>
<li>for:  reconstruction of a surface from a point cloud</li>
<li>methods:  neural network with Poisson smoothness prior</li>
<li>results:  addresses limitations of existing work and can be fully integrated into the 3D scanning pipeline<details>
<summary>Abstract</summary>
Reconstructing a surface from a point cloud is an underdetermined problem. We use a neural network to study and quantify this reconstruction uncertainty under a Poisson smoothness prior. Our algorithm addresses the main limitations of existing work and can be fully integrated into the 3D scanning pipeline, from obtaining an initial reconstruction to deciding on the next best sensor position and updating the reconstruction upon capturing more data.
</details>
<details>
<summary>摘要</summary>
重建表面从点云是一个不充分定义的问题。我们使用神经网络来研究和评估这种重建不确定性，采用波尼尔平滑性先验来做估计。我们的算法解决了现有工作的主要局限性，可以全面地整合到3D扫描管道中，从获取初始重建到决定下一个感知器位置并更新重建。
</details></li>
</ul>
<hr>
<h2 id="Crop-Row-Switching-for-Vision-Based-Navigation-A-Comprehensive-Approach-for-Efficient-Crop-Field-Navigation"><a href="#Crop-Row-Switching-for-Vision-Based-Navigation-A-Comprehensive-Approach-for-Efficient-Crop-Field-Navigation" class="headerlink" title="Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation"></a>Crop Row Switching for Vision-Based Navigation: A Comprehensive Approach for Efficient Crop Field Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11989">http://arxiv.org/abs/2309.11989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajitha de Silva, Grzegorz Cielniak, Junfeng Gao</li>
<li>for: 这个论文旨在提出一种基于视觉的移动机器人 navigate 方法，可以在耕地中横跨多个行。</li>
<li>methods: 该方法使用单个前置摄像头和深度数据进行RGB图像分割，以探测行末和下一行的重新入口点。</li>
<li>results: 在实际的甘蔗ibe字段中，该方法能够成功地使移动机器人从一行到下一行 navigate， WITH median errors 为19.25 cm和6.77°。<details>
<summary>Abstract</summary>
Vision-based mobile robot navigation systems in arable fields are mostly limited to in-row navigation. The process of switching from one crop row to the next in such systems is often aided by GNSS sensors or multiple camera setups. This paper presents a novel vision-based crop row-switching algorithm that enables a mobile robot to navigate an entire field of arable crops using a single front-mounted camera. The proposed row-switching manoeuvre uses deep learning-based RGB image segmentation and depth data to detect the end of the crop row, and re-entry point to the next crop row which would be used in a multi-state row switching pipeline. Each state of this pipeline use visual feedback or wheel odometry of the robot to successfully navigate towards the next crop row. The proposed crop row navigation pipeline was tested in a real sugar beet field containing crop rows with discontinuities, varying light levels, shadows and irregular headland surfaces. The robot could successfully exit from one crop row and re-enter the next crop row using the proposed pipeline with absolute median errors averaging at 19.25 cm and 6.77{\deg} for linear and rotational steps of the proposed manoeuvre.
</details>
<details>
<summary>摘要</summary>
视觉基于移动机器人Navigation系统通常仅限于行间导航。在这些系统中，从一行农作物到下一行的过程经常受GNSS传感器或多个摄像头的帮助。本文介绍了一种新的视觉基于的农作物行转换算法，使得移动机器人可以使用单个前置摄像头探测整个农作物场。提议的行转换举动使用深度学习基于RGB图像分割和深度数据探测农作物行的结束和下一行的重新入口点，并在多个状态的管道中使用视觉反馈或机器人轮胎的运动来成功导航到下一行农作物。这个管道在实际的甘蔗ibeet场中进行测试，包括具有不连续的农作物行、不同的照明水平、阴影和不规则的机器人进场面。机器人使用提议的管道成功地离开了一行农作物并重新进入下一行农作物， median误差平均值为19.25cm和6.77度 для直线和旋转步骤。
</details></li>
</ul>
<hr>
<h2 id="ZS6D-Zero-shot-6D-Object-Pose-Estimation-using-Vision-Transformers"><a href="#ZS6D-Zero-shot-6D-Object-Pose-Estimation-using-Vision-Transformers" class="headerlink" title="ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers"></a>ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11986">http://arxiv.org/abs/2309.11986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Ausserlechner, David Haberger, Stefan Thalhammer, Jean-Baptiste Weibel, Markus Vincze</li>
<li>for: recognize diverse objects in complex and unconstrained real-world scenarios</li>
<li>methods: use pre-trained Vision Transformers (ViT) to extract visual descriptors, and RANSAC-based PnP to estimate object’s 6D pose</li>
<li>results: improve the Average Recall on three datasets (LMO, YCBV, and TLESS) compared to two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning.<details>
<summary>Abstract</summary>
As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object's 6D pose with RANSAC-based PnP. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to one of the two methods we improve the Average Recall on all three datasets and compared to the second method we improve on two datasets.
</details>
<details>
<summary>摘要</summary>
As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a growing need to recognize diverse objects. However, current state-of-the-art 6D object pose estimation methods rely on object-specific training and do not generalize well to unseen objects. To address this issue, recent novel object pose estimation methods have used task-specific fine-tuned convolutional neural networks (CNNs) for deep template matching. However, this approach still requires expensive data rendering and training procedures.To overcome this limitation, we propose a novel zero-shot method for 6D object pose estimation, called ZS6D. Our approach uses visual descriptors extracted using pre-trained Vision Transformers (ViT) to match rendered templates against query images of objects, and establish local correspondences. These local correspondences are then used to estimate the object's 6D pose using RANSAC-based Perspective-n-Point (PnP).Experiments on three datasets (LMO, YCBV, and TLESS) show that our approach achieves a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Specifically, we improve the Average Recall on all three datasets compared to one of the two methods, and improve on two datasets compared to the second method.Here is the translation in Simplified Chinese:随着机器人系统遇到越来越复杂的实际场景，需要认izers多种物体。现状下的6D物体pose估计方法都是基于物体特定的训练，不能泛化到未看过的物体。为了解决这个问题，最新的novel object pose estimation方法都是使用任务特定的深度学习模型进行深度模板匹配。但是，这种方法仍需要费时的数据渲染和训练过程。为了突破这个局限性，我们提出了一种 zeroshot的6D物体pose估计方法，即ZS6D。我们的方法使用预训练的Vision Transformer（ViT）提取的视觉描述符来匹配渲染的模板和查询图像，并建立地方匹配。这些地方匹配然后用RANSAC基于Perspective-n-Point（PnP）来估计物体的6Dpose。在LMO、YCBV和TLESS三个dataset上进行了实验，我们发现我们的方法可以不需要任务特定的微调，就可以在这三个dataset上达到较好的性能。具体来说，我们在这三个dataset上的平均回归率都高于一个方法，并在两个dataset上高于另一个方法。
</details></li>
</ul>
<hr>
<h2 id="NeuralLabeling-A-versatile-toolset-for-labeling-vision-datasets-using-Neural-Radiance-Fields"><a href="#NeuralLabeling-A-versatile-toolset-for-labeling-vision-datasets-using-Neural-Radiance-Fields" class="headerlink" title="NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields"></a>NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11966">http://arxiv.org/abs/2309.11966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floris Erich, Naoya Chiba, Yusuke Yoshiyasu, Noriaki Ando, Ryo Hanai, Yukiyasu Domae</li>
<li>for: 本研究旨在提出一种基于Neural Radiance Fields（NeRF）的图像标注方法和工具集，用于生成分割图、可用性图、2D bounding box、3D bounding box、6DOF对象位置、深度图和物体mesh等。</li>
<li>methods: 本研究使用NeRF作为渲染器，通过使用多视点图像输入和3D空间工具进行标注，并利用图像内容和几何特征（如 occlusion）来提高标注精度。</li>
<li>results: 在对30000帧透明物体RGB和噪音深度图数据集进行训练后，使用 annotated depth maps 进行监督训练的深度神经网络实现了更高的重建性能，比之前使用弱监督法则更高。<details>
<summary>Abstract</summary>
We present NeuralLabeling, a labeling approach and toolset for annotating a scene using either bounding boxes or meshes and generating segmentation masks, affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as renderer, allowing labeling to be performed using 3D spatial tools while incorporating geometric clues such as occlusions, relying only on images captured from multiple viewpoints as input. To demonstrate the applicability of NeuralLabeling to a practical problem in robotics, we added ground truth depth maps to 30000 frames of transparent object RGB and noisy depth maps of glasses placed in a dishwasher captured using an RGBD sensor, yielding the Dishwasher30k dataset. We show that training a simple deep neural network with supervision using the annotated depth maps yields a higher reconstruction performance than training with the previously applied weakly supervised approach.
</details>
<details>
<summary>摘要</summary>
我们提出了NeuralLabeling，一种Scene Labeling的方法和工具集，可以使用矩形框或网格来标识场景，并生成分类图、可用性图、2D矩形框、3D矩形框、6DOF物体位置、深度图和物体网格。NeuralLabeling使用Neural Radiance Fields（NeRF）作为渲染器，允许使用3D空间工具进行标识，同时考虑到隐藏和 occlusion 的几何假设，仅基于多个视角的图像作为输入。为了评估NeuralLabeling在 robotics 中的实用性，我们将添加了透明物体RGB和杂音深度图档案，创建了Dishwasher30k数据集。我们显示了，对于训练一个简单的深度神经网络，使用这些标识的深度图作为超级训练可以获得更高的重建性能，比过去的弱种超级训练方法。
</details></li>
</ul>
<hr>
<h2 id="Ego3DPose-Capturing-3D-Cues-from-Binocular-Egocentric-Views"><a href="#Ego3DPose-Capturing-3D-Cues-from-Binocular-Egocentric-Views" class="headerlink" title="Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views"></a>Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11962">http://arxiv.org/abs/2309.11962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeho Kang, Kyungjin Lee, Jinrui Zhang, Youngki Lee</li>
<li>for:  Egocentric 3D pose reconstruction system</li>
<li>methods:  Two novel approaches: 1) two-path network architecture with independent pose estimation for each limb, and 2) perspective-aware representation using trigonometry to estimate 3D orientation of limbs.</li>
<li>results:  Outperforms state-of-the-art models by 23.1% in MPJPE reduction in the UnrealEgo dataset, with superior performance across a range of scenarios and challenges.<details>
<summary>Abstract</summary>
We present Ego3DPose, a highly accurate binocular egocentric 3D pose reconstruction system. The binocular egocentric setup offers practicality and usefulness in various applications, however, it remains largely under-explored. It has been suffering from low pose estimation accuracy due to viewing distortion, severe self-occlusion, and limited field-of-view of the joints in egocentric 2D images. Here, we notice that two important 3D cues, stereo correspondences, and perspective, contained in the egocentric binocular input are neglected. Current methods heavily rely on 2D image features, implicitly learning 3D information, which introduces biases towards commonly observed motions and leads to low overall accuracy. We observe that they not only fail in challenging occlusion cases but also in estimating visible joint positions. To address these challenges, we propose two novel approaches. First, we design a two-path network architecture with a path that estimates pose per limb independently with its binocular heatmaps. Without full-body information provided, it alleviates bias toward trained full-body distribution. Second, we leverage the egocentric view of body limbs, which exhibits strong perspective variance (e.g., a significantly large-size hand when it is close to the camera). We propose a new perspective-aware representation using trigonometry, enabling the network to estimate the 3D orientation of limbs. Finally, we develop an end-to-end pose reconstruction network that synergizes both techniques. Our comprehensive evaluations demonstrate that Ego3DPose outperforms state-of-the-art models by a pose estimation error (i.e., MPJPE) reduction of 23.1% in the UnrealEgo dataset. Our qualitative results highlight the superiority of our approach across a range of scenarios and challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍Ego3DPose，一种高度准确的双目 egocentric 3D姿态重建系统。双目 egocentric 设置提供了实用性和有用性，但它尚未得到充分探索。它因视图扭曲、严重的自遮掩和 Egocentric 2D 图像中关节的视场有限而受到低姿态估计精度的影响。我们注意到，在 egocentric 双目输入中含有两种重要的3D准确度信息：立体匹配和投影。现有方法强调2D图像特征，潜在地学习3D信息，导致对常见动作的偏好和全局精度低下。我们发现它们不仅在困难的遮掩情况下失败，而且在可见关节位置的估计也失败。为解决这些挑战，我们提出了两个新的方法。首先，我们设计了一种两路网络架构，其中一路用于独立地估计每个肢体的姿态，使用双目热图。无需全身信息提供，这种方法减少了对训练全身份布的偏好。其次，我们利用 egocentric 视角中的身体部分，其中具有强大的投影变化（例如，相对较大的手在相机较近时）。我们提出了一种新的投影意识表示，使得网络能够估计肢体的3D方向。最后，我们开发了一个综合的端到端姿态重建网络，将两种技术相结合。我们对 UnrealEgo 数据集进行了广泛的评估，并证明Ego3DPose 相比 estado-of-the-art 模型，MPJPE 估计误差降低23.1%。我们的质量结果表明我们的方法在各种情况和挑战中具有优势。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Forward-Forward-Algorithm-for-Self-Supervised-Learning"><a href="#A-Study-of-Forward-Forward-Algorithm-for-Self-Supervised-Learning" class="headerlink" title="A Study of Forward-Forward Algorithm for Self-Supervised Learning"></a>A Study of Forward-Forward Algorithm for Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11955">http://arxiv.org/abs/2309.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Brenig, Radu Timofte</li>
<li>for: 本研究是 investigate the performance of forward-forward algorithm for self-supervised representation learning, and provide insights into the learned representation spaces.</li>
<li>methods: 本研究使用了 four standard datasets (MNIST, F-MNIST, SVHN, CIFAR-10) and three commonly used self-supervised representation learning techniques (rotation, flip, jigsaw) to compare the performance of forward-forward algorithm and backpropagation.</li>
<li>results: 研究发现，forward-forward algorithm在(self-)supervised training中与backpropagation相当，但在所有 studied settings中的转移性能却明显落后。这可能由多种因素引起，包括每层有独立损失函数和forward-forward paradigm中的自tek抽象学习方法。相比backpropagation，forward-forward algorithm更注重边界和抛弃一些无用于做出决策的信息，这会妨碍自然语言处理的表征学习目标。<details>
<summary>Abstract</summary>
Self-supervised representation learning has seen remarkable progress in the last few years, with some of the recent methods being able to learn useful image representations without labels. These methods are trained using backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the forward-forward algorithm as an alternative training method. It utilizes two forward passes and a separate loss function for each layer to train the network without backpropagation.   In this study, for the first time, we study the performance of forward-forward vs. backpropagation for self-supervised representation learning and provide insights into the learned representation spaces. Our benchmark employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and three commonly used self-supervised representation learning techniques, namely rotation, flip and jigsaw.   Our main finding is that while the forward-forward algorithm performs comparably to backpropagation during (self-)supervised training, the transfer performance is significantly lagging behind in all the studied settings. This may be caused by a combination of factors, including having a loss function for each layer and the way the supervised training is realized in the forward-forward paradigm. In comparison to backpropagation, the forward-forward algorithm focuses more on the boundaries and drops part of the information unnecessary for making decisions which harms the representation learning goal. Further investigation and research are necessary to stabilize the forward-forward strategy for self-supervised learning, to work beyond the datasets and configurations demonstrated by Geoffrey Hinton.
</details>
<details>
<summary>摘要</summary>
自我监督学习在最近几年内取得了非常出色的进步，一些最新的方法可以在无标签情况下学习有用的图像表示。这些方法通过反向传播来进行训练，反向传播是现今标准的训练方法。在这一研究中，我们首次比较了前向前法和反向传播两种训练方法的性能，并对学习的表示空间提供了深入的探讨。我们的标准测试集包括MNIST、F-MNIST、SVHN和CIFAR-10等四个数据集，以及rotation、flip和jigsaw等三种常用的自我监督表示学习技术。我们的主要发现是，虽然前向前法和反向传播在自我监督训练中表现相似，但在所有研究情况下，转移性能明显落后。这可能是由多种因素共同影响的，包括每层有自己的损失函数以及在前向前法中实现自我监督训练的方式。相比反向传播，前向前法更注重边缘和抛弃一些无用于做出决定的信息，这对图像表示学习的目标产生了负面影响。进一步的研究和调查是必要的，以稳定前向前法在自我监督学习中的应用，并在不同的数据集和配置下进行更广泛的探索。
</details></li>
</ul>
<hr>
<h2 id="Fully-Transformer-Equipped-Architecture-for-End-to-End-Referring-Video-Object-Segmentation"><a href="#Fully-Transformer-Equipped-Architecture-for-End-to-End-Referring-Video-Object-Segmentation" class="headerlink" title="Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation"></a>Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11933">http://arxiv.org/abs/2309.11933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Li, Yu Zhang, Li Yuan, Xianghua Xu</li>
<li>for: 这篇论文是为了解决视频对话引用（RVOS）问题，即根据自然语言查询语义地 segment 视频中的对象。</li>
<li>methods: 该论文提出了一种基于 transformer 完全架构（FTEA）的解决方案，即将 RVOS 任务看作是一个 mask 序列学习问题，对所有视频中的对象进行候选对象的搜索和学习。</li>
<li>results: 实验结果表明，该方法在三个 benchmark 上表现出色，例如，在 A2D Sentences 和 J-HMDB Sentences 上达到了 45.1% 和 38.7% 的 mAP 分数，在 Ref-YouTube-VOS 上达到了 56.6% 的 $\mathcal{J&amp;F}$ 分数。特别是，与最佳候选方法相比，该方法在 P$@$0.5 上具有了 2.1% 和 3.2% 的提升，在 $\mathcal{J}$ 上具有了 2.9% 的提升。<details>
<summary>Abstract</summary>
Referring Video Object Segmentation (RVOS) requires segmenting the object in video referred by a natural language query. Existing methods mainly rely on sophisticated pipelines to tackle such cross-modal task, and do not explicitly model the object-level spatial context which plays an important role in locating the referred object. Therefore, we propose an end-to-end RVOS framework completely built upon transformers, termed \textit{Fully Transformer-Equipped Architecture} (FTEA), which treats the RVOS task as a mask sequence learning problem and regards all the objects in video as candidate objects. Given a video clip with a text query, the visual-textual features are yielded by encoder, while the corresponding pixel-level and word-level features are aligned in terms of semantic similarity. To capture the object-level spatial context, we have developed the Stacked Transformer, which individually characterizes the visual appearance of each candidate object, whose feature map is decoded to the binary mask sequence in order directly. Finally, the model finds the best matching between mask sequence and text query. In addition, to diversify the generated masks for candidate objects, we impose a diversity loss on the model for capturing more accurate mask of the referred object. Empirical studies have shown the superiority of the proposed method on three benchmarks, e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782 videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% in terms of $\mathcal{J\&F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects). Particularly, compared to the best candidate method, it has a gain of 2.1% and 3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gain of 2.9% in terms of $\mathcal{J}$ on the latter one.
</details>
<details>
<summary>摘要</summary>
参考视频对象 segmentation（RVOS）需要将视频中的对象与自然语言查询相关联。现有方法主要依靠复杂的管道来解决这种跨模态任务，并未直接模型对象水平的空间上下文，这上下文在定位引用对象中扮演重要角色。因此，我们提出了一个 completel y built upon transformers 的框架，称为 Fully Transformer-Equipped Architecture（FTEA），它将 RVOS 任务视为面征序列学习问题，并将所有视频中的对象视为候选对象。给定一个视频剪辑和自然语言查询，视觉语言特征是通过Encoder生成的，而对应的像素级和单词级特征则是在semantic similarity的基础上对准。为了捕捉对象水平的空间上下文，我们开发了堆叠transformer，它可以个别地描述每个候选对象的视觉特征，并将其特征图直接解码到二进制mask sequence中。最后，模型会找到与文本查询最佳匹配的mask sequence。此外，为了捕捉更加准确的mask，我们对模型进行多样性损失，以便在候选对象中捕捉更多的详细信息。实验表明，我们的方法在三个标准准的benchmark上表现出色，例如，FETA在A2D Sentences（3782个视频）和J-HMDB Sentences（928个视频）上的mAP分别达到45.1%和38.7%，在Ref-YouTube-VOS（3975个视频和7451个对象）上的$\mathcal{J\&F}$分别达到56.6%。特别是，相比最佳候选方法，FETA在前两个benchmark上的P$@$0.5分别提高了2.1%和3.2%，而在Ref-YouTube-VOS上的$\mathcal{J}$分别提高了2.9%。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Learning-Pace-Synchronization-for-Open-World-Semi-Supervised-Learning"><a href="#Bridging-the-Gap-Learning-Pace-Synchronization-for-Open-World-Semi-Supervised-Learning" class="headerlink" title="Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning"></a>Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11930">http://arxiv.org/abs/2309.11930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Ye, Kai Gan, Tong Wei, Min-Ling Zhang</li>
<li>for: 这个研究目的是解决开放世界半监督学习中的新类别发现问题，即使用不监督的数据来探索新的类别，同时维持已知类别的性能。</li>
<li>methods: 我们提出了两个方法来解决这个问题：1）适应的margin损失基于估计的类别分布，以将学习速度均衡化，2）pseudo-label对称分组，将可能属于同一类别的样本集中，以增强新类别发现。</li>
<li>results: 我们的广泛评估表明，现有模型仍然对新类别学习产生问题，而我们的方法则能够平衡已知类别和新类别，在ImageNet dataset上达到了3%的平均精度提升，相比先前的州ike-of-the-art。此外，我们发现了精益调整自我监督预训练模型可以很大程度上提高性能。<details>
<summary>Abstract</summary>
In open-world semi-supervised learning, a machine learning model is tasked with uncovering novel categories from unlabeled data while maintaining performance on seen categories from labeled data. The central challenge is the substantial learning gap between seen and novel categories, as the model learns the former faster due to accurate supervisory information. To address this, we introduce 1) an adaptive margin loss based on estimated class distribution, which encourages a large negative margin for samples in seen classes, to synchronize learning paces, and 2) pseudo-label contrastive clustering, which pulls together samples which are likely from the same class in the output space, to enhance novel class discovery. Our extensive evaluations on multiple datasets demonstrate that existing models still hinder novel class learning, whereas our approach strikingly balances both seen and novel classes, achieving a remarkable 3% average accuracy increase on the ImageNet dataset compared to the prior state-of-the-art. Additionally, we find that fine-tuning the self-supervised pre-trained backbone significantly boosts performance over the default in prior literature. After our paper is accepted, we will release the code.
</details>
<details>
<summary>摘要</summary>
在开放世界半监督学习中，一个机器学习模型被要求探索未经标注的数据中的新分类，同时保持已经标注的分类的性能。中心挑战是seen和novel分类之间的学习差距，因为模型在高精度的指导信息下快速学习seen分类。为此，我们提出了以下两点方法：1. 适应margin损失基于估计类分布，该损失函数鼓励在seen分类中的样本具有大负margin，以同步学习速度。2.  Pseudo-label对比分 clustering，该方法在输出空间中吸引同类样本相互吸引，以促进novel分类的发现。我们对多个数据集进行了广泛的评估，发现现有模型仍然受到novel分类学习的限制，而我们的方法能够很好地均衡seen和novel分类，在ImageNet数据集上实现了3%的平均准确率提升 compared to Prior State-of-the-art。此外，我们发现在先前的文献中 defaults 的自然语言预训练模型进行了显著提升性能的观察。在我们的论文被接受后，我们将释放代码。
</details></li>
</ul>
<hr>
<h2 id="Video-Scene-Location-Recognition-with-Neural-Networks"><a href="#Video-Scene-Location-Recognition-with-Neural-Networks" class="headerlink" title="Video Scene Location Recognition with Neural Networks"></a>Video Scene Location Recognition with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11928">http://arxiv.org/abs/2309.11928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Korel, Petr Pulc, Jiří Tumpach, Martin Holeňa</li>
<li>for: 这篇论文探讨了从视频序列中提取场景的可能性，使用人工神经网络。</li>
<li>methods: 该方法选择每个场景中的一些帧，通过预训练的单个图像预处理卷积网络进行转换，然后使用后续层的神经网络来确定场景位置。</li>
<li>results: 研究人员对从《大带费》电视剧获取的数据集进行测试和比较，发现只有某些方法适合当务。<details>
<summary>Abstract</summary>
This paper provides an insight into the possibility of scene recognition from a video sequence with a small set of repeated shooting locations (such as in television series) using artificial neural networks. The basic idea of the presented approach is to select a set of frames from each scene, transform them by a pre-trained singleimage pre-processing convolutional network, and classify the scene location with subsequent layers of the neural network. The considered networks have been tested and compared on a dataset obtained from The Big Bang Theory television series. We have investigated different neural network layers to combine individual frames, particularly AveragePooling, MaxPooling, Product, Flatten, LSTM, and Bidirectional LSTM layers. We have observed that only some of the approaches are suitable for the task at hand.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文探讨了通过视频序列中重复拍摄的小集合来实现场景认识的可能性，使用人工神经网络。提出的方法选择每个场景中的一些帧，通过预训练的单张图像预处理卷积神经网络进行转换，然后使用神经网络的后续层来确定场景位置。我们在《大咖大爆》电视剧集中获取的数据集上测试和比较了不同的神经网络层，包括AveragePooling、MaxPooling、Product、Flatten、LSTM和Bidirectional LSTM层。我们发现只有某些方法适合这种任务。
</details></li>
</ul>
<hr>
<h2 id="TextCLIP-Text-Guided-Face-Image-Generation-And-Manipulation-Without-Adversarial-Training"><a href="#TextCLIP-Text-Guided-Face-Image-Generation-And-Manipulation-Without-Adversarial-Training" class="headerlink" title="TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training"></a>TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11923">http://arxiv.org/abs/2309.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhou You, Jian Zhang</li>
<li>for: 文章目标是提出一种基于文本指导的图像生成和修改方法，而不需要对敌对模型进行训练。</li>
<li>methods: 方法基于StyleGAN和CLIP的协同适应力，通过特制的映射网络将文本转化为图像。</li>
<li>results: 对多Modal CelebA-HQ数据集进行了广泛的实验，并证明了该方法在文本指导生成和修改任务上具有优于现有方法的性能。<details>
<summary>Abstract</summary>
Text-guided image generation aimed to generate desired images conditioned on given texts, while text-guided image manipulation refers to semantically edit parts of a given image based on specified texts. For these two similar tasks, the key point is to ensure image fidelity as well as semantic consistency. Many previous approaches require complex multi-stage generation and adversarial training, while struggling to provide a unified framework for both tasks. In this work, we propose TextCLIP, a unified framework for text-guided image generation and manipulation without adversarial training. The proposed method accepts input from images or random noise corresponding to these two different tasks, and under the condition of the specific texts, a carefully designed mapping network that exploits the powerful generative capabilities of StyleGAN and the text image representation capabilities of Contrastive Language-Image Pre-training (CLIP) generates images of up to $1024\times1024$ resolution that can currently be generated. Extensive experiments on the Multi-modal CelebA-HQ dataset have demonstrated that our proposed method outperforms existing state-of-the-art methods, both on text-guided generation tasks and manipulation tasks.
</details>
<details>
<summary>摘要</summary>
文本干预图像生成和文本干预图像修改都是类似的任务，关键点是保持图像准确性和 semantics 一致性。许多先前的方法需要复杂的多stage生成和对抗训练，而且很难提供一个简单的框架 для这两个任务。在这项工作中，我们提出了 TextCLIP，一个简单的框架 для文本干预图像生成和修改，不需要对抗训练。该方法接受图像或随机噪声作为输入，根据特定的文本来生成图像，可以生成高分辨率图像（最大 $1024\times1024$）。经验表明，我们提出的方法在多模态 CelebA-HQ 数据集上表现出了比例性，在文本干预图像生成和修改任务中都超过了现有的状态泰技术。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Temporal-Transformer-based-Video-Compression-Framework"><a href="#Spatial-Temporal-Transformer-based-Video-Compression-Framework" class="headerlink" title="Spatial-Temporal Transformer based Video Compression Framework"></a>Spatial-Temporal Transformer based Video Compression Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11913">http://arxiv.org/abs/2309.11913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbo Gao, Wenjia Huang, Shuai Li, Hui Yuan, Mao Ye, Siwei Ma</li>
<li>For: 这个研究旨在提出一种基于 transformer 框架的learned video compression（LVC）方法，以提高视频编码的效率和质量。* Methods: 该方法使用了一种叫做 Relaxed Deformable Transformer（RDT）的新型自适应变换器，以稳定地估算视频帧之间的运动信息。同时，该方法还使用了一种多级划分预测（MGP）模块，以更好地利用多个参照帧的信息，以及一种空间特征分布预测（SFD-T）模块，以减少视频的空间-时间重复性。* Results: 实验结果表明，该方法可以与 VTM 比较，并且 achieved the best result with 13.5% BD-Rate saving。<details>
<summary>Abstract</summary>
Learned video compression (LVC) has witnessed remarkable advancements in recent years. Similar as the traditional video coding, LVC inherits motion estimation/compensation, residual coding and other modules, all of which are implemented with neural networks (NNs). However, within the framework of NNs and its training mechanism using gradient backpropagation, most existing works often struggle to consistently generate stable motion information, which is in the form of geometric features, from the input color features. Moreover, the modules such as the inter-prediction and residual coding are independent from each other, making it inefficient to fully reduce the spatial-temporal redundancy. To address the above problems, in this paper, we propose a novel Spatial-Temporal Transformer based Video Compression (STT-VC) framework. It contains a Relaxed Deformable Transformer (RDT) with Uformer based offsets estimation for motion estimation and compensation, a Multi-Granularity Prediction (MGP) module based on multi-reference frames for prediction refinement, and a Spatial Feature Distribution prior based Transformer (SFD-T) for efficient temporal-spatial joint residual compression. Specifically, RDT is developed to stably estimate the motion information between frames by thoroughly investigating the relationship between the similarity based geometric motion feature extraction and self-attention. MGP is designed to fuse the multi-reference frame information by effectively exploring the coarse-grained prediction feature generated with the coded motion information. SFD-T is to compress the residual information by jointly exploring the spatial feature distributions in both residual and temporal prediction to further reduce the spatial-temporal redundancy. Experimental results demonstrate that our method achieves the best result with 13.5% BD-Rate saving over VTM.
</details>
<details>
<summary>摘要</summary>
历年来，学习视频压缩（LVC）技术已经经历了很大的发展。LVC技术继承了传统视频编码中的运动估计/补做、剩余编码等模块，并且通过神经网络（NN）的实现。然而，大多数现有的工作在NN和其训练机制中使用梯度倒逆时，很难一致地生成稳定的运动信息，这种运动信息通常是输入颜色特征的几何特征。此外，模块如 междуPrediction和剩余编码是独立的，这使得它们之间的重叠不充分。为解决这些问题，在这篇论文中，我们提出了一种新的空间-时间变换基本的视频压缩（STT-VC）框架。它包括一个宽度缓和变换（RDT）、基于Uformer的偏移估计，以及一个多级别预测（MGP）模块和一个空间特征分布先验基于变换（SFD-T）。具体来说，RDT是通过彻底调查相似性基于几何运动特征提取和自注意力来稳定地估计运动信息 между帧。MGP是通过有效地探索压缩动作信息中的粗糙预测特征来融合多个参照帧信息。SFD-T是通过同时探索剩余信息中的空间特征分布来进一步减少空间-时间重复。实验结果表明，我们的方法可以在VTM比较下实现13.5%的BD-Rate节省。
</details></li>
</ul>
<hr>
<h2 id="Heart-Rate-Detection-Using-an-Event-Camera"><a href="#Heart-Rate-Detection-Using-an-Event-Camera" class="headerlink" title="Heart Rate Detection Using an Event Camera"></a>Heart Rate Detection Using an Event Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11891">http://arxiv.org/abs/2309.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Jagtap, RamaKrishna Venkatesh Saripalli, Joe Lemley, Waseem Shariff, Alan F. Smeaton</li>
<li>for: 这个研究旨在使用事件摄像机来不侵入式地监测心率（HR）。</li>
<li>methods: 研究使用事件摄像机捕捉手臂部分的皮肤微妙变化，并通过自动检测事件来测量心率。</li>
<li>results: 实验结果显示，使用事件摄像机可以精确地检测心率，并且比其他非接触式心率测量方法更高精度。 However, the method is limited by light-induced flickering and subconscious tremors of the individual during data capture.<details>
<summary>Abstract</summary>
Event cameras, also known as neuromorphic cameras, are an emerging technology that offer advantages over traditional shutter and frame-based cameras, including high temporal resolution, low power consumption, and selective data acquisition. In this study, we propose to harnesses the capabilities of event-based cameras to capture subtle changes in the surface of the skin caused by the pulsatile flow of blood in the wrist region. We investigate whether an event camera could be used for continuous noninvasive monitoring of heart rate (HR). Event camera video data from 25 participants, comprising varying age groups and skin colours, was collected and analysed. Ground-truth HR measurements obtained using conventional methods were used to evaluate of the accuracy of automatic detection of HR from event camera data. Our experimental results and comparison to the performance of other non-contact HR measurement methods demonstrate the feasibility of using event cameras for pulse detection. We also acknowledge the challenges and limitations of our method, such as light-induced flickering and the sub-conscious but naturally-occurring tremors of an individual during data capture.
</details>
<details>
<summary>摘要</summary>
事件摄像机也称为神经模型摄像机，是一种emerging技术，它们在传统的闭合式摄像机和帧摄像机方面具有优势，包括高时间分辨率、低功耗和选择性数据收集。在这项研究中，我们利用事件摄像机来捕捉血液径向流动在臂部区域 superficies 上的微小变化。我们调查了事件摄像机是否可以用于无侵入式、连续监测心率（HR）。我们收集了25名参与者的事件摄像机视频数据，其中年龄层width 和肤色各不相同。我们使用传统方法获取的真实心率值来评估自动从事件摄像机数据中检测HR的准确性。我们的实验结果和与其他非接触式心率测量方法的比较表明了使用事件摄像机进行脉吸检测的可行性。然而，我们也承认使用这种方法时存在挑战和限制，如光学辐射引起的闪烁和个体在数据采集过程中自然发生的微小颤动。
</details></li>
</ul>
<hr>
<h2 id="On-the-Fly-SfM-What-you-capture-is-What-you-get"><a href="#On-the-Fly-SfM-What-you-capture-is-What-you-get" class="headerlink" title="On-the-Fly SfM: What you capture is What you get"></a>On-the-Fly SfM: What you capture is What you get</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11883">http://arxiv.org/abs/2309.11883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongqian Zhan, Rui Xia, Yifei Yu, Yibo Xu, Xin Wang</li>
<li>for: 这 paper 的目的是提出一种在线 Structure from Motion（SfM）方法，可以在图像捕捉过程中实时进行pose和稀疏点云的计算。</li>
<li>methods: 该方法使用了一个学习基于全局特征的词汇树来快速查找新抵达的图像，然后使用了一种robust的特征匹配机制（LSM）来提高图像Alignment性能。最后，通过研究新抵达图像的相邻图像的影响，提出了一种高效的层次权重本地加重（BA）优化方法。</li>
<li>results: 实验结果表明，在线 SfM 可以坚定地将图像注册，而不需要先将图像 fed into SfM 管道。<details>
<summary>Abstract</summary>
Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Saliency-and-Cropping-to-Improve-Video-Memorability"><a href="#Using-Saliency-and-Cropping-to-Improve-Video-Memorability" class="headerlink" title="Using Saliency and Cropping to Improve Video Memorability"></a>Using Saliency and Cropping to Improve Video Memorability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11881">http://arxiv.org/abs/2309.11881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Vaibhav Mudgal, Qingyang Wang, Lorin Sweeney, Alan F. Smeaton</li>
<li>for: 这项研究旨在提高视频的记忆性，以便提高视频的分享、播放和讨论。</li>
<li>methods: 研究人员通过选择ively cropping帧图像来提高视频的记忆性。他们使用了基本的固定剪辑以及动态剪辑，其中剪辑的大小和位置在视频播放时随着图像精力的变化。</li>
<li>results: 研究人员发现，特别是对低初始记忆性的视频，通过这些方法可以提高视频的记忆性。<details>
<summary>Abstract</summary>
Video memorability is a measure of how likely a particular video is to be remembered by a viewer when that viewer has no emotional connection with the video content. It is an important characteristic as videos that are more memorable are more likely to be shared, viewed, and discussed. This paper presents results of a series of experiments where we improved the memorability of a video by selectively cropping frames based on image saliency. We present results of a basic fixed cropping as well as the results from dynamic cropping where both the size of the crop and the position of the crop within the frame, move as the video is played and saliency is tracked. Our results indicate that especially for videos of low initial memorability, the memorability score can be improved.
</details>
<details>
<summary>摘要</summary>
视频记忆性是观看者没有情感连接的视频内容记忆的度量。它是一个重要的特性，因为更有记忆性的视频更有可能被分享、播放和讨论。本文报告了一系列实验，我们通过选择性剪辑帧来提高视频的记忆性。我们发现，特别是初始记忆性较低的视频，记忆性分数可以得到提高。
</details></li>
</ul>
<hr>
<h2 id="TCOVIS-Temporally-Consistent-Online-Video-Instance-Segmentation"><a href="#TCOVIS-Temporally-Consistent-Online-Video-Instance-Segmentation" class="headerlink" title="TCOVIS: Temporally Consistent Online Video Instance Segmentation"></a>TCOVIS: Temporally Consistent Online Video Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11857">http://arxiv.org/abs/2309.11857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jun-long-li/tcovis">https://github.com/jun-long-li/tcovis</a></li>
<li>paper_authors: Junlong Li, Bingyao Yu, Yongming Rao, Jie Zhou, Jiwen Lu</li>
<li>for: 这 paper 是为了提出一种新的在线视频实例分割方法，以实现高效精度的视频实例分割。</li>
<li>methods: 这 paper 使用了一种全新的 global instance assignment strategy 和 spatio-temporal enhancement module，以提高模型的时间一致性。</li>
<li>results: 这 paper 在四个广泛采用的视频实例分割benchmark上 achieved state-of-the-art performance，包括 YouTube-VIS 2019&#x2F;2021&#x2F;2022 和 OVIS。例如，在 YouTube-VIS 2021 上，TCOVIS 实现了 49.5 AP 和 61.3 AP 的最佳性能，使用 ResNet-50 和 Swin-L 的背景。<details>
<summary>Abstract</summary>
In recent years, significant progress has been made in video instance segmentation (VIS), with many offline and online methods achieving state-of-the-art performance. While offline methods have the advantage of producing temporally consistent predictions, they are not suitable for real-time scenarios. Conversely, online methods are more practical, but maintaining temporal consistency remains a challenging task. In this paper, we propose a novel online method for video instance segmentation, called TCOVIS, which fully exploits the temporal information in a video clip. The core of our method consists of a global instance assignment strategy and a spatio-temporal enhancement module, which improve the temporal consistency of the features from two aspects. Specifically, we perform global optimal matching between the predictions and ground truth across the whole video clip, and supervise the model with the global optimal objective. We also capture the spatial feature and aggregate it with the semantic feature between frames, thus realizing the spatio-temporal enhancement. We evaluate our method on four widely adopted VIS benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve state-of-the-art performance on all benchmarks without bells-and-whistles. For instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with ResNet-50 and Swin-L backbones, respectively. Code is available at https://github.com/jun-long-li/TCOVIS.
</details>
<details>
<summary>摘要</summary>
近年来，视频实例分割（VIS）领域内有很大的进步，许多离线和在线方法已经达到了状态艺术水平。然而，离线方法在实时场景下不够实用，而在线方法尽管更加实用，但维护时间一致性仍然是一个挑战。在这篇论文中，我们提出了一种新的在线视频实例分割方法，称为TCOVIS，它可以充分利用视频帧序中的时间信息。TCOVIS的核心包括全局实例分配策略和空间时间增强模块，这两个部分都有助于提高视频帧序中特征的时间一致性。具体来说，我们在整个视频帧序中进行全局最佳匹配，并通过全局最佳目标进行监督。同时，我们还捕捉了空间特征，将其与semantic特征相加，实现了空间时间增强。我们在四个广泛采用的 VIS 评测benchmark上进行评测，分别是 YouTube-VIS 2019/2021/2022 和 OVIS，并在所有benchmark上取得了状态艺术性的表现。例如，在 YouTube-VIS 2021 上，TCOVIS 取得了 49.5 AP 和 61.3 AP，使用 ResNet-50 和 Swin-L 的背景中。代码可以在 GitHub 上找到：https://github.com/jun-long-li/TCOVIS。
</details></li>
</ul>
<hr>
<h2 id="DEYOv3-DETR-with-YOLO-for-Real-time-Object-Detection"><a href="#DEYOv3-DETR-with-YOLO-for-Real-time-Object-Detection" class="headerlink" title="DEYOv3: DETR with YOLO for Real-time Object Detection"></a>DEYOv3: DETR with YOLO for Real-time Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11851">http://arxiv.org/abs/2309.11851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodong Ouyang</li>
<li>For: This paper proposes a new training method called step-by-step training to improve the practical application and design flexibility of end-to-end object detectors, particularly for DETR-like models.* Methods: The proposed method first initializes the end-to-end detector with a pre-trained YOLO detector and then trains the backbone and encoder of the DETR-like model from scratch. The step-by-step training method eliminates the need for additional training data and reduces the training cost of the detector.* Results: The proposed DEYOv3 model achieves higher accuracy than traditional DETR-like models while maintaining real-time speed (270 FPS on T4 GPU). DEYOv3-N reaches 41.1% AP on COCO val2017 and 270 FPS, while DEYOv3-L achieves 51.3% AP and 102 FPS. All of these results are achieved without using additional training data, making DEYOv3 the best real-time object detection model in terms of both speed and accuracy.<details>
<summary>Abstract</summary>
Recently, end-to-end object detectors have gained significant attention from the research community due to their outstanding performance. However, DETR typically relies on supervised pretraining of the backbone on ImageNet, which limits the practical application of DETR and the design of the backbone, affecting the model's potential generalization ability. In this paper, we propose a new training method called step-by-step training. Specifically, in the first stage, the one-to-many pre-trained YOLO detector is used to initialize the end-to-end detector. In the second stage, the backbone and encoder are consistent with the DETR-like model, but only the detector needs to be trained from scratch. Due to this training method, the object detector does not need the additional dataset (ImageNet) to train the backbone, which makes the design of the backbone more flexible and dramatically reduces the training cost of the detector, which is helpful for the practical application of the object detector. At the same time, compared with the DETR-like model, the step-by-step training method can achieve higher accuracy than the traditional training method of the DETR-like model. With the aid of this novel training method, we propose a brand-new end-to-end real-time object detection model called DEYOv3. DEYOv3-N achieves 41.1% on COCO val2017 and 270 FPS on T4 GPU, while DEYOv3-L achieves 51.3% AP and 102 FPS. Without the use of additional training data, DEYOv3 surpasses all existing real-time object detectors in terms of both speed and accuracy. It is worth noting that for models of N, S, and M scales, the training on the COCO dataset can be completed using a single 24GB RTX3090 GPU.
</details>
<details>
<summary>摘要</summary>
近期，端到端对象检测器在研究社区中受到了广泛关注，因为它们的表现非常出色。然而，DETR通常需要在ImageNet上进行supervised预训练，这限制了DETR的实际应用和设计，影响了模型的总体适应能力。在这篇论文中，我们提出了一种新的训练方法called step-by-step training。具体来说，在第一个阶段，使用pre-trained YOLO检测器来初始化端到端检测器。在第二个阶段，背部和编码器与DETR-like模型相同，但是只需要从scratch训练检测器。由于这种训练方法，对象检测器不需要额外的数据集（ImageNet）来训练背部，这使得背部的设计变得更加灵活，对检测器的训练成本减少了极大，这有助于实际应用。同时，相比DETR-like模型，step-by-step training方法可以在同样的精度下达到更高的精度。通过这种新的训练方法，我们提出了一个全新的端到端实时对象检测模型called DEYOv3。DEYOv3-N在COCO val2017上达到了41.1%的分数，而DEYOv3-L在T4 GPU上达到了51.3%的AP和270 FPS。没有使用额外的训练数据，DEYOv3超过了所有现有的实时对象检测器，包括速度和精度两个方面。值得注意的是，对N、S、M缩放的模型，在COCO数据集上进行训练可以使用单个24GB RTX3090 GPU。
</details></li>
</ul>
<hr>
<h2 id="MEFLUT-Unsupervised-1D-Lookup-Tables-for-Multi-exposure-Image-Fusion"><a href="#MEFLUT-Unsupervised-1D-Lookup-Tables-for-Multi-exposure-Image-Fusion" class="headerlink" title="MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion"></a>MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11847">http://arxiv.org/abs/2309.11847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hedlen/meflut">https://github.com/hedlen/meflut</a></li>
<li>paper_authors: Ting Jiang, Chuan Wang, Xinpeng Li, Ru Li, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 高品质多曝光图像融合 (高品质MEF)</li>
<li>methods: 利用一维Lookup表 (1D LUT) 将曝光权重编码为 pixel 强度值输入，并通过注意力机制在不同维度（帧、通道和空间）进行学习，以提高高品质和效率的融合。</li>
<li>results: 与现有最佳方法比较，新方法实现高品质和效率的融合，并且实现了4K影像在PC GPU上的几乎即时运行（低于4ms）。code available at：<a target="_blank" rel="noopener" href="https://github.com/Hedlen/MEFLUT%E3%80%82">https://github.com/Hedlen/MEFLUT。</a><details>
<summary>Abstract</summary>
In this paper, we introduce a new approach for high-quality multi-exposure image fusion (MEF). We show that the fusion weights of an exposure can be encoded into a 1D lookup table (LUT), which takes pixel intensity value as input and produces fusion weight as output. We learn one 1D LUT for each exposure, then all the pixels from different exposures can query 1D LUT of that exposure independently for high-quality and efficient fusion. Specifically, to learn these 1D LUTs, we involve attention mechanism in various dimensions including frame, channel and spatial ones into the MEF task so as to bring us significant quality improvement over the state-of-the-art (SOTA). In addition, we collect a new MEF dataset consisting of 960 samples, 155 of which are manually tuned by professionals as ground-truth for evaluation. Our network is trained by this dataset in an unsupervised manner. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the SOTA in our and another representative dataset SICE, both qualitatively and quantitatively. Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC GPU. Given its high quality, efficiency and robustness, our method has been shipped into millions of Android mobiles across multiple brands world-wide. Code is available at: https://github.com/Hedlen/MEFLUT.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的高质量多曝光图像融合（MEF）方法。我们表明，曝光权重可以被编码成1D Lookup Table（LUT），该LUT接受像素强度值作输入，并生成权重作出输出。我们在每个曝光中学习了1D LUT，然后所有的像素从不同的曝光状态可以独立地查询该曝光的1D LUT，以实现高质量和高效的融合。特别是，为了学习这些1D LUT，我们在MEF任务中包含了注意力机制在帧、通道和空间方向中，从而使我们在SOTA中得到了显著的质量改善。此外，我们收集了一个新的MEF数据集，包含960个样本，其中155个是由专业人员 manually tuned 的参考标准 для评估。我们的网络是通过这个数据集在无监督的情况下进行训练。我们进行了广泛的实验，以证明所有新提出的组件的效iveness，结果表明，我们的方法在我们的数据集和另一个代表性数据集SICE中， Both qualitatively and quantitativelysuperior to the SOTA。此外，我们的1D LUT方法在4K图像上只需0.4毫秒，在PC GPU上运行。由于其高质量、高效和稳定性，我们的方法已经被运送到了世界各地的几百万Android手机中。代码可以在https://github.com/Hedlen/MEFLUT中找到。
</details></li>
</ul>
<hr>
<h2 id="MoPA-Multi-Modal-Prior-Aided-Domain-Adaptation-for-3D-Semantic-Segmentation"><a href="#MoPA-Multi-Modal-Prior-Aided-Domain-Adaptation-for-3D-Semantic-Segmentation" class="headerlink" title="MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation"></a>MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11839">http://arxiv.org/abs/2309.11839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haozhi Cao, Yuecong Xu, Jianfei Yang, Pengyu Yin, Shenghai Yuan, Lihua Xie</li>
<li>for: 提高3D semantic segmentation中罕见对象的性能，使用多Modal无监督领域适应（MM-UDA）方法。</li>
<li>methods: 提出了多Modal Prior Aided（MoPA）领域适应方法，包括 Valid Ground-based Insertion（VGI）和 SAM consistency loss。</li>
<li>results: 实验显示，我们的方法在多Modal无监督领域适应 benchmark 上达到了状态机器人表现。代码将在 GitHub 上公开。<details>
<summary>Abstract</summary>
Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA.
</details>
<details>
<summary>摘要</summary>
多Modal无监督领域适应（MM-UDA）用于3Dsemantic segmentation是一种实用的解决方案，以实现无需昂贵点级标注的semantic理解。previous MM-UDA方法可以获得总体改进，但它们受到分类偏好的问题，这限制了它们在实际应用中的采用。这种偏好性问题主要来自于：1）自我帮助学习偏好数据，2）缺乏像素级2D监视信号。在这项工作中，我们提出了多Modal Prior帮助（MoPA）领域适应，以提高罕见对象的性能。具体来说，我们开发了有效的基准图Insertion（VGI）技术，以修正不均匀的监视信号，而不是引入人工artefacts，以避免导致轻微解决方案。此外，我们的SAM一致损失函数利用了2D Prior semantic masks从SAM中的像素级监视信号，以强制每个对象在semantic mask中具有一致的预测。知识从modalSpecific Prior中学习的知识然后被共享到不同模式之间，以达到更好的罕见对象分割。广泛的实验表明，我们的方法在MM-UDA benchmark上达到了最佳性能。代码将在https://github.com/AronCao49/MoPA中提供。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Endoscopic-Ultrasound-Station-Recognition-with-Limited-Data"><a href="#Automatic-Endoscopic-Ultrasound-Station-Recognition-with-Limited-Data" class="headerlink" title="Automatic Endoscopic Ultrasound Station Recognition with Limited Data"></a>Automatic Endoscopic Ultrasound Station Recognition with Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11820">http://arxiv.org/abs/2309.11820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amrita-medical-ai/eusml-labeller">https://github.com/amrita-medical-ai/eusml-labeller</a></li>
<li>paper_authors: Abhijit Ramesh, Anantha Nandanan, Anantha Nandanan, Priya Nair MD, Gilad Gressel</li>
<li>for: 这个论文的目的是提高检测胆囊癌的精度，以提高患者的诊断和存活率。</li>
<li>methods: 这个论文使用了人工智能技术，以帮助医生更准确地检测胆囊癌。特别是，它使用了深度学习算法来识别胆囊癌的不同区域，以提高检测的精度。</li>
<li>results: 研究表明，使用这个人工智能检测工具可以提高检测胆囊癌的精度，并且只需要43个检测过程，无需进行价值革新的训练。此外，这个工具还可以提供可读性和可解释的视觉化技术，帮助医生更好地理解检测结果。<details>
<summary>Abstract</summary>
Pancreatic cancer is a lethal form of cancer that significantly contributes to cancer-related deaths worldwide. Early detection is essential to improve patient prognosis and survival rates. Despite advances in medical imaging techniques, pancreatic cancer remains a challenging disease to detect. Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting pancreatic cancer. However, it requires expert interpretation of complex ultrasound images to complete a reliable patient scan. To obtain complete imaging of the pancreas, practitioners must learn to guide the endoscope into multiple "EUS stations" (anatomical locations), which provide different views of the pancreas. This is a difficult skill to learn, involving over 225 proctored procedures with the support of an experienced doctor. We build an AI-assisted tool that utilizes deep learning techniques to identify these stations of the stomach in real time during EUS procedures. This computer-assisted diagnostic (CAD) will help train doctors more efficiently. Historically, the challenge faced in developing such a tool has been the amount of retrospective labeling required by trained clinicians. To solve this, we developed an open-source user-friendly labeling web app that streamlines the process of annotating stations during the EUS procedure with minimal effort from the clinicians. Our research shows that employing only 43 procedures with no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable to the current state of the art. In addition, we employ Grad-CAM, a visualization technology that provides clinicians with interpretable and explainable visualizations.
</details>
<details>
<summary>摘要</summary>
胰腺癌是一种致命的癌症，对全球癌症相关死亡率具有重要贡献。早期发现是提高病人 прогноosis 和存生率的关键。 despite advances in medical imaging techniques, pancreatic cancer remains a challenging disease to detect. Endoscopic ultrasound (EUS) is the most effective diagnostic tool for detecting pancreatic cancer, but it requires expert interpretation of complex ultrasound images to complete a reliable patient scan. To obtain complete imaging of the pancreas, practitioners must learn to guide the endoscope into multiple "EUS stations" (anatomical locations), which provide different views of the pancreas. This is a difficult skill to learn, involving over 225 proctored procedures with the support of an experienced doctor. We have developed an AI-assisted tool that utilizes deep learning techniques to identify these stations of the stomach in real time during EUS procedures. This computer-assisted diagnostic (CAD) will help train doctors more efficiently. Historically, the challenge faced in developing such a tool has been the amount of retrospective labeling required by trained clinicians. To solve this, we have developed an open-source, user-friendly labeling web app that streamlines the process of annotating stations during the EUS procedure with minimal effort from the clinicians. Our research shows that employing only 43 procedures with no hyperparameter fine-tuning obtained a balanced accuracy of 90%, comparable to the current state of the art. In addition, we employ Grad-CAM, a visualization technology that provides clinicians with interpretable and explainable visualizations.
</details></li>
</ul>
<hr>
<h2 id="FGFusion-Fine-Grained-Lidar-Camera-Fusion-for-3D-Object-Detection"><a href="#FGFusion-Fine-Grained-Lidar-Camera-Fusion-for-3D-Object-Detection" class="headerlink" title="FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection"></a>FGFusion: Fine-Grained Lidar-Camera Fusion for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11804">http://arxiv.org/abs/2309.11804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaviergrool/fgfusion">https://github.com/xaviergrool/fgfusion</a></li>
<li>paper_authors: Zixuan Yin, Han Sun, Ningzhong Liu, Huiyu Zhou, Jiaquan Shen</li>
<li>for: 本文旨在提出一种细化的 lidar-camera 结合方法，以便在自动驾驶中提高 3D 检测的精度。</li>
<li>methods: 本方法使用了 dual pathway hierarchy 结构，以提取图像中的高级 semantics 和低级细节信息，并在 auxiliary network 的帮助下，点云特征更好地学习细致的空间信息。 最后，我们提出了多级 fusión（MSF）方法，将图像和点云的最后 N 个特征图 fusion。</li>
<li>results: 我们在 KITTI 和 Waymo 两个流行的自动驾驶 benchmark 上进行了广泛的实验，并证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Lidars and cameras are critical sensors that provide complementary information for 3D detection in autonomous driving. While most prevalent methods progressively downscale the 3D point clouds and camera images and then fuse the high-level features, the downscaled features inevitably lose low-level detailed information. In this paper, we propose Fine-Grained Lidar-Camera Fusion (FGFusion) that make full use of multi-scale features of image and point cloud and fuse them in a fine-grained way. First, we design a dual pathway hierarchy structure to extract both high-level semantic and low-level detailed features of the image. Second, an auxiliary network is introduced to guide point cloud features to better learn the fine-grained spatial information. Finally, we propose multi-scale fusion (MSF) to fuse the last N feature maps of image and point cloud. Extensive experiments on two popular autonomous driving benchmarks, i.e. KITTI and Waymo, demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
<SYS>请将给定文本翻译成简化中文。</SYS>感谢！以下是文本的翻译： lidar和摄像头是自动驾驶中不可或缺的感知器，大多数现有方法是逐渐减小3D点云和摄像头图像，然后将高级特征进行融合。然而，下降的特征无法保留低级细节信息。在这篇论文中，我们提出了细腻的激光干涉摄像头融合（FGFusion），使用多尺度特征来融合图像和点云的细节信息。首先，我们设计了两个层次结构，一个用于提取图像的高级semantic特征，另一个用于提取低级细节特征。其次，我们引入了一个协助网络，以便点云特征更好地学习细腻的空间信息。最后，我们提出了多尺度融合（MSF），用于融合图像和点云的最后N个特征图。我们在KITTI和Waymo两个流行的自动驾驶测试平台上进行了广泛的实验，结果表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Real-Time-Multi-Task-Learning-System-for-Joint-Detection-of-Face-Facial-Landmark-and-Head-Pose"><a href="#A-Real-Time-Multi-Task-Learning-System-for-Joint-Detection-of-Face-Facial-Landmark-and-Head-Pose" class="headerlink" title="A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose"></a>A Real-Time Multi-Task Learning System for Joint Detection of Face, Facial Landmark and Head Pose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11773">http://arxiv.org/abs/2309.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingtian Wu, Liming Zhang</li>
<li>for: 这篇论文主要针对面部分析任务中的极大角度头 pose 问题，提出了一种实时多任务检测系统，可同时检测面部、 facial landmark 和头 pose。</li>
<li>methods: 该系统基于广泛采用的 YOLOv8 检测框架，添加了更多的landmark regression head，以便高效地确定面部关键点。此外，我们对 YOLOv8 框架中的多个模块进行了优化和提升。</li>
<li>results: 我们在 300W-LP 和 AFLW2000-3D 数据集上进行了广泛的实验，结果表明我们的模型可以有效地处理大角度头 pose 问题，同时具有实时性。<details>
<summary>Abstract</summary>
Extreme head postures pose a common challenge across a spectrum of facial analysis tasks, including face detection, facial landmark detection (FLD), and head pose estimation (HPE). These tasks are interdependent, where accurate FLD relies on robust face detection, and HPE is intricately associated with these key points. This paper focuses on the integration of these tasks, particularly when addressing the complexities posed by large-angle face poses. The primary contribution of this study is the proposal of a real-time multi-task detection system capable of simultaneously performing joint detection of faces, facial landmarks, and head poses. This system builds upon the widely adopted YOLOv8 detection framework. It extends the original object detection head by incorporating additional landmark regression head, enabling efficient localization of crucial facial landmarks. Furthermore, we conduct optimizations and enhancements on various modules within the original YOLOv8 framework. To validate the effectiveness and real-time performance of our proposed model, we conduct extensive experiments on 300W-LP and AFLW2000-3D datasets. The results obtained verify the capability of our model to tackle large-angle face pose challenges while delivering real-time performance across these interconnected tasks.
</details>
<details>
<summary>摘要</summary>
极端头 pose  pose 是一种常见的挑战，涉及到脸部检测、脸部关键点检测 (FLD) 和头 pose 估算 (HPE) 等多个面部分析任务。这些任务之间存在互相关系，精准的 FLD 需要正确的脸部检测，而 HPE 则取决于关键点的确定。本文关注这些任务的集成，特别是在处理大角度头 pose 时的复杂性。我们提出了一个实时多任务检测系统，可同时检测脸部、脸部关键点和头 pose。这个系统基于广泛采用的 YOLOv8 检测框架。我们在原始的对象检测头上添加了附加的关键点 regression 头，以便高效地确定脸部关键点。此外，我们对 YOLOv8 框架中的各个模块进行了优化和改进。为了证明我们提出的模型的有效性和实时性，我们在 300W-LP 和 AFLW2000-3D 数据集上进行了广泛的实验。实验结果表明，我们的模型能够 effectively 处理大角度头 pose 挑战，并在这些相关任务中提供实时性。
</details></li>
</ul>
<hr>
<h2 id="Fast-Satellite-Tensorial-Radiance-Field-for-Multi-date-Satellite-Imagery-of-Large-Size"><a href="#Fast-Satellite-Tensorial-Radiance-Field-for-Multi-date-Satellite-Imagery-of-Large-Size" class="headerlink" title="Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery of Large Size"></a>Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery of Large Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11767">http://arxiv.org/abs/2309.11767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtong Zhang, Yuanxiang Li</li>
<li>for: 提高卫星图像NeRF模型的速度和可扩展性，并解决了大尺寸卫星图像处理的限制。</li>
<li>methods: 使用多尺度矩阵分解方法来表示颜色、体积密度和辅助变量，并使用总变量损失来修正多日图像的不一致性。</li>
<li>results: 与state-of-the-art Sat-NeRF系列相比，SatensoRF在新视图合成性能方面表现出色，并且需要 fewer parameters для训练和推断，具有更快的训练和推断速度和更低的计算需求。<details>
<summary>Abstract</summary>
Existing NeRF models for satellite images suffer from slow speeds, mandatory solar information as input, and limitations in handling large satellite images. In response, we present SatensoRF, which significantly accelerates the entire process while employing fewer parameters for satellite imagery of large size. Besides, we observed that the prevalent assumption of Lambertian surfaces in neural radiance fields falls short for vegetative and aquatic elements. In contrast to the traditional hierarchical MLP-based scene representation, we have chosen a multiscale tensor decomposition approach for color, volume density, and auxiliary variables to model the lightfield with specular color. Additionally, to rectify inconsistencies in multi-date imagery, we incorporate total variation loss to restore the density tensor field and treat the problem as a denosing task.To validate our approach, we conducted assessments of SatensoRF using subsets from the spacenet multi-view dataset, which includes both multi-date and single-date multi-view RGB images. Our results clearly demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in terms of novel view synthesis performance. Significantly, SatensoRF requires fewer parameters for training, resulting in faster training and inference speeds and reduced computational demands.
</details>
<details>
<summary>摘要</summary>
现有的卫星图像NeRF模型受到慢速、必需日升信息作为输入以及处理大型卫星图像的限制。作为回应，我们提出了SatensoRF，它可以快速加速整个过程，并使用 fewer parameters 来处理大型卫星图像。此外，我们发现了传统的LAMBERTIAN表面假设在神经采集场景中失足，特别是 для植物和水生元素。相比传统的层次MLP基本Scene表示，我们选择了多尺度矩阵分解方法来表示颜色、体积密度和辅助变量，以模型光场。此外，为了纠正多日图像之间的不一致，我们添加了总变量损失来修复密度矩阵场景，并将问题视为锈除task。为验证我们的方法，我们对SpaceNet多视点数据集中的subset进行了评估，该数据集包括了多日和单日多视点RGB图像。我们的结果表明，SatensoRF超越了state-of-the-art Sat-NeRF系列在新视图合成性能方面。特别是，SatensoRF需要 fewer parameters 进行训练，导致更快的训练和推理速度，以及减少的计算占用。
</details></li>
</ul>
<hr>
<h2 id="Dictionary-Attack-on-IMU-based-Gait-Authentication"><a href="#Dictionary-Attack-on-IMU-based-Gait-Authentication" class="headerlink" title="Dictionary Attack on IMU-based Gait Authentication"></a>Dictionary Attack on IMU-based Gait Authentication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11766">http://arxiv.org/abs/2309.11766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajeshjnu2006/dictionaryattackonimugait">https://github.com/rajeshjnu2006/dictionaryattackonimugait</a></li>
<li>paper_authors: Rajesh Kumar, Can Isik, Chilukuri K. Mohan</li>
<li>for: 本研究旨在攻击使用IMU嵌入式加速度计记录的步征 Authentication系统。</li>
<li>methods: 本研究使用了一种基于字典的攻击方法，即使用一个IMUGait模式字典来攻击 Authentication系统。</li>
<li>results: 研究发现，可以通过使用IMUGait模式字典来攻击多种用户认证模型，并且对认证系统的安全性提出了挑战。<details>
<summary>Abstract</summary>
We present a novel adversarial model for authentication systems that use gait patterns recorded by the inertial measurement unit (IMU) built into smartphones. The attack idea is inspired by and named after the concept of a dictionary attack on knowledge (PIN or password) based authentication systems. In particular, this work investigates whether it is possible to build a dictionary of IMUGait patterns and use it to launch an attack or find an imitator who can actively reproduce IMUGait patterns that match the target's IMUGait pattern. Nine physically and demographically diverse individuals walked at various levels of four predefined controllable and adaptable gait factors (speed, step length, step width, and thigh-lift), producing 178 unique IMUGait patterns. Each pattern attacked a wide variety of user authentication models. The deeper analysis of error rates (before and after the attack) challenges the belief that authentication systems based on IMUGait patterns are the most difficult to spoof; further research is needed on adversarial models and associated countermeasures.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的反攻击模型，用于authentication系统，利用智能手机内置的抗重力测量单元（IMU）记录的步态特征。这种攻击的想法来自于和知识（PIN或密码）基于的字典攻击。本研究是否可以构建一个IMUGait特征的词典，并使用其发动攻击或找到一个能够活泼地复制目标用户的IMUGait特征。我们采集了9名物理和人口多样化的个体，在四种可控和可适应的步态因素（速度、步长、步宽和膝盖）的不同水平上步行，共生成178个唯一的IMUGait特征。每个特征攻击了多种用户身份验证模型。我们进行了更深层次的错误率分析（之前和之后攻击），挑战了基于IMUGait特征的身份验证系统是最难模仿的假设。需要进一步的研究反攻击模型和相关的防范措施。
</details></li>
</ul>
<hr>
<h2 id="SAM-OCTA-A-Fine-Tuning-Strategy-for-Applying-Foundation-Model-to-OCTA-Image-Segmentation-Tasks"><a href="#SAM-OCTA-A-Fine-Tuning-Strategy-for-Applying-Foundation-Model-to-OCTA-Image-Segmentation-Tasks" class="headerlink" title="SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks"></a>SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11758">http://arxiv.org/abs/2309.11758</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shellredia/sam-octa">https://github.com/shellredia/sam-octa</a></li>
<li>paper_authors: Chengliang Wang, Xinrun Chen, Haojian Ning, Shiying Li</li>
<li>for: 这个论文的目的是提出一种基于低级别适应技术的方法，用于处理多种分类任务在optical coherence tomography angiography（OCTA）图像上。</li>
<li>methods: 该方法使用了基于低级别适应技术的基础模型细化和相关提示点生成策略，以处理OCTA图像上的多种分类任务。</li>
<li>results: 该方法在公共可用的OCTA-500数据集上进行实验，并实现了当前最佳性能指标，同时可以高效地进行本地血管分 segmentation以及有效的血管-血管分 segmentation，这些任务在前一些工作中尚未得到很好的解决。<details>
<summary>Abstract</summary>
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.
</details>
<details>
<summary>摘要</summary>
在Optical coherence tomography angiography（OCTA）图像分析中，需要进行特定目标分割。现有方法通常在有限样本（约几百个）上进行supervised学习，这可能导致过拟合。为解决这个问题，我们采用了low-rank adaptation技术来修改基本模型，并提出了相应的提示点生成策略来处理不同的分割任务。这种方法被称为SAM-OCTA，并在公共可用的OCTA-500 dataset上进行实验。它不仅实现了state-of-the-art性能指标，还能够成功地进行本地血管分割以及有效的动脉-静脉分割，这在前一些工作中尚未得到解决。代码可以在：https://github.com/ShellRedia/SAM-OCTA中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-Centric-Approach-for-Static-Map-Element-Annotation"><a href="#A-Vision-Centric-Approach-for-Static-Map-Element-Annotation" class="headerlink" title="A Vision-Centric Approach for Static Map Element Annotation"></a>A Vision-Centric Approach for Static Map Element Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11754">http://arxiv.org/abs/2309.11754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manymuch/cama">https://github.com/manymuch/cama</a></li>
<li>paper_authors: Jiaxin Zhang, Shiyuan Chen, Haoran Yin, Ruohong Mei, Xuan Liu, Cong Yang, Qian Zhang, Wei Sui<br>for:CAMA is designed to provide high-quality, consistent, and accurate annotations for training machine learning models in the field of computer vision and autonomous driving.methods:CAMA uses a vision-centric approach that leverages cameras to generate 3D annotations of static map elements without relying on LiDAR inputs. The framework achieves high reprojection accuracy across multiple cameras and is spatial-temporally consistent across the entire sequence.results:The proposed CAMA framework is evaluated on the popular nuScenes dataset and shows improved performance compared to the original nuScenes static map elements, with lower reprojection errors (e.g., 4.73 vs. 8.03 pixels).<details>
<summary>Abstract</summary>
The recent development of online static map element (a.k.a. HD Map) construction algorithms has raised a vast demand for data with ground truth annotations. However, available public datasets currently cannot provide high-quality training data regarding consistency and accuracy. To this end, we present CAMA: a vision-centric approach for Consistent and Accurate Map Annotation. Without LiDAR inputs, our proposed framework can still generate high-quality 3D annotations of static map elements. Specifically, the annotation can achieve high reprojection accuracy across all surrounding cameras and is spatial-temporal consistent across the whole sequence. We apply our proposed framework to the popular nuScenes dataset to provide efficient and highly accurate annotations. Compared with the original nuScenes static map element, models trained with annotations from CAMA achieve lower reprojection errors (e.g., 4.73 vs. 8.03 pixels).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PIE-Simulating-Disease-Progression-via-Progressive-Image-Editing"><a href="#PIE-Simulating-Disease-Progression-via-Progressive-Image-Editing" class="headerlink" title="PIE: Simulating Disease Progression via Progressive Image Editing"></a>PIE: Simulating Disease Progression via Progressive Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11745">http://arxiv.org/abs/2309.11745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhao Liang, Xu Cao, Kuei-Da Liao, Tianren Gao, Zhengyu Chen, Tejas Nama</li>
<li>for: 预测疾病进展，促进临床诊断、预后评估和治疗方案设计。</li>
<li>methods: 基于文本生成图像模型，实现疾病进展精准模拟和个性化。iterative refining过程中的梯度下降算法进行了 teoretic 分析。</li>
<li>results: 比对 CLIP 分数（现实度）和疾病分类信息确定率，PIE 超过 Stable Diffusion Walk 和 Style-Based Manifold Extrapolation 等方法。用户测试中，76.2% 的反馈表明生成的进程准确性高。PIE 是首个实现真实标准的疾病进展图像生成方法，有望在医疗机构中应用，改善病人结果。<details>
<summary>Abstract</summary>
Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease Classification Confidence (Alignment). Our user study collected feedback from 35 veteran physicians to assess the generated progressions. Remarkably, 76.2% of the feedback agrees with the fidelity of the generated progressions. To our best knowledge, PIE is the first of its kind to generate disease progression images meeting real-world standards. It is a promising tool for medical research and clinical practice, potentially allowing healthcare providers to model disease trajectories over time, predict future treatment responses, and improve patient outcomes.
</details>
<details>
<summary>摘要</summary>
疾病发展模拟是医学研究中的一个重要领域，具有诊断、预后和治疗中的重要意义。然而，现有的医学影像监测技术存在缺乏连续监测的问题，这限制了疾病发展模拟的准确性和可靠性。为解决这个问题，我们提出了一种新的框架，称为进程图像编辑（PIE），它可以控制疾病相关的图像特征，实现精准和现实的疾病发展模拟。具体来说，我们利用了最新的文本生成图像技术，模拟疾病发展的过程，并为每个患者个性化模拟。我们对PIE框架的迭代纠正过程进行了理论分析，认为它可以视为一种梯度下降算法，其学习率逐渐减少。为验证PIE框架，我们在医学影像领域进行了三项实验。我们的结果显示，PIE框架比现有的方法，如稳定扩散步和基于CLIP的Style-Based Manifold Extrapolation，在CLIP分数（现实）和疾病分类信心度（对齐）方面具有更高的超越性。我们的用户研究收集了35名 veteran physician 的反馈，评估生成的进程是否准确。结果显示，76.2%的反馈同意生成的进程准确性。到我们知道的 extend，PIE是首个满足现代医学标准的疾病发展图像生成框架。它是一种有前途的工具，可以帮助医疗专业人员模拟疾病轨迹，预测未来治疗响应，并提高患者的结果。
</details></li>
</ul>
<hr>
<h2 id="CPR-Coach-Recognizing-Composite-Error-Actions-based-on-Single-class-Training"><a href="#CPR-Coach-Recognizing-Composite-Error-Actions-based-on-Single-class-Training" class="headerlink" title="CPR-Coach: Recognizing Composite Error Actions based on Single-class Training"></a>CPR-Coach: Recognizing Composite Error Actions based on Single-class Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11718">http://arxiv.org/abs/2309.11718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunli Wang, Qing Yu, Shuaibing Wang, Dingkang Yang, Liuzhen Su, Xiao Zhao, Haopeng Kuang, Peixuan Zhang, Peng Zhai, Lihua Zhang</li>
<li>for: 这篇论文的目的是为了提高细部医疗动作分析和技能评估，特别是在紧急救援中Cardiopulmonary Resuscitation（CPR）技能的评估。</li>
<li>methods: 本文使用了视觉基于的系统来完成CPR动作识别和技能评估，并开发了名为CPR-Coach的视频数据集。另外，本文还提出了一个人类认知驿站（ImagineNet）框架，以解决单一类别训练和多类别测试的问题。</li>
<li>results: 实验结果显示，ImagineNet框架能够提高模型的多错识别性，并且在受限监督下进行训练。<details>
<summary>Abstract</summary>
The fine-grained medical action analysis task has received considerable attention from pattern recognition communities recently, but it faces the problems of data and algorithm shortage. Cardiopulmonary Resuscitation (CPR) is an essential skill in emergency treatment. Currently, the assessment of CPR skills mainly depends on dummies and trainers, leading to high training costs and low efficiency. For the first time, this paper constructs a vision-based system to complete error action recognition and skill assessment in CPR. Specifically, we define 13 types of single-error actions and 74 types of composite error actions during external cardiac compression and then develop a video dataset named CPR-Coach. By taking the CPR-Coach as a benchmark, this paper thoroughly investigates and compares the performance of existing action recognition models based on different data modalities. To solve the unavoidable Single-class Training & Multi-class Testing problem, we propose a humancognition-inspired framework named ImagineNet to improve the model's multierror recognition performance under restricted supervision. Extensive experiments verify the effectiveness of the framework. We hope this work could advance research toward fine-grained medical action analysis and skill assessment. The CPR-Coach dataset and the code of ImagineNet are publicly available on Github.
</details>
<details>
<summary>摘要</summary>
《细腔医学动作分析任务在图像识别领域内已经吸引了广泛的关注，但是它面临着数据和算法不足的问题。心肺复苏（CPR）是紧急情况下的重要技能之一，现在CPR技能评估主要靠假人和教练进行，导致训练成本高、效率低。本文首次构建了一个视觉基于的系统，用于完成CPR动作识别和技能评估。特别是，我们定义了13种单个错误动作和74种复合错误动作 durante la compressión cardíaca externa，并开发了名为CPR-Coach的视频数据集。通过使用CPR-Coach作为标准，本文对现有动作识别模型基于不同数据模式进行了广泛的 investigate 和比较。为解决不可避免的单类训练和多类测试问题，我们提出了一个人类认知 inspirited 框架名为ImagineNet，以提高模型的多错误识别性能。广泛的实验证明了效果性。我们希望这项工作能够推动细腔医学动作分析和技能评估的研究进步。CPR-Coach数据集和ImagineNet框架的代码都公开可用于GitHub。
</details></li>
</ul>
<hr>
<h2 id="Deshadow-Anything-When-Segment-Anything-Model-Meets-Zero-shot-shadow-removal"><a href="#Deshadow-Anything-When-Segment-Anything-Model-Meets-Zero-shot-shadow-removal" class="headerlink" title="Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal"></a>Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11715">http://arxiv.org/abs/2309.11715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao</li>
<li>for: Image shadow removal and image restoration.</li>
<li>methods: Deshadow-Anything model, Fine-tuning on large-scale datasets, diffusion model, Multi-Self-Attention Guidance (MSAG), and adaptive input perturbation (DDPM-AIP).</li>
<li>results: Effective improvement in image restoration performance.Here’s the simplified Chinese text:</li>
<li>for: 图像阴影除去和图像修复。</li>
<li>methods: Deshadow-Anything模型、大规模数据集微调、扩散模型、多自注意导航（MSAG）和适应输入扰动（DDPM-AIP）。</li>
<li>results: 图像修复性能得到明显改进。<details>
<summary>Abstract</summary>
Segment Anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.
</details>
<details>
<summary>摘要</summary>
segments anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.Here's the breakdown of the text in Simplified Chinese:segments anything (SAM)：这是一个先进的通用图像分割模型，通过一个庞大的视觉数据集进行训练，为图像分割和计算机视觉设置了新的标准。However, it faced challenges when it came to distinguishing between shadows and their backgrounds：这个模型在分割阴影和背景之间困难。To address this, we developed Deshadow-Anything：为解决这个问题，我们开发了Deshadow-Anything。considering the generalization of large-scale datasets：我们考虑了大规模数据集的通用性。and we performed Fine-tuning on large-scale datasets to achieve image shadow removal：我们在大规模数据集上进行细化调整，以实现图像阴影除去。The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image：涉游模型可以在图像的边缘和Texture上扩散，帮助去除阴影，保留图像的细节。Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion：我们还设计了多重自我注意力指导（MSAG）和适应输入扰动（DDPM-AIP），以加速涉游训练的迭代速度。Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance：实验表明，这些方法可以有效提高图像恢复性能。
</details></li>
</ul>
<hr>
<h2 id="MoDA-Leveraging-Motion-Priors-from-Videos-for-Advancing-Unsupervised-Domain-Adaptation-in-Semantic-Segmentation"><a href="#MoDA-Leveraging-Motion-Priors-from-Videos-for-Advancing-Unsupervised-Domain-Adaptation-in-Semantic-Segmentation" class="headerlink" title="MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation"></a>MoDA: Leveraging Motion Priors from Videos for Advancing Unsupervised Domain Adaptation in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11711">http://arxiv.org/abs/2309.11711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Pan, Xu Yin, Seokju Lee, Sungeui Yoon, In So Kweon</li>
<li>for: 这篇论文的目的是提出一个更实用的领域对应（UDA）方法，用于处理 semantic segmentation 任务中缺乏目标领域标签的问题。</li>
<li>methods: 这篇论文使用了自我指导学习，将 object motion 自然地从无标签的影像中学习出高效的表示。</li>
<li>results: 实验结果显示，MoDA 比已有方法更有效地处理领域对应题，并且可以与现有的 state-of-the-art 方法相结合以进一步提高性能。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) is an effective approach to handle the lack of annotations in the target domain for the semantic segmentation task. In this work, we consider a more practical UDA setting where the target domain contains sequential frames of the unlabeled videos which are easy to collect in practice. A recent study suggests self-supervised learning of the object motion from unlabeled videos with geometric constraints. We design a motion-guided domain adaptive semantic segmentation framework (MoDA), that utilizes self-supervised object motion to learn effective representations in the target domain. MoDA differs from previous methods that use temporal consistency regularization for the target domain frames. Instead, MoDA deals separately with the domain alignment on the foreground and background categories using different strategies. Specifically, MoDA contains foreground object discovery and foreground semantic mining to align the foreground domain gaps by taking the instance-level guidance from the object motion. Additionally, MoDA includes background adversarial training which contains a background category-specific discriminator to handle the background domain gaps. Experimental results on multiple benchmarks highlight the effectiveness of MoDA against existing approaches in the domain adaptive image segmentation and domain adaptive video segmentation. Moreover, MoDA is versatile and can be used in conjunction with existing state-of-the-art approaches to further improve performance.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（USDA）是一种有效的方法，用于处理目标领域中缺乏标注的问题。在这项工作中，我们考虑了更实用的USDA设定，其中目标领域包含序列帧的无标注视频，这些视频易于在实践中收集。一项latest study suggests self-supervised learning of object motion from unlabeled videos with geometric constraints。我们设计了一个基于自我指导的领域适应Semantic segmentation框架（MoDA），该框架利用无标注视频中的自我指导对象运动来学习有效的表示。MoDA与之前的方法不同，它不使用目标领域帧的时间一致约束。相反，MoDA在前景和背景类别上分别进行领域对接，使用不同的策略。具体来说，MoDA包括前景 объек discovery和前景semantic mining，用于对接前景领域的差距。此外，MoDA还包括背景对抗培训，其中包括一个特定于背景类别的挑战器，用于处理背景领域的差距。实验结果表明，MoDA在多个benchmark上比既有approaches更有效，并且MoDA可以与现有的state-of-the-art方法相结合，以进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Long-Short-Temporal-Attention-Network-for-Unsupervised-Video-Object-Segmentation"><a href="#Efficient-Long-Short-Temporal-Attention-Network-for-Unsupervised-Video-Object-Segmentation" class="headerlink" title="Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation"></a>Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11707">http://arxiv.org/abs/2309.11707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Li, Yu Zhang, Li Yuan, Huaxin Xiao, Binbin Lin, Xianghua Xu</li>
<li>For: Unsupervised video object segmentation (VOS) in real-time, without prior knowledge.* Methods: Proposed Long-Short Temporal Attention (LSTA) network, consisting of Long Temporal Memory and Short Temporal Attention modules, with efficient projection and locality-based sliding window techniques for speedup.* Results: Promising performances on several benchmarks with high efficiency.<details>
<summary>Abstract</summary>
Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial-temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (termed LSTA) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.
</details>
<details>
<summary>摘要</summary>
Unsupervised Video Object Segmentation (VOS) targets identifying primary foreground object contours in videos without prior knowledge. However, previous methods do not fully utilize spatial-temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient Long-Short Temporal Attention network (LSTA) for unsupervised VOS from a holistic view. Specifically, LSTA consists of two dominant modules: Long Temporal Memory and Short Temporal Attention. The former captures long-term global pixel relations of past frames and the current frame, modeling constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals short-term local pixel relations of one nearby frame and the current frame, modeling moving objects by encoding motion pattern. To speed up inference, efficient projection and locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.
</details></li>
</ul>
<hr>
<h2 id="Meta-OOD-Learning-for-Continuously-Adaptive-OOD-Detection"><a href="#Meta-OOD-Learning-for-Continuously-Adaptive-OOD-Detection" class="headerlink" title="Meta OOD Learning for Continuously Adaptive OOD Detection"></a>Meta OOD Learning for Continuously Adaptive OOD Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11705">http://arxiv.org/abs/2309.11705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinheng Wu, Jie Lu, Zhen Fang, Guangquan Zhang</li>
<li>for: 这篇论文目的是为了提出一种基于流动分布的假值检测方法，以适应现实世界中的动态和不断变化的分布。</li>
<li>methods: 本文使用了元学习的方法，包括设计了一个学习到适应图表，以便在训练过程中初始化一个好的假值检测模型，并在测试过程中快速适应新的分布。</li>
<li>results: 实验结果显示，本文的方法可以保持ID类别准确率和假值检测性能，在流动分布下进行测试。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is crucial to modern deep learning applications by identifying and alerting about the OOD samples that should not be tested or used for making predictions. Current OOD detection methods have made significant progress when in-distribution (ID) and OOD samples are drawn from static distributions. However, this can be unrealistic when applied to real-world systems which often undergo continuous variations and shifts in ID and OOD distributions over time. Therefore, for an effective application in real-world systems, the development of OOD detection methods that can adapt to these dynamic and evolving distributions is essential. In this paper, we propose a novel and more realistic setting called continuously adaptive out-of-distribution (CAOOD) detection which targets on developing an OOD detection model that enables dynamic and quick adaptation to a new arriving distribution, with insufficient ID samples during deployment time. To address CAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt diagram such that a good initialized OOD detection model is learned during the training process. In the testing process, MOL ensures OOD detection performance over shifting distributions by quickly adapting to new distributions with a few adaptations. Extensive experiments on several OOD benchmarks endorse the effectiveness of our method in preserving both ID classification accuracy and OOD detection performance on continuously shifting distributions.
</details>
<details>
<summary>摘要</summary>
现代深度学习应用中，外部分布（OOD）检测是关键性能的一部分，可以识别并警示不应该进行测试或预测的外部样本。现有的OOD检测方法在固定分布下已经做出了重要的进步。然而，这可能是不切实际的，因为实际系统经常发生连续变化和分布的更新。因此，为了有效应用于实际系统，需要开发一种能够适应动态和演变分布的OOD检测方法。在这篇论文中，我们提出了一种新的设定，即连续适应外部分布（CAOOD）检测，旨在开发一种能够在部署时间内动态适应新到达的分布，并且只需要很少的标注样本。为了解决CAOOD，我们开发了元外部分布学习（MOL），它通过设计学习适应图来使得一个初始化好的OOD检测模型在训练过程中快速适应新的分布。在测试过程中，MOL确保OOD检测性能在分布Shift过程中保持高效，只需要很少的适应。我们在多个OOD benchmark上进行了广泛的实验，并证明了我们的方法可以保持ID分类精度和OOD检测性能在连续变化的分布下。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.CV_2023_09_21/" data-id="clmvt7t9s00de26rd2ghrbp2n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/21/cs.SD_2023_09_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-21
        
      </div>
    </a>
  
  
    <a href="/2023/09/21/cs.AI_2023_09_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-09-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">21</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
