
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-13 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07120 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ucsc-vlaa&#x2F;sight-beyond-text paper_authors: Haoq">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-13">
<meta property="og:url" content="https://nullscc.github.io/2023/09/13/cs.LG_2023_09_13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07120 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ucsc-vlaa&#x2F;sight-beyond-text paper_authors: Haoq">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-13T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:20.662Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/13/cs.LG_2023_09_13/" class="article-date">
  <time datetime="2023-09-13T10:00:00.000Z" itemprop="datePublished">2023-09-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-13
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sight-Beyond-Text-Multi-Modal-Training-Enhances-LLMs-in-Truthfulness-and-Ethics"><a href="#Sight-Beyond-Text-Multi-Modal-Training-Enhances-LLMs-in-Truthfulness-and-Ethics" class="headerlink" title="Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics"></a>Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07120">http://arxiv.org/abs/2309.07120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/sight-beyond-text">https://github.com/ucsc-vlaa/sight-beyond-text</a></li>
<li>paper_authors: Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie</li>
<li>for: 这项研究旨在探讨多模态大语言模型（MLLMs）的准确性和道德Alignment在纯文本处理领域的能力。</li>
<li>methods: 研究人员使用视觉指令调整，一种常见的将文本模型转化为多模态模型的策略，并证明这种方法可以帮助模型在纯文本处理领域达到更高的准确性和道德Alignment。</li>
<li>results: 研究人员发现，视觉指令调整可以使模型在TruthfulQA-mc和Ethics benchmark上表现更好，比如一个视觉指令调整的LLaMA2 7B模型超过了一个基于一百万人注释的LLaMA2-chat 7B模型的性能。进一步分析表明，这种改善的Alignment可以归功于视觉文本数据中的更高质量指令。<details>
<summary>Abstract</summary>
Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at github.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration into the intrinsic value of visual-text synergies and, in a broader scope, multi-modal interactions in alignment research.
</details>
<details>
<summary>摘要</summary>
多模态大语言模型（MLLM）通过大语言模型（LLM）进行训练，具有更好地理解多模态输入并生成文本响应的能力。虽然它们在多模态任务中表现出色，但纯NLP能力的MLLM often underestimated 和未经测试。在这个研究中，我们跳出了obox 和探索了一个MLLM的惊喜特征——视觉指令调整，一种常见的将LLM转换为MLLM的策略。我们发现，这种策略不仅能够提高模型的真实性和道德跟进性，而且在纯NLP上也能够达到这些目标。例如，一个视觉指令调整的 LLaMA2 7B 模型在 TruthfulQA-mc 和 Ethics 标准上超越了一个 fine-tuned 的 LLaMA2-chat 7B 模型，这个模型在一百万个人注释后进行了微调。进一步的分析表明，改进的协调可以归功于视觉文本数据中的高质量指令。在发布我们的代码在github.com/UCSC-VLAA/Sight-Beyond-Text 上，我们希望能够激发更多人对多模态交互和视觉文本的内在价值进行进一步的探索，以及在更广泛的范围内进行对适应性研究。
</details></li>
</ul>
<hr>
<h2 id="PILOT-A-Pre-Trained-Model-Based-Continual-Learning-Toolbox"><a href="#PILOT-A-Pre-Trained-Model-Based-Continual-Learning-Toolbox" class="headerlink" title="PILOT: A Pre-Trained Model-Based Continual Learning Toolbox"></a>PILOT: A Pre-Trained Model-Based Continual Learning Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07117">http://arxiv.org/abs/2309.07117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sun-hailong/lamda-pilot">https://github.com/sun-hailong/lamda-pilot</a></li>
<li>paper_authors: Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan</li>
<li>for: 本研究旨在开发基于预训练模型的不间断学习工具箱（PILOT），以满足实际场景中新数据的进来。</li>
<li>methods: 本研究使用了state-of-the-art的预训练模型基于的分类不间断学习算法，如L2P、DualPrompt和CODA-Prompt，同时也将典型的分类不间断学习算法（如DER、FOSTER和MEMO）置于预训练模型的 Context中进行评估。</li>
<li>results: 本研究通过PILOT实现了一个可靠的、灵活的和高效的不间断学习工具箱，可以帮助研究人员更好地适应实际场景中的新数据进来。<details>
<summary>Abstract</summary>
While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the context of pre-trained models to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
Traditional machine learning 可以有效地解决各种问题，但它主要在封闭世界中运行，这限制了对流动数据的处理。为了解决实际情况，增量学习出现了，它可以适应新数据的到来。最近，预训练得到了广泛的关注和进步，这些预训练模型（PTM）在实际场景中表现出了强大的能力。因此，利用 PTM 进行增量学习的研究已成为必要。这篇文章介绍了基于预训练模型的增量学习工具箱，称为 PILOT。一方面，PILOT 实现了一些状态之arte class-incremental learning算法，基于预训练模型，如 L2P、DualPrompt 和 CODA-Prompt。另一方面，PILOT 还可以在预训练模型中适应典型的 class-incremental learning算法（例如 DER、FOSTER 和 MEMO），以评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Learning-for-Audio-Visual-Speaker-Verification"><a href="#Weakly-Supervised-Multi-Task-Learning-for-Audio-Visual-Speaker-Verification" class="headerlink" title="Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification"></a>Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07115">http://arxiv.org/abs/2309.07115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anith Selvakumar, Homa Fashandi</li>
<li>for: 本研究旨在实现robust多modal人体表示，optimized for open-set audio-visual speaker verification。</li>
<li>methods: 我们explored multitask learning techniques和Generalized end-to-end loss (GE2E) approach，以增强distance metric learning (DML)的性能。</li>
<li>results: 我们的网络实现了speaker verification的state of the art performance，reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on VoxCeleb1-O&#x2F;E&#x2F;H。<details>
<summary>Abstract</summary>
In this paper, we present a methodology for achieving robust multimodal person representations optimized for open-set audio-visual speaker verification. Distance Metric Learning (DML) approaches have typically dominated this problem space, owing to strong performance on new and unseen classes. In our work, we explored multitask learning techniques to further boost performance of the DML approach and show that an auxiliary task with weak labels can increase the compactness of the learned speaker representation. We also extend the Generalized end-to-end loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce a non-synchronous audio-visual sampling random strategy during training time that has shown to improve generalization. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official trial lists of VoxCeleb1-O/E/H, which is to our knowledge, the best published results on VoxCeleb1-E and VoxCeleb1-H.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来实现可靠的多Modal人表示，优化为开放集 audio-visual speaker验证。通常，Distance Metric Learning（DML）方法在这个问题空间中占据主导地位，因为它在新和未经见过的类型上表现强。在我们的工作中，我们explored multitask learning技术来进一步提高DML方法的性能，并证明了一个 auxiliary task 的弱标签可以增加学习的说话人表示的紧凑性。此外，我们扩展了Generalized end-to-end loss（GE2E）到多Modal输入，并证明它可以在 audio-visual 空间 achieve competitive performance。最后，我们引入了异步 audio-visual 采样随机策略 durante training time，这已经显示了它可以提高泛化性。我们的网络实现了 speaker verification 的州OF the art表现，报告的 Equal Error Rate（EER）为0.244%、0.252% 和0.441% 在 VoxCeleb1-O/E/H 的三个官方试用列表上，这是我们所知道的最佳发表结果。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Deep-Encoding-Enables-Uncertainty-aware-Machine-learning-assisted-Histopathology"><a href="#Contrastive-Deep-Encoding-Enables-Uncertainty-aware-Machine-learning-assisted-Histopathology" class="headerlink" title="Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology"></a>Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07113">http://arxiv.org/abs/2309.07113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirhoshan Sivaroopan, Chamuditha Jayanga, Chalani Ekanayake, Hasindri Watawana, Jathurshan Pradeepkumar, Mithunjha Anandakumar, Ranga Rodrigo, Chamira U. S. Edussooriya, Dushan N. Wadduwage</li>
<li>for: 这个论文的目的是用深度神经网络模型从百万个 histopathology 图像中学习临床有用的特征。</li>
<li>methods: 这篇论文使用了大量的公共领域数据进行预训练，然后使用一小部分标注数据进行精度训练。另外，它还提出了一种不确定性感知损失函数，以衡量模型在推断中的信任度。</li>
<li>results: 这篇论文的结果表明，使用预训练和不确定性感知损失函数可以达到现有最佳实践（SOTA）的水平，只需使用1-10%的随机标注数据。此外，它还证明了在整个扫描图像分类任务中，使用预训练后的模型可以超过现有最佳实践。<details>
<summary>Abstract</summary>
Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select the best instances to label for further training. Our uncertainty-aware labeling reaches the SOTA with significantly fewer annotations compared to random labeling. Last, we demonstrate how our pre-trained encoders can surpass current SOTA for whole-slide image classification with weak supervision. Our work lays the foundation for data and task-agnostic pre-trained deep networks with quantified uncertainty.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "histopathology images" 转换为 " histopathology 图像"* "clinically relevant features" 转换为 "临床有用的特征"* "terabytes of training data" 转换为 "训练数据的天terabytes"* "pre-train deep networks" 转换为 "预训练深度网络"* "fine-tune" 转换为 "精细调整"* "patch-level classification" 转换为 "幂分类"* "whole-slide image classification" 转换为 "整幅图像分类"* "weak supervision" 转换为 "弱监督"* "uncertainty-aware loss function" 转换为 "不确定性认知的损失函数"* "quantified uncertainty" 转换为 "量化不确定性"
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-via-Subgroup-Mixup-for-Improving-Fairness"><a href="#Data-Augmentation-via-Subgroup-Mixup-for-Improving-Fairness" class="headerlink" title="Data Augmentation via Subgroup Mixup for Improving Fairness"></a>Data Augmentation via Subgroup Mixup for Improving Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07110">http://arxiv.org/abs/2309.07110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madeline Navarro, Camille Little, Genevera I. Allen, Santiago Segarra</li>
<li>for: 提高群体公平性（improve group fairness）</li>
<li>methods: 使用对称混合（pairwise mixup）方法来增强训练数据，并且使用混合来增强决策边界的公平性（fairness）和准确性（accuracy）</li>
<li>results: 比较于现有的数据增强和偏见缓解方法，提高了群体公平性和准确性（improved group fairness and accuracy）<details>
<summary>Abstract</summary>
In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to under-representation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了通过对 subgroup 进行对应混合来提高群体公平性。许多现实世界中机器学习系统的应用受到 subgroup 之间的偏见的影响，这是因为这些 subgroup 的数据不充分表示社会偏见。 inspirited by mixup 的成功，我们开发了对应混合方案来增强训练数据，并促进所有 subgroup 的公平和准确的决策边界。通过数据增强来提高群体公平性，我们可以添加更多的受 represened subgroup 的样本来均衡 subgroup。此外，我们的方法可以利用混合的通用能力来提高公平性和准确性。我们在synthetic simulations和实际世界中的benchmark fair classification数据上比较了我们的提议混合和现有的数据增强和偏见缓解方法，结果表明我们能够实现公平的结果，同时甚至提高准确性。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Speed-Performance-of-Multi-Agent-Reinforcement-Learning"><a href="#Characterizing-Speed-Performance-of-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Characterizing Speed Performance of Multi-Agent Reinforcement Learning"></a>Characterizing Speed Performance of Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07108">http://arxiv.org/abs/2309.07108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna</li>
<li>for: 这篇论文的目的是分析多智能体强化学习（MARL）算法的速度性能，以及将来文献中该metric的重要性。</li>
<li>methods: 该论文使用了一种分类方法来分类MARL算法，并对三种目前顶尖MARL算法进行系统性的性能分析，以找出它们的性能瓶颈。</li>
<li>results: 该论文发现，MARL算法的速度性能是一个关键瓶颈，并且提出了将来文献中关于MARL算法的性能评价应该包括速度性能的要求。<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform. We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.
</details>
<details>
<summary>摘要</summary>
In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform.We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Group-Bias-in-Federated-Learning-for-Heterogeneous-Devices"><a href="#Mitigating-Group-Bias-in-Federated-Learning-for-Heterogeneous-Devices" class="headerlink" title="Mitigating Group Bias in Federated Learning for Heterogeneous Devices"></a>Mitigating Group Bias in Federated Learning for Heterogeneous Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07085">http://arxiv.org/abs/2309.07085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khotso Selialia, Yasra Chandio, Fatima M. Anwar</li>
<li>for: 这篇论文旨在提出一个隐私保护的聚合学习框架，以减少对特定社群或群体的不公平偏袋。</li>
<li>methods: 本文提出使用平均条件概率来计算跨领域群体重要性加权，以优化具有最差性能的群体表现。此外，本文还提出了调整技术来最小化最差和最好表现之间的差异，以保持对偏袋的平衡。</li>
<li>results: 本文的评估结果显示，在真实世界的不同领域和环境下，该框架能够实现公平的决策。<details>
<summary>Abstract</summary>
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.   Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance weights} derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while making sure through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of human emotion recognition and image classification benchmarks assesses the fair decision-making of our framework in real-world heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
《联合学习》在分布式边缘应用中emerging为一种隐私保护的模型训练方法。因此，大多数边缘部署都是不同的 nature，即感应能力和环境。这个边缘多样性违反了本地数据的独立性和相同分布（IID）性，从而导致模型偏好和不公正决策。现有的偏好缓和技术仅对于标签多样性中的偏好而不考虑特定领域的多样性和特性，并未Address全球集体公平性性。我们的工作提出了一个集体公平的联合学习框架，可以降低集体偏好而保持隐私和资源利用 overhead。我们的主要想法是利用average conditional probabilities来计算跨Domain的集体重要性，从不同的训练数据中提取出对各个集体的优化性能。此外，我们也提出了调整技术来降低最差和最好的集体之间的差异，同时通过阈值机制来确保偏好缓和和集体性能的平衡。我们在人们情感识别和图像分类benchmark中进行了真实世界多样的评估，以评估我们的框架在实际中的公平决策。
</details></li>
</ul>
<hr>
<h2 id="The-Boundaries-of-Verifiable-Accuracy-Robustness-and-Generalisation-in-Deep-Learning"><a href="#The-Boundaries-of-Verifiable-Accuracy-Robustness-and-Generalisation-in-Deep-Learning" class="headerlink" title="The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning"></a>The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07072">http://arxiv.org/abs/2309.07072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Bastounis, Alexander N. Gorban, Anders C. Hansen, Desmond J. Higham, Danil Prokhorov, Oliver Sutton, Ivan Y. Tyukin, Qinghua Zhou</li>
<li>for: 本研究探讨了神经网络在分类任务中确定稳定性和准确性的理论限制。</li>
<li>methods: 我们考虑了经典的分布不依赖性框架和算法，以最小化实际风险并可能受到权重正则化。</li>
<li>results: 我们发现了一大家族任务，其中计算和验证理想稳定和准确的神经网络在上述设置下是极为困难，甚至可能无法实现，即使理想的解在给定的神经网络架构中存在。<details>
<summary>Abstract</summary>
In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们评估了神经网络在分类任务中的理论限制。我们考虑了经典的分布不依赖于框架和算法，用来降低实际风险，并可能受到权重正则化的限制。我们显示出，存在一个大家庭任务，计算和验证理想稳定和准确的神经网络在上述设置下是极其困难，甚至可能无法完成，即使理想的解决方案存在于给定的神经网络架构中。
</details></li>
</ul>
<hr>
<h2 id="Deep-Quantum-Graph-Dreaming-Deciphering-Neural-Network-Insights-into-Quantum-Experiments"><a href="#Deep-Quantum-Graph-Dreaming-Deciphering-Neural-Network-Insights-into-Quantum-Experiments" class="headerlink" title="Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments"></a>Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07056">http://arxiv.org/abs/2309.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tareq Jaouni, Sören Arlt, Carlos Ruiz-Gonzalez, Ebrahim Karimi, Xuemei Gu, Mario Krenn</li>
<li>for: 这个论文旨在解释神经网络在量子光学实验中的学习过程和结果。</li>
<li>methods: 这篇论文使用了一种名为inception或deep dreaming的可解释AI技术，该技术在计算机视觉领域已经应用了。通过这种技术，我们可以探索神经网络对量子系统的学习。</li>
<li>results: 我们发现，神经网络可以将量子系统的初始性质分布Shift，并且可以描述神经网络学习的策略。 Interestingly,我们发现，在核心层次上，神经网络可以识别简单的性质，而在深层次上，它可以识别复杂的量子结构，甚至是量子紧密相关。这与计算机视觉领域已有的长期认知的性质相似。我们的方法可能可以用于开发更加解释性的AI基于科学发现技术。<details>
<summary>Abstract</summary>
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones, it can identify complex quantum structures and even quantum entanglement. This is in reminiscence of long-understood properties known in computer vision, which we now identify in a complex natural science task. Our approach could be useful in a more interpretable way to develop new advanced AI-based scientific discovery techniques in quantum physics.
</details>
<details>
<summary>摘要</summary>
儿童 neural networks 的透明性问题带来了解释新发现的挑战。我们使用一种名为 $inception$ 或 $deep$ $dreaming$ 的 explainable-AI（XAI）技术，这种技术在计算机视觉中发明。我们使用这种技术来探索 neural networks 学习 quantum optics 实验的内容。我们的故事开始于训练一个深度 neural networks 在量子系统的性质上。一旦训练完成，我们将 neural network “反转”，即问其如何假设一个具有特定性质的量子系统，并如何在改变性质时不断修改量子系统。我们发现，在早些层次中， neural network 可以快速地标识简单的性质，而在更深层次中，它可以标识复杂的量子结构和甚至量子共振。这与计算机视觉中已久认知的性质有很大相似之处。我们的方法可能会在更加可解的方式下发展出新的高级 AI 基于科学发现技术。
</details></li>
</ul>
<hr>
<h2 id="An-Extreme-Learning-Machine-Based-Method-for-Computational-PDEs-in-Higher-Dimensions"><a href="#An-Extreme-Learning-Machine-Based-Method-for-Computational-PDEs-in-Higher-Dimensions" class="headerlink" title="An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions"></a>An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07049">http://arxiv.org/abs/2309.07049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiran Wang, Suchuan Dong</li>
<li>for: 这两种方法可以帮助解决高维partial differential equations (PDE)问题，并且可以生成高精度的解决方案。</li>
<li>methods: 第一种方法使用随机化神经网络来表示未知解决场的数据，并通过将PDE和边界&#x2F;初始条件等约束表示为一个或多个随机点的系统来解决问题。第二种方法则是基于approximate theory of functional connections (A-TFC)来重新表示高维PDE问题，并使用随机神经网络来表示自由场函数。</li>
<li>results: 数据示出，这两种方法可以生成高精度的解决方案，并且相比Physics-informed neural network (PINN)方法，这两种方法更加成本效果和精度高。<details>
<summary>Abstract</summary>
We present two effective methods for solving high-dimensional partial differential equations (PDE) based on randomized neural networks. Motivated by the universal approximation property of this type of networks, both methods extend the extreme learning machine (ELM) approach from low to high dimensions. With the first method the unknown solution field in $d$ dimensions is represented by a randomized feed-forward neural network, in which the hidden-layer parameters are randomly assigned and fixed while the output-layer parameters are trained. The PDE and the boundary/initial conditions, as well as the continuity conditions (for the local variant of the method), are enforced on a set of random interior/boundary collocation points. The resultant linear or nonlinear algebraic system, through its least squares solution, provides the trained values for the network parameters. With the second method the high-dimensional PDE problem is reformulated through a constrained expression based on an Approximate variant of the Theory of Functional Connections (A-TFC), which avoids the exponential growth in the number of terms of TFC as the dimension increases. The free field function in the A-TFC constrained expression is represented by a randomized neural network and is trained by a procedure analogous to the first method. We present ample numerical simulations for a number of high-dimensional linear/nonlinear stationary/dynamic PDEs to demonstrate their performance. These methods can produce accurate solutions to high-dimensional PDEs, in particular with their errors reaching levels not far from the machine accuracy for relatively lower dimensions. Compared with the physics-informed neural network (PINN) method, the current method is both cost-effective and more accurate for high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
我们提出了两种有效的方法来解决高维度partial differential equation (PDE)，基于随机化神经网络。这两种方法都是基于extreme learning machine (ELM)的扩展，将ELM的方法从低维度扩展到高维度。在第一种方法中，不知的解析场在d维度上是由随机化的Feed-Forward神经网络表示，其隐藏层参数随机分配并固定，而出力层参数则是通过训练来学习。PDE和边界/初始条件，以及当地的连续条件（ для本地方法），都是在一组随机的内部/边界点上强制实现。从这个线性或非线性的代数系统中，通过最小二乘法解，获得训练完成的神经网络参数。在第二种方法中，高维度PDE问题被重新表述为一个受限的表述，基于一个 Approximate 的Functional Connections 理论（A-TFC），这样可以避免在维度增加时，TFC 中的指数增长。免�ayer 的自由场函数在 A-TFC 受限的表述中被随机化神经网络表示，通过一种与第一种方法相似的训练 процес来训练。我们提供了丰富的数据显示，用于训练这些方法，以及对高维度线性/非线性站立/动态 PDE 的应用。这些方法可以精确地解决高维度 PDE，特别是其误差可以达到机器精度的水平，尤其是维度较低的情况下。相比于Physics-Informed Neural Network 方法（PINN），现在的方法更加成本效益和精确。
</details></li>
</ul>
<hr>
<h2 id="Optimal-transport-distances-for-directed-weighted-graphs-a-case-study-with-cell-cell-communication-networks"><a href="#Optimal-transport-distances-for-directed-weighted-graphs-a-case-study-with-cell-cell-communication-networks" class="headerlink" title="Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks"></a>Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07030">http://arxiv.org/abs/2309.07030</a></li>
<li>repo_url: None</li>
<li>paper_authors: James S. Nagai, Ivan G. Costa, Michael T. Schaub</li>
<li>for:  comparing directed graphs based on optimal transport distances</li>
<li>methods:  earth movers distance (Wasserstein) and Gromov-Wasserstein (GW) distance</li>
<li>results: evaluation of two distance measures on simulated graph data and real-world directed cell-cell communication graphs, with discussion of relative performance<details>
<summary>Abstract</summary>
Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
</details>
<details>
<summary>摘要</summary>
对图像的优质运输比较已经收到了广泛的关注，因为优质运输所导出的距离提供了一种原理上的图像间距离度量，同时也提供了一种可解释的图像变化描述，基于一个运输计划。然而，由于不具有对称性，通常考虑的优质运输形式ulations往往难以应用于指定的图像。在这篇文章中，我们提出了两种用于比较指定图像的优质运输距离：（i）地球搬运距离（沃氏天然距离）和（ii）格罗莫夫-沃氏天然距离（GW距离）。我们评估了这两种距离的表现，并对 simulated 图像数据和实际的指定细胞通信图像进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Contrast-Consistent-Ranking-with-Language-Models"><a href="#Unsupervised-Contrast-Consistent-Ranking-with-Language-Models" class="headerlink" title="Unsupervised Contrast-Consistent Ranking with Language Models"></a>Unsupervised Contrast-Consistent Ranking with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06991">http://arxiv.org/abs/2309.06991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro, Rajarshi Bhowmik</li>
<li>for: 这个论文的目的是探讨语言模型中的排序知识，以及如何使用不同的提示技术来激活这种知识。</li>
<li>methods: 这篇论文使用了一种叫做对比一致搜索（CCS）的无监督探索方法，用于训练一个排序模型。</li>
<li>results: 实验结果表明，使用CCR探索方法可以比使用提示技术更好地激活语言模型中的排序知识，并且可以与更大的语言模型相比。<details>
<summary>Abstract</summary>
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression objective. Our results confirm that, for the same language model, CCR probing outperforms prompting and even performs on a par with prompting much larger language models.
</details>
<details>
<summary>摘要</summary>
语言模型含有排序知识，是Contextual Ranking任务的强大解决方案。例如，它们可能具有国家大小的参数知识或可以根据 sentiment 排序评论。现有的研究主要关注用于提取语言模型的排序知识的对话、点对点和列表技术。然而，我们发现，即使使用精心调整和受限的解码，提取技术可能并不总是自consistent的排名。这使我们探索一种不同的方法，即基于无监督探测方法Contrast-Consistent Search (CCS)的inspired的方法。这个想法是训练一个探测模型，其中模型对于一个陈述和其否定的表示都需要被映射到冲突真假极点一致的多个陈述中。我们 hypothesize 在排序任务中，所有的项目都是通过一致的对比或列表比较关联的。为此，我们将 binary CCS 方法扩展为 Contrast-Consistent Ranking (CCR)，并采用现有的排名方法，如最大margin损失、 triplet损失和ORDINAL REGRESSION objective。我们的结果表明，对于同一个语言模型，CCR探测超过提取和甚至与更大的语言模型相当。
</details></li>
</ul>
<hr>
<h2 id="MASTERKEY-Practical-Backdoor-Attack-Against-Speaker-Verification-Systems"><a href="#MASTERKEY-Practical-Backdoor-Attack-Against-Speaker-Verification-Systems" class="headerlink" title="MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"></a>MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06981">http://arxiv.org/abs/2309.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan</li>
<li>for: 这个论文旨在攻击移动系统中广泛使用的声音特征识别（Speaker Verification，SV）模型，以实现非法用户身份验证。</li>
<li>methods: 作者提出了一种名为“MASTERKEY”的后门攻击，利用攻击者没有受害者的知识的情况下，破坏SV模型。作者首先调查了现有恶意攻击模型的局限性，然后优化了一个通用的后门，可以攻击任意目标。然后，作者将说话者特征和 semantics信息embedded到后门中，使其隐蔽。最后，作者估算了通信扭曲并将其纳入后门中。</li>
<li>results: 作者验证了这种攻击在6种流行的SV模型上，总共恶意攻击53个模型，并使用触发器攻击310个目标说话者，包括53个恶意攻击模型中的310个目标说话者。攻击成功率为100%，且恶意攻击率为15%。当降低恶意攻击率到3%时，攻击成功率仍然约为50%。作者在3个实际场景中成功地实现了这种攻击，包括通过无线电和电话线进行攻击。<details>
<summary>Abstract</summary>
Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a 15% poison rate. By decreasing the poison rate to 3%, the attack success rate remains around 50%. We validate our attack in 3 real-world scenarios and successfully demonstrate the attack through both over-the-air and over-the-telephony-line scenarios.
</details>
<details>
<summary>摘要</summary>
speaker verification (SV) 广泛部署在移动系统中，以使用用户的声音特征来验证合法用户。在这项工作中，我们提出了一种后门攻击 named MASTERKEY，以破坏 SV 模型。与之前的攻击不同，我们在实际的实际情况下，假设攻击者没有受害者的任何知识。我们 Investigate 现有恶意攻击模型的局限性，然后优化一个通用的后门，可以攻击任意目标。接着，我们嵌入了发音人的特征和语义信息到后门中，使其隐蔽。最后，我们估算了通信扭曲，并将其纳入后门中。我们验证了我们的攻击，并在 6 个流行的 SV 模型上进行了验证。特别是，我们毒害了 53 个模型，并使用我们的触发器攻击 16,430 名注册用户，包括 310 名目标用户，其中 53 个模型中的每个模型都有 310 名目标用户。我们的攻击得到了 100% 的攻击成功率，但是降低毒害率到 3% 时，攻击成功率仍然保持在 50% 左右。我们在 3 个实际情况下验证了我们的攻击，并成功地通过了 both over-the-air 和 over-the-telephony-line 方式进行了攻击。
</details></li>
</ul>
<hr>
<h2 id="Auto-Regressive-Next-Token-Predictors-are-Universal-Learners"><a href="#Auto-Regressive-Next-Token-Predictors-are-Universal-Learners" class="headerlink" title="Auto-Regressive Next-Token Predictors are Universal Learners"></a>Auto-Regressive Next-Token Predictors are Universal Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06979">http://arxiv.org/abs/2309.06979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Malach</li>
<li>for: 这个论文旨在研究自然语言处理方面的语言模型，特别是使用下一个字符预测任务来实现逻辑和数学逻辑的能力。</li>
<li>methods: 该论文使用了一种名为Chain-of-Thought（CoT）数据集，并使用了线性next-token predictor和浅层多层感知器（MLP）来训练语言模型。</li>
<li>results: 实验表明，使用这些简单的模型可以非常高效地解决文本生成和数学任务，而这些能力归功于自动预测下一个字符的训练方法，而不是特定的架构选择。<details>
<summary>Abstract</summary>
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.
</details>
<details>
<summary>摘要</summary>
大型语言模型displayed出了很好的逻辑和数学理解能力，可以解决复杂的任务。奇怪的是，这些能力会在以下任务为基础的网络上缔造出来：下一个字 Predictor。在这个工作中，我们提出了一个理论框架来研究自动预测下一个字的网络。我们证明了，简单的模型，如线性下一个字预测模型，可以将任何由Turing机computed的函数高效地替换。我们引入了一个新的复杂度度量——字符串复杂度——用于度量CoT字串中用于替换某个目标函数所需的 intermediate tokens数量，并分析这些复杂度度量与其他的复杂度度量之间的关系。最后，我们显示了实验结果，证明了简单的下一个字预测模型，如线性网络和浅层多层感知机（MLP），在文本生成和数学任务上显示出了非常有趣的表现。我们的结果显示，语言模型的力量可以很大程度上归因于自动预测下一个字的训练方案，而不是特定的架构选择。
</details></li>
</ul>
<hr>
<h2 id="DNNShifter-An-Efficient-DNN-Pruning-System-for-Edge-Computing"><a href="#DNNShifter-An-Efficient-DNN-Pruning-System-for-Edge-Computing" class="headerlink" title="DNNShifter: An Efficient DNN Pruning System for Edge Computing"></a>DNNShifter: An Efficient DNN Pruning System for Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06973">http://arxiv.org/abs/2309.06973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blessonvar/dnnshifter">https://github.com/blessonvar/dnnshifter</a></li>
<li>paper_authors: Bailey J. Eccles, Philip Rodgers, Peter Kilpatrick, Ivor Spence, Blesson Varghese</li>
<li>for: 这个论文是为了解决深度神经网络（DNNs）在移动和嵌入式设备上进行推理时的资源问题，这些设备具有有限的计算和存储资源。</li>
<li>methods: 这篇论文提出了一种基于结构剔除的DNN预处理方法，可以快速生成适合移动和嵌入式设备进行推理的轻量级模型变体。</li>
<li>results: 论文的实验结果表明，DNNShifter可以在几乎无变化的时间和资源 overhead下，生成与原始 dense model 相似的准确性模型变体，并且可以快速将模型变体 swap 到不同的操作系统和网络条件下。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching system that addresses the challenges mentioned above. At the heart of DNNShifter is a novel methodology that prunes sparse models using structured pruning. The pruned model variants generated by DNNShifter are smaller in size and thus faster than dense and sparse model predecessors, making them suitable for inference at the edge while retaining near similar accuracy as of the original dense model. DNNShifter generates a portfolio of model variants that can be swiftly interchanged depending on operational conditions. DNNShifter produces pruned model variants up to 93x faster than conventional training methods. Compared to sparse models, the pruned model variants are up to 5.14x smaller and have a 1.67x inference latency speedup, with no compromise to sparse model accuracy. In addition, DNNShifter has up to 11.9x lower overhead for switching models and up to 3.8x lower memory utilisation than existing approaches.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在机器学习应用中扮演着重要角色。生产质量DNN模型在训练数百万个参数后可以达到高精度推理。这种情况带来了对Edge网络中的资源进行挑战，例如移动设备和嵌入式设备具有有限的计算和内存资源。为解决这个问题，模型会被剪辑，以创建轻量级、适用于这些设备的模型变体。现有的剪辑方法无法提供与原始模型无销毁的质量模型，或者需要过度时间和负担。我们的工作可以快速生成适合设备的模型变体，同时保持原始模型的准确性。这些模型变体可以快速交换，根据系统和网络条件变化，以适应工作荷负。本文介绍了DNNShifter，一个全流程DNN训练、空间剪辑和模型交换系统。DNNShifter的核心技术是基于结构剪辑的稀疏模型剪辑。剪辑后的模型变体由DNNShifter生成，比 dense和稀疏模型前任更小，更快，同时保持原始模型的准确性。DNNShifter生成了一个模型集合，可以根据操作条件快速交换。相比传统训练方法，DNNShifter可以生成剪辑后的模型变体，速度达93倍。相比稀疏模型，剪辑后的模型变体具有1.67倍的推理速度减少，同时保持稀疏模型的准确性。此外，DNNShifter的模型交换过程 overhead低至11.9倍，内存利用率下降至3.8倍。
</details></li>
</ul>
<hr>
<h2 id="Setting-the-Right-Expectations-Algorithmic-Recourse-Over-Time"><a href="#Setting-the-Right-Expectations-Algorithmic-Recourse-Over-Time" class="headerlink" title="Setting the Right Expectations: Algorithmic Recourse Over Time"></a>Setting the Right Expectations: Algorithmic Recourse Over Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06969">http://arxiv.org/abs/2309.06969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Fonseca, Andrew Bell, Carlo Abrate, Francesco Bonchi, Julia Stoyanovich</li>
<li>for: 研究算法决策系统在高度决策中的帮助</li>
<li>methods: 使用代理模型来研究环境不断变化对算法补救的影响</li>
<li>results: 发现只有特定参数化情况下的补救可以在时间上保持可靠性，需要进一步研究以确保补救努力得到奖励。<details>
<summary>Abstract</summary>
Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals.   In this work we propose an agent-based simulation framework for studying the effects of a continuously changing environment on algorithmic recourse. In particular, we identify two main effects that can alter the reliability of recourse for individuals represented by the agents: (1) competition with other agents acting upon recourse, and (2) competition with new agents entering the environment. Our findings highlight that only a small set of specific parameterizations result in algorithmic recourse that is reliable for agents over time. Consequently, we argue that substantial additional work is needed to understand recourse reliability over time, and to develop recourse methods that reward agents' effort.
</details>
<details>
<summary>摘要</summary>
算法系统常被召集来协助高度决策。由于这一点，算法补救（individuals should be able to take action against an undesirable outcome made by an algorithmic system）在receiving growing attention。现有大部分文献对于算法补救的研究都集中在如何为单个个体提供补救，而忽略了一个关键因素：Context changover time。忽略这个因素是一项重要的漏洞，因为补救通常包括个体首先尝试不成功，然后在后续时间获得一次或多次的机会。这可能创造false expectations，因为初始补救建议可能会变得 menos reliable over time due to model drift and competition for access to the favorable outcome between individuals。在这项工作中，我们提出一种基于代理人的模拟框架，用于研究算法补救在不断变化的环境中的效果。我们确定了两个主要的效果可能使补救无效的代理人：（1）代理人在补救时与其他代理人竞争，（2）新代理人入境环境时与其他代理人竞争。我们的发现表明，只有一小集特定的参数化可以在长期内保持代理人的补救可靠。因此，我们认为需要进一步的研究，以确定补救可靠性在时间上的变化，并开发补救方法， reward agents' effort。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Multiple-Description-for-DNA-based-data-storage"><a href="#Implicit-Neural-Multiple-Description-for-DNA-based-data-storage" class="headerlink" title="Implicit Neural Multiple Description for DNA-based data storage"></a>Implicit Neural Multiple Description for DNA-based data storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06956">http://arxiv.org/abs/2309.06956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trung Hieu Le, Xavier Pic, Jeremy Mateos, Marc Antonini</li>
<li>for: 这篇论文的目的是探讨用DNA作为数据存储媒体的可能性，并解决存储和生物操作引起的错误问题。</li>
<li>methods: 这篇论文使用了一种新的压缩方法和一种基于神经网络的多描述编码（MDC）技术来解决DNA数据存储中的错误问题。</li>
<li>results: 实验结果表明，这种解决方案在DNA数据存储方面具有竞争力，具有更高的压缩率和更好的雑音抗性。<details>
<summary>Abstract</summary>
DNA exhibits remarkable potential as a data storage solution due to its impressive storage density and long-term stability, stemming from its inherent biomolecular structure. However, developing this novel medium comes with its own set of challenges, particularly in addressing errors arising from storage and biological manipulations. These challenges are further conditioned by the structural constraints of DNA sequences and cost considerations. In response to these limitations, we have pioneered a novel compression scheme and a cutting-edge Multiple Description Coding (MDC) technique utilizing neural networks for DNA data storage. Our MDC method introduces an innovative approach to encoding data into DNA, specifically designed to withstand errors effectively. Notably, our new compression scheme overperforms classic image compression methods for DNA-data storage. Furthermore, our approach exhibits superiority over conventional MDC methods reliant on auto-encoders. Its distinctive strengths lie in its ability to bypass the need for extensive model training and its enhanced adaptability for fine-tuning redundancy levels. Experimental results demonstrate that our solution competes favorably with the latest DNA data storage methods in the field, offering superior compression rates and robust noise resilience.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effect-of-hyperparameters-on-variable-selection-in-random-forests"><a href="#Effect-of-hyperparameters-on-variable-selection-in-random-forests" class="headerlink" title="Effect of hyperparameters on variable selection in random forests"></a>Effect of hyperparameters on variable selection in random forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06943">http://arxiv.org/abs/2309.06943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imbs-hl/rf-hyperparameters-and-variable-selection">https://github.com/imbs-hl/rf-hyperparameters-and-variable-selection</a></li>
<li>paper_authors: Cesaire J. K. Fouodo, Lea L. Kronziel, Inke R. König, Silke Szymczak</li>
<li>for: 这个论文的目的是为了研究 Random Forest 算法中的 hyperparameter 的影响，以及这些参数对变量选择的影响。</li>
<li>methods: 这个论文使用了两个 simulation studies，一个使用理论分布，一个使用实际的基因表达数据，来评估 Random Forest 算法中的 hyperparameter 对变量选择的影响。</li>
<li>results: 研究发现，Random Forest 算法中的 hyperparameter mtry.prop 和 sample.fraction 对变量选择有着很大的影响，而 drawing strategy 和 minimal terminal node size 对变量选择的影响较小。此外，在不同的数据 correlations  Structures 下，适合的 hyperparameter 设置也会不同。<details>
<summary>Abstract</summary>
Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the correlation structure in the data. For weakly correlated predictor variables, the default value of mtry is optimal, but smaller values of sample.fraction result in larger sensitivity. In contrast, the difference in sensitivity of the optimal compared to the default value of sample.fraction is negligible for strongly correlated predictor variables, whereas smaller values than the default are better in the other settings. In conclusion, the default values of the hyperparameters will not always be suitable for identifying important variables. Thus, adequate values differ depending on whether the aim of the study is optimizing prediction performance or variable selection.
</details>
<details>
<summary>摘要</summary>
Random forests (RFs) 适用于预测模型和变量选择高维Omics研究中。RF算法中的hyperparameter对预测性能和变量重要性估计的影响已经被研究过。但是，RF算法中hyperparameter对变量选择的影响尚未清楚。我们通过两个 simulations studies使用理论分布和实际基因表达数据来评估hyperparameter的影响。我们评估选择重要变量的能力（敏感度），同时控制false discovery rate（FDR）。我们的结果表明，在training集中的分割候选变量的比例（mtry.prop）和训练集的样本分数（sample.fraction）对选择过程产生更大的影响，而不是在训练集中的抽样策略和最小终节点大小。一个适合的RF hyperparameter设置取决于数据中变量之间的相关性。对弱相关变量预测器，默认值的mtry是优化的，但是较小的sample.fraction会导致更大的敏感度。相反，对强相关变量预测器，默认值的sample.fraction的差异对预测性能没有很大的影响，但是较小的值比默认值更好。因此， defaults 的 hyperparameter 并不总是适合用于确定重要变量。因此，适合的 hyperparameter 的值取决于研究的目标是优化预测性能还是变量选择。
</details></li>
</ul>
<hr>
<h2 id="Collectionless-Artificial-Intelligence"><a href="#Collectionless-Artificial-Intelligence" class="headerlink" title="Collectionless Artificial Intelligence"></a>Collectionless Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06938">http://arxiv.org/abs/2309.06938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Gori, Stefano Melacci</li>
<li>for: 本研究目的是提出一种新的学习协议，使机器学习能够像人类一样在环境交互中获得智能能力。</li>
<li>methods: 本研究使用了环境互动中的数据来更新内部环境表示，不允许机器记录时间流信息。</li>
<li>results: 该方法可以推动机器学习发展自组织记忆技能，并且可以更好地处理隐私、控制和个性化问题。<details>
<summary>Abstract</summary>
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of self-organized memorization skills at a more abstract level, instead of relying on bare storage to simulate learning dynamics that are typical of offline learning algorithms. This purposely extreme position is intended to stimulate the development of machines that learn to dynamically organize the information by following human-based schemes. The proposition of this challenge suggests developing new foundations on computational processes of learning and reasoning that might open the doors to a truly orthogonal competitive track on AI technologies that avoid data accumulation by design, thus offering a framework which is better suited concerning privacy issues, control and customizability. Finally, pushing towards massively distributed computation, the collectionless approach to AI will likely reduce the concentration of power in companies and governments, thus better facing geopolitical issues.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)总之，职业处理大量数据的能力被认为是机器学习的基本成分，以及相关领域的spectacular achievements，同时也有一致的意见是关于中央数据收集的风险。这篇论文支持 Machine learning protocols that machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Therefore, it promotes the development of self-organized memorization skills at a more abstract level, instead of relying on bare storage to simulate learning dynamics that are typical of offline learning algorithms. This purposely extreme position is intended to stimulate the development of machines that learn to dynamically organize the information by following human-based schemes. The proposal of this challenge suggests developing new foundations on computational processes of learning and reasoning that might open the doors to a truly orthogonal competitive track on AI technologies that avoid data accumulation by design, thus offering a framework which is better suited concerning privacy issues, control and customizability. Finally, pushing towards massively distributed computation, the collectionless approach to AI will likely reduce the concentration of power in companies and governments, thus better facing geopolitical issues.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Dislocation-Dynamics-Data-Using-Semantic-Web-Technologies"><a href="#Modeling-Dislocation-Dynamics-Data-Using-Semantic-Web-Technologies" class="headerlink" title="Modeling Dislocation Dynamics Data Using Semantic Web Technologies"></a>Modeling Dislocation Dynamics Data Using Semantic Web Technologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06930">http://arxiv.org/abs/2309.06930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Zainul Ihsan, Said Fathalla, Stefan Sandfeld</li>
<li>for: 这个论文主要针对材料科学和工程领域中的晶体材料，包括金属和半导体材料。晶体材料中的一种常见缺陷是“杂点”，这种缺陷会影响材料的强度、裂解强度和塑性。</li>
<li>methods: 这篇论文使用了semantic web技术来模型来自杂点动力学 simulations的数据，并使用ontology来注解数据。研究人员将已有的杂点ontology扩展，添加了缺失的概念并与其他两个领域相关的 ontology（Elementary Multi-perspective Material Ontology和Materials Design Ontology）进行了对应，以便有效地表示杂点动力学数据。</li>
<li>results: 这篇论文通过创建了一个知识图表（DisLocKG）来表示杂点动力学数据的关系，并开发了一个SPARQL终结点，以便高度灵活地查询DisLocKG。<details>
<summary>Abstract</summary>
Research in the field of Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a distinct type of defect called "dislocation". This defect significantly affects various material properties, including strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behavior through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modeled using semantic web technologies through annotating data with ontologies. We extend the already existing Dislocation Ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspective Material Ontology and the Materials Design Ontology) allowing for representing the dislocation simulation data efficiently. Moreover, we show a real-world use case by representing the discrete dislocation dynamics data as a knowledge graph (DisLocKG) that illustrates the relationship between them. We also developed a SPARQL endpoint that brings extensive flexibility to query DisLocKG.
</details>
<details>
<summary>摘要</summary>
研究在材料科学和工程领域的重点是设计、合成、性能和表现的材料。一种广泛研究的材料是晶体材料，包括金属和半导体。晶体材料通常含有一种特定的缺陷，即“杂点”。这种缺陷对材料的各种性能产生重要影响，如强度、裂变强度和柔性。研究人员在过去几年中对杂点行为进行了大量的研究，包括实验测量技术和模拟。本文介绍了如何使用Semantic Web技术来模型来自杂点动力学 simulations的数据，包括使用ontology进行数据注释。我们将已经存在的杂点ontology扩展，添加缺失的概念并与其他两个领域相关的 ontology（即多元素物理材料 ontology和材料设计 ontology）进行对应，以便有效地表示杂点动力学数据。此外，我们还构建了一个知识图（DisLocKG），用于表示杂点动力学数据的关系。此外，我们还开发了一个 SPARQL 终点，以便高度灵活地查询 DisLocKG。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Impact-of-Action-Representations-in-Policy-Gradient-Algorithms"><a href="#Investigating-the-Impact-of-Action-Representations-in-Policy-Gradient-Algorithms" class="headerlink" title="Investigating the Impact of Action Representations in Policy Gradient Algorithms"></a>Investigating the Impact of Action Representations in Policy Gradient Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06921">http://arxiv.org/abs/2309.06921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Schneider, Pierre Schumacher, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler</li>
<li>for: 这篇论文探讨了在强化学习中行为表示的影响，以及不同分析技术的效果。</li>
<li>methods: 论文使用了多种分析技术，包括行为表示的变化对学习性能的影响。</li>
<li>results: 实验结果显示，行动表示可以很大程度上影响强化学习算法的学习性能，一些性能差异可以归因于优化困难度的变化。<details>
<summary>Abstract</summary>
Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continual-Learning-with-Dirichlet-Generative-based-Rehearsal"><a href="#Continual-Learning-with-Dirichlet-Generative-based-Rehearsal" class="headerlink" title="Continual Learning with Dirichlet Generative-based Rehearsal"></a>Continual Learning with Dirichlet Generative-based Rehearsal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06917">http://arxiv.org/abs/2309.06917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Zeng, Wei Xue, Qifeng Liu, Yike Guo</li>
<li>for: 这篇论文主要是为了解决数据驱动任务对话系统（ToDs）的不断学习问题，特别是因为计算限制和时间consuming的问题。</li>
<li>methods: 这篇论文提出了一种新的生成基于策略，称为Dirichlet Continual Learning（DCL），它使用Dirichlet分布来模型 latent prior 变量，从而高效地捕捉上一个任务的句子水平特征。此外， authors 还提出了 Jensen-Shannon Knowledge Distillation（JSKD）方法，用于增强知识传递 During pseudo sample 生成。</li>
<li>results:  experiments 表明，DCL 方法在意图检测和插入检测任务中表现出色，超过了现有方法的性能。<details>
<summary>Abstract</summary>
Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based knowledge distillation method that enhances knowledge transfer during pseudo sample generation. Our experiments confirm the efficacy of our approach in both intent detection and slot-filling tasks, outperforming state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-the-TopMost-A-Topic-Modeling-System-Toolkit"><a href="#Towards-the-TopMost-A-Topic-Modeling-System-Toolkit" class="headerlink" title="Towards the TopMost: A Topic Modeling System Toolkit"></a>Towards the TopMost: A Topic Modeling System Toolkit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06908">http://arxiv.org/abs/2309.06908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bobxwu/topmost">https://github.com/bobxwu/topmost</a></li>
<li>paper_authors: Xiaobao Wu, Fengjun Pan, Anh Tuan Luu</li>
<li>for: 这篇论文主要是为了提供一个全面的话题模型系统工具套件（Topic Modeling System Toolkit，简称TopMost），用于促进话题模型的研究和应用。</li>
<li>methods: 这篇论文使用了一种高度吸收和解耦的模块化设计，覆盖了更广泛的话题模型场景，包括数据预处理、模型训练、测试和评估。</li>
<li>results: 相比现有的工具套件，TopMost可以促进话题模型的快速使用、公正比较和扩展。这将促进话题模型的研究和应用进展。<details>
<summary>Abstract</summary>
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-Aware-Augmentations-for-Unsupervised-Online-General-Continual-Learning"><a href="#Domain-Aware-Augmentations-for-Unsupervised-Online-General-Continual-Learning" class="headerlink" title="Domain-Aware Augmentations for Unsupervised Online General Continual Learning"></a>Domain-Aware Augmentations for Unsupervised Online General Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06896">http://arxiv.org/abs/2309.06896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Romain Negrel, Giovanni Chierchia, Jean-François Bercher</li>
<li>for: 提高Unsupervised Online General Continual Learning (UOGCL)中的学习Agent忘记度，尤其是在没有类boundary或任务变化信息的情况下。</li>
<li>methods: 提出了一种使用流程依赖的数据增强和一些实现技巧来增强对比学习的内存使用情况，以提高UOGCL中的学习效果。</li>
<li>results: 比较其他无监督方法，本方法在所有考虑的设置中实现了最佳效果，并将监督学习和无监督学习之间的差异降低到最低。domain-aware增强程序可以与其他回放基于方法结合使用，这使得本方法成为可行的循环学习策略。<details>
<summary>Abstract</summary>
Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: continual learning 具有挑战，特别是在无监督enario中，如无监督在线通用 continual learning (UOGCL) 中，学习代理没有任务变化信息或类别边界信息的先前知识。而过去的研究主要集中在降低监督setup中的忘却。 latest studies have shown that self-supervised learners are more resistant to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well.
</details></li>
</ul>
<hr>
<h2 id="MagiCapture-High-Resolution-Multi-Concept-Portrait-Customization"><a href="#MagiCapture-High-Resolution-Multi-Concept-Portrait-Customization" class="headerlink" title="MagiCapture: High-Resolution Multi-Concept Portrait Customization"></a>MagiCapture: High-Resolution Multi-Concept Portrait Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06895">http://arxiv.org/abs/2309.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junha Hyung, Jaeyo Shin, Jaegul Choo</li>
<li>for: 这篇论文是针对如何使用几张自然像素来生成高品质的人像照片，并且将主题和风格概念融合到一起。</li>
<li>methods: 这篇论文使用了一种名为MagiCapture的个性化方法，通过将主题和风格概念融合到一起，生成高分辨率的人像照片。</li>
<li>results: 根据论文的评估，MagiCapture可以生成高品质的人像照片，并且比其他基准更高。它还可以在其他非人类物品上进行应用。<details>
<summary>Abstract</summary>
Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects.
</details>
<details>
<summary>摘要</summary>
大规模的文本到图像模型，包括稳定扩散，能够生成高品质、实际的人脸图像。有一个活跃的研究领域专门用于个性化这些模型，以生成特定主题或风格使用提供的参考图像。然而，尽管这些个性化方法可能会生成可信的结果，但它们通常会生成图像，具有不够的真实感和 comercial viability。这是特别明显在人脸图像生成中，因为人类的面部特征具有强烈的人类偏好。为解决这一问题，我们介绍了 MagiCapture，一种基于主题和风格概念的个性化方法，可以使用只需要几张主题和风格参考图像来生成高分辨率的人脸图像。例如，通过一些随机的自拍照，我们的精度调整后的模型可以生成高质量的人脸图像，例如护照照片或profile照片。主要挑战在这个任务中是缺乏compose的ground truth，导致最终输出质量下降和源主题的标识混乱。为解决这些问题，我们提出了一种新的注意力重新定向损失，以及auxiliary priors，它们都可以在弱相关学习Setting中Robust learning。我们的管道还包括额外的后处理步骤，以确保创造出高度真实的输出。 MagiCapture在量化和质量上的评价中表现出色，并且可以扩展到其他非人物对象。
</details></li>
</ul>
<hr>
<h2 id="Keep-It-SimPool-Who-Said-Supervised-Transformers-Suffer-from-Attention-Deficit"><a href="#Keep-It-SimPool-Who-Said-Supervised-Transformers-Suffer-from-Attention-Deficit" class="headerlink" title="Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?"></a>Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06891">http://arxiv.org/abs/2309.06891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bill Psomas, Ioannis Kakogeorgiou, Konstantinos Karantzalos, Yannis Avrithis<br>for:This paper aims to improve the performance of both convolutional and transformer encoders by developing a generic pooling framework and a simple attention-based pooling mechanism called SimPool.methods:The paper uses a generic pooling framework to formulate existing methods as instantiations and derives SimPool, a simple attention-based pooling mechanism that improves performance on pre-training and downstream tasks.results:The paper finds that SimPool improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases, regardless of whether the training is supervised or self-supervised. Additionally, the paper obtains attention maps of at least as good quality as self-supervised, without explicit losses or modifying the architecture, which is a new contribution.<details>
<summary>Abstract</summary>
Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?   In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.
</details>
<details>
<summary>摘要</summary>
“卷积网络和视transformer具有不同的对比式交互方式，包括层内卷积和网络结束的卷积。后者是否真的需要不同？作为卷积的副产品，视transformer提供了自然语言的空间注意力，但这通常是低质量的，除非是自我超视，这并未被研究得够。是超级视还是视的问题呢？在这项工作中，我们开发了一个通用的卷积框架，然后将一些现有的方法视为实体的实现。通过对每个组方法的质量进行讨论，我们 derivate SimPool，一种简单的注意力基于卷积机制，用于取代 convolutional和transformer核心Encoder中的默认卷积方法。我们发现，无论是有监督或自我监督，这种方法可以提高预训练和下游任务的性能，并提供了对象边界的注意力图。因此，我们可以称SimPool为通用的。我们知道，我们是第一个在有监督的情况下，通过不修改架构和不使用显式损失函数，从transformer中获得了至少等效的注意力图。代码在：https://github.com/billpsomas/simpool。”
</details></li>
</ul>
<hr>
<h2 id="ProMap-Datasets-for-Product-Mapping-in-E-commerce"><a href="#ProMap-Datasets-for-Product-Mapping-in-E-commerce" class="headerlink" title="ProMap: Datasets for Product Mapping in E-commerce"></a>ProMap: Datasets for Product Mapping in E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06882">http://arxiv.org/abs/2309.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kateřina Macková, Martin Pilát<br>for:这两个 datasets 用于评估产品 mapping 模型的性能，以填充现有的数据集中存在的缺失或只包含远方的产品对。methods:这两个 datasets 包含了图像和文本描述产品特性，并且通过两个零售商的爬虫抓取。非匹配产品被选择在两个阶段，创造了两种类型的非匹配 – 近似非匹配和中等非匹配。results:这两个 datasets 是一个完整的产品 mapping 数据集，可以用于评估和进一步研究产品 mapping 模型。这些数据集包含了许多详细的产品特性，例如品牌和价格，使得它们成为了产品 mapping 领域的黄金标准。<details>
<summary>Abstract</summary>
The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types of non-matches -- close non-matches and medium non-matches. Even the medium non-matches are pairs of products that are much more similar than non-matches in other datasets -- for example, they still need to have the same brand and similar name and price. After simple data preprocessing, several machine learning algorithms were trained on these and two the other datasets to demonstrate the complexity and completeness of ProMap datasets. ProMap datasets are presented as a golden standard for further research of product mapping filling the gaps in existing ones.
</details>
<details>
<summary>摘要</summary>
目标是判断两个电商平台上的两个产品是否描述同一种产品。现有的匹配和不匹配产品集合经常受到产品信息的不完整性或者只包含很遥距的不匹配产品的影响，因此训练在这些集合上的预测模型可以达到好的结果，但在实践中无法 distinguishing 非常相似的 но不匹配产品。本文介绍了两个新的产品映射 datasets：ProMapCz 和 ProMapEn，它们分别包含 1,495 个捷克产品对和 1,555 个英文产品对，由两个电商平台上的匹配和不匹配产品手动抽取。这些 datasets 包含产品图片和文本描述，包括产品规格信息，使其成为目前最完整的产品映射 datasets。此外，非匹配产品被选择在两个阶段，创造了两种类型的非匹配产品： close non-matches 和 medium non-matches。即使medium non-matches 与其他 datasets 中的非匹配产品不同，它们仍需要具有同一个品牌和相似的名称和价格。经过简单的数据处理后，多种机器学习算法被训练在这些和两个其他 datasets 上，以示 ProMap datasets 的复杂性和完整性。ProMap datasets 被提出为未来研究产品映射的 golden standard，填充现有的空白。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-control-of-self-assembly-of-quasicrystalline-structures-through-reinforcement-learning"><a href="#Dynamic-control-of-self-assembly-of-quasicrystalline-structures-through-reinforcement-learning" class="headerlink" title="Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning"></a>Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06869">http://arxiv.org/abs/2309.06869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uyen Tu Lieu, Natsuhiko Yoshinaga</li>
<li>for: 控制动力自组装的多十二角晶体（DDQC）从粒子杂化的方向</li>
<li>methods: 使用反射学习控制温度，通过Q学习方法获得最佳策略</li>
<li>results: 通过反射学习获得的温度规则可以更高效地生成DDQC，并且可以避免缺陷。<details>
<summary>Abstract</summary>
We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance the chance of forming a globally stable state. The estimated policy guides the system toward the critical temperature to assist the formation of DDQC.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们提议使用回归学习控制patchy particle自组织成dodecagonal quasi-crystal（DDQC）。patchy particle之间的相互作用具有方向性，并且形成DDQC。然而，其稳态结构受到其结构形成的动力学路径的影响。我们使用Q学习方法来估算最佳的温度控制策略，并示出我们可以使用这种策略来生成DDQC几乎无损。我们通过对一个简单的三个凹坑潜在能障的模型进行分析，发现回归学习自动发现了具有增强 globally stable 状态的温度极限。我们认为，这种温度控制策略可以帮助系统形成DDQC。
</details></li>
</ul>
<hr>
<h2 id="Supervised-Machine-Learning-and-Physics-based-Machine-Learning-approach-for-prediction-of-peak-temperature-distribution-in-Additive-Friction-Stir-Deposition-of-Aluminium-Alloy"><a href="#Supervised-Machine-Learning-and-Physics-based-Machine-Learning-approach-for-prediction-of-peak-temperature-distribution-in-Additive-Friction-Stir-Deposition-of-Aluminium-Alloy" class="headerlink" title="Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy"></a>Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06838">http://arxiv.org/abs/2309.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshansh Mishra</li>
<li>for: 本研究旨在掌握Additive friction stir deposition（AFSD）过程参数与微结构之间的关系，以便优化AFSD过程以获得desired properties。</li>
<li>methods: 本研究使用了 cutting-edge框架， combining supervised machine learning（SML）和physics-informed neural networks（PINNs）来预测AFSD过程中热点 distribuition的 peak temperature。</li>
<li>results: 研究发现， ensemble techniques like gradient boosting 在 SML 中表现最佳，具有最低MSE值（165.78）。此外，通过融合数据驱动学和基本物理学，这种双重方法可以提供AFSD过程中热点分布的全面理解，并且可以用来调整微结构以获得desired properties。<details>
<summary>Abstract</summary>
Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied to classify deposition quality from process factors, with logistic regression delivering robust accuracy. By fusing data-driven learning and fundamental physics, this dual methodology provides comprehensive insights into tailoring microstructure through thermal management in AFSD. The work demonstrates the power of bridging statistical and physics-based modeling for elucidating AM process-property relationships.
</details>
<details>
<summary>摘要</summary>
添加性摩擦挤出（AFSD）是一种新的固体添加制造技术，可以避免传统粉末压缩和导向能量激发的问题，如缺陷、裂缝和性能不均。然而，AFSD中的过程参数与温度分布、结果结构之间的关系仍然不够了解。这难以优化过程以获得适合的性能。这项工作使用了前沿的框架，结合监督学习（SML）和物理学习网络（PINNs），预测AFSD中过程参数的温度分布。这里使用了8种回归算法来实现SML模型，而PINNs则利用了运动、声速、热传导和量子力学的普遍方程。在多个统计度量上，ensemble技术如折衣加boosting表现最佳，最低MSE为165.78。此外，这种集成的ML方法还被应用于分类过程参数对应的材料质量，使用了логистиック回归得到了可靠的准确率。通过融合数据驱动学习和基本物理学习，这种双重方法提供了全面的理解添加制造过程中的热管理对微structure的影响，从而透视AM过程-性能关系。这项工作表明了将统计学和物理学基础模型融合的力量，用于解释添加制造过程中的关键关系。
</details></li>
</ul>
<hr>
<h2 id="Safe-Reinforcement-Learning-with-Dual-Robustness"><a href="#Safe-Reinforcement-Learning-with-Dual-Robustness" class="headerlink" title="Safe Reinforcement Learning with Dual Robustness"></a>Safe Reinforcement Learning with Dual Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06835">http://arxiv.org/abs/2309.06835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Yunan Wang, Yujie Yang, Shengbo Eben Li</li>
<li>for: 本研究旨在解决强化学习（RL）代理人面临恶性干扰时的安全性和可靠性问题。</li>
<li>methods: 本研究提出了一种系统性框架，该框架将安全RL和稳健RL融合在一起，包括问题设定、迭代方案、收敛分析以及实践算法设计。此外，本研究还提出了一种深度RL算法，称为双重稳健actor-critic（DRAC）。</li>
<li>results: 根据安全性重要的标准彩色数据集评估，DRAC算法在所有情况下（无恶意对手、安全对手、性能对手）表现出了高性能和持续的安全性，与所有基elines显著升级。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) agents are vulnerable to adversarial disturbances, which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneously optimizes a task policy and a safety policy. The convergence of this iteration scheme is proved. Furthermore, we design a deep RL algorithm for practical implementation, called dually robust actor-critic (DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC achieves high performance and persistent safety under all scenarios (no adversary, safety adversary, performance adversary), outperforming all baselines significantly.
</details>
<details>
<summary>摘要</summary>
�� Reinforcement learning (RL) ��� engine ��� vulnerable to adversarial disturbances, ��� which can deteriorate task performance or compromise safety specifications. Existing methods either address safety requirements under the assumption of no adversary (e.g., safe RL) or only focus on robustness against performance adversaries (e.g., robust RL). Learning one policy that is both safe and robust remains a challenging open problem. The difficulty is how to tackle two intertwined aspects in the worst cases: feasibility and optimality. Optimality is only valid inside a feasible region, while identification of maximal feasible region must rely on learning the optimal policy. To address this issue, we propose a systematic framework to unify safe RL and robust RL, including problem formulation, iteration scheme, convergence analysis and practical algorithm design. This unification is built upon constrained two-player zero-sum Markov games. A dual policy iteration scheme is proposed, which simultaneously optimizes a task policy and a safety policy. The convergence of this iteration scheme is proved. Furthermore, we design a deep RL algorithm for practical implementation, called dually robust actor-critic (DRAC). The evaluations with safety-critical benchmarks demonstrate that DRAC achieves high performance and persistent safety under all scenarios (no adversary, safety adversary, performance adversary), outperforming all baselines significantly.
</details></li>
</ul>
<hr>
<h2 id="UniBrain-Universal-Brain-MRI-Diagnosis-with-Hierarchical-Knowledge-enhanced-Pre-training"><a href="#UniBrain-Universal-Brain-MRI-Diagnosis-with-Hierarchical-Knowledge-enhanced-Pre-training" class="headerlink" title="UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training"></a>UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06828">http://arxiv.org/abs/2309.06828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljy19970415/unibrain">https://github.com/ljy19970415/unibrain</a></li>
<li>paper_authors: Jiayu Lei, Lisong Dai, Haoyun Jiang, Chaoyi Wu, Xiaoman Zhang, Yao Zhang, Jiangchao Yao, Weidi Xie, Yanyong Zhang, Yuehua Li, Ya Zhang, Yanfeng Wang</li>
<li>for: 这 paper 旨在提出一种基于大规模数据集的高效缩放性脑磁共振成像诊断方法，以提高脑病诊断的准确率和效率。</li>
<li>methods: 该方法利用了大规模的成像报告对策，并建立了一种层次结构的知识匹配机制，以强化特征学习效果。</li>
<li>results: 对于三个实际 dataset 和 BraTS2019 公共数据集，UniBrain 能够一致性地超越所有现有的诊断方法，并在某些疾病类型上达到了专业放射学家的性能水平。<details>
<summary>Abstract</summary>
Magnetic resonance imaging~(MRI) have played a crucial role in brain disease diagnosis, with which a range of computer-aided artificial intelligence methods have been proposed. However, the early explorations usually focus on the limited types of brain diseases in one study and train the model on the data in a small scale, yielding the bottleneck of generalization. Towards a more effective and scalable paradigm, we propose a hierarchical knowledge-enhanced pre-training framework for the universal brain MRI diagnosis, termed as UniBrain. Specifically, UniBrain leverages a large-scale dataset of 24,770 imaging-report pairs from routine diagnostics. Different from previous pre-training techniques for the unitary vision or textual feature, or with the brute-force alignment between vision and language information, we leverage the unique characteristic of report information in different granularity to build a hierarchical alignment mechanism, which strengthens the efficiency in feature learning. Our UniBrain is validated on three real world datasets with severe class imbalance and the public BraTS2019 dataset. It not only consistently outperforms all state-of-the-art diagnostic methods by a large margin and provides a superior grounding performance but also shows comparable performance compared to expert radiologists on certain disease types.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparative-Analysis-of-Contextual-Relation-Extraction-based-on-Deep-Learning-Models"><a href="#Comparative-Analysis-of-Contextual-Relation-Extraction-based-on-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models"></a>Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06814">http://arxiv.org/abs/2309.06814</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Priyadharshini, G. Jeyakodi, P. Shanthi Bala</li>
<li>for: constructing a knowledge graph with the help of ontology, performing various tasks such as semantic search, query answering, and textual entailment</li>
<li>methods: deep learning techniques, including hybrid models, to extract relations from complex sentences effectively</li>
<li>results: more accurate and efficient relation extraction, particularly for complex sentences with multiple relations and unspecified entities<details>
<summary>Abstract</summary>
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are not suited for complex sentences that consist of the words that have various meanings. To address these issues, hybrid deep learning models have been used to extract the relations from complex sentence effectively. This paper explores the analysis of various deep learning models that are used for relation extraction.
</details>
<details>
<summary>摘要</summary>
Contextual Relation Extraction (CRE) 主要用于构建知识图库，帮助 ontology 中的实体之间建立关系。它完成了多种任务，如semantic search、查询回答和文本推理。关系提取可以从原始文本中提取实体和其间的关系。在生物医学领域，一个高效和准确的 CRE 系统是建立领域知识的关键。现有的机器学习和自然语言处理（NLP）技术不适用于 efficiently 预测复杂关系从多个句子中。为解决这些问题，深度学习技术被用来确定上下文中的相应含义。虽然多种机器学习模型已经用于关系提取，但它们只能提供二元关系（即 sentence 中的两个实体之间的关系）的更好结果。机器学习模型不适用于包含多个意思的词语的复杂句子。为了解决这些问题， hybrid deep learning 模型被用来从复杂句子中提取关系。本文探讨了不同深度学习模型的关系提取分析。
</details></li>
</ul>
<hr>
<h2 id="FedDIP-Federated-Learning-with-Extreme-Dynamic-Pruning-and-Incremental-Regularization"><a href="#FedDIP-Federated-Learning-with-Extreme-Dynamic-Pruning-and-Incremental-Regularization" class="headerlink" title="FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization"></a>FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06805">http://arxiv.org/abs/2309.06805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericloong/feddip">https://github.com/ericloong/feddip</a></li>
<li>paper_authors: Qianyu Long, Christos Anagnostopoulos, Shameem Puthiya Parambath, Daning Bi</li>
<li>for: 这个论文旨在提出一个基于 Federated Learning 的新框架，可以在分布式训练和推导大规模深度神经网络时，实现高精度和快速训练。</li>
<li>methods: 这个框架使用了动态模型剔除和错误反馈来删除无用的资讯交换，并且运用了增量调整来实现极高精度模型。</li>
<li>results: 在比较以前的方法和其他模型剔除方法时，这个框架能够高效地控制模型精度，并且能够在分布式训练中实现类似或更好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against state-of-the-art methods using benchmark data sets and DNN models. Our results showcase that FedDIP not only controls the model sparsity but efficiently achieves similar or better performance compared to other model pruning methods adopting incremental regularization during distributed model training. The code is available at: https://github.com/EricLoong/feddip.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）已成功应用于分布式训练和推理大规模深度神经网络（DNN）。然而，DNN具有极高的参数数量，从而导致参数之间的交换和内存管理具有极大的挑战。虽然最近的DNN压缩方法（例如减少和截断）解决了这些挑战，但它们不总是考虑灵活控制参数交换的减少而保持高精度水平。为此，我们提出了一种新的FL框架（名为FedDIP），它结合（i）动态模型剪辑和错误反馈来消除重复的信息交换，从而使得性能得到显著改善，并且（ii）逐步REG regularization可以实现极高精度的模型。我们对FedDIP的收敛分析和性能比较分析，并使用标准数据集和DNN模型进行广泛的性能评估。我们的结果表明，FedDIP不仅可以控制模型精度，而且能够高效地实现与其他模型剪辑方法在分布式模型训练中的同等或更好的性能。代码可以在以下地址下获取：https://github.com/EricLoong/feddip。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Traffic-Prediction-under-Missing-Data"><a href="#Uncertainty-aware-Traffic-Prediction-under-Missing-Data" class="headerlink" title="Uncertainty-aware Traffic Prediction under Missing Data"></a>Uncertainty-aware Traffic Prediction under Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06800">http://arxiv.org/abs/2309.06800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Mei, Junxian Li, Zhiming Liang, Guanjie Zheng, Bin Shi, Hua Wei</li>
<li>for: 预测交通流量是 Transportation 领域中的一个关键问题，因为它有广泛的应用场景。最近几年，许多研究都取得了显著的成果，但大多数研究假设预测地点有完整或至少部分的历史记录，无法扩展到没有历史记录的地点。在实际应用中，投入感知器的部署可能受到预算限制和安装可用性的限制，使得现有的模型不适用。</li>
<li>methods: 该研究基于前一代步骤 inductive graph neural network 的思想，提出了一个能够扩展预测至缺失记录的地点，同时生成 probabilistic 预测和uncertainty quantification，以帮助管理风险和决策。</li>
<li>results: 通过对实际数据进行广泛的实验，研究结果表明，我们的方法在预测任务中取得了显著的成果，并且uncertainty quantification 的结果与历史数据和无历史数据的地点之间呈高度相关。此外，我们的模型还能够帮助 optimize sensor deployment 任务，以实现更高的准确率。<details>
<summary>Abstract</summary>
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-aware framework with the ability to 1) extend prediction to missing locations with no historical records and significantly extend spatial coverage of prediction locations while reducing deployment of sensors and 2) generate probabilistic prediction with uncertainty quantification to help the management of risk and decision making in the down-stream tasks. Through extensive experiments on real-life datasets, the result shows our method achieved promising results on prediction tasks, and the uncertainty quantification gives consistent results which highly correlated with the locations with and without historical data. We also show that our model could help support sensor deployment tasks in the transportation field to achieve higher accuracy with a limited sensor deployment budget.
</details>
<details>
<summary>摘要</summary>
宽泛应用于交通领域的交通预测是一个关键的话题，因为它的广泛应用可以提高交通效率和安全性。在过去几年中，许多研究已经取得了成功的结果。然而，大多数研究假设预测位置具有完整或至少部分的历史记录，无法扩展到没有历史记录的位置。在实际应用中，投放感知器的限制可能会导致大多数当前模型无法应用。虽然一些文献尝试了填充交通状态的方法，但这些方法需要同时观察的数据，使得它们无法应用于预测任务。另外，当前的预测模型缺乏量化不确定性的能力，使得过去的作品不适用于风险敏感任务或决策过程。为了填补这一空白，我们受到过去的卷积图 neural network 的启发，提出了一个不确定性意识框架，能够1) 扩展预测到缺失历史记录的位置，大幅减少感知器的投放，并大幅提高预测位置的准确率，2) 生成概率预测，对于风险敏感任务和决策过程提供量化不确定性的支持。经过广泛的实验，我们的方法在预测任务中取得了出色的结果，不确定性评估与历史数据位置相吻合。此外，我们的模型还可以帮助交通领域中的感知器投放任务，以实现更高的准确率，即使有限的感知器投放预算。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Mirage-A-Review-of-Hallucinations-in-Large-Language-Models"><a href="#Cognitive-Mirage-A-Review-of-Hallucinations-in-Large-Language-Models" class="headerlink" title="Cognitive Mirage: A Review of Hallucinations in Large Language Models"></a>Cognitive Mirage: A Review of Hallucinations in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06794">http://arxiv.org/abs/2309.06794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongbinye/cognitive-mirage-hallucinations-in-llms">https://github.com/hongbinye/cognitive-mirage-hallucinations-in-llms</a></li>
<li>paper_authors: Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia</li>
<li>for: This paper is written for researchers and developers working on text generation systems, particularly those interested in understanding and addressing the issue of hallucinations in large language models (LLMs).</li>
<li>methods: The paper presents a novel taxonomy of hallucinations in text generation tasks, based on a detailed analysis of various examples and theoretical insights. It also discusses existing detection and improvement methods for hallucinations in LLMs.</li>
<li>results: The paper provides a comprehensive overview of hallucinations in LLMs, including a detailed taxonomy, theoretical analyses, and existing detection and improvement methods. It also proposes several future research directions that can be explored to address the issue of hallucinations in text generation systems.<details>
<summary>Abstract</summary>
As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
</details>
<details>
<summary>摘要</summary>
大语言模型在人工智能领域的发展中，文本生成系统容易受到一种关注的现象，即幻视。本研究将给出最近的有力关幻视在大语言模型中的新发现，并提供了一个新的分类法，以及幻视的理论分析、检测方法和改善方法。此外，我们还提出了未来的研究方向。我们的贡献有三个方面：1. 我们提供了文本生成任务中幻视的详细和完整的分类法；2. 我们提供了幻视在大语言模型中的理论分析，并提供了现有的检测和改善方法；3. 我们提出了未来的研究方向。当幻视在社区中受到重视时，我们将继续更新有关的研究进展。
</details></li>
</ul>
<hr>
<h2 id="Electricity-Demand-Forecasting-through-Natural-Language-Processing-with-Long-Short-Term-Memory-Networks"><a href="#Electricity-Demand-Forecasting-through-Natural-Language-Processing-with-Long-Short-Term-Memory-Networks" class="headerlink" title="Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks"></a>Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06793">http://arxiv.org/abs/2309.06793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Bai, Simon Camal, Andrea Michiorri</li>
<li>for: 预测英国国家电力需求</li>
<li>methods: 使用Long and Short-Term Memory（LSTM）网络，并将新闻文本特征纳入模型中</li>
<li>results: 研究发现，公众情绪和交通和地opolitics相关词语表达具有时间连续性效应，并且LSTM WITH textual features比基准值超过3%，并且比官方 benchmark超过10%。此外，提出的模型有效减少预测不确定性，缩短信息分布和确定性。<details>
<summary>Abstract</summary>
Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence interval and bringing the forecast distribution closer to the truth.
</details>
<details>
<summary>摘要</summary>
电力需求预测是一个已经very well established的研究领域。通常这个任务是通过历史负荷、天气预报、日历信息和已知的主要事件来完成。在最近，关注的新信息来源是从文本新闻中提取的。这篇论文提出了一种使用Long and Short-Term Memory（LSTM）网络，并在这个网络中添加文本新闻特征，成功地预测了英国国家电力需求的 deterministic 和 probabilistic 任务。研究发现，公众情绪和交通和地opolitics相关的词向量表示在电力需求中有时间连续性效应。实验结果表明，LSTM与文本特征相加的模型在相对评benchmark中提高了超过3%，并在官方benchmark中提高了接近10%。此外，提议的模型可以有效地减少预测不确定性，减小信息interval并使预测分布更接近真实。
</details></li>
</ul>
<hr>
<h2 id="Scalable-neural-network-models-and-terascale-datasets-for-particle-flow-reconstruction"><a href="#Scalable-neural-network-models-and-terascale-datasets-for-particle-flow-reconstruction" class="headerlink" title="Scalable neural network models and terascale datasets for particle-flow reconstruction"></a>Scalable neural network models and terascale datasets for particle-flow reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06782">http://arxiv.org/abs/2309.06782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joosep Pata, Eric Wulff, Farouk Mokhtar, David Southwick, Mengke Zhang, Maria Girone, Javier Duarte</li>
<li>for: 这个论文旨在开发一种可扩展的机器学习模型，用于高能电子- позитроン碰撞实验中的全事件重建。</li>
<li>methods: 该论文使用图гра树神经网络和核心变换器，以避免 quadratic memory allocation 和计算成本，同时实现有realistic的PF重建。</li>
<li>results: 研究发现，通过超级计算机进行hyperparameter优化，可以提高物理性能表现。此外，模型可以在不同的硬件处理器上高度可портабе，支持Nvidia、AMD和Intel Habana卡。最后，研究表明，使用高度粒度的轨迹和calorimeter射频输入，可以实现与基eline相当的物理性能。<details>
<summary>Abstract</summary>
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following the findable, accessible, interoperable, and reusable (FAIR) principles.
</details>
<details>
<summary>摘要</summary>
我们研究高可扩展机器学习模型以实现高能电子- позиトрон撞击中全事件重建，基于高度粒子化仪器模拟。流体（PF）重建可以表示为监督学习任务，使用轨迹和calorimeter层或hit。我们比较了图 neuron网络和基于kernel的transformer，并证明它们可以避免quadratic内存分配和计算成本，同时实现现实主义PF重建。我们表明了超参数调整在超级计算机上有 significanthysics性能提升。我们还表明了模型可以在不同的硬件处理器上高度可移植，支持Nvidia、AMD和Intel Habana卡。最后，我们示出了模型可以在高度粒子化输入上进行训练， resulting in competitive physics performance with the baseline。数据集和软件用于重现研究按照可找到、可达、可操作和可重用（FAIR）原则发布。
</details></li>
</ul>
<hr>
<h2 id="Fundamental-Limits-of-Deep-Learning-Based-Binary-Classifiers-Trained-with-Hinge-Loss"><a href="#Fundamental-Limits-of-Deep-Learning-Based-Binary-Classifiers-Trained-with-Hinge-Loss" class="headerlink" title="Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss"></a>Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06774">http://arxiv.org/abs/2309.06774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tilahun M. Getu, Georges Kaddoum</li>
<li>for: 本研究旨在解释深度学习（DL）在多个领域中的成功原因，并提供一个综合理解深度学习的理论基础。</li>
<li>methods: 本研究使用了优化、泛化和近似等方法。</li>
<li>results: 本研究提出了一种测试性能限制，用于评估深度学习基于折衣函数（ReLU）Feedforward神经网络（FNN）和深度FNN的测试性能。这些测试性能限制被 validate by extensive computer experiments。<details>
<summary>Abstract</summary>
Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedforward neural networks (FNNs) and ones that are based on deep FNNs with ReLU and Tanh activation, we derive their respective novel asymptotic testing performance limits. The derived testing performance limits are validated by extensive computer experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MTD-Multi-Timestep-Detector-for-Delayed-Streaming-Perception"><a href="#MTD-Multi-Timestep-Detector-for-Delayed-Streaming-Perception" class="headerlink" title="MTD: Multi-Timestep Detector for Delayed Streaming Perception"></a>MTD: Multi-Timestep Detector for Delayed Streaming Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06742">http://arxiv.org/abs/2309.06742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yulin1004/mtd">https://github.com/yulin1004/mtd</a></li>
<li>paper_authors: Yihui Huang, Ningjiang Chen</li>
<li>for: 提高自动驾驶系统的实时环境感知，以确保用户安全和体验。</li>
<li>methods: 提出了多时步探测器（MTD）和延迟分析模块（DAM），以及一种新的时间步分支模块（TBM），用于适应延迟波动。</li>
<li>results: 在Argoverse-HD dataset上进行了实验，并实现了在不同延迟设置下的state-of-the-art表现。<details>
<summary>Abstract</summary>
Autonomous driving systems require real-time environmental perception to ensure user safety and experience. Streaming perception is a task of reporting the current state of the world, which is used to evaluate the delay and accuracy of autonomous driving systems. In real-world applications, factors such as hardware limitations and high temperatures inevitably cause delays in autonomous driving systems, resulting in the offset between the model output and the world state. In order to solve this problem, this paper propose the Multi- Timestep Detector (MTD), an end-to-end detector which uses dynamic routing for multi-branch future prediction, giving model the ability to resist delay fluctuations. A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively predict specific timesteps according to the delay trend. The proposed method has been evaluated on the Argoverse-HD dataset, and the experimental results show that it has achieved state-of-the-art performance across various delay settings.
</details>
<details>
<summary>摘要</summary>
A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively predict specific timesteps according to the delay trend. The proposed method has been evaluated on the Argoverse-HD dataset, and the experimental results show that it has achieved state-of-the-art performance across various delay settings.Translation notes:* "streaming perception" is translated as "实时感知" (shízhí gǎngrán), which means "real-time perception" or "streaming sensing".* "delay" is translated as "延迟" (diànyì), which means "delay" or "lag".* "timestep" is translated as "时间步" (shíjiān bù), which means "time step" or "time increment".* "end-to-end detector" is translated as "端到端检测器" (dìngdào dào dīngkèshì), which means "end-to-end detector" or "full-stack detector".* "dynamic routing" is translated as "动态路由" (dòngtài lùyòu), which means "dynamic routing" or "adaptive routing".* "multi-branch future prediction" is translated as "多支未来预测" (duō zhī wèilái yùjì), which means "multi-branch future prediction" or "multi-path future prediction".* "delay trend" is translated as "延迟趋势" (diànyì xiàngxìng), which means "delay trend" or "delay pattern".* "Timestep Branch Module" is translated as "时间步分支模块" (shíjiān bù fēnzhī móudì), which means "time step branch module" or "time increment branch module".* "static flow" is translated as "静态流" (jìngtài liú), which means "static flow" or "stationary flow".* "adaptive flow" is translated as "适应流" (shìyìng liú), which means "adaptive flow" or "adaptive stream".
</details></li>
</ul>
<hr>
<h2 id="MCNS-Mining-Causal-Natural-Structures-Inside-Time-Series-via-A-Novel-Internal-Causality-Scheme"><a href="#MCNS-Mining-Causal-Natural-Structures-Inside-Time-Series-via-A-Novel-Internal-Causality-Scheme" class="headerlink" title="MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme"></a>MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06739">http://arxiv.org/abs/2309.06739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Liu, Dehui Du, Zihan Jiang, Anyan Huang, Yiyang Li</li>
<li>for: 本研究旨在探讨时序序列中的内在 causality，以提高人工神经网络（NN）的准确性和可读性。</li>
<li>methods: 该研究提出了一种名为 Mining Causal Natural Structure（MCNS）的新框架，可自动找到时序序列中的内在 causality 结构，并将其应用于 NN 中。</li>
<li>results: 实验结果表明，通过使用 MCNS 杜然 NN 的准确性和可读性，同时提供了更深入、固定的时序序列和数据概要。<details>
<summary>Abstract</summary>
Causal inference permits us to discover covert relationships of various variables in time series. However, in most existing works, the variables mentioned above are the dimensions. The causality between dimensions could be cursory, which hinders the comprehension of the internal relationship and the benefit of the causal graph to the neural networks (NNs). In this paper, we find that causality exists not only outside but also inside the time series because it reflects a succession of events in the real world. It inspires us to seek the relationship between internal subsequences. However, the challenges are the hardship of discovering causality from subsequences and utilizing the causal natural structures to improve NNs. To address these challenges, we propose a novel framework called Mining Causal Natural Structure (MCNS), which is automatic and domain-agnostic and helps to find the causal natural structures inside time series via the internal causality scheme. We evaluate the MCNS framework and impregnation NN with MCNS on time series classification tasks. Experimental results illustrate that our impregnation, by refining attention, shape selection classification, and pruning datasets, drives NN, even the data itself preferable accuracy and interpretability. Besides, MCNS provides an in-depth, solid summary of the time series and datasets.
</details>
<details>
<summary>摘要</summary>
causal inference 允许我们发现时间序列中变量之间的隐藏关系。然而，现有的大多数工作中的变量都是维度，这使得变量之间的相互关系受限，阻碍我们理解内部关系以及 causal graph 对神经网络 (NN) 的利用。在这篇文章中，我们发现时间序列中的 causality 不仅存在在外部，而且也存在在内部，因为它反映了实际世界中的事件顺序。这使我们感到需要检查内部 subsequences 之间的关系。然而，挑战是从 subsequences 中发现 causality 以及使用 causal natural structure 来改进 NN。为解决这些挑战，我们提出了一个新的框架called Mining Causal Natural Structure (MCNS)，它是自动化的、领域不依赖的，可以在时间序列中找到内部 causality scheme。我们评估了 MCNS 框架和使用 MCNS 修饰 NN 的时间序列分类任务。实验结果表明，我们的涂抹，通过修改注意力、形状选择分类和减少数据集，使得 NN 的准确率和可读性得到了改进。此外，MCNS 还提供了深入、坚实的时间序列和数据集概括。
</details></li>
</ul>
<hr>
<h2 id="Deep-Nonparametric-Convexified-Filtering-for-Computational-Photography-Image-Synthesis-and-Adversarial-Defense"><a href="#Deep-Nonparametric-Convexified-Filtering-for-Computational-Photography-Image-Synthesis-and-Adversarial-Defense" class="headerlink" title="Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense"></a>Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06724">http://arxiv.org/abs/2309.06724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Wangni</li>
<li>for:  recuperate the real scene from imperfect images</li>
<li>methods:  Deep Nonparametric Convexified Filtering (DNCF)</li>
<li>results:  defends image classification deep networks against adversary attack algorithms in real-time<details>
<summary>Abstract</summary>
We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
</details>
<details>
<summary>摘要</summary>
我们目标是提供一个通用的计算摄影框架，通过深度非 Parametric 几何 filtering (DNCF) 来重建真实场景从不完美的图像中。DNCF 包括一个非 Parametric 深度网络，用于模拟图像形成物理方程，如降噪、超分解、填充和闪光。由于DNCF 没有依赖于训练数据的参数化，因此具有强大的泛化和鲁棒性，可以防止对图像进行恶意修改。在推理过程中，我们还鼓励网络参数具有非负性，创建了输入和参数之间的双凸函数，这使得可以使用不够的运行时间的第二个优化算法进行加速，相比 Deep Image Prior 的10倍加速。通过这些工具，我们经验证明DNCF 可以在实时中防止图像分类深度网络受到攻击。
</details></li>
</ul>
<hr>
<h2 id="Bias-Amplification-Enhances-Minority-Group-Performance"><a href="#Bias-Amplification-Enhances-Minority-Group-Performance" class="headerlink" title="Bias Amplification Enhances Minority Group Performance"></a>Bias Amplification Enhances Minority Group Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06717">http://arxiv.org/abs/2309.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaotang Li, Jiarui Liu, Wei Hu</li>
<li>for: 这篇论文目的是提高对罕见分组的准确率，即使使用标准训练。</li>
<li>methods: 本文提出了一个名为BAM的两阶段训练算法，包括一个偏好增强方案和一个重新权重样本的步骤。</li>
<li>results: BAM在computer vision和自然语言处理中的伪 correlate测试中取得了竞争性的性能，并且提出了一个简单的停止条件，可以无需群体标注来获得优化的性能。<details>
<summary>Abstract</summary>
Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared with existing methods evaluated on spurious correlation benchmarks in computer vision and natural language processing. Moreover, we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations, with little or no loss in worst-group accuracy. We perform extensive analyses and ablations to verify the effectiveness and robustness of our algorithm in varying class and group imbalance ratios.
</details>
<details>
<summary>摘要</summary>
neuronal networks 生成出来的标准训练是知道低精度在罕见分组上，即使 дости得了平均标签的高精度，这是由某些偶极特征和标签之间的相关性引起的。现有的方法（如集群损失最小化）可以提高罕见分组的精度，但是需要训练样本集中的所有样本的集群注释。在这篇论文中，我们关注了较为具有挑战性和实际性的设定，即集群注释只有小 Validation 集中或者完全没有。我们提出了一种新的两阶段训练算法：在第一阶段，模型通过引入每个训练样本的学习可迭代变量进行偏好增强；在第二阶段，我们增重点的样本，并继续使用增重点的数据进行训练同样的模型。实际上，BAM 达到了与现有方法相当的性能，并且我们发现了一个简单的停止 criterion，可以根据最小类别差异来移除集群注释，而且几乎不会导致罕见分组的精度下降。我们进行了广泛的分析和缺陷分析，以证明我们的算法的效果和可靠性在不同的类和分组异质比例下。
</details></li>
</ul>
<hr>
<h2 id="Crystal-structure-prediction-using-neural-network-potential-and-age-fitness-Pareto-genetic-algorithm"><a href="#Crystal-structure-prediction-using-neural-network-potential-and-age-fitness-Pareto-genetic-algorithm" class="headerlink" title="Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm"></a>Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06710">http://arxiv.org/abs/2309.06710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadmanomee/ParetoCSP">https://github.com/sadmanomee/ParetoCSP</a></li>
<li>paper_authors: Sadman Sadeed Omee, Lai Wei, Jianjun Hu</li>
<li>for: 这篇论文的目的是提出一种新的晶体结构预测算法（ParetoCSP），用于解决晶体结构预测问题，并且使用了多目标遗传算法（MOGA）和神经网络间位能模型（IAP）来找到化学组成下的能量最优晶体结构。</li>
<li>methods: 该算法首先使用MOGA进行多目标优化，然后使用IAP模型来导向GA搜索，并且还含有一个改进后的NSGA-III算法，使用生物体龄作为独立的优化因素。</li>
<li>results: 与GN-OA算法相比，ParetoCSP显示出了明显的优势，在55个多样化的 bench mark结构上，通过7种性能指标进行评估，ParetoCSP的预测性能高于GN-OA的2.562倍。而且，对所有算法的搜索过程的轨迹分析表明，ParetoCSP生成了更多的有效结构，这有助于GA更好地搜索优质结构。<details>
<summary>Abstract</summary>
While crystal structure prediction (CSP) remains a longstanding challenge, we introduce ParetoCSP, a novel algorithm for CSP, which combines a multi-objective genetic algorithm (MOGA) with a neural network inter-atomic potential (IAP) model to find energetically optimal crystal structures given chemical compositions. We enhance the NSGA-III algorithm by incorporating the genotypic age as an independent optimization criterion and employ the M3GNet universal IAP to guide the GA search. Compared to GN-OA, a state-of-the-art neural potential based CSP algorithm, ParetoCSP demonstrated significantly better predictive capabilities, outperforming by a factor of $2.562$ across $55$ diverse benchmark structures, as evaluated by seven performance metrics. Trajectory analysis of the traversed structures of all algorithms shows that ParetoCSP generated more valid structures than other algorithms, which helped guide the GA to search more effectively for the optimal structures
</details>
<details>
<summary>摘要</summary>
“单晶结构预测（CSP）仍然是一个长期挑战，我们介绍了一个新的算法，即ParetoCSP，它结合了多个目标遗传algorithm（MOGA）和神经网络间原子 potential（IAP）模型，以获取化学成分提供的能量最佳晶体结构。我们将NSGA-III算法加以改进，通过包括遗传年龄作为独立优化条件，并使用M3GNet通用IAP导引GA搜索。与GN-OA，一个现有的神经 potential基于CSP算法相比，ParetoCSP在55个多样化的benchmark结构上显示出了明显的改善，其中七种性能指标中的一个改善因子为2.562。对所有算法的探索过程中的构造轨迹分析表明，ParetoCSP产生了更多的有效构造，帮助GA更有效地寻找优化结构。”
</details></li>
</ul>
<hr>
<h2 id="Predicting-Fatigue-Crack-Growth-via-Path-Slicing-and-Re-Weighting"><a href="#Predicting-Fatigue-Crack-Growth-via-Path-Slicing-and-Re-Weighting" class="headerlink" title="Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting"></a>Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06708">http://arxiv.org/abs/2309.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyj21/fcg">https://github.com/zhaoyj21/fcg</a></li>
<li>paper_authors: Yingjie Zhao, Yong Liu, Zhiping Xu</li>
<li>for: 预测结构元件疲劳的可能性，以便在工程设计中进行评估和预测。</li>
<li>methods: 使用统计学学习框架，利用高精度物理 simulate crack pattern 和剩余寿命，然后使用维度减少和神经网络架构来学习历史相依性和非线性。</li>
<li>results: 预测的疲劳裂隙模式和剩余寿命可以在实时结构健康监测和疲劳生命预测中提供数字双方式enario，帮助进行维护管理决策。<details>
<summary>Abstract</summary>
Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples with fatigue cracks in plates, which showcase the digital-twin scenario in real-time structural health monitoring and fatigue life prediction for maintenance management decision-making.
</details>
<details>
<summary>摘要</summary>
预测关键结构组件的疲劳风险是工程设计中的关键任务。然而，疲劳通常会带来材料微结构和服务条件之间的复杂互连关系，使诊断和预测疲劳损害具有挑战性。我们报告了一种统计学学习框架，用于预测加载条件下疲劳裂隙的增长和组件的寿命。通过高精度物理 simulate 得到的数字图书馆，包括疲劳裂隙和剩余寿命的数据。使用维度减少和神经网络架构，学习疲劳裂隙历史依赖性和非线性。使用路径架和重量补做技术来处理统计噪声和罕见事件。预测的疲劳裂隙 Pattern 会自动更新和自我修正。我们验证了这种终端方法，通过板件中的疲劳裂隙示例，展示了数字神经网络在实时结构健康监测和疲劳寿命预测中的可行性。
</details></li>
</ul>
<hr>
<h2 id="VLSlice-Interactive-Vision-and-Language-Slice-Discovery"><a href="#VLSlice-Interactive-Vision-and-Language-Slice-Discovery" class="headerlink" title="VLSlice: Interactive Vision-and-Language Slice Discovery"></a>VLSlice: Interactive Vision-and-Language Slice Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06703">http://arxiv.org/abs/2309.06703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slymane/vlslice">https://github.com/slymane/vlslice</a></li>
<li>paper_authors: Eric Slyman, Minsuk Kahng, Stefan Lee</li>
<li>for: 这篇论文目的是发展一个可互动地发现可视语言相互关联的变量分布，从未 labels 的图像集中获得更好的表现。</li>
<li>methods: 这篇论文使用了大规模预训来学习可转移的模型，并使用了用户引导的方法来发现可视语言 slice。</li>
<li>results: 在用户研究中（n&#x3D;22），VLSlice 能够快速生成多元高凝集的可视语言 slice，并且发布了这个工具给公众。<details>
<summary>Abstract</summary>
Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly.
</details>
<details>
<summary>摘要</summary>
近期的视觉语言工作显示，大规模预训练可以学习通用的模型，高效地传输到下游任务。这可能提高数据集级别的统计量表现，但是分析围绕特定偏见维度的手工 subgroup 表现可能存在不良行为。然而，这种 subgroup 分析通常受到注释的限制，需要大量的时间和资源来收集必要的数据。先前的艺术尝试自动发现 subgroup 以绕过这些限制，通常基于现有的任务特定签名，但是这些模型很快地在更复杂的输入上崩溃。本文提出了 VLSlice，一种互动系统，帮助用户通过手动导航发现具有相互关联的视觉语言行为，称为视觉语言slice，从无标注图像集中。我们证明了 VLSlice 在用户研究中（n=22）可以快速生成多样性高准确性 slice，并公开发布了这个工具。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Non-IID-Issue-in-Heterogeneous-Federated-Learning-by-Gradient-Harmonization"><a href="#Tackling-the-Non-IID-Issue-in-Heterogeneous-Federated-Learning-by-Gradient-Harmonization" class="headerlink" title="Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization"></a>Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06692">http://arxiv.org/abs/2309.06692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Weiyu Sun, Ying Chen</li>
<li>for: 这篇论文目的是解决联合学习（Federated Learning，FL）中数据和设备不均的挑战，具体来说是通过减轻服务器端的梯度冲突来提高FL的性能。</li>
<li>methods: 这篇论文使用了Gradient Harmonization（梯度融合）技术来减轻服务器端的梯度冲突，该技术可以将多个客户端之间的梯度向量投影到彼此 orthogonal 的平面上，从而减少梯度冲突。</li>
<li>results: 实验表明，使用FedGH技术可以在多种不同的benchmark和非同分布场景下提高FL的性能，特别是在强度不均的场景下更是如此。此外，FedGH技术可以轻松地与任何FL框架集成，不需要对hyperparameter进行调整。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios with stronger heterogeneity. As a plug-and-play module, FedGH can be seamlessly integrated into any FL framework without requiring hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
Federation learning (FL) 是一种隐私保护的 paradigm，用于在分散的客户端上共同训练全球模型。然而，FL 的性能受到非独立和同分布数据（non-IID）和设备不同性的影响。在这项工作中，我们重新检视了这一关键挑战，通过服务器端的梯度冲突来查看。我们首先调查了多个客户端之间的梯度冲突现象，并发现强化不同性导致更严重的梯度冲突。为解决这个问题，我们提议 FedGH，一种简单 yet effective的方法，通过把一个梯度向量投影到另一个客户端对 conflicting 的梯度向量的正交平面。我们的实验表明，FedGH 可以增强多个 state-of-the-art FL 基elines  across 多种不同的 benchmarks 和 non-IID 场景。特别是，在更强的不同性情况下，FedGH 的改进更为显著。作为一个插件模块，FedGH 可以顺利地与任何 FL 框架集成，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Attention-Loss-Adjusted-Prioritized-Experience-Replay"><a href="#Attention-Loss-Adjusted-Prioritized-Experience-Replay" class="headerlink" title="Attention Loss Adjusted Prioritized Experience Replay"></a>Attention Loss Adjusted Prioritized Experience Replay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06684">http://arxiv.org/abs/2309.06684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoying Chen, Huiping Li, Rizhong Wang</li>
<li>for: 提高深度强化学习训练速率</li>
<li>methods:  integrate improved Self-Attention network with Double-Sampling mechanism</li>
<li>results: 提高训练精度和效率，适用于多种强化学习算法和环境<details>
<summary>Abstract</summary>
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
</details>
<details>
<summary>摘要</summary>
优先经验回归（PER）是一种深度强化学习技术，通过选择具有更多知识量的经验样本来提高神经网络训练速率。然而，PER中的非均匀采样无可避免地导致状态动作空间分布的改变，从而引起Q值函数的估计误差。在本文中，一种含有改进自注意网络和双采样机制的Attention Loss Adjusted Prioritized（ALAP）经验回归算法被提出，以适应调整重要采样权重，消除PER引起的估计误差。为证明算法的有效性和通用性，ALAP在OPENAI gym中使用值函数基于、政策梯度基于和多Agent强化学习算法进行了测试，并进行了对比研究，以证明提议的训练框架的优势和高效性。
</details></li>
</ul>
<hr>
<h2 id="Federated-PAC-Bayesian-Learning-on-Non-IID-data"><a href="#Federated-PAC-Bayesian-Learning-on-Non-IID-data" class="headerlink" title="Federated PAC-Bayesian Learning on Non-IID data"></a>Federated PAC-Bayesian Learning on Non-IID data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06683">http://arxiv.org/abs/2309.06683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhao, Yang Liu, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 这个论文是为了解决非独立不同数据（non-IID）的 Federated Learning（FL）中的 Probably Approximately Correct（PAC）极限问题而写的。</li>
<li>methods: 这篇论文使用了唯一先验知识（unique prior knowledge）和变量汇集权（variable aggregation weights）来提出了首个非虚诞的非独立FL PAC极限下界。它还介绍了一种对这个下界的优化问题的新方法和对实际数据进行验证。</li>
<li>results: 该论文的结果验证了这种新的非独立FL PAC极限下界在实际数据上的有效性。<details>
<summary>Abstract</summary>
Existing research has either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
</details>
<details>
<summary>摘要</summary>
现有研究 Either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.Here's the word-for-word translation:现有研究 Either 已经适应了 Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or 使用了 information-theoretic PAC-Bayesian bounds while introducing their theorems, but few 考虑了 non-IID 挑战 in FL. Our work 发表了 first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound 假设了 each client 和 variable aggregation weights 的 unique prior knowledge. We also introduce an objective function 和 an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-improvement-of-the-Spalart-Allmaras-model-through-assimilation-of-experimental-data"><a href="#Generalizable-improvement-of-the-Spalart-Allmaras-model-through-assimilation-of-experimental-data" class="headerlink" title="Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data"></a>Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06679">http://arxiv.org/abs/2309.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepinder Jot Singh Aulakh, Romit Maulik</li>
<li>For: The paper aims to improve the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows by using model and data fusion.* Methods: The paper uses data assimilation, specifically the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. The calibration relies on experimental data collected for velocity profiles, skin friction, and pressure coefficients for separated flows.* Results: The recalibrated SA model demonstrates generalization to other separated flows and significant improvement in the quantities of interest, such as skin friction coefficient ($C_f$) and pressure coefficient ($C_p$), for each flow tested. The individually calibrated terms in the SA model are targeted towards specific flow-physics, with the calibrated production term improving the re-circulation zone and the destruction term improving the recovery zone.<details>
<summary>Abstract</summary>
This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to other separated flows, including cases such as the 2D-bump and modified BFS. Significant improvement is observed in the quantities of interest, i.e., skin friction coefficient ($C_f$) and pressure coefficient ($C_p$) for each flow tested. Finally, it is also demonstrated that the newly proposed model recovers SA proficiency for external, unseparated flows, such as flow around a NACA-0012 airfoil without any danger of extrapolation, and that the individually calibrated terms in the SA model are targeted towards specific flow-physics wherein the calibrated production term improves the re-circulation zone while destruction improves the recovery zone.
</details>
<details>
<summary>摘要</summary>
To achieve this, ensemble Kalman filtering (EnKF) is used to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented by parameterizing the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected for velocity profiles, skin friction, and pressure coefficients for separated flows.Despite using data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to other separated flows, including 2D-bump and modified BFS flows. Significant improvements are observed in the quantities of interest, such as skin friction coefficient ($C_f$) and pressure coefficient ($C_p$), for each flow tested.Furthermore, the proposed model recovers SA proficiency for external, unseparated flows, such as flow around a NACA-0012 airfoil, without any danger of extrapolation. Additionally, the individually calibrated terms in the SA model are targeted towards specific flow-physics, where the calibrated production term improves the re-circulation zone while the destruction term improves the recovery zone.
</details></li>
</ul>
<hr>
<h2 id="Sound-field-decomposition-based-on-two-stage-neural-networks"><a href="#Sound-field-decomposition-based-on-two-stage-neural-networks" class="headerlink" title="Sound field decomposition based on two-stage neural networks"></a>Sound field decomposition based on two-stage neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06661">http://arxiv.org/abs/2309.06661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryo Matsuda, Makoto Otani</li>
<li>for: 该研究提出了一种基于神经网络的声场分解方法，用于实时声源地理位确定。</li>
<li>methods: 该方法包括两个阶段：声场分离阶段和单源地理位定位阶段。在第一阶段，通过多个源 зву频压力在 Mikrofone 上的合成，分离出每个音源启动的声压力。在第二阶段，通过 Mikrofone 上的声压力 regression 来获取源位置。不同于传统方法，该阶段采用 regression 而不是分类，因此不受精度的影响。</li>
<li>results: 数值实验表明，与传统方法相比，提出的方法可以 дости得更高的源地理位准确率和声场重建准确率。<details>
<summary>Abstract</summary>
A method for sound field decomposition based on neural networks is proposed. The method comprises two stages: a sound field separation stage and a single-source localization stage. In the first stage, the sound pressure at microphones synthesized by multiple sources is separated into one excited by each sound source. In the second stage, the source location is obtained as a regression from the sound pressure at microphones consisting of a single sound source. The estimated location is not affected by discretization because the second stage is designed as a regression rather than a classification. Datasets are generated by simulation using Green's function, and the neural network is trained for each frequency. Numerical experiments reveal that, compared with conventional methods, the proposed method can achieve higher source-localization accuracy and higher sound-field-reconstruction accuracy.
</details>
<details>
<summary>摘要</summary>
提出一种基于神经网络的声场分解方法。该方法包括两个阶段：声场分离阶段和单源定位阶段。在第一阶段，通过多个源的声压在 Mikrophone 中合成的声场被分离成每个声源引起的一个声场。在第二阶段，源location 通过 Mikrophone 上的声压 regression 来获得估算，而不是分类。通过绿函数 simulated 生成的数据集进行训练，每个频率都有自己的神经网络。 numerics 实验表明，相比 conventient 方法，提出的方法可以实现更高的源定位精度和声场重建精度。Note: "Mikrophone" is the Simplified Chinese term for "microphone".
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Neural-Fields-as-Partially-Observed-Neural-Processes"><a href="#Generalizable-Neural-Fields-as-Partially-Observed-Neural-Processes" class="headerlink" title="Generalizable Neural Fields as Partially Observed Neural Processes"></a>Generalizable Neural Fields as Partially Observed Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06660">http://arxiv.org/abs/2309.06660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Gu, Kuan-Chieh Wang, Serena Yeung</li>
<li>for: 本研究旨在提出一种新的方法来大规模训练神经场，以便在不同的信号上进行快速和高效的表示。</li>
<li>methods: 该方法基于偏置神经网络，并利用神经过程算法来解决这个问题。</li>
<li>results: 对比 gradient-based 元学习方法和hypernetwork方法，本研究的方法在大规模训练神经场时显示出了更高的性能。<details>
<summary>Abstract</summary>
Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorithms to solve this task. We demonstrate that this approach outperforms both state-of-the-art gradient-based meta-learning approaches and hypernetwork approaches.
</details>
<details>
<summary>摘要</summary>
neural fields，代表信号为函数参数化的神经网络，是传统权值或格子型表示方式的有前途的代替方案。相比于权值或格子表示方式，神经表示方式可以规格化、连续和可多次导数，但是对于一个数据集的信号表示，需要为每个信号分别优化神经场，这是不高效的。现有的普适方法将此视为一个meta学习问题，使用梯度基本的meta学习学习初始化，然后在测试时进行优化，或者学习层次网络生成神经场的权重。我们提出了一种新的思路，视为大规模神经场训练为部分观察神经过程框架的一部分，并利用神经过程算法解决这个问题。我们示示了这种方法在比革 Gradient-based meta-学习方法和层次网络方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Dissipative-Imitation-Learning-for-Discrete-Dynamic-Output-Feedback-Control-with-Sparse-Data-Sets"><a href="#Dissipative-Imitation-Learning-for-Discrete-Dynamic-Output-Feedback-Control-with-Sparse-Data-Sets" class="headerlink" title="Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets"></a>Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06658">http://arxiv.org/abs/2309.06658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amy K. Strong, Ethan J. LoCicero, Leila J. Bridgeman</li>
<li>for: 该论文旨在实现基于模仿学习的控制器 synthesis，并提供稳定性保证，但方法通常需要大量数据和&#x2F;或已知的植物模型。</li>
<li>methods: 该论文使用输入输出稳定性方法，通过使用专家数据、粗略的IO植物模型和新的约束来实现稳定性。学习目标是非凸的，并使用迭代凸上升（ICO）和投影加速算法（PGD）来成功学习控制器。</li>
<li>results: 该论文通过应用这种新的模仿学习方法，在两个未知的植物上实现了稳定的关闭循环和成功模仿专家控制器的行为，而传统的学习动态输出反馈控制器和神经网络控制器通常无法保持稳定和达到好的性能。<details>
<summary>Abstract</summary>
Imitation learning enables the synthesis of controllers for complex objectives and highly uncertain plant models. However, methods to provide stability guarantees to imitation learned controllers often rely on large amounts of data and/or known plant models. In this paper, we explore an input-output (IO) stability approach to dissipative imitation learning, which achieves stability with sparse data sets and with little known about the plant model. A closed-loop stable dynamic output feedback controller is learned using expert data, a coarse IO plant model, and a new constraint to enforce dissipativity on the learned controller. While the learning objective is nonconvex, iterative convex overbounding (ICO) and projected gradient descent (PGD) are explored as methods to successfully learn the controller. This new imitation learning method is applied to two unknown plants and compared to traditionally learned dynamic output feedback controller and neural network controller. With little knowledge of the plant model and a small data set, the dissipativity constrained learned controller achieves closed loop stability and successfully mimics the behavior of the expert controller, while other methods often fail to maintain stability and achieve good performance.
</details>
<details>
<summary>摘要</summary>
通过依据学习，我们可以Synthesize控制器来实现复杂的目标和高度不确定的植物模型。然而，为了提供稳定性保证给依据学习获得的控制器，通常需要大量数据和/或已知的植物模型。在这篇论文中，我们探索了输入输出（IO）稳定性方法来实现不可逆依据学习，这种方法可以在稀缺数据集和具有少量知识的植物模型下实现稳定性。我们使用专家数据、粗略的IO植物模型和一个新的约束来学习一个闭环稳定的动态输出反馈控制器。虽然学习目标是非几何的，但我们使用迭代凸包练（ICO）和投影向量Descents（PGD）来成功地学习控制器。这种新的依据学习方法在两个未知的植物上应用，与传统学习的动态输出反馈控制器和神经网络控制器进行比较。即使具有少量的植物模型知识和小数据集，依据学习的控制器可以在关闭环中保持稳定性，并成功地模仿专家控制器的行为，而其他方法经常无法保持稳定性和达到好的性能。
</details></li>
</ul>
<hr>
<h2 id="Offline-Prompt-Evaluation-and-Optimization-with-Inverse-Reinforcement-Learning"><a href="#Offline-Prompt-Evaluation-and-Optimization-with-Inverse-Reinforcement-Learning" class="headerlink" title="Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning"></a>Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06553">http://arxiv.org/abs/2309.06553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holarissun/Prompt-OIRL">https://github.com/holarissun/Prompt-OIRL</a></li>
<li>paper_authors: Hao Sun</li>
<li>for: 提高大语言模型（LLMs）的效果，减少人工评估的成本</li>
<li>methods: 使用离线 inverse reinforcement learning（Inverse-RL）来评估和优化提示</li>
<li>results: 可以准确预测提示的表现，高效、低成本、生成人读ABLE的结果，可以有效地探索提示空间<details>
<summary>Abstract</summary>
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable results, and efficiently navigates the prompt space. We validate our method across four LLMs and three arithmetic datasets, highlighting its potential as a robust and effective tool for offline prompt evaluation and optimization. Our code as well as the offline datasets are released, and we highlight the Prompt-OIRL can be reproduced within a few hours using a single laptop using CPU
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的发展，如ChatGPT，已经实现了很好的性能，但是完全利用这些模型的复杂任务执行需要在自然语言提示的庞大搜索空间中穿梭。虽然提示工程学已经显示了承诺，但是需要在尝试和错误中手动制定提示，以及相关的成本带来了 significiant challenges。关键在于提示优化的效率，它取决于提示评估的成本。这个工作介绍了Prompt-OIRL，一种基于离线反冲采集学习的方法，旨在bridging提示评估的效率和可持续性之间的 gab。我们的方法利用离线数据集，从专家评估中提取出一个倒计时间的提示评估模型。Prompt-OIRL的优点包括：它预测提示的性能，成本效益，生成人类可读的结果，并快速导航提示空间。我们在四个LLM和三个数学数据集上验证了我们的方法，并 highlighted its potential as a robust and effective tool for offline prompt evaluation and optimization。我们的代码以及离线数据集都已经发布，并且在一个个人的 laptop 上使用 CPU 进行了数个小时的 reproduce。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Detection-via-Domain-Informed-Gaussian-Process-State-Space-Models"><a href="#Out-of-Distribution-Detection-via-Domain-Informed-Gaussian-Process-State-Space-Models" class="headerlink" title="Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models"></a>Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06655">http://arxiv.org/abs/2309.06655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alonso Marco, Elias Morley, Claire J. Tomlin</li>
<li>for: 本研究旨在使用学习方法让机器人在未经训练的场景中安全 Navigation。</li>
<li>methods: 本文提出了一种将现有领域知识嵌入GPSSM kernel中的新方法，以及一种基于往回预测的OoD在线监测器。</li>
<li>results: 实验结果表明，在小数据集上，具有领域知识的GPSSM kernel可以提供更高质量的回归预测，而OoD监测器在实际四足机器人内部环境中可靠地分类未经训练的地形。<details>
<summary>Abstract</summary>
In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard kernel choices. We demonstrate the effectiveness of the OoD monitor on a real quadruped navigating an indoor setting, which reliably classifies previously unseen terrains.
</details>
<details>
<summary>摘要</summary>
为了让机器人在未经训练的enario中安全 navigate，使用学习基本方法是重要的。在线上准确探测出training distribution不符（OoD）的情况是关键。近些年， Gaussian process state-space models（GPSSM）已经证明可以用来区分不warted observations。然而，模型正确地 distinguish between in-和out-of-training distribution observations取决于预测的准确性，主要受到GPSSM kernel中函数类型的影响。在这篇论文中，我们提出（i）一种将现有领域知识 embed在kernel中的新方法，以及（ii）一种在运行时 monitoring OoD的方法，基于往返 Horizon 预测。领域知识假设为一个已经收集并且分类的数据集，可以是在模拟或者使用标准模型来获得。我们的数据显示， informed kernel 可以提供更好的回归质量，只需使用小型数据集。我们还证明了这种 OoD 监控器在一个真实的四足机器人中 navigate indoor 环境中的可靠性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="ConR-Contrastive-Regularizer-for-Deep-Imbalanced-Regression"><a href="#ConR-Contrastive-Regularizer-for-Deep-Imbalanced-Regression" class="headerlink" title="ConR: Contrastive Regularizer for Deep Imbalanced Regression"></a>ConR: Contrastive Regularizer for Deep Imbalanced Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06651">http://arxiv.org/abs/2309.06651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/borealisai/conr">https://github.com/borealisai/conr</a></li>
<li>paper_authors: Mahsa Keramati, Lili Meng, R. David Evans</li>
<li>for: 提高深度异质回归模型的性能，解决深度学习模型面临异质数据的挑战。</li>
<li>methods: 提出了一种异质对准方法，通过模拟全球和本地相似性，在特征空间中模型 labels 之间的相似性，避免少数类样本的特征被折叠到多数类样本中。</li>
<li>results: 对三个大规模深度异质回归数据集进行了广泛的实验，显示 ConR 可以显著提高所有现状的状态机制方法的性能。<details>
<summary>Abstract</summary>
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penalized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly extends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on three large-scale deep imbalanced regression benchmarks. Our code is publicly available in https://github.com/BorealisAI/ConR.
</details>
<details>
<summary>摘要</summary>
偏度分布很普遍存在现实世界数据中。它们限制深度神经网络来表示少数标签，并避免强调多数标签。然而，大量的偏度方法只能处理分类标签空间，而不能有效扩展到回归问题，其标签空间是连续的。相反，地方和全局相关性在特征空间提供了有价值的信息，用于有效地模型特征空间中的关系。在这项工作中，我们提出了 ConR，一种强制对比 regularizer，它在特征空间中模型全局和地方标签相似性，并避免少数样本的特征被折叠到其多数 neighborgood中。通过对预测结果的相似性作为特征相似性的指标，ConR挖掘了标签空间和特征空间之间的不一致，并对这些不一致进行了罚款。ConR 把重要考虑因素综合化为一种普遍适用、容易集成、高效的方法，有效地解决深度偏度回归问题。此外，ConR 是已有方法的正交，可以顺利扩展到一维和多维标签空间。我们的广泛实验表明，ConR 可以在三个大规模的深度偏度回归benchmark上提高现有状态的方法表现。我们的代码可以在 <https://github.com/BorealisAI/ConR> 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/13/cs.LG_2023_09_13/" data-id="clmjn91n3008j0j881d4wai0b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/13/cs.SD_2023_09_13/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-13
        
      </div>
    </a>
  
  
    <a href="/2023/09/13/eess.IV_2023_09_13/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-13</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
