
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-13 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Can Whisper perform speech-based in-context learning paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07081 repo_url: None paper_authors: Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang for: 这种研究旨在探讨OpenAI发布的Wh">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-13">
<meta property="og:url" content="https://nullscc.github.io/2023/09/13/cs.SD_2023_09_13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Can Whisper perform speech-based in-context learning paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07081 repo_url: None paper_authors: Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang for: 这种研究旨在探讨OpenAI发布的Wh">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-13T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:20.667Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/13/cs.SD_2023_09_13/" class="article-date">
  <time datetime="2023-09-13T15:00:00.000Z" itemprop="datePublished">2023-09-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-13
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Whisper-perform-speech-based-in-context-learning"><a href="#Can-Whisper-perform-speech-based-in-context-learning" class="headerlink" title="Can Whisper perform speech-based in-context learning"></a>Can Whisper perform speech-based in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07081">http://arxiv.org/abs/2309.07081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</li>
<li>for: 这种研究旨在探讨OpenAI发布的Whisper自动语音识别（ASR）模型在语音识别任务中的上下文学习能力。</li>
<li>methods: 提出了一种基于语音的上下文学习（SICL）方法，可以在测试时进行适应，并且可以降低单个批处理语音样本的词错率（WER），无需梯度下降。</li>
<li>results: 对中文方言进行语言级适应实验，使用Whisper模型的任何大小在两种方言上实现了Consistent和较大的相对WER减少率，平均减少32.3%。此外，基于k-最近邻的受过过滤的上下文示例选择技术可以进一步提高SICL的效率，可以增加平均相对WER减少率至36.4%。<details>
<summary>Abstract</summary>
This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to shed light on SICL's adaptability to phonological variances and dialect-specific lexical nuances.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Flexible-Online-Framework-for-Projection-Based-STFT-Phase-Retrieval"><a href="#A-Flexible-Online-Framework-for-Projection-Based-STFT-Phase-Retrieval" class="headerlink" title="A Flexible Online Framework for Projection-Based STFT Phase Retrieval"></a>A Flexible Online Framework for Projection-Based STFT Phase Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07043">http://arxiv.org/abs/2309.07043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Peer, Simon Welker, Johannes Kolhoff, Timo Gerkmann</li>
<li>for: 提高iterative STFT阶段的phaserecovery性能，尤其是在speech communication应用中。</li>
<li>methods: 使用相同的投影运算符，但 combinates them in innovative ways，以提高重建质量和需要的迭代次数，同时保持相同的计算复杂性。</li>
<li>results: 在speech signals上evaluation results show that these algorithms can achieve a considerable performance gain compared to RTISI。<details>
<summary>Abstract</summary>
Several recent contributions in the field of iterative STFT phase retrieval have demonstrated that the performance of the classical Griffin-Lim method can be considerably improved upon. By using the same projection operators as Griffin-Lim, but combining them in innovative ways, these approaches achieve better results in terms of both reconstruction quality and required number of iterations, while retaining a similar computational complexity per iteration. However, like Griffin-Lim, these algorithms operate in an offline manner and thus require an entire spectrogram as input, which is an unrealistic requirement for many real-world speech communication applications. We propose to extend RTISI -- an existing online (frame-by-frame) variant of the Griffin-Lim algorithm -- into a flexible framework that enables straightforward online implementation of any algorithm based on iterative projections. We further employ this framework to implement online variants of the fast Griffin-Lim algorithm, the accelerated Griffin-Lim algorithm, and two algorithms from the optics domain. Evaluation results on speech signals show that, similarly to the offline case, these algorithms can achieve a considerable performance gain compared to RTISI.
</details>
<details>
<summary>摘要</summary>
Recent contributions to iterative STFT phase retrieval have shown that the classical Griffin-Lim method can be significantly improved upon. These approaches use the same projection operators as Griffin-Lim, but combine them in new ways to achieve better reconstruction quality and faster convergence, while maintaining similar computational complexity per iteration. However, like Griffin-Lim, these algorithms operate in an offline manner and require the entire spectrogram as input, which is unrealistic for many real-world speech communication applications. We propose to extend RTISI, an existing online (frame-by-frame) variant of the Griffin-Lim algorithm, into a flexible framework that enables straightforward online implementation of any algorithm based on iterative projections. We further employ this framework to implement online variants of the fast Griffin-Lim algorithm, the accelerated Griffin-Lim algorithm, and two algorithms from the optics domain. Evaluation results on speech signals show that these algorithms can achieve a considerable performance gain compared to RTISI.
</details></li>
</ul>
<hr>
<h2 id="Reorganization-of-the-auditory-perceptual-space-across-the-human-vocal-range"><a href="#Reorganization-of-the-auditory-perceptual-space-across-the-human-vocal-range" class="headerlink" title="Reorganization of the auditory-perceptual space across the human vocal range"></a>Reorganization of the auditory-perceptual space across the human vocal range</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06946">http://arxiv.org/abs/2309.06946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Friedrichs, Volker Dellwo</li>
<li>for: 本研究探讨了人类语音范围内（220-1046 Hz）的听觉空间，使用多维度排序分析法分析了从 Friedrichs et al. (2017) 所study的cochlea扩展的spectrum。</li>
<li>methods: 研究使用了250毫秒的vowel段和3名德国女性speaker的数据集，包括&#x2F;i y e {\o}  {\epsilon} a o u&#x2F;（共240个音）。</li>
<li>results: 研究发现，在一个closed-set认知任务中，listeners recognition的&#x2F;i a u&#x2F; 在基本频率（fo）附近1kHz的情况下明显高于其他元音的认知，而其他元音在更高的频率下的认知则降低。此外，研究还发现了元音高低的系统性 spectral shift，特别是在523Hz以上的情况下，这些观察结果强调了元音的spectral shape在听觉中的重要性，同时也提供了语言演化中元音的可能影响。<details>
<summary>Abstract</summary>
We analyzed the auditory-perceptual space across a substantial portion of the human vocal range (220-1046 Hz) using multidimensional scaling analysis of cochlea-scaled spectra from 250-ms vowel segments, initially studied in Friedrichs et al. (2017) J. Acoust. Soc. Am. 142 1025-1033. The dataset comprised the vowels /i y e {\o} {\epsilon} a o u/ (N=240) produced by three native German female speakers, encompassing a broad range of their respective voice frequency ranges. The initial study demonstrated that, during a closed-set identification task involving 21 listeners, the point vowels /i a u/ were significantly recognized at fundamental frequencies (fo) nearing 1 kHz, whereas the recognition of other vowels decreased at higher pitches. Building on these findings, our study revealed systematic spectral shifts associated with vowel height and frontness as fo increased, with a notable clustering around /i a u/ above 523 Hz. These observations underscore the pivotal role of spectral shape in vowel perception, illustrating the reliance on acoustic anchors at higher pitches. Furthermore, this study sheds light on the quantal nature of these vowels and their potential impact on language evolution, offering a plausible explanation for their widespread presence in the world's languages.
</details>
<details>
<summary>摘要</summary>
我们通过多维度规划分析了人声 vocal 范围（220-1046 Hz）中的听觉空间，使用 Friedrichs et al. (2017) J. Acoust. Soc. Am. 142 1025-1033 中提供的cochlea扩大的spectra，分析了250毫秒的vowel段。数据集包括了3名德国女性 speaker 所 произноси的4个元音 /i y e ø æ/（共240个），涵盖了它们的声音频谱范围。初始研究显示，在一个关闭的设置标识任务中，listeners 中的21名listeners 能够准确地识别基本频率（fo）接近1kHz的点元音 /i a u/，而其他元音的识别则随着高频的减少。在这些发现基础上，我们的研究发现，随着高频增加，元音的高度和前端性呈现出系统性的spectral shift，特别是在523Hz以上的位置。这些观察结果提醒我们，听觉元音的形态在高频下具有重要作用，而且listeners 会利用高频的acoustic anchors来识别元音。此外，这种研究还暴露了元音的量化性和其可能对语言演化的影响，提供了可能解释语言中的广泛存在的可能性。
</details></li>
</ul>
<hr>
<h2 id="VRDMG-Vocal-Restoration-via-Diffusion-Posterior-Sampling-with-Multiple-Guidance"><a href="#VRDMG-Vocal-Restoration-via-Diffusion-Posterior-Sampling-with-Multiple-Guidance" class="headerlink" title="VRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple Guidance"></a>VRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06934">http://arxiv.org/abs/2309.06934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Hernandez-Olivan, Koichi Saito, Naoki Murata, Chieh-Hsin Lai, Marco A. Martínez-Ramirez, Wei-Hsiang Liao, Yuki Mitsufuji</li>
<li>for: 提高音频质量，进行下游音乐修饰</li>
<li>methods:  diffusion posterior sampling (DPS) 以及其他多种推导方法，如 RePaint (RP) 策略和 Pseudoinverse-Guided Diffusion Models ($\Pi$GDM)</li>
<li>results: 在各种损害和切割频率下，对 vocals 修复和宽频扩展任务中，我们的方法表现出色，超过了目前的 DPS-based 音频修复标准。可参考 \url{<a target="_blank" rel="noopener" href="http://carlosholivan.github.io/demos/audio-restoration-2023.html%7D">http://carlosholivan.github.io/demos/audio-restoration-2023.html}</a> 获取修复后的音频示例。<details>
<summary>Abstract</summary>
Restoring degraded music signals is essential to enhance audio quality for downstream music manipulation. Recent diffusion-based music restoration methods have demonstrated impressive performance, and among them, diffusion posterior sampling (DPS) stands out given its intrinsic properties, making it versatile across various restoration tasks. In this paper, we identify that there are potential issues which will degrade current DPS-based methods' performance and introduce the way to mitigate the issues inspired by diverse diffusion guidance techniques including the RePaint (RP) strategy and the Pseudoinverse-Guided Diffusion Models ($\Pi$GDM). We demonstrate our methods for the vocal declipping and bandwidth extension tasks under various levels of distortion and cutoff frequency, respectively. In both tasks, our methods outperform the current DPS-based music restoration benchmarks. We refer to \url{http://carlosholivan.github.io/demos/audio-restoration-2023.html} for examples of the restored audio samples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将降低音质的音乐信号恢复为重要的音乐处理前期任务。最近的扩散基于音乐恢复方法有出色表现，其中扩散 posterior 抽样（DPS）在多种恢复任务中表现出色，因此在这篇论文中，我们认为DPS有潜在的问题，这些问题会降低当前DPS基于的音乐恢复方法的性能。我们介绍了一种通过多种扩散指导技术，包括RePaint（RP）策略和 Pseudoinverse-Guided Diffusion Models（$\Pi$GDM）来 Mitigate these issues。我们在 vocals 剪辑和带宽扩展任务中应用了我们的方法，并在不同的扭曲和阈值频率下进行了评估。在两个任务中，我们的方法超过了当前DPS基于的音乐恢复标准准。更多细节请参考 \url{http://carlosholivan.github.io/demos/audio-restoration-2023.html} 获取 restore 后的音频样本。
</details></li>
</ul>
<hr>
<h2 id="EMALG-An-Enhanced-Mandarin-Lombard-Grid-Corpus-with-Meaningful-Sentences"><a href="#EMALG-An-Enhanced-Mandarin-Lombard-Grid-Corpus-with-Meaningful-Sentences" class="headerlink" title="EMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful Sentences"></a>EMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful Sentences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06858">http://arxiv.org/abs/2309.06858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baifeng Li, Qingmu Liu, Yuhong Yang, Hongyang Chen, Weiping Tu, Song Lin</li>
<li>for: This study investigates the Lombard effect in Mandarin Chinese, specifically the impact of meaningful sentences on the Lombard effect and the differences between male and female speakers.</li>
<li>methods: The study uses an enhanced Mandarin Lombard grid (EMALG) corpus with 34 speakers and improved recording setups to address challenges faced by the previous Mandarin Lombard grid (MALG) corpus.</li>
<li>results: The findings show that female speakers exhibit a more pronounced Lombard effect than male speakers, particularly when uttering meaningful sentences. Additionally, the study found that nonsense sentences negatively impact Lombard effect analysis, and the results are consistent with previous research comparing the Lombard effect in English and Mandarin.Here are the three pieces of information in Simplified Chinese text:</li>
<li>for: 这个研究研究了托尔曼效应在普通话中，具体来说是研究意义语句对托尔曼效应的影响以及男女 speaker之间的差异。</li>
<li>methods: 这个研究使用了提高后的普通话托尔曼格子(EMALG) corpus，有34名 speaker，并使用改进的录音设置来解决前一个普通话托尔曼格子(MALG) corpus 面临的挑战。</li>
<li>results: 研究发现，女性 speaker在意义语句中更为明显地表现出托尔曼效应，而男性 speaker则较为弱。此外，研究还发现，无意义语句对托尔曼效应的分析有负面影响。此外，研究结果与之前英语和普通话之间的托尔曼效应比较的研究结果一致。<details>
<summary>Abstract</summary>
This study investigates the Lombard effect, where individuals adapt their speech in noisy environments. We introduce an enhanced Mandarin Lombard grid (EMALG) corpus with meaningful sentences , enhancing the Mandarin Lombard grid (MALG) corpus. EMALG features 34 speakers and improves recording setups, addressing challenges faced by MALG with nonsense sentences. Our findings reveal that in Mandarin, female exhibit a more pronounced Lombard effect than male, particularly when uttering meaningful sentences. Additionally, we uncover that nonsense sentences negatively impact Lombard effect analysis. Moreover, our results reaffirm the consistency in the Lombard effect comparison between English and Mandarin found in previous research.
</details>
<details>
<summary>摘要</summary>
这个研究调查了洛伯达效应，即在噪音环境中人们的语言适应。我们介绍了改进过的满语 Lombard 格式（EMALG）词库，包含有意义的句子，从而提高了满语 Lombard 格式（MALG）词库。EMALG 包含34名说话者，并改进了录音设备，解决了 MAGL 面临的无意义句子问题。我们的发现显示，在满语中，女性更加明显地表现出洛伯达效应，特别是当发表有意义的句子时。此外，我们还发现，无意义句子对洛伯达效应分析产生负面影响。此外，我们的结果证明了之前研究发现的洛伯达效应在英语和满语之间的一致性。
</details></li>
</ul>
<hr>
<h2 id="DCTTS-Discrete-Diffusion-Model-with-Contrastive-Learning-for-Text-to-speech-Generation"><a href="#DCTTS-Discrete-Diffusion-Model-with-Contrastive-Learning-for-Text-to-speech-Generation" class="headerlink" title="DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation"></a>DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06787">http://arxiv.org/abs/2309.06787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Wu, Qiulin Li, Sixing Liu, Qun Yang</li>
<li>for: 本文提出了一种基于抽象空间的粒子扩散模型 для文本译语 synthesis，以提高扩散模型的计算成本和批处速度。</li>
<li>methods: 本文使用了一种基于抽象空间的粒子扩散模型，并采用了对比学习方法来增强语音和文本之间的对应关系，从而提高扩散速度和质量。此外，文本编码器也被使用以简化模型参数和提高计算效率。</li>
<li>results: 实验结果显示，提出的方法可以实现出色的语音生成质量和批处速度，同时减少了扩散模型的计算成本和资源消耗。实验结果可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/lawtherWu/DCTTS%E3%80%82">https://github.com/lawtherWu/DCTTS。</a><details>
<summary>Abstract</summary>
In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.
</details>
<details>
<summary>摘要</summary>
在文本到语音（TTS）任务中，抽象扩散模型具有优秀的准确性和通用性，但它的资源消耗和推理速度总是一个挑战。这篇论文提出了粒子扩散模型与对比学习 для文本到语音生成（DCTTS）。这个论文的贡献包括：1. 基于粒子空间的TTS扩散模型可以显著降低扩散模型的计算摄用量和提高抽取速度;2. 基于粒子空间的对比学习方法可以增强语音和文本之间的对应关系，提高抽取质量;3. 它使用高效的文本编码器，简化模型的参数和提高计算效率。实验结果表明，提出的方法可以在保持高质量语音生成的同时，显著降低扩散模型的资源消耗。生成的示例可以在https://github.com/lawtherWu/DCTTS中找到。
</details></li>
</ul>
<hr>
<h2 id="Distinguishing-Neural-Speech-Synthesis-Models-Through-Fingerprints-in-Speech-Waveforms"><a href="#Distinguishing-Neural-Speech-Synthesis-Models-Through-Fingerprints-in-Speech-Waveforms" class="headerlink" title="Distinguishing Neural Speech Synthesis Models Through Fingerprints in Speech Waveforms"></a>Distinguishing Neural Speech Synthesis Models Through Fingerprints in Speech Waveforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06780">http://arxiv.org/abs/2309.06780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chu Yuan Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Xinrui Yan</li>
<li>for: 本研究旨在提高声音合成技术中的识别源能力，以应对不良用途的威胁和违反。</li>
<li>methods: 本研究使用多种说话者的 LibriTTS 数据集，研究生成的声音波形中的模型印记的存在和对整体声音波形的影响。</li>
<li>results: 研究发现，模型和 vocoder 都会印记生成的声音波形，但是 vocoder 的印记更为主要，可能会隐藏 acoustic model 的印记。这些发现表明模型印记在声音合成中具有广泛的应用前景。<details>
<summary>Abstract</summary>
Recent strides in neural speech synthesis technologies, while enjoying widespread applications, have nonetheless introduced a series of challenges, spurring interest in the defence against the threat of misuse and abuse. Notably, source attribution of synthesized speech has value in forensics and intellectual property protection, but prior work in this area has certain limitations in scope. To address the gaps, we present our findings concerning the identification of the sources of synthesized speech in this paper. We investigate the existence of speech synthesis model fingerprints in the generated speech waveforms, with a focus on the acoustic model and the vocoder, and study the influence of each component on the fingerprint in the overall speech waveforms. Our research, conducted using the multi-speaker LibriTTS dataset, demonstrates two key insights: (1) vocoders and acoustic models impart distinct, model-specific fingerprints on the waveforms they generate, and (2) vocoder fingerprints are the more dominant of the two, and may mask the fingerprints from the acoustic model. These findings strongly suggest the existence of model-specific fingerprints for both the acoustic model and the vocoder, highlighting their potential utility in source identification applications.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the identification of the sources of synthesized speech, with a particular focus on the existence of speech synthesis model fingerprints in the generated speech waveforms. We investigate the contributions of both the acoustic model and the vocoder to the fingerprints, and examine how each component influences the overall waveform.Our research uses the multi-speaker LibriTTS dataset, and we find two key insights. First, both the acoustic model and the vocoder impart distinct, model-specific fingerprints on the waveforms they generate. Second, the vocoder fingerprints are more dominant and may mask the fingerprints from the acoustic model. These findings suggest that both the acoustic model and the vocoder have the potential to be used for source identification, and that the vocoder fingerprints may be particularly useful in this context.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Foundation-models-for-Unsupervised-Audio-Visual-Segmentation"><a href="#Leveraging-Foundation-models-for-Unsupervised-Audio-Visual-Segmentation" class="headerlink" title="Leveraging Foundation models for Unsupervised Audio-Visual Segmentation"></a>Leveraging Foundation models for Unsupervised Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06728">http://arxiv.org/abs/2309.06728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Xiatian Zhu</li>
<li>for: This paper aims to develop an unsupervised audio-visual segmentation method, which can precisely outline audible objects in a visual scene at the pixel level without requiring fine-grained annotations of audio-mask pairs.</li>
<li>methods: The proposed method, called Cross-Modality Semantic Filtering (CMSF), leverages off-the-shelf multi-modal foundation models (e.g., detection, open-world segmentation, and multi-modal alignment) to accurately associate the underlying audio-mask pairs. The method has two training-free variants: AT-GDINO-SAM and OWOD-BIND.</li>
<li>results: Extensive experiments on the AVS-Bench dataset show that the unsupervised approach proposed in this paper can perform well in comparison to prior art supervised counterparts across complex scenarios with multiple auditory objects, and can excel in accurately segmenting overlapped auditory objects.<details>
<summary>Abstract</summary>
Audio-Visual Segmentation (AVS) aims to precisely outline audible objects in a visual scene at the pixel level. Existing AVS methods require fine-grained annotations of audio-mask pairs in supervised learning fashion. This limits their scalability since it is time consuming and tedious to acquire such cross-modality pixel level labels. To overcome this obstacle, in this work we introduce unsupervised audio-visual segmentation with no need for task-specific data annotations and model training. For tackling this newly proposed problem, we formulate a novel Cross-Modality Semantic Filtering (CMSF) approach to accurately associate the underlying audio-mask pairs by leveraging the off-the-shelf multi-modal foundation models (e.g., detection [1], open-world segmentation [2] and multi-modal alignment [3]). Guiding the proposal generation by either audio or visual cues, we design two training-free variants: AT-GDINO-SAM and OWOD-BIND. Extensive experiments on the AVS-Bench dataset show that our unsupervised approach can perform well in comparison to prior art supervised counterparts across complex scenarios with multiple auditory objects. Particularly, in situations where existing supervised AVS methods struggle with overlapping foreground objects, our models still excel in accurately segmenting overlapped auditory objects. Our code will be publicly released.
</details>
<details>
<summary>摘要</summary>
audio-visual分割（AVS）目标是在视觉场景中像素级准确定义可听对象。现有的AVS方法需要在监督学习方式下精细标注音频mask对。这限制了其可扩展性，因为获得这种跨ModalPixel级别标注是时间consuming和繁琐的。为解决这个问题，在这里我们介绍了无监督音频视觉分割方法，无需任务特定数据标注和模型训练。为解决这个新提出的问题，我们提出了一种新的跨ModalSemantic过滤（CMSF）方法，以准确关联下方音频mask对，利用市面上多Modal基础模型（例如检测 [1]、开放世界分割 [2]和多Modal匹配 [3]）。通过音频或视觉提示，我们设计了两种没有训练的变种：AT-GDINO-SAM和OWOD-BIND。广泛的实验表明，我们的无监督方法可以与先前的监督AVS方法相比，在复杂的场景中表现良好，特别是当存在多个混合对象时。我们的代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="PIAVE-A-Pose-Invariant-Audio-Visual-Speaker-Extraction-Network"><a href="#PIAVE-A-Pose-Invariant-Audio-Visual-Speaker-Extraction-Network" class="headerlink" title="PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network"></a>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06723">http://arxiv.org/abs/2309.06723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Liu, Meng Ge, Zhizheng Wu, Haizhou Li</li>
<li>for: This paper focuses on improving audio-visual speaker extraction by incorporating a pose-invariant view to handle varying talking faces.</li>
<li>methods: The proposed Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) generates a pose-invariant view from each original pose orientation, creating a multi-view visual input for the speaker.</li>
<li>results: Experimental results on the multi-view MEAD and in-the-wild LRS3 datasets show that PIAVE outperforms the state-of-the-art and is more robust to pose variations.<details>
<summary>Abstract</summary>
It is common in everyday spoken communication that we look at the turning head of a talker to listen to his/her voice. Humans see the talker to listen better, so do machines. However, previous studies on audio-visual speaker extraction have not effectively handled the varying talking face. This paper studies how to take full advantage of the varying talking face. We propose a Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) that incorporates an additional pose-invariant view to improve audio-visual speaker extraction. Specifically, we generate the pose-invariant view from each original pose orientation, which enables the model to receive a consistent frontal view of the talker regardless of his/her head pose, therefore, forming a multi-view visual input for the speaker. Experiments on the multi-view MEAD and in-the-wild LRS3 dataset demonstrate that PIAVE outperforms the state-of-the-art and is more robust to pose variations.
</details>
<details>
<summary>摘要</summary>
通常在日常口语交流中，我们会看向讲话人的头部来听到他/她的voice。人类看到讲话人，以便更好地听到他/她的voice，同样地，机器也会这样做。然而，过去关于音频视频说话人提取的研究没有有效地处理变化的讲话面孔。这篇论文研究如何利用变化的讲话面孔来提高音频视频说话人提取。我们提议一种pose-invariant audio-visual speaker extraction network（PIAVE），该网络包含一个额外的pose-invariant视图，以提高音频视频说话人提取的精度。具体来说，我们将每个原始的poseorientation中生成一个pose-invariant视图，使得模型能够收到不同poseorientation下的讲话人的一致前视图，从而形成多视图的视觉输入。实验表明，PIAVE在多视图MEAD和野外LRS3 dataset上比前一些状态的方法更高效和更抗pose变化。
</details></li>
</ul>
<hr>
<h2 id="Attention-based-Encoder-Decoder-End-to-End-Neural-Diarization-with-Embedding-Enhancer"><a href="#Attention-based-Encoder-Decoder-End-to-End-Neural-Diarization-with-Embedding-Enhancer" class="headerlink" title="Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer"></a>Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06672">http://arxiv.org/abs/2309.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Chen, Bing Han, Shuai Wang, Yanmin Qian</li>
<li>for: 这个论文的目的是提出一种简单的注意力基本网络 для端到端神经网络演算（AED-EEND），以提高Speaker分类的性能。</li>
<li>methods: 这个论文使用了教师强制策略来解决Speaker permutation问题，并使用了迭代解码方法来输出每个Speaker的演算结果。此外，它还提出了一个增强器模块来增强每帧的Speaker嵌入，以处理未见数量的Speaker。</li>
<li>results: 该论文的实验结果表明，使用 simulated 训练数据，可以提高Consistency性能。此外，该论文的最佳系统在CALLHOME、DIHARD II 和 AMI 等评测标准上达到了新的状态态-of-the-art（DER）性能，无需使用oracle Voice Activity Detection（VAD）。此外，该论文的 AED-EEND 系统还表现出了极高的Speech类型检测性能。<details>
<summary>Abstract</summary>
Deep neural network-based systems have significantly improved the performance of speaker diarization tasks. However, end-to-end neural diarization (EEND) systems often struggle to generalize to scenarios with an unseen number of speakers, while target speaker voice activity detection (TS-VAD) systems tend to be overly complex. In this paper, we propose a simple attention-based encoder-decoder network for end-to-end neural diarization (AED-EEND). In our training process, we introduce a teacher-forcing strategy to address the speaker permutation problem, leading to faster model convergence. For evaluation, we propose an iterative decoding method that outputs diarization results for each speaker sequentially. Additionally, we propose an Enhancer module to enhance the frame-level speaker embeddings, enabling the model to handle scenarios with an unseen number of speakers. We also explore replacing the transformer encoder with a Conformer architecture, which better models local information. Furthermore, we discovered that commonly used simulation datasets for speaker diarization have a much higher overlap ratio compared to real data. We found that using simulated training data that is more consistent with real data can achieve an improvement in consistency. Extensive experimental validation demonstrates the effectiveness of our proposed methodologies. Our best system achieved a new state-of-the-art diarization error rate (DER) performance on all the CALLHOME (10.08%), DIHARD II (24.64%), and AMI (13.00%) evaluation benchmarks, when no oracle voice activity detection (VAD) is used. Beyond speaker diarization, our AED-EEND system also shows remarkable competitiveness as a speech type detection model.
</details>
<details>
<summary>摘要</summary>
深度神经网络基于系统在说话人分类任务中表现得非常出色。然而，末端神经网络分类（EEND）系统经常在未见到speaker数量的情况下难以泛化，而目标说话人活动检测（TS-VAD）系统往往过于复杂。在这篇论文中，我们提出了一种简单的注意力基于编码器-解码器网络 для末端神经网络分类（AED-EEND）。在我们的训练过程中，我们引入了一种教师强制策略，以解决说话人排序问题，从而快速适应模型。为评价，我们提出了一种逐个输出每个说话人的排序结果的迭代解码方法。此外，我们还提出了增强器模块，以提高帧级别的说话人嵌入，使模型能够处理未见到的说话人数量。此外，我们还发现了常用的 simulate datasets 的重要性。我们发现，使用更加符合实际数据的训练数据可以实现更好的一致性。我们的最佳系统在所有 CALLHOME (10.08%), DIHARD II (24.64%), 和 AMI (13.00%) 评价标准上达到了新的state-of-the-art diarization error rate（DER）性能，无需使用 oracle voice activity detection（VAD）。此外，我们的 AED-EEND 系统还表现出了极高的语音类型检测能力。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Modelling-of-Percussive-Audio-with-Transient-and-Spectral-Synthesis"><a href="#Differentiable-Modelling-of-Percussive-Audio-with-Transient-and-Spectral-Synthesis" class="headerlink" title="Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis"></a>Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06649">http://arxiv.org/abs/2309.06649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordie Shier, Franco Caspe, Andrew Robertson, Mark Sandler, Charalampos Saitis, Andrew McPherson</li>
<li>for: This paper aims to address transient generation and percussive synthesis within a differentiable digital signal processing (DDSP) framework.</li>
<li>methods: The proposed method builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. The method uses a modified sinusoidal peak picking algorithm and differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds.</li>
<li>results: The method leads to improved onset signal reconstruction for membranophone percussion instruments, as shown by a set of reconstruction metrics computed using a large dataset of acoustic and electronic percussion samples.<details>
<summary>Abstract</summary>
Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.
</details>
<details>
<summary>摘要</summary>
diferenciable procesamiento de señales digitales (DDSP) técnicas, incluyendo métodos para la síntesis de audio, han recibido atención en los últimos años y se prestan a la interpretabilidad en el espacio de parámetros. Sin embargo, los métodos de síntesis diferenciable actuales no han buscado explícitamente modelar la parte transitoria de las señales, lo cual es importante para los sonidos percusivos. En este trabajo, presentamos un marco de síntesis unificada que busca abordar la generación de transientes y la síntesis percusiva dentro de un marco DDSP. Para lograr esto, propusimos un modelo de síntesis percusiva que se basa en la modelización sinusoidal y incorpora una red convolutional temporal modulada para la generación de transientes. Usamos un algoritmo de selección de picos sinusoidales modificado para generar ondas no armónicas tiempo-varias y las acompañamos con codificadores de ruido y transientes diferenciables que se entrenan juntos para reconstruir sonidos de platillos. Computamos un conjunto de métricas de reconstrucción utilizando una gran base de datos de muestras de percusión acústica y electrónica que demuestran que nuestro método lleva a una reconstrucción más precisa de la señal de onset para los instrumentos membranófonos.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/13/cs.SD_2023_09_13/" data-id="clmjn91ok00cb0j885wwiaiy4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/09/13/cs.LG_2023_09_13/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-13</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
