
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-12 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Bregman Graph Neural Network paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.06645 repo_url: None paper_authors: Jiayu Zhai, Lequan Lin, Dai Shi, Junbin Gao for: 本研究旨在提高图 neural network (GNN) 的表现，特别是在节点分类任务中避免过度">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-12">
<meta property="og:url" content="https://nullscc.github.io/2023/09/12/cs.LG_2023_09_12/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Bregman Graph Neural Network paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.06645 repo_url: None paper_authors: Jiayu Zhai, Lequan Lin, Dai Shi, Junbin Gao for: 本研究旨在提高图 neural network (GNN) 的表现，特别是在节点分类任务中避免过度">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-12T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:20.155Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.LG_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T10:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-12
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bregman-Graph-Neural-Network"><a href="#Bregman-Graph-Neural-Network" class="headerlink" title="Bregman Graph Neural Network"></a>Bregman Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06645">http://arxiv.org/abs/2309.06645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Zhai, Lequan Lin, Dai Shi, Junbin Gao</li>
<li>for: 本研究旨在提高图 neural network (GNN) 的表现，特别是在节点分类任务中避免过度简化节点表示和标签，以避免过度融合和错误分类。</li>
<li>methods: 本研究提出了一种基于布格曼距离的双层优化框架，通过引入一种类似于“跳过连接”的机制，以避免GNN层中的过度简化问题。</li>
<li>results: 经验研究表明，布格曼增强的GNN可以有效地避免过度简化问题，并在同性和不同性图上显示出较好的表现。此外，研究还发现布格曼GNN可以在层数较多的情况下产生更稳定的学习结果，表明提出的方法有效。<details>
<summary>Abstract</summary>
Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers is high, suggesting the effectiveness of the proposed method in alleviating the over-smoothing issue.
</details>
<details>
<summary>摘要</summary>
多个最近的研究对图 нейрон网络（GNNs）都集中在将GNN架构设为优化问题的假设上，然而在节点分类任务中，GNN所引入的缓和效应会使连接节点的表示被折衣和混同，导致过度缓和和错分问题。在这篇论文中，我们提出了一种新的二级优化框架 для GNNs， inspirited by the notion of Bregman distance。我们证明了GNN层按照这种方法提出的可以有效地解决过度缓和问题，并且在具有多层的情况下，Bregman GNNs还能够提供更加稳定的学习精度。我们通过了广泛的实验研究，证明了我们的理论结果，并且发现在同类和不同类图中，Bregman GNNs都能够超越原始GNNs的性能。此外，我们的实验还表明，当层数较高时，Bregman GNNs能够更加稳定地学习，这表明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models"><a href="#Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models" class="headerlink" title="Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models"></a>Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06642">http://arxiv.org/abs/2309.06642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi<br>for:* The paper is written for solving inverse problems, specifically focusing on the difficulty of reconstruction tasks and the need for adaptive compute power.methods:* The paper proposes a novel method called “severity encoding” to estimate the degradation severity of noisy, degraded signals in the latent space of an autoencoder.* The method leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and achieve sample-adaptive inference times.results:* The paper demonstrates that the proposed method achieves performance comparable to state-of-the-art diffusion-based techniques, with significant improvements in computational efficiency.Here’s the simplified Chinese translation of the three key points:for:* 本文是为解决反射问题而写的，特别是关注重建任务的难度和计算能力的调整。methods:* 本文提出了一种新的方法，即”严重编码”，用于在自编码器的幂值空间中估计受损信号的严重程度。* 该方法利用预测的损害严重程度来细化反射抽取的轨迹，以实现采样适应的计算时间。results:* 本文表明，提出的方法可以与当前最佳的扩散基本技术相比，在计算效率方面获得显著提高。<details>
<summary>Abstract</summary>
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose a novel method that we call severity encoding, to estimate the degradation severity of noisy, degraded signals in the latent space of an autoencoder. We show that the estimated severity has strong correlation with the true corruption level and can give useful hints at the difficulty of reconstruction problems on a sample-by-sample basis. Furthermore, we propose a reconstruction method based on latent diffusion models that leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and thus achieve sample-adaptive inference times. We utilize latent diffusion posterior sampling to maintain data consistency with observations. We perform experiments on both linear and nonlinear inverse problems and demonstrate that our technique achieves performance comparable to state-of-the-art diffusion-based techniques, with significant improvements in computational efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的推理方法在各种恢复任务中表现不佳，主要是因为它们缺乏适应性。在这篇论文中，我们提出了一种新的方法，即“严重度编码”，可以在循环神经网络中估计干扰严重程度。我们发现，严重度编码对干扰严重程度具有强相关性，并且可以为每个样本提供有用的干扰难度提示。此外，我们还提出了一种基于涉及扩散模型的重建方法，该方法利用预测的严重度编码来精制推理时间。我们使用涉及扩散 posterior 抽样来保持数据一致性。我们在线性和非线性恢复任务中进行了实验，并证明了我们的技术可以与当前状态态别的推理方法相比，具有显著的计算效率提高。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Data-Center-Perspectives"><a href="#Quantum-Data-Center-Perspectives" class="headerlink" title="Quantum Data Center: Perspectives"></a>Quantum Data Center: Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06641">http://arxiv.org/abs/2309.06641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Liang Jiang</li>
<li>for: Quantum Data Center (QDC) is designed to provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing.</li>
<li>methods: QDC combines Quantum Random Access Memory (QRAM) and quantum networks.</li>
<li>results: QDC has the potential to impact business and science, especially in the machine learning and big data industries.Here’s the text in Simplified Chinese:</li>
<li>for: 量子数据中心（QDC）提供了效率、安全性和精度等多种优势，有助于量子计算、通信和探测等领域。</li>
<li>methods: QDC结合量子随机访问存储器（QRAM）和量子网络。</li>
<li>results: QDC具有业务和科学领域的潜在影响，特别是机器学习和大数据领域。<details>
<summary>Abstract</summary>
A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
</details>
<details>
<summary>摘要</summary>
一个量子化的数据中心可能在量子时代发挥重要作用。在这篇论文中，我们介绍了量子数据中心（QDC），它是现有的类型数据中心的量子版本，强调了结合量子随机存储（QRAM）和量子网络的组合。我们认为，QDC将为客户提供高效、安全、精度的好处，并对量子计算、通信和探测产生重要影响。我们通过硬件实现和可能的具体应用来研究这一新的研究方向的科学和商业机会。我们显示了QDC在业务和科学领域的可能影响，特别是机器学习和大数据领域。
</details></li>
</ul>
<hr>
<h2 id="G-Mapper-Learning-a-Cover-in-the-Mapper-Construction"><a href="#G-Mapper-Learning-a-Cover-in-the-Mapper-Construction" class="headerlink" title="$G$-Mapper: Learning a Cover in the Mapper Construction"></a>$G$-Mapper: Learning a Cover in the Mapper Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06634">http://arxiv.org/abs/2309.06634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Alvarado, Robin Belton, Emily Fischer, Kang-Ju Lee, Sourabh Palande, Sarah Percival, Emilie Purvine</li>
<li>for: 这篇论文是关于图像数据分析（TDA）中的Mapper算法可视化技术，其中Mapper算法需要调整多个参数以生成一个”漂亮”的Mapper图。本论文关注选择cover参数。</li>
<li>methods: 我们提出了一种基于$G$-means归一化 clustering的算法，通过逐次进行安德森-达瑞尔测试来选择最佳的cover。我们的拆分过程使用 Gaussian mixture model，以便精心地选择cover根据数据的分布。</li>
<li>results: 我们在 synthetic 和实际数据上进行了实验，结果显示我们的算法可以生成一个保留数据的核心特征的Mapper图。<details>
<summary>Abstract</summary>
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
</details>
<details>
<summary>摘要</summary>
Mapper算法是一种可视化技术在拓扑数据分析（TDA）中，输出一个图表示数据集的结构。Mapper算法需要调整一些参数以生成一个"漂亮"的Mapper图。本文关注选择覆盖参数。我们提出一种算法，通过 repeatedly splitting a cover based on a statistical test for normality，来优化Mapper图的覆盖。我们的拆分过程使用Gaussian mixture model，以便选择精心的覆盖，基于数据的分布。实验表明，我们的算法可以生成覆盖，使得Mapper图保留数据的本质。
</details></li>
</ul>
<hr>
<h2 id="Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning"><a href="#Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning" class="headerlink" title="Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning"></a>Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06628">http://arxiv.org/abs/2309.06628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples">https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples</a></li>
<li>paper_authors: Atticus Beachy, Harok Bae, Jose Camberos, Ramana Grandhi</li>
<li>for: 用于快速trainnear-instantaneously emulator embedded neural network，以便在goal-oriented adaptive learning中减少计算成本。</li>
<li>methods: 使用rapid neural network paradigm，只对最后层连接网络进行微调，通过线性回归技术进行训练。</li>
<li>results: 在多个分析示例和一个Generic hypersonic vehicle飞行参数研究中，提出了一种快速trainnear-instantaneously的Emulator embedded neural network方法，可以减少计算成本，而且Prediction accuracy不受影响。<details>
<summary>Abstract</summary>
Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-based backpropagation, rapid neural network training adjusts only the last layer connection weights by applying a linear regression technique. It is found that the proposed emulator embedded neural network trains near-instantaneously, typically without loss of prediction accuracy. The proposed method is demonstrated on multiple analytical examples, as well as an aerospace flight parameter study of a generic hypersonic vehicle.
</details>
<details>
<summary>摘要</summary>
嵌入式神经网络（physics informed neural network）可以有效地利用多源数据来进行航空工程系统的设计探索。多个神经网络模型的实现被训练于不同的随机初始化。 ensemble 模型的使用可以评估因为训练样本缺乏而导致的 epistemic 模型不确定性。这种不确定性评估是在成功目标适应学习中的关键信息。然而，训练ensemble 模型的成本常常变得不可持和计算挑战，尤其是在不同步骤的适应学习中。在这种情况下，一种新的嵌入式神经网络模型被提出，使用了快速神经网络 Paradigma。这种模型与传统神经网络训练不同，只是在最后层连接 весов而使用了线性回归技术。这种方法可以在几乎实时内训练神经网络，通常不会产生减少预测精度的情况。这种方法在多个分析例题上进行了证明，以及一个涉及到一种常见的高速飞行器的航空飞行参数研究。
</details></li>
</ul>
<hr>
<h2 id="A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes"><a href="#A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes" class="headerlink" title="A Sequentially Fair Mechanism for Multiple Sensitive Attributes"></a>A Sequentially Fair Mechanism for Multiple Sensitive Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06627">http://arxiv.org/abs/2309.06627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phi-ra/SequentialFairness">https://github.com/phi-ra/SequentialFairness</a></li>
<li>paper_authors: François Hu, Philipp Ratz, Arthur Charpentier</li>
<li>for: 本研究旨在解决多个敏感特征之间的关系，以提高决策的公平性。</li>
<li>methods: 本文提出了一种顺序框架，通过多元 Wasserstein 质量函数来逐步实现公平性。此方法提供了一个具有简单解释性的关键函数，可以帮助理解多个敏感特征之间的相互关系。</li>
<li>results: 本研究的实验结果表明，提出的顺序框架可以有效地提高决策的公平性，并且可以适应具有多个敏感特征的实际应用场景。<details>
<summary>Abstract</summary>
In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approximate fairness, enveloping a framework accommodating the trade-off between risk and unfairness. This extension permits a targeted prioritization of fairness improvements for a specific attribute within a set of sensitive attributes, allowing for a case specific adaptation. A data-driven estimation procedure for the derived solution is developed, and comprehensive numerical experiments are conducted on both synthetic and real datasets. Our empirical findings decisively underscore the practical efficacy of our post-processing approach in fostering fair decision-making.
</details>
<details>
<summary>摘要</summary>
通常情况下的算法公平使得敏感变量与相应的分数之间的关系消失。在过去几年，科学社区已经开发出了许多定义和工具来解决这个问题，这些工具在许多实际应用中都很有效。然而，在多个敏感特征的情况下，这些工具和定义的可行性和有效性就变得更加复杂。为解决这个问题，我们提出了一个顺序框架，可以逐步实现公平性 across a set of sensitive features。我们通过利用多重水星凝聚来扩展标准的强人口平衡定义，将多个敏感特征的相互关系进行进一步的调整。这种方法还提供了一个关闭式解的最优、顺序公平预测器，允许直观地了解多个敏感特征之间的相互关系。我们的方法可以逐步扩展到相似公平，包括一个折衔着的批处理方法，允许对特定敏感特征进行Targeted 改进。我们开发了一种基于数据驱动的估计过程，并在Synthetic和实际数据上进行了广泛的数值实验。我们的实验结果明确地证明了我们的后处理方法在促进公平决策中的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Deep-Neural-Networks-via-Semi-Structured-Activation-Sparsity"><a href="#Accelerating-Deep-Neural-Networks-via-Semi-Structured-Activation-Sparsity" class="headerlink" title="Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity"></a>Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06626">http://arxiv.org/abs/2309.06626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Grimaldi, Darshan C. Ganji, Ivan Lazarevich, Sudhakar Sah</li>
<li>for: 提高深度神经网络（DNNs）在嵌入式设备上的处理效率，以便更广泛地部署。</li>
<li>methods: 利用网络特征图的稀疏性来降低推理延迟。我们提出一种解决方案，通过轻量级的运行时修改来实现半结构化活动稀疏性。</li>
<li>results: 我们对各种图像分类和物体检测任务使用了多种模型进行了广泛的评估。结果显示，我们的方法可以提高速度，同时减少精度下降。例如，使用ResNet18模型在ImageNet dataset上，我们的方法可以提高速度$1.25\times$，同时精度下降$1.1%$。此外，当与当前最佳结构减少技术相结合时，得到的模型可以提供良好的时间-准确性质量比。<details>
<summary>Abstract</summary>
The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop of $1.1\%$ for the ResNet18 model on the ImageNet dataset. Furthermore, when combined with a state-of-the-art structured pruning method, the resulting models provide a good latency-accuracy trade-off, outperforming models that solely employ structured pruning techniques.
</details>
<details>
<summary>摘要</summary>
需求深度神经网络（DNNs）在嵌入式设备上高效处理是一个重要的挑战，限制其部署。利用网络特征图的稀疏性可以降低其推理延迟。已知无结构稀疏会导致更低的准确度下降，而结构稀疏则需要大量的推理引擎更改以获得延迟优化。为解决这个挑战，我们提出一种inducedemi-structured activation稀疏性可以通过小改动的推理引擎来实现。为了在推理时 достичь高速度倍数，我们设计了一种稀疏训练方法，拥有最终活动的位置信息来计算General Matrix Multiplication（GEMM）。我们对多种模型进行了广泛的评估，包括图像分类和物体检测任务。结果显示，我们的方法可以提高ResNet18模型的速度倍数，同时准确度下降只有1.1%，在ImageNet数据集上。此外，当与现有的结构剪枝方法结合使用时，得到的模型具有良好的延迟准确度质量平衡，超过只使用结构剪枝技术的模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems"><a href="#On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems" class="headerlink" title="On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems"></a>On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06622">http://arxiv.org/abs/2309.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexis M. H. Teter, Yongxin Chen, Abhishek Halder</li>
<li>for:  solves the Schrödinger bridge problem, a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints.</li>
<li>methods:  uses contractive fixed point recursions, which are dynamic versions of the Sinkhorn iterations, to solve the Schrödinger systems with guaranteed linear convergence.</li>
<li>results:  provides a priori estimates for the contraction coefficients associated with the convergence of the Schrödinger systems, and provides new geometric and control-theoretic interpretations for the same. Additionally, the paper shows the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.<details>
<summary>Abstract</summary>
Schr\"{o}dinger bridge is a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints. A popular method to numerically solve the Schr\"{o}dinger bridge problems, in both classical and in the linear system settings, is via contractive fixed point recursions. These recursions can be seen as dynamic versions of the well-known Sinkhorn iterations, and under mild assumptions, they solve the so-called Schr\"{o}dinger systems with guaranteed linear convergence. In this work, we study a priori estimates for the contraction coefficients associated with the convergence of respective Schr\"{o}dinger systems. We provide new geometric and control-theoretic interpretations for the same. Building on these newfound interpretations, we point out the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.
</details>
<details>
<summary>摘要</summary>
“尹朋因桥”是一个测度控制问题，旨在将一个初始状态分布调整到另一个状态，并且受到导入的扩散和时间限制。一种广泛使用的方法来确解“尹朋因桥”问题是透过对称固定点回归，这些回归可以看作是动态版本的well-known Sinkhorn迭代，并且在某些假设下，它们可以将“尹朋因桥”系统解决，并且具有线性对称增强的特性。在这个研究中，我们研究“尹朋因桥”系统的内在稳定性，并且提供新的几何和控制理论的解释。基于这些新的解释，我们点出了改进 computation的可能性，尤其是针对线性SBP的最差情况下的对称系数。
</details></li>
</ul>
<hr>
<h2 id="RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models"><a href="#RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models" class="headerlink" title="RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models"></a>RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06619">http://arxiv.org/abs/2309.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Li, Zexin Li, Wei Yang, Cong Liu</li>
<li>for: 这个论文主要是为了解决语言模型（LM）在不同设备上的部署问题，因为它们的计算成本和不可预测的推理延迟问题。</li>
<li>methods: 这个论文使用了一种名为RT-LM的实时推理 uncertainty-aware 资源管理环境，以解决LM的不确定性induced延迟性能下降问题。</li>
<li>results: 实验结果表明，RT-LM可以减少响应时间和提高吞吐量，但带来较小的运行时开销。<details>
<summary>Abstract</summary>
Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM, an uncertainty-aware resource management ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how specific input uncertainties, adversely affect latency, often leading to an increased output length. Exploiting these insights, we devise a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. Utilizing this quantification as a latency heuristic, we integrate the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading. Quantitative experiments across five state-of-the-art LMs on two hardware platforms demonstrates that RT-LM can significantly reduce the average response time and improve throughput while incurring a rather small runtime overhead.
</details>
<details>
<summary>摘要</summary>
最近的语言模型（LM）的进步已经吸引了广泛的关注，因为它们可以生成人类化的回复。虽然在各种应用程序中，如对话AI，它们展示了一个美好的未来，但是由于其极高的计算成本和不可预测的推理延迟，LM在不同的设备上部署具有挑战性。这种不确定性引起的延迟variation，被认为是由语言自身的不确定性所致，可能导致计算不效率和总性性能下降，特别是在高负载下。为了理解和改善LM在实时响应需求下的影响，我们首先要理解、量化和优化LM中uncertainty-induced延迟性性能变化。特别是，我们介绍了RT-LM，一个uncertainty-awareness资源管理生态系统，用于实时推理LM。RT-LM可以量化特定输入不确定性如何 adversely affect延迟，并经常导致输出长度增加。利用这些 Insight，我们提出了一种轻量级 yet effective的方法，通过在运行时 dynamically correlating输入文本不确定性与输出长度。通过这种量化作为延迟heuristic，我们将不确定性信息integrated into a system-level调度器，并探索了多种由不确定性引起的优化机会，包括不确定性-aware prioritization、动态 консолидация和策略性CPU卸载。实验表明，RT-LM可以在五种state-of-the-art LM上对两种硬件平台进行quantitative experiments，显著降低响应时间和提高吞吐量，而且runtime overhead很小。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials"><a href="#Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials" class="headerlink" title="Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials"></a>Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06613">http://arxiv.org/abs/2309.06613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang, Clémence Bos, Stefan Sandfeld, Ruth Schwaiger</li>
<li>for: 这个论文是研究氧化镍-钴复合材料的，使用了探针压测技术。</li>
<li>methods: 论文使用了无监督学习技术，特别是加aussian mixture模型，来分析数据，并确定了不同 mechanical phases的机械性能。</li>
<li>results: 研究发现，通过 cross-validation方法，可以确定数据量是否充分，并提出了数据量所需的建议。<details>
<summary>Abstract</summary>
In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of "mechanical phases" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.
</details>
<details>
<summary>摘要</summary>
在这项研究中，氧化铜-铬复合材料被使用材料测试设备进行纳米压测。数列压测点被布置在样品上，以便获得大量数据，包括不同压测深度下的材料年轻模ulus和硬度的数百个测量结果。在这项研究中，我们采用了一种无监督学习技术—— Gaussian mixture model，来分析数据，并确定了机械相的数量和相应的机械性能。此外，我们还引入了一种验证方法，以判断数据量是否足够，并建议数据的充足量，以解决物理学问题中常见 yet difficult to resolve的数据不够问题。
</details></li>
</ul>
<hr>
<h2 id="Harmonic-NAS-Hardware-Aware-Multimodal-Neural-Architecture-Search-on-Resource-constrained-Devices"><a href="#Harmonic-NAS-Hardware-Aware-Multimodal-Neural-Architecture-Search-on-Resource-constrained-Devices" class="headerlink" title="Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices"></a>Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06612">http://arxiv.org/abs/2309.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Imed Eddine Ghebriout, Halima Bouzidi, Smail Niar, Hamza Ouarnoughi</li>
<li>for: 该论文旨在提出一种整合硬件考虑的多Modal Neural Networks（MM-NN）框架，以提高在有限资源设备上的多模态信息处理能力。</li>
<li>methods: 该论文提出了一种两级优化策略，包括对单模态网络架构和多模态融合策略进行优化，同时考虑硬件约束。</li>
<li>results: 试验结果表明，使用该框架可以在多个设备和多模态数据集上实现Accuracy提高约10.9%，延迟降低约1.91倍，能效率提高约2.14倍。<details>
<summary>Abstract</summary>
The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of Harmonic-NAS over state-of-the-art approaches achieving up to 10.9% accuracy improvement, 1.91x latency reduction, and 2.14x energy efficiency gain.
</details>
<details>
<summary>摘要</summary>
Recent interest in Multimodal Neural Networks (MM-NN) has surged due to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this enhances the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Moreover, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy.In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of Harmonic-NAS over state-of-the-art approaches, achieving up to 10.9% accuracy improvement, 1.91x latency reduction, and 2.14x energy efficiency gain.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach"><a href="#Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach" class="headerlink" title="Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach"></a>Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06604">http://arxiv.org/abs/2309.06604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Esmaeili, Eric T. Matson, Julia T. Rayz</li>
<li>for: 提出一种完全自动和协同的Agent-based机制，用于分布式机器学习算法选择和hyperparameter调整。</li>
<li>methods: 基于现有的Agent-based层次机器学习平台，增强查询结构以支持选择和调整机制，不受限于具体的学习、选择和调整机制。</li>
<li>results: 经过理论评估、正式验证和分析研究，确认方法的正确性和资源利用率，时间复杂度和空间复杂度与资源大小成线性关系。<details>
<summary>Abstract</summary>
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical study to demonstrate the correctness, resource utilization, and computational efficiency of our technique. According to the results, our solution is totally correct and exhibits linear time and space complexity in relation to the size of available resources. To provide concrete examples of how the proposed methodologies can effectively adapt and perform across a range of algorithmic options and datasets, we have also conducted a series of experiments using a system comprised of 24 algorithms and 9 datasets.
</details>
<details>
<summary>摘要</summary>
Algorithm selection和超参调整是学术和应用机器学习的关键步骤，然而这些步骤正在逐渐变得更加细腻，这是因为机器学习资源的急剧增加、多样性和分布化的问题。在应用多智能体系统的设计机器学习平台时，这些特点就变得非常明显。这篇论文提出了一种完全自动和合作的智能体系统来选择分布式组织的机器学习算法和同时调整其超参数。我们的方法基于现有的智能体系统机器学习平台，并在其查询结构上增加了支持这些功能的扩展。我们已经对我们的技术进行了理论评估、正式验证和分析研究，以证明我们的方法的正确性、资源利用率和计算效率。根据结果，我们的解决方案是完全正确的，并且在资源大小为基准的情况下显示出线性时间和空间复杂度。为了让读者更好地理解我们的方法在不同的算法和数据集上的应用和表现，我们还进行了一系列实验，使用了24种算法和9个数据集。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning"><a href="#Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning" class="headerlink" title="Reasoning with Latent Diffusion in Offline Reinforcement Learning"></a>Reasoning with Latent Diffusion in Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06599">http://arxiv.org/abs/2309.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, Glen Berseth</li>
<li>for: 这篇论文的目的是提出一种基于离线学习的强化学习方法，以学习从静止数据集中获得高奖励策略，而不需要进一步与环境交互。</li>
<li>methods: 该方法使用了缺省方法，即使用 Monte Carlo 返回到去的样本来为奖励进行条件。然而，这些方法需要精细地调整，并且在多模态数据上表现不佳。而该文提出的新方法则是使用缺省扩散来模型支持流程中的压缩 latent skill，从而避免了推断错误。</li>
<li>results: 该文的实验结果表明，使用该新方法可以在 D4RL 测试环境中实现state-of-the-art 性能，特别是在长时间 horizon 和罕见奖励任务中表现出色。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks.
</details>
<details>
<summary>摘要</summary>
偏向RL（偏向学习）在线上不需要继续与环境交互，可以从静止数据集中学习高奖策略。然而，偏向RL中的一个关键挑战在于有效地将静止数据集中的部分不优轨迹缝合，以避免因数据集缺失支持而导致的推断错误。现有的方法使用保守的方法，困难调整并在多模态数据上强制性做出差异（我们所示），或者基于噪声的Monte Carlo返回值来修改奖励。在这种工作中，我们提议一种新的方法，利用潜在扩散来模型在支持轨迹序列中的压缩 latent 技能。这使得学习Q函数时避免推断错误，并且在批处理中使用批处理来约束。潜在空间也是表达力强，慈善地处理多模态数据。我们显示，学习的时间抽象 latent 空间包含更加细致的任务特定信息，比raw state-actions 更好地进行奖励分配和奖励扩散。我们的方法在 D4RL 测试上达到了状态之上的性能，特别是在长期、稀缺奖励任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning"><a href="#Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning" class="headerlink" title="Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning"></a>Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06597">http://arxiv.org/abs/2309.06597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Behzad Dariush, Chiho Choi, Mykel Kochenderfer</li>
<li>for: This paper is written for researchers working on visual scene understanding and related fields, with the goal of improving the trustworthiness and interpretability of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS).</li>
<li>methods: The paper introduces a novel dataset called Rank2Tell, which provides dense annotations of various semantic, spatial, temporal, and relational attributes of important objects in complex traffic scenarios. The dataset is multi-modal and includes ego-centric data, and the authors propose a joint model for joint importance level ranking and natural language captions generation to benchmark the dataset.</li>
<li>results: The authors demonstrate the performance of their dataset and joint model with quantitative evaluations, and show that the dataset is a valuable resource for researchers working on visual scene understanding and related fields.<details>
<summary>Abstract</summary>
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
广泛的商业自动驾车（AV）和高级驾驶辅助系统（ADAS）的普及可能受到社会的接受程度的限制，这对于驾驶人的信任和理解度是非常重要。然而，现代自动驾驶系统软件往往运用黑盒式人工智能模型，实现这个目标是非常具有挑战性。为了解决这个问题，本文提出了一个新的数据集，名为Rank2Tell，这是一个多modal的自我中心数据集，用于排名重要性和说明重要性的原因。这个数据集使用了多种关闭和开放式的视觉问题回答，提供了丰富的标签和各种semantic、空间、时间和关联性的属性。这个数据集的独特特点和标签使其成为了研究视景理解和相关领域的重要资源。此外，我们也引入了一个共同模型，用于联合重要性水平排名和自然语言描述生成，以评估我们的数据集和展示表现。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Gradient-based-MAML-in-LQR"><a href="#Convergence-of-Gradient-based-MAML-in-LQR" class="headerlink" title="Convergence of Gradient-based MAML in LQR"></a>Convergence of Gradient-based MAML in LQR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06588">http://arxiv.org/abs/2309.06588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negin Musavi, Geir E. Dullerud</li>
<li>for: 这个研究报告的主要目标是调查Model-agnostic Meta-learning（MAML）在线性系统quadratic optimal control（LQR）中的本地受欢迎性。</li>
<li>methods: 这个研究使用MAML和其变种来快速适应新任务，并且利用过去学习知识来进行预测。但是，MAML的理论保证仍然未知，因为非 convex 和其结构，使得在动态系统设置下保持稳定性变得更加困难。这个研究将MAML应用于LQR设置，提供本地收敛保证，同时维护动态系统的稳定性。</li>
<li>results: 这个研究提供了简单的数字结果，以证明MAML在LQR任务中的收敛性质。<details>
<summary>Abstract</summary>
The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
</details>
<details>
<summary>摘要</summary>
本研究的主要目标是调查模型独立元学习（MAML）在线性系统剑方优控（LQR）中的本地叉度特性。MAML和其变种在各种学习领域，如回归、分类和奖励学习中，用于快速适应新任务，但其理论保证仍然未知，这使得在动态系统设置下稳定性变得更加挑战。本研究将MAML应用于LQR设置，提供本地收敛保证，同时保持动态系统的稳定性。此外，文章还提供了简单的数字结果，以证明MAML在LQR任务中的收敛性质。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction"><a href="#Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction" class="headerlink" title="Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction"></a>Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06584">http://arxiv.org/abs/2309.06584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, Cui Tao</li>
<li>for: 预测阿尔茨海默症和相关性股症 (ADRD) 的风险。</li>
<li>methods: 使用图 neural network (GNN) 与养成数据进行 ADRD 风险预测，并 introducing 一种新的关系重要性评估方法，以提供全面的解释。</li>
<li>results: VGNN 比 Random Forest 和 Light Gradient Boost Machine 高出 10% 的 receiver operating characteristic 领域下的覆盖率。这种方法可能在提供 ADRD 进程中的值得考虑因素方面发挥重要作用。<details>
<summary>Abstract</summary>
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.   We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient Boost Machine as baselines. We further used our relation importance method to clarify the key relationships for ADRD risk prediction. VGNN surpassed other baseline models by 10% in the area under the receiver operating characteristic. The integration of the GNN model and relation importance interpretation could potentially play an essential role in providing valuable insight into factors that may contribute to or delay ADRD progression.   Employing a GNN approach with claims data enhances ADRD risk prediction and provides insights into the impact of interconnected medical code relationships. This methodology not only enables ADRD risk modeling but also shows potential for other image analysis predictions using claims data.
</details>
<details>
<summary>摘要</summary>
来自美国的数据显示，阿尔茨海默症和相关的认知障碍（ADRD）是死亡的第六大原因，强调了 precisione ADRD 预测的重要性。虽然最近的进步主要依靠图像分析，但不是所有病人都会进行医疗图像诊断。我们的目标是使用图像分析（GNN）与保险资料进行 ADRD 预测。为了解释预测的原因，我们引入了一个创新的方法，以评估关系的重要性和其影响 ADRD 预测的可能性。我们使用了Variational Regularized Encoder-decoder Graph Neural Network（VGNN）估计 ADRD 可能性。我们创建了三个情况来评估模型的效率，使用 Random Forest 和 Light Gradient Boost Machine 作为基准。我们还使用我们的关系重要性方法来阐明 ADRD 预测中的关键关系。VGNN 在受到Receiver Operating Characteristic 的面积下表现出乎 Random Forest 和 Light Gradient Boost Machine 的10%。通过融合 GNN 模型和关系重要性解释，我们可以实现更好的 ADRD 预测和获得关于可能导致或延迟 ADRD 进程的有用信息。使用 GNN 方法与保险资料可以提高 ADRD 预测的精度，并且显示了这种方法在其他图像分析预测中的应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype"><a href="#Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype" class="headerlink" title="Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype"></a>Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06582">http://arxiv.org/abs/2309.06582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roger Rusack, Bhargav Joshi, Alpana Alpana, Seema Sharma, Thomas Vadnais</li>
<li>for: 这项研究的目的是为了开发一种能够高效准确地重建电子 incident 的机器学习方法，以应对 future 高能物理实验中的探测器技术问题。</li>
<li>methods: 这项研究使用了最新的机器学习技术，包括三维粒子探测器和时间分辨精度测试等方法，来重建 incident 电子的能量。</li>
<li>results: 研究人员通过使用机器学习方法，从三维粒子探测器中的能量值来重建 incident 电子的能量，并获得了一定的成果。<details>
<summary>Abstract</summary>
We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.
</details>
<details>
<summary>摘要</summary>
我们提供了一个新的公共可用的数据集，包含一种新的加速器实验室中的射电计仪器的模拟数据。这个仪器将有超过六百万个通道，每个通道都可以进行位置、离子化和精度时间测量。重建这些事件的方式具有巨大的挑战，我们正在使用最新的机器学习技术来解决这个问题。在这个开发过程中，我们建立了一个大型原型，包含12,000个通道，并将高能电子束照射到它上。使用机器学习方法，我们已经从三维hit的能量中重建了入射电子的能量，准确程度已知。通过公开这些数据，我们希望能吸引专业人士使用机器学习技术来开发高效、准确的图像重建方法。
</details></li>
</ul>
<hr>
<h2 id="Promises-of-Deep-Kernel-Learning-for-Control-Synthesis"><a href="#Promises-of-Deep-Kernel-Learning-for-Control-Synthesis" class="headerlink" title="Promises of Deep Kernel Learning for Control Synthesis"></a>Promises of Deep Kernel Learning for Control Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06569">http://arxiv.org/abs/2309.06569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Reed, Luca Laurenti, Morteza Lahijanian</li>
<li>for: 本文用于控制synthesis的stochastic dynamical systems against complex specifications, using Deep Kernel Learning (DKL) </li>
<li>methods: 使用DKL学习未知系统从数据中，并正确地抽象DKL模型为Interval Markov Decision Process (IMDP)进行控制synthesis with correctness guarantees.</li>
<li>results: 在various benchmarks上，包括5-D nonlinear stochastic system, 控制synthesis with DKL可以大幅超过现有竞争方法的性能。<details>
<summary>Abstract</summary>
Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-of-the-art competitive methods.
</details>
<details>
<summary>摘要</summary>
深度kernel学习（DKL）结合神经网络的表示能力和泊松过程的不确定性量化，因此它可能是控制复杂动力系统的有望工具。在这项工作中，我们开发了可扩展的抽象基础结构，使得使用DKL进行动力系统控制合成可以遵循复杂规范。 Specifically，我们考虑了时间逻辑规范，并创建了从数据学习DKL模型到Interval Markov Decision Process（IMDP）的端到端框架，以实现控制合成具有正确保证。此外，我们确定了深度架构，使得精准学习和快速抽象计算可以准确地学习和控制动力系统。我们的方法在多个标准准例上进行了证明，包括5个维的非线性随机系统，显示了DKL控制合成可以大幅超越现有竞争方法。
</details></li>
</ul>
<hr>
<h2 id="Commands-as-AI-Conversations"><a href="#Commands-as-AI-Conversations" class="headerlink" title="Commands as AI Conversations"></a>Commands as AI Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06551">http://arxiv.org/abs/2309.06551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dspinellis/ai-cli">https://github.com/dspinellis/ai-cli</a></li>
<li>paper_authors: Diomidis Spinellis</li>
<li>for: 这个论文主要目标是提高命令行界面的用户体验，使其更加智能和易用。</li>
<li>methods: 该论文提出了一种基于OpenAI API的自然语言处理技术，可以将用户的自然语言提示转换成可执行的命令行指令。</li>
<li>results: 该论文通过对多个命令行工具的集成，使得命令行界面更加智能和易用，开辟了新的可能性和应用场景。<details>
<summary>Abstract</summary>
Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? "ai-cli," an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, "ai-cli" transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The "ai-cli" tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platform applicability.
</details>
<details>
<summary>摘要</summary>
开发者和数据科学家经常遇到写入命令行输入的困难，即使有图形界面或工具如ChatGPT可以帮助。 Solution? “ai-cli”，一个开源系统， Draw inspiration from GitHub Copilot，可以将自然语言提示转换成多种Linux命令行工具可执行的命令。通过对OpenAI的API进行交互，通过JSON HTTP请求转换用户查询到可执行的命令。然而，在多个命令行工具之间集成人工智能帮助，特别是在开源环境中，可能会复杂。历史上，操作系统可以仅仅作为中间人，但每个工具的特有功能和缺乏一致的方法使得中心化集成变得困难。 “ai-cli”工具通过在每个程序的Readline库API中动态加载和链接，使命令行界面变得更加智能和易用，开启了进一步改进和跨平台应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Transfer-Learning"><a href="#Distributionally-Robust-Transfer-Learning" class="headerlink" title="Distributionally Robust Transfer Learning"></a>Distributionally Robust Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06534">http://arxiv.org/abs/2309.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rvr-account/rvr">https://github.com/rvr-account/rvr</a></li>
<li>paper_authors: Xin Xiong, Zijian Guo, Tianxi Cai</li>
<li>for: 这篇论文是为了解决对于转移学习中的一个问题，即如何将来自不同来源模型的知识融合到目标数据上，以提高预测性能。</li>
<li>methods: 这篇论文提出了一个新的转移学习方法，即分布robust优化（TransDRO），它不受来源数据的严格相似性限制，而是通过最大化类似损失函数的调整，以提高预测性能。</li>
<li>results: 研究人员透过实验和分析多元机构电子健康纪录数据，证明了TransDRO的具有稳定性和准确性，并且在转移学习应用中具有优秀的表现。<details>
<summary>Abstract</summary>
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.
</details>
<details>
<summary>摘要</summary>
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.Here is the translation in Traditional Chinese:Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations"><a href="#Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations" class="headerlink" title="Hierarchical Multi-Task Learning Framework for Session-based Recommendations"></a>Hierarchical Multi-Task Learning Framework for Session-based Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06533">http://arxiv.org/abs/2309.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sejoon Oh, Walid Shalaby, Amir Afsharinejad, Xiquan Cui</li>
<li>for: 这篇论文的目的是提出一种基于Session-based Recommender Systems (SBRSs)的多任务学习（MTL）架构，以提高推荐性能和泛化性。</li>
<li>methods: 这篇论文使用了一种叫做 Hierarchical MTL (H-MTL) 的架构，它在预测任务之间设置了层次结构，并将auxiliary task的输出传递给主要任务。这种结构使得主要任务得到了更丰富的输入特征，并且提高了预测结果的可解释性。</li>
<li>results: 实验结果表明，与现有的 SBRSs 相比， HierSRec 在两个session-based recommendation dataset上的下一个项预测精度得到了提高。此外，HierSRec 使用了一种精炼的候选项集（比如4%的总项），以实现可扩展的推荐。<details>
<summary>Abstract</summary>
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (e.g., 4% of total items) per test example using the category prediction. Experiments show that HierSRec outperforms existing SBRSs as per next-item prediction accuracy on two session-based recommendation datasets. The accuracy of HierSRec measured with the carefully-curated candidate items aligns with the accuracy of HierSRec calculated with all items, which validates the usefulness of our candidate generation scheme via H-MTL.
</details>
<details>
<summary>摘要</summary>
Session-based recommender systems (SBRSs) 已经表现出优秀的推荐性能，而多任务学习 (MTL) 也在 SBRSs 中应用，以提高预测准确性和泛化能力。层次多任务学习 (H-MTL) 设置一个层次结构，在预测任务之间，并将auxiliary任务的输出传递给主任务。这种层次结构导致主任务的输入特征更加丰富，并提高预测结果的解释性，相比既有MTL框架。然而，H-MTL框架在 SBRSs 中还没有被研究。在这篇论文中，我们提出了 HierSRec，它将 H-MTL 框架应用于 SBRSs。HierSRec 使用metadata-aware Transformer来编码给定的会话，然后进行下一个类型预测（即auxiliary任务），使用会话编码。接着，HierSRec 使用类型预测结果和会话编码进行下一个项目预测（即主任务）。为了可扩展的推理，HierSRec 创建了一个具有4%的总项目数的紧凑集（例如，每个测试示例）的候选项目。实验结果显示，HierSRec 在两个会话基于推荐数据集上的下一个项目预测精度方面高于现有的 SBRSs。HierSRec 对于 carefully-curated 的候选项目的准确率与所有项目的准确率相匹配，这 Validates 我们的候选项目生成方案via H-MTL。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers"><a href="#Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers" class="headerlink" title="Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers"></a>Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06526">http://arxiv.org/abs/2309.06526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/dp-tabtransformer">https://github.com/ibm/dp-tabtransformer</a></li>
<li>paper_authors: Xilong Wang, Chia-Mu Yu, Pin-Yu Chen</li>
<li>for: 本研究探讨了将数据隐私和机器学习表格数据结合使用的可能性，具体来说是在预训练和精度调整中使用TabTransformer模型，并使用多种参数效率调整方法（PEFT），包括Adapter、LoRA和Prompt Tuning。</li>
<li>methods: 本研究使用了多种PEFT方法，包括Adapter、LoRA和Prompt Tuning，以提高预训练和精度调整的效率。</li>
<li>results: 对ACSIncome数据集进行了广泛的实验，发现PEFT方法可以在预训练和精度调整中超越传统方法，以至于实现更好的参数效率、隐私和准确率之间的平衡。<details>
<summary>Abstract</summary>
For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.
</details>
<details>
<summary>摘要</summary>
为机器学习 tabular 数据，表格转换器（TabTransformer）是一个现代神经网络模型，而数据隐私（DP）是保证数据隐私的必要组成部分。在这篇论文中，我们探索将这两个方面结合在一起的enario，即具有权限限制的预训练和精细调整（PEFT）方法，包括 Adapter、LoRA 和 Prompt Tuning。我们在 ACSIncome 数据集进行了广泛的实验，结果表明，这些 PEFT 方法在下游任务的准确率和可训练参数数量方面占据了优势，从而实现了参数效率、隐私和准确率之间的改进的平衡。我们的代码可以在github.com/IBM/DP-TabTransformer 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Q-learning-Approach-for-Adherence-Aware-Recommendations"><a href="#A-Q-learning-Approach-for-Adherence-Aware-Recommendations" class="headerlink" title="A Q-learning Approach for Adherence-Aware Recommendations"></a>A Q-learning Approach for Adherence-Aware Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06519">http://arxiv.org/abs/2309.06519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Faros, Aditya Dave, Andreas A. Malikopoulos</li>
<li>for: 本研究目的是解决人工智能提供的建议received by human decision-makers (HDM) in high-stakes and safety-critical scenarios, while HDM retains the ultimate decision-making responsibility.</li>
<li>methods: 该算法使用了”遵循度”来捕捉HDML suivre the recommended actions的频率，并 derive the optimal recommendation policy in real time.</li>
<li>results: 我们证明了该Q学习算法的优化性和效果，并在多种场景中进行了性能评估。<details>
<summary>Abstract</summary>
In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an "adherence-aware Q-learning" algorithm to address this problem. The algorithm learns the "adherence level" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.
</details>
<details>
<summary>摘要</summary>
在许多高度危险和安全性有着重要意义的实际场景中，人工智能推荐（HDM）可能会收到人工智能的建议，而持有最终决策责任。在这封信中，我们开发了一种“遵循度意识Q学习”算法，以解决这个问题。该算法学习推荐行为的遵循度，并在实时基础上计算最佳推荐策略。我们证明算法的优化目标函数的整体收敛性，并对各种场景进行性能评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets"><a href="#Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets" class="headerlink" title="Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets"></a>Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06503">http://arxiv.org/abs/2309.06503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramya Tekumalla, Juan M. Banda</li>
<li>for: 本研究的目的是为了评估大语言模型GPT-4和弱监督来自动标注COVID-19疫苗相关的推文，以比较其表现和人工标注者。</li>
<li>methods: 本研究使用了GPT-4（2023年3月23日版本）和弱监督来自动标注COVID-19疫苗相关的推文，没有进行任何额外定制或指导。</li>
<li>results: 研究发现，使用GPT-4和弱监督可以准确地标注COVID-19疫苗相关的推文，并且表现比人工标注者更好。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对医疗业和社会的挑战很大。随着 COVID-19 疫苗的快速发展，社交媒体平台上关于疫苗的话题成为了公众的焦点。可以通过分析社交媒体上的 tweet 获得有价值的公共健康研究和政策制定者的信息。然而，手动标注大量 tweet 是时间consuming 和昂贵的。在本研究中，我们评估了 Large Language Models（在这种情况下是 GPT-4 （2023年3月23日版））和弱级指导，以标识 COVID-19 疫苗相关的 tweet，并与人工标注者进行比较。我们利用了 manually 精心准备的金标准数据集，并使用 GPT-4 提供标签，没有任何额外的 fine-tuning 或指导，在单击模式下（没有额外的推荐）。
</details></li>
</ul>
<hr>
<h2 id="A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale"><a href="#A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale" class="headerlink" title="A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale"></a>A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06497">http://arxiv.org/abs/2309.06497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, Michael Rabbat</li>
<li>for: 这个论文是用来训练神经网络的在线和随机优化算法，属于AdaGrad家族的方法。</li>
<li>methods: 这个算法使用了一个块对角减少矩阵AdaGrad方法，每个块都包含一个粗略的克로内кер产品矩阵approxiamtion来处理每个神经网络参数。</li>
<li>results: 作者在这篇论文中提供了这个算法的完整描述以及实现方法，并通过对 ImageNet ResNet50 进行ablation研究，证明了Shampoo算法在训练深度神经网络时的优势。<details>
<summary>Abstract</summary>
Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo's superiority over standard training recipes with minimal hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
<!--TRANSLATION START-->帕逻拜（Shampoo）是一种在线和随机优化算法，属于AdaGrad家族的方法，用于训练神经网络。它构建了一个块对角预conditioner，每个块包含一个粗略 kronecker 产品approximation来全矩度AdaGrad方法的每个参数。在这项工作中，我们提供了完整的算法描述以及我们实现的性能优化，以在PyTorch中训练深度网络。我们的实现使得可以快速分布在多个GPU上进行分布式数据并行训练，通过PyTorch的DTensor数据结构分布内存和计算相关于块的每个参数，并在每个迭代执行一个AllGather primitives来计算搜索方向。这一主要性能提升使得我们可以在每步时钟时间中减少至少10%的性能。我们验证了我们的实现，通过对ImageNet ResNet50的训练进行减少研究， demonstrating Shampoo的优越性，与标准训练方法相比，需要 minimal hyperparameter tuning。<!--TRANSLATION END-->Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons"><a href="#Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons" class="headerlink" title="Learning topological operations on meshes with application to block decomposition of polygons"></a>Learning topological operations on meshes with application to block decomposition of polygons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06484">http://arxiv.org/abs/2309.06484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Yulong Pan, Per-Olof Persson</li>
<li>for: 提高无结构三角形和四角形网格质量</li>
<li>methods: 使用自适应学习法，不含先验规则，通过自身玩 игры来提高网格质量，涉及到标准的本地和全局元素操作</li>
<li>results: 实现节点度差与理想值的差异最小化，即内部顶点的差异最小化，以提高网格质量<details>
<summary>Abstract</summary>
We present a learning based framework for mesh quality improvement on unstructured triangular and quadrilateral meshes. Our model learns to improve mesh quality according to a prescribed objective function purely via self-play reinforcement learning with no prior heuristics. The actions performed on the mesh are standard local and global element operations. The goal is to minimize the deviation of the node degrees from their ideal values, which in the case of interior vertices leads to a minimization of irregular nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于学习的框架，用于改进不结构化三角形和四边形的网格质量。我们的模型通过自动游戏学习方法，不含任何先验知识，来提高网格质量。Actions performed on the mesh include标准的本地和全局元素操作。我们的目标是将节点度 deviation from their ideal values as close as possible, which in the case of interior vertices leads to a minimization of irregular nodes.Here's the word-for-word translation:我们提出了一种基于学习的框架，用于改进不结构化三角形和四边形的网格质量。我们的模型通过自动游戏学习方法，不含任何先验知识，来提高网格质量。Actions performed on the mesh include标准的本地和全局元素操作。我们的目标是将节点度 deviation from their ideal values as close as possible, which in the case of interior vertices leads to a minimization of irregular nodes.
</details></li>
</ul>
<hr>
<h2 id="Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation"><a href="#Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation" class="headerlink" title="Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation"></a>Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06472">http://arxiv.org/abs/2309.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Golling, Samuel Klein, Radha Mastandrea, Benjamin Nachman, John Andrew Raine</li>
<li>for: 这篇论文是为了解决高能物理和其他领域中数据分析中的数据变换问题，通过重新权重而不是直接使用权重来实现数据变换。</li>
<li>methods: 这篇论文提出了一种叫做”流体流”的协议，该协议可以在无法直接知道起始数据集的概率密度情况下，使用最大 LIKELIHOOD 估计来训练正则化流来变换一个数据集到另一个数据集。</li>
<li>results: 论文的实验结果表明，使用”流体流”协议可以成功地将一个数据集变换到另一个数据集，并且可以根据特定的条件来创建一个适应每个条件的变换函数。<details>
<summary>Abstract</summary>
Many components of data analysis in high energy physics and beyond require morphing one dataset into another. This is commonly solved via reweighting, but there are many advantages of preserving weights and shifting the data points instead. Normalizing flows are machine learning models with impressive precision on a variety of particle physics tasks. Naively, normalizing flows cannot be used for morphing because they require knowledge of the probability density of the starting dataset. In most cases in particle physics, we can generate more examples, but we do not know densities explicitly. We propose a protocol called flows for flows for training normalizing flows to morph one dataset into another even if the underlying probability density of neither dataset is known explicitly. This enables a morphing strategy trained with maximum likelihood estimation, a setup that has been shown to be highly effective in related tasks. We study variations on this protocol to explore how far the data points are moved to statistically match the two datasets. Furthermore, we show how to condition the learned flows on particular features in order to create a morphing function for every value of the conditioning feature. For illustration, we demonstrate flows for flows for toy examples as well as a collider physics example involving dijet events
</details>
<details>
<summary>摘要</summary>
很多高能物理数据分析中的组件需要将一个数据集转换为另一个。通常通过重新权重来解决这个问题，但是保留权重并将数据点Shift而不是重新权重有多个优点。通过流形模型可以在多种 particle physics 任务中获得非常高精度。然而，通常情况下，我们无法直接使用流形模型进行 morphing，因为它们需要知道开始数据集的概率密度。在大多数情况下，我们可以生成更多的例子，但我们不知道概率密度的确切值。我们提出了一种叫做“流形模型 для流形模型”的协议，用于训练流形模型，以便将一个数据集转换为另一个，即使 neither 数据集的概率密度不知道。这种方法可以通过最大化 likelihood estimation 来训练，这种方法在相关任务中已经被证明是非常有效的。我们还研究了这种协议的变化，以探索数据点是如何统计匹配两个数据集的。此外，我们还示出了如何通过特定的特征来Conditional 学习的流形函数，以创建一个每个特征值的 morphing 函数。为了示例，我们使用了一些 Toy 示例以及一个 collider physics 示例，涉及到 dijet 事件。
</details></li>
</ul>
<hr>
<h2 id="LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning"><a href="#LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning" class="headerlink" title="LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"></a>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06440">http://arxiv.org/abs/2309.06440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth Shaw, Ananye Agarwal, Deepak Pathak</li>
<li>For: This paper is written for researchers and developers working in the field of robotics, particularly those interested in dexterous manipulation and machine learning.* Methods: The paper presents a low-cost, dexterous, and anthropomorphic hand called LEAP Hand, which is designed for machine learning research. The hand has a novel kinematic structure that allows for maximal dexterity regardless of finger pose, and it can be assembled in 4 hours at a cost of 2000 USD from readily available parts.* Results: The paper shows that LEAP Hand can be used to perform several manipulation tasks in the real world, including visual teleoperation and learning from passive video data. The hand significantly outperforms its closest competitor Allegro Hand in all experiments while being 1&#x2F;8th of the cost.<details>
<summary>Abstract</summary>
Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed assembly instructions, the Sim2Real pipeline and a development platform with useful APIs on our website at https://leap-hand.github.io/
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>dexterous manipulation在 робо技术中已经是一个长期的挑战。虽然机器学习技术已经显示了一些 promise, 但结果主要是在simulation中得到的。这可以把lack of suitable hardware归结为主要原因。在这篇论文中，我们提出了LEAP Hand，一个低成本的dexterous和人工手臂 для机器学习研究。与之前的手臂不同，LEAP Hand具有一种新的骨骼结构，允许无论手指 pose都能够达到最大的dexterity。LEAP Hand是低成本的，可以在4个小时内为2000美元组装，使用可得到的部件。它可以在长时间内一直施加大力，并且可以在现实世界中完成多种抓取任务，从视觉Remote控制到学习从沉默视频数据和sim2real。LEAP Hand在我们所有实验中都能够在与Allegro Hand的比赛中表现出色，而且只需1/8的成本。我们在网站https://leap-hand.github.io/上发布了详细的组装指南，Sim2Real管道和开发平台，包括有用的API。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones"><a href="#Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones" class="headerlink" title="Unveiling the potential of large language models in generating semantic and cross-language clones"></a>Unveiling the potential of large language models in generating semantic and cross-language clones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06424">http://arxiv.org/abs/2309.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palash R. Roy, Ajmain I. Alam, Farouq Al-omari, Banani Roy, Chanchal K. Roy, Kevin A. Schneider</li>
<li>for: 本研究旨在探讨OpenAI的GPT模型在Semantic Clone Bench上的表现，以及该模型在代码生成、代码理解、重构和比较中的潜在应用。</li>
<li>methods: 本研究使用了SemanticCloneBench作为测试平台，并采用了多种代码片段和语言模型来评估GPT-3模型的表现。</li>
<li>results: GPT-3模型在生成semantic和跨语言代码变体方面表现出色，其精度达62.14%和0.55 BLEU分数，并在跨语言生成中达到91.25%的精度。<details>
<summary>Abstract</summary>
Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance. In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones
</details>
<details>
<summary>摘要</summary>
semantic和 Cross-language code clone生成可能有用于代码重用、代码理解、重构和benchmarking。OpenAI的GPT模型有潜力在这些clone生成方面，因为GPT用于文本生成。当开发者从Stack Overflow（SO）或系统中复制代码时，可能会出现不一致的更改，导致不预期的行为。 similarly，如果有一个代码片断在特定编程语言中，但寻找与其功能相同的代码在另一种语言中， semantic Cross-language code clone生成方法可以提供有价值的帮助。在本研究中，使用SemanticCloneBench作为载体，我们评估了GPT-3模型是否可以为给定的片断生成semantic和cross-language clone variant。我们组成了多样化的代码片断，并评估GPT-3模型在生成代码variant方面的表现。经过广泛的实验和分析，9名评审员在158个小时内验证了我们的结果，我们探讨了模型是否能够生成准确和semantically correct的variant。我们的发现 shed light on GPT-3模型在代码生成方面的能力，并提供了使用高级语言模型在软件开发中的潜在应用和挑战。我们的量化分析显示了惊人的结果。在semantic clones领域，GPT-3达到了62.14%的准确率和0.55 BLEU分数，通过少量的提示工程来实现。此外，模型在跨语言领域表现出色，达到了91.25%的准确率在生成cross-language clones。
</details></li>
</ul>
<hr>
<h2 id="On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions"><a href="#On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions" class="headerlink" title="On Computationally Efficient Learning of Exponential Family Distributions"></a>On Computationally Efficient Learning of Exponential Family Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06413">http://arxiv.org/abs/2309.06413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhin Shah, Devavrat Shah, Gregory W. Wornell</li>
<li>for: 学习 truncated  экспоненциаль家族模型的自然参数，使用 i.i.d. 样本，并且Computational efficiency和Statistical efficiency。</li>
<li>methods: 提出了一个新的损失函数和一种 computationally efficient 的估计器，该估计器是consistent 和 asymptotically normal 的，并且可以在轻量级的样本量下达到 $\alpha$ 误差水平。</li>
<li>results: 我们的方法可以视为最大 likelihood estimation 的一种特例，并且可以在 population 级别上看作是一种 re-parameterized distribution 的最大 likelihood estimation。我们还证明了我们的估计器可以看作是一种 minimizing 特定 Bregman Score 的解，以及一种 minimizing 代理 likelihood 的解。我们还提供了Finite sample guarantees，可以在 $\ell_2$ 范围内达到 $\alpha$ 误差水平的样本量为 $O({\sf poly}(k)&#x2F;\alpha^2)$。在特殊情况下，我们的方法可以 achieve order-optimal sample complexity $O({\sf log}(k)&#x2F;\alpha^2)$。<details>
<summary>Abstract</summary>
We consider the classical problem of learning, with arbitrary accuracy, the natural parameters of a $k$-parameter truncated \textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a novel loss function and a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. Further, we show that our estimator can be interpreted as a solution to minimizing a particular Bregman score as well as an instance of minimizing the \textit{surrogate} likelihood. We also provide finite sample guarantees to achieve an error (in $\ell_2$-norm) of $\alpha$ in the parameter estimation with sample complexity $O({\sf poly}(k)/\alpha^2)$. Our method achives the order-optimal sample complexity of $O({\sf log}(k)/\alpha^2)$ when tailored for node-wise-sparse Markov random fields. Finally, we demonstrate the performance of our estimator via numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们考虑了классический问题学习，即在具有任意准确性的情况下，计算和统计上efficient地学习含有k参数的简化的EXPFamily的自然参数。我们关注在支持和自然参数之间存在有限的约束下进行学习。尽管传统的最大化可能性估计器在这类EXPFamily中是一致的、 asymptotically normal 和 asymptotically efficient，但计算很复杂。在这种情况下，我们提出了一个新的损失函数和一种计算高效的估计器，该估计器在一定条件下是一致的和 asymptotically normal。我们还证明了，在人口水平上，我们的方法可以视为EXPFamily中的最大化可能性估计器。此外，我们还证明了我们的估计器可以视为一种Bregman分数的最小化解决方案以及一种surrogate likelihood的最小化解决方案。我们还提供了一些finite sample guarantees，可以在样本复杂度为$O({\sf poly}(k)/\alpha^2)$下达到参数估计中的错误（在$\ell_2$-norm）为 $\alpha$ 的目标。在特制化为节点离散Markov随机场景下，我们的方法可以 достичь顺序优化的样本复杂度$O({\sf log}(k)/\alpha^2)$。最后，我们通过数值实验证明了我们的估计器的性能。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Mask-Networks"><a href="#Ensemble-Mask-Networks" class="headerlink" title="Ensemble Mask Networks"></a>Ensemble Mask Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06382">http://arxiv.org/abs/2309.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lok-18/GeSeNet">https://github.com/lok-18/GeSeNet</a></li>
<li>paper_authors: Jonny Luntzel</li>
<li>for: 该研究是否可以使用 $\mathbb{R}^n\rightarrow \mathbb{R}^n$ Feedforward Network 来学习矩阵-向量乘法？</li>
<li>methods: 研究提出了两种机制：灵活的面masking 来处理矩阵输入，以及特有的网络剪辑来尊重面mask的依赖结构。</li>
<li>results: 研究表明，可以使用这些机制来近似固定操作，如矩阵-向量乘法 $\phi(A,x) \rightarrow Ax$，并应用于测试依赖关系或交互顺序在图模型中。<details>
<summary>Abstract</summary>
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
</details>
<details>
<summary>摘要</summary>
可以不是$\mathbb{R}^n\to \mathbb{R}^n$的Feedforward网络学习矩阵-向量乘法吗？这个研究提出了两种机制：灵活的面 masking来接受矩阵输入，以及专门针对面 mask的网络剪辑来尊重面 mask的依赖结构。网络可以近似固定操作，如矩阵-向量乘法 $\phi(A,x) \to Ax$，激励我们提出的机制和应用于图模型中的依赖测试或交互顺序。
</details></li>
</ul>
<hr>
<h2 id="InstaFlow-One-Step-is-Enough-for-High-Quality-Diffusion-Based-Text-to-Image-Generation"><a href="#InstaFlow-One-Step-is-Enough-for-High-Quality-Diffusion-Based-Text-to-Image-Generation" class="headerlink" title="InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation"></a>InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06380">http://arxiv.org/abs/2309.06380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gnobitab/instaflow">https://github.com/gnobitab/instaflow</a></li>
<li>paper_authors: Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu</li>
<li>for: 这篇论文是为了提出一种新的一步 diffusion-based text-to-image generator，使得 diffusion 模型可以更快速地生成高质量图像。</li>
<li>methods: 该论文使用了一种新的Rectified Flow方法，该方法可以更好地考虑图像和噪声之间的关系，从而提高 diffusion 模型的 sampling 速度和图像质量。</li>
<li>results: 该论文通过使用 Rectified Flow 方法和一种新的 text-conditioned pipeline，成功地将 diffusion 模型转化为一种快速的一步模型，并实现了在 MS COCO 2017-5k 上的 FID 值为 $23.3$，比之前的最佳技术进行了显著的提升 ($37.2$ $\rightarrow$ $23.3$ 在 FID 上)。此外，该论文还提出了一种基于 expanded network 的一步模型，可以进一步提高 FID 值。<details>
<summary>Abstract</summary>
Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37.2$ $\rightarrow$ $23.3$ in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to $22.4$. We call our one-step models \emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow yields an FID of $13.1$ in just $0.09$ second, the best in $\leq 0.1$ second regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Project page:~\url{https://github.com/gnobitab/InstaFlow}.
</details>
<details>
<summary>摘要</summary>
Diffusion模型已经革命化了文本到图像生成，其品质和创造力都非常出色。然而，它的多步采样过程相对较慢，通常需要数十个推理步骤以获得满意的结果。先前的尝试使用热化法提高采样速度和降低计算成本通过热化法，但是没有实现一步模型。在这篇论文中，我们探讨一种名为Rectified Flow的新方法，它只在小数据集上使用过。Rectified Flow的核心在于它的“重定向”过程，它折叠概率流的轨迹，细化图像和噪声之间的协同关系，并且通过学生模型进行热化。我们提出了一种基于Stable Diffusion（SD）的文本条件管道，将Stable Diffusion转化为超快一步模型。我们发现，在这种管道中，重定向扮演了关键的作用，使图像和噪声之间的分配得到了改善。我们的一步模型命名为InstaFlow。在 MS COCO 2017-5k 上，InstaFlow 的 FID 为 $23.3$，超过了之前的最佳技术进行热化的进步 ($37.2$ $\rightarrow$ $23.3$ 的 FID)。通过使用扩展的网络和 1.7B 参数，我们进一步提高了 FID 到 $22.4$。我们的一步模型在 MS COCO 2014-30k 上的 FID 为 $13.1$，在 $0.09$ 秒钟内完成，超过了最近的 StyleGAN-T ($13.9$ 在 $0.1$ 秒钟内完成)。值得注意的是，我们在训练 InstaFlow 时只需要 199 A100 GPU 天。更多细节请参考我们的项目页面：https://github.com/gnobitab/InstaFlow。
</details></li>
</ul>
<hr>
<h2 id="Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery"><a href="#Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery" class="headerlink" title="Using Reed-Muller Codes for Classification with Rejection and Recovery"></a>Using Reed-Muller Codes for Classification with Rejection and Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06359">http://arxiv.org/abs/2309.06359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfenth/rmaggnet">https://github.com/dfenth/rmaggnet</a></li>
<li>paper_authors: Daniel Fentham, David Parker, Mark Ryan</li>
<li>for: 防止攻击者透过对于训练数据集的类别器进行适应的攻击，使其给出错误的输出。</li>
<li>methods: 使用Reed-Muller 错误修复码以实现类别-弃置方法，可以正确地拒绝类别器对于训练数据集之外的输入。</li>
<li>results: 可以在多种不同的类别攻击下，以低 incorrectness 维持良好的正确率，并且可以降低类别器对于输入资料进行额外处理的量。<details>
<summary>Abstract</summary>
When deploying classifiers in the real world, users expect them to respond to inputs appropriately. However, traditional classifiers are not equipped to handle inputs which lie far from the distribution they were trained on. Malicious actors can exploit this defect by making adversarial perturbations designed to cause the classifier to give an incorrect output. Classification-with-rejection methods attempt to solve this problem by allowing networks to refuse to classify an input in which they have low confidence. This works well for strongly adversarial examples, but also leads to the rejection of weakly perturbed images, which intuitively could be correctly classified. To address these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), a classifier inspired by Reed-Muller error-correction codes which can correct and reject inputs. This paper shows that RMAggNet can minimise incorrectness while maintaining good correctness over multiple adversarial attacks at different perturbation budgets by leveraging the ability to correct errors in the classification process. This provides an alternative classification-with-rejection method which can reduce the amount of additional processing in situations where a small number of incorrect classifications are permissible.
</details>
<details>
<summary>摘要</summary>
（在实际应用中，用户们对分类器的输入应答预期正确。然而，传统的分类器不能处理与训练数据集远离的输入。恶意者可以通过制作针对分类器输出错误的攻击来利用这个漏洞。分类-拒绝方法试图解决这个问题，allowing networks to refuse to classify an input in which they have low confidence。这些方法在强有攻击例子上工作良好，但也会拒绝弱有攻击的图像，这些图像可能可以正确地分类。为解决这些问题，我们提出了Reed-Muller Aggregation Networks（RMAggNet），一种基于Reed-Muller错误修复码的分类器，可以对输入进行修复和拒绝。这个论文显示，RMAggNet可以最小化错误的同时保持多种攻击下的良好正确率，通过在分类过程中利用错误修复的能力。这提供了一种更加可靠的分类-拒绝方法，可以在有限的额外处理下减少错误数量。）
</details></li>
</ul>
<hr>
<h2 id="Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors"><a href="#Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors" class="headerlink" title="Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors"></a>Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06349">http://arxiv.org/abs/2309.06349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Jaiswal, Debdeep Pati, Anirban Bhattacharya, Bani K. Mallick</li>
<li>For: The paper proposes a new variant of Thompson Sampling (TS) algorithm, called $\alpha$-TS, which uses a fractional posterior distribution to improve the performance of TS in stochastic multi-armed bandit problems.* Methods: The paper uses a novel regret analysis technique that combines recent theoretical developments in non-asymptotic concentration analysis and Bernstein-von Mises type results to derive instance-dependent and instance-independent regret bounds for $\alpha$-TS.* Results: The paper obtains both instance-dependent and instance-independent regret bounds for $\alpha$-TS, which are of the order $\mathcal{O}(\sum_{k \neq i^*} \Delta_k(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2}))$ and $\mathcal{O}(\sqrt{KT\log K})$, respectively, under very mild conditions on the prior and reward distributions. The results show that $\alpha$-TS achieves a better performance than the standard TS algorithm.<details>
<summary>Abstract</summary>
Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\alpha$-TS, where we use a fractional or $\alpha$-posterior ($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute an $\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior distribution just require its density to be positive, continuous, and bounded. We also establish another instance-dependent regret upper bound that matches (up to constants) to that of improved UCB [Auer and Ortner, 2010]. Our regret analysis carefully combines recent theoretical developments in the non-asymptotic concentration analysis and Bernstein-von Mises type results for the $\alpha$-posterior distribution. Moreover, our analysis does not require additional structural properties such as closed-form posteriors or conjugate priors.
</details>
<details>
<summary>摘要</summary>
Thompson 抽取（TS）是多臂抽奖问题中最受欢迎并且最早的算法之一。我们考虑了一种变体TS，称为α-TS，其中我们使用一个分数或α- posterior（α belongs to (0,1)）而不是标准 posterior distribution。为计算α-posterior，抽奖likelihood在标准 posterior定义中被温和了一个因子α。对于α-TS，我们得到了两种不同的频见 regret bounds：一种是 $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$，另一种是 $\mathcal{O}(\sqrt{KT\log K})$，它们都是在非常轻微的假设下得到的，其中 $\Delta_k$ 是真实奖劵的 gap，$C(\alpha)$ 是已知的常量。两者都满足我们的奖劵分布的总体条件。我们的假设只需要其density是正态，连续和受限。我们还证明了另一种相当于改进UCb的频见 regret upper bound，它与（在常量上）相同。我们的 regret分析结合了非对称峰度分析的最新成果和 Bernstein-von Mises 类型的结果。此外，我们的分析不需要额外的结构性质，例如封闭的 posterior 或 conjugate prior。
</details></li>
</ul>
<hr>
<h2 id="Band-gap-regression-with-architecture-optimized-message-passing-neural-networks"><a href="#Band-gap-regression-with-architecture-optimized-message-passing-neural-networks" class="headerlink" title="Band-gap regression with architecture-optimized message-passing neural networks"></a>Band-gap regression with architecture-optimized message-passing neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06348">http://arxiv.org/abs/2309.06348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Bechtel, Daniel T. Speckhard, Jonathan Godwin, Claudia Draxl</li>
<li>for: 本研究使用Graph-based neural networks和Message-passing neural networks（MPNNs）预测物质的物理性质。</li>
<li>methods: 研究使用density functional theory数据从AFLOW数据库进行分类物质为半导体&#x2F;绝缘体或金属。然后进行神经建模搜索，探索MPNNs模型结构和 гипер参数空间，预测非金属材料的带隙。搜索参数包括消息传递步数、隐藏大小和活动函数等。</li>
<li>results: 搜索得到的最佳模型组成一个ensemble，与现有文献模型相比显著提高了预测性能。不确定性评估使用Monte-Carlo Dropout和拟合，ensemble方法胜利。研究分析了ensemble模型的适用范围，包括晶系、包括Hubbard参数在内的density functional计算、物质组成元素等。<details>
<summary>Abstract</summary>
Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the inclusion of a Hubbard parameter in the density functional calculations, and the atomic species building up the materials.
</details>
<details>
<summary>摘要</summary>
基于图的神经网络和特别是消息传递神经网络（MPNN）在预测固体物理性质方面表现出了很大的潜力。在这项工作中，我们使用MPNN对AFLOW数据库中的密度函数理论数据进行分类，将材料分为金属和半导体/离子体两类。然后，我们进行神经建筑搜索，探索MPNN的模型结构和超参空间，以预测非金属材料的带隙。搜索的参数包括消息传递步数、隐藏大小和活动函数等。最佳性能的模型从搜索中选拔，组成了一个ensemble，该ensemble在比较之前 существу的模型中表现出了显著的优异。uncertainty量化通过蒙特卡罗Dropout和拟合来评估，ensemble方法更为超越。预测模型的适用范围通过晶系、包括Hubbard参数在内的密度函数计算以及物质的原子组成来分析。
</details></li>
</ul>
<hr>
<h2 id="Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning"><a href="#Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning" class="headerlink" title="Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning"></a>Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06315">http://arxiv.org/abs/2309.06315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole-Christoffer Granmo, Per-Arne Andersen, Lei Jiao, Xuan Zhang, Christian Blakely, Tor Tveit</li>
<li>for: 这个论文的目的是为了提出一种新的Tsetlin机器（TM）反馈方案，该方案可以补充类型I和类型II反馈，并通过一种新的Finite State Automaton（自适应独立自动机）来学习目标变量的Markov边界。</li>
<li>methods: 该论文使用了一种新的Context-Specific Independence Automaton（自适应独立自动机）来学习目标变量的Markov边界，并通过TM反馈方案来推断目标变量。</li>
<li>results: 该论文的实验结果表明，新的TM反馈方案可以借助自适应独立自动机来找到Markov边界，并且可以减少不必要的变量，从而提高模型的准确率和效率。<details>
<summary>Abstract</summary>
A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific Independence Automaton. The automaton learns which features are outside the Markov boundary of the target, allowing them to be pruned from the TM during learning. We investigate the new scheme empirically, showing how it is capable of exploiting context-specific independence to find Markov boundaries. Further, we provide a theoretical analysis of convergence. Our approach thus connects the field of Bayesian networks (BN) with TMs, potentially opening up for synergies when it comes to inference and learning, including TM-produced Bayesian knowledge bases and TM-based Bayesian inference.
</details>
<details>
<summary>摘要</summary>
一个集合的变量是一个Markov幕墙（Markov blanket），如果它包含一个变量的所有预测信息，那么它就是一个Markov边界（Markov boundary）。如果幕墙不能简化而失去有用信息，那么它就是一个Markov边界。确定一个变量的Markov边界有利，因为所有外部变量都是 redundant。因此，Markov边界提供了一个优化的特征集。然而，从数据中学习Markov边界是具有挑战性的，因为如果从Markov边界中移除一个变量，那么外部变量可能会提供信息，而变量在边界中可能会停止提供信息。每个候选变量的真实角色只有在Markov边界已经确定后才能manifest。在这篇论文中，我们提出了一种新的Tsetlin机器（TM）反馈方案，该方案在Type I和Type II反馈的基础之上补充了一种新的Finite State Automaton（Context-Specific Independence Automaton）。这个自动机学习Markov边界中的变量是否外部提供信息，以便在TM学习过程中从Markov边界中除去这些变量。我们通过实验证明了这种新方案的可行性，并证明了其可以利用上下文特定的独立性来找到Markov边界。此外，我们还提供了一种理论分析的结论，证明我们的方法可以 converges。我们的方法因此将Bayesian网络（BN）和TM相连接，可能开启了在推理和学习方面的 synergies，包括TM生成的Bayesian知识库和TM基于Bayesian推理。
</details></li>
</ul>
<hr>
<h2 id="Semantic-and-Articulated-Pedestrian-Sensing-Onboard-a-Moving-Vehicle"><a href="#Semantic-and-Articulated-Pedestrian-Sensing-Onboard-a-Moving-Vehicle" class="headerlink" title="Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle"></a>Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06313">http://arxiv.org/abs/2309.06313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Priisalu</li>
<li>for: 本研究旨在提高交通安全，通过利用LiDAR探测器直接测量人员动作和距离来提高人员检测和预测。</li>
<li>methods: 本研究使用LiDAR探测器获取数据，并应用图像处理技术来实现人员检测和预测。</li>
<li>results: 本研究显示，通过使用LiDAR探测器和图像处理技术，可以准确地检测和预测人员动作，提高交通安全。<details>
<summary>Abstract</summary>
It is difficult to perform 3D reconstruction from on-vehicle gathered video due to the large forward motion of the vehicle. Even object detection and human sensing models perform significantly worse on onboard videos when compared to standard benchmarks because objects often appear far away from the camera compared to the standard object detection benchmarks, image quality is often decreased by motion blur and occlusions occur often. This has led to the popularisation of traffic data-specific benchmarks. Recently Light Detection And Ranging (LiDAR) sensors have become popular to directly estimate depths without the need to perform 3D reconstructions. However, LiDAR-based methods still lack in articulated human detection at a distance when compared to image-based methods. We hypothesize that benchmarks targeted at articulated human sensing from LiDAR data could bring about increased research in human sensing and prediction in traffic and could lead to improved traffic safety for pedestrians.
</details>
<details>
<summary>摘要</summary>
很难从车辆上捕捉的视频进行3D重建，因为车辆前进速度很快。even object detection和人体感知模型在车辆上视频中表现比标准 referential benchmarks 更差，因为物体在摄像头中的距离比标准 referential benchmarks 更远，图像质量通常受到运动模糊和遮挡的影响。这导致了交通数据特有的benchmarks的普及。recently, Light Detection And Ranging (LiDAR) 感知器已经成为了直接测量深度的方法。然而，LiDAR基本方法仍然缺乏远程人体检测的弯曲性，相比于图像基本方法。我们提出了人体检测和预测在交通中的benchmarks可能会促进人体检测和预测的研究，从而提高了步行人员的交通安全。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Supply-and-Demand-in-Public-Transportation-Systems"><a href="#Modeling-Supply-and-Demand-in-Public-Transportation-Systems" class="headerlink" title="Modeling Supply and Demand in Public Transportation Systems"></a>Modeling Supply and Demand in Public Transportation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06299">http://arxiv.org/abs/2309.06299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Miranda Bihler, Hala Nelson, Erin Okey, Noe Reyes Rivas, John Webb, Anna White</li>
<li>for: 帮助海arrisonburg公共交通部门（HDPT）利用数据提高运营效率和效果。</li>
<li>methods: 我们构建了两个供应和需求模型，帮助部门识别服务中的缺陷。这些模型考虑了许多变量，包括HDPT向联邦政府报告的方式和海arrisonburg市最为易受影响的区域。我们使用数据分析和机器学习技术进行预测。</li>
<li>results: 我们的预测可以帮助HDPT更好地了解其服务的需求和供应，从而提高运营效率和效果。<details>
<summary>Abstract</summary>
The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
</details>
<details>
<summary>摘要</summary>
哈里逊堡公共交通部门（HDPT）想要通过数据来提高其运营效率和效果。我们构建了两个供应和需求模型，帮助部门确定其服务中的缺陷。这些模型考虑了许多变量，包括哈里逊堡市政府报告给联邦政府的方式以及城市中最为投降的地区。我们使用数据分析和机器学习技术进行预测。
</details></li>
</ul>
<hr>
<h2 id="Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition"><a href="#Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition" class="headerlink" title="Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition"></a>Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06286">http://arxiv.org/abs/2309.06286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutahar Safdar, Jiarui Xie, Hyunwoong Ko, Yan Lu, Guy Lamouche, Yaoyao Fiona Zhao</li>
<li>for: 这个研究旨在提高数据驱动的Additive Manufacturing（AM）研究的效率和可重用性，通过将AM和人工智能（AI）两个context中的知识融合并形式化。</li>
<li>methods: 该研究提出了一个三步知识传递可能性分析框架，包括预传递、传递和后传递步骤，以支持数据驱动的AM知识传递。在这个框架中，AM知识被抽象为特定的知识组件，并进行了对 flagship metal AM process之间的比较。</li>
<li>results: 研究发现，可以成功地将LPBF（激光粉末床印刷）中的数据驱动解决方案转移到DED（指定能量激光处理）中，包括数据表示、模型架构和模型参数。这种数据驱动的AM知识传递可以在未来通过自动化pipeline进行效率地跨context或跨过程传递。<details>
<summary>Abstract</summary>
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The framework consists of pre-transfer, transfer, and post-transfer steps to accomplish knowledge transfer. A case study is conducted between flagship metal AM processes. Laser Powder Bed Fusion (LPBF) is the source of knowledge motivated by its relative matureness in applying AI over Directed Energy Deposition (DED), which drives the need for knowledge transfer as the less explored target process. We show successful transfer at different levels of the data-driven solution, including data representation, model architecture, and model parameters. The pipeline of AM knowledge transfer can be automated in the future to allow efficient cross-context or cross-process knowledge exchange.
</details>
<details>
<summary>摘要</summary>
“数据驱动的研究在加法制造（AM）领域在最近几年内取得了显著的成功。这导致了大量的科学文献的出现。这些文献中的知识包括AM和人工智能（AI）上下文的知识，它们尚未被完整地整合和形式化。此外，没有工具或指南来支持数据驱动知识的交叉Context Transfer。因此，为了解决特定AM过程技术的问题，开发和验证了特定AI技术的数据驱动解决方案。我们提出了一种三步知识可传递性分析框架，用于支持数据驱动AM知识的交叉Context Transfer。在这个框架中，AM知识被Feature化成标识的知识组件。框架包括前传、传输和后传步骤，以完成知识交叉Context Transfer。我们通过将LPBF（激光粉末床干）作为知识源，对DPD（导向热处理）进行了知识传输，并成功实现了数据表示、模型架构和模型参数之间的交叉Context Transfer。未来，可以自动化AM知识交叉Context Transfer的管道，以便有效地进行Context Cross-Process知识交换。”
</details></li>
</ul>
<hr>
<h2 id="ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method"><a href="#ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method" class="headerlink" title="ELRA: Exponential learning rate adaption gradient descent optimization method"></a>ELRA: Exponential learning rate adaption gradient descent optimization method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06274">http://arxiv.org/abs/2309.06274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kleinsorge, Stefan Kupper, Alexander Fauck, Felix Rothe</li>
<li>For: The paper presents a new, fast, and universal gradient-based optimizer algorithm called Exponential Learning Rate Adaptation (ELRA) that adapts the learning rate by situational awareness and does not rely on hand-tuned parameters.* Methods: The method uses a novel approach to adapt the learning rate based on the orthogonality of neighboring gradients, which leads to a high success rate and fast convergence. The method is also rotation invariant and applies to problems of any dimension and scale.* Results: The paper demonstrates the impressive performance of ELRA through extensive experiments on the MNIST benchmark dataset against state-of-the-art optimizers. The authors also present two variants of the algorithm, c2min and p2min, with slightly different control.<details>
<summary>Abstract</summary>
We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential Learning Rate Adaption - ELRA. We present it in two variants c2min and p2min with slightly different control. The authors strongly believe that ELRA will open a completely new research direction for gradient descent optimize.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的、快速（加速率适应）、无参数（无参数）的梯度基本算法。该方法的主要想法是根据情况意识来适应学习率α，主要寻求垂直邻域梯度的平行性。该方法具有高成功率和快速收敛率，不需要手动调整参数，因此具有更广泛的通用性。它可以应用于任意维度n和问题的缩放问题，并且只linearly（对数(n)）随问题维度增长。它可以优化凸和非凸连续景观，只要提供一定的梯度。与Ada家族（AdaGrad、AdaMax、AdaDelta、Adam等）不同，该方法是坐标选择不依赖的：优化路径和性能独立于坐标选择。我们在MNIST数据集上进行了广泛的实验，证明了该新类optimizers的出色性，我们命名它为泛化学习率适应（ELRA）。我们在c2min和p2min两种不同的控制下提出了两种变体。作者们认为，ELRA将打开一个 Completely new的研究方向，为梯度下降优化带来很大的发展。
</details></li>
</ul>
<hr>
<h2 id="ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation"><a href="#ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation" class="headerlink" title="ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation"></a>ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06268">http://arxiv.org/abs/2309.06268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Snigdha Sen, Saurabh Singh, Hayley Pye, Caroline Moore, Hayley Whitaker, Shonit Punwani, David Atkinson, Eleftheria Panagiotaki, Paddy J. Slator<br>for: 这个研究旨在利用MRI技术诊断前列腺癌（PCa），并且使用Diffusion MRI（dMRI）来估计细胞大小等微struc特征。methods: 这个研究使用了一种名为VERDICT（Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours）的三个分类 compartment biophysical模型，并使用自动化学习（self-supervised learning）来学习这个模型的参数。results: 研究发现，使用自动化学习方法可以实现不需要明确训练标签的模型适配，并且比起传统的非线性最小二乘（NLLS）和专案化神经网络（DNNs）方法，这种方法可以提高估计精度和减少偏差。此外，这个方法还可以实现高度的自信度水平，用于精确诊断前列腺癌和正常组织之间的区别。<details>
<summary>Abstract</summary>
MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for the first time, fitting of a complex three-compartment biophysical model with machine learning without the requirement of explicit training labels. We compare the estimation performance to baseline NLLS and supervised DNN methods, observing improvement in estimation accuracy and reduction in bias with respect to ground truth values. Our approach also achieves a higher confidence level for discrimination between cancerous and benign prostate tissue in comparison to the other methods on a dataset of 20 PCa patients, indicating potential for accurate tumour characterisation.
</details>
<details>
<summary>摘要</summary>
MRI 在诊断 prostates cancer (PCa) 中变得越来越普遍，diffusion MRI (dMRI) 扮演着重要的角色。当与计算模型结合时，dMRI 可以估算微结构信息，如细胞大小。传统上，这些模型通常使用非线性最小二乘 (NLLS) 曲线函数拟合方法，具有高计算成本。深度神经网络 (DNNs) 是一种有效的替代方案，但它们的性能受训练数据的下面分布的影响很大。无监督学习是一个吸引人的选择，它可以让网络学习输入数据的特征自身，而不需要单独的训练数据集。在这种情况下，我们介绍了一种无监督 DNN，用于估算VERDICT（血管、 extracellular 和限制的扩散 для细胞学）模型的参数。我们在20名PCa患者的数据集上示示出，我们的方法可以在没有明确训练标签的情况下，高度准确地估算模型参数，并且与基准值之间的偏差更小。此外，我们的方法还可以在区分患有PCa和正常组织时达到更高的信任度，表明它具有潜在的精准肿瘤特征化能力。
</details></li>
</ul>
<hr>
<h2 id="Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning"><a href="#Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning" class="headerlink" title="Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning"></a>Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06260">http://arxiv.org/abs/2309.06260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Beck, Marius Kurz</li>
<li>For: 该研究旨在开发一种基于Reinforcement Learning（RL）的精度适应闭合模型，以解决在含有滤波器的大气流 simulate（LES）中的隐式滤波器问题。* Methods: 该研究使用了Markov决策过程（MDP）和RL来优化LES闭合模型的精度，并应用到了explicit和implicit的闭合模型中。在explicit模型中，RL被应用来优化一个元素本地的液化粘度模型。在implicit模型中，RL被用来优化一种hybrid的Discontinuous Galerkin（DG）和finite volume方法的混合策略。* Results: 该研究发现，使用RL优化的闭合模型可以提供精度适应的闭合，并且能够减少LES中的滤波器问题引起的不确定性。另外，explicit模型在不同的精度和分辨率下都能够达到或超过现有的STATE-OF-THE-ART模型的准确性。同时，implicit模型在hybrid scheme中显示出了一定的可能性，可能成为一种新的高级别模型。<details>
<summary>Abstract</summary>
We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous Galerkin (DG) and finite volume scheme. All newly derived models achieve accurate and consistent results, either matching or outperforming classical state-of-the-art models for different discretizations and resolutions. Moreover, the explicit model is demonstrated to adapt its distribution of viscosity within the DG elements to the inhomogeneous discretization properties of the operator. In the implicit case, the optimized hybrid scheme renders itself as a viable modeling ansatz that could initiate a new class of high order schemes for compressible turbulence. Overall, the results demonstrate that the proposed RL optimization can provide discretization-consistent closures that could reduce the uncertainty in implicitly filtered LES.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于开发适应离散级别的关闭方法，用于不可见滤波的大气动学模拟（LES）。在不可见滤波LES中，涉及到网格和离散算法的filter核心，以及关闭项，都是由网格和离散算法的性质决定的，这会导致额外的计算 подgrid项，这些项通常在先前分析中不可知道。因此，我们将 adapting  coefficients of LES closure models  reformulated as a Markov decision process, and solved in an a posteriori manner with Reinforcement Learning (RL)。这 allows to adjust the model to the actual discretization, and also incorporates the interaction between the discretization and the model itself。这个优化框架应用于 both explicit and implicit closure models。在这个框架中，我们使用了一种元素本地的质量系数模型作为explicit模型。对于implicit模型，我们使用RL来确定一个最佳的混合策略，用于 hybrid离散格（DG）和质量量算法。所有新 derivation的模型都达到了准确和一致的结果， either matching or outperforming classical state-of-the-art models for different discretizations and resolutions。此外，explicit模型被示出可以在 DG 元素中适应不同的离散属性。在implicit caso，优化的混合方案被证明是一种可能的高级模型 ansatz，可以开启一种新的高级方法 для压缩性液体动力学。总之，结果表明，我们提出的RL优化可以提供适应离散级别的关闭，以减少不可见滤波LES中的uncertainty。
</details></li>
</ul>
<hr>
<h2 id="Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models"><a href="#Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models" class="headerlink" title="Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models"></a>Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06256">http://arxiv.org/abs/2309.06256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, Tong Zhang</li>
<li>for: 这篇研究探讨了基础模型（包括 Computer Vision Language Models（VLMs）和 Large Language Models（LLMs））在不同分布和任务上的应用，以及如何在这些基础模型上进行微调以提高任务性能或调整模型的行为，以使其获得特化。</li>
<li>methods: 这篇研究使用了多种常规化方法来处理基础模型的特化和通用性之间的贸易，包括 continual learning、Wise-FT 方法和 Low-Rank Adaptation（LoRA）方法。</li>
<li>results: 研究结果显示，使用 continual learning 和 Wise-FT 方法可以有效地抵消特化和通用性之间的贸易，并且 Wise-FT 方法在将特化和通用性平衡的情况下表现最佳。<details>
<summary>Abstract</summary>
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions and common sense.   To address the trade-off between the speciality and generality, we investigate multiple regularization methods from continual learning, the weight averaging method (Wise-FT) from out-of-distributional (OOD) generalization, which interpolates parameters between pre-trained and fine-tuned models, and parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our findings show that both continual learning and Wise-ft methods effectively mitigate the loss of generality, with Wise-FT exhibiting the strongest performance in balancing speciality and generality.
</details>
<details>
<summary>摘要</summary>
基础模型，包括视觉语言模型（VLM）和大型语言模型（LLM），具有涵盖多种分布和任务的通用性，这是由于它们在预训练 dataset 的广泛采集得到的。然而，在精度调整过程中，使用小型 dataset 可能无法完全覆盖在预训练中遇到的多种分布和任务。因此，在精度调整过程中寻求特点可能会导致模型失去通用性，这与深度学习中的恐慌忘记（CF）有关。在这种情况下，我们在 VLM 和 LLM 中进行了实验，发现精度调整后模型对多种分布的处理能力减退，以及模型无法遵循指令和常识。为了解决特点和通用性之间的负担，我们研究了多种 kontinual learning 策略，包括 weight averaging method（Wise-FT）和 parameter-efficient fine-tuning 方法如 Low-Rank Adaptation（LoRA）。我们的发现表明， kontinual learning 和 Wise-FT 方法都能有效地减轻失去通用性的问题，其中 Wise-FT 方法在均衡特点和通用性方面表现最佳。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation"><a href="#Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation" class="headerlink" title="Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation"></a>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06255">http://arxiv.org/abs/2309.06255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu</li>
<li>for: 这篇论文旨在jointly incorporating heterogeneous information from different modalities, but most models often suffer from unsatisfactory multi-modal cooperation.</li>
<li>methods: 我们提出了一种 fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level, and further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities.</li>
<li>results: 我们的方法可以reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.<details>
<summary>Abstract</summary>
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities in a targeted manner. Overall, our methods reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.
</details>
<details>
<summary>摘要</summary>
一个主要的多Modal学习话题是将多种不同模式的信息合并 incorporate 到一起。然而，大多数模型经常受到不满足的多模式协作的影响，无法合理地利用所有模式。一些方法可以识别并提高不良学习的模式，但是往往没有可靠的理论支持，并且具有精细化的样本级别的多模式协作观察能力。因此，我们需要合理地观察和改进多模式之间的细化协作，特别是在面临实际场景中，模式差异可能会随着不同的样本而变化。为此，我们引入了细化的模式价值指标，以评估每个模式在样本级别的贡献。通过模式价值评估，我们发现多模式模型往往依赖于一个特定的模式，导致其他模式成为低贡献的。我们进一步分析这一问题，并在targeted 的方式提高低贡献的模式的探测能力，以提高多模式模型的性能。总之，我们的方法可以合理地观察样本级别的细化单模式贡献，并实现了不同多模式模型的显著改进。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data"><a href="#Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data" class="headerlink" title="Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data"></a>Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06248">http://arxiv.org/abs/2309.06248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Euihyeon Choi, Jooyoung Kim, Wonkyung Lee</li>
<li>for: 这种研究的目的是为了评估电子竞技中赢球概率估计模型的可靠性，并提出一种新的评估指标来取代准确率。</li>
<li>methods: 这种研究使用了布里尔分数和预期准确错误（ECE）来评估赢球概率估计模型的性能，并提出了一种新的评估指标called Balance score。</li>
<li>results: 研究发现，Balance score具有六个好的属性，并且在普遍情况下可以效果地 aproximate true expected calibration error。此外，通过实验研究和真实游戏快照数据，证明了该指标的批处性和可靠性。<details>
<summary>Abstract</summary>
Probability estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. Among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. The win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. However, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. In this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. Based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good properties that probability estimation metric should have. Under the general condition, we also found that the Balance score can be an effective approximation of the true expected calibration error which has been imperfectly approximated by ECE using the binning technique. Extensive evaluations using simulation studies and real game snapshot data demonstrate the promising potential to adopt the proposed metric not only for the win probability estimation model for esports but also for evaluating general probability estimation models.
</details>
<details>
<summary>摘要</summary>
概率估计模型在各个领域中扮演着重要的角色，如天气预测、推荐系统和体育分析。然而，评估这些模型提供的可靠性很困难，因为真实的概率不可获得。电竞赛事中的赢场概率估计模型是活跃的研究领域之一。然而，大多数前一些工作使用准确率作为评估metric，这只能衡量推理的性能。在这种情况下，我们首次研究了布里尔分数和预期准确性错误（ECE）作为评估win probability estimation模型的metric。基于分析，我们提出了一个新的metriccalled Balance score，它具有六个好的性能特点。在总的来说，我们发现Balance score可以作为true expected calibration error的有效近似，而ECE使用分组技术的近似结果不够 precisel。通过 simulated studies和实际游戏快照数据，我们展示了提议的metric在不只是win probability estimation模型，还可以用于评估总体概率估计模型的承诺潜力。
</details></li>
</ul>
<hr>
<h2 id="Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks"><a href="#Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks" class="headerlink" title="Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks"></a>Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06240">http://arxiv.org/abs/2309.06240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Pernot<br>for:This paper focuses on reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks, specifically in materials and chemical science.methods:The paper proposes and illustrates adapted validation methods that assess both consistency and adaptivity of ML-UQ methods. These methods include so-called reliability diagrams, as well as adaptive validation techniques that evaluate the reliability of predictions and uncertainties for any point in feature space.results:The paper shows that consistency and adaptivity are complementary validation targets, and that a good consistency does not necessarily imply a good adaptivity. The proposed adapted validation methods are illustrated on a representative example, demonstrating their effectiveness in evaluating the reliability of ML-UQ predictions.<details>
<summary>Abstract</summary>
Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
</details>
<details>
<summary>摘要</summary>
可靠的不确定量评估（UQ）在机器学习（ML）回归任务中成为了许多研究的关注点，特别是在材料和化学科学领域。现在已经广泛认可，平均调整不够，大多数研究通过测试条件调整和不确定性之间的一致性来评估Consistency。一致性通常通过所谓的可靠性图表进行评估。然而，有另一种超出平均调整的方法，即基于输入特征的conditional calibration，即适应性。在实践中，适应性是最终用户的ML-UQ方法的可靠性和不确定性预测和任何点特征空间的关键问题。本文目的是表明一致性和适应性是补充的验证目标，并且一个好的一致性不一定意味着一个好的适应性。适应验证方法被提出并在一个代表性的例子中 illustrate。
</details></li>
</ul>
<hr>
<h2 id="Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory"><a href="#Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory" class="headerlink" title="Risk-Aware Reinforcement Learning through Optimal Transport Theory"></a>Risk-Aware Reinforcement Learning through Optimal Transport Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06239">http://arxiv.org/abs/2309.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>For: 这篇论文旨在探讨如何在动态和不确定环境中使用奖励学习（RL），同时考虑风险管理。* Methods: 该论文提出了结合优化运输（OT）理论和RL的风险感知框架，以便在RL目标函数中考虑风险因素。* Results: 该论文提供了一系列定理，证明了风险分布、优化值函数和策略行为之间的关系。通过OT的数学精度，该论文提供了一种权衡奖励和风险考虑的RL框架。<details>
<summary>Abstract</summary>
In the dynamic and uncertain environments where reinforcement learning (RL) operates, risk management becomes a crucial factor in ensuring reliable decision-making. Traditional RL approaches, while effective in reward optimization, often overlook the landscape of potential risks. In response, this paper pioneers the integration of Optimal Transport (OT) theory with RL to create a risk-aware framework. Our approach modifies the objective function, ensuring that the resulting policy not only maximizes expected rewards but also respects risk constraints dictated by OT distances between state visitation distributions and the desired risk profiles. By leveraging the mathematical precision of OT, we offer a formulation that elevates risk considerations alongside conventional RL objectives. Our contributions are substantiated with a series of theorems, mapping the relationships between risk distributions, optimal value functions, and policy behaviors. Through the lens of OT, this work illuminates a promising direction for RL, ensuring a balanced fusion of reward pursuit and risk awareness.
</details>
<details>
<summary>摘要</summary>
在RL环境中，决策是不确定和动态的，风险管理就成为决策的关键因素。传统RL方法虽然能够优化奖励，但frequently ignore潜在的风险风险。为了解决这问题，这篇论文提出了结合RL和优化运输（OT）理论的风险意识框架。我们的方法修改了目标函数，使得结果策略不仅最大化预期奖励，而且也遵循OT距离 zwischen state visitation distributions和欲望的风险质量。通过OT的数学精度，我们提供了一种形ulation，将风险考虑与传统RL目标函数相结合。我们的贡献得到了一系列定理的证明，映射了风险分布、优化值函数和策略行为之间的关系。通过OT的视角，这项工作探讨了RL中风险意识和奖励追求的平衡 fusion的可能性。
</details></li>
</ul>
<hr>
<h2 id="The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models"><a href="#The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models" class="headerlink" title="The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"></a>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06236">http://arxiv.org/abs/2309.06236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Spathis, Fahim Kawsar</li>
<li>for: 本研究旨在探讨语言模型如何处理numerical&#x2F;temporal数据，以及如何使用这些模型进行人类中心的任务，如移动健康感知。</li>
<li>methods: 本研究使用了各种语言模型，包括各种批处理大语言模型，以及提前调整和多模态适配器等方法来解决模态差距问题。</li>
<li>results: 研究发现，当 feeding numerical&#x2F;temporal数据到语言模型时，它们可能会 incorrectly tokenize temporal data，导致输出无意义。此外，提出了一些解决方案，如提前调整和多模态适配器，以帮助 bridge the “modality gap”。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore recent works that use LLMs for human-centric tasks such as mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address this issue, we propose potential solutions such as prompt tuning with lightweight embedding layers and multimodal adapters, which can help bridge the "modality gap". While the ability of language models to generalize to other modalities with minimal or no fine-tuning is promising, this paper highlights the fact that their outputs cannot be meaningful if they struggle with input nuances.
</details></li>
</ul>
<hr>
<h2 id="A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models"><a href="#A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models" class="headerlink" title="A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models"></a>A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06230">http://arxiv.org/abs/2309.06230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borui Tang, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</li>
<li>for: 这 paper 的目的是提出一种可扩展的算法，用于在高维数据中选择最佳子集，以便实现最佳subset selection。</li>
<li>methods: 该 paper 使用了一种新的普适的信息梯度函数，以确定最佳子集的大小。这种方法不需要选择模型或链函数，因此非常灵活。</li>
<li>results:  simulations 表明，该 algorithm 不仅是计算效率高，还能准确地回归最佳子集。这些结果表明，该 paper 提出的方法是一种可靠的、灵活的和可扩展的选择最佳子集方法。<details>
<summary>Abstract</summary>
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specific link function and hence is flexible to apply. Extensive simulation results demonstrate that our method is not only computationally efficient but also able to exactly recover the best subset in various settings (e.g., linear regression, Poisson regression, heteroscedastic models).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:分析高维数据引起了对单指数模型（SIMs）和最佳子集选择的增加兴趣。 SIMs 提供一种可解释和灵活的模型框架，而最佳子集选择则 aimsto 找到高维模型中的稀疏模型。然而，高维模型中的最佳子集选择是计算上不可能的。现有方法通常是放弃选择，但不会得到最佳子集解决方案。在这篇论文中，我们直接面临计算上的困难，并提出了首个可扩展的算法，用于高维 SIMs 中的最佳子集选择。我们的算法具有选择子集的一致性和高概率性。我们使用一种通用信息整合函数来确定回归系数的支持大小，从而消除了模型选择调整。此外，我们的方法不假设错误分布或特定的链函数，因此可以适应多种应用场景。我们的实验结果表明，我们的方法不仅是计算效率高，还能够在不同的设置中（例如线性回归、波尔兹回归、不同的误差分布）准确地恢复最佳子集。
</details></li>
</ul>
<hr>
<h2 id="Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data"><a href="#Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data" class="headerlink" title="Long-term drought prediction using deep neural networks based on geospatial weather data"></a>Long-term drought prediction using deep neural networks based on geospatial weather data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06212">http://arxiv.org/abs/2309.06212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vsevolod Grabar, Alexander Marusov, Alexey Zaytsev, Yury Maximov, Nazar Sotiriadi, Alexander Bulkin<br>for:这篇研究的目的是精确预测特定区域的旱情可能性，以便为农业实践中做出知 Informed Decision。methods:本研究使用了多种空间时间神经网络模型，包括卷积LSTM和Transformer，以预测旱情强度基于Palmer旱情严重指数（PDSI）。results:比较分析表明，卷积LSTM和Transformer模型在一个月到六个月的预测 horizons 中具有更高的ROC AUC分数，并且在不同环境条件下进行了广泛验证。<details>
<summary>Abstract</summary>
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.   Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 to 0.70 for forecast horizons from one to six months, outperforming baseline models. The transformer showed superiority for shorter horizons, while ConvLSTM did so for longer horizons. Thus, we recommend selecting the models accordingly for long-term drought forecasting.   To ensure the broad applicability of the considered models, we conduct extensive validation across regions worldwide, considering different environmental conditions. We also run several ablation and sensitivity studies to challenge our findings and provide additional information on how to solve the problem.
</details>
<details>
<summary>摘要</summary>
预测具体地区旱情概率的准确性是农业决策中的关键因素。一年前的预测特别重要，以便进行长期决策。然而，预测这个概率存在复杂的因素间互动和邻近地区的影响，带来挑战。本研究提出了一种综合解决方案，基于多种空间时间神经网络。我们考虑的模型集中 focuses on 根据Palmer旱情严重指数（PDSI）预测旱情程度，利用自然因素和气候模型的信息进行加强旱情预测。对比评估表明，Convolutional LSTM（ConvLSTM）和 transformer 模型在基eline gradient boosting 和 logistic regression 模型的基础上表现出了更高的准确性。这两个前者模型在一到六个月的预测 horizon 上 achievement ROC AUC 分数在0.90到0.70之间，超过基eline模型。transformer 模型在短期预测 horizon 上表现出了优异性，而 ConvLSTM 模型在长期预测 horizon 上具有优势。因此，我们建议根据预测时间长短选择合适的模型。为确保考虑的模型在不同环境条件下的可靠性，我们进行了广泛的验证，覆盖了世界各地的区域。我们还进行了多个减少和敏感性研究，以提供额外的信息和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding"><a href="#Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding" class="headerlink" title="Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding"></a>Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06195">http://arxiv.org/abs/2309.06195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaik Basheeruddin Shah, Pradyumna Pradhan, Wei Pu, Ramunaidu Randhi, Miguel R. D. Rodrigues, Yonina C. Eldar<br>for:这 paper 的目的是研究用于解决线性逆问题的算法，以便在各种应用中更好地使用。methods:这 paper 使用了基于 ISTA 和 ADMM 算法的数据驱动模型感知方法，并在训练过程中使用了梯度下降法。results:这 paper 研究了训练损失的优化保证，并证明在架构层 unfolded 网络中，随着训练样本数量的增加，梯度下降法可以达到near-zero 训练损失。此外，这 paper 还证明了这些 unfolded 网络在训练过程中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gradient descent based methods. Hence, we provide conditions, in terms of the network width and the number of training samples, on these unfolded networks for the PL$^*$ condition to hold. We achieve this by deriving the Hessian spectral norm of these networks. Additionally, we show that the threshold on the number of training samples increases with the increase in the network width. Furthermore, we compare the threshold on training samples of unfolded networks with that of a standard fully-connected feed-forward network (FFNN) with smooth soft-thresholding non-linearity. We prove that unfolded networks have a higher threshold value than FFNN. Consequently, one can expect a better expected error for unfolded networks than FFNN.
</details>
<details>
<summary>摘要</summary>
解决线性逆问题在许多应用中扮演着关键性的角色。基于数据驱动的模型意识 Algorithm unfolding 技术在这些问题上得到了广泛的关注。例如，learned iterative soft-thresholding algorithm (LISTA) 和 alternating direction method of multipliers compressive sensing network (ADMM-CSNet) 等等，都是基于 ISTA 和 ADMM 算法的常用方法。在这种研究中，我们研究了优化保证，即在学习轮数增加时，训练损失减少趋近于零的情况， для Finite-layer unfolded networks  such as LISTA 和 ADMM-CSNet 在过参数化（OP） regime 中。我们通过利用一种修改后的 Polyak-Lojasiewicz 条件（PL$^*$）来实现这一点。在特定的损失 landscape 中满足 PL$^*$ 条件，则可以确保存在全局最小值，并且使用梯度下降法 converge 到该最小值。因此，我们提供了网络宽度和训练样本数的条件，以确保 PL$^*$ 条件在这些网络中成立。我们通过计算这些网络的梯度spectral norm来实现这一点。此外，我们还证明了在网络宽度增加时，训练样本数的阈值也会增加。最后，我们比较了这些折叠网络和标准的完全连接Feed-forward network（FFNN）的训练样本数阈值，证明折叠网络的阈值高于 FFNN。因此，我们可以预期在折叠网络中获得更好的预期错误。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments"><a href="#Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments" class="headerlink" title="Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments"></a>Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06183">http://arxiv.org/abs/2309.06183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Gonzalez, Tommy Sonne Alstrøm, Tobias May</li>
<li>for: 这个研究旨在评估学习型声音提取系统的泛化能力，以及声音提取任务的难度如何影响系统的性能。</li>
<li>methods: 这个研究使用了一种新的泛化评估框架，通过使用测试条件下的参考模型，以评估系统在不同的数据库中的泛化性能。</li>
<li>results: 研究发现，对于所有模型，声音匹配度下的性能下降最多，而好声音和房间泛化性能可以通过训练多个数据库来实现。此外，最新的模型在匹配条件下表现出色，但在匹配不符条件下表现差，可能比FFNN-based系统更差。<details>
<summary>Abstract</summary>
The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test condition. This allows to disentangle the effect of the change in task difficulty from the effect of dealing with new data, and thus to define a new measure of generalization performance termed the generalization gap. The procedure is repeated in a cross-validation fashion by cycling through multiple speech, noise, and BRIR databases to accurately estimate the generalization gap. The proposed framework is applied to evaluate the generalization potential of a feedforward neural network (FFNN), Conv-TasNet, DCCRN and MANNER. We find that for all models, the performance degrades the most in speech mismatches, while good noise and room generalization can be achieved by training on multiple databases. Moreover, while recent models show higher performance in matched conditions, their performance substantially decreases in mismatched conditions and can become inferior to that of the FFNN-based system.
</details>
<details>
<summary>摘要</summary>
干扰性的语音混合物的音频特征是多种因素的影响，如目标说话人的spectro-temporal特征、干扰噪音和房间特性。这种大量的变化对学习基于语音增强系统的性能造成了主要的挑战，因为训练和测试条件之间的匹配可能会导致系统性能下降。通常，系统的普适性是通过在训练和测试集之间进行交互测试，并评估系统在新的语音、噪音和双耳响应函数（BRIR）数据库中的性能。但是，任务难度可能会在不同的数据库中发生变化，这会对结果产生重要的影响。本研究提出了一种普适性评估框架，该框架使用训练在测试条件下的参考模型，以便用其作为测试条件的difficulty水平的代理。这允许分解出与数据库变化的影响和与新数据处理的影响，并定义一个新的普适性度量——普适差。这种方法在批处理方式下重复进行，通过循环多个语音、噪音和BRIR数据库来准确估计普适差。我们在这些模型中应用这种框架，并发现：1）所有模型在语音匹配度下的性能最差；2）在多个数据库中训练可以实现好的噪音和房间普适性。此外，最新的模型在匹配条件下的性能较高，但在匹配不符条件下性能显著下降，可能变得落后于基于FFNN的系统。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>Efficient Memory Management for Large Language Model Serving with PagedAttention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06180">http://arxiv.org/abs/2309.06180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
<li>paper_authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica</li>
<li>for: 提高大语言模型（LLM）的高throughput服务，需要批处理足够多的请求 simultaneously。但现有系统受到缓存（KV cache）的内存占用限制，导致批处理量有限。</li>
<li>methods: 我们提出了PagedAttention算法， inspirited by classical virtual memory和paging技术。此外，我们还构建了vLLM服务系统，可以实现近于零的缓存内存浪费和请求间共享缓存。</li>
<li>results: 我们的evaluaion表明，vLLM可以提高具有同等响应时间的LLM服务系统的吞吐量，比如FasterTransformer和Orca的2-4倍。这种改进更加明显地出现在 longer sequences、更大的模型、以及更复杂的解码算法中。vLLM的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/vllm-project/vllm上下载。</a><details>
<summary>Abstract</summary>
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm
</details>
<details>
<summary>摘要</summary>
高速服务大语言模型（LLM）需要批处理足够多的请求。然而，现有系统受到缓存（KV cache）内存的巨大增长和减少的限制，导致批处理大小受限。当缓存管理不充分时，这些内存可能会受到重叠和重复占用，从而限制批处理大小。为解决这个问题，我们提出了 PagedAttention，一种基于经典虚拟内存和分页技术的注意机制。在其基础之上，我们构建了 vLLM，一个能够实现（1）缓存内存几乎为零浪费和（2）请求之间和请求内 flexible分享缓存的 LLM 服务系统。我们的评估表明，vLLM 可以在同等延迟下提高流行的 LLM 的 Throughput 2-4 倍，比如 FasterTransformer 和 Orca。这种改进更加明显地出现在 longer sequences、更大的模型、更复杂的解码算法上。vLLM 的源代码可以在 https://github.com/vllm-project/vLLM 上获取。
</details></li>
</ul>
<hr>
<h2 id="Elucidating-the-solution-space-of-extended-reverse-time-SDE-for-diffusion-models"><a href="#Elucidating-the-solution-space-of-extended-reverse-time-SDE-for-diffusion-models" class="headerlink" title="Elucidating the solution space of extended reverse-time SDE for diffusion models"></a>Elucidating the solution space of extended reverse-time SDE for diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06169">http://arxiv.org/abs/2309.06169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinpeng Cui, Xinyi Zhang, Zongqing Lu, Qingmin Liao<br>for:This paper focuses on improving the sampling efficiency of diffusion models (DMs) for image generation tasks.methods:The authors formulate the sampling process as an extended reverse-time stochastic differential equation (ER SDE) and leverage the semi-linear structure of the solutions to offer exact and approximate solutions.results:The authors achieve high-quality image generation with fast sampling efficiency, achieving 3.45 FID in 20 function evaluations and 2.24 FID in 50 function evaluations on the ImageNet 64x64 dataset. Additionally, they demonstrate that their proposed ER-SDE solvers are training-free and can elevate the efficiency of stochastic samplers to unprecedented levels.<details>
<summary>Abstract</summary>
Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE counterparts. Finally, we devise fast and training-free samplers, ER-SDE Solvers, elevating the efficiency of stochastic samplers to unprecedented levels. Experimental results demonstrate achieving 3.45 FID in 20 function evaluations and 2.24 FID in 50 function evaluations on the ImageNet 64$\times$64 dataset.
</details>
<details>
<summary>摘要</summary>
Diffusion models (DMs) 展示了强大的图像生成能力在不同的生成模型任务中。然而，它们的主要局限性在于慢的采样速度，需要数百或千个顺序的函数评估过大的神经网络来生成高质量图像。采样从 DMs 可以看作解决对应的随机 differential equations (SDEs) 或 ordinary differential equations (ODEs)。在这项工作中，我们将采样过程转化为延长的反时间 SDE (ER SDE)，统一先前的探索。利用 ER SDE 解的半线性结构，我们提供了精确解和高阶估计解 для VP SDE 和 VE SDE，分别。基于 ER SDE 的解空间，我们获得了数学意义，解释了 ODE 解算法在采样速度方面的优越性。此外，我们发现 VP SDE 解算法与 VE SDE 解算法相当。最后，我们设计了快速、无需训练的采样器，ER-SDE Solvers，使得杂样器的效率提升至历史最高水平。实验结果表明在 ImageNet 64x64 数据集上，我们可以在 20 个函数评估下获得 3.45 FID，并在 50 个函数评估下获得 2.24 FID。
</details></li>
</ul>
<hr>
<h2 id="Certified-Robust-Models-with-Slack-Control-and-Large-Lipschitz-Constants"><a href="#Certified-Robust-Models-with-Slack-Control-and-Large-Lipschitz-Constants" class="headerlink" title="Certified Robust Models with Slack Control and Large Lipschitz Constants"></a>Certified Robust Models with Slack Control and Large Lipschitz Constants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06166">http://arxiv.org/abs/2309.06166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlosch/cll">https://github.com/mlosch/cll</a></li>
<li>paper_authors: Max Losch, David Stutz, Bernt Schiele, Mario Fritz</li>
<li>for: 提高预测精度和证明 robustness</li>
<li>methods: 使用 Lipschitz-based regularizers 或 constraint，同时增加预测差</li>
<li>results: 提出 Calibrated Lipschitz-Margin Loss (CLL) 方法，解决了输入变化导致的攻击例子问题，并提高了证明可靠性，而不是降低精度。在 CIFAR-10、CIFAR-100 和 Tiny-ImageNet 上，我们的模型consistently outperform不考虑常量的损失函数。在 CIFAR-100 和 Tiny-ImageNet 上，CLL 超过了现有的 deterministic $L_2$ Robust Accuracy。在 contradiction 中，我们解锁了小型模型的潜在，不需要 $K&#x3D;1$ 约束。<details>
<summary>Abstract</summary>
Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establishing full control over slack and improving robustness certificates even with larger Lipschitz constants. On CIFAR-10, CIFAR-100 and Tiny-ImageNet, our models consistently outperform losses that leave the constant unattended. On CIFAR-100 and Tiny-ImageNet, CLL improves upon state-of-the-art deterministic $L_2$ robust accuracies. In contrast to current trends, we unlock potential of much smaller models without $K=1$ constraints.
</details>
<details>
<summary>摘要</summary>
儿童护理模型尚未具备证明的Robustness，尤其是面对输入变化（例如恶意示例）时。为了获得证明的Robustness， latest work 考虑了Lipschitz-based regularizers或约束，同时增加预测margin。然而，这会导致减少准确率。在这篇论文中，我们提出了准确证明的Lipschitz-Margin损失函数（CLL），解决了这些问题，并提高了证明Robustness。CLL通过explicitly calibrating loss w.r.t. margin和Lipschitz常量($K$)，以获得全面控制 над slack，并提高了证明Robustness certificates。我们的CLL在CIFAR-10、CIFAR-100和Tiny-ImageNet上的模型 consistently outperform不考虑$K$的损失函数。在CIFAR-100和Tiny-ImageNet上，CLL超越了当前的state-of-the-art deterministic $L_2$ Robust accuracy。而且，我们在许多小型模型中解锁了无需$K=1$约束的潜力。
</details></li>
</ul>
<hr>
<h2 id="Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines"><a href="#Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines" class="headerlink" title="Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines"></a>Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06157">http://arxiv.org/abs/2309.06157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tran, Hai-Canh Vu, Lam Pham, Nassim Boudaoud</li>
<li>for: 这个研究目的是为了预测旋转机器的剩下有用生命（Remaining Useful Life，RUL）和状况操作（Condition Operations，CO）。</li>
<li>methods: 本研究提出了一个具有多支分支深度学习架构的强健多支分支深度学习系统，用于预测旋转机器的RUL和CO。系统包括以下主要 ком成分：（1）一个LSTM自动encoder来降噪振声数据;（2）一个特征提取器来从降噪后的数据中生成时间频率域和时间频率域的特征;（3）一个新的和强健的多支分支深度学习网络架构来利用多个特征。</li>
<li>results: 本研究的实验结果显示，提出的系统在两个标准资料集XJTU-SY和PRONOSTIA上与现有的系统进行比较，表现出色，并且在实际应用中具有潜在的应用前景。<details>
<summary>Abstract</summary>
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于深度学习的多分支系统，用于预测旋转机件的剩余有用寿命（RUL）和conditions operation（CO）。特别是，该系统包括以下主要组成部分：1. LSTM自适应网络，用于减除振荡数据的噪声；2. 特征提取，用于从减除后的数据中提取时域、频域和时域频谱特征；3. 一种新的和可靠的多分支深度学习网络架构，用于利用多个特征。我们提posed系统的性能在两个benchmark数据集XJTU-SY和PRONOSTIA上进行了评估和比较，并经验表明，我们的提posed系统在RUL预测和CO认识方面表现出了优于现有系统的可靠性和实用性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Reliable-Domain-Generalization-A-New-Dataset-and-Evaluations"><a href="#Towards-Reliable-Domain-Generalization-A-New-Dataset-and-Evaluations" class="headerlink" title="Towards Reliable Domain Generalization: A New Dataset and Evaluations"></a>Towards Reliable Domain Generalization: A New Dataset and Evaluations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06142">http://arxiv.org/abs/2309.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiao Zhang, Xu-Yao Zhang, Cheng-Lin Liu</li>
<li>for: 这个论文的目的是提出一个新的领域泛化任务，用于提高针对中文字符识别的领域泛化方法的研究。</li>
<li>methods: 该论文使用了 eighteen 种领域泛化方法在 PaHCC（打印和手写中文字符）数据集上进行评估。</li>
<li>results: 研究发现，现有的领域泛化方法在该数据集上表现并不满意，而且在一种设计的动态领域泛化设定下，发现了许多领域泛化方法的缺陷。<details>
<summary>Abstract</summary>
There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the existing research, we propose a new domain generalization task for handwritten Chinese character recognition (HCCR) to enrich the application scenarios of DG method research. We evaluate eighteen DG methods on the proposed PaHCC (Printed and Handwritten Chinese Characters) dataset and show that the performance of existing methods on this dataset is still unsatisfactory. Besides, under a designed dynamic DG setting, we reveal more properties of DG methods and argue that only the leave-one-domain-out protocol is unreliable. We advocate that researchers in the DG community refer to dynamic performance of methods for more comprehensive and reliable evaluation. Our dataset and evaluations bring new perspectives to the community for more substantial progress. We will make our dataset public with the article published to facilitate the study of domain generalization.
</details>
<details>
<summary>摘要</summary>
有很多不同的分布Shift在实际世界中存在。然而，深度神经网络（DNNs）容易偏向训练集，这会导致它们接收到不同分布数据时表现出严重的性能下降。许多方法在域间泛化（DG） литературе中被研究，但是最近的DomainBed和WILDS bencmark挑战了这些方法的有效性。针对现有研究中的问题，我们提出了一个新的域间泛化任务 для手写中文字识别（HCCR），以扩展DG方法研究的应用场景。我们在提出的PaHCC（印刷和手写中文字） dataset上评估了 eighteen DG 方法的性能，并发现现有方法在这个 dataset 上的性能仍然不满足。此外，在我们设计的动态DG设定下，我们揭示了更多的DG方法的性能特性，并 argue that只有离开一个频道的协议是不可靠的。我们建议研究人员在DG社区中参考动态方法的性能进行更加全面和可靠的评估。我们的 dataset 和评估将为社区带来新的视角，以便更加快速的进步。我们将在文章发表时将 dataset 公开，以便研究域间泛化。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs"><a href="#Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs" class="headerlink" title="Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs"></a>Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06127">http://arxiv.org/abs/2309.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhananjaya Wijerathne, Zhaoying Li, Tulika Mitra</li>
<li>for: Edge AI applications</li>
<li>methods: Architecture-adaptive Coarse-Grained Reconfigurable Arrays (CGRAs) and a comprehensive ecosystem of tools including a tailored compiler, simulator, accelerator synthesis, and validation framework.</li>
<li>results: Automatic compilation and verification of AI application kernels onto user-defined CGRA architectures, facilitating efficient edge AI applications for a wide range of embedded AI workloads.Here is the text in Simplified Chinese:</li>
<li>for: 边缘AI应用</li>
<li>methods: 基于CGRA架构的自适应 architecture-adaptive Coarse-Grained Reconfigurable Arrays (CGRAs) 和一个完整的开发环境，包括一个特定的编译器、仿真器、加速器合成和验证框架。</li>
<li>results: 通过自动将AI应用核心编译到用户定义的CGRA架构上，实现了高效的边缘AI应用，覆盖了广泛的嵌入式AI工作负荷。<details>
<summary>Abstract</summary>
Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as power-efficient edge accelerator, offering versatility beyond AI applications. Morpher, an open-source, architecture-adaptive CGRA design framework, is specifically designed to explore the vast design space of CGRAs. The comprehensive ecosystem of Morpher includes a tailored compiler, simulator, accelerator synthesis, and validation framework. This study provides an overview of Morpher, highlighting its capabilities in automatically compiling AI application kernels onto user-defined CGRA architectures and verifying their functionality. Through the Morpher framework, the versatility of CGRAs is harnessed to facilitate efficient compilation and verification of edge AI applications, covering important kernels representative of a wide range of embedded AI workloads. Morpher is available online at https://github.com/ecolab-nus/morpher-v2.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。Coarse-Grained Reconfigurable Arrays (CGRAs) 提供了高效的Edge加速器， offering versatility beyond AI应用程序。 Morpher，一个开源的、architecture-adaptive CGRA设计框架，是专门为探索CGRA的庞大设计空间而设计的。 Morpher的完整生态系统包括特制的编译器、模拟器、加速器合成和验证框架。本研究提供了Morpher的概述， highlighting its capabilities in automatically compiling AI应用程序核心 onto user-defined CGRA架构并验证其功能。通过Morpher框架，CGRA的灵活性被利用，以便高效地编译和验证边缘AI应用程序，覆盖重要的核心代表许多嵌入式AI工作负荷。 Morpher可以在https://github.com/ecolab-nus/morpher-v2中下载。>>>
</details></li>
</ul>
<hr>
<h2 id="AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy"><a href="#AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy" class="headerlink" title="AstroLLaMA: Towards Specialized Foundation Models in Astronomy"></a>AstroLLaMA: Towards Specialized Foundation Models in Astronomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06126">http://arxiv.org/abs/2309.06126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciucă, Charlie O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz Różański, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodríguez Méndez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney, Kevin Schawinski, UniverseTBD</li>
<li>for:  bridging the gap between large language models and highly specialized domains like scholarly astronomy</li>
<li>methods:  fine-tuning a 7-billion-parameter model from LLaMA-2 using over 300,000 astronomy abstracts from arXiv, optimized for traditional causal language modeling</li>
<li>results:  achieving a 30% lower perplexity than Llama-2, generating more insightful and scientifically relevant text completions and embedding extraction than state-of-the-art foundation models despite having significantly fewer parameters<details>
<summary>Abstract</summary>
Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多人类语言任务中表现出色，但在高度特殊化的学术天文领域中经常表现不佳。为了bridging这个差距，我们引入AstroLLaMA，一个基于LLaMA-2的70亿个parameters的模型，通过arXiv上的300,000篇天文摘要进行了微调。我们的模型优化了传统的 causal language modeling，并在天文领域中表现出30%的负据况下降，显示了明显的领域适应。我们的模型在对天文领域的文本完成和嵌入EXTRACTING方面表现出更多的科学和技术相关的内容，即使有较少的参数。AstroLLaMA作为一个专业的天文模型，具有广泛的微调潜力。我们将其公开发布，以促进天文研究，包括自动摘要和对话代理开发。
</details></li>
</ul>
<hr>
<h2 id="A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM"><a href="#A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM" class="headerlink" title="A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)"></a>A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06122">http://arxiv.org/abs/2309.06122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Rangel DaCosta, Katherine Sytwu, Catherine Groschner, Mary Scott</li>
<li>for: 本研究旨在开发高精度自动分析工具，用于Characterization of nanomaterials，包括高分辨率电子镜 transmision electron microscopy (HRTEM)。</li>
<li>methods: 本研究使用Python package Construction Zone， quickly generate complex nanoscale atomic structures，并开发了一个端到端的工作流程，用于创建大量的模拟数据库。</li>
<li>results: 通过对不同subset的模拟数据进行训练，我们实现了对实验图像中的粒子 segmentation的最佳性能，并研究了数据准备过程中不同因素的影响，包括模拟精度、结构分布和捕捉条件分布。<details>
<summary>Abstract</summary>
Machine learning techniques are attractive options for developing highly-accurate automated analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapidly generating complex nanoscale atomic structures, and develop an end-to-end workflow for creating large simulated databases for training neural networks. Construction Zone enables fast, systematic sampling of realistic nanomaterial structures, and can be used as a random structure generator for simulated databases, which is important for generating large, diverse synthetic datasets. Using HRTEM imaging as an example, we train a series of neural networks on various subsets of our simulated databases to segment nanoparticles and holistically study the data curation process to understand how various aspects of the curated simulated data -- including simulation fidelity, the distribution of atomic structures, and the distribution of imaging conditions -- affect model performance across several experimental benchmarks. Using our results, we are able to achieve state-of-the-art segmentation performance on experimental HRTEM images of nanoparticles from several experimental benchmarks and, further, we discuss robust strategies for consistently achieving high performance with machine learning in experimental settings using purely synthetic data.
</details>
<details>
<summary>摘要</summary>
machine learning技术是开发高精度自动分析工具的优选方案，包括高分辨率电子显微镜（HRTEM）。然而，实现这些机器学习工具可能困难，因为实验获得大量、高质量训练数据的挑战。在这种情况下，我们介绍了“Construction Zone”Python包，用于快速生成复杂的nanoscale原子结构，并开发了终端工作流程，用于创建大规模的模拟数据库。Construction Zone可以快速、系统地采样真实的nanomaterial结构，并可以用作模拟数据库的随机结构生成器，这对于生成大量、多样化的 sintetic数据集是非常重要。使用HRTEM成像为例，我们在不同的模拟数据库中训练了一系列神经网络，以分类nanoparticles并全面研究数据准备过程，以了解不同的模拟数据的准确性、原子结构分布和成像条件分布如何影响模型性能。使用我们的结果，我们可以在多个实验室中实现状态机器学习性能的最佳分 segmentationHRTEM图像，并讨论了一些可靠的机器学习术语在实验设置中使用纯 sintetic数据时实现高性能的策略。
</details></li>
</ul>
<hr>
<h2 id="Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning"><a href="#Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning" class="headerlink" title="Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning"></a>Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06097">http://arxiv.org/abs/2309.06097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Liu, Wubing Chen, Mao Tan</li>
<li>for: 解释DRLagent的决策过程，提高用户对DRLagent的信任和强点的了解。</li>
<li>methods: 基于Interpretable Policy Extraction（IPE）方法，通过纳入一种新的准确度测量机制，以提高DRLagent的决策过程的可读性和一致性。</li>
<li>results: 在StarCraft II Complex Control Environment中进行实验，FIPE方法在交互性和一致性两个方面都超过基eline，同时易于理解。<details>
<summary>Abstract</summary>
Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE methods. The experiment results demonstrate that FIPE outperforms the baselines in terms of interaction performance and consistency, meanwhile easy to understand.
</details>
<details>
<summary>摘要</summary>
我们开始是分析现有的 IPE 方法的优化机制，探讨了忽略一致性而提高总奖励的问题。然后，我们设计了一种准确度引导机制，通过将准确度measurementintegrated into the reinforcement learning feedback。我们在StarCraft II 中进行了实验，这是现有 IPE 方法通常避免的复杂控制环境。实验结果表明，FIPE 在交互性性和一致性方面都超过了基eline，而且易于理解。
</details></li>
</ul>
<hr>
<h2 id="A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis"><a href="#A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis" class="headerlink" title="A General Verification Framework for Dynamical and Control Models via Certificate Synthesis"></a>A General Verification Framework for Dynamical and Control Models via Certificate Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06090">http://arxiv.org/abs/2309.06090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Andrea Peruffo, Alessandro Abate</li>
<li>for: 本研究旨在提供一个通用框架，用于编码系统特性和定义相应的证书，以及自动生成控制器和证书。</li>
<li>methods: 我们使用神经网络提供候选控制函数和证书函数，并使用SMT解决器确保正确性。</li>
<li>results: 我们对一个大量和多样化的 benchmark 进行了验证，并证明了我们的框架能够成功地验证控制和证书。<details>
<summary>Abstract</summary>
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of correctness. We test our framework by developing a prototype software tool, and assess its efficacy at verification via control and certificate synthesis over a large and varied suite of benchmarks.
</details>
<details>
<summary>摘要</summary>
一种新兴的控制理论分支是证书学习，关注某个自主或控制模型的行为特性的规范，通过函数基本证明来分析。然而，将这些复杂的要求满足是一个非常困难的任务，可能会让最有经验的控制工程师感到惑乱。这导致了一种自动化的方法的需求，能够设计控制器并分析广泛的复杂规范。在这篇论文中，我们提供一个通用的框架来编码系统规范和对应的证书，并提出了一种自动化的控制器和证书Synthesize的方法。我们的方法在安全学习控制领域中发挥了灵活性的神经网络提供候选控制和证书函数，而使用SMT-解决方案提供正式的正确性保证。我们测试了我们的框架，开发了一个原型软件工具，并通过控制和证书验证 benchmarks 进行了效果的评估。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies"><a href="#Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies" class="headerlink" title="Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies"></a>Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06089">http://arxiv.org/abs/2309.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boshko Koloski, Blaž Škrlj, Marko Robnik-Šikonja, Senja Pollak</li>
<li>for: 这个研究旨在比较两种精致化方法，以及零扩展和全扩展学习方法，以测试大语言模型在跨语言设定下的性能。</li>
<li>methods: 研究使用了两种精致化方法：一是参数有效的适配器方法，另一是精致化所有参数。另外，研究还使用了两种跨语言转移策略：一是中途训练（IT），另一是跨语言验证（CLV）。</li>
<li>results: 研究结果显示，在两个不同的分类问题（ hate speech detection 和产品评价）中，IT 跨语言策略比 CLV 策略在目标语言中表现出色。此外，研究还发现，在多个跨语言转移中，CLV 策略在基础语言（英文）中具有较好的知识储存性，而 IT 策略则在跨语言转移中导致更多的知识损失。<details>
<summary>Abstract</summary>
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several languages, show that the \textit{IT} cross-lingual strategy outperforms \textit{CLV} for the target language. Our findings indicate that, in the majority of cases, the \textit{CLV} strategy demonstrates superior retention of knowledge in the base language (English) compared to the \textit{IT} strategy, when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</details>
<details>
<summary>摘要</summary>
cross-lingual transfer是一种有前途的技术，用于解决少 ressourced languages中的任务。在这个实验性研究中，我们比较了两种 fine-tuning approaches 在 cross-lingual Setting 中的性能。为 fine-tuning 策略，我们比较了参数效率的 adapter 方法和所有参数的 fine-tuning。为 cross-lingual transfer 策略，我们比较了中间训练 (\textit{IT}) 和 across-lingual validation (\textit{CLV})。我们评估了交互语言转移的成功和原语言中的知识损失，即在学习新语言时， previously acquired 的知识是否会产生恶性忘记。我们在 hate speech detection 和 product reviews 两个不同的分类任务中，每个任务包含多种语言的数据集，得到的结果表明，\textit{IT} cross-lingual 策略在目标语言上表现出色，而 \textit{CLV} 策略在基语言（英语）中的知识保留性比 \textit{IT} 策略更高。
</details></li>
</ul>
<hr>
<h2 id="Plasticity-Optimized-Complementary-Networks-for-Unsupervised-Continual-Learning"><a href="#Plasticity-Optimized-Complementary-Networks-for-Unsupervised-Continual-Learning" class="headerlink" title="Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning"></a>Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06086">http://arxiv.org/abs/2309.06086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alviur/pocon_wacv2024">https://github.com/alviur/pocon_wacv2024</a></li>
<li>paper_authors: Alex Gomez-Villa, Bartlomiej Twardowski, Kai Wang, Joost van de Weijer</li>
<li>for: 本研究旨在提高无监督学习（SSL）技术的 continuous unsupervised representation learning（CURL）方法，以提高无监督学习方法在多任务数据流中的表现。</li>
<li>methods: 我们提出了一种训练专家网络，以释放 previous 知识的责任，并且可以完全适应新任务（优化流动性）。在第二阶段，我们将新的知识与之前的网络相结合，以避免忘记和初始化一个新的专家网络（适应-反思阶段）。</li>
<li>results: 我们的方法在 few-task 和 many-task 分配 Setting 中比其他 CURL 无 exemplar 方法表现出色，并且在 semi-supervised continual learning（Semi-SCL） Setting 中，我们的方法超过其他 exemplar-free 方法的准确率，并达到其他使用 exemplars 的方法的结果。<details>
<summary>Abstract</summary>
Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提议 trains an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars.
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events"><a href="#A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events" class="headerlink" title="A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events"></a>A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06082">http://arxiv.org/abs/2309.06082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Jain, Xueqing Sun, Sohom Datta, Abhishek Somani</li>
<li>For: The paper aims to analyze the primary drivers behind price spike events in modern electricity markets with high renewable energy penetration, using machine learning techniques.* Methods: The authors propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets.* Results: The framework is applied to open-source publicly available datasets from California Independent System Operator (CAISO) and ISO New England (ISO-NE) to identify the main drivers behind price spike events. The results can be used for various critical aspects of market design, renewable dispatch and curtailment, operations, and cyber-security applications.<details>
<summary>Abstract</summary>
Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch and curtailment, operations, and cyber-security applications. The framework can be applied to any ISO or market data; however, in this paper, it is applied to open-source publicly available datasets from California Independent System Operator (CAISO) and ISO New England (ISO-NE).
</details>
<details>
<summary>摘要</summary>
电力网络正在往100%可再生能源源扩大，电力市场的总趋势和电力系统运行方式都在变化。电力市场不仅经济地分配资源，还考虑了各种可控行为，如可再生能源削减、电网压缩缓冲和能量存储优化，以确保网络可靠性。因此，电力市场的价格形成变得非常复杂。传统的根本原因分析和统计方法在现代网络和市场中变得无法应用，用于分析和推导现代电力市场中价格形成的主要驱动力。本文提出了一种基于机器学习的分析框架，用于分解现代电力市场中价格峰值事件的主要驱动力。该框架可以应用于任何ISO或市场数据，但在这篇论文中，它被应用于开源的公共数据集中。
</details></li>
</ul>
<hr>
<h2 id="Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case"><a href="#Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case" class="headerlink" title="Information Flow in Graph Neural Networks: A Clinical Triage Use Case"></a>Information Flow in Graph Neural Networks: A Clinical Triage Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06081">http://arxiv.org/abs/2309.06081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Valls, Mykhaylo Zayats, Alessandra Pascale</li>
<li>for: 本文研究了图神经网络（GNNs）在医疗领域和其他领域中的应用，以及如何高效地训练GNNs。</li>
<li>methods: 本文提出了一个数学模型，该模型将GNN连接与图数据连接解耦开，以评估GNNs在临床排序Use Case中的性能。</li>
<li>results: 结果表明，在 integrate 域知识到GNN连接中时，GNNs的性能更好than使用同一个KG连接或允许不制embedding传播。此外，我们发现负边Edge play a crucial role in achieving good predictions, and using too many GNN layers can degrade performance.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained popularity in healthcare and other domains due to their ability to process multi-modal and multi-relational graphs. However, efficient training of GNNs remains challenging, with several open research questions. In this paper, we investigate how the flow of embedding information within GNNs affects the prediction of links in Knowledge Graphs (KGs). Specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case. Our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
</details>
<details>
<summary>摘要</summary>
граф нейронные сети (GNNs) 在医疗领域和其他领域中获得了广泛的应用，这是因为它们可以处理多Modal和多关系图。然而，fficiently 训练 GNNs 仍然是一个开放的研究问题，有多个未解决的问题。在这篇论文中，我们 investigate 如何在 GNNs 中流动嵌入信息的方式affects the prediction of links in Knowledge Graphs (KGs).specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case.our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation.moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.Here's the translation in Traditional Chinese:граф нейрон网络 (GNNs) 在医疗领域和其他领域中获得了广泛的应用，这是因为它们可以处理多Modal 和多关系图。然而，fficiently 训练 GNNs 仍然是一个开放的研究问题，有多个未解决的问题。在这篇论文中，我们 investigate 如何在 GNNs 中流动嵌入信息的方式affects the prediction of links in Knowledge Graphs (KGs).specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case.our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation.moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
</details></li>
</ul>
<hr>
<h2 id="A2V-A-Semi-Supervised-Domain-Adaptation-Framework-for-Brain-Vessel-Segmentation-via-Two-Phase-Training-Angiography-to-Venography-Translation"><a href="#A2V-A-Semi-Supervised-Domain-Adaptation-Framework-for-Brain-Vessel-Segmentation-via-Two-Phase-Training-Angiography-to-Venography-Translation" class="headerlink" title="A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation"></a>A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06075">http://arxiv.org/abs/2309.06075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga</li>
<li>for: 这个 paper 的目的是提出一种 semi-supervised domain adaptation 框架，用于脑血管分割不同像模式下。</li>
<li>methods: 该方法 基于 annotated angiographies 和有限数量的 annotated venographies，实现图像到图像的翻译和semantic segmentation，并且利用分离的和semantically rich的幂 space 来表示多样化数据，并实现图像级别的适应。</li>
<li>results: 对 magnetic resonance angiographies 和 venographies 进行评估，该方法 在源领域中实现了状态空间的表现，而在目标领域中的 Dice 分数减少了8.9%，这说明了该方法 在不同频谱下的脑血管图像分割中具有可靠的潜在性。<details>
<summary>Abstract</summary>
We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art performance in the source domain, our method attains a Dice score coefficient in the target domain that is only 8.9% lower, highlighting its promising potential for robust cerebrovascular image segmentation across different modalities.
</details>
<details>
<summary>摘要</summary>
我们提出了一种半监督频道适应框架，用于不同频谱图像的脑血管分割。现有的状态艺术方法都将注意力集中在单一频谱上，忽略了脑血管成像技术的广泛应用。这可能会导致重要的分布偏移，从而影响到不同频谱之间的泛化性。我们的框架利用注释的材料和有限的注释掌肘图像，实现图像到图像的翻译和semantic分割，通过分离和semanticallyRich的幂谱空间来表示不同数据类型，并在目标频谱上进行图像级适应。此外，我们减少了循环结构的TypicalComplexity和减少了对抗训练，这使得我们可以建立高效和直观的模型，并保证了固定的训练。我们在核磁共振成像和掌肘成像上评估了我们的方法，而在源频谱上达到了状态艺术性的性能，而在目标频谱上的Dice分数系数只下降了8.9%，这表明了我们的方法在不同频谱之间的精准脑血管图像分割的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Selection-of-contributing-factors-for-predicting-landslide-susceptibility-using-machine-learning-and-deep-learning-models"><a href="#Selection-of-contributing-factors-for-predicting-landslide-susceptibility-using-machine-learning-and-deep-learning-models" class="headerlink" title="Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models"></a>Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06062">http://arxiv.org/abs/2309.06062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Chen, Lei Fan</li>
<li>for: 这个论文的目的是研究降坡可能性评估中因素选择对机器学习和深度学习模型的预测精度的影响。</li>
<li>methods: 这个论文使用了四种因素选择方法，包括信息增益比率（IGR）、回归特征选择（RFE）、血统群Optimization（PSO）和最小绝对减少和选择操作（LASSO），以及哈里斯鸥鹰优化（HHO）。此外，这个论文还研究了基于自适应器的因素选择方法 для深度学习模型。</li>
<li>results: 这个论文的研究结果表明，不同的因素选择方法对机器学习和深度学习模型的预测精度有所不同。 certain methods can improve the prediction accuracy of the models, while others may not have a significant impact.<details>
<summary>Abstract</summary>
Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO) and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...
</details>
<details>
<summary>摘要</summary>
Landslides are a common natural disaster that can cause casualties, property safety threats, and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO), and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...Here's the translation in Traditional Chinese:Landslides are a common natural disaster that can cause casualties, property safety threats, and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more important factors is still a challenging task and there is no generally accepted method. Furthermore, the effects of factor selection using various methods on the prediction accuracy of ML and DL models are unclear. In this study, the impact of the selection of contributing factors on the accuracy of landslide susceptibility predictions using ML and DL models was investigated. Four methods for selecting contributing factors were considered for all the aforementioned ML and DL models, which included Information Gain Ratio (IGR), Recursive Feature Elimination (RFE), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operators (LASSO), and Harris Hawk Optimization (HHO). In addition, autoencoder-based factor selection methods for DL models were also investigated. To assess their performances, an exhaustive approach was adopted,...
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems"><a href="#Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems" class="headerlink" title="Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems"></a>Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06061">http://arxiv.org/abs/2309.06061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Toreini, Maryam Mehrnezhad, Aad van Moorsel</li>
<li>for: The paper aims to provide a secure, verifiable, and privacy-preserving protocol for computing and verifying the fairness of any machine learning (ML) model.</li>
<li>methods: The proposed method, called Fairness as a Service (FaaS), uses cryptograms to represent data and outcomes, and zero-knowledge proofs to guarantee the well-formedness of the cryptograms and underlying data. The method is model-agnostic and can support various fairness metrics, making it a versatile tool for auditing the fairness of any ML model.</li>
<li>results: The paper demonstrates the effectiveness of FaaS using a publicly available data set with thousands of entries, and shows that the method can provide secure and verifiable fairness metrics without the need for trusted third parties or private channels.<details>
<summary>Abstract</summary>
Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.
</details>
<details>
<summary>摘要</summary>
《公平机器学习》是一个日益繁荣的研究领域。在这篇论文中，我们提出了一种名为《公平服务（FaaS）》的安全、可靠和隐私保持的协议，用于计算和验证任何机器学习（ML）模型的公平性。在FaaS的设计中，数据和结果都是通过加密来保护隐私。此外，零知识证明 garantizesthe well-formedness of the cryptograms and underlying data。FaaS是模型无关的，可以支持多种公平度量表，因此可以作为对任何ML模型的公平性进行审核的服务。我们的解决方案不需要任何不信任第三方或私人通道来计算公平度量。安全保证和承诺是通过一系列安全、透明和可靠的步骤来实现的，从计算开始到结束。所有输入数据的加密文本都是公开可见的，例如对审计人、社会活动人员和专家来说，以便验证过程的正确性。我们实现了FaaS，以评估性能和示case用FaaS对公共数据集进行成功使用。
</details></li>
</ul>
<hr>
<h2 id="How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task"><a href="#How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task" class="headerlink" title="How does representation impact in-context learning: A exploration on a synthetic task"></a>How does representation impact in-context learning: A exploration on a synthetic task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06054">http://arxiv.org/abs/2309.06054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwen Fu, Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng<br>for: 这种研究探讨了 transformer 模型在受限样本下进行学习的能力，即受限上下文学习能力。methods: 该研究使用了一种新的创新任务，并设计了两种探测器来评估两种不同的组成部分，即受限模型重量和上下文样本的影响。results: 研究发现，受限上下文组分对受限学习性能具有极高相关性，表明受限学习和表示学习之间存在束缚关系。此外，研究还发现一个好的受限模型重量可以有助于上下文组分的学习， indicating that 受限模型重量应该是受限学习的基础。<details>
<summary>Abstract</summary>
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation learning. Furthermore, we find that a good in-weights component can actually benefit the learning of the in-context component, indicating that in-weights learning should be the foundation of in-context learning. To further understand the the in-context learning mechanism and importance of the in-weights component, we proof by construction that a simple Transformer, which uses pattern matching and copy-past mechanism to perform in-context learning, can match the in-context learning performance with more complex, best tuned Transformer under the perfect in-weights component assumption. In short, those discoveries from representation learning perspective shed light on new approaches to improve the in-context capacity.
</details>
<details>
<summary>摘要</summary>
Contextual learning，即从Contextual samples中学习，是Transformer卓越的能力。然而，这种学习机制的具体原理尚未完全理解。在这项研究中，我们从表示学习的新角度来调查。表示更复杂的场景是Contextual learning scenario，其表示可以受到模型参数和Contextual samples的影响。我们将这两个概念性方面的表示称为内部预测（in-weight component）和Contextual component，分别。为了研究这两个组件如何影响Contextual learning能力，我们构建了一个新的人工任务，可以设计两个探针，即内部预测探针和Contextual探针，来评估这两个组件。我们发现，Contextual component的质量与Contextual learning性能高度相关，这表明Contextual learning和表示学习之间存在紧密的关系。此外，我们发现一个好的内部预测组件可以有助于Contextual learning组件的学习，这表明内部预测学习应该是Contextual learning的基础。为了更深入理解Contextual learning机制和内部预测组件的重要性，我们证明了一个简单的Transformer模型，通过使用模式匹配和复制机制来实现Contextual learning，可以与最佳调参的Transformer模型匹配Contextual learning性能，假设内部预测组件是完美的。总之，这些从表示学习角度出发的发现可能为Contextual learning提供新的改进方法。
</details></li>
</ul>
<hr>
<h2 id="A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation"><a href="#A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation" class="headerlink" title="A Perceptron-based Fine Approximation Technique for Linear Separation"></a>A Perceptron-based Fine Approximation Technique for Linear Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06049">http://arxiv.org/abs/2309.06049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ákos Hajnal</li>
<li>for: 本研究提出了一种新的在线学习方法，用于找到标注为正或负的数据点之间的分隔hyperplane。</li>
<li>methods: 该方法基于Perceptron算法，但是只在搜索分隔hyperplane时调整神经元的权重。</li>
<li>results: 实验结果表明，该方法可以更高效地训练基于机器学习的二分类器，特别是当数据集的大小超过数据维度时。<details>
<summary>Abstract</summary>
This paper presents a novel online learning method that aims at finding a separator hyperplane between data points labelled as either positive or negative. Since weights and biases of artificial neurons can directly be related to hyperplanes in high-dimensional spaces, the technique is applicable to train perceptron-based binary classifiers in machine learning. In case of large or imbalanced data sets, use of analytical or gradient-based solutions can become prohibitive and impractical, where heuristics and approximation techniques are still applicable. The proposed method is based on the Perceptron algorithm, however, it tunes neuron weights in just the necessary extent during searching the separator hyperplane. Due to an appropriate transformation of the initial data set we need not to consider data labels, neither the bias term. respectively, reducing separability to a one-class classification problem. The presented method has proven converge; empirical results show that it can be more efficient than the Perceptron algorithm, especially, when the size of the data set exceeds data dimensionality.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的在线学习方法，旨在找到标注为正或负的数据点之间的分隔hyperplane。由于人工神经元的权重和偏置可以直接关联到高维空间中的hyperplane，因此该技术可以在机器学习中训练基于perceptron的二分类器。在面临大或不均匀数据集时，使用分析或梯度基于的解决方案可能变得不可能和不实际，而恰当的决策和近似技术仍然可以应用。提出的方法基于Perceptron算法，但它在搜索分隔hyperplane时只需要调整神经元权重的必要范围。由于数据集的初始变换，我们不需要考虑数据标签，也不需要考虑偏置项，因此将分类问题降低到一个一类问题。表示的方法已经证明是可靠的，实验结果表明，它在数据集大于数维度时比Perceptron算法更高效。
</details></li>
</ul>
<hr>
<h2 id="BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise"><a href="#BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise" class="headerlink" title="BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise"></a>BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06046">http://arxiv.org/abs/2309.06046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeroen M. Galjaard, Robert Birke, Juan Perez, Lydia Y. Chen</li>
<li>for: This paper studies the impact of label noise on the performance of state-of-the-art meta-learners and proposes two sampling techniques to mitigate the impact of label noise.</li>
<li>methods: The paper uses gradient-based $N$-way $K$-shot learners and proposes two sampling techniques, namely manifold (Man) and batch manifold (BatMan), to transform noisy supervised learners into semi-supervised learners.</li>
<li>results: The paper shows that the proposed sampling techniques can effectively mitigate the impact of meta-training label noise, limiting the meta-testing accuracy drop to 2.5%, 9.4%, and 1.1% respectively, with existing meta-learners across the Omniglot, CifarFS, and MiniImagenet datasets, even with 60% wrong labels.<details>
<summary>Abstract</summary>
The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastive-shot tasks through augmentation, learning the embedding via a contrastive loss in meta-training, and then perform classification through zeroing on the embedding in meta-testing. We show that our approach can effectively mitigate the impact of meta-training label noise. Even with 60% wrong labels \batman and \man can limit the meta-testing accuracy drop to ${2.5}$, ${9.4}$, ${1.1}$ percent points, respectively, with existing meta-learners across the Omniglot, CifarFS, and MiniImagenet datasets.
</details>
<details>
<summary>摘要</summary>
经典超级学习中 label noise 的负面影响已得到广泛研究，然而在多学习领域中仍然是一个开放的研究问题。多学习器 aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. 在这篇论文中，我们提供了首次对多学习器在 label noise 的影响进行了广泛分析，特别是 gradient-based $N$-way $K$-shot learners。我们发现在 Omniglot 和 CifarFS 数据集上，随着 label noise 的增加，Reptile、iMAML 和 foMAML 的精度下降了最多 42%。为强化对 label noise 的抵抗力，我们提议了两种抽样技术，namely manifold (Man) 和 batch manifold (BatMan)，这些技术可以将杂 Label 转化为 semi-supervised learners，以提高杂 Label 的使用价值。我们首先通过扩展来构建 $N$-way $2$-contrastive-shot 任务的 manifold 样本，然后通过 zeroing 来进行分类，并在 meta-testing 中使用嵌入来进行识别。我们表明，我们的方法可以有效地减轻 meta-training label noise 的影响。甚至在 60% 的杂 Label 下，我们的 \batman 和 \man 可以限制 meta-testing 精度下降到 ${2.5}$, ${9.4}$, ${1.1}$ 个百分点，分别在 Omniglot、CifarFS 和 MiniImagenet 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model"><a href="#Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model" class="headerlink" title="Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model"></a>Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06453">http://arxiv.org/abs/2309.06453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao</li>
<li>for: 本研究的目的是解释受过超级vised和无级vised培训的句子嵌入学习模型（CSE）在训练过程中的性能差距，以及如何缩小这个差距。</li>
<li>methods: 本研究使用了empirical experiments进行了比较，以 Investigate the behavior of supervised and unsupervised CSE during their respective training processes。 We also introduce a metric called Fitting Difficulty Increment (FDI) to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset。</li>
<li>results: 我们发现，supervised和无级vised CSE在训练过程中的行为有 significan differences，主要归结于训练数据集的适应难度差异。 Based on these insights, we propose a method to increase the fitting difficulty of the training dataset by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns。 This approach effectively narrows the performance gap between supervised and unsupervised CSE.<details>
<summary>Abstract</summary>
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We observe a significant difference in fitting difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment (FDI), to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset, and use the metric to answer the "What" question. Then, based on the insights gained from the "What" question, we tackle the "How" question by increasing the fitting difficulty of the training dataset. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE.
</details>
<details>
<summary>摘要</summary>
我们首先通过对supervised和unsupervised CSE的训练过程进行比较，从比较中获得了一些有趣的发现。我们发现，supervised和unsupervised CSE在训练过程中的不同行为是导致性能差距的主要原因。为了回答 "What" 问题，我们提出了一个度量，即 Fitting Difficulty Increment (FDI)，用于度量训练数据集和保留数据集之间的适应难度差异。通过度量的帮助，我们回答了 "What" 问题。然后，基于我们从 "What" 问题中获得的启示，我们通过增加训练数据集的适应难度来缩小性能差距。我们利用 Large Language Model (LLM) 的 In-Context Learning (ICL) 能力生成数据，以模拟复杂的句子模式。通过利用 LLM 生成的数据中的层次句子模式，我们有效地缩小了 supervised 和 unsupervised CSE 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning"><a href="#Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning" class="headerlink" title="Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning"></a>Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06034">http://arxiv.org/abs/2309.06034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixdjc/nlgad">https://github.com/felixdjc/nlgad</a></li>
<li>paper_authors: Jingcan Duan, Pei Zhang, Siwei Wang, Jingtao Hu, Hu Jin, Jiaxin Zhang, Haifang Zhou, Haifang Zhou</li>
<li>for: 本 paper 的目的是提出一种基于多尺度对比学习网络的图生差检测方法 (GAD)，以提高检测性能。</li>
<li>methods: 该方法首先使用多尺度对比网络初始化模型，然后采用有效的混合策略选择可靠的正常节点，最后使用只有正常节点的输入进行模型练习，以学习更加准确的正常模式，从而更好地发现异常节点。</li>
<li>results: 对六个图Dataset进行了广泛的实验，并证明了该方法的效iveness，与当前最佳方法相比，检测性能提高了5.89%。<details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</details>
<details>
<summary>摘要</summary>
GRAPH anomaly detection (GAD) 在机器学习和数据挖掘领域受到了越来越多的关注。最近的研究主要集中在如何更好地捕捉更多的信息，以提高节点嵌入的质量。despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples, which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</details></li>
</ul>
<hr>
<h2 id="Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA"><a href="#Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA" class="headerlink" title="Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA"></a>Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06033">http://arxiv.org/abs/2309.06033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Valente da Silva, Onel L. Alcaraz López, Richard Demo Souza</li>
<li>for: 这篇论文旨在解决 federated learning (FL) 中的 Edge 设备电源不足问题，提出了一种将能源收集 (EH) 设备 integrate into FL 网络中，以确保低能源停机概率和未来任务执行成功。</li>
<li>methods: 该论文提出了一种使用多渠道 ALOHA 技术，并提出了一种方法来确保低能源停机概率和未来任务执行成功。</li>
<li>results: 数值结果显示，该方法可以在某些关键设置下具有更高的效率，特别是在平均能源收入不足以覆盖迭代成本的情况下。该方法也在衡量时间和电池水平方面比norm based解决方案更高效。<details>
<summary>Abstract</summary>
Distributed learning on edge devices has attracted increased attention with the advent of federated learning (FL). Notably, edge devices often have limited battery and heterogeneous energy availability, while multiple rounds are required in FL for convergence, intensifying the need for energy efficiency. Energy depletion may hinder the training process and the efficient utilization of the trained model. To solve these problems, this letter considers the integration of energy harvesting (EH) devices into a FL network with multi-channel ALOHA, while proposing a method to ensure both low energy outage probability and successful execution of future tasks. Numerical results demonstrate the effectiveness of this method, particularly in critical setups where the average energy income fails to cover the iteration cost. The method outperforms a norm based solution in terms of convergence time and battery level.
</details>
<details>
<summary>摘要</summary>
随着联合学习（FL）的出现，分布式学习在边缘设备上得到了更多的关注。然而，边缘设备通常具有有限的电池和多样化的能源可用性，而多轮训练是FL的必需之物，因此提高了能效性的需求。如果能源抽取不足可能会妨碍训练过程和模型的有效使用。为解决这些问题，本文考虑了将能量收集（EH）设备integrated into FL网络中，同时提出了一种方法，以确保低能源停机概率和未来任务的成功执行。数据显示，该方法在某些关键设置下表现出色，特别是当平均能源收入不足于迭代成本时。此方法在融合率和电池水平上也表现出优势，比一种基于范数的解决方案更快 converges。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks"><a href="#Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks" class="headerlink" title="Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks"></a>Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06021">http://arxiv.org/abs/2309.06021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwa Chafii, Salmane Naoumi, Reda Alami, Ebtesam Almazrouei, Mehdi Bennis, Merouane Debbah</li>
<li>for: 本研究旨在探讨未来 sixth generation 无线网络中，通过多代理学习和自适应通信协议，实现高维数据交换和复杂任务解决方案。</li>
<li>methods: 本研究使用多代理学习和自适应通信协议（EC-MARL）解决高维数据交换和复杂任务问题，并提供了EC-MARL算法和设计要求的概述。</li>
<li>results: 本研究提供了EC-MARL在未来 sixth generation 无线网络中的应用案例和研究机遇，以及其在自动驾驶、机器人导航、飞行基站网络规划和智能城市应用中的可能性。<details>
<summary>Abstract</summary>
In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and their design criteria are provided while presenting use cases and research opportunities on this emerging topic.
</details>
<details>
<summary>摘要</summary>
不同无线网络enario中，多个网络元件需要合作以实现最小延迟和能源消耗的共同任务。未来的无线网络将要交换高维度数据在动态和不确定环境中，因此实现通信控制任务会变得极其复杂和困难。多智能抽象学习（EC-MARL）是一种可能解决高维度连续控制问题的方法，并且可以在合作方式下解决半可观测的状态下的复杂任务。本文强调EC-MARL在未来6G无线网络中的重要性，具体是将自主决策能力传递到网络元件中，以解决自动驾驶、机器人定位、飞行基站网络规划和智能城市应用等复杂任务。文章提供了EC-MARL算法的简介和设计标准，同时还提供了实际应用和研究机会的案例。
</details></li>
</ul>
<hr>
<h2 id="Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks"><a href="#Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks" class="headerlink" title="Interpolation, Approximation and Controllability of Deep Neural Networks"></a>Interpolation, Approximation and Controllability of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06015">http://arxiv.org/abs/2309.06015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen</li>
<li>for:  investigate the expressive power of deep residual neural networks as continuous dynamical systems through control theory</li>
<li>methods:  consider two properties of supervised learning, universal interpolation and universal approximation, and their relationship in the context of general control systems</li>
<li>results:  show that universal interpolation holds for essentially any architecture with non-linearity, and identify conditions for the equivalence of universal interpolation and universal approximation<details>
<summary>Abstract</summary>
We investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory. Specifically, we consider two properties that arise from supervised learning, namely universal interpolation - the ability to match arbitrary input and target training samples - and the closely related notion of universal approximation - the ability to approximate input-target functional relationships via flow maps. Under the assumption of affine invariance of the control family, we give a characterisation of universal interpolation, showing that it holds for essentially any architecture with non-linearity. Furthermore, we elucidate the relationship between universal interpolation and universal approximation in the context of general control systems, showing that the two properties cannot be deduced from each other. At the same time, we identify conditions on the control family and the target function that ensures the equivalence of the two notions.
</details>
<details>
<summary>摘要</summary>
我们调查深度复原神经网络的表达力，理解它们作为连续时间系统的控制理论中的一部分。具体而言，我们考虑两个从超级学习获得的性质：一是通用 interpolate - 能将任意输入和目标训练样本匹配 - 以及与之相关的通用替代 - 能将输入-目标函数关系通过流图汇入。我们假设控制家族的 afine invariance，我们提供了 universal interpolate 的特征化，证明这个性质适用于大多数架构，并且与 universal approximation 之间的关系。此外，我们还详细介绍了两个概念之间的关系，并识别了控制家族和目标函数的条件，以确保它们的等价。
</details></li>
</ul>
<hr>
<h2 id="ATTA-Anomaly-aware-Test-Time-Adaptation-for-Out-of-Distribution-Detection-in-Segmentation"><a href="#ATTA-Anomaly-aware-Test-Time-Adaptation-for-Out-of-Distribution-Detection-in-Segmentation" class="headerlink" title="ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation"></a>ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05994">http://arxiv.org/abs/2309.05994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gaozhitong/atta">https://github.com/gaozhitong/atta</a></li>
<li>paper_authors: Zhitong Gao, Shipeng Yan, Xuming He</li>
<li>for: 提高异常点检测模型对不同领域的抗衰减性和Semantic shift检测能力</li>
<li>methods: 提出了一种双级异常点检测框架，利用全局低级特征来判断领域转换，并使用高级特征映射来检测semantic shift</li>
<li>results: 对多个OOD segmentation benchmark进行了验证， observe了不同基线模型的性能改进， indicating the effectiveness of the proposed method in handling domain shift and semantic shift.<details>
<summary>Abstract</summary>
Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, observing consistent performance improvements across various baseline models.
</details>
<details>
<summary>摘要</summary>
近年来， dense out-of-distribution (OOD) 检测技术的主要研究方向是在具有相同领域的训练和测试数据集之间进行研究，假设测试数据集中没有领域转换。然而，在实际应用中，领域转换通常存在，并且会对现有 OOD 检测模型的准确率产生重要影响。在这种情况下，我们提议一种双级 OOD 检测框架，同时处理领域转换和语义转换。第一级通过全局低级特征来判断图像中是否存在领域转换，而第二级通过高级特征地图来特征化语义转换。这种方法可以在无法见到的领域中适应模型，同时提高模型的检测新类的能力。我们在多个 OOD 分割benchmark上验证了我们的提议方法，包括具有显著领域转换的和无领域转换的benchmark，并观察到了不同基线模型的表现中的一致性提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach"><a href="#Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach" class="headerlink" title="Learning Unbiased News Article Representations: A Knowledge-Infused Approach"></a>Learning Unbiased News Article Representations: A Knowledge-Infused Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05981">http://arxiv.org/abs/2309.05981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadia Kamal, Jimmy Hartford, Jeremy Willis, Arunkumar Bagavathi</li>
<li>for: 这篇论文的目的是研究在线新闻文章的政治倾向，以便更好地理解社会团体中的政治意识形态和控制其影响。</li>
<li>methods: 这篇论文使用了知识感知深度学习模型，利用新闻文章的全球和本地上下文来学习不受束缚的表示。</li>
<li>results: 研究人员通过将数据设置为训练和测试集中的新闻域或新闻发布者完全不同，发现提议的模型可以减少算法政治偏见并超过基线方法预测新闻文章的政治倾向，准确率高达73%。<details>
<summary>Abstract</summary>
Quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. However, predicting the accurate political leaning of a news article with machine learning models is a challenging task. This is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. There is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. In this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles using their global and local contexts. We evaluate the proposed model by setting the data in such a way that news domains or news publishers in the test set are completely unseen during the training phase. With this setup we show that the proposed model mitigates algorithmic political bias and outperforms baseline methods to predict the political leaning of news articles with up to 73% accuracy.
</details>
<details>
<summary>摘要</summary>
（以下是简化中文版本）在线新闻文章的政治倾向可以帮助我们理解社会团体中政治意识形态的变化和调控方法。然而，使用机器学习模型预测新闻文章的政治倾向是一项具有挑战性的任务。这是因为新闻文章的政治倾向由多个因素定义，而现有的学习模型具有新闻发布者的政治偏见，导致模型在训练过程中受到潜在的政治偏见影响。目前只有有限的方法可以研究新闻文章的政治倾向，这些方法也不考虑算法政治偏见，这限制了机器学习模型对新闻文章的预测精度。在这种情况下，我们提出了一种具有知识注入的深度学习模型，该模型利用可靠的外部数据资源来学习不受政治偏见影响的新闻文章表示。我们通过在训练集中将新闻域或新闻发布者完全未看到的方式设置数据来评估该模型。结果表明，我们的模型可以减少算法政治偏见并超过基线方法预测新闻文章的政治倾向精度达73%。
</details></li>
</ul>
<hr>
<h2 id="CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram"><a href="#CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram" class="headerlink" title="CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram"></a>CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05975">http://arxiv.org/abs/2309.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Kong, Wei Ping, Ambrish Dantrey, Bryan Catanzaro</li>
<li>for: 这篇论文是为了提出一种能够结合波形denoiser和spectrogram denoiser的语音干净模型，以获得最佳的两种方法的优点。</li>
<li>methods: 这篇论文使用了一种基于流行的语音生成方法的两个阶段框架，包括波形模型和spectrogram模型。具体来说，CleanUNet 2  builds upon CleanUNet，当前的波形denoiser顶峰模型，并通过使用预测的spectrograms来提高其性能。</li>
<li>results: 根据多个 объектив和主观评价标准，CleanUNet 2 比前一代方法表现更好，包括语音干净度、语音质量和用户满意度等。<details>
<summary>Abstract</summary>
In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍CleanUNet 2，一种混合波形频谱滤波器和频谱滤波器的语音干净模型，实现了两者的优点。CleanUNet 2使用两个阶段框架， Drawing inspiration from popular speech synthesis methods, it consists of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further enhances its performance by taking predicted spectrograms from a spectrogram denoiser as input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.Here's the translation in Traditional Chinese:在这个工作中，我们介绍CleanUNet 2，一种结合波形频谱滤波器和频谱滤波器的语音干净模型，实现了两者的优点。CleanUNet 2使用两个阶段架构， drawing inspiration from popular speech synthesis methods， it consists of a waveform model and a spectrogram model。Specifically, CleanUNet 2 builds upon CleanUNet， the state-of-the-art waveform denoiser， and further enhances its performance by taking predicted spectrograms from a spectrogram denoiser as input。We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations。
</details></li>
</ul>
<hr>
<h2 id="Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation"><a href="#Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation" class="headerlink" title="Circuit Breaking: Removing Model Behaviors with Targeted Ablation"></a>Circuit Breaking: Removing Model Behaviors with Targeted Ablation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05973">http://arxiv.org/abs/2309.05973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xanderdavies/circuit-breaking">https://github.com/xanderdavies/circuit-breaking</a></li>
<li>paper_authors: Maximilian Li, Xander Davies, Max Nadeau</li>
<li>for: 降低 GPT-2 语言生成的恶性行为</li>
<li>methods: 简单地禁用模型组件之间的一些 causal 路径</li>
<li>results: 约 12 个 causal 边的禁用可以减轻恶性语言生成，而不会影响其他输入的性能Here’s the English version for reference:</li>
<li>for: Reducing GPT-2 toxic language generation</li>
<li>methods: Simply disabling a small number of causal paths between model components</li>
<li>results: Ablating approximately 12 causal edges can mitigate toxic generation with minimal degradation of performance on other inputs.<details>
<summary>Abstract</summary>
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
</details>
<details>
<summary>摘要</summary>
语模型经常表现出改善预训目标性能的行为，但对下游任务造成害。我们提出了一种新的方法，通过缩减一小部分 causal pathway 来除掉不良行为。 Given 一小量的输入，我们学习缩减一小部分重要的 causal pathway。在 GPT-2 毒性语言生成中，我们发现，缩减 12 条 causal edge 可以减少毒性语言生成，而不会对其他输入造成严重的影响。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity"><a href="#Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity" class="headerlink" title="Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity"></a>Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05968">http://arxiv.org/abs/2309.05968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ng Shyh-Chang, A-Li Luo, Bo Qiu</li>
<li>for: 这paper主要是为了证明神经网络（NN）编码定理的逆命题，即每个稳定收敛的NN的权重矩阵实际上编码了一个连续函数，该函数在训练集上准确地 aproximate 到一个有限的误差范围内。</li>
<li>methods: 该paper使用了Eckart-Young theorem来解释 truncated singular value decomposition（SVD）of NN层的weight矩阵，以及Latent Space Manifold（LSM）的概念来解释NN层中数据的编码和表示。</li>
<li>results: 研究发现，NN可以破坏维度约束，并通过吸收内存来提高表达性。此外，NN层的eigen-decomposition和Hopfield网络以及Transformer NN模型之间存在紧密的关系。<details>
<summary>Abstract</summary>
We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advances in conceptualizations of Hopfield networks and Transformer NN models.
</details>
<details>
<summary>摘要</summary>
我们证明了对应 theorem的逆命运算，即一个神经网络（NN）编码定理，显示任何稳定地训练完成的 NN 的weight矩阵实际上将一个连续函数所代表，这个函数可以在受训数据的范围内对应到该数据集，并且在这个范围内有一定的错误 bound。我们还证明了使用 Eckart-Young 定理对 NN 层的剪枝值分解，可以显示每个 NN 层的矩阵 matrix 中的latent space manifold，并且这些数据表示了 NN 层中的数学操作的几何性。我们的结果显示了如何 NN 突破维度紧缩的问题，通过将记忆容量作为表达能力的一部分，并且这两者是补充的。此外，我们的层矩阵分解（LMD）还 suggets了神经网络层的对� Hopfield 网络和 transformer 神经网络的最新发展有着密切的关系。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms"><a href="#Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms" class="headerlink" title="Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms"></a>Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05961">http://arxiv.org/abs/2309.05961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rima Hazra, Agnik Saha, Somnath Banerjee, Animesh Mukherjee</li>
<li>for: This paper aims to investigate the factors that contribute to the speed of responses on Community Question Answering (CQA) platforms.</li>
<li>methods: The authors examine six highly popular CQA platforms and analyze various metadata and patterns of user interaction to predict which queries will receive prompt responses.</li>
<li>results: The study finds a correlation between the time taken to yield the first response to a question and several variables, including the metadata, the formulation of the questions, and the level of interaction among users.<details>
<summary>Abstract</summary>
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection"><a href="#GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection" class="headerlink" title="GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection"></a>GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05953">http://arxiv.org/abs/2309.05953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yul091/GraphLogAD">https://github.com/yul091/GraphLogAD</a></li>
<li>paper_authors: Yufei Li, Yanchi Liu, Haoyu Wang, Zhengzhang Chen, Wei Cheng, Yuncong Chen, Wenchao Yu, Haifeng Chen, Cong Liu</li>
<li>for: 本文旨在检测系统日志中的异常现象，特别是关系系统组件和用户的异常现象。</li>
<li>methods: 本文提出了一种基于图的日志异常检测框架（GLAD），它利用日志内容中的关系、预测模型和序列模式来检测异常。</li>
<li>results: 本文对三个数据集进行了实验，结果表明GLAD可以准确地检测异常，并且可以捕捉到异常的关系特征。<details>
<summary>Abstract</summary>
Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events parsed from the log parser. These graphs represent events and fields as nodes and their relations as edges. Subsequently, GLAD utilizes a temporal-attentive graph edge anomaly detection model for identifying anomalous relations in these dynamic log graphs. This model employs a Graph Neural Network (GNN)-based encoder enhanced with transformers to capture content, structural and temporal features. We evaluate our proposed method on three datasets, and the results demonstrate the effectiveness of GLAD in detecting anomalies indicated by varying relational patterns.
</details>
<details>
<summary>摘要</summary>
日志扮演着重要的监控和调试系统的角色，记录了系统中重要的信息，包括事件和状态。虽然有很多方法用于检测日志序列中的异常，但它们通常忽视了系统组件之间的关系，例如服务和用户，这些关系可以从日志内容中提取出来。理解这些关系是检测异常和其下面的原因的关键。为解决这个问题，我们提出了GLAD，一个基于图的日志异常检测框架，用于检测系统日志中的关系异常。GLAD结合了日志 semantics、关系模式和时序模式，实现了一个综合的异常检测框架。具体来说，GLAD首先引入一个场景EXTRACT模块，使用推荐的几个shot学习来确定日志中的重要场景。然后，GLAD构建了动态日志图，用于滤波窗口内的日志事件和场景。这些图表示事件和场景为节点，以及它们之间的关系为边。接着，GLAD使用一个时间注意力图边异常检测模型，用于检测动态日志图中的异常关系。这个模型使用图神经网络（GNN）基础加强 transformer，以捕捉内容、结构和时序特征。我们对GLAD进行了三个数据集的评估，结果表明GLAD可以有效地检测异常，异常的关系异常。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models"><a href="#Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models" class="headerlink" title="Language Models as Black-Box Optimizers for Vision-Language Models"></a>Language Models as Black-Box Optimizers for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05950">http://arxiv.org/abs/2309.05950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shihongl1998/llm-as-a-blackbox-optimizer">https://github.com/shihongl1998/llm-as-a-blackbox-optimizer</a></li>
<li>paper_authors: Samuel Yu, Shihong Liu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan</li>
<li>for: 这paper的目的是提出一种基于自然语言提示的训练方法，以适应现有的vision-language模型（VLM）不可见参数的情况。</li>
<li>methods: 这paper使用了chat-based大型自然语言模型（LLM）作为黑盒优化器，通过自动“山丘攀登”过程，找到最佳的文本提示，以提高图像分类 tasks的性能。</li>
<li>results: 这paper在1shot学习setup下，使用自动提取的文本提示，超过了白盒连续提示方法CoOp的平均提升率1.5%，并且超过了OpenAI手动制作的提示和其他黑盒方法。此外， paper还发现了文本反馈中的负反馈可以帮助LLM更快地找到最佳提示。最后，paper发现通过自己的策略生成的文本提示不仅更易于理解，还可以适应不同的CLIP架构。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prompt by evaluating the accuracy of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot learning setup, our simple approach surpasses the white-box continuous prompting method CoOp by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms OpenAI's manually crafted prompts and is more efficient than other black-box methods like iterative APE. Additionally, we highlight the advantage of conversational feedback incorporating both positive and negative prompts, suggesting that LLMs can utilize the implicit "gradient" direction in textual feedback for a more efficient search. Lastly, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different CLIP architectures in a black-box manner.
</details>
<details>
<summary>摘要</summary>
现代Computer Vision Models (VLMs) 在大规模网络数据上进行预训后，在多种 Computer Vision 和多modal任务中显示了杰出的能力。目前，对 VLMs 的调整主要是在白盒子设定下进行，需要 Parameters 的存取。但是，许多 VLMs 靠赖 Proprietary 数据，因此不能使用白盒子方法进行调整。尽管受欢迎的Private Large Language Models (LLMs) 如 ChatGPT 仍然提供语言基于的使用者界面，我们想要发展一种以自然语言提示为基础的调整方法 для VLMs，因此不需要存取模型 Parameters、特征嵌入或输出核心。在这个设置下，我们提议使用 Chat-based LLMs 作为黑盒子优化器，通过询问自然语言提示来找到最佳提示，扩展了 CoOp 的白盒子连续提示方法。在具有挑战性的一击学习设置下，我们的简单方法比 CoOp 的白盒子连续提示方法高一个均值1.5%，涵盖11个数据集，包括 ImageNet。我们的方法还超越了 OpenAI 手动构成的提示，并且比其他黑盒方法，如迭代 APE，更高效。此外，我们发现，通过我们的策略，LLMs 可以利用文本反馈中的隐式 "梯度" 方向来更有效率地寻找。最后，我们发现，通过我们的策略生成的文本提示不仅更易于理解，而且可以跨不同 CLIP 架构在黑盒子方式进行转移。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals"><a href="#Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals" class="headerlink" title="Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals"></a>Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05927">http://arxiv.org/abs/2309.05927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Liu, Ellen L. Zippi, Hadi Pouransari, Chris Sandino, Jingping Nie, Hanlin Goh, Erdrin Azemi, Ali Moin</li>
<li>for: 本研究旨在提出一种适应多模态信号的预训练方法，以优化人体physical和mental状态的 representations。</li>
<li>methods: 该方法基于频率意识的掩码自动encoder（$\texttt{bio}$FAME），利用频率空间来参数化生物信号的表示。具有频率相关的变换器，可以在不同的输入大小和采样率下进行全球化混合。</li>
<li>results: 在多种传输试验中，该方法可以提高平均5.5%的分类精度，并在模式匹配情况下表现稳定。代码即将上线。<details>
<summary>Abstract</summary>
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. We evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of $\uparrow$5.5% improvement in classification accuracy over the previous state-of-the-art. Furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. Code will be available soon.
</details>
<details>
<summary>摘要</summary>
使用多modal信息自生物信号是关键以建立完整的人们的物理和心理状态表示。然而，多modal生物信号经常会在预训练和测试数据集之间存在显著的分布性变化，这可能是由任务规定的变化或多modal组合的变化引起的。为实现有效的预训练在存在潜在的分布性变化的情况下，我们提议一种频率意识的掩码自动编码器（$\texttt{bio}$FAME），该模型学习在频率空间中表示生物信号的参数。$\texttt{bio}$FAME使用了一个频率意识变换器，该变换器利用固定大小的福柯尔基于的运算符进行全token混合，不виси于输入的长度和抽象率。为保持每个输入通道中的频率组件，我们再次使用一种保持频率措施，该措施在幂 espacio中进行掩码自动编码。这种架构可以有效利用多modal信息进行预训练，并可以在测试时适应多种任务和多modal，无论输入的大小和顺序。我们在一个多样化的转移试验中，average的提高了5.5%的分类精度，相比前一个状态的艺术。此外，我们还证明了我们的架构在多modal异常场景中是有实用性的，包括预期的模式掉载或替换，这证明了它在实际应用中的实用性。代码即将上传。
</details></li>
</ul>
<hr>
<h2 id="On-Regularized-Sparse-Logistic-Regression"><a href="#On-Regularized-Sparse-Logistic-Regression" class="headerlink" title="On Regularized Sparse Logistic Regression"></a>On Regularized Sparse Logistic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05925">http://arxiv.org/abs/2309.05925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset">https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset</a></li>
<li>paper_authors: Mengyuan Zhang, Kai Liu</li>
<li>for: 本研究旨在同时进行分类和特征选择，以适应高维数据的分类问题。</li>
<li>methods: 本文提出了解决 $\ell_1$-regulized sparse logistic regression 和一些非 convex 违约项 regularized sparse logistic regression 问题的优化框架。</li>
<li>results: Empirical experiments on binary classification tasks with real-world datasets demonstrate that our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.<details>
<summary>Abstract</summary>
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
</details>
<details>
<summary>摘要</summary>
sparse 预测分类和特征选择同时进行高维数据处理。虽然许多研究已经解决了 $\ell_1$-regularized 预测分类，但对于非 convex 罚 penalty 的预测分类并没有相当的文献。在本文中，我们提议解决 $\ell_1$-regularized sparse 预测分类和一些非 convex 罚 penalty 的 sparse 预测分类，当非 convex 罚 penalty 满足某些前提条件时，使用同样的优化框架。在我们的优化框架中，我们使用不同的搜索 критери来保证不同的规则项的优化性。实际应用在实际数据集上的二分类任务中，我们的提议算法能够有效地进行分类和特征选择，并且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection"><a href="#Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection" class="headerlink" title="Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection"></a>Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06449">http://arxiv.org/abs/2309.06449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sabbir Alam, Walid Al Misba, Jayasimha Atulasimha</li>
<li>for: 本研究旨在提高边缘设备中卷积神经网络模型的异常检测性能，并且解决实时学习和硬件限制问题。</li>
<li>methods: 该研究提出了一种基于自适应神经网络的磁道束Synapse设计，并使用了有效的量化神经网络学习算法。</li>
<li>results: 研究结果显示，使用限定状态（5状） synaptic weights和磁力矩（SOT）电流脉冲来控制磁道束Synapse可以提高异常检测性能，并且比浮点数权重学习更高。此外，该方法还能够大幅降低训练过程中的Weight更新次数，从而降低能耗。<details>
<summary>Abstract</summary>
In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of quantized states and the inherent stochastic nature of DW synaptic weights in nanoscale devices are known to negatively impact the performance, our hardware-aware training algorithm is shown to leverage these imperfect device characteristics to generate an improvement in anomaly detection accuracy (90.98%) compared to accuracy obtained with floating-point trained weights. Furthermore, our DW-based approach demonstrates a remarkable reduction of at least three orders of magnitude in weight updates during training compared to the floating-point approach, implying substantial energy savings for our method. This work could stimulate the development of extremely energy efficient non-volatile multi-state synapse-based processors that can perform real-time training and inference on the edge with unsupervised data.
</details>
<details>
<summary>摘要</summary>
在基于自适应器的异常检测 paradigm中，在边缘设备中实现自适应器是极其困难的，因为边缘设备的硬件、能源和计算资源都受限。我们表明，这些限制可以通过设计一个具有低分辨率非朋义存储器基于 synapse 的 autoencoder，并使用有效的量化神经网络学习算法来解决。我们提议一种 ferromagnetic racetrack 上的引入不ches 承载一个 магнит的Domain Wall (DW) 作为 autoencoder synapse，其中有限状态（5-state） synaptic weights 通过 spin orbit torque (SOT) 电流脉冲来 manipulate。我们对 propose 的 autoencoder 模型进行了 NSL-KDD 数据集上的异常检测性能评估。我们采用了限定分辨率和 DW 设备不确定性意识的 trains 算法，实现了与浮点数精度 weights 相对的异常检测性能（90.98%）。尽管限定数量的量化状态和 nanoscale 设备的自然随机性会对性能产生负面影响，但我们的硬件意识 Train 算法可以利用这些不完美的设备特点来生成一个异常检测精度的提高（90.98%）。此外，我们的 DW 方法在训练中的weight 更新数量减少至少三个数量级，implying substantial energy savings for our method。这种工作可能会驱动非朋义多状态 synapse 基于处理器的开发，以实现实时在边缘进行训练和推断，并使用不supervised 数据。
</details></li>
</ul>
<hr>
<h2 id="ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning"><a href="#ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning" class="headerlink" title="ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning"></a>ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05915">http://arxiv.org/abs/2309.05915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang, Yang Yu</li>
<li>for: 提高offline政策优化中DT的表现，抗衡环境随机性的问题。</li>
<li>methods: 使用动态计划方法，包括样本值迭代、估计优势和ACT等。</li>
<li>results: 通过使用动态计划方法，ACT可以在不同的benchmark上表现出色，提高路径组合和动作生成的稳定性，并且通过对不同设计选择的ablation study进行深入分析。<details>
<summary>Abstract</summary>
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation results validate that, by leveraging the power of dynamic programming, ACT demonstrates effective trajectory stitching and robust action generation in spite of the environmental stochasticity, outperforming baseline methods across various benchmarks. Additionally, we conduct an in-depth analysis of ACT's various design choices through ablation studies.
</details>
<details>
<summary>摘要</summary>
第一步，我们使用内样值迭代来获取 Approximated 值函数，这包括在 Markov Decision Process 结构上进行动态计划。第二步，我们评估行动质量在上下文中的优势，我们引入了两种优势估计器，IAE 和 GAE，这些优势估计器适合不同的任务。第三步，我们训练一个优势决策 Transformer (ACT)，以生成基于优势估计的行动。在测试中，ACT 生成基于想要的优势的行动。我们的评估结果表明，通过利用动态计划的力量，ACT 能够在环境随机性的情况下生成有效的轨迹和稳定的行动，超越基eline 方法，在多个 benchmark 上达到了优秀的表现。此外，我们通过减少研究不同设计选择的影响的 ablation study 进行了深入的分析。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-Assessment-of-Salient-Object-Detection-via-Symbolic-Learning"><a href="#Adversarial-Attacks-Assessment-of-Salient-Object-Detection-via-Symbolic-Learning" class="headerlink" title="Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning"></a>Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05900">http://arxiv.org/abs/2309.05900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gustavo Olague, Roberto Pineda, Gerardo Ibarra-Vazquez, Matthieu Olague, Axel Martinez, Sambit Bakshi, Jonathan Vargas, Isnardo Reducindo</li>
<li>for: 这种研究旨在提高深度学习视觉注意系统的可靠性和安全性，以满足野生动物保护和生物多样性保护的需求。</li>
<li>methods: 这种研究使用了生物学的进化计算方法，以检验深度学习视觉注意系统的可靠性和安全性。</li>
<li>results: 研究结果表明，深度学习视觉注意系统在遭受恶意攻击和干扰时，表现出了明显的性能下降，而符号学习方法则能够坚持不变。此外，研究还发现，深度学习视觉注意系统在面对野生动物的攻击时，也会表现出性能下降。<details>
<summary>Abstract</summary>
Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers' attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodology against several adversarial attacks and noise perturbations using standard databases and a real-world problem of a shorebird called the Snowy Plover portraying a visual attention task. We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness. All neural networks suffer significant performance losses, while brain programming stands its ground and remains unaffected. Also, by studying the Snowy Plover, we remark on the importance of security in surveillance activities regarding wildlife protection and conservation.
</details>
<details>
<summary>摘要</summary>
机器学习在主流技术中占中心位置，并且超越了经典的手工特征设计方法。除了学习过程中的人工特征抽取外，它还有一个端到端的 paradigm从输入到输出，达到了极高的准确性。然而，由于其鲁棒性对恶意和隐触的扰动的问题引起了关注，其预测结果可以被完全改变。静观检测是一个研究领域，深度卷积神经网络在这里表现出了效果，但其可靠性成为了一个主要的问题，需要分析和解决。布朗编程是一种符号学习，与传统的人工智能有着共同点。本研究提供了证据，表明符号学习的鲁棒性在设计可靠视觉注意力系统方面是关键，它可以抵御even the most intense perturbations。我们在多个对抗攻击和噪声扰动使用标准数据库和实际问题中测试了我们的方法ологи，并与五种不同的深度学习方法进行比较，发现它们不能与符号学习模型相比。所有的神经网络都uffered significant performance losses，而布朗编程则保持不变。此外，我们通过研究雪亮燕鸥（Snowy Plover）来评论野生生物保护和保育的安全问题。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Conditional-Semi-Paired-Image-to-Image-Translation-For-Multi-Task-Image-Defect-Correction-On-Shopping-Websites"><a href="#Hierarchical-Conditional-Semi-Paired-Image-to-Image-Translation-For-Multi-Task-Image-Defect-Correction-On-Shopping-Websites" class="headerlink" title="Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites"></a>Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05883">http://arxiv.org/abs/2309.05883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyan Li, Jinmiao Fu, Shaoyuan Xu, Huidong Liu, Jia Liu, Bryan Wang</li>
<li>for:  correction of multiple defects in product images on shopping websites</li>
<li>methods:  unified Image-to-Image (I2I) translation model with attention mechanism and semi-paired training</li>
<li>results:  reduced Frechet Inception Distance (FID) by 24.6% compared to state-of-the-art I2I method, and reduced FID by 63.2% compared to state-of-the-art semi-paired I2I method<details>
<summary>Abstract</summary>
On shopping websites, product images of low quality negatively affect customer experience. Although there are plenty of work in detecting images with different defects, few efforts have been dedicated to correct those defects at scale. A major challenge is that there are thousands of product types and each has specific defects, therefore building defect specific models is unscalable. In this paper, we propose a unified Image-to-Image (I2I) translation model to correct multiple defects across different product types. Our model leverages an attention mechanism to hierarchically incorporate high-level defect groups and specific defect types to guide the network to focus on defect-related image regions. Evaluated on eight public datasets, our model reduces the Frechet Inception Distance (FID) by 24.6% in average compared with MoNCE, the state-of-the-art I2I method. Unlike public data, another practical challenge on shopping websites is that some paired images are of low quality. Therefore we design our model to be semi-paired by combining the L1 loss of paired data with the cycle loss of unpaired data. Tested on a shopping website dataset to correct three image defects, our model reduces (FID) by 63.2% in average compared with WS-I2I, the state-of-the art semi-paired I2I method.
</details>
<details>
<summary>摘要</summary>
在购物网站上，产品图像质量低下对客户体验有负面影响。尽管有很多研究检测不同缺陷的图像，但对于大规模纠正缺陷却受到了少数努力。主要挑战在于有 thousands 种产品类型，每种有特定的缺陷，因此建立缺陷特定的模型是不可能的。在这篇论文中，我们提出了一种统一的图像到图像（I2I）纠正模型，可以同时纠正不同产品类型的多种缺陷。我们的模型利用了注意机制，将高级缺陷组和特定缺陷类型层次地包含到网络中，使网络能够更好地关注缺陷相关的图像区域。在八个公共数据集上进行评估，我们的模型可以将 Frechet Inception Distance（FID）降低至 24.6% 以上，比 MoNCE，当前的 I2I 方法，更高效。不同于公共数据，在购物网站上，一些对应图像质量低下是另一个实际挑战。因此，我们设计了我们的模型为半对应的，将 L1 损失与对应数据的径规整共同使用，以便更好地纠正图像缺陷。在一个购物网站数据集上测试，我们的模型可以将 FID 降低至 63.2% 以上，比 WS-I2I，当前的半对应 I2I 方法，更高效。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Attacks-on-Face-Verification-Systems"><a href="#Generalized-Attacks-on-Face-Verification-Systems" class="headerlink" title="Generalized Attacks on Face Verification Systems"></a>Generalized Attacks on Face Verification Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05879">http://arxiv.org/abs/2309.05879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Nazari, Paula Branco, Guy-Vincent Jourdan</li>
<li>for: 本研究旨在探讨面部验证系统受到攻击的问题，特别是针对深度神经网络模型。</li>
<li>methods: 本研究使用了深度神经网络模型进行面部验证，并提出了一种新的攻击方法——逃脱人脸攻击（DodgePersonation Attack）。</li>
<li>results: 本研究表明，逃脱人脸攻击可以创造出面部图像，用于骚扰面部验证系统，并且可以在不同的场景下实现更高的攻击效果。此外，研究还提出了一种新的攻击方法——“一个人脸掌控全部”攻击（One Face to Rule Them All Attack）。<details>
<summary>Abstract</summary>
Face verification (FV) using deep neural network models has made tremendous progress in recent years, surpassing human accuracy and seeing deployment in various applications such as border control and smartphone unlocking. However, FV systems are vulnerable to Adversarial Attacks, which manipulate input images to deceive these systems in ways usually unnoticeable to humans. This paper provides an in-depth study of attacks on FV systems. We introduce the DodgePersonation Attack that formulates the creation of face images that impersonate a set of given identities while avoiding being identified as any of the identities in a separate, disjoint set. A taxonomy is proposed to provide a unified view of different types of Adversarial Attacks against FV systems, including Dodging Attacks, Impersonation Attacks, and Master Face Attacks. Finally, we propose the ''One Face to Rule Them All'' Attack which implements the DodgePersonation Attack with state-of-the-art performance on a well-known scenario (Master Face Attack) and which can also be used for the new scenarios introduced in this paper. While the state-of-the-art Master Face Attack can produce a set of 9 images to cover 43.82% of the identities in their test database, with 9 images our attack can cover 57.27% to 58.5% of these identifies while giving the attacker the choice of the identity to use to create the impersonation. Moreover, the 9 generated attack images appear identical to a casual observer.
</details>
<details>
<summary>摘要</summary>
face 验证（FV）使用深度神经网络模型在最近几年内得到了很大的进步，超过了人类准确率并在不同的应用中使用，如边境控制和智能手机锁定。然而，FV系统容易受到敌意攻击，这些攻击可以通过修改输入图像来诱导FV系统进行误判。本文提供了对FV系统攻击的深入研究。我们介绍了《 dodgingPersonation 攻击》，该攻击可以生成一组面图像，以便在不同的身份集中隐藏身份。我们还提出了对FV系统攻击的分类，包括拦截攻击、冒充攻击和主面攻击。最后，我们提出了《一面控制所有》攻击，这是一种实现《 dodgingPersonation 攻击》的新方法，可以在已知的攻击方法（主面攻击）中实现更高的性能。这种攻击可以在9个图像中覆盖57.27%至58.5%的身份，而且这些图像看起来都是完全一样的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.LG_2023_09_12/" data-id="clmjn91n1008f0j88bqe7ctic" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/12/eess.AS_2023_09_12/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-09-12
        
      </div>
    </a>
  
  
    <a href="/2023/09/12/eess.IV_2023_09_12/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-12</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
