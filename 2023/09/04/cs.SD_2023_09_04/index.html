
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-04 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02466 repo_url: None paper_authors: Paul Myles Eugenio for: 这">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-04">
<meta property="og:url" content="https://nullscc.github.io/2023/09/04/cs.SD_2023_09_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.02466 repo_url: None paper_authors: Paul Myles Eugenio for: 这">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-04T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.982Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.SD_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T15:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-04
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech"><a href="#Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech" class="headerlink" title="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech"></a>Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02466">http://arxiv.org/abs/2309.02466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Myles Eugenio</li>
<li>for: 这篇论文是关于语音学的一篇研究，旨在探讨 spoken words 的学习和生成。</li>
<li>methods: 这篇论文使用了一种基于 tensor-network 的 locally-connected 模型，利用了地方phonetic correlations来促进 spoken words 的学习和生成。</li>
<li>results: 研究发现，这种模型可以帮助学习者更好地理解和生成 spoken words，同时还可以提供一 hierarchy of the most likely errors 的信息，帮助学习者更好地改进语音表达。 I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)
</details>
<details>
<summary>摘要</summary>
spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech"><a href="#A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech" class="headerlink" title="A Comparative Analysis of Pretrained Language Models for Text-to-Speech"></a>A Comparative Analysis of Pretrained Language Models for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01576">http://arxiv.org/abs/2309.01576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Granero-Moya, Penny Karanasou, Sri Karlapati, Bastian Schnell, Nicole Peinelt, Alexis Moinet, Thomas Drugman</li>
<li>for: 这个研究旨在investigate the impact of different pre-trained language models (PLMs) on text-to-speech (TTS) systems, specifically in prosody prediction and pause prediction tasks.</li>
<li>methods: 研究者使用了15种不同的PLMs进行训练，并对模型的不同大小和表情类型进行了分析。</li>
<li>results: 研究发现，模型大小与质量之间存在对数关系，而且在不同的表情类型下表现出显著的差异。此外， pause prediction 任务对小型模型的敏感性较低，并且发现了与GLUE分数相关的关系。<details>
<summary>Abstract</summary>
State-of-the-art text-to-speech (TTS) systems have utilized pretrained language models (PLMs) to enhance prosody and create more natural-sounding speech. However, while PLMs have been extensively researched for natural language understanding (NLU), their impact on TTS has been overlooked. In this study, we aim to address this gap by conducting a comparative analysis of different PLMs for two TTS tasks: prosody prediction and pause prediction. Firstly, we trained a prosody prediction model using 15 different PLMs. Our findings revealed a logarithmic relationship between model size and quality, as well as significant performance differences between neutral and expressive prosody. Secondly, we employed PLMs for pause prediction and found that the task was less sensitive to small models. We also identified a strong correlation between our empirical results and the GLUE scores obtained for these language models. To the best of our knowledge, this is the first study of its kind to investigate the impact of different PLMs on TTS.
</details>
<details>
<summary>摘要</summary>
现代文本到语音（TTS）系统已经使用预训练语言模型（PLM）来提高气质和创造更自然的语音。然而，虽然PLM在自然语言理解（NLU）方面已经得到了广泛的研究，但它们在TTS方面的影响却被忽视了。在这项研究中，我们想要解决这个差距，通过对不同PLM进行比较分析，以便更好地理解它们在TTS任务中的表现。首先，我们使用15种不同的PLM来训练一个气质预测模型。我们的发现表明，模型大小与质量之间存在对数的关系，同时，中性和表达气质之间存在显著的性能差异。其次，我们使用PLM进行停顿预测任务，发现这个任务对小型模型来说较为不敏感。我们还发现了这些实验结果和GLUE分数中的语言模型所获得的相关性强。根据我们所知，这是首次对不同PLM在TTS方面的影响进行研究。
</details></li>
</ul>
<hr>
<h2 id="Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models"><a href="#Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models" class="headerlink" title="Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models"></a>Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01535">http://arxiv.org/abs/2309.01535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eike J. Nustede, Jörn Anemüller</li>
<li>for: 提高speech底层提取的深度和复杂性</li>
<li>methods:  integrate probabilistic（i.e., variational）latent space model，Complex-value处理和自注意</li>
<li>results: 在MS-DNS 2020和Voicebank+Demand datasets上取得了高水平的评价，比如SI-SDR达到20.2dB，与无 probabilistic latent space版本相比提高了0.5-1.4dB，与WaveUNet相比提高了2-2.4dB，与PHASEN相比提高了6.7dB。<details>
<summary>Abstract</summary>
In this paper, we propose to extend the deep, complex U-Network architecture for speech enhancement by incorporating a probabilistic (i.e., variational) latent space model. The proposed model is evaluated against several ablated versions of itself in order to study the effects of the variational latent space model, complex-value processing, and self-attention. Evaluation on the MS-DNS 2020 and Voicebank+Demand datasets yields consistently high performance. E.g., the proposed model achieves an SI-SDR of up to 20.2 dB, about 0.5 to 1.4 dB higher than its ablated version without probabilistic latent space, 2-2.4 dB higher than WaveUNet, and 6.7 dB above PHASEN. Compared to real-valued magnitude spectrogram processing with a variational U-Net, the complex U-Net achieves an improvement of up to 4.5 dB SI-SDR. Complex spectrum encoding as magnitude and phase yields best performance in anechoic conditions whereas real and imaginary part representation results in better generalization to (novel) reverberation conditions, possibly due to the underlying physics of sound.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了扩展深度、复杂的U-网络架构，以提高语音增强。我们在这个模型中添加了一个 probabilistic（即变量）干元空间模型。我们对这个模型进行了一些简化版本的比较，以研究变量干元空间模型、复杂值处理和自注意的影响。在MS-DNS 2020和Voicebank+Demand datasets上进行评估，我们发现这个模型在语音增强中表现出色，例如在MS-DNS 2020 dataset上达到20.2 dB的SI-SDR，比不含变量干元空间模型的模型高出0.5-1.4 dB，比WaveUNet高出2.2-4.4 dB，并高出PHASEN的6.7 dB。与实数值spectrogram处理的变量U-Net进行比较，复杂spectrum编码为实数值spectrogram的情况下，实现最佳性能，而实数值和虚数值表示的情况下，更好地泛化到（新的）频率响应条件，可能是因为音波的物理学习。
</details></li>
</ul>
<hr>
<h2 id="Quid-Manumit-–-Freeing-the-Qubit-for-Art"><a href="#Quid-Manumit-–-Freeing-the-Qubit-for-Art" class="headerlink" title="Quid Manumit – Freeing the Qubit for Art"></a>Quid Manumit – Freeing the Qubit for Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03104">http://arxiv.org/abs/2309.03104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Carney</li>
<li>for: 这篇论文描述了如何将量子计算应用到音乐创作中，创造出独立的量子音乐效果和乐器。</li>
<li>methods: 本论文使用了ARM基于Raspberry Pi Pico嵌入式微控制器的量子模拟器代码，并提供了一些示例，包括一个量子MIDI处理器，可以根据输入音符生成附加的伴奏和具有量子生成的乐器。</li>
<li>results: 本论文的结果包括一个量子扭曲模块，可以根据量子电路改变乐器的原始声音，以及一个自包含的量子钢琴和一个名为“量子搅拌器”的效果模块插件。这些示例都提供了开源代码，并且这是作者知道的第一个嵌入式量子模拟器 для乐器（另一个QSIM）。<details>
<summary>Abstract</summary>
This paper describes how to `Free the Qubit' for art, by creating standalone quantum musical effects and instruments. Previously released quantum simulator code for an ARM-based Raspberry Pi Pico embedded microcontroller is utilised here, and several examples are built demonstrating different methods of utilising embedded resources: The first is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, which is presented in two forms; a self-contained Quantum Stylophone, and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1. This paper also discusses future work and directions for quantum instruments, and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (another QSIM).
</details>
<details>
<summary>摘要</summary>
One example is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum-generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. Another example is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, presented in two forms: a self-contained Quantum Stylophone and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1.The paper discusses future work and directions for quantum instruments and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (QSIM).
</details></li>
</ul>
<hr>
<h2 id="RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes"><a href="#RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes" class="headerlink" title="RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes"></a>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01513">http://arxiv.org/abs/2309.01513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Jung-Woo Choi</li>
<li>for: 这篇论文主要是为了提出一种基于深度神经网络的房间几何学推理（RGI）方法，以便在实际的3D音频渲染中使用。</li>
<li>methods: 这篇论文使用了一种名为RGI-Net的深度神经网络，该网络可以基于房间响应函数（RIR）中的高阶反射关系学习和利用 Room geometry。而不像传统的RGI技术，RGI-Net不需要 convex 形房间、先知数量和第一阶反射的假设。</li>
<li>results: RGI-Net可以在实际测量的声学数据上高精度地估算房间几何学，并且可以处理非几何形房间和缺失第一阶反射的情况。<details>
<summary>Abstract</summary>
Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphone array and a single loudspeaker, which greatly improves its practical applicability. RGI-Net includes the evaluation network that separately evaluates the presence probability of walls, so the geometry inference is possible without prior knowledge of the number of walls.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment"><a href="#BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment" class="headerlink" title="BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"></a>BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01480">http://arxiv.org/abs/2309.01480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan</li>
<li>for: 这个论文是为了研究非侵入式语音质量评估（NISQA）系统中的安全问题，特别是针对不可靠的资源引入的攻击。</li>
<li>methods: 该论文提出了一种基于存在事件的新型后门攻击方法，用于攻击NISQA系统。该方法利用存在事件作为触发器，以实现高度隐蔽的攻击。</li>
<li>results: 实验结果表明，该提出的后门攻击方法可以在四个基准数据集上达到99%的攻击成功率，仅需3%的毒素率。<details>
<summary>Abstract</summary>
Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting the mean opinion score (MOS) of speech without requiring the reference speech. In practical NISQA scenarios, untrusted third-party resources are often employed during deep neural network training to reduce costs. However, it would introduce a potential security vulnerability as specially designed untrusted resources can launch backdoor attacks against NISQA systems. Existing backdoor attacks primarily focus on classification tasks and are not directly applicable to NISQA which is a regression task. In this paper, we propose a novel backdoor attack on NISQA tasks, leveraging presence events as triggers to achieving highly stealthy attacks. To evaluate the effectiveness of our proposed approach, we conducted experiments on four benchmark datasets and employed two state-of-the-art NISQA models. The results demonstrate that the proposed backdoor attack achieved an average attack success rate of up to 99% with a poisoning rate of only 3%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation"><a href="#Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation" class="headerlink" title="Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation"></a>Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02459">http://arxiv.org/abs/2309.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Weinan Tong, Yaoxun Xu, Changhe Song, Zhiyong Wu, Zhao You, Dan Su, Dong Yu, Helen Meng</li>
<li>for: 提高新领域自动语音识别（ASR）性能使用文本数据进行适应性调整</li>
<li>methods: 通过下采样声音表示进行与文本模式匹配，并通过引入连续积 integrate-and-fire（CIF）模块生成声音表示，使ASR模型能够更好地学习各个模式的统一表示</li>
<li>results: 实验结果表明，提posed方法可以更好地适应新领域数据进行适应性调整<details>
<summary>Abstract</summary>
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
Mapping two modalities, speech和text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.Here's the translation in Traditional Chinese as well:Mapping two modalities, speech和text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge"><a href="#SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge" class="headerlink" title="SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge"></a>SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01437">http://arxiv.org/abs/2309.01437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng</li>
<li>for: 提高语音识别效果，特别是对域外数据和长尾数据的处理。</li>
<li>methods: 引入sememe知识来增强语音识别模型，包括sememe知识的概念定义和语音识别模型的结构设计。</li>
<li>results: 实验结果表明，sememe知识可以提高语音识别效果，并且可以提高模型对域外数据和长尾数据的处理能力。<details>
<summary>Abstract</summary>
Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
</details>
<details>
<summary>摘要</summary>
最近，speech recognition领域内已经取得了很好的进步。然而，纯数据驱动方法在不同领域和长尾数据上困难解决问题。考虑到知识驱动方法可以帮助数据驱动方法缓解弊端，我们引入sememe基于语言定义的语义知识信息到speech recognition中（SememeASR）。sememe按照语言定义是语言中最小的语义单元，可以非常好地表示每个词的隐式语义信息。我们的实验显示，通过添加sememe信息可以提高speech recognition的效果。此外，我们的进一步实验还表明，sememe知识可以提高模型对长尾数据的识别和提高模型在不同领域的适应能力。
</details></li>
</ul>
<hr>
<h2 id="MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and"><a href="#MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and" class="headerlink" title="MDSC: Towards Evaluating the Style Consistency Between Music and"></a>MDSC: Towards Evaluating the Style Consistency Between Music and</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01340">http://arxiv.org/abs/2309.01340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Zhou, Baoyuan Wang</li>
<li>for: 本研究开发了一个新的评估指标（Music-Dance-Style Consistency，MDSC），用于评估Generated dance motion sequences和conditioning music sequences之间的风格相似性。</li>
<li>methods: 本研究使用了一个 clustering 方法，将 dance motion embedding 和 music embedding 映射到共同空间中，并且将 intra-cluster 距离和 inter-cluster 距离做最大化。</li>
<li>results: 本研究通过评估多种 music-conditioned motion generation 方法的结果，结果显示 MDSC 是一个Robust的评估指标，可以准确地评估 dance motion 和 music 的风格相似性。<details>
<summary>Abstract</summary>
We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric which assesses to what degree the dance moves and music match. Existing metrics can only evaluate the fidelity and diversity of motion and the degree of rhythmic matching between music and motion. MDSC measures how stylistically correlated the generated dance motion sequences and the conditioning music sequences are. We found that directly measuring the embedding distance between motion and music is not an optimal solution. We instead tackle this through modelling it as a clustering problem. Specifically, 1) we pre-train a music encoder and a motion encoder, then 2) we learn to map and align the motion and music embedding in joint space by jointly minimizing the intra-cluster distance and maximizing the inter-cluster distance, and 3) for evaluation purpose, we encode the dance moves into embedding and measure the intra-cluster and inter-cluster distances, as well as the ratio between them. We evaluate our metric on the results of several music-conditioned motion generation methods, combined with user study, we found that our proposed metric is a robust evaluation metric in measuring the music-dance style correlation. The code is available at: https://github.com/zixiangzhou916/MDSC.
</details>
<details>
<summary>摘要</summary>
我们提出了MDSC（音乐舞蹈风格一致性），这是评估音乐和舞蹈动作之间的一致性的首个评价指标。现有的指标只能评估动作和音乐的准确性和多样性，以及它们之间的节奏匹配度。而MDSC则衡量了生成的舞蹈动作序列和conditioning音乐序列之间的风格相关性。我们发现直接测量动作和音乐的嵌入距离并不是最佳解决方案。我们相反地通过模型化它为一个聚类问题来解决。具体来说，我们先预训练了音乐编码器和动作编码器，然后学习将动作和音乐嵌入空间中的mapping和对齐。最后，我们用这些mapping来评估生成的舞蹈动作是否符合音乐风格。我们对几种音乐受控动作生成方法的结果进行评估，并结合用户调查，发现我们提出的指标是一个有力的评价指标，能够准确地衡量音乐和舞蹈风格之间的相关性。代码可以在：https://github.com/zixiangzhou916/MDSC 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.SD_2023_09_04/" data-id="clmjn91oc00br0j88fgj72w30" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/05/eess.IV_2023_09_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-05
        
      </div>
    </a>
  
  
    <a href="/2023/09/04/cs.LG_2023_09_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-04</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
