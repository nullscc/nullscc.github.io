
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-04 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01860 repo_url: None paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-04">
<meta property="og:url" content="https://nullscc.github.io/2023/09/04/cs.LG_2023_09_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01860 repo_url: None paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-04T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.979Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.LG_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T10:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-04
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation"><a href="#Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation" class="headerlink" title="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation"></a>Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01860">http://arxiv.org/abs/2309.01860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim Swargo, Muhammad Abdullah Adnan</li>
<li>for: 这 paper 是为了增加多Modal信息到现有的连续手语识别和翻译管道中。</li>
<li>methods: 我们使用了 Optical flow 信息和 RGB 图像来增强特征，并使用了 Cross-modal 编码器。</li>
<li>results: 我们在 recognition 和翻译任务中各提高了result，在识别任务中降低了 WER 0.9，在翻译任务中提高了大多数 BLEU 分数约 0.6。<details>
<summary>Abstract</summary>
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们设计了一种多modal信息添加机制，用于与现有的连续手语识别和翻译管道结合。在我们的过程中，我们将光流信息与RGB图像结合，以增强特征以 движения相关信息。本研究证明了多modal信息包含的可行性，并使用交叉模态编码器进行实现。我们使用的插件非常轻量级，不需要在端到端方式中添加新模态的特征提取器。我们对手语识别和翻译 task 都进行了应用，并在每个任务上提高了结果。我们在 RWTH-PHOENIX-2014 数据集上进行了手语识别任务的评估，并在 RWTH-PHOENIX-2014T 数据集上进行了翻译任务的评估。在识别任务上，我们的方法降低了 WER 值0.9，在翻译任务上，我们的方法提高了大多数 BLEU 分数的测试集值约0.6。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks"><a href="#Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks" class="headerlink" title="Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks"></a>Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01838">http://arxiv.org/abs/2309.01838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kacem Khaled, Mouna Dhaouadi, Felipe Gohring de Magalhães, Gabriela Nicolescu</li>
<li>for: 防止深度学习模型被盗用攻击</li>
<li>methods: 提出了一种简单 yet effective 和高效的防御方法，通过对输出概率进行干扰来防止模型盗用攻击</li>
<li>results: 对三种state-of-the-art盗用攻击进行了验证，与现有防御方法相比，具有$\times37$快的执行速度，不需要额外训练，并且对模型性能有较低的影响。<details>
<summary>Abstract</summary>
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details>
<details>
<summary>摘要</summary>
模型盗取攻击已成为深度学习模型的严重问题，攻击者可以通过访问其黑盒API来盗取已训练的模型。这可能会导致知识产权盗取和安全隐私问题。当前状态的攻击防御建议添加扰动到预测概率中，但它们受到重度计算和对对手的假设的限制。它们经常需要额外训练auxiliary模型，这可能是时间consuming和资源占用的，这阻碍了这些防御在实际应用中的部署。在这篇论文中，我们提出了一种简单 yet有效的防御方案。我们介绍了一种论据 Approach来扰动输出概率。我们的防御可以轻松地与模型一起 интегра，无需额外训练。我们表明了我们的防御能够对三种当前最佳攻击方法进行防御。我们对大量和压缩（i.e., 压缩）的 convolutional Neural Networks（CNNs）进行评估，我们的技术在各个视觉数据集上表现出色，并且在执行速度方面具有$\times37$快的优势，而且不需要任何额外模型，同时具有低的影响于模型性能。我们还 validate了我们的防御是适用于边缘设备的quantized CNNs。
</details></li>
</ul>
<hr>
<h2 id="Delegating-Data-Collection-in-Decentralized-Machine-Learning"><a href="#Delegating-Data-Collection-in-Decentralized-Machine-Learning" class="headerlink" title="Delegating Data Collection in Decentralized Machine Learning"></a>Delegating Data Collection in Decentralized Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01837">http://arxiv.org/abs/2309.01837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nivasini Ananthakrishnan, Stephen Bates, Michael I. Jordan, Nika Haghtalab</li>
<li>for: study the delegation of data collection in decentralized machine learning ecosystems</li>
<li>methods: design optimal and near-optimal contracts to address challenges of lack of certainty and lack of knowledge regarding optimal performance</li>
<li>results: achieve 1-1&#x2F;e fraction of the first-best utility with simple linear contracts, and give sufficient conditions for achieving a vanishing additive approximation to the optimal utility with adaptive and efficient convex programming.<details>
<summary>Abstract</summary>
Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1-1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal's test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.
</details>
<details>
<summary>摘要</summary>
受到分散式机器学习生态的启发，我们研究数据收集的委托。从合约理论为起点，我们设计优化和近似优化的合约，解决机器学习中两个基本挑战：模型评估的不确定性和任何模型的最佳性知识不足。我们示出缺乏确定性可以通过简单的线性合约，实现1-1/e的最佳 utility，即使主人只有一小部分的测试集。此外，我们提供了让数据集大小满足的条件，以实现几乎减少的加法误差估计。为了解决任何模型的最佳性知识不足，我们提供了一个可靠地和高效地computing优化合约的凸程程式。
</details></li>
</ul>
<hr>
<h2 id="Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties"><a href="#Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties" class="headerlink" title="Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties"></a>Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03094">http://arxiv.org/abs/2309.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Vinay Chakravarthi Gogineni, Stefan Werner</li>
<li>for: 这个论文研究了带有非凸非积分违偏的量化回归问题，特别是使用最大凸函数偏好（MCP）和缓和凸函数偏好（SCAD）。</li>
<li>methods: 这个论文使用了 alternating direction method of multipliers（ADMM），并提出了一种单循环缓和ADMM算法（SIAD），以加速量化回归的速度。</li>
<li>results: 数据表明，SIAD算法比现有方法更快和稳定，提供了更好的积分回归解决方案。<details>
<summary>Abstract</summary>
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and establish the necessary conditions for convergence. Theoretically, we confirm a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of augmented Lagrangian. Subsequently, we provide numerical results to showcase the effectiveness of the SIAD algorithm. Our findings highlight that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details>
<details>
<summary>摘要</summary>
To improve convergence speed, we use the alternating direction method of multipliers (ADMM) and develop a novel single-loop smoothing ADMM algorithm called SIAD, which is tailored for sparse-penalized quantile regression. We first examine the convergence properties of the SIAD algorithm and establish the necessary conditions for convergence.Theoretically, we prove that the sub-gradient bound of the augmented Lagrangian has a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$. Numerical results show that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression. Our findings demonstrate that the SIAD method is effective in solving this problem, with a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$.
</details></li>
</ul>
<hr>
<h2 id="Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks"><a href="#Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks" class="headerlink" title="Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks"></a>Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01829">http://arxiv.org/abs/2309.01829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakash Ravindra Shinde, Charu Jain, Amir Kalev</li>
<li>for: 这个论文的目的是研究量子卷积神经网络（QCNN）中的过拟合问题，以及一种常见的过拟合缓解方法（post-training dropout）在量子设置下的应用。</li>
<li>methods: 本文使用了一种简单的 dropout 方法来缓解 QCNN 中的过拟合问题，并发现了该方法在量子设置下可能导致 QCNN 的成功概率下降。作者还提出了一种软化的 dropout 方法来解决过拟合问题。</li>
<li>results: 本文通过测试几个案例来证明软化 dropout 方法可以成功地缓解 QCNN 中的过拟合问题，并且可以保持 QCNN 的成功概率。<details>
<summary>Abstract</summary>
Quantum convolutional neural network (QCNN), an early application for quantum computers in the NISQ era, has been consistently proven successful as a machine learning (ML) algorithm for several tasks with significant accuracy. Derived from its classical counterpart, QCNN is prone to overfitting. Overfitting is a typical shortcoming of ML models that are trained too closely to the availed training dataset and perform relatively poorly on unseen datasets for a similar problem. In this work we study the adaptation of one of the most successful overfitting mitigation method, knows as the (post-training) dropout method, to the quantum setting. We find that a straightforward implementation of this method in the quantum setting leads to a significant and undesirable consequence: a substantial decrease in success probability of the QCNN. We argue that this effect exposes the crucial role of entanglement in QCNNs and the vulnerability of QCNNs to entanglement loss. To handle overfitting, we proposed a softer version of the dropout method. We find that the proposed method allows us to handle successfully overfitting in the test cases.
</details>
<details>
<summary>摘要</summary>
量子卷积神经网络（QCNN），在不完全Quantum Computer（NISQ）时代的早期应用，已经一直证明为机器学习（ML）算法的成功应用，对于多个任务具有显著的准确率。基于其类传统对应的Counterpart，QCNN受到过拟合的影响。过拟合是机器学习模型在训练数据集过于仔细地学习，对于未见数据集的类似问题表现较差的典型缺点。在这项工作中，我们研究了在量子设置中对一个最successful overfitting mitigation方法（post-training dropout method）的适应。我们发现，直接在量子设置中实现这种方法会导致重要的和不 DESirable consequence：量子神经网络成功率的显著减少。我们认为，这种效果暴露了量子神经网络中的束缚和量子神经网络对束缚的敏感性。为了处理过拟合，我们提议了一种软化的Dropout方法。我们发现，该方法可以成功地处理测试 caso中的过拟合。
</details></li>
</ul>
<hr>
<h2 id="Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation"><a href="#Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation" class="headerlink" title="Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation"></a>Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01828">http://arxiv.org/abs/2309.01828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Elmahallawy, Tie Luo, Mohamed I. Ibrahem</li>
<li>for: 这篇论文旨在解决在低地球轨道（LEO）上运行小卫星的资料汇集和训练人工智能模型（AI模型）时所遇到的问题。</li>
<li>methods: 本论文提出了一个名为FedSecure的安全 federated learning（FL）方法，包括两个新的 комponents：（1）对于每个卫星的隐私资料进行保护，使用功能加密方案，和（2）在轨道上进行模型传输和聚合，从而最小化陌生卫星进入可见区域的时间等待时间。</li>
<li>results: 我们的分析和结果显示，FedSecure可以保护每个卫星的资料隐私，并且具有较低的通信和计算负载，对比其他关于隐私保护的FL聚合方法。此外，FedSecure可以对抗听到者、侦错服务器或侦错卫星的攻击，并且可以快速地将模型训练完成，从天到地只需几个小时，并且可以达到高准确度的85.35%。<details>
<summary>Abstract</summary>
Satellite technologies have advanced drastically in recent years, leading to a heated interest in launching small satellites into low Earth orbit (LEOs) to collect massive data such as satellite imagery. Downloading these data to a ground station (GS) to perform centralized learning to build an AI model is not practical due to the limited and expensive bandwidth. Federated learning (FL) offers a potential solution but will incur a very large convergence delay due to the highly sporadic and irregular connectivity between LEO satellites and GS. In addition, there are significant security and privacy risks where eavesdroppers or curious servers/satellites may infer raw data from satellites' model parameters transmitted over insecure communication channels. To address these issues, this paper proposes FedSecure, a secure FL approach designed for LEO constellations, which consists of two novel components: (1) decentralized key generation that protects satellite data privacy using a functional encryption scheme, and (2) on-orbit model forwarding and aggregation that generates a partial global model per orbit to minimize the idle waiting time for invisible satellites to enter the visible zone of the GS. Our analysis and results show that FedSecure preserves the privacy of each satellite's data against eavesdroppers, a curious server, or curious satellites. It is lightweight with significantly lower communication and computation overheads than other privacy-preserving FL aggregation approaches. It also reduces convergence delay drastically from days to only a few hours, yet achieving high accuracy of up to 85.35% using realistic satellite images.
</details>
<details>
<summary>摘要</summary>
卫星技术在最近几年内发展了惊人的进步，导致了发射小卫星到低地球轨道（LEO）收集大量数据，如卫星图像。将这些数据下载到地面站（GS）以进行中央学习建立人工智能模型并不实际，因为卫星和GS之间的带宽有限且昂贵。 Federated learning（FL）提供了一个可能的解决方案，但它会导致非常大的融合延迟，因为LEO卫星和GS之间的连接是不规则和不稳定的。此外，在卫星图像的传输过程中，有可能有人 intercept或curious服务器/卫星探测到卫星的模型参数，这会导致数据泄露和隐私泄露。为解决这些问题，本文提出了FedSecure，一种安全的FL方法，设计为LEO卫星团组，它包括两个新的组成部分：1. 分布式钥匙生成，使用功能加密算法保护卫星数据隐私。2. 在轨道上进行模型转发和聚合，每次轨道上的卫星生成部分全球模型，以降低不可见的卫星进入可见区的等待时间。我们的分析和结果表明，FedSecure能够保护每个卫星的数据隐私，对于扰乱者、curious服务器或curious卫星来说。它的通信和计算开销较低，与其他隐私保护FL聚合方法相比，它也可以快速融合，只需几个小时，而不是天数。此外，FedSecure可以达到85.35%的准确率，使用实际的卫星图像。
</details></li>
</ul>
<hr>
<h2 id="LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning"><a href="#LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning" class="headerlink" title="LoopTune: Optimizing Tensor Computations with Reinforcement Learning"></a>LoopTune: Optimizing Tensor Computations with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01825">http://arxiv.org/abs/2309.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dejan Grubisic, Bram Wasti, Chris Cummins, John Mellor-Crummey, Aleksandar Zlateski</li>
<li>for: 这篇论文旨在应对机器学习应用程序在新硬件上运行时，传统编译器无法提供性能，受欢迎的自动适应器很长的搜寻时间，并且专家优化的库会带来不可持续的成本。</li>
<li>methods: 为了解决这个问题，我们开发了LoopTune，一个基于深度遗传学习的编译器，用于优化深度学习模型中的tensor计算。LoopTune使用了Lightweight code generator LoopNest来进行硬件特定优化，并使用了一个新的图形基于表示和动作空间，将LoopNest加速了3.2倍。</li>
<li>results: 根据实验结果，LoopTune可以实时对LoopNest进行优化，实现了对Numpy手动优化的性能水平，并且与TVM、MetaSchedule和AutoTVM相比，LoopTune的代码速度为1.08倍、2.8倍和3.2倍。此外，LoopTune可以在秒钟内完成代码优化。<details>
<summary>Abstract</summary>
Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
</details>
<details>
<summary>摘要</summary>
高级编译技术是深度学习应用的关键，但传统的编译器无法提供性能。受欢迎的自动调整工具有长时间搜索时间，专家优化库引入不可持续的成本。为解决这个问题，我们开发了LoopTune，一个基于深度学习的编译器，用于优化深度学习模型中的矩阵计算。LoopTune优化矩阵遍历顺序，使用精灵的Lightweight代码生成器LoopNest执行硬件特定优化。通过Graph基于表示和行动空间，LoopTune提高了LoopNest的速度，生成的代码比TVM快3.2倍，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，一直保持与手动优化库Numpy的水平。此外，LoopTune在秒钟级别调整代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision"><a href="#On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision" class="headerlink" title="On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision"></a>On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01824">http://arxiv.org/abs/2309.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav<br>for: 这篇论文是为了提高边缘设备上进行视觉处理时的效率和精度而写的。methods: 这篇论文使用了一种名为 AdaptiveActivation 的新技术，这个技术可以在运行时自动调整 DNN 的精度和能 consumption，不需要重新训练。results: 实验结果显示，使用 AdaptiveActivation 技术可以让 DNN 在不同的边缘环境中提高精度和效率，并且需要10%-38% less memory than baseline techniques。<details>
<summary>Abstract</summary>
Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve the efficiency of DNNs by using sparsity or quantization. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper presents a novel technique to allow DNNs to adapt their accuracy and energy consumption during run-time, without the need for any re-training. Our technique called AdaptiveActivation introduces a hyper-parameter that controls the output range of the DNNs' activation function to dynamically adjust the sparsity and precision in the DNN. AdaptiveActivation can be applied to any existing pre-trained DNN to improve their deployability in diverse edge environments. We conduct experiments on popular edge devices and show that the accuracy is within 1.5% of the baseline. We also show that our approach requires 10%--38% less memory than the baseline techniques leading to more accuracy-efficiency tradeoff options
</details>
<details>
<summary>摘要</summary>
处理移动设备上的视觉数据有很多应用，例如应急响应和跟踪。现状顶尖计算机视觉技术利用大深度神经网络（DNNs），但这些DNNs通常是资源受限的边缘设备上不可deploy。许多技术改进DNNs的效率，使用稀疏或量化。但这些技术的准确率和效率无法适应不同的硬件限制和准确要求。这篇论文介绍了一种新的技术，允许DNNs在运行时根据需要调整其准确率和能耗。我们的技术被称为AdaptiveActivation，它在DNNs的活化函数输出范围中引入了一个超参数，以动态调整DNNs的稀疏和精度。AdaptiveActivation可以应用于任何已经预训练的DNN，以提高其在多样化边缘环境中的部署性。我们在流行的边缘设备上进行了实验，并证明了与基准值相比，准确率在1.5%之间。我们还证明了我们的方法需要10%-38% less memory than基准技术，导致更多的准确率-效率质量Tradeoff。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension"><a href="#Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension" class="headerlink" title="Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension"></a>Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02465">http://arxiv.org/abs/2309.02465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idealab-isu/llm4g-code">https://github.com/idealab-isu/llm4g-code</a></li>
<li>paper_authors: Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde, Adarsh Krishnamurthy</li>
<li>for: 这篇论文旨在评估现有的基础大语言模型（LLMs）是否能够理解和修复3D打印的G-code文件中的错误。</li>
<li>methods: 作者使用了六种当前领先的基础大语言模型（LLMs），并设计了有效的提示来使这些模型理解和操纵G-code文件。</li>
<li>results: 研究发现这些模型在检测和修复常见错误方面具有不同的优势和不足，并讨论了使用LLMs进行G-code理解的限制和局限性。<details>
<summary>Abstract</summary>
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strengths and weaknesses for understanding complete G-code files. We also discuss the implications and limitations of using LLMs for G-code comprehension.
</details>
<details>
<summary>摘要</summary>
三次元印刷或添加制造技术是一种革命性的技术，允许将数字模型转化为实际物体。然而，3D印刷的质量和准确性取决于G-code的正确性和效率，G-code是一种低级数控程序语言，用于指示3D印刷机器人如何移动和挤出材料。调试G-code是一项复杂的任务，需要对G-code格式和数据的语法和 semantics有深入的理解，以及要印刷的部件的几何学结构。在这篇论文中，我们提出了第一次对六种当前最佳的基础大型自然语言模型（LLMs）进行了广泛的评估，以确定它们在理解和调试G-code文件方面的能力。我们设计了有效的提示，使得预训练的LLMs能够理解和操纵G-code，并测试了它们在不同方面的G-code调试和操纵方面的性能，包括检测和修复常见错误以及执行几何变换。我们分析了它们对完整G-code文件的理解的优劣点，以及使用LLMs进行G-code理解的局限性。
</details></li>
</ul>
<hr>
<h2 id="Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks"><a href="#Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks" class="headerlink" title="Computation and Communication Efficient Federated Learning over Wireless Networks"></a>Computation and Communication Efficient Federated Learning over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01816">http://arxiv.org/abs/2309.01816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaonan Liu, Tharmalingam Ratnarajah</li>
<li>For: The paper aims to improve the efficiency and accuracy of federated learning (FL) on edge devices with limited computational capability and wireless resources.* Methods: The proposed FL framework uses partial model pruning and personalization to adapt the model size for each device, reducing both computation and communication overhead. The framework also includes a novel optimization problem to maximize the convergence rate under a latency threshold.* Results: The proposed FL framework achieves a remarkable reduction of approximately 50% in computation and communication latency compared to a scheme only with model personalization.Here is the text in Simplified Chinese:* For: 本 paper 的目的是提高边缘设备上的 Federated learning (FL) 的效率和准确率，这些设备具有有限的计算能力和无线资源。* Methods: 提议的 FL 框架使用 partial model pruning 和个性化，以适应每个设备的模型大小，从而降低计算和通信负担。框架还包括一个优化问题，以最大化在延迟限制下的整合率。* Results: 提议的 FL 框架可以 achieve 约 50% 的计算和通信负担减少，相比只有模型个性化的方案。<details>
<summary>Abstract</summary>
Federated learning (FL) allows model training from local data by edge devices while preserving data privacy. However, the learning accuracy decreases due to the heterogeneity of devices data, and the computation and communication latency increase when updating large scale learning models on devices with limited computational capability and wireless resources. To overcome these challenges, we consider a novel FL framework with partial model pruning and personalization. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine tuned for a specific device, which adapts the model size during FL to reduce both computation and communication overhead and minimize the overall training time, and increases the learning accuracy for the device with non independent and identically distributed (non IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically analyzed. Based on the convergence analysis, an optimization problem is formulated to maximize the convergence rate under a latency threshold by jointly optimizing the pruning ratio and wireless resource allocation. By decoupling the optimization problem and deploying Karush Kuhn Tucker (KKT) conditions, we derive the closed form solutions of pruning ratio and wireless resource allocation. Finally, experimental results demonstrate that the proposed FL framework achieves a remarkable reduction of approximately 50 percents computation and communication latency compared with the scheme only with model personalization.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 允许本地数据进行模型训练，保持数据隐私。然而，由于设备数据的不同性，学习精度下降，并且更新大规模学习模型在设备上的计算和通信延迟增加。为了解决这些挑战，我们考虑了一种新的FL框架，包括部分模型剔除和个性化。这个框架将学习模型分为全球部分，用于学习数据表示，以及个性化部分，用于特定设备进行细化。这些部分可以在FL中适应不同设备的数据，从而降低计算和通信开销，最小化总训练时间，并提高设备上的学习精度。然后，我们对这个FL框架的计算和通信延迟和整体训练时间进行数学分析。基于这个分析，我们形ulated一个优化问题，以 maximize 训练速率在延迟阈值下。通过分离优化问题并应用KKT条件，我们得到了封装形式的解。最后，我们通过实验结果表明，提案的FL框架可以降低约50%的计算和通信延迟。
</details></li>
</ul>
<hr>
<h2 id="DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research"><a href="#DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research" class="headerlink" title="DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"></a>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01808">http://arxiv.org/abs/2309.01808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ynchuang/discoverpath">https://github.com/ynchuang/discoverpath</a></li>
<li>paper_authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Kwei-Herng Lai, Daochen Zha, Ruixiang Tang, Fan Yang, Alfredo Costilla Reyes, Kaixiong Zhou, Xiaoqian Jiang, Xia Hu</li>
<li>For: The paper aims to provide an efficient and intuitive search engine for biomedical research articles, particularly in interdisciplinary fields where diverse terminologies are used.* Methods: The system uses Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts and create a knowledge graph (KG). It also includes a query recommendation system to help users iteratively refine their searches.* Results: The system provides a focused subgraph containing the queried entity and its neighboring nodes, as well as an accessible Graphical User Interface (GUI) that visualizes the KG, query recommendations, and detailed article information, enabling efficient article retrieval and fostering interdisciplinary knowledge exploration.<details>
<summary>Abstract</summary>
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at https://github.com/ynchuang/DiscoverPath.
</details>
<details>
<summary>摘要</summary>
随着学术论文的激增增长，需要更高级的工具来有效检索相关的论文，尤其在跨学科领域中，其中的不同术语可能用于描述类似的研究。传统的关键词基本搜索引擎经常无法帮助用户找到他们不熟悉的特定术语。为解决这个问题，我们提出了一种基于知识图的论文检索引擎，以提高用户在找到相关问题和论文的经验。该系统，名为DiscoverPath，利用命名实体识别（NER）和语言类型（POS）标记来从论文摘要中提取术语和关系，并将其转换为知识图。为了降低信息混乱，DiscoverPath将用户展示一个专注于查询实体和相邻节点的子图，并提供了查询建议系统，允许用户逐步缩小查询范围。系统具有访问ible的图形用户界面，提供了知识图的直观化、查询建议和详细的论文信息，以便有效检索论文，从而推动跨学科知识探索。DiscoverPath的源代码可以在https://github.com/ynchuang/DiscoverPath上下载。
</details></li>
</ul>
<hr>
<h2 id="Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation"><a href="#Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation" class="headerlink" title="Marginalized Importance Sampling for Off-Environment Policy Evaluation"></a>Marginalized Importance Sampling for Off-Environment Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01807">http://arxiv.org/abs/2309.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Katdare, Nan Jiang, Katherine Driggs-Campbell</li>
<li>for: 本研究旨在评估真实世界中RL策略的性能，而不需要真实世界的部署。</li>
<li>methods: 该方法利用了模拟器和现实世界的停挂数据来评估RL策略的性能，基于重要抽象采样（MIS）框架。</li>
<li>results: 该方法可以减少采样的复杂性和预测误差，并且在多种Sim2Sim环境和不同的目标策略下表现良好。此外，该方法还在一个Sim2Real任务中评估了一个7个自由度的机器人臂的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation, requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies without deploying them in the real world. The proposed approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude, thus making it easier to run. We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demonstrate the performance of our algorithm on a Sim2Real task of validating the performance of a 7 DOF robotic arm using offline data along with a gazebo based arm simulator.
</details>
<details>
<summary>摘要</summary>
深度学习（RL）方法通常是样本不fficient，这使得在实际世界中训练和部署RL策略变得困难。即使是一个坚固的策略在模拟中训练，它仍需要在实际世界中进行评估。这篇论文提出了一种新的方法，用于在实际世界中评估代理策略的性能，无需在实际世界中部署策略。该方法利用模拟器和实际世界的停滞数据来评估任何策略，基于多样化重要性抽样（MIS）框架。现有的MIS方法面临两个挑战：（1）巨大的概率差，导致抽样难以控制，（2）间接监督，需要间接地推断抽样比率，从而增加估计误差。我们的方法解决了这两个挑战，通过引入目标策略在模拟器中的占用率作为中间变量，并将抽样比率分解为两个可分别学习的部分。第一部分可以通过直接监督学习，而第二部分的大小较小，因此更容易进行。我们也分析了我们的两步程序的样本复杂度以及误差卷积。此外，我们还通过实验评估我们的方法，在Cartpole、Reacher和Half-Cheetah等Sim2Sim环境中获得了良好的普适性和可靠性。此外，我们还在一个Sim2Real任务中验证了一个7度OF机械臂的性能，使用了卡特底抽象和加兹环境。我们的结果表明，我们的方法可以在不同的Sim2Sim差距、目标策略和在线数据收集策略之间进行广泛的普适性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization"><a href="#Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization" class="headerlink" title="Asymmetric matrix sensing by gradient descent with small random initialization"></a>Asymmetric matrix sensing by gradient descent with small random initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01796">http://arxiv.org/abs/2309.01796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan S. Wind</li>
<li>For:  Matrix sensing, a problem of reconstructing a low-rank matrix from a few linear measurements, is studied.* Methods:  The paper uses factorized gradient descent to solve the overparameterized regression problem of matrix sensing. The key contribution is introducing a continuous differential equation called the “perturbed gradient flow,” which converges quickly to the true target matrix when the perturbation is sufficiently bounded.* Results:  The paper proves that the perturbed gradient flow converges quickly to the true target matrix, and demonstrates the utility of the continuous formulation in simplifying the proofs of matrix sensing with factorized gradient descent.<details>
<summary>Abstract</summary>
We study matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements. It can be formulated as an overparameterized regression problem, which can be solved by factorized gradient descent when starting from a small random initialization.   Linear neural networks, and in particular matrix sensing by factorized gradient descent, serve as prototypical models of non-convex problems in modern machine learning, where complex phenomena can be disentangled and studied in detail. Much research has been devoted to studying special cases of asymmetric matrix sensing, such as asymmetric matrix factorization and symmetric positive semi-definite matrix sensing.   Our key contribution is introducing a continuous differential equation that we call the $\textit{perturbed gradient flow}$. We prove that the perturbed gradient flow converges quickly to the true target matrix whenever the perturbation is sufficiently bounded. The dynamics of gradient descent for matrix sensing can be reduced to this formulation, yielding a novel proof of asymmetric matrix sensing with factorized gradient descent. Compared to directly analyzing the dynamics of gradient descent, the continuous formulation allows bounding key quantities by considering their derivatives, often simplifying the proofs. We believe the general proof technique may prove useful in other settings as well.
</details>
<details>
<summary>摘要</summary>
我们研究矩阵感知问题，即从一些线性测量中重建一个低级矩阵的问题。可以表述为过Parameterized regression问题，可以通过分解 gradient descent 来解决，当从小随机 initialization 开始时。 Linear neural networks 和特别是矩阵感知通过分解 gradient descent serve as 现代机器学习中非 convex 问题的典型模型，其中复杂的现象可以详细分析和研究。许多研究都投入到了异symmetric matrix factorization 和Symmetric positive semi-definite matrix sensing 的特殊情况中。我们的关键贡献在于引入一个名为 $\textit{perturbed gradient flow}$ 的连续偏微分方程。我们证明这个方程在干扰足够小时，它快速收敛到真正的目标矩阵。gradient descent 的动态可以将 reduced to this formulation，得到了一个 novel proof of asymmetric matrix sensing with factorized gradient descent。相比直接分析 gradient descent 的动态，连续形式允许通过考虑其导数来简化证明，从而简化证明。我们认为这种总体技巧可能在其他设置中也有用。
</details></li>
</ul>
<hr>
<h2 id="Composite-federated-learning-with-heterogeneous-data"><a href="#Composite-federated-learning-with-heterogeneous-data" class="headerlink" title="Composite federated learning with heterogeneous data"></a>Composite federated learning with heterogeneous data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01795">http://arxiv.org/abs/2309.01795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaojiao Zhang, Jiang Hu, Mikael Johansson</li>
<li>For: 解决复杂的 Federated Learning（FL）问题* Methods: 使用新的算法，并将 proximal 算符和通信分解，解决客户端漂移问题，每个工作者使用本地更新减少与服务器的通信频率，并只在每个通信循环中传输 $d$-维数组* Results: Proof 算法 linearly converges to a neighborhood of the optimal solution，并在数学实验中证明了对现有方法的超越性<details>
<summary>Abstract</summary>
We propose a novel algorithm for solving the composite Federated Learning (FL) problem. This algorithm manages non-smooth regularization by strategically decoupling the proximal operator and communication, and addresses client drift without any assumptions about data similarity. Moreover, each worker uses local updates to reduce the communication frequency with the server and transmits only a $d$-dimensional vector per communication round. We prove that our algorithm converges linearly to a neighborhood of the optimal solution and demonstrate the superiority of our algorithm over state-of-the-art methods in numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的Algorithm，用于解决复合式 Federated Learning（FL）问题。这个算法利用推导操作和通信分离，处理非滑动正则化，而无需数据相似性假设。此外，每个工作者使用本地更新来减少与服务器之间的通信频率，并仅在通信轮次中传输一个 $d$-维向量。我们证明了我们的算法会线性征向优解的解，并在实验中证明了我们的算法在比 estado-of-the-art 方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction"><a href="#Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction" class="headerlink" title="Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction"></a>Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01788">http://arxiv.org/abs/2309.01788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmh14/Geo-DEG">https://github.com/gmh14/Geo-DEG</a></li>
<li>paper_authors: Minghao Guo, Veronika Thost, Samuel W Song, Adithya Balachandran, Payel Das, Jie Chen, Wojciech Matusik</li>
<li>for: 预测分子性质，以便在物质和药物探索领域中进行更好的预测和设计。</li>
<li>methods: 使用深度学习技术，利用可学习的层次分子agram来生成分子结构，并使用图内傅里勒扩散来预测分子性质。</li>
<li>results: 在小和大数据集上，该方法比一系列基线方法（包括supervised和预训练图内傅里勒网络）表现出色，并且在具有极限数据情况下也能够达到良好的结果。<details>
<summary>Abstract</summary>
The prediction of molecular properties is a crucial task in the field of material and drug discovery. The potential benefits of using deep learning techniques are reflected in the wealth of recent literature. Still, these techniques are faced with a common challenge in practice: Labeled data are limited by the cost of manual extraction from literature and laborious experimentation. In this work, we propose a data-efficient property predictor by utilizing a learnable hierarchical molecular grammar that can generate molecules from grammar production rules. Such a grammar induces an explicit geometry of the space of molecular graphs, which provides an informative prior on molecular structural similarity. The property prediction is performed using graph neural diffusion over the grammar-induced geometry. On both small and large datasets, our evaluation shows that this approach outperforms a wide spectrum of baselines, including supervised and pre-trained graph neural networks. We include a detailed ablation study and further analysis of our solution, showing its effectiveness in cases with extremely limited data. Code is available at https://github.com/gmh14/Geo-DEG.
</details>
<details>
<summary>摘要</summary>
“物料和药物发现领域中预测分子性质的任务是非常重要的。Recent literature中的大量文献表明，深度学习技术在这个领域中的潜在利益很大。然而，在实践中，这些技术面临一个共同的挑战：标注数据受到文献中手动EXTRACTION的成本和劳动密集的实验室试验的限制。在这种情况下，我们提出了一种数据效率的属性预测器，利用可学习的层次分子语法生成分子结构。这种语法induces一个explicit的分子图 geometry，该geometry提供了一个有用的分子结构相似性的准确信息。我们使用图神经扩散来预测属性，并在小型和大型数据集上评估了我们的方法。结果显示，我们的方法在许多基elines上比超越，包括超级vised和预训练图神经网络。我们还提供了细化的折衔分析和进一步的分析，证明我们的解决方案在有限数据情况下的有效性。代码可以在https://github.com/gmh14/Geo-DEG中找到。”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ATMS-Algorithmic-Trading-Guided-Market-Simulation"><a href="#ATMS-Algorithmic-Trading-Guided-Market-Simulation" class="headerlink" title="ATMS: Algorithmic Trading-Guided Market Simulation"></a>ATMS: Algorithmic Trading-Guided Market Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01784">http://arxiv.org/abs/2309.01784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wei, Andrea Coletta, Svitlana Vyetrenko, Tucker Balch</li>
<li>for: 提出了一种metric来衡量市场差异，以便在算法交易（AT）策略的构建中使用市场模拟器。</li>
<li>methods: 提出了一种基于强化学习（RL）的Algorithmic Trading-guided Market Simulation（ATMS）方法，使用策略梯度更新来缓解非导数操作。</li>
<li>results: 通过对半实际市场数据进行广泛的实验，证明了提出的metric的有效性，并示出ATMS生成的市场数据与现实更加相似，并且生成的市场数据中的BUY和SELL量更加均衡。<details>
<summary>Abstract</summary>
The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real market data, we demonstrate the effectiveness of our metric and show that ATMS generates market data with improved similarity to reality compared to the state-of-the-art conditional Wasserstein Generative Adversarial Network (cWGAN) approach. Furthermore, ATMS produces market data with more balanced BUY and SELL volumes, mitigating the bias of the cWGAN baseline approach, where a simple strategy can exploit the BUY/SELL imbalance for profit.
</details>
<details>
<summary>摘要</summary>
“algorithmic trading（AT）策略的有效建立frequently rely on market simulators，但现有方法困难寻味到贸易活动的顺序和动态性。这种工作填补了这一空白，提出了一个市场偏差度量。这个度量测量了 causal effect的差异，由AT agent与市场的交互来评估。我们还提出了基于RL的 Algorithmic Trading-guided Market Simulation（ATMS）。受SeqGAN的启发，ATMS将 simulator形式化为一个随机政策，以考虑贸易活动的顺序性。此外，ATMS使用策略梯度更新，以 circumvent differentiating the proposed metric，该度量包括非分diff运算，如市场中的订单删除。经过对半真实市场数据的广泛实验，我们证明了我们的度量的有效性，并表明ATMS生成的市场数据与现有的conditional Wasserstein Generative Adversarial Network（cWGAN）方法相比，具有更高的真实性。此外，ATMS生成的市场数据具有更好的BUY和SELL量均衡，这可以 mitigate cWGAN基线方法的偏袋，其中一个简单的策略可以通过BUY/SELL偏袋来获利。”
</details></li>
</ul>
<hr>
<h2 id="Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers"><a href="#Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers" class="headerlink" title="Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers"></a>Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01783">http://arxiv.org/abs/2309.01783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Soleimani, Mahsa Bahrami, Mansour Vali<br>for: 这篇研究的目的是为了预测抗癌化学疗法后的肝癌patient的1、3、5年生存率，并且特别针对高度不均衡的1年生存预测任务进行研究。methods: 本研究使用了一些标准的平衡技术来增加True Positive率，包括Edited Nearest Neighbor、Repeated edited nearest neighbor (RENN)、Synthetic Minority Over-sampling Techniques (SMOTE) 等方法，并且将其与树型分类器结合使用。results: 本研究使用了5-fold Cross-Validation方法进行性能评估，结果显示，在高度不均衡的1年生存预测任务中，我们的提案方法（使用Light Gradient Boosting）具有72.30%的sensitivity，而且在3年生存预测任务中，使用RENN和Light Gradient Boosting的结合方法具有80.81%的sensitivity，显示了我们的提案方法在高度不均衡的数据上具有优秀的预测性。<details>
<summary>Abstract</summary>
Background and Objective: Colorectal cancer is a high mortality cancer. Clinical data analysis plays a crucial role in predicting the survival of colorectal cancer patients, enabling clinicians to make informed treatment decisions. However, utilizing clinical data can be challenging, especially when dealing with imbalanced outcomes. This paper focuses on developing algorithms to predict 1-, 3-, and 5-year survival of colorectal cancer patients using clinical datasets, with particular emphasis on the highly imbalanced 1-year survival prediction task. To address this issue, we propose a method that creates a pipeline of some of standard balancing techniques to increase the true positive rate. Evaluation is conducted on a colorectal cancer dataset from the SEER database. Methods: The pre-processing step consists of removing records with missing values and merging categories. The minority class of 1-year and 3-year survival tasks consists of 10% and 20% of the data, respectively. Edited Nearest Neighbor, Repeated edited nearest neighbor (RENN), Synthetic Minority Over-sampling Techniques (SMOTE), and pipelines of SMOTE and RENN approaches were used and compared for balancing the data with tree-based classifiers. Decision Trees, Random Forest, Extra Tree, eXtreme Gradient Boosting, and Light Gradient Boosting (LGBM) are used in this article. Method. Results: The performance evaluation utilizes a 5-fold cross-validation approach. In the case of highly imbalanced datasets (1-year), our proposed method with LGBM outperforms other sampling methods with the sensitivity of 72.30%. For the task of imbalance (3-year survival), the combination of RENN and LGBM achieves a sensitivity of 80.81%, indicating that our proposed method works best for highly imbalanced datasets. Conclusions: Our proposed method significantly improves mortality prediction for the minority class of colorectal cancer patients.
</details>
<details>
<summary>摘要</summary>
背景和目标：肠RECTAL癌是高mortality癌症，临床数据分析对于预测肠RECTAL癌患者的存活率起着关键性的作用，帮助临床医生作出有知识的治疗决策。然而，利用临床数据可能会困难，特别是面临着严重的存在偏好问题。这篇论文关注于使用临床数据来预测肠RECTAL癌患者1-, 3-, 5-年存活率，特别是面临着高度偏好的1-年存活预测任务。为解决这一问题，我们提出了一种方法，该方法包括一系列标准平衡技术，以增加真正正确率。我们在SEER数据库中的肠RECTAL癌数据集上进行评价。方法：数据预处理步骤包括移除缺失值和合并类别。肠RECTAL癌1-年和3-年存活任务的少数类刚占数据集中的10%和20%，分别。我们使用Edited Nearest Neighbor（ENN）、Repeated edited nearest neighbor（RENN）、Synthetic Minority Over-sampling Techniques（SMOTE）以及这些技术的管道来平衡数据，并与树型分类器进行比较。我们使用Decision Trees、Random Forest、Extra Tree、eXtreme Gradient Boosting和Light Gradient Boosting（LGBM）这些分类器。结果：我们使用5-fold Cross-validation方法进行性能评价。在面临着高度偏好的1-年存活任务时，我们提出的方法与LGBM结合的性能最高，即72.30%的sensitivity。对于3-年存活任务，我们的方法的敏感性达到80.81%，表明我们的方法在高度偏好的数据集上工作最好。结论：我们的方法显著提高了肠RECTAL癌患者少数类的存活预测率。
</details></li>
</ul>
<hr>
<h2 id="3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream"><a href="#3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream" class="headerlink" title="3D View Prediction Models of the Dorsal Visual Stream"></a>3D View Prediction Models of the Dorsal Visual Stream</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01782">http://arxiv.org/abs/2309.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Sarch, Hsiao-Yu Fish Tung, Aria Wang, Jacob Prince, Michael Tarr</li>
<li>for: 这个论文是为了检验一种基于3D场景特征记忆的自适应循环神经网络（GRNN）是否可以更好地与脑动力学活动在背部视觉流程中相匹配。</li>
<li>methods: 这个论文使用了自适应基线模型，并将其训练在一个3D特征记忆中，以预测新的摄像头视图。</li>
<li>results: 研究发现，GRNN可以更好地预测背部视觉区域的神经响应，而基线模型则更好地预测前部视觉区域的神经响应。这些结果表明了使用任务相关的模型可以探索视觉流程中的表征差异。<details>
<summary>Abstract</summary>
Deep neural network representations align well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details>
<details>
<summary>摘要</summary>
深度神经网络表示与大脑活动在脊梁视觉流程中吻合得非常好。然而，猴子视觉系统有一个不同的脊梁处理流程，具有不同的功能性。为了测试一个模型可以更好地捕捉3D场景几何结构，我们训练了一个自我超vised geometry-aware循环神经网络（GRNN），用于预测新的摄像头视图。我们比较了GRNN与已知对于脊梁区域具有良好适应性的自我超vised基线模型。我们发现，虽然基线模型更好地捕捉了脊梁区域的大脑活动，但GRNN更好地捕捉了脊梁区域的大脑活动的较大的差异。我们的发现表明，可以使用任务相关的模型来探索不同的视觉流程之间的表达差异。
</details></li>
</ul>
<hr>
<h2 id="Self-concordant-Smoothing-for-Convex-Composite-Optimization"><a href="#Self-concordant-Smoothing-for-Convex-Composite-Optimization" class="headerlink" title="Self-concordant Smoothing for Convex Composite Optimization"></a>Self-concordant Smoothing for Convex Composite Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01781">http://arxiv.org/abs/2309.01781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl">https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl</a></li>
<li>paper_authors: Adeyemi D. Adeoye, Alberto Bemporad</li>
<li>for: 这篇论文是为了研究自适应缓和的减小方法，用于最小化两个凸函数，其中一个是光滑的，另一个可能是不光滑的。</li>
<li>methods: 该方法基于 partial smoothing 技术，只是将一部分不光滑函数缓和了。这个方法的关键优点在于它提供了一种自然的变量 метри选择法和步长选择规则，特别适合 proximal Newton 类算法。此外，它可以高效地处理一些由不光滑函数带来的特殊结构，如 $\ell_1$ 规范和 group-lasso 罚。</li>
<li>results: 作者证明了两种结果算法的本地二次几何速度，即 Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。而 Prox-GGN-SCORE 算法则描述了一种重要的近似过程，它可以减少大部分的计算负担，特别是在过参数机器学习模型和 mini-batch 设置下。实验表明，该方法的效率高于现有方法。<details>
<summary>Abstract</summary>
We introduce the notion of self-concordant smoothing for minimizing the sum of two convex functions: the first is smooth and the second may be nonsmooth. Our framework results naturally from the smoothing approximation technique referred to as partial smoothing in which only a part of the nonsmooth function is smoothed. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove local quadratic convergence rates for two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton (GGN) algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful for overparameterized machine learning models and in the mini-batch settings. Numerical examples on both synthetic and real datasets demonstrate the efficiency of our approach and its superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
我们介绍自步调和缓和运算的概念，用于最小化两个凸函数：第一个是平滑的，第二个可能是不凸的。我们的框架从 partial smoothing 技术获得，仅将部分不凸函数缓和。我们的方法的关键特点在于它具有自然的变数度量选择方法和步长选择规则，特别适合 proximal Newton 类型的算法。此外，我们可以有效地处理特定的非凸函数结构，例如 $\ell_1$ 正则化和群lasso 罚则。我们证明了两个结果的本地二阶径凹降率：Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。Prox-GGN-SCORE 算法显示了一个重要的近似程序，帮助将大部分的计算负担与 inverse Hessian 相关联系。这个近似是对过参量机器学习模型和小批量设定中具有重要的应用。实际例子表明了我们的方法的效率和其优于现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments"><a href="#Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments" class="headerlink" title="Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments"></a>Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01780">http://arxiv.org/abs/2309.01780</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Enouen, Tianshu Sun, Yan Liu</li>
<li>for: 这篇研究旨在提供一个易于实现的算法公平度量度框架，以便在实际应用中评估和改善算法的公平性。</li>
<li>methods: 这篇研究使用了现代的 causal inference 和可读性 Machine Learning 技术，开发了一个测试算法偏见的数据验证方法，并使用了这些技术来开发了一个可解释的机器学习模型，以便将算法的偏见评估为可读性。</li>
<li>results: 这篇研究的结果显示，使用这个数据验证方法和可解释的机器学习模型，可以实现在实际应用中评估和改善算法的公平性，并且可以评估算法的公平性在实际应用中的成本。<details>
<summary>Abstract</summary>
Algorithm fairness has become a central problem for the broad adoption of artificial intelligence. Although the past decade has witnessed an explosion of excellent work studying algorithm biases, achieving fairness in real-world AI production systems has remained a challenging task. Most existing works fail to excel in practical applications since either they have conflicting measurement techniques and/ or heavy assumptions, or require code-access of the production models, whereas real systems demand an easy-to-implement measurement framework and a systematic way to correct the detected sources of bias.   In this paper, we leverage recent advances in causal inference and interpretable machine learning to present an algorithm-agnostic framework (MIIF) to Measure, Interpret, and Improve the Fairness of an algorithmic decision. We measure the algorithm bias using randomized experiments, which enables the simultaneous measurement of disparate treatment, disparate impact, and economic value. Furthermore, using modern interpretability techniques, we develop an explainable machine learning model which accurately interprets and distills the beliefs of a blackbox algorithm. Altogether, these techniques create a simple and powerful toolset for studying algorithm fairness, especially for understanding the cost of fairness in practical applications like e-commerce and targeted advertising, where industry A/B testing is already abundant.
</details>
<details>
<summary>摘要</summary>
“算法公平性已成为人工智能普及的中心问题。过去十年里，我们所有的工作都集中在研究算法偏见上，但在实际应用中实现公平性仍然是一个挑战。现有的大多数工作都具有不兼容的测量技术和假设，或者需要生产模型的代码访问，而实际应用需要一个简单易用的测量框架和一系列系统化的偏见纠正方法。在这篇论文中，我们利用最新的 causal inference 和可读性机器学习来提出一个算法不依赖的框架（MIIF），用于测量、解释和改进算法决策中的公平性。我们使用随机实验测量算法偏见，这些测量可同时测量不同待遇、不同影响和经济价值。此外，我们使用现代可读性技术开发了一个可解释的机器学习模型，可以准确地解释和总结黑obox算法的信念。总之，这些技术创造了一个简单强大的工具集，特别适用于实际应用中的电商和targeted广告等， где行业A/B测试严重。”
</details></li>
</ul>
<hr>
<h2 id="DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data"><a href="#DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data" class="headerlink" title="DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data"></a>DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01779">http://arxiv.org/abs/2309.01779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Zhu, Jingjing Zhang, Shengyun Liu, Xin Wang</li>
<li>for: 这篇论文的目的是解决 Federated Learning (FL) 中存在各个工作者的不同数据分布导致每个工作者更新本地模型而导致“客户端漂移”现象，使得训练速度减慢。</li>
<li>methods: 这篇论文提出了一个名为“振幅基于的自适应组合”（DRAG）算法，利用一个称为“振幅差”的度量来动态地“拖”每个当地更新向参考方向，不需要额外的通信开销。</li>
<li>results: 这篇论文的实验结果显示，DRAG 比前一代算法更有效地控制客户端漂移现象，并且具有较低的训练速度。此外，DRAG 也具有对某些拜尼攻击的抗性。<details>
<summary>Abstract</summary>
Local stochastic gradient descent (SGD) is a fundamental approach in achieving communication efficiency in Federated Learning (FL) by allowing individual workers to perform local updates. However, the presence of heterogeneous data distributions across working nodes causes each worker to update its local model towards a local optimum, leading to the phenomenon known as ``client-drift" and resulting in slowed convergence. To address this issue, previous works have explored methods that either introduce communication overhead or suffer from unsteady performance. In this work, we introduce a novel metric called ``degree of divergence," quantifying the angle between the local gradient and the global reference direction. Leveraging this metric, we propose the divergence-based adaptive aggregation (DRAG) algorithm, which dynamically ``drags" the received local updates toward the reference direction in each round without requiring extra communication overhead. Furthermore, we establish a rigorous convergence analysis for DRAG, proving its ability to achieve a sublinear convergence rate. Compelling experimental results are presented to illustrate DRAG's superior performance compared to state-of-the-art algorithms in effectively managing the client-drift phenomenon. Additionally, DRAG exhibits remarkable resilience against certain Byzantine attacks. By securely sharing a small sample of the client's data with the FL server, DRAG effectively counters these attacks, as demonstrated through comprehensive experiments.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的度量指标，称为“分布度”，用于量化本地梯度和全局参考方向之间的夹角。基于这个指标，我们提出了一种动态调整收集的算法，称为分布度基于的整合（DRAG）算法，不需要额外的通信开销。此外，我们还提供了一种准确的收敛分析，证明DRAG可以实现下线收敛率。实验结果表明，DRAG在管理客户端漂移现象方面表现出色，并且具有remarkable的抗拒迟攻击能力。Here is a word-for-word translation of the text into Simplified Chinese:本文提出了一种新的度量指标，称为“分布度”，用于量化本地梯度和全局参考方向之间的夹角。基于这个指标，我们提出了一种动态调整收集的算法，称为分布度基于的整合（DRAG）算法，不需要额外的通信开销。此外，我们还提供了一种准确的收敛分析，证明DRAG可以实现下线收敛率。实验结果表明，DRAG在管理客户端漂移现象方面表现出色，并且具有remarkable的抗拒迟攻击能力。
</details></li>
</ul>
<hr>
<h2 id="CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence"><a href="#CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence" class="headerlink" title="CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence"></a>CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01778">http://arxiv.org/abs/2309.01778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Carlevaro, Sara Narteni, Fabrizio Dabbene, Marco Muselli, Maurizio Mongelli</li>
<li>for: 这篇论文的目的是提出一种方法来链接确定性预测与可解释机器学习，以确保人工智能系统的可靠性和信任性。</li>
<li>methods: 这篇论文提出了一种方法，即CONFIDERAI，它是一种基于规则的分类模型的新的分数函数，可以利用规则的预测能力和点在规则boundaries的几何位置来进行可解释性预测。此外，论文还使用了技术来控制特征空间中的非确形样本数量，以确保遵循规则的预测结果。</li>
<li>results: 论文通过测试实验在 benchmark 和实际数据集上达到了承诺的结果，例如 DNS 隧道检测和冠军疾病预测。<details>
<summary>Abstract</summary>
Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conformal samples in conformal regions based on support vector data description (SVDD). The overall methodology is tested with promising results on benchmark and real datasets, such as DNS tunneling detection or cardiovascular disease prediction.
</details>
<details>
<summary>摘要</summary>
日常生活中越来越多地受到人工智能的影响，而机器学习算法必须设计为所有人都可靠和信任worthy。特别是计算机科学家认为一个人工智能系统安全和可靠的 Five Pillars：解释性、稳定性、透明度、公平性和隐私。此外，我们还提出了一个第六个基本方面：遵循性，即机器学习人员对系统的预期行为的概率 ensure。在这篇论文中，我们提出了将CONFIDERAI作为新的分数函数，用于规则型模型，该函数利用规则预测的能力和规则边界上的点的几何位置。我们还解决了在特征空间中定义遵循保证的区域的问题，通过控制特征空间中非遵循样本的数量来基于支持向量数据描述（SVDD）。总的来说，我们的方法在评价数据集和实际应用中表现良好。
</details></li>
</ul>
<hr>
<h2 id="Gated-recurrent-neural-networks-discover-attention"><a href="#Gated-recurrent-neural-networks-discover-attention" class="headerlink" title="Gated recurrent neural networks discover attention"></a>Gated recurrent neural networks discover attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01775">http://arxiv.org/abs/2309.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes von Oswald, Maxime Larcher, Angelika Steger, João Sacramento</li>
<li>for: 这种论文旨在探讨现代RNN的发展，以及它们是如何实现自注意的。</li>
<li>methods: 这种RNN使用了两种主要设计元素：线性循环层和循环路径与多项式抑制。</li>
<li>results: 研究发现，通过反编程已经训练过的RNN，gradient descent实际上找到了这种结构。具体来说，研究发现RNN在解决简单的上下文学习任务上表现出与Transformers相同的注意力机制。这些结果表明了多项式相互作用在神经网络中的重要性，以及某些RNN可能在实际中不知不觉地实现注意力。<details>
<summary>Abstract</summary>
Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention under the hood.
</details>
<details>
<summary>摘要</summary>
现代建筑设计已经使得回传神经网络（RNN）在某些序列处理任务上超越了Transformers的表现。这些现代RNN具有一个突出的设计模式：线性循环层通过调用循环路径和多元化闸道相互连接。我们示出了这些RNN具有这两个设计元素可以实现（线性）自我注意，trasformer中的主要建筑块。通过逆引擎一些训练好的RNN，我们发现了 Gradient Descent 在实践中发现了我们的建筑。具体来说，我们对 Transformers 训练好的简单内容学习任务进行了研究，发现了 Gradient Descent 在 RNN 中实现了相同的注意力基本块，与 Transformers 中的注意力运算相同。我们的发现显示了神经网络中的乘法互动的重要性，并表明某些RNN可能在实际中隐藏式地实现注意力。
</details></li>
</ul>
<hr>
<h2 id="ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation"><a href="#ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation" class="headerlink" title="ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation"></a>ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01771">http://arxiv.org/abs/2309.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nastaran Darabi, Maeesha Binte Hashem, Hongyi Pan, Ahmet Cetin, Wilfred Gomes, Amit Ranjan Trivedi</li>
<li>for: 这篇论文旨在提出一种能效的频域运算深度神经网络（DNN）加速方法，以减少延迟和能耗。</li>
<li>methods: 本文提出了一种基于频域运算的神经网络加速方法，使用了频域运算的应用频率对称（WHT）。但是，这种方法对于运算时的 multiply-accumulate（MAC）操作需求增加，从而导致效率下降。本文提出了一种新的类比方法，利用类比频域运算来实现更高效的神经网络加速。</li>
<li>results: 根据论文的数据，这种新方法可以在16×16的交叉板上实现8位数据处理的能效运算，能效率为1602 TOPS&#x2F;W（未使用早期终止策略）和5311 TOPS&#x2F;W（使用早期终止策略，VDD &#x3D; 0.8 V）。<details>
<summary>Abstract</summary>
The edge processing of deep neural networks (DNNs) is becoming increasingly important due to its ability to extract valuable information directly at the data source to minimize latency and energy consumption. Frequency-domain model compression, such as with the Walsh-Hadamard transform (WHT), has been identified as an efficient alternative. However, the benefits of frequency-domain processing are often offset by the increased multiply-accumulate (MAC) operations required. This paper proposes a novel approach to an energy-efficient acceleration of frequency-domain neural networks by utilizing analog-domain frequency-based tensor transformations. Our approach offers unique opportunities to enhance computational efficiency, resulting in several high-level advantages, including array micro-architecture with parallelism, ADC/DAC-free analog computations, and increased output sparsity. Our approach achieves more compact cells by eliminating the need for trainable parameters in the transformation matrix. Moreover, our novel array micro-architecture enables adaptive stitching of cells column-wise and row-wise, thereby facilitating perfect parallelism in computations. Additionally, our scheme enables ADC/DAC-free computations by training against highly quantized matrix-vector products, leveraging the parameter-free nature of matrix multiplications. Another crucial aspect of our design is its ability to handle signed-bit processing for frequency-based transformations. This leads to increased output sparsity and reduced digitization workload. On a 16$\times$16 crossbars, for 8-bit input processing, the proposed approach achieves the energy efficiency of 1602 tera operations per second per Watt (TOPS/W) without early termination strategy and 5311 TOPS/W with early termination strategy at VDD = 0.8 V.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的边处理在日益重要，因为它可以直接从数据源提取有价值信息，以降低延迟和能耗。频域模型压缩，如沃尔什-哈达姆变换（WHT），已被证明为有效的备选方案。然而，频域处理的好处经常被 multiply-accumulate（MAC）操作所抵消。这篇论文提出了一种新的能效加速频域神经网络的方法，利用频域频率基于的tensor变换。我们的方法具有增强计算效率的多种高级优势，包括数组微架构、并行计算、ADC/DAC无需计算、以及增加输出稀热性。我们的方法可以消除转换矩阵中的可训练参数，从而实现更加紧凑的细胞。此外，我们的新数组微架构允许列 wise和行 wise的自适应封装，从而实现了完美的并行计算。此外，我们的方法可以免除ADC/DAC计算，通过对高度量化矩阵-向量乘法进行训练，利用矩阵乘法的参数自由性。另外，我们的方法还可以处理有符号位处理，从而增加输出稀热性和减少数字化工作负荷。在0.8V的电压下，使用16x16十字架，8位输入处理，我们的方法可以达到1602 tera操作每秒每瓦特（TOPS/W）的能效率，没有早期终止策略，以及5311 TOPS/W的能效率，使用早期终止策略。
</details></li>
</ul>
<hr>
<h2 id="On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation"><a href="#On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation" class="headerlink" title="On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation"></a>On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01753">http://arxiv.org/abs/2309.01753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Dohyun Kwon, Steve Wright, Robert Nowak</li>
<li>for: 本文研究了一种基于 penalty 方法的first-order算法，用于解决具有缓冲函数和可能非凸的 BO 问题。</li>
<li>methods: 本文使用 penalty 函数将 BO 问题转化为一个等价的优化问题，然后使用 first-order 算法来解决这个优化问题。</li>
<li>results: 本文提出了一种 $O(\epsilon)$-stationary点的算法，可以在 $O(\epsilon^{-3})$ 和 $O(\epsilon^{-7})$ 的时间复杂度下实现。此外，当 oracle 是 deterministic 时，这种算法可以在单loop 模式下实现，即每个迭代只需要 $O(1)$ 个样本。<details>
<summary>Abstract</summary>
In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\epsilon$-stationary solution by optimizing the penalty formulation with $\sigma = O(\epsilon)$. When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an $\epsilon$-stationary point of the penalty function, using in total $O(\epsilon^{-3})$ and $O(\epsilon^{-7})$ accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully {\it single-loop} manner, i.e., with $O(1)$ samples per iteration, and achieves the improved oracle-complexity of $O(\epsilon^{-3})$ and $O(\epsilon^{-5})$, respectively.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了一类first-order算法来解决层次优化问题（BO），其目标函数是光滑的，但可能不具有凸形的两个水平。我们首先通过罚函数方法来研究BO的地形，具体来说，我们明确地表示罚函数和超对象函数之间的关系，并且确定了罚函数的梯度的计算方法。作为一个第一步，我们通过将上下水平目标函数组合在一个权重加权和罚参数 $\sigma > 0$ 中来研究BO的地形。在这个过程中，我们发现了罚函数和超对象函数之间的强联系，并且确定了罚函数的梯度的计算方法。作为下一步，我们视罚函数为$O(\sigma)$-近似于原始BO的问题，并提出了一种first-order算法来找到一个$\epsilon$-稳定解。当下水平问题具有小误差 proximal 障碍函数（EB）条件时，我们提出了一种first-order算法，可以在$O(\epsilon^{-3})$ 和 $O(\epsilon^{-7})$ 访问权重函数时 converge to an $\epsilon$-稳定点。在更进一步的假设下，我们展示了这种算法可以在单 Loop 模式下实现，即在每个迭代中只需要 $O(1)$ 样本，并且可以达到改进的 Oracle 复杂度 $O(\epsilon^{-3})$ 和 $O(\epsilon^{-5})$。
</details></li>
</ul>
<hr>
<h2 id="Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models"><a href="#Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models" class="headerlink" title="Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models"></a>Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01745">http://arxiv.org/abs/2309.01745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Kohl, Li-Wei Chen, Nils Thuerey</li>
<li>for:  simulate turbulent flows and achieve rollout stability in machine learning-based solvers</li>
<li>methods:  use an autoregressive rollout based on conditional diffusion models</li>
<li>results:  improved rollout stability without compromising sample quality, and successful generalization to flow parameters beyond the training regime<details>
<summary>Abstract</summary>
Simulating turbulent flows is crucial for a wide range of applications, and machine learning-based solvers are gaining increasing relevance. However, achieving stability when generalizing to longer rollout horizons remains a persistent challenge for learned PDE solvers. We address this challenge by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models. We show that this approach offers clear advantages in terms of rollout stability compared to other learned baselines. Remarkably, these improvements in stability are achieved without compromising the quality of generated samples, and our model successfully generalizes to flow parameters beyond the training regime. Additionally, the probabilistic nature of the diffusion approach allows for inferring predictions that align with the statistics of the underlying physics. We quantitatively and qualitatively evaluate the performance of our method on a range of challenging scenarios, including incompressible and transonic flows, as well as isotropic turbulence.
</details>
<details>
<summary>摘要</summary>
模拟湍流是广泛应用的关键任务，机器学习基于的解决方案在不断增长。然而，在扩展到更长的执行时间范围时，学习得到的稳定性问题仍然是持续的挑战。我们解决这个问题 by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models.我们发现这种方法可以在扩展到更长的执行时间范围时提供明显的稳定性优势，而无需妥协生成样本的质量。此外，Diffusion方法的 probabilistic nature 允许我们对下游物理统计进行预测。我们对一系列复杂的场景进行了量化和质量地评估，包括不压缩和超音速流动，以及iso tropic turbulence。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports"><a href="#An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports" class="headerlink" title="An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"></a>An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01740">http://arxiv.org/abs/2309.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Dack, Lorenzo Brigato, Matthew McMurray, Matthias Fontanellaz, Thomas Frauenfelder, Hanno Hoppe, Aristomenis Exadaktylos, Thomas Geiser, Manuela Funke-Chambour, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou</li>
<li>for: 该研究利用医院的不结构数据，通过计算机断层成像（CT）扫描来自动诊断 COVID-19。</li>
<li>methods: 该研究使用了多种零shot模型，通过对比视觉语言学习来实现多个标签分类。</li>
<li>results: 该研究表明，采用这些零shot模型可以帮助放射学专业人员更好地诊断肺动脉堵塞和识别肺部细节，如玻璃化肿坏和吸收。<details>
<summary>Abstract</summary>
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis community by addressing some challenges associated with unstructured data and fine-grained multi-label classification.
</details>
<details>
<summary>摘要</summary>
“covid-19大流行导致了庞大的不结构数据存储，包括骨科报告。以前的自动诊断 covid-19 研究主要集中在X射线图像上，尽管它们的精度比 computed tomography (CT) 扫描低。在这项工作中，我们利用医院的不结构数据，并利用 CT 扫描提供的细节来实现零shot多标签分类，基于对比性视觉语言学习。与人类专家合作，我们研究了多种零shot模型是否能够帮助 radiologist 诊断肺动脉塞栓和识别复杂肺脏的细节，如云母膜和肺扩散。我们的实验分析提供了对这类细节任务的可能解决方案的概述，这些任务在医疗多Modal预训练文献中尚未得到了 sufficient 的注意。我们的调查承诺未来医学图像分析社区的进步，解决一些不结构数据和细节多标签分类的挑战。”
</details></li>
</ul>
<hr>
<h2 id="Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment"><a href="#Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment" class="headerlink" title="Hybrid data driven&#x2F;thermal simulation model for comfort assessment"></a>Hybrid data driven&#x2F;thermal simulation model for comfort assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01734">http://arxiv.org/abs/2309.01734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Barbedienne, Sara Yasmine Ouerk, Mouadh Yagoubi, Hassan Bouia, Aurelie Kaemmerlen, Benoit Charrier</li>
<li>for: 提高物理模型的速度和质量，但需要大量数据，而获取数据往往困难和成本高。</li>
<li>methods: 将真实数据与模拟数据杂合以实现 thermal comfort 预测。使用 Modelica Language 进行模拟。比较不同机器学习方法的 benchmarking 研究。</li>
<li>results: 实现了一个 F1 分数为 0.999 的Random Forest 模型，结果看好。<details>
<summary>Abstract</summary>
Machine learning models improve the speed and quality of physical models. However, they require a large amount of data, which is often difficult and costly to acquire. Predicting thermal comfort, for example, requires a controlled environment, with participants presenting various characteristics (age, gender, ...). This paper proposes a method for hybridizing real data with simulated data for thermal comfort prediction. The simulations are performed using Modelica Language. A benchmarking study is realized to compare different machine learning methods. Obtained results look promising with an F1 score of 0.999 obtained using the random forest model.
</details>
<details>
<summary>摘要</summary>
机器学习模型可以提高物理模型的速度和质量，但它们需要大量数据，而这些数据往往困难和成本高昂地获得。预测冷暖舒适性需要控制环境，参与者具有不同特征（年龄、性别、...)。这篇论文提议将实际数据与 simulate 数据相结合以预测冷暖舒适性。模拟使用 Modelica 语言进行。实现了不同机器学习方法的比较研究。获得的结果很有 promise，使用随机森林模型获得 F1 分数为 0.999。Note:* “Machine learning models”in the original text is translated as “机器学习模型”in Simplified Chinese, which is a common way to refer to machine learning models in China.* “thermal comfort”in the original text is translated as “冷暖舒适性”in Simplified Chinese, which is a common way to refer to thermal comfort in China.* “Participants”in the original text is translated as “参与者”in Simplified Chinese, which is a common way to refer to participants in scientific studies in China.* “benchmarking study”in the original text is translated as “比较研究”in Simplified Chinese, which is a common way to refer to benchmarking studies in China.* “obtained results”in the original text is translated as “获得的结果”in Simplified Chinese, which is a common way to refer to results in China.* “F1 score”in the original text is translated as “ F1 分数”in Simplified Chinese, which is a common way to refer to F1 score in China.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning"><a href="#Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning" class="headerlink" title="Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning"></a>Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01730">http://arxiv.org/abs/2309.01730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Kalntis, George Iosifidis, Fernando A. Kuipers</li>
<li>for: 提高Open Radio Access Network系统中资源分配的效率，使得运营商能够更好地利用资源，降低成本，提高系统的灵活性和可替性。</li>
<li>methods: 提出了一种在线学习算法，能够在不精准环境下快速适应变化，并兼顾效率和能源消耗之间的trade-off。同时，提出了一种元学习方案，可以在不同的环境下选择最佳的算法，提高整体系统的多样性和效果。</li>
<li>results: 经过实验证明，提出的算法可以在不同的环境下具有低于线性偏差的性能，并且可以在实际数据上实现资源分配的效率提高，最高可以达64.5%的能源减少。<details>
<summary>Abstract</summary>
Open Radio Access Network systems, with their virtualized base stations (vBSs), offer operators the benefits of increased flexibility, reduced costs, vendor diversity, and interoperability. Optimizing the allocation of resources in a vBS is challenging since it requires knowledge of the environment, (i.e., "external'' information), such as traffic demands and channel quality, which is difficult to acquire precisely over short intervals of a few seconds. To tackle this problem, we propose an online learning algorithm that balances the effective throughput and vBS energy consumption, even under unforeseeable and "challenging'' environments; for instance, non-stationary or adversarial traffic demands. We also develop a meta-learning scheme, which leverages the power of other algorithmic approaches, tailored for more "easy'' environments, and dynamically chooses the best performing one, thus enhancing the overall system's versatility and effectiveness. We prove the proposed solutions achieve sub-linear regret, providing zero average optimality gap even in challenging environments. The performance of the algorithms is evaluated with real-world data and various trace-driven evaluations, indicating savings of up to 64.5% in the power consumption of a vBS compared with state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Softmax-Bias-Correction-for-Quantized-Generative-Models"><a href="#Softmax-Bias-Correction-for-Quantized-Generative-Models" class="headerlink" title="Softmax Bias Correction for Quantized Generative Models"></a>Softmax Bias Correction for Quantized Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01729">http://arxiv.org/abs/2309.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, Markus Nagel</li>
<li>for: 提高逻辑推理模型在资源有限的边缘设备上的运行时间和能耗效率。</li>
<li>methods:  investigate the source of the softmax sensitivity to quantization and propose an offline bias correction technique to improve the quantizability of softmax without additional compute during deployment.</li>
<li>results:  achieved significant accuracy improvement for 8-bit quantized softmax on stable diffusion v1.5 and 125M-size OPT language model.<details>
<summary>Abstract</summary>
Post-training quantization (PTQ) is the go-to compression technique for large generative models, such as stable diffusion or large language models. PTQ methods commonly keep the softmax activation in higher precision as it has been shown to be very sensitive to quantization noise. However, this can lead to a significant runtime and power overhead during inference on resource-constraint edge devices. In this work, we investigate the source of the softmax sensitivity to quantization and show that the quantization operation leads to a large bias in the softmax output, causing accuracy degradation. To overcome this issue, we propose an offline bias correction technique that improves the quantizability of softmax without additional compute during deployment, as it can be readily absorbed into the quantization parameters. We demonstrate the effectiveness of our method on stable diffusion v1.5 and 125M-size OPT language model, achieving significant accuracy improvement for 8-bit quantized softmax.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>Post-training quantization（PTQ）是大型生成模型的压缩技术，如稳定扩散或大语言模型。PTQ方法通常保留高精度的软 макс激活函数，因为它们对压缩噪声非常敏感。然而，这可能导致在资源有限的边缘设备上的 runtime 和功耗开销增加。在这种情况下，我们调查了软 макс对压缩的敏感性的原因，并证明了压缩操作导致软 макс输出的大跃迁偏移，从而导致准确性下降。为解决这个问题，我们提议了一种离线偏移修正技术，可以在部署时对软 макс进行不添加计算的偏移，从而提高压缩的可靠性。我们在稳定扩散 v1.5 和 125M 大小的 OPT 语言模型上证明了我们的方法的效果，实现了8位压缩软 макс的准确性提升。
</details></li>
</ul>
<hr>
<h2 id="Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction"><a href="#Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction" class="headerlink" title="Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"></a>Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01715">http://arxiv.org/abs/2309.01715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/20001LastOrder/Taxonomy-GPT">https://github.com/20001LastOrder/Taxonomy-GPT</a></li>
<li>paper_authors: Boqi Chen, Fandi Yi, Dániel Varró</li>
<li>for: 本研究旨在提出一种通用框架，以便自动生成符合结构约束的分类系统。</li>
<li>methods: 本研究使用了适当的用户输入（称为“提示”），以引导大型自然语言处理器（LLM）在多种NLP任务中进行自动分类。</li>
<li>results: 研究结果显示，无需显式重新训练，提示方法可以更高效地生成符合结构约束的分类系统，尤其是当训练数据集较小时。但是， Fine-tuning方法生成的分类系统可以轻松地进行后处理，以满足所有约束。<details>
<summary>Abstract</summary>
Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details>
<details>
<summary>摘要</summary>
TAXONOMIES 代表了实体之间的层次关系，通常在软件模型化和自然语言处理（NLP）活动中使用。它们通常受到一组结构约束的限制。然而，手动构建税onomy可以耗时、不充分和成本高。latest studies of large language models（LLMs）have shown that appropriate user inputs（called prompting）can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit（re）training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our results reveal the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise"><a href="#On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise" class="headerlink" title="On the Robustness of Post-hoc GNN Explainers to Label Noise"></a>On the Robustness of Post-hoc GNN Explainers to Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01706">http://arxiv.org/abs/2309.01706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin</li>
<li>for: 本研究旨在评估post-hoc graph neural network（GNN）解释器在不同水平的标签噪声下的可靠性。</li>
<li>methods: 本研究使用了多种post-hoc GNN解释器，包括LIME、SHAP、TreeExplainer等，对受损标签进行评估。</li>
<li>results: 研究发现，post-hoc GNN解释器受标签噪声的影响，导致解释质量下降。对于不同水平的标签噪声，解释器的效果不同。<details>
<summary>Abstract</summary>
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
</details>
<details>
<summary>摘要</summary>
提出为解决图 neural network (GNN) 的内在黑盒限制，后续 GNN 解释器寻求提供精准和深入的 GNN 行为解释。 DESPITE 的最近学术和工业上的进展，后续 GNN 解释器对听力噪声的Robustness 还未得到探讨。为了bridging这个差距，我们进行了系统性的实验研究，评估不同后续 GNN 解释器在不同水平的标签噪声下的效果。我们的结果显示了以下几个关键发现：一、后续 GNN 解释器对标签扰动很敏感。二、即使标签噪声非常低，也会对解释质量造成很大的干扰。三、随着噪声水平的增加，解释效果可以逐渐恢复。
</details></li>
</ul>
<hr>
<h2 id="Robust-Online-Classification-From-Estimation-to-Denoising"><a href="#Robust-Online-Classification-From-Estimation-to-Denoising" class="headerlink" title="Robust Online Classification: From Estimation to Denoising"></a>Robust Online Classification: From Estimation to Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01698">http://arxiv.org/abs/2309.01698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changlong Wu, Ananth Grama, Wojciech Szpankowski</li>
<li>for: 本研究探讨在含有噪音标签的线上分类问题中，learner如何适应噪音标签的不确定性。</li>
<li>methods: 本研究使用了一种通用kernel模型来模型噪音机制，其中，对于任何特征标签对，提供了一组知道的分布集合来生成噪音标签。learner在每个时间步骤上根据实际特征标签对 Observation selected an unknown distribution from the distribution set specified by the kernel，并生成噪音标签。learner then makes a prediction based on the actual features and noisy labels observed thus far, and incurs loss $1$ if the prediction differs from the underlying truth (and $0$ otherwise).</li>
<li>results: 本研究表明，对于许多自然的噪音kernel，选择性的特征和finite类别labeling函数，minimax risk可以在无限时间horizon和对数几何函数类型的labeling函数class上紧紧约束。此外，我们还扩展了这些结果到无限类和随机生成的特征上，通过stoquastic sequential covering的概念。本研究的结果超过和Ben-David et al. (2009)的发现，并提供了对这些结论的直观理解，通过一种novel reduction to online conditional distribution estimation。<details>
<summary>Abstract</summary>
We study online classification in the presence of noisy labels. The noise mechanism is modeled by a general kernel that specifies, for any feature-label pair, a (known) set of distributions over noisy labels. At each time step, an adversary selects an unknown distribution from the distribution set specified by the kernel based on the actual feature-label pair, and generates the noisy label from the selected distribution. The learner then makes a prediction based on the actual features and noisy labels observed thus far, and incurs loss $1$ if the prediction differs from the underlying truth (and $0$ otherwise). The prediction quality is quantified through minimax risk, which computes the cumulative loss over a finite horizon $T$. We show that for a wide range of natural noise kernels, adversarially selected features, and finite class of labeling functions, minimax risk can be upper bounded independent of the time horizon and logarithmic in the size of labeling function class. We then extend these results to inifinite classes and stochastically generated features via the concept of stochastic sequential covering. Our results extend and encompass findings of Ben-David et al. (2009) through substantial generality, and provide intuitive understanding through a novel reduction to online conditional distribution estimation.
</details>
<details>
<summary>摘要</summary>
我们研究在噪声标签下的在线分类。噪声机制由一个通用的kernel模型，每个特征标签对的(已知)Set of 噪声标签的分布。在每个时间步骤，一个反对手选择一个未知的分布从特定的kernel中，并生成噪声标签。学习者根据实际特征和观测到的噪声标签进行预测，并且计算损失。我们表示，对于广泛的自然噪声kernel、反对手选择的特征和标签分类函数的Finite类型，最小最大风险可以独立于时间框架和对数减少。我们然后扩展这些结果到无限类和随机生成的特征上，通过随机掩码覆盖概念。我们的结果超越和涵盖了Ben-David等人（2009）的发现，并提供了更加广泛的通用性和INTUITIVEunderstanding通过一种novel的减少到在线条件分布估计。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Polynomial-Chaos-Expansions"><a href="#Physics-Informed-Polynomial-Chaos-Expansions" class="headerlink" title="Physics-Informed Polynomial Chaos Expansions"></a>Physics-Informed Polynomial Chaos Expansions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01697">http://arxiv.org/abs/2309.01697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Novák, Himanshu Sharma, Michael D. Shields</li>
<li>for: This paper presents a novel methodology for constructing physics-informed polynomial chaos expansions (PCE) that combines conventional experimental design with additional constraints from the physics of the model.</li>
<li>methods: The proposed method combines differential equations and boundary conditions to construct physically constrained PCEs, which are more accurate and efficient than standard sparse PCEs.</li>
<li>results: The proposed method leads to superior accuracy of the approximation and does not add significant computational burden. Additionally, the constrained PCEs can be easily applied for uncertainty quantification through analytical post-processing of a reduced PCE filtering out the influence of all deterministic space-time variables.<details>
<summary>Abstract</summary>
Surrogate modeling of costly mathematical models representing physical systems is challenging since it is typically not possible to create a large experimental design. Thus, it is beneficial to constrain the approximation to adhere to the known physics of the model. This paper presents a novel methodology for the construction of physics-informed polynomial chaos expansions (PCE) that combines the conventional experimental design with additional constraints from the physics of the model. Physical constraints investigated in this paper are represented by a set of differential equations and specified boundary conditions. A computationally efficient means for construction of physically constrained PCE is proposed and compared to standard sparse PCE. It is shown that the proposed algorithms lead to superior accuracy of the approximation and does not add significant computational burden. Although the main purpose of the proposed method lies in combining data and physical constraints, we show that physically constrained PCEs can be constructed from differential equations and boundary conditions alone without requiring evaluations of the original model. We further show that the constrained PCEs can be easily applied for uncertainty quantification through analytical post-processing of a reduced PCE filtering out the influence of all deterministic space-time variables. Several deterministic examples of increasing complexity are provided and the proposed method is applied for uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
实验设计资料不够时，代理模型化的成本模型表示物理系统具有挑战。因此，将拓扑受限于物理模型所知道的物理法则。本文提出了一种新的物理受限的多项几何函数扩展（PCE）的建构方法，它结合了传统实验设计和模型物理法则的限制。这些物理限制通过一系列的数学方程和边界条件表示。提出了一种计算效率高的建构方法，并与标准的罕见PCE进行比较。结果显示，提议的算法可以实现更高精度的拓扑，而且不增加计算负担。尽管主要目的是将数据和物理限制结合起来，但我们显示了可以从数学方程和边界条件alone constructor physically constrained PCE，不需要评估原始模型。此外，我们还显示了受限PCE可以轻松地应用于不确定量化，通过对几何函数进行分析后处理，排除所有决定性空间时间变数。本文提供了一些具有不同复杂度的几何示例，并应用了不确定量化。
</details></li>
</ul>
<hr>
<h2 id="No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets"><a href="#No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets" class="headerlink" title="No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets"></a>No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01694">http://arxiv.org/abs/2309.01694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Brigato, Stavroula Mougiakakou</li>
<li>for: 提高小训练集图像分类任务的性能</li>
<li>methods: 使用不同规则强制平衡和权重衰减来提高模型性能</li>
<li>results: 在1%的原始CIFAR-10训练集和ciFAIR-10测试集上达到66.5%的测试精度，与当前最佳方法相当<details>
<summary>Abstract</summary>
Solving image classification tasks given small training datasets remains an open challenge for modern computer vision. Aggressive data augmentation and generative models are among the most straightforward approaches to overcoming the lack of data. However, the first fails to be agnostic to varying image domains, while the latter requires additional compute and careful design. In this work, we study alternative regularization strategies to push the limits of supervised learning on small image classification datasets. In particular, along with the model size and training schedule scaling, we employ a heuristic to select (semi) optimal learning rate and weight decay couples via the norm of model parameters. By training on only 1% of the original CIFAR-10 training set (i.e., 50 images per class) and testing on ciFAIR-10, a variant of the original CIFAR without duplicated images, we reach a test accuracy of 66.5%, on par with the best state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉中解决小训练集图像分类任务仍然是一个开放的挑战。使用激进数 augmentation 和生成模型是最直接的方法来缓解不足的数据问题，但是前者不具备适应不同图像领域的特性，而后者需要额外的计算资源和精心的设计。在这项工作中，我们研究了不同的常规化策略，以推动超越小训练集图像分类任务的深度学习。特别是，我们与模型大小和训练计划的扩大相关，采用一种恰当的学习率和权重衰减对的选择方法，通过使用模型参数的 нор方法来选择最佳的学习率和权重衰减。通过使用原始 CIFAR-10 训练集的 1%（即每个类50张图像）进行训练，并测试在 ciFAIR-10 上，一种不含重复图像的 CIFAR-10 变体，我们达到了66.5%的测试准确率，与现有最佳方法相当。
</details></li>
</ul>
<hr>
<h2 id="Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning"><a href="#Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning" class="headerlink" title="Blind Biological Sequence Denoising with Self-Supervised Set Learning"></a>Blind Biological Sequence Denoising with Self-Supervised Set Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01670">http://arxiv.org/abs/2309.01670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Ng, Ji Won Park, Jae Hyeon Lee, Ryan Lewis Kelly, Stephen Ra, Kyunghyun Cho</li>
<li>for: 本研究旨在提高高通量DNA测序数据的精度，尤其是对于短序列的识别和修复。</li>
<li>methods: 该研究提出了一种新的自动准备方法，即Self-Supervised Set Learning（SSSL），可以在不直接观察干净源序列标签的情况下，对短序列进行自动准备。该方法通过聚集短序列在嵌入空间中，并估计短序列的集合嵌入点，从而修复短序列中的错误。</li>
<li>results: 实验结果表明，SSSL方法可以在模拟的长读DNA数据上减少小读的错误率，对于长读的错误率也有所下降。在实际的抗体序列数据上，SSSL方法也有较好的性能，特别是在小读上，这些读取了大约60%的测试集。<details>
<summary>Abstract</summary>
Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy observations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the midpoint of the subreads in both the latent and sequence spaces. This set embedding represents the "average" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of $\leq 6$ subreads with 17% fewer errors and large reads of $>6$ subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.
</details>
<details>
<summary>摘要</summary>
生物序列分析 rely 于能够去噪掉高通量长读平台输出的不精准序列。我们考虑一种常见的设定，在其中短序列被重复读取多次，生成多个噪声观测。尝试使用对 alignment 的方法去噪掉这些噪声观测可能会失败，当有太少的噪声观测或者错误率太高时。在这篇论文中，我们提出了一种新的方法，即 Self-Supervised Set Learning（SSSL）。SSSL 方法将噪声观测集结集中在一个抽象空间中，并估算这些噪声观测的集中点作为latent space和序列空间中的midpoint。这个集中点表示“平均”的噪声观测，可以被解码成一个估算clean sequence的预测。在对模拟的长读DNA数据进行实验时，SSSL方法可以将小读数据（≤6个噪声观测）和大读数据（>6个噪声观测）中的错误数量降低17%和8%。在一个实际的抗体序列数据集上，SSSL方法也在两个自我超vised metric上进行改进，特别是在difficult small reads中，这些读数据占总测试集的60%以上。通过准确地去噪掉这些读数据，SSSL方法可以更好地实现高通量DNA测序数据的下游科学应用。
</details></li>
</ul>
<hr>
<h2 id="Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data"><a href="#Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data" class="headerlink" title="Robust penalized least squares of depth trimmed residuals regression for high-dimensional data"></a>Robust penalized least squares of depth trimmed residuals regression for high-dimensional data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01666">http://arxiv.org/abs/2309.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Zuo</li>
<li>for: This paper aims to address the challenges of analyzing high-dimensional data in the big-data era, specifically the issues of outliers and contaminated points.</li>
<li>methods: The paper proposes a novel robust penalized regression method based on the least sum of squares of depth trimmed residuals to handle these challenges.</li>
<li>results: The proposed method outperforms some leading competitors in estimation and prediction accuracy in simulated and real data cases.<details>
<summary>Abstract</summary>
Challenges with data in the big-data era include (i) the dimension $p$ is often larger than the sample size $n$ (ii) outliers or contaminated points are frequently hidden and more difficult to detect. Challenge (i) renders most conventional methods inapplicable. Thus, it attracts tremendous attention from statistics, computer science, and bio-medical communities. Numerous penalized regression methods have been introduced as modern methods for analyzing high-dimensional data. Disproportionate attention has been paid to the challenge (ii) though. Penalized regression methods can do their job very well and are expected to handle the challenge (ii) simultaneously. Most of them, however, can break down by a single outlier (or single adversary contaminated point) as revealed in this article.   The latter systematically examines leading penalized regression methods in the literature in terms of their robustness, provides quantitative assessment, and reveals that most of them can break down by a single outlier. Consequently, a novel robust penalized regression method based on the least sum of squares of depth trimmed residuals is proposed and studied carefully. Experiments with simulated and real data reveal that the newly proposed method can outperform some leading competitors in estimation and prediction accuracy in the cases considered.
</details>
<details>
<summary>摘要</summary>
大数据时代中的数据分析挑战包括（i）样本大小n比维度p更小（ii）外围点或杂质点更难于检测。挑战（i）使得大多数传统方法无法应用。这引起了统计、计算机科学和生物医学领域的极大关注。众所周知，许多现代高维数据分析方法已经被引入。虽然挑战（ii）获得了过多的关注，但是抑约方法可以很好地处理挑战（ii）。然而，大多数方法都可以受到单个外围点（或单个杂质点）的影响，这在本文中得到了证明。为了解决这个问题，我们提出了一种基于深度剔除差异的新robust抑约方法，并且仔细研究了这种方法的性能。实验表明，新提出的方法可以在考虑的情况下超越一些领先竞争对手的准确性和预测性。
</details></li>
</ul>
<hr>
<h2 id="Locally-Stationary-Graph-Processes"><a href="#Locally-Stationary-Graph-Processes" class="headerlink" title="Locally Stationary Graph Processes"></a>Locally Stationary Graph Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01657">http://arxiv.org/abs/2309.01657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Canbolat, Elif Vural</li>
<li>for: 本文旨在提出一种基于不规则网络结构的本地站立性图像处理模型（LSGP），以扩展传统的全球站立性模型。</li>
<li>methods: 本文提出一种算法来计算LSGP模型，并研究将LSGP模型当地简化为WSS进程。</li>
<li>results: 实验表明，提出的进程模型可以提供与现状最佳竞争的准确信号表示。<details>
<summary>Abstract</summary>
Stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. While most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. In this work, we propose a locally stationary graph process (LSGP) model that aims to extend the classical concept of local stationarity to irregular graph domains. We characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. We propose an algorithm for computing LSGP models from realizations of the process, and also study the approximation of LSGPs locally with WSS processes. Experiments on signal interpolation problems show that the proposed process model provides accurate signal representations competitive with the state of the art.
</details>
<details>
<summary>摘要</summary>
Stationary graph process模型通常用于分析和推断非 régulière网络拓扑上的数据集。大多数现有方法使用全球静态过程模型来表示图像信号，但在实际问题中，过程特性可能在不同地方Graph上具有本地差异。在这种情况下，我们提议使用本地静态图像过程（LSGP）模型，以扩展传统的本地静态性概念到不规则图像领域。我们通过表示过程的总体组合，其中每个组件过程的承袭程度在图像上变化平滑来定义本地静态性。我们提出了计算LSGP模型的算法，以及对LSGP进行本地简化的WSS过程的研究。实验表明，我们的过程模型可以与当前状态革命竞争。
</details></li>
</ul>
<hr>
<h2 id="Which-algorithm-to-select-in-sports-timetabling"><a href="#Which-algorithm-to-select-in-sports-timetabling" class="headerlink" title="Which algorithm to select in sports timetabling?"></a>Which algorithm to select in sports timetabling?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03229">http://arxiv.org/abs/2309.03229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertomrosati/sa4stt">https://github.com/robertomrosati/sa4stt</a></li>
<li>paper_authors: David Van Bulck, Dries Goossens, Jan-Patrick Clarner, Angelos Dimitsas, George H. G. Fonseca, Carlos Lamas-Fernandez, Martin Mariusz Lester, Jaap Pedersen, Antony E. Phillips, Roberto Maria Rosati</li>
<li>for: 这个论文的目的是提供体育调度问题的时间表，并分析八种现有的算法的优劣。</li>
<li>methods: 这篇论文使用机器学习技术开发了一个算法选择系统，可以根据体育调度问题的特点来预测哪一个算法会最好。</li>
<li>results: 根据大规模的 computation experiments，这篇论文获得了关于八种算法的性能分析和体育调度问题的特点重要性的深入了解。<details>
<summary>Abstract</summary>
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational experiments involving about 50 years of CPU time on more than 500 newly generated problem instances.
</details>
<details>
<summary>摘要</summary>
任何体育竞赛都需要一份时间表，确定队伍在哪里和何时相遇。最近的国际时间安排竞赛（ITC2021）显示，虽然可以开发通用算法，但每个算法在具体的问题实例上表现会很不同。这篇论文提供了体育时间安排的实例空间分析，从而获得了八种现代算法的强大视角和缺点。基于机器学习技术，我们提出了一个算法选择系统，可以根据体育时间安排问题实例的特点预测最佳的算法。此外，我们还确定了影响这种预测的重要特征，从而提供了算法表现的理解和提高建议。最后，我们评估了实际难度的问题实例。我们的结果基于超过50年的CPU时间和更多于500个新生成的问题实例进行了大规模的计算实验。
</details></li>
</ul>
<hr>
<h2 id="Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis"><a href="#Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis" class="headerlink" title="Relay Diffusion: Unifying diffusion process across resolutions for image synthesis"></a>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03350">http://arxiv.org/abs/2309.03350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion">https://github.com/THUDM/RelayDiffusion</a></li>
<li>paper_authors: Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang</li>
<li>for: 这篇论文主要针对高分辨率图像生成中的散射模型问题进行研究，以实现高品质的图像生成。</li>
<li>methods: 本文提出了一种名为遮盾散射模型（RDM），它可以将低分辨率图像或噪声转换为等效的高分辨率图像，并且可以继续进行散射过程，无需重新从噪声或低分辨率条件开始。</li>
<li>results: RDM在CelebA-HQ和ImageNet 256$\times$256上 achieved state-of-the-art FID和sFID，大幅超过了前一些工作，如ADM、LDM和DiT。所有代码和检查点都可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion%7D">https://github.com/THUDM/RelayDiffusion}</a> 上取得。<details>
<summary>Abstract</summary>
Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
</details>
<details>
<summary>摘要</summary>
Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at <https://github.com/THUDM/RelayDiffusion>.Here's the translation in Traditional Chinese:Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at <https://github.com/THUDM/RelayDiffusion>.
</details></li>
</ul>
<hr>
<h2 id="Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD"><a href="#Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD" class="headerlink" title="Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD"></a>Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01640">http://arxiv.org/abs/2309.01640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etay Livne, Gal Kaplun, Eran Malach, Shai Shalev-Schwatz</li>
<li>for: 提高 Stochastic Gradient Descent (SGD) 训练机器学习模型的效率，尤其是对于大规模数据存储在云端的情况。</li>
<li>methods: 提出了一种在线洗混算法 called CorgiPile，可以大幅提高数据访问效率，但是会付出一定的性能损失，尤其是对于 homogeneous shards (例如视频数据)。</li>
<li>results: 提出了一种新的两步 partial data shuffling 策略，将 offline 的 CorgiPile 方法和 online 的 SGD 方法结合在一起，可以同时保持数据访问效率和性能。提供了对方法的完整理论分析，并通过实验结果证明其实际优势。<details>
<summary>Abstract</summary>
When using Stochastic Gradient Descent (SGD) for training machine learning models, it is often crucial to provide the model with examples sampled at random from the dataset. However, for large datasets stored in the cloud, random access to individual examples is often costly and inefficient. A recent work \cite{corgi}, proposed an online shuffling algorithm called CorgiPile, which greatly improves efficiency of data access, at the cost some performance loss, which is particularly apparent for large datasets stored in homogeneous shards (e.g., video datasets). In this paper, we introduce a novel two-step partial data shuffling strategy for SGD which combines an offline iteration of the CorgiPile method with a subsequent online iteration. Our approach enjoys the best of both worlds: it performs similarly to SGD with random access (even for homogenous data) without compromising the data access efficiency of CorgiPile. We provide a comprehensive theoretical analysis of the convergence properties of our method and demonstrate its practical advantages through experimental results.
</details>
<details>
<summary>摘要</summary>
当使用渐进算法（Stochastic Gradient Descent，SGD）训练机器学习模型时，通常需要将模型提供随机选择的示例。然而，对于大规模存储在云端的数据集，随机访问单个示例可以非常昂贵和不效率。一项最近的研究（\cite{corgi））提出了一种在线混淆算法called CorgiPile，可以大幅提高数据访问效率，但是会付出一定的性能损失，尤其是对于具有同质数据集（例如视频集）。在这篇论文中，我们提出了一种新的两步半数据混淆策略，将在线迭代CorgiPile方法与后续的SGD迭代结合。我们的方法能够兼顾两者的优点：它能够与随机访问SGD（即使对同质数据）达到相似的性能，而无需牺牲CorgiPile的数据访问效率。我们对方法的整体理论分析和实验结果进行了全面的描述。
</details></li>
</ul>
<hr>
<h2 id="Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes"><a href="#Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes" class="headerlink" title="Representing Edge Flows on Graphs via Sparse Cell Complexes"></a>Representing Edge Flows on Graphs via Sparse Cell Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01632">http://arxiv.org/abs/2309.01632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Hoppe, Michael T. Schaub</li>
<li>for:  obtained sparse, interpretable representations of observable data</li>
<li>methods:  lifted graph structure to a simplicial complex, used Hodge decomposition to represent observed data</li>
<li>results:  outperformed current state-of-the-art methods while being computationally efficient, demonstrated on real-world and synthetic data<details>
<summary>Abstract</summary>
Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for its solution. Experiments on real-world and synthetic data demonstrate that our algorithm outperforms current state-of-the-art methods while being computationally efficient.
</details>
<details>
<summary>摘要</summary>
获取简洁可解释的数据表示是许多机器学习和信号处理任务中的关键。在图structure上的数据流动中，可以使用升级图结构到 simplicial complex来获得这些表示。图结构的eigenvector和相关矩阵可以引入一个Hodge分解，用于表示观察到的边流动。在这篇论文中，我们扩展了这种方法到细胞复杂体系，并引入细胞推理优化问题，即在观察到的图上添加一组细胞，使得图结构的eigenvector提供简洁可解释的表示。我们证明这个问题是NP困难的，并提出了一种有效的近似算法来解决它。实验结果表明，我们的算法在实际世界数据和模拟数据上都能够超过当前状态艺术方法，同时 computationally efficient。
</details></li>
</ul>
<hr>
<h2 id="DeViL-Decoding-Vision-features-into-Language"><a href="#DeViL-Decoding-Vision-features-into-Language" class="headerlink" title="DeViL: Decoding Vision features into Language"></a>DeViL: Decoding Vision features into Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01617">http://arxiv.org/abs/2309.01617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeViL">https://github.com/ExplainableML/DeViL</a></li>
<li>paper_authors: Meghal Dani, Isabel Rio-Torto, Stephan Alaniz, Zeynep Akata</li>
<li>For: This paper aims to provide natural language descriptions for what different layers of a vision backbone have learned, and to generate textual descriptions of visual features at different layers of the network.* Methods: The DeViL method uses a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. The model employs dropout both per-layer and per-spatial-location to generalize training on image-text pairs and produce localized explanations.* Results: DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Additionally, DeViL outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset.<details>
<summary>Abstract</summary>
Post-hoc explanation methods have often been criticised for abstracting away the decision-making process of deep neural networks. In this work, we would like to provide natural language descriptions for what different layers of a vision backbone have learned. Our DeViL method decodes vision features into language, not only highlighting the attribution locations but also generating textual descriptions of visual features at different layers of the network. We train a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. By employing dropout both per-layer and per-spatial-location, our model can generalize training on image-text pairs to generate localized explanations. As it uses a pre-trained language model, our approach is fast to train, can be applied to any vision backbone, and produces textual descriptions at different layers of the vision network. Moreover, DeViL can create open-vocabulary attribution maps corresponding to words or phrases even outside the training scope of the vision model. We demonstrate that DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Finally, we show DeViL also outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset. Code available at https://github.com/ExplainableML/DeViL
</details>
<details>
<summary>摘要</summary>
后期解释方法经常被批评为忽略深度神经网络的决策过程。在这项工作中，我们想提供深度神经网络各层学习的自然语言描述。我们的DeViL方法将视觉特征转换为语言描述，不仅高亮各层网络的贡献位置，还生成对应的语言描述。我们使用 transformer 网络将任意视觉层特征转换为可decode的自然语言描述。我们的方法快速训练，可以应用于任何视觉底层，并生成各层网络学习的文本描述。此外，DeViL 还可以生成对应于训练之外词汇的开放词汇映射。我们示示了 DeViL 对 CC3M 图像的描述与前期轻量级captioning模型具有比较好的性能，并且可以描述视觉模型学习的概念。最后，我们还证明了 DeViL 在 MILANNOTATIONS 数据集上的 neuron-wise 描述性能比现有的状态之前。代码可以在 GitHub 上找到：https://github.com/ExplainableML/DeViL。
</details></li>
</ul>
<hr>
<h2 id="Dropout-Attacks"><a href="#Dropout-Attacks" class="headerlink" title="Dropout Attacks"></a>Dropout Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01614">http://arxiv.org/abs/2309.01614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngunnar/Robustness_tutorial">https://github.com/ngunnar/Robustness_tutorial</a></li>
<li>paper_authors: Andrew Yuan, Alina Oprea, Cheng Tan</li>
<li>for: 本文旨在描述一种新的深度学习攻击方法 named DROPOUTATTACK，该攻击方法利用Dropout操作的杂 randomly drop neurons during training中的杂 randomly drop neurons during training 来防止过拟合。</li>
<li>methods: 本文使用了 four DROPOUTATTACK variants to cover a broad range of scenarios, including slowing or stopping training, destroying prediction accuracy of target classes, and sabotaging either precision or recall of a target class.</li>
<li>results: 在训练 VGG-16 模型在 CIFAR-100 上，我们的攻击可以降低目标类的精度从 81.7% 降低到 47.1% 而无需对模型精度产生影响。<details>
<summary>Abstract</summary>
Dropout is a common operator in deep learning, aiming to prevent overfitting by randomly dropping neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy
</details>
<details>
<summary>摘要</summary>
<<Dropout是深度学习中常用的操作，目的是防止过拟合by randomly dropped neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy.>>Here's the breakdown of the translation:* Dropout (Dropout) is a common operator in deep learning that aims to prevent overfitting by randomly dropping neurons during training.* This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK.* DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random.* We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios.* These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class.* In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy.
</details></li>
</ul>
<hr>
<h2 id="On-the-Query-Strategies-for-Efficient-Online-Active-Distillation"><a href="#On-the-Query-Strategies-for-Efficient-Online-Active-Distillation" class="headerlink" title="On the Query Strategies for Efficient Online Active Distillation"></a>On the Query Strategies for Efficient Online Active Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01612">http://arxiv.org/abs/2309.01612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Boldo, Enrico Martini, Mirco De Marchi, Stefano Aldegheri, Nicola Bombieri</li>
<li>for: 提高人姿估计（HPE）模型训练效率和实时适应性。</li>
<li>methods: 使用活动学习（AL）和在线热退出（online distillation）技术，评估不同查询策略以实现最佳训练结果。</li>
<li>results: 在一个流行的HPE数据集上，实现轻量级模型在实时上进行有效适应，并且在不同的查询策略下进行了评估。<details>
<summary>Abstract</summary>
Deep Learning (DL) requires lots of time and data, resulting in high computational demands. Recently, researchers employ Active Learning (AL) and online distillation to enhance training efficiency and real-time model adaptation. This paper evaluates a set of query strategies to achieve the best training results. It focuses on Human Pose Estimation (HPE) applications, assessing the impact of selected frames during training using two approaches: a classical offline method and a online evaluation through a continual learning approach employing knowledge distillation, on a popular state-of-the-art HPE dataset. The paper demonstrates the possibility of enabling training at the edge lightweight models, adapting them effectively to new contexts in real-time.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）需要大量时间和数据，导致计算需求很高。现在，研究人员通过活动学习（AL）和在线热静解释来提高训练效率和实时模型适应。这篇论文评估了一组查询策略以实现最佳训练结果。它专注于人姿估计（HPE）应用，评估选择的帧数据在训练中的影响，使用两种方法：一种 classical offline 方法和一种在线评估通过持续学习方法进行知识传递，在一个流行的HPE数据集上进行评估。这篇论文示出了在边缘进行轻量级模型训练，并在实时上适应新上下文的可能性。
</details></li>
</ul>
<hr>
<h2 id="Fair-Ranking-under-Disparate-Uncertainty"><a href="#Fair-Ranking-under-Disparate-Uncertainty" class="headerlink" title="Fair Ranking under Disparate Uncertainty"></a>Fair Ranking under Disparate Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01610">http://arxiv.org/abs/2309.01610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Rastogi, Thorsten Joachims</li>
<li>for: 这篇论文关注了排名的偏见问题，具体来说是在排名时存在不同群体之间的不公正问题。</li>
<li>methods: 作者提出了一种新的公平排名标准（Equal-Opportunity Ranking，EOR），并提供了一种实现这种标准的算法，该算法可以在 $O(n \log(n))$ 时间内计算出公平排名。</li>
<li>results: 作者在 synthetic data、US Census 数据和 Amazon 搜索查询的实验中证明了该算法可以准确地保证公平排名，而且可以提供有效的排名。<details>
<summary>Abstract</summary>
Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation far more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, since the relevance estimates for minority groups tend to have higher uncertainty due to a lack of data or appropriate features. To overcome this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking that provably corrects for the disparity in uncertainty between groups. Furthermore, we present a practical algorithm for computing EOR rankings in time $O(n \log(n))$ and prove its close approximation guarantee to the globally optimal solution. In a comprehensive empirical evaluation on synthetic data, a US Census dataset, and a real-world case study of Amazon search queries, we find that the algorithm reliably guarantees EOR fairness while providing effective rankings.
</details>
<details>
<summary>摘要</summary>
“排名是一种广泛使用的方法，用于引导人类评估者对可管理的选项进行筛选。它的应用范围从电商网站上出现可能有用的产品到审核学校申请。虽然排名可以使人类评估变得非常有效，但它会引入不公平性，如果在选项群中存在不同群体的uncertainty的差异。实际情况是，对少数群体的相关性估计通常具有更高的uncertainty，因为这些群体没有足够的数据或适当的特征。为解决这个公平问题，我们提出了一种新的公平准则——equal-opportunity ranking（EOR），可以正确地修正不同群体之间的uncertainty差异。此外，我们还提出了一种实用的计算EOR排名的算法，时间复杂度为O(nlog(n))，并证明其具有近似的优化策略。在 synthetic data、US Census dataset和amazon搜索查询的实验研究中，我们发现这种算法可靠地保证EOR公平性，同时提供有效的排名。”
</details></li>
</ul>
<hr>
<h2 id="Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning"><a href="#Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning" class="headerlink" title="Active flow control for three-dimensional cylinders through deep reinforcement learning"></a>Active flow control for three-dimensional cylinders through deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02462">http://arxiv.org/abs/2309.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pol Suárez, Francisco Alcántara-Ávila, Arnau Miró, Jean Rabault, Bernat Font, Oriol Lehmkuhl, R. Vinuesa</li>
<li>for: 降低缆线阻力</li>
<li>methods: 使用深度强化学习框架，结合计算流体动力学模拟器和智能代理人，实现多个独立控制的零负质量流体排气机制</li>
<li>results: 在三种不同配置下，通过应用DRL控制实现了显著的阻力减少<details>
<summary>Abstract</summary>
This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文首次报道了基于多个独立控制的零质量流体Synthetic jet的活动流控方法。jets被安装在三维圆柱上，以减少抗力系数。该方法基于深度学习束缚政策优化算法（DRL），并与计算流体力学解题结合使用。我们实现了多个代理人学习框架，它们具有许多优势：它利用本地 invariants，使控制可适应不同的几何结构，促进了转移学习和交叉应用代理人和结果，并导致了显著的训练加速。在这篇论文中，我们报道了在三个不同配置下应用DRL控制后的显著抗力减少。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data"><a href="#Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data" class="headerlink" title="Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data"></a>Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01593">http://arxiv.org/abs/2309.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqin Li, Jun Liu, Shengliang Zhong, Licheng Zhou, Shoubin Dong, Zejia Liu, Liqun Tang</li>
<li>for: 该论文主要用于车辆过载identification，尤其是对长跨度桥梁的Structural Health Monitoring数据进行分析。</li>
<li>methods: 该论文提出了一种基于深度学习的车辆过载identification方法（DOVI），使用时间卷积架构提取输入序列数据的空间和时间特征，提供了一个端到端的车辆过载identification解决方案，不需要影响线或者先知道车辆的速度和车辋基本信息。</li>
<li>results: 模型评估在简支桥和长跨度斜拉桥上下Random traffic flow下，结果表明，提出的深度学习车辆过载identification方法比其他机器学习和深度学习方法更有效和更强健。<details>
<summary>Abstract</summary>
Overloaded vehicles bring great harm to transportation infrastructures. BWIM (bridge weigh-in-motion) method for overloaded vehicle identification is getting more popular because it can be implemented without interruption to the traffic. However, its application is still limited because its effectiveness largely depends on professional knowledge and extra information, and is susceptible to occurrence of multiple vehicles. In this paper, a deep learning based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges by the use of structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus provides an end-to-end overloaded vehicle identification solution which neither needs the influence line nor needs to obtain velocity and wheelbase information in advance and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness, compared with other machine learning and deep learning approaches.
</details>
<details>
<summary>摘要</summary>
过载车辆对交通基础设施造成巨大的危害。 bridge weigh-in-motion（BWIM）方法为过载车辆识别 becoming more popular, because it can be implemented without interrupting traffic. However, its application is still limited because its effectiveness relies heavily on professional knowledge and additional information, and is susceptible to the occurrence of multiple vehicles. In this paper, a deep learning-based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges using structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus providing an end-to-end overloaded vehicle identification solution that does not require the influence line nor needs to obtain velocity and wheelbase information in advance, and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness compared with other machine learning and deep learning approaches.
</details></li>
</ul>
<hr>
<h2 id="Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width"><a href="#Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width" class="headerlink" title="Les Houches Lectures on Deep Learning at Large &amp; Infinite Width"></a>Les Houches Lectures on Deep Learning at Large &amp; Infinite Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01592">http://arxiv.org/abs/2309.01592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Bahri, Boris Hanin, Antonin Brossollet, Vittorio Erba, Christian Keup, Rosalba Pacelli, James B. Simon</li>
<li>for: 这些讲座在2022年的Les Houches夏学校上探讨了深度神经网络的无穷宽限和大宽限。</li>
<li>methods: 讲座讨论了深度神经网络的各种统计和动力学性质，包括随机深度神经网络、训练后深度神经网络与线性模型、核函数和高斯过程之间的连接，以及训练后大宽网络的不同初始化和训练方式。</li>
<li>results: 讲座介绍了训练后深度神经网络的性质和行为，包括 inicialization和训练后的性质，以及在不同宽度下的性质和行为。<details>
<summary>Abstract</summary>
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
</details>
<details>
<summary>摘要</summary>
这些讲座，发表于2022年勒舍瑞夏学校的统计物理和机器学习讲座，关注深度神经网络的无穷宽限和大宽限。讲座讨论了这些网络的各种统计和动力学性质。特别是讲者讨论了随机深度神经网络的属性；训练后的深度神经网络与线性模型、kernels和 Gaussian Processes在无穷宽限下的连接；以及训练前和训练后的大宽网络的不可逆和可逆处理。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models"><a href="#Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models" class="headerlink" title="Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models"></a>Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01590">http://arxiv.org/abs/2309.01590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdst-team/probablistic_precision_recall">https://github.com/kdst-team/probablistic_precision_recall</a></li>
<li>paper_authors: Dogyun Park, Suhyun Kim</li>
<li>for: 本文旨在检验生成模型的准确性和多样性，并提出了新的评估指标方法。</li>
<li>methods: 本文使用了$k$NN基于精度-回归指标来分解统计距离，并进行了其分析和评估。</li>
<li>results: 本文发现了$k$NN指标存在偏 toward outliers和不敏感于分布变化的问题，并提出了基于概率方法的新指标PP&amp;PR，并通过了extensive experiments表明其可靠性。Here’s the breakdown of each point in English:</li>
<li>for: The paper is aimed at evaluating the fidelity and diversity of generative models, and proposes new evaluation metrics.</li>
<li>methods: The paper uses $k$NN-based precision-recall metrics to break down the statistical distance, and analyzes and assesses these metrics.</li>
<li>results: The paper finds that the $k$NN indicators are susceptible to outliers and insensitive to distributional changes, and proposes novel indicators PP&amp;PR based on a probabilistic approach, which provide more reliable estimates for comparing fidelity and diversity.<details>
<summary>Abstract</summary>
Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advancement. So, recent papers have introduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break down the statistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable properties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P-recall (PP\&PR), based on a probabilistic approach that address the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP\&PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics. The codes are available at \url{https://github.com/kdst-team/Probablistic_precision_recall}.
</details>
<details>
<summary>摘要</summary>
评估生成模型的准确性和多样性是技术进步的重要问题。因此，最近的论文已经引入基于k-最近邻居($k$NN)的精度-回快指标来分解统计距离。这些指标提供了直观的方法，但我们进行了全面的分析，并发现了kNN指标存在强制约束和不可靠评估的问题，如受到异常值的影响和分布变化的不敏感。因此，我们提出了新的指标P-精度和P-回快（PP＆PR），基于概率方法，解决了这些问题。通过对实验和当前的生成模型进行广泛的调查，我们显示了我们的PP＆PR可以更可靠地评估生成模型的准确性和多样性。代码可以在GitHub上下载：https://github.com/kdst-team/Probablistic_precision_recall。
</details></li>
</ul>
<hr>
<h2 id="SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices"><a href="#SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices" class="headerlink" title="SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices"></a>SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01587">http://arxiv.org/abs/2309.01587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Montgomerie-Corcoran, Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis</li>
<li>for: 这个研究旨在解决目前边缘设备上实现现代物体检测模型的问题，并提供一个可靠的、低延迟的方案。</li>
<li>methods: 这个研究使用了一个流动架构设计，实现了完整的YOLO模型在FPGA设备上的加速。它还使用了一些新的硬件元件来支持YOLO模型的操作，以及外部内存缓冲来解决限制性的内存资源问题。</li>
<li>results: 这个研究的结果显示，使用这个工具流程生成的加速器设计可以与GPU设备相比，并且比现有的FPGA加速器更高性能和更低能耗。<details>
<summary>Abstract</summary>
AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion"><a href="#DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion" class="headerlink" title="DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion"></a>DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01575">http://arxiv.org/abs/2309.01575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</li>
<li>for: 这个论文的目的是提出一种新的三维人体姿态估计（3D-HPE）方法，通过粉批模型来提高人体姿态估计的准确性、可靠性和一致性。</li>
<li>methods: 这篇论文使用了 diffusion models，这种方法在多个领域中已经引起了革命，但在3D-HPE中尚未得到广泛研究。作者们在3D-HPE中应用了这种新的粉批模型策略，并证明了它能够改善标准的超级vised 3D-HPE。</li>
<li>results: 作者们通过使用 Human,3.6M 数据集，证明了他们的方法的有效性和其与现有模型的超越。他们还证明了粉批模型能够更好地处理 occlusions，并且改善时间协调和 sagittal 对称性的预测。<details>
<summary>Abstract</summary>
We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by integrating cutting-edge diffusion models, which have revolutionized diverse fields, but are relatively unexplored in 3D-HPE. We show that diffusion models enhance the accuracy, robustness, and coherence of human pose estimations. We introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE, and demonstrate its ability to refine standard supervised 3D-HPE. We also show how diffusion models lead to more robust estimations in the face of occlusions, and improve the time-coherence and the sagittal symmetry of predictions. Using the Human\,3.6M dataset, we illustrate the effectiveness of our approach and its superiority over existing models, even under adverse situations where the occlusion patterns in training do not match those in inference. Our findings indicate that while standalone diffusion models provide commendable performance, their accuracy is even better in combination with supervised models, opening exciting new avenues for 3D-HPE research.
</details>
<details>
<summary>摘要</summary>
我们提出了一种创新的三维人姿估计（3D-HPE）方法，通过结合进步的扩散模型，这些模型在多个领域引发革命，但在3D-HPE中尚未得到广泛探索。我们表明，扩散模型可以提高人姿估计的准确性、可靠性和一致性。我们提出了一种新的战略——DiffHPE，用于在3D-HPE中利用扩散模型，并证明其能够改善标准的超级vised 3D-HPE。我们还表明，扩散模型可以在 occlusion 情况下提供更加稳定的估计，并且可以改善时间听应和 sagittal 准确性。使用 Human\,3.6M 数据集，我们证明了我们的方法的有效性，并与现有模型进行比较，即使在训练和推理中的 occlusion 情况不符。我们的发现表明，独立使用扩散模型可以提供优秀的性能，而与超级vised 模型结合使用可以提高性能，开启了3D-HPE研究的新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs"><a href="#Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs" class="headerlink" title="Rail Crack Propagation Forecasting Using Multi-horizons RNNs"></a>Rail Crack Propagation Forecasting Using Multi-horizons RNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01569">http://arxiv.org/abs/2309.01569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Yasmine Ouerk, Olivier Vo Van, Mouadh Yagoubi</li>
<li>for: 这个论文主要用于预测铁路裂隙长度升溃的问题，这个问题对于物料和结构的维护和安全评估具有重要意义。</li>
<li>methods: 这个论文使用了机器学习技术，特别是循环神经网络（RNN）来预测时间序列数据。这种方法可以模拟时间序列数据，并将外生变量纳入模型中。</li>
<li>results: 实验结果显示，使用多个时间预测 horizon的 bayesian multi-horizons 模型，可以更好地预测铁路裂隙长度升溃的趋势，比如LSTM和GRU等状态 искусственный神经网络模型。<details>
<summary>Abstract</summary>
The prediction of rail crack length propagation plays a crucial role in the maintenance and safety assessment of materials and structures. Traditional methods rely on physical models and empirical equations such as Paris law, which often have limitations in capturing the complex nature of crack growth. In recent years, machine learning techniques, particularly Recurrent Neural Networks (RNNs), have emerged as promising methods for time series forecasting. They allow to model time series data, and to incorporate exogenous variables into the model. The proposed approach involves collecting real data on the French rail network that includes historical crack length measurements, along with relevant exogenous factors that may influence crack growth. First, a pre-processing phase was performed to prepare a consistent data set for learning. Then, a suitable Bayesian multi-horizons recurrent architecture was designed to model the crack propagation phenomenon. Obtained results show that the Multi-horizons model outperforms state-of-the-art models such as LSTM and GRU.
</details>
<details>
<summary>摘要</summary>
“预测铁路裂解长度传播的预测具有关键的作用在物料和结构的维护和安全评估中。传统方法往往靠赖物理模型和实验方程式如巴黎法则，它们经常无法捕捉裂解生长的复杂性。在最近几年，机器学习技术，特别是回归神经网络（RNN），在时间序列预测方面表现出色。它们允许模型时间序列数据，并将外生因素 integrate到模型中。本研究的方法是收集了法国铁路网络的实际裂解长度测量数据，以及可能影响裂解生长的相关外生因素。首先，进行了一个预处理阶段，以确保数据集的一致性。然后，适用了一个适当的 bayesian multi-horizons recurrent架构，以模elling裂解传播现象。获得的结果显示，Multi-horizons模型在LSTM和GRU模型之上表现出色。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking"><a href="#OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking" class="headerlink" title="OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking"></a>OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01552">http://arxiv.org/abs/2309.01552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blaž Škrlj, Blaž Mramor</li>
<li>for: 本研究旨在提高现代推荐系统的设计，包括了哪些特征空间是解决某个推荐任务的重要部分。</li>
<li>methods: 本研究使用了一种效果很好的算法来帮助发现有用的特征，即特征排名算法。此外，还使用了自动化机器学习（AutoML）技术来寻找更加紧凑且性能更高的模型。</li>
<li>results: 研究结果表明，使用OutRank系统可以在实际的Click-through-rate预测数据集上寻找更高性能的模型，并且可以在各种具有不同特征 cardinality的数据集上进行高效的模型搜索。此外，OutRank还可以快速地检测数据质量相关的异常情况。<details>
<summary>Abstract</summary>
The design of modern recommender systems relies on understanding which parts of the feature space are relevant for solving a given recommendation task. However, real-world data sets in this domain are often characterized by their large size, sparsity, and noise, making it challenging to identify meaningful signals. Feature ranking represents an efficient branch of algorithms that can help address these challenges by identifying the most informative features and facilitating the automated search for more compact and better-performing models (AutoML). We introduce OutRank, a system for versatile feature ranking and data quality-related anomaly detection. OutRank was built with categorical data in mind, utilizing a variant of mutual information that is normalized with regard to the noise produced by features of the same cardinality. We further extend the similarity measure by incorporating information on feature similarity and combined relevance. The proposed approach's feasibility is demonstrated by speeding up the state-of-the-art AutoML system on a synthetic data set with no performance loss. Furthermore, we considered a real-life click-through-rate prediction data set where it outperformed strong baselines such as random forest-based approaches. The proposed approach enables exploration of up to 300% larger feature spaces compared to AutoML-only approaches, enabling faster search for better models on off-the-shelf hardware.
</details>
<details>
<summary>摘要</summary>
现代推荐系统的设计需要理解哪些特征空间中的特征是解决某个推荐任务的关键。然而，实际世界数据集经常具有庞大、稀疏和噪声等特征，使得找到有意义的信号变得困难。特征排名算法是一种有效的方法，可以帮助解决这些挑战，并且可以自动搜索更紧凑和性能更高的模型（AutoML）。我们介绍了OutRank，一种多样化特征排名和数据质量相关异常检测的系统。OutRank采用了类别数据的视角，利用类别数据中特征之间的相互信息来 норmalize噪声。我们还将相互信息与共同相关性融合到相互信息中。我们的方法的可行性被证明了，通过加速现有AutoML系统在 sintetic数据集上的速度，而无损失性。此外，我们还考虑了一个真实的点击率预测数据集，其中OutRank exceeded strong baselines，如随机森林方法。我们的方法可以探索更大的特征空间，比AutoML-only方法多达300%，以便更快地搜索更好的模型。
</details></li>
</ul>
<hr>
<h2 id="Are-We-Using-Autoencoders-in-a-Wrong-Way"><a href="#Are-We-Using-Autoencoders-in-a-Wrong-Way" class="headerlink" title="Are We Using Autoencoders in a Wrong Way?"></a>Are We Using Autoencoders in a Wrong Way?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01532">http://arxiv.org/abs/2309.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GabMartino/icrst_trst_autoencoder">https://github.com/GabMartino/icrst_trst_autoencoder</a></li>
<li>paper_authors: Gabriele Martino, Davide Moroni, Massimo Martinelli</li>
<li>for: 这个研究旨在改进完全自适应神经网络（Autoencoder）的标准训练方法，以增强其准确性和灵活性。</li>
<li>methods: 该研究使用了不同的整形方法，包括不使用显式规则化项的方法，以改变自适应神经网络的含义空间的形状。</li>
<li>results: 研究发现，通过修改含义空间的形状，可以提高自适应神经网络的准确性和灵活性。此外，研究还发现，在重建整个数据集中随机样本的情况下，含义空间的行为也具有重要的意义。<details>
<summary>Abstract</summary>
Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的Autoencoder是非常广泛研究和应用的深度学习模型，其核心思想是通过训练模型来重建输入数据。Autoencoder模型具有压缩信息的特点，创造了所谓的缺省空间（Latent Space）。这些模型通常用于维度减少、异常检测和特征提取。这些模型已经得到了广泛的研究和更新，因为它们具有高度的简单性和力量。例如，（i）噪声Autoencoder，其中模型通过噪声图像重建原始图像；（ii）稀疏Autoencoder，其中瓶颈是通过损失函数中的正则化项来创建的；（iii）变量Autoencoder，其中缺省空间用于生成新的一致数据。在这篇文章中，我们重新训练了标准的Autoencoder模型，不使用任何显式的正则化项在损失函数中。我们让模型重建不同的输入观测，而不是原始输入观测。我们还探索了在重建整个数据集中的行为。
</details></li>
</ul>
<hr>
<h2 id="Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data"><a href="#Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data" class="headerlink" title="Passing Heatmap Prediction Based on Transformer Model and Tracking Data"></a>Passing Heatmap Prediction Based on Transformer Model and Tracking Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01526">http://arxiv.org/abs/2309.01526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisheng Pei, Varuna De Silva, Mike Caine</li>
<li>for: This research aims to provide a better understanding of how players’ off-ball movement contributes to their team’s defensive performance, and to develop a novel deep-learning network architecture that can predict the potential end location of passes and the impact of players’ movement before the pass.</li>
<li>methods: The research uses a novel deep-learning network architecture to predict the potential end location of passes and the impact of players’ movement before the pass, and analyzes more than 28,000 pass events to achieve a robust prediction with over 0.7 Top-1 accuracy.</li>
<li>results: The research finds that players’ off-ball movement has a significant impact on their team’s defensive performance, and provides a better understanding of how players’ movement over time contributes to the game strategy and final victory. The novel deep-learning network architecture developed in the research offers a better tool and metric for football analysts to evaluate players’ off-ball movement contribution.<details>
<summary>Abstract</summary>
Although the data-driven analysis of football players' performance has been developed for years, most research only focuses on the on-ball event including shots and passes, while the off-ball movement remains a little-explored area in this domain. Players' contributions to the whole match are evaluated unfairly, those who have more chances to score goals earn more credit than others, while the indirect and unnoticeable impact that comes from continuous movement has been ignored. This research presents a novel deep-learning network architecture which is capable to predict the potential end location of passes and how players' movement before the pass affects the final outcome. Once analysed more than 28,000 pass events, a robust prediction can be achieved with more than 0.7 Top-1 accuracy. And based on the prediction, a better understanding of the pitch control and pass option could be reached to measure players' off-ball movement contribution to defensive performance. Moreover, this model could provide football analysts a better tool and metric to understand how players' movement over time contributes to the game strategy and final victory.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)尽管数据驱动分析足球运动员表现已经在年份开发了几年，大多数研究仅集中于球场上的射门和传球事件，而偏离球场的运动员的贡献仍然是未探索的领域。运动员的整体贡献被不公正地评估，那些有更多的得分机会的运动员获得更多的赞誉，而不断移动的影响则被忽略。本研究提出了一种新的深度学习网络架构，可以预测传球的可能性结束位置以及运动员在传球之前的运动如何影响最终结果。经过分析超过28,000个传球事件，可以实现超过0.7 Top-1的准确率。基于预测结果，可以更好地理解球场控制和传球选择，并用这些指标来评估运动员的防守表现。此外，这个模型可以为足球分析师提供更好的工具和指标，以更好地理解运动员在时间上的运动如何影响游戏策略和最终胜利。
</details></li>
</ul>
<hr>
<h2 id="A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case"><a href="#A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case" class="headerlink" title="A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case"></a>A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01523">http://arxiv.org/abs/2309.01523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussein Aly, Abdulaziz Al-Ali, Abdullah Al-Ali, Qutaibah Malluhi</li>
<li>for: 本研究探讨智能电网中forecasting模型的隐私风险，尤其是深度学习和启发式学习模型在智能电网应用中的隐私风险。</li>
<li>methods: 本研究使用了深度学习和启发式学习模型，包括Long Short Term Memory（LSTM）模型，来分析智能电网系统中的隐私风险。</li>
<li>results: 研究发现，forecasting模型可以泄露全局性和隐私威胁，而LSTM模型可以泄露大量信息，相当于直接访问数据本身，这显示了保护forecasting模型的重要性。<details>
<summary>Abstract</summary>
This paper investigates the potential privacy risks associated with forecasting models, with specific emphasis on their application in the context of smart grids. While machine learning and deep learning algorithms offer valuable utility, concerns arise regarding their exposure of sensitive information. Previous studies have focused on classification models, overlooking risks associated with forecasting models. Deep learning based forecasting models, such as Long Short Term Memory (LSTM), play a crucial role in several applications including optimizing smart grid systems but also introduce privacy risks. Our study analyzes the ability of forecasting models to leak global properties and privacy threats in smart grid systems. We demonstrate that a black box access to an LSTM model can reveal a significant amount of information equivalent to having access to the data itself (with the difference being as low as 1% in Area Under the ROC Curve). This highlights the importance of protecting forecasting models at the same level as the data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究智能电网中预测模型的隐私风险，尤其是深度学习和预测模型在智能电网系统中的应用。虽然机器学习和深度学习算法提供了 valuabe的功能，但是隐私问题引起了关注，因为预测模型可能泄露敏感信息。先前的研究主要集中在分类模型上，忽略了预测模型中的隐私风险。深度学习基于的预测模型，如Long Short Term Memory（LSTM），在许多应用中扮演着关键的角色，包括智能电网系统的优化，但也会引起隐私风险。我们的研究分析了预测模型是否可以泄露全局性和隐私威胁在智能电网系统中。我们示出，通过访问LSTM模型，可以获取大量信息，与直接访问数据相当，差异只有1%左右（在ROC曲线下的面积）。这说明预测模型需要与数据一样受到保护。
</details></li>
</ul>
<hr>
<h2 id="Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning"><a href="#Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning" class="headerlink" title="Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning"></a>Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01519">http://arxiv.org/abs/2309.01519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Peng, Zhengwei Lv, Jiarong Fu, Jiayuan Liang, Zhao Zhang, Ajitha Rajan, Ping Yang</li>
<li>for: 本研究旨在提高Android应用程序更新的正确性，以避免在用户端引入潜在错误。</li>
<li>methods: 本研究提出了一种引导测试方法，使用深度强化学习来优先执行更新后影响的GUI操作。</li>
<li>results: 对10个开源App和1个商业App进行比较，发现 Hawkeye 能够更可靠地生成targeting更改的GUI事件序列，而 FastBot2 和 ARES 在开源App上表现较差。 Hawkeye 在小型开源App上也表现相对较好。此外，在商业应用程序的开发流水线中部署 Hawkeye 也显示了它的IDEA。<details>
<summary>Abstract</summary>
Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events focussing on improving the test coverage of the entire App rather than prioritising updates and its impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise the updates and impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. We propose directed testing of App updates with Hawkeye that is able to prioritise executing GUI actions associated with code changes based on deep reinforcement learning from historical exploration data. Our empirical evaluation compares Hawkeye with state-of-the-art model-based and reinforcement learning-based testing tools FastBot2 and ARES using 10 popular open-source and 1 commercial App. We find that Hawkeye is able to generate GUI event sequences targeting changed functions more reliably than FastBot2 and ARES for the open source Apps and the large commercial App. Hawkeye achieves comparable performance on smaller open source Apps with a more tractable exploration space. The industrial deployment of Hawkeye in the development pipeline also shows that Hawkeye is ideal to perform smoke testing for merge requests of a complicated commercial App.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval"><a href="#MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval" class="headerlink" title="MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"></a>MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01516">http://arxiv.org/abs/2309.01516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/multiway-adapter">https://github.com/longkukuhi/multiway-adapter</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: 这篇论文的目的是提出一个框架，以增强大型多modal模型（LMMs）的适应性和转移性，以便在新任务上进行高效的适应。</li>
<li>methods: 这篇论文使用了一个名为“Alignment Enhancer”的新方法，它能够深入对模组进行调整，以提高模组之间的对齐性。这个方法只需要将LMMs中的少于1.25%的参数进行调整，即使不需要完全重新训练。</li>
<li>results: 这篇论文的实验结果显示，使用 Multiway-Adapter 框架可以在零传入情感领域中实现高效的适应，并且可以降低 fine-tuning 时间的耗用率，较之完全训练的模型更高。<details>
<summary>Abstract</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose Multiway-Adapter, an innovative framework that incorporates an "Alignment Enhancer" to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25% additional parameters to LMMs, as demonstrated by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57% reduction in fine-tuning time.Our approach offers a resource-efficient and effective adaptation pathway for LMMs, expanding their applicability. The source code is publicly available at: <https://github.com/longkukuhi/MultiWay-Adapter>.
</details></li>
</ul>
<hr>
<h2 id="Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning"><a href="#Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning" class="headerlink" title="Federated cINN Clustering for Accurate Clustered Federated Learning"></a>Federated cINN Clustering for Accurate Clustered Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01515">http://arxiv.org/abs/2309.01515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Zhou, Minjia Shi, Yuxin Tian, Yuanxi Li, Qing Ye, Jiancheng Lv</li>
<li>for: 这篇论文旨在提出一种用于聚合分布式机器学习的 Federated Learning (FL) 方法，并且可以实现高效的人群智能。</li>
<li>methods: 这篇论文提出的 Federated cINN Clustering Algorithm (FCCA) 使用全球Encoder将每个客户端的私人数据转换为多元 Gaussian 分布，然后使用生成模型进行最大 LIKELIHOOD 估计，以简化优化并避免模式崩溃。</li>
<li>results: 实验结果显示 FCCA 比其他已知的分布式 Federated Learning 算法更有优势，在不同的模型和数据集上进行评估。这些结果表明这种方法具有实际应用中 Federated Learning 任务的优化和精度。<details>
<summary>Abstract</summary>
Federated Learning (FL) presents an innovative approach to privacy-preserving distributed machine learning and enables efficient crowd intelligence on a large scale. However, a significant challenge arises when coordinating FL with crowd intelligence which diverse client groups possess disparate objectives due to data heterogeneity or distinct tasks. To address this challenge, we propose the Federated cINN Clustering Algorithm (FCCA) to robustly cluster clients into different groups, avoiding mutual interference between clients with data heterogeneity, and thereby enhancing the performance of the global model. Specifically, FCCA utilizes a global encoder to transform each client's private data into multivariate Gaussian distributions. It then employs a generative model to learn encoded latent features through maximum likelihood estimation, which eases optimization and avoids mode collapse. Finally, the central server collects converged local models to approximate similarities between clients and thus partition them into distinct clusters. Extensive experimental results demonstrate FCCA's superiority over other state-of-the-art clustered federated learning algorithms, evaluated on various models and datasets. These results suggest that our approach has substantial potential to enhance the efficiency and accuracy of real-world federated learning tasks.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）提出了一种革新的隐私保护分布式机器学习方法，实现大规模的人群智慧。然而，当联邦学习与人群智慧集成时，却会遇到一个挑战，那是由于客户组的数据多样性或对于不同任务的调整。为解决这个挑战，我们提出了联邦cINN排序算法（FCCA），以静态排序客户组，避免客户组之间的互相干扰，并将全球模型的性能提高。具体来说，FCCA使用全球编码器将每个客户组的私人数据转换为多元normal分布。然后，它运用生成模型学习编码特征，通过最大 LIKELIHOOD估计，实现优化和避免模型塌陷。最后，中央服务器收集了各客户组的融合模型，估计客户组之间的相似性，并将其分组。实验结果显示，FCCA与其他现有的分布式学习排序算法相比，在不同的模型和数据集上具有明显的优势。这些结果表明，我们的方法具有实际的应用潜力，以提高现实世界中的联邦学习任务的效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Optimizers-with-4-bit-States"><a href="#Memory-Efficient-Optimizers-with-4-bit-States" class="headerlink" title="Memory Efficient Optimizers with 4-bit States"></a>Memory Efficient Optimizers with 4-bit States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01507">http://arxiv.org/abs/2309.01507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/low-bit-optimizers">https://github.com/thu-ml/low-bit-optimizers</a></li>
<li>paper_authors: Bingrui Li, Jianfei Chen, Jun Zhu</li>
<li>for: 这篇论文目的是对训练神经网络时的优化器状态进行储存压缩，以降低训练内存负载。</li>
<li>methods: 本文使用了详细的实验分析，从第一个和第二个 moments 中获得了较低的位元数字宽度，并且使用了小尺寸的封包和列对组合来更好地量化。此外，本文还解决了量化第二个 moments 时出现的零点问题，使用了一个排除零点的直线量化器。</li>
<li>results: 本文在训练多种任务，包括自然语言理解、机器翻译、图像分类和指令调整等，都可以与全精度版本的优化器相比，具有更好的记忆储存效率。<details>
<summary>Abstract</summary>
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Layer-wise-training-for-self-supervised-learning-on-graphs"><a href="#Layer-wise-training-for-self-supervised-learning-on-graphs" class="headerlink" title="Layer-wise training for self-supervised learning on graphs"></a>Layer-wise training for self-supervised learning on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01503">http://arxiv.org/abs/2309.01503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Pina, Verónica Vilaplana</li>
<li>for: 本研究旨在开发一种可以在大 graphs 上进行端到端训练的图 neural network (GNN) 算法，以解决深度增加时的内存和计算复杂性问题。</li>
<li>methods: 我们提出了层 wise REGularized Graph Infomax（Layer-wise REG-GI）算法，它通过分解 GNN 中的特征传播和特征转换来学习节点表示，然后根据预测输入的估计来 derive 一个损失函数。</li>
<li>results: 我们在 inductive 大 graphs 中评估了该算法，并与其他端到端方法相比较，发现它具有更高的效率和相同的性能，可以在一个单个设备上训练更复杂的模型。此外，我们还发现该算法可以避免 oversmoothing 的问题。<details>
<summary>Abstract</summary>
End-to-end training of graph neural networks (GNN) on large graphs presents several memory and computational challenges, and limits the application to shallow architectures as depth exponentially increases the memory and space complexities. In this manuscript, we propose Layer-wise Regularized Graph Infomax, an algorithm to train GNNs layer by layer in a self-supervised manner. We decouple the feature propagation and feature transformation carried out by GNNs to learn node representations in order to derive a loss function based on the prediction of future inputs. We evaluate the algorithm in inductive large graphs and show similar performance to other end to end methods and a substantially increased efficiency, which enables the training of more sophisticated models in one single device. We also show that our algorithm avoids the oversmoothing of the representations, another common challenge of deep GNNs.
</details>
<details>
<summary>摘要</summary>
大规模图gnn的端到端训练存在内存和计算上的挑战，深度随着增加而 exponential 增加内存和空间复杂性。在这篇论文中，我们提出层 wise 常数化图信息媒体кс（Layer-wise Regularized Graph Infomax），一种在自适应模式下层个 Train GNN。我们将 GNN 中的特征传播和特征变换分解出来，以学习节点表示，并从这些表示中提取一个基于未来输入预测的损失函数。我们在 inductive 大raph 中评估了该算法，并与其他端到端方法相比，实现了相似的性能，同时提高了效率，使得可以在一个设备上训练更复杂的模型。此外，我们还证明了我们的算法可以避免深度 GNN 的抽象过滤。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging"><a href="#On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging" class="headerlink" title="On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging"></a>On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01488">http://arxiv.org/abs/2309.01488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryanthony/mahalanobis-ood-detection">https://github.com/harryanthony/mahalanobis-ood-detection</a></li>
<li>paper_authors: Harry Anthony, Konstantinos Kamnitsas</li>
<li>For: This paper aims to improve the detection of out-of-distribution (OOD) patterns in neural networks for medical applications.* Methods: The paper challenges the existing understanding that there is an optimal layer or combination of layers for applying Mahalanobis distance for OOD detection, and instead shows that the optimum layer changes depending on the type of OOD pattern. The paper also proposes separating the OOD detector into multiple detectors at different depths of the network to enhance robustness.* Results: The paper validates these insights on real-world OOD tasks using CheXpert chest X-rays with unseen pacemakers and unseen sex as OOD cases, and provides best-practices for the use of Mahalanobis distance for OOD detection.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提高神经网络中的外围数据探测（OOD）性能，用于医疗应用。</li>
<li>methods: 该论文挑战现有的认知，即存在一个最佳层或层组的神经网络来应用 Mahalanobis 距离进行 OOD 探测，而是显示了 OOD 类型改变最佳层的问题。该论文还提议将 OOD 探测器分解成不同深度的网络中的多个探测器，以提高 robustness。</li>
<li>results: 论文 validate 这些发现在实际的 OOD 任务上，使用 CheXpert 胸部X射像素进行训练，并使用不同的 pacemaker 和性别作为 OOD 情况。结果提供了 OOD 探测中 Mahalanobis 距离的最佳做法。 manually 标注的 pacemaker 标签和项目代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/HarryAnthony/Mahalanobis-OOD-detection%E3%80%82">https://github.com/HarryAnthony/Mahalanobis-OOD-detection。</a><details>
<summary>Abstract</summary>
Implementing neural networks for clinical use in medical applications necessitates the ability for the network to detect when input data differs significantly from the training data, with the aim of preventing unreliable predictions. The community has developed several methods for out-of-distribution (OOD) detection, within which distance-based approaches - such as Mahalanobis distance - have shown potential. This paper challenges the prevailing community understanding that there is an optimal layer, or combination of layers, of a neural network for applying Mahalanobis distance for detection of any OOD pattern. Using synthetic artefacts to emulate OOD patterns, this paper shows the optimum layer to apply Mahalanobis distance changes with the type of OOD pattern, showing there is no one-fits-all solution. This paper also shows that separating this OOD detector into multiple detectors at different depths of the network can enhance the robustness for detecting different OOD patterns. These insights were validated on real-world OOD tasks, training models on CheXpert chest X-rays with no support devices, then using scans with unseen pacemakers (we manually labelled 50% of CheXpert for this research) and unseen sex as OOD cases. The results inform best-practices for the use of Mahalanobis distance for OOD detection. The manually annotated pacemaker labels and the project's code are available at: https://github.com/HarryAnthony/Mahalanobis-OOD-detection.
</details>
<details>
<summary>摘要</summary>
实施神经网络在医疗应用中需要神经网络能够检测输入数据与训练数据之间的差异，以避免不可靠的预测。社区已经开发出多种对外生数据（OOD）检测方法，其中距离基本方法，如 Mahalanobis 距离，有潜力。这篇论文探讨了社区对神经网络应用 Mahalanobis 距离时的共识，即存在一个最佳层或组合层可以检测任何 OOD 模式。使用人工生成的 artifacts 模拟 OOD 模式，这篇论文显示了应用 Mahalanobis 距离的最佳层不同于 OOD 模式类型，表明没有一个通用的解决方案。此外，这篇论文还表明，将 OOD 检测器分解到神经网络的不同层次可以提高对不同 OOD 模式的检测稳定性。这些发现得到了实际 OOD 任务的验证，使用无支持设备训练 CheXpert 胸部X射像，然后使用未看到过 pacemaker 和未看到过性别为 OOD 例子。结果提供了对 Mahalanobis 距离的使用最佳做法的指导，以及相关的手动标注 pacemaker 标签和项目代码，可以在 GitHub 上获取：https://github.com/HarryAnthony/Mahalanobis-OOD-detection。
</details></li>
</ul>
<hr>
<h2 id="CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification"><a href="#CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification" class="headerlink" title="CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification"></a>CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01483">http://arxiv.org/abs/2309.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Zhang, Zhibin Zhao, Deyu Meng, Xingwu Zhang, Xuefeng Chen</li>
<li>for: 提高机器学习模型在实际应用中的可靠性，即一类分类（OCC）问题。</li>
<li>methods: 使用预训练特征进行适应，并对目标数据集进行类型不确定的情况下进行适应。</li>
<li>results: 在不同的训练数据类型下，包括1到1024个类，都能够提高OCC性能，并且超越了当前的状态艺术方法。<details>
<summary>Abstract</summary>
One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the number of classes. This contradicts the real scenario where the number of classes is unknown. In this work, we propose a simple class-agnostic adaptive feature adaptation method (CA2). We generalize the center-based method to unknown classes and optimize this objective based on the prior existing in the pre-trained network, i.e., pre-trained features that belong to the same class are adjacent. CA2 is validated to consistently improve OCC performance across a spectrum of training data classes, spanning from 1 to 1024, outperforming current state-of-the-art methods. Code is available at https://github.com/zhangzilongc/CA2.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC），即确定例子属于训练数据的同一分布，是机器学习模型在实际应用中的重要任务。适应预训练特征到目标数据集的方法已经证明是提高OCC性能的有效方法。现有方法受到类数假设的限制，这与实际情况不符。在这项工作中，我们提出了一种简单的类型不假设的适应特征适应方法（CA2）。我们扩展了中心基于方法到未知类，并基于预训练网络中的先前存在的假设，即预训练特征属于同一类的情况下进行优化。CA2在训练数据类的范围从1到1024之间，并且在不同类型的训练数据上表现出色，超越当前的状态艺术方法。代码可以在https://github.com/zhangzilongc/CA2上下载。
</details></li>
</ul>
<hr>
<h2 id="FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation"><a href="#FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation" class="headerlink" title="FinDiff: Diffusion Models for Financial Tabular Data Generation"></a>FinDiff: Diffusion Models for Financial Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01472">http://arxiv.org/abs/2309.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timur Sattarov, Marco Schreyer, Damian Borth</li>
<li>For:  This paper aims to provide a solution for the challenge of sharing microdata, such as fund holdings and derivative instruments, by regulatory institutions, by using generative models to synthesize data that mimics the underlying distributions of real-world data.* Methods: The paper introduces ‘FinDiff’, a diffusion model that uses embedding encodings to model mixed modality financial data, and evaluates its performance in generating synthetic tabular financial data against state-of-the-art baseline models using three real-world financial datasets.* Results: The results show that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.<details>
<summary>Abstract</summary>
The sharing of microdata, such as fund holdings and derivative instruments, by regulatory institutions presents a unique challenge due to strict data confidentiality and privacy regulations. These challenges often hinder the ability of both academics and practitioners to conduct collaborative research effectively. The emergence of generative models, particularly diffusion models, capable of synthesizing data mimicking the underlying distributions of real-world data presents a compelling solution. This work introduces 'FinDiff', a diffusion model designed to generate real-world financial tabular data for a variety of regulatory downstream tasks, for example economic scenario modeling, stress tests, and fraud detection. The model uses embedding encodings to model mixed modality financial data, comprising both categorical and numeric attributes. The performance of FinDiff in generating synthetic tabular financial data is evaluated against state-of-the-art baseline models using three real-world financial datasets (including two publicly available datasets and one proprietary dataset). Empirical results demonstrate that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.
</details>
<details>
<summary>摘要</summary>
共享微数据，如基金投资和 derivate 工具，由 regulatory 机构提供的挑战具有坚实的数据保密和隐私规定，这些挑战经常阻碍学者和实践者进行合作研究。随着生成模型的出现，特别是扩散模型，可以模拟实际世界数据的下WFDistribution，这提供了一个吸引人的解决方案。本文介绍了 'FinDiff'，一种适用于生成实际世界金融表格数据的扩散模型，用于经济enario模拟、压力测试和欺诈探测等下游任务。FinDiff 使用嵌入编码来模型金融数据的混合模式特征，包括分类和数值特征。FinDiff 在生成Synthetic tabular financial data的性能上与state-of-the-art基eline模型进行比较，使用三个真实世界金融数据集（其中两个公共可用数据集和一个专用数据集）进行实验。实验结果表明，FinDiff 能够生成高度准确、隐私和有用的Synthetic tabular financial data。
</details></li>
</ul>
<hr>
<h2 id="Pure-Monte-Carlo-Counterfactual-Regret-Minimization"><a href="#Pure-Monte-Carlo-Counterfactual-Regret-Minimization" class="headerlink" title="Pure Monte Carlo Counterfactual Regret Minimization"></a>Pure Monte Carlo Counterfactual Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03084">http://arxiv.org/abs/2309.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Qi, Ting Feng, Falun Hei, Zhemei Fang, Yunfeng Luo</li>
<li>for:  solves large-scale incomplete information games</li>
<li>methods:  builds upon CFR and Fictitious Play, combines counterfactual regret and best response strategy</li>
<li>results:  achieves better performance, reduces time and space complexity, and converges faster than MCCFR with a new warm start algorithm<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strategies elimination method. Consequently, the PMCCFR with new warm start algorithm can converge by two orders of magnitude faster than the CFR+ algorithm.
</details>
<details>
<summary>摘要</summary>
大规模不完整信息游戏的最佳算法是Counterfactual Regret Minimization（CFR）和其变种。这篇论文基于CFR，提出了一种新的算法named Pure CFR（PCFR），以实现更高的性能。PCFR可以看作CFR和Fictitious Play（FP）的组合，继承CFR中的反factual regret（价值）概念，并使用下一轮的最佳回应策略而不是 regret matching策略。我们的理论证明，PCFR可以 дости到黑套接近性，这使得PCFR可以与任何CFR变种，包括Monte Carlo CFR（MCCFR）结合。结果，得到的Pure MCCFR（PMCCFR）可以大幅降低时间和空间复杂度。尤其是，PMCCFR的整合速度至少三倍于MCCFR的整合速度。此外，由于PMCCFR不通过严格dominated策略的路径，我们开发了一种新的温开始算法，它是基于严格dominated策略的消除方法。因此，PMCCFR与新的温开始算法可以在CFR+算法的两个数量级更快 converges。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning"><a href="#Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning" class="headerlink" title="Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning"></a>Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01458">http://arxiv.org/abs/2309.01458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Huanqian Wang, Mukun Tong, Wenjie Shi, Gao Huang, Shiji Song</li>
<li>for: 该研究旨在解释和解释深度强化学习（RL）Agent的黑盒性质，以便在实际应用中使用。</li>
<li>methods: 该研究提出了一种基于奖励（reward）的解释方法，即RL-in-RL框架，以维护奖励一致性并实现高质量的特征归属。</li>
<li>results: 研究在Atari 2600游戏和Duckietown自驾汽车 simulate环境中进行了验证和评估，结果表明，该方法可以保持奖励一致性并实现高质量的特征归属。此外，一系列的分析实验也证明了动作匹配原则的局限性。<details>
<summary>Abstract</summary>
The black-box nature of deep reinforcement learning (RL) hinders them from real-world applications. Therefore, interpreting and explaining RL agents have been active research topics in recent years. Existing methods for post-hoc explanations usually adopt the action matching principle to enable an easy understanding of vision-based RL agents. In this paper, it is argued that the commonly used action matching principle is more like an explanation of deep neural networks (DNNs) than the interpretation of RL agents. It may lead to irrelevant or misplaced feature attribution when different DNNs' outputs lead to the same rewards or different rewards result from the same outputs. Therefore, we propose to consider rewards, the essential objective of RL agents, as the essential objective of interpreting RL agents as well. To ensure reward consistency during interpretable feature discovery, a novel framework (RL interpreting RL, denoted as RL-in-RL) is proposed to solve the gradient disconnection from actions to rewards. We verify and evaluate our method on the Atari 2600 games as well as Duckietown, a challenging self-driving car simulator environment. The results show that our method manages to keep reward (or return) consistency and achieves high-quality feature attribution. Further, a series of analytical experiments validate our assumption of the action matching principle's limitations.
</details>
<details>
<summary>摘要</summary>
深度强化学习（RL）的黑盒特性使其在实际应用中受限。因此，解释和解释RL代理的研究成为了近年active topic。现有的后续解释方法通常采用行动匹配原则来使得视觉RL代理更易于理解。在这篇文章中， argue that通常使用的行动匹配原则更多地是对深度神经网络（DNNs）的解释，而不是RL代理的解释。这可能会导致不相关或错位的特征归因，因为不同的DNNs输出可能导致同样的奖励，或者同样的奖励可能来自不同的输出。因此，我们提议将奖励作为RL代理解释的关键对象，以确保在可见特征发现过程中保持奖励一致性。为解决动作和奖励之间的梯度分离问题，我们提出了一种新的框架（RL解释RL，简称为RL-in-RL）。我们在Atari 2600游戏和Difficult自驾车 simulator环境中进行了验证和评估。结果表明，我们的方法可以保持奖励一致性，并实现高质量的特征归因。此外，一系列的分析实验validate了我们对行动匹配原则的假设的限制。
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification"><a href="#On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification" class="headerlink" title="On the Consistency and Robustness of Saliency Explanations for Time Series Classification"></a>On the Consistency and Robustness of Saliency Explanations for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01457">http://arxiv.org/abs/2309.01457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Bin Li, Emmanuel Müller</li>
<li>for: 本文旨在探讨时序序列数据的解释和解释模型的稳定性和一致性问题。</li>
<li>methods: 本文使用了两种常见的解释模型，即干扰基因模型和梯度基因模型，来生成时序序列数据的解释。</li>
<li>results: 实验结果表明，这两种解释模型在五个实际 dataset 上均存在一定的不稳定和不一致性问题，无法提供可靠的解释。<details>
<summary>Abstract</summary>
Interpretable machine learning and explainable artificial intelligence have become essential in many applications. The trade-off between interpretability and model performance is the traitor to developing intrinsic and model-agnostic interpretation methods. Although model explanation approaches have achieved significant success in vision and natural language domains, explaining time series remains challenging. The complex pattern in the feature domain, coupled with the additional temporal dimension, hinders efficient interpretation. Saliency maps have been applied to interpret time series windows as images. However, they are not naturally designed for sequential data, thus suffering various issues.   This paper extensively analyzes the consistency and robustness of saliency maps for time series features and temporal attribution. Specifically, we examine saliency explanations from both perturbation-based and gradient-based explanation models in a time series classification task. Our experimental results on five real-world datasets show that they all lack consistent and robust performances to some extent. By drawing attention to the flawed saliency explanation models, we motivate to develop consistent and robust explanations for time series classification.
</details>
<details>
<summary>摘要</summary>
《机器学习可解释性和人工智能可解释性在许多应用中变得必备。模型性能和可解释性之间的质量是发展内在和模型自适应解释方法的障碍。虽然模型解释方法在视觉和自然语言领域得到了显著成功，但是解释时间序列仍然具有挑战。时间序列特征的复杂pattern，加上额外的时间维度，使得有效的解释受到阻碍。在图像上应用saliency map来解释时间序列窗口，但这些map不是专门为sequential数据设计的，因此会出现各种问题。》This paper conducts an extensive analysis of the consistency and robustness of saliency maps for time series features and temporal attribution. We examine saliency explanations from both perturbation-based and gradient-based explanation models in a time series classification task. Our experimental results on five real-world datasets show that they all lack consistent and robust performances to some extent. By drawing attention to the flawed saliency explanation models, we motivate the development of consistent and robust explanations for time series classification.
</details></li>
</ul>
<hr>
<h2 id="Toward-Defensive-Letter-Design"><a href="#Toward-Defensive-Letter-Design" class="headerlink" title="Toward Defensive Letter Design"></a>Toward Defensive Letter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01452">http://arxiv.org/abs/2309.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Rentaro Kataoka, Akisato Kimura, Seiichi Uchida</li>
<li>for: 防御 adversarial 攻击，提高图像分类器的抗击能力，并不关心图像中的视觉对象（如熊猫和车辆）的安全性。</li>
<li>methods: 使用 Iterative Fast Gradient Sign Method (I-FGSM) 测试字母图像的防御能力，并建立深度回归模型来估算每个字母图像的防御能力。提议基于生成 adversarial 网络 (GAN) 的两步方法来生成具有更高防御能力的字母图像。</li>
<li>results: 通过测试和分析，发现字母图像具有较高的防御能力，并且可以通过生成新的字母图像来提高防御能力。<details>
<summary>Abstract</summary>
A major approach for defending against adversarial attacks aims at controlling only image classifiers to be more resilient, and it does not care about visual objects, such as pandas and cars, in images. This means that visual objects themselves cannot take any defensive actions, and they are still vulnerable to adversarial attacks. In contrast, letters are artificial symbols, and we can freely control their appearance unless losing their readability. In other words, we can make the letters more defensive to the attacks. This paper poses three research questions related to the adversarial vulnerability of letter images: (1) How defensive are the letters against adversarial attacks? (2) Can we estimate how defensive a given letter image is before attacks? (3) Can we control the letter images to be more defensive against adversarial attacks? For answering the first and second questions, we measure the defensibility of letters by employing Iterative Fast Gradient Sign Method (I-FGSM) and then build a deep regression model for estimating the defensibility of each letter image. We also propose a two-step method based on a generative adversarial network (GAN) for generating character images with higher defensibility, which solves the third research question.
</details>
<details>
<summary>摘要</summary>
一种主要方法 для防御对抗攻击是控制图像分类器更加抗御，而不在意图像中的视觉对象，如�anda和车。这意味着视觉对象本身无法采取任何防御行动，仍然易受到对抗攻击。然而，字符是人工符号，我们可以自由地控制它们的外表，除非失去可读性。这意味着我们可以使字符更加抗御对抗攻击。这篇论文提出了三个研究问题关于对字符图像的抗御敏感性：1. 字符是如何抗御对抗攻击的？2. 我们可以在进行攻击之前对给定字符图像进行评估其抗御性吗？3. 我们可以通过生成推论网络（GAN）来生成更加抗御的字符图像吗？为回答第一个和第二个问题，我们使用迭代快速梯度签名方法（I-FGSM）测量字符的抗御性，然后建立深度回归模型来预测每个字符图像的抗御性。我们还提出了一种基于GAN的两步方法，可以生成更加抗御的字符图像，解决第三个研究问题。
</details></li>
</ul>
<hr>
<h2 id="Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks"><a href="#Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks" class="headerlink" title="Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks"></a>Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02460">http://arxiv.org/abs/2309.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi, Qing Li, Jiannong Cao</li>
<li>for: 这篇研究是为了探索黑钱账户探测在加密货币交易网络上，以减少在线金融市场中发生的损失。</li>
<li>methods: 这篇研究使用了一种名为DIAM的新型多графі神经网络模型，它包括了Edge2Seq模块和多графі异常性（MGD）模块，以及一个实际上的终端训练方法。</li>
<li>results: 根据实验结果，DIAM在4个大加密货币数据集上的比较中，实现了最高的准确率和效率，比如在一个 Bitcoin 数据集上，DIAM 的 F1 分数为 96.55%，较最好的竞争者高出 83.92%。<details>
<summary>Abstract</summary>
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing the multigraph topology, DIAM employs a new Multigraph Discrepancy (MGD) module with a well-designed message passing mechanism to capture the discrepant features between normal and illicit nodes, supported by an attention mechanism. Assembling all techniques, DIAM is trained in an end-to-end manner. Extensive experiments, comparing against 14 existing solutions on 4 large cryptocurrency datasets of Bitcoin and Ethereum, demonstrate that DIAM consistently achieves the best performance to accurately detect illicit accounts, while being efficient. For instance, on a Bitcoin dataset with 20 million nodes and 203 million edges, DIAM achieves F1 score 96.55%, significantly higher than the F1 score 83.92% of the best competitor.
</details>
<details>
<summary>摘要</summary>
我们研究 криптовалютных交易网络上的非法帐户检测，这些网络在在线金融市场中变得越来越重要。非法活动的上升导致了数百亿元的损失，从正常用户手中。现有的解决方案可以分为两类：一是 tedious feature engineering 来获取手工特征，二是不充分利用 криптовалю transaction 数据的 semantics，因此效果不佳。在这篇论文中，我们将非法帐户检测问题定义为一个分类任务，并提出了 DIAM，一种新的多格 neural network 模型，用于有效地检测 криптовалюTransaction 网络上的非法帐户。首先，DIAM 包括一个 Edge2Seq 模块，可以自动学习有效的节点表示，保留并行边的交易模式特征，通过考虑边Attributes和导向边顺序依赖关系。然后，DIAM 使用一种新的多格缺失（MGD）模块，通过一种 Well-designed 的消息传递机制，捕捉非法节点与正常节点之间的不一致特征，并通过注意力机制进行强调。将所有技术组合在一起，DIAM 在端到端方式进行训练。对于 14 个现有解决方案，我们进行了广泛的实验，测试了在 4 个大型 криптовалюTransaction 数据集上的性能。结果表明，DIAM 在检测非法帐户方面具有最高的精度，同时具有高效性。例如，在一个 Bitcoin 数据集上，DIAM 的 F1 分数为 96.55%，远高于最佳竞争者的 F1 分数 83.92%。
</details></li>
</ul>
<hr>
<h2 id="Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance"><a href="#Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance" class="headerlink" title="Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance"></a>Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01448">http://arxiv.org/abs/2309.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Shenzhi Wang, Qihang Zhang, Gao Huang, Shiji Song</li>
<li>for: 提高Offline Reinforcement Learning（RL）的策略优化效果，解决分布shift问题。</li>
<li>methods: 提出了一种基于引导网络的Plug-in方法，使用只需几个专家示范来自适应性地确定每个样本的策略改进和策略限制的重要性。</li>
<li>results: 经过广泛的实验表明，GORL可以轻松地安装在大多数Offline RL算法上，并且具有 statistically significant performance improvements。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) optimizes the policy on a previously collected dataset without any interactions with the environment, yet usually suffers from the distributional shift problem. To mitigate this issue, a typical solution is to impose a policy constraint on a policy improvement objective. However, existing methods generally adopt a ``one-size-fits-all'' practice, i.e., keeping only a single improvement-constraint balance for all the samples in a mini-batch or even the entire offline dataset. In this work, we argue that different samples should be treated with different policy constraint intensities. Based on this idea, a novel plug-in approach named Guided Offline RL (GORL) is proposed. GORL employs a guiding network, along with only a few expert demonstrations, to adaptively determine the relative importance of the policy improvement and policy constraint for every sample. We theoretically prove that the guidance provided by our method is rational and near-optimal. Extensive experiments on various environments suggest that GORL can be easily installed on most offline RL algorithms with statistically significant performance improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>Offline reinforcement learning (RL) 优化策略基于先前收集的数据集，无需与环境交互，然而通常受到分布问题的影响。为解决这个问题，一般采用策略约束对策进步目标进行做出约束。然而，现有方法通常采用“一个适用于所有样本”的做法，即保留每个样本集中或者整个offline数据集中的所有样本的一个改进-约束平衡。在这种情况下，我们认为不同的样本应该被对待不同的策略约束强度。基于这个想法，我们提出了一种名为指导式OfflineRL（GORL）的新方法。GORL使用一个引导网络，以及只需几个专家示范，来动态确定每个样本的策略改进和策略约束之间的相对重要性。我们证明了我们的方法提供的指导是理性的和近似优化的。广泛的实验表明，GORL可以轻松地在大多数OfflineRL算法上安装，并且具有 statistically significant的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity"><a href="#Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity" class="headerlink" title="Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity"></a>Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01424">http://arxiv.org/abs/2309.01424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nour Abdelmoneim, Dattaraj B. Dhuri, Dimitra Atri, Germán Martínez<br>for: This paper aims to improve the accuracy and efficiency of Martian climate modeling using machine learning techniques.methods: The authors use a deep neural network to model relative humidity in Gale Crater, based on simulated meteorological variables from a Global Circulation Model. They also utilize an interpretable model architecture to analyze the internal mechanisms and decision-making processes of the model.results: The authors achieve a mean error of 3% and an $R^2$ score of 0.92 in predicting relative humidity, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. The approach can also be used to fill spatial and temporal gaps in observations, providing a fast and efficient method for modeling climate variables on Mars.Here is the Chinese translation of the three points:for: 这篇论文目的是使用机器学习技术提高火星气候模拟的准确性和效率。methods: 作者使用深度神经网络来模拟加莱沟的相对湿度，基于火星全球气候模型生成的 simulate 的物理变量。他们还利用可解释的模型架构来分析模型内部的机制和决策过程。results: 作者在predicting相对湿度方面达到了3%的平均误差和0.92的$R^2$分数，其中月均表层水层厚度、气层高度、循环风速和太阳zenith角被确定为主要的预测因素。此方法可以填充观测数据中的空间和时间间隔，提供一种快速和高效的火星气候模拟方法。<details>
<summary>Abstract</summary>
For the past several decades, numerous attempts have been made to model the climate of Mars with extensive studies focusing on the planet's dynamics and the understanding of its climate. While physical modeling and data assimilation approaches have made significant progress, uncertainties persist in comprehensively capturing and modeling the complexities of Martian climate. In this work, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory ``Curiosity'' rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3\% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details>
<details>
<summary>摘要</summary>
For several decades, numerous attempts have been made to model the climate of Mars, with extensive studies focusing on the planet's dynamics and understanding its climate. Although physical modeling and data assimilation approaches have made significant progress, uncertainties still exist in comprehensively capturing and modeling the complexities of Martian climate. In this study, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory "Curiosity" rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision-making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details></li>
</ul>
<hr>
<h2 id="Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging"><a href="#Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging" class="headerlink" title="Towards frugal unsupervised detection of subtle abnormalities in medical imaging"></a>Towards frugal unsupervised detection of subtle abnormalities in medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02458">http://arxiv.org/abs/2309.02458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geoffroyo/onlineem">https://github.com/geoffroyo/onlineem</a></li>
<li>paper_authors: Geoffroy Oudoumanessah, Carole Lartizien, Michel Dojat, Florence Forbes</li>
<li>for: 这篇论文目的是为了探讨医疗影像中的异常检测，特别是在没有标注的情况下。</li>
<li>methods: 这篇论文使用了混合分布的方法，这些方法具有广泛适用于不同数据和任务的特点，并且不需要过多的设计或对应。</li>
<li>results: 这篇论文的结果显示，这些混合分布方法可以实现高精度和高效的异常检测，并且可以处理大量数据。特别是在检测 Parkinson 病患的 MR 脑成像中，可以显示出病理变化，并且与 Hoehn 和 Yahr scale的变化相符。<details>
<summary>Abstract</summary>
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data volumes as they require high memory usage. To address this issue, we propose to incrementally compute inferential quantities. This online approach is illustrated on the challenging detection of subtle abnormalities in MR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The identified structural abnormalities are consistent with the disease progression, as accounted by the Hoehn and Yahr scale.
</details>
<details>
<summary>摘要</summary>
医学成像异常检测在没有标注异常情况下是一个挑战。这个问题可以通过无监督异常检测（USAD）方法解决，这些方法可以标识不符合参照模型的normal profile。人工神经网络已经广泛应用于USAD，但它们通常不能实现最佳的均衡 между精度和计算开销。作为一个alternative，我们研究混合分布的使用。这种方法的 universality 在多种数据和任务中得到了广泛的认可，而不需要过度的设计或调整。它们的表达能力使得它们成为质量异常检测的好 кандидат。但标准估计过程，如期望最大化算法，不适用于大量数据，因为它们需要高的内存使用。为解决这个问题，我们提议逐步计算推理量。这种在线方法在MR brain scan中检测parkinsonian patients的轻微异常情况上进行了示例。检测到的结构异常情况与疾病进程相符，如根据Hoehn和Yahr scale的评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design"><a href="#Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design" class="headerlink" title="Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design"></a>Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01408">http://arxiv.org/abs/2309.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Engel, Leon Sick, Timo Ropinski</li>
<li>for: 这篇论文主要关注于volume rendering中的传输函数设定，它们是用于分类结构物和赋予光学性质的1D或2D函数。</li>
<li>methods: 该论文提出了一种基于自我supervised pre-trained vision transformers的新方法，用于互动地定义传输函数。用户只需在slice viewer中选择结构物，方法会自动选择相似的结构物基于由神经网络提取的高级特征。</li>
<li>results: 与传统学习基于的方法相比，该方法不需要训练模型，允许快速的推理，使得可以在几秒钟内设定传输函数，从而实现互动的探索。此外，该方法可以减少必要的标注量，通过在用户设定传输函数时提供反馈，使得用户可以更专注于需要标注的结构物。<details>
<summary>Abstract</summary>
In volume rendering, transfer functions are used to classify structures of interest, and to assign optical properties such as color and opacity. They are commonly defined as 1D or 2D functions that map simple features to these optical properties. As the process of designing a transfer function is typically tedious and unintuitive, several approaches have been proposed for their interactive specification. In this paper, we present a novel method to define transfer functions for volume rendering by leveraging the feature extraction capabilities of self-supervised pre-trained vision transformers. To design a transfer function, users simply select the structures of interest in a slice viewer, and our method automatically selects similar structures based on the high-level features extracted by the neural network. Contrary to previous learning-based transfer function approaches, our method does not require training of models and allows for quick inference, enabling an interactive exploration of the volume data. Our approach reduces the amount of necessary annotations by interactively informing the user about the current classification, so they can focus on annotating the structures of interest that still require annotation. In practice, this allows users to design transfer functions within seconds, instead of minutes. We compare our method to existing learning-based approaches in terms of annotation and compute time, as well as with respect to segmentation accuracy. Our accompanying video showcases the interactivity and effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
在Volume Rendering中，传输函数用于分类结构体 интереса，并将光学性质如颜色和透明度赋予这些结构体。传输函数通常是1D或2D函数，它们将简单特征映射到这些光学性质。由于设计传输函数的过程通常是繁琐和不直观的，因此有几种方法被提出来对其进行交互式规定。在这篇论文中，我们提出了一种新的方法，使用自动提取的高级特征来定义传输函数。用户只需选择 interess structures in a slice viewer，我们的方法会自动选择相似的结构，基于通过神经网络提取的高级特征。与之前的学习基于的传输函数方法不同，我们的方法不需要训练模型，可以快速进行推理，使得可以在Volume Data中进行交互式探索。我们的方法可以减少需要的注释量，通过在用户操作时提供反馈，使得用户可以专注于需要注释的结构体。在实践中，我们的方法可以在秒钟内完成设计传输函数，而不是分钟内。我们与现有的学习基于的方法进行比较，包括注释量和计算时间，以及 segmentation 的准确率。我们的视频证明了我们的方法的交互性和效果。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance"><a href="#Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance" class="headerlink" title="Differentiable Bayesian Structure Learning with Acyclicity Assurance"></a>Differentiable Bayesian Structure Learning with Acyclicity Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01392">http://arxiv.org/abs/2309.01392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Duy Tran, Phuoc Nguyen, Bao Duong, Thin Nguyen</li>
<li>for: 本研究旨在提出一种新的方法，可以强制限制生成的图为无环图，同时降低推理复杂度。</li>
<li>methods: 本研究使用了一种Integration of topological orderings的方法，通过约束生成图的结构，确保图为无环图。</li>
<li>results: 实验结果表明，我们的方法可以比相关的 bayesian 分数基于方法更高效，并且能够确保生成的图为无环图。<details>
<summary>Abstract</summary>
Score-based approaches in the structure learning task are thriving because of their scalability. Continuous relaxation has been the key reason for this advancement. Despite achieving promising outcomes, most of these methods are still struggling to ensure that the graphs generated from the latent space are acyclic by minimizing a defined score. There has also been another trend of permutation-based approaches, which concern the search for the topological ordering of the variables in the directed acyclic graph in order to limit the search space of the graph. In this study, we propose an alternative approach for strictly constraining the acyclicty of the graphs with an integration of the knowledge from the topological orderings. Our approach can reduce inference complexity while ensuring the structures of the generated graphs to be acyclic. Our empirical experiments with simulated and real-world data show that our approach can outperform related Bayesian score-based approaches.
</details>
<details>
<summary>摘要</summary>
Score-based方法在结构学习任务中升温，主要是因为它们可扩展性。继续relaxation是这一进步的关键。虽然它们在实现出色的结果，但大多数这些方法仍然困难确保生成的图从 latent space 中的图是无向的，通过定义的分数来最小化。此外，there has also been another trend of permutation-based approaches，即在搜索 directed acyclic graph 中变量的顺序排序，以限制搜索空间的图。在这种研究中，我们提出了一种替代方法，即通过 integrate 知识从 topological orderings 中来严格限制图的无向性。我们的方法可以降低推理复杂性，同时确保生成的图是无向的。我们的实验表明，我们的方法可以超越相关的 Bayesian score-based 方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences"><a href="#Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences" class="headerlink" title="Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences"></a>Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01381">http://arxiv.org/abs/2309.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sengopal/classic-ml-review-paper">https://github.com/sengopal/classic-ml-review-paper</a></li>
<li>paper_authors: Senthilkumar Gopal</li>
<li>for: 本文主要用于对常见的超vised学习算法进行实验和数学分析，以了解它们在不同情况下的性能和特性。</li>
<li>methods: 本文使用了多种常见的超vised学习算法，包括决策树、扩展、支持向量机和k-最近邻居。</li>
<li>results: 本文对这些算法在稀疏表格数据上进行分类任务进行了评估，并观察了不同的超参数对这些算法的影响。结果表明，这些经典算法在稀疏数据上也能够保持良好的泛化能力，并且可以通过不同的超参数来提高分类精度。<details>
<summary>Abstract</summary>
Classic machine learning algorithms have been reviewed and studied mathematically on its performance and properties in detail. This paper intends to review the empirical functioning of widely used classical supervised learning algorithms such as Decision Trees, Boosting, Support Vector Machines, k-nearest Neighbors and a shallow Artificial Neural Network. The paper evaluates these algorithms on a sparse tabular data for classification task and observes the effect on specific hyperparameters on these algorithms when the data is synthetically modified for higher noise. These perturbations were introduced to observe these algorithms on their efficiency in generalizing for sparse data and their utility of different parameters to improve classification accuracy. The paper intends to show that these classic algorithms are fair learners even for such limited data due to their inherent properties even for noisy and sparse datasets.
</details>
<details>
<summary>摘要</summary>
经典机器学习算法已经详细地研究和分析其性能和属性。这篇论文想要评估广泛使用的古典监督学习算法，如决策树、扩展、支持向量机器、k最近邻居和杂谱神经网络。这篇论文评估这些算法在稀疏表格分类任务上的表现，并观察这些算法对不同的超参数的影响，当数据被人工改变以增加噪音时。这些改变是为了评估这些算法在欠拥有数据的情况下的泛化能力和不同超参数的使用来提高分类精度。这篇论文想要证明这些经典算法在有限数据的情况下仍然是公正学习器。
</details></li>
</ul>
<hr>
<h2 id="ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction"><a href="#ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction" class="headerlink" title="ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction"></a>ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01370">http://arxiv.org/abs/2309.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kracr/reonto-relation-extraction">https://github.com/kracr/reonto-relation-extraction</a></li>
<li>paper_authors: Monika Jain, Kuldeep Singh, Raghava Mutharaju</li>
<li>for: 本研究旨在提高生物医学文献中关系抽取（RE）任务的精度，提出了一种基于神经符号知识的新方法called ReOnto。</li>
<li>methods: ReOnto使用图神经网络获得句子表示，并利用公共可 accessible的 ontologies 作为先知知识来确定两个实体之间的句子关系。</li>
<li>results: 实验结果表明，使用ontologies 作为Symbolic知识和图神经网络结合可以提高RE任务的精度，在两个公共生物医学数据集 BioRel 和 ADE 上，我们的方法比基eline 高出约3%。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is the task of extracting semantic relationships between entities in a sentence and aligning them to relations defined in a vocabulary, which is generally in the form of a Knowledge Graph (KG) or an ontology. Various approaches have been proposed so far to address this task. However, applying these techniques to biomedical text often yields unsatisfactory results because it is hard to infer relations directly from sentences due to the nature of the biomedical relations. To address these issues, we present a novel technique called ReOnto, that makes use of neuro symbolic knowledge for the RE task. ReOnto employs a graph neural network to acquire the sentence representation and leverages publicly accessible ontologies as prior knowledge to identify the sentential relation between two entities. The approach involves extracting the relation path between the two entities from the ontology. We evaluate the effect of using symbolic knowledge from ontologies with graph neural networks. Experimental results on two public biomedical datasets, BioRel and ADE, show that our method outperforms all the baselines (approximately by 3\%).
</details>
<details>
<summary>摘要</summary>
relation extraction (RE) 是将实体之间的 semantic 关系提取出来，并将其与知识图(KG)或ontology中的关系对应的任务。目前已经有很多方法被提出来解决这个任务。然而，在生物医学文本中应用这些技术时，通常会得到不满足的结果，因为生物医学关系很难直接从句子中推理出来。为了解决这些问题，我们提出了一种新的技术 called ReOnto，该技术利用 neural symbolic knowledge 来解决 RE 任务。ReOnto 使用图神经网络来获取句子表示，并利用公开 accessible 的 ontology 作为先知知识来确定两个实体之间的句子关系。该方法包括从 ontology 中提取两个实体之间的关系路径。我们通过将符号知识从 ontology 与图神经网络结合来评估符号知识的效果。我们在两个公共的生物医学数据集（BioRel 和 ADE）上进行实验，结果表明，我们的方法在所有基线方法（约为 3%）之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance"><a href="#Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance" class="headerlink" title="Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance"></a>Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01363">http://arxiv.org/abs/2309.01363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Lee, Myeongjin Shin, Junseo Lee, Kabgyun Jeong</li>
<li>for: 该研究旨在提出一种基于量子机器学习的新方法，用于解决生成 adversarial networks 中的模式塌陷问题。</li>
<li>methods: 该方法基于量子 neural network 的 Gradient Descent 算法，并使用 Mutual Information Neural Estimator (MINE) 来估算高维连续Random variables 之间的共聚信息。</li>
<li>results: 该方法能够成功地解决生成 adversarial networks 中的模式塌陷问题，并应用于金融领域，例如动态资产分配以生成股票返杂分布。<details>
<summary>Abstract</summary>
One of the most promising applications in the era of NISQ (Noisy Intermediate-Scale Quantum) computing is quantum machine learning. Quantum machine learning offers significant quantum advantages over classical machine learning across various domains. Specifically, generative adversarial networks have been recognized for their potential utility in diverse fields such as image generation, finance, and probability distribution modeling. However, these networks necessitate solutions for inherent challenges like mode collapse. In this study, we capitalize on the concept that the estimation of mutual information between high-dimensional continuous random variables can be achieved through gradient descent using neural networks. We introduce a novel approach named InfoQGAN, which employs the Mutual Information Neural Estimator (MINE) within the framework of quantum generative adversarial networks to tackle the mode collapse issue. Furthermore, we elaborate on how this approach can be applied to a financial scenario, specifically addressing the problem of generating portfolio return distributions through dynamic asset allocation. This illustrates the potential practical applicability of InfoQGAN in real-world contexts.
</details>
<details>
<summary>摘要</summary>
一个有前途的应用在NISQ（噪声中间规模量计算）时代是量子机器学习。量子机器学习在不同领域提供了显著的量子优势。特别是生成对抗网络在图像生成、金融和概率分布模型领域具有潜在的应用前景。然而，这些网络具有内置的挑战，如模式塌缩。在本研究中，我们利用Gradient Descent使 neural network estimate mutual information between high-dimensional continuous random variables的概念，并提出一种名为InfoQGAN的新方法。InfoQGAN在量子生成对抗网络框架中使用Mutual Information Neural Estimator（MINE）来解决模式塌缩问题。此外，我们还详细介绍了如何应用InfoQGAN到金融场景中，具体是通过动态资产配置来生成 portefolio return distribution。这 Illustrates the potential practical applicability of InfoQGAN in real-world contexts.
</details></li>
</ul>
<hr>
<h2 id="Random-Projections-of-Sparse-Adjacency-Matrices"><a href="#Random-Projections-of-Sparse-Adjacency-Matrices" class="headerlink" title="Random Projections of Sparse Adjacency Matrices"></a>Random Projections of Sparse Adjacency Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01360">http://arxiv.org/abs/2309.01360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Qiu</li>
<li>for: 本文研究了一种随机投影方法，用于表示稀疏图。</li>
<li>methods: 本文使用了随机投影法，并证明了这种方法可以保持稀疏图的功能，同时具有一些有利的特性，如能够表示不同大小的图和不同顶点集的图在同一空间中。</li>
<li>results: 本文显示了随机投影法可以准确地保持图的操作，并且可以随着顶点数量的增加，Scale linearly with the number of vertices while accurately retaining first-order graph information。<details>
<summary>Abstract</summary>
We analyze a random projection method for adjacency matrices, studying its utility in representing sparse graphs. We show that these random projections retain the functionality of their underlying adjacency matrices while having extra properties that make them attractive as dynamic graph representations. In particular, they can represent graphs of different sizes and vertex sets in the same space, allowing for the aggregation and manipulation of graphs in a unified manner. We also provide results on how the size of the projections need to scale in order to preserve accurate graph operations, showing that the size of the projections can scale linearly with the number of vertices while accurately retaining first-order graph information. We conclude by characterizing our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details>
<details>
<summary>摘要</summary>
我们分析了一种随机投影方法 для邻接矩阵，研究其在表示稀疏图的实用性。我们表明这些随机投影保留了它们的基本邻接矩阵功能，同时具有一些有利的特性，使其成为动态图表示的优选。具体来说，它们可以表示不同大小的图和顶点集在同一个空间中， allowing for the aggregation and manipulation of graphs in a unified manner。我们还提供了保持 precisions 的投影大小的规则，显示投影大小可以线性增长与顶点数量相关，并准确地保持首领信息。我们最后 Characterize our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details></li>
</ul>
<hr>
<h2 id="MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware"><a href="#MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware" class="headerlink" title="MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware"></a>MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01350">http://arxiv.org/abs/2309.01350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas</li>
<li>For: The paper is written to address the slow adoption of machine learning (ML) based solutions against malware threats, and to introduce a new method for precise identification of novel malware families.* Methods: The paper proposes a new method that unifies the capability for malware&#x2F;benign-ware classification and malware family classification into a single framework.* Results: The paper showcases preliminary capabilities of the new method and demonstrates its ability to perform precise identification of novel malware families.<details>
<summary>Abstract</summary>
Malware is one of the most dangerous and costly cyber threats to national security and a crucial factor in modern cyber-space. However, the adoption of machine learning (ML) based solutions against malware threats has been relatively slow. Shortcomings in the existing ML approaches are likely contributing to this problem. The majority of current ML approaches ignore real-world challenges such as the detection of novel malware. In addition, proposed ML approaches are often designed either for malware/benign-ware classification or malware family classification. Here we introduce and showcase preliminary capabilities of a new method that can perform precise identification of novel malware families, while also unifying the capability for malware/benign-ware classification and malware family classification into a single framework.
</details>
<details>
<summary>摘要</summary>
马拉ware是现代计算机网络中最危险和成本最高的网络威胁之一，同时也是现代网络空间中一个关键因素。然而，使用机器学习（ML）解决马拉ware威胁的采用速度相对较慢。现有的ML方法缺乏现实世界中真实的挑战，如检测新型马拉ware。此外，现有的提议的ML方法通常是为马拉ware/正常软件分类或马拉ware家族分类而设计的。我们现在介绍一种新的方法，可以准确地识别新型马拉ware家族，同时也整合了malware/正常软件分类和马拉ware家族分类的能力。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification"><a href="#Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification"></a>Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01342">http://arxiv.org/abs/2309.01342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marzi Heidari, Abdullah Alchihabi, Qing En, Yuhong Guo</li>
<li>for: 这篇论文targets cross-domain few-shot classification problem, which is more challenging than in-domain few-shot learning due to domain shifts.</li>
<li>methods: 本文提出了一种名为 Adaptive Parametric Prototype Learning (APPL) 的方法，它是一种基于元学习的方法，通过强制在查询集上实现prototype-based regularization来学习分类标本。</li>
<li>results: 实验结果显示，APPL 在多个 cross-domain few-shot benchmark dataset上表现较好，超过了许多现有的州际几招学习方法。<details>
<summary>Abstract</summary>
Cross-domain few-shot classification induces a much more challenging problem than its in-domain counterpart due to the existence of domain shifts between the training and test tasks. In this paper, we develop a novel Adaptive Parametric Prototype Learning (APPL) method under the meta-learning convention for cross-domain few-shot classification. Different from existing prototypical few-shot methods that use the averages of support instances to calculate the class prototypes, we propose to learn class prototypes from the concatenated features of the support set in a parametric fashion and meta-learn the model by enforcing prototype-based regularization on the query set. In addition, we fine-tune the model in the target domain in a transductive manner using a weighted-moving-average self-training approach on the query instances. We conduct experiments on multiple cross-domain few-shot benchmark datasets. The empirical results demonstrate that APPL yields superior performance than many state-of-the-art cross-domain few-shot learning methods.
</details>
<details>
<summary>摘要</summary>
cross-domain 少数目标分类比其域内对应的问题更加具有挑战性，这是因为训练和测试任务之间存在域shift。在这篇论文中，我们开发了一种名为 Adaptive Parametric Prototype Learning（APPL）方法，该方法基于元学习惯例下进行cross-domain 少数目标分类。与现有的概念性少数目标方法不同，我们提议使用支持集中的 concatenated 特征来学习类prototype，并通过在查询集上应用 prototype-based 正则化来meta-learn模型。此外，我们在目标域中进行了适应性训练，使用权重移动平均自适应法在查询集上进行了微调。我们在多个 cross-domain 少数目标benchmark datasets上进行了实验，结果显示，APPL的性能优于许多当前state-of-the-art cross-domain 少数目标学习方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach"><a href="#Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach" class="headerlink" title="Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach"></a>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01336">http://arxiv.org/abs/2309.01336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan</li>
<li>for: 预测电力需求的准确性对小聚合负荷Setting中的微型电网运行进行管理是非常重要。由于低聚合，电力需求可以高度抽象和点估计会导致误差的膨胀。这篇论文提出了一种Interval estimation算法来提供未来值可能存在的范围，以便量化点估计附近的误差。</li>
<li>methods: 本论文使用了一种机器学习算法来获得点估计电力需求和相应的偏差值，并将这些偏差值存储在内存中。然后，使用不监督学习算法将天气相似的日期分组成 clusters，并使用这些 clusters来分区内存。在测试日期上，使用点估计来找到最相似的天气日期，然后从选择的cluster中bootstrap偏差。</li>
<li>results: 本论文对实际的电力需求数据进行了 evaluate，并与其他 bootstrapping 方法进行了比较，包括不同的信度范围。<details>
<summary>Abstract</summary>
Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
</details>
<details>
<summary>摘要</summary>
正确的电力需求预测是小聚合负载设定 like Microgrid 的管理操作中的必要条件。由于低聚合，电力需求可以具有高度� stoochastic 的特性，单点估计将会导致膨胀的错误。在这种情况下，间隔估计将提供未来值的范围内的值，并帮助量化这些点估计的错误。本文提出了一种剩余 bootstrap 算法，用于生成日前电力需求的间隔估计。使用机器学习算法取得电力需求点估计和相应的剩余在训练集上。取得的剩余会被储存，并且将天数分组为相似的需求模式cluster。在试验天数上使用这些cluster来选择最相似的天数，然后从选择的cluster中bootstrap 剩余。这个算法被评估了真实的电力需求数据 from EULR（End Use Load Research），并与其他剩余方法进行比较，以评估不同的信度 интервала。
</details></li>
</ul>
<hr>
<h2 id="In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems"><a href="#In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems" class="headerlink" title="In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems"></a>In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01335">http://arxiv.org/abs/2309.01335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Weiming Liu, Jun Wang, Wenjie Cheng, Yuyuan Li</li>
<li>for:  addressing the user-oriented fairness (UOF) issue in recommender systems, where the existing research is limited and fails to deal with the root cause of the issue.</li>
<li>methods:  proposing an In-processing User Constrained Dominant Sets (In-UCDS) framework, which is a general framework that can be applied to any backbone recommendation model to achieve user-oriented fairness. The framework consists of two stages: UCDS modeling stage and in-processing training stage.</li>
<li>results:  outperforming the state-of-the-art methods in addressing the UOF issue while maintaining the overall recommendation performance, as demonstrated by comprehensive experiments on three real-world datasets.<details>
<summary>Abstract</summary>
Recommender systems are typically biased toward a small group of users, leading to severe unfairness in recommendation performance, i.e., User-Oriented Fairness (UOF) issue. The existing research on UOF is limited and fails to deal with the root cause of the UOF issue: the learning process between advantaged and disadvantaged users is unfair. To tackle this issue, we propose an In-processing User Constrained Dominant Sets (In-UCDS) framework, which is a general framework that can be applied to any backbone recommendation model to achieve user-oriented fairness. We split In-UCDS into two stages, i.e., the UCDS modeling stage and the in-processing training stage. In the UCDS modeling stage, for each disadvantaged user, we extract a constrained dominant set (a user cluster) containing some advantaged users that are similar to it. In the in-processing training stage, we move the representations of disadvantaged users closer to their corresponding cluster by calculating a fairness loss. By combining the fairness loss with the original backbone model loss, we address the UOF issue and maintain the overall recommendation performance simultaneously. Comprehensive experiments on three real-world datasets demonstrate that In-UCDS outperforms the state-of-the-art methods, leading to a fairer model with better overall recommendation performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>推荐系统通常偏向一小组用户，导致推荐性能的严重不公，即用户 oriented fairness (UOF) 问题。现有的 UOF 研究有限，无法解决 UOF 问题的根本原因：用户和不advantaged 用户之间的学习过程不公。为解决这个问题，我们提出了 In-processing User Constrained Dominant Sets (In-UCDS) 框架，这是一个可以应用于任何基础推荐模型来实现用户 oriented fairness 的通用框架。我们将 In-UCDS 分成两个阶段：UCDS 模型化阶段和在处理训练阶段。在 UCDS 模型化阶段，每个劣势用户都提取一个受限制的主导集 (用户群)，其中包含一些有利用户的用户。在处理训练阶段，我们将劣势用户的表示移动到其相应的群体中，通过计算公平损失来实现公平性。将公平损失与基础模型损失结合，我们同时解决 UOF 问题和维护推荐性能。我们在三个实际数据集上进行了广泛的实验，并证明 In-UCDS 超过了当前状态的方法，导致一个更公平的模型，同时保持推荐性能。
</details></li>
</ul>
<hr>
<h2 id="An-ML-assisted-OTFS-vs-OFDM-adaptable-modem"><a href="#An-ML-assisted-OTFS-vs-OFDM-adaptable-modem" class="headerlink" title="An ML-assisted OTFS vs. OFDM adaptable modem"></a>An ML-assisted OTFS vs. OFDM adaptable modem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01319">http://arxiv.org/abs/2309.01319</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. Zakir Ahmed, Hamid R. Sadjadpour</li>
<li>for: 这篇论文是关于高速移动场景下的信号处理技术选择和改进。</li>
<li>methods: 这篇论文使用了深度神经网络（DNN）基于的变换方案，以优化信号处理链的选择，以提高含义平均方差（MSE）性能。</li>
<li>results:  simulations 表明，使用提议的变换方案可以提高信号处理的MSE性能，比OFDM和OTFS signal processing chain更佳。<details>
<summary>Abstract</summary>
The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient to doubly-dispersive channels, which impacts high mobility scenarios. On the other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms enjoy the benefits of the reuse of legacy architectures, simplicity of receiver design, and low-complexity detection. Several studies that compare the performance of OFDM and OTFS have indicated mixed outcomes due to the plethora of system parameters at play beyond high-mobility conditions. In this work, we exemplify this observation using simulations and propose a deep neural network (DNN)-based adaptation scheme to switch between using either an OTFS or OFDM signal processing chain at the transmitter and receiver for optimal mean-squared-error (MSE) performance. The DNN classifier is trained to switch between the two schemes by observing the channel condition, received SNR, and modulation format. We compare the performance of the OTFS, OFDM, and the proposed switched-waveform scheme. The simulations indicate superior performance with the proposed scheme with a well-trained DNN, thus improving the MSE performance of the communication significantly.
</details>
<details>
<summary>摘要</summary>
“orthogonal-time-frequency-space（OTFS）信号处理显示高移动场景下具有双杂分通道的抗性，而orthogonal-frequency-division-multiplexing（OFDM）波形具有 reuse 的传统架构、接收器设计的简单性和低复杂性检测的优点。但是，由于系统参数的各种因素，一些研究对OFDM和OTFS的性能比较结果呈混合的趋势。在这种情况下，我们通过 simulations 和深度神经网络（DNN）化适应方案来 switching  между使用 OTFS 或 OFDM 信号处理链，以实现优化的mean-squared-error（MSE）性能。DNN 分类器通过观察通道条件、接收信号强度和模ulationFormat来选择使用哪一种信号处理链。我们对 OTFS、OFDM 和我们提议的 switched-waveform  scheme 进行比较，实验结果显示，将 DNN Well 训练后，提出的方案可以显著改善通信的MSE性能。”
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates"><a href="#Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates" class="headerlink" title="Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates"></a>Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03227">http://arxiv.org/abs/2309.03227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysjegal/ysjegal-drug-repositioning">https://github.com/ysjegal/ysjegal-drug-repositioning</a></li>
<li>paper_authors: Yongseung Jegal, Jaewoong Choi, Jiho Lee, Ki-Su Park, Seyoung Lee, Janghyeok Yoon</li>
<li>For: The paper is written to explore a novel protocol for identifying drug repositioning candidates with both technological potential and scientific evidence.* Methods: The protocol involves constructing a scientific biomedical knowledge graph (s-BKG) and a patent-informed biomedical knowledge graph (p-BKG), and using a graph embedding protocol to ascertain the structure of the p-BKG and calculate the relevance scores of potential drug candidates.* Results: The case study on Alzheimer’s disease demonstrates the efficacy and feasibility of the proposed method, and the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.Here’s the information in Simplified Chinese text:* For: 本研究是为了探讨一种新的药物重定向候选者选择方法，以实现药物重定向的科学价值和实际应用。* Methods: 方法包括构建一个科学生物医学知识图(s-BKG)和一个专利信息 Informed biomedical knowledge graph(p-BKG)，并使用图像嵌入方法来确定p-BKG的结构，计算潜在药物候选者与target疾病相关专利的相似度。* Results: 对阿尔茨heimer病的case study表明方法的有效性和实施性，并且预计通过计算机发现和成功应用在药物重定向研究中，能够填补计算发现和成功应用之间的空白。<details>
<summary>Abstract</summary>
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we developed a graph embedding protocol to ascertain the structure of the p-BKG, thereby calculating the relevance scores of those candidates with target disease-related patents to evaluate their technological potential. Our case study on Alzheimer's disease demonstrates its efficacy and feasibility, while the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.
</details>
<details>
<summary>摘要</summary>
药物重新定位策略已经在计算科学文献中得到了越来越多的探索，使用生物医学数据库。然而，技术潜力具有药物重新定位候选者往往被忽略。本研究提出了一种新的协议，用于全面分析各种来源，例如药品专利和生物医学数据库，并从科学角度鉴定药物重新定位候选者。为此，我们首先建立了一个科学生物医学知识图（s-BKG），其中包含药品、疾病和基因之间的关系，从生物医学数据库中获取。我们的协议包括在s-BKG中找到药品，它们与目标疾病之间的关系较弱，但在s-BKG中与其他药品和疾病之间有紧密的关系。我们还建立了一个受专利信息支持的生物医学知识图（p-BKG），并开发了一种图像嵌入协议，以确定p-BKG的结构，并计算与疾病相关专利的相关性分数，以评估候选者的技术潜力。我们的案例研究表明，这种方法在阿尔茨海默病中得到了成功和可行性，而量化结果和系统方法均预期能够把计算发现与成功市场应用之间的空隙填充。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection"><a href="#Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection" class="headerlink" title="Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection"></a>Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01312">http://arxiv.org/abs/2309.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Audrey Paleczny, Shubham Parab, Maxwell Zhang</li>
<li>For: The paper aims to improve the accuracy of Magnetic Resonance Imaging (MRI) classification for Alzheimer’s disease using Convolutional Neural Networks (CNNs) and out-of-distribution (OOD) detection.* Methods: The authors trained supervised Random Forest models with segmented brain volumes and CNN outputs to classify different Alzheimer’s stages. They also applied OOD detection to the CNN model to reduce false diagnoses.* Results: The CNN model outperformed the segmented volume model with an accuracy of 98% for detection and 95% for classification. The OOD detection enabled the CNN model to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction.Here are the three points in Simplified Chinese text:</li>
<li>for: 本研究使用MRI进行阿尔ツ海默病诊断，使用Convolutional Neural Networks (CNNs)和out-of-distribution (OOD)检测来提高诊断精度。</li>
<li>methods: 作者使用了supervised Random Forest模型，并将分割的脑部Volume和CNN输出作为输入来分类不同的阿尔ツ海默病阶段。他们还应用了OOD检测到CNN模型，以降低false positives的发生。</li>
<li>results: CNN模型比分割模型高度出色，具有98%的检测精度和95%的分类精度。OOD检测使得CNN模型能够准确地标识出脑出血图像，准确率为96%，并没有对整体精度造成明显的影响。<details>
<summary>Abstract</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. Source code available upon request.
</details>
<details>
<summary>摘要</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. 源代码可以根据需要请求。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles"><a href="#Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles" class="headerlink" title="Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles"></a>Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01297">http://arxiv.org/abs/2309.01297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Riley Kilfoyle, Zixiang Xiong, Ligang Lu</li>
<li>for: 这篇论文的目的是提出一个可靠且高效的时间序列预测模型，用于预测电动车充电站的能源利用状况。</li>
<li>methods: 这篇论文使用了最新的transformer架构和联边学习（Federated Learning）技术，实现了分布式训练。</li>
<li>results: 这篇论文的预测性能与其他模型相比，具有与其他模型相同的性能，但是训练时间的数据资料负载量则明显减少。此外，这个模型还能够在其他时间序列 datasets 上进行通用化预测。<details>
<summary>Abstract</summary>
Machine learning (ML) applications to time series energy utilization forecasting problems are a challenging assignment due to a variety of factors. Chief among these is the non-homogeneity of the energy utilization datasets and the geographical dispersion of energy consumers. Furthermore, these ML models require vast amounts of training data and communications overhead in order to develop an effective model. In this paper, we propose a communication-efficient time series forecasting model combining the most recent advancements in transformer architectures implemented across a geographically dispersed series of EV charging stations and an efficient variant of federated learning (FL) to enable distributed training. The time series prediction performance and communication overhead cost of our FL are compared against their counterpart models and shown to have parity in performance while consuming significantly lower data rates during training. Additionally, the comparison is made across EV charging as well as other time series datasets to demonstrate the flexibility of our proposed model in generalized time series prediction beyond energy demand. The source code for this work is available at https://github.com/XuJiacong/LoGTST_PSGF
</details>
<details>
<summary>摘要</summary>
机器学习（ML）应用到时间序列能源利用预测问题是一个具有许多挑战性的任务，主要包括时间序列数据的非均匀性和能源消耗者的地域分散。此外，这些 ML 模型需要很大的训练数据和通信频率以建立有效的模型。在这篇论文中，我们提出了一个具有最新的变数架构的时间序列预测模型，利用了分布式训练的有效变体 federated learning（FL），以便在地域分散的 EV 充电站上进行分布式训练。我们的 FL 模型与对照模型相比，在预测性能和通信负载成本方面具有相等的性能，并且在训练时间中显著降低了数据资料率。此外，我们还对 EV 充电和其他时间序列数据进行比较，以示出我们的提案模型在通用时间序列预测方面的灵活性。source code 可以在 GitHub 上找到：https://github.com/XuJiacong/LoGTST_PSGF。
</details></li>
</ul>
<hr>
<h2 id="AlphaZero-Gomoku"><a href="#AlphaZero-Gomoku" class="headerlink" title="AlphaZero Gomoku"></a>AlphaZero Gomoku</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01294">http://arxiv.org/abs/2309.01294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suragnair/alpha-zero-general">https://github.com/suragnair/alpha-zero-general</a></li>
<li>paper_authors: Wen Liang, Chao Yu, Brian Whiteaker, Inyoung Huh, Hua Shao, Youzhi Liang</li>
<li>for: 研究扩展AlphaZero算法到五子棋，以提高人工智能游戏玩家的能力。</li>
<li>methods: 利用深度学习和Monte Carlo搜索算法，将AlphaZero算法应用到五子棋中。</li>
<li>results: AlphaZero算法在五子棋中表现出色，能够快速适应不同的游戏环境，并且能够增强人工智能游戏玩家的能力。<details>
<summary>Abstract</summary>
In the past few years, AlphaZero's exceptional capability in mastering intricate board games has garnered considerable interest. Initially designed for the game of Go, this revolutionary algorithm merges deep learning techniques with the Monte Carlo tree search (MCTS) to surpass earlier top-tier methods. In our study, we broaden the use of AlphaZero to Gomoku, an age-old tactical board game also referred to as "Five in a Row." Intriguingly, Gomoku has innate challenges due to a bias towards the initial player, who has a theoretical advantage. To add value, we strive for a balanced game-play. Our tests demonstrate AlphaZero's versatility in adapting to games other than Go. MCTS has become a predominant algorithm for decision processes in intricate scenarios, especially board games. MCTS creates a search tree by examining potential future actions and uses random sampling to predict possible results. By leveraging the best of both worlds, the AlphaZero technique fuses deep learning from Reinforcement Learning with the balancing act of MCTS, establishing a fresh standard in game-playing AI. Its triumph is notably evident in board games such as Go, chess, and shogi.
</details>
<details>
<summary>摘要</summary>
在过去几年，AlphaZero的在复杂游戏中表现出色，引发了广泛的关注。AlphaZero最初是设计用于围棋游戏，这种革命性的算法将深度学习技术与 Monte Carlo 搜索树（MCTS）相结合，超越了之前的顶层方法。在我们的研究中，我们扩展了AlphaZero的使用范围，将其应用于古老的策略游戏“五子棋”（也称为“五在一行”）。有趣的是，五子棋具有初始玩家偏好的偏袋问题，因此我们努力寻找一个平衡的游戏环境。我们的测试表明，AlphaZero在不同于围棋的游戏中也能够表现出色。MCTS在复杂enario中的决策过程中变得越来越普遍，特别是在棋盘游戏中。MCTS通过检查未来动作的可能性，并使用随机抽样来预测可能的结果，创造了一棵搜索树。AlphaZero技术将深度学习从回归学习与 MCTS 的平衡运算相结合，创造了一个新的游戏AI标准。其胜利特别明显在棋盘游戏such as Go、棋盘和将棋中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.LG_2023_09_04/" data-id="clmjn91mw00810j889vfigigx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/04/cs.SD_2023_09_04/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-04
        
      </div>
    </a>
  
  
    <a href="/2023/09/04/eess.IV_2023_09_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-04</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
