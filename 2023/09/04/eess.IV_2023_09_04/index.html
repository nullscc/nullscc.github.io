
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-04 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01823 repo_url: None paper_authors: Shaoyan Pan, Yiqiao Liu, S">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-04">
<meta property="og:url" content="https://nullscc.github.io/2023/09/04/eess.IV_2023_09_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01823 repo_url: None paper_authors: Shaoyan Pan, Yiqiao Liu, S">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-04T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.986Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/eess.IV_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T09:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-04
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations"><a href="#Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations" class="headerlink" title="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations"></a>Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01823">http://arxiv.org/abs/2309.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyan Pan, Yiqiao Liu, Sarah Halek, Michal Tomaszewski, Shubing Wang, Richard Baumgartner, Jianda Yuan, Gregory Goldmacher, Antong Chen</li>
<li>for: automated 3D lesion segmentation for radiomics and tumor growth modeling studies</li>
<li>methods: multi-dimension unified Swin transformer (MDU-ST) model with a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks and fine-tuning with labeled 2D and 3D volumes</li>
<li>results: significant improvement over competing models, demonstrated through Dice similarity coefficient (DSC) and Hausdorff distance (HD) on an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations.<details>
<summary>Abstract</summary>
In oncology research, accurate 3D segmentation of lesions from CT scans is essential for the modeling of lesion growth kinetics. However, following the RECIST criteria, radiologists routinely only delineate each lesion on the axial slice showing the largest transverse area, and delineate a small number of lesions in 3D for research purposes. As a result, we have plenty of unlabeled 3D volumes and labeled 2D images, and scarce labeled 3D volumes, which makes training a deep-learning 3D segmentation model a challenging task. In this work, we propose a novel model, denoted a multi-dimension unified Swin transformer (MDU-ST), for 3D lesion segmentation. The MDU-ST consists of a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, allowing it to adapt to 2D and 3D inputs and learn the corresponding semantic information in the same encoder. Based on this model, we introduce a three-stage framework: 1) leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks to learn the underlying pattern of lesion anatomy in the Swin-transformer encoder; 2) fine-tune the Swin-transformer encoder to perform 2D lesion segmentation with 2D RECIST slices to learn slice-level segmentation information; 3) further fine-tune the Swin-transformer encoder to perform 3D lesion segmentation with labeled 3D volumes. The network's performance is evaluated by the Dice similarity coefficient (DSC) and Hausdorff distance (HD) using an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations. The proposed MDU-ST demonstrates significant improvement over the competing models. The proposed method can be used to conduct automated 3D lesion segmentation to assist radiomics and tumor growth modeling studies. This paper has been accepted by the IEEE International Symposium on Biomedical Imaging (ISBI) 2023.
</details>
<details>
<summary>摘要</summary>
在肿瘤研究中，准确的3D肿瘤分割从CT扫描图中获得是至关重要的，以便肿瘤生长动态模型的建立。然而，根据RECIST标准， radiologists通常只在最大横坐标的AXIAL slice上画出肿瘤，并且只为研究目的画出一些LESION的3D分割。因此，我们有很多未标注的3D体积和标注的2D图像，而且罕见的标注3D体积，这使得培育深度学习3D分割模型成为一项挑战。在这种情况下，我们提出了一种新的模型，称为多维度统一Swin变换（MDU-ST），用于肿瘤分割。MDU-ST包括Swin变换encoder和卷积神经网络（CNN）解决器，允许它适应2D和3D输入，并学习相应的semantic信息。基于这种模型，我们提出了一个三个阶段框架：1）通过自动适应的Pretext Task来利用大量未标注3D肿瘤体积来学习肿瘤生长的下面特征；2）使用2D RECIST slice来精度调整Swin变换encoder，以学习slice级别的分割信息；3）进一步精度调整Swin变换encoder，以进行3D肿瘤分割。网络性能被评估使用内部3D肿瘤数据集中的Dice相似度（DSC）和 Hausdorff距离（HD）。我们的提出的MDU-ST在与其他模型进行比较时表现出了显著的改善。这种方法可以用于自动化3D肿瘤分割，以帮助辐射学和肿瘤生长模型研究。这篇论文已经在2023年IEEE国际生物医学影像学会（ISBI）上被接受。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain"><a href="#Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain" class="headerlink" title="Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain"></a>Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01797">http://arxiv.org/abs/2309.01797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchang Jiang, Marius Rüetschi, Vivien Sainte Fare Garnot, Mauro Marty, Konrad Schindler, Christian Ginzler, Jan D. Wegner</li>
<li>for: 本研究旨在提高瑞士国家森林资源调查（NFI）的更新周期，使之能够更好地评估和管理环境。</li>
<li>methods: 本研究使用空间 remote sensing 和深度学习生成大规模的植被高程地图，以降低成本并提高分析效率。</li>
<li>results: 研究结果表明，使用 Sentinel-2 卫星图像生成的植被高程地图可以准确地捕捉到瑞士各地的植被变化，并且可以与普通的 Airborne Laser Scanning 数据进行比较。这些地图还可以用于检测小规模的变化，例如冬季风暴所引起的变化。<details>
<summary>Abstract</summary>
Monitoring and understanding forest dynamics is essential for environmental conservation and management. This is why the Swiss National Forest Inventory (NFI) provides countrywide vegetation height maps at a spatial resolution of 0.5 m. Its long update time of 6 years, however, limits the temporal analysis of forest dynamics. This can be improved by using spaceborne remote sensing and deep learning to generate large-scale vegetation height maps in a cost-effective way. In this paper, we present an in-depth analysis of these methods for operational application in Switzerland. We generate annual, countrywide vegetation height maps at a 10-meter ground sampling distance for the years 2017 to 2020 based on Sentinel-2 satellite imagery. In comparison to previous works, we conduct a large-scale and detailed stratified analysis against a precise Airborne Laser Scanning reference dataset. This stratified analysis reveals a close relationship between the model accuracy and the topology, especially slope and aspect. We assess the potential of deep learning-derived height maps for change detection and find that these maps can indicate changes as small as 250 $m^2$. Larger-scale changes caused by a winter storm are detected with an F1-score of 0.77. Our results demonstrate that vegetation height maps computed from satellite imagery with deep learning are a valuable, complementary, cost-effective source of evidence to increase the temporal resolution for national forest assessments.
</details>
<details>
<summary>摘要</summary>
监测和理解森林动态是环境保护和管理的关键。为了实现这一目标，瑞士国家森林资产库（NFI）提供了全国覆盖率0.5米的植被高度地图。然而，这些地图的更新周期为6年，限制了森林动态的时间分析。可以使用空间遥感和深度学习生成大规模的植被高度地图，以便在成本效益的方式下提高 temporal resolution。在这篇论文中，我们对这些方法进行了深入的分析，并在瑞士进行了实际应用。我们使用Sentinel-2卫星图像生成了2017年至2020年的年度、全国覆盖率10米的植被高度地图。与之前的研究相比，我们进行了大规模和细化的随机分配分析，发现模型准确率与地形特征（坡度和方向）之间存在紧密的关系。我们评估了深度学习得到的高度地图的变化检测潜力，发现这些地图可以检测到250平方米级别的变化。在更大的规模上，受冬季风暴影响的变化的F1分数为0.77。我们的结果表明，通过卫星图像使用深度学习计算的植被高度地图是一种有价值的、补充性的、成本效益的证据，可以增加国家森林评估的时间分辨率。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations"><a href="#Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations" class="headerlink" title="Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations"></a>Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01769">http://arxiv.org/abs/2309.01769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aren Beagley, Hannah Richards, Joshua W. Giles</li>
<li>for:  corrected partial volume effects in CT images</li>
<li>methods:  developed and validated a new algorithm that uses a combination of image processing techniques to correct partial volume effects at cortical bone boundaries, without requiring pre-processing or user input.</li>
<li>results:  demonstrated improved accuracy of surface strain predictions using models created with the corrected CT images compared to those created with the original, uncorrected images.<details>
<summary>Abstract</summary>
Partial Volume effects are present at the boundary between any two types of material in a CT image due to the scanner's Point Spread Function, finite voxel resolution, and importantly, the discrepancy in radiodensity between the two materials. In this study a new algorithm is developed and validated that builds on previously published work to enable the correction of partial volume effects at cortical bone boundaries. Unlike past methods, this algorithm does not require pre-processing or user input to achieve the correction, and the correction is applied directly onto a set of CT images, which enables it to be used in existing computational modelling workflows. The algorithm was validated by performing experimental three point bending tests on porcine fibulae specimen and comparing the experimental results to finite element results for models created using either the original, uncorrected CT images or the partial volume corrected images. Results demonstrated that the models created using the partial volume corrected images did improved the accuracy of the surface strain predictions. Given this initial validation, this algorithm is a viable method for overcoming the challenge of partial volume effects in CT images. Thus, future work should be undertaken to further validate the algorithm with human tissues and through coupling it with a range of different finite element creation workflows to verify that it is robust and agnostic to the chosen material mapping strategy.
</details>
<details>
<summary>摘要</summary>
《部分体积影响在CT图像中存在于任何两种材料的边界之间，由扫描仪的点扩散函数、精度幂和材料差异引起。本研究开发了一种新的算法，基于之前发表的工作，以消除CT图像中的部分体积影响。与过去的方法不同，这个算法不需要先期处理或用户输入，直接应用到CT图像集中，可以在现有的计算模型工作流中使用。这个算法通过对猪肋骨三点弯测试实验和finite element分析来验证，对表层弯曲率预测的准确性进行了改进。由于这个初步验证结果，这个算法是一种可靠的方法，用于解决CT图像中的部分体积影响。因此，未来的工作应该继续进行验证，以确保这个算法在人类组织中的可靠性和与不同材料映射策略相关的可变性。》
</details></li>
</ul>
<hr>
<h2 id="Multispectral-Indices-for-Wildfire-Management"><a href="#Multispectral-Indices-for-Wildfire-Management" class="headerlink" title="Multispectral Indices for Wildfire Management"></a>Multispectral Indices for Wildfire Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01751">http://arxiv.org/abs/2309.01751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso Oliveira, João P. Matos-Carvalho, Filipe Moutinho, Nuno Fachada</li>
<li>for: 本研究旨在为火灾预防和管理方面提供多спектル индекс和相关方法，以帮助研究人员和决策者更好地理解和利用这些 индекс。</li>
<li>methods: 本研究涉及了多种领域，包括植被和土壤特征提取、水体 mapping、人工结构标识和火灾后烧区估计。这些方法都是基于多спектル индекс的，并且可以帮助解决 especific issues in wildfire management。</li>
<li>results: 本研究提出了多种有效的多спектル индекс，包括NDVI和NDWI等，可以用于 Addressing specific issues in wildfire management。此外，为了提高准确性和缓解个体 индекс应用的局限性，建议使用补充处理解决方案和其他数据源，如高分辨率图像和地面测量。<details>
<summary>Abstract</summary>
This paper highlights and summarizes the most important multispectral indices and associated methodologies for fire management. Various fields of study are examined where multispectral indices align with wildfire prevention and management, including vegetation and soil attribute extraction, water feature mapping, artificial structure identification, and post-fire burnt area estimation. The versatility and effectiveness of multispectral indices in addressing specific issues in wildfire management are emphasized. Fundamental insights for optimizing data extraction are presented. Concrete indices for each task, including the NDVI and the NDWI, are suggested. Moreover, to enhance accuracy and address inherent limitations of individual index applications, the integration of complementary processing solutions and additional data sources like high-resolution imagery and ground-based measurements is recommended. This paper aims to be an immediate and comprehensive reference for researchers and stakeholders working on multispectral indices related to the prevention and management of fires.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文把关注和总结了最重要的多spectral指标和相关的方法ологиías，用于林火预防和管理。讨论的领域包括 vegetation和soil特征提取、水特征地图、人工结构标识和林火后烧区面积估计。这些多spectral指标在林火预防和管理中的 versatility和有效性被强调。文中提供了数据提取优化的基本 Insights，并建议了每个任务的具体指标，包括 NDVI 和 NDWI。此外，为了提高准确性和解决个体指标应用中的限制，建议 integrating complementary processing solutions and additional data sources，如高分辨率图像和地面测量。这篇论文旨在为研究人员和相关利益人员提供一份立即和全面的参考，关于多spectral指标在林火预防和管理中的应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors"><a href="#Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors" class="headerlink" title="Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors"></a>Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01384">http://arxiv.org/abs/2309.01384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Sai Mu Dalike Abaxi, Nawaz Mehmood, Luoquan Li, Fuyang Qu, Guangyao Cheng, Dehua Hu, Yi-Ping Ho, Scott Wu Yuan, Ho-Pui Ho</li>
<li>for: 这个研究的目的是为了实现快速、精准的生物样本测量，以便更好地了解生物系统的工作机理。</li>
<li>methods: 这个研究使用了深度学习技术，实现了自动 segmentation 和分类的 GFP（绿色荧光蛋白）标记微反应室，从而实现了实时精准测量。</li>
<li>results: 研究发现，使用这种技术可以快速（只需2.5秒）、精准地测量 GFP 标记微反应室的大小和占据状态，并且具有广阔的动态范围（从56.52到1569.43个浮镜分子&#x2F;微升）。此外，这种 Deep-dGFP 算法具有remarkable泛化能力，可以直接应用于多种 GFP 标记场景。<details>
<summary>Abstract</summary>
Absolute quantification of biological samples entails determining expression levels in precise numerical copies, offering enhanced accuracy and superior performance for rare templates. However, existing methodologies suffer from significant limitations: flow cytometers are both costly and intricate, while fluorescence imaging relying on software tools or manual counting is time-consuming and prone to inaccuracies. In this study, we have devised a comprehensive deep-learning-enabled pipeline that enables the automated segmentation and classification of GFP (green fluorescent protein)-labeled microreactors, facilitating real-time absolute quantification. Our findings demonstrate the efficacy of this technique in accurately predicting the sizes and occupancy status of microreactors using standard laboratory fluorescence microscopes, thereby providing precise measurements of template concentrations. Notably, our approach exhibits an analysis speed of quantifying over 2,000 microreactors (across 10 images) within remarkably 2.5 seconds, and a dynamic range spanning from 56.52 to 1569.43 copies per micron-liter. Furthermore, our Deep-dGFP algorithm showcases remarkable generalization capabilities, as it can be directly applied to various GFP-labeling scenarios, including droplet-based, microwell-based, and agarose-based biological applications. To the best of our knowledge, this represents the first successful implementation of an all-in-one image analysis algorithm in droplet digital PCR (polymerase chain reaction), microwell digital PCR, droplet single-cell sequencing, agarose digital PCR, and bacterial quantification, without necessitating any transfer learning steps, modifications, or retraining procedures. We firmly believe that our Deep-dGFP technique will be readily embraced by biomedical laboratories and holds potential for further development in related clinical applications.
</details>
<details>
<summary>摘要</summary>
完全量化生物样本的过程涉及到准确地测量表达水平，提供了更高的精度和性能，特别是 для罕见的模板。然而，现有的方法ologies有许多限制：流环计仪器昂贵且复杂，而基于软件工具或手动计数的抗体影像扫描是时间consuming且容易出错。在这种研究中，我们开发了一个全面的深度学习启用的管道，允许自动分割和分类GFP（绿色荧光蛋白）标记的微 реактор，实现实时精确量化。我们的发现表明该技术可以准确预测微 реактор的大小和占用状态，从而提供精确的模板含量测量。尤其是，我们的方法在2.5秒钟内可以量化10个图像中的超过2,000个微 реактор，并且具有从56.52到1569.43个浮现每毫升的范围。此外，我们的深度dGFP算法具有remarkable泛化能力，可以直接应用于不同的GFP标记场景，包括液滴基础、微瓶基础和agarose基础的生物应用。根据我们所知，这是第一个成功地实现的所有在一个图像分析算法，无需进行转移学习步骤、修改或重新训练。我们 firmly believe that our Deep-dGFP technique will be readily embraced by biomedical laboratories and holds potential for further development in related clinical applications。
</details></li>
</ul>
<hr>
<h2 id="FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation"><a href="#FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation" class="headerlink" title="FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation"></a>FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01322">http://arxiv.org/abs/2309.01322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cesar Quihui-Rubio, Daniel Flores-Araiza, Miguel Gonzalez-Mendoza, Christian Mata, Gilberto Ochoa-Ruiz</li>
<li>for: 这篇研究旨在提出一种基于U-Net的深度学习方法来分类肝脏中的不同区域，以提高肝癌检测和诊断的工作流程。</li>
<li>methods: 本研究使用了增加和特征层 pyramid 注意模组，并与七种不同的 U-Net 架构进行比较。</li>
<li>results: 实验结果显示，提案的方法在测试集中具有了84.15%的自动分类性能和76.9%的重 overlap率，与大多数研究中的模型相比，仅次于 R2U-Net 和 attention R2U-Net 架构。<details>
<summary>Abstract</summary>
This contribution presents a deep learning method for the segmentation of prostate zones in MRI images based on U-Net using additive and feature pyramid attention modules, which can improve the workflow of prostate cancer detection and diagnosis. The proposed model is compared to seven different U-Net-based architectures. The automatic segmentation performance of each model of the central zone (CZ), peripheral zone (PZ), transition zone (TZ) and Tumor were evaluated using Dice Score (DSC), and the Intersection over Union (IoU) metrics. The proposed alternative achieved a mean DSC of 84.15% and IoU of 76.9% in the test set, outperforming most of the studied models in this work except from R2U-Net and attention R2U-Net architectures.
</details>
<details>
<summary>摘要</summary>
这个贡献提出了基于U-Net深度学习方法的抑制肾阶段分割方法，使用加法和特征层 pyramid 注意模块，以提高肾癌检测和诊断的工作流程。该提案的模型与七种不同的U-Net建筑物进行比较。自动 segmentation 性能的评价指标包括中心zone (CZ)、 périphérique zone (PZ)、 transition zone (TZ) 和肿瘤等部分的 dice 分数 (DSC) 和交集 overlap 指标 (IoU)。提案的代替方案在测试集上实现了84.15%的平均 DSC 和 76.9%的交集 overlap，比大多数研究模型都高，只有R2U-Net和注意力 R2U-Net 建筑物超过。
</details></li>
</ul>
<hr>
<h2 id="An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery"><a href="#An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery" class="headerlink" title="An FPGA smart camera implementation of segmentation models for drone wildfire imagery"></a>An FPGA smart camera implementation of segmentation models for drone wildfire imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01318">http://arxiv.org/abs/2309.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Guarduño-Martinez, Jorge Ciprian-Sanchez, Gerardo Valente, Vazquez-Garcia, Gerardo Rodriguez-Hernandez, Adriana Palacios-Rosas, Lucile Rossi-Tisson, Gilberto Ochoa-Ruiz</li>
<li>for: 这个论文旨在应对野火战，使用附加了可见和红外摄像头的无人机进行识别、监测和火势评估。</li>
<li>methods: 这个研究使用了智能摄像头，基于低功耗的场程可编程阵列（FPGAs），并与二进制神经网络（BNNs）结合，实现在边缘计算中的实时识别。</li>
<li>results: 研究人员对 corsica 火灾数据库进行了 segmentation 模型的实现，通过减少和量化原始模型，将参数数量减少了90%，并通过进一步优化提高了原始模型的吞吐率从8帧每秒（FPS）提高到33.63 FPS，而无损失分 segmentation 性能。<details>
<summary>Abstract</summary>
Wildfires represent one of the most relevant natural disasters worldwide, due to their impact on various societal and environmental levels. Thus, a significant amount of research has been carried out to investigate and apply computer vision techniques to address this problem. One of the most promising approaches for wildfire fighting is the use of drones equipped with visible and infrared cameras for the detection, monitoring, and fire spread assessment in a remote manner but in close proximity to the affected areas. However, implementing effective computer vision algorithms on board is often prohibitive since deploying full-precision deep learning models running on GPU is not a viable option, due to their high power consumption and the limited payload a drone can handle. Thus, in this work, we posit that smart cameras, based on low-power consumption field-programmable gate arrays (FPGAs), in tandem with binarized neural networks (BNNs), represent a cost-effective alternative for implementing onboard computing on the edge. Herein we present the implementation of a segmentation model applied to the Corsican Fire Database. We optimized an existing U-Net model for such a task and ported the model to an edge device (a Xilinx Ultra96-v2 FPGA). By pruning and quantizing the original model, we reduce the number of parameters by 90%. Furthermore, additional optimizations enabled us to increase the throughput of the original model from 8 frames per second (FPS) to 33.63 FPS without loss in the segmentation performance: our model obtained 0.912 in Matthews correlation coefficient (MCC),0.915 in F1 score and 0.870 in Hafiane quality index (HAF), and comparable qualitative segmentation results when contrasted to the original full-precision model. The final model was integrated into a low-cost FPGA, which was used to implement a neural network accelerator.
</details>
<details>
<summary>摘要</summary>
野火是全球最重要的自然灾害之一，它对社会和环境层次产生了深远的影响。因此，许多研究已经进行了，以应用计算机掌握技术来解决这个问题。一种非常有前途的方法是使用具有可见光和红外线摄像头的无人机，以远程方式进行野火探测、监控和火伤评估。然而，实现有效的计算机掌握算法在无人机上是不可能的，因为它们的高功耗和无人机的传输能力有限。因此，在这个工作中，我们认为使用智能摄像头，基于低功耗的可程式遮盾类（FPGAs），可以成为一种成本效益的选择。我们在这里透过将存储在FPGAs上的低功耗摄像头与二进制神经网络（BNNs）联合使用，以实现在边缘上的处理。我们将一个原始的U-Net模型优化 для这个任务，并将模型转移到边缘设备（Xilinx Ultra96-v2 FPGA）上。通过剪裁和数值化原始模型，我们缩减了模型的参数数量，从8帧每秒（FPS）提高到33.63 FPS，而无损于分类性能：我们的模型获得了0.912的均方误差系数（MCC）、0.915的F1分数和0.870的哈菲安质量指数（HAF），并且和原始全精度模型的分类结果相似。最终模型被集成到一个低成本的FPGAs上，实现了一个神经网络加速器。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/eess.IV_2023_09_04/" data-id="clmjn91qu00hh0j883oc3ckcc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/04/cs.LG_2023_09_04/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-04
        
      </div>
    </a>
  
  
    <a href="/2023/09/03/cs.SD_2023_09_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-03</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
