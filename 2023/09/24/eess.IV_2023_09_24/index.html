
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-24 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.13675 repo_url: None paper_authors: Anissa Alloula, Daniel R McGowan, Bartłomiej W. P">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-24">
<meta property="og:url" content="https://nullscc.github.io/2023/09/24/eess.IV_2023_09_24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.13675 repo_url: None paper_authors: Anissa Alloula, Daniel R McGowan, Bartłomiej W. P">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-24T09:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:18.913Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/eess.IV_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T09:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-24
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Autopet-Challenge-2023-nnUNet-based-whole-body-3D-PET-CT-Tumour-Segmentation"><a href="#Autopet-Challenge-2023-nnUNet-based-whole-body-3D-PET-CT-Tumour-Segmentation" class="headerlink" title="Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation"></a>Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13675">http://arxiv.org/abs/2309.13675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anissa Alloula, Daniel R McGowan, Bartłomiej W. Papież</li>
<li>For: 这个论文的目的是用nnUNet进行全身PET-CT扫描中的肿瘤分 segmentation，并对不同的训练和后处理策略进行调查。* Methods: 这个论文使用的方法是nnUNet，并进行了不同的训练和后处理策略的调查。* Results: 这个论文的最佳模型在内部测试集上获得了69%的Dice分数和6.27 mL的假正和5.78 mL的假负量。<details>
<summary>Abstract</summary>
Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) combined with Computed Tomography (CT) scans are critical in oncology to the identification of solid tumours and the monitoring of their progression. However, precise and consistent lesion segmentation remains challenging, as manual segmentation is time-consuming and subject to intra- and inter-observer variability. Despite their promise, automated segmentation methods often struggle with false positive segmentation of regions of healthy metabolic activity, particularly when presented with such a complex range of tumours across the whole body. In this paper, we explore the application of the nnUNet to tumour segmentation of whole-body PET-CT scans and conduct different experiments on optimal training and post-processing strategies. Our best model obtains a Dice score of 69\% and a false negative and false positive volume of 6.27 and 5.78 mL respectively, on our internal test set. This model is submitted as part of the autoPET 2023 challenge. Our code is available at: https://github.com/anissa218/autopet\_nnunet
</details>
<details>
<summary>摘要</summary>
fluorodeoxyglucose positron emission tomography（FDG-PET）与计算机扫描（CT）扫描结合是肿瘤诊断和肿瘤进展评估中非常重要。然而，准确和一致性的肿瘤分割仍然是一项挑战，因为手动分割时间费时且存在内外观察者差异。尽管自动分割方法在承诺的表现不佳，特别是在面临整个身体的复杂肿瘤时，容易出现健康代谢活动的假阳性分割。在这篇论文中，我们探讨使用nnuNet进行肿瘤分割的整体PET-CT扫描，并进行了不同的训练和后处理策略的试验。我们的最佳模型在我们的内部测试集上得到了69%的Dice分数和6.27和5.78 mL的假阴性和假正面量。这个模型已经被提交到autoPET 2023挑战中。我们的代码可以在以下链接中找到：https://github.com/anissa218/autopet_nnunet。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-regularized-coded-ptychography-for-robust-and-efficient-lensless-microscopy-on-a-chip"><a href="#Sparsity-regularized-coded-ptychography-for-robust-and-efficient-lensless-microscopy-on-a-chip" class="headerlink" title="Sparsity-regularized coded ptychography for robust and efficient lensless microscopy on a chip"></a>Sparsity-regularized coded ptychography for robust and efficient lensless microscopy on a chip</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13611">http://arxiv.org/abs/2309.13611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninghe Liu, Qianhao Zhao, Guoan Zheng</li>
<li>for: 提高ptychographic imaging的快速性和高分辨率</li>
<li>methods: 使用稀烈约束来减少测量频率，并通过梯度下降法进行优化</li>
<li>results: 能够生成高精度的重建图像，只需要八个Intensity测量值，并且可以在各种光学设备上实验 validate<details>
<summary>Abstract</summary>
In ptychographic imaging, the trade-off between the number of acquisitions and the resultant imaging quality presents a complex optimization problem. Increasing the number of acquisitions typically yields reconstructions with higher spatial resolution and finer details. Conversely, a reduction in measurement frequency often compromises the quality of the reconstructed images, manifesting as increased noise and coarser details. To address this challenge, we employ sparsity priors to reformulate the ptychographic reconstruction task as a total variation regularized optimization problem. We introduce a new computational framework, termed the ptychographic proximal total-variation (PPTV) solver, designed to integrate into existing ptychography settings without necessitating hardware modifications. Through comprehensive numerical simulations, we validate that PPTV-driven coded ptychography is capable of producing highly accurate reconstructions with a minimal set of eight intensity measurements. Convergence analysis further substantiates the robustness, stability, and computational feasibility of the proposed PPTV algorithm. Experimental results obtained from optical setups unequivocally demonstrate that the PPTV algorithm facilitates high-throughput, high-resolution imaging while significantly reducing the measurement burden. These findings indicate that the PPTV algorithm has the potential to substantially mitigate the resource-intensive requirements traditionally associated with high-quality ptychographic imaging, thereby offering a pathway toward the development of more compact and efficient ptychographic microscopy systems.
</details>
<details>
<summary>摘要</summary>
在ptychographic imaging中，数据量和图像质量之间的交换存在一个复杂的优化问题。增加数据量通常会提高图像的空间分辨率和细节，而减少测量频率则可能会丑化图像的重建效果，表现为增加杂变和粗化细节。为解决这个挑战，我们利用简约约束来修改ptychographic重建任务，将其转化为一个total variation regularized优化问题。我们提出了一种新的计算框架，名为ptychographic proximal total-variation（PPTV）解决方案，可以无需修改现有的ptychography设备。通过广泛的数字实验，我们验证了PPTV驱动的coded ptychography可以生成高精度的重建图像，只需要八个Intensity测量。对于PPTV算法的收敛分析，我们进一步证明了其稳定性、计算可行性和robustness。实验结果表明，PPTV算法可以提高高速、高分辨率的图像重建，同时减少测量负担。这些发现表明，PPTV算法有可能大幅减少传统ptychographic imaging中的资源占用，从而开 up a new pathway towards the development of more compact and efficient ptychographic microscopy systems。
</details></li>
</ul>
<hr>
<h2 id="MediViSTA-SAM-Zero-shot-Medical-Video-Analysis-with-Spatio-temporal-SAM-Adaptation"><a href="#MediViSTA-SAM-Zero-shot-Medical-Video-Analysis-with-Spatio-temporal-SAM-Adaptation" class="headerlink" title="MediViSTA-SAM: Zero-shot Medical Video Analysis with Spatio-temporal SAM Adaptation"></a>MediViSTA-SAM: Zero-shot Medical Video Analysis with Spatio-temporal SAM Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13539">http://arxiv.org/abs/2309.13539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sekeun Kim, Kyungsang Kim, Jiang Hu, Cheng Chen, Zhiliang Lyu, Ren Hui, Sunghwan Kim, Zhengliang Liu, Aoxiao Zhong, Xiang Li, Tianming Liu, Quanzheng Li</li>
<li>for: 这篇研究旨在适应医疗影像分类任务中使用Segmentation Anything Model (SAM)。</li>
<li>methods: 这篇研究引入了一种名为MediViSTA-SAM的新方法，它是一种适应医疗影像分类的Video Segmentation方法，使用了类似框架的对应运算，以及多尺度融合。</li>
<li>results: 实验结果显示，MediViSTA-SAM可以实现高准确性和有效性在医疗影像分类任务中。<details>
<summary>Abstract</summary>
In recent years, the Segmentation Anything Model (SAM) has attracted considerable attention as a foundational model well-known for its robust generalization capabilities across various downstream tasks. However, SAM does not exhibit satisfactory performance in the realm of medical image analysis. In this study, we introduce the first study on adapting SAM on video segmentation, called MediViSTA-SAM, a novel approach designed for medical video segmentation. Given video data, MediViSTA, spatio-temporal adapter captures long and short range temporal attention with cross-frame attention mechanism effectively constraining it to consider the immediately preceding video frame as a reference, while also considering spatial information effectively. Additionally, it incorporates multi-scale fusion by employing a U-shaped encoder and a modified mask decoder to handle objects of varying sizes. To evaluate our approach, extensive experiments were conducted using state-of-the-art (SOTA) methods, assessing its generalization abilities on multi-vendor in-house echocardiography datasets. The results highlight the accuracy and effectiveness of our network in medical video segmentation.
</details>
<details>
<summary>摘要</summary>
Recently, the Segmentation Anything Model (SAM) has gained significant attention as a foundational model known for its robust generalization capabilities across various downstream tasks. However, SAM does not exhibit satisfactory performance in the field of medical image analysis. In this study, we introduce the first study on adapting SAM for video segmentation, called MediViSTA-SAM, a novel approach designed for medical video segmentation. Given video data, MediViSTA, a spatio-temporal adapter, captures long and short range temporal attention with a cross-frame attention mechanism, effectively constraining it to consider the immediately preceding video frame as a reference while also considering spatial information effectively. Additionally, it incorporates multi-scale fusion by employing a U-shaped encoder and a modified mask decoder to handle objects of varying sizes. To evaluate our approach, extensive experiments were conducted using state-of-the-art (SOTA) methods, assessing its generalization abilities on multi-vendor in-house echocardiography datasets. The results highlight the accuracy and effectiveness of our network in medical video segmentation.Here's the word-for-word translation of the text into Simplified Chinese:近年来，Segmentation Anything Model（SAM）已经吸引了较大的关注，作为许多下游任务的基础模型，其robust generalization能力在各种领域得到了证明。然而，SAM在医学影像分析领域表现不 satisfactory。在这种研究中，我们介绍了首个采用SAM进行视频分 segmentation的研究，称为MediViSTA-SAM，这是一种专门为医学视频分 segmentation设计的新方法。给定视频数据，MediViSTA使用空间temporal adapter， capture long和short range temporal attention，通过跨帧注意力机制，有效地将其限制为考虑 immediately preceding video frame作为参考，同时也考虑空间信息。此外，它还 incorporates multi-scale fusion，通过使用U型编码器和修改的mask decoder来处理各种大小的对象。为了评估我们的方法，我们进行了广泛的实验，使用现有的state-of-the-art方法，评估我们的网络在医学视频分 segmentation领域的普适性。结果表明，我们的网络在医学视频分 segmentation中具有高度的准确性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-workflow-for-accelerated-industrial-X-ray-Computed-Tomography"><a href="#Deep-learning-based-workflow-for-accelerated-industrial-X-ray-Computed-Tomography" class="headerlink" title="Deep learning based workflow for accelerated industrial X-ray Computed Tomography"></a>Deep learning based workflow for accelerated industrial X-ray Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14371">http://arxiv.org/abs/2309.14371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Singanallur V. Venkatakrishnan, Luke Scime, Paul Brackman, Curtis Frederick, Ryan Dehoff, Vincent Paquit, Amirkoushyar Ziabari</li>
<li>for: 用于高精度非 destruktive characterization of additively-manufactured metal components</li>
<li>methods: 使用两个神经网络来获得快速加速的重建</li>
<li>results: 可以准确地检测瑕疵和损害，并且可以robustly generalizes across several alloys和不同的缺乏数据情况<details>
<summary>Abstract</summary>
X-ray computed tomography (XCT) is an important tool for high-resolution non-destructive characterization of additively-manufactured metal components. XCT reconstructions of metal components may have beam hardening artifacts such as cupping and streaking which makes reliable detection of flaws and defects challenging. Furthermore, traditional workflows based on using analytic reconstruction algorithms require a large number of projections for accurate characterization - leading to longer measurement times and hindering the adoption of XCT for in-line inspections. In this paper, we introduce a new workflow based on the use of two neural networks to obtain high-quality accelerated reconstructions from sparse-view XCT scans of single material metal parts. The first network, implemented using fully-connected layers, helps reduce the impact of BH in the projection data without the need of any calibration or knowledge of the component material. The second network, a convolutional neural network, maps a low-quality analytic 3D reconstruction to a high-quality reconstruction. Using experimental data, we demonstrate that our method robustly generalizes across several alloys, and for a range of sparsity levels without any need for retraining the networks thereby enabling accurate and fast industrial XCT inspections.
</details>
<details>
<summary>摘要</summary>
X射 Computed Tomography (XCT) 是一种重要的不破坏性高分辨材料成型件的测量工具。 XCT 重建结果可能受到材料硬化的影响，导致识别瑕疵和缺陷困难。此外，传统的工作流程基于使用分析重建算法，需要较多的投射来进行准确的测量 - 导致测量时间长，阻碍 XCT 在生产线上的应用。在这篇论文中，我们介绍了一种新的工作流程，基于使用两个神经网络来从稀疏视图 XCT 扫描数据中获得高质量加速重建。首先，我们使用全连接层实现的第一个神经网络，帮助减少投射数据中的硬化效应，无需任何准备或组合物质知识。其次，我们使用卷积神经网络将低质量的分析3D重建映射到高质量的重建。使用实验数据，我们表明了我们的方法可靠地在不同的合金和稀疏程度上进行泛化，无需任何重新训练神经网络，以便快速和准确地进行工业 XCT 检测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/eess.IV_2023_09_24/" data-id="clogy1z93014xffrafna4cs3m" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/24/cs.LG_2023_09_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-24
        
      </div>
    </a>
  
  
    <a href="/2023/09/24/eess.SP_2023_09_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-09-24</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
