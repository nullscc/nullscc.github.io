
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Natural Language Supervision for General-Purpose Audio Representations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05767 repo_url: None paper_authors: Benjamin Elizalde, Soham Deshmukh, Huaming Wang for: 这篇论">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-11">
<meta property="og:url" content="https://nullscc.github.io/2023/09/11/cs.SD_2023_09_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Natural Language Supervision for General-Purpose Audio Representations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05767 repo_url: None paper_authors: Benjamin Elizalde, Soham Deshmukh, Huaming Wang for: 这篇论">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-11T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:19.636Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.SD_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T15:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Natural-Language-Supervision-for-General-Purpose-Audio-Representations"><a href="#Natural-Language-Supervision-for-General-Purpose-Audio-Representations" class="headerlink" title="Natural Language Supervision for General-Purpose Audio Representations"></a>Natural Language Supervision for General-Purpose Audio Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05767">http://arxiv.org/abs/2309.05767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Elizalde, Soham Deshmukh, Huaming Wang</li>
<li>for: 这篇论文旨在提出一种新的语音模型，即异种语音预训练模型，用于学习语音和文本之间的多模式关系，并可以 Zero-Shot 推理。</li>
<li>methods: 该模型使用了两种创新的编码器来学习语音表示，并使用对抗学习来将语音和文本表示带入共同空间。</li>
<li>results: 模型在26个下游任务中表现出色，包括一些任务的state-of-the-art results，这证明了该模型在普用语音表示领域的可靠性和灵活性。<details>
<summary>Abstract</summary>
Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a performance gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text pairs employing two innovative encoders for Zero-Shot inference. To learn audio representations, we trained an audio encoder on 22 audio tasks, instead of the standard training of sound event classification. To learn language representations, we trained an autoregressive decoder-only model instead of the standard encoder-only models. Then, the audio and language representations are brought into a joint multimodal space using Contrastive Learning. We used our encoders to improve the downstream performance by a margin. We extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature. Our model achieves state of the art results in several tasks leading the way towards general-purpose audio representations.
</details>
<details>
<summary>摘要</summary>
音频语言模型共同学习多模态文本和音频表示，以实现零参数推理。模型依靠Encoder创造强大的输入表示，并能泛化到多种任务，包括声音、乐曲和语音。虽然模型已经实现了出色的表现，但还存在任务特定模型的性能差距。在这篇论文中，我们提议了一种对比语言音频预训练模型（Contrastive Language-Audio Pretraining），该模型通过使用多样化的460万个音频文本对进行预训练，并使用两种创新的Encoder来实现零参数推理。为了学习音频表示，我们在22个音频任务上训练了一个音频Encoder，而不是标准的声音分类训练。为了学习语言表示，我们训练了一个自动生成的拟合Decoder模型，而不是标准的Encoder-only模型。然后，音频和语言表示被带入一个共同多模态空间，使用对比学习来学习。我们使用我们的Encoder来改进下游任务的表现，并进行了广泛的评估。我们的表示在26个下游任务中得到了最佳结果，创下了多种任务的新纪录。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects"><a href="#Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects" class="headerlink" title="Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects"></a>Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05634">http://arxiv.org/abs/2309.05634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoichi Koyama, Masaki Nakada, Juliano G. C. Ribeiro, Hiroshi Saruwatari</li>
<li>for: 这篇论文是为了计算在含散射物体的区域内的入射声场而写的。</li>
<li>methods: 该方法使用 kernel ridge regression 算法，并在入射场和散射场之间进行分离，使得不需要先知或测量散射物体的属性。此外，我们还引入了一个权重矩阵，以便在角度方向上强制散射场的平滑性，从而减少了扩展级别对估计精度的影响。</li>
<li>results: 实验结果表明，提出的方法比不带分离的 kernel ridge regression 更高精度地估计入射声场。<details>
<summary>Abstract</summary>
A method for estimating the incident sound field inside a region containing scattering objects is proposed. The sound field estimation method has various applications, such as spatial audio capturing and spatial active noise control; however, most existing methods do not take into account the presence of scatterers within the target estimation region. Although several techniques exist that employ knowledge or measurements of the properties of the scattering objects, it is usually difficult to obtain them precisely in advance, and their properties may change during the estimation process. Our proposed method is based on the kernel ridge regression of the incident field, with a separation from the scattering field represented by a spherical wave function expansion, thus eliminating the need for prior modeling or measurements of the scatterers. Moreover, we introduce a weighting matrix to induce smoothness of the scattering field in the angular direction, which alleviates the effect of the truncation order of the expansion coefficients on the estimation accuracy. Experimental results indicate that the proposed method achieves a higher level of estimation accuracy than the kernel ridge regression without separation.
</details>
<details>
<summary>摘要</summary>
“一种用于估算各个区域内各种散射物的各个方向的声场的方法被提出。这种声场估算方法有各种应用，如空间音采和空间活动噪声控制，但大多数现有方法都不会考虑目标估算区域内的散射物。虽然有一些技术利用了散射物的性能知识或测量结果，但是通常很难在进行估算之前精确地获得这些属性，而且这些属性可能会在估算过程中发生变化。我们提出的方法基于均匀核ridge regression的各个方向的声场，并通过减去散射场的圆形波函数展开，因此不需要先行模型或测量散射物。此外，我们引入了一个权重矩阵来增加散射场的角度方向的平滑性，这有助于减少扩展级别对估算精度的影响。实验结果表明，我们的方法在估算精度方面高于不含分离的均匀核ridge regression。”
</details></li>
</ul>
<hr>
<h2 id="Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making"><a href="#Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making" class="headerlink" title="Undecidability Results and Their Relevance in Modern Music Making"></a>Undecidability Results and Their Relevance in Modern Music Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05595">http://arxiv.org/abs/2309.05595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halley Young</li>
<li>for: 这篇论文探讨计算理论和音乐之间的交叠，探讨了音乐作曲和制作中对不可解决性的具体、又被忽略的应用。</li>
<li>methods: 这篇论文采用多维度方法，对五个关键领域进行研究：（1）Ableton的Turing完善性，（2）各种效果的满足不可解决性，（3） polymeters的音乐作品中的约束不可解决性，（4）正律和谐制约的满足不可解决性，（5）“新的订制系统”的不可解决性。</li>
<li>results: 这篇论文不仅提供了这些assertion的理论证明，还解释了这些概念在非计算机科学领域的实践 relevance。最终目标是促进对音乐中不可解决性的新理解，高亮其更广泛的应用和计算机助手（以及传统）音乐创作的潜在影响。<details>
<summary>Abstract</summary>
This paper delves into the intersection of computational theory and music, examining the concept of undecidability and its significant, yet overlooked, implications within the realm of modern music composition and production. It posits that undecidability, a principle traditionally associated with theoretical computer science, extends its relevance to the music industry. The study adopts a multidimensional approach, focusing on five key areas: (1) the Turing completeness of Ableton, a widely used digital audio workstation, (2) the undecidability of satisfiability in sound creation utilizing an array of effects, (3) the undecidability of constraints on polymeters in musical compositions, (4) the undecidability of satisfiability in just intonation harmony constraints, and (5) the undecidability of "new ordering systems". In addition to providing theoretical proof for these assertions, the paper elucidates the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to foster a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The Turing completeness of Ableton, a widely used digital audio workstation2. The undecidability of sound creation using an array of effects3. The undecidability of constraints on polymeters in musical compositions4. The undecidability of satisfiability in just intonation harmony constraints5. The undecidability of “new ordering systems”In addition to providing theoretical proof for these assertions, the paper explains the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to promote a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.</details></li>
</ol>
<hr>
<h2 id="LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech"><a href="#LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech" class="headerlink" title="LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech"></a>LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05472">http://arxiv.org/abs/2309.05472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier<br>for:这篇论文是用于探讨自动学习（SSL）在语音处理领域中的应用和发展。methods:这篇论文使用了多种自动学习方法，包括wav2vec 2.0 模型，并提供了一个开源的框架LeBenchmark 2.0，用于评估和建立法语音技术。results:这篇论文获得了多个下游任务的评估结果，包括六个下游任务的评估协议，并进行了静止 versus 精确化、任务特定 versus 任务共通预训练模型的比较。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic versus task-specific pre-trained models as well as a discussion on the carbon footprint of large-scale model training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP"><a href="#Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP" class="headerlink" title="Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP"></a>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05423">http://arxiv.org/abs/2309.05423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzuomu Zhong, Yang Li, Hui Huang, Jie Liu, Zhiba Su, Jing Guo, Benlai Tang, Fengjie Zhu</li>
<li>for: 提高文本译 Speech 自然性和可控性</li>
<li>methods: 提出了一种两stage自动标注管道，包括对 Speech-Silence 和 Word-Punctuation 对的对比预训练，以及一种简单 yet effective的文本-语音特征融合方案和序列分类器</li>
<li>results: 实验结果表明，提出的方法可以自动生成高质量的语音抑抑标注，并达到当前最佳性能水平。此外，模型还在不同数据量下进行了remarkable resilience的测试。<details>
<summary>Abstract</summary>
In the realm of expressive Text-to-Speech (TTS), explicit prosodic boundaries significantly advance the naturalness and controllability of synthesized speech. While human prosody annotation contributes a lot to the performance, it is a labor-intensive and time-consuming process, often resulting in inconsistent outcomes. Despite the availability of extensive supervised data, the current benchmark model still faces performance setbacks. To address this issue, a two-stage automatic annotation pipeline is novelly proposed in this paper. Specifically, in the first stage, we propose contrastive text-speech pretraining of Speech-Silence and Word-Punctuation (SSWP) pairs. The pretraining procedure hammers at enhancing the prosodic space extracted from joint text-speech space. In the second stage, we build a multi-modal prosody annotator, which consists of pretrained encoders, a straightforward yet effective text-speech feature fusion scheme, and a sequence classifier. Extensive experiments conclusively demonstrate that our proposed method excels at automatically generating prosody annotation and achieves state-of-the-art (SOTA) performance. Furthermore, our novel model has exhibited remarkable resilience when tested with varying amounts of data.
</details>
<details>
<summary>摘要</summary>
在表达力强的文本至语音（TTS）领域，明确的语音分界有助于提高合成语音的自然性和可控性。虽然人工语音标注带来了很多 помощ，但是这是一项劳动密集和时间消耗的过程，经常导致不一致的结果。尽管有庞大的超级vised数据可用，现有的标准模型仍然面临性能下降。为解决这个问题，本文提出了一种两个阶段自动标注管道。在第一阶段，我们提议了文本-语音预训练（SSWP）对照练习。预训练过程强化抽象出的语音空间，从joint文本-语音空间提取的语音特征。在第二阶段，我们构建了多模态语音注解器，包括预训练的编码器、简单又有效的文本-语音特征融合方案以及序列分类器。广泛的实验证明了我们提出的方法可以高效地生成语音注解，并达到了当前领域的最佳性能（SOTA）。此外，我们的新模型在不同数据量测试时表现出了很好的抗性。
</details></li>
</ul>
<hr>
<h2 id="SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus"><a href="#SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus" class="headerlink" title="SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus"></a>SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05396">http://arxiv.org/abs/2309.05396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxu Wang, Fan Yu, Xian Shi, Yuezhang Wang, Shiliang Zhang, Ming Li</li>
<li>for: 这个论文主要目的是提高自动语音识别系统的性能，通过利用多 modal 信息，包括视频和文本信息。</li>
<li>methods: 该论文提出了一种基eline方法，利用视频材料中的文本信息来提高语音识别性能。这些方法包括关键词提取和contextual ASR方法。</li>
<li>results: 该论文通过实验示出，通过利用视频材料中的文本信息，可以提高语音识别性能。 Specifically, the paper shows that incorporating textual information from slides into the benchmark system can improve speech recognition performance.<details>
<summary>Abstract</summary>
Multi-Modal automatic speech recognition (ASR) techniques aim to leverage additional modalities to improve the performance of speech recognition systems. While existing approaches primarily focus on video or contextual information, the utilization of extra supplementary textual information has been overlooked. Recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details>
<details>
<summary>摘要</summary>
多modal自动语音识别（ASR）技术目的是利用其他modalities提高语音识别系统的性能。现有的方法主要关注视频或上下文信息，而使用附加的文本信息则被忽略了。 recognize the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.Here's the word-for-word translation:多modal自动语音识别（ASR）技术目的是利用其他modalities提高语音识别系统的性能。现有的方法主要关注视频或上下文信息，而使用附加的文本信息则被忽略了。recognize the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details></li>
</ul>
<hr>
<h2 id="Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations"><a href="#Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations" class="headerlink" title="Towards generalisable and calibrated synthetic speech detection with self-supervised representations"></a>Towards generalisable and calibrated synthetic speech detection with self-supervised representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05384">http://arxiv.org/abs/2309.05384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Oneata, Adriana Stan, Octavian Pascu, Elisabeta Oneata, Horia Cucu</li>
<li>for: 这个论文的目的是提高深度伪造检测器的可靠性，并且研究如何在未经见过的数据上达到良好的泛化能力。</li>
<li>methods: 这个论文使用预训练的自我超vised表示 followed by a simple logistic regression classifier来实现强大的泛化能力，从而提高了Equal Error Rate从30%降至8%在新引入的In-the-Wild dataset上。</li>
<li>results: 这个方法不仅可以提高泛化能力，还可以生成更加准确的模型，并且可以用于下游任务，如uncertainty estimation。具体来说，这个方法可以通过计算预测概率 entropy来拒绝不确定的样本并进一步提高准确性。<details>
<summary>Abstract</summary>
Generalisation -- the ability of a model to perform well on unseen data -- is crucial for building reliable deep fake detectors. However, recent studies have shown that the current audio deep fake models fall short of this desideratum. In this paper we show that pretrained self-supervised representations followed by a simple logistic regression classifier achieve strong generalisation capabilities, reducing the equal error rate from 30% to 8% on the newly introduced In-the-Wild dataset. Importantly, this approach also produces considerably better calibrated models when compared to previous approaches. This means that we can trust our model's predictions more and use these for downstream tasks, such as uncertainty estimation. In particular, we show that the entropy of the estimated probabilities provides a reliable way of rejecting uncertain samples and further improving the accuracy.
</details>
<details>
<summary>摘要</summary>
通用化 -- 模型能够在未经见过的数据上表现出色 -- 是深度假设检测器的关键要素。然而，最近的研究表明，当前的音频深度假设模型缺乏这一要素。在这篇论文中，我们表明了预训练自我超视的表示后加上简单的логистиック回归分类器可以实现强大的通用化能力，从而降低新引入的In-the-Wild数据集上的平均错误率从30%降至8%。此外，这种方法还生成了较好的准确性模型，与之前的方法相比。这意味着我们可以更加信任我们的模型预测结果，并使其用于下游任务，如uncertainty估计。具体来说，我们发现了估计概率的熵可以可靠地拒绝不确定的样本，并进一步提高准确率。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Feature-Imbalance-in-Sound-Source-Separation"><a href="#Addressing-Feature-Imbalance-in-Sound-Source-Separation" class="headerlink" title="Addressing Feature Imbalance in Sound Source Separation"></a>Addressing Feature Imbalance in Sound Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05287">http://arxiv.org/abs/2309.05287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaechang Kim, Jeongyeon Hwang, Soheun Yi, Jaewoong Cho, Jungseul Ok</li>
<li>for: 这个论文主要用于解决神经网络在分类任务中存在的特征偏好问题，即神经网络会过分依赖特定的特征来解决任务，忽略其他重要的特征。</li>
<li>methods: 这篇论文提出了一种解决特征偏好问题的方法，即FEAture BAlancing by Suppressing Easy feature (FEABASE)。这种方法可以有效地利用数据中的隐藏信息，以解决忽略的特征。</li>
<li>results: 作者在一个多通道源分离任务中评估了他们的方法，发现该方法可以有效地避免特征偏好问题，提高分离性能。<details>
<summary>Abstract</summary>
Neural networks often suffer from a feature preference problem, where they tend to overly rely on specific features to solve a task while disregarding other features, even if those neglected features are essential for the task. Feature preference problems have primarily been investigated in classification task. However, we observe that feature preference occurs in high-dimensional regression task, specifically, source separation. To mitigate feature preference in source separation, we propose FEAture BAlancing by Suppressing Easy feature (FEABASE). This approach enables efficient data utilization by learning hidden information about the neglected feature. We evaluate our method in a multi-channel source separation task, where feature preference between spatial feature and timbre feature appears.
</details>
<details>
<summary>摘要</summary>
神经网络经常受到特征偏好问题的困扰，即它们倾向于仅仅依赖特定的特征来解决任务，而忽略其他特征，即使这些忽略的特征对任务非常重要。特征偏好问题主要在分类任务中被研究，但我们发现，在高维度回归任务中，特征偏好也存在，具体来说是来自源分解。为了解决源分解中的特征偏好，我们提议了FEAture BAlancing by Suppressing Easy feature（FEABASE）方法。这种方法可以有效地利用数据，并且学习遗弃的特征中的隐藏信息。我们在多通道源分解任务中评估了我们的方法，发现特征偏好 между空间特征和气质特征存在。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach"><a href="#Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach" class="headerlink" title="Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach"></a>Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05248">http://arxiv.org/abs/2309.05248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, Kunal Dhawan, Nithin Koluguri, Jagadeesh Balam</li>
<li>for: 这个论文的目的是提出一种基于大语言模型（LLM）的语音识别方法，以利用语音和文本模式的共同信息来改进语音识别效果。</li>
<li>methods: 这种方法建立于现有的音频基于的语音识别系统之上，通过在推理阶段添加语言模型（LLM）的lexical信息来利用语音和文本模式的共同信息。这个方法使用 probabilistic 模型来模型多模态混合解码过程，并在joint acoustic和lexical beam search中合并两个模式的信息。</li>
<li>results: 实验结果表明，通过在音频基于的语音识别系统中添加LLM的lexical信息，可以提高总的speaker-attributed word error rate（SA-WER）。这个方法在比对基eline系统时显示出达39.8%的相对 delta-SA-WER提升。这些结果表明，LLM可以为语音识别任务提供补充的信息，并且这种方法可以利用LLM捕捉到语音和文本模式之间的共同信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown great promise for capturing contextual information in natural language processing tasks. We propose a novel approach to speaker diarization that incorporates the prowess of LLMs to exploit contextual cues in human dialogues. Our method builds upon an acoustic-based speaker diarization system by adding lexical information from an LLM in the inference stage. We model the multi-modal decoding process probabilistically and perform joint acoustic and lexical beam search to incorporate cues from both modalities: audio and text. Our experiments demonstrate that infusing lexical knowledge from the LLM into an acoustics-only diarization system improves overall speaker-attributed word error rate (SA-WER). The experimental results show that LLMs can provide complementary information to acoustic models for the speaker diarization task via proposed beam search decoding approach showing up to 39.8% relative delta-SA-WER improvement from the baseline system. Thus, we substantiate that the proposed technique is able to exploit contextual information that is inaccessible to acoustics-only systems which is represented by speaker embeddings. In addition, these findings point to the potential of using LLMs to improve speaker diarization and other speech processing tasks by capturing semantic and contextual cues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.SD_2023_09_11/" data-id="clmjn91oi00c70j886rbt0ia4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/12/eess.IV_2023_09_12/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-12
        
      </div>
    </a>
  
  
    <a href="/2023/09/11/cs.LG_2023_09_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-11</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
