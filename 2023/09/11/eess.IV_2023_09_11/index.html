
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Radiomics Boosts Deep Learning Model for IPMN Classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05857 repo_url: None paper_authors: Lanhong Yao, Zheyuan Zhang, Ugur Demir, Elif Keles, Camila Vendram">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-11">
<meta property="og:url" content="https://nullscc.github.io/2023/09/11/eess.IV_2023_09_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Radiomics Boosts Deep Learning Model for IPMN Classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05857 repo_url: None paper_authors: Lanhong Yao, Zheyuan Zhang, Ugur Demir, Elif Keles, Camila Vendram">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-11T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:19.641Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/eess.IV_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T09:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification"><a href="#Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification" class="headerlink" title="Radiomics Boosts Deep Learning Model for IPMN Classification"></a>Radiomics Boosts Deep Learning Model for IPMN Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05857">http://arxiv.org/abs/2309.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanhong Yao, Zheyuan Zhang, Ugur Demir, Elif Keles, Camila Vendrami, Emil Agarunov, Candice Bolan, Ivo Schoots, Marc Bruno, Rajesh Keswani, Frank Miller, Tamas Gonda, Cemal Yazici, Temel Tirkes, Michael Wallace, Concetto Spampinato, Ulas Bagci</li>
<li>for: 检测和分类IPMN瘤的风险水平，以便有效地规划治疗和疾病控制。</li>
<li>methods: 提出了一种基于计算机机器学的诊断管道，包括高效的自适应分割策略和深度学习模型。</li>
<li>results: 在多中心数据集上测试了并获得了新的最高性能记录（81.9%），比国际指南和已发表研究（61.3%）更高。<details>
<summary>Abstract</summary>
Intraductal Papillary Mucinous Neoplasm (IPMN) cysts are pre-malignant pancreas lesions, and they can progress into pancreatic cancer. Therefore, detecting and stratifying their risk level is of ultimate importance for effective treatment planning and disease control. However, this is a highly challenging task because of the diverse and irregular shape, texture, and size of the IPMN cysts as well as the pancreas. In this study, we propose a novel computer-aided diagnosis pipeline for IPMN risk classification from multi-contrast MRI scans. Our proposed analysis framework includes an efficient volumetric self-adapting segmentation strategy for pancreas delineation, followed by a newly designed deep learning-based classification scheme with a radiomics-based predictive approach. We test our proposed decision-fusion model in multi-center data sets of 246 multi-contrast MRI scans and obtain superior performance to the state of the art (SOTA) in this field. Our ablation studies demonstrate the significance of both radiomics and deep learning modules for achieving the new SOTA performance compared to international guidelines and published studies (81.9\% vs 61.3\% in accuracy). Our findings have important implications for clinical decision-making. In a series of rigorous experiments on multi-center data sets (246 MRI scans from five centers), we achieved unprecedented performance (81.9\% accuracy).
</details>
<details>
<summary>摘要</summary>
Traduzco el texto dado a Chinese simplificado.《Intraductal Papillary Mucinous Neoplasm (IPMN) cysts are pre-malignant pancreas lesions, and they can progress into pancreatic cancer. Therefore, detecting and stratifying their risk level is of ultimate importance for effective treatment planning and disease control. However, this is a highly challenging task because of the diverse and irregular shape, texture, and size of the IPMN cysts as well as the pancreas. In this study, we propose a novel computer-aided diagnosis pipeline for IPMN risk classification from multi-contrast MRI scans. Our proposed analysis framework includes an efficient volumetric self-adapting segmentation strategy for pancreas delineation, followed by a newly designed deep learning-based classification scheme with a radiomics-based predictive approach. We test our proposed decision-fusion model in multi-center data sets of 246 multi-contrast MRI scans and obtain superior performance to the state of the art (SOTA) in this field. Our ablation studies demonstrate the significance of both radiomics and deep learning modules for achieving the new SOTA performance compared to international guidelines and published studies (81.9% vs 61.3% in accuracy). Our findings have important implications for clinical decision-making. In a series of rigorous experiments on multi-center data sets (246 MRI scans from five centers), we achieved unprecedented performance (81.9% accuracy).》Here's the translation in Simplified Chinese:《IPMN肿瘤是肾脏癌前期肿瘤，可能进展到肾脏癌。因此，检测和分级IPMN肿瘤的风险水平是至关重要的，以便为肾脏癌的治疗规划和疾病控制提供有效的方案。然而，这是一个非常困难的任务，因为IPMN肿瘤的形态、 текстура和大小具有多样性和不规则性。在这个研究中，我们提出了一个新的计算机辅助诊断管线，用于从多标示MRI扫描中检测IPMN肿瘤的风险等级。我们的提案分析框架包括一个高效的自适应分割策略，用于肾脏定义，以及一个新的深度学习基于类型的分类方案，以及一个基于几个标示的预测方法。我们在多中心数据集（246个多标示MRI扫描）中试用我们的决策融合模型，并获得了领域的新纪录（81.9%的准确性）。我们的实验表明，深度学习和类型的模组均具有重要的作用，并且在国际指南和已出版的研究中具有显著的优势（81.9% vs 61.3%的准确性）。我们的发现具有重要的依据，对临床决策有重要的影响。在多中心数据集（246个MRI扫描）中进行了一系列的严格实验，获得了无前例的性能（81.9%的准确性）。》
</details></li>
</ul>
<hr>
<h2 id="Designs-and-Implementations-in-Neural-Network-based-Video-Coding"><a href="#Designs-and-Implementations-in-Neural-Network-based-Video-Coding" class="headerlink" title="Designs and Implementations in Neural Network-based Video Coding"></a>Designs and Implementations in Neural Network-based Video Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05846">http://arxiv.org/abs/2309.05846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Li, Junru Li, Chaoyi Lin, Kai Zhang, Li Zhang, Franck Galpin, Thierry Dumas, Hongtao Wang, Muhammed Coban, Jacob Ström, Du Liu, Kenneth Andersson</li>
<li>for: 本文主要针对的是如何通过神经网络来实现视频编码，以提高视频编码的效率和质量。</li>
<li>methods: 本文主要介绍了两种神经网络基于的视频编码技术：神经网络基于的内部预测和神经网络基于的循环过滤。这两种技术已经在JVET中进行了详细的研究和探讨，并最终被收录到了NNVC的参考软件中。</li>
<li>results: 对于Y、Cb、Cr三种颜色块，使用神经网络基于的编码工具可以实现{11.94%、21.86%、22.59%}的BD率减少平均值，相比VTM-11.0_nnvc。此外，对于随机访问、低延迟和所有内部配置，使用神经网络基于的编码工具可以实现{9.18%、19.76%、20.92%}、{10.63%、21.56%、23.02%}和{11.94%、21.86%、22.59%}的BD率减少平均值。<details>
<summary>Abstract</summary>
The past decade has witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with neural network-based video coding being one of them. Neural network-based video coding can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This paper elaborates some of the recent exploration efforts of JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of neural network-based video coding (NNVC), falling in the former category. Specifically, this paper discusses two major NN-based video coding technologies, i.e. neural network-based intra prediction and neural network-based in-loop filtering, which have been investigated for several meeting cycles in JVET and finally adopted into the reference software of NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed NN-based coding tools in NNVC-4.0 could achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations respectively.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie  witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with neural network-based video coding being one of them. Neural network-based video coding can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This paper elaborates some of the recent exploration efforts of JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of neural network-based video coding (NNVC), falling in the former category. Specifically, this paper discusses two major NN-based video coding technologies, i.e. neural network-based intra prediction and neural network-based in-loop filtering, which have been investigated for several meeting cycles in JVET and finally adopted into the reference software of NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed NN-based coding tools in NNVC-4.0 could achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations respectively.
</details></li>
</ul>
<hr>
<h2 id="Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging"><a href="#Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging" class="headerlink" title="Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging"></a>Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05818">http://arxiv.org/abs/2309.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yara Ali Alnaggar, Ahmad Sebaq, Karim Amer, ElSayed Naeem, Mohamed Elhelw</li>
<li>for: 本研究旨在提供一个公共多spectral和RGB图像集和一个深度学习管道，用于检测rice Plant疾病。</li>
<li>methods: 本研究使用多spectral和RGB图像作为输入，通过深度学习管道进行疾病检测。</li>
<li>results: 研究发现，使用多spectral和RGB图像作为输入可以实现更高的F1准确率，比使用RGB输入只有更高的准确率。<details>
<summary>Abstract</summary>
Rice is considered a strategic crop in Egypt as it is regularly consumed in the Egyptian people's diet. Even though Egypt is the highest rice producer in Africa with a share of 6 million tons per year, it still imports rice to satisfy its local needs due to production loss, especially due to rice disease. Rice blast disease is responsible for 30% loss in rice production worldwide. Therefore, it is crucial to target limiting yield damage by detecting rice crops diseases in its early stages. This paper introduces a public multispectral and RGB images dataset and a deep learning pipeline for rice plant disease detection using multi-modal data. The collected multispectral images consist of Red, Green and Near-Infrared channels and we show that using multispectral along with RGB channels as input archives a higher F1 accuracy compared to using RGB input only.
</details>
<details>
<summary>摘要</summary>
rice 被视为埃及的战略作物，由于埃及人的日常饮食中Regularly consumed，即使埃及每年出产6万吨的rice，仍然需要进口rice来满足本地需求，主要是由于生产损失，尤其是rice disease。rice blast disease是全球rice生产损失的30%原因。因此，适当检测rice crops的疾病在早期是非常重要。这篇论文介绍了一个公共多спектral和RGB图像集和一个深度学习管道，用于rice plant疾病检测，使用多Modal数据。收集的多спектral图像包括红、绿和近红外通道，我们显示使用多спектral和RGB通道作为输入，可以达到高于RGB输入Only的F1准确率。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Adversarial-Purification-for-Robust-Deep-MRI-Reconstruction"><a href="#Diffusion-based-Adversarial-Purification-for-Robust-Deep-MRI-Reconstruction" class="headerlink" title="Diffusion-based Adversarial Purification for Robust Deep MRI Reconstruction"></a>Diffusion-based Adversarial Purification for Robust Deep MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05794">http://arxiv.org/abs/2309.05794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismail Alkhouri, Shijun Liang, Rongrong Wang, Qing Qu, Saiprasad Ravishankar</li>
<li>for: 提高深度学习（DL）方法在核磁共振成像（MRI）重建中的性能</li>
<li>methods: 利用扩散模型提高DL方法的鲁棒性</li>
<li>results: 对比主流防御方法（如对抗训练和随机平滑），我们的提议方法在MRI重建中显示出更高的效果<details>
<summary>Abstract</summary>
Deep learning (DL) methods have been extensively employed in magnetic resonance imaging (MRI) reconstruction, demonstrating remarkable performance improvements compared to traditional non-DL methods. However, recent studies have uncovered the susceptibility of these models to carefully engineered adversarial perturbations. In this paper, we tackle this issue by leveraging diffusion models. Specifically, we introduce a defense strategy that enhances the robustness of DL-based MRI reconstruction methods through the utilization of pre-trained diffusion models as adversarial purifiers. Unlike conventional state-of-the-art adversarial defense methods (e.g., adversarial training), our proposed approach eliminates the need to solve a minimax optimization problem to train the image reconstruction model from scratch, and only requires fine-tuning on purified adversarial examples. Our experimental findings underscore the effectiveness of our proposed technique when benchmarked against leading defense methodologies for MRI reconstruction such as adversarial training and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）方法在核磁共振成像（MRI）重建中得到了广泛应用，并表现出了非常出色的性能改善。然而，最近的研究发现，这些模型对特制的恶意扰动受到极大的抑制。在这篇论文中，我们解决这个问题，通过利用扩散模型。具体来说，我们介绍了一种防御策略，通过使用预训练的扩散模型来增强DL-based MRI重建方法的Robustness。与传统的state-of-the-art adversarial防御方法（例如， adversarial Training）不同，我们的提议方法不需要从头开始训练图像重建模型，只需要练习在纯化的恶意示例上。我们的实验发现，我们的提议方法在比较leading defense方法（如 adversarial training和随机平滑）的基础上，对MRI重建方法的防御性能具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images"><a href="#LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images" class="headerlink" title="LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images"></a>LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05780">http://arxiv.org/abs/2309.05780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Fhima, Jan Van Eijgen, Hana Kulenovic, Valérie Debeuf, Marie Vangilbergen, Marie-Isaline Billen, Heloïse Brackenier, Moti Freiman, Ingeborg Stalmans, Joachim A. Behar</li>
<li>for: The paper is written for the purpose of developing a novel deep learning architecture for high resolution segmentation of retinal arterioles and venules in digital fundus images.</li>
<li>methods: The paper uses active learning to create a new dataset of crowd-sourced manual segmentations, and develops a novel deep learning architecture called LUNet that includes a double dilated convolutional block and a long tail to enhance the receptive field and refine the segmentation.</li>
<li>results: The paper shows that LUNet significantly outperforms two state-of-the-art segmentation algorithms on both the local test set and four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种高分辨率的抽象血管图像的深度学习架构。</li>
<li>methods: 这篇论文使用了活动学习来创建一个新的数据集，并开发了一种名为LUNet的深度学习架构，包括一个双扩展 convolutional block 和一个长尾来增强模型的接受范围和精度。</li>
<li>results: 这篇论文表明，LUNet 在本地测试集和四个外部测试集上都显著超越了两种现有的分割算法。<details>
<summary>Abstract</summary>
The retina is the only part of the human body in which blood vessels can be accessed non-invasively using imaging techniques such as digital fundus images (DFI). The spatial distribution of the retinal microvasculature may change with cardiovascular diseases and thus the eyes may be regarded as a window to our hearts. Computerized segmentation of the retinal arterioles and venules (A/V) is essential for automated microvasculature analysis. Using active learning, we created a new DFI dataset containing 240 crowd-sourced manual A/V segmentations performed by fifteen medical students and reviewed by an ophthalmologist, and developed LUNet, a novel deep learning architecture for high resolution A/V segmentation. LUNet architecture includes a double dilated convolutional block that aims to enhance the receptive field of the model and reduce its parameter count. Furthermore, LUNet has a long tail that operates at high resolution to refine the segmentation. The custom loss function emphasizes the continuity of the blood vessels. LUNet is shown to significantly outperform two state-of-the-art segmentation algorithms on the local test set as well as on four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators. We make the newly created dataset open access (upon publication).
</details>
<details>
<summary>摘要</summary>
眼睛是人体唯一一部可以非侵入式地访问血管的部位，通过数字背景图像（DFI）进行成像。眼睛中血管网络的空间分布可能会与冠状病变发生变化，因此眼睛可以被看作是心脏的窗口。计算机化的血管分离是自动化微血管分析的关键。我们使用了活动学习，创建了240个人工制定的血管分离（A/V），由15名医学生和一名眼科医生审核，并开发了LUNet，一种新的深度学习架构，用于高分辨率血管分离。LUNet架构包括一个双扩展 convolutional block，用于提高模型的感知范围和减少参数计数。此外，LUNet还有一个长尾，用于在高分辨率下精细化分割。我们定义了一个自适应损失函数，以优化血管之间的连续性。LUNet在本地测试集上以及四个外部测试集上，与两种现有的分割算法进行比较，显著地超越它们。我们将新创建的数据集公开访问（在出版后）。
</details></li>
</ul>
<hr>
<h2 id="From-Capture-to-Display-A-Survey-on-Volumetric-Video"><a href="#From-Capture-to-Display-A-Survey-on-Volumetric-Video" class="headerlink" title="From Capture to Display: A Survey on Volumetric Video"></a>From Capture to Display: A Survey on Volumetric Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05658">http://arxiv.org/abs/2309.05658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Jin, Kaiyuan Hu, Junhua Liu, Fangxin Wang, Xue Liu</li>
<li>for: This paper provides a comprehensive review of the existing literature on volumetric video services, with a focus on the challenges and opportunities in this field.</li>
<li>methods: The paper discusses various methodologies for each stage of the volumetric video service pipeline, including capturing, compression, transmission, rendering, and display techniques.</li>
<li>results: The paper explores various applications enabled by volumetric video services and identifies potential research challenges and opportunities in this field, with the aim of bringing the vision of volumetric video to fruition.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文为探讨现有的volumetric video服务相关文献而进行了全面的审视，并评估了这一领域的挑战和机遇。</li>
<li>methods: 论文讨论了volumetric video服务管道中每个阶段的方法ologies，包括捕捉、压缩、传输、渲染和显示技术。</li>
<li>results: 论文探讨了volumetric video服务所带来的各种应用程序，并确定了这一领域的研究挑战和机遇，以便实现volumetric video的视野。<details>
<summary>Abstract</summary>
Volumetric video, which offers immersive viewing experiences, is gaining increasing prominence. With its six degrees of freedom, it provides viewers with greater immersion and interactivity compared to traditional videos. Despite their potential, volumetric video services poses significant challenges. This survey conducts a comprehensive review of the existing literature on volumetric video. We firstly provide a general framework of volumetric video services, followed by a discussion on prerequisites for volumetric video, encompassing representations, open datasets, and quality assessment metrics. Then we delve into the current methodologies for each stage of the volumetric video service pipeline, detailing capturing, compression, transmission, rendering, and display techniques. Lastly, we explore various applications enabled by this pioneering technology and we present an array of research challenges and opportunities in the domain of volumetric video services. This survey aspires to provide a holistic understanding of this burgeoning field and shed light on potential future research trajectories, aiming to bring the vision of volumetric video to fruition.
</details>
<details>
<summary>摘要</summary>
三维视频技术在 immerse 观看体验方面占据着越来越重要的地位。它提供了六个自由度，让观看者感受到更加深入和互动性，比传统视频更加出色。然而，三维视频服务还面临着一系列挑战。这份调查提供了三维视频服务的全面评估，首先提供三维视频服务的总体框架，然后讨论三维视频的前提条件，包括表示、开放数据集和质量评估指标。接着，我们详细介绍了每个三维视频服务管道阶段的方法ologies，包括捕捉、压缩、传输、渲染和显示技术。最后，我们探讨了三维视频服务在不同领域的应用，以及这个领域的研究挑战和机遇。这份调查的目的是为了提供三维视频服务领域的总体认知，并照料未来研究方向，以实现三维视频的视野。
</details></li>
</ul>
<hr>
<h2 id="A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images"><a href="#A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images" class="headerlink" title="A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images"></a>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05446">http://arxiv.org/abs/2309.05446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medcai/l2snet">https://github.com/medcai/l2snet</a></li>
<li>paper_authors: Linghan Cai, Jianhao Huang, Zihang Zhu, Jinpeng Lu, Yongbing Zhang<br>for: 本研究旨在提出一种localization-to-segmentation框架(L2SNet)，用于精准诊断肿瘤。methods: 本方法首先在肿瘤localization阶段使用了可能的肿瘤含气扩散矩阵(PET)和 Computed Tomography(CT)图像进行定位，然后使用定位提示符进行肿瘤分割。results: 在MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET&#x2F;CT挑战数据集上进行实验后，我们的方法在预测测试集上取得了竞争力强的结果，并列入了前7名。<details>
<summary>Abstract</summary>
Fluorodeoxyglucose (FDG) positron emission tomography(PET) combined with computed tomography (CT) is considered the primary solution for detecting some cancers, such as lung cancer and melanoma. Automatic segmentation of tumors in PET/CT images can help reduce doctors' workload, thereby improving diagnostic quality. However, precise tumor segmentation is challenging due to the small size of many tumors and the similarity of high-uptake normal areas to the tumor regions. To address these issues, this paper proposes a localization-to-segmentation framework (L2SNet) for precise tumor segmentation. L2SNet first localizes the possible lesions in the lesion localization phase and then uses the location cues to shape the segmentation results in the lesion segmentation phase. To further improve the segmentation performance of L2SNet, we design an adaptive threshold scheme that takes the segmentation results of the two phases into consideration. The experiments with the MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset show that our method achieved a competitive result and was ranked in the top 7 methods on the preliminary test set. Our work is available at: https://github.com/MedCAI/L2SNet.
</details>
<details>
<summary>摘要</summary>
fluorodeoxyglucose (FDG) positron emission tomography（PET）combined with computed tomography（CT）被视为检测一些肿瘤的主要解决方案，如肺癌和黑色素瘤。自动 segmentation of tumors in PET/CT images可以帮助医生减少工作量，从而提高诊断质量。然而，精准肿瘤 segmentation 是具有挑战性，因为许多肿瘤的Size 小，而高uptake normal areas 和肿瘤区域之间的相似性也使 segmentation 更加困难。为解决这些问题，本文提出了一个 localization-to-segmentation 框架（L2SNet），用于精准肿瘤 segmentation。L2SNet 先在 lesion localization 阶段确定可能的肿瘤，然后使用 Location 提示来形成 segmentation 结果。为了进一步提高 L2SNet 的 segmentation 性能，我们设计了一种适应reshold 方案，该方案根据 segmentation 结果进行调整。经过对 MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge 数据集的实验，我们的方法达到了竞争力强的结果，在预测测试集上排名第 7。我们的工作可以在：https://github.com/MedCAI/L2SNet 中找到。
</details></li>
</ul>
<hr>
<h2 id="Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction"><a href="#Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction" class="headerlink" title="Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction"></a>Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05406">http://arxiv.org/abs/2309.05406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghui Liu, Elies Fuster-Garcia, Ivar Thokle Hovden, Donatas Sederevicius, Karoline Skogen, Bradley J MacIntosh, Edvard Grødem, Till Schellhorn, Petter Brandal, Atle Bjørnerud, Kyrre Eeg Emblem</li>
<li>for: 这篇论文旨在模型Diffuse gliomas的肿瘤增长，以帮助临床决策。</li>
<li>methods: 这篇论文使用了一个 novel end-to-end 网络，能够生成未来肿瘤的图像和实际的 MRI 影像，以及不同的治疗方案下肿瘤增长的预测。这个模型基于了进步的扩散几率模型和深度分类神经网络。</li>
<li>results: 这篇论文的模型已经在一系列任务上表现出色，包括生成高品质的伪造 MRI 影像、时间序列肿瘤分类、和不确定性估计。与治疗认知的生成 MRI 结合，肿瘤增长预测 WITH 不确定性估计可以提供有用的信息供临床决策。<details>
<summary>Abstract</summary>
Diffuse gliomas are malignant brain tumors that grow widespread through the brain. The complex interactions between neoplastic cells and normal tissue, as well as the treatment-induced changes often encountered, make glioma tumor growth modeling challenging. In this paper, we present a novel end-to-end network capable of generating future tumor masks and realistic MRIs of how the tumor will look at any future time points for different treatment plans. Our model is built upon cutting-edge diffusion probabilistic models and deep-segmentation neural networks. We extended a diffusion model to include sequential multi-parametric MRI and treatment information as conditioning input to guide the generative diffusion process. This allows us to estimate tumor growth at any given time point. We trained the model using real-world postoperative longitudinal MRI data with glioma tumor growth trajectories represented as tumor segmentation maps over time. The model has demonstrated promising performance across a range of tasks, including the generation of high-quality synthetic MRIs with tumor masks, time-series tumor segmentations, and uncertainty estimation. Combined with the treatment-aware generated MRIs, the tumor growth predictions with uncertainty estimates can provide useful information for clinical decision-making.
</details>
<details>
<summary>摘要</summary>
Diffuse gliomas 是肿瘤性脑肿，通过脑内多个区域延伸生长。受到肿瘤细胞和正常组织之间复杂互动以及治疗引起的变化影响，肿瘤增长模型非常困难。在这篇论文中，我们提出了一种新的终端网络，能够生成未来肿瘤掩蔽图和真实的MRI图像，以反映不同治疗方案下肿瘤的增长情况。我们基于 cutting-edge 扩散概率模型和深度分割神经网络构建了这种模型。我们将扩散模型扩展到包括 sequential multi-parametric MRI 和治疗信息作为条件输入，以引导生成扩散过程。这使得我们可以估算肿瘤在任何时间点增长。我们使用了实际的postoperative longitudinal MRI数据，其中肿瘤增长轨迹由肿瘤分割图表示。模型在多个任务上表现出色，包括生成高质量的 sintetic MRI 图像、时序分割和不确定性估计。与治疗意识的生成MRIs相结合，肿瘤增长预测与不确定性估计可以为临床决策提供有用信息。
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT"><a href="#Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT" class="headerlink" title="Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT"></a>Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05405">http://arxiv.org/abs/2309.05405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Liu, Tong Tian, Weijin Xu, Lemeng Wang, Haoyuan Li, Huihua Yang</li>
<li>for: 这篇论文的目的是提出一个混合式监督框架，将部分 labels 和无labels 数据结合，以提高腹部器官和肿瘤分类的精度。</li>
<li>methods: 这篇论文使用了一个分阶段的分类管线和整个量子资料来进行分类，并使用了自己教学和平均教师的方法来提高分类精度。</li>
<li>results: 论文的实验结果显示，这个方法在FLARE2023的验证集上实现了出色的分类性能，同时也实现了快速且具有低资源的模型测试。在验证集上，这个方法的平均DSC分数为89.79%和45.55%，而平均运行时间和GPU内存使用量分别为11.25秒和9627.82MB。<details>
<summary>Abstract</summary>
Abdominal organ and tumour segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manual assessment is inherently subjective with considerable inter- and intra-expert variability. In the paper, we propose a hybrid supervised framework, StMt, that integrates self-training and mean teacher for the segmentation of abdominal organs and tumors using partially labeled and unlabeled data. We introduce a two-stage segmentation pipeline and whole-volume-based input strategy to maximize segmentation accuracy while meeting the requirements of inference time and GPU memory usage. Experiments on the validation set of FLARE2023 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. Our method achieved an average DSC score of 89.79\% and 45.55 \% for the organs and lesions on the validation set and the average running time and area under GPU memory-time cure are 11.25s and 9627.82MB, respectively.
</details>
<details>
<summary>摘要</summary>
腹部器官和肿瘤分割有很多重要的临床应用，如器官量化、手术规划和疾病诊断。然而，手动评估是内在地主观，具有较大的交叉和内部专家变化。在文章中，我们提出了一种混合supervised框架，StMt，以实现腹部器官和肿瘤分割，使用部分标注和无标注数据。我们提出了一种两个阶段分割管道和整体量化输入策略，以最大化分割精度，同时满足推断时间和GPU内存使用的要求。在FLARE2023验证集上，我们的方法实现了出色的分割性能，同时具有快速和低资源模型推断。我们的方法在验证集上的平均DSC分数为89.79%和45.55%，平均运行时间和GPU内存使用率分别为11.25s和9627.82MB。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-real-time-3D-scene-reconstruction-with-SLAM-methods-in-embedded-systems"><a href="#A-survey-on-real-time-3D-scene-reconstruction-with-SLAM-methods-in-embedded-systems" class="headerlink" title="A survey on real-time 3D scene reconstruction with SLAM methods in embedded systems"></a>A survey on real-time 3D scene reconstruction with SLAM methods in embedded systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05349">http://arxiv.org/abs/2309.05349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Picard, Stephane Chevobbe, Mehdi Darouich, Jean-Yves Didier</li>
<li>for: 本文探讨了在交通系统中的同时定位和地图构建（SLAM）三维重建问题，尤其是在无人机、服务机器人和移动AR&#x2F;VR设备中。</li>
<li>methods: 本文介绍了一种基于图像的视觉三维场景重建管道的实现，包括感知器到3D重建的传统SLAM管道，以及可能的深度学习应用。</li>
<li>results: 文章讨论了在具有有限资源的硬件平台上实现高级功能的挑战，以及在实时地理位置和重建中做出的质量和资源投入的负担。<details>
<summary>Abstract</summary>
The 3D reconstruction of simultaneous localization and mapping (SLAM) is an important topic in the field for transport systems such as drones, service robots and mobile AR/VR devices. Compared to a point cloud representation, the 3D reconstruction based on meshes and voxels is particularly useful for high-level functions, like obstacle avoidance or interaction with the physical environment. This article reviews the implementation of a visual-based 3D scene reconstruction pipeline on resource-constrained hardware platforms. Real-time performances, memory management and low power consumption are critical for embedded systems. A conventional SLAM pipeline from sensors to 3D reconstruction is described, including the potential use of deep learning. The implementation of advanced functions with limited resources is detailed. Recent systems propose the embedded implementation of 3D reconstruction methods with different granularities. The trade-off between required accuracy and resource consumption for real-time localization and reconstruction is one of the open research questions identified and discussed in this paper.
</details>
<details>
<summary>摘要</summary>
三维重建（3D reconstruction）是交通系统如无人机、服务机器人和移动AR/VR设备等领域的重要话题。相比点云表示，基于网格和体积的3D重建更有用于高级功能，如避免障碍物或与物理环境交互。本文介绍了嵌入式硬件平台上视觉基于的3D场景重建管线的实现。实时性、内存管理和低功耗是嵌入式系统的关键要求。文章描述了潜在使用深度学习的SLAM管线，以及嵌入式实现高级功能的方法。不同粒度的3D重建方法的嵌入实现被详细介绍。文章还讨论了实时地理位和重建所需的资源占用和精度质量之间的负担。
</details></li>
</ul>
<hr>
<h2 id="AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration"><a href="#AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration" class="headerlink" title="AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration"></a>AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05271">http://arxiv.org/abs/2309.05271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-autofuse">https://github.com/mungomeng/registration-autofuse</a></li>
<li>paper_authors: Mingyuan Meng, Michael Fulham, Dagan Feng, Lei Bi, Jinman Kim</li>
<li>for: 这 paper 的目的是提出一种数据驱动的拟合策略，以便在医疗图像注射中实现高效的拟合。</li>
<li>methods: 这 paper 使用的方法是基于深度神经网络 (DNNs) 的拟合策略，并提出了一种自动化拟合策略（AutoFuse），以便在不同的网络位置进行信息融合。</li>
<li>results: 这 paper 的实验结果表明，AutoFuse 可以在两个 Well-benchmarked 医疗注射任务（inter- 和 intra-patient registration）上超过现有的无监督和半监督拟合方法。<details>
<summary>Abstract</summary>
Deformable image registration aims to find a dense non-linear spatial correspondence between a pair of images, which is a crucial step for many medical tasks such as tumor growth monitoring and population analysis. Recently, Deep Neural Networks (DNNs) have been widely recognized for their ability to perform fast end-to-end registration. However, DNN-based registration needs to explore the spatial information of each image and fuse this information to characterize spatial correspondence. This raises an essential question: what is the optimal fusion strategy to characterize spatial correspondence? Existing fusion strategies (e.g., early fusion, late fusion) were empirically designed to fuse information by manually defined prior knowledge, which inevitably constrains the registration performance within the limits of empirical designs. In this study, we depart from existing empirically-designed fusion strategies and develop a data-driven fusion strategy for deformable image registration. To achieve this, we propose an Automatic Fusion network (AutoFuse) that provides flexibility to fuse information at many potential locations within the network. A Fusion Gate (FG) module is also proposed to control how to fuse information at each potential network location based on training data. Our AutoFuse can automatically optimize its fusion strategy during training and can be generalizable to both unsupervised registration (without any labels) and semi-supervised registration (with weak labels provided for partial training data). Extensive experiments on two well-benchmarked medical registration tasks (inter- and intra-patient registration) with eight public datasets show that our AutoFuse outperforms state-of-the-art unsupervised and semi-supervised registration methods.
</details>
<details>
<summary>摘要</summary>
图像变形注册的目标是找到两个图像之间的密集非线性空间匹配，这是医学任务中的关键步骤，如肿瘤增长监测和人口分析。近年来，深度神经网络（DNNs）在执行快速端到端注册方面表现出色，但是DNN-based注册需要挖掘图像中的空间信息，并将这些信息融合以定义空间匹配。这引出了一个关键问题：何种最佳的融合策略来定义空间匹配？现有的融合策略（例如早期融合、晚期融合）是基于手动定义的先验知识，这会限制注册性能在Empirical设计的限制下。在这项研究中，我们弃寸现有的Empirical设计的融合策略，开发了一种数据驱动的融合策略。为实现这一点，我们提出了自动融合网络（AutoFuse），它可以在网络中自动选择融合的位置，并通过一个控制器模块（Fusion Gate，FG）来控制如何融合信息。我们的AutoFuse可以在训练时自动优化融合策略，并可以泛化到无标签注册（无标注注册）和半标签注册（半标注注册）中。我们在两个well-benchmarked医学注册任务（Inter-和Intra-patient注册）上进行了八个公共数据集的实验，结果表明，我们的AutoFuse在无标签注册和半标签注册中都能够超越当前的状态艺。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/eess.IV_2023_09_11/" data-id="clmjn91r000hx0j8865lndzvg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/11/cs.LG_2023_09_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-11
        
      </div>
    </a>
  
  
    <a href="/2023/09/10/cs.SD_2023_09_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-10</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
