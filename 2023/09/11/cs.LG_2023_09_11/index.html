
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Reaction coordinate flows for model reduction of molecular kinetics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05878 repo_url: None paper_authors: Hao Wu, Frank Noé for: 该论文旨在开发一种基于流程的机器学习方法，称为反应坐拱（RC）流，用于分">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-11">
<meta property="og:url" content="https://nullscc.github.io/2023/09/11/cs.LG_2023_09_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Reaction coordinate flows for model reduction of molecular kinetics paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.05878 repo_url: None paper_authors: Hao Wu, Frank Noé for: 该论文旨在开发一种基于流程的机器学习方法，称为反应坐拱（RC）流，用于分">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-11T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:19.633Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.LG_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T10:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics"><a href="#Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics" class="headerlink" title="Reaction coordinate flows for model reduction of molecular kinetics"></a>Reaction coordinate flows for model reduction of molecular kinetics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05878">http://arxiv.org/abs/2309.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wu, Frank Noé</li>
<li>for: 该论文旨在开发一种基于流程的机器学习方法，称为反应坐拱（RC）流，用于分子系统的低维度动力学模型的发现。</li>
<li>methods: 该方法使用正规化流来设计坐拱变换，并使用布朗运动模型来近似RC的动力学。所有模型参数都可以通过数据驱动的方式进行估计。</li>
<li>results: 计算结果显示，提出的方法可以高效地从仪器估计出来的全状态动力学中提取低维度的准确和可读的表示。<details>
<summary>Abstract</summary>
In this work, we introduce a flow based machine learning approach, called reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models of molecular systems. The RC flow utilizes a normalizing flow to design the coordinate transformation and a Brownian dynamics model to approximate the kinetics of RC, where all model parameters can be estimated in a data-driven manner. In contrast to existing model reduction methods for molecular kinetics, RC flow offers a trainable and tractable model of reduced kinetics in continuous time and space due to the invertibility of the normalizing flow. Furthermore, the Brownian dynamics-based reduced kinetic model investigated in this work yields a readily discernible representation of metastable states within the phase space of the molecular system. Numerical experiments demonstrate how effectively the proposed method discovers interpretable and accurate low-dimensional representations of given full-state kinetics from simulations.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种基于流的机器学习方法，称为反应坐标（RC）流，用于分子系统的低维度动力学模型的发现。 RC流使用了 нормализа化流来设计坐标变换，并使用布朗运动模型来近似RC的动力学，其中所有模型参数都可以在数据驱动下进行估计。与现有的分子动力学减少方法不同，RC流提供了可训练和可追踪的维度减少模型，因为正则化流的逆函数是可逆的。此外，基于布朗运动的减少动力学模型在研究中提供了一个可见的和准确的低维度表示形式，用于描述分子系统中的中间态。数值实验表明，提议的方法可以高效地从仿真数据中提取可读取和准确的低维度表示。
</details></li>
</ul>
<hr>
<h2 id="Force-directed-graph-embedding-with-hops-distance"><a href="#Force-directed-graph-embedding-with-hops-distance" class="headerlink" title="Force-directed graph embedding with hops distance"></a>Force-directed graph embedding with hops distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05865">http://arxiv.org/abs/2309.05865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Lotfalizadeh, Mohammad Al Hasan</li>
<li>for: 本研究旨在提出一种基于力的图嵌入方法，用于保持图结构和结构特征。</li>
<li>methods: 本方法使用定量的吸引和排斥力来嵌入节点，并通过牛顿第二定律计算每个节点的加速度。方法易于理解，可并行，高度扩展。</li>
<li>results: 在多个图分析任务中，本方法与现有无监督嵌入方法相比，实现了竞争性的性能。<details>
<summary>Abstract</summary>
Graph embedding has become an increasingly important technique for analyzing graph-structured data. By representing nodes in a graph as vectors in a low-dimensional space, graph embedding enables efficient graph processing and analysis tasks like node classification, link prediction, and visualization. In this paper, we propose a novel force-directed graph embedding method that utilizes the steady acceleration kinetic formula to embed nodes in a way that preserves graph topology and structural features. Our method simulates a set of customized attractive and repulsive forces between all node pairs with respect to their hop distance. These forces are then used in Newton's second law to obtain the acceleration of each node. The method is intuitive, parallelizable, and highly scalable. We evaluate our method on several graph analysis tasks and show that it achieves competitive performance compared to state-of-the-art unsupervised embedding techniques.
</details>
<details>
<summary>摘要</summary>
“图像投影”已成为分析图形数据的重要技术。通过将图像中的节点转化为低维度空间中的向量，图像投影可以帮助提高图像处理和分析任务的效率，如节点分类、链接预测和视觉化。在这篇论文中，我们提出了一种新的力导向的图像投影方法，该方法利用稳定加速公式来嵌入节点，以保持图像的结构特征和特征。我们的方法模拟了一组自定义吸引和排斥力 zwischen所有节点对，根据它们的跳跃距离。这些力量然后用新顿第二定律来获得每个节点的加速度。我们的方法是直观、并行化和可扩展的。我们对一些图像分析任务进行评估，并证明了我们的方法与当前的无监督嵌入技术相比，具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="The-bionic-neural-network-for-external-simulation-of-human-locomotor-system"><a href="#The-bionic-neural-network-for-external-simulation-of-human-locomotor-system" class="headerlink" title="The bionic neural network for external simulation of human locomotor system"></a>The bionic neural network for external simulation of human locomotor system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05863">http://arxiv.org/abs/2309.05863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Shuhao Ma, Yihui Zhao<br>for:This paper aims to develop a physics-informed deep learning method to predict joint motion and muscle forces using musculoskeletal (MSK) modeling techniques.methods:The proposed method embeds the MSK model into a neural network as an ordinary differential equation (ODE) loss function, which enables the automatic estimation of subject-specific MSK physiological parameters during the training process.results:Experimental validations on two datasets from six healthy subjects demonstrate that the proposed deep learning method can effectively identify subject-specific MSK physiological parameters and the trained physics-informed forward-dynamics surrogate yields accurate motion and muscle forces predictions.Here is the Chinese version of the three key points:for:这篇论文的目的是通过musculoskeletal（MSK）模型技术预测 JOINT动作和肌肉力。方法:提议的方法将MSK模型 embedding到神经网络中作为常微分方程（ODE）损失函数，这使得在训练过程中自动估算subject特定的MSK生物物理参数成为可能。结果:对两个数据集（一个benchmark dataset和一个自收集dataset）中的六名健康志愿者进行实验验证，结果表明提议的深度学习方法可以有效地自动估算subject特定的MSK生物物理参数，并且训练的物理学习前继模型可以准确预测 JOINT动作和肌肉力。<details>
<summary>Abstract</summary>
Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These parameters are automatically estimated during the training process which guides the prediction of muscle forces combined with the MSK forward dynamics model. Experimental validations on two groups of data, including one benchmark dataset and one self-collected dataset from six healthy subjects, are performed. The results demonstrate that the proposed deep learning method can effectively identify subject-specific MSK physiological parameters and the trained physics-informed forward-dynamics surrogate yields accurate motion and muscle forces predictions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用musculoskeletal（MSK）模型技术计算运动质量的muscle forces和 JOINT动态可以提供有用的度量。模型基于的计算MSK模型可以解释动态INTERACTION between neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics。然而，这种解决方案受到高计算时间和muscle recruitment问题的困扰，特别是在复杂的模拟中。在最近几年，数据驱动方法作为一种可能的替代方案而出现，这是因为它们具有灵活性和适应性的优点。然而，大量标注训练数据很难被获得。本文提出了一种基于MSK模型的物理学习方法，用于预测 JOINT动态和muscle forces。MSK模型被嵌入到神经网络中作为常微分方程（ODE）损失函数，以便在训练过程中自动计算 OUT physiological parameters of muscle activation dynamics和muscle contraction dynamics。这些参数被自动计算出来，并且指导预测muscle forces的计算，与MSK前向动力学模型相结合。对于两个数据集进行了实验验证，包括一个benchmark数据集和一个自己收集的数据集，从六名健康者获得。结果表明，提议的深度学习方法可以有效地Identify subject-specific MSK physiological parameters，并且训练的物理学习前向动力学Surrogate可以准确预测 JOINT动态和muscle forces。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-mesa-optimization-algorithms-in-Transformers"><a href="#Uncovering-mesa-optimization-algorithms-in-Transformers" class="headerlink" title="Uncovering mesa-optimization algorithms in Transformers"></a>Uncovering mesa-optimization algorithms in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05858">http://arxiv.org/abs/2309.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jimmieliu/transformer-mesa-layer">https://github.com/jimmieliu/transformer-mesa-layer</a></li>
<li>paper_authors: Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Agüera y Arcas, Max Vladymyrov, Razvan Pascanu, João Sacramento</li>
<li>for: 这篇论文探讨了Transformers模型的性能优势的原因，并提出了一种假设，即Transformers的强大表现与其架构偏好有关，即内部的mesa-优化过程。</li>
<li>methods: 作者通过对一系列autoregressive Transformers模型进行反向工程，揭示了这些模型内部的gradient-based mesa-优化算法，并证明了这些算法可以用于解决少量预测任务。</li>
<li>results: 作者发现，mesa-优化可以在几乎没有预训练数据的情况下，使模型在几种supervised few-shot任务中表现出色，这表明mesa-优化可能是大型自然语言模型中的一种隐藏的功能。此外，作者还提出了一种新的自注意层，即mesa-层，可以证明这种假设。<details>
<summary>Abstract</summary>
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.
</details>
<details>
<summary>摘要</summary>
卷积Transformers已成为深度学习中最具有优势的模型，但它们的高性能的原因尚未完全理解。在这里，我们提出了一种假设，即Transformers的强大表现归功于模型的内部优化偏好，即在前向传播中的搜索过程中学习出的一种内部学习目标，以及该目标的优化解决方法。为检验这一假设，我们将对一系列基于序列模型的 autoregressive Transformers 进行反向工程，揭示了这些模型中的gradient-based mesa-optimization算法，以及它们如何驱动预测的生成。此外，我们还证明了这些学习前向传播优化算法可以立即应用于解决一些简单的supervised few-shot任务，表明了mesa-optimization可能在大语言模型中具有卷积学习的能力。最后，我们提出了一种新的自注意层，即mesa-层，它可以Explicitly and efficiently solve context-specified optimization problems。我们发现，这层可以在synthetic和初步语言模型实验中提高性能，加强了我们假设，即mesa-optimization是训练过的Transformers中隐藏的重要操作。
</details></li>
</ul>
<hr>
<h2 id="Energy-Preservation-and-Stability-of-Random-Filterbanks"><a href="#Energy-Preservation-and-Stability-of-Random-Filterbanks" class="headerlink" title="Energy Preservation and Stability of Random Filterbanks"></a>Energy Preservation and Stability of Random Filterbanks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05855">http://arxiv.org/abs/2309.05855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Haider, Vincent Lostanlen, Martin Ehler, Peter Balazs</li>
<li>for: 这篇论文主要写于哪？</li>
<li>methods: 这篇论文使用了哪些方法？</li>
<li>results: 这篇论文得到了哪些结果？Here are the answers to these questions, in Simplified Chinese:</li>
<li>for: 这篇论文主要写于哪？	+ 这篇论文探讨了waveform-based deep learning在filterbank设计方面的困难，尤其是 convolutional neural networks (convnets) 在实际应用中 often fail to outperform hand-crafted baselines.</li>
<li>methods: 这篇论文使用了哪些方法？	+ 这篇论文使用了 random convolutional operators 来描述 convnets 的统计性质，并发现了FIR filterbanks with random Gaussian weights 在大filter和本地 périodic input signal的情况下是不稳定的。</li>
<li>results: 这篇论文得到了哪些结果？	+ 这篇论文发现了 expected energy preservation 不 suficient for numerical stability,并 derive了 theoretical bounds for its expected frame bounds.<details>
<summary>Abstract</summary>
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为什么波形基于深度学习 så difficult? 虽然许多人尝试用 convolutional neural networks (convnets) 来设计滤波器，但它们经常无法超越手动设计的基线。这对于这些基线来说更加怪异，因为它们是线性时间不变的系统，因此它们的传输函数可以由一个大感知范围的 convnet 准确地表示。在这篇文章中，我们从数学角度深入探讨简单的 convnet 的统计性质。我们发现，使用随机 Gaussian 权重的 FIR 滤波器在大 filter 和本地 периодиic 输入信号下是不可靠的，这些输入信号是音频信号处理应用中的常见情况。此外，我们发现，预期能量保持不够以确保数学稳定性，我们 derive 了预期帧边界的理论上限。
</details></li>
</ul>
<hr>
<h2 id="ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation"><a href="#ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation" class="headerlink" title="ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation"></a>ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05853">http://arxiv.org/abs/2309.05853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory W. Kyro, Anton Morgunov, Rafael I. Brent, Victor S. Batista</li>
<li>for: 这个研究的目的是开发一种新的、有效的 semi-supervised active learning 方法，以便在生成型人工智能模型中进行评估和调整。</li>
<li>methods: 这种方法利用了一种构建的样本空间代理，通过灵活地操作在这个代理中，以便调整一个基于 GPT 的分子生成器，使其与一个蛋白质目标之间具有吸引力的互动。</li>
<li>results: 研究人员通过使用这种方法，能够快速地和有效地调整 GPT 模型，以便在分子生成中实现更高的吸引力和互动性。此外，这种方法不需要评估所有的数据点，因此可以使用 computationally expensive metrics。<details>
<summary>Abstract</summary>
The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-tuning, enabling the incorporation of computationally expensive metrics. We are hopeful that the inherent generality of this methodology ensures that it will remain applicable as this exciting field evolves. To facilitate implementation and reproducibility, we have made all of our software available through the open-source ChemSpaceAL Python package.
</details>
<details>
<summary>摘要</summary>
具有强大生成能力的人工智能模型在药物发现领域的应用已经是不可避免的。因此，开发能够提高这些强大工具的能力和可应用性的方法是非常重要的。在这个工作中，我们提出了一种新的、高效的半监督学习方法，可以让一个生成模型与一个目标函数进行精细调整，通过在一个建立的样本空间中策略性操作。在聚合分子生成中，我们示示了如何通过在一个化学空间代理中精细调整一个基于GPT的分子生成器，以便 maximize 分子和蛋白质目标之间的有吸引力相互作用。值得注意的是，我们的方法不需要评估所有用于精细调整的数据点，因此可以包含计算成本高昂的指标。我们希望这种方法的内在通用性可以保证它在这个赏心感激的领域中保持可靠。为了促进实现和重现性，我们将所有的软件公开发布在开源的ChemSpaceAL Python包中。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Compiler-Optimization"><a href="#Large-Language-Models-for-Compiler-Optimization" class="headerlink" title="Large Language Models for Compiler Optimization"></a>Large Language Models for Compiler Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07062">http://arxiv.org/abs/2309.07062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Kim Hazelwood, Gabriel Synnaeve, Hugh Leather</li>
<li>for: 这篇论文主要目的是提出一种使用大语言模型进行代码优化的新应用。</li>
<li>methods: 论文使用了一个7亿参数的变换器模型，从头开始训练，以优化 LLVM  assembly 代码大小。模型接受不优化的Assembly输入，并输出一个包含编译器选项的列表，以最优化程序。在训练过程中，模型需要预测未优化代码和优化后代码的指令计数，以及优化后的代码本身。这些辅助学习任务有助于提高优化模型的性能和代码理解深度。</li>
<li>results: 论文在一个大量测试程序中进行了评估。对比两个基线，我们的方法实现了减少指令计数的3.0%的提高，并在70%的时间内完全评估器的输出。此外，模型还表现出了强大的代码理解能力，在91%的时间内生成可编译代码，并在70%的时间内完全评估器的输出。<details>
<summary>Abstract</summary>
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.   We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
</details>
<details>
<summary>摘要</summary>
我们探索了大型语言模型在代码优化中的新应用。我们介绍了一个7亿参数的变换器模型，从零开始训练以优化LLVMAssembly代码的大小。该模型接受未优化的Assembly输入，并输出一个包含编译器选项的列表，以最佳化程序。在训练时，我们请求模型预测未优化代码和优化后代码中的指令计数，以及优化后的代码本身。这些辅助学习任务显著提高了优化性能，并提高了模型对代码的深度理解。我们对一个大量测试程序进行评估。我们的方法实现了减少指令计数的3.0%的提高，比两个状态流行的基线要好，这些基线需要数千次编译。此外，模型显示了奇异的代码理解能力，生成可 COMPILE 的代码91%的时间，并完美地模拟了编译器的输出70%的时间。
</details></li>
</ul>
<hr>
<h2 id="Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data"><a href="#Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data" class="headerlink" title="Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data"></a>Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05845">http://arxiv.org/abs/2309.05845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjia Niu, Yuchen Zhao, Hamed Haddadi</li>
<li>for: 这篇论文是为了探讨如何实现精准的异常活动探测在智能健康领域中，使用多元时间系列数据。</li>
<li>methods: 本研究使用了一种剩余基于的异常探测方法（Rs-AD），实现了异常活动探测的有效表示学习和探测。</li>
<li>results: 实验结果显示，使用Rs-AD方法可以在一个真实世界的步态数据中达到F1分数0.839的高精度异常探测。<details>
<summary>Abstract</summary>
Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）数据从多个传感器获取，可以准确地探测智能医疗场景中异常活动。然而，异常现象在MTS数据中表现出多种模式，容易被忽略。因此，实现准确的异常探测是一项挑战，因为我们需要捕捉时间序列的 temporal dependencies 和变量之间的关系。为解决这个问题，我们提出了基于差异的异常检测方法（Rs-AD），用于有效地学习表示和异常检测。我们在一个真实的步态数据集上进行了实验，并得到了 F1 分数为 0.839。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals"><a href="#Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals" class="headerlink" title="Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals"></a>Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05843">http://arxiv.org/abs/2309.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Blankemeier, Sebastien Baur, Wei-Hung Weng, Jake Garrison, Yossi Matias, Shruthi Prabhakara, Diego Ardila, Zaid Nabulsi</li>
<li>for: 这个论文主要用于健康相关的声学信号，如喊喊和呼吸 зву频，以便医疗诊断和连续健康监测。</li>
<li>methods: 这个论文使用了一种自动学习框架，即SimCLR，并使用了一种名为Slowfast NFNet的底层模型。这些模型通过对健康声学信号进行对比学习来学习。</li>
<li>results: 研究人员通过对不同健康声学任务进行分析，发现合适的声音变换策略可以提高Slowfast NFNet声音编码器的性能。这些变换策略可以单独应用或相互结合，以实现更高的性能。<details>
<summary>Abstract</summary>
Health-related acoustic signals, such as cough and breathing sounds, are relevant for medical diagnosis and continuous health monitoring. Most existing machine learning approaches for health acoustics are trained and evaluated on specific tasks, limiting their generalizability across various healthcare applications. In this paper, we leverage a self-supervised learning framework, SimCLR with a Slowfast NFNet backbone, for contrastive learning of health acoustics. A crucial aspect of optimizing Slowfast NFNet for this application lies in identifying effective audio augmentations. We conduct an in-depth analysis of various audio augmentation strategies and demonstrate that an appropriate augmentation strategy enhances the performance of the Slowfast NFNet audio encoder across a diverse set of health acoustic tasks. Our findings reveal that when augmentations are combined, they can produce synergistic effects that exceed the benefits seen when each is applied individually.
</details>
<details>
<summary>摘要</summary>
医疗相关的声学信号，如喊喊和呼吸声，对医疗诊断和连续健康监测有重要意义。现有的大多数机器学习方法对健康声学是特定任务的训练和评估，这限制了它们在各种医疗应用中的一致性。在这篇论文中，我们利用了一个自我超vised学习框架，SimCLR，并与Slowfast NFNet骨干结构进行对比学习健康声学。我们认为对于这种应用，适合NFNet音频编码器的优化是一个关键性的问题。我们进行了各种声音变换策略的深入分析，并证明了合适的声音变换策略可以提高Slowfast NFNet音频编码器在多种健康声学任务中的表现。我们的发现表明，当变换策略相互组合时，它们可以产生相互补做的效果，超过每个策略应用 separately的效果。
</details></li>
</ul>
<hr>
<h2 id="The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems"><a href="#The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems" class="headerlink" title="The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems"></a>The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05837">http://arxiv.org/abs/2309.05837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai-Chieh Hsu, Haimin Hu, Jaime Fernández Fisac</li>
<li>for: 本文旨在探讨安全筛选技术的综述，以便更好地理解、比较和结合不同类型的安全筛选方法。</li>
<li>methods: 本文使用了数据驱动方法和模型驱动方法的结合，以提高安全筛选技术的可扩展性和可靠性。</li>
<li>results: 本文提出了一种统一的技术框架，可以更好地理解和结合不同类型的安全筛选方法，并且提供了未来研究的指导方向，包括更加扩展的synthesis、更加稳定的监测和更加高效的干涉。<details>
<summary>Abstract</summary>
Recent years have seen significant progress in the realm of robot autonomy, accompanied by the expanding reach of robotic technologies. However, the emergence of new deployment domains brings unprecedented challenges in ensuring safe operation of these systems, which remains as crucial as ever. While traditional model-based safe control methods struggle with generalizability and scalability, emerging data-driven approaches tend to lack well-understood guarantees, which can result in unpredictable catastrophic failures. Successful deployment of the next generation of autonomous robots will require integrating the strengths of both paradigms. This article provides a review of safety filter approaches, highlighting important connections between existing techniques and proposing a unified technical framework to understand, compare, and combine them. The new unified view exposes a shared modular structure across a range of seemingly disparate safety filter classes and naturally suggests directions for future progress towards more scalable synthesis, robust monitoring, and efficient intervention.
</details>
<details>
<summary>摘要</summary>
近年来，机器人自主技术受到了 significiant progress，同时机器人技术的扩展也随之扩大。然而，新的部署领域的出现带来了前所未有的安全操作问题的挑战，这些问题仍然具有极高的重要性。传统的模型基于的安全控制方法在普适性和可扩展性方面受到限制，而新兴的数据驱动方法则具有不可预期的崩溃问题。成功部署下一代自主机器人需要结合两个 парадиг之力。本文提供了安全筛 Approaches 的评论，并将现有技术相关的重要连接点推广，并提出了一个统一的技术框架，以便更好地理解、比较和结合它们。新的统一视图暴露了一系列看起来有所不同的安全筛类型之间的共同模块结构，自然地指明了未来进程中更好的扩展、稳定监测和高效 intervención的方向。
</details></li>
</ul>
<hr>
<h2 id="PACE-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis"><a href="#PACE-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis" class="headerlink" title="PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis"></a>PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05833">http://arxiv.org/abs/2309.05833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Zhang, Xuchao Zhang, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan</li>
<li>for: 提高云事件管理中的root cause分析工具的可靠性，以确保服务可靠性和客户信任。</li>
<li>methods: 利用提取-扩展大语言模型（LLMs）来提高root cause分析工具的自信估计。该方法包括两个阶段：首先，模型根据历史事件数据评估自己的自信程度；然后，模型评估由预测器生成的root cause。最后，优化步骤将这两个评估结果组合起来确定最终的自信分配。</li>
<li>results: 实验结果表明，我们的方法可以让模型更好地表达自己的自信程度，提供更加准确的分数。我们回答了一系列研究问题，包括使用LLMs生成calibrated confidence scores的能力，针对特定领域检索到的示例对自信估计的影响，以及该方法在不同的root cause分析模型中的普适性。通过这些研究，我们希望bridge the confidence estimation gap，帮助云事件管理人员更加快速和准确地做出决策。<details>
<summary>Abstract</summary>
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the final confidence assignment. Experimental results illustrate that our method enables the model to articulate its confidence effectively, providing a more calibrated score. We address research questions evaluating the ability of our method to produce calibrated confidence scores using LLMs, the impact of domain-specific retrieved examples on confidence estimates, and its potential generalizability across various root cause analysis models. Through this, we aim to bridge the confidence estimation gap, aiding on-call engineers in decision-making and bolstering the efficiency of cloud incident management.
</details>
<details>
<summary>摘要</summary>
This paper proposes a method to enhance the confidence estimation in root cause analysis tools by leveraging retrieval-augmented large language models (LLMs). This approach consists of two phases. First, the model evaluates its confidence based on historical incident data, considering the strength of the evidence. Then, the model reviews the root cause generated by the predictor and an optimization step combines these evaluations to determine the final confidence assignment.Experimental results show that our method enables the model to provide more calibrated confidence scores. To address research questions, we evaluate the ability of our method to produce calibrated confidence scores using LLMs, the impact of domain-specific retrieved examples on confidence estimates, and its potential generalizability across various root cause analysis models. Our goal is to bridge the confidence estimation gap, helping on-call engineers make informed decisions and improve the efficiency of cloud incident management.
</details></li>
</ul>
<hr>
<h2 id="Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning"><a href="#Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning" class="headerlink" title="Instance-Agnostic Geometry and Contact Dynamics Learning"></a>Instance-Agnostic Geometry and Contact Dynamics Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05832">http://arxiv.org/abs/2309.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengti Sun, Bowen Jiang, Bibit Bianchini, Camillo Jose Taylor, Michael Posa</li>
<li>for: 学习机器人对物体的shape、pose和物理性能的同时学习</li>
<li>methods: 使用geometry作为共享表示，将视觉和动力学 fusion在一起，不需要知道形状先验或动作捕捉输入</li>
<li>results: 实验表明，该框架可以学习rigid和圆形物体的geometry和动力学性能，并且超越当前的跟踪框架<details>
<summary>Abstract</summary>
This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety"><a href="#Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety" class="headerlink" title="Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety"></a>Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05831">http://arxiv.org/abs/2309.05831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Bertrand, Nick Griffey, Ming-Lun Lu, Rashmi Jha</li>
<li>for: 本研究旨在将实验室训练的机器学习模型移植到实际世界中。</li>
<li>methods: 本研究使用了四种可能的解决方案来增强模型性能，包括实验室训练数据的调整、实际世界资料的调整、模型的升级和降级。</li>
<li>results: 经过四种解决方案的实践，模型的性能有所提高，但仍然不如实验室训练数据中的性能。<details>
<summary>Abstract</summary>
Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
</details>
<details>
<summary>摘要</summary>
将实验室训练的机器学习模型应用到实际世界中一直是一个挑战。本文讨论了将实验室训练的抓推模型在实际世界中的应用，其性能与训练数据之间存在很大差异。我们探索了失败的原因，并提出了四种可能的解决方案以提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting"><a href="#Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting" class="headerlink" title="Exploring Geometric Deep Learning For Precipitation Nowcasting"></a>Exploring Geometric Deep Learning For Precipitation Nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05828">http://arxiv.org/abs/2309.05828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Zhao, Sudipan Saha, Zhitong Xiong, Niklas Boers, Xiao Xiang Zhu</li>
<li>for: 预测 precipitation nowcasting (up to a few hours)</li>
<li>methods: 使用 Geometric deep learning-based temporal Graph Convolutional Network (GCN)</li>
<li>results: 提高了地方详细云Profile的模型效果和预测精度， achievement decreased error measures.<details>
<summary>Abstract</summary>
Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value during the training procedure. Then, the spatial relationship is refined by GCN layers while the temporal information is extracted by 1D convolution with various kernel lengths. The neighboring information is fed as auxiliary input layers to improve the final result. We test the model on sequences of radar reflectivity maps over the Trento/Italy area. The results show that GCNs improves the effectiveness of modeling the local details of the cloud profile as well as the prediction accuracy by achieving decreased error measures.
</details>
<details>
<summary>摘要</summary>
现在降水预测（几个小时）仍然是一个挑战，因为需要准确地捕捉当地复杂的地方交互。卷积神经网络通过卷积核心与格子数据进行卷积，并提取特征被限制的接受范围内，通常表现为过度平滑的输出与真实值之间的差异。因此它们缺乏模型地方域之间的复杂关系的能力。非ユーク利德学习 targets 非ユーク利德空间中的神经网络模型，这些模型可以更加灵活地定义节点和边，并有效地捕捉地理Grid中的动态空间关系。为了实现这一目标，我们尝试使用非ユーク利德学习基于 temporal Graph Convolutional Network (GCN)  для降水预测。在训练过程中，自动学习 adjacency 矩阵，该矩阵模拟地理Grid 之间的交互，并通过 L1 损失函数与真实值像素值进行比较。然后，GCN 层可以更好地模型地方域之间的空间关系，同时1D 卷积可以提取时间信息。邻居信息被作为辅助输入层提供，以提高最终结果。我们在 Trento/意大利 地区的雷达反射率图序列上测试了模型。结果显示，GCNs 可以更好地模型云Profile 的本地细节以及预测精度，并实现降低误差度量。
</details></li>
</ul>
<hr>
<h2 id="KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks"><a href="#KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks" class="headerlink" title="KD-FixMatch: Knowledge Distillation Siamese Neural Networks"></a>KD-FixMatch: Knowledge Distillation Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05826">http://arxiv.org/abs/2309.05826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien-Chih Wang, Shaoyuan Xu, Jinmiao Fu, Yang Liu, Bryan Wang<br>for: 这个论文的目的是提出一种新的 semi-supervised learning（SSL）算法，以Addressing the challenge of limited labeled data in deep learning。methods: 这个论文使用了一种siamese neural network（SNN），同时采用了知识传播（KD）技术来增强性能和减少性能下降。results: 实验结果表明，KD-FixMatch在四个公开的数据集上都表现比FixMatch更好，实验结果显示KD-FixMatch在训练开始点上有更好的性能，从而导致模型的性能提高。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has become a crucial approach in deep learning as a way to address the challenge of limited labeled data. The success of deep neural networks heavily relies on the availability of large-scale high-quality labeled data. However, the process of data labeling is time-consuming and unscalable, leading to shortages in labeled data. SSL aims to tackle this problem by leveraging additional unlabeled data in the training process. One of the popular SSL algorithms, FixMatch, trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN). However, it is prone to performance degradation when the pseudo labels are heavily noisy in the early training stage. We present KD-FixMatch, a novel SSL algorithm that addresses the limitations of FixMatch by incorporating knowledge distillation. The algorithm utilizes a combination of sequential and simultaneous training of SNNs to enhance performance and reduce performance degradation. Firstly, an outer SNN is trained using labeled and unlabeled data. After that, the network of the well-trained outer SNN generates pseudo labels for the unlabeled data, from which a subset of unlabeled data with trusted pseudo labels is then carefully created through high-confidence sampling and deep embedding clustering. Finally, an inner SNN is trained with the labeled data, the unlabeled data, and the subset of unlabeled data with trusted pseudo labels. Experiments on four public data sets demonstrate that KD-FixMatch outperforms FixMatch in all cases. Our results indicate that KD-FixMatch has a better training starting point that leads to improved model performance compared to FixMatch.
</details>
<details>
<summary>摘要</summary>
深度学习中的半指导学习（SSL）已成为深度学习的一种重要方法，以解决有限的标注数据的挑战。深度神经网络的成功几乎完全取决于大规模高质量的标注数据。然而，数据标注是时间consuming和不可扩展的，导致标注数据的短缺。SSL通过利用额外的无标注数据来解决这个问题，从而提高模型的性能。 FixMatch 是一种流行的 SSL 算法，它使用同一个权重共享教师和学生网络来同时训练 identical 的 Siamese 神经网络（SNN）。然而， FixMatch 在早期训练阶段 pseudo 标签具有很大的噪声，可能导致性能下降。我们提出了 KD-FixMatch，一种新的 SSL 算法，它通过杂合Sequential 和同时训练 SNNs来提高性能并降低性能下降。首先，一个外部 SNN 通过标注和无标注数据进行训练。然后，外部 SNN 网络已经训练好的部分生成 pseudo 标签 для无标注数据，并从中选择一 subset 的无标注数据，通过高信息抽象和深度嵌入划分来生成可信 pseudo 标签。最后，一个内部 SNN 通过标注数据、无标注数据和可信 pseudo 标签进行训练。我们在四个公共数据集上进行了实验，结果表明 KD-FixMatch 在所有情况下都高于 FixMatch。我们的结果表明 KD-FixMatch 具有更好的训练起点，导致模型的性能得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems"><a href="#Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems" class="headerlink" title="Ensemble-based modeling abstractions for modern self-optimizing systems"></a>Ensemble-based modeling abstractions for modern self-optimizing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05823">http://arxiv.org/abs/2309.05823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Töpfer, Milad Abdullah, Tomáš Bureš, Petr Hnětynka, Martin Kruliš</li>
<li>for: 这篇论文旨在扩展DEECo ensemble模型，以便在自动化组件ensemble中使用机器学习和优化规则。</li>
<li>methods: 论文使用机器学习和优化规则在模型层次上捕捉了这些概念，并给出了应用于工业4.0场景中访问控制问题的示例。</li>
<li>results: 论文表明，在智能系统中包含机器学习和优化规则是关键特征，可以让系统在运行时根据环境不确定性进行学习和优化。<details>
<summary>Abstract</summary>
In this paper, we extend our ensemble-based component model DEECo with the capability to use machine-learning and optimization heuristics in establishing and reconfiguration of autonomic component ensembles. We show how to capture these concepts on the model level and give an example of how such a model can be beneficially used for modeling access-control related problem in the Industry 4.0 settings. We argue that incorporating machine-learning and optimization heuristics is a key feature for modern smart systems which are to learn over the time and optimize their behavior at runtime to deal with uncertainty in their environment.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对我们的ensemble-based组件模型DEECo进行扩展，以便在自动化组件集合的建立和重新配置中使用机器学习和优化办法。我们示出了这些概念在模型层面的捕捉方式，并给出了在Industry 4.0设置下模型访问控制相关问题的示例。我们认为，在运行时使用机器学习和优化办法是现代智能系统的关键特征，以便在环境中适应不确定性并优化其行为。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-learning-of-effective-dynamics-for-multiscale-systems"><a href="#Interpretable-learning-of-effective-dynamics-for-multiscale-systems" class="headerlink" title="Interpretable learning of effective dynamics for multiscale systems"></a>Interpretable learning of effective dynamics for multiscale systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05812">http://arxiv.org/abs/2309.05812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Menier, Sebastian Kaltenbach, Mouadh Yagoubi, Marc Schoenauer, Petros Koumoutsakos</li>
<li>for: 本研究旨在提出一种可解释的学习有效动力框架（iLED），以提高高维多Scale系统的模型和 simulations 的精度和可读性。</li>
<li>methods: 该研究基于深度回归神经网络，并结合了Mori-Zwanzig和Koopman运算符理论，以提供可解释的模型和 simulations 结果。</li>
<li>results: 研究在三个 benchmark 高维多Scale系统中的 simulations 中展示了 iLED 框架的可行性和精度，并且可以获得可读的动力学结果。<details>
<summary>Abstract</summary>
The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit of interpretability. The iLED framework is motivated by Mori-Zwanzig and Koopman operator theory, which justifies the choice of the specific architecture. We demonstrate the effectiveness of the proposed framework in simulations of three benchmark multiscale systems. Our results show that the iLED framework can generate accurate predictions and obtain interpretable dynamics, making it a promising approach for solving high-dimensional multiscale systems.
</details>
<details>
<summary>摘要</summary>
高维度多尺度系统的模型和仿真是科学和工程领域的关键挑战。广泛认为，即使今天的计算机技术不断发展，仍然无法解决所有空间时间尺度的方程。这一现实启发了对模型简化技术的激烈尝试。在过去几年，基于深度循环神经网络的技术已经生成了模拟复杂空间时间系统的出色结果，并且可以采用实验和计算数据来扩展模型。但是，神经网络缺乏可读性，这限制了它们在复杂系统中的应用和普遍性。我们提出了一种新的框架——可读性学习有效动力（iLED）框架，它可以与当前最佳的神经网络技术相比，并提供可读性的加值。iLED框架是由莫里- Zwanzig 和库曼操作理论所驱动，这种架构设计是有理由的。我们在三个标准多尺度系统的仿真中展示了iLED框架的效果，结果表明，iLED框架可以生成准确预测和获得可读性的动力学，这使其成为解决高维度多尺度系统的有力的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models"><a href="#Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models"></a>Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05811">http://arxiv.org/abs/2309.05811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Xu, Stella Offner, Robert Gutermuth, Michael Grudic, David Guszejnov, Philip Hopkins</li>
<li>for: 这 paper 是为了量化遮盖环境中的辐射反馈的影响而写的。</li>
<li>methods: 这 paper 使用了深度学习技术，特别是推 diffusion probabilistic models (DDPMs) 来预测遮盖环境中的辐射场强。</li>
<li>results: 这 paper 的结果表明，使用 DDPMs 预测辐射场强可以准确地捕捉遮盖环境中的辐射反馈变化。<details>
<summary>Abstract</summary>
Accurately quantifying the impact of radiation feedback in star formation is challenging. To address this complex problem, we employ deep learning techniques, denoising diffusion probabilistic models (DDPMs), to predict the interstellar radiation field (ISRF) strength based on three-band dust emission at 4.5 \um, 24 \um, and 250 \um. We adopt magnetohydrodynamic simulations from the STARFORGE (STAR FORmation in Gaseous Environments) project that model star formation and giant molecular cloud (GMC) evolution. We generate synthetic dust emission maps matching observed spectral energy distributions in the Monoceros R2 (MonR2) GMC. We train DDPMs to estimate the ISRF using synthetic three-band dust emission. The dispersion between the predictions and true values is within a factor of 0.1 for the test set. We extended our assessment of the diffusion model to include new simulations with varying physical parameters. While there is a consistent offset observed in these out-of-distribution simulations, the model effectively constrains the relative intensity to within a factor of 2. Meanwhile, our analysis reveals weak correlation between the ISRF solely derived from dust temperature and the actual ISRF. We apply our trained model to predict the ISRF in MonR2, revealing a correspondence between intense ISRF, bright sources, and high dust emission, confirming the model's ability to capture ISRF variations. Our model robustly predicts radiation feedback distribution, even in complex, poorly constrained ISRF environments like those influenced by nearby star clusters. However, precise ISRF predictions require an accurate training dataset mirroring the target molecular cloud's unique physical conditions.
</details>
<details>
<summary>摘要</summary>
准确量化辐射反馈在星系形成中的影响是一个复杂的问题。为了解决这个问题，我们使用深度学习技术，即杂流扩散概率模型（DDPM），预测辐射场强度基于3个频率尘埃辐射（4.5μm、24μm和250μm）。我们采用了 magnetohydrodynamic模拟（STARFORGE）项目，模拟星系形成和大分子云（GMC）的发展。我们生成了匹配观测spectral energy distribution的人造尘埃辐射图像。我们使用DDPM进行训练，使其估算辐射场。我们发现在测试集上，模型的误差在0.1的因子范围内。我们对模型进行了进一步的评估，包括在不同物理参数下运行的新模拟。尽管在这些外部 simulate 中出现了一定的偏差，但模型仍能够将辐射场的相对强度限制在2的因子范围内。此外，我们发现尘埃温度 alone 不能准确地预测辐射场。我们应用我们已经训练的模型，预测Monoceros R2（MonR2）星系中的辐射场，发现辐射场的强度与亮度和尘埃辐射之间存在相关性。这表明我们的模型可以准确地预测辐射反馈的分布。然而，精准预测辐射场的精度需要一个准确地反映目标分子云的物理条件的训练集。
</details></li>
</ul>
<hr>
<h2 id="SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors"><a href="#SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors" class="headerlink" title="SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors"></a>SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05810">http://arxiv.org/abs/2309.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongge Chen, Zhao Chen, Gregory P. Meyer, Dennis Park, Carl Vondrick, Ashish Shrivastava, Yuning Chai</li>
<li>for: 提高自动驾驶系统的安全性，检测3D对象的潜在漏洞。</li>
<li>methods: 使用signed distanced function(SDF)表示3D对象，通过权重错误信号来平滑变形对象的形状或姿态，以惑动下游3D检测器。</li>
<li>results: 生成的3D对象与基eline对象有semantic recognizable shape，但物理上有所不同。提供可解释的失败模式，帮助预测3D检测系统中的 potential safety risks。<details>
<summary>Abstract</summary>
We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.
</details>
<details>
<summary>摘要</summary>
我们介绍SHIFT3D，一个可导的管道用于生成3D形状，这些形状具有可能挑战3D物体检测器的结构性可能性。在自动驾驶等安全关键应用中，发现这些新的挑战性对象可以提供对3D检测器的不明之处的见解。通过使用签名距离函数(SDF)表示对象，我们表明了梯度错误信号允许我们平滑地变形或者对3D对象的形状或者姿态进行干扰，以让下游3D检测器困惑。重要的是，SHIFT3D生成的对象与基eline对象有所不同，但它们仍然保留了semantically可识别的形状。我们的方法提供了可解释的失败模式，可以帮助在3D感知系统中预先发现可能的安全隐患，以避免这些隐患在3D检测器失效之前成为重要的故障。
</details></li>
</ul>
<hr>
<h2 id="Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans"><a href="#Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans" class="headerlink" title="Divergences in Color Perception between Deep Neural Networks and Humans"></a>Divergences in Color Perception between Deep Neural Networks and Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05809">http://arxiv.org/abs/2309.05809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan O. Nadler, Elise Darragh-Ford, Bhargav Srinivasa Desikan, Christian Conaway, Mark Chu, Tasker Hull, Douglas Guilbeault</li>
<li>for: 本研究旨在探讨深度神经网络（DNNs）是否能够模型人类视觉，以及DNNs是否能够捕捉人类视觉中的基本特征。</li>
<li>methods: 本研究采用了新的实验方法来评估DNNs中的色彩协调性，并对DNNs的色彩预测结果与人类色彩判断结果进行比较。</li>
<li>results: 研究发现，现今的DNN模型（包括卷积神经网络和视transformer）在处理图像时的色彩预测结果与人类色彩判断结果存在很大差异，特别是对于图像中控制了色彩属性的图像、来自网络搜索的图像和真实世界的CIFAR-10 dataset中的图像。此外，研究还发现了一种可解释性和认知可能性的色彩模型，基于wavelet decomposition，可以更好地预测人类色彩判断结果。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are increasingly proposed as models of human vision, bolstered by their impressive performance on image classification and object recognition tasks. Yet, the extent to which DNNs capture fundamental aspects of human vision such as color perception remains unclear. Here, we develop novel experiments for evaluating the perceptual coherence of color embeddings in DNNs, and we assess how well these algorithms predict human color similarity judgments collected via an online survey. We find that state-of-the-art DNN architectures $-$ including convolutional neural networks and vision transformers $-$ provide color similarity judgments that strikingly diverge from human color judgments of (i) images with controlled color properties, (ii) images generated from online searches, and (iii) real-world images from the canonical CIFAR-10 dataset. We compare DNN performance against an interpretable and cognitively plausible model of color perception based on wavelet decomposition, inspired by foundational theories in computational neuroscience. While one deep learning model $-$ a convolutional DNN trained on a style transfer task $-$ captures some aspects of human color perception, our wavelet algorithm provides more coherent color embeddings that better predict human color judgments compared to all DNNs we examine. These results hold when altering the high-level visual task used to train similar DNN architectures (e.g., image classification versus image segmentation), as well as when examining the color embeddings of different layers in a given DNN architecture. These findings break new ground in the effort to analyze the perceptual representations of machine learning algorithms and to improve their ability to serve as cognitively plausible models of human vision. Implications for machine learning, human perception, and embodied cognition are discussed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-ML-Self-adaptation-in-Face-of-Traps"><a href="#Online-ML-Self-adaptation-in-Face-of-Traps" class="headerlink" title="Online ML Self-adaptation in Face of Traps"></a>Online ML Self-adaptation in Face of Traps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05805">http://arxiv.org/abs/2309.05805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Töpfer, František Plášil, Tomáš Bureš, Petr Hnětynka, Martin Kruliš, Danny Weyns</li>
<li>for: 本研究旨在探讨在智能农业场景中使用在线机器学习自适应系统时遇到的困难和抓子。</li>
<li>methods: 本研究使用在线机器学习来实现自适应机制，并采用了许多方法来评估ML基于估计器的 specification和在线训练的影响。</li>
<li>results: 本研究发现了一些在线机器学习自适应系统中遇到的困难和抓子，包括估计器的规范和在线训练的影响，以及如何评估这些陷阱的方法。<details>
<summary>Abstract</summary>
Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.
</details>
<details>
<summary>摘要</summary>
在线机器学习（ML）经常用于自适应系统，以增强自适应机制并提高系统的用途。尽管如此，将线机器学习应用于自适应可能是问题，而且不多的文献报告了这些问题的限制。我们在实验中将线机器学习应用于智能农业情况下的自适应，并遇到了许多未料的困难--陷阱。在这篇文章中，我们详细讨论了这些陷阱，包括估计器的规格和线上训练、它们对自适应的影响，以及评估估计器的方法。我们的这些陷阱的概述提供了一个列表的教训，可以作为其他研究人员和实践者在应用线机器学习于自适应时的指南。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models"><a href="#Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models" class="headerlink" title="Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models"></a>Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05803">http://arxiv.org/abs/2309.05803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet Singh, Stephen Tu, Vikas Sindhwani</li>
<li>for: 这个论文主要针对的问题是Robot学习管道中的策略表示方式选择，具体来说是使用可能的模型来生成下一个机器人动作的模型。</li>
<li>methods: 这篇论文提出了一种实用的训练目标和算法来训练能量模型（EBM）作为策略表示方式，包括层次抽象强化学习（R-NCE）、可学习负样本和非对抗共同训练等多个关键元素。</li>
<li>results: 论文的实验结果表明，使用提议的训练目标和算法可以训练能量模型作为策略表示方式，并且在多个复杂的多Modalbenchmark中与扩散模型和其他现有方法竞争，甚至超越它们。<details>
<summary>Abstract</summary>
A crucial design decision for any robot learning pipeline is the choice of policy representation: what type of model should be used to generate the next set of robot actions? Owing to the inherent multi-modal nature of many robotic tasks, combined with the recent successes in generative modeling, researchers have turned to state-of-the-art probabilistic models such as diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklore -- that energy models in high dimensional continuous spaces are impractical to train -- is false. We develop a practical training objective and algorithm for energy models which combines several key ingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative samplers, and (iii) non-adversarial joint training. We prove that our proposed objective function is asymptotically consistent and quantify its limiting variance. On the other hand, we show that the Implicit Behavior Cloning (IBC) objective is actually biased even at the population level, providing a mathematical explanation for the poor performance of IBC trained energy policies in several independent follow-up works. We further extend our algorithm to learn a continuous stochastic process that bridges noise and data, modeling this process with a family of EBMs indexed by scale variable. In doing so, we demonstrate that the core idea behind recent progress in generative modeling is actually compatible with EBMs. Altogether, our proposed training algorithms enable us to train energy-based models as policies which compete with -- and even outperform -- diffusion models and other state-of-the-art approaches in several challenging multi-modal benchmarks: obstacle avoidance path planning and contact-rich block pushing.
</details>
<details>
<summary>摘要</summary>
robot学习管道中的一个关键设计决策是选择策略表示方式：用什么类型的模型生成下一个机器人动作？由于许多机器人任务的本质是多模态的，加上近年来的生成模型的成功，研究人员就转向了当今最先进的概率模型，如扩散模型，作为策略表示方式。在这个工作中，我们重新评估了能量模型（EBM）作为策略类型。我们证明了一些人们常见的假设——在高维连续空间中使用能量模型是不实用的——是错误的。我们开发了一个实用的训练目标和算法，该算法结合了多个关键组成部分：（i）排名噪声对比估计（R-NCE），（ii）可学习的负样本，以及（iii）非对抗联合训练。我们证明了我们的提出的目标函数是极限共轭的，并且量化了其极限干扰。相比之下，我们显示了冲击行为塑化（IBC）目标函数实际上偏导向，并提供了一个数学解释，以解释 diffusion models 和其他当前最佳方法在多种独立跟踪工作中的Poor performance。此外，我们还扩展了我们的算法，以学习一个连续随机过程，该过程将噪声和数据相连，并使用一家EBMs索引的扩展。在这样做的过程中，我们证明了生成模型的核心思想和EBMs之间的Compatibility。总之，我们的提出的训练算法可以让我们在多种复杂的多模态benchmark中训练能量模型，与 diffusion models 和其他当前最佳方法竞争，甚至超越它们。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning"><a href="#Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning" class="headerlink" title="Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning"></a>Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05798">http://arxiv.org/abs/2309.05798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yy-ko/cash">https://github.com/yy-ko/cash</a></li>
<li>paper_authors: Yunyong Ko, Hanghang Tong, Sang-Wook Kim</li>
<li>for: 这篇论文旨在预测未知的超页结 (hyperedge)，协助解决实际应用中的许多基本问题 (e.g., 集体推荐)。</li>
<li>methods: 本文提出了一个名为 CASH 的新型超页预测框架，利用具有上下文识别的节点聚合来精确地捕捉节点之间的复杂关系，以及在超页预测中使用自我超vised contrastive learning 来强化超页 Representation。</li>
<li>results: 实验结果显示，CASH 能够在六个真实世界的超页上预测超页的精度高于所有竞争方法，并且每个建议的策略都能够提高 CASH 模型的准确度。<details>
<summary>Abstract</summary>
Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmentation method to fully exploit the latent semantics behind the original hypergraph and consider both node-level and group-level contrasts (i.e., dual contrasts) for better node and hyperedge representations. Extensive experiments on six real-world hypergraphs reveal that CASH consistently outperforms all competing methods in terms of the accuracy in hyperedge prediction and each of the proposed strategies is effective in improving the model accuracy of CASH. For the detailed information of CASH, we provide the code and datasets at: https://github.com/yy-ko/cash.
</details>
<details>
<summary>摘要</summary>
《Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmentation method to fully exploit the latent semantics behind the original hypergraph and consider both node-level and group-level contrasts (i.e., dual contrasts) for better node and hyperedge representations. Extensive experiments on six real-world hypergraphs reveal that CASH consistently outperforms all competing methods in terms of the accuracy in hyperedge prediction and each of the proposed strategies is effective in improving the model accuracy of CASH. For the detailed information of CASH, we provide the code and datasets at: https://github.com/yy-ko/cash.》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models"><a href="#On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models" class="headerlink" title="On the Fine-Grained Hardness of Inverting Generative Models"></a>On the Fine-Grained Hardness of Inverting Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05795">http://arxiv.org/abs/2309.05795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feyza Duman Keles, Chinmay Hegde</li>
<li>for: 本研究主要目标是为Generative模型 inverse提供细致的视野。</li>
<li>methods: 本文使用了几种不同的方法，包括：	+ 几何学方法：使用了几何学方法来研究Generative模型 inverse的计算复杂性。	+ 带紧度推理方法：使用了带紧度推理方法来研究Generative模型 inverse的计算复杂性。</li>
<li>results: 本文的主要结果包括：	+ 提出了一些新的硬度下界，用于描述Generative模型 inverse的计算复杂性。	+ 证明了对于exact inverse，计算复杂性是Ω(2^n)的。	+ 证明了对于approximate inverse，计算复杂性是Ω(2^n)的，当p是正整数时。	+ 提出了一些新的难题，用于描述Generative模型 inverse的计算复杂性。<details>
<summary>Abstract</summary>
The objective of generative model inversion is to identify a size-$n$ latent vector that produces a generative model output that closely matches a given target. This operation is a core computational primitive in numerous modern applications involving computer vision and NLP. However, the problem is known to be computationally challenging and NP-hard in the worst case. This paper aims to provide a fine-grained view of the landscape of computational hardness for this problem. We establish several new hardness lower bounds for both exact and approximate model inversion. In exact inversion, the goal is to determine whether a target is contained within the range of a given generative model. Under the strong exponential time hypothesis (SETH), we demonstrate that the computational complexity of exact inversion is lower bounded by $\Omega(2^n)$ via a reduction from $k$-SAT; this is a strengthening of known results. For the more practically relevant problem of approximate inversion, the goal is to determine whether a point in the model range is close to a given target with respect to the $\ell_p$-norm. When $p$ is a positive odd integer, under SETH, we provide an $\Omega(2^n)$ complexity lower bound via a reduction from the closest vectors problem (CVP). Finally, when $p$ is even, under the exponential time hypothesis (ETH), we provide a lower bound of $2^{\Omega (n)}$ via a reduction from Half-Clique and Vertex-Cover.
</details>
<details>
<summary>摘要</summary>
目标是使用生成模型进行逆转换，以便将生成模型输出与给定的目标匹配。这是现代计算机视觉和自然语言处理中的一个重要计算基础。然而，这个问题已知为计算上具有NP困难的worst-case性。这篇论文的目标是为这个问题提供细腻的视野。我们建立了一些新的困难下界，以确定生成模型逆转换的计算复杂性。在精确的逆转换中，我们的目标是判断给定的目标是否在生成模型的范围内。在STRONG EXPONENTIAL TIME HYPOTHESIS（SETH）下，我们通过 $k$-SAT 的减reduction示出，该问题的计算复杂性为 $\Omega(2^n)$。在更实际上，我们考虑了近似的逆转换问题，即判断模型范围中的一个点是否与给定的目标准确匹配。当 $p$ 是正的奇数时，在 SETH 下，我们提供了 $\Omega(2^n)$ 的下界，via  closest vectors problem（CVP）的减reduction。而当 $p$ 是偶数时，在 EXPONENTIAL TIME HYPOTHESIS（ETH）下，我们提供了 $2^{\Omega(n)}$ 的下界，via Half-Clique 和 Vertex-Cover 的减reduction。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems"><a href="#Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems" class="headerlink" title="Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems"></a>Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05787">http://arxiv.org/abs/2309.05787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Gomaa, Michael Feld</li>
<li>for: 提升人工智能水平，允许自动化系统理解对象和环境更深层次。</li>
<li>methods:  combinatorial 输入和输出功能，以及人类在循环和学习过程中的协作。</li>
<li>results: 提出了一些假设和设计指南，并在相关工作中展示了一个使用情况。<details>
<summary>Abstract</summary>
Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advancing the field of artificial intelligence and enabling autonomous systems to learn like humans. We propose several hypotheses and design guidelines and highlight a use case from related work to achieve this goal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的机器学习进展 Machine learning, particularly deep learning, 已经使得自动系统可以在一种感知和理解对象和环境的层次上进行探测和融合感知数据。这些系统可以现在完成对象检测、感知数据融合和语言理解任务。然而，有一种增长的需求，即使自动系统能够更好地理解对象和环境的概念和符号性。因此，系统必须设计为支持多模态输入和输出，以便支持隐式和显式交互模型。在这篇位点论文中，我们 argued for considering both types of inputs，以及人类在Loop和增量学习技术，以提高人工智能领域的进步和让自动系统学习如人类一样。我们提出了一些假设和设计指南，并高亮了相关工作的一个应用例子，以实现这个目标。
</details></li>
</ul>
<hr>
<h2 id="Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments"><a href="#Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments" class="headerlink" title="Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments"></a>Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05784">http://arxiv.org/abs/2309.05784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadan Golestan, Omid Ardakanian, Pierre Boulanger</li>
<li>for: 本研究旨在提高协助生活空间中的垂直检测、indoor localization和活动识别的可靠性，通过优化感知器的配置和位置。</li>
<li>methods: 我们提出了一种新的、高效的方法，基于灰色Box Bayesian优化和模拟评估来找到高质量的感知器位置在任意indoor空间中。我们的关键技术是利用活动空间特有的领域知识，在 iterative 选择查询点中 capture bayesian 优化中的域特性。</li>
<li>results: 我们在两个 simulations 环境和一个实际数据集中，证明我们的提出的方法比 state-of-the-art 黑色盒 optimize 技术更高效，可以准确地识别人类活动，F1 分数高达 90.1%，并且需要更少（51.3% on average）的贵重函数查询。<details>
<summary>Abstract</summary>
Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function queries.
</details>
<details>
<summary>摘要</summary>
优化感知器的配置和位置对于可靠的落体检测、室内定位和活动识别在助生活空间是关键。我们提出了一种新的、样本效率高的方法，通过灰色 Box  bayesian优化和模拟基于评估来找到高质量感知器的配置。我们的关键技术在于捕捉室内活动的空间分布知识，并将其包含到 Bayesian 优化的迭代选择中。使用两个模拟的室内环境和一个真实世界数据集，我们显示我们的提议方法在比state-of-the-art黑色箱优化技术更高的准确性和活动识别的F1分数，同时也需要明显的下降（51.3%的平均下降）的昂贵函数查询。
</details></li>
</ul>
<hr>
<h2 id="Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning"><a href="#Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning" class="headerlink" title="Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning"></a>Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05777">http://arxiv.org/abs/2309.05777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasunori Yamada, Kaoru Shinkawa, Masatomo Kobayashi, Miyuki Nemoto, Miho Ota, Kiyotaka Nemoto, Tetsuaki Arai<br>for:This study aimed to investigate the use of acoustic features from voice data as objective markers for detecting deficits in everyday functioning in older adults, with the potential for early detection of neurodegenerative diseases such as Alzheimer’s disease.methods:The study used a smartwatch-based application to collect acoustic features during cognitive tasks and daily conversation, and machine learning models were used to detect deficits in everyday functioning.results:The study found that acoustic features from voice data could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than the 68.5% accuracy with standard neuropsychological tests. Common acoustic features were also identified for robustly discriminating deficits in everyday functioning across both types of voice data.<details>
<summary>Abstract</summary>
Detection of subtle deficits in everyday functioning due to cognitive impairment is important for early detection of neurodegenerative diseases, particularly Alzheimer's disease. However, current standards for assessment of everyday functioning are based on qualitative, subjective ratings. Speech has been shown to provide good objective markers for cognitive impairments, but the association with cognition-relevant everyday functioning remains uninvestigated. In this study, we demonstrate the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning. We collected voice data during the performance of cognitive tasks and daily conversation, as possible application scenarios, from 54 older adults, along with a measure of everyday functioning. Machine learning models using acoustic features could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than the 68.5% accuracy with standard neuropsychological tests. We also identified common acoustic features for robustly discriminating deficits in everyday functioning across both types of voice data (cognitive tasks and daily conversation). Our results suggest that common acoustic features extracted from different types of voice data can be used as markers for deficits in everyday functioning.
</details>
<details>
<summary>摘要</summary>
检测轻微功能障碍的重要性在早期检测脑神经疾病，特别是阿尔茨海默病，已有广泛的研究。然而，当前评估日常功能的标准是基于主观的评价。speech已经被证明可以提供好的对象标记器 для认知障碍，但与认知有关的日常功能之间的关系还没有被研究。本研究表明使用智能手表应用程序收集语音特征可以作为对日常功能障碍的对象标记器。我们收集了54名老年人的语音数据，包括认知任务和日常对话，以及一种测量日常功能的指标。机器学习模型使用语音特征可以在68.5%的准确率上检测出日常功能障碍，高于标准神经心理测试的准确率。我们还确定了对日常功能障碍的共同语音特征，可以在不同类型的语音数据中强制性地分类。我们的结果表明，共同的语音特征可以作为日常功能障碍的标记器。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression"><a href="#The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression" class="headerlink" title="The Effect of Intrinsic Dimension on Metric Learning under Compression"></a>The Effect of Intrinsic Dimension on Metric Learning under Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05751">http://arxiv.org/abs/2309.05751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efstratios Palias, Ata Kabán</li>
<li>for: 提高距离算法性能的度量学习</li>
<li>methods: 使用随机压缩数据进行全级度metric学习，并提供了关于误差的理论保证</li>
<li>results: 在高维设置下，经验证明了理论批处理的正确性，并且 автоматиче地紧缩至有利的几何结构存在In English:</li>
<li>for: Improving the performance of distance-based learning algorithms through metric learning</li>
<li>methods: Training a full-rank metric on a randomly compressed version of high-dimensional data</li>
<li>results: Providing theoretical guarantees on the error of distance-based metric learning with respect to the random compression, without assuming any explicit properties of the data other than i.i.d. and bounded support. Experimental results support the theoretical findings on high-dimensional data sets.<details>
<summary>Abstract</summary>
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation"><a href="#CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation" class="headerlink" title="CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation"></a>CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05704">http://arxiv.org/abs/2309.05704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FLC-QU-hep/CaloClouds-2">https://github.com/FLC-QU-hep/CaloClouds-2</a></li>
<li>paper_authors: Erik Buhmann, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown</li>
<li>for: 高精度探测器的快速能量储存模拟，以便未来的加速器实验室中的实验。</li>
<li>methods: 使用生成器机器学习模型（ML）加速和补充传统模拟链，但大多数前一些努力受到固定、定期探测器读取几何的限制。</li>
<li>results: CaloClouds II 模型实现了许多关键改进，包括 Kontinuous time score-based 模型，可以在相同精度下与 CaloClouds 实现 $6\times$ 速度提升，并且可以在单个 CPU 上实现 $5\times$ 速度提升。此外， diffusion 模型被论述为一种准确的探测器模型，可以在单步实现高精度探测器的样本，从而实现 $46\times$ ($37\times$) 速度提升。<details>
<summary>Abstract</summary>
Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).   In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a consistency model allowing for accurate sampling in a single step and resulting in a $46\times$ ($37\times$) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce CaloClouds II, which features a number of key improvements. This includes continuous time score-based modeling, which allows for a 25-step sampling with comparable fidelity to CaloClouds while yielding a 6x speed-up over Geant4 on a single CPU (5x over CaloClouds). We further distill the diffusion model into a consistency model, allowing for accurate sampling in a single step and resulting in a 46x (37x) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality"><a href="#Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality" class="headerlink" title="Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality"></a>Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05702">http://arxiv.org/abs/2309.05702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rak-Kyeong Seong</li>
<li>for: 这 paper 是用于研究 4d N&#x3D;1 维超Symmetric gauge theory 的toric phase的。</li>
<li>methods: 这 paper 使用了不监督学习技术，包括principal component analysis (PCA) 和 t-distributed stochastic neighbor embedding (t-SNE)，来研究 complex structure moduli 的变化对 toric phase 的影响。</li>
<li>results: 这 paper 获得了一个 2-dimensional phase diagram for brane tilings corresponding to the cone over the zeroth Hirzebruch surface F0，以及其相应的Seiberg duality phase boundaries。<details>
<summary>Abstract</summary>
We introduce unsupervised machine learning techniques in order to identify toric phases of 4d N=1 supersymmetric gauge theories corresponding to the same toric Calabi-Yau 3-fold. These 4d N=1 supersymmetric gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold and are realized in terms of a Type IIB brane configuration known as a brane tiling. It corresponds to the skeleton graph of the coamoeba projection of the mirror curve associated to the toric Calabi-Yau 3-fold. When we vary the complex structure moduli of the mirror Calabi-Yau 3-fold, the coamoeba and the corresponding brane tilings change their shape, giving rise to different toric phases related by Seiberg duality. We illustrate that by employing techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), we can project the space of coamoeba labelled by complex structure moduli down to a lower dimensional phase space with phase boundaries corresponding to Seiberg duality. In this work, we illustrate this technique by obtaining a 2-dimensional phase diagram for brane tilings corresponding to the cone over the zeroth Hirzebruch surface F0.
</details>
<details>
<summary>摘要</summary>
我们引入无监控机器学习技术来识别四维N=1瑞利对偶 gauge theory的托立阶段，这些 gauge theory 是 D3- branes 在托立 Calabi-Yau 3-fold 上的世界体理论，并且可以通过 Type IIB  branes 配置来实现。这些配置相应于托立 Calabi-Yau 3-fold 的镜射曲线的对偶图形。当我们变化托立 Calabi-Yau 3-fold 的复素结构参数时，这些对偶图形和相应的 branes 配置会改变形状，从而产生不同的托立阶段，这些阶段相关联系到 Seiberg 对偶。我们使用技术如主成分分析 (PCA) 和 t-分布随机邻接 embedding (t-SNE)，将托立 Calabi-Yau 3-fold 的复素结构参数下的空间投射到一个低维度的阶段空间，这个阶段空间中的边界与 Seiberg 对偶相关。在这个研究中，我们使用这种技术来得到一个二维的阶段图表，它对应于 F0 的托立阶段。
</details></li>
</ul>
<hr>
<h2 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05665">http://arxiv.org/abs/2309.05665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZiwenZhuang/parkour">https://github.com/ZiwenZhuang/parkour</a></li>
<li>paper_authors: Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao</li>
<li>for: 该论文旨在开发一种基于视觉的爬行策略，以便机器人可以在复杂环境中快速跨越各种障碍。</li>
<li>methods: 该论文使用了人工智能的补偿学习方法，通过直接拓展来生成多种爬行技能，包括爬过高障碍物、跃越大坑、跪穿低障碍物、缩进细缝等。</li>
<li>results: 该论文通过将这些爬行技能总结成一个视觉基于的爬行策略，并将其转移到一个四肢动物上，成功地让两个低成本机器人在真实世界环境中自动选择和执行合适的爬行技能。<details>
<summary>Abstract</summary>
Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.
</details>
<details>
<summary>摘要</summary>
园地攀登是一个大型挑战，需要机器人在复杂环境中快速穿越多种障碍。现有方法可以生成 Either 多样化但是盲目的行动技巧或视觉基于的特殊技巧，但是自主园地攀登需要机器人学习通用的技能，能够通过多种情况来见解和应对。在这项工作中，我们提出了一个系统，可以不使用参考动物数据，通过简单的奖励来学习多样化的视觉基于的园地攀登策略。我们开发了一种基于irect collocation的强化学习方法，用于生成园地攀登技能，包括爬上高障碍、跳过大差、蹲下低障碍、缩进窄障碍和跑步。我们将这些技能练习成一个单一的视觉基于的园地攀登策略，并将其传递到一只四足机器人使用其 egocentric depth camera。我们示出了我们的系统可以让两个不同的低成本机器人自主选择和执行适合的园地攀登技能，以快速穿越实际环境中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Hypothesis-Search-Inductive-Reasoning-with-Language-Models"><a href="#Hypothesis-Search-Inductive-Reasoning-with-Language-Models" class="headerlink" title="Hypothesis Search: Inductive Reasoning with Language Models"></a>Hypothesis Search: Inductive Reasoning with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05660">http://arxiv.org/abs/2309.05660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman</li>
<li>for: This paper aims to improve the inductive reasoning ability of large language models (LLMs) by generating explicit hypotheses at multiple levels of abstraction.</li>
<li>methods: The proposed method involves prompting the LLM to propose multiple abstract hypotheses about the problem in natural language, and then implementing the natural language hypotheses as concrete Python programs. The method also includes a middle step to filter the set of hypotheses that will be implemented into programs, either by asking the LLM to summarize into a smaller set of hypotheses or by asking human annotators to select a subset of the hypotheses.</li>
<li>results: The paper demonstrates the effectiveness of the proposed pipeline on the Abstraction and Reasoning Corpus (ARC) visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. The results show that the automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%.<details>
<summary>Abstract</summary>
Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.
</details>
<details>
<summary>摘要</summary>
人类可以通过推理来解决问题，如果给他们一些示例，他们就可以找出下面的原则，然后将其应用到新的情况中。在最近的研究中，人们评估了大型自然语言模型（LLM）在推理任务上的能力，并通过直接提示它们来实现“在上下文学习”。这种方法可以在一些简单的推理任务上工作良好，但在更复杂的任务，如抽象和理解集（ARC）上表现非常差。在这项工作中，我们提出了使得LLM在推理任务上的能力更强的方法：我们会让LLM提出多个层次抽象的假设，然后将这些假设转换成自然语言中的语言表达，最后将其转换成Python程序。这些程序可以直接在观察到的示例上验证，并将其扩展到新的输入。由于现有的LLM生成成的成本过高，我们考虑了一个中间步骤，即使LLM提出的假设集中的一个子集，或者请人工标注员选择一个子集。我们在ARC视觉推理benchmark、其变种1D-ARC和字符串变换集SyGuS上验证了我们的管道的效果。在随机选择ARC中的40个问题上，我们的自动管道使用LLM总结而获得27.5%的准确率，与直接提示基线（准确率为12.5%）相比，有显著的提高。在人工标注员选择LLM生成的候选者的情况下，性能更高，达到37.5%。（我们认为这是我们方法无需筛选的下限）。我们的剖析研究表明，LLM在推理任务上的抽象假设生成和具体程序表示都是有利的。
</details></li>
</ul>
<hr>
<h2 id="On-the-quality-of-randomized-approximations-of-Tukey’s-depth"><a href="#On-the-quality-of-randomized-approximations-of-Tukey’s-depth" class="headerlink" title="On the quality of randomized approximations of Tukey’s depth"></a>On the quality of randomized approximations of Tukey’s depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05657">http://arxiv.org/abs/2309.05657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Briend, Gábor Lugosi, Roberto Imbuzeiro Oliveira</li>
<li>for: 这个论文目的是解决高维度数据中Tukey深度的精确计算是一个困难的问题。</li>
<li>methods: 论文使用随机化方法来近似Tukey深度。</li>
<li>results: 论文证明在一些特定情况下，随机化方法可以准确地 aproximate Tukey深度，但是对于中间深度的点，任何好的approximation都需要对数复杂度。<details>
<summary>Abstract</summary>
Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
</details>
<details>
<summary>摘要</summary>
图基深度（或半空间深度）是多变量数据中广泛使用的中心度量。然而，对高维数据进行准确计算图基深度是一个困难的问题。为了解决这问题，随机化Tukey深度的算法已经被提出。在这篇论文中，我们研究了这些随机算法在图基深度的计算中是否返回良好的 aproximation。我们研究了从Log-凹形分布中采样数据的情况。我们证明，如果要求算法在维度上运行时间为多项式时间，那么随机算法会正确地approximates最大深度为1/2和深度很近于零。然而，对于任何中间深度的点，任何好的approximation都需要无限次复杂度。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands"><a href="#Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands" class="headerlink" title="Dynamic Handover: Throw and Catch with Bimanual Hands"></a>Dynamic Handover: Throw and Catch with Bimanual Hands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05655">http://arxiv.org/abs/2309.05655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang</li>
<li>for:  solves the problem of dynamic handover of objects in robotic systems</li>
<li>methods:  uses Multi-Agent Reinforcement Learning and Sim2Real transfer, with novel algorithm designs such as trajectory prediction models</li>
<li>results:  shows significant improvements over multiple baselines in real-world experiments with diverse objects.<details>
<summary>Abstract</summary>
Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at \url{https://binghao-huang.github.io/dynamic_handover/}.
</details>
<details>
<summary>摘要</summary>
人类常常投掷和捕捉物体，但这种各种动作却对机器人带来了很多挑战：机器人需要在高速下进行动作协作，并且与多种物体进行准确协作。在这篇论文中，我们设计了两个多指手 attachment 到机器人臂，以解决这个问题。我们使用多机器人学习强化学习在模拟环境中训练我们的系统，并进行了实际系统中的Sim2Real传输。为了 bridge 模拟和实际之间的差异，我们提供了多种新的算法设计，包括学习物体的轨迹预测模型。这种模型可以帮助机器人捕手在实时获得物体的运动轨迹，然后根据此进行反应。我们在实际系统中进行了多个物体的实验，并显示了多个基eline的改进。我们的项目页面可以在 \url{https://binghao-huang.github.io/dynamic_handover/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck"><a href="#Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck" class="headerlink" title="Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck"></a>Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05649">http://arxiv.org/abs/2309.05649</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. Michael Martini, Ilya Nemenman</li>
<li>for:  simultaneous compression of two random variables to preserve information between their compressed versions</li>
<li>methods:  Generalized Symmetric Information Bottleneck (GSIB) and exploration of different functional forms of the cost of simultaneous reduction</li>
<li>results:  qualitatively less data required for simultaneous compression compared to compressing variables one at a time, with bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions provided.<details>
<summary>Abstract</summary>
The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
</details>
<details>
<summary>摘要</summary>
symmetric information bottleneck (SIB) 是一种维度减少技术，它同时压缩两个随机变量，以保留它们压缩后的信息之间的关系。我们介绍了通用的 Symmetric Information Bottleneck (GSIB)，它探讨了不同的函数形式，以减少这种同时压缩的成本。然后，我们研究了这种同时压缩的数据集大小要求。我们通过计算涨落函数的上界和方差估计，发现在一般情况下，同时压缩需要更少的数据来达到相同的错误率，相比于一个一个压缩每个输入变量。我们认为这是一种更一般的原理，即同时压缩是独立压缩每个输入变量的更有效的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN"><a href="#A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN" class="headerlink" title="A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)"></a>A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05646">http://arxiv.org/abs/2309.05646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks">https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks</a></li>
<li>paper_authors: Vedanth Ramanathan, Krish Mahadevan, Sejal Dua</li>
<li>For: The paper is written to detect Distributed Denial of Service (DDoS) attacks in network traffic using deep learning techniques.* Methods: The paper employs a novel deep learning-based approach that utilizes Convolutional Neural Networks (CNN) and common deep learning algorithms to classify benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length, and then uses a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to perform binary classification.* Results: The paper achieves an accuracy of .9883 on 2000 unseen flows in network traffic, demonstrating the effectiveness of the proposed algorithm in detecting DDoS attacks. The results also show that the proposed model is scalable for any network environment.Here are the three points in Simplified Chinese text:* For: 这篇论文是用来探讨分布式拒绝服务（DDoS）攻击的网络流量检测方法。* Methods: 该论文使用了一种新的深度学习基于方法，利用卷积神经网络（CNN）和常见的深度学习算法来分类正常和恶意流量。提出的模型从网络流量中提取包流，并将其normal化到固定长度，然后使用自定义的架构 containing layers regulating node dropout, normalization, and a sigmoid activation function来实现二分类。* Results: 该论文在2000个未看过的流量中达到了.9883的准确率，证明了提出的方法在检测DDoS攻击的效果。结果还表明了该方法可扩展到任何网络环境。<details>
<summary>Abstract</summary>
Cybersecurity attacks are becoming increasingly sophisticated and pose a growing threat to individuals, and private and public sectors. Distributed Denial of Service attacks are one of the most harmful of these threats in today's internet, disrupting the availability of essential services. This project presents a novel deep learning-based approach for detecting DDoS attacks in network traffic using the industry-recognized DDoS evaluation dataset from the University of New Brunswick, which contains packet captures from real-time DDoS attacks, creating a broader and more applicable model for the real world. The algorithm employed in this study exploits the properties of Convolutional Neural Networks (CNN) and common deep learning algorithms to build a novel mitigation technique that classifies benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length which is fed into a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to out a binary classification. This allows for the model to process the flows effectively and look for the nodes that contribute to DDoS attacks while dropping the "noise" or the distractors. The results of this study demonstrate the effectiveness of the proposed algorithm in detecting DDOS attacks, achieving an accuracy of .9883 on 2000 unseen flows in network traffic, while being scalable for any network environment.
</details>
<details>
<summary>摘要</summary>
“黑客攻击不断地变得更加复杂和危险，对个人和公共领域的潜在威胁都在增加。分布式拒绝服务（DDoS）攻击是网络上最危险的攻击之一，可以破坏网络服务的可用性。这个项目提出了一个基于深度学习的新方法，用于网络流量中的DDoS攻击探测，使用了新不伯纳瑞大学的DDoS评估数据集，这个数据集包含了实时DDoS攻击的封包捕捉，创造了更加广泛和实用的模型。这个算法利用了卷积神经网络的性能和通用深度学习算法，建立了一个新的防护技术，通过分析网络流量中的封包，分别归类为有害和无害流量。这个提案的模型首先将数据进行处理，提取封包流和对其进行 нор化，然后将其输入到自定义的架构中，这个架构包含了节点排除、normalization和sigmoid活化函数等，以生成一个二分类。这样可以让模型有效地处理流量，寻找对DDoS攻击有贡献的节点，同时忽略“噪音”或“掩蔽”。研究结果显示，提案的算法具有优秀的检测DDoS攻击的精度，在2000个未见的网络流量中，获得了.9883的准确率，同时具有扩展性，适用于任何网络环境。”
</details></li>
</ul>
<hr>
<h2 id="Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets"><a href="#Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets" class="headerlink" title="Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets"></a>Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06538">http://arxiv.org/abs/2309.06538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Mitsuo Akita, Everton Josue da Silva</li>
<li>for: 预测股票市场价格</li>
<li>methods: 使用 iFeel 2.0 平台提取推特上关于 Petrobras 公司的19个情感特征，然后使用 XBoot 模型预测未来股票价格</li>
<li>results: 在250天内，使用模型预测的股票价格比Random Models的平均表现带来了R$88,82（净）的收益<details>
<summary>Abstract</summary>
Training machine learning models for predicting stock market share prices is an active area of research since the automatization of trading such papers was available in real time. While most of the work in this field of research is done by training Neural networks based on past prices of stock shares, in this work, we use iFeel 2.0 platform to extract 19 sentiment features from posts obtained from microblog platform Twitter that mention the company Petrobras. Then, we used those features to train XBoot models to predict future stock prices for the referred company. Later, we simulated the trading of Petrobras' shares based on the model's outputs and determined the gain of R$88,82 (net) in a 250-day period when compared to a 100 random models' average performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用iFeel 2.0平台提取Twitter上关于Petrobras公司的19个情感特征，然后使用这些特征训练XBoot模型预测Petrobras股票价格。后来，我们使用模型的输出进行了Petrobras股票的虚拟交易，并发现在250天内比Random Models的平均性能提高R$88,82（净）。Note: "iFeel 2.0" is a platform for sentiment analysis, and "XBoot" is a machine learning model. "Petrobras" is a company name. "R$" is the symbol for Brazilian real, the currency of Brazil.
</details></li>
</ul>
<hr>
<h2 id="Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling"><a href="#Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling" class="headerlink" title="Boundary Peeling: Outlier Detection Method Using One-Class Peeling"></a>Boundary Peeling: Outlier Detection Method Using One-Class Peeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05630">http://arxiv.org/abs/2309.05630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheikh Arafat, Na Sun, Maria L. Weese, Waldyn G. Martinez</li>
<li>for: 本研究旨在提出一种基于一类支持向量机的无监督异常检测算法，以提高数据分析中异常检测的效率和稳定性。</li>
<li>methods: 该算法使用一类支持向量机生成的轮廓来实现无监督异常检测，并可以通过调整模型的参数来适应不同的数据分布。</li>
<li>results: 对于 synthetic 数据集，该算法在无异常情况下表现更好，而且在异常情况下与标准方法相比，其性能仍然保持在相当高水平。<details>
<summary>Abstract</summary>
Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_text: "Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets."<</SYS>>Here's the translation in Simplified Chinese:一种无监督的异常点检测算法，是数据分析中的一个关键阶段，而且是一个动态的研究领域。一个好的异常点检测算法应该具有计算效率、对参数调整鲁棒性和能够在不同的数据分布下表现一致。我们介绍了一种名为One-Class Boundary Peeling的无监督异常点检测算法。这种算法使用一类支持向量机生成的迭代剥离、灵活的边界，并使用这些边界的平均签名距离来检测异常点。One-Class Boundary Peeling具有良好的参数设置和可以作为一个ensemble方法进行拓展。在synthetic数据 simulations中，One-Class Boundary Peeling在没有异常点时的性能明显超过了所有现有的方法，而且在异常点存在时也可以保持与 benchmark方法相当或更高的性能。在常用的benchmark数据集上，One-Class Boundary Peeling与其他方法相比，在正确分类、AUC和处理时间方面表现竞争性。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Side-Channels-in-Machine-Learning-Systems"><a href="#Privacy-Side-Channels-in-Machine-Learning-Systems" class="headerlink" title="Privacy Side Channels in Machine Learning Systems"></a>Privacy Side Channels in Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05610">http://arxiv.org/abs/2309.05610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</li>
<li>for: 本研究旨在探讨机器学习（ML）模型的隐私保护方法，并发现了系统级别的隐私泄露渠道。</li>
<li>methods: 本研究使用了四种类型的隐私泄露渠道，包括训练数据筛选、输入预处理、输出后处理和查询筛选，这些渠道可以导致模型中的隐私泄露或者新的威胁，如EXTRACTING用户的测试查询。</li>
<li>results: 本研究发现，在应用拥有隐私保证的训练方法时，过滤训练数据可以创造一个副annel，使得任何证明性隐私保证都无效。此外，系统屏蔽语言模型重新生成训练数据的功能可以被恶用，以重建私钥，即使模型没有记忆这些私钥。总之，本研究表明了机器学习隐私保护的综合、端到端分析是必要的。<details>
<summary>Abstract</summary>
Most current approaches for protecting privacy in machine learning (ML) assume that models exist in a vacuum, when in reality, ML models are part of larger systems that include components for training data filtering, output monitoring, and more. In this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. We propose four categories of side channels that span the entire ML lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. For example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. Moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. Taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.
</details>
<details>
<summary>摘要</summary>
现有的隐私保护方法在机器学习（ML）中假设模型存在独立的环境中，而实际上ML模型是更大的系统的一部分，包括训练数据筛选、输出监测和更多的组件。在这项工作中，我们介绍隐私侧频：利用这些系统级别的组件来提取私人信息的攻击。我们提出了四种侧频类型，覆盖了整个ML生命周期（训练数据筛选、输入预处理、输出后处理和查询筛选），并允许扩展证据推理攻击或者新的威胁，如提取用户的测试查询。例如，我们表明了对归一化训练数据前进行减重可以创建一个侧频，完全覆盖任何可证明隐私保证的保证。此外，我们还表明了阻止语言模型重新生成训练数据可以让私钥被私钥提取，即使模型没有记忆这些私钥。总之，我们的结果表明了机器学习的隐私分析应该是结束到终端的，综合考虑系统的所有组件。
</details></li>
</ul>
<hr>
<h2 id="Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models"><a href="#Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models" class="headerlink" title="Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models"></a>Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05605">http://arxiv.org/abs/2309.05605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster</li>
<li>for: 提高大型自然语言模型（LLM）在多步逻辑问题回答中的表现。</li>
<li>methods: 通过targeted memory injection在LLM注意头中注入pertinent prompt-specific信息来pinpoint和修正LLM的多步逻辑问题失败。</li>
<li>results: 通过实验证明，可以通过 injecting pertinent prompt-specific information into a key attention layer during inference, enhance the quality of multi-hop prompt completions, and increase the probability of the desired next token by up to 424%.<details>
<summary>Abstract</summary>
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
</details>
<details>
<summary>摘要</summary>
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.Here's the text in Traditional Chinese:Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias"><a href="#Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias" class="headerlink" title="Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias"></a>Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05589">http://arxiv.org/abs/2309.05589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinath Sai Tripuraneni, Sadia Kamal, Arunkumar Bagavathi</li>
<li>for: 本研究旨在 combat misinformation 和 echo chamber effects 的措施，通过 computationally characterizing political bias。</li>
<li>methods: 我们提出了一种 heuristic 方法，可以分类社交媒体内容为五种不同政治倾向。我们还使用了现有的时间序列预测模型，以 analysing 和比较不同政治意识形态的社交媒体资料。</li>
<li>results: 我们的实验和分析表明，exist 的时间序列预测模型可以对社交媒体上的政治倾向进行预测，并且可以帮助发现不同政治意识形态的社交媒体资料。<details>
<summary>Abstract</summary>
Understanding and mitigating political bias in online social media platforms are crucial tasks to combat misinformation and echo chamber effects. However, characterizing political bias temporally using computational methods presents challenges due to the high frequency of noise in social media datasets. While existing research has explored various approaches to political bias characterization, the ability to forecast political bias and anticipate how political conversations might evolve in the near future has not been extensively studied. In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different political ideologies, specifically Twitter and Gab. Through our experiments and analyses, we seek to shed light on the challenges and opportunities in forecasting political bias in social media platforms. Ultimately, our work aims to pave the way for developing more effective strategies to mitigate the negative impact of political bias in the digital realm.
</details>
<details>
<summary>摘要</summary>
理解和 Mitigating 政治偏见在在线社交媒体平台上是关键的措施，以遏制谣言和复顾室效应。然而，使用计算方法来 temps 政治偏见的问题存在高频噪声在社交媒体数据集中， existing research 已经探讨了多种方法来 Characterizing 政治偏见，但是预测政治偏见和未来政治对话的发展方向尚未得到了广泛的研究。在这篇论文中，我们提出了一种启发式的方法，将社交媒体帖子分为五个不同政治倾向类别。由于没有先前的工作，我们进行了深入的基线模型分析，以确定最适合预测政治倾向时间序列的模型。我们的方法包括在 Twitter 和 Gab 两个社交媒体数据集上使用现有的时间序列预测模型。通过我们的实验和分析，我们希望探讨政治偏见预测的挑战和机遇，以便更好地开发适应社交媒体平台的政治偏见预测模型。最终，我们的工作旨在为数字化时代带来更有效的政治偏见预测和控制策略。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning"><a href="#Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning" class="headerlink" title="Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning"></a>Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05582">http://arxiv.org/abs/2309.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Sebastian Blaes, Cristina Pineri, Georg Martius</li>
<li>for: 管理模型基 reinforcement learning中的风险，通过轨迹采样和概率安全约束，并考虑 epistemic 不确定性和 aleatoric 不确定性的平衡。</li>
<li>methods: 使用概率安全约束和轨迹采样来管理模型基 reinforcement learning 中的风险，并在不确定环境中进行数据驱动MPCapproaches。</li>
<li>results: 实验显示，将不确定性分离是在不确定和安全控制环境中使用数据驱动MPCapproaches时非常重要的。<details>
<summary>Abstract</summary>
We introduce a simple but effective method for managing risk in model-based reinforcement learning with trajectory sampling that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks.Various experiments indicate that the separation of uncertainties is essential to performing well with data-driven MPC approaches in uncertain and safety-critical control environments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet 有效的方法，用于在基于模型的学习控制中管理风险，这种方法包括可能性风险约束和对风险不确定性的平衡。各种实验表明，在数据驱动的MPC方法中，分离不确定性是关键以达到良好的性能。Here's a breakdown of the translation:* 我们 (wǒmen) - we* 提出 (tīchū) - propose* 一种 (yīzhǒng) - a kind of* 简单 (jiǎndān) - simple* 有效 (yǒuxìng) - effective* 方法 (fāngché) - method* 用于 (yòngyù) - for* 管理 (guǎnlǐ) - managing* 风险 (fēngxǐ) - risk* 包括 (bāoxīn) - including* 可能性 (kěnéixìng) - possibility* 风险约束 (fēngxǐjièshì) - probabilistic safety constraints* 对 (duì) - towards* 风险不确定性 (fēngxǐbùjiànshì) - epistemic uncertainty* 平衡 (píngyì) - balance* 各种 (gèzhōng) - various* 实验 (shíyàn) - experiments* 表明 (biǎozhèng) - indicate* 在 (zài) - in* 数据驱动 (shùdào) - data-driven* MPC (MPC) - model predictive control* 方法 (fāngché) - method* 中 (zhōng) - in* uncertainty (bùjiànshì) - uncertainty* 环境 (huánjīng) - environmentI hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations"><a href="#Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations" class="headerlink" title="Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations"></a>Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05575">http://arxiv.org/abs/2309.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Schrader, Joachim Weickert, Michael Krause</li>
<li>for: 这 paper 主要针对的是数值近似二维非对称傅立叶过程中的琐纤细分数问题，以及这些过程在图像分析、物理和工程中的应用。</li>
<li>methods: 作者使用了 splitting 技术来分解二维非对称傅立叶为四个一维傅立叶，从而得到了一个包含一个自由参数的练习类。这个练习类包含了 Weickert et al. (2013) 中的全练习家族，并证明了这两个参数之间存在冗余性。作者还提供了一个 spectral norm 的下界，以保证数值方法的稳定性。</li>
<li>results: 作者通过使用 neural network 库和 GPU 实现了一个高效的并行实现方案，并实现了一个可靠的径向拆分方法。这个方法可以在 Euclidean  нор下保证数值方法的稳定性。<details>
<summary>Abstract</summary>
Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple and highly efficient parallel implementations on GPUs.
</details>
<details>
<summary>摘要</summary>
“散射过程 WITH 散射矩阵在图像分析、物理和工程中具有重要意义。然而，它们的数值近似带来强烈的消耗残差和旋转不变性的偏差。在这个工作中，我们研究了一个大家族的finite difference积分方法，包括一个自由参数。我们通过将2D散射分解为4个1D散射来 derivation 这个stencil家族。结果表明，这个stencil家族包括Weickert et al. (2013)的全stencil家族，并且显示了两个参数之间的重复性。此外，我们提出了一个对stencil矩阵的spectral norm的下界，这个下界给出了稳定性的时间步长限制。我们的方向分解也允许非常自然地将批处理转化为ResNet块。通过使用 neural network 库，我们可以在GPU上实现高效并简单的并行实现。”Note that Simplified Chinese is used here, which is a more common writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="ITI-GEN-Inclusive-Text-to-Image-Generation"><a href="#ITI-GEN-Inclusive-Text-to-Image-Generation" class="headerlink" title="ITI-GEN: Inclusive Text-to-Image Generation"></a>ITI-GEN: Inclusive Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05569">http://arxiv.org/abs/2309.05569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, Fernando De la Torre</li>
<li>for: 这个研究旨在开发一个包容性的文本到图像生成模型，以实现从文本描述中生成具有均匀分布的图像，并且能够减少训练数据中的偏见。</li>
<li>methods: 这个研究使用了一个新的方法，名为ITI-GEN，它使用可用的参考图像来学习一组启发式描述，以生成具有均匀分布的图像。ITI-GEN不需要模型精微调整，因此可以轻松地将现有的文本到图像模型进行扩展。</li>
<li>results: 实验结果显示，ITI-GEN比前一代模型更好地实现从文本描述中生成具有均匀分布的图像，并且可以减少训练数据中的偏见。<details>
<summary>Abstract</summary>
Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that "a picture is worth a thousand words". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt. Project page: https://czhang0528.github.io/iti-gen.
</details>
<details>
<summary>摘要</summary>
文本到图生成模型经常受训练数据的偏见影响，导致特定群体的不平等表现。这项研究探讨了包容型文本到图生成模型，该模型根据人写的提示生成图像，并确保生成图像在想要的特征上具有均匀分布。然而，直接在提示中表达欲表达的特性常导致语言ambiguity或模型误表示，因此这项研究提出了一种截然不同的方法。我们发现，对于一些特性，图像可以更有表达力地表达概念，比如皮肤色调。基于这些发现，我们提出了一种新的方法，名为ITI-GEN，该方法利用可以获得的参考图像为包容型文本到图生成提供了新的思路。我们学习一组提示嵌入，以生成能够有效表示所有欲表达特性类别的图像。此外，ITI-GEN不需要模型精度调整，因此可以efficient地增强现有的文本到图生成模型。我们的实验表明，ITI-GEN在生成包容图像方面具有显著优势。项目页面：https://czhang0528.github.io/iti-gen.
</details></li>
</ul>
<hr>
<h2 id="Distance-Aware-eXplanation-Based-Learning"><a href="#Distance-Aware-eXplanation-Based-Learning" class="headerlink" title="Distance-Aware eXplanation Based Learning"></a>Distance-Aware eXplanation Based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05548">http://arxiv.org/abs/2309.05548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msgun/xbl-d">https://github.com/msgun/xbl-d</a></li>
<li>paper_authors: Misgina Tsighe Hagos, Niamh Belton, Kathleen M. Curran, Brian Mac Namee</li>
<li>for: 这个论文的目的是提出一种基于对话式学习的深度学习模型训练方法，通过与模型的解释交互来提高模型的性能。</li>
<li>methods: 该方法使用了距离感知的解释损失函数，将损失函数与分类损失函数相结合，以训练学习者关注重要的训练集区域。</li>
<li>results: 该方法在三个图像分类任务上进行了评估，并提出了一种新的可解释性度量来评估视觉特征归属模型的性能。<details>
<summary>Abstract</summary>
eXplanation Based Learning (XBL) is an interactive learning approach that provides a transparent method of training deep learning models by interacting with their explanations. XBL augments loss functions to penalize a model based on deviation of its explanations from user annotation of image features. The literature on XBL mostly depends on the intersection of visual model explanations and image feature annotations. We present a method to add a distance-aware explanation loss to categorical losses that trains a learner to focus on important regions of a training dataset. Distance is an appropriate approach for calculating explanation loss since visual model explanations such as Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictly bounded as annotations and their intersections may not provide complete information on the deviation of a model's focus from relevant image regions. In addition to assessing our model using existing metrics, we propose an interpretability metric for evaluating visual feature-attribution based model explanations that is more informative of the model's performance than existing metrics. We demonstrate performance of our proposed method on three image classification tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "eXplanation Based Learning (XBL) is an interactive learning approach that provides a transparent method of training deep learning models by interacting with their explanations. XBL augments loss functions to penalize a model based on deviation of its explanations from user annotation of image features. The literature on XBL mostly depends on the intersection of visual model explanations and image feature annotations. We present a method to add a distance-aware explanation loss to categorical losses that trains a learner to focus on important regions of a training dataset. Distance is an appropriate approach for calculating explanation loss since visual model explanations such as Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictly bounded as annotations and their intersections may not provide complete information on the deviation of a model's focus from relevant image regions. In addition to assessing our model using existing metrics, we propose an interpretability metric for evaluating visual feature-attribution based model explanations that is more informative of the model's performance than existing metrics. We demonstrate performance of our proposed method on three image classification tasks."Translation:<<SYS>> explanation 基本学习（XBL）是一种互动式学习方法，通过与其解释互动来提供透明的深度学习模型训练方法。 XBL 增加损失函数，以惩戒模型的解释与用户图像特征注解的差异。 文献中大多数基于视觉模型解释和图像特征注解的交集。 我们提出了将距离意识导的解释损失添加到分类损失中，以训练学习者专注于训练集中重要的区域。 距离是一个适当的计算解释损失的方法，因为视觉模型解释，如Gradient-weighted Class Activation Mapping（Grad-CAMs），不是精确的注解，它们的交集可能不会提供完整的图像区域的差异信息。 除了使用现有的指标评估我们的模型，我们还提出了一个更加有用的可读性指标，用于评估基于视觉特征归属的模型解释。 我们在三个图像分类任务上展示了我们的提议方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis"><a href="#Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis" class="headerlink" title="Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis"></a>Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05525">http://arxiv.org/abs/2309.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendiqian/GNN4FL">https://github.com/chendiqian/GNN4FL</a></li>
<li>paper_authors: Wenxuan Ye, Chendi Qian, Xueli An, Xueqiang Yan, Georg Carle</li>
<li>for: 本研究旨在提出一种可信的分布式学习架构，以满足6G中Native AI支持的需求。</li>
<li>methods: 该架构使用分布式策略技术和图神经网络，包括三个关键特征：首先，使用同质加密来安全地聚合本地模型，保护个体模型的隐私。其次，利用分布式结构和图神经网络来识别异常本地模型，提高系统安全性。最后，使用分布式技术来归一化系统，选择一个候选人来执行中心服务器的功能。</li>
<li>results: 通过实验 validate the feasibility of the proposed architecture, the results show that the novel architecture can improve the detection of anomalous models and the accuracy of the global model compared to relevant baselines.<details>
<summary>Abstract</summary>
Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions. Additionally, DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger. The feasibility of the novel architecture is validated through simulations, demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines.
</details>
<details>
<summary>摘要</summary>
sixth generation （6G）中的Native AI支持集成到网络架构是一个重要的目标。 Federated Learning（FL）emerges as a potential paradigm，实现分布式AI模型训练 across a diverse range of devices under the coordination of a central server。然而，several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls。 This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology（DLT）and Graph Neural Network（GNN），including three key features。 First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models，preserving the privacy of individual models。Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer，GNN is leveraged to identify abnormal local models，enhancing system security。Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions。Additionally，DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger。The feasibility of the novel architecture is validated through simulations，demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Re-formalization-of-Individual-Fairness"><a href="#Re-formalization-of-Individual-Fairness" class="headerlink" title="Re-formalization of Individual Fairness"></a>Re-formalization of Individual Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05521">http://arxiv.org/abs/2309.05521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toshihiro Kamishima</li>
<li>for: 本研究旨在形式化个人公平的伦理原则，即“对相似的案例进行相似的对待”，这一原则在机器学习上被讨论。</li>
<li>methods: 本研究使用了Dwork等人提出的个人公平形式化，即将不公平空间中的相似对象映射到公平空间中相似的位置。我们提议重新形式化个人公平，通过个人独特性 Conditional statistical independence来实现。</li>
<li>results: 本研究的重新形式化个人公平可以与Dwork等人的形式化相兼容，同时允许与等化概率或充分性的公平观念结合。此外，本研究的形式化可以在预处理、进程处理或后处理阶段应用。<details>
<summary>Abstract</summary>
The notion of individual fairness is a formalization of an ethical principle, "Treating like cases alike," which has been argued such as by Aristotle. In a fairness-aware machine learning context, Dwork et al. firstly formalized the notion. In their formalization, a similar pair of data in an unfair space should be mapped to similar positions in a fair space. We propose to re-formalize individual fairness by the statistical independence conditioned by individuals. This re-formalization has the following merits. First, our formalization is compatible with that of Dwork et al. Second, our formalization enables to combine individual fairness with the fairness notion, equalized odds or sufficiency, as well as statistical parity. Third, though their formalization implicitly assumes a pre-process approach for making fair prediction, our formalization is applicable to an in-process or post-process approach.
</details>
<details>
<summary>摘要</summary>
“个人公平”是一个形式化的道德原则，“对于相似的案例进行相似的对待”，这个原则在 Aristotle 等人的讨论中已经被提出。在一个公平意识的机器学习上下，Dwork 等人首先将这个原则形式化。在他们的形式化中，一对相似的数据在不公平的空间中应该被映射到公平的空间中相似的位置。我们提议重新形式化个人公平，通过个人独特的统计独立性来条件。这个重新形式化具有以下优点：一、我们的形式化与 Dwork 等人的形式化相容。二、我们的形式化可以与平等机会或充分性的公平原则结合。三、他们的形式化假设了预先进行公平预测的前置方法，而我们的形式化则适用于进程或后置方法。
</details></li>
</ul>
<hr>
<h2 id="NExT-GPT-Any-to-Any-Multimodal-LLM"><a href="#NExT-GPT-Any-to-Any-Multimodal-LLM" class="headerlink" title="NExT-GPT: Any-to-Any Multimodal LLM"></a>NExT-GPT: Any-to-Any Multimodal LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05519">http://arxiv.org/abs/2309.05519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/NExT-GPT">https://github.com/kyegomez/NExT-GPT</a></li>
<li>paper_authors: Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua</li>
<li>for: 这个论文的目的是开发一个通用的任意模式任意输入输出的多Modal大语言模型（NExT-GPT），以便模拟人类在不同modalities中的感知和交流方式。</li>
<li>methods: 这个论文使用了一种结合多modal adapter和不同的扩散解码器的端到端总体方法，使得NExT-GPT能够处理输入和生成输出在任意组合的文本、图像、视频和音频modalities之间。它还使用了已经训练过的高性能encoder和decoder，并且只需要一小部分的参数（1%）来调整投影层，以便低成本训练和扩展到更多的modalities。</li>
<li>results: 这个论文通过引入模式转换指令调整（MosIT）和手动精心编辑的高质量多modal数据集，使得NExT-GPT具备了跨modalities的 semantic理解和内容生成能力。总的来说，这个研究表明了在建立人类水平的AI研究中，可以开发一个模型 universal modalities的AI代理，为更多的人类样本提供更多的可能性。<details>
<summary>Abstract</summary>
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/
</details>
<details>
<summary>摘要</summary>
Recently, Multimodal Large Language Models (MM-LLMs) have made significant progress, but they are limited to only understanding input-side multimodality and cannot produce content in multiple modalities. As humans perceive and communicate with the world through various modalities, developing any-to-any MM-LLMs that can accept and deliver content in any modality is essential for human-level AI. To address this gap, we propose an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging existing well-trained and highly-performing encoders and decoders, NExT-GPT is trained with only a small amount of parameters (1% of certain projection layers), which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Our research demonstrates the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: <https://next-gpt.github.io/>
</details></li>
</ul>
<hr>
<h2 id="Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss"><a href="#Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss" class="headerlink" title="Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss"></a>Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05517">http://arxiv.org/abs/2309.05517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Schmidt, Stephan Günnemann</li>
<li>for: 这个论文旨在应用活动学习（Active Learning）技术来对机器学习模型进行训练，以减少需要的标注数据量。</li>
<li>methods: 本论文提出了一种基于时间特性的image流进行Active Learning的方法，即temporal predicted loss（TPL）方法，并在GTA V街道和A2D2街道 dataset上进行了实验评估。</li>
<li>results: 实验结果显示，TPL方法可以对应用于感知应用中的pool-based Active Learning方法，并且在不同的模型上实现了更高的多标示性和更快的训练速度。相比之下，pool-based方法需要2.5个标注点（pp）更多的数据，而且训练速度较慢。<details>
<summary>Abstract</summary>
Active learning (AL) reduces the amount of labeled data needed to train a machine learning model by intelligently choosing which instances to label. Classic pool-based AL requires all data to be present in a datacenter, which can be challenging with the increasing amounts of data needed in deep learning. However, AL on mobile devices and robots, like autonomous cars, can filter the data from perception sensor streams before reaching the datacenter. We exploited the temporal properties for such image streams in our work and proposed the novel temporal predicted loss (TPL) method. To evaluate the stream-based setting properly, we introduced the GTA V streets and the A2D2 streets dataset and made both publicly available. Our experiments showed that our approach significantly improves the diversity of the selection while being an uncertainty-based method. As pool-based approaches are more common in perception applications, we derived a concept for comparing pool-based and stream-based AL, where TPL out-performed state-of-the-art pool- or stream-based approaches for different models. TPL demonstrated a gain of 2.5 precept points (pp) less required data while being significantly faster than pool-based methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs"><a href="#Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs" class="headerlink" title="Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"></a>Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05516">http://arxiv.org/abs/2309.05516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor">https://github.com/intel/neural-compressor</a></li>
<li>paper_authors: Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv</li>
<li>for: 提高大型语言模型（LLMs）的部署效率，解决它们的存储和内存需求问题。</li>
<li>methods: 使用Weight-only quantization，特别是3和4位的Weight-only quantization，并对准确性进行了优化。</li>
<li>results: 提出了一种高效且简洁的方法SignRound，通过使用签名式降降优化，在400步内达到了出色的结果，并与现有的RTN基eline和最新方法竞争。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead. The source code will be publicly available at https://github.com/intel/neural-compressor soon.
</details>
<details>
<summary>摘要</summary>
To optimize the weight rounding task, we propose a concise and highly effective approach called SignRound. Our method utilizes lightweight block-wise tuning with signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead. The source code will be publicly available at <https://github.com/intel/neural-compressor> soon.
</details></li>
</ul>
<hr>
<h2 id="Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning"><a href="#Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning" class="headerlink" title="Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning"></a>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05505">http://arxiv.org/abs/2309.05505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenzebang/centaur-privacy-federated-representation-learning">https://github.com/shenzebang/centaur-privacy-federated-representation-learning</a></li>
<li>paper_authors: Zebang Shen, Jiayuan Ye, Anmin Kang, Hamed Hassani, Reza Shokri</li>
<li>for: 本研究的目的是提高联合学习中的数据隐私保护，特别是在 Federated Learning 中，通过设计一种具有均衡保护和本地个性化的 Representation Federated Learning 目标函数。</li>
<li>methods: 本研究使用了 State-of-the-art 的权限保证算法，以及一种新的权限保证 Algorithm \DPFEDREP，以保证数据隐私。另外，本研究还使用了 Linear Representation Setting 来降低数据隐私风险。</li>
<li>results: 本研究的实验结果表明，使用 \DPFEDREP 算法可以在小于当前最佳比率的情况下，提高联合学习中的数据隐私保护和性能。此外，在 CIFAR10、CIFAR100 和 EMNIST 等图像分类任务上，本研究的方法也得到了显著的性能提升。<details>
<summary>Abstract</summary>
Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget. With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension. We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link: https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>共享参数的重复在联合学习中导致了重要的隐私泄露问题，从而让其主要目的——数据隐私——失效。使用当今最先进的权限减少算法可以减少这种隐私泄露的风险，但这并不免费。随机机制可以防止模型学习到有用的表示函数，特别是当地方模型在分类函数上存在差异（由于数据不同）时。在这篇论文中，我们考虑了一种带有隐私保证的联合学习目标，即让不同方面共同修改共识部分的模型，而不Release 个人化自由。我们证明在线性表示设置下，尽管目标函数不是凸形，但我们提出的新算法\DPFEDREP 在 linear rate 下 convergence to a ball centered around the global optimal solution，并且球体半径与隐私预算reciprocal proportional。通过这种新的实用分析，我们提高了最佳性和隐私之间的质量-价格比例，提高了$d$ 的输入维度。我们通过对 CIFAR10、CIFAR100 和 EMNIST 图像分类任务进行实验，发现我们的方法在同一小隐私预算下显著提高了前一个工作的性能。代码可以在以下链接找到：https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning。
</details></li>
</ul>
<hr>
<h2 id="Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images"><a href="#Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images" class="headerlink" title="Learning Semantic Segmentation with Query Points Supervision on Aerial Images"></a>Learning Semantic Segmentation with Query Points Supervision on Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05490">http://arxiv.org/abs/2309.05490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem</li>
<li>for: 这个研究是为了提高卫星图像Semantic segmentation的效率和准确性，并且仅使用问题点标签进行训练。</li>
<li>methods: 这个研究使用了弱型指导学习算法，将问题点标签扩展到类似含义的superpixels中，然后训练Semantic segmentation模型。</li>
<li>results: 这个研究在一个遥测图像集合上展示了具有竞争力的性能，并且降低了手动标签的时间和成本。<details>
<summary>Abstract</summary>
Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models, supervised with images partially labeled with the superpixels pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort.
</details>
<details>
<summary>摘要</summary>
我们的方法包括以下步骤：首先，我们生成 superpixels，然后将查询点注解扩展到这些 superpixels 中，这些 superpixels 是类似的含义 semantics 的分割。然后，我们训练基于这些 pseudo-labels 的 semantic segmentation 模型。我们在一个飞行图像 dataset 上 benchmark 我们的弱型supervised 训练方法，并使用不同的 semantic segmentation 架构，发现我们可以与完全supervised 训练方法相比，提高效率，同时减少人工注解的努力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes"><a href="#Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes" class="headerlink" title="Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes"></a>Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05477">http://arxiv.org/abs/2309.05477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timsey/npal">https://github.com/timsey/npal</a></li>
<li>paper_authors: Tim Bakker, Herke van Hoof, Max Welling</li>
<li>for: 提高机器学习模型的数据效率</li>
<li>methods: 使用Attentive Conditional Neural Process模型学习活动学习策略，利用活动学习问题的对称性和独立性属性</li>
<li>results: 比较多种基eline的性能，实验显示我们的Neural Process模型在这些设置下表现出色，并且对于不同的分类器表现稳定。<details>
<summary>Abstract</summary>
Pool-based active learning (AL) is a promising technology for increasing data-efficiency of machine learning models. However, surveys show that performance of recent AL methods is very sensitive to the choice of dataset and training setting, making them unsuitable for general application. In order to tackle this problem, the field Learning Active Learning (LAL) suggests to learn the active learning strategy itself, allowing it to adapt to the given setting. In this work, we propose a novel LAL method for classification that exploits symmetry and independence properties of the active learning problem with an Attentive Conditional Neural Process model. Our approach is based on learning from a myopic oracle, which gives our model the ability to adapt to non-standard objectives, such as those that do not equally weight the error on all data points. We experimentally verify that our Neural Process model outperforms a variety of baselines in these settings. Finally, our experiments show that our model exhibits a tendency towards improved stability to changing datasets. However, performance is sensitive to choice of classifier and more work is necessary to reduce the performance the gap with the myopic oracle and to improve scalability. We present our work as a proof-of-concept for LAL on nonstandard objectives and hope our analysis and modelling considerations inspire future LAL work.
</details>
<details>
<summary>摘要</summary>
池化活动学习（AL）是一种有前途的技术，可以提高机器学习模型的数据效率。然而，调查显示，现代AL方法在不同的数据集和训练环境中表现的性能很敏感，使其不适用于通用应用。为解决这个问题，学习活动学习（LAL）领域建议学习活动学习策略自己，让它适应给定的环境。在这个工作中，我们提出了一种基于条件神经过程模型的LAL方法，利用活动学习问题的对称和独立性。我们的方法基于学习一种短视镜，让我们的模型适应不标准目标函数，例如不对所有数据点Error具有相同的权重。我们实验证明，我们的神经过程模型在这些设置下表现出色，超过了多种基elines。最后，我们的实验表明，我们的模型具有随变数据集的稳定性。然而，性能仍然受到选择类ifiers的影响，需要更多的工作来减少与偏斜镜的性能差距，以及提高扩展性。我们的工作作为LAL在非标准目标函数的证明，并希望我们的分析和模型考虑能够激发未来LAL的工作。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-the-dimension-of-a-Fano-variety"><a href="#Machine-learning-the-dimension-of-a-Fano-variety" class="headerlink" title="Machine learning the dimension of a Fano variety"></a>Machine learning the dimension of a Fano variety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05473">http://arxiv.org/abs/2309.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/mldim">https://bitbucket.org/fanosearch/mldim</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这篇论文是关于斐论变体的研究，旨在探讨斐论变体的基本结构和特性。</li>
<li>methods: 作者使用了机器学习技术来分析斐论变体的量子期，并通过对量子期的分析来确定斐论变体的维度。</li>
<li>results: 作者通过实验发现，使用简单的批处理神经网络可以准确地确定斐论变体的维度，并且可以在不知道斐论变体的情况下确定其维度。这些结果表明，机器学习可以从复杂的数学数据中提取结构，并且可以在理论上无法解释的情况下提供正面的证据。<details>
<summary>Abstract</summary>
Fano varieties are basic building blocks in geometry - they are `atomic pieces' of mathematical shapes. Recent progress in the classification of Fano varieties involves analysing an invariant called the quantum period. This is a sequence of integers which gives a numerical fingerprint for a Fano variety. It is conjectured that a Fano variety is uniquely determined by its quantum period. If this is true, one should be able to recover geometric properties of a Fano variety directly from its quantum period. We apply machine learning to the question: does the quantum period of X know the dimension of X? Note that there is as yet no theoretical understanding of this. We show that a simple feed-forward neural network can determine the dimension of X with 98% accuracy. Building on this, we establish rigorous asymptotics for the quantum periods of a class of Fano varieties. These asymptotics determine the dimension of X from its quantum period. Our results demonstrate that machine learning can pick out structure from complex mathematical data in situations where we lack theoretical understanding. They also give positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details>
<details>
<summary>摘要</summary>
We apply machine learning to this question: can the quantum period of X reveal the dimension of X? While there is currently no theoretical understanding of this, we show that a simple feed-forward neural network can accurately determine the dimension of X with 98% accuracy. Building on this, we establish rigorous asymptotics for the quantum periods of a class of Fano varieties, which determine the dimension of X from its quantum period.Our results demonstrate that machine learning can extract structure from complex mathematical data in situations where there is no theoretical understanding. They also provide positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review"><a href="#Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review" class="headerlink" title="Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review"></a>Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05457">http://arxiv.org/abs/2309.05457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Niu, Nian Xue, Christina Pöpper</li>
<li>for: 本研究旨在探讨人工智能在学术安全会议 peer review 中的表现，以及Automated Support Approaches 在这一过程中的潜在优势和局限性。</li>
<li>methods: 本研究使用了 ChatGPT 和 Doc2Vec 模型，以及两个阶段的分类方法，对 Computer Science 会议和 arXiv 预印网站上的 thousands of paper 进行了大规模的数据收集和分析。</li>
<li>results: 研究发现，使用 Doc2Vec 模型和两个阶段的分类方法可以具有高度的预测精度（超过 90%），而 ChatGPT 的预测精度较低。同时，研究还发现了 Automated Support Approaches 在 certain aspects 的局限性和人工智能技术无法匹配的地方。<details>
<summary>Abstract</summary>
Peer review is the method employed by the scientific community for evaluating research advancements. In the field of cybersecurity, the practice of double-blind peer review is the de-facto standard. This paper touches on the holy grail of peer reviewing and aims to shed light on the performance of AI in reviewing for academic security conferences. Specifically, we investigate the predictability of reviewing outcomes by comparing the results obtained from human reviewers and machine-learning models. To facilitate our study, we construct a comprehensive dataset by collecting thousands of papers from renowned computer science conferences and the arXiv preprint website. Based on the collected data, we evaluate the prediction capabilities of ChatGPT and a two-stage classification approach based on the Doc2Vec model with various classifiers. Our experimental evaluation of review outcome prediction using the Doc2Vec-based approach performs significantly better than the ChatGPT and achieves an accuracy of over 90%. While analyzing the experimental results, we identify the potential advantages and limitations of the tested ML models. We explore areas within the paper-reviewing process that can benefit from automated support approaches, while also recognizing the irreplaceable role of human intellect in certain aspects that cannot be matched by state-of-the-art AI techniques.
</details>
<details>
<summary>摘要</summary>
科学社区使用 peer review 方法来评估研究进步。在领域安全方面，双重盲测 peer review 是非正式标准。这篇论文探讨 peer review 的圣杯，旨在探讨 AI 在学术安全会议上进行评审的表现。我们 investigate 评审结果的预测可能性，并将 compare 人工评审者和机器学习模型的结果。为了进行研究，我们建立了全面的数据集，收集了 thousands 篇计算机科学会议和 arXiv 预印website 上的论文。基于收集的数据，我们评估 Doc2Vec 模型和多Stage 分类器的预测能力。我们的实验评估结果表明，使用 Doc2Vec 模型可以实现预测精度高于 90%。我们分析实验结果，并发现 ML 模型在某些方面的优势和局限性。我们探讨文献评审过程中可以通过自动支持方法获得的优势，同时也认可人类智慧在某些方面无法由当前 AI 技术匹配的地方。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation"><a href="#Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation" class="headerlink" title="Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation"></a>Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05455">http://arxiv.org/abs/2309.05455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow</li>
<li>for: The paper is written for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023, which aims to develop systems that can generate human-like co-speech gestures in agents that carry semantic meaning.</li>
<li>methods: The paper proposes a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture to achieve a semantic coupling between these modalities. The CSMP module is used as a conditioning signal in a diffusion-based gesture synthesis model to generate semantically-aware co-speech gestures.</li>
<li>results: The paper reports that the proposed system achieved the highest human-likeness and highest speech appropriateness rating among the submitted entries to the GENEA Challenge 2023, indicating that the system is a promising approach to generating human-like co-speech gestures in agents that carry semantic meaning.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为GENEA（Generation and Evaluation of Non-verbal Behaviour for Embodied Agents）挑战2023所写的，旨在开发能够生成人类化的同时语句姿势的系统。</li>
<li>methods: 论文提出了一种对比式Speech和姿势预训练（CSMP）模块，该模块学习了语音和姿势的共同表示，以实现这两种模式之间的 semantic coupling。CSMP模块作为 diffusion-based 姿势生成模型的conditioning信号，以实现具有 semantic 意义的同时语句姿势生成。</li>
<li>results: 论文报告说，提案的系统在GENEA挑战2023中提交的entries中得到了最高的人类化度和语言相符度评分，表明该系统是一种有前途的方法，可以生成具有 semantic 意义的人类化同时语句姿势。<details>
<summary>Abstract</summary>
This paper describes a system developed for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Our solution builds on an existing diffusion-based motion synthesis model. We propose a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture with the aim to learn a semantic coupling between these modalities. The output of the CSMP module is used as a conditioning signal in the diffusion-based gesture synthesis model in order to achieve semantically-aware co-speech gesture generation. Our entry achieved highest human-likeness and highest speech appropriateness rating among the submitted entries. This indicates that our system is a promising approach to achieve human-like co-speech gestures in agents that carry semantic meaning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning"><a href="#Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning" class="headerlink" title="Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"></a>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05444">http://arxiv.org/abs/2309.05444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, Sara Hooker</li>
<li>for: 这个论文旨在推广 Mixture of Experts（MoE） neural network架构，并在固定计算成本下提高总性能。</li>
<li>methods: 该论文提出了一种EXTREMELY parameter-efficient MoE方法，通过结合 MoE 架构和轻量级专家来实现。</li>
<li>results: 该方法可以在不更新核心模型的情况下，对已有模型进行精细调整，并且可以在不知道先前任务知识的情况下进行泛化。 论文的代码可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/for-ai/parameter-efficient-moe%E3%80%82">https://github.com/for-ai/parameter-efficient-moe。</a><details>
<summary>Abstract</summary>
The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: https://github.com/for-ai/parameter-efficient-moe.
</details>
<details>
<summary>摘要</summary>
难以扩展的混合专家（MoE）是一种广泛使用的神经网络架构，其中一组专业化的子模型共同优化总性性能，而无需增加计算成本。然而，传统的MoE受到存储所有专家的限制，难以扩展到大规模。在这篇论文中，我们推动MoE的限制。我们提出了EXTREMELY parameter-efficient MoE，通过独特地结合MoE架构和轻量级专家来实现。我们的MoE架构超越了标准的参数高效精度调整（PEFT）方法，并与全面精度调整相当，只需更新轻量级专家（占11B参数模型的0.1%以下）。此外，我们的方法能够扩展到未经见过任务，因为它不依赖任何先前任务知识。我们的研究强调了混合专家架构的灵活性，展示它能够在受到严格参数限制的情况下提供稳定性的性能。我们所用的实验代码可以在以下链接获取：https://github.com/for-ai/parameter-efficient-moe。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models"><a href="#Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models" class="headerlink" title="Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models"></a>Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05436">http://arxiv.org/abs/2309.05436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuripsANON2023/QFF">https://github.com/neuripsANON2023/QFF</a></li>
<li>paper_authors: Frederiek Wesel, Kim Batselier</li>
<li>for: 该 paper 的目的是提出一种基于 tensor 网络的kernel machine，以提高模型的通用能力和精度。</li>
<li>methods: 该 paper 使用了归纳映射和 Fourier 特征来提供一种非线性扩展，并利用 tensor 结构来降低模型的维度。</li>
<li>results: 该 paper 实验表明，对于同样的特征，量化模型可以提供更高的VC-维度 bound，而无需额外的计算成本。此外，量化模型还可以增强模型的泛化能力。<details>
<summary>Abstract</summary>
In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on large regression task, achieving state-of-the-art results on a laptop computer.
</details>
<details>
<summary>摘要</summary>
在核心机器学中， polynomial 和 Fourier 特征通常用于提供非线性扩展，将数据映射到更高维度空间。 Unless one considers the dual formulation of the learning problem, which makes exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits tackling high-dimensional problems. One possible approach to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper, we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization, we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on a large regression task, achieving state-of-the-art results on a laptop computer.Here's the translation in Traditional Chinese:在核心机器学中，多项和傅立做特征通常用于提供非线性扩展，将数据映射到更高维度空间。 unless one considers the dual formulation of the learning problem, which makes exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits tackling high-dimensional problems. one possible approach to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper, we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization, we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on a large regression task, achieving state-of-the-art results on a laptop computer.
</details></li>
</ul>
<hr>
<h2 id="A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding"><a href="#A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding" class="headerlink" title="A parameterised model for link prediction using node centrality and similarity measure based on graph embedding"></a>A parameterised model for link prediction using node centrality and similarity measure based on graph embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05434">http://arxiv.org/abs/2309.05434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohui Lu, Shahadat Uddin</li>
<li>for: 预测新形成的网络连接，如疾病预测、社交网络推荐和药物发现等应用。</li>
<li>methods: 提出了一种基于图 neural network 的参数化模型（NCSM），该模型独特地集成了节点中心性和相似度度量作为边特征，有效地利用大网络的 topological 信息。</li>
<li>results: 对五个基准图 dataset 进行了评估，结果显示 NCSM 在不同的维度和数据集上都超过了现有状态的 искусственный neural network 和变量图自动编码器模型，表现出色。这一Exceptional performance 可以归因于 NCSM 的创新性的集成节点中心性、相似度度量和有效地利用 topological 信息。<details>
<summary>Abstract</summary>
Link prediction is a key aspect of graph machine learning, with applications as diverse as disease prediction, social network recommendations, and drug discovery. It involves predicting new links that may form between network nodes. Despite the clear importance of link prediction, existing models have significant shortcomings. Graph Convolutional Networks, for instance, have been proven to be highly efficient for link prediction on a variety of datasets. However, they encounter severe limitations when applied to short-path networks and ego networks, resulting in poor performance. This presents a critical problem space that this work aims to address. In this paper, we present the Node Centrality and Similarity Based Parameterised Model (NCSM), a novel method for link prediction tasks. NCSM uniquely integrates node centrality and similarity measures as edge features in a customised Graph Neural Network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterised GNN-based link prediction model that considers topological information. The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>链接预测是图机器学习的关键方面之一，具有多种应用，如疾病预测、社交网络推荐和药物发现。它涉及预测网络节点之间可能会形成的新链接。虽然链接预测的重要性明显，现有模型却存在重大缺陷。图 convolutional neural networks 例如，在多种数据集上证明了高效性，但在短路网络和我的网络上应用时，它们会遇到严重的限制，导致性能差异。这个问题空间是这项工作的目标。在这篇论文中，我们提出了节点中心性和相似度基于参数化模型（NCSM），这是一种新的链接预测模型。NCSM Uniquely integrates node centrality and similarity measures as edge features in a customized graph neural network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterized GNN-based link prediction model that considers topological information. The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Auditory-Perception-by-Neural-Spiketrum"><a href="#Neuromorphic-Auditory-Perception-by-Neural-Spiketrum" class="headerlink" title="Neuromorphic Auditory Perception by Neural Spiketrum"></a>Neuromorphic Auditory Perception by Neural Spiketrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05430">http://arxiv.org/abs/2309.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huajin Tang, Pengjie Gu, Jayawan Wijekoon, MHD Anas Alsakkal, Ziming Wang, Jiangrong Shen, Rui Yan</li>
<li>for: 用于实现脑内智能的能效计算和稳定学习性能。</li>
<li>methods: 使用适应生物神经系统的 neuromorphic 硬件架构和适应生物神经系统的硬件友好算法，并使用时变整流模型来转换时间变化的整流信号。</li>
<li>results: 实现了减少信息损失和精确控制脉冲率的精炼和高效编码方法，并通过 neuromorphic 耳膜原型实现了算法硬件协设。<details>
<summary>Abstract</summary>
Neuromorphic computing holds the promise to achieve the energy efficiency and robust learning performance of biological neural systems. To realize the promised brain-like intelligence, it needs to solve the challenges of the neuromorphic hardware architecture design of biological neural substrate and the hardware amicable algorithms with spike-based encoding and learning. Here we introduce a neural spike coding model termed spiketrum, to characterize and transform the time-varying analog signals, typically auditory signals, into computationally efficient spatiotemporal spike patterns. It minimizes the information loss occurring at the analog-to-spike transformation and possesses informational robustness to neural fluctuations and spike losses. The model provides a sparse and efficient coding scheme with precisely controllable spike rate that facilitates training of spiking neural networks in various auditory perception tasks. We further investigate the algorithm-hardware co-designs through a neuromorphic cochlear prototype which demonstrates that our approach can provide a systematic solution for spike-based artificial intelligence by fully exploiting its advantages with spike-based computation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing"><a href="#Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing" class="headerlink" title="Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing"></a>Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05686">http://arxiv.org/abs/2309.05686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Sponner, Julius Ott, Lorenzo Servadei, Bernd Waschneck, Robert Wille, Akash Kumar</li>
<li>for: 这篇论文旨在提高资料流程中的深度学习推断效率，并且在有限的嵌入式平台上进行处理。</li>
<li>methods: 本论文提出了一些新的技术，将测量数据流中的时间相关性利用来强化深度学习推断的精确性，并且在执行过程中进行决策。</li>
<li>results: 本论文的结果显示，这些技术可以节省至多26%的运算量每次推断，并且可以与传统优化技术结合使用，以提高资源受限的嵌入式平台上的效率。<details>
<summary>Abstract</summary>
Radar sensors offer power-efficient solutions for always-on smart devices, but processing the data streams on resource-constrained embedded platforms remains challenging. This paper presents novel techniques that leverage the temporal correlation present in streaming radar data to enhance the efficiency of Early Exit Neural Networks for Deep Learning inference on embedded devices. These networks add additional classifier branches between the architecture's hidden layers that allow for an early termination of the inference if their result is deemed sufficient enough by an at-runtime decision mechanism. Our methods enable more informed decisions on when to terminate the inference, reducing computational costs while maintaining a minimal loss of accuracy.   Our results demonstrate that our techniques save up to 26% of operations per inference over a Single Exit Network and 12% over a confidence-based Early Exit version. Our proposed techniques work on commodity hardware and can be combined with traditional optimizations, making them accessible for resource-constrained embedded platforms commonly used in smart devices. Such efficiency gains enable real-time radar data processing on resource-constrained platforms, allowing for new applications in the context of smart homes, Internet-of-Things, and human-computer interaction.
</details>
<details>
<summary>摘要</summary>
雷达感知器提供了功率有效的解决方案，但在资源有限的嵌入式平台上处理数据流仍然是挑战。这篇论文介绍了新的技术，它们利用流动雷达数据中的时间相关性来增强深度学习批处理器的有效性。这些网络添加了在架构中隐藏层之间的额外分支，以实现在运行时决策机制判断是否可以提前终止批处理。我们的方法可以更好地决定 WHEN 终止批处理，降低计算成本，同时保持最小的准确性损失。我们的结果表明，我们的技术可以将批处理操作减少至 26%，相比单exit网络。此外，与信任度基于的早期终止版本相比，我们的技术可以减少操作数至 12%。我们的提议的技术可以在商业硬件上使用，并且可以与传统优化结合使用，使其在资源有限的嵌入式平台上可用。这种效率提升允许实时雷达数据处理，开启了新的应用场景，如智能家居、物联网和人机交互。
</details></li>
</ul>
<hr>
<h2 id="Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing"><a href="#Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing" class="headerlink" title="Learning noise-induced transitions by multi-scaling reservoir computing"></a>Learning noise-induced transitions by multi-scaling reservoir computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05413">http://arxiv.org/abs/2309.05413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zequn Lin, Zhaofan Lu, Zengru Di, Ying Tang</li>
<li>for: 本研究旨在通过机器学习模型，具体是流体计算，提取时间序列数据中的随机过程，包括随机过程的转移和稳定状态之间的关系。</li>
<li>methods: 该研究使用了流体计算，一种类型的循环神经网络，来学习吸收噪声所致的转移。研究人员还提出了一种简洁的训练协议，以便调整模型的超参数，特别是控制模型动力学的时间尺度。</li>
<li>results: 研究人员通过使用流体计算模型，可以准确地预测时间序列数据中的转移时间和转移次数的统计。此外，模型还能够捕捉到多稳定系统中的转移，以及非详细平衡引起的旋转动力学。例如，对蛋白质折叠实验数据进行预测，可以获得转移时间的统计。研究结果表明，机器学习方法可以准确地捕捉到噪声引起的随机过程。<details>
<summary>Abstract</summary>
Noise is usually regarded as adversarial to extract the effective dynamics from time series, such that the conventional data-driven approaches usually aim at learning the dynamics by mitigating the noisy effect. However, noise can have a functional role of driving transitions between stable states underlying many natural and engineered stochastic dynamics. To capture such stochastic transitions from data, we find that leveraging a machine learning model, reservoir computing as a type of recurrent neural network, can learn noise-induced transitions. We develop a concise training protocol for tuning hyperparameters, with a focus on a pivotal hyperparameter controlling the time scale of the reservoir dynamics. The trained model generates accurate statistics of transition time and the number of transitions. The approach is applicable to a wide class of systems, including a bistable system under a double-well potential, with either white noise or colored noise. It is also aware of the asymmetry of the double-well potential, the rotational dynamics caused by non-detailed balance, and transitions in multi-stable systems. For the experimental data of protein folding, it learns the transition time between folded states, providing a possibility of predicting transition statistics from a small dataset. The results demonstrate the capability of machine-learning methods in capturing noise-induced phenomena.
</details>
<details>
<summary>摘要</summary>
噪声通常被视为时间序列数据中的障碍物，以致传统的数据驱动方法通常努力于减少噪声的影响。然而，噪声可以扮演一个功能性的角色，即驱动多种自然和工程化的随机动力。为捕捉数据中的随机转换，我们发现可以利用机器学习模型，即激流计算作为一种循环神经网络，学习噪声引起的转换。我们开发了一种简洁的训练协议，专注于控制激流动力的时间尺度的关键参数。训练后的模型可以准确地计算转换时间和转换次数的统计。这种方法适用于广泛的系统，包括一个下降征 Double-well 潜在能量下的二stable系统，以及白噪声和颜色噪声。它还能够考虑下降征的非细节平衡、旋转动力学和多stable系统中的转换。对蛋白质折叠的实验数据，它学习了转换时间 между折叠态，提供了预测转换统计信息的可能性。结果表明机器学习方法可以捕捉噪声引起的现象。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions"><a href="#Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions" class="headerlink" title="Physics-informed reinforcement learning via probabilistic co-adjustment functions"></a>Physics-informed reinforcement learning via probabilistic co-adjustment functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05404">http://arxiv.org/abs/2309.05404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nat Wannawas, A. Aldo Faisal</li>
<li>for: 这个论文的目的是提出一种数据效率的强化学习方法，用于训练基于实际世界任务的系统。</li>
<li>methods: 该方法使用了两种新的方法：协同拟合调整（CKA）和ridge regression调整（RRA），将数据集和模拟模型结合起来，以提高模型的准确性和不确定性评估。</li>
<li>results: 研究发现，使用CKA和RRA可以获得更高的模型准确性和不确定性评估，并且可以在实际世界中实现数据效率的强化学习。<details>
<summary>Abstract</summary>
Reinforcement learning of real-world tasks is very data inefficient, and extensive simulation-based modelling has become the dominant approach for training systems. However, in human-robot interaction and many other real-world settings, there is no appropriate one-model-for-all due to differences in individual instances of the system (e.g. different people) or necessary oversimplifications in the simulation models. This requires two approaches: 1. either learning the individual system's dynamics approximately from data which requires data-intensive training or 2. using a complete digital twin of the instances, which may not be realisable in many cases. We introduce two approaches: co-kriging adjustments (CKA) and ridge regression adjustment (RRA) as novel ways to combine the advantages of both approaches. Our adjustment methods are based on an auto-regressive AR1 co-kriging model that we integrate with GP priors. This yield a data- and simulation-efficient way of using simplistic simulation models (e.g., simple two-link model) and rapidly adapting them to individual instances (e.g., biomechanics of individual people). Using CKA and RRA, we obtain more accurate uncertainty quantification of the entire system's dynamics than pure GP-based and AR1 methods. We demonstrate the efficiency of co-kriging adjustment with an interpretable reinforcement learning control example, learning to control a biomechanical human arm using only a two-link arm simulation model (offline part) and CKA derived from a small amount of interaction data (on-the-fly online). Our method unlocks an efficient and uncertainty-aware way to implement reinforcement learning methods in real world complex systems for which only imperfect simulation models exist.
</details>
<details>
<summary>摘要</summary>
现实世界中的强化学习任务很数据不效率，广泛采用模拟基本的方法进行系统训练。然而，在人机交互和许多现实世界中的情况下，没有适合一个模型所有的选择，因为系统实例之间存在差异（例如不同的人）或者模拟模型中需要忽略一些细节。这要求两种方法：1. either从数据中学习个体系统的动力学特性，需要大量的数据训练；2. 使用完整的数字双方案，但在许多情况下可能不可能实现。我们介绍两种新的方法：协同拟合调整（CKA）和ridge regression调整（RRA），这两种方法可以结合数据和模拟的优点，并快速适应个体实例（例如人体生物力学）。使用CKA和RRA，我们可以获得更加准确的整体系统动力学的不确定性评估，比洁GP基础和AR1方法更加高效。我们通过一个可解释的强化学习控制示例来说明CKA的效果，我们通过只使用小量的互动数据（在线部分）和CKA来学习控制一个生物力学人工臂。我们的方法可以帮助实现现实世界中复杂系统中的强化学习方法，只有不准确的模拟模型存在。
</details></li>
</ul>
<hr>
<h2 id="Practical-Homomorphic-Aggregation-for-Byzantine-ML"><a href="#Practical-Homomorphic-Aggregation-for-Byzantine-ML" class="headerlink" title="Practical Homomorphic Aggregation for Byzantine ML"></a>Practical Homomorphic Aggregation for Byzantine ML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05395">http://arxiv.org/abs/2309.05395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Choffrut, Rachid Guerraoui, Rafael Pinot, Renaud Sirdey, John Stephan, Martin Zuber</li>
<li>for: 该论文旨在提出一种完全卷积的分布式学习算法，以保护数据隐私和抵御 Byzantine 攻击。</li>
<li>methods: 该算法使用了一种新的普通文本编码方法，使得可以实现robust aggregator在批处理-Friendly BGV 上进行实现。此外，这种编码方法还可以加速当前的同样安全大小的排序。</li>
<li>results: 该算法在图像分类任务上进行了广泛的实验，并显示了实际执行时间和与非私有版本相匹配的机器学习性能。<details>
<summary>Abstract</summary>
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. However, due to HE's large computational demand especially for high-dimensional ML models, there has not yet been any attempt to design purely homomorphic operators for non-linear robust aggregators. In this work, we present SABLE, the first completely homomorphic and Byzantine robust distributed learning algorithm. SABLE essentially relies on a novel plaintext encoding method that enables us to implement the robust aggregator over batching-friendly BGV. Moreover, this encoding scheme also accelerates state-of-the-art homomorphic sorting with larger security margins and smaller ciphertext size. We perform extensive experiments on image classification tasks and show that our algorithm achieves practical execution times while matching the ML performance of its non-private counterpart.
</details>
<details>
<summary>摘要</summary>
因为大规模数据的可用性，机器学习（ML）算法在分布式架构中被部署，其中不同的节点协作训练 ML 模型，通过交换模型相关信息（如梯度）与中央服务器进行交互。然而，分布式学习方案受到两种威胁。首先，拜占庭节点可以单手损害学习，通过向服务器发送错误信息，如误差梯度。标准的应对方法是使用非线性Robust Aggregation方法。第二，服务器可以违反节点的隐私。 latest attacks 表明，在交换（未加密）梯度时，服务器可以恢复所有节点的数据。为保护隐私，homomorphic encryption（HE），一种金标准安全 primitives，已经广泛研究在分布式学习的非拜占庭场景中。然而，由于 HE 的计算复杂度，特别是高维 ML 模型，没有任何尝试设计纯正Homomorphic Operator。在这种情况下，我们提出了 SABLE，首个完全Homomorphic和拜占庭Robust分布式学习算法。SABLE 基本上依靠一种新的普通文本编码方法，使我们可以实现Robust Aggregator在批处理友好 BGV 上。此外，这种编码方案还加速了当前的 homomorphic sorting ，具有更大的安全优势和更小的 ciphertext size。我们对图像分类任务进行了广泛的实验，并证明了我们的算法可以实现实际执行时间，与非隐私 counterpart 的 ML 性能相匹配。
</details></li>
</ul>
<hr>
<h2 id="Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach"><a href="#Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach" class="headerlink" title="Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach"></a>Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05391">http://arxiv.org/abs/2309.05391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Avlonitis, Dor Lavi, Masoud Mansoury, David Graus</li>
<li>for: 这个研究旨在使用强化学习算法提高职业规划过程。</li>
<li>methods: 研究使用了Markov Decision Process（MDP）形式表示荷兰就业市场，并使用机器学习算法such as Sarsa, Q-Learning, and A2C来学习优化员工的长期收入轨迹。</li>
<li>results: 研究结果表明，使用RL模型（特别是Q-Learning和Sarsa）可以提高员工的收入轨迹，平均提高5%compared to observed career paths。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This study explores the potential of reinforcement learning algorithms to enhance career planning processes. Leveraging data from Randstad The Netherlands, the study simulates the Dutch job market and develops strategies to optimize employees' long-term income. By formulating career planning as a Markov Decision Process (MDP) and utilizing machine learning algorithms such as Sarsa, Q-Learning, and A2C, we learn optimal policies that recommend career paths with high-income occupations and industries. The results demonstrate significant improvements in employees' income trajectories, with RL models, particularly Q-Learning and Sarsa, achieving an average increase of 5% compared to observed career paths. The study acknowledges limitations, including narrow job filtering, simplifications in the environment formulation, and assumptions regarding employment continuity and zero application costs. Future research can explore additional objectives beyond income optimization and address these limitations to further enhance career planning processes.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Chinese that uses simpler grammar and vocabulary, making it easier to understand for speakers of Mandarin Chinese. However, the translation may not be exact, as the nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory"><a href="#Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory" class="headerlink" title="Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory"></a>Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05386">http://arxiv.org/abs/2309.05386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan C. Schulze, Danimir T. Doncevic, Nils Erwes, Alexander Mitsos</li>
<li>for: 这个论文的目的是提出一种基于Koopman理论的数据驱动模型减少策略，用于实现非线性预测控制（NMPC）的实时能力。</li>
<li>methods: 该论文使用了Koopman理论基于自编码器和线性隐藏动力学的数据驱动模型减少策略，并结合了非线性预测控制（NMPC）的实现。</li>
<li>results: 该论文的实验结果表明，使用该数据驱动模型减少策略和NMPC实现，可以在ASU（气体分离器）中实现98%的CPU时间减少，实现实时NMPC控制。<details>
<summary>Abstract</summary>
Achieving real-time capability is an essential prerequisite for the industrial implementation of nonlinear model predictive control (NMPC). Data-driven model reduction offers a way to obtain low-order control models from complex digital twins. In particular, data-driven approaches require little expert knowledge of the particular process and its model, and provide reduced models of a well-defined generic structure. Herein, we apply our recently proposed data-driven reduction strategy based on Koopman theory [Schulze et al. (2022), Comput. Chem. Eng.] to generate a low-order control model of an air separation unit (ASU). The reduced Koopman model combines autoencoders and linear latent dynamics and is constructed using machine learning. Further, we present an NMPC implementation that uses derivative computation tailored to the fixed block structure of reduced Koopman models. Our reduction approach with tailored NMPC implementation enables real-time NMPC of an ASU at an average CPU time decrease by 98 %.
</details>
<details>
<summary>摘要</summary>
实现实时能力是非Linear Model Predictive Control（NMPC）的industrial化先要有一个重要的前提条件。数据驱动模型减少提供了一种获取低阶控制模型的复杂数字孪生。特别是，数据驱动方法不需要对特定过程和其模型的专家知识，并提供了一种具有定制结构的减少模型。在这种情况下，我们使用我们最近提出的数据驱动减少策略，基于Koopman理论([Schulze et al., 2022，Comput. Chem. Eng.])，来生成一个低阶控制模型。这个减少模型结合了自动编码器和线性隐动动力，通过机器学习构建。此外，我们还提供了适应于固定块结构减少Koopman模型的导函数计算，以实现NMPC实现。我们的减少方法与适应NMPC实现使得ASU的实时NMPC可以在CPU时间减少98%。
</details></li>
</ul>
<hr>
<h2 id="Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation"><a href="#Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation" class="headerlink" title="Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation"></a>Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05361">http://arxiv.org/abs/2309.05361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengshuo Shen, Wei Zheng, Bihao Guo, Dalong Chen, Xinkun Ai, Fengming Xue, Yu Zhong, Nengchao Wang, Biao Shen, Binjia Xiao, Yonghua Ding, Zhongyong Chen, Yuan Pan, J-TEXT team<br>for: 这个研究旨在预测未来tokamak中的干扰，并且使用了域对对映算法CORAL。methods: 这个研究使用了域对对映算法CORAL，并且将目标领域（未来tokamak）的少量数据与来源领域（现有tokamak）的大量数据进行了对映，以训练机器学习模型。results: 这个研究发现，使用supervised CORAL可以增强未来tokamak中的干扰预测性能（AUC值由0.764提高至0.890），并且通过可解释分析发现，使用supervised CORAL可以让数据分布更加类似未来tokamak。<details>
<summary>Abstract</summary>
The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak only using a few discharges based on a domain adaptation algorithm called CORAL. It is the first attempt at applying domain adaptation in the disruption prediction task. In this paper, this disruption prediction approach aligns a few data from the future tokamak (target domain) and a large amount of data from the existing tokamak (source domain) to train a machine learning model in the existing tokamak. To simulate the existing and future tokamak case, we selected J-TEXT as the existing tokamak and EAST as the future tokamak. To simulate the lack of disruptive data in future tokamak, we only selected 100 non-disruptive discharges and 10 disruptive discharges from EAST as the target domain training data. We have improved CORAL to make it more suitable for the disruption prediction task, called supervised CORAL. Compared to the model trained by mixing data from the two tokamaks, the supervised CORAL model can enhance the disruption prediction performance for future tokamaks (AUC value from 0.764 to 0.890). Through interpretable analysis, we discovered that using the supervised CORAL enables the transformation of data distribution to be more similar to future tokamak. An assessment method for evaluating whether a model has learned a trend of similar features is designed based on SHAP analysis. It demonstrates that the supervised CORAL model exhibits more similarities to the model trained on large data sizes of EAST. FTDP provides a light, interpretable, and few-data-required way by aligning features to predict disruption using small data sizes from the future tokamak.
</details>
<details>
<summary>摘要</summary>
“高的探索成本和未来tokamak中需要干扰发生的需求导致了探索预测研究中的悖论。在这篇研究中，我们提出了一种新的方法来预测未来tokamak中的干扰，只使用了几次探索。我们使用了域化适应算法CORAL，这是在探索预测任务中的首次应用。在这篇研究中，我们将未来tokamak中的探索训练数据和现有tokamak中的大量数据进行了对接，以训练一个机器学习模型。为了模拟现有和未来tokamak的情况，我们选择了J-TEXT作为现有tokamak，而EAST则作为未来tokamak。为了模拟未来tokamak中缺乏干扰数据的情况，我们仅选择了EAST中的100次非干扰探索和10次干扰探索作为目标领域训练数据。我们将CORAL更新为适合探索预测任务，称为监督式CORAL。相比于将数据从两个tokamak混合训练的模型，监督式CORAL模型可以对未来tokamak的干扰预测表现更好（AUC值由0.764提高至0.890）。通过可解析分析，我们发现使用监督式CORAL可以让数据分布变得更加相似。我们还设计了一个根据SHAP分析的评估方法，以验证模型是否学习了类似特征的趋势。结果显示监督式CORAL模型在SHAP分析中表现更加相似于在大量EAST数据上训练的模型。FTDP提供了一个轻量、可解析且需要少量数据的方法，通过将特征转换为预测干扰的方法。”
</details></li>
</ul>
<hr>
<h2 id="EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection"><a href="#EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection" class="headerlink" title="EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection"></a>EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05357">http://arxiv.org/abs/2309.05357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edac-ml4h/edac-ml4h">https://github.com/edac-ml4h/edac-ml4h</a></li>
<li>paper_authors: Andrej Jovanović, Mario Mihaly, Lennon Donaldson</li>
<li>for: 本研究旨在开发一种可靠、可deploy的预creening方法，用于检测COVID-19病毒的存在。</li>
<li>methods: 本研究使用机器学习方法，利用CT扫描和喘音记录等输入特征，通过深度神经网络架构实现高度准确的预测。</li>
<li>results: 研究人员通过网络剪辑和量化来压缩两个模型，实现了模型文件尺寸的压缩和减少计算时间，同时保持模型的预测性能。 specifically, 研究人员实现了一个105.76倍和一个19.34倍的压缩比例，以及相应的1.37倍和1.71倍的计算时间减少。<details>
<summary>Abstract</summary>
The global spread of COVID-19 had severe consequences for public health and the world economy. The quick onset of the pandemic highlighted the potential benefits of cheap and deployable pre-screening methods to monitor the prevalence of the disease in a population. Various researchers made use of machine learning methods in an attempt to detect COVID-19. The solutions leverage various input features, such as CT scans or cough audio signals, with state-of-the-art results arising from deep neural network architectures. However, larger models require more compute; a pertinent consideration when deploying to the edge. To address this, we first recreated two models that use cough audio recordings to detect COVID-19. Through applying network pruning and quantisation, we were able to compress these two architectures without reducing the model's predictive performance. Specifically, we were able to achieve an 105.76x and an 19.34x reduction in the compressed model file size with corresponding 1.37x and 1.71x reductions in the inference times of the two models.
</details>
<details>
<summary>摘要</summary>
全球COVID-19疫情的严重性对公共卫生和世界经济造成了严重的影响。快速蔓延的疫情推祟了使用便宜可部署的预卷方法来监测人口中疫苗的存在。各种研究人员使用机器学习方法来探索COVID-19的检测方法。这些解决方案利用了不同的输入特征，如CT扫描或喊喊声信号，并通过深度神经网络架构实现了国际一流的检测效果。然而，更大的模型需要更多的计算资源，这是在部署到边缘时需要考虑的。为此，我们首先重建了两个使用喊喊声记录来检测COVID-19的模型。通过网络剪辑和量化，我们成功地压缩了这两个架构，而无需降低模型的预测性能。具体来说，我们实现了一个105.76倍和一个19.34倍的压缩模型文件大小减少，同时对两个模型的执行时间也实现了1.37倍和1.71倍的减少。
</details></li>
</ul>
<hr>
<h2 id="Neural-Discovery-of-Permutation-Subgroups"><a href="#Neural-Discovery-of-Permutation-Subgroups" class="headerlink" title="Neural Discovery of Permutation Subgroups"></a>Neural Discovery of Permutation Subgroups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05352">http://arxiv.org/abs/2309.05352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Karjol, Rohan Kashyap, Prathosh A P</li>
<li>for: 本文研究了找到 permutation group $S_{n}$ 中的子群 $H$。 unlike traditional $H$-invariant networks, 我们提出了一种方法，通过学习 $S_{n}$-invariant function 和线性变换，找到 underlying subgroup。</li>
<li>methods: 我们使用了 $S_{n}$-invariant function 和线性变换来找到 subgroup $H$。我们也证明了类似结论对征群 $S_{k} (k \leq n)$ 和征子群 $C_{n}$、$D_{n}$ 成立。</li>
<li>results: 我们的结果表明，可以通过学习 $S_{n}$-invariant function 和线性变换，找到任何类型的 subgroup $H$。我们还提供了一般化的定理，可以扩展到其他 $S_{n}$ 中的 subgroup。 数据图像权重和对称多项式回归任务中的实验结果表明了我们的方法的可行性。<details>
<summary>Abstract</summary>
We consider the problem of discovering subgroup $H$ of permutation group $S_{n}$. Unlike the traditional $H$-invariant networks wherein $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. Our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. We also prove similar results for cyclic and dihedral subgroups. Finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. We also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.
</details>
<details>
<summary>摘要</summary>
我们考虑找到 permutation group $S_{n}$ 中的子群 $H$ 的问题。 unlike traditional $H$-invariant networks, where $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. we also prove similar results for cyclic and dihedral subgroups. finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. we also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.Here's the translation in Traditional Chinese:我们考虑找到 permutation group $S_{n}$ 中的子群 $H$ 的问题。 unlike traditional $H$-invariant networks, where $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. we also prove similar results for cyclic and dihedral subgroups. finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. we also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.
</details></li>
</ul>
<hr>
<h2 id="Learning-Geometric-Representations-of-Objects-via-Interaction"><a href="#Learning-Geometric-Representations-of-Objects-via-Interaction" class="headerlink" title="Learning Geometric Representations of Objects via Interaction"></a>Learning Geometric Representations of Objects via Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05346">http://arxiv.org/abs/2309.05346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reichlin/geomrepobj">https://github.com/reichlin/geomrepobj</a></li>
<li>paper_authors: Alfredo Reichlin, Giovanni Luca Marchetti, Hang Yin, Anastasiia Varava, Danica Kragic</li>
<li>for: 学习 scene 中 agent 和外部对象的表示</li>
<li>methods: 基于 agent 行为作为监督，提取 Physical space 中 agent 和对象的位置</li>
<li>results: 比对 vision-based 方法高效，解耦 agent 和对象，并在下游任务中efficient 地使用 reinforcement learning 解决问题<details>
<summary>Abstract</summary>
We address the problem of learning representations from observations of a scene involving an agent and an external object the agent interacts with. To this end, we propose a representation learning framework extracting the location in physical space of both the agent and the object from unstructured observations of arbitrary nature. Our framework relies on the actions performed by the agent as the only source of supervision, while assuming that the object is displaced by the agent via unknown dynamics. We provide a theoretical foundation and formally prove that an ideal learner is guaranteed to infer an isometric representation, disentangling the agent from the object and correctly extracting their locations. We evaluate empirically our framework on a variety of scenarios, showing that it outperforms vision-based approaches such as a state-of-the-art keypoint extractor. We moreover demonstrate how the extracted representations enable the agent to solve downstream tasks via reinforcement learning in an efficient manner.
</details>
<details>
<summary>摘要</summary>
我们对将Scene中的代理和外部物品的学习表现学习的问题进行了处理。为此，我们提出了一个基于代理的动作进行学习框架，从无结构的观察中提取代理和物品的物理空间位置。我们的框架仅对代理的动作进行超参考，并假设物品被代理驱动的运动是未知的。我们提供了理论基础，正式证明了理想学习者可以将代理和物品分离，并正确地提取它们的位置。我们在实验中证明了我们的框架在多种情况下表现较好，并且通过强化学习解决下游任务。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications"><a href="#A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications" class="headerlink" title="A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications"></a>A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05343">http://arxiv.org/abs/2309.05343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Wang, Peizheng Li, Angela Doufexi, Mark A Beach</li>
<li>for: 这个论文旨在研究基于弹性智能表面（RIS）对无线通信系统的干扰依赖性。</li>
<li>methods: 本文使用深度强化学习（DRL）来优化单反射和多反射配置，以提高对于分布式用户的传输性能。</li>
<li>results: 比较随机搜寻和探索搜寻，DRL优化方法在对于多反射配置的优化中表现出优秀的性能，实现了1.2 dB的干扰峰值增加和更宽的传输幕。<details>
<summary>Abstract</summary>
In reconfigurable intelligent surface (RIS)-assisted wireless communication systems, the pointing accuracy and intensity of reflections depend crucially on the 'profile,' representing the amplitude/phase state information of all elements in a RIS array. The superposition of multiple single-reflection profiles enables multi-reflection for distributed users. However, the optimization challenges from periodic element arrangements in single-reflection and multi-reflection profiles are understudied. The combination of periodical single-reflection profiles leads to amplitude/phase counteractions, affecting the performance of each reflection beam. This paper focuses on a dual-reflection optimization scenario and investigates the far-field performance deterioration caused by the misalignment of overlapped profiles. To address this issue, we introduce a novel deep reinforcement learning (DRL)-based optimization method. Comparative experiments against random and exhaustive searches demonstrate that our proposed DRL method outperforms both alternatives, achieving the shortest optimization time. Remarkably, our approach achieves a 1.2 dB gain in the reflection peak gain and a broader beam without any hardware modifications.
</details>
<details>
<summary>摘要</summary>
在具有自适应智能表面（RIS）的无线通信系统中，点向精度和反射强度取决于“profile”，表示所有RIS数组元素的振荡状态信息。多个单反射profile的超пози合 enables distributed users的多反射。然而，单反射profile的 periodic 排序和多反射profile的优化挑战尚未得到充分研究。这篇论文关注 dual-reflection 优化方案，并investigates the far-field performance degradation caused by the misalignment of overlapped profiles。为解决这个问题，我们提出了一种基于深度学习（DRL）的优化方法。对比于随机搜索和极限搜索，我们的提议DRL方法在优化时间上表现出明显的优势，并且实现了无硬件修改的1.2 dB增强和更广泛的射频覆盖。
</details></li>
</ul>
<hr>
<h2 id="PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics"><a href="#PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics" class="headerlink" title="PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics"></a>PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05339">http://arxiv.org/abs/2309.05339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claus Smitt, Michael Halstead, Patrick Zimmer, Thomas Läbe, Esra Guclu, Cyrill Stachniss, Chris McCool</li>
<li>for: 实现园林 robot监控和干预任务中的高精度景象理解</li>
<li>methods: 使用NeRF技术建立3D积极Scene理解系统</li>
<li>results: 在严难的农业景象中，实现了3D积极Scene理解、真实图形和一致的panoptic表现，并且比基eline方法提高了21.34dB的峰值信号吟强和56.65%的panoptic质量。<details>
<summary>Abstract</summary>
Precise scene understanding is key for most robot monitoring and intervention tasks in agriculture. In this work we present PAg-NeRF which is a novel NeRF-based system that enables 3D panoptic scene understanding. Our representation is trained using an image sequence with noisy robot odometry poses and automatic panoptic predictions with inconsistent IDs between frames. Despite this noisy input, our system is able to output scene geometry, photo-realistic renders and 3D consistent panoptic representations with consistent instance IDs. We evaluate this novel system in a very challenging horticultural scenario and in doing so demonstrate an end-to-end trainable system that can make use of noisy robot poses rather than precise poses that have to be pre-calculated. Compared to a baseline approach the peak signal to noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be tuned to improve inference time by more than a factor of 2 while being memory efficient with approximately 12 times fewer parameters.
</details>
<details>
<summary>摘要</summary>
precise scene understanding 是 robot 监测和干预任务中的关键。在这项工作中，我们介绍了 PAg-NeRF，一种基于 NeRF 的新系统，允许3D�anoptic scene 理解。我们的表示被训练使用含有噪声机器人定位姿态和自动生成的�anoptic预测，带有不一致的ID между帧。尽管输入含有噪声，但我们的系统仍能输出场景几何、真实渲染和3D一致的�anoptic表示，并且实例ID保持一致。我们在挑战性较高的园艺场景中评估了这种新系统，并在这种情况下表明了可以使用噪声机器人定位而不需要先计算精确定位。相比基准方法，峰峰信号响应比例提高了21.34dB到23.37dB，而�anoptic质量也提高了从56.65%到70.08%。此外，我们的方法更快，可以通过调整来提高推理时间，并且具有较少的参数，约12倍少于基准方法。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems"><a href="#Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems" class="headerlink" title="Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems"></a>Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05337">http://arxiv.org/abs/2309.05337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Chiara Angelini, Angelo Giorgio Cavaliere, Raffaele Marino, Federico Ricci-Tersenghi</li>
<li>for: 这paper aimed to investigate the relationship between Stochastic Gradient Descent (SGD) and Glauber dynamics, and to show that they are substantially similar in discrete optimization and inference problems.</li>
<li>methods: The paper used an SGD-like algorithm and compared its dynamics with Metropolis Monte Carlo, with a properly chosen temperature that depends on the mini-batch size.</li>
<li>results: The paper found that the dynamics of the SGD-like algorithm are very similar to those of Metropolis Monte Carlo, both at equilibrium and in the out-of-equilibrium regime, despite the two algorithms having fundamental differences. This equivalence allows for the use of results about the performances and limits of Monte Carlo algorithms to optimize the mini-batch size in the SGD-like algorithm and make it efficient in recovering the signal in hard inference problems.<details>
<summary>Abstract</summary>
Is Stochastic Gradient Descent (SGD) substantially different from Glauber dynamics? This is a fundamental question at the time of understanding the most used training algorithm in the field of Machine Learning, but it received no answer until now. Here we show that in discrete optimization and inference problems, the dynamics of an SGD-like algorithm resemble very closely that of Metropolis Monte Carlo with a properly chosen temperature, which depends on the mini-batch size. This quantitative matching holds both at equilibrium and in the out-of-equilibrium regime, despite the two algorithms having fundamental differences (e.g.\ SGD does not satisfy detailed balance). Such equivalence allows us to use results about performances and limits of Monte Carlo algorithms to optimize the mini-batch size in the SGD-like algorithm and make it efficient at recovering the signal in hard inference problems.
</details>
<details>
<summary>摘要</summary>
是 Stochastic Gradient Descent (SGD) 和 Glauber dynamics 有重要区别吗？这是机器学习领域内最常用的训练算法问题，但既没有得到答案。我们现在显示，在离散优化和推理问题中，SGD-like algorithm 的动力学与 Metropolis Monte Carlo 的温度适当选择有关，这个温度与 mini-batch 大小有关。这种量化匹配在平衡状态和非平衡状态下都存在，尽管两种算法有深刻的不同（例如，SGD 不满足详细平衡）。这种等价性使我们能够使用 Monte Carlo 算法的性能和限制来优化 mini-batch 大小在 SGD-like algorithm 中，以便在困难推理问题中效率地恢复信号。Note: "SGD-like algorithm" refers to a stochastic gradient descent algorithm with a mini-batch size that is chosen adaptively, rather than a fixed size.
</details></li>
</ul>
<hr>
<h2 id="Neural-Koopman-prior-for-data-assimilation"><a href="#Neural-Koopman-prior-for-data-assimilation" class="headerlink" title="Neural Koopman prior for data assimilation"></a>Neural Koopman prior for data assimilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05317">http://arxiv.org/abs/2309.05317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthony-frion/sentinel2ts">https://github.com/anthony-frion/sentinel2ts</a></li>
<li>paper_authors: Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon, Abdeldjalil Aïssa El Bey</li>
<li>for: 这篇论文是为了描述如何使用神经网络模型来描述动态系统的趋势和特征。</li>
<li>methods: 该论文使用了 Koopman 算子理论来嵌入动态系统在隐藏空间中，使其动态可以用线性方式描述。另外，该论文还介绍了一种用于长期连续重建的训练方法，以及如何使用已经训练的动态模型作为Variational数据整合的先验知识。</li>
<li>results: 该论文的结果表明，使用神经网络模型可以在困难的时间序列数据上实现长期连续重建，并且可以通过自动批处理和数据整合来提高模型的准确性和稳定性。此外，该论文还示出了使用已经训练的动态模型作为Variational数据整合的先验知识可以有效地实现时间序列 interpolate 和预测。<details>
<summary>Abstract</summary>
With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show the promising use of trained dynamical models as priors for variational data assimilation techniques, with applications to e.g. time series interpolation and forecasting.
</details>
<details>
<summary>摘要</summary>
Note:* "sequential data" is translated as "时序数据" (shíxìng shùxī)* "dynamical model" is translated as "动态模型" (dòngtǐ módel)* "Koopman operator theory" is translated as "库普曼运算理论" (kùpènmàn yùngcéng lǐlùn)* "latent spaces" is translated as "隐藏空间" (hìnxiǎn kōngjī)* "long-term continuous reconstruction" is translated as "长期连续重建" (chángjì liánxù zhòngjiàn)* "self-supervised learning" is translated as "自我指导学习" (zìwǒ zhǐguī xuéxí)
</details></li>
</ul>
<hr>
<h2 id="Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data"><a href="#Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data" class="headerlink" title="Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data"></a>Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05305">http://arxiv.org/abs/2309.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen<br>for:FC-STGNN is proposed to effectively model the Spatial-Temporal (ST) dependencies in Multivariate Time-Series (MTS) data, which is crucial in various application fields.methods:FC-STGNN includes two key components: (1) FC graph construction, which connects sensors across all timestamps based on their temporal distances, and (2) FC graph convolution with a moving-pooling GNN layer to capture the ST dependencies.results:Compared to existing State-of-the-Art (SOTA) methods, FC-STGNN shows effectiveness in capturing the ST dependencies in MTS data, as demonstrated by extensive experiments on multiple MTS datasets.<details>
<summary>Abstract</summary>
Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolution. For graph construction, we design a decay graph to connect sensors across all timestamps based on their temporal distances, enabling us to fully model the ST dependencies by considering the correlations between DEDT. Further, we devise FC graph convolution with a moving-pooling GNN layer to effectively capture the ST dependencies for learning effective representations. Extensive experiments show the effectiveness of FC-STGNN on multiple MTS datasets compared to SOTA methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）数据在各种应用领域中扮演着关键性的角色。MTS数据具有顺序和多源（多感器）性质，因此自然地具有时空依赖关系，包括时间戳和时间戳之间的时间相关性，以及每个时间戳中的感器之间的空间相关性。为了有效利用这些信息，图神经网络（GNN）在各种应用中广泛应用。然而，现有的方法通常分别捕捉时间相关性和空间相关性，而忽略了不同感器之间的不同时间戳之间的相关性（DEDT）。这会限制现有的GNNs从学习有效表示。为了解决这一限制，我们提出了一种新的方法called Fully-Connected Spatial-Temporal Graph Neural Network（FC-STGNN），其包括以下两个关键组成部分：FC图构建和FC图卷积。FC图构建中，我们设计了衰减图来连接不同时间戳之间的感器，基于他们的时间距离，以便全面模型时空依赖关系，并考虑不同感器之间的DEDT相关性。此外，我们开发了FC图卷积层，其中包括一个移动pooling GNN层，以有效地捕捉时空依赖关系，以便学习有效表示。我们对多个MTS数据集进行了广泛的实验，并证明了FC-STGNN在相对于状态艺术方法的情况下的效果。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization"><a href="#Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization" class="headerlink" title="Discrete Denoising Diffusion Approach to Integer Factorization"></a>Discrete Denoising Diffusion Approach to Integer Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05295">http://arxiv.org/abs/2309.05295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlisfre/diffusion-factorization">https://github.com/karlisfre/diffusion-factorization</a></li>
<li>paper_authors: Karlis Freivalds, Emils Ozolins, Guntis Barzdins</li>
<li>for: 本研究是 investigate whether deep neural networks can facilitate faster integer factorization.</li>
<li>methods: 本文提出了一种使用深度神经网络和粗粒推 diffusion 的方法，通过 iteratively correcting errors in a partially-correct solution 来实现因数分解。</li>
<li>results: 该方法可以对数字的因数进行分解，并且可以在56比特长度的数字上进行因数分解。同时，我们的分析表明，在训练投入增长时，需要的抽样步骤数量会下降 exponential，从而抵消因数长度的增长。<details>
<summary>Abstract</summary>
Integer factorization is a famous computational problem unknown whether being solvable in the polynomial time. With the rise of deep neural networks, it is interesting whether they can facilitate faster factorization. We present an approach to factorization utilizing deep neural networks and discrete denoising diffusion that works by iteratively correcting errors in a partially-correct solution. To this end, we develop a new seq2seq neural network architecture, employ relaxed categorical distribution and adapt the reverse diffusion process to cope better with inaccuracies in the denoising step. The approach is able to find factors for integers of up to 56 bits long. Our analysis indicates that investment in training leads to an exponential decrease of sampling steps required at inference to achieve a given success rate, thus counteracting an exponential run-time increase depending on the bit-length.
</details>
<details>
<summary>摘要</summary>
整数因数分解是一个著名的计算问题，不确定它是否可以在多项式时间内解决。随着深度神经网络的出现，是否它们可以促进更快的因数分解吸引了关注。我们提出了一种利用深度神经网络和离散杂变滤波器进行因数分解的方法，通过逐步纠正错误来实现。为此，我们开发了一种新的seq2seq神经网络架构，使用宽松的分类分布和逆扩散过程来更好地处理杂变步骤中的不准确。这种方法可以为整数的长度达56位的因数分解。我们的分析表明，在训练投入量增加时，推理步骤所需的抽样步骤数随着扩展幂率下降，从而将执行时间减少到一定程度。
</details></li>
</ul>
<hr>
<h2 id="The-fine-print-on-tempered-posteriors"><a href="#The-fine-print-on-tempered-posteriors" class="headerlink" title="The fine print on tempered posteriors"></a>The fine print on tempered posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05292">http://arxiv.org/abs/2309.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Pitas, Julyan Arbel</li>
<li>for:  investigate the tempered posteriors and their relationship with test accuracy and calibration</li>
<li>methods:  use empirical studies and PAC-Bayesian analysis to explore the optimal temperature parameter for Bayesian models</li>
<li>results:  show that the coldest temperature is often optimal for test accuracy, and that targeting Frequentist metrics can lead to degradation in test accuracy, and that the temperature parameter cannot be seen as simply fixing a misspecified prior or likelihood.Here is the same information in Traditional Chinese:</li>
<li>for:  investigate the 减冷 posterior和它对测试准确和调整的关系</li>
<li>methods:  use empirical studies和PAC-Bayesian analysis来探索 Bayesian models 中的温度参数最佳化</li>
<li>results:  show that the 最冷温度通常是测试准确最佳，并且对测试准确进行调整可能会导致测试准确下降，并且温度参数不能被简单地看作是错误的假设或概率 Distribution 的修正。<details>
<summary>Abstract</summary>
We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.
</details>
<details>
<summary>摘要</summary>
我们进行了详细的探讨模拟后采用的抑制 posterior 的研究，并发现了一些重要且前所未讲的点。与前一些结果不同，我们首先表明了，在现实模型和数据集下，精确控制 Laplace 近似 posterior 的情况下， Stochasticity 不一定提高测试准确率。最低温度通常是最佳的。一 might think Bayesian 模型具有一定的随机性可以至少获得准确性的改进。但我们通过实验表明，当获得了这些改进时，这来的代价是测试准确率的下降。然后我们讨论了如何使用 Bayesian 模型来目标 Frequentist 度量，并通过 PAC-Bayesian 分析表明，温度参数 $\lambda$ 不能被简单地看作是修复错误的先前或 posterior。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Finite-Initialization-for-Tensorized-Neural-Networks"><a href="#Efficient-Finite-Initialization-for-Tensorized-Neural-Networks" class="headerlink" title="Efficient Finite Initialization for Tensorized Neural Networks"></a>Efficient Finite Initialization for Tensorized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06577">http://arxiv.org/abs/2309.06577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i3bquantumteam/q4real">https://github.com/i3bquantumteam/q4real</a></li>
<li>paper_authors: Alejandro Mata Ali, Iñigo Perez Delgado, Marina Ristol Roura, Aitor Moreno Fdez. de Leceta</li>
<li>For: The paper is written for layers of tensorized neural networks, specifically those with a high number of nodes and a connection to the input or output of all or most of the nodes.* Methods: The paper proposes a novel method for initializing these layers, which involves using the Frobenius norm of the layer in an iterative partial form. This method is efficient to compute and can be applied to different layers.* Results: The paper shows the performance of the proposed method on various layers and demonstrates its effectiveness in avoiding the explosion of the parameters of the matrix it emulates. The method is available in a Python function in the i3BQuantum repository, which can be run on an arbitrary layer.<details>
<summary>Abstract</summary>
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的层初始化方法，以避免tensorized神经网络中层的参数爆炸。这种方法适用于具有较高节点数的层，其中每个节点都与输入或输出相连。我们的方法的核心是使用层的 Frobenius  нор的iterative partial form，使其必须是有限的并在某个范围内。这个norm是可以高效计算的，并且可以在大多数情况下完全或部分计算。我们对不同的层进行了应用，并检查了其性能。我们还创建了一个Python函数来实现这种方法，可以应用于任意层，可以在 GitHub 上找到：https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb。
</details></li>
</ul>
<hr>
<h2 id="Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving"><a href="#Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving" class="headerlink" title="Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"></a>Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05282">http://arxiv.org/abs/2309.05282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Keysan, Andreas Look, Eitan Kosman, Gonca Gürsun, Jörg Wagner, Yu Yao, Barbara Rakitsch</li>
<li>for: 这项研究旨在提出一种新的文本基于表示方法，用于描述交通场景，并利用预训练语言编码器进行处理。</li>
<li>methods: 该研究使用文本基于表示法，与经典的矩阵图像表示法相结合，从而生成描述性的场景嵌入。</li>
<li>results: 研究表明，文本基于表示法和矩阵图像基于表示法的 JOINT 编码器，在 nuScenes 数据集上的预测比基eline 做出了显著改进，而且在减少折损率方面也有较好的表现。<details>
<summary>Abstract</summary>
In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.   First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
</details>
<details>
<summary>摘要</summary>
自主驾驶任务中，场景理解是Predicting the future behavior of surrounding traffic participants的首先步骤。然而，如何表示给定场景和提取其特征仍是开放的研究问题。在本研究中，我们提议一种文本基于表示交通场景的方法，并使用预训练语言编码器处理。首先，我们显示文本基于表示、与经典化的图像表示结合使得场景嵌入得到描述性的表示。其次，我们在nuScenes数据集上进行了比较，并显示与基eline的预测具有显著的改善。最后，我们在ablation研究中表明，将文本和图像的编码器结合使用，比单独使用图像或文本编码器更高效，确认了两种表示具有不同的优势。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning"><a href="#Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning" class="headerlink" title="Class-Incremental Grouping Network for Continual Audio-Visual Learning"></a>Class-Incremental Grouping Network for Continual Audio-Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05281">http://arxiv.org/abs/2309.05281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stonemo/cign">https://github.com/stonemo/cign</a></li>
<li>paper_authors: Shentong Mo, Weiguo Pian, Yapeng Tian</li>
<li>for: 这个研究旨在提出一个可以进行类别增量学习的数据流过掌握模型，并且可以在多个类别和多个数据类型（音频和影像）之间进行跨modal的学习。</li>
<li>methods: 这个模型使用了一个新的类别增量Grouping Network（CIGN），可以学习Category-wise semantic features来进行类别增量学习。CIGN使用可学习的音频-影像类别标签和音频-影像分组，以便不断地累累数据类型的类别-aware特征。此外，CIGN还使用了类别标签激发和不断分组来防止忘记，从而提高模型的捕捉数据类型的敏感性。</li>
<li>results: 我们对VGGSound-Instruments、VGGSound-100和VGG-Sound Sources的实验结果显示，CIGN可以在类别增量学习中实现州���进步的表现。<details>
<summary>Abstract</summary>
Continual learning is a challenging problem in which models need to be trained on non-stationary data across sequential tasks for class-incremental learning. While previous methods have focused on using either regularization or rehearsal-based frameworks to alleviate catastrophic forgetting in image classification, they are limited to a single modality and cannot learn compact class-aware cross-modal representations for continual audio-visual learning. To address this gap, we propose a novel class-incremental grouping network (CIGN) that can learn category-wise semantic features to achieve continual audio-visual learning. Our CIGN leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features. Additionally, it utilizes class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks, thereby improving the model's ability to capture discriminative audio-visual categories. We conduct extensive experiments on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks. Our experimental results demonstrate that the CIGN achieves state-of-the-art audio-visual class-incremental learning performance. Code is available at https://github.com/stoneMo/CIGN.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>逐渐学习是一个挑战性的问题，在该问题中，模型需要在不同任务上逐渐学习非站ARY数据。而之前的方法往往是使用Regularization或者熵练习框架来缓解忘却现象，但这些方法受限于单一模式，无法学习整体的混合模式，因此无法实现逐渐audio-visual学习。为了解决这个差距，我们提出了一种新的逐渐分组网络（CIGN），该网络可以学习类别wise的semantic特征，以实现逐渐audio-visual学习。我们的CIGN利用可学习的音频视频类别符号和音频视频分组来不断归纳类别相关的特征。此外，它还利用类别符号熔化和不断分组来防止忘却之前学习的参数，从而提高模型的捕捉混合类别的能力。我们在VGGSound-Instruments、VGGSound-100和VGG-Sound Sources的benchmark上进行了广泛的实验。我们的实验结果表明，CIGN可以在逐渐audio-visual学习中实现state-of-the-art的性能。代码可以在https://github.com/stoneMo/CIGN上找到。
</details></li>
</ul>
<hr>
<h2 id="Beamforming-in-Wireless-Coded-Caching-Systems"><a href="#Beamforming-in-Wireless-Coded-Caching-Systems" class="headerlink" title="Beamforming in Wireless Coded-Caching Systems"></a>Beamforming in Wireless Coded-Caching Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05276">http://arxiv.org/abs/2309.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Madhusudan, Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson</li>
<li>for: 这个论文的目的是提出一种无线传输网络架构，以利用射频和编码缓存策略来解决访问网络带宽问题。</li>
<li>methods: 该论文使用了一种基于遗传算法的射频优化策略，以提高编码缓存系统的效率。</li>
<li>results: 研究发现，该 JOINT 设计可以增加多播机会数、减少干扰和下行带宽负荷。 Comparative analysis also shows that the proposed approach outperforms traditional, uncoded caching schemes. Additionally, the study finds that proper beamforming is useful in enhancing the effectiveness of the coded-caching technique, resulting in significant reduction in peak backhaul traffic.<details>
<summary>Abstract</summary>
Increased capacity in the access network poses capacity challenges on the transport network due to the aggregated traffic. However, there are spatial and time correlation in the user data demands that could potentially be utilized. To that end, we investigate a wireless transport network architecture that integrates beamforming and coded-caching strategies. Especially, our proposed design entails a server with multiple antennas that broadcasts content to cache nodes responsible for serving users. Traditional caching methods face the limitation of relying on the individual memory with additional overhead. Hence, we develop an efficient genetic algorithm-based scheme for beam optimization in the coded-caching system. By exploiting the advantages of beamforming and coded-caching, the architecture achieves gains in terms of multicast opportunities, interference mitigation, and reduced peak backhaul traffic. A comparative analysis of this joint design with traditional, un-coded caching schemes is also conducted to assess the benefits of the proposed approach. Additionally, we examine the impact of various buffering and decoding methods on the performance of the coded-caching scheme. Our findings suggest that proper beamforming is useful in enhancing the effectiveness of the coded-caching technique, resulting in significant reduction in peak backhaul traffic.
</details>
<details>
<summary>摘要</summary>
增加了Access网络的容量会导致传输网络的压力增加，但是用户数据需求存在空间和时间相关性，这些相关性可能可以利用。为此，我们研究了一种具有广播和编码缓存策略的无线传输网络架构。具体来说，我们的提议包括一个有多个天线的服务器，通过广播内容到缓存节点来服务用户。传统的缓存方法受到各个缓存器的独立存储器的限制，此外还增加了额外的开销。因此，我们开发了一种基于遗传算法的 beam优化方案。通过利用广播和编码缓存的优势，该架构实现了多播机会增加、干扰 Mitigation 和传输峰值下行带宽的改善。我们对这种共同设计与传统、未编码缓存方案进行比较分析，以评估提议的优势。此外，我们还研究了缓存 scheme 的缓存和解码方法对性能的影响。我们的发现表明，正确的 beamforming 可以提高编码缓存技术的效iveness，从而实现显著减少传输峰值下行带宽。
</details></li>
</ul>
<hr>
<h2 id="EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction"><a href="#EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction" class="headerlink" title="EANet: Expert Attention Network for Online Trajectory Prediction"></a>EANet: Expert Attention Network for Online Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05683">http://arxiv.org/abs/2309.05683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Yao, Tianlu Mao, Min Shi, Jingkai Sun, Zhaoqi Wang</li>
<li>for: 这个研究旨在解决自动驾驶中的路径预测问题，特别是在突然变化的情况下，传统方法的预测精度很低，并且无法实时更新模型。</li>
<li>methods: 我们提出了专家注意力网络（Expert Attention Network），一个完整的在线学习框架，使用专家注意力来调整网络层次的重要性，以避免模型因为梯度问题而更新太慢，并且快速学习新情况知识以恢复预测精度。</li>
<li>results: 我们的方法可以快速降低预测错误，并且在突然变化的情况下保持预测精度在state-of-the-art水平。实验结果显示，传统方法受到梯度问题的影响，而我们的方法可以快速适应新情况，并且在短时间内恢复预测精度。<details>
<summary>Abstract</summary>
Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. The experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.
</details>
<details>
<summary>摘要</summary>
干线预测在自动驾驶中扮演着关键的角色。现有的主流研究和连续学习基于方法都需要训练完整的数据集，导致enario的快速变化时预测精度低下和模型更新缓慢。这些方法是否可以在实时中预测和使用数据实例立即更新模型（即在线学习设置）是一个问题。另外，数据实例流中的梯度爆炸或消失问题也需要解决。 Drawing inspiration from Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different network layer depths, avoiding the model updated slowly due to gradient problems and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. Experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.Here's the text with some additional information about the Simplified Chinese translation:The Simplified Chinese translation is written in a more formal and conservative style, which is common in academic writing. The vocabulary and grammar used are also more standardized and consistent with the language used in academic papers.In the translation, we tried to preserve the original meaning and structure of the text as much as possible, while also taking into account the nuances of the Simplified Chinese language. For example, we used the phrase "干线预测" (trajectory prediction) instead of "路径预测" (path prediction) to emphasize the importance of predicting the trajectory of the vehicle. We also used the phrase "数据实例流" (data instance stream) to refer to the stream of data used for training the model, which is a more common way of expressing this concept in Simplified Chinese.Overall, we hope that the translation will be helpful for readers who are more familiar with Simplified Chinese and will allow them to better understand the ideas and techniques presented in the original text.
</details></li>
</ul>
<hr>
<h2 id="CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling"><a href="#CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling" class="headerlink" title="CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling"></a>CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05270">http://arxiv.org/abs/2309.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsin Ali, Kandukuri Sai Teja, Neeharika Gupta, Parth Patwa, Anubhab Chatterjee, Vinija Jain, Aman Chadha, Amitava Das</li>
<li>for: 这个论文主要针对 Multilingual societies 中的 Code-Mixing (CM) 问题，即多种语言杂mixing在一起的社会现象。</li>
<li>methods: 作者采用了 Neural Language Models (NLMs)  LIKE transformers 来解决这个问题，并特别强调在 Switching Points (SPs) 处进行模型化。</li>
<li>results: 作者通过实验表明，使用 rotatory positional encoding 和 switching point information 可以更好地训练 Code-Mixed Language Models (LMs)，并在 sentiment analysis 和 machine translation 两个任务上达到了最佳效果。<details>
<summary>Abstract</summary>
The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.   We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details>
<details>
<summary>摘要</summary>
mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results. We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details></li>
</ul>
<hr>
<h2 id="UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs"><a href="#UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs" class="headerlink" title="UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs"></a>UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05269">http://arxiv.org/abs/2309.05269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yide-qiu/unikg">https://github.com/yide-qiu/unikg</a></li>
<li>paper_authors: Yide Qiu, Shaoxiang Ling, Tong Zhang, Bo Huang, Zhen Cui</li>
<li>for: 这篇论文是为了探讨大规模不规则数据中的知识挖掘和多类节点图表示学问题而写的。</li>
<li>methods: 这篇论文使用了semantic alignment策略和提档扩展 propagation模块（APM）来解决大规模多类节点图中信息传递和多Attribute association挖掘问题。</li>
<li>results: 在UniKG dataset上，这些方法可以高效地传递信息和挖掘多Attribute association，并且在节点类划 зада上达到了比较高的分类精度。<details>
<summary>Abstract</summary>
Irregular data in real-world are usually organized as heterogeneous graphs (HGs) consisting of multiple types of nodes and edges. To explore useful knowledge from real-world data, both the large-scale encyclopedic HG datasets and corresponding effective learning methods are crucial, but haven't been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken, including (i) the semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field; (ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset, and evaluate multiple baseline methods which are constructed by embedding our APM into large-scale homogenous graph learning methods. Our UniKG dataset and the baseline codes have been released at https://github.com/Yide-Qiu/UniKG.
</details>
<details>
<summary>摘要</summary>
real-world irregular data usually organizes as heterogeneous graphs (HGs) with multiple types of nodes and edges. To explore useful knowledge from real-world data, both large-scale encyclopedic HG datasets and effective learning methods are crucial, but haven't been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken:(i) semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field;(ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset and evaluate multiple baseline methods constructed by embedding our APM into large-scale homogeneous graph learning methods. Our UniKG dataset and the baseline codes have been released at <https://github.com/Yide-Qiu/UniKG>.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Bias-Detection-in-College-Student-Newspapers"><a href="#Unsupervised-Bias-Detection-in-College-Student-Newspapers" class="headerlink" title="Unsupervised Bias Detection in College Student Newspapers"></a>Unsupervised Bias Detection in College Student Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06557">http://arxiv.org/abs/2309.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam M. Lehavi, William McCormack, Noah Kornfeld, Solomon Glazer</li>
<li>for: 本文提出了一个可以自动抽取偏见的报纸archivepipeline，以Minimal human influence的方式进行抽取和检测偏见。</li>
<li>methods: 本文引入了一种自动化工具无法抓取数据的框架，并生成了14名学生的14篇论文，共23154个数据项。此外，本文还使用了一种基于大语言模型的 summarization 技术来计算偏见。</li>
<li>results: 本文的结果表明，该方法可以对政治敏感词和控制词进行计算，并且可以通过比较大语言模型的概要和原始文章来计算偏见。这种方法比重建ruction bias更加准确，并且需要 menos标注数据。<details>
<summary>Abstract</summary>
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance"><a href="#Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance" class="headerlink" title="Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance"></a>Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05260">http://arxiv.org/abs/2309.05260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchao Jian, Feng Ji, Wee Peng Tay</li>
<li>for: 本文研究了稀疏图序列的收敛性，并提出了一种基于泛函图和延展距离的方法来描述这种收敛性。</li>
<li>methods: 本文使用了通用图和延展距离来描述稀疏图序列的收敛性，并使用了一种随机图生成过程来模拟增长的稀疏图。</li>
<li>results: 本文证明了稀疏图序列的 adjacency 矩阵 eigenvalues 的收敛性，并进行了实验验证。 results suggest the possibility of transfer learning between sparse graphs.<details>
<summary>Abstract</summary>
Graphons have traditionally served as limit objects for dense graph sequences, with the cut distance serving as the metric for convergence. However, sparse graph sequences converge to the trivial graphon under the conventional definition of cut distance, which make this framework inadequate for many practical applications. In this paper, we utilize the concepts of generalized graphons and stretched cut distance to describe the convergence of sparse graph sequences. Specifically, we consider a random graph process generated from a generalized graphon. This random graph process converges to the generalized graphon in stretched cut distance. We use this random graph process to model the growing sparse graph, and prove the convergence of the adjacency matrices' eigenvalues. We supplement our findings with experimental validation. Our results indicate the possibility of transfer learning between sparse graphs.
</details>
<details>
<summary>摘要</summary>
GRAPHONS 传统上作为稠密图序列的限制对象，剪距作为 convergence 的度量。但是，稀疏图序列在传统定义下的剪距中 converge 到平凡图像，这使得这个框架无法满足许多实际应用中的需求。在这篇论文中，我们利用通用化的 GRAPHON 和延展剪距来描述稀疏图序列的 convergence。我们考虑一个基于通用化 GRAPHON 的随机图过程，该过程 converge 到通用化 GRAPHON 中的延展剪距。我们使用这个随机图过程来模拟增长的稀疏图，并证明连接矩阵的特征值的散射。我们的结果表明可以在稀疏图中进行特征值的传递学习。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction"><a href="#A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction" class="headerlink" title="A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction"></a>A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05259">http://arxiv.org/abs/2309.05259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohao Qu, Haoxuan Kuang, Jun Li, Linlin You</li>
<li>for: 这篇论文的目的是优化电动车充电空间的使用，以减轻智能交通系统的负载。</li>
<li>methods: 本篇论文提出了一种基于几何和时间注意力机制的特征提取方法，以及使用物理授业统一学习来将知识转移到预测模型中。</li>
<li>results: 试验结果显示，提出的方法（PAG）可以实现预测性和可解释性，并能够理解价格变化导致充电需求的适应变化。<details>
<summary>Abstract</summary>
Along with the proliferation of electric vehicles (EVs), optimizing the use of EV charging space can significantly alleviate the growing load on intelligent transportation systems. As the foundation to achieve such an optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed by using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations to correctly handle the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability in understanding the adaptive changes in charging demands caused by price fluctuations.
</details>
<details>
<summary>摘要</summary>
alongside the proliferation of electric vehicles (EVs), optimizing the use of EV charging space can significantly alleviate the growing load on intelligent transportation systems. As the foundation to achieve such an optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed by using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations to correctly handle the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability in understanding the adaptive changes in charging demands caused by price fluctuations.Here's the translation in Traditional Chinese:随着电动车（EV）的普及， ottimizzare l'utilizzo dell'EV充电空间可以有效缓解城市智能交通系统中的负载。为了 achieve such optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations in correctly handling the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability to understand the adaptive changes in charging demands caused by price fluctuations.
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effect-of-Pre-training-on-Time-Series-Classification"><a href="#Examining-the-Effect-of-Pre-training-on-Time-Series-Classification" class="headerlink" title="Examining the Effect of Pre-training on Time Series Classification"></a>Examining the Effect of Pre-training on Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05256">http://arxiv.org/abs/2309.05256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Pu, Shiwei Zhao, Ling Cheng, Yongzhu Chang, Runze Wu, Tangjie Lv, Rongsheng Zhang</li>
<li>for: 这项研究旨在探讨无监督预训练后练习的 paradigm在新模式下的效果。</li>
<li>methods: 研究人员对150个时间序列分类数据集进行了严格的检验，包括单变量时间序列（UTS）和多变量时间序列（MTS）benchmark。</li>
<li>results: 研究结果显示，预训练只有在模型适应数据较差时才能帮助改进优化过程。预训练不会在充分训练时间内产生常见化的效果，而且预训练只能加速模型适应数据的速度，不会提高总体性能。增加更多预训练数据不会提高总体性能，但可以强化预训练对原始数据量的优势，如更快的CONVERGENCE。模型结构在这种情况下扮演着更重要的角色。<details>
<summary>Abstract</summary>
Although the pre-training followed by fine-tuning paradigm is used extensively in many fields, there is still some controversy surrounding the impact of pre-training on the fine-tuning process. Currently, experimental findings based on text and image data lack consensus. To delve deeper into the unsupervised pre-training followed by fine-tuning paradigm, we have extended previous research to a new modality: time series. In this study, we conducted a thorough examination of 150 classification datasets derived from the Univariate Time Series (UTS) and Multivariate Time Series (MTS) benchmarks. Our analysis reveals several key conclusions. (i) Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well. (ii) Pre-training does not exhibit the effect of regularization when given sufficient training time. (iii) Pre-training can only speed up convergence if the model has sufficient ability to fit the data. (iv) Adding more pre-training data does not improve generalization, but it can strengthen the advantage of pre-training on the original data volume, such as faster convergence. (v) While both the pre-training task and the model structure determine the effectiveness of the paradigm on a given dataset, the model structure plays a more significant role.
</details>
<details>
<summary>摘要</summary>
尽管预训练后细化的方法在多个领域广泛应用，但是预训练对细化过程的影响仍存在一定的争议。现在，基于文本和图像数据的实验研究结果并未达成一致。为更深入地探讨无监督预训练后细化的方法，我们在新的模式上进行了扩展研究：时间序列。在这个研究中，我们对150个分类数据集进行了全面的分析，这些数据集来自于单变量时间序列（UTS）和多变量时间序列（MTS） benchmark。我们的分析发现了以下几点：（i）预训练只能帮助改善模型不适合数据的优化过程，而不是适合数据的模型。（ii）预训练不会在充足的训练时间下显示正则化效果。（iii）预训练只能快速 convergence的模型，如果模型具有足够的适应能力。（iv）增加更多的预训练数据不会提高通用性，但可以强化预训练在原始数据量上的优势，如更快的 convergence。（v）预训练任务和模型结构共同决定了预训练在给定数据集上的效果，但模型结构更加重要。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces"><a href="#A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces" class="headerlink" title="A quantum tug of war between randomness and symmetries on homogeneous spaces"></a>A quantum tug of war between randomness and symmetries on homogeneous spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05253">http://arxiv.org/abs/2309.05253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Arvind, Kishor Bharti, Jun Yong Khoo, Dax Enshan Koh, Jian Feng Kong</li>
<li>for: 研究量子信息中Symmetry和Randomness的关系。</li>
<li>methods: 采用几何方法，考虑状态为$H$-相似的集合，并引入$\mathbb{U}&#x2F;H$上的哈恩推测，以characterize真实的随机性。</li>
<li>results: 提出了一种基于几何空间的方法来 caracterize Symmetry in quantum information，并研究了 approximate true randomness和pseudorandomness。最后，通过 praktical demonstration，研究量子机器学习模型在几何空间中的表达性。<details>
<summary>Abstract</summary>
We explore the interplay between symmetry and randomness in quantum information. Adopting a geometric approach, we consider states as $H$-equivalent if related by a symmetry transformation characterized by the group $H$. We then introduce the Haar measure on the homogeneous space $\mathbb{U}/H$, characterizing true randomness for $H$-equivalent systems. While this mathematical machinery is well-studied by mathematicians, it has seen limited application in quantum information: we believe our work to be the first instance of utilizing homogeneous spaces to characterize symmetry in quantum information. This is followed by a discussion of approximations of true randomness, commencing with $t$-wise independent approximations and defining $t$-designs on $\mathbb{U}/H$ and $H$-equivalent states. Transitioning further, we explore pseudorandomness, defining pseudorandom unitaries and states within homogeneous spaces. Finally, as a practical demonstration of our findings, we study the expressibility of quantum machine learning ansatze in homogeneous spaces. Our work provides a fresh perspective on the relationship between randomness and symmetry in the quantum world.
</details>
<details>
<summary>摘要</summary>
我们探索量子信息中对偶和随机性的交互关系。我们采用几何方法，将状态视为$H$-相似的情况，其中$H$是一个群。然后，我们引入$\mathbb{U}/H$上的同调度量，用于描述真正的随机性。这种数学工具已经由数学家们广泛研究，但在量子信息领域却很少应用。我们认为我们的工作是量子信息领域中首次利用同调空间来描述对称性的。接着，我们讨论了真正随机性的近似，包括$t$-wise独立的近似和$\mathbb{U}/H$和$H$-相似状态上的$t$-设计。在继续探索中，我们研究了假随机性，定义了在同调空间中的假随机变换和状态。最后，我们通过实际示例，研究了基于同调空间的量子机器学习模型的表达性。我们的工作为量子世界中对偶和随机性之间的关系提供了一种新的视角。
</details></li>
</ul>
<hr>
<h2 id="SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block"><a href="#SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block" class="headerlink" title="SparseSwin: Swin Transformer with Sparse Transformer Block"></a>SparseSwin: Swin Transformer with Sparse Transformer Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05224">http://arxiv.org/abs/2309.05224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krisnapinasthika/sparseswin">https://github.com/krisnapinasthika/sparseswin</a></li>
<li>paper_authors: Krisna Pinasthika, Blessius Sheldo Putra Laksono, Riyandi Banovbi Putera Irsal, Syifa Hukma Shabiyya, Novanto Yudistira</li>
<li>for: 这个论文目的是提高计算机视觉模型的效率，使其更加高效。</li>
<li>methods: 这篇论文使用了一种名为Sparse Transformer（SparTa）块，这是一种带有缺省tokenConverter的转换器块，可以减少计算token的数量。它还使用了Swin T体系结构（SparseSwin），该体系结构可以减少输入的大小并且减少初始的计算token数量。</li>
<li>results: 提交的模型（SparseSwin）在图像分类任务上达到了86.96%、97.43%和85.35%的准确率在ImageNet100、CIFAR10和CIFAR100 datasets上。尽管它具有较少的参数，但结果表明使用缺省tokenConverter可以优化转换器的使用，提高其性能。<details>
<summary>Abstract</summary>
Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.
</details>
<details>
<summary>摘要</summary>
（简化中文）计算机视觉研究的进步使得转换器体系成为计算机视觉任务的状态码。转换器体系的一个已知缺点是高数量的参数，这可能导致更复杂和不fficient的算法。本文的目标是减少参数数量，从而使转换器更加高效。我们提出了 sparse transformer（SparTa）块，它是一种增强了转换器块的模型，并添加了一个稀疏的标记转换器，以减少使用的标记数量。我们使用SparTa块在Swin T体系（SparseSwin）中，以利用Swin的下采样能力和减少初始标记数量。我们提出的SparseSwin模型在图像分类任务中的准确率为86.96%, 97.43%, 和85.35%，分别在ImageNet100、CIFAR10和CIFAR100数据集上。尽管它具有更少的参数，但结果表明了使用稀疏的标记转换器和有限数量的标记来优化转换器，并提高其性能的潜力。
</details></li>
</ul>
<hr>
<h2 id="Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer"><a href="#Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer" class="headerlink" title="Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?"></a>Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06574">http://arxiv.org/abs/2309.06574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingsonglv/CFG">https://github.com/jingsonglv/CFG</a></li>
<li>paper_authors: Jingsong Lv, Hongyang Chen, Yao Qi, Lei Yu</li>
<li>for: 本研究旨在提出两种本地图像特征用于缺失链接预测任务中的 ogbl-citation2。</li>
<li>methods: 我们提出了圆形特征，它们是基于圆形朋友圈的概念。我们还详细介绍了计算这些特征的方法。</li>
<li>results: 我们实验结果表明，基于 SIEG 网络的圆形特征意识graph transformer（CFG）模型在 ogbl-citation2 数据集上达到了状态率。<details>
<summary>Abstract</summary>
In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了两种本地图像特征 для缺失链接预测任务中的ogbl-citation2。我们定义了这些特征为圈feature，它们来自圈子的概念。我们提出了计算这些特征的详细计算公式。首先，我们定义了第一个圈特征为修改的摆动，它来自于分合图。其次，我们定义了第二个圈特征为桥，它表示两个节点之间的圈子之间的重要性。此外，我们首先提出了这些特征作为偏好，以便通过改进图自注意机制来提高图自注意机制。我们实现了基于SIEG网络的圈特征意识graph transformer（CFG）模型，该模型使用双塔结构来捕捉全局和本地结构特征。实验结果表明，CFG在dataset ogbl-citation2上达到了状态艺术性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout"><a href="#Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout" class="headerlink" title="Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout"></a>Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05213">http://arxiv.org/abs/2309.05213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Guo, Warren Richard Morningstar, Raviteja Vemulapalli, Karan Singhal, Vishal M. Patel, Philip Andrew Mansfield</li>
<li>for: 这篇论文旨在解决联合学习在边缘设备上训练大型机器学习模型时的问题，具体来说是降低每个客户端的内存、计算和通信成本，以便在Edge设备上训练更大的模型。</li>
<li>methods: 该论文提出了一种简单 yet effective的策略，即联合层wise学习，使得每个客户端只需要训练一层，从而大幅降低客户端的内存、计算和通信成本。此外，论文还提出了一种补充技术，即联合深度随机Dropout，可以进一步降低资源使用。</li>
<li>results: 论文表明，通过结合这两种技术，可以在Edge设备上训练更大的模型，并且与传统联合自动学习的性能相似。具体来说，在联合自我监督学习中，训练内存使用量降低了5倍或更多，而下游任务的性能与传统联合自动学习的性能相似。<details>
<summary>Abstract</summary>
Large machine learning models trained on diverse data have recently seen unprecedented success. Federated learning enables training on private data that may otherwise be inaccessible, such as domain-specific datasets decentralized across many clients. However, federated learning can be difficult to scale to large models when clients have limited resources. This challenge often results in a trade-off between model size and access to diverse data. To mitigate this issue and facilitate training of large models on edge devices, we introduce a simple yet effective strategy, Federated Layer-wise Learning, to simultaneously reduce per-client memory, computation, and communication costs. Clients train just a single layer each round, reducing resource costs considerably with minimal performance degradation. We also introduce Federated Depth Dropout, a complementary technique that randomly drops frozen layers during training, to further reduce resource usage. Coupling these two techniques enables us to effectively train significantly larger models on edge devices. Specifically, we reduce training memory usage by 5x or more in federated self-supervised representation learning and demonstrate that performance in downstream tasks is comparable to conventional federated self-supervised learning.
</details>
<details>
<summary>摘要</summary>
大型机器学习模型在各种数据上进行训练已经得到了历史上无 precedent的成功。联邦学习可以训练在私有数据上，这些数据可能elsewhere decentralized across many clients。然而，联邦学习可能难以扩展到大型模型，因为客户端的资源有限。这种挑战通常导致模型大小和数据多样性之间的交易。为了缓解这个问题并在边缘设备上训练大型模型，我们提出了一个简单 yet effective的策略：联邦层次学习。在每个回合中，客户端只需要训练一个层，这将大幅降低客户端的内存、计算和通信成本。此外，我们还引入了联邦层次随机Dropout，这是在训练过程中随机Drop frozen层的技术。这两种技术的结合可以有效地在边缘设备上训练较大的模型。具体来说，我们可以在联邦自然学习中降低训练内存使用量，并证明在下游任务中表现与传统联邦自然学习相似。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification"><a href="#Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification" class="headerlink" title="Graph Contextual Contrasting for Multivariate Time Series Classification"></a>Graph Contextual Contrasting for Multivariate Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05202">http://arxiv.org/abs/2309.05202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen</li>
<li>for: 本研究旨在提高多变量时间序列（MTS）分类 tasks 的表现，通过保持多感器数据的空间一致性和时间一致性。</li>
<li>methods: 我们提出了图结构增强（Graph Contextual Contrasting，GCC），包括节点增强、边增强和图增强，以保持感器稳定性和相关性。我们还引入了多窗口时间增强，以确保每感器的时间一致性。</li>
<li>results: 我们的GCC方法在多个MTS分类任务上达到了现状最佳表现。<details>
<summary>Abstract</summary>
Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph Contextual Contrasting (GCC) for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by graph contrasting with both node- and graph-level contrasting to extract robust sensor- and global-level features. We further introduce multi-window temporal contrasting to ensure temporal consistency in the data for each sensor. Extensive experiments demonstrate that our proposed GCC achieves state-of-the-art performance on various MTS classification tasks.
</details>
<details>
<summary>摘要</summary>
contrastive learning，作为一种自助学习 paradigm，在多变量时间序列（MTS）分类中变得流行。它确保不同视图中的无标样本之间的一致性，然后学习这些样本的有效表示。现有的对比学习方法主要关注实现时间一致性，通过时间扩展和对比技术来保持时间特征的稳定性，以适应MTS数据。然而，它们忽略了空间一致性，即感知器的稳定性和相关性。由于MTS数据通常来自多个感知器，保证空间一致性是对MTS数据的总性表现的关键。因此，我们提出图结构启发对比（GCC）来保证MTS数据的空间一致性。具体来说，我们提出图ixel augmentation和边augmentation来保持感知器的稳定性和相关性，然后进行图像对比，包括节点对比和图像对比，以提取感知器和全局级别的特征。我们还引入多窗口时间对比来确保每个感知器的时间一致性。我们的GCC方法在多种MTS分类任务上实现了状态的杰出表现。
</details></li>
</ul>
<hr>
<h2 id="CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization"><a href="#CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization" class="headerlink" title="CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization"></a>CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05200">http://arxiv.org/abs/2309.05200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shepherd-gregory/bkio-exploration">https://github.com/shepherd-gregory/bkio-exploration</a></li>
<li>paper_authors: Yang Xu, Ronghao Zheng, Senlin Zhang, Meiqin Liu, Shoudong Huang</li>
<li>for: 提高无人机在未知和复杂环境中的信息基于自主探索效率</li>
<li>methods: 使用 Gaussian process 回归学习一个估计函数来推算信息强度相关的控制动作，并采用基于 GP 的 Bayesian 优化（GPBO）来实现让推敲和探索之间的负荷权衡。</li>
<li>results: 提出了一种新的轻量级信息增加推理方法，可以在不需要训练的情况下实现 Logarithmic 复杂度，并且可以在不同的无结构和堆存环境中保持探索性。<details>
<summary>Abstract</summary>
In this paper, we consider improving the efficiency of information-based autonomous robot exploration in unknown and complex environments. We first utilize Gaussian process (GP) regression to learn a surrogate model to infer the confidence-rich mutual information (CRMI) of querying control actions, then adopt an objective function consisting of predicted CRMI values and prediction uncertainties to conduct Bayesian optimization (BO), i.e., GP-based BO (GPBO). The trade-off between the best action with the highest CRMI value (exploitation) and the action with high prediction variance (exploration) can be realized. To further improve the efficiency of GPBO, we propose a novel lightweight information gain inference method based on Bayesian kernel inference and optimization (BKIO), achieving an approximate logarithmic complexity without the need for training. BKIO can also infer the CRMI and generate the best action using BO with bounded cumulative regret, which ensures its comparable accuracy to GPBO with much higher efficiency. Extensive numerical and real-world experiments show the desired efficiency of our proposed methods without losing exploration performance in different unstructured, cluttered environments. We also provide our open-source implementation code at https://github.com/Shepherd-Gregory/BKIO-Exploration.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了自动化机器人在未知和复杂环境中提高信息基于的探索效率。我们首先利用 Gaussian 过程（GP）回归来学习一个假设模型，以便对查询控制动作的信息充足积极（CRMI）进行推测，然后采用一个包含预测值和预测不确定性的目标函数来进行 Bayesian 优化（BO），即 GP 基于的 BO（GPBO）。通过考虑最佳动作的 CRMI 值（利用）和高预测变异（探索）的交互来实现财富均衡。为了进一步提高 GPBO 的效率，我们提出了一种新的轻量级信息增加推测方法，基于抽象kernel推断和优化（BKIO），实现了logs平方复杂度而不需要训练。BKIO 可以将 CRMI 推测出来，并使用 BO 实现 bounded 累累积 regret，这保证了它与 GPBO 相对较高效的准确性。我们在不同的无结构、拥堵环境中进行了广泛的数值和实际实验，并证明了我们的提议的效率无需失去探索性。我们还在 GitHub 上提供了我们的开源实现代码，可以在 <https://github.com/Shepherd-Gregory/BKIO-Exploration> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Does-Writing-with-Language-Models-Reduce-Content-Diversity"><a href="#Does-Writing-with-Language-Models-Reduce-Content-Diversity" class="headerlink" title="Does Writing with Language Models Reduce Content Diversity?"></a>Does Writing with Language Models Reduce Content Diversity?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05196">http://arxiv.org/abs/2309.05196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vishakhpk/hai-diversity">https://github.com/vishakhpk/hai-diversity</a></li>
<li>paper_authors: Vishakh Padmakumar, He He</li>
<li>For: The paper aims to measure the impact of co-writing on diversity in generated content using large language models (LLMs).* Methods: The study uses a controlled experiment where users write argumentative essays in three setups: with a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and without model help. The authors develop a set of diversity metrics to evaluate the impact of co-writing on diversity.* Results: The study finds that writing with InstructGPT (but not GPT3) results in a statistically significant reduction in diversity, as it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. The effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays, while the user-contributed text remains unaffected by model collaboration.Here are the results in Simplified Chinese text:* For: 这个研究是为了测量大语言模型（LLM）的合作写作对内容多样性的影响。* Methods: 这个研究使用了一个控制试验，在不同的设置下让用户写作Argumentative Essays：使用基础的 LLM（GPT3）、反馈调整的 LLM（InstructGPT）以及没有模型帮助。作者们开发了一组多样性指标来评估合作写作对多样性的影响。* Results: 研究发现，使用 InstructGPT（而不是 GPT3）会导致对多样性的统计学上的减少，这是因为它会使用者的写作更加相似，并降低总的语言和内容多样性。这种效果主要是由 InstructGPT 在合写文章中提供的文本变得更加一致，而不是用户提供的文本。<details>
<summary>Abstract</summary>
Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization"><a href="#Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization" class="headerlink" title="Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization"></a>Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05183">http://arxiv.org/abs/2309.05183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Tang</li>
<li>for:  solves the two-stage submodular maximization problem with non-monotone submodular functions, which has applications in data summarization and other domains.</li>
<li>methods:  uses constant-factor approximation algorithms to address the more general case of non-monotone submodular functions.</li>
<li>results:  pioneers the extension of submodular maximization research to accommodate non-monotone functions, and introduces the first constant-factor approximation algorithms for this more general case.<details>
<summary>Abstract</summary>
The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:两个阶段的子模式最大化问题的目标是使用提供的训练函数来减少基aset，以确保将新的目标函数应用于减少后的基aset上的优化结果与原始基aset上的优化结果相似。这个问题在不同领域，如数据概要化中有应用。现有研究通常假设目标函数的 monotonicity，而我们的工作则是扩展这些研究，以适应非 monotone 的子模式函数。我们已经提出了首个常数因子approximation算法。
</details></li>
</ul>
<hr>
<h2 id="DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning"><a href="#DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"></a>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05173">http://arxiv.org/abs/2309.05173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengxiangshi/dept">https://github.com/zhengxiangshi/dept</a></li>
<li>paper_authors: Zhengxiang Shi, Aldo Lipani<br>for: 这个研究的目的是提高语言模型的parameter-efficient fine-tuning（PEFT）性能，以及实现更好的内存和时间成本。methods: 这个研究使用Prompt Tuning（PT）技术，将一小量可变软题（continuous）标识给输入语言模型（LM），以提高模型的演算效率和精度。results: 这个研究发现，使用Decomposed Prompt Tuning（DePT）技术可以在23个自然语言处理（NLP）和描述语言（VL）任务中，实现更好的性能，并在一些情况下超越基eline。同时，DePT可以实现更好的内存和时间成本，比基eline节省20%以上。<details>
<summary>Abstract</summary>
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving over 20% memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.
</details>
<details>
<summary>摘要</summary>
Prompt tuning (PT)，一种将小量可调软 vectors（连续）附加到语言模型（LM）的输入处，已经在多种任务和模型上显示出了有 promise的结果。PT与其他PEFTapproaches不同，因为它在模型大小增加时不会很快扩展参数。然而，PT引入了额外的软提示字符，导致输入序列变长，从而对训练和推理时间和内存使用有很大影响，特别是对大型语言模型（LLM）来说。为解决这个问题，我们提出了Decomposed Prompt Tuning（DePT），它将软提示分解成一个 shorter soft prompt和一对低级矩阵，然后将这些矩阵优化两个不同的学习率。这使得DePT可以实现更好的性能，同时减少了20%以上的内存和时间成本，不改变可调参数的大小。通过对23种自然语言处理（NLP）和视觉语言（VL）任务进行了广泛的实验，我们证明了DePT在PEFT方法中比标准PT和其他变体表现更好，甚至在某些情况下超过了全 Fine-tuning基线。此外，我们还观察到DePT在模型大小增加时变得更加高效。我们进一步的研究表明，DePT可以轻松地与parameter-efficient transfer learning在少量学习设定中集成，并且可以适应不同的模型结构和大小。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.LG_2023_09_11/" data-id="clmjn91n1008d0j8883an5p0c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/11/cs.SD_2023_09_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-11
        
      </div>
    </a>
  
  
    <a href="/2023/09/11/eess.IV_2023_09_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-11</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
