
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-03 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Generative Social Choice paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01291 repo_url: https:&#x2F;&#x2F;github.com&#x2F;babatundeibukun&#x2F;simple-social-learning-environment paper_authors: Sara Fish, Paul Gölz, David C. Parkes">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-03">
<meta property="og:url" content="https://nullscc.github.io/2023/09/03/cs.LG_2023_09_03/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Generative Social Choice paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01291 repo_url: https:&#x2F;&#x2F;github.com&#x2F;babatundeibukun&#x2F;simple-social-learning-environment paper_authors: Sara Fish, Paul Gölz, David C. Parkes">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-03T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.459Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/cs.LG_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T10:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-03
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generative-Social-Choice"><a href="#Generative-Social-Choice" class="headerlink" title="Generative Social Choice"></a>Generative Social Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01291">http://arxiv.org/abs/2309.01291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/babatundeibukun/simple-social-learning-environment">https://github.com/babatundeibukun/simple-social-learning-environment</a></li>
<li>paper_authors: Sara Fish, Paul Gölz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, Manuel Wüthrich</li>
<li>for: 这篇论文是为了探讨人工智能在民主过程中的应用，具体来说是如何使用自然语言处理技术来实现民主选举。</li>
<li>methods: 这篇论文使用了社会选择理论的数学严谨性和大自然语言模型的文本生成能力，提出了一个生成社会选择框架，可以帮助解决复杂的民主选举问题。</li>
<li>results: 通过应用这个框架，可以生成一个代表民意的评论文本，例如在在线审议过程中。<details>
<summary>Abstract</summary>
Traditionally, social choice theory has only been applicable to choices among a few predetermined alternatives but not to more complex decisions such as collectively selecting a textual statement. We introduce generative social choice, a framework that combines the mathematical rigor of social choice theory with large language models' capability to generate text and extrapolate preferences. This framework divides the design of AI-augmented democratic processes into two components: first, proving that the process satisfies rigorous representation guarantees when given access to oracle queries; second, empirically validating that these queries can be approximately implemented using a large language model. We illustrate this framework by applying it to the problem of generating a slate of statements that is representative of opinions expressed as free-form text, for instance in an online deliberative process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Orthogonal-Training-Mitigating-Global-Catastrophic-Forgetting-in-Continual-Federated-Learning"><a href="#Federated-Orthogonal-Training-Mitigating-Global-Catastrophic-Forgetting-in-Continual-Federated-Learning" class="headerlink" title="Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning"></a>Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01289">http://arxiv.org/abs/2309.01289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yavuz Faruk Bakman, Duygu Nur Yaldiz, Yahya H. Ezzeldin, Salman Avestimehr</li>
<li>for: 这个研究旨在解决在 Federated Learning (FL) 中发生的 Global Catastrophic Forgetting (GCF) 问题，GCF 是指当模型学习新任务时，对旧任务的性能下降的问题。</li>
<li>methods: 我们提出了一个新的方法，即 Federated Orthogonal Training (FOT)，它可以减少 зада务之间的干扰，并且避免违反 Federated Learning 的隐私原则。FOT 使用了对 old tasks 的全球输入子空间抽出，并且对 new tasks 的聚合更新进行修改，使其与 old tasks 的全球主成分空间垂直对错。</li>
<li>results: 我们的实验结果显示，FOT 可以在 Continual Federated Learning (CFL) 设定中优化性能，实现对旧任务的15%的精度提升，同时降低了27%的遗传和通信成本。<details>
<summary>Abstract</summary>
Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL. Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer. This decreases the interference between tasks, which is the main cause for forgetting. We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 已经吸引了广泛关注，因为它可以在分布式数据上进行隐私保护的训练。现有文献中大多数研究都是单任务学习。然而，随着时间的推移，客户端上可能会出现新的任务，global模型应该学习这些任务而不会忘记之前的任务。这种真实情况被称为 Continual Federated Learning (CFL)。CFL的主要挑战是全球性衰减，即当全球模型在新任务上进行训练时，其对于老任务的性能下降。有些最近的研究提出了一些方法来解决全球性衰减问题，但这些方法都假设了可以获得过去数据样本的可用性，或者违反了 Federated Learning 的隐私原则。我们提出了一种新的方法，即 Federated Orthogonal Training (FOT)，以解决这些挑战。我们的算法抽取每层的全球输入子空间 для老任务，并将新任务的聚合更新修改为在每层上与老任务的全球主成分空间垂直的。这会减少任务之间的干扰，这是全球性衰减的主要原因。我们实验表明，FOT可以在 CFL 设置中超越当前最佳的连续学习方法，实现了最高的准确率提升达到 15%，同时具有27% 更低的忘记率，只有 minimal 的计算和通信成本。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Evaluation-of-FedAvg-and-Per-FedAvg-Algorithms-for-Dirichlet-Distributed-Heterogeneous-Data"><a href="#A-Comparative-Evaluation-of-FedAvg-and-Per-FedAvg-Algorithms-for-Dirichlet-Distributed-Heterogeneous-Data" class="headerlink" title="A Comparative Evaluation of FedAvg and Per-FedAvg Algorithms for Dirichlet Distributed Heterogeneous Data"></a>A Comparative Evaluation of FedAvg and Per-FedAvg Algorithms for Dirichlet Distributed Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01275">http://arxiv.org/abs/2309.01275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Reguieg, Mohammed El Hanjri, Mohamed El Kamili, Abdellatif Kobbane</li>
<li>for:  investigate Federated Learning (FL) and compare two strategies within this paradigm: Federated Averaging (FedAvg) and Personalized Federated Averaging (Per-FedAvg)</li>
<li>methods:  use Non-Identically and Independently Distributed (Non-IID) data to evaluate the performance of both strategies</li>
<li>results:  Per-FedAvg shows superior robustness in conditions of high data heterogeneity, and our results provide insights into the development of more effective and efficient machine learning strategies in a decentralized setting.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文 investigate Federated Learning (FL) 并比较这个 парадиг中两种策略：Federated Averaging (FedAvg) 和 Personalized Federated Averaging (Per-FedAvg)</li>
<li>methods: 使用 Non-Identically and Independently Distributed (Non-IID) 数据来评估这两种策略的性能</li>
<li>results: Per-FedAvg 在高度不一致的数据下表现出较好的Robustness，我们的结果提供了在分布式设置下开发更有效和高效的机器学习策略的指导。<details>
<summary>Abstract</summary>
In this paper, we investigate Federated Learning (FL), a paradigm of machine learning that allows for decentralized model training on devices without sharing raw data, there by preserving data privacy. In particular, we compare two strategies within this paradigm: Federated Averaging (FedAvg) and Personalized Federated Averaging (Per-FedAvg), focusing on their performance with Non-Identically and Independently Distributed (Non-IID) data. Our analysis shows that the level of data heterogeneity, modeled using a Dirichlet distribution, significantly affects the performance of both strategies, with Per-FedAvg showing superior robustness in conditions of high heterogeneity. Our results provide insights into the development of more effective and efficient machine learning strategies in a decentralized setting.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了联邦学习（Federated Learning，FL），一种机器学习 paradigma，允许在设备上进行分散式模型训练，而不需要分享原始数据，从而保持数据隐私。特别是，我们对两种策略进行比较：联邦均值（FedAvg）和个性化联邦均值（Per-FedAvg），并将注重非标一样分布（Non-IID）数据的性能。我们的分析表明，数据不同程度的不同，通过 Dirichlet 分布来模型，对两种策略的性能产生了显著影响，Per-FedAvg 在高度不同化的情况下表现出了更高的鲁棒性。我们的结果为开发更有效率的分布式机器学习策略提供了洞察。
</details></li>
</ul>
<hr>
<h2 id="COMEDIAN-Self-Supervised-Learning-and-Knowledge-Distillation-for-Action-Spotting-using-Transformers"><a href="#COMEDIAN-Self-Supervised-Learning-and-Knowledge-Distillation-for-Action-Spotting-using-Transformers" class="headerlink" title="COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers"></a>COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01270">http://arxiv.org/abs/2309.01270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliendenize/eztorch">https://github.com/juliendenize/eztorch</a></li>
<li>paper_authors: Julien Denize, Mykola Liashuha, Jaonary Rabarisoa, Astrid Orcesi, Romain Hérault</li>
<li>for: 这 paper 是为了提出一种用于动作检测的 Initialization 管道，即 COMEDIAN，该管道包括自动学习和知识储存两个 initialization 阶段。</li>
<li>methods: 该 paper 使用了两个 initialization 阶段，首先是使用短视频作为输入进行自动学习初始化 spatial transformer，然后是通过知识储存来增强 spatial transformer 的输出，并在最后一步进行 fine-tuning。</li>
<li>results: 实验结果表明，COMEDIAN 的预训练方法可以在 SoccerNet-v2 数据集上达到状态作卷积的性能，并且比非预训练模型更快地 converges。这些结果表明 COMEDIAN 的预训练管道的有效性。<details>
<summary>Abstract</summary>
We present COMEDIAN, a novel pipeline to initialize spatio-temporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence compared to non-pretrained models.
</details>
<details>
<summary>摘要</summary>
我团队现象了一个名为COMEDIAN的新的引导管道，用于初始化时空转换器以进行动作检测，这个任务是基于时间戳的时间序列动作检测。我们的管道包括三个步骤，其中有两个初始化阶段。第一个阶段是使用短视频作为输入进行自主学习初始化一个空间转换器。其次，我们使用知识储存的技术进行知识储存初始化，使得时空转换器的输出得到全局上下文的增强。最后，我们精度调整transformer来进行动作检测任务。在 SoccerNet-v2 数据集上进行的实验表明，我们的预训练模型可以达到领先的性能水平，并且比非预训练模型更快地 converge。我们的结果表明，COMEDIAN的预训练方案具有以下优势：性能提高和更快的 converge。
</details></li>
</ul>
<hr>
<h2 id="Learning-Aware-Safety-for-Interactive-Autonomy"><a href="#Learning-Aware-Safety-for-Interactive-Autonomy" class="headerlink" title="Learning-Aware Safety for Interactive Autonomy"></a>Learning-Aware Safety for Interactive Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01267">http://arxiv.org/abs/2309.01267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimin Hu, Zixu Zhang, Kensuke Nakamura, Andrea Bajcsy, Jaime F. Fisac</li>
<li>for: 本研究旨在提供一种新的关闭Loop方法，以确保机器人系统在实时学习和适应的情况下保持安全交互。</li>
<li>methods: 该方法使用反抗搅ء深度学习来规避未来可能的enario，并同时考虑机器人学习算法的内部信念的变化。</li>
<li>results: 研究人员使用这种方法可以 tractable safety analysis，并且可以处理高维度的情况。此外，他们还能够证明这种方法可以与 bayesian belief propagation和大型预训练神经轨迹预测器结合使用。<details>
<summary>Abstract</summary>
One of the outstanding challenges for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing efficiency. Existing safety analysis methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the system's evolving uncertainty under possible future scenarios. The formulation reasons jointly about the physical dynamics and the robot's learning algorithm, which updates its internal belief over time. We leverage adversarial deep reinforcement learning (RL) for scaling to high dimensions, enabling tractable safety analysis even for implicit learning dynamics induced by state-of-the-art prediction models. We demonstrate our framework's ability to work with both Bayesian belief propagation and the implicit learning induced by a large pre-trained neural trajectory predictor.
</details>
<details>
<summary>摘要</summary>
一个重要挑战是使机器人系统广泛部署，是确保安全地与人类交互，不 sacrificing efficiency。现有的安全分析方法经常忽视机器人的学习和运行时间的可变性，导致行为过于保守。这篇论文提出了一种新的封闭循环方案，用于生成安全的控制策略，并直接考虑系统的演化uncertainty的可能性。我们利用对抗式深度学习游戏学习（RL），以便在高维度上进行可观察的安全分析，即使是由state-of-the-art预测模型引起的隐式学习动态。我们示例了我们框架的可用性，使用 bayesian belief propagation 和大规模预测模型来驱动机器人的学习和控制。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Contrastive-Learning-with-Hard-Negative-Sampling-for-Human-Activity-Recognition"><a href="#Multimodal-Contrastive-Learning-with-Hard-Negative-Sampling-for-Human-Activity-Recognition" class="headerlink" title="Multimodal Contrastive Learning with Hard Negative Sampling for Human Activity Recognition"></a>Multimodal Contrastive Learning with Hard Negative Sampling for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01262">http://arxiv.org/abs/2309.01262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeongju Choi, Apoorva Beedu, Irfan Essa</li>
<li>for: 本研究旨在提高人体活动识别（HAR）系统的性能，使其能够在有限数据量的情况下进行自监督学习。</li>
<li>methods: 本研究使用了自监督学习的contrastive学习方法，并提出了一种基于硬negative sampling的方法来选择良好的负样本。</li>
<li>results: 通过对UTD-MHAD和MMAct两个benchmark dataset进行广泛的实验，本研究证明了我们的方法可以在有限数据量下学习强大的特征表示，并在下游活动识别任务上超过所有state-of-the-art方法。<details>
<summary>Abstract</summary>
Human Activity Recognition (HAR) systems have been extensively studied by the vision and ubiquitous computing communities due to their practical applications in daily life, such as smart homes, surveillance, and health monitoring.   Typically, this process is supervised in nature and the development of such systems requires access to large quantities of annotated data.   However, the higher costs and challenges associated with obtaining good quality annotations have rendered the application of self-supervised methods an attractive option and contrastive learning comprises one such method.   However, a major component of successful contrastive learning is the selection of good positive and negative samples.   Although positive samples are directly obtainable, sampling good negative samples remain a challenge.   As human activities can be recorded by several modalities like camera and IMU sensors, we propose a hard negative sampling method for multimodal HAR with a hard negative sampling loss for skeleton and IMU data pairs.   We exploit hard negatives that have different labels from the anchor but are projected nearby in the latent space using an adjustable concentration parameter.   Through extensive experiments on two benchmark datasets: UTD-MHAD and MMAct, we demonstrate the robustness of our approach forlearning strong feature representation for HAR tasks, and on the limited data setting.   We further show that our model outperforms all other state-of-the-art methods for UTD-MHAD dataset, and self-supervised methods for MMAct: Cross session, even when uni-modal data are used during downstream activity recognition.
</details>
<details>
<summary>摘要</summary>
人类活动识别（HAR）系统已经广泛研究于计算机视觉和无线计算领域，因为它们在日常生活中有广泛的应用，如智能家居、监控和健康监测。通常，这个过程是有监督性的，需要大量的注释数据来进行开发。然而，获取高质量的注释数据的高成本和挑战使得自监学习方法变得吸引人。在这些方法中，对比学习是一种吸引人的选择。然而，成功的对比学习需要选择好的正例和负例样本。正例样本直接可以获得，但是找到好的负例样本仍然是一个挑战。因为人类活动可以通过相机和IMU传感器记录，我们提议一种基于硬negative sampling的多modal HAR方法。我们利用硬negative sampling的损失函数，通过调整可变的集中参数，将不同标签的样本映射到近似的 latent space 中。通过大量的实验在UTD-MHAD和MMAct两个标准 benchmark dataset 上，我们证明了我们的方法可以学习强大的特征表示，并在有限的数据设置下进行下游活动识别任务。我们还证明了我们的模型在UTD-MHAD数据集上超过所有现有的状态艺术方法，并在单Modal数据被用于下游活动识别任务时也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Large-AI-Model-Empowered-Multimodal-Semantic-Communications"><a href="#Large-AI-Model-Empowered-Multimodal-Semantic-Communications" class="headerlink" title="Large AI Model Empowered Multimodal Semantic Communications"></a>Large AI Model Empowered Multimodal Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01249">http://arxiv.org/abs/2309.01249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You</li>
<li>for: 提供一个具有低延迟和高质量的语义层次沟通（SC）体验，使用多modal信号（包括文本、音频、图像和视频）进行集成。</li>
<li>methods: 使用大型AI模型（特别是多modal语言模型（MLM）和大型语言模型（LLM））解决数据不一致性、semantic抽象和信号抖动等问题。</li>
<li>results: 提出一个基于大型AI模型的多modal SC（LAM-MSC）框架，包括MLM基于多modalAlignment（MMA）、个性化LLM基于知识库（LKB）和Conditional Generative Adversarial Networks（CGE）等方法，实现高质量、低延迟的语义层次沟通。<details>
<summary>Abstract</summary>
Multimodal signals, including text, audio, image and video, can be integrated into Semantic Communication (SC) for providing an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal fading. Recent advancements in large AI models, particularly in Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, in which we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery through the LLM. This effectively addresses the semantic ambiguity. Finally, we apply the Conditional Generative adversarial networks-based channel Estimation (CGE) to obtain Channel State Information (CSI). This approach effectively mitigates the impact of fading channels in SC. Finally, we conduct simulations that demonstrate the superior performance of the LAM-MSC framework.
</details>
<details>
<summary>摘要</summary>
多Modal信号，包括文本、音频、图像和视频，可以在Semantic Communication（SC）中集成，以提供低延迟和高质量的具体性体验。然而，多Modal SC 存在多个挑战，包括数据不一致、 semantics 抽象和信号衰减。现代大AI模型，特别是多Modal语言模型（MLM）和大语言模型（LLM），提供了解决这些问题的可能性。为此，我们提议一个基于大AI模型的多Modal SC 框架（LAM-MSC），在这里我们首先提出了基于 MLM 的多Modal对接（MMA），使得在多Modal和单Modal数据之间进行转换，保持 semantics 一致。然后，我们提出了个性化的 LLM-based Knowledge Base（LKB），允许用户进行个性化的具体性抽取或恢复，从而有效解决 semantics 抽象。最后，我们应用 Conditional Generative Adversarial Networks 基于渠道估计（CGE），以获取渠道状态信息（CSI）。这种方法有效地减少了干扰通道的影响，从而提高 SC 的性能。最后，我们进行了临床实验，并证明了 LAM-MSC 框架的超越性。
</details></li>
</ul>
<hr>
<h2 id="Modified-Step-Size-for-Enhanced-Stochastic-Gradient-Descent-Convergence-and-Experiments"><a href="#Modified-Step-Size-for-Enhanced-Stochastic-Gradient-Descent-Convergence-and-Experiments" class="headerlink" title="Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and Experiments"></a>Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01248">http://arxiv.org/abs/2309.01248</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Soheil Shamaee, S. Fathi Hafshejani</li>
<li>for: 提高泊braceSGD算法性能</li>
<li>methods:  integrate modified decay step size based on $\frac{1}{\sqrt{t}$，含有对数函数，选择最小值</li>
<li>results: 实验表明，在图像分类任务中，使用提档的步长策略可以提高准确率，与传统步长策略相比，提高0.5%和1.4%。Here’s the breakdown of each sentence:* “for”: 指出文章的目的是提高泊braceSGD算法性能。* “methods”: 描述文章提出的方法，即使 modified decay step size based on $\frac{1}{\sqrt{t}$，含有对数函数，选择最小值。* “results”:  SUMMARIZE the main findings of the paper, which is the improvement in accuracy achieved by using the proposed step size strategy in image classification tasks.<details>
<summary>Abstract</summary>
This paper introduces a novel approach to enhance the performance of the stochastic gradient descent (SGD) algorithm by incorporating a modified decay step size based on $\frac{1}{\sqrt{t}$. The proposed step size integrates a logarithmic term, leading to the selection of smaller values in the final iterations. Our analysis establishes a convergence rate of $O(\frac{\ln T}{\sqrt{T})$ for smooth non-convex functions without the Polyak-{\L}ojasiewicz condition. To evaluate the effectiveness of our approach, we conducted numerical experiments on image classification tasks using the FashionMNIST, and CIFAR10 datasets, and the results demonstrate significant improvements in accuracy, with enhancements of $0.5\%$ and $1.4\%$ observed, respectively, compared to the traditional $\frac{1}{\sqrt{t}$ step size. The source code can be found at \\\url{https://github.com/Shamaeem/LNSQRTStepSize}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Privacy-Utility-Tradeoff-of-OLS-with-Random-Projections"><a href="#Privacy-Utility-Tradeoff-of-OLS-with-Random-Projections" class="headerlink" title="Privacy-Utility Tradeoff of OLS with Random Projections"></a>Privacy-Utility Tradeoff of OLS with Random Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01243">http://arxiv.org/abs/2309.01243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Lu, Malik Magdon-Ismail, Yu Wei, Vassilis Zikas</li>
<li>for: 这个论文主要是为了研究线性最小二乘（OLS）问题中的分布式隐私（DP）。</li>
<li>methods: 这个论文使用了随机化的ALS算法（Sarlos，2006）来解决OLS问题，并证明了这种算法可以保持隐私。</li>
<li>results: 这个论文得到了一个更好的隐私&#x2F;用途质量比例，而无需进行修改或噪声处理，相比于其他私有OLS算法。此外，论文还提供了一个紧跟的DP分析，并引入了一些新的工具，如DP分谱（DP spectrum）和改进的随机投影DP分谱。<details>
<summary>Abstract</summary>
We study the differential privacy (DP) of a core ML problem, linear ordinary least squares (OLS), a.k.a. $\ell_2$-regression. Our key result is that the approximate LS algorithm (ALS) (Sarlos, 2006), a randomized solution to the OLS problem primarily used to improve performance on large datasets, also preserves privacy. ALS achieves a better privacy/utility tradeoff, without modifications or further noising, when compared to alternative private OLS algorithms which modify and/or noise OLS. We give the first {\em tight} DP-analysis for the ALS algorithm and the standard Gaussian mechanism (Dwork et al., 2014) applied to OLS. Our methodology directly improves the privacy analysis of (Blocki et al., 2012) and (Sheffet, 2019)) and introduces new tools which may be of independent interest: (1) the exact spectrum of $(\epsilon, \delta)$-DP parameters (``DP spectrum") for mechanisms whose output is a $d$-dimensional Gaussian, and (2) an improved DP spectrum for random projection (compared to (Blocki et al., 2012) and (Sheffet, 2019)).   All methods for private OLS (including ours) assume, often implicitly, restrictions on the input database, such as bounds on leverage and residuals. We prove that such restrictions are necessary. Hence, computing the privacy of mechanisms such as ALS must estimate these database parameters, which can be infeasible in big datasets. For more complex ML models, DP bounds may not even be tractable. There is a need for blackbox DP-estimators (Lu et al., 2022) which empirically estimate a data-dependent privacy. We demonstrate the effectiveness of such a DP-estimator by empirically recovering a DP-spectrum that matches our theory for OLS. This validates the DP-estimator in a nontrivial ML application, opening the door to its use in more complex nonlinear ML settings where theory is unavailable.
</details>
<details>
<summary>摘要</summary>
我们研究了数据隐私（DP）的一个核心机器学习（ML）问题，即线性最小二乘（OLS）问题。我们的关键结果是表示 approximate least squares algorithm（ALS），一种对大规模数据进行隐私处理的Randomized解决方案，同时也维护了隐私。相比于其他修改和噪音OLS算法，ALS在隐私/使用价比方面表现更好，而且无需进行修改或进一步噪音。我们提供了第一个严格的DP分析，并将杜夫-尼库库-帕特森（Dwork et al., 2014）所提出的标准 Gaussian mechanism 应用到OLS问题上。我们的方法直接改进了（Blocki et al., 2012）和（Sheffet, 2019）的隐私分析，并引入了新的工具，包括：（1）$(\epsilon, \delta)$-DP  парамет域的精确谱（DP spectrum），以及（2）对于随机投影的改进DP spectrum。所有私人OLS（包括我们的）都假设了输入数据库中的约束，例如 bounds on leverage 和 residuals。我们证明了这些约束是必要的。因此，为了计算私人OLS的隐私，必须估计这些数据库中的 Parameters，这可能是大规模数据中的问题。对于更复杂的机器学习模型，DP bound 可能无法实际 tractable。为了解决这问题，我们提出了黑盒DP估计器（Lu et al., 2022），可以在实际应用中估计资料依赖的隐私。我们透过实验证明了这个DP估计器在一个简单的 ML 应用中的可行性，这开启了这个技术在更复杂的非线性 ML 设置中的使用。
</details></li>
</ul>
<hr>
<h2 id="lfads-torch-A-modular-and-extensible-implementation-of-latent-factor-analysis-via-dynamical-systems"><a href="#lfads-torch-A-modular-and-extensible-implementation-of-latent-factor-analysis-via-dynamical-systems" class="headerlink" title="lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems"></a>lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01230">http://arxiv.org/abs/2309.01230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew R. Sedler, Chethan Pandarinath</li>
<li>for: 高维神经活动噪声去噪（denoising），用于科学和工程应用。</li>
<li>methods: 使用RNN基于变量序列自动编码器（Variational Sequential Autoencoder，VSAE）实现潜在因子分析（Latent Factor Analysis，LFA）。</li>
<li>results: 达到了高级表现，可以应用于各种 neuroscience 问题。<details>
<summary>Abstract</summary>
Latent factor analysis via dynamical systems (LFADS) is an RNN-based variational sequential autoencoder that achieves state-of-the-art performance in denoising high-dimensional neural activity for downstream applications in science and engineering. Recently introduced variants and extensions continue to demonstrate the applicability of the architecture to a wide variety of problems in neuroscience. Since the development of the original implementation of LFADS, new technologies have emerged that use dynamic computation graphs, minimize boilerplate code, compose model configuration files, and simplify large-scale training. Building on these modern Python libraries, we introduce lfads-torch -- a new open-source implementation of LFADS that unifies existing variants and is designed to be easier to understand, configure, and extend. Documentation, source code, and issue tracking are available at https://github.com/arsedler9/lfads-torch .
</details>
<details>
<summary>摘要</summary>
Latent Factor Analysis via Dynamical Systems (LFADS) 是一种基于 Recurrent Neural Network (RNN) 的可变量序列自动编码器，可以在高维神经活动噪声去噪应用中达到状态之 arts 的性能。最近引入的变体和扩展继续证明了该架构在神经科学中的广泛应用。自 LFADS 的原始实现以来，新技术出现了，包括动态计算图、最小化 boilerplate 代码、组合模型配置文件以及大规模训练的技术。基于这些现代 Python 库，我们介绍了 lfads-torch -- 一个新的开源 LFADS 实现，它将 существующие变体集成到一起，并设计为更易于理解、配置和扩展。文档、源代码和问题跟踪可以在 GitHub 上找到：https://github.com/arsedler9/lfads-torch。
</details></li>
</ul>
<hr>
<h2 id="Saturn-An-Optimized-Data-System-for-Large-Model-Deep-Learning-Workloads"><a href="#Saturn-An-Optimized-Data-System-for-Large-Model-Deep-Learning-Workloads" class="headerlink" title="Saturn: An Optimized Data System for Large Model Deep Learning Workloads"></a>Saturn: An Optimized Data System for Large Model Deep Learning Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01226">http://arxiv.org/abs/2309.01226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knagrecha/saturn">https://github.com/knagrecha/saturn</a></li>
<li>paper_authors: Kabir Nagrecha, Arun Kumar</li>
<li>for: 这项研究旨在帮助深度学习（DL）用户更好地选择并使用大型语言模型，如GPT-3和ChatGPT，以及其他模型。</li>
<li>methods: 该研究使用了一种新的信息系统架构，以帮助DL用户更好地处理多个模型并行运行、资源分配和调度问题。它们还提出了一种可重复的模型并行方案，并与runtime estimation相结合。</li>
<li>results: 实验结果显示，使用该新方法可以减少DL模型选择运行时间，比现有实践更高效。特别是，Saturn系统可以在39-49%的时间内完成模型选择运行。<details>
<summary>Abstract</summary>
Large language models such as GPT-3 & ChatGPT have transformed deep learning (DL), powering applications that have captured the public's imagination. These models are rapidly being adopted across domains for analytics on various modalities, often by finetuning pre-trained base models. Such models need multiple GPUs due to both their size and computational load, driving the development of a bevy of "model parallelism" techniques & tools. Navigating such parallelism choices, however, is a new burden for end users of DL such as data scientists, domain scientists, etc. who may lack the necessary systems knowhow. The need for model selection, which leads to many models to train due to hyper-parameter tuning or layer-wise finetuning, compounds the situation with two more burdens: resource apportioning and scheduling. In this work, we tackle these three burdens for DL users in a unified manner by formalizing them as a joint problem that we call SPASE: Select a Parallelism, Allocate resources, and SchedulE. We propose a new information system architecture to tackle the SPASE problem holistically, representing a key step toward enabling wider adoption of large DL models. We devise an extensible template for existing parallelism schemes and combine it with an automated empirical profiler for runtime estimation. We then formulate SPASE as an MILP.   We find that direct use of an MILP-solver is significantly more effective than several baseline heuristics. We optimize the system runtime further with an introspective scheduling approach. We implement all these techniques into a new data system we call Saturn. Experiments with benchmark DL workloads show that Saturn achieves 39-49% lower model selection runtimes than typical current DL practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型如GPT-3和ChatGPT已经改变深度学习（DL），推动了许多应用程序，吸引了大众的关注。这些模型在不同领域进行分析，通常是通过调整预训计划的方式进行训练。这些模型需要多个GPU，因为它们的大小和计算负载，这推动了多个"模型平行化"技术和工具的发展。但是，为DL使用者如数据科学家和领域科学家等选择和调整这些平行化方法，是一个新的负担。因为需要模型选择，这会导致训练多个模型，增加资源分配和平行化的问题。在这个工作中，我们解决了这三个负担，我们统称这些问题为SPASE：选择平行化、分配资源和安排。我们提出了一个新的资讯系统架构，以便对SPASE问题进行整体解决。我们创建了一个可扩展的平行化方案模板，并与一个自动化的实验性能评估工具结合。我们将SPASE转换为一个MILP，并发现直接使用MILP解决方案是较有效的。我们还进一步优化系统执行时间使用一个自我反思的安排方法。我们实现了这些技术，并命名为Saturn。我们对标准DL工作负载进行实验，发现Saturn可以降低39-49%的模型选择执行时间，比 Typical current DL实践更高效。
</details></li>
</ul>
<hr>
<h2 id="Siren’s-Song-in-the-AI-Ocean-A-Survey-on-Hallucination-in-Large-Language-Models"><a href="#Siren’s-Song-in-the-AI-Ocean-A-Survey-on-Hallucination-in-Large-Language-Models" class="headerlink" title="Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models"></a>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01219">http://arxiv.org/abs/2309.01219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hillzhang1999/llm-hallucination-survey">https://github.com/hillzhang1999/llm-hallucination-survey</a></li>
<li>paper_authors: Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi</li>
<li>for: 本文旨在探讨大语言模型（LLM）在实际应用中的可靠性问题，即LLM occasional hallucination phenomenon。</li>
<li>methods: 本文 Survey recent efforts on LLM hallucination detection, explanation, and mitigation, with an emphasis on the unique challenges posed by LLMs.</li>
<li>results: 本文 present taxonomies of LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs) 的表现很出色，但是也存在一定的问题：LLMs  occasionaly generates content that deviates from user input, contradicts previous generated context, or misaligns with established world knowledge. This phenomenon poses a significant challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Implicit-regularization-of-deep-residual-networks-towards-neural-ODEs"><a href="#Implicit-regularization-of-deep-residual-networks-towards-neural-ODEs" class="headerlink" title="Implicit regularization of deep residual networks towards neural ODEs"></a>Implicit regularization of deep residual networks towards neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01213">http://arxiv.org/abs/2309.01213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Marion, Yu-Han Wu, Michael E. Sander, Gérard Biau</li>
<li>for: 这篇论文旨在建立深度学习模型中抽象层次结构的固有关系，即深度神经网络（ResNet）与神经泛化方程（Neural ODE）之间的数学基础。</li>
<li>methods: 该论文使用了 gradient flow 来训练非线性网络，并在训练过程中 prove 了深度神经网络的离散化可以被视为神经泛化方程的离散化。</li>
<li>results: 该论文的结果表明，如果初始化网络为神经泛化方程的离散化，那么这种离散化将在训练过程中保持不变，并且这种结果适用于有限的训练时间和训练时间趋于无穷大的情况下，只要网络满足一定的 Polyak-Lojasiewicz 条件。<details>
<summary>Abstract</summary>
Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to a global minimum. Numerical experiments illustrate our results.
</details>
<details>
<summary>摘要</summary>
深度学习模型中的剩余神经网络是当前领导的模型之一。它们的连续深度对应的神经泛化方程（ODE）也广泛使用。尽管它们的成功，但是这两种模型之间的数学基础仍然缺乏坚实的理论基础。在这篇文章中，我们向这个方向发展了一个隐式正则化，使得深度剩余神经网络向神经泛化方程进行傅里叶训练。我们证明，如果网络在训练时 initialized 为神经泛化方程的离散化，那么这种离散化会在训练过程中保持不变。我们的结果适用于 finite 训练时间，以及训练时间趋向于无穷大 provided that the network satisfies a Polyak-Lojasiewicz condition。这种条件适用于一家 residual 网络，其中 residual 是两层感知器，并且具有只有线性的宽度过参数。这意味着梯度流会 converges to a global minimum。 numerics 实验证明了我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Adversarial-MixUp-for-Few-Shot-Unsupervised-Domain-Adaptation"><a href="#Spectral-Adversarial-MixUp-for-Few-Shot-Unsupervised-Domain-Adaptation" class="headerlink" title="Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation"></a>Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01207">http://arxiv.org/abs/2309.01207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/SAMix">https://github.com/RPIDIAL/SAMix</a></li>
<li>paper_authors: Jiajin Zhang, Hanqing Chao, Amit Dhurandhar, Pin-Yu Chen, Ali Tajer, Yangyang Xu, Pingkun Yan</li>
<li>For: This paper aims to address the problem of few-shot unsupervised domain adaptation (FSUDA) in clinical applications, where only a limited number of unlabeled target domain samples are available for training.* Methods: The proposed method uses a spectral sensitivity map to characterize the generalization weaknesses of models in the frequency domain, and a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images that effectively suppress the model’s sensitivity, leading to improved model generalizability in the target domain.* Results: The proposed method was demonstrated and evaluated on multiple tasks using several public datasets, showing improved performance compared to existing methods.<details>
<summary>Abstract</summary>
Domain shift is a common problem in clinical applications, where the training images (source domain) and the test images (target domain) are under different distributions. Unsupervised Domain Adaptation (UDA) techniques have been proposed to adapt models trained in the source domain to the target domain. However, those methods require a large number of images from the target domain for model training. In this paper, we propose a novel method for Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a limited number of unlabeled target domain samples are available for training. To accomplish this challenging task, first, a spectral sensitivity map is introduced to characterize the generalization weaknesses of models in the frequency domain. We then developed a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images to effectively suppresses the model sensitivity, which leads to improved model generalizability in the target domain. We demonstrated the proposed method and rigorously evaluated its performance on multiple tasks using several public datasets.
</details>
<details>
<summary>摘要</summary>
域名转换是在医学应用中的一个常见问题，source domain 和 target domain 的图像分布不同。不supervised Domain Adaptation（UDA）技术已经提出来适应source domain 中训练的模型到 target domain。然而，这些方法需要大量的目标域图像来训练模型。在这篇论文中，我们提出了一种新的方法，即 Few-Shot Unsupervised Domain Adaptation（FSUDA），只需要有限数量的目标域样本来训练模型。为了完成这个复杂的任务，我们首先引入了一个spectral sensitivity map来Characterize模型在频率域的泛化弱点。然后，我们开发了一种Sensitivity-guided Spectral Adversarial MixUp（SAMix）方法，可以生成目标风格的图像，以有效地减少模型的敏感性，从而提高模型在目标域的泛化性。我们在多个任务上证明了我们的方法的性能，并且进行了严格的评估。
</details></li>
</ul>
<hr>
<h2 id="LogGPT-Exploring-ChatGPT-for-Log-Based-Anomaly-Detection"><a href="#LogGPT-Exploring-ChatGPT-for-Log-Based-Anomaly-Detection" class="headerlink" title="LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection"></a>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01189">http://arxiv.org/abs/2309.01189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong Yang, Depei Qian<br>for: 这个研究旨在应用ChatGPT进行日志型异常检测。methods: 本研究使用ChatGPT的语言解释能力，将知识传递到日志型异常检测中。results: LogGPT在BGL和Spirit datasets上的实验结果显示了良好的性能和可解释性。<details>
<summary>Abstract</summary>
The increasing volume of log data produced by software-intensive systems makes it impractical to analyze them manually. Many deep learning-based methods have been proposed for log-based anomaly detection. These methods face several challenges such as high-dimensional and noisy log data, class imbalance, generalization, and model interpretability. Recently, ChatGPT has shown promising results in various domains. However, there is still a lack of study on the application of ChatGPT for log-based anomaly detection. In this work, we proposed LogGPT, a log-based anomaly detection framework based on ChatGPT. By leveraging the ChatGPT's language interpretation capabilities, LogGPT aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection. We conduct experiments to evaluate the performance of LogGPT and compare it with three deep learning-based methods on BGL and Spirit datasets. LogGPT shows promising results and has good interpretability. This study provides preliminary insights into prompt-based models, such as ChatGPT, for the log-based anomaly detection task.
</details>
<details>
<summary>摘要</summary>
“随着软件数据量的增加，手动分析成本过高。许多深度学习方法已经被提出供征兆检测。这些方法面临许多挑战，例如高维度和噪音的日志数据、类别对称性、通过率和模型解释性。最近，ChatGPT已经在不同领域表现出色。然而，还没有对ChatGPT在日志征兆检测中的应用进行了研究。在这个工作中，我们提出了LogGPT，基于ChatGPT的日志征兆检测框架。通过利用ChatGPT的语言解释能力，LogGPT希望能够将大规模数据库中的知识转移到日志征兆检测中。我们对LogGPT进行了实验，并与三种深度学习方法进行比较。LogGPT的成果获得了良好的表现，并且具有良好的解释性。这个研究提供了对于提示型模型，例如ChatGPT，在日志征兆检测任务中的首选点。”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cognition-Mode-Aware-Variational-Representation-Learning-Framework-for-Knowledge-Tracing"><a href="#Cognition-Mode-Aware-Variational-Representation-Learning-Framework-for-Knowledge-Tracing" class="headerlink" title="Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing"></a>Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01179">http://arxiv.org/abs/2309.01179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zmy-9/CMVF">https://github.com/zmy-9/CMVF</a></li>
<li>paper_authors: Moyu Zhang, Xinning Zhu, Chunhong Zhang, Feng Pan, Wenchen Qian, Hui Zhao</li>
<li>for: 提高个性化学习中的知识追踪（KT）任务的robustness，解决数据稀缺性问题。</li>
<li>methods: 提出了一种基于变量表示学习框架（CMVF），使用 probabilistic model生成每个学生的分布，考虑学生偏好的uncertainty，并通过变量推理（VI）来估算学生的分布。另外，还引入了认知模式相关的多omial分布作为先验知识，以避免学生偏好的过拟合。</li>
<li>results: 经过广泛的实验 validate CMVF可以有效地帮助现有KT方法学习更加Robust的学生表示。<details>
<summary>Abstract</summary>
The Knowledge Tracing (KT) task plays a crucial role in personalized learning, and its purpose is to predict student responses based on their historical practice behavior sequence. However, the KT task suffers from data sparsity, which makes it challenging to learn robust representations for students with few practice records and increases the risk of model overfitting. Therefore, in this paper, we propose a Cognition-Mode Aware Variational Representation Learning Framework (CMVF) that can be directly applied to existing KT methods. Our framework uses a probabilistic model to generate a distribution for each student, accounting for uncertainty in those with limited practice records, and estimate the student's distribution via variational inference (VI). In addition, we also introduce a cognition-mode aware multinomial distribution as prior knowledge that constrains the posterior student distributions learning, so as to ensure that students with similar cognition modes have similar distributions, avoiding overwhelming personalization for students with few practice records. At last, extensive experimental results confirm that CMVF can effectively aid existing KT methods in learning more robust student representations. Our code is available at https://github.com/zmy-9/CMVF.
</details>
<details>
<summary>摘要</summary>
《知识追踪（KT）任务在个性化学习中扮演着关键性的角色，其目的是预测学生的回答基于他们的历史实践行为序列。然而，KT任务受到数据稀缺的影响，这使得学习学生的Robust表示变得困难，增加模型适应性的风险。因此，在这篇论文中，我们提出了一种基于Variational Representation Learning Framework（CMVF）的认知模式自适应学习框架。我们的框架使用一个概率模型来生成每个学生的分布，考虑到有限实践记录中的不确定性，并通过变量推理（VI）来估算学生的分布。此外，我们还引入了认知模式自适应多omial分布作为先验知识，以避免学生 WITH few practice records 的过度个性化。最后，我们进行了广泛的实验，证明了CMVF可以有效地帮助现有KT方法学习更加Robust的学生表示。我们的代码可以在https://github.com/zmy-9/CMVF上获取。》Note: Simplified Chinese is used here, which is a standardized form of Chinese that is widely used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="FusionAI-Decentralized-Training-and-Deploying-LLMs-with-Massive-Consumer-Level-GPUs"><a href="#FusionAI-Decentralized-Training-and-Deploying-LLMs-with-Massive-Consumer-Level-GPUs" class="headerlink" title="FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs"></a>FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01172">http://arxiv.org/abs/2309.01172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu</li>
<li>for: 这个论文旨在探讨如何使用consumer-level GPUs进行大语言模型（LLMs）的训练和部署，并提供隐私保护。</li>
<li>methods: 本论文使用了一个分布式系统，允许consumer-level GPUs在预读、测试和精度调整中参与LLMs的训练。这个系统面临了一些挑战，包括限制的CPU和GPU内存、低网络带宽和对等设备和设备多样性的影响。</li>
<li>results: 研究人员发现使用50个RTX 3080 GPUs可以 дости到和4个H100 GPUs相似的运算能力，这些GPUs更加昂贵。<details>
<summary>Abstract</summary>
The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs. However, consumer-level GPUs, which constitute a larger market share, are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth. Additionally, users may have privacy concerns when interacting with remote LLMs. In this paper, we envision a decentralized system unlocking the potential vast untapped consumer-level GPUs in pre-training, inference and fine-tuning of LLMs with privacy protection. However, this system faces critical challenges, including limited CPU and GPU memory, low network bandwidth, the variability of peer and device heterogeneity. To address these challenges, our system design incorporates: 1) a broker with backup pool to implement dynamic join and quit of computing providers; 2) task scheduling with hardware performance to improve system efficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to achieve model and task universality; 4) abstracting intermediate represention and execution planes to ensure compatibility of various devices and deep learning (DL) frameworks. Our performance analysis demonstrates that 50 RTX 3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are significantly more expensive.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的存储和计算需求在快速增长，超过硬件的开发速度，使得没有大规模高级GPU的人无法训练或部署LLM。然而，消费级GPU，占据市场份额的大多数，通常在LLM中被忽略，因为它们的计算性能较弱，存储容量较小，通信带宽较低。此外，用户可能有隐私问题与远程LLM进行交互。在这篇论文中，我们描述了一个分布式系统，使得大量的消费级GPU可以在预训练、推理和细化LLM中发挥作用，同时保护隐私。然而，这个系统面临着严重的挑战，包括CPU和GPU内存有限，网络带宽低，积分和设备多样性。为解决这些挑战，我们的系统设计包括：1）一个备用池的执行者来实现动态加入和退出计算提供商；2）根据硬件性能进行任务调度，提高系统效率；3）将ML过程抽象为导向无环图（DAG），实现模型和任务通用性；4）抽象中间表示和执行计划，确保各种设备和深度学习（DL）框架的兼容性。我们的性能分析表明，50个RTX 3080 GPU可以实现与4个H100 GPU相当的吞吐量，后者具有更高的成本。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Learning-on-Multimodal-Knowledge-Graphs"><a href="#End-to-End-Learning-on-Multimodal-Knowledge-Graphs" class="headerlink" title="End-to-End Learning on Multimodal Knowledge Graphs"></a>End-to-End Learning on Multimodal Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01169">http://arxiv.org/abs/2309.01169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/wxwilcke/mrgcn">https://gitlab.com/wxwilcke/mrgcn</a></li>
<li>paper_authors: W. X. Wilcke, P. Bloem, V. de Boer, R. H. van t Veer</li>
<li>for: 学习hetereogeneous知识图的数据科学家</li>
<li>methods: 提议一种多modalmessage passing网络，不仅从图的结构学习端到端，还从不同类型的多modal节点特征学习embeds</li>
<li>results: 实现并测试模型在node classification和link prediction任务上，并进行了 inverse ablation study，结果表明可以从任何不同的知识图学习，并且包含多modal信息可以显著改善性能，但是具体取决于数据的特点。<details>
<summary>Abstract</summary>
Knowledge graphs enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case we lose potentially relevant information which could have otherwise been exploited by our learning methods. We propose a multimodal message passing network which not only learns end-to-end from the structure of graphs, but also from their possibly divers set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-worlds datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.
</details>
<details>
<summary>摘要</summary>
知识图Enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case, we lose potentially relevant information that could have otherwise been exploited by our learning methods. We propose a multimodal message passing network that not only learns end-to-end from the structure of graphs but also from their possibly diverse set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images, and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-world datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.Here's the translation in Traditional Chinese as well:知识图Enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case, we lose potentially relevant information that could have otherwise been exploited by our learning methods. We propose a multimodal message passing network that not only learns end-to-end from the structure of graphs but also from their possibly diverse set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images, and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-world datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.
</details></li>
</ul>
<hr>
<h2 id="Symbolically-integrating-tensor-networks-over-various-random-tensors-–-the-second-version-of-Python-RTNI"><a href="#Symbolically-integrating-tensor-networks-over-various-random-tensors-–-the-second-version-of-Python-RTNI" class="headerlink" title="Symbolically integrating tensor networks over various random tensors – the second version of Python RTNI"></a>Symbolically integrating tensor networks over various random tensors – the second version of Python RTNI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01167">http://arxiv.org/abs/2309.01167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/motohisafukuda/pyrtni2">https://github.com/motohisafukuda/pyrtni2</a></li>
<li>paper_authors: Motohisa Fukuda</li>
<li>for: 本研究旨在扩展Python中的RTNI，使其可以对响应杂度分布的 unitary 矩阵进行Symbolic интеграル。</li>
<li>methods: 本文使用 Haar-distributed 对偶 orthogonal matrices 和实数和复数正态幂 tensors 的符号 интеграル，并可以导出 tensor networks 格式的 TensorNetwork，以便进一步计算。</li>
<li>results: 本文解释了 PyRTNI2 的数学基础和如何使用它进行tensor network计算，并展示了它可以处理低维度的 tensor network 计算，其中 Weingarten 函数与高维度不同。 tutorial notebooks 可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/MotohisaFukuda/PyRTNI2%E3%80%82">https://github.com/MotohisaFukuda/PyRTNI2。</a><details>
<summary>Abstract</summary>
We are upgrading the Python-version of RTNI, which symbolically integrates tensor networks over the Haar-distributed unitary matrices. Now, PyRTNI2 can treat the Haar-distributed orthogonal matrices and the real and complex normal Gaussian tensors as well. Moreover, it can export tensor networks in the format of TensorNetwork so that one can make further calculations with concrete tensors, even for low dimensions, where the Weingarten functions differ from the ones for high dimensions. The tutorial notebooks are found at GitHub: https://github.com/MotohisaFukuda/PyRTNI2. In this paper, we explain maths behind the program and show what kind of tensor network calculations can be made with it. For the former, we interpret the element-wise moment calculus of the above random matrices and tensors in terms of tensor network diagrams, and argue that the view is natural, relating delta functions in the calculus to edges in tensor network diagrams.
</details>
<details>
<summary>摘要</summary>
我们正在升级Python中的RTNI，这个Symbolic Integration of Tensor Networks over Haar-Distributed Unitary Matrices。现在PyRTNI2可以处理Haar分布的正交矩阵和实数和复数正态分布矩阵，同时还可以导出tensor网络在TensorNetwork格式下，以便进行进一步的计算，即使是低维度的情况下，其中Weingarten函数与高维度不同。教程Notebook可以在GitHub上找到：https://github.com/MotohisaFukuda/PyRTNI2。在这篇论文中，我们解释了PyRTNI2的数学基础和可以通过它进行哪些tensor网络计算。对于前者，我们将元素级极限积分算符和矩阵和tensor的 delta函数相关联，并证明这种视图是自然的。
</details></li>
</ul>
<hr>
<h2 id="Noise-robust-speech-emotion-recognition-with-signal-to-noise-ratio-adapting-speech-enhancement"><a href="#Noise-robust-speech-emotion-recognition-with-signal-to-noise-ratio-adapting-speech-enhancement" class="headerlink" title="Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement"></a>Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01164">http://arxiv.org/abs/2309.01164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Julia Hirschberg, Yu Tsao</li>
<li>for: 提高speech emotion recognition（SER）系统的听力 robustness，使其能够在干扰声中进行正确的情绪识别。</li>
<li>methods: 提出了一种Noise Robust Speech Emotion Recognition（NRSER）系统，利用speech enhancement（SE）技术来减少干扰声，并通过信号听力比例（SNR）级别检测结构和波形重建策略来减少SE对无或少干扰声的影响。</li>
<li>results: 实验结果表明，NRSER系统可以有效提高听力Robustness，包括避免系统对干扰声只 consisting的信号进行情绪识别。此外，提出的SNR级别检测结构可以独立地应用于数据选择等任务。<details>
<summary>Abstract</summary>
Speech emotion recognition (SER) often experiences reduced performance due to background noise. In addition, making a prediction on signals with only background noise could undermine user trust in the system. In this study, we propose a Noise Robust Speech Emotion Recognition system, NRSER. NRSER employs speech enhancement (SE) to effectively reduce the noise in input signals. Then, the signal-to-noise-ratio (SNR)-level detection structure and waveform reconstitution strategy are introduced to reduce the negative impact of SE on speech signals with no or little background noise. Our experimental results show that NRSER can effectively improve the noise robustness of the SER system, including preventing the system from making emotion recognition on signals consisting solely of background noise. Moreover, the proposed SNR-level detection structure can be used individually for tasks such as data selection.
</details>
<details>
<summary>摘要</summary>
听音识别（SER）经常受到背景噪声的影响，这会导致系统的性能下降。此外，基于背景噪声的预测也可能会让用户对系统失去信任。在本研究中，我们提出了一种噪声RobustSpeech Emotion Recognition系统（NRSER）。NRSER使用了speech enhancement（SE）技术来有效减少输入信号中的噪声。然后，我们引入了信号噪声比例检测结构和波形重建策略，以减少SE对于无或少量背景噪声的输入信号的负面影响。我们的实验结果表明，NRSER可以有效提高听音识别系统的噪声抗性，包括避免系统对背景噪声 alone的预测。此外，我们的提案的SNR级别检测结构也可以单独用于任务such as data selection。
</details></li>
</ul>
<hr>
<h2 id="An-Accurate-Graph-Generative-Model-with-Tunable-Features"><a href="#An-Accurate-Graph-Generative-Model-with-Tunable-Features" class="headerlink" title="An Accurate Graph Generative Model with Tunable Features"></a>An Accurate Graph Generative Model with Tunable Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01158">http://arxiv.org/abs/2309.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takahiro Yokoyama, Yoshiki Sato, Sho Tsugawa, Kohei Watabe</li>
<li>for: 这 paper 是为了提出一种基于实际网络数据的图生成模型，以提高现有图生成模型的精度和灵活性。</li>
<li>methods: 该 paper 使用了一种新的Feedback机制，以及独立地训练图feature和生成图feature的方法，以提高图生成模型的精度。</li>
<li>results: 实际tests 表明，该方法可以更高精度地调整图feature，并且与传统模型相比，生成的图更加准确地满足用户需求。<details>
<summary>Abstract</summary>
A graph is a very common and powerful data structure used for modeling communication and social networks. Models that generate graphs with arbitrary features are important basic technologies in repeated simulations of networks and prediction of topology changes. Although existing generative models for graphs are useful for providing graphs similar to real-world graphs, graph generation models with tunable features have been less explored in the field. Previously, we have proposed GraphTune, a generative model for graphs that continuously tune specific graph features of generated graphs while maintaining most of the features of a given graph dataset. However, the tuning accuracy of graph features in GraphTune has not been sufficient for practical applications. In this paper, we propose a method to improve the accuracy of GraphTune by adding a new mechanism to feed back errors of graph features of generated graphs and by training them alternately and independently. Experiments on a real-world graph dataset showed that the features in the generated graphs are accurately tuned compared with conventional models.
</details>
<details>
<summary>摘要</summary>
graph是一种非常常见和强大的数据结构，用于模型社交和通信网络。生成 graphs 的模型是重要的基础技术，它们可以提供与实际世界 graphs 相似的图像。然而，已有的生成模型 для graphs 的特性是可以调整的，在预测图像结构变化和重复 simulations 中具有重要的意义。在这篇论文中，我们提出了 GraphTune，一种可以不断调整特定图像特性的生成模型。然而，GraphTune 中的调整精度没有达到实际应用中的要求。在这篇论文中，我们提出了一种方法，通过错误反馈和独立地训练来提高 GraphTune 的调整精度。实验表明，对实际图像数据进行训练后，GraphTune 可以准确地调整图像特性。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-machine-learning-based-sampling-motivated-by-lattice-quantum-chromodynamics"><a href="#Advances-in-machine-learning-based-sampling-motivated-by-lattice-quantum-chromodynamics" class="headerlink" title="Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics"></a>Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01156">http://arxiv.org/abs/2309.01156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Cranmer, Gurtej Kanwar, Sébastien Racanière, Danilo J. Rezende, Phiala E. Shanahan</li>
<li>for: 这篇论文是关于计算物理学中的生成机器学习（ML）模型，用于实现量子色动力学理论中的结构和物质之间的相互作用计算。</li>
<li>methods: 论文使用的方法包括生成机器学习模型，用于解决物理学中的复杂Symmetry和精确性问题。</li>
<li>results: 论文的结果表明，使用生成机器学习模型可以实现量子色动力学理论中的计算，并且可以在最大的超级计算机上扩展自定义的ML架构。<details>
<summary>Abstract</summary>
Sampling from known probability distributions is a ubiquitous task in computational science, underlying calculations in domains from linguistics to biology and physics. Generative machine-learning (ML) models have emerged as a promising tool in this space, building on the success of this approach in applications such as image, text, and audio generation. Often, however, generative tasks in scientific domains have unique structures and features -- such as complex symmetries and the requirement of exactness guarantees -- that present both challenges and opportunities for ML. This Perspective outlines the advances in ML-based sampling motivated by lattice quantum field theory, in particular for the theory of quantum chromodynamics. Enabling calculations of the structure and interactions of matter from our most fundamental understanding of particle physics, lattice quantum chromodynamics is one of the main consumers of open-science supercomputing worldwide. The design of ML algorithms for this application faces profound challenges, including the necessity of scaling custom ML architectures to the largest supercomputers, but also promises immense benefits, and is spurring a wave of development in ML-based sampling more broadly. In lattice field theory, if this approach can realize its early promise it will be a transformative step towards first-principles physics calculations in particle, nuclear and condensed matter physics that are intractable with traditional approaches.
</details>
<details>
<summary>摘要</summary>
估算known概率分布是计算科学中的一项普遍任务，覆盖从语言学到生物和物理等领域的计算。生成机器学习（ML）模型已经在这些应用中获得成功，如图像、文本和音频生成。然而，在科学领域中的生成任务通常具有特殊的结构和特点，如复杂的对称和精确保证的需求，这些特点都对ML领域呈现挑战和机遇。这篇观点文章总结了基于ML的估算，尤其是基于粒子物理学的粒子场理论。通过实现粒子物理学中的结构和物质之间的互动的计算，粒子场理论是全球开源超级计算机器中最大的用户之一。设计ML算法 для这个应用面临着挑战，包括扩展自定义ML架构到最大的超级计算机器，但也承诺巨大的利益，这已经导致了ML基于估算的发展。在粒子场理论中，如果这种方法实现其早期的承诺，它将是对初始原理物理计算的转变步骤，包括particle、核物理和 Condensed matter物理等领域的计算，这些计算是使用传统方法不可能进行的。
</details></li>
</ul>
<hr>
<h2 id="FedFwd-Federated-Learning-without-Backpropagation"><a href="#FedFwd-Federated-Learning-without-Backpropagation" class="headerlink" title="FedFwd: Federated Learning without Backpropagation"></a>FedFwd: Federated Learning without Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01150">http://arxiv.org/abs/2309.01150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghwan Park, Dahun Shin, Jinseok Chung, Namhoon Lee</li>
<li>for: 提高 Federated Learning 中客户端的训练效率，采用 Hinton (2022) 提出的无后向传播算法 Forward Forward。</li>
<li>methods: 使用层 wise 本地更新方法，不需要存储所有间接活动值。</li>
<li>results: 在 MNIST 和 CIFAR-10 标准数据集上进行了多种实验，与其他基于后向传播的 Federated Learning 方法相比，FedFwd 表现竞争力强。<details>
<summary>Abstract</summary>
In federated learning (FL), clients with limited resources can disrupt the training efficiency. A potential solution to this problem is to leverage a new learning procedure that does not rely on backpropagation (BP). We present a novel approach to FL called FedFwd that employs a recent BP-free method by Hinton (2022), namely the Forward Forward algorithm, in the local training process. FedFwd can reduce a significant amount of computations for updating parameters by performing layer-wise local updates, and therefore, there is no need to store all intermediate activation values during training. We conduct various experiments to evaluate FedFwd on standard datasets including MNIST and CIFAR-10, and show that it works competitively to other BP-dependent FL methods.
</details>
<details>
<summary>摘要</summary>
在联合学习（FL）中，客户端具有有限资源可能会影响训练效率。我们提出了一种新的学习方法，不需要梯度反射（BP）。我们称之为FedFwd，它使用了2022年Hinton提出的一种BP自由方法，即前进前进算法，在本地训练过程中进行层次化更新。因此，无需在训练过程中存储所有中间活动值。我们在标准数据集MNIST和CIFAR-10上进行了多种实验，并示出FedFwd与其他BP依赖的FL方法相比具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Sequence-Clustering"><a href="#Interpretable-Sequence-Clustering" class="headerlink" title="Interpretable Sequence Clustering"></a>Interpretable Sequence Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01140">http://arxiv.org/abs/2309.01140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jd445/Interpretable-Sequence-Clustering-Tree">https://github.com/jd445/Interpretable-Sequence-Clustering-Tree</a></li>
<li>paper_authors: Junjie Dong, Xinyi Yang, Mudi Jiang, Lianyu Hu, Zengyou He</li>
<li>for: 提高序列划分 interpretability， Addressing the challenge of lacking interpretability in sequence clustering.</li>
<li>methods:  combining sequential patterns with a concise and interpretable tree structure, leveraging k-1 patterns to generate k leaf nodes.</li>
<li>results: 提供了一个Intuitive explanation on how each cluster is formed, delivering fast and accurate cluster assignments on 14 real-world data sets.Please note that the results are in Simplified Chinese, as requested.<details>
<summary>Abstract</summary>
Categorical sequence clustering plays a crucial role in various fields, but the lack of interpretability in cluster assignments poses significant challenges. Sequences inherently lack explicit features, and existing sequence clustering algorithms heavily rely on complex representations, making it difficult to explain their results. To address this issue, we propose a method called Interpretable Sequence Clustering Tree (ISCT), which combines sequential patterns with a concise and interpretable tree structure. ISCT leverages k-1 patterns to generate k leaf nodes, corresponding to k clusters, which provides an intuitive explanation on how each cluster is formed. More precisely, ISCT first projects sequences into random subspaces and then utilizes the k-means algorithm to obtain high-quality initial cluster assignments. Subsequently, it constructs a pattern-based decision tree using a boosting-based construction strategy in which sequences are re-projected and re-clustered at each node before mining the top-1 discriminative splitting pattern. Experimental results on 14 real-world data sets demonstrate that our proposed method provides an interpretable tree structure while delivering fast and accurate cluster assignments.
</details>
<details>
<summary>摘要</summary>
总结序列汇编在不同领域中扮演着重要角色，但缺乏汇编结果的解释性对于汇编成果带来重大挑战。序列本身缺乏明确的特征，现有的序列汇编算法严重依赖复杂的表示方式，这使得汇编结果的解释变得困难。为了解决这个问题，我们提出了一种方法 called Interpretable Sequence Clustering Tree (ISCT)，它结合了序列特征和简洁可解的树结构。ISCT利用k-1个模式来生成k个叶点，对应k个汇编结果，这提供了汇编结果的直观解释。更精确地说，ISCT首先将序列 проек到随机的子空间中，然后使用k-means算法获得高品质的初始汇编分配。接着，它使用boosting-based建构策略在每个节点上建构一个基于模式的决策树。在每个节点上，序列会被重新对应并重新汇编，直到获得最佳的一个分类分支模式。实验结果显示，我们的提案方法可以在14个真实世界数据集上提供可解的树结构，同时实现快速和精准的汇编结果。
</details></li>
</ul>
<hr>
<h2 id="Financial-Fraud-Detection-using-Quantum-Graph-Neural-Networks"><a href="#Financial-Fraud-Detection-using-Quantum-Graph-Neural-Networks" class="headerlink" title="Financial Fraud Detection using Quantum Graph Neural Networks"></a>Financial Fraud Detection using Quantum Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01127">http://arxiv.org/abs/2309.01127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouhaila Innan, Abhishek Sawaika, Ashim Dhor, Siddhant Dutta, Sairupa Thota, Husayn Gokal, Nandan Patel, Muhammad Al-Zafar Khan, Ioannis Theodonis, Mohamed Bennai</li>
<li>for: 防止金融欺诈和维护金融机构的名誉</li>
<li>methods: 使用量子图网络（Quantum Graph Neural Networks，QGNNs）和可变量子电路（Variational Quantum Circuits，VQC）进行金融欺诈探测</li>
<li>results: QGNNs achieved an AUC of $0.85$, outperforming classical GNNs<details>
<summary>Abstract</summary>
Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Financial fraud detection" is translated as "金融诈骗检测" (jīnróng kuòhòu jiǎnèsè)* "conventional methods" is translated as "传统方法" (chuánliàng fāngyì)* "Quantum Graph Neural Networks" is translated as "量子图神经网络" (liàngzǐ túshén xīnnéijī)* "Variational Quantum Circuits" is translated as "变量量子电路" (biàngliàng liàngzǐ diànluò)* "AUC" is translated as "AUC" (AUC)* "real-world financial fraud detection dataset" is translated as "真实的金融诈骗检测数据集" (zhēnshí de jīnróng kuòhòu jiǎnèsè numérics)
</details></li>
</ul>
<hr>
<h2 id="AutoML-GPT-Large-Language-Model-for-AutoML"><a href="#AutoML-GPT-Large-Language-Model-for-AutoML" class="headerlink" title="AutoML-GPT: Large Language Model for AutoML"></a>AutoML-GPT: Large Language Model for AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01125">http://arxiv.org/abs/2309.01125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Da Tsai, Yu-Che Tsai, Bo-Wei Huang, Chun-Pai Yang, Shou-De Lin</li>
<li>for: 这个论文旨在提供一个名为AutoML-GPT的框架，用于自动化机器学习 pipeline。</li>
<li>methods: 该框架使用了一系列的数据处理技术、特征工程方法和模型选择算法，并通过对话式界面，让用户可以指定自己的需求、约束和评价指标。</li>
<li>results: 通过对多种数据集进行实验，我们表明了AutoML-GPT可以减少机器学习任务的时间和努力，同时可以提供有价值的意见、预测 potential pitfalls 和解决常见的模型训练问题。<details>
<summary>Abstract</summary>
With the emerging trend of GPT models, we have established a framework called AutoML-GPT that integrates a comprehensive set of tools and libraries. This framework grants users access to a wide range of data preprocessing techniques, feature engineering methods, and model selection algorithms. Through a conversational interface, users can specify their requirements, constraints, and evaluation metrics. Throughout the process, AutoML-GPT employs advanced techniques for hyperparameter optimization and model selection, ensuring that the resulting model achieves optimal performance. The system effectively manages the complexity of the machine learning pipeline, guiding users towards the best choices without requiring deep domain knowledge. Through our experimental results on diverse datasets, we have demonstrated that AutoML-GPT significantly reduces the time and effort required for machine learning tasks. Its ability to leverage the vast knowledge encoded in large language models enables it to provide valuable insights, identify potential pitfalls, and suggest effective solutions to common challenges faced during model training.
</details>
<details>
<summary>摘要</summary>
With the emerging trend of GPT models, we have established a framework called AutoML-GPT that integrates a comprehensive set of tools and libraries. This framework grants users access to a wide range of data preprocessing techniques, feature engineering methods, and model selection algorithms. Through a conversational interface, users can specify their requirements, constraints, and evaluation metrics. Throughout the process, AutoML-GPT employs advanced techniques for hyperparameter optimization and model selection, ensuring that the resulting model achieves optimal performance. The system effectively manages the complexity of the machine learning pipeline, guiding users towards the best choices without requiring deep domain knowledge. Through our experimental results on diverse datasets, we have demonstrated that AutoML-GPT significantly reduces the time and effort required for machine learning tasks. Its ability to leverage the vast knowledge encoded in large language models enables it to provide valuable insights, identify potential pitfalls, and suggest effective solutions to common challenges faced during model training.Here's the translation in Traditional Chinese:随着GPT模型的兴起，我们已经建立了一个名为AutoML-GPT的框架，这个框架集成了丰富的工具和库。这个框架给用户提供了丰富的数据预处理技术、特征工程方法和模型选择算法。通过对话式界面，用户可以指定他们的需求、限制和评估度量。在过程中，AutoML-GPT使用进步的超参数优化和模型选择技术，以确保所得到的模型实现最佳性能。系统有效地管理机器学习管线的复杂性，帮助用户选择最佳选择而无需深入领域知识。我们在多个 dataset 上进行了实验，展示了AutoML-GPT可以很大幅降低机器学习任务的时间和努力。它的能力将大量的语言模型中的知识转移到机器学习任务中，帮助提供有价的见解、识别潜在的问题和建议常见的挑战解决方案。
</details></li>
</ul>
<hr>
<h2 id="AI-driven-B-cell-Immunotherapy-Design"><a href="#AI-driven-B-cell-Immunotherapy-Design" class="headerlink" title="AI driven B-cell Immunotherapy Design"></a>AI driven B-cell Immunotherapy Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01122">http://arxiv.org/abs/2309.01122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruna Moreira da Silva, David B. Ascher, Nicholas Geard, Douglas E. V. Pires</li>
<li>for: 本研究旨在概述人工智能和机器学习技术在B细胞免疫疗法设计方面的进步，包括线性和结构融合识别、抗体设计等方法。</li>
<li>methods: 本文使用的数据源、评价指标和方法有哪些，以及它们的意义和局限性，并讨论了主要的挑战。</li>
<li>results: 本研究总结了机器学习基于的工具和框架在B细胞免疫疗法设计方面的进步，并评估了这些工具的 significanc和局限性。<details>
<summary>Abstract</summary>
Antibodies, a prominent class of approved biologics, play a crucial role in detecting foreign antigens. The effectiveness of antigen neutralisation and elimination hinges upon the strength, sensitivity, and specificity of the paratope-epitope interaction, which demands resource-intensive experimental techniques for characterisation. In recent years, artificial intelligence and machine learning methods have made significant strides, revolutionising the prediction of protein structures and their complexes. The past decade has also witnessed the evolution of computational approaches aiming to support immunotherapy design. This review focuses on the progress of machine learning-based tools and their frameworks in the domain of B-cell immunotherapy design, encompassing linear and conformational epitope prediction, paratope prediction, and antibody design. We mapped the most commonly used data sources, evaluation metrics, and method availability and thoroughly assessed their significance and limitations, discussing the main challenges ahead.
</details>
<details>
<summary>摘要</summary>
抗体，一种常见的批准生物学药物，在检测外源抗原方面发挥关键作用。抗原中和消除的效iveness取决于抗体和抗原之间的复合体-蛋白质结合的强度、敏感度和特异性，这需要资源充足的实验技术来Characterization。在过去的十年中，人工智能和机器学习方法得到了 significanthuge strides，对蛋白质结构和复合物的预测做出了重大贡献。本文将评论机器学习基本工具和框架在B细胞免疫治疗设计领域的进步，包括线性和 conformational epitope预测、蛋白质预测、抗体设计。我们对最常用的数据源、评价指标和方法可用性进行了地图，并且详细评估了它们的重要性和局限性，讨论了主要的挑战。
</details></li>
</ul>
<hr>
<h2 id="Double-Clipping-Less-Biased-Variance-Reduction-in-Off-Policy-Evaluation"><a href="#Double-Clipping-Less-Biased-Variance-Reduction-in-Off-Policy-Evaluation" class="headerlink" title="Double Clipping: Less-Biased Variance Reduction in Off-Policy Evaluation"></a>Double Clipping: Less-Biased Variance Reduction in Off-Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01120">http://arxiv.org/abs/2309.01120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Malte Lichtenberg, Alexander Buchholz, Giuseppe Di Benedetto, Matteo Ruffini, Ben London</li>
<li>for: 提高counterfactual off-policy estimator的精度和稳定性</li>
<li>methods: 使用clipping variance-reduction技术，并提出double clipping扩展来减少偏误</li>
<li>results: double clipping可以减少总偏误，保持原始 estimator 的 variance reduction 性能<details>
<summary>Abstract</summary>
"Clipping" (a.k.a. importance weight truncation) is a widely used variance-reduction technique for counterfactual off-policy estimators. Like other variance-reduction techniques, clipping reduces variance at the cost of increased bias. However, unlike other techniques, the bias introduced by clipping is always a downward bias (assuming non-negative rewards), yielding a lower bound on the true expected reward. In this work we propose a simple extension, called $\textit{double clipping}$, which aims to compensate this downward bias and thus reduce the overall bias, while maintaining the variance reduction properties of the original estimator.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Carbon-Emission-Prediction-and-Clean-Industry-Transformation-Based-on-Machine-Learning-A-Case-Study-of-Sichuan-Province"><a href="#Carbon-Emission-Prediction-and-Clean-Industry-Transformation-Based-on-Machine-Learning-A-Case-Study-of-Sichuan-Province" class="headerlink" title="Carbon Emission Prediction and Clean Industry Transformation Based on Machine Learning: A Case Study of Sichuan Province"></a>Carbon Emission Prediction and Clean Industry Transformation Based on Machine Learning: A Case Study of Sichuan Province</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01115">http://arxiv.org/abs/2309.01115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanming Zhang, Xiaoxue Wang, Yonghang Chen</li>
<li>for: 本研究使用矩阵正常化处理2000-2019年四川省46个关键产业的能源消耗数据，并使用DBSCAN封顶 clustering方法对产业分类。</li>
<li>methods: 本研究使用了DBSCAN封顶 clustering方法和罚款回归模型，以便控制过拟合、处理高维数据和选择特征。</li>
<li>results: 研究发现，第二个群是煤炭生产的需求导致的高排放，而汽油和焦炭生产的群也有显著的排放。根据这些结果，提出了清洁煤炭技术、交通管理、钢铁生产中的煤电取代和产业标准化等减排策略。<details>
<summary>Abstract</summary>
This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to better inform decision-making using algorithms like DBSCAN and penalized regression models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这项研究处理了四川省46个关键产业的能源消耗数据从2000年到2019年，使用矩阵正常化。使用DBSCAN划分 clustering，并确定了16个特征类来对产业进行 объектив分组。使用了惩罚回归模型，具有控制过拟合、处理高维数据和选择特征的优点。研究结果显示，第二个带有煤炭的群组 emission 最高，这是因为生产需要。汽油销售和焦炭销售群组的排放也很 significanthigh。根据这些发现，研究提出了减排建议，如使用清洁煤炭技术、交通管理、钢铁生产中的煤电取代、产业标准化。研究引入了无监督学习，可以 объектив地选择因素，以探索新的减排途径。总的来说，研究确定了产业集群、评估排放驱动因素和提出科学减排策略，以更好地决策使用DBSCAN和惩罚回归模型。
</details></li>
</ul>
<hr>
<h2 id="Acoustic-to-articulatory-inversion-for-dysarthric-speech-Are-pre-trained-self-supervised-representations-favorable"><a href="#Acoustic-to-articulatory-inversion-for-dysarthric-speech-Are-pre-trained-self-supervised-representations-favorable" class="headerlink" title="Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?"></a>Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01108">http://arxiv.org/abs/2309.01108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Kumar Maharana, Krishna Kamal Adidam, Shoumik Nandi, Ajitesh Srivastava</li>
<li>for: 这个研究的目的是为了实现对具有问题话语的人的语音转换（AAI）。</li>
<li>methods: 这个研究使用了自动监督学习（SSL）模型所生成的表示，以便对具有问题话语的人进行AAI。</li>
<li>results: 研究发现，使用DeCoAR并在精致调整的情况下，可以实现Relative improvement of Pearson Correlation Coefficient（CC）大约1.81%和4.56%，对于健康控制和患者而言。<details>
<summary>Abstract</summary>
$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, respectively, over MFCCs. In the unseen case, we observe similar average trends for different SSL features. Overall, SSL networks like wav2vec, APC, and DeCoAR, which are trained with feature reconstruction or future timestep prediction tasks, perform well in predicting dysarthric articulatory trajectories.
</details>
<details>
<summary>摘要</summary>
$ $Acoustic-to-articulatory inversion (AAI) 涉及将语音空间映射到语音生成空间。通用的信号处理特征如MFCCs，已经广泛用于AAI任务中。对于具有瘫疡语言的谈话者而言，AAI 是一个挑战性的任务，因为他们的语音调音不精确和模糊。在这个工作中，我们使用预训学得的SSL模型来进行AAI。我们展示了不同预训特征对这个具有挑战性的AAI任务的影响，以及在低资源条件下使用这些特征进行训练。此外，我们还将x-vector与提取的SSL特征进行训练，以训练一个BLSTM网络。在seen情况下，我们实验了三种AAI训练方案（对象特定、汇集和精致调整）。结果显示，在精致调整方案下，DeCoAR 的相对改善比（Pearson Correlation Coefficient，CC）为${\sim}1.81\%}$和${\sim}4.56\%}$，对于健康控制和瘫疡患者而言。在unseen情况下，我们观察到了相似的平均趋势。总的来说，SSL网络如wav2vec、APC和DeCoAR，它们通过对特征重建或未来时间步骤预测任务进行训练，在预测瘫疡语言生成的调音轨迹方面表现良好。
</details></li>
</ul>
<hr>
<h2 id="Solving-Non-Rectangular-Reward-Robust-MDPs-via-Frequency-Regularization"><a href="#Solving-Non-Rectangular-Reward-Robust-MDPs-via-Frequency-Regularization" class="headerlink" title="Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization"></a>Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01107">http://arxiv.org/abs/2309.01107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Gadot, Esther Derman, Navdeep Kumar, Maxence Mohamed Elfatihi, Kfir Levy, Shie Mannor</li>
<li>for: 本研究targets maximal return under the most adversarial model from a given uncertainty set in robust Markov decision processes (RMDPs).</li>
<li>methods: 本研究提出了一种基于policy visitation frequency regularization的policy-gradient方法，并证明其 converges。</li>
<li>results: numerical experiments show that the learned policy is more robust and less conservative than traditional rectangular uncertainty.<details>
<summary>Abstract</summary>
In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method, and prove its convergence. Numerical experiments illustrate the learned policy's robustness and its less conservative behavior when compared to rectangular uncertainty.
</details>
<details>
<summary>摘要</summary>
在Robust Markov决策过程（RMDP）中，假设奖励和转移动力在给定的不确定集中。通过目标最大返回在最敌对模型下，RMDP 解决了环境不准确性对性能的敏感性。然而，以保持计算 tractability，传统上uncertainty集是独立地结构化于每个状态。这种叫做矩形性condition solely 是基于计算 Concerns 的，而且缺乏实践的驱动力，可能会导致过度保守的行为。在这项工作中，我们研究了连接奖励 RMDP，其中转移函数固定，但奖励函数在一个 $\alpha $-距离内的 Nominal 奖励函数。我们从连接这种非矩形奖励 RMDP 和应用策略访问频率规范化。我们引入了一种策略梯度法，并证明其 converge。 numerics 实验表明学习的策略具有强大的Robustness 和相比矩形不确定情况下更加保守的行为。
</details></li>
</ul>
<hr>
<h2 id="Turn-Fake-into-Real-Adversarial-Head-Turn-Attacks-Against-Deepfake-Detection"><a href="#Turn-Fake-into-Real-Adversarial-Head-Turn-Attacks-Against-Deepfake-Detection" class="headerlink" title="Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection"></a>Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01104">http://arxiv.org/abs/2309.01104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Wang, Zhengyu Zhao, Nicu Sebe, Bruno Lepri</li>
<li>For: 评估深伪检测器的Robustness，提出了首个基于三维面视 synthesis的对深伪检测器的3D对抗攻击（AdvHeat）。* Methods: 基于单个伪图片的面视生成，实现了对深伪检测器的3D对抗攻击。* Results: 通过实验证明了多种检测器对AdvHeat的攻击性能高，并且比传统攻击更好地适应实际场景和防御措施。生成的对抗图像也具有自然的 Looks。<details>
<summary>Abstract</summary>
Malicious use of deepfakes leads to serious public concerns and reduces people's trust in digital media. Although effective deepfake detectors have been proposed, they are substantially vulnerable to adversarial attacks. To evaluate the detector's robustness, recent studies have explored various attacks. However, all existing attacks are limited to 2D image perturbations, which are hard to translate into real-world facial changes. In this paper, we propose adversarial head turn (AdvHeat), the first attempt at 3D adversarial face views against deepfake detectors, based on face view synthesis from a single-view fake image. Extensive experiments validate the vulnerability of various detectors to AdvHeat in realistic, black-box scenarios. For example, AdvHeat based on a simple random search yields a high attack success rate of 96.8% with 360 searching steps. When additional query access is allowed, we can further reduce the step budget to 50. Additional analyses demonstrate that AdvHeat is better than conventional attacks on both the cross-detector transferability and robustness to defenses. The adversarial images generated by AdvHeat are also shown to have natural looks. Our code, including that for generating a multi-view dataset consisting of 360 synthetic views for each of 1000 IDs from FaceForensics++, is available at https://github.com/twowwj/AdvHeaT.
</details>
<details>
<summary>摘要</summary>
恶势所用的深度假像引起了公众的严重关注，导致人们对数字媒体的信任减退。虽然有效的深度假像检测器已经被提出，但它们却容易受到反对攻击。为评估检测器的可靠性，latest studies have explored various attacks.然而，所有的攻击都是基于2D图像杂化，这些杂化很难在实际情况中翻译成真实的脸部变化。在这篇论文中，我们提出了3D反对攻击（AdvHeat），这是基于单个假图像 Synthesize 的脸部视图生成。我们进行了广泛的实验，证明了许多检测器对 AdvHeat 的攻击成功率非常高，例如，基于随机搜索的 AdvHeat 可以在360个搜索步骤中达到96.8%的攻击成功率。当允许更多的查询访问时，我们可以进一步减少步骤数量到50。此外，我们还进行了进一步的分析，证明 AdvHeat 比传统攻击更好地在跨检测器的转移性和防御机制上。生成的反对图像也展示出自然的外观。我们的代码，包括生成360个视图的多视图Dataset，可以在https://github.com/twowwj/AdvHeaT中下载。
</details></li>
</ul>
<hr>
<h2 id="M2HGCL-Multi-Scale-Meta-Path-Integrated-Heterogeneous-Graph-Contrastive-Learning"><a href="#M2HGCL-Multi-Scale-Meta-Path-Integrated-Heterogeneous-Graph-Contrastive-Learning" class="headerlink" title="M2HGCL: Multi-Scale Meta-Path Integrated Heterogeneous Graph Contrastive Learning"></a>M2HGCL: Multi-Scale Meta-Path Integrated Heterogeneous Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01101">http://arxiv.org/abs/2309.01101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Guo, Yu Xia, Rui Wang, Rongcheng Duan, Lu Li, Jiangmeng Li</li>
<li>for: 研究者尝试将对比学习方法应用于非同质Graph，以提高对非同质Graph的理解和预测能力。</li>
<li>methods: 提出了一种基于多级MetaPath的hetrogeneous graph contrastive learning（M2HGCL）模型，不需要将非同质Graph转换为同质Graph，从而保留有价值信息。</li>
<li>results: 通过EXTENSIVE实验表明，M2HGCL在三个实际 datasets上表现出色，与当前状态方法相比，具有更高的性能。<details>
<summary>Abstract</summary>
Inspired by the successful application of contrastive learning on graphs, researchers attempt to impose graph contrastive learning approaches on heterogeneous information networks. Orthogonal to homogeneous graphs, the types of nodes and edges in heterogeneous graphs are diverse so that specialized graph contrastive learning methods are required. Most existing methods for heterogeneous graph contrastive learning are implemented by transforming heterogeneous graphs into homogeneous graphs, which may lead to ramifications that the valuable information carried by non-target nodes is undermined thereby exacerbating the performance of contrastive learning models. Additionally, current heterogeneous graph contrastive learning methods are mainly based on initial meta-paths given by the dataset, yet according to our deep-going exploration, we derive empirical conclusions: only initial meta-paths cannot contain sufficiently discriminative information; and various types of meta-paths can effectively promote the performance of heterogeneous graph contrastive learning methods. To this end, we propose a new multi-scale meta-path integrated heterogeneous graph contrastive learning (M2HGCL) model, which discards the conventional heterogeneity-homogeneity transformation and performs the graph contrastive learning in a joint manner. Specifically, we expand the meta-paths and jointly aggregate the direct neighbor information, the initial meta-path neighbor information and the expanded meta-path neighbor information to sufficiently capture discriminative information. A specific positive sampling strategy is further imposed to remedy the intrinsic deficiency of contrastive learning, i.e., the hard negative sample sampling issue. Through extensive experiments on three real-world datasets, we demonstrate that M2HGCL outperforms the current state-of-the-art baseline models.
</details>
<details>
<summary>摘要</summary>
研究人员受到同性图像学上的成功应用启发，尝试将同性图像学方法应用于不同类型节点和边的异构信息网络。与同性图像学不同的是，异构图像中节点和边的类型多样化，因此需要特化的同性图像学方法。现有的异构图像学方法多数是将异构图像转换为同性图像，这可能会导致非目标节点上的有价信息被忽略，从而恶化对异构图像学模型的性能。此外，现有的异构图像学方法主要基于数据集提供的初始meta路径，但据我们深入探索，我们得出了实际的结论：初始meta路径不含充分的洗礼信息；同时，不同类型的meta路径可以有效地提高异构图像学模型的性能。为此，我们提出了一种新的多级meta路径集成的异构图像学模型（M2HGCL），它不需要将异构图像转换为同性图像，而是在一起进行图像学学习。具体来说，我们将meta路径扩展，并将直接邻居信息、初始meta路径邻居信息和扩展meta路径邻居信息集成以便充分捕捉洗礼信息。此外，我们还采用了一种特殊的正样本选择策略，以解决对异构图像学的内在缺陷，即困难的负样本选择问题。经过了三个实际 datasets 的广泛实验，我们证明了 M2HGCL 在当前状态的基eline模型中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Stabilize-to-Act-Learning-to-Coordinate-for-Bimanual-Manipulation"><a href="#Stabilize-to-Act-Learning-to-Coordinate-for-Bimanual-Manipulation" class="headerlink" title="Stabilize to Act: Learning to Coordinate for Bimanual Manipulation"></a>Stabilize to Act: Learning to Coordinate for Bimanual Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01087">http://arxiv.org/abs/2309.01087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Grannen, Yilin Wu, Brandon Vu, Dorsa Sadigh</li>
<li>for: 这篇论文的目的是提出一种新的角色分配框架，以便在双手控制系统中实现高级别的 manipulate 能力。</li>
<li>methods: 该框架基于人类的做法，即使用学习的稳定化分类器， alternate between 维护环境不变和执行任务。</li>
<li>results: 在四种不同复杂度的双手任务上，使用BUDS实现了76.9%的任务成功率，并能够在不同类型的物体上进行扩展。相比之下，不结构化的基准点实验结果只有52.7%的成功率。<details>
<summary>Abstract</summary>
Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying complexities on real-world robots, such as zipping jackets and cutting vegetables. Given only 20 demonstrations, BUDS achieves 76.9% task success across our task suite, and generalizes to out-of-distribution objects within a class with a 52.7% success rate. BUDS is 56.0% more successful than an unstructured baseline that instead learns a BC stabilizing policy due to the precision required of these complex tasks. Supplementary material and videos can be found at https://sites.google.com/view/stabilizetoact .
</details>
<details>
<summary>摘要</summary>
针对实际世界中灵活 manipulate 的关键是控制两手的协调。然而，构建 dual arm 自动控制策略带来了内在的困难。一个这种困难是双手动作空间的高维度，这将加载到 both model-based 和 data-driven 方法上。我们从人类中突破这个挑战，提出了一种新的角色分配框架：一个稳定化手持物品，以简化环境，而另一个执行手执行任务。我们实现了这个框架，并将其称为 BimanUal Dexterity from Stabilization (BUDS)。BUDS 使用一个学习得到的稳定化位置更新器， alternate  между维持环境不变和通过示例学习得到的执行策略来完成任务。我们在实际 робоット上进行了四种不同复杂度的双手任务的评估，包括 zip 上衣和切 vegetables。只需要20个示例，BUDS 在我们的任务集中达到了76.9%的任务成功率，并能够在不同类型的物品上进行扩展。BUDS 比不结构化基eline 更successful，因为它需要这些复杂任务的精度。补充材料和视频可以在 <https://sites.google.com/view/stabilizetoact> 找到。
</details></li>
</ul>
<hr>
<h2 id="Tropical-Geometric-Tools-for-Machine-Learning-the-TML-package"><a href="#Tropical-Geometric-Tools-for-Machine-Learning-the-TML-package" class="headerlink" title="Tropical Geometric Tools for Machine Learning: the TML package"></a>Tropical Geometric Tools for Machine Learning: the TML package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01082">http://arxiv.org/abs/2309.01082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/barnhilldave/tml">https://github.com/barnhilldave/tml</a></li>
<li>paper_authors: David Barnhill, Ruriko Yoshida, Georges Aliatimis, Keiji Miura</li>
<li>for: 这篇论文主要是为了应用推理学中的推理方法和数据分析技术。</li>
<li>methods: 这篇论文使用的方法包括使用极值加法的热征链 Monte Carlo 抽象法，以及一些基于极值加法的主成分分析、极值逻辑回归和极值核密度估计等方法。</li>
<li>results: 这篇论文的结果包括一个完整的R包，包含了基本的极值加法计算和视觉化、以及基于极值加法的一些统计学模型，如极值主成分分析、极值逻辑回归和极值核密度估计等。<details>
<summary>Abstract</summary>
In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
</details>
<details>
<summary>摘要</summary>
过去一个十年，极地几何的发展提供了许多直接适用于统计学学习问题的用途。TML包是R包中首个包含极地几何相关基本计算、极地几何集的视觉化以及使用极地度量下的最大加法预测树的所有工具和方法的完整集。TML包主要使用极地哈希迪恩-麦克风 Monte Carlo抽取器与极地度量进行统计推断。此外，TML包还包括极地几何HAR抽取器的多种超VISS和无监督学习模型，包括极地主成分分析、极地逻辑回归和极地核密度估计。
</details></li>
</ul>
<hr>
<h2 id="Robust-Adversarial-Defense-by-Tensor-Factorization"><a href="#Robust-Adversarial-Defense-by-Tensor-Factorization" class="headerlink" title="Robust Adversarial Defense by Tensor Factorization"></a>Robust Adversarial Defense by Tensor Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01077">http://arxiv.org/abs/2309.01077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manish Bhattarai, Mehmet Cagri Kaymak, Ryan Barron, Ben Nebgen, Kim Rasmussen, Boian Alexandrov</li>
<li>for: 防止机器学习模型受到敌意攻击</li>
<li>methods: 使用输入数据约化和神经网络参数分解，并将其组合使用</li>
<li>results: 提供了一种可以增强对抗攻击的有效防御策略，并且比其他基于约化的防御策略更高效<details>
<summary>Abstract</summary>
As machine learning techniques become increasingly prevalent in data analysis, the threat of adversarial attacks has surged, necessitating robust defense mechanisms. Among these defenses, methods exploiting low-rank approximations for input data preprocessing and neural network (NN) parameter factorization have shown potential. Our work advances this field further by integrating the tensorization of input data with low-rank decomposition and tensorization of NN parameters to enhance adversarial defense. The proposed approach demonstrates significant defense capabilities, maintaining robust accuracy even when subjected to the strongest known auto-attacks. Evaluations against leading-edge robust performance benchmarks reveal that our results not only hold their ground against the best defensive methods available but also exceed all current defense strategies that rely on tensor factorizations. This study underscores the potential of integrating tensorization and low-rank decomposition as a robust defense against adversarial attacks in machine learning.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术在数据分析中越来越普遍，对抗攻击的威胁也在增加。为了应对这些威胁，使用输入数据减少维度和神经网络参数 факторизация的方法已经显示出了潜在的防御能力。我们的工作将这些方法进一步推进，通过将输入数据矩化和神经网络参数矩化相结合，以提高对抗攻击的防御能力。我们的方法在面对最强知识的自动攻击时仍然保持了高度的鲁棒精度，并在与现有的tensor化防御策略进行比较时表现出了超越。这一研究证明了将矩化和低级分解integrated为机器学习中的鲁棒防御策略的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Federated-Few-shot-Learning-for-Cough-Classification-with-Edge-Devices"><a href="#Federated-Few-shot-Learning-for-Cough-Classification-with-Edge-Devices" class="headerlink" title="Federated Few-shot Learning for Cough Classification with Edge Devices"></a>Federated Few-shot Learning for Cough Classification with Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01076">http://arxiv.org/abs/2309.01076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngan Dao Hoang, Dat Tran-Anh, Manh Luong, Cong Tran, Cuong Pham</li>
<li>for: 本研究旨在开发一种能够有效地分类喘音，即使没有庞大的喘音数据，并且保持隐私性的框架。</li>
<li>methods: 我们采用了少量学习和联合学习来解决这个问题，并设计了一个新的框架 termed F2LCough。</li>
<li>results: 我们的结果显示，使用这种新的方法可以在 COVID-19 热面&amp;喘音数据集上达到平均 F1-Score 的 86%，较其他方法更高。这显示了少量学习与联合学习的可行性，并且能够在数据缺乏情况下进行喘音分类。这种新方法可以用于建立喘音分类模型，并且保持隐私性。<details>
<summary>Abstract</summary>
Automatically classifying cough sounds is one of the most critical tasks for the diagnosis and treatment of respiratory diseases. However, collecting a huge amount of labeled cough dataset is challenging mainly due to high laborious expenses, data scarcity, and privacy concerns. In this work, our aim is to develop a framework that can effectively perform cough classification even in situations when enormous cough data is not available, while also addressing privacy concerns. Specifically, we formulate a new problem to tackle these challenges and adopt few-shot learning and federated learning to design a novel framework, termed F2LCough, for solving the newly formulated problem. We illustrate the superiority of our method compared with other approaches on COVID-19 Thermal Face & Cough dataset, in which F2LCough achieves an average F1-Score of 86%. Our results show the feasibility of few-shot learning combined with federated learning to build a classification model of cough sounds. This new methodology is able to classify cough sounds in data-scarce situations and maintain privacy properties. The outcomes of this work can be a fundamental framework for building support systems for the detection and diagnosis of cough-related diseases.
</details>
<details>
<summary>摘要</summary>
自动分类咳声是抑制呼吸疾病诊断和治疗中最关键的任务。然而，收集庞大量标注咳声数据是困难的，主要是因为高度劳动成本、数据稀缺和隐私问题。在这种情况下，我们的目标是开发一个框架，能够有效地进行咳声分类，同时解决隐私问题。我们提出了一个新的问题，并采用了几招学习和联合学习来设计一个新的框架，称为F2LCough。我们在COVID-19 Thermal Face & Cough数据集上运行了F2LCough，并获得了86%的平均F1分数。这些结果表明，将几招学习与联合学习结合使用可以建立一个分类咳声的模型，并在数据稀缺情况下保持隐私性。这种新的方法可以帮助建立抑制呼吸疾病的支持系统。
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-Modeling-and-Inference-in-Multi-Dimensional-Gaussian-Process-State-Space-Models"><a href="#Towards-Efficient-Modeling-and-Inference-in-Multi-Dimensional-Gaussian-Process-State-Space-Models" class="headerlink" title="Towards Efficient Modeling and Inference in Multi-Dimensional Gaussian Process State-Space Models"></a>Towards Efficient Modeling and Inference in Multi-Dimensional Gaussian Process State-Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01074">http://arxiv.org/abs/2309.01074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhidilin/gpssmproj">https://github.com/zhidilin/gpssmproj</a></li>
<li>paper_authors: Zhidi Lin, Juan Maroñas, Ying Li, Feng Yin, Sergios Theodoridis</li>
<li>for: 模型复杂非线性动态系统</li>
<li>methods:  integrate efficient transformed Gaussian process (ETGP) into Gaussian process state-space model (GPSSM)，并发展相应的变量推断算法</li>
<li>results: 实验结果表明，提议方法可以减少参数量和计算复杂度，并达到与现有方法相当的推断性能。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/zhidilin/gpssmProj%7D">https://github.com/zhidilin/gpssmProj}</a> 上获取。<details>
<summary>Abstract</summary>
The Gaussian process state-space model (GPSSM) has attracted extensive attention for modeling complex nonlinear dynamical systems. However, the existing GPSSM employs separate Gaussian processes (GPs) for each latent state dimension, leading to escalating computational complexity and parameter proliferation, thus posing challenges for modeling dynamical systems with high-dimensional latent states. To surmount this obstacle, we propose to integrate the efficient transformed Gaussian process (ETGP) into the GPSSM, which involves pushing a shared GP through multiple normalizing flows to efficiently model the transition function in high-dimensional latent state space. Additionally, we develop a corresponding variational inference algorithm that surpasses existing methods in terms of parameter count and computational complexity. Experimental results on diverse synthetic and real-world datasets corroborate the efficiency of the proposed method, while also demonstrating its ability to achieve similar inference performance compared to existing methods. Code is available at \url{https://github.com/zhidilin/gpssmProj}.
</details>
<details>
<summary>摘要</summary>
Gaussian 进程状态空间模型（GPSSM）已经吸引了广泛的注意力，用于模型复杂非线性动力系统。然而，现有的 GPSSM 使用每个隐藏状态维度上的分开 Gaussian 进程（GP），导致计算复杂性和参数增加，从而对高维隐藏状态系统的模型 pose 难题。为了缓解这个障碍，我们提议将高效转换 Gaussian 进程（ETGP）integrated 到 GPSSM 中，该方法涉及将共享 GP  pushed  through multiple normalizing flows 以高效地模型高维隐藏状态空间中的过渡函数。此外，我们还开发了相应的可视化推理算法，其比现有方法具有更少的参数数和更低的计算复杂性。实验结果在多个 Synthetic 和实际数据集上证明了提议方法的效率，同时也表明了它可以与现有方法相比达到类似的推理性能。代码可以在 \url{https://github.com/zhidilin/gpssmProj} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Separable-Hamiltonian-Neural-Networks"><a href="#Separable-Hamiltonian-Neural-Networks" class="headerlink" title="Separable Hamiltonian Neural Networks"></a>Separable Hamiltonian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01069">http://arxiv.org/abs/2309.01069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zykhoo/separablenns">https://github.com/zykhoo/separablenns</a></li>
<li>paper_authors: Zi-Yu Khoo, Jonathan Sze Choong Low, Stéphane Bressan</li>
<li>for: 这篇论文主要是为了研究如何从离散观察数据中预测汉密利系统的汉密利函数和其向量场。</li>
<li>methods: 该论文提出了三种可分解汉密利神经网络模型，其中每一种模型都利用了汉密利系统的可分解性来降低状态变量之间的复杂性。第一种模型通过 quadratic scaling 方法增加了训练数据量，第二种模型将可分解性 embed 到汉密利神经网络的损失函数中，第三种模型通过拼接多层感知器的 Architecture 实现了可分解性。</li>
<li>results: 作者通过对比这三种模型与现有的汉密利神经网络模型进行实验，发现 separable Hamiltonian neural networks 可以更有效地预测汉密利函数和其向量场。<details>
<summary>Abstract</summary>
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additive separability within Hamiltonian neural networks. The first model uses additive separability to quadratically scale the amount of data for training Hamiltonian neural networks. The second model embeds additive separability within the loss function of the Hamiltonian neural network. The third model embeds additive separability through the architecture of the Hamiltonian neural network using conjoined multilayer perceptions. We empirically compare the three models against state-of-the-art Hamiltonian neural networks, and demonstrate that the separable Hamiltonian neural networks, which alleviate complexity between the state variables, are more effective at regressing the Hamiltonian and its vector field.
</details>
<details>
<summary>摘要</summary>
现代科学和工程数据系统中，从离散观察获取动力系统模型是一项挑战。哈密顿系统是这类基本和普遍存在的动力系统之一。哈密顿神经网络是目前最先进的模型之一，可以不监督地将哈密顿系统的哈密顿函数回归到离散观察的向量场下。然而，哈密顿动力学往往复杂，特别在高维度情况下，状态空间中的哈密顿系统状态变量的数量相对于样本数量较大。为了缓解状态变量之间的复杂性，我们提出了一种新的策略：利用哈密顿系统的加法分解性，将其 embedding到哈密顿神经网络中。按照物理学 informed machine learning 的命名法，我们提出了三种分解哈密顿神经网络模型。这些模型在哈密顿神经网络中嵌入加法分解性。第一个模型使用加法分解性来循环增加训练哈密顿神经网络的数据量。第二个模型在哈密顿神经网络的损失函数中嵌入加法分解性。第三个模型通过哈密顿神经网络的架构嵌入加法分解性，使用 conjunction 多层感知。我们对这三种模型与当前最先进的哈密顿神经网络进行了实验比较，并证明了这些分解哈密顿神经网络模型，可以更有效地回归哈密顿函数和其向量场。
</details></li>
</ul>
<hr>
<h2 id="MQENet-A-Mesh-Quality-Evaluation-Neural-Network-Based-on-Dynamic-Graph-Attention"><a href="#MQENet-A-Mesh-Quality-Evaluation-Neural-Network-Based-on-Dynamic-Graph-Attention" class="headerlink" title="MQENet: A Mesh Quality Evaluation Neural Network Based on Dynamic Graph Attention"></a>MQENet: A Mesh Quality Evaluation Neural Network Based on Dynamic Graph Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01067">http://arxiv.org/abs/2309.01067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxuan Zhang, Haisheng Li, Nan Li, Xiaochuan Wang</li>
<li>for: 这篇论文是为了提高计算流体动力学中的流体模拟精度而设计的，即使是在工业应用中。</li>
<li>methods: 该论文提出了一种基于动态图注意力的结构化网格质量评估神经网络（MQENet），用于评估输入结构化网格的质量。为了使得从结构化网格中生成的图更加信息充沛，MQENet引入了两种新的结构化网格预处理算法。这两种算法还可以提高结构化网格数据的转换效率。</li>
<li>results: 在NACA-Market标准结构化网格数据集上进行了实验，结果表明MQENet在网格质量评估任务中具有效果。<details>
<summary>Abstract</summary>
With the development of computational fluid dynamics, the requirements for the fluid simulation accuracy in industrial applications have also increased. The quality of the generated mesh directly affects the simulation accuracy. However, previous mesh quality metrics and models cannot evaluate meshes comprehensively and objectively. To this end, we propose MQENet, a structured mesh quality evaluation neural network based on dynamic graph attention. MQENet treats the mesh evaluation task as a graph classification task for classifying the quality of the input structured mesh. To make graphs generated from structured meshes more informative, MQENet introduces two novel structured mesh preprocessing algorithms. These two algorithms can also improve the conversion efficiency of structured mesh data. Experimental results on the benchmark structured mesh dataset NACA-Market show the effectiveness of MQENet in the mesh quality evaluation task.
</details>
<details>
<summary>摘要</summary>
随着计算流体力学的发展，产业应用中的流体模拟精度的要求也在不断提高。模型的网格质量直接影响模拟精度。然而，过去的网格质量指标和模型无法全面、客观地评估网格质量。为此，我们提出MQENet，基于动态图注意力的结构网格质量评估神经网络。MQENet将网格评估任务定义为图分类任务，用于评估输入的结构网格质量。为了使结构网格数据更加有用，MQENet引入了两种新的结构网格预处理算法。这两种算法还可以提高结构网格数据的转换效率。在NACA-Market标准结构网格数据集上，MQENet的实验结果表明MQENet在网格质量评估任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-3D-Video-Information-Retrieval-with-Deep-Neural-Network-and-Bi-directional-Dynamic-time-Warping-Algorithm"><a href="#Semi-supervised-3D-Video-Information-Retrieval-with-Deep-Neural-Network-and-Bi-directional-Dynamic-time-Warping-Algorithm" class="headerlink" title="Semi-supervised 3D Video Information Retrieval with Deep Neural Network and Bi-directional Dynamic-time Warping Algorithm"></a>Semi-supervised 3D Video Information Retrieval with Deep Neural Network and Bi-directional Dynamic-time Warping Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01063">http://arxiv.org/abs/2309.01063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yintai Ma, Diego Klabjan</li>
<li>for: 本研究提出了一种基于视觉内容的semi-supervised深度学习算法，用于检索相似的2D和3D视频。</li>
<li>methods: 该算法结合深度卷积神经网络和循环神经网络，并使用动态时间戳匹配作为相似度度量。</li>
<li>results: 该算法在多个公共数据集上测试，包括CC_WEB_VIDEO、YouTube-8m、S3DIS和Synthia等，与参考深度学习模型相比，得到了良好的结果，能够有效地解决视频检索任务。<details>
<summary>Abstract</summary>
This paper presents a novel semi-supervised deep learning algorithm for retrieving similar 2D and 3D videos based on visual content. The proposed approach combines the power of deep convolutional and recurrent neural networks with dynamic time warping as a similarity measure. The proposed algorithm is designed to handle large video datasets and retrieve the most related videos to a given inquiry video clip based on its graphical frames and contents. We split both the candidate and the inquiry videos into a sequence of clips and convert each clip to a representation vector using an autoencoder-backed deep neural network. We then calculate a similarity measure between the sequences of embedding vectors using a bi-directional dynamic time-warping method. This approach is tested on multiple public datasets, including CC\_WEB\_VIDEO, Youtube-8m, S3DIS, and Synthia, and showed good results compared to state-of-the-art. The algorithm effectively solves video retrieval tasks and outperforms the benchmarked state-of-the-art deep learning model.
</details>
<details>
<summary>摘要</summary>
To achieve this, the candidate and inquiry videos are split into a sequence of clips, and each clip is converted into a representation vector using an autoencoder-backed deep neural network. The similarity measure between the sequences of embedding vectors is then calculated using a bi-directional dynamic time-warping method.The proposed algorithm is tested on multiple public datasets, including CC\_WEB\_VIDEO, Youtube-8m, S3DIS, and Synthia, and shows good results compared to state-of-the-art. The algorithm effectively solves video retrieval tasks and outperforms the benchmarked state-of-the-art deep learning model.
</details></li>
</ul>
<hr>
<h2 id="Distribution-learning-via-neural-differential-equations-a-nonparametric-statistical-perspective"><a href="#Distribution-learning-via-neural-differential-equations-a-nonparametric-statistical-perspective" class="headerlink" title="Distribution learning via neural differential equations: a nonparametric statistical perspective"></a>Distribution learning via neural differential equations: a nonparametric statistical perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01043">http://arxiv.org/abs/2309.01043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Marzouk, Zhi Ren, Sven Wang, Jakob Zech</li>
<li>for: 该论文旨在研究均匀方程（ODE）模型在分布学习中的性能。</li>
<li>methods: 该论文使用了基于潜在函数的极限映射来 Parametrize invertible transformation，并通过最大化可能性函数来训练模型。</li>
<li>results: 该论文提出了一种基于 $C^1$  метри entropy的非 Parametric 统计归一化分析方法，并在 $C^k$ 光滑目标分布中实现了nearly minimax-optimal的归一化率。<details>
<summary>Abstract</summary>
Ordinary differential equations (ODEs), via their induced flow maps, provide a powerful framework to parameterize invertible transformations for the purpose of representing complex probability distributions. While such models have achieved enormous success in machine learning, particularly for generative modeling and density estimation, little is known about their statistical properties. This work establishes the first general nonparametric statistical convergence analysis for distribution learning via ODE models trained through likelihood maximization. We first prove a convergence theorem applicable to arbitrary velocity field classes $\mathcal{F}$ satisfying certain simple boundary constraints. This general result captures the trade-off between approximation error (`bias') and the complexity of the ODE model (`variance'). We show that the latter can be quantified via the $C^1$-metric entropy of the class $\mathcal F$. We then apply this general framework to the setting of $C^k$-smooth target densities, and establish nearly minimax-optimal convergence rates for two relevant velocity field classes $\mathcal F$: $C^k$ functions and neural networks. The latter is the practically important case of neural ODEs.   Our proof techniques require a careful synthesis of (i) analytical stability results for ODEs, (ii) classical theory for sieved M-estimators, and (iii) recent results on approximation rates and metric entropies of neural network classes. The results also provide theoretical insight on how the choice of velocity field class, and the dependence of this choice on sample size $n$ (e.g., the scaling of width, depth, and sparsity of neural network classes), impacts statistical performance.
</details>
<details>
<summary>摘要</summary>
常微分方程（ODEs），通过它们引导的流场图，提供了一个强大的框架来参数化可逆变换，以便表示复杂的概率分布。尽管这些模型在机器学习中已经取得了很大的成功，特别是在生成模型和概率预测方面，但对它们的统计特性知之少。这项工作建立了首次通用非 Parametric 统计归一化分析，用于via ODE 模型通过最大化 likelihood 来学习概率分布。我们首先证明适用于任何速度场类 $\mathcal{F}$ 满足某些简单的边界约束的抽象结论。这个总体结果捕捉了在approximation error（偏差）和 ODE 模型（幂等）之间的质量。我们表明可以通过 $C^1$  метри entropy 来衡量 $\mathcal F$ 的复杂性。然后，我们将这个通用框架应用到 $C^k$ 光滑目标分布的情况，并确定 nearly minimax-optimal 的归一化率。在这个过程中，我们采用了精细的分析技术，包括 ODE 的 Analytic 稳定性、M-估计的古典理论和近期的 Approximation 率和 metric entropy 的研究。结果还提供了对模型选择和样本大小 $n$ （例如，宽度、深度和稀疏性）的依赖性的理论听见。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/cs.LG_2023_09_03/" data-id="clmjn91mv007x0j887m8y3g6i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/03/cs.SD_2023_09_03/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-03
        
      </div>
    </a>
  
  
    <a href="/2023/09/03/eess.IV_2023_09_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-03</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
