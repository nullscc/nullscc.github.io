
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-03 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS) paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01235 repo_url: None paper_authors: Joseph Drahos, Richar">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-03">
<meta property="og:url" content="https://nullscc.github.io/2023/09/03/eess.IV_2023_09_03/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS) paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01235 repo_url: None paper_authors: Joseph Drahos, Richar">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-03T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.463Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/eess.IV_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T09:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-03
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generalizability-and-Application-of-the-Skin-Reflectance-Estimate-Based-on-Dichromatic-Separation-SREDS"><a href="#Generalizability-and-Application-of-the-Skin-Reflectance-Estimate-Based-on-Dichromatic-Separation-SREDS" class="headerlink" title="Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS)"></a>Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01235">http://arxiv.org/abs/2309.01235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Drahos, Richard Plesh, Keivan Bahmani, Mahesh Banavar, Stephanie Schuckers</li>
<li>for: 这个论文旨在探讨面Recognition（FR）系统中的不同人种性能差异，以及可以使用皮肤颜色指标来补做这些差异。</li>
<li>methods: 本文使用了皮肤颜色分析方法，包括皮肤颜色指标基于 dichromatic separation（SREDS），以及其他一些皮肤颜色指标。</li>
<li>results: 研究发现，SREDS 指标在不同人种群体中的变化范围较小，可以作为自我报告的种族标签的替换。此外，使用 SREDS 指标可以在面Recognition 系统中保持隐私。<details>
<summary>Abstract</summary>
Face recognition (FR) systems have become widely used and readily available in recent history. However, differential performance between certain demographics has been identified within popular FR models. Skin tone differences between demographics can be one of the factors contributing to the differential performance observed in face recognition models. Skin tone metrics provide an alternative to self-reported race labels when such labels are lacking or completely not available e.g. large-scale face recognition datasets. In this work, we provide a further analysis of the generalizability of the Skin Reflectance Estimate based on Dichromatic Separation (SREDS) against other skin tone metrics and provide a use case for substituting race labels for SREDS scores in a privacy-preserving learning solution. Our findings suggest that SREDS consistently creates a skin tone metric with lower variability within each subject and SREDS values can be utilized as an alternative to the self-reported race labels at minimal drop in performance. Finally, we provide a publicly available and open-source implementation of SREDS to help the research community. Available at https://github.com/JosephDrahos/SREDS
</details>
<details>
<summary>摘要</summary>
人脸识别（FR）系统在近几年内广泛应用并可获得。然而，Popular FR 模型中的差异性在不同民族人群中被识别出来。皮肤颜色差异是FR模型中的一个因素。皮肤颜色指标提供了自我报告的种族标签缺失或完全无的大规模面部识别数据集中的一种代替方案。在这项工作中，我们进一步分析了SREDS的普适性，与其他皮肤颜色指标进行比较，并提供了一个使用SREDS分数代替自我报告种族标签的隐私保护学习解决方案的示例。我们的发现表明SREDS在每个人群中的变化更小，并且SREDS分数可以作为自我报告种族标签的替代品，减少性能下降。最后，我们提供了一个公开可用和开源的SREDS实现，以帮助研究人员。可以在https://github.com/JosephDrahos/SREDS上获取。
</details></li>
</ul>
<hr>
<h2 id="Breast-MRI-radiomics-and-machine-learning-radiomics-based-predictions-of-response-to-neoadjuvant-chemotherapy-–-how-are-they-affected-by-variations-in-tumour-delineation"><a href="#Breast-MRI-radiomics-and-machine-learning-radiomics-based-predictions-of-response-to-neoadjuvant-chemotherapy-–-how-are-they-affected-by-variations-in-tumour-delineation" class="headerlink" title="Breast MRI radiomics and machine learning radiomics-based predictions of response to neoadjuvant chemotherapy – how are they affected by variations in tumour delineation?"></a>Breast MRI radiomics and machine learning radiomics-based predictions of response to neoadjuvant chemotherapy – how are they affected by variations in tumour delineation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01210">http://arxiv.org/abs/2309.01210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepideh Hatamikia, Geevarghese George, Florian Schwarzhans, Amirreza Mahbod, Ramona Woitek<br>for: 这个研究的目的是为了评估静脉内医疗化治疗前Magnetic Resonance Imaging（MRI）扫描的变化对静脉内医疗化预测模型的影响。methods: 研究使用了不同的数学操作，包括磨砾、缓和、膨润、随机化和椭圆适掩，以模拟医生对肿瘤预定mask的手动修改。results: 研究发现，使用不同的预定mask可能会对静脉内医疗化预测模型的性能有所影响。 Specifically, smoothing and erosion yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation led to the lowest robustness and prediction performance for both breast cancer subtypes. Additionally, the study found that differences in VOI delineation can affect different steps of the radiomics analysis, and their quantification is therefore important for the development of standardized radiomics research.<details>
<summary>Abstract</summary>
Manual delineation of volumes of interest (VOIs) by experts is considered the gold-standard method in radiomics analysis. However, it suffers from inter- and intra-operator variability. A quantitative assessment of the impact of variations in these delineations on the performance of the radiomics predictors is required to develop robust radiomics based prediction models. In this study, we developed radiomics models for the prediction of pathological complete response to neoadjuvant chemotherapy in patients with two different breast cancer subtypes based on contrast-enhanced magnetic resonance imaging acquired prior to treatment (baseline MRI scans). Different mathematical operations such as erosion, smoothing, dilation, randomization, and ellipse fitting were applied to the original VOIs delineated by experts to simulate variations of segmentation masks. The effects of such VOI modifications on various steps of the radiomics workflow, including feature extraction, feature selection, and prediction performance, were evaluated. Using manual tumor VOIs and radiomics features extracted from baseline MRI scans, an AUC of up to 0.96 and 0.89 was achieved for human epidermal growth receptor 2 positive and triple-negative breast cancer, respectively. For smoothing and erosion, VOIs yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation lead to the lowest robustness and prediction performance for both breast cancer subtypes. At most 28% of the selected features were similar to manual VOIs when different VOI delineation data were used. Differences in VOI delineation affects different steps of radiomics analysis, and their quantification is therefore important for development of standardized radiomics research.
</details>
<details>
<summary>摘要</summary>
manually delineated volumes of interest (VOIs) by experts is considered the gold standard method in radiomics analysis, but it suffers from inter- and intra-operator variability. To develop robust radiomics-based prediction models, a quantitative assessment of the impact of variations in these delineations on the performance of radiomics predictors is required. In this study, we developed radiomics models for the prediction of pathological complete response to neoadjuvant chemotherapy in patients with two different breast cancer subtypes based on contrast-enhanced magnetic resonance imaging acquired prior to treatment (baseline MRI scans). We applied different mathematical operations such as erosion, smoothing, dilation, randomization, and ellipse fitting to the original VOIs delineated by experts to simulate variations of segmentation masks. We evaluated the effects of such VOI modifications on various steps of the radiomics workflow, including feature extraction, feature selection, and prediction performance. Using manual tumor VOIs and radiomics features extracted from baseline MRI scans, we achieved an AUC of up to 0.96 and 0.89 for human epidermal growth receptor 2 positive and triple-negative breast cancer, respectively. For smoothing and erosion, VOIs yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation led to the lowest robustness and prediction performance for both breast cancer subtypes. At most 28% of the selected features were similar to manual VOIs when different VOI delineation data were used. Our results show that differences in VOI delineation affect different steps of radiomics analysis, and their quantification is therefore important for the development of standardized radiomics research.
</details></li>
</ul>
<hr>
<h2 id="Deep-Unfolding-Convolutional-Dictionary-Model-for-Multi-Contrast-MRI-Super-resolution-and-Reconstruction"><a href="#Deep-Unfolding-Convolutional-Dictionary-Model-for-Multi-Contrast-MRI-Super-resolution-and-Reconstruction" class="headerlink" title="Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction"></a>Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01171">http://arxiv.org/abs/2309.01171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpcccc-cv/MC-CDic">https://github.com/lpcccc-cv/MC-CDic</a></li>
<li>paper_authors: Pengcheng Lei, Faming Fang, Guixu Zhang, Ming Xu</li>
<li>for:  This paper proposes a multi-contrast convolutional dictionary (MC-CDic) model for magnetic resonance imaging (MRI) tasks, which aims to explore the complementary information from the multi-contrast images and improve the super-resolution (SR) and reconstruction performance.</li>
<li>methods:  The proposed MC-CDic model uses an observation model to explicitly model the multi-contrast images as common features and unique features, and employs a proximal gradient algorithm to optimize the model. The model also uses learnable ResNet as proximal operators and multi-scale dictionaries to further improve the performance.</li>
<li>results:  The experimental results demonstrate the superior performance of the proposed MC-CDic model against existing state-of-the-art (SOTA) methods on multi-contrast MRI SR and reconstruction tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lpcccc-cv/MC-CDic">https://github.com/lpcccc-cv/MC-CDic</a>.<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) tasks often involve multiple contrasts. Recently, numerous deep learning-based multi-contrast MRI super-resolution (SR) and reconstruction methods have been proposed to explore the complementary information from the multi-contrast images. However, these methods either construct parameter-sharing networks or manually design fusion rules, failing to accurately model the correlations between multi-contrast images and lacking certain interpretations. In this paper, we propose a multi-contrast convolutional dictionary (MC-CDic) model under the guidance of the optimization algorithm with a well-designed data fidelity term. Specifically, we bulid an observation model for the multi-contrast MR images to explicitly model the multi-contrast images as common features and unique features. In this way, only the useful information in the reference image can be transferred to the target image, while the inconsistent information will be ignored. We employ the proximal gradient algorithm to optimize the model and unroll the iterative steps into a deep CDic model. Especially, the proximal operators are replaced by learnable ResNet. In addition, multi-scale dictionaries are introduced to further improve the model performance. We test our MC-CDic model on multi-contrast MRI SR and reconstruction tasks. Experimental results demonstrate the superior performance of the proposed MC-CDic model against existing SOTA methods. Code is available at https://github.com/lpcccc-cv/MC-CDic.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 任务经常涉及多个对比。最近，许多深度学习基于多对比MRI超分解（SR）和重建方法被提出，以探索多个对比图像之间的共同信息。然而，这些方法可能会构建参数共享网络或手动设计融合规则，失去准确地模型多个对比图像之间的相关性，并且缺乏一定的解释性。在这篇论文中，我们提出了一种多对比卷积字典（MC-CDic）模型，通过优化算法和数据准确性项来引导。具体来说，我们建立了多对比MRI图像的观察模型，以显式地模型多个对比图像为共同特征和特有特征。这样，只有参照图像中有用的信息可以被传递到目标图像中，而不相关的信息将被忽略。我们使用 proximal 梯度算法来优化模型，并将 proximal 操作器替换为学习 ResNet。此外，我们还引入了多尺度字典，以进一步提高模型性能。我们在多对比MRI SR 和重建任务上测试了我们的 MC-CDic 模型。实验结果表明，我们的 MC-CDic 模型比现有的 SOTA 方法表现出更高的性能。代码可以在 <https://github.com/lpcccc-cv/MC-CDic> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Channel-Attention-Separable-Convolution-Network-for-Skin-Lesion-Segmentation"><a href="#Channel-Attention-Separable-Convolution-Network-for-Skin-Lesion-Segmentation" class="headerlink" title="Channel Attention Separable Convolution Network for Skin Lesion Segmentation"></a>Channel Attention Separable Convolution Network for Skin Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01072">http://arxiv.org/abs/2309.01072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changlu Guo, Jiangyan Dai, Marton Szemenyei, Yugen Yi</li>
<li>for: 静脉皮肤癌病症诊断，提高早期诊断精度</li>
<li>methods: 提出Channel Attention Separable Convolution Network (CASCN)，借鉴U-Net、DenseNet、分割核函数、通道注意力和尺寸空间 pyramid pooling等高级机制</li>
<li>results: 在PH2 dataset上取得了state-of-the-art表现， dice similarity coefficient为0.9461，准确率为0.9645，不需过多预&#x2F;后处理图像<details>
<summary>Abstract</summary>
Skin cancer is a frequently occurring cancer in the human population, and it is very important to be able to diagnose malignant tumors in the body early. Lesion segmentation is crucial for monitoring the morphological changes of skin lesions, extracting features to localize and identify diseases to assist doctors in early diagnosis. Manual de-segmentation of dermoscopic images is error-prone and time-consuming, thus there is a pressing demand for precise and automated segmentation algorithms. Inspired by advanced mechanisms such as U-Net, DenseNet, Separable Convolution, Channel Attention, and Atrous Spatial Pyramid Pooling (ASPP), we propose a novel network called Channel Attention Separable Convolution Network (CASCN) for skin lesions segmentation. The proposed CASCN is evaluated on the PH2 dataset with limited images. Without excessive pre-/post-processing of images, CASCN achieves state-of-the-art performance on the PH2 dataset with Dice similarity coefficient of 0.9461 and accuracy of 0.9645.
</details>
<details>
<summary>摘要</summary>
皮肤癌是人类常见的癌症， Early diagnosis 是非常重要的。检测皮肤损伤的形态变化，提取特征以确定疾病的位置和识别，是诊断皮肤癌的关键步骤。然而，手动分割DERMOSCOPIC图像是时间consuming和容易出错的，因此需要精准和自动化的分割算法。 drawing inspiration from advanced mechanisms such as U-Net, DenseNet, Separable Convolution, Channel Attention, and Atrous Spatial Pyramid Pooling (ASPP), we propose a novel network called Channel Attention Separable Convolution Network (CASCN) for skin lesions segmentation. The proposed CASCN is evaluated on the PH2 dataset with limited images, and achieves state-of-the-art performance with Dice similarity coefficient of 0.9461 and accuracy of 0.9645 without excessive pre-/post-processing of images.
</details></li>
</ul>
<hr>
<h2 id="AB2CD-AI-for-Building-Climate-Damage-Classification-and-Detection"><a href="#AB2CD-AI-for-Building-Climate-Damage-Classification-and-Detection" class="headerlink" title="AB2CD: AI for Building Climate Damage Classification and Detection"></a>AB2CD: AI for Building Climate Damage Classification and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01066">http://arxiv.org/abs/2309.01066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Nitsche, S. Karthik Mukkavilli, Niklas Kühl, Thomas Brunschwiler</li>
<li>for: 这个论文旨在探讨深度学习技术在自然灾害中精准建筑损害评估中的实现，通过远程感知数据。</li>
<li>methods: 该论文使用了多种深度学习模型，包括异常残差、压缩和刺激、双路网络等，以及 ensemble 技术。</li>
<li>results: 研究结果显示，最小卫星图像分辨率以下3米为有效建筑损害检测的下限，而1米以下分辨率可以进行建筑物种类分类。此外，U-Net Siamese网络集成模型在xView2挑战标准准点中获得了0.812的F-1分数。<details>
<summary>Abstract</summary>
We explore the implementation of deep learning techniques for precise building damage assessment in the context of natural hazards, utilizing remote sensing data. The xBD dataset, comprising diverse disaster events from across the globe, serves as the primary focus, facilitating the evaluation of deep learning models. We tackle the challenges of generalization to novel disasters and regions while accounting for the influence of low-quality and noisy labels inherent in natural hazard data. Furthermore, our investigation quantitatively establishes that the minimum satellite imagery resolution essential for effective building damage detection is 3 meters and below 1 meter for classification using symmetric and asymmetric resolution perturbation analyses. To achieve robust and accurate evaluations of building damage detection and classification, we evaluated different deep learning models with residual, squeeze and excitation, and dual path network backbones, as well as ensemble techniques. Overall, the U-Net Siamese network ensemble with F-1 score of 0.812 performed the best against the xView2 challenge benchmark. Additionally, we evaluate a Universal model trained on all hazards against a flood expert model and investigate generalization gaps across events, and out of distribution from field data in the Ahr Valley. Our research findings showcase the potential and limitations of advanced AI solutions in enhancing the impact assessment of climate change-induced extreme weather events, such as floods and hurricanes. These insights have implications for disaster impact assessment in the face of escalating climate challenges.
</details>
<details>
<summary>摘要</summary>
我们研究了深度学习技术的实施以便精准评估自然灾害中的建筑物损害，使用遥感数据。xBD数据集，包括全球各地不同类型灾害事件，作为主要研究对象，以便评估深度学习模型。我们解决了对新灾害和地区总是泛化的挑战，同时考虑了自然灾害数据中的低质量和噪音标签的影响。此外，我们通过对不同的深度学习模型（包括剩余、压缩和刺激、双路网络）和 ensemble技术进行评估，发现了最佳的 U-Net 同时性网络 ensemble，其 F-1 分数为 0.812，可以在 xView2 挑战数据集上达到最佳效果。此外，我们还评估了对所有灾害的通用模型，并与洪水专家模型进行比较，探讨了不同事件之间的总体差异和场景数据中的外部泛化差异。我们的研究发现，高级 AI 解决方案在气候变化引起的极端天气事件的影响评估中具有潜在的潜力和局限性。这些发现对气候挑战的不断增长的面临带来了重要的启示。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/eess.IV_2023_09_03/" data-id="clmjn91qv00hj0j88bdpz8ftw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/03/cs.LG_2023_09_03/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-03
        
      </div>
    </a>
  
  
    <a href="/2023/09/02/cs.SD_2023_09_02/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-02</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
