
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-03 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01212 repo_url: None paper_authors: Wen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, Yuexian Zou">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-03">
<meta property="og:url" content="https://nullscc.github.io/2023/09/03/cs.SD_2023_09_03/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01212 repo_url: None paper_authors: Wen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, Yuexian Zou">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-03T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:15.461Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/cs.SD_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T15:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-03
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NADiffuSE-Noise-aware-Diffusion-based-Model-for-Speech-Enhancement"><a href="#NADiffuSE-Noise-aware-Diffusion-based-Model-for-Speech-Enhancement" class="headerlink" title="NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement"></a>NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01212">http://arxiv.org/abs/2309.01212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, Yuexian Zou</li>
<li>for: 提高干扰音频信号的清晰度（Speech Enhancement）</li>
<li>methods: 使用扩散模型（Diffusion Model）和 anchor-based inference算法，以及三种模型变体（Multi-stage SE based on preprocessing networks for Mel spectrograms）</li>
<li>results: NADiffuSE模型比其他基于GPC结构的扩散模型（Diffusion Model）表现更好，并且可以更好地处理实际中的干扰音频信号。<details>
<summary>Abstract</summary>
The goal of speech enhancement (SE) is to eliminate the background interference from the noisy speech signal. Generative models such as diffusion models (DM) have been applied to the task of SE because of better generalization in unseen noisy scenes. Technical routes for the DM-based SE methods can be summarized into three types: task-adapted diffusion process formulation, generator-plus-conditioner (GPC) structures and the multi-stage frameworks. We focus on the first two approaches, which are constructed under the GPC architecture and use the task-adapted diffusion process to better deal with the real noise. However, the performance of these SE models is limited by the following issues: (a) Non-Gaussian noise estimation in the task-adapted diffusion process. (b) Conditional domain bias caused by the weak conditioner design in the GPC structure. (c) Large amount of residual noise caused by unreasonable interpolation operations during inference. To solve the above problems, we propose a noise-aware diffusion-based SE model (NADiffuSE) to boost the SE performance, where the noise representation is extracted from the noisy speech signal and introduced as a global conditional information for estimating the non-Gaussian components. Furthermore, the anchor-based inference algorithm is employed to achieve a compromise between the speech distortion and noise residual. In order to mitigate the performance degradation caused by the conditional domain bias in the GPC framework, we investigate three model variants, all of which can be viewed as multi-stage SE based on the preprocessing networks for Mel spectrograms. Experimental results show that NADiffuSE outperforms other DM-based SE models under the GPC infrastructure. Audio samples are available at: https://square-of-w.github.io/NADiffuSE-demo/.
</details>
<details>
<summary>摘要</summary>
目标是减少背景干扰，使得讲话信号更加清晰。生成模型如扩散模型（DM）已经应用于讲话减少频率频谱中的讲话干扰。技术 Routes 可以分为三类：任务适应扩散过程形式、生成器+条件器（GPC）结构和多阶段框架。我们主要关注前两种方法，它们都是基于 GPC 架构，并使用任务适应的扩散过程来更好地处理真实的干扰。然而，这些减少模型的性能受到以下问题的限制：（a）任务适应扩散过程中的非高斯噪声估计。（b）GPC 结构中的弱条件器设计导致的条件频谱偏见。（c）在推理过程中不合理的插值操作导致的剩余噪声。为了解决以上问题，我们提出一种噪声意识的扩散基于减少模型（NADiffuSE），其中噪声表示被提取自干扰讲话信号，并作为全局的条件信息来估计非高斯噪声成分。此外，我们采用了 anchor-based 推理算法，以实现在推理过程中取得讲话质量和噪声剩余之间的平衡。为了减少 GPC 框架中的条件频谱偏见，我们进行了三种模型变体的调查，它们都可以视为基于 Mel spectrogram 的预处理网络的多阶段减少。实验结果显示，NADiffuSE 在 GPC 结构下表现出色，超过了其他 DM-based 减少模型。听音amples 可以在以下网址中找到：https://square-of-w.github.io/NADiffuSE-demo/.
</details></li>
</ul>
<hr>
<h2 id="MAGMA-Music-Aligned-Generative-Motion-Autodecoder"><a href="#MAGMA-Music-Aligned-Generative-Motion-Autodecoder" class="headerlink" title="MAGMA: Music Aligned Generative Motion Autodecoder"></a>MAGMA: Music Aligned Generative Motion Autodecoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01202">http://arxiv.org/abs/2309.01202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sohan Anisetty, Amit Raj, James Hays</li>
<li>for: 本研究旨在实现将音乐转换为舞蹈的问题，需要空间和时间协调，并且随音乐进程的同步。</li>
<li>methods: 我们提出了一个2步方法，使用Vector Quantized-Variational Autoencoder（VQ-VAE）将动作转换为基本动作，然后使用Transformer解oder进行正确的动作顺序排序。</li>
<li>results: 我们的提案方法可以实现现代最佳的音乐到动作转换结果，并且可以生成较长的动作序列，且可以将动作序列串接起来无缝汇流，同时也可以根据 Style 需求进行易于自定义的动作序列。<details>
<summary>Abstract</summary>
Mapping music to dance is a challenging problem that requires spatial and temporal coherence along with a continual synchronization with the music's progression. Taking inspiration from large language models, we introduce a 2-step approach for generating dance using a Vector Quantized-Variational Autoencoder (VQ-VAE) to distill motion into primitives and train a Transformer decoder to learn the correct sequencing of these primitives. We also evaluate the importance of music representations by comparing naive music feature extraction using Librosa to deep audio representations generated by state-of-the-art audio compression algorithms. Additionally, we train variations of the motion generator using relative and absolute positional encodings to determine the effect on generated motion quality when generating arbitrarily long sequence lengths. Our proposed approach achieve state-of-the-art results in music-to-motion generation benchmarks and enables the real-time generation of considerably longer motion sequences, the ability to chain multiple motion sequences seamlessly, and easy customization of motion sequences to meet style requirements.
</details>
<details>
<summary>摘要</summary>
mapping music to dance 是一个具有挑战性的问题，需要空间和时间协调，同时与音乐的进程保持同步。参考大型自然语言模型，我们提出了一个二步方法，使用量化-自适应学来将动作压缩成基本动作，然后使用Transformer解oder来学习正确的动作顺序。我们还评估了音乐表示的重要性，比较了使用Librosa提取音乐特征和深度音乐特征生成的音乐特征。此外，我们还训练了不同的动作生成器使用相对和绝对位置编码，以决定生成动作质量的影响，并且可以生成无限长的动作序列，排序动作序列，和根据风格需求轻松定制动作序列。我们的提案方法在音乐到动作生成的标准参考资料上实现了州立顶峰结果，并且可以实现无限长的动作序列生成、排序动作序列和根据风格需求轻松定制动作序列。
</details></li>
</ul>
<hr>
<h2 id="MSM-VC-High-fidelity-Source-Style-Transfer-for-Non-Parallel-Voice-Conversion-by-Multi-scale-Style-Modeling"><a href="#MSM-VC-High-fidelity-Source-Style-Transfer-for-Non-Parallel-Voice-Conversion-by-Multi-scale-Style-Modeling" class="headerlink" title="MSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-scale Style Modeling"></a>MSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-scale Style Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01142">http://arxiv.org/abs/2309.01142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Wang, Xinsheng Wang, Qicong Xie, Tao Li, Lei Xie, Qiao Tian, Yuping Wang</li>
<li>for: 这篇论文主要针对的是voice conversion（VC）任务中的语言表达风格模型化。</li>
<li>methods: 本文提出了一种多尺度风格模型化方法（MSM-VC），该方法从不同层次模型源语言风格，包括帧层、本地层和全局层。具体来说，该方法使用不同的特征表示来模型每个层的风格，包括语音特征、预训练ASR模型的瓶颈特征和通过自然语言处理技术学习的特征。</li>
<li>results: 实验结果表明，MSM-VC在高度表情语言库中比前置的VC方法更好地模型源语言风格，同时保持良好的语音质量和 speaker相似性。<details>
<summary>Abstract</summary>
In addition to conveying the linguistic content from source speech to converted speech, maintaining the speaking style of source speech also plays an important role in the voice conversion (VC) task, which is essential in many scenarios with highly expressive source speech, such as dubbing and data augmentation. Previous work generally took explicit prosodic features or fixed-length style embedding extracted from source speech to model the speaking style of source speech, which is insufficient to achieve comprehensive style modeling and target speaker timbre preservation. Inspired by the style's multi-scale nature of human speech, a multi-scale style modeling method for the VC task, referred to as MSM-VC, is proposed in this paper. MSM-VC models the speaking style of source speech from different levels. To effectively convey the speaking style and meanwhile prevent timbre leakage from source speech to converted speech, each level's style is modeled by specific representation. Specifically, prosodic features, pre-trained ASR model's bottleneck features, and features extracted by a model trained with a self-supervised strategy are adopted to model the frame, local, and global-level styles, respectively. Besides, to balance the performance of source style modeling and target speaker timbre preservation, an explicit constraint module consisting of a pre-trained speech emotion recognition model and a speaker classifier is introduced to MSM-VC. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to improve the disentanglement ability and alleviate the mismatch between training and inference. Experiments performed on the highly expressive speech corpus demonstrate that MSM-VC is superior to the state-of-the-art VC methods for modeling source speech style while maintaining good speech quality and speaker similarity.
</details>
<details>
<summary>摘要</summary>
在 voice conversion (VC) 任务中，保持源语音的发音风格对于许多场景都是非常重要的，如 dubbing 和数据增强。过去的工作通常使用源语音中的显式拥有的拥有特征或固定长度的风格嵌入来模型源语音的发音风格，这是不够完整地模型发音风格和目标说话人的时光质量保持。根据人类语音的多尺度特征，本文提出了一种多尺度风格模型ing方法（MSM-VC），用于模型源语音的发音风格。MSM-VC 模型源语音的发音风格从不同的水平。为了有效地传递发音风格并避免源语音的时光泄露，每个水平的风格都使用特定的表示方式进行模型。具体来说，使用 prosodic 特征、预训练 ASR 模型的瓶颈特征和通过一种自我超VI等方法提取的特征来模型帧、本地和全局级别的风格。此外，为了保持源风格模型和目标说话人的时光质量，我们引入了一个显式约束模块，该模块包括一个预训练的语音情感识别模型和一个说话人分类器。这个约束模块也使得在训练时可以模拟风格传递推理过程，以提高分离度和减少训练和推理之间的差异。在高度表达的语音集上进行的实验表明，MSM-VC 在模型源语音风格的同时保持良好的语音质量和说话人相似性方面表现出色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/cs.SD_2023_09_03/" data-id="clmjn91oa00bn0j88895tg3t8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/04/eess.IV_2023_09_04/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-04
        
      </div>
    </a>
  
  
    <a href="/2023/09/03/cs.LG_2023_09_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-03</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
