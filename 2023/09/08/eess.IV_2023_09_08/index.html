
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-08 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Non-convex regularization based on shrinkage penalty function paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04593 repo_url: None paper_authors: Manu Ghulyani, Muthuvel Arigovindan for: 这篇论文关注于图像恢复中的Total Varia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-08">
<meta property="og:url" content="https://nullscc.github.io/2023/09/08/eess.IV_2023_09_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Non-convex regularization based on shrinkage penalty function paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04593 repo_url: None paper_authors: Manu Ghulyani, Muthuvel Arigovindan for: 这篇论文关注于图像恢复中的Total Varia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-08T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:18.127Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/eess.IV_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T09:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-08
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Non-convex-regularization-based-on-shrinkage-penalty-function"><a href="#Non-convex-regularization-based-on-shrinkage-penalty-function" class="headerlink" title="Non-convex regularization based on shrinkage penalty function"></a>Non-convex regularization based on shrinkage penalty function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04593">http://arxiv.org/abs/2309.04593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Ghulyani, Muthuvel Arigovindan</li>
<li>for: 这篇论文关注于图像恢复中的Total Variation（TV）和Hessian Schatten norm（HSN）两种正则化方法的比较。</li>
<li>methods: 这篇论文使用了HSN正则化方法，其基于图像导数 Matrix 的二阶偏导数，并通过迭代法实现恢复图像。</li>
<li>results: 实验结果表明，使用HSN正则化方法可以提供更加精细的图像恢复结果，而且比使用TV正则化方法更加能够保持图像的结构。<details>
<summary>Abstract</summary>
Total Variation regularization (TV) is a seminal approach for image recovery. TV involves the norm of the image's gradient, aggregated over all pixel locations. Therefore, TV leads to piece-wise constant solutions, resulting in what is known as the "staircase effect." To mitigate this effect, the Hessian Schatten norm regularization (HSN) employs second-order derivatives, represented by the pth norm of eigenvalues in the image hessian, summed across all pixels. HSN demonstrates superior structure-preserving properties compared to TV. However, HSN solutions tend to be overly smoothed. To address this, we introduce a non-convex shrinkage penalty applied to the Hessian's eigenvalues, deviating from the convex lp norm. It is important to note that the shrinkage penalty is not defined directly in closed form, but specified indirectly through its proximal operation. This makes constructing a provably convergent algorithm difficult as the singular values are also defined through a non-linear operation. However, we were able to derive a provably convergent algorithm using proximal operations. We prove the convergence by establishing that the proposed regularization adheres to restricted proximal regularity. The images recovered by this regularization were sharper than the convex counterparts.
</details>
<details>
<summary>摘要</summary>
全Variation规则化（TV）是一种创新的图像恢复方法。TV是基于图像梯度的 нор，通过所有像素位置的梯度进行积加。因此，TV会导致piece-wise常数解，这被称为“梯形效应”。为了缓解这个效应，Hessian Schatten norm regularization（HSN）使用了图像的第二 derivatives，即图像梯度矩阵的第二导数，通过所有像素位置的积加。HSN的结构保持性比TV更好，但HSN的解征比TV更加平滑。为了解决这个问题，我们引入了一种非 conjugate penalty，该 penalty是应用到梯度矩阵的特征值上。需要注意的是，这种penalty不直接定义为closed form，而是通过其 proximal 操作定义。这使得构建可提able convergent algorithmdifficult。然而，我们成功地 deriv了一个可提able convergent algorith。我们证明了这种regularization的整体 convergent性，通过证明它遵循restricted proximal regularity。recovered by this regularization were sharper than the convex counterparts.
</details></li>
</ul>
<hr>
<h2 id="Motion-Compensated-Unsupervised-Deep-Learning-for-5D-MRI"><a href="#Motion-Compensated-Unsupervised-Deep-Learning-for-5D-MRI" class="headerlink" title="Motion Compensated Unsupervised Deep Learning for 5D MRI"></a>Motion Compensated Unsupervised Deep Learning for 5D MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04552">http://arxiv.org/abs/2309.04552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Kettelkamp, Ludovica Romanin, Davide Piccini, Sarv Priya, Mathews Jacob</li>
<li>for: 提高5D心脏MRI数据重建速度和质量，降低计算时间和数据不均匀影响。</li>
<li>methods: 使用深度学习算法模型数据的压缩和扩展，并利用心脏和呼吸信号来估算数据的变换。</li>
<li>results: 提出了一种数据效率的无监督深度学习算法，可以快速重建高品质5D心脏MRI数据，并且不受数据不均匀的影响。<details>
<summary>Abstract</summary>
We propose an unsupervised deep learning algorithm for the motion-compensated reconstruction of 5D cardiac MRI data from 3D radial acquisitions. Ungated free-breathing 5D MRI simplifies the scan planning, improves patient comfort, and offers several clinical benefits over breath-held 2D exams, including isotropic spatial resolution and the ability to reslice the data to arbitrary views. However, the current reconstruction algorithms for 5D MRI take very long computational time, and their outcome is greatly dependent on the uniformity of the binning of the acquired data into different physiological phases. The proposed algorithm is a more data-efficient alternative to current motion-resolved reconstructions. This motion-compensated approach models the data in each cardiac/respiratory bin as Fourier samples of the deformed version of a 3D image template. The deformation maps are modeled by a convolutional neural network driven by the physiological phase information. The deformation maps and the template are then jointly estimated from the measured data. The cardiac and respiratory phases are estimated from 1D navigators using an auto-encoder. The proposed algorithm is validated on 5D bSSFP datasets acquired from two subjects.
</details>
<details>
<summary>摘要</summary>
我们提议一种无监督深度学习算法，用于从3D radial获取的5D卡地脉搏MRI数据进行运动补做的重建。无束自由呼吸5D MRI简化扫描规划，提高了患者的舒适度，并提供了许多临床利益，包括均匀的空间分辨率和可以在任意视图中扩展数据。然而，当前的5D MRI重建算法需要非常长的计算时间，并且其结果受到数据的匀压缩程度的影响。我们提出的算法是一种更数据效率的替代方案。这种运动补做方法模型了数据在每个卡地脉搏/呼吸期间的数据为Fourier样本的扭曲版本的3D图像模板。扭曲地图是由一个卷积神经网络驱动的物理阶段信息。然后，模板和扭曲地图在测量数据中被并行估计。卡地脉搏和呼吸阶段是通过1D导航器使用自动编码器来估计。我们的算法被验证在5D bSSFP数据集上，从两个主体中获取。
</details></li>
</ul>
<hr>
<h2 id="Poster-Making-Edge-assisted-LiDAR-Perceptions-Robust-to-Lossy-Point-Cloud-Compression"><a href="#Poster-Making-Edge-assisted-LiDAR-Perceptions-Robust-to-Lossy-Point-Cloud-Compression" class="headerlink" title="Poster: Making Edge-assisted LiDAR Perceptions Robust to Lossy Point Cloud Compression"></a>Poster: Making Edge-assisted LiDAR Perceptions Robust to Lossy Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04549">http://arxiv.org/abs/2309.04549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Heo, Gregorie Phillips, Per-Erik Brodin, Ada Gavrilovska</li>
<li>for: 提高雷达点云的质量，以减少因压缩导致的感知性能下降。</li>
<li>methods: 使用深度Gradient来 interpolate点云中的点，以提高点云的质量。</li>
<li>results: 对于点云的重建，我们的算法比现有的图像插值算法更好的结果。<details>
<summary>Abstract</summary>
Real-time light detection and ranging (LiDAR) perceptions, e.g., 3D object detection and simultaneous localization and mapping are computationally intensive to mobile devices of limited resources and often offloaded on the edge. Offloading LiDAR perceptions requires compressing the raw sensor data, and lossy compression is used for efficiently reducing the data volume. Lossy compression degrades the quality of LiDAR point clouds, and the perception performance is decreased consequently. In this work, we present an interpolation algorithm improving the quality of a LiDAR point cloud to mitigate the perception performance loss due to lossy compression. The algorithm targets the range image (RI) representation of a point cloud and interpolates points at the RI based on depth gradients. Compared to existing image interpolation algorithms, our algorithm shows a better qualitative result when the point cloud is reconstructed from the interpolated RI. With the preliminary results, we also describe the next steps of the current work.
</details>
<details>
<summary>摘要</summary>
现实时光压感和测距（LiDAR）感知需要大量计算资源，例如3D对象检测和同时定位和地图生成。由于移动设备的限制资源，LiDAR感知 часто被卸载到边缘进行处理。卸载LiDAR感知需要压缩原始传感器数据，而失真压缩可以有效减少数据量。然而，失真压缩会降低LiDAR点云的质量，导致感知性能下降。在这种情况下，我们提出了一种插值算法，用于提高LiDAR点云的质量，从而抵消因为失真压缩而导致的感知性能下降。该算法targets点云（RI）表示中的距离图像，并在RI基于深度梯度的点插值。与现有的图像插值算法相比，我们的算法在重建点云时显示更好的质量。在这里，我们还描述了现有工作的下一步。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Review-of-Techniques-in-Brain-Image-Synthesis-using-Deep-Learning"><a href="#Systematic-Review-of-Techniques-in-Brain-Image-Synthesis-using-Deep-Learning" class="headerlink" title="Systematic Review of Techniques in Brain Image Synthesis using Deep Learning"></a>Systematic Review of Techniques in Brain Image Synthesis using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04511">http://arxiv.org/abs/2309.04511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Singh, Ammar Ranapurwala, Mrunal Bewoor, Sheetal Patil, Satyam Rai</li>
<li>for: 该评论文章探讨了医学成像领域的现状，尤其是使用深度学习技术进行脑图像生成。文章强调了利用深度学习提高诊断精度和减少医学过程的侵入性的重要性，以及深度学习在医学成像中的潜在作用。</li>
<li>methods: 文章详细描述了不同的脑图像生成方法和技术，包括2D到3D构建、MRI生成和使用转换器。文章还讨论了这些方法的限制和挑战，如获得充分的准备数据和解决脑超声问题。</li>
<li>results: 文章总结了这些方法的结果，包括脑图像生成的精度和可靠性问题，以及未来这些技术的发展前景和潜在应用。文章还强调了转换器的潜在作用和可能性，以及解决这些技术的缺点和限制的可能性。<details>
<summary>Abstract</summary>
This review paper delves into the present state of medical imaging, with a specific focus on the use of deep learning techniques for brain image synthesis. The need for medical image synthesis to improve diagnostic accuracy and decrease invasiveness in medical procedures is emphasized, along with the role of deep learning in enabling these advancements. The paper examines various methods and techniques for brain image synthesis, including 2D to 3D constructions, MRI synthesis, and the use of transformers. It also addresses limitations and challenges faced in these methods, such as obtaining well-curated training data and addressing brain ultrasound issues. The review concludes by exploring the future potential of this field and the opportunities for further advancements in medical imaging using deep learning techniques. The significance of transformers and their potential to revolutionize the medical imaging field is highlighted. Additionally, the paper discusses the potential solutions to the shortcomings and limitations faced in this field. The review provides researchers with an updated reference on the present state of the field and aims to inspire further research and bridge the gap between the present state of medical imaging and the future possibilities offered by deep learning techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-Can-We-Tame-the-Long-Tail-of-Chest-X-ray-Datasets"><a href="#How-Can-We-Tame-the-Long-Tail-of-Chest-X-ray-Datasets" class="headerlink" title="How Can We Tame the Long-Tail of Chest X-ray Datasets?"></a>How Can We Tame the Long-Tail of Chest X-ray Datasets?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04293">http://arxiv.org/abs/2309.04293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arsh Verma</li>
<li>for: 这个论文的目的是提出一种新的初始化方法，以提高自动检测器的性能。</li>
<li>methods: 这个论文使用的方法是选择一个更加适合目标数据集的初始化方法，以提高模型的性能。</li>
<li>results: 研究发现，通过选择合适的初始化方法，可以很好地提高模型的性能，而且这种方法可以轻松扩展到新的标签。<details>
<summary>Abstract</summary>
Chest X-rays (CXRs) are a medical imaging modality that is used to infer a large number of abnormalities. While it is hard to define an exhaustive list of these abnormalities, which may co-occur on a chest X-ray, few of them are quite commonly observed and are abundantly represented in CXR datasets used to train deep learning models for automated inference. However, it is challenging for current models to learn independent discriminatory features for labels that are rare but may be of high significance. Prior works focus on the combination of multi-label and long tail problems by introducing novel loss functions or some mechanism of re-sampling or re-weighting the data. Instead, we propose that it is possible to achieve significant performance gains merely by choosing an initialization for a model that is closer to the domain of the target dataset. This method can complement the techniques proposed in existing literature, and can easily be scaled to new labels. Finally, we also examine the veracity of synthetically generated data to augment the tail labels and analyse its contribution to improving model performance.
</details>
<details>
<summary>摘要</summary>
胸部X射影（CXR）是医疗影像模式，用于推断许多异常。尽管难以列举涵盖所有这些异常的完整列表，但许多其中的异常相对常见，广泛存在于用于训练深度学习模型的CXR数据集中。然而，目前的模型困难于分离独立的特征，用于标签的极少数 Label，即使这些标签的重要性很高。先前的工作集中在多标签和长尾问题上，通过引入新的损失函数或数据重新抽样或重新权重的机制来解决。在这篇文章中，我们提出，可以通过选择更加适应目标数据集的初始化来实现显著性提升。这种方法可以补充现有文献中提出的方法，并可以轻松扩展到新的标签。 finally，我们还检查了通过生成的数据进行补充的方法，并分析其对提高模型性能的贡献。
</details></li>
</ul>
<hr>
<h2 id="SegmentAnything-helps-microscopy-images-based-automatic-and-quantitative-organoid-detection-and-analysis"><a href="#SegmentAnything-helps-microscopy-images-based-automatic-and-quantitative-organoid-detection-and-analysis" class="headerlink" title="SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis"></a>SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04190">http://arxiv.org/abs/2309.04190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaodanxing/sam4organoid">https://github.com/xiaodanxing/sam4organoid</a></li>
<li>paper_authors: Xiaodan Xing, Chunling Tang, Yunzhe Guo, Nicholas Kurniawan, Guang Yang</li>
<li>for: 研究器官发育、药物发现和毒性评估</li>
<li>methods: 使用SegmentAnything进行精准分割个体器官，以及引入了一组量化器官结构的 morphological properties</li>
<li>results: 自动化分析结果与手动分析结果相一致，表明方法的可靠性和效果<details>
<summary>Abstract</summary>
Organoids are self-organized 3D cell clusters that closely mimic the architecture and function of in vivo tissues and organs. Quantification of organoid morphology helps in studying organ development, drug discovery, and toxicity assessment. Recent microscopy techniques provide a potent tool to acquire organoid morphology features, but manual image analysis remains a labor and time-intensive process. Thus, this paper proposes a comprehensive pipeline for microscopy analysis that leverages the SegmentAnything to precisely demarcate individual organoids. Additionally, we introduce a set of morphological properties, including perimeter, area, radius, non-smoothness, and non-circularity, allowing researchers to analyze the organoid structures quantitatively and automatically. To validate the effectiveness of our approach, we conducted tests on bright-field images of human induced pluripotent stem cells (iPSCs) derived neural-epithelial (NE) organoids. The results obtained from our automatic pipeline closely align with manual organoid detection and measurement, showcasing the capability of our proposed method in accelerating organoids morphology analysis.
</details>
<details>
<summary>摘要</summary>
“团聚体”是指自组织的3D细胞团，它们与生体中的组织和器官结构和功能具有高度相似性。量化团聚体 morphology 可以帮助研究器官发展、药物探索和药物毒性评估。现有的微scopie技术为研究提供了一个强大的工具，但是手动图像分析仍然是一项劳动和时间耗费的过程。因此，这篇论文提出了一个完整的微scopie分析管线，利用SegmentAnything进行精确地划分团聚体。此外，我们还引入了一组 morphological 特性，包括周长、面积、半径、非稳定性和非圆形性，这些特性可以让研究人员对团聚体结构进行量化和自动化的分析。为验证我们的方法的有效性，我们对人类干细胞 derivated neural-epithelial（NE）团聚体的明亮场图进行了测试。结果表明，我们的自动化管线与手动团聚体检测和测量结果高度相似，这证明了我们提出的方法在加速团聚体形态分析方面的能力。
</details></li>
</ul>
<hr>
<h2 id="PRISTA-Net-Deep-Iterative-Shrinkage-Thresholding-Network-for-Coded-Diffraction-Patterns-Phase-Retrieval"><a href="#PRISTA-Net-Deep-Iterative-Shrinkage-Thresholding-Network-for-Coded-Diffraction-Patterns-Phase-Retrieval" class="headerlink" title="PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval"></a>PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04171">http://arxiv.org/abs/2309.04171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuaxou/prista-net">https://github.com/liuaxou/prista-net</a></li>
<li>paper_authors: Aoxu Liu, Xiaohong Fan, Yin Yang, Jianping Zhang</li>
<li>For: The paper is written for addressing the problem of phase retrieval (PR) in computational imaging and image processing, specifically developing a deep learning method that combines interpretability with fast inference ability.* Methods: The paper proposes a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA), which utilizes a learnable nonlinear transformation and an attention mechanism to address the proximal-point mapping sub-problem and enhance local information. The fast Fourier transform (FFT) is used to learn global features.* Results: The proposed PRISTA-Net framework outperforms the existing state-of-the-art methods in terms of qualitative and quantitative evaluations on Coded Diffraction Patterns (CDPs) measurements, demonstrating the effectiveness of the proposed method in handling noise and improving recovery quality.<details>
<summary>Abstract</summary>
The problem of phase retrieval (PR) involves recovering an unknown image from limited amplitude measurement data and is a challenge nonlinear inverse problem in computational imaging and image processing. However, many of the PR methods are based on black-box network models that lack interpretability and plug-and-play (PnP) frameworks that are computationally complex and require careful parameter tuning. To address this, we have developed PRISTA-Net, a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA). This network utilizes a learnable nonlinear transformation to address the proximal-point mapping sub-problem associated with the sparse priors, and an attention mechanism to focus on phase information containing image edges, textures, and structures. Additionally, the fast Fourier transform (FFT) is used to learn global features to enhance local information, and the designed logarithmic-based loss function leads to significant improvements when the noise level is low. All parameters in the proposed PRISTA-Net framework, including the nonlinear transformation, threshold parameters, and step size, are learned end-to-end instead of being manually set. This method combines the interpretability of traditional methods with the fast inference ability of deep learning and is able to handle noise at each iteration during the unfolding stage, thus improving recovery quality. Experiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that our approach outperforms the existing state-of-the-art methods in terms of qualitative and quantitative evaluations. Our source codes are available at \emph{https://github.com/liuaxou/PRISTA-Net}.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Deep Unfolding Network (DUN) 和 First-order Iterative Shrinkage Thresholding Algorithm (ISTA) 开发了一种名为 PRISTA-Net 的方法，用于解决phas Retrieval (PR) 问题。这种问题的核心是从有限幅度测量数据中 recuperate 未知图像，是计算成像和图像处理中的一个非线性反向问题。然而，许多现有的 PR 方法基于黑盒网络模型，lack  interpretability 和 plug-and-play (PnP) 框架， computationally complex 并且需要精心调整参数。为了解决这个问题，我们在 PRISTA-Net 中使用了 learnable nonlinear transformation 来Address  proximal-point mapping 子问题，并使用 attention mechanism 来Focus  on phase information containing image edges, textures, and structures。此外，我们还使用 Fast Fourier Transform (FFT) 来学习全球特征，以增强本地信息。设计的 logarithmic-based loss function 对于噪声水平较低的情况带来了显著改进。在我们的方法中，所有参数都是通过 end-to-end 学习而不是手动设置的。这种方法结合了传统方法的可 interpretability 和深度学习的快速推理能力，可以在每个螺旋阶段中处理噪声，从而提高恢复质量。我们的实验表明，我们的方法在 Coded Diffraction Patterns (CDPs) 测量数据上的表现较为出色，胜过现有的状态 искус家方法。我们的源代码可以在 <https://github.com/liuaxou/PRISTA-Net> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-SDRTV-to-HDRTV-by-Learning-from-Image-Formation"><a href="#Towards-Efficient-SDRTV-to-HDRTV-by-Learning-from-Image-Formation" class="headerlink" title="Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation"></a>Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04084">http://arxiv.org/abs/2309.04084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Zheyuan Li, Zhengwen Zhang, Jimmy S. Ren, Yihao Liu, Jingwen He, Yu Qiao, Jiantao Zhou, Chao Dong</li>
<li>for: 这篇论文是用于将标准动态范围（SDR）内容转换成高动态范围（HDR）标准的。</li>
<li>methods: 该论文提出了一种三步解决方案 called HDRTVNet++, 包括 adaptive global color mapping、local enhancement 和 highlight refinement。</li>
<li>results: 该方法可以在4K分辨率图像上实现高效和轻量级的SDRTV-to-HDRTV转换，并且在量化和视觉上达到了状态方法。<details>
<summary>Abstract</summary>
Modern displays are capable of rendering video content with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of available resources are still in standard dynamic range (SDR). As a result, there is significant value in transforming existing SDR content into the HDRTV standard. In this paper, we define and analyze the SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Our analysis and observations indicate that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this issue, we propose a novel three-step solution pipeline called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step uses global statistics as guidance to perform image-adaptive color mapping. A local enhancement network is then deployed to enhance local details. Finally, we combine the two sub-networks above as a generator and achieve highlight consistency through GAN-based joint training. Our method is primarily designed for ultra-high-definition TV content and is therefore effective and lightweight for processing 4K resolution images. We also construct a dataset using HDR videos in the HDR10 standard, named HDRTV1K that contains 1235 and 117 training images and 117 testing images, all in 4K resolution. Besides, we select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Our final results demonstrate state-of-the-art performance both quantitatively and visually. The code, model and dataset are available at https://github.com/xiaom233/HDRTVNet-plus.
</details>
<details>
<summary>摘要</summary>
现代显示器可以渲染视频内容高动态范围（HDR）和宽色彩范围（WCG）。然而，大多数可用资源仍然是标准动态范围（SDR）。因此，将现有的SDR内容转化到HDRTV标准具有重要价值。在这篇论文中，我们定义和分析SDRTV-to-HDRTV任务，并模型了SDRTV/HDRTV内容的形成。我们的分析和观察结果表明，直接使用端到端超vised训练管道会导致严重的色彩范围过渡错误。为解决这个问题，我们提议一种新的三步解决方案管道，称为HDRTVNet++。该管道包括适应性全局颜色映射、本地增强和突出点细节优化。适应性全局颜色映射步骤使用全局统计为导航，对图像进行适应性颜色映射。本地增强网络然后用于提高本地细节。最后，我们将两个子网络组合成一个生成器，通过GAN基于的共同训练实现突出点一致性。我们的方法主要针对超高Definition TV内容，因此效果精灵和轻量级适用于4K分辨率图像处理。此外，我们还构建了一个名为HDRTV1K的HDR视频集，包含1235个和117个训练图像和测试图像，모都在4K分辨率。此外，我们选择了五个维度来评估SDRTV-to-HDRTV算法的结果。最终结果表明我们的方法具有状态级表现，同时也有出色的视觉表现。代码、模型和数据集可以在https://github.com/xiaom233/HDRTVNet-plus上下载。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Hierarchical-Transformers-for-Whole-Brain-Segmentation-with-Intracranial-Measurements-Integration"><a href="#Enhancing-Hierarchical-Transformers-for-Whole-Brain-Segmentation-with-Intracranial-Measurements-Integration" class="headerlink" title="Enhancing Hierarchical Transformers for Whole Brain Segmentation with Intracranial Measurements Integration"></a>Enhancing Hierarchical Transformers for Whole Brain Segmentation with Intracranial Measurements Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04071">http://arxiv.org/abs/2309.04071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/unest">https://github.com/masilab/unest</a></li>
<li>paper_authors: Xin Yu, Yucheng Tang, Qi Yang, Ho Hin Lee, Shunxing Bao, Yuankai Huo, Bennett A. Landman<br>for:The paper aims to enhance the existing whole brain segmentation methodology by incorporating intracranial measurements, specifically total intracranial volume (TICV) and posterior fossa volume (PFV), to provide a more comprehensive analysis of brain structures.methods:The authors use a hierarchical transformer model called UNesT, which is first pretrained on a large dataset of T1-weighted (T1w) 3D volumes from 8 different sites, and then finetuned with a smaller dataset of T1w 3D volumes from Open Access Series Imaging Studies (OASIS) that includes both 133 whole brain classes and TICV&#x2F;PFV labels.results:The authors evaluate the performance of their method using Dice similarity coefficients (DSC) and show that their model is able to accurately estimate TICV&#x2F;PFV while maintaining the performance of the 132 brain regions at a comparable level.Here is the information in Simplified Chinese text:for: 本文目的是增强现有的全脑分割方法，通过包含内侧体积量（TICV）和后底斜板体积（PFV）的测量，以提供更全面的脑结构分析。methods: 作者使用一种层次转换器模型called UNesT，首先在8个不同的数据集上进行预训练，然后在Open Access Series Imaging Studies（OASIS）中进行微调，使用这些数据集包含133个全脑类和TICV&#x2F;PFV标签。results: 作者使用 dice相似度（DSC）评估方法来评估模型的性能，并显示其模型能够准确地估算TICV&#x2F;PFV，同时保持132个脑区域的性能在相似水平。<details>
<summary>Abstract</summary>
Whole brain segmentation with magnetic resonance imaging (MRI) enables the non-invasive measurement of brain regions, including total intracranial volume (TICV) and posterior fossa volume (PFV). Enhancing the existing whole brain segmentation methodology to incorporate intracranial measurements offers a heightened level of comprehensiveness in the analysis of brain structures. Despite its potential, the task of generalizing deep learning techniques for intracranial measurements faces data availability constraints due to limited manually annotated atlases encompassing whole brain and TICV/PFV labels. In this paper, we enhancing the hierarchical transformer UNesT for whole brain segmentation to achieve segmenting whole brain with 133 classes and TICV/PFV simultaneously. To address the problem of data scarcity, the model is first pretrained on 4859 T1-weighted (T1w) 3D volumes sourced from 8 different sites. These volumes are processed through a multi-atlas segmentation pipeline for label generation, while TICV/PFV labels are unavailable. Subsequently, the model is finetuned with 45 T1w 3D volumes from Open Access Series Imaging Studies (OASIS) where both 133 whole brain classes and TICV/PFV labels are available. We evaluate our method with Dice similarity coefficients(DSC). We show that our model is able to conduct precise TICV/PFV estimation while maintaining the 132 brain regions performance at a comparable level. Code and trained model are available at: https://github.com/MASILab/UNesT/wholebrainSeg.
</details>
<details>
<summary>摘要</summary>
整个脑部分 segmentation with magnetic resonance imaging（MRI）可以实现非侵入性测量脑部分，包括总脑室体积（TICV）和后底室体积（PFV）。增强现有整个脑部分分 segmentation方法，以包括脑部分测量提供了更高的全面性分析脑结构的可能性。然而，将深度学习技术推广到脑部分测量面临数据可用性限制，因为有限的手动标注图集覆盖整个脑部分和TICV/PFV标签。在这篇论文中，我们改进了层次转换器UNesT，以实现同时分 segmenting整个脑部分和TICV/PFV。为了解决数据缺乏问题，我们首先在8个不同的地点上获取了4859个T1-加重（T1w）3D图像，并使用多Atlas分割管道生成标签。然后，我们在Open Access Series Imaging Studies（OASIS）上练习45个T1w 3D图像，其中包括133个整个脑部分类和TICV/PFV标签。我们使用 dice相似度系数（DSC）进行评估。我们发现，我们的模型能够准确地计算TICV/PFV，而同时保持132个脑部分性能在相似水平。代码和训练模型可以在以下链接中下载：https://github.com/MASILab/UNesT/wholebrainSeg。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/eess.IV_2023_09_08/" data-id="clmjn91qy00ht0j88czkz61zm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/08/cs.LG_2023_09_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-08
        
      </div>
    </a>
  
  
    <a href="/2023/09/07/cs.SD_2023_09_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
