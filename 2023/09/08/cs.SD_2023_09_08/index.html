
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-08 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04641 repo_url: None paper_authors: Ashwin Pillay, Sage Betko, Ari Liloia, Hao Chen, Ankit Sh">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-08">
<meta property="og:url" content="https://nullscc.github.io/2023/09/08/cs.SD_2023_09_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04641 repo_url: None paper_authors: Ashwin Pillay, Sage Betko, Ari Liloia, Hao Chen, Ankit Sh">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-08T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:18.106Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/cs.SD_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T15:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-08
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-Domain-Specific-Enhancements-for-a-Neural-Foley-Synthesizer"><a href="#Exploring-Domain-Specific-Enhancements-for-a-Neural-Foley-Synthesizer" class="headerlink" title="Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer"></a>Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04641">http://arxiv.org/abs/2309.04641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashwin Pillay, Sage Betko, Ari Liloia, Hao Chen, Ankit Shah</li>
<li>for: 这个论文是为了研究如何使用神经网络来生成真实、设定的电影或广播剧中的 Foley 音效的。</li>
<li>methods: 这个论文使用了多种改进了现有的文本到Audio频域的模型，以提高生成的 Foley 音效的多样性和听觉特征。 其中包括使用预训练的encoder保留音频和音乐特征，实现类别conditioning来增强不同类型 Foley 的中间表示的分 differentiability，以及开发了一种新的 transformer 结构来优化自注意计算在非常大的输入上 без sacrificing valuable information。</li>
<li>results: 作者们在实现后提供了中间结果，超过了基线，并讨论了在实现优化结果时遇到的实际挑战，以及可能的后续研究方向。<details>
<summary>Abstract</summary>
Foley sound synthesis refers to the creation of authentic, diegetic sound effects for media, such as film or radio. In this study, we construct a neural Foley synthesizer capable of generating mono-audio clips across seven predefined categories. Our approach introduces multiple enhancements to existing models in the text-to-audio domain, with the goal of enriching the diversity and acoustic characteristics of the generated foleys. Notably, we utilize a pre-trained encoder that retains acoustical and musical attributes in intermediate embeddings, implement class-conditioning to enhance differentiability among foley classes in their intermediate representations, and devise an innovative transformer-based architecture for optimizing self-attention computations on very large inputs without compromising valuable information. Subsequent to implementation, we present intermediate outcomes that surpass the baseline, discuss practical challenges encountered in achieving optimal results, and outline potential pathways for further research.
</details>
<details>
<summary>摘要</summary>
法莱音频合成指的是为媒体，如电影或广播，制作真实的抽象声效。在这个研究中，我们构建了一个基于神经网络的法莱合成器，能够生成单声道音频片段，涵盖七种预定的类别。我们的方法在文本到Audio领域中的现有模型中进行了多种改进，以增强生成的法莱的多样性和听觉特征。特别是，我们使用预训练的编码器保留了听觉和音乐特征在中间 Representations中，实施了类别划分来增强不同类别的中间表示的分化，并开发了一种创新的转换器结构，以优化自我注意计算在非常大的输入上，无需损失重要信息。在实施后，我们提供了中间结果，讨论了实际遇到的挑战和可能的进一步研究方向。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Pretrained-Image-text-Models-for-Improving-Audio-Visual-Learning"><a href="#Leveraging-Pretrained-Image-text-Models-for-Improving-Audio-Visual-Learning" class="headerlink" title="Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning"></a>Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04628">http://arxiv.org/abs/2309.04628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabhchand Bhati, Jesús Villalba, Laureano Moro-Velazquez, Thomas Thebaud, Najim Dehak<br>for: 这个论文的目的是提高基于视频和文本的语音识别系统的性能。methods: 这个论文使用了预训练的图像和文本编码器，并且提出了一种层次 segmental speech CLIP 模型来生成字元单元序列。results: 该论文的实验结果表明，使用预训练的文本编码器可以提高语音识别系统的性能，而且audio-only系统可以与audio-visual系统相似的性能。<details>
<summary>Abstract</summary>
Visually grounded speech systems learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. We proposed Segmental SpeechCLIP which used a hierarchical segmental speech encoder to generate sequences of word-like units. We used the pretrained CLIP text encoder on top of these word-like unit representations and showed significant improvements over the cascaded variant of SpeechCLIP. Segmental SpeechCLIP directly learns the word embeddings as input to the CLIP text encoder bypassing the vocabulary embeddings. Here, we explore mapping audio to CLIP vocabulary embeddings via regularization and quantization. As our objective is to distill semantic information into the speech encoders, we explore the usage of large unimodal pretrained language models as the text encoders. Our method enables us to bridge image and text encoders e.g. DINO and RoBERTa trained with uni-modal data. Finally, we extend our framework in audio-only settings where only pairs of semantically related audio are available. Experiments show that audio-only systems perform close to the audio-visual system.
</details>
<details>
<summary>摘要</summary>
视觉固定的语音系统学习从对应的图像和语音标签上的Pairing图像和语音标签。最近，有人尝试使用已经训练过图像和语音标签的视觉固定模型，如CLIP，来提高语音基于视觉固定模型的性能。然而，大多数这些模型只使用预训练的图像编码器。另一方面，Cascaded SpeechCLIP尝试生成本地化的单词级信息，并使用预训练的图像和语音编码器。尽管使用了两者，但它们发现了重要的搜索性能下降。我们提出了Segmental SpeechCLIP，它使用层次 segmental 语音编码器来生成序列化的单词单元表示。我们使用预训练的 CLIP 文本编码器在这些单词单元表示上，并显示了重要的改进。Segmental SpeechCLIP直接学习 word 嵌入，而不是使用词汇嵌入。我们 explore 将音频映射到 CLIP 词汇嵌入，并通过归一化和量化来进行规范。我们的目标是将语音编码器中的semantic信息储存下来。我们 explore 使用大型单modal预训练语言模型作为文本编码器，以 bridge 图像和文本编码器，例如 DINO 和 RoBERTa 在单modal数据上进行训练。最后，我们扩展了框架，在具有单个相关的音频对象的 audio-only 设置中进行学习。实验结果表明， audio-only 系统在 audio-visual 系统之间几乎具有相同的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-Sound-TPoS-Audio-Reactive-Video-Generation-with-Stable-Diffusion"><a href="#The-Power-of-Sound-TPoS-Audio-Reactive-Video-Generation-with-Stable-Diffusion" class="headerlink" title="The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion"></a>The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04509">http://arxiv.org/abs/2309.04509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, Jinkyu Kim</li>
<li>for:  Audio-to-video generation, incorporating temporal semantics and magnitude from audio input.</li>
<li>methods:  Latent stable diffusion model with textual semantic information, guided by sequential audio embedding from pretrained Audio Encoder.</li>
<li>results:  Audio reactive video contents, demonstrated effective across various tasks and compared with current state-of-the-art techniques.Here’s the full Chinese text:</li>
<li>for: 这篇论文是为了探讨音频到视频生成的问题，利用音频输入中的时间 semantics和幅度。</li>
<li>methods: 这篇论文提出了一种基于稳定扩散模型的 latent stable diffusion model，通过与预训练的音频编码器的文本semantic信息相结合，以便在视频帧中引入音频信息。</li>
<li>results: 这种方法可以生成响应音频的视频内容，在多个任务上进行了证明，并与当前领域的状态级技术进行了比较。更多示例可以在 <a target="_blank" rel="noopener" href="https://ku-vai.github.io/TPoS/">https://ku-vai.github.io/TPoS/</a> 上找到。<details>
<summary>Abstract</summary>
In recent years, video generation has become a prominent generative tool and has drawn significant attention. However, there is little consideration in audio-to-video generation, though audio contains unique qualities like temporal semantics and magnitude. Hence, we propose The Power of Sound (TPoS) model to incorporate audio input that includes both changeable temporal semantics and magnitude. To generate video frames, TPoS utilizes a latent stable diffusion model with textual semantic information, which is then guided by the sequential audio embedding from our pretrained Audio Encoder. As a result, this method produces audio reactive video contents. We demonstrate the effectiveness of TPoS across various tasks and compare its results with current state-of-the-art techniques in the field of audio-to-video generation. More examples are available at https://ku-vai.github.io/TPoS/
</details>
<details>
<summary>摘要</summary>
在最近的年头，视频生成技术已经成为了辉煌的一种生成工具，吸引了广泛的注意力。然而，尚未得到足够的关注是声音输入的生成，尽管声音含有独特的时间 semantics和幅度。因此，我们提出了声音力（TPoS）模型，以包括声音输入，包括可变的时间 semantics和幅度。为生成视频帧，TPoS使用了稳定扩散模型，并使用我们预训练的音频编码器提供的顺序音频嵌入。因此，这种方法可以生成响应声音的视频内容。我们在不同任务上证明了 TPoS 的效果，并与当前领域的音频到视频生成技术进行了比较。更多示例可以在 <https://ku-vai.github.io/TPoS/> 查看。
</details></li>
</ul>
<hr>
<h2 id="A-Long-Tail-Friendly-Representation-Framework-for-Artist-and-Music-Similarity"><a href="#A-Long-Tail-Friendly-Representation-Framework-for-Artist-and-Music-Similarity" class="headerlink" title="A Long-Tail Friendly Representation Framework for Artist and Music Similarity"></a>A Long-Tail Friendly Representation Framework for Artist and Music Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04182">http://arxiv.org/abs/2309.04182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Xiang, Junyu Dai, Xuchen Song, Furao Shen</li>
<li>for: 这篇论文的目的是提出一种适应长尾现象的艺术家和音乐相似性模型，以提高音乐搜索和推荐的精度。</li>
<li>methods: 该论文提出了一种基于神经网络的长尾友好表示框架（LTFRF），该框架将音乐、用户、元数据和关系数据集成到一个统一的 métric 学习框架中，并使用多种关系为正则项引入多关系损失。</li>
<li>results: 根据对 AllMusic 数据集的实验和分析，提出的 LTFRF 模型在类似艺术家&#x2F;音乐推荐任务中表现出色，比基eline 高出9.69%&#x2F;19.42% 的 Hit Ratio@10，而在长尾情况下，该模型达到了11.05%&#x2F;14.14% 的提高。<details>
<summary>Abstract</summary>
The investigation of the similarity between artists and music is crucial in music retrieval and recommendation, and addressing the challenge of the long-tail phenomenon is increasingly important. This paper proposes a Long-Tail Friendly Representation Framework (LTFRF) that utilizes neural networks to model the similarity relationship. Our approach integrates music, user, metadata, and relationship data into a unified metric learning framework, and employs a meta-consistency relationship as a regular term to introduce the Multi-Relationship Loss. Compared to the Graph Neural Network (GNN), our proposed framework improves the representation performance in long-tail scenarios, which are characterized by sparse relationships between artists and music. We conduct experiments and analysis on the AllMusic dataset, and the results demonstrate that our framework provides a favorable generalization of artist and music representation. Specifically, on similar artist/music recommendation tasks, the LTFRF outperforms the baseline by 9.69%/19.42% in Hit Ratio@10, and in long-tail cases, the framework achieves 11.05%/14.14% higher than the baseline in Consistent@10.
</details>
<details>
<summary>摘要</summary>
investigation of artist and music similarity crucial in music retrieval and recommendation, addressing long-tail challenge increasingly important. This paper proposes Long-Tail Friendly Representation Framework (LTFRF), utilizes neural networks to model similarity relationship. Our approach integrates music, user, metadata, and relationship data into unified metric learning framework, employs meta-consistency relationship as regular term to introduce Multi-Relationship Loss. Compared to Graph Neural Network (GNN), our proposed framework improves representation performance in long-tail scenarios, characterized by sparse relationships between artists and music. We conduct experiments and analysis on AllMusic dataset, results demonstrate that our framework provides favorable generalization of artist and music representation. Specifically, on similar artist/music recommendation tasks, LTFRF outperforms baseline by 9.69%/19.42% in Hit Ratio@10, and in long-tail cases, framework achieves 11.05%/14.14% higher than baseline in Consistent@10.
</details></li>
</ul>
<hr>
<h2 id="Cross-Utterance-Conditioned-VAE-for-Speech-Generation"><a href="#Cross-Utterance-Conditioned-VAE-for-Speech-Generation" class="headerlink" title="Cross-Utterance Conditioned VAE for Speech Generation"></a>Cross-Utterance Conditioned VAE for Speech Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04156">http://arxiv.org/abs/2309.04156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Cheng Yu, Guangzhi Sun, Weiqin Zu, Zheng Tian, Ying Wen, Wei Pan, Chao Zhang, Jun Wang, Yang Yang, Fanglei Sun</li>
<li>for: 提高语音生成的自然性和表情能力，以及提供更加灵活的语音编辑功能。</li>
<li>methods: 利用预训练语言模型和变量自动编码器（VAEs）的 Representational Capabilities，并开发了两种实用算法：CUC-VAE TTS 和 CUC-VAE SE。</li>
<li>results: 实验结果表明，我们的提议模型可以显著提高语音生成和编辑，生成更加自然和表情强的语音。<details>
<summary>Abstract</summary>
Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNSpeech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.Note: The "translate_language" parameter is set to "zh-CN" to specify Simplified Chinese as the target language.
</details></li>
</ul>
<hr>
<h2 id="A-Two-Stage-Training-Framework-for-Joint-Speech-Compression-and-Enhancement"><a href="#A-Two-Stage-Training-Framework-for-Joint-Speech-Compression-and-Enhancement" class="headerlink" title="A Two-Stage Training Framework for Joint Speech Compression and Enhancement"></a>A Two-Stage Training Framework for Joint Speech Compression and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04132">http://arxiv.org/abs/2309.04132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jscscloris/SEStream">https://github.com/jscscloris/SEStream</a></li>
<li>paper_authors: Jiayi Huang, Zeyu Yan, Wenbin Jiang, Fei Wen</li>
<li>for: 这 paper 考虑了对噪音 speech signal 的共同压缩和改进问题。</li>
<li>methods: 这 paper 使用了一种两个阶段优化过程，首先优化一个 encoder-decoder 对，然后使用 perception loss 优化一个 perceived decoder。</li>
<li>results: 实验结果表明，基于这两个阶段优化过程，可以在噪音 condition 下实现更高的评价质量和更低的Distortion。这种 two-stage 训练方法比传统的 heuristic 方法更有理论基础。<details>
<summary>Abstract</summary>
This paper considers the joint compression and enhancement problem for speech signal in the presence of noise. Recently, the SoundStream codec, which relies on end-to-end joint training of an encoder-decoder pair and a residual vector quantizer by a combination of adversarial and reconstruction losses,has shown very promising performance, especially in subjective perception quality. In this work, we provide a theoretical result to show that, to simultaneously achieve low distortion and high perception in the presence of noise, there exist an optimal two-stage optimization procedure for the joint compression and enhancement problem. This procedure firstly optimizes an encoder-decoder pair using only distortion loss and then fixes the encoder to optimize a perceptual decoder using perception loss. Based on this result, we construct a two-stage training framework for joint compression and enhancement of noisy speech signal. Unlike existing training methods which are heuristic, the proposed two-stage training method has a theoretical foundation. Finally, experimental results for various noise and bit-rate conditions are provided. The results demonstrate that a codec trained by the proposed framework can outperform SoundStream and other representative codecs in terms of both objective and subjective evaluation metrics. Code is available at \textit{https://github.com/jscscloris/SEStream}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/cs.SD_2023_09_08/" data-id="clmjn91of00bz0j888ol860vx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/09/eess.IV_2023_09_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-09
        
      </div>
    </a>
  
  
    <a href="/2023/09/08/eess.AS_2023_09_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-09-08</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
