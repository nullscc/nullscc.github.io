
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-08 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04640 repo_url: None paper_authors: Marina Y. Aoyama, Joã">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-08">
<meta property="og:url" content="https://nullscc.github.io/2023/09/08/cs.LG_2023_09_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.04640 repo_url: None paper_authors: Marina Y. Aoyama, Joã">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-08T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:18.103Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/cs.LG_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T10:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-08
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Few-Shot-Learning-of-Force-Based-Motions-From-Demonstration-Through-Pre-training-of-Haptic-Representation"><a href="#Few-Shot-Learning-of-Force-Based-Motions-From-Demonstration-Through-Pre-training-of-Haptic-Representation" class="headerlink" title="Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation"></a>Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04640">http://arxiv.org/abs/2309.04640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marina Y. Aoyama, João Moura, Namiko Saito, Sethu Vijayakumar</li>
<li>for: 能够快速适应不同物体的物理特性，提高机器人抓取物体的能力。</li>
<li>methods: 使用半监督学习自动机制，将学习模型分解成感觉表示编码器和动作生成解码器。首先使用大量未经监督的数据进行预训练，然后使用少量监督学习来训练动作生成解码器，以便快速适应不同物体的物理特性。</li>
<li>results: 对干洗任务使用不同弹性和表面黏度的毛巾进行验证，结果表明预训练可以大幅提高下游任务中机器人对物体物理特性的认识和生成恰当的动作，超过了没有预训练的LfD方法。此外，我们还验证了在物理机器人硬件上运行的动作是否符合预期，并证明感觉表示编码器在实际物体上采集的数据上具有良好的表达能力，从而解释了它在下游任务中的贡献。<details>
<summary>Abstract</summary>
In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalisation of the downstream task.
</details>
<details>
<summary>摘要</summary>
多个contact-rich任务中，力感Play an essential role in adapting motion to the physical properties of the manipulated object。To enable robots to capture the underlying distribution of object properties necessary for generalizing learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations。Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder。This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans。We validate the approach on the wiping task using sponges with different stiffness and surface friction。Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognize physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training。We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm。We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalization of the downstream task。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Safety-Regions-Via-Finite-Families-of-Scalable-Classifiers"><a href="#Probabilistic-Safety-Regions-Via-Finite-Families-of-Scalable-Classifiers" class="headerlink" title="Probabilistic Safety Regions Via Finite Families of Scalable Classifiers"></a>Probabilistic Safety Regions Via Finite Families of Scalable Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04627">http://arxiv.org/abs/2309.04627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Carlevaro, Teodoro Alamo, Fabrizio Dabbene, Maurizio Mongelli</li>
<li>for: 这个论文主要探讨了超vision的分类问题，即在分类器中控制错误率的问题。</li>
<li>methods: 该论文提出了一种基于概率安全区的方法，通过控制输入空间中错误类的数量来实现错误率的控制。此外，该论文还提出了一种可扩展的分类器，可以适应不同的错误率要求。</li>
<li>results: 经过一些测试，该论文得出了一些可靠的结论，表明该方法可以有效地控制错误率，并且可以适应不同的分类任务。<details>
<summary>Abstract</summary>
Supervised classification recognizes patterns in the data to separate classes of behaviours. Canonical solutions contain misclassification errors that are intrinsic to the numerical approximating nature of machine learning. The data analyst may minimize the classification error on a class at the expense of increasing the error of the other classes. The error control of such a design phase is often done in a heuristic manner. In this context, it is key to develop theoretical foundations capable of providing probabilistic certifications to the obtained classifiers. In this perspective, we introduce the concept of probabilistic safety region to describe a subset of the input space in which the number of misclassified instances is probabilistically controlled. The notion of scalable classifiers is then exploited to link the tuning of machine learning with error control. Several tests corroborate the approach. They are provided through synthetic data in order to highlight all the steps involved, as well as through a smart mobility application.
</details>
<details>
<summary>摘要</summary>
超visited分类可以识别数据中的模式，以分类行为类型。可能的解决方案中包含了内在的分类错误，这是机器学习的数值近似性所带来的。数据分析师可能会为某个类减少分类错误，但是这将导致其他类的错误增加。这种设计阶段的错误控制通常是 empirical的。在这种情况下，我们引入了概率安全区域，用于描述输入空间中的一个子集，其中数据分类错误的概率是控制的。我们然后利用可扩展分类器来联系机器学习的调整与错误控制。多个测试证明了这种方法的有效性，其中包括synthetic数据和一个智能移动应用。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-adjustment-queries-and-an-inverted-measurement-paradigm-for-low-rank-metric-learning"><a href="#Perceptual-adjustment-queries-and-an-inverted-measurement-paradigm-for-low-rank-metric-learning" class="headerlink" title="Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning"></a>Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04626">http://arxiv.org/abs/2309.04626</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/austinxu87/paq">https://github.com/austinxu87/paq</a></li>
<li>paper_authors: Austin Xu, Andrew D. McRae, Jingyan Wang, Mark A. Davenport, Ashwin Pananjady</li>
<li>for: 这个论文是为了提出一种新的查询机制，以收集人类反馈，并且这种查询机制被称为媒体调整查询（PAQ）。</li>
<li>methods: 这种查询机制采用倒掌计量方式，并且结合了 cardinal 和 ordinal 查询的优点。</li>
<li>results: 作者在度量学习问题中使用 PAQ 获取了 unknown Mahalanobis distance，并且这导致了一个高维、低级 matrix estimation 问题，这个问题不能由标准矩阵估计器解决。因此，作者提出了一种两个阶段估计器，并提供了样本复杂性保证。作者还通过 numerics  simulate 这种估计器的性能和其特点。<details>
<summary>Abstract</summary>
We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的查询机制，即感知调整查询（PAQ），它同时具有信息量和认知强度的优点。PAQ采用倒推计量方式，并将 cardinal和ordinal查询的优点结合在一起。我们在 métric learning 问题中使用 PAQ 进行学习未知的 Mahalanobis 距离。这导致了一个高维、低纬度矩阵估计问题，标准矩阵估计器无法应用。因此，我们开发了一种两个阶段的估计器，并提供了样本复杂性保证。我们通过数学实验证明了估计器的性能和其他特点。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-Empowered-Digital-Twin-for-Anomaly-Detection"><a href="#Knowledge-Distillation-Empowered-Digital-Twin-for-Anomaly-Detection" class="headerlink" title="Knowledge Distillation-Empowered Digital Twin for Anomaly Detection"></a>Knowledge Distillation-Empowered Digital Twin for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04616">http://arxiv.org/abs/2309.04616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Xu, Shaukat Ali, Tao Yue, Zaimovic Nedim, Inderjeet Singh</li>
<li>for: 这篇论文的目的是提出一种基于语言模型和长短期记忆网络的异常检测方法，以确保铁路控制和管理系统（TCMS）的可靠性。</li>
<li>methods: 这篇论文使用了语言模型（LM）和长短期记忆网络（LSTM）来提取上下文特征和时间特征，并通过知识储存（KD）来增加数据量。</li>
<li>results: 根据两个来自我司合作伙伴阿尔斯通的数据集， authors 证明了 KDDT 的效果，其中 F1 分数分别为 0.931 和 0.915。此外，作者还进行了一项完整的实验研究，发现 KDDT 模型中语言模型、LSTM 网络和 KD 的个人贡献均有显著提高效果，其中平均 F1 分数提高12.4%、3% 和 6.05%。<details>
<summary>Abstract</summary>
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4%, 3%, and 6.05%, respectively.
</details>
<details>
<summary>摘要</summary>
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming more and more common in critical infrastructures. As safety-critical systems, ensuring their reliability during operation is crucial. Digital twins (DTs) have been studied more and more in recent years because they can monitor and warn of anomalies in real-time, predict and detect anomalies, etc. However, creating a DT for anomaly detection in TCMS requires a lot of training data and extracting both chronological and context features of high quality. Therefore, in this paper, we propose a new method called KDDT for TCMS anomaly detection. KDDT uses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To increase the amount of data, KDDT uses knowledge distillation (KD) to benefit from out-of-domain data. We evaluated KDDT with two datasets from our industry partner Alstom and obtained F1 scores of 0.931 and 0.915, respectively, which demonstrates the effectiveness of KDDT. We also conducted a comprehensive empirical study to explore the individual contributions of the DT model, LM, and KD to the overall performance of KDDT, and observed average F1 score improvements of 12.4%, 3%, and 6.05%, respectively.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-World-Model-Disentanglement-in-Value-Based-Multi-Agent-Reinforcement-Learning"><a href="#Leveraging-World-Model-Disentanglement-in-Value-Based-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning"></a>Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04615">http://arxiv.org/abs/2309.04615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhizun Wang, David Meger</li>
<li>for:  addresses the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity.</li>
<li>methods:  uses a modularized world model, variational auto-encoders, and variational graph auto-encoders to learn latent representations and predict the joint action-value function.</li>
<li>results:  achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines, as demonstrated in experiments on StarCraft II micro-management challenges.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出了一种基于模型的多体强化学习方法，名为Value Decomposition Framework with Disentangled World Model，用于解决多体系统中 agent 之间交互的同时 achievement 目标的挑战，并且减少样本复杂性。</li>
<li>methods: 该方法使用了模块化的世界模型，包括动作决定、动作无关、静态分支，通过 past 经验来推断环境动力学，而不是直接从真实环境中采样。使用了 variational auto-encoders 和 variational graph auto-encoders 来学习 latent 表示，并将其与值基 Framework 混合，以预测 JOINT 动作价值函数，并优化整体训练目标。</li>
<li>results:  experiments 表明，该方法可以减少样本复杂性，并且在 StarCraft II 微管理挑战中表现出色，比基eline 高效，能够征服敌方军队。<details>
<summary>Abstract</summary>
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的模型基于多智能人工智能学习方法，名为值分解框架，以解决多智能系统中多个智能机器人之间共同目标的挑战，即降低训练样本复杂性。由于多智能系统的扩展性和不确定性问题，无模型方法需要训练很多样本。相比之下，我们使用模块化的世界模型，包括动作受控、动作无关和静态分支，来解释环境动力学和生成基于过去经验的假设结果，而不是直接从实际环境中采样。我们使用变量自动编码器和变量图自动编码器来学习世界模型的秘密表示，然后将其与值基于框架相结合，预测共同动作价值函数，并优化总训练目标。我们在Easy、Hard和Super-Hard StarCraft II微管理挑战中实现了实验，并证明了我们的方法可以具有高效样本率和在击败敌方军队方面表现出色，相比其他基准值。
</details></li>
</ul>
<hr>
<h2 id="Self-optimizing-Feature-Generation-via-Categorical-Hashing-Representation-and-Hierarchical-Reinforcement-Crossing"><a href="#Self-optimizing-Feature-Generation-via-Categorical-Hashing-Representation-and-Hierarchical-Reinforcement-Crossing" class="headerlink" title="Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing"></a>Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04612">http://arxiv.org/abs/2309.04612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingwangyang/hrc_feature_cross">https://github.com/yingwangyang/hrc_feature_cross</a></li>
<li>paper_authors: Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, Yanjie Fu</li>
<li>for: 本文主要针对自动生成有用特征的问题，以便创建一个决定性表示空间。</li>
<li>methods: 本文提出了一种原则性的和通用的表示 crossing 框架，用于解决自动生成特征的挑战。这个框架包括特征精度化、特征哈希和描述性概要。</li>
<li>results: 经过EXTENSIVE实验 validate，提出的方法能够效果地和有效地解决自动生成特征的问题。代码可以在<a target="_blank" rel="noopener" href="https://github.com/yingwangyang/HRC_feature_cross.git%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yingwangyang/HRC_feature_cross.git中找到。</a><details>
<summary>Abstract</summary>
Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and descriptive summarization. To achieve reinforcement crossing, we develop a hierarchical reinforcement feature crossing approach.We present extensive experimental results to demonstrate the effectiveness and efficiency of the proposed method. The code is available at https://github.com/yingwangyang/HRC_feature_cross.git.
</details>
<details>
<summary>摘要</summary>
Feature Generation的目的是生成新的有意义特征，以创建一个分类表示空间。一个生成的特征是有意义的当该特征来自一个内在具有自然交互的特征对。在现实世界中，经验丰富的数据科学家可以识别和生成有用的特征对互动，从极大的搜索空间中选择优化的生成路径，以获得优化的特征生成。但是，机器有限的人类能力。因此，我们总结这类学习任务为自动化特征生成。自动化特征生成面临许多未得到充分解决的挑战：有意义、Robust和高效的生成。为解决这些挑战，我们提出了一种原则性的和通用的表示交叉框架，用于解决自动化特征生成。为实现哈希表示，我们提出了三步方法：特征缩短、特征哈希和描述概要。为实现强化交叉，我们开发了层次强化特征交叉方法。我们提供了广泛的实验结果，以证明我们的方法的有效性和高效性。代码可以在<https://github.com/yingwangyang/HRC_feature_cross.git>上获取。
</details></li>
</ul>
<hr>
<h2 id="Online-Infinite-Dimensional-Regression-Learning-Linear-Operators"><a href="#Online-Infinite-Dimensional-Regression-Learning-Linear-Operators" class="headerlink" title="Online Infinite-Dimensional Regression: Learning Linear Operators"></a>Online Infinite-Dimensional Regression: Learning Linear Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06548">http://arxiv.org/abs/2309.06548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinod Raman, Unique Subedi, Ambuj Tewari</li>
<li>for: 本文研究了在线学习线性算子，具体是在两个无穷dimensional勒比希尔бер空间之间的平方损失下学习线性算子。</li>
<li>methods: 本文使用了在线学习方法，包括uniform bounded $p$-Schatten norm和operator norm等方法。</li>
<li>results: 本文证明了线性算子的在线学习可行性，但是uniform convergence不是必要的，同时还提供了一个分离uniform convergence和学习可行性的示例。此外，本文还证明了这些结论在agnostic PAC Setting下也成立。<details>
<summary>Abstract</summary>
We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在线学习线性算子的问题，特别是在两个无穷维希尔伯特空间之间的平方损失下学习线性算子。我们表明了任何 $p \in [1, \infty) $ 的线性算子具有 uniformly bounded $p$-Schatten norm 是在线学习的。然而，我们证明了一个不可能性结论，即对于Operator norm 上的 uniformly bounded线性算子，不是在线学习的。此外，我们还表明了在线学习和在线统一收敛之间的分离，通过证明一个受限的线性算子是在线学习的，但uniform convergence不成立。最后，我们证明了这些结论也适用于agnostic PAC Setting。
</details></li>
</ul>
<hr>
<h2 id="Motif-aware-Attribute-Masking-for-Molecular-Graph-Pre-training"><a href="#Motif-aware-Attribute-Masking-for-Molecular-Graph-Pre-training" class="headerlink" title="Motif-aware Attribute Masking for Molecular Graph Pre-training"></a>Motif-aware Attribute Masking for Molecular Graph Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04589">http://arxiv.org/abs/2309.04589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/einae-nd/moama-dev">https://github.com/einae-nd/moama-dev</a></li>
<li>paper_authors: Eric Inae, Gang Liu, Meng Jiang</li>
<li>for: 预训练图解链接网络的属性重建，以捕捉分子结构知识，并将其应用于化学、生物医学和材料科学中的多种下游性预测任务。</li>
<li>methods: 提出了一种基于分子结构的 attribute 掩码策略，通过沟通邻近分子结构的信息，捕捉高级分子结构的知识，并将其应用于图解链接网络的预训练中。</li>
<li>results: 对八个分子性质预测数据集进行了评估，并证明了该策略的优势。<details>
<summary>Abstract</summary>
Attribute reconstruction is used to predict node or edge features in the pre-training of graph neural networks. Given a large number of molecules, they learn to capture structural knowledge, which is transferable for various downstream property prediction tasks and vital in chemistry, biomedicine, and material science. Previous strategies that randomly select nodes to do attribute masking leverage the information of local neighbors However, the over-reliance of these neighbors inhibits the model's ability to learn from higher-level substructures. For example, the model would learn little from predicting three carbon atoms in a benzene ring based on the other three but could learn more from the inter-connections between the functional groups, or called chemical motifs. In this work, we propose and investigate motif-aware attribute masking strategies to capture inter-motif structures by leveraging the information of atoms in neighboring motifs. Once each graph is decomposed into disjoint motifs, the features for every node within a sample motif are masked. The graph decoder then predicts the masked features of each node within the motif for reconstruction. We evaluate our approach on eight molecular property prediction datasets and demonstrate its advantages.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>> attribute reconstruction 是用于预训练图 neural networks 中预测节点或边特征的技术。通过大量的分子，它们学习捕捉结构知识，这种知识可以转移到下游性质预测任务中，在化学、生物医学和材料科学中都非常重要。在先前的策略中，随机选择节点进行特征遮盲，利用当地节点的信息，但这会限制模型学习高级结构的能力。例如，模型在预测三个碳原子的benzene环中的预测结果很少，但可以从化学动机中获得更多的信息。在这种工作中，我们提出和研究motif-aware特征遮盲策略，通过利用邻近motif中的原子信息来捕捉高级结构。每个图被分解成不同的motif，然后每个样本motif中的节点特征被遮盲。图解码器然后预测每个节点的遮盲特征，以重建graph。我们在八个分子性质预测数据集上评估了我们的方法，并证明了它的优势。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Mesh-Aware-Radiance-Fields"><a href="#Dynamic-Mesh-Aware-Radiance-Fields" class="headerlink" title="Dynamic Mesh-Aware Radiance Fields"></a>Dynamic Mesh-Aware Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04581">http://arxiv.org/abs/2309.04581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YilingQiao/DMRF">https://github.com/YilingQiao/DMRF</a></li>
<li>paper_authors: Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, Ming C. Lin</li>
<li>For:  This paper aims to integrate NeRF (Neural Radiance Fields) into the traditional graphics pipeline, allowing for physically consistent rendering and simulation of mesh assets within NeRF volumes.* Methods: The paper proposes a two-way coupling between mesh and NeRF during rendering and simulation, using light transport equations to update radiance and throughput along a cast ray. The NeRF model is trained with HDR images to resolve color space discrepancies.* Results: The hybrid surface-volumetric formulation outperforms alternatives in visual realism for mesh insertion, as it allows for realistic light transport from volumetric NeRF media onto surfaces, affecting the appearance of reflective&#x2F;refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene.Here is the same information in Simplified Chinese:* For: 这篇论文目标是将NeRF（神经辐射场）集成到传统图形管道中，以实现物理逻辑的渲染和实时计算材质资产在NeRF卷积体中。* Methods: 论文提议一种在渲染和计算中建立材质和NeRF之间的两种方向的相互关联，使用光传输方程来更新辐射和通过put along a cast ray with an arbitrary number of bounces。NeRF模型通过高动态范围（HDR）图像进行训练，以解决颜色空间差异。* Results: 混合表面-卷积型形式的方法在渲染和计算中超越了其他方法，因为它允许真实的光传输从NeRF卷积体媒体 onto 表面，影响了镜面&#x2F;塑化表面和diffuse表面的照明。<details>
<summary>Abstract</summary>
Embedding polygonal mesh assets within photorealistic Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the traditional graphics pipeline. This paper designs a two-way coupling between mesh and NeRF during rendering and simulation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrepancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) images. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance physics simulator that supports cloth, rigid and soft bodies. The full rendering and simulation system can be run on a GPU at interactive rates. We show that a hybrid system approach outperforms alternatives in visual realism for mesh insertion, because it allows realistic light transport from volumetric NeRF media onto surfaces, which affects the appearance of reflective/refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene.
</details>
<details>
<summary>摘要</summary>
将多面体网格资产集成到拟真度场（NeRF）中，以便在physically consistent manner中渲染和模拟，是从系统角度来看还尚未得到充分的探讨。这篇论文提出了在渲染和模拟过程中两向相互作用的方法。我们首先评论了 mesh和NeRF的光传输方程，然后将它们转化为更有效的算法来更新光芒和通过率。为了解决mesh和NeRF之间的颜色空间不同，我们在训练NeRF时使用高动态范围（HDR）图像。此外，我们还提出了一种策略来估计NeRF中的光源和投射阴影。最后，我们考虑了如何高效地将混合表面-体积形式化与高性能物理引擎集成，以实现交互式帧率。我们表明，使用混合系统方法可以在渲染和模拟过程中提高视觉实际性，因为它允许真实的光传输从体积NeRF媒体onto表面，这对折射/折射表面的外观和透射表面上的照明产生了影响。
</details></li>
</ul>
<hr>
<h2 id="Circles-Inter-Model-Comparison-of-Multi-Classification-Problems-with-High-Number-of-Classes"><a href="#Circles-Inter-Model-Comparison-of-Multi-Classification-Problems-with-High-Number-of-Classes" class="headerlink" title="Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes"></a>Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05672">http://arxiv.org/abs/2309.05672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Mir, Ragaad AlTarawneh, Shah Rukh Humayoun</li>
<li>for: 这篇论文旨在解决大量类别的分类问题中的视觉化和模型比较问题。</li>
<li>methods: 作者提出了一种交互式视觉分析工具，名为“环”，可以同时显示多个分类模型的比较结果，并采用了圆形卷积布局来减少视觉干扰。</li>
<li>results: 试验结果表明，“环”工具可以准确地显示9个模型的比较结果，并且通过交互式的方式，帮助用户更好地理解和分析这些模型的性能。<details>
<summary>Abstract</summary>
The recent advancements in machine learning have motivated researchers to generate classification models dealing with hundreds of classes such as in the case of image datasets. However, visualization of classification models with high number of classes and inter-model comparison in such classification problems are two areas that have not received much attention in the literature, despite the ever-increasing use of classification models to address problems with very large class categories. In this paper, we present our interactive visual analytics tool, called Circles, that allows a visual inter-model comparison of numerous classification models with 1K classes in one view. To mitigate the tricky issue of visual clutter, we chose concentric a radial line layout for our inter-model comparison task. Our prototype shows the results of 9 models with 1K classes
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unleashing-the-Power-of-Graph-Learning-through-LLM-based-Autonomous-Agents"><a href="#Unleashing-the-Power-of-Graph-Learning-through-LLM-based-Autonomous-Agents" class="headerlink" title="Unleashing the Power of Graph Learning through LLM-based Autonomous Agents"></a>Unleashing the Power of Graph Learning through LLM-based Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04565">http://arxiv.org/abs/2309.04565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanning Wei, Zhiqiang He, Huan Zhao, Quanming Yao</li>
<li>for: 提高自动化图学 Task 的效率和智能化水平</li>
<li>methods: 使用自动语言模型（LLM）作为自动化图学代理，将复杂的图学任务分解为三个组成部分：检测学习目标、配置 AutoGraph 解决方案、并生成回应</li>
<li>results: 提出 Auto$^2$Graph 方法，可以自动生成基于具体数据和学习目标的解决方案，并在不同的数据集和学习任务上达到相当的性能水平，同时 agents 的决策具有人类智能特点<details>
<summary>Abstract</summary>
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph learning task is decomposed into three components following the agent planning, namely, detecting the learning intent, configuring solutions based on AutoGraph, and generating a response. The AutoGraph agents manage crucial procedures in automated graph learning, including data-processing, AutoML configuration, searching architectures, and hyper-parameter fine-tuning. With these agents, those components are processed by decomposing and completing step by step, thereby generating a solution for the given data automatically, regardless of the learning task on node or graph. The proposed method is dubbed Auto$^2$Graph, and the comparable performance on different datasets and learning tasks. Its effectiveness is demonstrated by its comparable performance on different datasets and learning tasks, as well as the human-like decisions made by the agents.
</details>
<details>
<summary>摘要</summary>
几乎所有实际应用中都存在Graph structured data，但是处理这些多样化的数据和学习任务在效率上是一大挑战。过去几年，专家们已经设计了多种Graph Neural Networks（GNNs）来解决这些复杂的学习任务。另外，他们还实现了AutoML in Graph，也就是AutoGraph，以自动生成数据特定的解决方案。尽管它们的成功，但它们仍然面临以下问题：（1）在不同的学习任务水平上管理多元的学习任务，（2）在传统的Graph learning beyond architecture design中进行不同的处理，以及（3）在使用AutoGraph时需要很大的专家知识。在这篇论文中，我们提议使用Large Language Models（LLMs）作为自主代理人来简化数据多样化的学习过程。具体来说，当用户发出请求，该请求可能包含不同的数据和学习目标，我们将复杂的Graph learning任务分解为三个Component，即检测学习目的、基于AutoGraph配置解决方案，以及产生回应。AutoGraph代理人处理关键的自动化Graph learning步骤，包括数据处理、AutoML配置、搜索架构和几何 Parameters fine-tuning。这些代理人通过拆分和完成步骤，将这些Component处理一推一击，从而自动生成对应数据的解决方案，不论学习任务是几何或数据水平。我们称这种方法为Auto$^2$Graph，并证明其效果通过与不同数据和学习任务相比较的比较性表现。
</details></li>
</ul>
<hr>
<h2 id="When-Less-is-More-Investigating-Data-Pruning-for-Pretraining-LLMs-at-Scale"><a href="#When-Less-is-More-Investigating-Data-Pruning-for-Pretraining-LLMs-at-Scale" class="headerlink" title="When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"></a>When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04564">http://arxiv.org/abs/2309.04564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, Sara Hooker</li>
<li>for: 这个论文目的是提出一种可扩展的数据质量估计方法，用于系统地评估预训练数据的质量。</li>
<li>methods: 这个论文使用了一些计算机科学中的复杂估计方法，如Error L2-Norm和memorization，以评估预训练数据的质量。</li>
<li>results: 研究发现，使用简单的plexity估计方法可以更好地评估预训练数据的质量，并且可以提高模型的性能。此外，研究还发现，只使用30%的原始训练数据可以达到与全量训练数据相同的性能。<details>
<summary>Abstract</summary>
Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Interpretable-Solar-Flare-Prediction-with-Attention-based-Deep-Neural-Networks"><a href="#Towards-Interpretable-Solar-Flare-Prediction-with-Attention-based-Deep-Neural-Networks" class="headerlink" title="Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks"></a>Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04558">http://arxiv.org/abs/2309.04558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/fulldiskattention">https://bitbucket.org/gsudmlab/fulldiskattention</a></li>
<li>paper_authors: Chetraj Pandey, Anli Ji, Rafal A. Angryk, Berkay Aydin</li>
<li>for: 这个研究的目的是提出一种基于注意力的深度学习模型，用于预测下一天内出现M1.0级或更高级的太阳风暴。</li>
<li>methods: 这个模型使用了数据增强折射，并使用了true skill statistic（TSS）和Heidke skill score（HSS）来评估模型的性能。</li>
<li>results: 研究发现，使用注意力机制可以帮助模型更好地从全天照片中提取有关活跃区域的信息，并且模型可以预测近日至远日的太阳风暴。<details>
<summary>Abstract</summary>
Solar flare prediction is a central problem in space weather forecasting and recent developments in machine learning and deep learning accelerated the adoption of complex models for data-driven solar flare forecasting. In this work, we developed an attention-based deep learning model as an improvement over the standard convolutional neural network (CNN) pipeline to perform full-disk binary flare predictions for the occurrence of $\geq$M1.0-class flares within the next 24 hours. For this task, we collected compressed images created from full-disk line-of-sight (LoS) magnetograms. We used data-augmented oversampling to address the class imbalance issue and used true skill statistic (TSS) and Heidke skill score (HSS) as the evaluation metrics. Furthermore, we interpreted our model by overlaying attention maps on input magnetograms and visualized the important regions focused on by the model that led to the eventual decision. The significant findings of this study are: (i) We successfully implemented an attention-based full-disk flare predictor ready for operational forecasting where the candidate model achieves an average TSS=0.54$\pm$0.03 and HSS=0.37$\pm$0.07. (ii) we demonstrated that our full-disk model can learn conspicuous features corresponding to active regions from full-disk magnetogram images, and (iii) our experimental evaluation suggests that our model can predict near-limb flares with adept skill and the predictions are based on relevant active regions (ARs) or AR characteristics from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
太阳风暴预测是 espacio weather forecasting 中的中心问题，而最近的机器学习和深度学习技术的发展使得复杂的模型在数据驱动太阳风暴预测中得到了广泛的应用。在这项工作中，我们开发了一种注意力基于的深度学习模型，用于在下一个24小时内预测全盘二分类太阳风暴（M1.0级以上）的发生。为此，我们收集了全盘线性视图（LoS）磁场agram的压缩图像。我们使用数据增强抽样来解决类别不均匀问题，并使用真正技能统计（TSS）和海德ке技能分数（HSS）作为评估指标。此外，我们还进行了模型解释，将注意力地图涂抹到输入磁场agram上，并视频化了模型决策过程中关键的区域。研究的主要发现包括：（i）我们成功地实现了全盘注意力基于的太阳风暴预测模型，该模型在下一个24小时内的平均TSS为0.54±0.03，HSS为0.37±0.07。（ii）我们表明了全盘模型可以从全盘磁场agram图像中学习明显的活跃区域特征，（iii）我们的实验评估表明，我们的模型可以预测近地风暴，并且预测基于全盘磁场agram中的相关活跃区域（ARs）或AR特征。
</details></li>
</ul>
<hr>
<h2 id="Regret-Optimal-Federated-Transfer-Learning-for-Kernel-Regression-with-Applications-in-American-Option-Pricing"><a href="#Regret-Optimal-Federated-Transfer-Learning-for-Kernel-Regression-with-Applications-in-American-Option-Pricing" class="headerlink" title="Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing"></a>Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04557">http://arxiv.org/abs/2309.04557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/regretoptimalfederatedtransferlearning">https://github.com/floriankrach/regretoptimalfederatedtransferlearning</a></li>
<li>paper_authors: Xuwei Yang, Anastasis Kratsios, Florian Krach, Matheus Grasselli, Aurelien Lucchi</li>
<li>For: The paper proposes an optimal iterative scheme for federated transfer learning, aiming to minimize the cumulative deviation of the generated parameters from the specialized parameters across all iterations, while respecting the loss function for the model produced by the algorithm upon halting.* Methods: The proposed algorithm allows for continual communication between each specialized model and the central planner at each iteration, and derives explicit updates for the regret-optimal algorithm. Additionally, the paper develops a nearly regret-optimal heuristic that runs with fewer elementary operations.* Results: The paper shows that the regret-optimal algorithm has a regret bound of $\mathcal{O}(\varepsilon q \bar{N}^{1&#x2F;2})$, where $\bar{N}$ is the aggregate number of training pairs, and an adversary which perturbs $q$ training pairs by at-most $\varepsilon&gt;0$ cannot reduce the algorithm’s regret by more than this bound. The paper also conducts numerical experiments in the context of American option pricing to validate the theoretical findings.<details>
<summary>Abstract</summary>
We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of the parameter space. Additionally, we investigate the adversarial robustness of the regret-optimal algorithm showing that an adversary which perturbs $q$ training pairs by at-most $\varepsilon>0$, across all training sets, cannot reduce the regret-optimal algorithm's regret by more than $\mathcal{O}(\varepsilon q \bar{N}^{1/2})$, where $\bar{N}$ is the aggregate number of training pairs. To validate our theoretical findings, we conduct numerical experiments in the context of American option pricing, utilizing a randomly generated finite-rank kernel.
</details>
<details>
<summary>摘要</summary>
我们提出一种优化的迭代方案 для联邦学习传输，其中中央规划者有Access to datasets $\mathcal{D}_1, \ldots, \mathcal{D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1}, \ldots, \theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of the parameter space. Additionally, we investigate the adversarial robustness of the regret-optimal algorithm showing that an adversary which perturbs $q$ training pairs by at-most $\varepsilon>0$, across all training sets, cannot reduce the regret-optimal algorithm's regret by more than $\mathcal{O}(\varepsilon q \bar{N}^{1/2})$, where $\bar{N}$ is the aggregate number of training pairs. To validate our theoretical findings, we conduct numerical experiments in the context of American option pricing, utilizing a randomly generated finite-rank kernel.Note: Simplified Chinese is a written form of Chinese that uses simpler grammar and vocabulary than Traditional Chinese. It is commonly used in mainland China and other countries where Simplified Chinese is the official writing system.
</details></li>
</ul>
<hr>
<h2 id="Connecting-NTK-and-NNGP-A-Unified-Theoretical-Framework-for-Neural-Network-Learning-Dynamics-in-the-Kernel-Regime"><a href="#Connecting-NTK-and-NNGP-A-Unified-Theoretical-Framework-for-Neural-Network-Learning-Dynamics-in-the-Kernel-Regime" class="headerlink" title="Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime"></a>Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04522">http://arxiv.org/abs/2309.04522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yehonatan Avidan, Qianyi Li, Haim Sompolinsky</li>
<li>for: 本文旨在提供深度神经网络学习过程的完整理论框架，尤其是在无穷宽网络 regime 中。</li>
<li>methods: 本文使用 Markov  proximal 学习模型，derive 了一个 exact 的分析表达式，描述了网络输入-输出函数在学习过程中和学习完成后的行为。</li>
<li>results: 本文发现了两个不同时间尺度的学习阶段：梯度驱动学习阶段和扩散学习阶段。在梯度驱动学习阶段，动力是由 deterministic 梯度下降控制的，可以使用 NTK 理论来描述。而在扩散学习阶段，网络参数 sampling 了解决空间，最终 approached 到 NNGP 理论中的平衡分布。这种研究帮助 closure 了 NTK 和 NNGP 两种理论之间的知识渊盈，为深度神经网络在无穷宽网络 regime 中的学习过程提供了一个完整的理论框架。<details>
<summary>Abstract</summary>
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP kernels can be derived. We identify two learning phases characterized by different time scales: gradient-driven and diffusive learning. In the initial gradient-driven learning phase, the dynamics is dominated by deterministic gradient descent, and is described by the NTK theory. This phase is followed by the diffusive learning stage, during which the network parameters sample the solution space, ultimately approaching the equilibrium distribution corresponding to NNGP. Combined with numerical evaluations on synthetic and benchmark datasets, we provide novel insights into the different roles of initialization, regularization, and network depth, as well as phenomena such as early stopping and representational drift. This work closes the gap between the NTK and NNGP theories, providing a comprehensive framework for understanding the learning process of deep neural networks in the infinite width limit.
</details>
<details>
<summary>摘要</summary>
人工神经网络在最近几年内革命化机器学习，但完整的理论框架仍然缺失。在无穷宽网络下，有两种不同的理论框架被用来描述网络输出，它们分别是基于神经汇抽象（NTK）和神经网络泊利过程（NNGP）框架。然而，这两种框架之间的关系仍然是一个谜。本文将这两种不同的理论联系起来，使用一种Markov靠近学习模型来描述学习过程中的动态。我们得到了一个精确的分析表达，描述了网络输入输出函数在学习和学习后的行为。此外，我们还引入了一种时间依赖的神经动力学kernel（NDK），它可以从NTK和NNGP框架中 derivation。我们在不同时间尺度上分类了学习阶段，包括梯度驱动的学习阶段和协同学习阶段。在梯度驱动学习阶段，动态是由deterministic梯度下降控制的，这与NTK理论相吻合。这个阶段后接着的协同学习阶段，网络参数在解决空间中走讲，最终 approaching NNGP的平衡分布。通过对synthetic和benchmark数据集进行数值评估，我们提供了新的视角，描述了不同的初始化、正则化和网络深度如何影响学习过程。这个工作 closure NTK和NNGP理论之间的差异，提供了深度学习过程的全面框架。
</details></li>
</ul>
<hr>
<h2 id="On-the-Actionability-of-Outcome-Prediction"><a href="#On-the-Actionability-of-Outcome-Prediction" class="headerlink" title="On the Actionability of Outcome Prediction"></a>On the Actionability of Outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04470">http://arxiv.org/abs/2309.04470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andrewmogbolu2/blockchain-technology">https://github.com/andrewmogbolu2/blockchain-technology</a></li>
<li>paper_authors: Lydia T. Liu, Solon Barocas, Jon Kleinberg, Karen Levy</li>
<li>for: 这 paper 的目的是探讨在社会影响领域中预测未来结果的应用，以及预测结果后的有效行动方法。</li>
<li>methods: 这 paper 使用了一个简单的模型，包括行动、潜在状态和测量，来研究在具有多种可能的行动时，纯粹的结果预测是否有助于选择最有效的行动。</li>
<li>results: 研究发现，纯粹的结果预测并不总是最有效的政策，即使与其他测量结合使用。 在大多数情况下，测量行动可能的状态会提高行动价值，并且这种提高的程度取决于行动成本和结果模型。 这种分析表明，在实施 Setting 中，需要超越基本的结果预测，并考虑可能的行动和潜在状态的知识。<details>
<summary>Abstract</summary>
Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.   In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable intervention?   Through a simple model encompassing actions, latent states, and measurements, we demonstrate that pure outcome prediction rarely results in the most effective policy for taking actions, even when combined with other measurements. We find that except in cases where there is a single decisive action for improving the outcome, outcome prediction never maximizes "action value", the utility of taking actions. Making measurements of actionable latent states, where specific actions lead to desired outcomes, considerably enhances the action value compared to outcome prediction, and the degree of improvement depends on action costs and the outcome model. This analysis emphasizes the need to go beyond generic outcome prediction in interventional settings by incorporating knowledge of plausible actions and latent states.
</details>
<details>
<summary>摘要</summary>
预测未来结果是社会影响领域中广泛应用的机器学习应用之一。例如，预测教育中学生成功的可能性，预测医疗领域中疾病的发展。实践者认为，最终目标不仅是预测，还是效果iveness。然而，有增加证据表明，基于结果预测的下游 intervención可能并不会达到所期望的效果。在大多数领域中，每个个体都有多种可能的 intervención，使得选择最有效的 intervención变得更加困难。即使理解个体的 latent state 与结果之间的 causal 机制，在特定学生或患者（specific student or patient）中，实践者仍需从预算中的 latent state 测量中推断哪一种可能的 intervención会对这个个体最有效。在这种情况下，我们问：精准的结果预测对于选择最适合的 intervención是有帮助的吗？通过一个简单的模型，包括行动、latent state 和测量，我们证明了纯粹的结果预测 rarely leads to the most effective policy for taking actions，即使与其他测量结合使用。我们发现，除非有单一的决定性的 action 可以提高结果，否则，结果预测不能够最大化“action value”，即行动的用处。通过测量行动可能导致的 desirable outcomes 的 latent state，可以显著提高 action value，并且该提高的程度取决于行动成本和结果模型。这种分析强调了在 intervenational 设置中要超越通用的结果预测，并包括 latent state 和可能的 action 的知识。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Improving-Chain-of-Thought-Reasoning-in-Vision-Language-Models"><a href="#Measuring-and-Improving-Chain-of-Thought-Reasoning-in-Vision-Language-Models" class="headerlink" title="Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"></a>Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04461">http://arxiv.org/abs/2309.04461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyi-chen/cotconsistency">https://github.com/yangyi-chen/cotconsistency</a></li>
<li>paper_authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran<br>for:* 这个论文旨在探讨视力语言模型（VLM）是否具备人类水平的视觉理解能力。methods:* 作者提出了一种链条（CoT）基于的一致度测试方法，以评估VLM的理解能力。* 作者还提出了一种人工智能在循环（LLM-Human-in-the-Loop）管道，以降低测试成本。results:* 作者发现，即使使用最佳模型，VLM也无法如人类般系统地进行视觉理解。* 作者提出了一种两Stage训练框架，以提高VLM的理解性能和一致度。Here is the result in Simplified Chinese text:for:* 这个论文旨在探讨视力语言模型（VLM）是否具备人类水平的视觉理解能力。methods:* 作者提出了一种链条（CoT）基于的一致度测试方法，以评估VLM的理解能力。* 作者还提出了一种人工智能在循环（LLM-Human-in-the-Loop）管道，以降低测试成本。results:* 作者发现，即使使用最佳模型，VLM也无法如人类般系统地进行视觉理解。* 作者提出了一种两Stage训练框架，以提高VLM的理解性能和一致度。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.
</details>
<details>
<summary>摘要</summary>
vision-language模型（VLM）最近已经展现出强大的能力，可以作为视觉助手，理解自然的视觉问题并生成人类如样的输出。在这项工作中，我们探索VLM的能力是否可以与人类一样进行视觉理解和逻辑推理。为了解决VLM的逻辑能力是否充分一致和基础的问题，我们还提出了链条（CoT）基于的一致度测试。然而，这种评估需要一个包括高级推理和细节逻辑链的启发式测试，这很costly。我们解决这个挑战，提出了一个LLM-人类 loop（LLM-Human-in-the-Loop）管道，可以大幅降低成本，同时保证生成高质量的数据集。基于这个管道和现有的粗略标注数据集，我们建立了CURE标准测试 benchmark，用于测试VLM的零基础理解和一致性。我们评估了现有的状态平台VLM，发现even最佳performing模型无法充分表现出视觉逻辑能力和一致性，这表明VLM需要进一步的努力，以使其能够系统地和一致地进行视觉逻辑。为此，我们提出了一个两 stage 训练框架，以提高VLM的逻辑性和一致性。第一个阶段是通过精心练习VLM，使其通过LLM自动生成的步骤 reasoning samples进行supervised fine-tuning。第二个阶段是通过 incorporating LLMs 的反馈，生成高一致性和基础的逻辑链。我们经验表明，我们的框架可以大幅提高VLM的逻辑性和一致性。
</details></li>
</ul>
<hr>
<h2 id="tSPM-a-high-performance-algorithm-for-mining-transitive-sequential-patterns-from-clinical-data"><a href="#tSPM-a-high-performance-algorithm-for-mining-transitive-sequential-patterns-from-clinical-data" class="headerlink" title="tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data"></a>tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05671">http://arxiv.org/abs/2309.05671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Hügel, Ulrich Sax, Shawn N. Murphy, Hossein Estiri</li>
<li>for: 这篇论文旨在探讨如何使用计算机技术来描述复杂的疾病，特别是通过时间序列 Pattern mining 和机器学习工作流程。</li>
<li>methods: 这篇论文使用的方法是时间序列 Pattern mining 算法，并在其中添加了时间长度的维度，以提高算法的性能。</li>
<li>results: 研究人员发现，使用 tSPM+ 算法可以提高计算速度，并且可以降低内存占用量，相比之下，传统的 tSPM 算法可以提高计算速度，但是内存占用量较高。此外，研究人员还提供了一个 Docker 容器和一个 R 包，以便轻松地 инте integrate 到现有的机器学习工作流程中。<details>
<summary>Abstract</summary>
The increasing availability of large clinical datasets collected from patients can enable new avenues for computational characterization of complex diseases using different analytic algorithms. One of the promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows. However, mining these temporal patterns is a computational intensive task and has memory repercussions. Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm, are already providing promising outcomes, but still leave room for optimization. In this paper, we present the tSPM+ algorithm, a high-performance implementation of the tSPM algorithm, which adds a new dimension by adding the duration to the temporal patterns. We show that the tSPM+ algorithm provides a speed up to factor 980 and a up to 48 fold improvement in memory consumption. Moreover, we present a docker container with an R-package, We also provide vignettes for an easy integration into already existing machine learning workflows and use the mined temporal sequences to identify Post COVID-19 patients and their symptoms according to the WHO definition.
</details>
<details>
<summary>摘要</summary>
随着大规模临床数据集的更加可用，可以开拓新的 computationally characterization of complex diseases 的avenues。一种promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows。However, mining these temporal patterns is a computationally intensive task and has memory repercussions。Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm，are already providing promising outcomes，but still leave room for optimization。在这篇论文中，我们提出了 tSPM+ 算法，一种高性能实现 tSPM 算法，它添加了时间长度到 temporalsequences。我们表明，tSPM+ 算法可以提高速度到 factor 980，并提高内存使用量。此外，我们还提供了一个 Docker 容器和一个 R 包，并提供了一些可以方便地integrate into existing machine learning workflows的 vignettes。此外，我们使用分析的 temporalsequences来识别患 COVID-19 病人和其症状，根据 WHO 定义。
</details></li>
</ul>
<hr>
<h2 id="Subwords-as-Skills-Tokenization-for-Sparse-Reward-Reinforcement-Learning"><a href="#Subwords-as-Skills-Tokenization-for-Sparse-Reward-Reinforcement-Learning" class="headerlink" title="Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning"></a>Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04459">http://arxiv.org/abs/2309.04459</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Yunis, Justin Jung, Falcon Dai, Matthew Walter</li>
<li>for: 提高鲁棒性学习在缺乏奖励的环境中的能力，尤其是在连续动作空间中。</li>
<li>methods: 使用分 clustering 将动作空间细分，并使用自然语言处理中的tokenization技术生成 temporally extended actions。</li>
<li>results: 相比基eline，该方法在一些具有挑战性的缺乏奖励环境中表现出色，并需要下降多个计算量级别的 computation。<details>
<summary>Abstract</summary>
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing to generate temporally extended actions. Such a method outperforms baselines for skill-generation in several challenging sparse-reward domains, and requires orders-of-magnitude less computation in skill-generation and online rollouts.
</details>
<details>
<summary>摘要</summary>
We propose a novel approach to skill-generation with two components. First, we discretize the action space through clustering, and second, we leverage a tokenization technique from natural language processing to generate temporally extended actions. Our approach outperforms baselines for skill-generation in several challenging sparse-reward domains and requires orders-of-magnitude less computation in skill-generation and online rollouts.Here is the text in Simplified Chinese:探索 sparse-reward 学习中的困难在于需要长 sequences of actions 才能获得任何奖励。此外，连续动作空间中有无限多个可能的动作，这只会使探索更加困难。为了解决这些问题，一种方法是通过从同一个领域中收集的交互数据来形成 temporally extended actions，或者技能，并且优化一个策略。然而，这通常需要一个较长的预训练阶段，尤其是在连续动作空间中。我们提出了一种新的方法，它包括两个组成部分。首先，我们使用 clustering 将动作空间细分，然后使用自然语言处理中的tokenization技术来生成 temporally extended actions。我们的方法在一些复杂的 sparse-reward 领域中表现出色，并且需要下注意力量的 computation 在技能生成和在线执行中。
</details></li>
</ul>
<hr>
<h2 id="Postprocessing-of-Ensemble-Weather-Forecasts-Using-Permutation-invariant-Neural-Networks"><a href="#Postprocessing-of-Ensemble-Weather-Forecasts-Using-Permutation-invariant-Neural-Networks" class="headerlink" title="Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks"></a>Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04452">http://arxiv.org/abs/2309.04452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khoehlein/Permutation-invariant-Postprocessing">https://github.com/khoehlein/Permutation-invariant-Postprocessing</a></li>
<li>paper_authors: Kevin Höhlein, Benedikt Schulz, Rüdiger Westermann, Sebastian Lerch</li>
<li>for: 这项研究旨在探讨使用 permutation-invariant neural network 进行 numerical weather forecasts 的 statistically postprocessing，以生成可靠的 probabilistic forecast distribution。</li>
<li>methods: 研究使用 permutation-invariant neural network，该网络将 ensemble forecast 视为一组无序成员预测，并学习链函数，以保证链函数对 permutation 的敏感性。</li>
<li>results: 对于 surface temperature 和 wind gust forecasts 的 posteprocessing，研究达到了 state-of-the-art 的预测质量。进一步的 permutation-based importance analysis 表明， ensemble forecast 中大多数重要信息都集中在几个内部度量上，这可能对 future ensemble forecasting 和 postprocessing 系统的设计产生影响。<details>
<summary>Abstract</summary>
Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance analysis for ensemble-valued predictors, which highlights specific aspects of the ensemble forecast that are considered important by the trained postprocessing models. Our results suggest that most of the relevant information is contained in few ensemble-internal degrees of freedom, which may impact the design of future ensemble forecasting and postprocessing systems.
</details>
<details>
<summary>摘要</summary>
统计处理是用于将原始的数值天气预报转换成可靠的 probabilistic 预报分布。在这种研究中，我们考虑使用卷积神经网络来实现这项任务。与之前的方法不同，这些方法通常操作于ensemble summary statistics，并丢弃预报分布的细节信息。我们提议的网络将预报集体看作一组无序成员预报，并学习链函数，这些链函数是设计具有排序无关性的。我们根据预报分布的准确性和锐度进行评价，并与经典和神经网络基础模型进行比较。在地面温度和风 Gust 预报的 случа例研究中，我们达到了当前预报质量的最佳标准。为深入了解所学的推理过程，我们还提出了 permutation-based importance 分析，该分析显示了预报集体中重要的特征，这些特征可能影响未来的ensemble forecasting和postprocessing系统的设计。我们的结果表明，大多数有关信息都包含在预报集体中的几个内部度量上，这可能对未来的预报系统设计产生影响。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Speech-Recognition-and-Disfluency-Removal-with-Acoustic-Language-Model-Pretraining"><a href="#End-to-End-Speech-Recognition-and-Disfluency-Removal-with-Acoustic-Language-Model-Pretraining" class="headerlink" title="End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining"></a>End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04516">http://arxiv.org/abs/2309.04516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidsroth/hubert-disfl">https://github.com/davidsroth/hubert-disfl</a></li>
<li>paper_authors: Saksham Bassi, Giulio Duregon, Siddhartha Jalagam, David Roth</li>
<li>for: 本研究旨在比较两种模型在不连续和对话性听说中的表现，并研究大规模自监学习对audio表示的影响。</li>
<li>methods: 本研究使用了两种模型：两个阶段模型和末端到终端模型。两个阶段模型使用了预训练的语言模型来进行分词和纠错，而末端到终端模型则直接使用大规模自监学习对audio表示进行学习。</li>
<li>results: 研究发现，使用大规模自监学习对audio表示可以帮助末端到终端模型在不连续和对话性听说中表现更好，并且选择适当的预训练目标可以有效地改善模型的适应能力。<details>
<summary>Abstract</summary>
The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and further, that the choice of pretraining objective substantially effects a model's ability to be adapted to the disfluency removal task.
</details>
<details>
<summary>摘要</summary>
现代最佳做法（SOTA）在不流畅和对话speech的转写中是分两个阶段的模型，分别是转写和清洁阶段。我们认为过去对不流畅speech中的缺失的尝试都受到了大规模语言模型预训练的 represencing优势的影响，导致了lexical模型的表达优势。在过去，大型音频数据集的高维度和有限的可用性妨碍了大规模自动化预训练目标的发展，从而给予了两阶段方法偏好。在现在的大规模音频预训练成功之后，我们重新评估了两阶段和终端模型的性能比较，发现 audio基于语言模型预训练使用弱自动化目标可以匹配或超越相同训练的两阶段模型，而且选择的预训练目标对模型的适应性具有很大的影响。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Networks-for-an-optimal-counterdiabatic-quantum-computation"><a href="#Physics-Informed-Neural-Networks-for-an-optimal-counterdiabatic-quantum-computation" class="headerlink" title="Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation"></a>Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04434">http://arxiv.org/abs/2309.04434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Ferrer-Sánchez, Carlos Flores-Garrigos, Carlos Hernani-Morales, José J. Orquín-Marqués, Narendra N. Hegade, Alejandro Gomez Cadavid, Iraitz Montalban, Enrique Solano, Yolanda Vives-Gilabert, José D. Martín-Guerrero</li>
<li>for:  Addressing the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits.</li>
<li>methods:  Utilizes Physics-Informed Neural Networks (PINNs) to leverage the strength of deep learning techniques and accurately solve the time evolution of physical observables within the quantum system, while imposing hermiticity conditions and using the principle of least action.</li>
<li>results:  Derives a desirable decomposition for the non-adiabatic terms through a linear combination utilizing Pauli operators, demonstrating significant advantages for practical implementation within quantum computing algorithms.Here is the same information in Simplified Chinese:</li>
<li>for:  Addressing the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits.</li>
<li>methods:  Utilizes Physics-Informed Neural Networks (PINNs) to leverage the strength of deep learning techniques and accurately solve the time evolution of physical observables within the quantum system, while imposing hermiticity conditions and using the principle of least action.</li>
<li>results:  Derives a desirable decomposition for the non-adiabatic terms through a linear combination utilizing Pauli operators, demonstrating significant advantages for practical implementation within quantum computing algorithms.<details>
<summary>Abstract</summary>
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations. Our method provides a general framework to obtain optimal results from the physical observables relevant to the problem, including the external parameterization in time known as scheduling function, the gauge potential or operator involving the non-adiabatic terms, as well as the temporal evolution of the energy levels of the system, among others. The main applications of this methodology have been the $\mathrm{H_{2}$ and $\mathrm{LiH}$ molecules, represented by a 2-qubit and 4-qubit systems employing the STO-3G basis. The presented results demonstrate the successful derivation of a desirable decomposition for the non-adiabatic terms, achieved through a linear combination utilizing Pauli operators. This attribute confers significant advantages to its practical implementation within quantum computing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的方法，利用物理学 informed neural networks（PINNs）来解决量子环境中的Counterdiabatic（CD）协议。我们的目标是使用物理灵感的深度学习技术来准确地解决量子系统中不同物理观测器的时间演化。为达到这个目标，我们将物理知识嵌入到一个底下的神经网中，以有效地处理问题。具体来说，我们将 hermiticity 条件套用到所有物理观测器上，并利用最小行动原理，从而获得最佳的counterdiabatic 条件，基于背景物理。我们的方法提供一个可靠的替代方案，免除了先前的方法对于量子环境中的CD驾驭问题的困难。我们的方法可以从物理观测器中获得最佳的结果，包括时间外推数函数、 gauge 潜在或操作含有非对称项的情况、以及量子系统中能阶的时间演化等。我们已经将这种方法应用到 $\mathrm{H_{2}$ 和 $\mathrm{LiH}$ 分子中，使用 STO-3G 基底，并获得了一个愉悦的分解，这是由 Pauli 算子的线性 комбінаition 实现的。这个特点具有实用实现量子计算 алгоритmi中的实际优点。
</details></li>
</ul>
<hr>
<h2 id="Variations-and-Relaxations-of-Normalizing-Flows"><a href="#Variations-and-Relaxations-of-Normalizing-Flows" class="headerlink" title="Variations and Relaxations of Normalizing Flows"></a>Variations and Relaxations of Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04433">http://arxiv.org/abs/2309.04433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keegan Kelly, Lorena Piedras, Sukrit Rao, David Roth</li>
<li>for: 本文描述了一类模型，称为正常化流（NFs），它们可以表示复杂目标分布为一系列简单基础分布的 compose。</li>
<li>methods: NFs 使用 diffeomorphisms 限制候选变换的空间，从而实现高效、准确的采样和总体值评估。但是，这会限制输入、输出和所有间隔空间的维度相同，从而降低了表达复杂 topology 目标分布的能力。</li>
<li>results: 本文介绍了一些最近的工作，通过结合其他生成模型类型，如 VAEs 和 score-based diffusion，使 NFs 可以更好地平衡表达能力、训练速度、采样效率和可评估性。<details>
<summary>Abstract</summary>
Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a balance of expressivity, training speed, sample efficiency and likelihood tractability.
</details>
<details>
<summary>摘要</summary>
normalizing flows（NF）描述一类模型，表示复杂目标分布为基于 simpler base distribution 的序列bijective变换的composite。通过限制候选变换空间为 diffeomorphisms，NFs 可以实现高效、准确的样本评估和扩散函数评估，使其可以作为权重分布和生成模型兼用。然而，NFs 的假设是输入、输出和所有中间空间具有同样的维度，这限制了它们对复杂分布的表示能力。此外，在 prior 和 target 分布不同HOMEOMORPHIC 时，NFs 可能会导致样本泄漏出目标分布的支持。这篇评论概述了一些最近的工作，把其他生成模型类型，如 VAEs 和 score-based diffusion 的特点相结合，以逃脱 NFs 的刻意对称性限制，以达到表达能力、训练速度、样本效率和概率可读性的平衡。
</details></li>
</ul>
<hr>
<h2 id="Soft-Quantization-using-Entropic-Regularization"><a href="#Soft-Quantization-using-Entropic-Regularization" class="headerlink" title="Soft Quantization using Entropic Regularization"></a>Soft Quantization using Entropic Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04428">http://arxiv.org/abs/2309.04428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajmadan96/softquantization">https://github.com/rajmadan96/softquantization</a></li>
<li>paper_authors: Rajmadan Lakshmanan, Alois Pichler</li>
<li>for: 解决量子化问题中的最佳折衔问题，使用finite, discrete措施来 aproximate probability measures在${\mathbb{R}}^d$中。</li>
<li>methods: 提出了一种基于entropy regularization的量子化问题，使用softmin函数来实现robustness both theoretically and practically。在evaluating the approximation quality中使用 entropy-regularized Wasserstein distance，并使用stochoastic gradient approach来实现最佳解。</li>
<li>results: 提出了一种可调控制参数的方法，以适应具有特殊挑战的问题。经验表明，该方法在不同场景下具有良好的性能。<details>
<summary>Abstract</summary>
The quantization problem aims to find the best possible approximation of probability measures on ${\mathbb{R}^d$ using finite, discrete measures. The Wasserstein distance is a typical choice to measure the quality of the approximation. This contribution investigates the properties and robustness of the entropy-regularized quantization problem, which relaxes the standard quantization problem. The proposed approximation technique naturally adopts the softmin function, which is well known for its robustness in terms of theoretical and practicability standpoints. Moreover, we use the entropy-regularized Wasserstein distance to evaluate the quality of the soft quantization problem's approximation, and we implement a stochastic gradient approach to achieve the optimal solutions. The control parameter in our proposed method allows for the adjustment of the optimization problem's difficulty level, providing significant advantages when dealing with exceptionally challenging problems of interest. As well, this contribution empirically illustrates the performance of the method in various expositions.
</details>
<details>
<summary>摘要</summary>
“量化问题”的目的是找到${\mathbb{R}}^d$上最佳的数值量化方法，使用有限、精确的数值量化方法。“ Wasserstein 距离”是一种常用的衡量该量化方法的质量的度量。本贡献 investigates the properties and robustness of the entropy-regularized quantization problem，这是对标准量化问题的放宽。我们使用了知名的“softmin函数”，它在理论和实践上都具有较好的Robustness。此外，我们使用了 entropy-regulated Wasserstein distance 来衡量该量化问题的数值量化方法的质量，并使用了随机梯度法来实现最佳解。控制参数在我们的提案中允许调整优化问题的困难度，具有对具有特别困难问题的优势。此外，本贡献透过实践探讨，详细显示了方法的表现。
</details></li>
</ul>
<hr>
<h2 id="Robust-Representation-Learning-for-Privacy-Preserving-Machine-Learning-A-Multi-Objective-Autoencoder-Approach"><a href="#Robust-Representation-Learning-for-Privacy-Preserving-Machine-Learning-A-Multi-Objective-Autoencoder-Approach" class="headerlink" title="Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach"></a>Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04427">http://arxiv.org/abs/2309.04427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofiane Ouaari, Ali Burak Ünal, Mete Akgün, Nico Pfeifer</li>
<li>for: 本研究旨在提供一种privacy-preserving machine learning（ppML）技术，以便在各种领域中使用机器学习（ML）而不需要折衣数据隐私。</li>
<li>methods: 本研究使用了深度学习来实现数据编码，并在多目标方面训练自适应神经网络，以优化隐私和实用性之间的质量衡量。</li>
<li>results: 实验结果表明，本方法在单模态和多模态设置中均可以获得改进的性能，并且在某些情况下超过了现有技术的水平。<details>
<summary>Abstract</summary>
Several domains increasingly rely on machine learning in their applications. The resulting heavy dependence on data has led to the emergence of various laws and regulations around data ethics and privacy and growing awareness of the need for privacy-preserving machine learning (ppML). Current ppML techniques utilize methods that are either purely based on cryptography, such as homomorphic encryption, or that introduce noise into the input, such as differential privacy. The main criticism given to those techniques is the fact that they either are too slow or they trade off a model s performance for improved confidentiality. To address this performance reduction, we aim to leverage robust representation learning as a way of encoding our data while optimizing the privacy-utility trade-off. Our method centers on training autoencoders in a multi-objective manner and then concatenating the latent and learned features from the encoding part as the encoded form of our data. Such a deep learning-powered encoding can then safely be sent to a third party for intensive training and hyperparameter tuning. With our proposed framework, we can share our data and use third party tools without being under the threat of revealing its original form. We empirically validate our results on unimodal and multimodal settings, the latter following a vertical splitting system and show improved performance over state-of-the-art.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:多个领域正在通过机器学习应用来增加依赖度，导致数据伦理和隐私法规的出现，以及隐私保护机器学习（ppML）技术的发展。现有ppML技术通常会影响模型性能，以换取隐私。为了提高ppML的性能，我们提议一种基于robust representation learning的框架，可以同时保证数据隐私和模型性能。我们的方法包括在多目标下进行autoencoder训练，然后将编码部分的秘密特征和学习特征 concatenate为数据的编码形式。这种深度学习力学托的编码可以安全地传输到第三方进行敏感训练和超参调整。我们的提案可以帮助我们共享数据，无需担心数据的原始形式泄露。我们在不同的模式下进行了实验 validate，包括单模态和多模态设置，并达到了状态 искусственный的提升。
</details></li>
</ul>
<hr>
<h2 id="Parallel-and-Limited-Data-Voice-Conversion-Using-Stochastic-Variational-Deep-Kernel-Learning"><a href="#Parallel-and-Limited-Data-Voice-Conversion-Using-Stochastic-Variational-Deep-Kernel-Learning" class="headerlink" title="Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning"></a>Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04420">http://arxiv.org/abs/2309.04420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamadreza Jafaryani, Hamid Sheikhzadeh, Vahid Pourahmadi</li>
<li>for: 这篇论文是针对voice conversion问题，具体地是如何使用有限数据进行deep learning模型的训练，并且能够模型复杂的映射函数。</li>
<li>methods: 这篇论文提出了一种基于stochastic variational deep kernel learning（SVDKL）的voice conversion方法，它能够结合深度神经网络和 Gaussian process，并且可以解决有限数据的问题。</li>
<li>results: 在使用约80秒的训练数据进行训练后，这篇论文的方法比较其他方法取得了更高的mean opinion score、更小的spectral distortion和更好的偏好测试结果。<details>
<summary>Abstract</summary>
Typically, voice conversion is regarded as an engineering problem with limited training data. The reliance on massive amounts of data hinders the practical applicability of deep learning approaches, which have been extensively researched in recent years. On the other hand, statistical methods are effective with limited data but have difficulties in modelling complex mapping functions. This paper proposes a voice conversion method that works with limited data and is based on stochastic variational deep kernel learning (SVDKL). At the same time, SVDKL enables the use of deep neural networks' expressive capability as well as the high flexibility of the Gaussian process as a Bayesian and non-parametric method. When the conventional kernel is combined with the deep neural network, it is possible to estimate non-smooth and more complex functions. Furthermore, the model's sparse variational Gaussian process solves the scalability problem and, unlike the exact Gaussian process, allows for the learning of a global mapping function for the entire acoustic space. One of the most important aspects of the proposed scheme is that the model parameters are trained using marginal likelihood optimization, which considers both data fitting and model complexity. Considering the complexity of the model reduces the amount of training data by increasing the resistance to overfitting. To evaluate the proposed scheme, we examined the model's performance with approximately 80 seconds of training data. The results indicated that our method obtained a higher mean opinion score, smaller spectral distortion, and better preference tests than the compared methods.
</details>
<details>
<summary>摘要</summary>
通常，voice conversion被视为一个工程问题，受限于有限的训练数据。深入研究的深度学习方法需要大量数据，但是这限制了实际应用的可行性。相反，统计方法可以使用有限数据，但是它们无法模型复杂的映射函数。这篇论文提出了一种基于有限数据的voice conversion方法，该方法基于Stochastic Variational Deep Kernel Learning（SVDKL）。同时，SVDKL使得可以使用深度神经网络的表达能力以及高灵活的Gaussian процес为 Bayesian和非 Parametric方法。当权重积分是加到深度神经网络时，可以估计非稀盐和更复杂的函数。此外，模型的稀盐变量Gaussian进程解决了扩展性问题，与传统kernel相比，允许学习整个声学空间的全局映射函数。我们的方法的一个重要特点是通过最大化含义概率来训练模型参数，这会考虑数据适应和模型复杂度。通过增加模型复杂度，我们降低了训练数据的量，从而降低了过拟合的风险。为评估我们的方法，我们对约80秒的训练数据进行了测试。结果表明，我们的方法在意见序列、spectral distortion和偏好测试中获得了更高的 mean opinion score、更小的spectral distortion和更好的偏好测试结果，比与比较方法更高。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Federated-Learning-with-Convolutional-Variational-Bottlenecks"><a href="#Privacy-Preserving-Federated-Learning-with-Convolutional-Variational-Bottlenecks" class="headerlink" title="Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks"></a>Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04515">http://arxiv.org/abs/2309.04515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Scheliga, Patrick Mäder, Marco Seeland<br>for: 防止梯度泄露攻击，保护隐私训练数据。methods: 使用variational modeling技术，实现隐私保护。results: 1. 发现variational modeling引入杂态性，使得梯度泄露攻击无法进行迭代加密攻击。 2. 提出了一种破坏隐私保护效果的攻击方法，并解释了如何保护隐私。 3. 提出了一种新的隐私模块——卷积变量瓶颈（CVB），可以在潜在攻击下保护隐私。<details>
<summary>Abstract</summary>
Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To preserve the privacy preserving effect of PRECODE, our analysis reveals that variational modeling must be placed early in the network. However, early placement of PRECODE is typically not feasible due to reduced model utility and the exploding number of additional model parameters. Therefore, as a third contribution, we propose a novel privacy module -- the Convolutional Variational Bottleneck (CVB) -- that can be placed early in a neural network without suffering from these drawbacks. We conduct an extensive empirical study on three seminal model architectures and six image classification datasets. We find that all architectures are susceptible to gradient leakage attacks, which can be prevented by our proposed CVB. Compared to PRECODE, we show that our novel privacy module requires fewer trainable parameters, and thus computational and communication costs, to effectively preserve privacy.
</details>
<details>
<summary>摘要</summary>
gradient inversion attacks 是联邦学习中的普遍威胁，它们利用Gradient泄露来重建训练资料。 recent work 提出了防止Gradient泄露而无损模型价值的方法，包括在模型中添加一个基于可变型模型的Privacy EnhanCing mODulE (PRECODE)。 without further analysis, it was shown that PRECODE 成功地保护 Against gradient inversion attacks。在这篇 paper 中，我们做了多个贡献。 first, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle。 we show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network。 the stochastic gradients of these layers prevent iterative gradient inversion attacks from converging。second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization。 to preserve the privacy preserving effect of PRECODE, our analysis reveals that variational modeling must be placed early in the network。 however, early placement of PRECODE is typically not feasible due to reduced model utility and the exploding number of additional model parameters。therefore, as a third contribution, we propose a novel privacy module -- the Convolutional Variational Bottleneck (CVB) -- that can be placed early in a neural network without suffering from these drawbacks。 we conduct an extensive empirical study on three seminal model architectures and six image classification datasets。 we find that all architectures are susceptible to gradient leakage attacks, which can be prevented by our proposed CVB。 compared to PRECODE, we show that our novel privacy module requires fewer trainable parameters, and thus computational and communication costs, to effectively preserve privacy。
</details></li>
</ul>
<hr>
<h2 id="Emergent-learning-in-physical-systems-as-feedback-based-aging-in-a-glassy-landscape"><a href="#Emergent-learning-in-physical-systems-as-feedback-based-aging-in-a-glassy-landscape" class="headerlink" title="Emergent learning in physical systems as feedback-based aging in a glassy landscape"></a>Emergent learning in physical systems as feedback-based aging in a glassy landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04382">http://arxiv.org/abs/2309.04382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidyesh Rao Anisetti, Ananth Kandala, J. M. Schwarz</li>
<li>for: 研究了使用线性物理网络学习线性变换的方法，并观察了这些网络的物理性质如何随权重更新规则的发展。</li>
<li>methods: 使用了线性物理网络来学习线性变换，并通过观察这些网络的物理性质来了解它们如何随权重更新规则的发展。</li>
<li>results: 发现了这些网络在学习过程中的相似性和生物系统中的记忆形成过程，以及在不同输入和反馈边界力的作用下，网络的相似性和记忆形成的机制。<details>
<summary>Abstract</summary>
By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundary forces, the process of emergent learning can be rather ubiquitous and, thus, serve as a very early physical mechanism, from an evolutionary standpoint, for learning in biological systems.
</details>
<details>
<summary>摘要</summary>
通过训练线性物理网络学习线性变换，我们发现其物理性质如何随权重更新规则而演化。我们的发现表明linear physical networks的学习行为和缺陷系统和玻璃系统中的年轻和记忆形成过程具有很大的相似性。我们显示，学习过程类似于年轻过程，系统在反复应用反馈边界力时会relax，从而记忆输入输出关系。这种relaxation会导致系统的相关性增加，这可以通过网络组件之间的两点相关函数来证明。此外，我们发现epoch随时间的平方根均方差的函数具有非线性形式，这是玻璃系统的典型特征。这种物理解释表明，通过增加输入和反馈边界力的细节信息，emergent learning的过程可以是非常普遍的，从EVOLUTIONARY的角度来看，这可能是生物系统中的极早物理机制。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Bounds-Perspectives-from-Information-Theory-and-PAC-Bayes"><a href="#Generalization-Bounds-Perspectives-from-Information-Theory-and-PAC-Bayes" class="headerlink" title="Generalization Bounds: Perspectives from Information Theory and PAC-Bayes"></a>Generalization Bounds: Perspectives from Information Theory and PAC-Bayes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04381">http://arxiv.org/abs/2309.04381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky</li>
<li>for: 本文旨在介绍信息理论上的泛化能力，以及与PAC-Bayesian方法的强关系。</li>
<li>methods: 本文使用信息论的泛化 bound 和 PAC-Bayesian方法来研究机器学习算法的泛化能力。</li>
<li>results: 本文提出了一种简单的泛化 bound，并证明了它在多个场景下具有泛化能力。此外，本文还应用到深度学习中。<details>
<summary>Abstract</summary>
A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demonstrate how many proofs in the area share a modular structure, through which the underlying ideas can be intuited. We pay special attention to the conditional mutual information (CMI) framework; analytical studies of the information complexity of learning algorithms; and the application of the proposed methods to deep learning. This monograph is intended to provide a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible. It is aimed broadly towards researchers with an interest in generalization and theoretical machine learning.
</details>
<details>
<summary>摘要</summary>
一个基本问题在理论机器学习领域是泛化。过去几十年，PAC-Bayesian方法在解决机器学习算法的泛化能力方面成为了一个灵活的框架，并设计了新的算法。最近几年，它受到了更多的关注，因为它可以应用于许多学习算法，包括深度神经网络。在平行的情况下，一种信息论视角的泛化发展出来，其中与泛化之间的关系与各种信息度量相关。这个框架与PAC-Bayesian方法密切相关，并在两个条件下独立地发现了一些结果。在这本善本中，我们强调这种强有力的连接，并提供一个统一的对待方法。我们介绍了这两个视角之间的共同技巧和结果，并讨论各种不同的方法和解释。特别是，我们示出了许多证明在这个领域中具有模块结构，从而可以直观到下面的基本想法。我们特别关注 conditional mutual information（CMI）框架，信息复杂度学习算法的分析研究，以及将提议的方法应用于深度学习。这本善本的目的是为研究人员提供一种全面的信息论泛化下限和其与PAC-Bayes之间的连接，作为对最新发展的基础。它是向研究泛化和理论机器学习的广泛兴趣 GROUP 的欢迎。
</details></li>
</ul>
<hr>
<h2 id="Seeing-Eye-Quadruped-Navigation-with-Force-Responsive-Locomotion-Control"><a href="#Seeing-Eye-Quadruped-Navigation-with-Force-Responsive-Locomotion-Control" class="headerlink" title="Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control"></a>Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04370">http://arxiv.org/abs/2309.04370</a></li>
<li>repo_url: None</li>
<li>paper_authors: David DeFazio, Eisuke Hirota, Shiqi Zhang</li>
<li>for: 这个论文的目的是开发一种可以抗 External tugging forces 的 seeing-eye robot系统，以帮助视障人士进行导航。</li>
<li>methods: 论文使用了强化学习（RL）和指导学习来同时训练一个稳定的步行控制器和一个外部力估计器。</li>
<li>results: 实验结果表明，我们的控制器具有 External forces 的抗衡能力，而我们的 seeing-eye 系统可以准确地检测力方向。我们在真实的四足机器人上进行了实验，并在视频中展示了一位盲人在使用我们的系统时的导航过程。Video可以在我们项目页面上查看：<a target="_blank" rel="noopener" href="https://bu-air-lab.github.io/guide_dog/">https://bu-air-lab.github.io/guide_dog/</a><details>
<summary>Abstract</summary>
Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: https://bu-air-lab.github.io/guide_dog/
</details>
<details>
<summary>摘要</summary>
seeing-eye  роботы非常有用于引导视障人士，可能会产生巨大的社会影响，因为现有的真正的引导狗非常scarce和昂贵。虽然一些seeing-eye robot系统已经被示出，但None of them considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: <https://bu-air-lab.github.io/guide_dog/>.Here's the word-for-word translation of the text into Simplified Chinese:seeing-eye 机器人非常有用于引导视障人士，可能会产生巨大的社会影响，因为现有的真正的引导狗非常scarce 和昂贵。虽然一些seeing-eye robot系统已经被示出，但None of them considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: <https://bu-air-lab.github.io/guide_dog/>.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Classifying-2D-Grid-Based-Level-Completability"><a href="#Active-Learning-for-Classifying-2D-Grid-Based-Level-Completability" class="headerlink" title="Active Learning for Classifying 2D Grid-Based Level Completability"></a>Active Learning for Classifying 2D Grid-Based Level Completability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04367">http://arxiv.org/abs/2309.04367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahsabazzaz/level-completabilty-x-active-learning">https://github.com/mahsabazzaz/level-completabilty-x-active-learning</a></li>
<li>paper_authors: Mahsa Bazzaz, Seth Cooper</li>
<li>for: 用于评估生成器生成的水平的可完成性</li>
<li>methods: 使用活动学习方法来标注生成的水平可完成性</li>
<li>results: 使用活动学习方法可以使得类ifier性能更高，使用同样量的标注数据。<details>
<summary>Abstract</summary>
Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same amount of labeled data.
</details>
<details>
<summary>摘要</summary>
确定由生成器模型生成的关卡的完整性可能是一项挑战，因为它可能需要使用解决器代理，这些解决器经常需要较长的时间来分析和解决关卡。活动学习并未广泛应用于游戏评估中，although it has been successfully applied in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive。在本文中，我们提议使用活动学习来学习关卡完整性分类。通过活动学习方法，我们用深度学习模型来分类生成的关卡的完整性，并对Super Mario Bros., Kid Icarus,和一款zelda-like游戏进行了测试。我们比较了使用活动学习方法来查询关卡的完整性标签与随机查询的性能。我们的结果显示，使用活动学习方法来标注关卡的完整性可以提高分类器的性能，使用同样的标签数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Power-Signals-An-Automated-Approach-to-Electrical-Disturbance-Identification-Within-a-Power-Transmission-System"><a href="#Learning-from-Power-Signals-An-Automated-Approach-to-Electrical-Disturbance-Identification-Within-a-Power-Transmission-System" class="headerlink" title="Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System"></a>Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04361">http://arxiv.org/abs/2309.04361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan D. Boyd, Joshua H. Tyler, Anthony M. Murphy, Donald R. Reising<br>for:这个研究是为了 automatizing the analysis of power quality events recorded by digital fault recorders and power quality monitors within a power transmission system.methods:这个方法使用了规则基于的分析来检查时间和频率域特性的电压和电流信号。研究人员可以设置自定义的阈值来分类每个异常事件。results:这个研究获得了九十九 percent的准确率，并且预计将减少内存需求，并且帮助检测小型信号变化，以预防问题发生。这个 проек 预计将减少工程师的时间投入，并提高了变电压系统的可靠性。<details>
<summary>Abstract</summary>
As power quality becomes a higher priority in the electric utility industry, the amount of disturbance event data continues to grow. Utilities do not have the required personnel to analyze each event by hand. This work presents an automated approach for analyzing power quality events recorded by digital fault recorders and power quality monitors operating within a power transmission system. The automated approach leverages rule-based analytics to examine the time and frequency domain characteristics of the voltage and current signals. Customizable thresholds are set to categorize each disturbance event. The events analyzed within this work include various faults, motor starting, and incipient instrument transformer failure. Analytics for fourteen different event types have been developed. The analytics were tested on 160 signal files and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data analysis is performed using an approach coined as the cyclic histogram. The cyclic histogram process will be integrated into the digital fault recorders themselves to facilitate the detection of subtle signal variations that are too small to trigger a disturbance event and that can occur over hours or days. In addition to reducing memory requirements by a factor of 320, it is anticipated that cyclic histogram processing will aid in identifying incipient events and identifiers. This project is expected to save engineers time by automating the classification of disturbance events and increase the reliability of the transmission system by providing near real time detection and identification of disturbances as well as prevention of problems before they occur.
</details>
<details>
<summary>摘要</summary>
随着电力质量在电力供应业中的重要性提高，发生事件数据的量继续增加。utilities没有足够的人员来手动分析每个事件。本工作提出了一种自动化分析力量质量事件记录器和电力质量监测器在电力传输系统中记录的数据的方法。该方法利用规则基本的分析来查看电压和电流信号在时域和频域的特征。可定制的阈值设置以 categorize each disturbance event。本工作分析了多种各种事件，包括各种缺陷、电机启动和潜在的仪器变换器故障。为了提高分析效率，本工作采用了 cycles histogram 技术，可以在实时near real-time detection and identification of disturbances, as well as preventing problems before they occur.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Value-Compressed-Sparse-Column-VCSC-Sparse-Matrix-Storage-for-Redundant-Data"><a href="#Value-Compressed-Sparse-Column-VCSC-Sparse-Matrix-Storage-for-Redundant-Data" class="headerlink" title="Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data"></a>Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04355">http://arxiv.org/abs/2309.04355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Ruiter, Seth Wolfgang, Marc Tunnell, Timothy Triche Jr., Erin Carrier, Zachary DeBruine</li>
<li>for: 本研究旨在提出两种扩展了简约矩阵存储格式（CSC），以优化稀疏数据的存储和计算效率。</li>
<li>methods: 研究人员提出了两种新的简约存储格式：Value-Compressed Sparse Column (VCSC)和Index- and Value-Compressed Sparse Column (IVCSC)，它们都利用了稀疏数据中的数据重复性来进行压缩，从而提高存储和计算效率。</li>
<li>results: 对于 simulated 和实际数据，研究人员的测试表明，VCSC 和 IVCSC 可以在压缩形式下读取稀疏数据，而且计算成本几乎没有增加。这两种新的压缩格式可以广泛应用于稀疏数据的存储和计算。<details>
<summary>Abstract</summary>
Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and real data show that VCSC and IVCSC can be read in compressed form with little added computational cost. These two novel compression formats offer a broadly useful solution to encoding and reading redundant sparse data.
</details>
<details>
<summary>摘要</summary>
压缩簇Column (CSC) 和坐标 (COO) 是对叠 sparse 矩阵的广泛储存格式。然而，CSC 和 COO 都是通用的，无法利用资料的其他特性，例如资料的重复性。许多机器学习应用程序中的叠 sparse 数据都具有高度的重复性，例如 genomics，而这些数据通常是 convention 叠 sparse 储存格式中的过大。在这篇文章中，我们提出了两个 CSC 的扩展：（1）值压缩簇Column (VCSC) 和（2） indeks 和值压缩簇Column (IVCSC)。VCSC 利用矩阵中每排的高度重复性，进一步将数据压缩到 3 倍以上 COO 和 2.25 倍以上 CSC，而无需对性能特性造成严重的负面影响。IVCSC 则是将 indeks 范围进行 delta 编码和字节封装，实现了与 COO 比较的 10 倍减少的内存使用量。我们对实验和实际数据进行了对于压缩形式的读取和解压的benchmark，发现 VCSC 和 IVCSC 可以实现轻便的压缩读取。这两种新的压缩格式可以广泛地应用于叠 sparse 数据的储存和处理。
</details></li>
</ul>
<hr>
<h2 id="Mobile-V-MoEs-Scaling-Down-Vision-Transformers-via-Sparse-Mixture-of-Experts"><a href="#Mobile-V-MoEs-Scaling-Down-Vision-Transformers-via-Sparse-Mixture-of-Experts" class="headerlink" title="Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts"></a>Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04354">http://arxiv.org/abs/2309.04354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Daxberger, Floris Weers, Bowen Zhang, Tom Gunter, Ruoming Pang, Marcin Eichner, Michael Emmersberger, Yinfei Yang, Alexander Toshev, Xianzhi Du</li>
<li>for: 本研究使用 sparse Mixture-of-Experts (MoE) 模型，以适应资源有限的视觉应用。</li>
<li>methods: 我们提议一种简化的 Mobile Vision MoE 设计，将整个图像 Routing 到专家中。我们还提出了一种稳定的 MoE 训练方法，使用超类信息导航Router。</li>
<li>results: 我们的 sparse Mobile Vision MoEs (V-MoEs) 可以实现更好的性能与效率平衡，比如 ViT-Tiny 模型，我们的 Mobile V-MoE 在 ImageNet-1k 上比 dense ViTs 高出3.39%。而只有54M FLOPs的推理成本的 ViT 变体，我们的 MoE 还提高了4.66%。<details>
<summary>Abstract</summary>
Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.
</details>
<details>
<summary>摘要</summary>
《稀疏混合专家模型（MoE）》在最近几年内得到了广泛的关注，因为它可以让模型大小与输入token的处理效率分离开来，只有对输入token进行少量的参数活动。这使得稀疏MoE模型在不同领域，如自然语言处理和计算机视觉等领域取得了无 precedent的成功。在这个工作中，我们尝试使用稀疏MoE模型来缩减视Transformers（ViTs），使其更适合具有资源限制的视觉应用。为此，我们提出了简单且移动设备友好的MoE设计，在整个图像被 routed 到专家中，而不是各个小块。我们还提出了稳定的MoE训练方法，使用超类信息来导航路由器。我们实际上示出，我们的稀疏移动视觉MoE（V-MoE）可以在 ImageNet-1k 上比 dense ViTs 更好地协议性和效率之间的平衡。例如，对于 ViT-Tiny 模型，我们的 Mobile V-MoE 在 ImageNet-1k 上比其密集对手提高了3.39%。而对于只有54M FLOPs的推理成本的 ViT 变体，我们的 MoE 提高了4.66%。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Robustification-of-Zero-Shot-Models-With-Foundation-Models"><a href="#Zero-Shot-Robustification-of-Zero-Shot-Models-With-Foundation-Models" class="headerlink" title="Zero-Shot Robustification of Zero-Shot Models With Foundation Models"></a>Zero-Shot Robustification of Zero-Shot Models With Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04344">http://arxiv.org/abs/2309.04344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sprocketlab/roboshot">https://github.com/sprocketlab/roboshot</a></li>
<li>paper_authors: Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala</li>
<li>for: 这篇论文旨在提高预训模型的敏感性和可靠性，并且不需要进一步训练。</li>
<li>methods: 本文使用零条件语言模型（LM）获取任务描述中有用的洞察，并将这些洞察与预训模型的嵌入相结合，以移除害虫和提高有益的 ком成分。这个方法不需要任何超vision。</li>
<li>results: 本文在九个图像和数据分类任务上评估了RoboShot方法，获得了15.98%的平均提升，比许多零条件基eline的表现更好。此外，本文还证明了RoboShot方法可以与多种预训模型和语言模型相容。<details>
<summary>Abstract</summary>
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models.
</details>
<details>
<summary>摘要</summary>
zero-shot推理是一种强大的思想框架，它允许在下游分类任务中使用已经预训练的模型，无需进一步训练。然而，这些模型可能受到遗传的偏见的影响，这可能会影响其性能。传统的解决方案是细化，但这会损害预训练模型的一个优点，即可以直接使用。我们提出了RoboShot，一种可以在完全无需supervision的情况下改进预训练模型的嵌入的方法。我们使用零批语言模型（LM）来获取任务描述中有用的洞察，并将这些洞察embedding在模型中，以移除害虫和增强有用的组分。理论上，我们提供了零批嵌入中偏见的简单和可追踪的模型，并给出了在哪些条件下我们的方法可以提高性能的结果。实验ally，我们在九个图像和NLP分类任务上评估了RoboShot，并显示了15.98%的平均提高。此外，我们还证明了RoboShot可以与多种预训练和语言模型相容。
</details></li>
</ul>
<hr>
<h2 id="Online-Submodular-Maximization-via-Online-Convex-Optimization"><a href="#Online-Submodular-Maximization-via-Online-Convex-Optimization" class="headerlink" title="Online Submodular Maximization via Online Convex Optimization"></a>Online Submodular Maximization via Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04339">http://arxiv.org/abs/2309.04339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tareq Si-Salem, Gözde Özcan, Iasonas Nikolaou, Evimaria Terzi, Stratis Ioannidis</li>
<li>for: 本研究探讨了幂等子模式最大化问题在一般环境中的在线优化。</li>
<li>methods: 本文使用了在线优化和准确轮征来解决这个问题，并证明了这种方法可以实现幂等子模式函数的优化。</li>
<li>results: 本文证明了在线优化可以将幂等子模式函数的优化转化为在线几何优化问题，并且这种方法可以在多种在线学习问题中实现低于线性 regret的性能。<details>
<summary>Abstract</summary>
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
</details>
<details>
<summary>摘要</summary>
我们研究简单升级最大化在通用环境中，具体来说是在在线设置中。我们证明在线优化某种大类卷积函数，即质量梯度 potential functions，可以转化为在线几何优化（OCO）。这是因为这类函数具有折衣函数的下降性，因此OCO策略，加上适当的轮减方案，可以实现在组合设置中的子线性 regret。我们表明我们的减少扩展到许多不同的在线学习问题，包括动态 regret、投筹和乐观学习设置。
</details></li>
</ul>
<hr>
<h2 id="Decreasing-the-Computing-Time-of-Bayesian-Optimization-using-Generalizable-Memory-Pruning"><a href="#Decreasing-the-Computing-Time-of-Bayesian-Optimization-using-Generalizable-Memory-Pruning" class="headerlink" title="Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning"></a>Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04510">http://arxiv.org/abs/2309.04510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander E. Siemenn, Tonio Buonassisi</li>
<li>for: 提高bayesian优化（BO）的计算效率，使其能够处理高维度或大量数据集。</li>
<li>methods: 使用内存剔除和约束优化来减少BO计算时间，并且可以与任何抽象函数和获取函数结合使用。</li>
<li>results: 实现了将BO计算时间从增长到静止的转变，无需牺牲 convergence 性能。此外，通过使用不同的数据集、抽象函数和获取函数，证明了该方法的通用性。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a non-increasing trend without sacrificing convergence performance. Furthermore, we illustrate the generalizability of the approach across two unique data sets, two unique surrogate models, and four unique acquisition functions. All model implementations are run on the MIT Supercloud state-of-the-art computing hardware.
</details>
<details>
<summary>摘要</summary>
bayesian 优化 (BO) 在处理高维度或大数据集时会面临长时间计算问题。这些长时间计算问题是由 Gaussian 过程替代模型的数据量呈指数增长的特性引起的，从而使 BO 在高维度或大数据集上运行成为不可能的。为了解决这个问题，人们已经开发出了一些替代的拟合模型，但这些方法需要修改潜在的拟合函数，这限制了它们的通用性。在这篇论文中，我们提出了一个通用的 BO 包装器，可以与任何拟合模型和争取函数结合使用。我们使用这种内存剔除方法，可以降低 BO 每次实验的墙 clock 计算时间，从 polynomial 增长趋势转变为 sawtooth 趋势，而无需牺牲整体性能。此外，我们还证明了这种方法在两个不同的数据集、两个不同的拟合模型和四个不同的争取函数上的普适性。所有模型实现都运行在 MIT Supercloud  cutting-edge 计算硬件上。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Multi-Domain-Scientific-Papers-by-Ensembling-Multiple-CLS-Tokens"><a href="#Encoding-Multi-Domain-Scientific-Papers-by-Ensembling-Multiple-CLS-Tokens" class="headerlink" title="Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens"></a>Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04333">http://arxiv.org/abs/2309.04333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ronaldseoh/multi2spe">https://github.com/ronaldseoh/multi2spe</a></li>
<li>paper_authors: Ronald Seoh, Haw-Shiuan Chang, Andrew McCallum</li>
<li>for: This paper is written for the task of scientific paper vector encoding, specifically in the context of multi-domain corpora.</li>
<li>methods: The paper proposes a new method called Multi2SPE, which uses multiple CLS tokens to learn diverse ways of aggregating token embeddings and then combines them to create a single vector representation.</li>
<li>results: The paper shows that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.Here’s the Chinese translation of the three pieces of information:</li>
<li>for: 这篇论文是为了科学论文向量编码任务而写的，具体来说是在多个领域的多个科学文档中进行。</li>
<li>methods: 论文提出了一种新方法 called Multi2SPE，它使用多个 CLS 标签来学习不同的方式归约token embedding，然后将它们合并成一个单一的向量表示。</li>
<li>results: 论文表明，Multi2SPE 可以在多个领域的多个科学文档中预测引用关系，错误率下降到最多 25%，同时只需要额外计算一个 BERT 前进 pass。<details>
<summary>Abstract</summary>
Many useful tasks on scientific documents, such as topic classification and citation prediction, involve corpora that span multiple scientific domains. Typically, such tasks are accomplished by representing the text with a vector embedding obtained from a Transformer's single CLS token. In this paper, we argue that using multiple CLS tokens could make a Transformer better specialize to multiple scientific domains. We present Multi2SPE: it encourages each of multiple CLS tokens to learn diverse ways of aggregating token embeddings, then sums them up together to create a single vector representation. We also propose our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector encoders under multi-domain settings. We show that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.
</details>
<details>
<summary>摘要</summary>
许多科学文档处理任务，如主题分类和引用预测，需要跨多个科学领域的文本集合。通常，这些任务通过使用 transformer 的单个 CLS token 获得vector embedding来完成。在这篇论文中，我们 argue 使用多个 CLS token 可以使 transformer 更好地特化到多个科学领域。我们提出 Multi2SPE：它鼓励每个多个 CLS token 学习不同的方式汇聚token embedding，然后将它们综合计算而成单个vector表示。我们还提出了我们的新的多领域测试套件 Multi-SciDocs，用于在多领域settings下测试科学论文vector编码器。我们发现 Multi2SPE 可以在多领域引用预测中减少错误率达到25%，只需要额外计算量与一个BERT前进 pass相当。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-Use-Graphs-When-They-Shouldn’t"><a href="#Graph-Neural-Networks-Use-Graphs-When-They-Shouldn’t" class="headerlink" title="Graph Neural Networks Use Graphs When They Shouldn’t"></a>Graph Neural Networks Use Graphs When They Shouldn’t</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04332">http://arxiv.org/abs/2309.04332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayabechlerspeicher/Graph_Neural_Networks_Overfit_Graphs">https://github.com/mayabechlerspeicher/Graph_Neural_Networks_Overfit_Graphs</a></li>
<li>paper_authors: Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, Amir Globerson</li>
<li>for:  This paper aims to address the issue of graph neural networks (GNNs) overfitting the graph structure, even when it is not informative for the predictive task.</li>
<li>methods:  The paper uses empirical and theoretical analyses to examine the phenomenon of GNNs overfitting graph structures, and proposes a graph-editing method to mitigate this issue.</li>
<li>results:  The proposed method improves the accuracy of GNNs across multiple benchmarks, demonstrating its effectiveness in mitigating the tendency of GNNs to overfit graph structures that should be ignored.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决图神经网络（GNNs）在无用于预测任务的图结构上过拟合的问题。</li>
<li>methods: 这篇论文使用实验和理论分析来研究GNNs在图结构上过拟合的现象，并提出了一种图编辑方法来解决这个问题。</li>
<li>results: 提出的方法在多个benchmark上提高了GNNs的准确率，证明了它的有效性在避免GNNs在无用于预测任务的图结构上过拟合。<details>
<summary>Abstract</summary>
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We then provide a theoretical explanation for this phenomenon, via analyzing the implicit bias of gradient-descent-based learning of GNNs in this setting. Finally, based on our empirical and theoretical findings, we propose a graph-editing method to mitigate the tendency of GNNs to overfit graph-structures that should be ignored. We show that this method indeed improves the accuracy of GNNs across multiple benchmarks.
</details>
<details>
<summary>摘要</summary>
Graf 预测在不同领域中发挥重要作用，如社交网络、分子生物学、医学等。图神经网络（GNNs）已经成为图数据学习的主要方法。实际情况下，图标签问题的实例包括图结构（即邻接矩阵）以及节点特征向量。在某些情况下，图结构无法为预测任务提供有用信息，例如分子性质如分子质量完全取决于节点特征，而不是分子结构。虽然GNNs具有忽略图结构的能力，但是不清楚它们会这样做。在这项工作中，我们发现GNNs实际上往往过拟合图结构，即它们在可以忽略图结构时仍然使用图结构。我们通过不同的图分布对这种现象进行了研究，并发现正则图更加鲁棒。然后，我们提供了一种理论解释，即使用梯度下降学习GNNs在这种设置下的隐式偏见。最后，根据我们的实际和理论发现，我们提出了一种图编辑方法来缓解GNNs过拟合图结构的现象。我们证明这种方法可以提高GNNs的准确性在多个标准测试集上。
</details></li>
</ul>
<hr>
<h2 id="Generating-the-Ground-Truth-Synthetic-Data-for-Label-Noise-Research"><a href="#Generating-the-Ground-Truth-Synthetic-Data-for-Label-Noise-Research" class="headerlink" title="Generating the Ground Truth: Synthetic Data for Label Noise Research"></a>Generating the Ground Truth: Synthetic Data for Label Noise Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04318">http://arxiv.org/abs/2309.04318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjoerd-de-vries/synlabel">https://github.com/sjoerd-de-vries/synlabel</a></li>
<li>paper_authors: Sjoerd de Vries, Dirk Thierens</li>
<li>for: 提高标签频率误差的研究</li>
<li>methods: 提出 SYNLABEL 框架，通过自定义函数和重新混合标签来生成干净的基准数据集，并使用软标签或标签分布来直接注入和量化标签噪声</li>
<li>results: 比较 SYNLABEL 与传统方法的性能表现，并说明 SYNLABEL 可以更好地处理标签噪声和提高模型的泛化能力<details>
<summary>Abstract</summary>
Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions allow for direct injection and quantification of label noise. The generated datasets serve as a clean baseline of adjustable complexity into which different types of noise may be introduced. We illustrate how the framework can be applied, how it enables quantification of label noise and how it improves over existing methodologies.
</details>
<details>
<summary>摘要</summary>
现实世界中的大多数分类任务都受到标签噪声的影响，这种噪声会使得学习的模型的泛化误差增大，同时也使得标签处理方法的评估变得更加复杂，因为无法准确测量噪声的影响。在标签噪声研究中，通常采用 either noisy or incomplex simulated data as a baseline, 并在这些数据中加入已知噪声。在这篇论文中，我们提出了 SYNLABEL 框架，这是一种旨在改进现有方法ologies的框架。它允许创建一个噪声free dataset，该dataset informed by real data，可以通过预先pecifying或学习函数，并将其定义为真实数据中的基准函数。此外，通过在函数空间中采样一些值，评估函数并对其进行汇总，每个数据点可以被赋予一个软标签或标签分布。这些分布可以直接对噪声进行直接注入和量化。生成的数据集可以作为一个清晰的基准，可以对不同类型的噪声进行直接引入。我们示例如如何应用该框架，如何使其能量量化标签噪声，以及如何它超过现有的方法ologies。
</details></li>
</ul>
<hr>
<h2 id="Actor-critic-learning-algorithms-for-mean-field-control-with-moment-neural-networks"><a href="#Actor-critic-learning-algorithms-for-mean-field-control-with-moment-neural-networks" class="headerlink" title="Actor critic learning algorithms for mean-field control with moment neural networks"></a>Actor critic learning algorithms for mean-field control with moment neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04317">http://arxiv.org/abs/2309.04317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huyên Pham, Xavier Warin</li>
<li>for: 解决连续时间奖励学习中的mean-field控制问题</li>
<li>methods: 使用参数化随机政策和值函数 gradient-based representation, 使用 moment neural network 函数在 Wasserstein 空间上学习</li>
<li>results: 提供了一系列数值结果，包括多维设置和非线性二次mean-field控制问题 with 控制幂等Here’s the English version for reference:</li>
<li>for: Solving mean-field control problems within a continuous time reinforcement learning setting</li>
<li>methods: Using a gradient-based representation of the value function with parametrized randomized policies, and moment neural network functions on the Wasserstein space of probability measures</li>
<li>results: Providing a comprehensive set of numerical results, including diverse examples such as multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.<details>
<summary>Abstract</summary>
We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.
</details>
<details>
<summary>摘要</summary>
我们开发了一种新的政策梯度和批评算法，用于在连续时间奖励学习设置中解决平均场控制问题。我们的方法利用梯度基于的值函数表示，使用参数化的随机政策。学习actor（政策）和批评（值函数）都是通过一类 moments neural network 函数在沃尔斯特朗空间上进行，并且关键是直接训练分布Trajectory。我们在这种研究中解决了中场框架特有的计算问题。为证明我们的方法的有效性，我们提供了广泛的数据结果，包括多维设置和控制Volatility的非线性quadratic mean-field控制问题。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Early-Dropout-Prediction-on-Healthy-Ageing-Applications"><a href="#Federated-Learning-for-Early-Dropout-Prediction-on-Healthy-Ageing-Applications" class="headerlink" title="Federated Learning for Early Dropout Prediction on Healthy Ageing Applications"></a>Federated Learning for Early Dropout Prediction on Healthy Ageing Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04311">http://arxiv.org/abs/2309.04311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chrysanthos Nikolaidis, Vasileios Perifanis, Nikolaos Pavlidis, Pavlos S. Efraimidis</li>
<li>for: 这种研究旨在提高老年人的生活质量，并帮助运营商提供早期干预。</li>
<li>methods: 这种研究使用了联合机器学习（FML）方法，以降低隐私问题，并允许分布式训练，不需要传输个人数据。</li>
<li>results: 研究结果显示，通过数据选择和类别不均衡处理技术，FML模型在不同客户端和分布式环境中进行训练，可以实现比传统机器学习模型更高的预测精度。<details>
<summary>Abstract</summary>
The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our approach is evaluated on a real-world dataset with non-independent and identically distributed (non-iid) data among clients, class imbalance and label ambiguity. Our results show that data selection and class imbalance handling techniques significantly improve the predictive accuracy of models trained under FML, demonstrating comparable or superior predictive performance than traditional ML models.
</details>
<details>
<summary>摘要</summary>
提供社会医疗应用程序对老年人群生活质量进行重要的提升，并允许操作人员提供早期干预。准确预测健康年龄应用程序用户退出是直接关系到个人健康状况的。机器学习（ML）算法可以实现高度准确的预测，超出了传统统计方法的cope能力。然而，ML需要大量数据进行训练，这是由于个人标识信息（PII）和法规的分布带来的挑战。在这篇论文中，我们提出了一种联邦机器学习（FML）方法，以减少隐私问题，并允许分布式训练，无需传输个人数据。我们采用了合作训练，考虑了客户端和组织之间的合作，这些模型包括跨设备和跨封包学习场景。我们的方法在实际数据集上进行了评估，该数据集具有非独立和相同分布（non-iid）数据、客户端的类别不均衡和标签抽象。我们的结果表明，数据选择和类别不均衡处理技术可以在FML中提高预测模型的准确性，并达到传统ML模型的相当或更高的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Out-of-Distribution-Electricity-Load-Forecasting-during-COVID-19-A-Continual-Learning-Approach-Leveraging-Human-Mobility"><a href="#Navigating-Out-of-Distribution-Electricity-Load-Forecasting-during-COVID-19-A-Continual-Learning-Approach-Leveraging-Human-Mobility" class="headerlink" title="Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility"></a>Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04296">http://arxiv.org/abs/2309.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arian Prabowo, Kaixuan Chen, Hao Xue, Subbu Sethuvenkatraman, Flora D. Salim</li>
<li>for: 这个论文是为了提高在COVID-19锁定期间的能源负载预测精度。</li>
<li>methods: 这篇论文使用了 continual learning 技术，包括 FSNet 算法，更新模型以应对新数据，并利用了隐私保护的行人计数数据。</li>
<li>results: 研究结果显示，continual learning 在锁定期间能够精确地预测能源负载，并且在对照方法失败时表现出较好的抗变性。<details>
<summary>Abstract</summary>
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the second longest total lockdown duration globally during the pandemic. Results underscore the crucial role of continual learning in accurate energy forecasting, particularly during Out-of-Distribution periods. Secondary data such as mobility and temperature provided ancillary support to the primary forecasting model. More importantly, while traditional methods struggled to adapt during lockdowns, models featuring at least online learning demonstrated resilience, with lockdown periods posing fewer challenges once armed with adaptive learning techniques. This study contributes valuable methodologies and insights to the ongoing effort to improve energy load forecasting during future Out-of-Distribution periods.
</details>
<details>
<summary>摘要</summary>
传统深度学习算法中一个关键假设是数据分布在训练和部署期间保持不变。然而，这个假设在面临外部数据（Out-of-Distribution，OOD）时变得问题。COVID-19封锁是一个典型的OOD场景，在这种情况下，数据分布与训练期间所见的数据分布存在很大差异。这篇论文采用了两重策略：利用 continual learning 技术更新模型，并使用隐私保护的行人计数器收集的人流数据。与在线学习不同， continual learning 可以保持过去的经验，并将新数据纳入模型中。这项研究使用 FSNet 算法对澳大利亚 Ме尔本市的 13 座大楼聚集区的实际数据进行应用。结果表明， continual learning 在异常情况下具有精度的能源预测作用，特别是在 OOD 期间。其他 auxiliary 数据，如流体和温度，也为主要预测模型提供了支持。此外，传统方法在封锁期间难以适应，而模型含有在线学习技术则能够更好地适应，封锁期间的挑战较少。这项研究对未来的异常情况下的能源负荷预测提供了有价值的方法和发现。
</details></li>
</ul>
<hr>
<h2 id="Viewing-the-process-of-generating-counterfactuals-as-a-source-of-knowledge-–-Application-to-the-Naive-Bayes-classifier"><a href="#Viewing-the-process-of-generating-counterfactuals-as-a-source-of-knowledge-–-Application-to-the-Naive-Bayes-classifier" class="headerlink" title="Viewing the process of generating counterfactuals as a source of knowledge – Application to the Naive Bayes classifier"></a>Viewing the process of generating counterfactuals as a source of knowledge – Application to the Naive Bayes classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04284">http://arxiv.org/abs/2309.04284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Lemaire, Nathan Le Boudec, Françoise Fessant, Victor Guyomard</li>
<li>for: 这篇论文是为了探讨机器学习算法做出决策的理解方法。</li>
<li>methods: 这篇论文使用了生成对例的方法，并视这个过程为创造知识的源，可以在不同的场景中 reuse。</li>
<li>results: 这篇论文通过对预测器的示例生成过程进行分析，揭示了预测器的有趣性和可能的应用场景。<details>
<summary>Abstract</summary>
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
</details>
<details>
<summary>摘要</summary>
现在有很多机器学习算法理解决策的方法。其中包括基于生成counterfactual例子的方法。本文提议视这个生成过程为创造一定量的知识，可以被存储并在不同的方式使用。这个过程在加法模型中得到了示例，并在naive Bayes分类器中更加细致地展示了其有趣的性质。Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Spatial-Temporal-Graph-Attention-Fuser-for-Calibration-in-IoT-Air-Pollution-Monitoring-Systems"><a href="#Spatial-Temporal-Graph-Attention-Fuser-for-Calibration-in-IoT-Air-Pollution-Monitoring-Systems" class="headerlink" title="Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems"></a>Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04508">http://arxiv.org/abs/2309.04508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivan Faghih Niresi, Mengjie Zhao, Hugo Bissig, Henri Baumann, Olga Fink</li>
<li>for: 用于提高物联网污染监测平台中仪器的准确性</li>
<li>methods: 使用图 neural network模型，具体是图注意力网络模块，将探测数组数据融合以提高仪器准确性</li>
<li>results: 经过我们的实验，表明我们的方法可以在物联网污染监测平台中显著提高仪器的准确性<details>
<summary>Abstract</summary>
The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT）传感器在空气污染监测中的应用已经增加了，从而实现了低成本传感器的部署。然而，在无控的环境下精确地干预这些传感器仍然是一个挑战。为解决这个问题，我们提出了一种新的方法，利用图形神经网络，具体是图形注意力网络模组，将数据库进行融合，以提高传感器的准确性。经过我们的实验，我们证明了我们的方法在IoT空气污染监测平台中具有明显的改善作用，实现了传感器的准确性提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Zero-Sum-Linear-Quadratic-Games-with-Improved-Sample-Complexity"><a href="#Learning-Zero-Sum-Linear-Quadratic-Games-with-Improved-Sample-Complexity" class="headerlink" title="Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity"></a>Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04272">http://arxiv.org/abs/2309.04272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wujiduan/zero-sum-lq-games">https://github.com/wujiduan/zero-sum-lq-games</a></li>
<li>paper_authors: Jiduan Wu, Anas Barakat, Ilyas Fatkhullin, Niao He</li>
<li>for: 这篇论文的目的是解决零 SUM 线性quadratic（LQ）游戏中的控制问题，它可以作为风险敏感或Robust控制的动态游戏形式，或者作为多智能机器人学习中的两个竞争者问题的标准设定。</li>
<li>methods: 这篇论文使用的方法是natural policy gradient方法，这种方法具有重要的隐式正则化性质，这使得控制器在学习过程中保持了稳定性和安全性。在没有模型参数知识的情况下，作者提出了第一个 polynomial sample complexity 算法，可以在 $\epsilon$- neighborhood 内达到 Nash 平衡，同时保持隐式正则化性质。</li>
<li>results: 作者提出了一种更加简单的 Zeroth-Order（ZO）算法，可以在同样的假设下提高样本复杂度的下降。主要结果表明，使用单点 ZO 统计器时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-3})$；使用两点 ZO 统计器时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-2})$。作者的关键提高来自于更加 Sample-efficient 的嵌套算法设计和 ZO 自然偏导量估计误差的细化控制。<details>
<summary>Abstract</summary>
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit regularization property. In this work, we propose a simpler nested Zeroth-Order (ZO) algorithm improving sample complexity by several orders of magnitude. Our main result guarantees a $\widetilde{\mathcal{O}(\epsilon^{-3})$ sample complexity under the same assumptions using a single-point ZO estimator. Furthermore, when the estimator is replaced by a two-point estimator, our method enjoys a better $\widetilde{\mathcal{O}(\epsilon^{-2})$ sample complexity. Our key improvements rely on a more sample-efficient nested algorithm design and finer control of the ZO natural gradient estimation error.
</details>
<details>
<summary>摘要</summary>
zero-sum linear quadratic（LQ）游戏是优化控制的基本问题，可以用作风险敏感或Robust控制的动态游戏形式，或者作为多个代理人游戏学习中的两个竞争者的标准设定。与单个代理人的线性quadratic regulator问题不同，zero-sum LQ游戏需要解决一个复杂非几何-非凹的最小值问题， objective function 缺乏凝结性。最近，张等人发现了自然策略强度法的隐式正则化性质，这是关键 для安全控制系统，因为它保持了控制器的稳定性在学习过程中。此外，在没有模型参数知识的场景下，张等人提出了首个 polynomial sample complexity 算法，可以在 достичь $\epsilon$-邻域的 Nash 平衡点上保持愉悦的隐式正则化性质。在这个工作中，我们提出了一种更简单的嵌套Zeroth-Order（ZO）算法，可以提高样本复杂度的数量级。我们的主要结果表明，使用单点ZO estimator时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-3})$。当使用两点 estimator时，我们的方法可以达到更好的 $\widetilde{\mathcal{O}(\epsilon^{-2})$ 样本复杂度。我们的关键改进来自于更有效的嵌套算法设计和ZO自然幂导 estimation error的更精细控制。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Rate-of-Kernel-Regression-in-Large-Dimensions"><a href="#Optimal-Rate-of-Kernel-Regression-in-Large-Dimensions" class="headerlink" title="Optimal Rate of Kernel Regression in Large Dimensions"></a>Optimal Rate of Kernel Regression in Large Dimensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04268">http://arxiv.org/abs/2309.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihao Lu, Haobo Zhang, Yicheng Li, Manyun Xu, Qian Lin</li>
<li>for: 本研究的目的是分析适用于大维度数据的内核回归算法（where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma &gt;0$）。</li>
<li>methods: 本研究使用了一种总体工具来Characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively。</li>
<li>results: 当目标函数 falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$时，我们使用了这种工具来显示 That the minimax rate of the excess risk of kernel regression is $n^{-1&#x2F;2}$ when $n\asymp d^{\gamma}$ for $\gamma &#x3D;2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma&gt;0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior} and the {\it periodic plateau behavior}. As an application, For the neural tangent kernel (NTK), we also provide a similar explicit description of the curve of optimal rate. As a direct corollary, we know these claims hold for wide neural networks as well.<details>
<summary>Abstract</summary>
We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma>0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior} and the {\it periodic plateau behavior}. As an application, For the neural tangent kernel (NTK), we also provide a similar explicit description of the curve of optimal rate. As a direct corollary, we know these claims hold for wide neural networks as well.
</details>
<details>
<summary>摘要</summary>
我们进行了一项研究，探讨了隐藏层扩散（Kernel Regression）在高维数据上的性能。我们首先构建了一种通用的工具来Characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively。当目标函数属于隐藏层扩散相关的($\mathbb{S}^{d}$上的)内积模型时，我们利用这种工具来证明隐藏层扩散的最佳误差率为$n^{-1/2}$，当$n\asymp d^{\gamma}$，其中$\gamma = 2, 4, 6, 8, \cdots$。然后，我们进一步确定了隐藏层扩散的最佳误差率，并发现了一些新现象，包括{\it 多重极值行为}和{\it 周期平台行为}。在应用中，我们也提供了类似的Explicit description of the curve of optimal rate for the neural tangent kernel (NTK)。这意味着这些结论也适用于宽频率神经网络。
</details></li>
</ul>
<hr>
<h2 id="Generating-drawdown-realistic-financial-price-paths-using-path-signatures"><a href="#Generating-drawdown-realistic-financial-price-paths-using-path-signatures" class="headerlink" title="Generating drawdown-realistic financial price paths using path signatures"></a>Generating drawdown-realistic financial price paths using path signatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04507">http://arxiv.org/abs/2309.04507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiel Lemahieu, Kris Boudt, Maarten Wyns</li>
<li>for: 这个论文的目的是提出一种基于机器学习的新方法，用于模拟金融价格数据序列中的下跌。</li>
<li>methods: 这种方法使用了变分自动编码器生成模型，并使用下跌重建loss函数。为了解决数值复杂性和非 diferenciabilidad问题， authors使用了下跌函数的 linear 函数 approximation，并证明了这种近似的正则性和一致性。</li>
<li>results: 通过使用这种方法， authors 得到了一系列具有实际下跌特征的金融价格数据序列。他们还通过对混合股票、债券、地产和商品资产的数据进行数值实验，并证明了这种方法的有效性。<details>
<summary>Abstract</summary>
A novel generative machine learning approach for the simulation of sequences of financial price data with drawdowns quantifiably close to empirical data is introduced. Applications such as pricing drawdown insurance options or developing portfolio drawdown control strategies call for a host of drawdown-realistic paths. Historical scenarios may be insufficient to effectively train and backtest the strategy, while standard parametric Monte Carlo does not adequately preserve drawdowns. We advocate a non-parametric Monte Carlo approach combining a variational autoencoder generative model with a drawdown reconstruction loss function. To overcome issues of numerical complexity and non-differentiability, we approximate drawdown as a linear function of the moments of the path, known in the literature as path signatures. We prove the required regularity of drawdown function and consistency of the approximation. Furthermore, we obtain close numerical approximations using linear regression for fractional Brownian and empirical data. We argue that linear combinations of the moments of a path yield a mathematically non-trivial smoothing of the drawdown function, which gives one leeway to simulate drawdown-realistic price paths by including drawdown evaluation metrics in the learning objective. We conclude with numerical experiments on mixed equity, bond, real estate and commodity portfolios and obtain a host of drawdown-realistic paths.
</details>
<details>
<summary>摘要</summary>
新的生成机器学习方法 для模拟金融价格数据序列，包括Drawdown，靠近实际数据的量化是介绍的。应用如评估Drawdown保险选项或开发 portefolio Drawdown控制策略，需要一系列Drawdown具有实际性的路径。历史场景可能不够充分用于训练和测试策略，而标准 Parametric Monte Carlo 方法不能够准确保持Drawdown。我们建议一种非 Parametric Monte Carlo 方法，结合变分自动编码生成模型和 Drawdown 重建损失函数。为了解决计算复杂性和不 diferenciabilidad 问题，我们 approximates Drawdown 为路径特征的线性函数，知道在 литературе como path signatures。我们证明 Drawdown 函数的必要的准确性和拟合的一致性。此外，我们通过线性回归获得了 fractional Brownian 和实际数据的准确近似值。我们 argue that linear combinations of path moments yield a mathematically non-trivial smoothing of the drawdown function, which allows one to simulate drawdown-realistic price paths by including drawdown evaluation metrics in the learning objective. We conclude with numerical experiments on mixed equity, bond, real estate and commodity portfolios and obtain a host of drawdown-realistic paths.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Distributed-Kernel-Ridge-Regression-A-Feasible-Distributed-Learning-Scheme-for-Data-Silos"><a href="#Adaptive-Distributed-Kernel-Ridge-Regression-A-Feasible-Distributed-Learning-Scheme-for-Data-Silos" class="headerlink" title="Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos"></a>Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04236">http://arxiv.org/abs/2309.04236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wang, Xiaotong Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 解决数据隔壳问题，帮助不同组织共同学习同一类数据，提高决策效果。</li>
<li>methods: 基于分治的分布式学习，实现参数自主选择、隐私保障和合作必要性。</li>
<li>results:  theoretically和实验两种方式证明AdaDKRR的可行性和有效性，并在几个应用领域中表现出优于其他分布式学习方法。<details>
<summary>Abstract</summary>
Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other distributed learning scheme can essentially beat AdaDKRR under the same conditions. Numerically, we test AdaDKRR on both toy simulations and two real-world applications to show that AdaDKRR is superior to other existing distributed learning schemes. All these results show that AdaDKRR is a feasible scheme to defend against data silos, which are highly desired in numerous application regions such as intelligent decision-making, pricing forecasting, and performance prediction for products.
</details>
<details>
<summary>摘要</summary>
“数据堡垒”，主要由隐私和兼容性所引起，对不同组织之间的合作带来了 significiant 限制。分布式学习基于分治提供了一个有 promise 的解决方案，但它面临着一些挑战，包括自主、隐私保证和合作的必要性。本文关注于基于自适应分布式内核ridge regression（AdaDKRR）的开发，包括自主在参数选择、隐私在交换非敏感信息以及合作的必要性。我们提供了坚实的理论验证和实验来证明 AdaDKRR 的可行性和有效性。理论上，我们证明在某些轻量级条件下，AdaDKRR 与在整个数据上运行最佳学习算法相当，证明了合作的必要性，并表明其他分布式学习方案无法 Essentially 超越 AdaDKRR 在同样的条件下。数字上，我们测试了 AdaDKRR 在实验和两个真实应用中，并证明它在其他已有分布式学习方案的同时显著 superior。这些结果表明 AdaDKRR 是一种有效的数据堡垒防御方案，可以在许多应用领域中实现更好的决策、估价和产品性能预测。
</details></li>
</ul>
<hr>
<h2 id="Offline-Recommender-System-Evaluation-under-Unobserved-Confounding"><a href="#Offline-Recommender-System-Evaluation-under-Unobserved-Confounding" class="headerlink" title="Offline Recommender System Evaluation under Unobserved Confounding"></a>Offline Recommender System Evaluation under Unobserved Confounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04222">http://arxiv.org/abs/2309.04222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olivierjeunen/confounding-consequences-2023">https://github.com/olivierjeunen/confounding-consequences-2023</a></li>
<li>paper_authors: Olivier Jeunen, Ben London</li>
<li>for: This paper focuses on the problem of unobserved confounders in off-policy estimation (OPE) methods for recommender systems.</li>
<li>methods: The paper uses policy-based estimators, which learn logging propensities from logged data, and demonstrates the statistical bias that arises due to confounding.</li>
<li>results: The paper shows that existing diagnostics are unable to uncover the bias caused by confounding, and that naive propensity estimation can lead to severely biased metric estimates.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了Off-Policy Estimation（OPE）方法在推荐系统中的问题，即未观察的偏见问题。</li>
<li>methods: 论文使用了政策基于的估计器，从日志数据中学习 logging propensities，并证明了因为偏见而导致的统计偏误。</li>
<li>results: 论文显示了现有的 диагностиcs无法检测到偏见导致的偏误，并且简单的 propensity estimation 可能会导致严重的偏误度量估计。<details>
<summary>Abstract</summary>
Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.   This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to confounding, and show how existing diagnostics are unable to uncover such cases. Because the bias depends directly on the true and unobserved logging propensities, it is non-identifiable. As the unconfoundedness assumption is famously untestable, this becomes especially problematic. This paper emphasises this common, yet often overlooked issue. Through synthetic data, we empirically show how na\"ive propensity estimation under confounding can lead to severely biased metric estimates that are allowed to fly under the radar. We aim to cultivate an awareness among researchers and practitioners of this important problem, and touch upon potential research directions towards mitigating its effects.
</details>
<details>
<summary>摘要</summary>
偏离政策估计（OPE）技术可以从记录数据中学习和评估决策策略。这使得它们在线上评估推荐系统中非常吸引人，而一些最近的研究也已经成功地采用了OPE技术。一个重要的假设是不存在隐藏的隐变量：在数据采集时影响行为和奖励的随机变量。因为数据采集策略通常是实际控制在手中，因此这个假设通常被遗弃，而其违反也 rarely 被文献中讨论。  这项工作想要强调在隐藏隐变量的存在下进行偏离政策估计时出现的问题，特别是在推荐用例中。我们关注policy-based estimators，其中logging propensities从记录数据中学习。我们描述了由隐藏隐变量引起的统计偏差，并显示了现有的 диагностиcs无法检测这些情况。因为偏差直接关于真实的和未观察 logging propensities，因此是不可识别的。正如无法测试的无关联假设，这就变得特别问题。这篇论文想要吸引研究人员和实践者对这种常见 yet 受过гля的问题进行注意。我们通过 sintetic data  empirically 示出了不经过propensity estimation 的偏离metric estimates可以导致严重偏差。我们的目标是在研究人员和实践者中培养对这个重要问题的意识，并触及可能的研究方向以解决其影响。
</details></li>
</ul>
<hr>
<h2 id="Concomitant-Group-Testing"><a href="#Concomitant-Group-Testing" class="headerlink" title="Concomitant Group Testing"></a>Concomitant Group Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04221">http://arxiv.org/abs/2309.04221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thach V. Bui, Jonathan Scarlett</li>
<li>For: The paper addresses the Concomitant Group Testing (ConcGT) problem, which involves identifying multiple disjoint semi-defective sets in a population using as few tests as possible.* Methods: The paper derives a variety of algorithms for ConcGT, including deterministic and randomized algorithms with different levels of adaptivity.* Results: The paper shows that the proposed algorithms are order-optimal in broad scaling regimes and improve significantly over baseline results.<details>
<summary>Abstract</summary>
In this paper, we introduce a variation of the group testing problem capturing the idea that a positive test requires a combination of multiple ``types'' of item. Specifically, we assume that there are multiple disjoint \emph{semi-defective sets}, and a test is positive if and only if it contains at least one item from each of these sets. The goal is to reliably identify all of the semi-defective sets using as few tests as possible, and we refer to this problem as \textit{Concomitant Group Testing} (ConcGT). We derive a variety of algorithms for this task, focusing primarily on the case that there are two semi-defective sets. Our algorithms are distinguished by (i) whether they are deterministic (zero-error) or randomized (small-error), and (ii) whether they are non-adaptive, fully adaptive, or have limited adaptivity (e.g., 2 or 3 stages). Both our deterministic adaptive algorithm and our randomized algorithms (non-adaptive or limited adaptivity) are order-optimal in broad scaling regimes of interest, and improve significantly over baseline results that are based on solving a more general problem as an intermediate step (e.g., hypergraph learning).
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种变种的群测试问题，把具有多个“类型”的项目组合的想法带入。特别是，我们假设有多个独立的半异常集，而测试是否阳性，则需要至少有一个项目来自每个半异常集。我们的目标是通过最少的测试来可靠地检测所有半异常集，我们称之为“并行群测试”（ConcGT）。我们 derivated 多种算法来解决这个问题，主要是在两个半异常集时。我们的算法可以分为（i）确定性（零错）或随机化（小错），以及（ii）非适应、完全适应或有限适应（例如，2或3个阶段）。我们的确定性适应算法和我们的随机算法（非适应或有限适应）在广泛的扩展 режиmen of interest 中都是最佳的，并在基eline结果（例如，超граápher学习）上进行了显著改进。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-via-Locally-guided-Sequential-Algorithmic-Recourse"><a href="#Counterfactual-Explanations-via-Locally-guided-Sequential-Algorithmic-Recourse" class="headerlink" title="Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse"></a>Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04211">http://arxiv.org/abs/2309.04211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward A. Small, Jeffrey N. Clark, Christopher J. McWilliams, Kacper Sokol, Jeffrey Chan, Flora D. Salim, Raul Santos-Rodriguez</li>
<li>for: 这篇论文旨在提供一种可操作的算法抗议方法，使人工智能系统更加可解释。</li>
<li>methods: 这篇论文使用了算法驱动的方法，包括梯度驱动方法，但这些方法无法保证抗议的可行性和对抗攻击。</li>
<li>results: 这篇论文提出了一种名为LocalFACE的模型无关技术，可以生成可操作的对抗性解释，并保护用户隐私和模型安全。<details>
<summary>Abstract</summary>
Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention."中文翻译：counterfactuals通过算法救济实现了人工智能系统的解释。从概念上说，对于被分类为y（事实）的个人，我们寻找一些行动，使其预测变为欲要的类y'（ counterfactual）。这个过程提供了一种简单易 interpretable的算法救济，其中（1）易于自定义和解释，（2）与每个个人的目标直接对应。然而，“好”的counterfactual的性质仍然是一个开放的挑战，尚未能够有效地找到counterfactual和其相应的救济。一些策略使用梯度驱动方法，但这些方法不能保证救济的可行性，并且容易受到伪装的扰动。这可能导致不公正和稳定性问题。其他方法是数据驱动的，它主要解决了可行性问题，但是在付出了隐私、安全和机密的代价。在这里，我们介绍LocalFACE，一种模型无关的技术，可以在每个步骤中使用当地获得的信息组合可行的counterfactual解释。我们的解释保持用户隐私，仅使用特定于构建行动的数据来构建可行的算法救济，并且保护模型，仅在必要的区域提供透明度。
</details></li>
</ul>
<hr>
<h2 id="COVID-19-Detection-System-A-Comparative-Analysis-of-System-Performance-Based-on-Acoustic-Features-of-Cough-Audio-Signals"><a href="#COVID-19-Detection-System-A-Comparative-Analysis-of-System-Performance-Based-on-Acoustic-Features-of-Cough-Audio-Signals" class="headerlink" title="COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals"></a>COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04505">http://arxiv.org/abs/2309.04505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asmaa Shati, Ghulam Mubashar Hassan, Amitava Datta<br>for: 这项研究旨在提出一种高效的COVID-19检测系统，通过分析咳声特征来识别患COVID-19人群。methods: 该研究使用了三种特征提取方法：Mel Frequency Cepstral Coefficients (MFCC)、Chroma和Spectral Contrast特征，并应用了支持向量机(SVM)和多层感知网络(MLP)两种机器学习算法。results: 研究结果表明，使用这三种特征提取方法和SVM和MLP两种机器学习算法可以提高COVID-19检测的准确率，并达到了现有技术的国际先进水平。<details>
<summary>Abstract</summary>
A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a practical solution and demonstrates higher state-of-the-art classification performance on COUGHVID and Virufy datasets for COVID-19 detection.
</details>
<details>
<summary>摘要</summary>
各种呼吸疾病，如感冒和流感、asma和 COVID-19，对全球人群的日常生活产生了深远的影响。在医疗实践中，呼吸音被广泛使用，以诊断各种呼吸疾病和肺脏疾患。传统的诊断方法需要专业知识，这可能是成本高昂的和人工智能依赖的。近些年，喷气音记录被用来自动诊断呼吸疾病。本研究旨在检验不同的音频特征，以提高机器学习（ML）模型在检测 COVID-19 的能力。本研究 investigate了 MFCC、Chroma 和 Spectral Contrast 等三种特征提取技术，并将其应用于 SVM 和 MLP 两种 ML 算法，因此提出了一种高效的 COVID-19 检测系统。该系统提供了实用的解决方案，并在 COUGHVID 和 Virufy 数据集上达到了高于当前领先水平的分类性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Mitigating-Architecture-Overfitting-in-Dataset-Distillation"><a href="#Towards-Mitigating-Architecture-Overfitting-in-Dataset-Distillation" class="headerlink" title="Towards Mitigating Architecture Overfitting in Dataset Distillation"></a>Towards Mitigating Architecture Overfitting in Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04195">http://arxiv.org/abs/2309.04195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyang Zhong, Chen Liu</li>
<li>for: 提高 neural network 使用有限训练数据时的性能。</li>
<li>methods: 提出了一系列 Architecture 设计和训练方案，可以在不同的网络架构下提高分布式数据 Synthesized 的性能。</li>
<li>results: 经过广泛的实验，我们的方法在不同的情况下（含不同的压缩数据大小）都能够与现有方法相比或者提高性能。<details>
<summary>Abstract</summary>
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Compositional-Learning-of-Visually-Grounded-Concepts-Using-Reinforcement"><a href="#Compositional-Learning-of-Visually-Grounded-Concepts-Using-Reinforcement" class="headerlink" title="Compositional Learning of Visually-Grounded Concepts Using Reinforcement"></a>Compositional Learning of Visually-Grounded Concepts Using Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04504">http://arxiv.org/abs/2309.04504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haidiazaman/rl-concept-learning-project">https://github.com/haidiazaman/rl-concept-learning-project</a></li>
<li>paper_authors: Zijun Lin, Haidi Azaman, M Ganesh Kumar, Cheston Tan</li>
<li>for:  investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task</li>
<li>methods: 使用深度强化学习代理人在多集数回合中训练，并研究代理人是否可以通过冻结文本编码器（例如CLIP、BERT）来学习单词组合，以及在不同颜色和形状概念上进行预训练的影响</li>
<li>results:  agents pretrained on concept and compositional learning achieve significantly higher reward when evaluated zero-shot on novel color-shape1-shape2 visual object combinations, and can solve unseen combinations of instructions with fewer episodes.<details>
<summary>Abstract</summary>
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show that agents pretrained on concept and compositional learning achieve significantly higher reward when evaluated zero-shot on novel color-shape1-shape2 visual object combinations. Overall, our results highlight the foundations needed to increase an agent's proficiency in composing word groups through reinforcement learning and its ability for zero-shot generalization to new combinations.
</details>
<details>
<summary>摘要</summary>
深度强化学学习代理需要在数百万集 episodes 中训练，以解决基于指令的导航任务。然而，其能够总结 novel 的指令组合仍然不清楚。然而，孩子们可以将语言基于的指令分解成组合，并导航到指定的 объек。因此，我们创建了三个3D环境，以研究深度RL代理如何学习和组合颜色形状基本指令，以解决 novel 的组合。首先，我们 investigate  whether agents can perform compositional learning，并可以使用冻结的文本编码器（例如CLIP、BERT）来学习单词组合。然后，我们示出了在 agents 在受训练集中减少了 20 倍的集数可以解决未看过的指令组合。最后，我们表明在概念学习和组合学习中训练的代理可以在零时执行中达到更高的奖励。总的来说，我们的结果阐明了如何通过强化学习提高代理的单词组合能力，以及这种能力在零时执行中的普适性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Prototype-Patient-Representations-with-Feature-Missing-Aware-Calibration-to-Mitigate-EHR-Data-Sparsity"><a href="#Leveraging-Prototype-Patient-Representations-with-Feature-Missing-Aware-Calibration-to-Mitigate-EHR-Data-Sparsity" class="headerlink" title="Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity"></a>Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04160">http://arxiv.org/abs/2309.04160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Zhu, Zixiang Wang, Long He, Shiyun Xie, Zixi Chen, Jingkun An, Liantao Ma, Chengwei Pan</li>
<li>for: 这个研究的目的是对电子健康纪录（EHR）数据进行预测模型建立，并解决EHR数据的稀畴特性带来的挑战。</li>
<li>methods: 本研究使用 indirect imputation，通过使用相似 patient 的原型表示来获得更为紧密的嵌入。此外，我们还提出了一个新的患者相似度量表，考虑到缺失特征的状态，以确保评估不仅基于可能不准确的填充值。</li>
<li>results: 我们的方法在 MIMIC-III 和 MIMIC-IV 数据集上预测医院死亡结果 task 上达到了 statistically significant 的改善，较前一代的 EHR- focused 模型。代码在 \url{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SparseEHR%7D">https://anonymous.4open.science/r/SparseEHR}</a> 上公开，以便重现。<details>
<summary>Abstract</summary>
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measuring similar patients, our approach designs a feature confidence learner module. This module is sensitive to the missing feature status, enabling the model to better judge the reliability of each feature. Moreover, we propose a novel patient similarity metric that takes feature confidence into account, ensuring that evaluations are not based merely on potentially inaccurate imputed values. Consequently, our work captures dense prototype patient representations with feature-missing-aware calibration process. Comprehensive experiments demonstrate that designed model surpasses established EHR-focused models with a statistically significant improvement on MIMIC-III and MIMIC-IV datasets in-hospital mortality outcome prediction task. The code is publicly available at \url{https://anonymous.4open.science/r/SparseEHR} to assure the reproducibility.
</details>
<details>
<summary>摘要</summary>
电子健康记录（EHR）数据经常具有稀畴特征，这会对预测模型造成挑战。目前的直接填充方法，如矩阵填充方法，基于参照相似的行或列来完成 Raw 缺失数据，而不能区分实际值和拟合值。因此，模型可能会意外地包含不相关或误导的信息，从而降低下游性能。而一些方法尝试通过重新调整或增强 EHR 嵌入来解决这些问题，但它们经常偏好拟合特征。这种偏好可能会引入偏见或错误到模型中。为了解决这些问题，我们的工作使用间接填充，利用相似病人的原型表示来获得密集的嵌入。Recognizing the limitation that missing features are typically treated the same as present ones when measuring similar patients, our approach designs a feature confidence learner module. This module is sensitive to the missing feature status, enabling the model to better judge the reliability of each feature. Moreover, we propose a novel patient similarity metric that takes feature confidence into account, ensuring that evaluations are not based merely on potentially inaccurate imputed values. Consequently, our work captures dense prototype patient representations with feature-missing-aware calibration process. Comprehensive experiments demonstrate that our designed model surpasses established EHR-focused models with a statistically significant improvement on MIMIC-III and MIMIC-IV datasets in-hospital mortality outcome prediction task. 我们的代码公开可用于 \url{https://anonymous.4open.science/r/SparseEHR}，以确保可重复性。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-on-hybrid-classical-quantum-Deep-Learning-models-for-Histopathological-Cancer-Detection"><a href="#Adversarial-attacks-on-hybrid-classical-quantum-Deep-Learning-models-for-Histopathological-Cancer-Detection" class="headerlink" title="Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection"></a>Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06377">http://arxiv.org/abs/2309.06377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biswaraj Baral, Reek Majumdar, Bhavika Bhalgamiya, Taposh Dutta Roy</li>
<li>for:  histopathological cancer detection</li>
<li>methods:  hybrid classical-quantum Deep Learning models ( incluiding transfer learning strategy and variational quantum circuits)</li>
<li>results:  better accuracy than classical image classification models under adversarial attacks<details>
<summary>Abstract</summary>
We present an effective application of quantum machine learning in histopathological cancer detection. The study here emphasizes two primary applications of hybrid classical-quantum Deep Learning models. The first application is to build a classification model for histopathological cancer detection using the quantum transfer learning strategy. The second application is to test the performance of this model for various adversarial attacks. Rather than using a single transfer learning model, the hybrid classical-quantum models are tested using multiple transfer learning models, especially ResNet18, VGG-16, Inception-v3, and AlexNet as feature extractors and integrate it with several quantum circuit-based variational quantum circuits (VQC) with high expressibility. As a result, we provide a comparative analysis of classical models and hybrid classical-quantum transfer learning models for histopathological cancer detection under several adversarial attacks. We compared the performance accuracy of the classical model with the hybrid classical-quantum model using pennylane default quantum simulator. We also observed that for histopathological cancer detection under several adversarial attacks, Hybrid Classical-Quantum (HCQ) models provided better accuracy than classical image classification models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种有效的量子机器学习应用于 histopathological cancer detection。这种研究主要强调两个主要应用：首先，建立一个基于混合古典-量子深度学习模型的分类模型，用于 histopathological cancer detection。其次，测试这个模型对各种敌意攻击的性能。而不是使用单一的传输学习模型，我们测试了多个传输学习模型，包括 ResNet18、VGG-16、Inception-v3 和 AlexNet 作为特征提取器，并将它们与多种量子电路基本变量量子圈（VQC）结合。因此，我们提供了对 классические模型和混合古典-量子传输学习模型的比较分析，并对 histopathological cancer detection 下多种敌意攻击的性能进行了比较。我们使用 pennylane 默认量子 simulate 器进行了比较。我们发现，对 histopathological cancer detection 下多种敌意攻击，混合古典-量子（HCQ）模型提供了更高的准确率，而非 классиical image classification 模型。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Method-for-Sensitivity-Enhancement-of-Deuterium-Metabolic-Imaging-DMI"><a href="#A-Deep-Learning-Method-for-Sensitivity-Enhancement-of-Deuterium-Metabolic-Imaging-DMI" class="headerlink" title="A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI)"></a>A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04100">http://arxiv.org/abs/2309.04100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Dong, Henk M. De Feyter, Monique A. Thomas, Robin A. de Graaf, James S. Duncan</li>
<li>for: 提高Deuterium Metabolic Imaging（DMI）的敏感度，从而提高肉眼图像的分辨率和扫描时间。</li>
<li>methods: 使用 convolutional neural network（CNN）来估计2H-标记物质浓度从低SNR和扭曲的DMI FIDs。通过MRI基于的边缘保持正则化来进一步提高估计精度。</li>
<li>results: PRECISE-DMI可以视觉改善低SNR数据中的代谢图像，并提供更高的精度 than标准的福rier重建。对于在鼠脑肿瘤模型中获得的数据进行处理，可以获得更高的空间分辨率（从&gt;8到2 $\mu$L）或更短的扫描时间（从32到4分），并且可以更正确地测量2H-标记的酮氧酸和γ-酮氧酸+γ-酮氧酸含量。但是，对于激活过度的边缘保持正则化，可能会产生结果的不准确性。<details>
<summary>Abstract</summary>
Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.   Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.   Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and quantitatively provided higher precision than the standard Fourier reconstruction. Processing of DMI data acquired in rat brain tumor models resulted in more precise determination of 2H-labeled lactate and glutamate + glutamine levels, at increased spatial resolution (from >8 to 2 $\mu$L) or shortened scan time (from 32 to 4 min) compared to standard acquisitions. However, rigorous SD-bias analyses showed that overuse of the edge-preserving regularization can compromise the accuracy of the results.   Conclusion: PRECISE-DMI allows a flexible trade-off between enhancing the sensitivity of DMI and minimizing the inaccuracies. With typical settings, the DMI sensitivity can be improved by 3-fold while retaining the capability to detect local signal variations.
</details>
<details>
<summary>摘要</summary>
目的：大多数MRSI技术的空间分解能力和最小扫描时间受到可 achievable SNR 的限制。这项工作提出了一种基于深度学习的敏感度提高方法 дляMRSI。方法：我们设计了一个卷积神经网络（CNN）来估算2H-标记的代谢物浓度从低SNR和扭曲的DMI FID中。CNN 被训练使用生成的数据，表征了通常在生物体内遇到的SNR水平范围。为了进一步提高估算精度，我们对每个DMI数据集使用MRI基于Edge-preserving 的正则化进行细调。我们称这种处理方法为PRECISE-DMI。结果：PRECISE-DMI可以视觉提高低SNR数据中的代谢图，并量测更高精度 than标准傅立叶重建。对于在鼠 brain tumor 模型中获得的数据进行处理，可以获得更高的空间分解能力（从>8到2 $\mu$L）或更短的扫描时间（从32到4分），相比标准采集。然而，我们通过严格的SD-偏移分析发现，过度使用Edge-preserving 正则化可能会伤害结果的准确性。结论：PRECISE-DMI允许在提高MRSI敏感度的同时，也可以保持检测地方信号变化的能力。通常情况下，可以通过调整参数来实现3倍的MRSI敏感度提高，而无需妥协准确性。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Recommender-Ecosystems-Research-Challenges-at-the-Intersection-of-Mechanism-Design-Reinforcement-Learning-and-Generative-Models"><a href="#Modeling-Recommender-Ecosystems-Research-Challenges-at-the-Intersection-of-Mechanism-Design-Reinforcement-Learning-and-Generative-Models" class="headerlink" title="Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models"></a>Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06375">http://arxiv.org/abs/2309.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Craig Boutilier, Martin Mladenov, Guy Tennenholtz</li>
<li>for: 提高推荐系统的长期价值和生态系统健康</li>
<li>methods: 使用奖励学习优化推荐策略，使用社会选择方法讨论不同actor的利益，减少信息不均衡，更好地模型用户和项目提供者的行为</li>
<li>results: 提出一个概念框架，并提出许多跨学科研究挑战<details>
<summary>Abstract</summary>
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymmetry, while accounting for incentives and strategic behavior, using the tools of mechanism design; better modeling of both user and item-provider behaviors by incorporating notions from behavioral economics and psychology; and exploiting recent advances in generative and foundation models to make these mechanisms interpretable and actionable. We propose a conceptual framework that encompasses these elements, and articulate a number of research challenges that emerge at the intersection of these different disciplines.
</details>
<details>
<summary>摘要</summary>
现代推荐系统位于复杂的生态系统中，包括用户行为、内容提供商、广告主和其他激发actor的互动。然而，大多数推荐研究和实践推荐系统的焦点都是在本地、短期优化推荐给单个用户。这会导致推荐系统在长期内的价值创造和生态系统健康受到很大的损害。我们认为，明确模型所有actor的奖励和行为，以及这些actor之间因推荐策略而产生的互动，是为了最大化推荐系统为actor带来的价值和改善整个生态系统的健康。这需要：使用增强学习来优化长期目标; 在不同actor之间进行负担和利益冲突的决策; 减少信息不均衡，并考虑激发和战略行为的工具 Mechanism Design; 更好地模型用户和项目提供者的行为，通过包括行为经济学和心理学的想法; 并利用最近的生成和基础模型来让这些机制可读性和操作性。我们提出一个涵盖这些元素的概念框架，并详细描述这些不同领域之间的研究挑战。
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-Co-Design-of-Robotic-Agents-Using-Multi-fidelity-Training-on-Universal-Policy-Network"><a href="#Sample-Efficient-Co-Design-of-Robotic-Agents-Using-Multi-fidelity-Training-on-Universal-Policy-Network" class="headerlink" title="Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network"></a>Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04085">http://arxiv.org/abs/2309.04085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kishan R. Nagiredla, Buddhika L. Semage, Thommen G. Karimpanal, Arun Kumar A. V, Santu Rana</li>
<li>for: 本研究旨在提高协同设计中控制优化和物理设计之间的同步优化效率，以及避免数据密集的 reinforcement learning 过程中的样本不足问题。</li>
<li>methods: 本研究提出了一种基于 Hyperband 的多信度探索策略，通过将控制器学习问题绑定到一个共享的政策学习器中，以实现热启动效果，从而提高样本效率。 此外，提出了一种 traverse Hyperband 生成的设计矩阵的方法，以减少 Hyperband 的随机性，并且随着 universal policy learner 的强化，对控制器学习问题的解决具有加大的热启动效果。</li>
<li>results: 实验表明，相比基eline，本研究的方法在各种 agent 设计问题上具有显著的优势，并且分析优化后的设计结果显示出了有趣的设计修改，包括设计简化和非INTUITIVE的修改，这些修改在生物世界中有很多应用。<details>
<summary>Abstract</summary>
Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of agent design problems demonstrate the superiority of our method compared to the baselines. Additionally, analysis of the optimized designs shows interesting design alterations including design simplifications and non-intuitive alterations that have emerged in the biological world.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Curve-Your-Attention-Mixed-Curvature-Transformers-for-Graph-Representation-Learning"><a href="#Curve-Your-Attention-Mixed-Curvature-Transformers-for-Graph-Representation-Learning" class="headerlink" title="Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning"></a>Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04082">http://arxiv.org/abs/2309.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjun Cho, Seunghyuk Cho, Sungwoo Park, Hankook Lee, Honglak Lee, Moontae Lee</li>
<li>for: 本研究旨在扩展Transformers模型到非欧几何空间，以更好地处理具有层次或循环结构的实际图像。</li>
<li>methods: 我们提出了一种基于产品符号投影的全级产品符号Transformer模型，可以在不同的常数曲率空间上进行扩展。此外，我们还提出了一种基于几何核函数的非欧几何注意力方法，可以在时间和空间成本线性增长的情况下保持对图像的准确性。</li>
<li>results: 我们的实验表明，通过将Transformers模型扩展到非欧几何空间，可以提高图像重建和节点分类的性能。此外，我们的kernelized非欧几何注意力方法可以在时间和空间成本线性增长的情况下保持对图像的准确性。<details>
<summary>Abstract</summary>
Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature initializations. We also provide a kernelized approach to non-Euclidean attention, which enables our model to run in time and memory cost linear to the number of nodes and edges while respecting the underlying geometry. Experiments on graph reconstruction and node classification demonstrate the benefits of generalizing Transformers to the non-Euclidean domain.
</details>
<details>
<summary>摘要</summary>
现实中的图 naturally exhibits层次或循环结构，这些结构不适合 typical Euclidean space。 existing graph neural networks 可以使用 hyperbolic 或 spherical 空间来学习 representation，但这些方法受到 message-passing  paradigm 的限制，导致模型容易受到 oversmoothing 和 oversquashing 的影响。 更近期的工作已经提出了 global attention-based graph Transformers，可以轻松模型长距离交互，但其 extensions towards non-Euclidean geometry 还未explored。 为了bridging这个 gap，我们提出了 Fully Product-Stereographic Transformer，一种基于 constant curvature spaces 的Transformers的扩展。 当与 tokenized graph Transformers 结合使用时，我们的模型可以在 end-to-end 的方式学习输入图的 curvature，不需要额外的调整不同 curvature 的初始化。 我们还提供了 kernelized approach to non-Euclidean attention，这使得我们的模型在时间和内存成本方面能够 linear 化，同时尊重下面 geometry。 实验表明，通过将 Transformers 扩展到 non-Euclidean 领域，可以获得更好的图重建和节点分类效果。
</details></li>
</ul>
<hr>
<h2 id="UER-A-Heuristic-Bias-Addressing-Approach-for-Online-Continual-Learning"><a href="#UER-A-Heuristic-Bias-Addressing-Approach-for-Online-Continual-Learning" class="headerlink" title="UER: A Heuristic Bias Addressing Approach for Online Continual Learning"></a>UER: A Heuristic Bias Addressing Approach for Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04081">http://arxiv.org/abs/2309.04081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FelixHuiweiLin/UER">https://github.com/FelixHuiweiLin/UER</a></li>
<li>paper_authors: Huiwei Lin, Shanshan Feng, Baoquan Zhang, Hongliang Qiao, Xutao Li, Yunming Ye</li>
<li>for: 本研究旨在解决在线CONTINUAL LEARNING中的偏见问题，即随着数据流入的新数据，模型倾向于新数据的类别，而忘记过去的知识。</li>
<li>methods: 本文提出了一种更直观和高效的方法，即使用投影因子和距离因子来解决偏见问题。投影因子可以学习新的知识，而距离因子可以帮助保持历史知识。</li>
<li>results: 对于三个数据集，论文提出的UER方法在比较多种state-of-the-art方法的情况下表现出色，得到了superior的性能。<details>
<summary>Abstract</summary>
Online continual learning aims to continuously train neural networks from a continuous data stream with a single pass-through data. As the most effective approach, the rehearsal-based methods replay part of previous data. Commonly used predictors in existing methods tend to generate biased dot-product logits that prefer to the classes of current data, which is known as a bias issue and a phenomenon of forgetting. Many approaches have been proposed to overcome the forgetting problem by correcting the bias; however, they still need to be improved in online fashion. In this paper, we try to address the bias issue by a more straightforward and more efficient method. By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits. On the contrary, the norm factor abandoned by existing methods helps remember historical knowledge. Based on this observation, we intuitively propose to leverage the norm factor to balance the new and old knowledge for addressing the bias. To this end, we develop a heuristic approach called unbias experience replay (UER). UER learns current samples only by the angle factor and further replays previous samples by both the norm and angle factors. Extensive experiments on three datasets show that UER achieves superior performance over various state-of-the-art methods. The code is in https://github.com/FelixHuiweiLin/UER.
</details>
<details>
<summary>摘要</summary>
在线持续学习目标是通过连续数据流进行单次通过数据进行神经网络的持续训练。现有的方法中最有效的方法是使用练习重温方法，其中通过重新训练部分过去数据来解决偏见问题。现有的预测器通常会生成偏见的dot乘积 logits，它们偏好当前数据中的类别，这被称为偏见问题和忘记现象。许多方法已经被提出来解决忘记问题，但 ainda需要进一步改进。在这篇论文中，我们尝试通过更简单和更高效的方法来解决偏见问题。我们发现，将 dot乘积 logits 分解成角度因子和 нор 因子，我们在实验中发现，偏见问题主要出现在角度因子中，可以用cosine logits来学习新知识。相反，norm因子被现有方法抛弃，可以帮助记忆历史知识。基于这一观察，我们提出了一种直观的方法 called 无偏经验重温（UER）。UER 通过角度因子来学习当前样本，并在过去样本中重新使用 norm 和角度因子来填充旧知识。我们在三个数据集上进行了广泛的实验，结果显示 UER 在多种当前顶峰方法之上获得了更高的性能。代码位于https://github.com/FelixHuiweiLin/UER。
</details></li>
</ul>
<hr>
<h2 id="Enabling-the-Evaluation-of-Driver-Physiology-Via-Vehicle-Dynamics"><a href="#Enabling-the-Evaluation-of-Driver-Physiology-Via-Vehicle-Dynamics" class="headerlink" title="Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics"></a>Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04078">http://arxiv.org/abs/2309.04078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Ordonez-Hurtado, Bo Wen, Nicholas Barra, Ryan Vimba, Sergio Cabrero-Barros, Sergiy Zhuk, Jeffrey L. Rogers</li>
<li>for: 这篇论文是为了设计一种基于连接式智能汽车系统的 Driver Physiology Assessment 系统，以提高道路安全性和早期发现健康问题。</li>
<li>methods: 论文使用了一系列商业传感器，包括汽车和数字健康领域的传感器，以记录 Driver 的生理参数和驾驶行为。这些数据流被处理，提取关键参数，以便了解 Driver 在外部环境和驾驶姿势之间的关系，以及 Driver 的生理反应。</li>
<li>results: 这种驾驶评估系统可以帮助提高道路安全性，并可以与传统医疗设施的数据结合，提高早期发现健康问题的可能性。<details>
<summary>Abstract</summary>
Driving is a daily routine for many individuals across the globe. This paper presents the configuration and methodologies used to transform a vehicle into a connected ecosystem capable of assessing driver physiology. We integrated an array of commercial sensors from the automotive and digital health sectors along with driver inputs from the vehicle itself. This amalgamation of sensors allows for meticulous recording of the external conditions and driving maneuvers. These data streams are processed to extract key parameters, providing insights into driver behavior in relation to their external environment and illuminating vital physiological responses. This innovative driver evaluation system holds the potential to amplify road safety. Moreover, when paired with data from conventional health settings, it may enhance early detection of health-related complications.
</details>
<details>
<summary>摘要</summary>
驾驶是许多人每天的日常 Routine 中的一部分。这篇论文介绍了将车辆转化成连接到电子环境的设备，以评估驾驶者的生理指标。我们结合了汽车和数字健康业界的商业传感器，以及车辆自身的驾驶输入。这些敏感器组合使得精准记录外部环境和驾驶动作。这些数据流被处理，以提取关键参数，了解驾驶者在外部环境下的行为，以及他们的生理响应。这种驾驶评估系统具有提高道路安全性的潜力，同时，与传统医疗设施的数据结合，可能增强早期发现健康问题的能力。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Langevin-Monte-Carlo-schemes-for-sampling-PSD-matrices-with-fixed-rank"><a href="#Riemannian-Langevin-Monte-Carlo-schemes-for-sampling-PSD-matrices-with-fixed-rank" class="headerlink" title="Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank"></a>Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04072">http://arxiv.org/abs/2309.04072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianmin Yu, Shixin Zheng, Jianfeng Lu, Govind Menon, Xiangxiong Zhang</li>
<li>for: 本 paper  introduce two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$.</li>
<li>methods: 这两种方案基于 Riemannian Langevin equation (RLE) 的 Euler-Maruyama 离散化和 Brownian motion on the manifold $\mathcal S^{n,p}_+$.</li>
<li>results: 我们提供了两种基于不同 метри的数学方案，其中一种基于 $\mathcal S^{n,p}_+ $ 的嵌入 metric，另一种基于 Bures-Wasserstein  метри对于 quotient geometry。我们还提供了一些具有显式 Gibbs distribution 的能量函数，使得这些方案可以进行数值验证。<details>
<summary>Abstract</summary>
This paper introduces two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama discretization of the Riemannian Langevin equation (RLE) with Brownian motion on the manifold. We present numerical schemes for RLE under two fundamental metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the Bures-Wasserstein metric corresponding to quotient geometry. We also provide examples of energy functions with explicit Gibbs distributions that allow numerical validation of these schemes.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了两种显式的方法来从 Gibbs 分布中采样矩阵，其中矩阵是 $\mathcal{S}^{n,p}_+ $ 上的实数正定阵列（PSD），size $n\times n$ 和 rank $p$。给定一个能量函数 $\mathcal{E}: \mathcal{S}^{n,p}_+ \to \mathbb{R}$，以及在 $\mathcal{S}^{n,p}_+ $ 上的一些里曼尼 мет里克 $g$，这些方法基于 Riemannian Langevin equation (RLE) 的 Euler-Maruyama 积分，并且使用布尔斯-沃asserstein  мет里克对于quotient geometry。我们还提供了一些能量函数的Explicit Gibbs 分布，以便numerically验证这些方法。
</details></li>
</ul>
<hr>
<h2 id="3D-Denoisers-are-Good-2D-Teachers-Molecular-Pretraining-via-Denoising-and-Cross-Modal-Distillation"><a href="#3D-Denoisers-are-Good-2D-Teachers-Molecular-Pretraining-via-Denoising-and-Cross-Modal-Distillation" class="headerlink" title="3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation"></a>3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04062">http://arxiv.org/abs/2309.04062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjun Cho, Dae-Woong Jeong, Sung Moon Ko, Jinwoo Kim, Sehui Han, Seunghoon Hong, Honglak Lee, Moontae Lee</li>
<li>for: 预处理分子表示，提高分子性质预测效果。</li>
<li>methods: 使用2D图表示法和3D杂谱推理法，以及知识储存法。</li>
<li>results: 比基eline方法提高30%左右，并且可以避免高效 compute量。<details>
<summary>Abstract</summary>
Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to accurate conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against other baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to accurate conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against other baselines." into Simplified Chinese.<<SYS>>大量未标注数据的预训练是分子性质预测中不可或缺的，因为获取准确标签的成本很高。现有许多2D图形基的分子预训练方法，但这些方法很难显著提高预测性能。最近的工作则是基于3D杂化的预训练，这些方法在预测任务中获得了有望的结果。然而，在下游训练中，使用3D杂化的模型需要对未seen分子的原子坐标精度准确，这是计算成本很高的。为此，我们提出了D&D，一种基于自我监督的分子表示学习框架。通过杂化后cross-modal知识储存，我们的方法可以充分利用杂化中获得的知识，同时在下游任务中不需要准确的杂化。实验表明，通过D&D预训练的2D图形Encoder可以基于2D图形上预测3D信息，并与其他基准模型相比显示出超越性和标签效率。>>>
</details></li>
</ul>
<hr>
<h2 id="Weighted-Unsupervised-Domain-Adaptation-Considering-Geometry-Features-and-Engineering-Performance-of-3D-Design-Data"><a href="#Weighted-Unsupervised-Domain-Adaptation-Considering-Geometry-Features-and-Engineering-Performance-of-3D-Design-Data" class="headerlink" title="Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data"></a>Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04499">http://arxiv.org/abs/2309.04499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungyeon Shin, Namwoo Kang</li>
<li>for: 加速设计优化和降低计算成本</li>
<li>methods: 使用深度学习对设计数据进行预测，并使用对抗学习抽取域无关特征，以及基于多输出回归任务预测工程性能</li>
<li>results: 可以减少目标风险，并提高对未经标注的目标领域的预测精度<details>
<summary>Abstract</summary>
The product design process in manufacturing involves iterative design modeling and analysis to achieve the target engineering performance, but such an iterative process is time consuming and computationally expensive. Recently, deep learning-based engineering performance prediction models have been proposed to accelerate design optimization. However, they only guarantee predictions on training data and may be inaccurate when applied to new domain data. In particular, 3D design data have complex features, which means domains with various distributions exist. Thus, the utilization of deep learning has limitations due to the heavy data collection and training burdens. We propose a bi-weighted unsupervised domain adaptation approach that considers the geometry features and engineering performance of 3D design data. It is specialized for deep learning-based engineering performance predictions. Domain-invariant features can be extracted through an adversarial training strategy by using hypothesis discrepancy, and a multi-output regression task can be performed with the extracted features to predict the engineering performance. In particular, we present a source instance weighting method suitable for 3D design data to avoid negative transfers. The developed bi-weighting strategy based on the geometry features and engineering performance of engineering structures is incorporated into the training process. The proposed model is tested on a wheel impact analysis problem to predict the magnitude of the maximum von Mises stress and the corresponding location of 3D road wheels. This mechanism can reduce the target risk for unlabeled target domains on the basis of weighted multi-source domain knowledge and can efficiently replace conventional finite element analysis.
</details>
<details>
<summary>摘要</summary>
制造过程中的产品设计包括迭代的设计模型和分析，以达到工程性能目标，但这种迭代过程需要大量的计算资源和时间。近些年，基于深度学习的工程性能预测模型已经被提出，以加速设计优化。然而，这些模型只能在训练数据上作出预测，并且在新领域数据上可能不准确。特别是3D设计数据具有复杂的特征，这意味着存在多种分布。因此，使用深度学习有限制，需要大量的数据采集和训练压力。我们提出了一种异杂预测方法，考虑了3D设计数据的几何特征和工程性能。我们采用了一种适应训练策略，通过使用假设差分来提取域无关特征，并在提取的特征上进行多输出回归任务来预测工程性能。具体来说，我们提出了一种源实例权重方法，适用于3D设计数据，以避免负性传递。我们在训练过程中包含了这种异杂预测策略。我们的模型在轮胎冲击分析问题中预测了轮胎的最大 von Mises 压力和相应的位置。这种机制可以降低目标风险，基于权重多源领域知识，并可以高效地取代传统的finite element分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/cs.LG_2023_09_08/" data-id="clmjn91my00870j88fi4aciet" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/08/eess.AS_2023_09_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-09-08
        
      </div>
    </a>
  
  
    <a href="/2023/09/08/eess.IV_2023_09_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-08</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
