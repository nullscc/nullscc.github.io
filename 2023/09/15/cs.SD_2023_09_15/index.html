
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Stack-and-Delay: a new codebook pattern for music generation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.08804 repo_url: None paper_authors: Gael Le Lan, Varun Nagaraja, Ernie Chang, David Kant, Zhaoheng Ni,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-15">
<meta property="og:url" content="https://nullscc.github.io/2023/09/15/cs.SD_2023_09_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Stack-and-Delay: a new codebook pattern for music generation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.08804 repo_url: None paper_authors: Gael Le Lan, Varun Nagaraja, Ernie Chang, David Kant, Zhaoheng Ni,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-15T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:58:27.844Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/15/cs.SD_2023_09_15/" class="article-date">
  <time datetime="2023-09-15T15:00:00.000Z" itemprop="datePublished">2023-09-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Stack-and-Delay-a-new-codebook-pattern-for-music-generation"><a href="#Stack-and-Delay-a-new-codebook-pattern-for-music-generation" class="headerlink" title="Stack-and-Delay: a new codebook pattern for music generation"></a>Stack-and-Delay: a new codebook pattern for music generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08804">http://arxiv.org/abs/2309.08804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gael Le Lan, Varun Nagaraja, Ernie Chang, David Kant, Zhaoheng Ni, Yangyang Shi, Forrest Iandola, Vikas Chandra</li>
<li>for: 这 paper 是为了提高语音生成模型的执行速度而写的。</li>
<li>methods: 这 paper 使用了一种新的 stack-and-delay 解码策略，以提高 auto-regressive 解码的速度。</li>
<li>results: 对于同等效果预算，这 paper 的新策略可以在对 GPU 进行批处理时提高生成速度，并且在对比 vanilla flat 解码法时，质量几乎相当。<details>
<summary>Abstract</summary>
In language modeling based music generation, a generated waveform is represented by a sequence of hierarchical token stacks that can be decoded either in an auto-regressive manner or in parallel, depending on the codebook patterns. In particular, flattening the codebooks represents the highest quality decoding strategy, while being notoriously slow. To this end, we propose a novel stack-and-delay style of decoding strategy to improve upon the flat pattern decoding where generation speed is four times faster as opposed to vanilla flat decoding. This brings the inference time close to that of the delay decoding strategy, and allows for faster inference on GPU for small batch sizes. For the same inference efficiency budget as the delay pattern, we show that the proposed approach performs better in objective evaluations, almost closing the gap with the flat pattern in terms of quality. The results are corroborated by subjective evaluations which show that samples generated by the new model are slightly more often preferred to samples generated by the competing model given the same text prompts.
</details>
<details>
<summary>摘要</summary>
在语言模型基于音乐生成中，生成波形被表示为一个层次化token堆，可以在某些情况下以自动递归方式或平行方式解码，具体取决于codebook Pattern。特别是，平滑codebooks表示最高质量解码策略，但却非常慢。为此，我们提出了一种新的堆延式解码策略，以提高对于平滑解码的速度。这将执行时间与延迟解码策略相似，并在小批处理时在GPU上进行更快的执行。对于同样的推理效率预算，我们表明了我们的方法在对象评价中比 delay Pattern 更好，几乎与平滑Pattern 相近的质量。结果得到了subjective评价的支持，显示新模型生成的样本在同一个文本提示下被轻微更多地选择。
</details></li>
</ul>
<hr>
<h2 id="Music-Source-Separation-Based-on-a-Lightweight-Deep-Learning-Framework-DTTNET-DUAL-PATH-TFC-TDF-UNET"><a href="#Music-Source-Separation-Based-on-a-Lightweight-Deep-Learning-Framework-DTTNET-DUAL-PATH-TFC-TDF-UNET" class="headerlink" title="Music Source Separation Based on a Lightweight Deep Learning Framework (DTTNET: DUAL-PATH TFC-TDF UNET)"></a>Music Source Separation Based on a Lightweight Deep Learning Framework (DTTNET: DUAL-PATH TFC-TDF UNET)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08684">http://arxiv.org/abs/2309.08684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junyuchen-cjy/dttnet-pytorch">https://github.com/junyuchen-cjy/dttnet-pytorch</a></li>
<li>paper_authors: Junyu Chen, Susmitha Vekkot, Pancham Shukla</li>
<li>for: 本研究旨在提出一种轻量级的音乐源分离模型（DTTNet），以提高音乐源分离的效果。</li>
<li>methods: 本文使用了一种基于双路模块和时域频域卷积的时间分布式全连接卷积神经网络（TFC-TDF UNet），并对模型进行了训练。</li>
<li>results: 对于 vocals 部分，DTTNet 可以达到 10.12 dB cSDR，比 Bandsplit RNN (BSRNN) 高出 0.11 dB，但具有 86.7%  fewer 参数。<details>
<summary>Abstract</summary>
Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and 'other' tracks from a piece of mixed music. While deep learning methods have shown impressive results, there is a trend toward larger models. In our paper, we introduce a novel and lightweight architecture called DTTNet, which is based on Dual-Path Module and Time-Frequency Convolutions Time-Distributed Fully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals' compared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer parameters. We also assess pattern-specific performance and model generalization for intricate audio patterns.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）目标是从混合音乐中提取“声乐”、“鼓”、“低音”和“其他”多个轨道。深度学习方法已经表现出色，但是现在有一趋势是增大模型。在我们的论文中，我们介绍了一种新的轻量级架构，称为DTTNet，它基于双路模块和时域频域卷积（TFC-TDF UNet）。DTTNet实现了10.12 dB的清晰度（cSDR），比BSRNN（Bandsplit RNN）的10.01 dB高，但具有86.7%的参数数量少。我们还评估了模型对复杂音乐 patrern的特定性能和通用性。
</details></li>
</ul>
<hr>
<h2 id="Chunked-Attention-based-Encoder-Decoder-Model-for-Streaming-Speech-Recognition"><a href="#Chunked-Attention-based-Encoder-Decoder-Model-for-Streaming-Speech-Recognition" class="headerlink" title="Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition"></a>Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08436">http://arxiv.org/abs/2309.08436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Zeineldeen, Albert Zeyer, Ralf Schlüter, Hermann Ney</li>
<li>for: 这篇论文旨在提出一种流处理的注意力基于encoder-decoder模型，其中 either the decoder, or both the encoder and decoder,  operate on pre-defined, fixed-size windows called chunks.</li>
<li>methods: 该模型使用特殊的 end-of-chunk (EOC) 符号来进行chunk boundaries的标识，而不是 convential end-of-sequence 符号。此外，模型还 explores 其他与标准转录器模型的差异。</li>
<li>results: 通过在 Librispeech 和 TED-LIUM-v2 上进行实验，并将 consecutives sequences concatenated for long-form trials，发现该流处理模型与非流处理模型的性能相对 compatible，并且在长型语音总结very well。<details>
<summary>Abstract</summary>
We study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. This modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where EOC corresponds to the blank symbol. We further explore the remaining differences between a standard transducer and our model. Additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. Through experiments on Librispeech and TED-LIUM-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.
</details>
<details>
<summary>摘要</summary>
我们研究了一个流处理器基于注意力的编解码器模型，其中编码器或解码器都操作在预定的固定大小窗口（chunk）上。特殊的结束chunk（EOC）符号在一个chunk到下一个chunk之间进行转移，从而替代传统的结束序列符号。这种修改虽小，但将我们的模型与帧模型等同起来，其中EOC符号与空符号相对应。我们进一步探讨了标准转录器和我们模型之间的剩余差异。我们还考虑了长форма语音总体化、扫描大小和长度 нормализация等相关因素。通过对Librispeech和TED-LIUM-v2上进行实验，并将 consecutivesequences concatenate для长形试验，我们发现我们的流处理器模型与非流处理器模型的性能相比具有竞争力，并且对长形语音总体化很好。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Active-Speaker-Extraction-for-Sparsely-Overlapped-Multi-talker-Speech"><a href="#Audio-Visual-Active-Speaker-Extraction-for-Sparsely-Overlapped-Multi-talker-Speech" class="headerlink" title="Audio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker Speech"></a>Audio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08408">http://arxiv.org/abs/2309.08408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrjunjieli/activeextract">https://github.com/mrjunjieli/activeextract</a></li>
<li>paper_authors: Junjie Li, Ruijie Tao, Zexu Pan, Meng Ge, Shuai Wang, Haizhou Li</li>
<li>for: 本研究旨在提高target speaker抽取的精度，特别是在稀有 overlap的场景下。</li>
<li>methods: 本文提出了一种名为ActiveExtract的音频视频 speaker抽取模型，该模型利用音频视频活跃 speaker检测（ASD）来直接提供目标说话者的帧级活动信息，同时使用ASD的中间特征表示来鉴别说话lip同步。</li>
<li>results: 实验结果表明， compared to基线，我们的模型在不同的 overlap ratio下均表现出超过4dB的提升，这表明我们的模型可以在稀有 overlap的场景下提高target speaker抽取的精度。<details>
<summary>Abstract</summary>
Target speaker extraction aims to extract the speech of a specific speaker from a multi-talker mixture as specified by an auxiliary reference. Most studies focus on the scenario where the target speech is highly overlapped with the interfering speech. However, this scenario only accounts for a small percentage of real-world conversations. In this paper, we aim at the sparsely overlapped scenarios in which the auxiliary reference needs to perform two tasks simultaneously: detect the activity of the target speaker and disentangle the active speech from any interfering speech. We propose an audio-visual speaker extraction model named ActiveExtract, which leverages speaking activity from audio-visual active speaker detection (ASD). The ASD directly provides the frame-level activity of the target speaker, while its intermediate feature representation is trained to discriminate speech-lip synchronization that could be used for speaker disentanglement. Experimental results show our model outperforms baselines across various overlapping ratios, achieving an average improvement of more than 4 dB in terms of SI-SNR.
</details>
<details>
<summary>摘要</summary>
target speaker extraction aims to extract the speech of a specific speaker from a multi-talker mixture as specified by an auxiliary reference. most studies focus on the scenario where the target speech is highly overlapped with the interfering speech. however, this scenario only accounts for a small percentage of real-world conversations. in this paper, we aim at the sparsely overlapped scenarios in which the auxiliary reference needs to perform two tasks simultaneously: detect the activity of the target speaker and disentangle the active speech from any interfering speech. we propose an audio-visual speaker extraction model named activeextract, which leverages speaking activity from audio-visual active speaker detection (asd). the asd directly provides the frame-level activity of the target speaker, while its intermediate feature representation is trained to discriminate speech-lip synchronization that could be used for speaker disentanglement. experimental results show our model outperforms baselines across various overlapping ratios, achieving an average improvement of more than 4 dB in terms of si-snr.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Audio-free-Prompt-Tuning-for-Language-Audio-Models"><a href="#Audio-free-Prompt-Tuning-for-Language-Audio-Models" class="headerlink" title="Audio-free Prompt Tuning for Language-Audio Models"></a>Audio-free Prompt Tuning for Language-Audio Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08357">http://arxiv.org/abs/2309.08357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Li, Xiangdong Wang, Hong Liu</li>
<li>for: 这个论文想要协助CLAP模型从语言 Audio域预训练中提取特征，以便在不需要标注的域音频数据下进行适应。</li>
<li>methods: 我们提议一种不需要域音频数据的CLAP模型调教方法，通过利用CLAP模型的modalitiesAlignment来调整一些文本提示符，以便更好地调整模型空间，避免过拟合见到的类别。此外，我们还探索了多层排序提示的策略，以 fusionglobal和local信息。</li>
<li>results: 我们的方法可以提高CLAP模型的性能和训练效率，并在零例推理中对未经见的类别进行识别，并且比vanilla CLAP更好地转移知识。此外，我们的方法还可以在只知道下游类别名称的情况下进行适应。<details>
<summary>Abstract</summary>
Contrastive Language-Audio Pretraining (CLAP) is pre-trained to associate audio features with human language, making it a natural zero-shot classifier to recognize unseen sound categories. To adapt CLAP to downstream tasks, prior works inevitably require labeled domain audios, which limits their scalability under data scarcity and deprives them of the capability to detect novel classes as the original CLAP. In this work, by leveraging the modality alignment in CLAP, we propose an efficient audio-free prompt tuning scheme aimed at optimizing a few prompt tokens from texts instead of audios, which regularizes the model space to avoid overfitting the seen classes as well. Based on this, a multi-grained prompt design is further explored to fuse global and local information. Experiments on several tasks demonstrate that our approach can boost the CLAP and outperform other training methods on model performance and training efficiency. While conducting zero-shot inference on unseen categories, it still shows better transferability than the vanilla CLAP. Moreover, our method is flexible enough even if only knowing the downstream class names. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
“对于语音识别任务，我们提出了一个有效的无音训练方法，可以将文本提示调整为CLAP模型的条件，以提高模型的性能和训练效率。这个方法基于CLAP模型中的modalità对齐，可以将文本提示调整为CLAP模型的条件，以避免模型过拟合见到的类别。我们还提出了一个多层次提示设计，可以融合全球和本地信息。实验结果显示，我们的方法可以提高CLAP模型的性能和训练效率，并且在零shot推断中也表现出比vanilla CLAP更好的转移性。此外，我们的方法可以让你只知道下游类别名称来进行训练，并且还可以在零shot推断中进行推断。我们将将代码发布 soon。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Sound-Event-Detection-with-Local-and-Global-Consistency-Regularization"><a href="#Semi-supervised-Sound-Event-Detection-with-Local-and-Global-Consistency-Regularization" class="headerlink" title="Semi-supervised Sound Event Detection with Local and Global Consistency Regularization"></a>Semi-supervised Sound Event Detection with Local and Global Consistency Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08355">http://arxiv.org/abs/2309.08355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Li, Xiangdong Wang, Hong Liu, Rui Tao, Long Yan, Kazushige Ouchi</li>
<li>for: 这 paper 是为了提高 semi-supervised sound event detection 的性能而写的。</li>
<li>methods: 这 paper 使用了 Local and Global Consistency (LGC) regularization scheme，包括 audio CutMix 和特制的 contrastive loss，以促进模型在 label- 和 feature-level 上的改进。</li>
<li>results: 实验结果表明，LGC 超越了同等设置的基eline system，并且可以与现有方法相结合以实现进一步的改进。<details>
<summary>Abstract</summary>
Learning meaningful frame-wise features on a partially labeled dataset is crucial to semi-supervised sound event detection. Prior works either maintain consistency on frame-level predictions or seek feature-level similarity among neighboring frames, which cannot exploit the potential of unlabeled data. In this work, we design a Local and Global Consistency (LGC) regularization scheme to enhance the model on both label- and feature-level. The audio CutMix is introduced to change the contextual information of clips. Then, the local consistency is adopted to encourage the model to leverage local features for frame-level predictions, and the global consistency is applied to force features to align with global prototypes through a specially designed contrastive loss. Experiments on the DESED dataset indicate the superiority of LGC, surpassing its respective competitors largely with the same settings as the baseline system. Besides, combining LGC with existing methods can obtain further improvements. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
学习有意义的帧级特征是 semi-supervised 音频事件检测中的关键。先前的工作ether maintain consistency on frame-level predictions or seek feature-level similarity among neighboring frames, 这些方法无法利用无标签数据的潜力。在这种工作中，我们设计了 Local and Global Consistency (LGC) 规范来提高模型在标签和特征水平上。音频 CutMix 被引入，改变clip的Contextual information。然后，本地一致性被采用，以便使模型利用本地特征进行帧级预测，而全球一致性被应用，通过特殊的对比损失来让特征与全球谱系对齐。DESED 数据集的实验表明 LGC 的优越性，大大超越了同样的设置的基eline system。此外，将 LGC 与现有方法结合可以获得进一步的改进。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="The-Multimodal-Information-Based-Speech-Processing-MISP-2023-Challenge-Audio-Visual-Target-Speaker-Extraction"><a href="#The-Multimodal-Information-Based-Speech-Processing-MISP-2023-Challenge-Audio-Visual-Target-Speaker-Extraction" class="headerlink" title="The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction"></a>The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08348">http://arxiv.org/abs/2309.08348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, Odette Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao</li>
<li>for: 本研究的目的是为了提高后端语音识别系统的准确率，通过使用音视频目标说话人抽取（AVTSE）任务。</li>
<li>methods: 本研究使用了音视频Speech Enhancement（MISP）挑战的数据集，并提供了一个基eline系统来支持参与者的参与。</li>
<li>results: 实验结果表明，AVTSE任务在真实的音响环境中非常具有挑战性，参与者可能会遇到各种问题。<details>
<summary>Abstract</summary>
Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Speech-dependent-Modeling-of-Own-Voice-Transfer-Characteristics-for-In-ear-Microphones-in-Hearables"><a href="#Speech-dependent-Modeling-of-Own-Voice-Transfer-Characteristics-for-In-ear-Microphones-in-Hearables" class="headerlink" title="Speech-dependent Modeling of Own Voice Transfer Characteristics for In-ear Microphones in Hearables"></a>Speech-dependent Modeling of Own Voice Transfer Characteristics for In-ear Microphones in Hearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08294">http://arxiv.org/abs/2309.08294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattes Ohlenbusch, Christian Rollwage, Simon Doclo</li>
<li>for: 增强听力器中的耳壳内麦icrophone信号质量使用算法，以joint bandwidth extension、equalization和噪声减少。</li>
<li>methods: 基于phoneme认识的speech-dependent系统Identification模型，用于模拟耳壳内麦icrophone recording。</li>
<li>results: 使用提议的speech-dependent模型可以更好地模拟耳壳内麦icrophone recording，并且可以更好地泛化到不同的说话人。<details>
<summary>Abstract</summary>
Many hearables contain an in-ear microphone, which may be used to capture the own voice of its user in noisy environments. Since the in-ear microphone mostly records body-conducted speech due to ear canal occlusion, it suffers from band-limitation effects while only capturing a limited amount of external noise. To enhance the quality of the in-ear microphone signal using algorithms aiming at joint bandwidth extension, equalization, and noise reduction, it is desirable to have an accurate model of the own voice transfer characteristics between the entrance of the ear canal and the in-ear microphone. Such a model can be used, e.g., to simulate a large amount of in-ear recordings to train supervised learning-based algorithms. Since previous research on ear canal occlusion suggests that own voice transfer characteristics depend on speech content, in this contribution we propose a speech-dependent system identification model based on phoneme recognition. We assess the accuracy of simulating own voice speech by speech-dependent and speech-independent modeling and investigate how well modeling approaches are able to generalize to different talkers. Simulation results show that using the proposed speech-dependent model is preferable for simulating in-ear recordings compared to using a speech-independent model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Head-Related-Transfer-Function-Interpolation-with-a-Spherical-CNN"><a href="#Head-Related-Transfer-Function-Interpolation-with-a-Spherical-CNN" class="headerlink" title="Head-Related Transfer Function Interpolation with a Spherical CNN"></a>Head-Related Transfer Function Interpolation with a Spherical CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08290">http://arxiv.org/abs/2309.08290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xingyuaudio/HRTF-SCNN">https://github.com/xingyuaudio/HRTF-SCNN</a></li>
<li>paper_authors: Xingyu Chen, Fei Ma, Yile Zhang, Amy Bastine, Prasanga N. Samarasinghe</li>
<li>for: 这篇论文旨在提出一种基于深度学习的高分辨率 Head-related transfer functions (HRTFs)  interpolating方法，以便在虚拟现实应用中实现高精度的声场 reproduce。</li>
<li>methods: 该方法基于圆形卷积神经网络，通过对 HRTF 的分解和重建来实现卷积过程。使用 Spherical Harmonics (SHs) 作为卷积函数，使得卷积层能够有效地捕捉声场特征。</li>
<li>results:  simulations 结果表明，提出的方法能够准确地从稀疏测量 interpolate HRTF，高效地超过 SH 方法和学习基于方法。<details>
<summary>Abstract</summary>
Head-related transfer functions (HRTFs) are crucial for spatial soundfield reproduction in virtual reality applications. However, obtaining personalized, high-resolution HRTFs is a time-consuming and costly task. Recently, deep learning-based methods showed promise in interpolating high-resolution HRTFs from sparse measurements. Some of these methods treat HRTF interpolation as an image super-resolution task, which neglects spatial acoustic features. This paper proposes a spherical convolutional neural network method for HRTF interpolation. The proposed method realizes the convolution process by decomposing and reconstructing HRTF through the Spherical Harmonics (SHs). The SHs, an orthogonal function set defined on a sphere, allow the convolution layers to effectively capture the spatial features of HRTFs, which are sampled on a sphere. Simulation results demonstrate the effectiveness of the proposed method in achieving accurate interpolation from sparse measurements, outperforming the SH method and learning-based methods.
</details>
<details>
<summary>摘要</summary>
HEAD-RELATED TRANSFER FUNCTIONS (HRTFs) 是虚拟现实应用中重要的空间声场重建技术。然而，获取个人化、高分辨率 HRTFs 是一项时间consuming 和成本高的任务。最近，深度学习基于方法在 interpolating 高分辨率 HRTFs 中表现出了搭配。其中一些方法将 HRTF  interpolating 视为一种图像超分辨率任务，忽略了空间声学特征。本文提出了一种圆柱体 convolutional neural network 方法，用于 HRTF  interpolating。该方法通过分解和重建 HRTF 通过圆柱体快推函数 (SHs) 来实现卷积过程。SHs 是定义在球体上的正交函数集，使卷积层能够有效地捕捉 HRTFs 的空间特征，这些特征在球体上被采样。 simulation 结果表明，提议的方法可以准确地从稀疏测量中 interpolate HRTFs，超越 SH 方法和学习基于方法。
</details></li>
</ul>
<hr>
<h2 id="One-Class-Knowledge-Distillation-for-Spoofing-Speech-Detection"><a href="#One-Class-Knowledge-Distillation-for-Spoofing-Speech-Detection" class="headerlink" title="One-Class Knowledge Distillation for Spoofing Speech Detection"></a>One-Class Knowledge Distillation for Spoofing Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08285">http://arxiv.org/abs/2309.08285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingze Lu, Yuxiang Zhang, Wenchao Wang, Zengqiang Shang, Pengyuan Zhang</li>
<li>for: 本研究旨在解决未知算法生成的假语音杀毒检测问题，尤其是traditional检测系统无法通过二分类分类方法普适地检测假语音。</li>
<li>methods: 本研究提出了一种教师-学生框架，通过一个教师模型来引导学生模型学习一类模型，从而提高假语音检测的普适性。</li>
<li>results: 实验结果表明，提出的一类知识储存方法在ASVspoof 21DF数据集和InTheWild数据集上具有更高的普适性和检测精度，相比之下其他现有方法。<details>
<summary>Abstract</summary>
The detection of spoofing speech generated by unseen algorithms remains an unresolved challenge. One reason for the lack of generalization ability is traditional detecting systems follow the binary classification paradigm, which inherently assumes the possession of prior knowledge of spoofing speech. One-class methods attempt to learn the distribution of bonafide speech and are inherently suited to the task where spoofing speech exhibits significant differences. However, training a one-class system using only bonafide speech is challenging. In this paper, we introduce a teacher-student framework to provide guidance for the training of a one-class model. The proposed one-class knowledge distillation method outperforms other state-of-the-art methods on the ASVspoof 21DF dataset and InTheWild dataset, which demonstrates its superior generalization ability.
</details>
<details>
<summary>摘要</summary>
检测假声音仍然是一个未解决的挑战。一个原因是传统的检测系统采用二分类分类方式，这意味着它们假设攻击者拥有假声音的知识。一类方法尝试学习正常的声音分布，但是在训练时需要大量的正常声音数据。在本文中，我们介绍了一种教师-学生框架，以帮助一类模型的训练。我们提出的一类知识填充方法在ASVspoof 21DF数据集和InTheWild数据集上显示出优于其他现有方法的性能，这 demonstartes its 的普遍性能。
</details></li>
</ul>
<hr>
<h2 id="Improving-Short-Utterance-Anti-Spoofing-with-AASIST2"><a href="#Improving-Short-Utterance-Anti-Spoofing-with-AASIST2" class="headerlink" title="Improving Short Utterance Anti-Spoofing with AASIST2"></a>Improving Short Utterance Anti-Spoofing with AASIST2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08279">http://arxiv.org/abs/2309.08279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Jingze Lu, Zengqiang Shang, Wenchao Wang, Pengyuan Zhang</li>
<li>for: 防止声音伪造 (anti-spoofing)</li>
<li>methods: 使用 wave2vec 2.0 和 интегрированный спектро-временной графический注意力网络 (AASIST)，并在检测过程中应用动态尺寸大小调整 (DCS) 和自适应大margin精度调整 (ALMFT)</li>
<li>results: 提高短语音识别性能，同时保持不同数据集的常规评估性能<details>
<summary>Abstract</summary>
The wav2vec 2.0 and integrated spectro-temporal graph attention network (AASIST) based countermeasure achieves great performance in speech anti-spoofing. However, current spoof speech detection systems have fixed training and evaluation durations, while the performance degrades significantly during short utterance evaluation. To solve this problem, AASIST can be improved to AASIST2 by modifying the residual blocks to Res2Net blocks. The modified Res2Net blocks can extract multi-scale features and improve the detection performance for speech of different durations, thus improving the short utterance evaluation performance. On the other hand, adaptive large margin fine-tuning (ALMFT) has achieved performance improvement in short utterance speaker verification. Therefore, we apply Dynamic Chunk Size (DCS) and ALMFT training strategies in speech anti-spoofing to further improve the performance of short utterance evaluation. Experiments demonstrate that the proposed AASIST2 improves the performance of short utterance evaluation while maintaining the performance of regular evaluation on different datasets.
</details>
<details>
<summary>摘要</summary>
“wav2vec 2.0 和嵌入式спектро-时间图注意力网络（AASIST）基于的防范措施在语音骗取中表现出色。然而，现有的骗取语音检测系统具有固定的训练和评估时间，而性能在短语音评估中明显下降。为解决这问题，AASIST可以改进为AASIST2，通过修改剩下块为Res2Net块来提取多级特征，提高不同时长语音的检测性能，因此提高短语音评估性能。另一方面，适应大margin微调（ALMFT）在短语音 speaker认证中实现了性能提高。因此，我们在语音骗取中应用动态块大小（DCS）和ALMFT 训练策略，以进一步提高短语音评估性能。实验表明，提出的AASIST2可以在不同的数据集上维持短语音评估性能的同时，提高短语音评估性能。”
</details></li>
</ul>
<hr>
<h2 id="Improving-Voice-Conversion-for-Dissimilar-Speakers-Using-Perceptual-Losses"><a href="#Improving-Voice-Conversion-for-Dissimilar-Speakers-Using-Perceptual-Losses" class="headerlink" title="Improving Voice Conversion for Dissimilar Speakers Using Perceptual Losses"></a>Improving Voice Conversion for Dissimilar Speakers Using Perceptual Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08263">http://arxiv.org/abs/2309.08263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suhita Ghosh, Yamini Sinha, Ingo Siegert, Sebastian Stober</li>
<li>for: 保护用户隐私和数据安全</li>
<li>methods: 使用语音转换技术实现语音数据匿名化</li>
<li>results: 成功地隐藏了语音数据的来源 speaker<details>
<summary>Abstract</summary>
The rising trend of using voice as a means of interacting with smart devices has sparked worries over the protection of users' privacy and data security. These concerns have become more pressing, especially after the European Union's adoption of the General Data Protection Regulation (GDPR). The information contained in an utterance encompasses critical personal details about the speaker, such as their age, gender, socio-cultural origins and more. If there is a security breach and the data is compromised, attackers may utilise the speech data to circumvent the speaker verification systems or imitate authorised users. Therefore, it is pertinent to anonymise the speech data before being shared across devices, such that the source speaker of the utterance cannot be traced. Voice conversion (VC) can be used to achieve speech anonymisation, which involves altering the speaker's characteristics while preserving the linguistic content.
</details>
<details>
<summary>摘要</summary>
声音作为智能设备交互方式的升温趋势，引发了用户隐私和数据安全保护的worries。这些问题在欧盟通过《个人数据保护条例》（GDPR）之后变得更加紧迫。语音中含有关键个人信息，如speaker的年龄、性别、社会文化背景等。如果数据被泄露，攻击者可能利用语音数据绕过speaker验证系统或模仿已经授权的用户。因此，需要对语音数据进行匿名处理，以隐藏语音的来源speaker。声音转换（VC）可以实现匿名处理，即改变speaker的特征，保留语言内容不变。
</details></li>
</ul>
<hr>
<h2 id="TF-SepNet-An-Efficient-1D-Kernel-Design-in-CNNs-for-Low-Complexity-Acoustic-Scene-Classification"><a href="#TF-SepNet-An-Efficient-1D-Kernel-Design-in-CNNs-for-Low-Complexity-Acoustic-Scene-Classification" class="headerlink" title="TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity Acoustic Scene Classification"></a>TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity Acoustic Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08200">http://arxiv.org/abs/2309.08200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiang Cai, Peihong Zhang, Shengchen Li</li>
<li>for: 这篇论文主要关注发展高效的语音Scene分类系统，使用扩展单元网络（CNNs），实现更好的效能和效率。</li>
<li>methods: 提案的TF-SepNet架构将特征处理分为时间和频率两个维度，并将每个维度的特征进行分类。相比于传统的二维（2D）核心，TF-SepNet使用一维（1D）核心，以减少计算成本。</li>
<li>results: 实验结果显示，TF-SepNet在TAU都市语音Scene 2022 Mobile development dataset上表现出色，较同类的State-of-the-arts之前。进一步的调查发现，TF-SepNet的分类器具有更大的有效接收场（ERF），使得更好地捕捉时间-频率特征。<details>
<summary>Abstract</summary>
Recent studies focus on developing efficient systems for acoustic scene classification (ASC) using convolutional neural networks (CNNs), which typically consist of consecutive kernels. This paper highlights the benefits of using separate kernels as a more powerful and efficient design approach in ASC tasks. Inspired by the time-frequency nature of audio signals, we propose TF-SepNet, a CNN architecture that separates the feature processing along the time and frequency dimensions. Features resulted from the separate paths are then merged by channels and directly forwarded to the classifier. Instead of the conventional two dimensional (2D) kernel, TF-SepNet incorporates one dimensional (1D) kernels to reduce the computational costs. Experiments have been conducted using the TAU Urban Acoustic Scene 2022 Mobile development dataset. The results show that TF-SepNet outperforms similar state-of-the-arts that use consecutive kernels. A further investigation reveals that the separate kernels lead to a larger effective receptive field (ERF), which enables TF-SepNet to capture more time-frequency features.
</details>
<details>
<summary>摘要</summary>
近期研究强调开发高效的听音场景分类（ASC）系统，使用核函数网络（CNN）来实现。通常情况下，CNN包含连续的核函数。这篇论文指出，使用独立的核函数可以作为更有力的和高效的设计方法。受听音信号的时间-频率特性启发，我们提出TF-SepNet架构，它在时间和频率维度上分离特征处理。从分离的道路中得到的特征然后通过通道直接传递给分类器。而不是传统的二维（2D）核函数，TF-SepNet使用一维（1D）核函数，以降低计算成本。经过实验，使用TAU都市听音场景2022移动开发 dataset，结果表明TF-SepNet超过了类似的状态艺术使用连续核函数的同类方法。进一步的调查表明，独立的核函数导致更大的有效收发场（ERF），这使得TF-SepNet能够捕捉更多的时间-频率特征。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Residual-Speaker-Representation-for-Voice-Conversion"><a href="#Controllable-Residual-Speaker-Representation-for-Voice-Conversion" class="headerlink" title="Controllable Residual Speaker Representation for Voice Conversion"></a>Controllable Residual Speaker Representation for Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08166">http://arxiv.org/abs/2309.08166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Xu, Jiangyan Yi, Jianhua Tao, Tao Wang, Yong Ren, Rongxiu Zhong</li>
<li>for: 提高voice conversion的高质量表现和 robustness</li>
<li>methods: 使用多层残差近似Token进行提高Robustness，并实现有效控制时声表现</li>
<li>results: 比基eline表现出色，在主观和客观评估中都达到了更高的性能和Robustness<details>
<summary>Abstract</summary>
Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. The introduction of multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in both subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available.
</details>
<details>
<summary>摘要</summary>
最近，voice conversion技术已经取得了 significative进步，以至于表现质量得到了提升。然而，这个领域仍然存在两个关键挑战。第一，现有的voice conversion方法在遇到未看过的speaker时，其Robustness具有有限的能力。第二，它们也有限制timbre表达的能力。为了解决这些挑战，本文提出了一种新的方法，利用多层径辐射近似token来增强对未看过的speaker的Robustness，称为剩余speaker模块。多层径辐射近似token的引入，使得信息从timbre中分离得到更好，以便有效控制timbre在voice conversion中。提议的方法在对比基准方法的主观和客观评估中表现出了superior的性能和更高的Robustness。我们的demo页面公开给公众。
</details></li>
</ul>
<hr>
<h2 id="RVAE-EM-Generative-speech-dereverberation-based-on-recurrent-variational-auto-encoder-and-convolutive-transfer-function"><a href="#RVAE-EM-Generative-speech-dereverberation-based-on-recurrent-variational-auto-encoder-and-convolutive-transfer-function" class="headerlink" title="RVAE-EM: Generative speech dereverberation based on recurrent variational auto-encoder and convolutive transfer function"></a>RVAE-EM: Generative speech dereverberation based on recurrent variational auto-encoder and convolutive transfer function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08157">http://arxiv.org/abs/2309.08157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-westlakeu/rvae-em">https://github.com/audio-westlakeu/rvae-em</a></li>
<li>paper_authors: Pengyu Wang, Xiaofei Li</li>
<li>for: 室内场景中的干扰声纳成为干扰speech质量和清晰度的主要因素。本文提出了一种生成抑干方法。</li>
<li>methods: 本方法基于一个概率模型，利用回卷变换自动编码器（RVAE）网络和卷积函数（CTF）的近似。与大多数前置方法不同，我们的输出是清晰speech的先验知识。我们通过预测最大 posteriori（MAP）算法来实现MAP估计清晰speech。</li>
<li>results: 对单通道speech抑干进行实验，我们发现提出的生成方法明显超过了先进的探测网络。<details>
<summary>Abstract</summary>
In indoor scenes, reverberation is a crucial factor in degrading the perceived quality and intelligibility of speech. In this work, we propose a generative dereverberation method. Our approach is based on a probabilistic model utilizing a recurrent variational auto-encoder (RVAE) network and the convolutive transfer function (CTF) approximation. Different from most previous approaches, the output of our RVAE serves as the prior of the clean speech. And our target is the maximum a posteriori (MAP) estimation of clean speech, which is achieved iteratively through the expectation maximization (EM) algorithm. The proposed method integrates the capabilities of network-based speech prior modelling and CTF-based observation modelling. Experiments on single-channel speech dereverberation show that the proposed generative method noticeably outperforms the advanced discriminative networks.
</details>
<details>
<summary>摘要</summary>
在室内场景中，干扰是影响speech perceived质量和 intelligibility的关键因素。在这项工作中，我们提出了一种生成抑干方法。我们的方法基于一个概率模型，使用回归变换自动编码器（RVAE）网络和卷积函数（CTF）的近似。与大多数前一代方法不同，我们的RVAE输出作为干扰前后的净speech的假设。我们的目标是使用期望最大化（EM）算法来实现MAP估计净speech。我们的方法结合了网络基于声音先验模型和CTF基于观察模型的能力。实验表明，我们的生成方法在单通道speech抑干方面明显超过了先进的探测网络。
</details></li>
</ul>
<hr>
<h2 id="Fine-tune-the-pretrained-ATST-model-for-sound-event-detection"><a href="#Fine-tune-the-pretrained-ATST-model-for-sound-event-detection" class="headerlink" title="Fine-tune the pretrained ATST model for sound event detection"></a>Fine-tune the pretrained ATST model for sound event detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08153">http://arxiv.org/abs/2309.08153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Audio-WestlakeU/ATST-SED">https://github.com/Audio-WestlakeU/ATST-SED</a></li>
<li>paper_authors: Nian Shao, Xian Li, Xiaofei Li</li>
<li>for: 这种研究是为了解决音频事件检测（SED）问题中的数据不足问题。</li>
<li>methods: 这个研究使用了大量预训练的自动学习（SelfSL）模型，以便生成更有特征的特征来进行 SED。</li>
<li>results: 我们的实验表明，我们的 fine-tuning 方法可以超越大型预训练网络的过拟合问题，并实现新的最佳性表现（SOTA），得到了 DCASE 挑战任务4 dataset 的 PSDS1&#x2F;PSDS2 分数为 0.587&#x2F;0.812。<details>
<summary>Abstract</summary>
Sound event detection (SED) often suffers from the data deficiency problem. The recent baseline system in the DCASE2023 challenge task 4 leverages the large pretrained self-supervised learning (SelfSL) models to mitigate such restriction, where the pretrained models help to produce more discriminative features for SED. However, the pretrained models are regarded as a frozen feature extractor in the challenge baseline system and most of the challenge submissions, and fine-tuning of the pretrained models has been rarely studied. In this work, we study the fine-tuning method of the pretrained models for SED. We first introduce ATST-Frame, our newly proposed SelfSL model, to the SED system. ATST-Frame was especially designed for learning frame-level representations of audio signals and obtained state-of-the-art (SOTA) performances on a series of downstream tasks. We then propose a fine-tuning method for ATST-Frame using both (in-domain) unlabelled and labelled SED data. Our experiments show that, the proposed method overcomes the overfitting problem when fine-tuning the large pretrained network, and our SED system obtains new SOTA results of 0.587/0.812 PSDS1/PSDS2 scores on the DCASE challenge task 4 dataset.
</details>
<details>
<summary>摘要</summary>
声音事件检测（SED）经常面临数据不足问题。最近的基eline系统在DCASE2023挑战任务4中利用大规模预先自监学习（SelfSL）模型来缓解这种限制，其中预先学习的模型帮助生成更有特征的特征来进行SED。然而，预先学习的模型通常被视为DCASE2023挑战系统和大多数挑战提交中的冰结特征提取器，并且微调这些预先学习的模型的研究很少。在这项工作中，我们研究了SED中预先学习模型的微调方法。我们首先介绍了我们新提出的ATST-Frame模型，它专门用于学习音频信号帧级表示，并在一系列下游任务上达到了状态之arte（SOTA）性能。然后，我们提议一种微调方法，用于微调ATST-Frame模型使用（域内）无标签和标签SED数据。我们的实验结果表明，提议的方法可以在微调大规模预先学习网络时解决过拟合问题，并且我们的SED系统在DCASE挑战任务4数据集上获得了新的SOTA结果，即0.587/0.812 PSDS1/PSDS2分数。
</details></li>
</ul>
<hr>
<h2 id="t-SOT-FNT-Streaming-Multi-talker-ASR-with-Text-only-Domain-Adaptation-Capability"><a href="#t-SOT-FNT-Streaming-Multi-talker-ASR-with-Text-only-Domain-Adaptation-Capability" class="headerlink" title="t-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation Capability"></a>t-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation Capability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08131">http://arxiv.org/abs/2309.08131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Wu, Naoyuki Kanda, Takuya Yoshioka, Rui Zhao, Zhuo Chen, Jinyu Li</li>
<li>for: 这篇论文主要targets推广多说话者自动语音识别（ASR）的挑战，提出了Token-level serialized output training（t-SOT）方法。</li>
<li>methods: 该方法使用了$\langle \text{cc}\rangle$符号间适应多说话者转录，但使用简单的神经网络推导器结构限制了其应用范围。为解决这个问题，我们提出了一种新的t-SOT模型结构，即factorized neural transducers（FNT）。该方法将语言模型（LM）和推导器 Predictor 分离开，并对$\langle \text{cc}\rangle$符号的不自然的字符顺序进行处理。我们通过保持多个隐藏状态和对$\langle \text{cc}\rangle$符号进行特殊处理来实现这一点。</li>
<li>results: 我们的t-SOT FNT模型在单个和多个说话者 dataset 上的Word Error Rate（WER）减少表现和原始 t-SOT 模型相似，同时保持了文本适应的能力。<details>
<summary>Abstract</summary>
Token-level serialized output training (t-SOT) was recently proposed to address the challenge of streaming multi-talker automatic speech recognition (ASR). T-SOT effectively handles overlapped speech by representing multi-talker transcriptions as a single token stream with $\langle \text{cc}\rangle$ symbols interspersed. However, the use of a naive neural transducer architecture significantly constrained its applicability for text-only adaptation. To overcome this limitation, we propose a novel t-SOT model structure that incorporates the idea of factorized neural transducers (FNT). The proposed method separates a language model (LM) from the transducer's predictor and handles the unnatural token order resulting from the use of $\langle \text{cc}\rangle$ symbols in t-SOT. We achieve this by maintaining multiple hidden states and introducing special handling of the $\langle \text{cc}\rangle$ tokens within the LM. The proposed t-SOT FNT model achieves comparable performance to the original t-SOT model while retaining the ability to reduce word error rate (WER) on both single and multi-talker datasets through text-only adaptation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diversity-based-core-set-selection-for-text-to-speech-with-linguistic-and-acoustic-features"><a href="#Diversity-based-core-set-selection-for-text-to-speech-with-linguistic-and-acoustic-features" class="headerlink" title="Diversity-based core-set selection for text-to-speech with linguistic and acoustic features"></a>Diversity-based core-set selection for text-to-speech with linguistic and acoustic features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08127">http://arxiv.org/abs/2309.08127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kentaro Seki, Shinnosuke Takamichi, Takaaki Saeki, Hiroshi Saruwatari</li>
<li>for: 提高语音识别系统的表达能力</li>
<li>methods: 使用多源数据集合（如audiobooks和YouTube）构建大规模的语音识别系统数据集，并使用多元度度量（measure the degree to which a subset encompasses a wide range）选择核心子集（known as \textit{core-set））</li>
<li>results: 对于不同语言和数据集大小，与基准方法相比，提议的方法表现出色，性能明显超过基准方法<details>
<summary>Abstract</summary>
This paper proposes a method for extracting a lightweight subset from a text-to-speech (TTS) corpus ensuring synthetic speech quality. In recent years, methods have been proposed for constructing large-scale TTS corpora by collecting diverse data from massive sources such as audiobooks and YouTube. Although these methods have gained significant attention for enhancing the expressive capabilities of TTS systems, they often prioritize collecting vast amounts of data without considering practical constraints like storage capacity and computation time in training, which limits the available data quantity. Consequently, the need arises to efficiently collect data within these volume constraints. To address this, we propose a method for selecting the core subset~(known as \textit{core-set}) from a TTS corpus on the basis of a \textit{diversity metric}, which measures the degree to which a subset encompasses a wide range. Experimental results demonstrate that our proposed method performs significantly better than the baseline phoneme-balanced data selection across language and corpus size.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种方法，用于从文本到语音（TTS）集合中提取轻量级的子集，保证合成语音质量。在过去的几年中，有人提出了大规模的 TTS 集合建构方法，通过收集各种媒体资源，如 audiobooks 和 YouTube。虽然这些方法吸引了大量的注意力，但它们经常忽略实际的存储容量和训练时间限制，导致可用数据量受限。因此，需要有效地收集数据，以满足这些容量限制。为此，我们提出了一种基于 \textit{多样度度量} 的核心子集选择方法（known as \textit{core-set}），用于从 TTS 集合中选择最佳的子集。实验结果表明，我们的提议方法在语言和集合大小方面具有显著的优势，比基eline phoneme-balanced 数据选择更好。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-Assisted-Automatic-Speech-Emotion-Recognition-Transcribing-Annotating-and-Augmenting"><a href="#Foundation-Model-Assisted-Automatic-Speech-Emotion-Recognition-Transcribing-Annotating-and-Augmenting" class="headerlink" title="Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting"></a>Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08108">http://arxiv.org/abs/2309.08108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Feng, Shrikanth Narayanan</li>
<li>for: 本研究旨在探讨使用基础模型自动化语音情感识别（SER），从杂音识别到增强。</li>
<li>methods: 本研究使用了基础模型进行自动化SER，包括自动生成转录和注释。</li>
<li>results: 研究发现，使用多个基础模型的输出可以提高情感注释质量，并且可以增强现有语音情感数据集的可用性。<details>
<summary>Abstract</summary>
Significant advances are being made in speech emotion recognition (SER) using deep learning models. Nonetheless, training SER systems remains challenging, requiring both time and costly resources. Like many other machine learning tasks, acquiring datasets for SER requires substantial data annotation efforts, including transcription and labeling. These annotation processes present challenges when attempting to scale up conventional SER systems. Recent developments in foundational models have had a tremendous impact, giving rise to applications such as ChatGPT. These models have enhanced human-computer interactions including bringing unique possibilities for streamlining data collection in fields like SER. In this research, we explore the use of foundational models to assist in automating SER from transcription and annotation to augmentation. Our study demonstrates that these models can generate transcriptions to enhance the performance of SER systems that rely solely on speech data. Furthermore, we note that annotating emotions from transcribed speech remains a challenging task. However, combining outputs from multiple LLMs enhances the quality of annotations. Lastly, our findings suggest the feasibility of augmenting existing speech emotion datasets by annotating unlabeled speech samples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>大量的进步在语音情感识别（SER）领域中使用深度学习模型。然而，训练SER系统仍然具有挑战性，需要大量的时间和资源。与其他机器学习任务类似，获取SER数据集需要大量的数据注释和标注。这些注释和标注过程中存在挑战，尝试扩大传统的SER系统。最近的基础模型的发展对SER领域有益，如ChatGPT等模型，它们提高了人机交互，包括带来了对SER领域的数据收集方面的新可能性。在本研究中，我们explore使用基础模型来帮助自动化SER，从译文和注释到增强。我们的研究表明，这些模型可以生成提高SER系统的性能的译文。此外，我们注意到从译文中注释情感仍然是一个挑战。然而，将多个LLMs的输出结合起来可以提高注释质量。最后，我们的发现表明可以使用未标注的语音样本来增强现有的speech emotion数据集。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Libriheavy-a-50-000-hours-ASR-corpus-with-punctuation-casing-and-context"><a href="#Libriheavy-a-50-000-hours-ASR-corpus-with-punctuation-casing-and-context" class="headerlink" title="Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context"></a>Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08105">http://arxiv.org/abs/2309.08105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/libriheavy">https://github.com/k2-fsa/libriheavy</a></li>
<li>paper_authors: Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, Daniel Povey</li>
<li>for: 本研究开发了一个大规模的语音识别 corpora，名为Libriheavy，包含50,000小时的英语朗读，来自LibriVox。这是目前所知道的最大的开源语音识别数据集。</li>
<li>methods: 作者提出了一种通用和高效的数据集创建管道，用于将Librilight中的音频与其相应的文本进行对应。同时，作者也提供了一些基线系统，包括CTC-Attention和探测器模型。</li>
<li>results: 作者在 Libriheavy 中建立了一个基eline系统，并对其进行了评估。研究结果显示，这个基线系统在 Libriheavy 中的识别精度高于其他相似的数据集。此外，作者还开源了其数据集创建管道，可以用于其他语音对应任务。<details>
<summary>Abstract</summary>
In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting of 50,000 hours of read English speech derived from LibriVox. To the best of our knowledge, Libriheavy is the largest freely-available corpus of speech with supervisions. Different from other open-sourced datasets that only provide normalized transcriptions, Libriheavy contains richer information such as punctuation, casing and text context, which brings more flexibility for system building. Specifically, we propose a general and efficient pipeline to locate, align and segment the audios in previously published Librilight to its corresponding texts. The same as Librilight, Libriheavy also has three training subsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We also extract the dev and test evaluation sets from the aligned audios and guarantee there is no overlapping speakers and books in training sets. Baseline systems are built on the popular CTC-Attention and transducer models. Additionally, we open-source our dataset creatation pipeline which can also be used to other audio alignment tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 Libriheavy，一个大规模的语音识别集合，包含50,000小时的英文语音，来自LibriVox。据我们所知，Libriheavy是目前最大的免费可用的语音识别集合。与其他开源数据集不同，Libriheavy包含更多的信息，例如括号、字母和文本上下文，这使得系统建设更加灵活。我们提出了一个通用和高效的管道来定位、对齐和分割 Librilight 中的音频。与 Librilight 相同，Libriheavy 也有三个训练subset：小、中、大，分别为 500h、5000h 和 50000h。我们还提取了评估集和测试集，并保证训练集中没有重复的 speaker 和书籍。基础系统使用了流行的 CTC-Attention 和批处理模型。此外，我们还开源了我们的数据创建管道，可以用于其他音频对齐任务。
</details></li>
</ul>
<hr>
<h2 id="SSL-Net-A-Synergistic-Spectral-and-Learning-based-Network-for-Efficient-Bird-Sound-Classification"><a href="#SSL-Net-A-Synergistic-Spectral-and-Learning-based-Network-for-Efficient-Bird-Sound-Classification" class="headerlink" title="SSL-Net: A Synergistic Spectral and Learning-based Network for Efficient Bird Sound Classification"></a>SSL-Net: A Synergistic Spectral and Learning-based Network for Efficient Bird Sound Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08072">http://arxiv.org/abs/2309.08072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyuan Yang, Kaichen Zhou, Niki Trigoni, Andrew Markham</li>
<li>for: 鸟叫声分类是生态学、栖息地保护和科研中重要的任务，因为它对鸟种分布和数量进行监测起重要作用。</li>
<li>methods: 我们提出了一种高效和通用的框架called SSL-Net，它将spectral和学习特征相结合，以分类不同的鸟叫声。</li>
<li>results: 我们在一个标准的野外采集的鸟叫声数据集上获得了鼓舞人的实验结果，证明我们的方法可以高效地提取特征和实现鸟叫声分类的高性能，即使工作样本数量有限。此外，我们还提出了三种特征融合策略，以便工程师和研究人员在选择中受益。<details>
<summary>Abstract</summary>
Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.
</details>
<details>
<summary>摘要</summary>
efficient和准确的鸟叫声分类对生态学、栖息地保护和科学研究非常重要，因为它在监测物种分布和数量方面扮演了中心角色。然而，现有的方法通常需要大量的标注音频数据和特定的框架，导致计算和标注负担很大。在这个研究中，我们提出了一种高效和通用的框架called SSL-Net，它将spectral和学习特征结合以分类不同的鸟叫声。我们从标准采集的鸟叫声数据集中获得了鼓舞人心的实验结果，证明了我们的方法可以高效地提取特征和实现鸟叫声分类 tasks，即使受限制的样本数量。此外，我们还提出了三种特征融合策略，以帮助工程师和研究人员在选择方面做出数据分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/15/cs.SD_2023_09_15/" data-id="clq0ru71e0135to88djb5fe0x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/16/eess.SP_2023_09_16/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-09-16
        
      </div>
    </a>
  
  
    <a href="/2023/09/15/cs.CV_2023_09_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-09-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">140</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">98</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">49</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">214</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
