
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Kidney abnormality segmentation in thorax-abdomen CT scans paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03383 repo_url: None paper_authors: Gabriel Efrain Humpire Mamani, Nikolas Lessmann, Ernst Th. Scholten,">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-06">
<meta property="og:url" content="https://nullscc.github.io/2023/09/06/eess.IV_2023_09_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Kidney abnormality segmentation in thorax-abdomen CT scans paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03383 repo_url: None paper_authors: Gabriel Efrain Humpire Mamani, Nikolas Lessmann, Ernst Th. Scholten,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-06T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.074Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/eess.IV_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T09:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Kidney-abnormality-segmentation-in-thorax-abdomen-CT-scans"><a href="#Kidney-abnormality-segmentation-in-thorax-abdomen-CT-scans" class="headerlink" title="Kidney abnormality segmentation in thorax-abdomen CT scans"></a>Kidney abnormality segmentation in thorax-abdomen CT scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03383">http://arxiv.org/abs/2309.03383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Efrain Humpire Mamani, Nikolas Lessmann, Ernst Th. Scholten, Mathias Prokop, Colin Jacobs, Bram van Ginneken</li>
<li>for:  This paper aims to support clinicians in identifying and quantifying renal abnormalities such as cysts, lesions, masses, metastases, and primary tumors using a deep learning approach for segmenting kidney parenchyma and kidney abnormalities.</li>
<li>methods:  The paper introduces an end-to-end segmentation method that was trained on 215 contrast-enhanced thoracic-abdominal CT scans, with half of these scans containing one or more abnormalities. The method incorporates four additional components: an end-to-end multi-resolution approach, a set of task-specific data augmentations, a modified loss function using top-$k$, and spatial dropout.</li>
<li>results:  The paper reports that the best-performing model attained Dice scores of 0.965 and 0.947 for segmenting kidney parenchyma in two test sets, outperforming an independent human observer. In segmenting kidney abnormalities within the 30 test scans containing them, the top-performing method achieved a Dice score of 0.585, while an independent second human observer reached a score of 0.664.<details>
<summary>Abstract</summary>
In this study, we introduce a deep learning approach for segmenting kidney parenchyma and kidney abnormalities to support clinicians in identifying and quantifying renal abnormalities such as cysts, lesions, masses, metastases, and primary tumors. Our end-to-end segmentation method was trained on 215 contrast-enhanced thoracic-abdominal CT scans, with half of these scans containing one or more abnormalities.   We began by implementing our own version of the original 3D U-Net network and incorporated four additional components: an end-to-end multi-resolution approach, a set of task-specific data augmentations, a modified loss function using top-$k$, and spatial dropout. Furthermore, we devised a tailored post-processing strategy. Ablation studies demonstrated that each of the four modifications enhanced kidney abnormality segmentation performance, while three out of four improved kidney parenchyma segmentation. Subsequently, we trained the nnUNet framework on our dataset. By ensembling the optimized 3D U-Net and the nnUNet with our specialized post-processing, we achieved marginally superior results.   Our best-performing model attained Dice scores of 0.965 and 0.947 for segmenting kidney parenchyma in two test sets (20 scans without abnormalities and 30 with abnormalities), outperforming an independent human observer who scored 0.944 and 0.925, respectively. In segmenting kidney abnormalities within the 30 test scans containing them, the top-performing method achieved a Dice score of 0.585, while an independent second human observer reached a score of 0.664, suggesting potential for further improvement in computerized methods.   All training data is available to the research community under a CC-BY 4.0 license on https://doi.org/10.5281/zenodo.8014289
</details>
<details>
<summary>摘要</summary>
在本研究中，我们介绍了一种深度学习方法用于分割肾脏和肾脏畸形，以支持临床医生在识别和评估肾脏畸形，如肿瘤、斑块、肿瘤、转移和原发性肿瘤。我们的端到端分割方法在215个对比增强的胸腹Computed Tomography（CT）扫描图像上进行训练，其中半数图像包含一个或多个畸形。我们开始实现我们自己的版本的原始3D U-Net网络，并添加了四个附加组件：端到端多分辨率方法、任务特定的数据增强、修改后-$k$损失函数和空间抽取。此外，我们设计了特制的后处理策略。减少学习中的每一个修改都提高了肾脏畸形分割性能，而三个中有四个提高了肾脏分割性能。然后，我们将nnUNet框架在我们的数据集上训练。通过将优化的3D U-Net和nnUNet框架 ensemble，我们实现了有些微的提高。我们最佳的模型在两个测试集（20个无畸形图像和30个含畸形图像）中，对肾脏分割达到了Dice分数为0.965和0.947，超过了一名独立的人类观察员的分数（0.944和0.925）。在30个测试图像中含有畸形的情况下，我们的最佳方法达到了Dice分数为0.585，而第二名独立的人类观察员达到了分数为0.664，表明计算机方法还有更多的提高空间。所有的训练数据都是通过CC-BY 4.0license在https://doi.org/10.5281/zenodo.8014289上公开发布。
</details></li>
</ul>
<hr>
<h2 id="Compact-Representation-of-n-th-order-TGV"><a href="#Compact-Representation-of-n-th-order-TGV" class="headerlink" title="Compact Representation of n-th order TGV"></a>Compact Representation of n-th order TGV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03359">http://arxiv.org/abs/2309.03359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Ghulyani, Muthuvel Arigovindan</li>
<li>for: 本研究旨在探讨高顺度 Television (TV) 变形的扩展，并提供一个简单实现的 nth 顺度全总变形 (TGV) 的方法。</li>
<li>methods: 本研究使用了两种简单的表示方法来解决 TGV 变形的问题，并且这些表示方法可以实现在现有的数位图像处理系统上。</li>
<li>results: 本研究获得了一个简单的 TGV 变形方法，并且该方法可以在高顺度下实现 piece-wise 多项函数的表示。此外，本研究还获得了一个可以实现在现有的数位图像处理系统上的 TGV 变形算法。<details>
<summary>Abstract</summary>
Although regularization methods based on derivatives are favored for their robustness and computational simplicity, research exploring higher-order derivatives remains limited. This scarcity can possibly be attributed to the appearance of oscillations in reconstructions when directly generalizing TV-1 to higher orders (3 or more). Addressing this, Bredies et. al introduced a notable approach for generalizing total variation, known as Total Generalized Variation (TGV). This technique introduces a regularization that generates estimates embodying piece-wise polynomial behavior of varying degrees across distinct regions of an image.Importantly, to our current understanding, no sufficiently general algorithm exists for solving TGV regularization for orders beyond 2. This is likely because of two problems: firstly, the problem is complex as TGV regularization is defined as a minimization problem with non-trivial constraints, and secondly, TGV is represented in terms of tensor-fields which is difficult to implement. In this work we tackle the first challenge by giving two simple and implementable representations of n th order TGV
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Real-Time-Dynamic-Data-Driven-Deformable-Registration-for-Image-Guided-Neurosurgery-Computational-Aspects"><a href="#Real-Time-Dynamic-Data-Driven-Deformable-Registration-for-Image-Guided-Neurosurgery-Computational-Aspects" class="headerlink" title="Real-Time Dynamic Data Driven Deformable Registration for Image-Guided Neurosurgery: Computational Aspects"></a>Real-Time Dynamic Data Driven Deformable Registration for Image-Guided Neurosurgery: Computational Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03336">http://arxiv.org/abs/2309.03336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Chrisochoides, Andrey Fedorov, Yixun Liu, Andriy Kot, Panos Foteinos, Fotis Drakopoulos, Christos Tsolakis, Emmanuel Billias, Olivier Clatz, Nicholas Ayache, Alex Golby, Peter Black, Ron Kikinis</li>
<li>for: 这篇论文是为了描述一种基于数学模型的脑成像注射技术，以帮助 neurosurgeon 在手术中准确定位脑肿和关键脑结构。</li>
<li>methods: 这种技术使用了动态数据驱动非固定准确注射方法（D4NRR），该方法可以在手术过程中动态调整先前的成像数据，以 compte pour intra-operative brain shift。</li>
<li>results: 这篇论文总结了这种技术的计算方面的不同变体，以及其在过去15年的进展和未来的发展方向。<details>
<summary>Abstract</summary>
Current neurosurgical procedures utilize medical images of various modalities to enable the precise location of tumors and critical brain structures to plan accurate brain tumor resection. The difficulty of using preoperative images during the surgery is caused by the intra-operative deformation of the brain tissue (brain shift), which introduces discrepancies concerning the preoperative configuration. Intra-operative imaging allows tracking such deformations but cannot fully substitute for the quality of the pre-operative data. Dynamic Data Driven Deformable Non-Rigid Registration (D4NRR) is a complex and time-consuming image processing operation that allows the dynamic adjustment of the pre-operative image data to account for intra-operative brain shift during the surgery. This paper summarizes the computational aspects of a specific adaptive numerical approximation method and its variations for registering brain MRIs. It outlines its evolution over the last 15 years and identifies new directions for the computational aspects of the technique.
</details>
<details>
<summary>摘要</summary>
现有的神经外科手段使用不同模式的医疗图像来准确定位肿瘤和critical brain structures，以便精准地进行肿瘤除脑手术。然而，使用过程中的脑组织变形（brain shift）会导致医疗图像与先前的配置存在差异，从而使得使用过程中的医疗图像变得更加困难。运行时的图像捕捉可以跟踪这些变形，但无法完全取代先前的数据质量。动态数据驱动非rigid Registration（D4NRR）是一种复杂且时间消耗的图像处理操作，它允许在手术过程中动态调整先前的MRI数据，以 compte pour intra-operative brain shift。这篇文章概述了D4NRR的计算方面的特点和其变化在过去15年中，并提出了新的计算方法的发展方向。
</details></li>
</ul>
<hr>
<h2 id="SADIR-Shape-Aware-Diffusion-Models-for-3D-Image-Reconstruction"><a href="#SADIR-Shape-Aware-Diffusion-Models-for-3D-Image-Reconstruction" class="headerlink" title="SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction"></a>SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03335">http://arxiv.org/abs/2309.03335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nivetha Jayakumar, Tonmoy Hossain, Miaomiao Zhang</li>
<li>for: 三维图像重建从有限数量的二维图像中是计算机视觉和图像分析领域的长期挑战。现有的深度学习基本方法已经实现了在这个领域的出色表现，但是现有的深度网络通常无法有效地利用图像中对象的形状结构。因此，重建的对象的topology可能不会很好地保留，导致图像中的缺陷，如缺陷、洞孔或不一致的连接。在这篇论文中，我们提出了一种基于扩散模型的形态认知网络，名为SADIR，以解决这些问题。</li>
<li>methods: 我们的SADIR模型在不同于先前的方法，它不仅仅基于图像INTENSITY的空间相关性来进行三维重建，而是利用在训练数据中学习的形态约束来引导重建过程。为此，我们开发了一个共同学习网络，该网络同时学习一个变换模型中的平均形状。每个重建的图像都被视为变换模型中的一个偏shifted变换。</li>
<li>results: 我们在大脑和心脏Magnetic Resonance Imaging (MRI)图像上验证了我们的SADIR模型。实验结果表明，我们的方法在减少重建错误和更好地保留图像中对象的形状结构方面都有更高的表现。<details>
<summary>Abstract</summary>
3D image reconstruction from a limited number of 2D images has been a long-standing challenge in computer vision and image analysis. While deep learning-based approaches have achieved impressive performance in this area, existing deep networks often fail to effectively utilize the shape structures of objects presented in images. As a result, the topology of reconstructed objects may not be well preserved, leading to the presence of artifacts such as discontinuities, holes, or mismatched connections between different parts. In this paper, we propose a shape-aware network based on diffusion models for 3D image reconstruction, named SADIR, to address these issues. In contrast to previous methods that primarily rely on spatial correlations of image intensities for 3D reconstruction, our model leverages shape priors learned from the training data to guide the reconstruction process. To achieve this, we develop a joint learning network that simultaneously learns a mean shape under deformation models. Each reconstructed image is then considered as a deformed variant of the mean shape. We validate our model, SADIR, on both brain and cardiac magnetic resonance images (MRIs). Experimental results show that our method outperforms the baselines with lower reconstruction error and better preservation of the shape structure of objects within the images.
</details>
<details>
<summary>摘要</summary>
三维图像重建从有限数量的二维图像是计算机视觉和图像分析领域的长期挑战。而深度学习基于的方法已经在这个领域 achieved impressive performance, but existing deep networks often fail to effectively utilize the shape structures of objects presented in images. As a result, the topology of reconstructed objects may not be well preserved, leading to the presence of artifacts such as discontinuities, holes, or mismatched connections between different parts. In this paper, we propose a shape-aware network based on diffusion models for 3D image reconstruction, named SADIR, to address these issues. In contrast to previous methods that primarily rely on spatial correlations of image intensities for 3D reconstruction, our model leverages shape priors learned from the training data to guide the reconstruction process. To achieve this, we develop a joint learning network that simultaneously learns a mean shape under deformation models. Each reconstructed image is then considered as a deformed variant of the mean shape. We validate our model, SADIR, on both brain and cardiac magnetic resonance images (MRIs). Experimental results show that our method outperforms the baselines with lower reconstruction error and better preservation of the shape structure of objects within the images.
</details></li>
</ul>
<hr>
<h2 id="Expert-Uncertainty-and-Severity-Aware-Chest-X-Ray-Classification-by-Multi-Relationship-Graph-Learning"><a href="#Expert-Uncertainty-and-Severity-Aware-Chest-X-Ray-Classification-by-Multi-Relationship-Graph-Learning" class="headerlink" title="Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning"></a>Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03331">http://arxiv.org/abs/2309.03331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengliang Zhang, Xinyue Hu, Lin Gu, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>For: 该论文旨在提高胸部X射线检测中的疾病标签检测精度，通过考虑疾病严重程度和不确定性进行精度更高的疾病标签检测。* Methods: 该论文采用了规则基本方法与临床专家讨论的关键词来重新提取疾病标签，并使用多关系图学习方法和专家不确定性感知损失函数进一步提高检测结果的可读性。* Results: 该论文的实验结果表明，考虑疾病严重程度和不确定性的模型可以超越之前的状态艺术方法的性能。<details>
<summary>Abstract</summary>
Patients undergoing chest X-rays (CXR) often endure multiple lung diseases. When evaluating a patient's condition, due to the complex pathologies, subtle texture changes of different lung lesions in images, and patient condition differences, radiologists may make uncertain even when they have experienced long-term clinical training and professional guidance, which makes much noise in extracting disease labels based on CXR reports. In this paper, we re-extract disease labels from CXR reports to make them more realistic by considering disease severity and uncertainty in classification. Our contributions are as follows: 1. We re-extracted the disease labels with severity and uncertainty by a rule-based approach with keywords discussed with clinical experts. 2. To further improve the explainability of chest X-ray diagnosis, we designed a multi-relationship graph learning method with an expert uncertainty-aware loss function. 3. Our multi-relationship graph learning method can also interpret the disease classification results. Our experimental results show that models considering disease severity and uncertainty outperform previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
病人在胸部X射线检查（CXR）时经常患有多种肺病。在评估病人情况时，由于肺脏病变的复杂性、不同肺脏病变图像中的微小Texture变化以及病人状况的不同， radiologist可能会对病理标签进行不确定的识别，即使他们已经经历了长期的临床培训和专业指导。这会带来很多噪音在基于CXR报告中提取病理标签。在这篇论文中，我们重新提取了病理标签，以使其更加真实性。我们的贡献包括：1. 我们使用规则基于方法和与临床专家讨论的关键词来重新提取病理标签，并考虑疾病严重性和不确定性。2. 为了进一步提高胸部X射线诊断的可解释性，我们设计了一种多关系图学学习方法，并使用专家不确定性感知损失函数。3. 我们的多关系图学学习方法还可以解释肺病分类结果。我们的实验结果表明，考虑疾病严重性和不确定性的模型可以超越前一代方法的性能。
</details></li>
</ul>
<hr>
<h2 id="CoNeS-Conditional-neural-fields-with-shift-modulation-for-multi-sequence-MRI-translation"><a href="#CoNeS-Conditional-neural-fields-with-shift-modulation-for-multi-sequence-MRI-translation" class="headerlink" title="CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation"></a>CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03320">http://arxiv.org/abs/2309.03320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyjdswx/cones">https://github.com/cyjdswx/cones</a></li>
<li>paper_authors: Yunjie Chen, Marius Staring, Olaf M. Neve, Stephan R. Romeijn, Erik F. Hensen, Berit M. Verbist, Jelmer M. Wolterink, Qian Tao</li>
<li>for:  This paper is written for the purpose of proposing a new method for multi-sequence magnetic resonance imaging (MRI) translation, which can overcome the limitations of conventional convolutional neural network (CNN) models and provide high-quality synthesized images for clinical downstream tasks.</li>
<li>methods:  The proposed method, called Conditional Neural fields with Shift modulation (CoNeS), uses a multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel mapping, and leverages shift modulation with a learned latent code to condition the target images on the source image.</li>
<li>results:  The proposed method was tested on two datasets, BraTS 2018 and an in-house clinical dataset of vestibular schwannoma patients, and was found to outperform state-of-the-art methods for multi-sequence MRI translation both visually and quantitatively. Additionally, spectral analysis showed that CoNeS was able to overcome the spectral bias issue common in conventional CNN models.<details>
<summary>Abstract</summary>
Multi-sequence magnetic resonance imaging (MRI) has found wide applications in both modern clinical studies and deep learning research. However, in clinical practice, it frequently occurs that one or more of the MRI sequences are missing due to different image acquisition protocols or contrast agent contraindications of patients, limiting the utilization of deep learning models trained on multi-sequence data. One promising approach is to leverage generative models to synthesize the missing sequences, which can serve as a surrogate acquisition. State-of-the-art methods tackling this problem are based on convolutional neural networks (CNN) which usually suffer from spectral biases, resulting in poor reconstruction of high-frequency fine details. In this paper, we propose Conditional Neural fields with Shift modulation (CoNeS), a model that takes voxel coordinates as input and learns a representation of the target images for multi-sequence MRI translation. The proposed model uses a multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel mapping. Hence, each target image is represented as a neural field that is conditioned on the source image via shift modulation with a learned latent code. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular schwannoma patients showed that the proposed method outperformed state-of-the-art methods for multi-sequence MRI translation both visually and quantitatively. Moreover, we conducted spectral analysis, showing that CoNeS was able to overcome the spectral bias issue common in conventional CNN models. To further evaluate the usage of synthesized images in clinical downstream tasks, we tested a segmentation network using the synthesized images at inference.
</details>
<details>
<summary>摘要</summary>
多序列磁共振成像（MRI）在现代临床研究和深度学习领域都有广泛的应用。然而，在临床实践中，有时一些MRI序列会缺失，这限制了由深度学习模型在多序列数据上训练的使用。一种有前途的方法是使用生成模型生成缺失的序列，这可以作为代用性的获得。现状的方法通常是使用卷积神经网络（CNN），它们通常会受到频率偏好，导致高频精细 Details的重建不佳。在这篇论文中，我们提出了基于 Conditional Neural fields with Shift modulation（CoNeS）模型，它将 voxel 坐标作为输入，学习target images的多序列 MRI 翻译。我们的模型使用多层感知神经网络（MLP） instead of CNN 作为像素到像素映射的解码器。因此，每个target image都被表示为一个 conditioned on the source image的神经场，通过 shift modulation with a learned latent code 来进行映射。我们在 BraTS 2018 和一个内部的临床数据集中进行了实验，结果显示，我们的方法超过了当前state-of-the-art方法在多序列 MRI 翻译中 both visually and quantitatively。此外，我们进行了spectral analysis，显示 CoNeS 能够超越 convential CNN 模型中的频率偏好问题。为了进一步评估生成图像在临床下渠道任务中的使用，我们在推理阶段使用生成图像进行 segmentation 网络的测试。
</details></li>
</ul>
<hr>
<h2 id="3D-Transformer-based-on-deformable-patch-location-for-differential-diagnosis-between-Alzheimer’s-disease-and-Frontotemporal-dementia"><a href="#3D-Transformer-based-on-deformable-patch-location-for-differential-diagnosis-between-Alzheimer’s-disease-and-Frontotemporal-dementia" class="headerlink" title="3D Transformer based on deformable patch location for differential diagnosis between Alzheimer’s disease and Frontotemporal dementia"></a>3D Transformer based on deformable patch location for differential diagnosis between Alzheimer’s disease and Frontotemporal dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03183">http://arxiv.org/abs/2309.03183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy-Dung Nguyen, Michaël Clément, Boris Mansencal, Pierrick Coupé</li>
<li>for: 该研究旨在提高阿尔ц海默病和前rontemporal dementia的多类差异诊断。</li>
<li>methods: 该研究使用了一种基于transformer的3D扩展模型，并提出了一种有效的数据增强技术来解决数据稀缺问题。</li>
<li>results: 实验结果表明，提出的方法可以与现有的状态艺术方法竞争，并且可以显示出每种疾病的诊断关键区域。Here’s the summary in English for reference:</li>
<li>for: The study aims to improve the multi-class differential diagnosis of Alzheimer’s disease and Frontotemporal dementia.</li>
<li>methods: The study uses a novel 3D transformer-based architecture with a deformable patch location module, and proposes an efficient combination of data augmentation techniques to overcome data scarcity.</li>
<li>results: Experimental results show that the proposed approach is competitive with state-of-the-art methods and can visualize the most relevant brain regions used for diagnosis.<details>
<summary>Abstract</summary>
Alzheimer's disease and Frontotemporal dementia are common types of neurodegenerative disorders that present overlapping clinical symptoms, making their differential diagnosis very challenging. Numerous efforts have been done for the diagnosis of each disease but the problem of multi-class differential diagnosis has not been actively explored. In recent years, transformer-based models have demonstrated remarkable success in various computer vision tasks. However, their use in disease diagnostic is uncommon due to the limited amount of 3D medical data given the large size of such models. In this paper, we present a novel 3D transformer-based architecture using a deformable patch location module to improve the differential diagnosis of Alzheimer's disease and Frontotemporal dementia. Moreover, to overcome the problem of data scarcity, we propose an efficient combination of various data augmentation techniques, adapted for training transformer-based models on 3D structural magnetic resonance imaging data. Finally, we propose to combine our transformer-based model with a traditional machine learning model using brain structure volumes to better exploit the available data. Our experiments demonstrate the effectiveness of the proposed approach, showing competitive results compared to state-of-the-art methods. Moreover, the deformable patch locations can be visualized, revealing the most relevant brain regions used to establish the diagnosis of each disease.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病和前rontemporal dementia 是常见的神经退化疾病，它们在临床表现上存在 overlap，这使得它们的分类诊断非常困难。各种努力已经在每种疾病的诊断方面做出了很多，但多类分类诊断问题尚未得到了active exploration。在最近的几年中，transformer-based模型在计算机视觉任务中表现出了惊人的成功。然而，它们在疾病诊断中的使用不寻常，主要是因为3D医疗数据的有限性，这些模型的大小。在本文中，我们提出了一种基于transformer的3D扩展模型，使用可变的质心位置模块来提高阿尔茨海默病和前rontemporal dementia的分类诊断。此外，为了解决数据缺乏问题，我们提议了一种有效的数据扩展技术组合，适用于在3D结构磁共振成像数据上训练transformer-based模型。最后，我们提议将我们的transformer-based模型与传统的机器学习模型相结合，以更好地利用可用的数据。我们的实验结果表明，我们的方法具有竞争力，与当前的方法相比，并且可以visualize可变的质心位置，表明在诊断每种疾病时，哪些脑区最 relevante。
</details></li>
</ul>
<hr>
<h2 id="The-Secrets-of-Non-Blind-Poisson-Deconvolution"><a href="#The-Secrets-of-Non-Blind-Poisson-Deconvolution" class="headerlink" title="The Secrets of Non-Blind Poisson Deconvolution"></a>The Secrets of Non-Blind Poisson Deconvolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03105">http://arxiv.org/abs/2309.03105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhiram Gnanasambandam, Yash Sanghvi, Stanley H. Chan</li>
<li>for: 非干扰式图像推整，适用于对于实际摄像机读取器所获得的图像进行推整，以提高图像质量。</li>
<li>methods: 本文使用了系统性分析来检查目前Literature中的波兰non-blind推整算法，包括传统和深度学习方法。我们组装了五个”秘密”，以帮助设计算法。</li>
<li>results: 我们建立了一个证明型方法， combinining the five secrets，并发现这个新方法可以与一些最新的方法相比，而且超过一些较老的方法。<details>
<summary>Abstract</summary>
Non-blind image deconvolution has been studied for several decades but most of the existing work focuses on blur instead of noise. In photon-limited conditions, however, the excessive amount of shot noise makes traditional deconvolution algorithms fail. In searching for reasons why these methods fail, we present a systematic analysis of the Poisson non-blind deconvolution algorithms reported in the literature, covering both classical and deep learning methods. We compile a list of five "secrets" highlighting the do's and don'ts when designing algorithms. Based on this analysis, we build a proof-of-concept method by combining the five secrets. We find that the new method performs on par with some of the latest methods while outperforming some older ones.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:非干擦图像去推算已经被研究数十年，但大多数现有工作都集中在模糊而不是噪声。在光子限制条件下，然而，过度的射频噪声使传统的推算算法失败。在寻找这些方法失败的原因时，我们提供了系统性的分析，涵盖了古典和深度学习方法。我们编辑了五个“秘密”，描述了设计算法时的做法和不做法。基于这种分析，我们构建了一个证明方法，并发现其与一些最新的方法性能相似，而对一些较老的方法性能更高。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-level-rain-image-generative-model-based-on-GAN"><a href="#Hierarchical-level-rain-image-generative-model-based-on-GAN" class="headerlink" title="Hierarchical-level rain image generative model based on GAN"></a>Hierarchical-level rain image generative model based on GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02964">http://arxiv.org/abs/2309.02964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyuan Liu, Tong Jia, Xingyu Xing, Jianfeng Wu, Junyi Chen</li>
<li>for:  This paper aims to improve the performance of visual perception algorithms in autonomous vehicles under various weather conditions, specifically rain, by generating realistic rain images using a hierarchical-level generative model called RCCycleGAN.</li>
<li>methods:  The RCCycleGAN model is based on a generative adversarial network (GAN) and uses a hierarchical structure to generate images of light, medium, and heavy rain. The model is trained using natural rain images and optimized with a specific training strategy to alleviate the problem of mode collapse.</li>
<li>results:  Compared to two baseline models, CycleGAN and DerainCycleGAN, RCCycleGAN achieves improved peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) on a test dataset. The ablation experiments also demonstrate the effectiveness of the model tuning.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高自动驾驶车辆在不同天气条件下视觉识别算法的性能，特别是在雨天下，通过使用层次结构的生成模型RCCycleGAN来生成真实的雨图像。</li>
<li>methods: RCCycleGAN模型基于生成对抗网络（GAN），使用层次结构生成雨图像的不同强度，并通过使用自然雨图像进行训练和优化，以解决模式混合问题。</li>
<li>results: 相比两个基eline模型CycleGAN和DerainCycleGAN，RCCycleGAN在测试数据集上的峰值信号噪声比（PSNR）和结构相似度（SSIM）都得到了提高，并通过ablation实验证明模型调整的效果。<details>
<summary>Abstract</summary>
Autonomous vehicles are exposed to various weather during operation, which is likely to trigger the performance limitations of the perception system, leading to the safety of the intended functionality (SOTIF) problems. To efficiently generate data for testing the performance of visual perception algorithms under various weather conditions, a hierarchical-level rain image generative model, rain conditional CycleGAN (RCCycleGAN), is constructed. RCCycleGAN is based on the generative adversarial network (GAN) and can generate images of light, medium, and heavy rain. Different rain intensities are introduced as labels in conditional GAN (CGAN). Meanwhile, the model structure is optimized and the training strategy is adjusted to alleviate the problem of mode collapse. In addition, natural rain images of different intensities are collected and processed for model training and validation. Compared with the two baseline models, CycleGAN and DerainCycleGAN, the peak signal-to-noise ratio (PSNR) of RCCycleGAN on the test dataset is improved by 2.58 dB and 0.74 dB, and the structural similarity (SSIM) is improved by 18% and 8%, respectively. The ablation experiments are also carried out to validate the effectiveness of the model tuning.
</details>
<details>
<summary>摘要</summary>
自适应汽车在运行过程中曝露于不同的天气条件，这可能会导致感知系统的性能限制，从而影响安全功能（SOTIF）的问题。为了有效地生成测试感知算法在不同天气条件下的性能数据，我们构建了一个层次结构的雨图生成模型——雨征 conditional CycleGAN（RCCycleGAN）。RCCycleGAN基于生成对抗网络（GAN），可以生成不同雨强度的雨图。在CGAN中，不同雨强度被用作标签。此外，模型结构被优化，并调整了训练策略，以解决模式混合问题。此外，我们还收集了不同雨强度的自然雨图，用于模型训练和验证。相比基eline模型CycleGAN和DerainCycleGAN，RCCycleGAN在测试集上的峰值信号噪比（PSNR）提高了2.58 dB和0.74 dB，同时 Structural Similarity（SSIM）提高了18%和8%。我们还进行了减少实验，以验证模型调整的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Non-Invasive-Interpretable-NAFLD-Diagnostic-Method-Combining-TCM-Tongue-Features"><a href="#A-Non-Invasive-Interpretable-NAFLD-Diagnostic-Method-Combining-TCM-Tongue-Features" class="headerlink" title="A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Features"></a>A Non-Invasive Interpretable NAFLD Diagnostic Method Combining TCM Tongue Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02959">http://arxiv.org/abs/2309.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cshan-github/selectornet">https://github.com/cshan-github/selectornet</a></li>
<li>paper_authors: Shan Cao, Qunsheng Ruan, Qingfeng Wu</li>
<li>for: 这个研究是为了提出一种非侵入性和可解释的非酒精肝病诊断方法，用户只需提供年龄、性别、身高、体重、腰围和腊围等几个数据，并且使用语言特征来进行诊断。</li>
<li>methods: 这个方法使用了一个名为SelectorNet的选择网络，该网络结合了注意力机制和特征选择机制，可以自动学习选择重要的特征。</li>
<li>results: 实验结果显示，提案的方法可以使用非侵入性数据 achieves an accuracy of 77.22%，并且提供了吸引人的解释矩阵。<details>
<summary>Abstract</summary>
Non-alcoholic fatty liver disease (NAFLD) is a clinicopathological syndrome characterized by hepatic steatosis resulting from the exclusion of alcohol and other identifiable liver-damaging factors. It has emerged as a leading cause of chronic liver disease worldwide. Currently, the conventional methods for NAFLD detection are expensive and not suitable for users to perform daily diagnostics. To address this issue, this study proposes a non-invasive and interpretable NAFLD diagnostic method, the required user-provided indicators are only Gender, Age, Height, Weight, Waist Circumference, Hip Circumference, and tongue image. This method involves merging patients' physiological indicators with tongue features, which are then input into a fusion network named SelectorNet. SelectorNet combines attention mechanisms with feature selection mechanisms, enabling it to autonomously learn the ability to select important features. The experimental results show that the proposed method achieves an accuracy of 77.22\% using only non-invasive data, and it also provides compelling interpretability matrices. This study contributes to the early diagnosis of NAFLD and the intelligent advancement of TCM tongue diagnosis. The project in this paper is available at: https://github.com/cshan-github/SelectorNet.
</details>
<details>
<summary>摘要</summary>
非酒精肝病 (NAFLD) 是一种临床生物学特征的疾病，表现为肝脂肪肿 formation，不含酒精和其他可识别的肝脏损害因素。它在全球范围内成为慢性肝病的主要原因。目前，NAFLD 的检测方法是成本高且不适合用户每天进行检测。为解决这个问题，本研究提出了一种非侵入性的 NAFLD 检测方法，需要用户提供的指标只有性别、年龄、身高、体重、腰围和股围，以及舌头图像。本方法将患者的生理指标与舌头特征合并，然后将其输入到选择器网络（SelectorNet）中。SelectorNet 结合了注意力机制和特征选择机制，可以自动学习选择重要的特征。实验结果表明，提议的方法在只使用非侵入数据时实现了77.22%的准确率，并且还提供了吸引人的解释矩阵。本研究对 NAFLD 的早期诊断和中医舌头诊断的智能进步做出了贡献。项目在以下链接可以获取：https://github.com/cshan-github/SelectorNet。
</details></li>
</ul>
<hr>
<h2 id="Bandwidth-efficient-Inference-for-Neural-Image-Compression"><a href="#Bandwidth-efficient-Inference-for-Neural-Image-Compression" class="headerlink" title="Bandwidth-efficient Inference for Neural Image Compression"></a>Bandwidth-efficient Inference for Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02855">http://arxiv.org/abs/2309.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanzhi Yin, Tongda Xu, Yongsheng Liang, Yuanyuan Wang, Yanghao Li, Yan Wang, Jingjing Liu</li>
<li>for: 这篇论文主要针对移动和边缘设备上实现神经网络推断时的对话带宽和电力限制问题。</li>
<li>methods: 本论文提出了一个终端对应、可微分的对话带宽有效神经推断方法，包括使用对应压缩、量化和Entropy coding。具体来说，使用了对应压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩、量化和Entropy coding的变数压缩。</li>
<li>results: 本论文的实验结果显示，可以透过优化现有的模型量化方法，实现19倍的对话带宽减少，同时节省6.21倍的能源。<details>
<summary>Abstract</summary>
With neural networks growing deeper and feature maps growing larger, limited communication bandwidth with external memory (or DRAM) and power constraints become a bottleneck in implementing network inference on mobile and edge devices. In this paper, we propose an end-to-end differentiable bandwidth efficient neural inference method with the activation compressed by neural data compression method. Specifically, we propose a transform-quantization-entropy coding pipeline for activation compression with symmetric exponential Golomb coding and a data-dependent Gaussian entropy model for arithmetic coding. Optimized with existing model quantization methods, low-level task of image compression can achieve up to 19x bandwidth reduction with 6.21x energy saving.
</details>
<details>
<summary>摘要</summary>
随着神经网络的深度和特征地图的大小增加，在移动和边缘设备上进行网络推理时限制了外部内存（或DRAM）的通信带宽和能源限制变成瓶颈。在这篇论文中，我们提出了一种终端到终端可微 differentiable带宽高效的神经推理方法，其中活动被压缩使用神经数据压缩方法。具体来说，我们提出了一个转换-量化-Entropy编码管道，用于活动压缩，并使用对称的加权几何压缩和数据依赖的 Gaussian Entropy 模型进行算术编码。通过与现有模型归一化方法结合优化，可以实现到 19 倍的带宽减少和 6.21 倍的能源浪费 reduction。
</details></li>
</ul>
<hr>
<h2 id="A-flexible-and-accurate-total-variation-and-cascaded-denoisers-based-image-reconstruction-algorithm-for-hyperspectrally-compressed-ultrafast-photography"><a href="#A-flexible-and-accurate-total-variation-and-cascaded-denoisers-based-image-reconstruction-algorithm-for-hyperspectrally-compressed-ultrafast-photography" class="headerlink" title="A flexible and accurate total variation and cascaded denoisers-based image reconstruction algorithm for hyperspectrally compressed ultrafast photography"></a>A flexible and accurate total variation and cascaded denoisers-based image reconstruction algorithm for hyperspectrally compressed ultrafast photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02835">http://arxiv.org/abs/2309.02835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Guo, Jiali Yao, Dalong Qi, Pengpeng Ding, Chengzhi Jin, Ning Xu, Zhiling Zhang, Yunhua Yao, Lianzhong Deng, Zhiyong Wang, Zhenrong Sun, Shian Zhang</li>
<li>for: 该研究旨在开发一种高级推理图像重建算法，以提高高速激光图像捕捉器（HCUP）中的图像重建质量。</li>
<li>methods: 该算法基于全量变量（TV）和堆式降噪器（CD），通过在多进程替换方法中的迭代执行，以保持图像平滑性而利用深度降噪网络获取更多约束，解决了HCUP中的通用稀热表示问题。</li>
<li>results: 实验和仿真结果表明，提出的TV-CD算法可以有效提高HCUP中图像重建精度和质量，并推动HCUP在捕捉高维复杂物理、化学和生物ultrafast光学场景中的实际应用。<details>
<summary>Abstract</summary>
Hyperspectrally compressed ultrafast photography (HCUP) based on compressed sensing and the time- and spectrum-to-space mappings can simultaneously realize the temporal and spectral imaging of non-repeatable or difficult-to-repeat transient events passively in a single exposure. It possesses an incredibly high frame rate of tens of trillions of frames per second and a sequence depth of several hundred, and plays a revolutionary role in single-shot ultrafast optical imaging. However, due to the ultra-high data compression ratio induced by the extremely large sequence depth as well as the limited fidelities of traditional reconstruction algorithms over the reconstruction process, HCUP suffers from a poor image reconstruction quality and fails to capture fine structures in complex transient scenes. To overcome these restrictions, we propose a flexible image reconstruction algorithm based on the total variation (TV) and cascaded denoisers (CD) for HCUP, named the TV-CD algorithm. It applies the TV denoising model cascaded with several advanced deep learning-based denoising models in the iterative plug-and-play alternating direction method of multipliers framework, which can preserve the image smoothness while utilizing the deep denoising networks to obtain more priori, and thus solving the common sparsity representation problem in local similarity and motion compensation. Both simulation and experimental results show that the proposed TV-CD algorithm can effectively improve the image reconstruction accuracy and quality of HCUP, and further promote the practical applications of HCUP in capturing high-dimensional complex physical, chemical and biological ultrafast optical scenes.
</details>
<details>
<summary>摘要</summary>
高级色彩压缩超快摄影（HCUP）基于压缩感知和时间-спектр空间映射，可同时实现非重复或困难重复的脉冲事件的时间和频谱成像，在单张拍摄下完成。它具有惊人的帧率上亿亿帧/秒和序列深度上百个，扮演了单板超快光学成像革命性角色。然而，由于超高压缩比引起的极大序列深度以及传统重建算法的限制，HCUP受到低图像重建质量和复杂场景中细节损失的限制。为了突破这些限制，我们提出了基于总变量（TV）和堆叠滤波器（CD）的灵活图像重建算法，称为TV-CD算法。它在融合了多个高级深度学习基础模型的迭代方向幂法框架中，应用TV滤波器先后堆叠多个高级深度学习基础模型，保持图像的平滑性，同时利用深度滤波器获取更多的前提，解决了常见的简并表示问题。实验和 simulate 结果表明，提议的 TV-CD 算法可以有效提高 HCUP 图像重建质量和精度，并且推动 HCUP 在高维复物理、化学和生物ultrafast光学场景中的应用。
</details></li>
</ul>
<hr>
<h2 id="Improving-Image-Classification-of-Knee-Radiographs-An-Automated-Image-Labeling-Approach"><a href="#Improving-Image-Classification-of-Knee-Radiographs-An-Automated-Image-Labeling-Approach" class="headerlink" title="Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach"></a>Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02681">http://arxiv.org/abs/2309.02681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jikai Zhang, Carlos Santos, Christine Park, Maciej Mazurowski, Roy Colglazier<br>for: 这个研究的目的是为了提高骨科X线成像诊断的图像分类模型，以便更好地识别正常骨科X线图像和异常骨科X线图像或过去的骨 JOINT 代替手术。methods: 这个研究使用了自动标签方法，以将大量未标注的骨科X线图像标签为正常或异常。这个自动标签方法首先在小型标注数据集上训练，然后将其扩展到更大的未标注数据集上。results: 这个研究发现，使用自动标签方法可以将骨科X线图像分类模型的性能提高，特别是在正常骨科X线图像和异常骨科X线图像之间的区别。在7,382名病人的数据集上训练和637名病人的验证数据集上验证，这个图像分类模型的性能在不同的类别中都有所提高。<details>
<summary>Abstract</summary>
Large numbers of radiographic images are available in knee radiology practices which could be used for training of deep learning models for diagnosis of knee abnormalities. However, those images do not typically contain readily available labels due to limitations of human annotations. The purpose of our study was to develop an automated labeling approach that improves the image classification model to distinguish normal knee images from those with abnormalities or prior arthroplasty. The automated labeler was trained on a small set of labeled data to automatically label a much larger set of unlabeled data, further improving the image classification performance for knee radiographic diagnosis. We developed our approach using 7,382 patients and validated it on a separate set of 637 patients. The final image classification model, trained using both manually labeled and pseudo-labeled data, had the higher weighted average AUC (WAUC: 0.903) value and higher AUC-ROC values among all classes (normal AUC-ROC: 0.894; abnormal AUC-ROC: 0.896, arthroplasty AUC-ROC: 0.990) compared to the baseline model (WAUC=0.857; normal AUC-ROC: 0.842; abnormal AUC-ROC: 0.848, arthroplasty AUC-ROC: 0.987), trained using only manually labeled data. DeLong tests show that the improvement is significant on normal (p-value<0.002) and abnormal (p-value<0.001) images. Our findings demonstrated that the proposed automated labeling approach significantly improves the performance of image classification for radiographic knee diagnosis, allowing for facilitating patient care and curation of large knee datasets.
</details>
<details>
<summary>摘要</summary>
大量的骨灰像可以在膝关节 radiology 实践中使用，以便用于深度学习模型的膝关节疾病诊断。然而，这些图像通常没有Ready available的标签，因为人类标注的限制。我们的研究旨在开发一种自动标注方法，以提高膝关节图像分类模型的精度。我们使用了7,382名患者和637名患者的分 sepate set来验证我们的方法。我们的方法使得图像分类模型可以更好地分辨正常膝关节图像和疾病图像或者过去arthroplasty。我们使用了 manually labeled和pseudo-labeled数据来训练我们的图像分类模型，其中weighted average AUC（WAUC）值为0.903，而且在所有类型的AUC-ROC值中都高于基线模型（WAUC=0.857）。DeLong测试表明，在正常（p-value<0.002）和疾病（p-value<0.001）图像上，我们的提高是 statistically significant。我们的发现表明，我们提posed的自动标注方法可以显著提高膝关节图像分类的性能，使得patient care和大量膝关节数据的护理成为可能。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Attention-Guidance-for-Whole-Slide-Vulvovaginal-Candidiasis-Screening"><a href="#Progressive-Attention-Guidance-for-Whole-Slide-Vulvovaginal-Candidiasis-Screening" class="headerlink" title="Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening"></a>Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02670">http://arxiv.org/abs/2309.02670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cjdbehumble/miccai2023-vvc-screening">https://github.com/cjdbehumble/miccai2023-vvc-screening</a></li>
<li>paper_authors: Jiangdong Cai, Honglin Xiong, Maosong Cao, Luyan Liu, Lichi Zhang, Qian Wang</li>
<li>for: 这项研究旨在开发一种自动检测和识别绒毛菌（VVC）感染的整片扫描图像（WSI）分类方法，以帮助缓解绒毛菌感染的巨大危机和防控措施。</li>
<li>methods: 这种方法首先使用一个预训练的检测模型作为先前的指导，然后设计了跳过自我注意模块来细化绒毛菌的注意力。最后，使用一种对比学习方法来缓解由扫描图像的风格差引起的过拟合，并降低注意力的扩散到假阳性区域。</li>
<li>results: 实验结果表明，我们的框架可以达到状态级表现，并且在绒毛菌感染检测中具有较高的准确率和特异性。代码和示例数据可以在<a target="_blank" rel="noopener" href="https://github.com/cjdbehumble/MICCAI2023-VVC-Screening%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/cjdbehumble/MICCAI2023-VVC-Screening上下载。</a><details>
<summary>Abstract</summary>
Vulvovaginal candidiasis (VVC) is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime. It will lead to several symptoms including pruritus, vaginal soreness, and so on. Automatic whole slide image (WSI) classification is highly demanded, for the huge burden of disease control and prevention. However, the WSI-based computer-aided VCC screening method is still vacant due to the scarce labeled data and unique properties of candida. Candida in WSI is challenging to be captured by conventional classification models due to its distinctive elongated shape, the small proportion of their spatial distribution, and the style gap from WSIs. To make the model focus on the candida easier, we propose an attention-guided method, which can obtain a robust diagnosis classification model. Specifically, we first use a pre-trained detection model as prior instruction to initialize the classification model. Then we design a Skip Self-Attention module to refine the attention onto the fined-grained features of candida. Finally, we use a contrastive learning method to alleviate the overfitting caused by the style gap of WSIs and suppress the attention to false positive regions. Our experimental results demonstrate that our framework achieves state-of-the-art performance. Code and example data are available at https://github.com/cjdbehumble/MICCAI2023-VVC-Screening.
</details>
<details>
<summary>摘要</summary>
“ vulvovaginal candidiasis (VVC) 是人类最常见的发炎菌感染，估计会影响约75%的所有女性至少一次生活中。 它会导致多种症状，包括痒痛、阴道疼痛等。 automatic whole slide image (WSI) 分类是很受需求的，因为疾病控制和预防的负担非常大。 但是， WSI 基于的计算机支持 VCC 检测方法仍然无人，因为罕有 Labelled 数据和发炎菌的特有性。 发炎菌在 WSI 中具有特殊的长条形和小比例的问题，以及 WSIs 的样本距离。 为了让模型更容易捕捉发炎菌，我们提出了注意力导向的方法。 我们首先使用预训的检测模型作为专案初始化的参考模型。 然后，我们设计了 Skip Self-Attention 模块，以精确地调整注意力到发炎菌的细节特征。 最后，我们使用了对抗学习方法，以解决由 WSIs 的样本距离导致的过滤过问题。 我们的实验结果显示，我们的框架可以实现州际表现。 Code 和示例数据可以在 https://github.com/cjdbehumble/MICCAI2023-VVC-Screening 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Review-of-photoacoustic-imaging-plus-X"><a href="#Review-of-photoacoustic-imaging-plus-X" class="headerlink" title="Review of photoacoustic imaging plus X"></a>Review of photoacoustic imaging plus X</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02638">http://arxiv.org/abs/2309.02638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daohuai Jiang, Luyao Zhu, Shangqing Tong, Yuting Shen, Feng Gao, Fei Gao</li>
<li>for: 本文提供了一种新的医疗技术综述，即 photoacoustic imaging（PAI） Plus X，其中X表示其他多种先进技术，包括但不限于PAI Plus treatment、PAI Plus 新电路设计、PAI Plus 精准位别系统、PAI Plus 快速扫描系统、PAI Plus 新式 ultrasound 探测器、PAI Plus 高级 Laser 源、PAI Plus 深度学习、PAI Plus 其他成像模式。</li>
<li>methods: 本文详细介绍了 PAI Plus X 技术的当前状况、技术优势和应用前景，报道了近三年来的研究进展。</li>
<li>results: 本文综述了 PAI Plus X 技术的挑战和未来发展前景。<details>
<summary>Abstract</summary>
Photoacoustic imaging (PAI) is a novel modality in biomedical imaging technology that combines the rich optical contrast with the deep penetration of ultrasound. To date, PAI technology has found applications in various biomedical fields. In this review, we present an overview of the emerging research frontiers on PAI plus other advanced technologies, named as PAI plus X, which includes but not limited to PAI plus treatment, PAI plus new circuits design, PAI plus accurate positioning system, PAI plus fast scanning systems, PAI plus novel ultrasound sensors, PAI plus advanced laser sources, PAI plus deep learning, and PAI plus other imaging modalities. We will discuss each technology's current state, technical advantages, and prospects for application, reported mostly in recent three years. Lastly, we discuss and summarize the challenges and potential future work in PAI plus X area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/eess.IV_2023_09_06/" data-id="clmjn91qw00hn0j881td2buuf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/06/cs.LG_2023_09_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-06
        
      </div>
    </a>
  
  
    <a href="/2023/09/05/cs.SD_2023_09_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-05</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
