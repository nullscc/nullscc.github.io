
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03386 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yangwu001&#x2F;putree paper_authors:">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-06">
<meta property="og:url" content="https://nullscc.github.io/2023/09/06/cs.LG_2023_09_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03386 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yangwu001&#x2F;putree paper_authors:">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-06T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.063Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/cs.LG_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T10:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Community-Based-Hierarchical-Positive-Unlabeled-PU-Model-Fusion-for-Chronic-Disease-Prediction"><a href="#Community-Based-Hierarchical-Positive-Unlabeled-PU-Model-Fusion-for-Chronic-Disease-Prediction" class="headerlink" title="Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction"></a>Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03386">http://arxiv.org/abs/2309.03386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangwu001/putree">https://github.com/yangwu001/putree</a></li>
<li>paper_authors: Yang Wu, Xurui Li, Xuhong Zhang, Yangyang Kang, Changlong Sun, Xiaozhong Liu</li>
<li>for: 这个研究旨在解决 chronic disease 预测问题，使用 positive-unlabeled 学习方法。</li>
<li>methods: 我们提出了一个 novel Positive-Unlabeled Learning Tree（PUtree）算法，考虑了不同的社区（例如年龄或收入），以提高疾病预测的准确性。我们还提出了一个 hierarchical PU 模型建立方法，以及一个 adversarial PU 风险估计方法，以捕捉阶层 PU 关系。</li>
<li>results: 我们在两个 bencmark 和一个新的 diabetes 预测数据集上证明了 PUtree 的超越性表现，以及其变型版本的表现。<details>
<summary>Abstract</summary>
Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individual communities. Additionally, the proposed approach includes an adversarial PU risk estimator to capture hierarchical PU-relationships, and a model fusion network that integrates data from each tree path, resulting in robust binary classification results. We demonstrate the superior performance of PUtree as well as its variants on two benchmarks and a new diabetes-prediction dataset.
</details>
<details>
<summary>摘要</summary>
Positive-Unlabeled（PU）学习是 binary 分类问题中的一个挑战，其中有庞大量的无标签数据以及一小量的正样本，可以用来解决慢性病creening问题。当前的PU学习方法已经导致了不同的风险估计器的开发，但它们忽视了不同人口群体之间的差异。为了解决这个问题，我们提出了一种新的Positive-Unlabeled学习树（PUtree）算法。PUtree 设计了考虑不同年龄或收入水平的社区，在慢性病预测任务中进行分类。我们提出了一种新的二元决策方法，即层次建立社区基于PU模型的 Hierarchical PU 模型，并将其拼接成 robust 的二元分类结果。此外，我们还提出了一种防御PU关系的风险估计器，以及一种数据 fusion 网络，以确保模型在每个树路径上的训练。我们在两个标准benchmark和一个新的 диабе特病预测 dataset 上展示了 PUtree 的superior 性能和其变体。
</details></li>
</ul>
<hr>
<h2 id="ViewMix-Augmentation-for-Robust-Representation-in-Self-Supervised-Learning"><a href="#ViewMix-Augmentation-for-Robust-Representation-in-Self-Supervised-Learning" class="headerlink" title="ViewMix: Augmentation for Robust Representation in Self-Supervised Learning"></a>ViewMix: Augmentation for Robust Representation in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03360">http://arxiv.org/abs/2309.03360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjon Das, Xin Zhong</li>
<li>for: 本研究旨在提高自动学习方法的表征学习能力。</li>
<li>methods: 本研究使用的是基于共同嵌入架构的自动学习方法，并采用了ViewMix增强策略来提高表征学习能力。</li>
<li>results: 研究表明，采用ViewMix增强策略可以提高自动学习方法的表征学习能力和 robustness。<details>
<summary>Abstract</summary>
Joint Embedding Architecture-based self-supervised learning methods have attributed the composition of data augmentations as a crucial factor for their strong representation learning capabilities. While regional dropout strategies have proven to guide models to focus on lesser indicative parts of the objects in supervised methods, it hasn't been adopted by self-supervised methods for generating positive pairs. This is because the regional dropout methods are not suitable for the input sampling process of the self-supervised methodology. Whereas dropping informative pixels from the positive pairs can result in inefficient training, replacing patches of a specific object with a different one can steer the model from maximizing the agreement between different positive pairs. Moreover, joint embedding representation learning methods have not made robustness their primary training outcome. To this end, we propose the ViewMix augmentation policy, specially designed for self-supervised learning, upon generating different views of the same image, patches are cut and pasted from one view to another. By leveraging the different views created by this augmentation strategy, multiple joint embedding-based self-supervised methodologies obtained better localization capability and consistently outperformed their corresponding baseline methods. It is also demonstrated that incorporating ViewMix augmentation policy promotes robustness of the representations in the state-of-the-art methods. Furthermore, our experimentation and analysis of compute times suggest that ViewMix augmentation doesn't introduce any additional overhead compared to other counterparts.
</details>
<details>
<summary>摘要</summary>
joint embedding architecture-based self-supervised learning方法中，数据增强的组合被认为是关键因素，使得模型具有强的表示学习能力。而区域排除策略已经在指导模型专注于对象中的较少指标部分，但它们没有被自动学习方法采用，因为这些方法不适合自动学习方法的输入采样过程。因为排除有用信息的像素会导致训练不efficient，而将对象中的特定部分替换为不同的像素会使模型尝试最大化不同的正方向对的协调。此外，joint embedding表示学习方法没有做到坚持robustness作为主要训练目标。因此，我们提出了ViewMix增强策略，特地设计为自动学习。在生成不同视图的同时，patches从一个视图中被剪辑并粘贴到另一个视图中。通过利用不同的视图，generated by这种增强策略，我们实现了多种joint embedding-based自动学习方法的更好的局部化能力，并且一直高于对应的基eline方法。此外，我们的实验和分析表明，ViewMix增强策略不会增加计算时间的额外负担。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-linear-interpolators-The-role-of-ensembling"><a href="#Ensemble-linear-interpolators-The-role-of-ensembling" class="headerlink" title="Ensemble linear interpolators: The role of ensembling"></a>Ensemble linear interpolators: The role of ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03354">http://arxiv.org/abs/2309.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqi Wu, Qiang Sun</li>
<li>for: 本文研究如何 ensemble 稳定化 interpolator，以提高 interpolator 的泛化性能。</li>
<li>methods: 本文使用 bagged linear interpolator 和 multiplier-bootstrap-based bagged least square estimator，以及类 bootstrap 和 Bernoulli bootstrap。</li>
<li>results: 研究发现，bagging 可以有效地减少 interpolator 的偏差，从而实现 bounded 的 out-of-sample prediction risk。此外， authors 还发现 bagging  acted as an implicit regularization，并与其 explicitly regularized counterpart 相当。<details>
<summary>Abstract</summary>
Interpolators are unstable. For example, the mininum $\ell_2$ norm least square interpolator exhibits unbounded test errors when dealing with noisy data. In this paper, we study how ensemble stabilizes and thus improves the generalization performance, measured by the out-of-sample prediction risk, of an individual interpolator. We focus on bagged linear interpolators, as bagging is a popular randomization-based ensemble method that can be implemented in parallel. We introduce the multiplier-bootstrap-based bagged least square estimator, which can then be formulated as an average of the sketched least square estimators. The proposed multiplier bootstrap encompasses the classical bootstrap with replacement as a special case, along with a more intriguing variant which we call the Bernoulli bootstrap.   Focusing on the proportional regime where the sample size scales proportionally with the feature dimensionality, we investigate the out-of-sample prediction risks of the sketched and bagged least square estimators in both underparametrized and overparameterized regimes. Our results reveal the statistical roles of sketching and bagging. In particular, sketching modifies the aspect ratio and shifts the interpolation threshold of the minimum $\ell_2$ norm estimator. However, the risk of the sketched estimator continues to be unbounded around the interpolation threshold due to excessive variance. In stark contrast, bagging effectively mitigates this variance, leading to a bounded limiting out-of-sample prediction risk. To further understand this stability improvement property, we establish that bagging acts as a form of implicit regularization, substantiated by the equivalence of the bagged estimator with its explicitly regularized counterpart. We also discuss several extensions.
</details>
<details>
<summary>摘要</summary>
interpolators 不稳定。例如，最小二乘法最小值 interpolator 对噪音数据处理时会表现出无限大的测试错误。在这篇论文中，我们研究如何 ensemble 使 interpolator 更加稳定，从而提高个体 interpolator 的泛化性能， measured by out-of-sample prediction risk。我们关注 bagged linear interpolators，因为 bagging 是一种流行的随机化基于ensemble方法，可以并行实现。我们介绍了 multiplier-bootstrap-based bagged least square estimator，可以表示为一系列sketched least square estimators的平均值。我们在 proportionate 模式下（即样本大小与特征维度成正比） investigate out-of-sample prediction risks of sketched and bagged least square estimators in both underparametrized and overparameterized regimes。我们的结果表明 sketching 和 bagging 的统计作用。特别是，sketching 改变了 minimum $\ell_2$ norm estimator 的方向比例和插值阈值，但是风险仍然是无限大，尤其是在插值阈值附近。与此相反，bagging 能够有效减少风险，导致 bounded 的外样预测风险。为了更好地理解这种稳定性改善性质，我们证明了 bagging 是一种含义隐藏的正则化，并且通过显式正则化的等价性证明了这一点。我们还讨论了一些扩展。
</details></li>
</ul>
<hr>
<h2 id="Source-Camera-Identification-and-Detection-in-Digital-Videos-through-Blind-Forensics"><a href="#Source-Camera-Identification-and-Detection-in-Digital-Videos-through-Blind-Forensics" class="headerlink" title="Source Camera Identification and Detection in Digital Videos through Blind Forensics"></a>Source Camera Identification and Detection in Digital Videos through Blind Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03353">http://arxiv.org/abs/2309.03353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venkata Udaya Sameer, Shilpa Mukhopadhyay, Ruchira Naskar, Ishaan Dali</li>
<li>for: 本研究旨在透过机器学习特征提取、特征选择和后续的来源分类，确定影片来源的身份，以解决数位影片的来源识别问题。</li>
<li>methods: 本研究使用机器学习特征提取、特征选择和来源分类的方法来解决数位影片的来源识别问题。</li>
<li>results: 我们的实验结果显示，与传统指纹基本法相比，机器学习特征提取和来源分类方法更高效地解决数位影片的来源识别问题。<details>
<summary>Abstract</summary>
Source camera identification in digital videos is the problem of associating an unknown digital video with its source device, within a closed set of possible devices. The existing techniques in source detection of digital videos try to find a fingerprint of the actual source in the video in form of PRNU (Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern Noise) of each possible device. The highest correlation indicates the correct source. We investigate the problem of identifying a video source through a feature based approach using machine learning. In this paper, we present a blind forensic technique of video source authentication and identification, based on feature extraction, feature selection and subsequent source classification. The main aim is to determine whether a claimed source for a video is actually its original source. If not, we identify its original source. Our experimental results prove the efficiency of the proposed method compared to traditional fingerprint based technique.
</details>
<details>
<summary>摘要</summary>
源 Camera 识别在数字视频中是一个问题，即将未知的数字视频与其源设备相关联，在一个封闭的可能性集中。现有的数字视频来源检测技术会找到视频中的实际来源指纹（PRNU），并将其与每个可能的设备的感知模式噪声（SPN）进行匹配。最高匹配指示正确的源。我们研究了通过机器学习特征基本方法来实现视频源认证和识别。在这篇论文中，我们提出了一种盲目的视频源认证和识别技术，基于特征提取、特征选择和后续源分类。主要目标是判断一个视频的来源是否实际上是其原始来源。如果不是，我们就可以确定其原始来源。我们的实验结果表明，我们提出的方法比传统的指纹基本技术更高效。
</details></li>
</ul>
<hr>
<h2 id="Using-Neural-Networks-for-Fast-SAR-Roughness-Estimation-of-High-Resolution-Images"><a href="#Using-Neural-Networks-for-Fast-SAR-Roughness-Estimation-of-High-Resolution-Images" class="headerlink" title="Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images"></a>Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03351">http://arxiv.org/abs/2309.03351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeovafarias/sar-roughness-estimation-neural-nets">https://github.com/jeovafarias/sar-roughness-estimation-neural-nets</a></li>
<li>paper_authors: Li Fan, Jeova Farias Sales Rocha Neto</li>
<li>for: 这个论文的目的是提出一种基于神经网络的高分辨率Synthetic Aperture Radar（SAR）图像处理方法，以便快速和可靠地估计SAR图像中的粗糙度参数。</li>
<li>methods: 这个方法使用神经网络学习模型来预测G_I^0分布下的参数，然后使用这些参数来估计SAR图像中的粗糙度。</li>
<li>results: 研究发现，使用这种神经网络基于的估计方法可以快速、准确地估计SAR图像中的粗糙度，并且可以在实时中进行像素级粗糙度估计，而且可以使用一个简单的网络来实现这一点。<details>
<summary>Abstract</summary>
The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimation procedures for this problem, even when we use a simple network. More importantly, we show that this same methodology can be generalized to handle image inputs and, even if trained on purely synthetic data for a few seconds, is able to perform real time pixel-wise roughness estimation for high resolution real SAR imagery.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimation procedures for this problem, even when we use a simple network. More importantly, we show that this same methodology can be generalized to handle image inputs and, even if trained on purely synthetic data for a few seconds, is able to perform real time pixel-wise roughness estimation for high resolution real SAR imagery."中文翻译：<<SYS>>Synthetic Aperture Radar（SAR）成像分析是远程感知应用中的一个重要步骤，但它受到自然的折射噪声的限制。一种常见的解决方案是使用 $G_I^0$ 分布来模型数据，并从其中提取粗糙信息，以便在后续图像处理任务中使用，如分 segmentation、分类和解释等。这导致了高分辨率图像中很快获得粗糙参数的估计的需求。然而，传统的估计方法过于慢且容易出现估计错误。在这种情况下，我们提出了一种基于神经网络的估计框架，它首先学习了 $G_I^0$ 样本下的下面参数，然后可以用来估计未经见过的数据中的粗糙。我们表明，这种方法比传统的估计方法快速、估计误差小于、失败率较低。此外，我们还证明了这种方法可以普遍应用于图像输入，即使只有几秒钟的synthetic数据训练，也可以在实时ixel-wise粗糙估计中心高分辨率实际SAR成像数据。
</details></li>
</ul>
<hr>
<h2 id="REBOOT-Reuse-Data-for-Bootstrapping-Efficient-Real-World-Dexterous-Manipulation"><a href="#REBOOT-Reuse-Data-for-Bootstrapping-Efficient-Real-World-Dexterous-Manipulation" class="headerlink" title="REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation"></a>REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03322">http://arxiv.org/abs/2309.03322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, Sergey Levine</li>
<li>for: 学习灵活的抓取技能，以提高机器人手臂在实际世界中的操作能力。</li>
<li>methods: 利用近期的可效RL和重播缓存启动技术，将不同任务或物品的数据作为新任务的启动点，大幅提高学习效率。</li>
<li>results: 在实际世界中使用四 fingers 机器人手臂快速学习复杂的抓取技能，并完成实际训练周期，无需人工重置和奖励工程。<details>
<summary>Abstract</summary>
Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)
</details>
<details>
<summary>摘要</summary>
dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)Here's the translation in Traditional Chinese:dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy as well as learned reward functions, eliminating the need for manual resets and reward engineering. We demonstrate the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisition of intricate manipulation skills in the real world on a four-fingered robotic hand. (Videos: https://sites.google.com/view/reboot-dexterous)
</details></li>
</ul>
<hr>
<h2 id="Fitness-Approximation-through-Machine-Learning"><a href="#Fitness-Approximation-through-Machine-Learning" class="headerlink" title="Fitness Approximation through Machine Learning"></a>Fitness Approximation through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03318">http://arxiv.org/abs/2309.03318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itaitzruia4/approxml">https://github.com/itaitzruia4/approxml</a></li>
<li>paper_authors: Itai Tzruia, Tomer Halperin, Moshe Sipper, Achiya Elyasaf</li>
<li>for: 这个论文主要目标是提出一种基于机器学习模型的遗传算法中的健康估计方法，以优化遗传算法的运行效率。</li>
<li>methods: 该方法使用机器学习模型来保持一个遗传算法中个体的健康估计，并在进行遗传算法的演化运行中不断更新该模型。</li>
<li>results: 实验结果表明，使用该方法可以显著提高遗传算法的运行效率，并且fitness分数与实际遗传算法的fitness分数相似或者只有轻微差异。<details>
<summary>Abstract</summary>
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用机器学习（ML）模型来实现遗传算法（GA）中的健康近似计算，专注于在游戏模拟器（Gymnasium）中的EVOLUTIONARY AGENTS，其中健康计算成本高。我们在EVOLUTIONARY RUN期间不断更新一个包含采样个体以及其实际健康分数的数据集。我们比较了不同的方法来：1）在actual和approximate fitness之间切换，2）采样人口，和3）Weighting the samples。实验结果表明，我们的方法可以显著减少EVOLUTIONARY RUN时间，并且fitness分数与完全运行GA的fitness分数相似或者略低，具体取决于approximate-to-actual fitness computation的比例。我们的方法可以应用到多个领域。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Table-Tennis-A-Case-Study-into-a-High-Speed-Learning-System"><a href="#Robotic-Table-Tennis-A-Case-Study-into-a-High-Speed-Learning-System" class="headerlink" title="Robotic Table Tennis: A Case Study into a High Speed Learning System"></a>Robotic Table Tennis: A Case Study into a High Speed Learning System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03315">http://arxiv.org/abs/2309.03315</a></li>
<li>repo_url: None</li>
<li>paper_authors: David B. D’Ambrosio, Jonathan Abelian, Saminda Abeyruwan, Michael Ahn, Alex Bewley, Justin Boyd, Krzysztof Choromanski, Omar Cortes, Erwin Coumans, Tianli Ding, Wenbo Gao, Laura Graesser, Atil Iscen, Navdeep Jaitly, Deepali Jain, Juhana Kangaspunta, Satoshi Kataoka, Gus Kouretas, Yuheng Kuang, Nevena Lazic, Corey Lynch, Reza Mahjourian, Sherry Q. Moore, Thinh Nguyen, Ken Oslund, Barney J Reed, Krista Reymann, Pannag R. Sanketi, Anish Shankar, Pierre Sermanet, Vikas Sindhwani, Avi Singh, Vincent Vanhoucke, Grace Vesom, Peng Xu</li>
<li>for: 这个论文是为了描述一个现实世界中的机器人学习系统，该系统在之前的研究中已经能够与人类进行了数百场网球比赛，并且能够准确地返回球到 DESIRED 目标。</li>
<li>methods: 这个系统使用了一个高度优化的感知子系统，高速低延迟的机器人控制器，一种可以避免实际世界中的损害并用于零拟合转移的 simulations  paradigm，以及自动化的实际世界环境重置，以便在物理机器人上进行自主的训练和评估。</li>
<li>results: 论文的结果表明，通过使用这些技术，该系统能够在实际世界中进行自主的训练和评估，并且可以在不同的环境下进行适应和复杂的任务。<details>
<summary>Abstract</summary>
We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the system and details of experimental results can be found at https://youtu.be/uFcnWjB42I0.
</details>
<details>
<summary>摘要</summary>
我们对一个实际应用的机器人学习系统进行深入探讨，之前的研究已经证明该系统可以与人类进行百场乒乓球比赛，并且具有准确返回球的目标能力。该系统结合了高度优化的感知子系统、高速低延迟的机器人控制器、可预防物理世界中的损害并训练政策的 simulate  парадигма，以及自动化的实际环境重置，允许无人控制和评估实际机器人。我们附加了完整的系统描述，包括许多不常公布的设计决策，以及一系列研究，以解释mitigating 多种延迟的重要性、训练和部署分布shift 的考虑、感知系统的Robustness、策略参数的敏感性和行为空间的选择。视频展示了系统的组件和实验结果的细节，可以在 https://youtu.be/uFcnWjB42I0 找到。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Learning-of-Intrusion-Responses-through-Recursive-Decomposition"><a href="#Scalable-Learning-of-Intrusion-Responses-through-Recursive-Decomposition" class="headerlink" title="Scalable Learning of Intrusion Responses through Recursive Decomposition"></a>Scalable Learning of Intrusion Responses through Recursive Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03292">http://arxiv.org/abs/2309.03292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kim Hammar, Rolf Stadler</li>
<li>for: 本研究旨在Automated Intrusion Response（AIR）技术的研究和IT基础设施中的攻击者和 защитник之间的互动作为一个偏见的随机游戏。</li>
<li>methods: 我们采用了一种基于强化学习和自我玩家的方法，其中攻击和防御策略在自我玩家中互相演化，从而到达一个平衡点。</li>
<li>results: 我们的实验结果显示，使用我们提出的DFSP算法可以在一个真实的基础设施配置下实现高效地学习和实现平衡策略。在虚拟环境中，我们发现了学习的策略在真实的攻击和应急回应中表现良好，并且DFSP算法在比较实际的基础设施配置下显示出了显著的优势。<details>
<summary>Abstract</summary>
We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stochastic approximation. We evaluate the learned strategies in an emulation environment where real intrusions and response actions can be executed. The results show that the learned strategies approximate an equilibrium and that DFSP significantly outperforms a state-of-the-art algorithm for a realistic infrastructure configuration.
</details>
<details>
<summary>摘要</summary>
我们研究自动化入侵应急应对措施，将攻击者和防御者的互动视为部分可见随机游戏。为解决这个游戏，我们采用一种方法，其中攻击和防御策略通过自适应学习和自我玩家相互演化而往往达到均衡。在过去的研究中，我们证明了这种方法对小规模基础设施是可行的，但是在实际场景中，由基础设施规模带来的计算复杂性呈指数增长，使得这种方法无法承受。为解决这个问题，我们提出了一种方法，即 recursively decomposing the game into subgames，可以并行解决。通过优化停止理论，我们显示了攻击和防御策略在这些子游戏中具有阈值结构，从而可以高效地计算它们。为解决分解后的游戏，我们提出了一种算法called Decompositional Fictitious Self-Play (DFSP)，它通过随机方法学习达到均衡。我们在一个模拟环境中测试了学习的策略，并发现它们接近均衡，并且DFSP在一个现实主义基础设施配置下显著超过了当前状态艺术 Algorithm的性能。
</details></li>
</ul>
<hr>
<h2 id="R2D2-Deep-neural-network-series-for-near-real-time-high-dynamic-range-imaging-in-radio-astronomy"><a href="#R2D2-Deep-neural-network-series-for-near-real-time-high-dynamic-range-imaging-in-radio-astronomy" class="headerlink" title="R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy"></a>R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03291">http://arxiv.org/abs/2309.03291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aghabiglou A, Chu C S, Jackson A, Dabbech A, Wiaux Y</li>
<li>for: 这个论文是用于描述一种基于深度学习和数据一致更新的高分辨率高动态范围成像技术，用于天文学中的电磁波探测。</li>
<li>methods: 这种技术使用了混合深度学习神经网络（DNN）和数据一致更新来实现高分辨率高动态范围成像。它的重建是建立为一系列的差异图像，每个差异图像是DNN的输出。</li>
<li>results: 论文通过对有敏感观测数据的测试和比较，发现这种技术可以提供高精度成像，并且比CLEAN和其他两种 Algorithms（AIRI和uSARA）更快速和有更低的计算成本。<details>
<summary>Abstract</summary>
We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the Very Large Array (VLA). R2D2 is validated against CLEAN and the recent RI algorithms AIRI and uSARA, which respectively inject a learned implicit regularization and an advanced handcrafted sparsity-based regularization into the RI data. With only few terms in its series, the R2D2 model is able to deliver high-precision imaging, significantly superior to CLEAN and matching the precision of AIRI and uSARA. In terms of computational efficiency, R2D2 runs at a fraction of the cost of AIRI and uSARA, and is also faster than CLEAN, opening the door to real-time precision imaging in RI.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的人工智能方法，用于高分辨率高动态范围成像探测（RI）在天文学中。我们称之为“R2D2”，即“差异-差异神经网络序列 для高动态范围成像”。这是一种基于混合深度神经网络（DNN）和数据一致更新的模型驱动方法。它的重建是建立为一系列的差异图像，每个图像是DNN的输出。这些图像可以看作是对差异废弃图像的学习版本，其中CLEAN是一个常见的例子。我们提出了两种R2D2模型，基于两种不同的DNN架构：标准的U-Net和一种新的卷积架构。我们在高敏感的S带观测数据上用这些模型进行灰度成像，并与CLEAN和最近的RI算法AIRI和uSARA进行比较。 results show that R2D2可以提供高精度的成像，与AIRI和uSARA的精度相当，且在计算效率方面更高，只需几个序列。这使得实时高精度成像在RI中成为可能。
</details></li>
</ul>
<hr>
<h2 id="Let-Quantum-Neural-Networks-Choose-Their-Own-Frequencies"><a href="#Let-Quantum-Neural-Networks-Choose-Their-Own-Frequencies" class="headerlink" title="Let Quantum Neural Networks Choose Their Own Frequencies"></a>Let Quantum Neural Networks Choose Their Own Frequencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03279">http://arxiv.org/abs/2309.03279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Jaderberg, Antonio A. Gentile, Youssef Achari Berrada, Elvira Shishenina, Vincent E. Elfving</li>
<li>for: 这篇论文旨在探讨Parameterized quantum circuits作为机器学习模型，以及其中的代表性 Fourier 系列。</li>
<li>methods: 论文提出了一种通过添加可调参数到数据编码器中，来实现可调频率（TF）量子模型。</li>
<li>results: 数字实验表明，TF 模型可以学习拥有恰当性和弹性频谱特性的生成器，并且在解决 Navier-Stokes 方程中显示出了改善的准确率。<details>
<summary>Abstract</summary>
Parameterized quantum circuits as machine learning models are typically well described by their representation as a partial Fourier series of the input features, with frequencies uniquely determined by the feature map's generator Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance, fixing the space of functions that can be represented. In this work we consider a generalization of quantum models to include a set of trainable parameters in the generator, leading to a trainable frequency (TF) quantum model. We numerically demonstrate how TF models can learn generators with desirable properties for solving the task at hand, including non-regularly spaced frequencies in their spectra and flexible spectral richness. Finally, we showcase the real-world effectiveness of our approach, demonstrating an improved accuracy in solving the Navier-Stokes equations using a TF model with only a single parameter added to each encoding operation. Since TF models encompass conventional fixed frequency models, they may offer a sensible default choice for variational quantum machine learning.
</details>
<details>
<summary>摘要</summary>
parameterized quantum circuits as machine learning models 通常可以用幂 fourier 系列来描述输入特征，频率唯一由特征映射生成器的哈密顿ians决定。通常，这些数据编码生成器会在先前选择，确定可以表示的函数空间。在这种工作中，我们考虑了一种扩展量子模型，即含有可训练参数的生成器，导致可训练频率（TF）量子模型。我们数值展示了TF模型可以学习生成器具有感兴趣的性质，包括非Regularly spaced 频谱和灵活的 спектral richness。最后，我们展示了我们的方法的实际效果，通过在 Navier-Stokes 方程中使用TF模型，只有每个编码操作中添加一个参数，提高解决问题的准确性。由于TF模型包括 fixede frequency 模型，它们可能会成为变量量子机器学习的合适默认选择。
</details></li>
</ul>
<hr>
<h2 id="Matcha-TTS-A-fast-TTS-architecture-with-conditional-flow-matching"><a href="#Matcha-TTS-A-fast-TTS-architecture-with-conditional-flow-matching" class="headerlink" title="Matcha-TTS: A fast TTS architecture with conditional flow matching"></a>Matcha-TTS: A fast TTS architecture with conditional flow matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03199">http://arxiv.org/abs/2309.03199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, Gustav Eje Henter</li>
<li>for: 这个论文是为了提出一种新的编码器-解码器架构，用于快速的语音合成模型训练。</li>
<li>methods: 这个方法使用最优运输 conditional flow匹配（OT-CFM）来训练一个基于ODE的解码器，能够在 fewer synthesis steps 中实现高质量输出。</li>
<li>results: 与强制搜索基线模型相比，这个系统具有最小的内存占用量，在长语音上具有与最快模型相当的速度，并在听力测试中获得最高意见分。Please note that the abstract is in English, so the Chinese translation may not be perfect.<details>
<summary>Abstract</summary>
We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
</details>
<details>
<summary>摘要</summary>
我们介绍Matcha-TTS，一种新的编码器-解码器架构，用于快速的语音合成模型训练，使用最佳交通流匹配（OT-CFM）。这种方法使得decoder可以在 fewer synthesis steps 中达到高质量输出，比使用 scored matching 训练的模型更快。我们在设计中也特别注重了每个合成步骤的运行速度。Matcha-TTS 是一种 probabilistic 、非自适应的语音合成系统，可以从零开始学习，不需要外部对齐。相比强大的预训练基线模型，Matcha-TTS 系统具有最小的内存占用量，可以在长句子上匹配最快的模型，并在听力测试中获得最高的意见分。请参考 https://shivammehta25.github.io/Matcha-TTS/ 获取音频示例、代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="Blink-Link-Local-Differential-Privacy-in-Graph-Neural-Networks-via-Bayesian-Estimation"><a href="#Blink-Link-Local-Differential-Privacy-in-Graph-Neural-Networks-via-Bayesian-Estimation" class="headerlink" title="Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation"></a>Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03190">http://arxiv.org/abs/2309.03190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhxchd/blink_gnn">https://github.com/zhxchd/blink_gnn</a></li>
<li>paper_authors: Xiaochen Zhu, Vincent Y. F. Tan, Xiaokui Xiao</li>
<li>for: 增强图像推理任务中节点嵌入的能力，但训练可能会涉及隐私问题。</li>
<li>methods: 使用分布式节点的链接地方ifferential privacy，协作不可信服务器训练Graph Neural Networks（GNNs），无需披露图像中任务的链接存在。</li>
<li>results: 我们的方法可以更好地减少隐私影响GNNs的准确性，并提供了不同隐私预算下的两种变体，能够在不同的隐私环境下表现更好。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is high, while the other utilizes more information and performs better given relatively higher privacy budgets. Furthermore, we propose a hybrid variant that combines both strategies and is able to perform better across different privacy budgets. Extensive experiments show that our approach outperforms existing methods in terms of accuracy under varying privacy budgets.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 在不同的推理任务中学习节点嵌入的能力得到了越来越多的推广，但是训练它们可能会引起隐私问题。为了解决这个问题，我们提出了使用分布式节点的链接本地敏感度保护（LDP），允许不可信服务器在训练 GNNs 时不 revel 链接的存在。我们的方法将隐私预算分配给链接和图度的两个部分，使服务器通过浮点估计来更好地净化图像 topology，从而减轻LDP对训练 GNNs 的影响。我们 bounds 推断链接概率的平均绝对误差与真实图像 topology 之间的差异。此外，我们还提出了两种不同的隐私设置下的LDP机制，其中一种在低隐私预算下更少地估计链接，以避免高度不确定性时的假阳性链接估计；另一种则更加利用信息，在相对较高的隐私预算下表现更好。 finally，我们提出了这两种机制的混合变体，可以在不同的隐私预算下表现更好。我们的实验表明，我们的方法在不同的隐私预算下可以比既有方法更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="SLiMe-Segment-Like-Me"><a href="#SLiMe-Segment-Like-Me" class="headerlink" title="SLiMe: Segment Like Me"></a>SLiMe: Segment Like Me</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03179">http://arxiv.org/abs/2309.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali Mahdavi Amiri, Ghassan Hamarneh</li>
<li>for: 这个研究用于提出一个单一示例的图像分割方法，使用大量的感知语言模型，例如稳定扩散（SD），以达到高精度的图像分割。</li>
<li>methods: 这个方法使用SD的优化器，将图像和其分割标识对应到彼此的数值表示，然后将这些数值表示优化为每个分割区域的图像特征。</li>
<li>results: 实验结果显示，SLiMe可以在单一示例下进行图像分割，并且在几个shot的情况下进行改进，得到更高的精度和可靠性。此外，SLiMe比其他一阶和几阶分割方法表现更好。<details>
<summary>Abstract</summary>
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segment any real-world image during inference with the granularity of the segmented region in the training image, using just one example. Moreover, leveraging additional training data when available, i.e. few-shot, improves the performance of SLiMe. We carried out a knowledge-rich set of experiments examining various design factors and showed that SLiMe outperforms other existing one-shot and few-shot segmentation methods.
</details>
<details>
<summary>摘要</summary>
significanthavebeenmadeusingslargelocation-language models,likeStableDiffusion(SD),fordownstreampurposes,includingimageediting,imagecorrespondence,and3Dshapegeneration.Inspiredbytheseadvances,weexploreleveragingtheseextensivevision-language modelsforsegmentingimagesatanydesiredgranularityusingasfewasoneannotatedsamplebyproposingSLiMe.SLiMeframesthisproblemasanoptimizationtask.Specifically,givenasingletrainingimageanditssegmentationmask,wefirstextractattentionmaps,includingournovel"weightedaccumulatedself-attentionmap"fromtheSDprior.Then,usingtheextractedattentionmaps,thetextembeddingsofStableDiffusionareoptimizedsuchthat,eachofthem,learnaboutasinglesegmentedregionfromthetrainingimage.Theselearnedembeddingsthenhighlightthesegmentedregionintheattentionmaps,whichinturncanthenbederivedfromthe segmentationmap.ThisenablesSLiMe to segmentanyreal-worldimage duringinferencewiththegranularityofthesegmentedregioninthe trainingimage,usingjustoneexample.Moreover,leveragingadditionaltrainingdatawhenavailable,i.e.few-shot,improves theperformanceofSLiMe.We carriedoutaknowledgerichsetofexperiments examiningvariousdesigndetailsandshowedthatSLiMeoutperformsother existingoneshotandfew-shotsegmentationmethods.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Inductive-Path-Neural-Network-for-Temporal-Knowledge-Graph-Reasoning"><a href="#Temporal-Inductive-Path-Neural-Network-for-Temporal-Knowledge-Graph-Reasoning" class="headerlink" title="Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning"></a>Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03251">http://arxiv.org/abs/2309.03251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Dong, Pengyang Wang, Meng Xiao, Zhiyuan Ning, Pengfei Wang, Yuanchun Zhou</li>
<li>for: 本文旨在提出一种基于历史信息的时间知识图（TKG）推理方法，以便预测未来事件。</li>
<li>methods: 本文提出了一种基于历史信息的时间induction neural network（TiPNN）模型，该模型通过一个统一的历史temporal图来捕捉和嵌入历史信息，然后通过定义的查询意识 temporal paths来模型历史路径信息相关于查询。</li>
<li>results: 经验表明，提出的模型不仅实现了显著性能提升，还能够处理 inductive 设置，并且可以通过历史temporal图提供推理证据。<details>
<summary>Abstract</summary>
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to comprehensively capture and encapsulate information from history. Subsequently, we utilize the defined query-aware temporal paths to model historical path information related to queries on history temporal graph for the reasoning. Extensive experiments illustrate that the proposed model not only attains significant performance enhancements but also handles inductive settings, while additionally facilitating the provision of reasoning evidence through history temporal graphs.
</details>
<details>
<summary>摘要</summary>
Temporal Knowledge Graph (TKG) 是传统知识图 (KG) 的扩展，它包含时间维度。这些知识图中的推理是一个关键的任务，旨在预测未来事实基于过去发生的事件。主要挑战在于发掘历史子图中的结构依赖和时间对称。现有的方法通常透过实体模型来建模 TKG，但在实际情况中，有着数量繁多的实体，且时间进程中新增的实体可能会增加。这使得实体依赖的方法对于处理大量实体和时间进程中新增的实体而具有挑战。因此，我们提出了时间导引路径神经网 (TiPNN)，它在实体独立的角度下模型历史信息。具体来说，TiPNN 使用了一个统一的图，即历史时间图，以全面捕捉和储存历史信息。然后，我们利用定义的查询相关时间路径来模型历史时间图中相关查询的路径信息。实验结果显示，我们的方法不仅实现了重要的性能提升，而且可以处理导入设定，同时还可以提供推理证据。
</details></li>
</ul>
<hr>
<h2 id="3D-Object-Positioning-Using-Differentiable-Multimodal-Learning"><a href="#3D-Object-Positioning-Using-Differentiable-Multimodal-Learning" class="headerlink" title="3D Object Positioning Using Differentiable Multimodal Learning"></a>3D Object Positioning Using Differentiable Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03177">http://arxiv.org/abs/2309.03177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Zanyk-McLean, Krishna Kumar, Paul Navratil</li>
<li>for: 该论文旨在优化计算机图形场景中对观察者或参照物的对象位置。</li>
<li>methods: 该论文使用了模拟的激光探测数据和可微分渲染来优化对象位置，并使用了两种感知模式（图像像素损失和激光探测）来加速 converges。</li>
<li>results: 该论文显示了将两种感知模式融合在一起可以更快地 converges 对象位置，这种方法可能对自动驾驶汽车的训练和Scene Understanding有很好的应用。<details>
<summary>Abstract</summary>
This article describes a multi-modal method using simulated Lidar data via ray tracing and image pixel loss with differentiable rendering to optimize an object's position with respect to an observer or some referential objects in a computer graphics scene. Object position optimization is completed using gradient descent with the loss function being influenced by both modalities. Typical object placement optimization is done using image pixel loss with differentiable rendering only, this work shows the use of a second modality (Lidar) leads to faster convergence. This method of fusing sensor input presents a potential usefulness for autonomous vehicles, as these methods can be used to establish the locations of multiple actors in a scene. This article also presents a method for the simulation of multiple types of data to be used in the training of autonomous vehicles.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇文章描述了一种多Modal方法，使用模拟的激光数据和图像像素损失，通过可微分渲染来优化对象在计算机图形场景中的位置，与观察者或参考物体的关系。这种方法使用梯度下降，损失函数受到两种模式的影响。通常的对象放置优化只使用图像像素损失和可微分渲染，这种工作表明使用第二种模式（激光）会更快 converges。这种整合感知输入的方法有可能为自动驾驶车辆提供用于场景中多个演员的位置确定的潜在用途，此外，这篇文章还描述了用于训练自动驾驶车辆的多种数据类型的模拟方法。
</details></li>
</ul>
<hr>
<h2 id="GPT-InvestAR-Enhancing-Stock-Investment-Strategies-through-Annual-Report-Analysis-with-Large-Language-Models"><a href="#GPT-InvestAR-Enhancing-Stock-Investment-Strategies-through-Annual-Report-Analysis-with-Large-Language-Models" class="headerlink" title="GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models"></a>GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03079">http://arxiv.org/abs/2309.03079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UditGupta10/GPT-InvestAR">https://github.com/UditGupta10/GPT-InvestAR</a></li>
<li>paper_authors: Udit Gupta</li>
<li>For: The paper aims to simplify the process of assessing annual reports of all firms by leveraging the capabilities of large language models (LLMs).* Methods: The paper uses LLMs to generate insights from annual reports and combines these insights with historical stock price data to train a machine learning model.* Results: The walkforward test results show promising outperformance compared to S&amp;P500 returns, indicating the effectiveness of the proposed framework.Here is the same information in Simplified Chinese:* For: 这篇论文目标是使用大自然语言模型（LLM）来简化所有公司年度报告的评估过程。* Methods: 论文使用LLM生成年度报告的 Insights，并将这些 Insights 与历史股票价格数据结合，用机器学习模型进行训练。* Results: 跨越测试结果表明，提档模型可以与S&amp;P500股票价格相比，表明提档模型的有效性。<details>
<summary>Abstract</summary>
Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&P500 returns. This paper intends to provide a framework for future work in this direction. To facilitate this, the code has been released as open source.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate="no"<</SYS>公开上市公司的年度报告含有关键信息，可以帮助评估公司股价的可能影响。这些报告是全面的，可以达到或超过100页。分析这些报告是繁琐的，即使只是对单个公司进行分析，更是对整个公司宇宙进行分析。过去数年，金融专家已经熟悉地从这些文档中提取有价值信息。然而，这需要多年的做实践和经验。这篇论文想要简化所有公司的年度报告评估过程，通过利用大语言模型（LLM）的能力。生成的各种数据被编译成Quant样式数据集，并与历史股票价格数据相结合。然后，使用机器学习模型训练LMM输出作为特征。walkforward测试结果表明，这种方法可以显著超越S&P500回报。这篇论文的目标是提供未来研究的框架。为此，我们已经公开发布了代码。注：下面的文本是使用Simplified Chinese表示的。如果您想要使用Traditional Chinese，请将<<SYS> translate="no"<</SYS> replaced with <<SYS> translate="yes"<</SYS>>>
</details></li>
</ul>
<hr>
<h2 id="Impression-Informed-Multi-Behavior-Recommender-System-A-Hierarchical-Graph-Attention-Approach"><a href="#Impression-Informed-Multi-Behavior-Recommender-System-A-Hierarchical-Graph-Attention-Approach" class="headerlink" title="Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach"></a>Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03169">http://arxiv.org/abs/2309.03169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Li, Divya Bhargavi, Vidya Sagar Ravipati</li>
<li>For: This paper aims to address the limitations of traditional recommender systems that rely solely on implicit feedback, such as item purchases, by incorporating multi-behavior interactions and hierarchical graph attention to improve recommendation accuracy.* Methods: The proposed Hierarchical Multi-behavior Graph Attention Network (HMGN) utilizes attention mechanisms to capture information from both inter and intra-behaviors, and a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. The approach also incorporates a specialized multi-behavior sub-graph sampling technique and can handle scalability, knowledge metadata, and time-series data.* Results: The proposed HMGN model demonstrates a notable performance boost of up to 64% in NDCG@100 metrics compared to conventional graph neural network methods, indicating its effectiveness in improving recommendation accuracy by leveraging multi-behavior interactions.<details>
<summary>Abstract</summary>
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. Recognizing the need for scalability, our approach integrates a specialized multi-behavior sub-graph sampling technique. Moreover, the adaptability of HMGN allows for the seamless inclusion of knowledge metadata and time-series data. Empirical results attest to our model's prowess, registering a notable performance boost of up to 64\% in NDCG@100 metrics over conventional graph neural network methods.
</details>
<details>
<summary>摘要</summary>
While recommender systems have greatly benefited from implicit feedback, they have often overlooked the subtleties of multi-behavior interactions between users and items. Historically, these systems either lumped all behaviors, such as  impression (formerly view), add-to-cart, and buy, under a single 'interaction' label, or prioritized only the target behavior, often the buy action, discarding valuable auxiliary signals. Although recent advancements tried addressing this oversimplification, they primarily focused on optimizing the target behavior alone, struggling with data scarcity. Additionally, they tended to ignore the intrinsic hierarchy of behaviors. To bridge these gaps, we introduce the  hierarchical multi-behavior graph attention network (HMGN). This groundbreaking framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-task Hierarchical Bayesian Personalized Ranking (HBPR) for optimization. Recognizing the need for scalability, our approach integrates a specialized multi-behavior sub-graph sampling technique. Moreover, the adaptability of HMGN allows for the seamless inclusion of knowledge metadata and time-series data. Empirical results demonstrate our model's excellence, achieving a notable performance boost of up to 64% in NDCG@100 metrics over conventional graph neural network methods.
</details></li>
</ul>
<hr>
<h2 id="Split-Boost-Neural-Networks"><a href="#Split-Boost-Neural-Networks" class="headerlink" title="Split-Boost Neural Networks"></a>Split-Boost Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03167">http://arxiv.org/abs/2309.03167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Raffaele Giuseppe Cestari, Gabriele Maroni, Loris Cannelli, Dario Piga, Simone Formentin</li>
<li>for: 该论文旨在提出一种新的训练策略，以提高Feed-Forward网络的性能，并自动包含正则化行为。</li>
<li>methods: 该论文提出了一种Split-Boost训练策略，通过分解网络 INTO smaller sub-networks，并在每个sub-network中进行boosting，以提高网络的性能和正则化。</li>
<li>results: 在一个真实世界（匿名）数据集上进行了 benchmark 医疗保险设计问题的测试，结果显示，使用Split-Boost训练策略可以提高网络的性能，并且自动包含正则化行为，从而降低了总体的hyperparameter数量和训练时间。<details>
<summary>Abstract</summary>
The calibration and training of a neural network is a complex and time-consuming procedure that requires significant computational resources to achieve satisfactory results. Key obstacles are a large number of hyperparameters to select and the onset of overfitting in the face of a small amount of data. In this framework, we propose an innovative training strategy for feed-forward architectures - called split-boost - that improves performance and automatically includes a regularizing behaviour without modeling it explicitly. Such a novel approach ultimately allows us to avoid explicitly modeling the regularization term, decreasing the total number of hyperparameters and speeding up the tuning phase. The proposed strategy is tested on a real-world (anonymized) dataset within a benchmark medical insurance design problem.
</details>
<details>
<summary>摘要</summary>
neural network 的准确和训练是一个复杂和时间消耗的过程，需要大量计算资源以获得满意的结果。关键障碍是大量的超参数选择和数据少量增强问题。在这个框架下，我们提出了一种新的训练策略 для批处理体系（split-boost），可以提高性能并自动包含正则化行为无需显式表示。这种新的方法最终允许我们避免显式表示正则化项，减少总体超参数数量和优化阶段速度。我们在一个匿名的实际数据集上测试了我们的策略，并在一个匿名的医疗保险设计问题中进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Recharge-UAV-Coverage-Path-Planning-through-Deep-Reinforcement-Learning"><a href="#Learning-to-Recharge-UAV-Coverage-Path-Planning-through-Deep-Reinforcement-Learning" class="headerlink" title="Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning"></a>Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03157">http://arxiv.org/abs/2309.03157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theilem/uavSim">https://github.com/theilem/uavSim</a></li>
<li>paper_authors: Mirco Theile, Harald Bayerlein, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</li>
<li>for: 这篇论文解决了功能有限的无人机（UAV）在覆盖区域时的能量有限的问题，通过使用深度学习（DRL）和地图观察来优化覆盖路径。</li>
<li>methods: 该论文提出了一种基于 proximal policy optimization（PPO）的DRL方法，使用动作掩模和折扣因子安排来优化覆盖 trajectory，并通过历史位置处理来处理出现的突然变化。</li>
<li>results: 该论文的方法在不同的目标区域和地图上表现出优于基线方法，并能够普适化到不同的地图上。<details>
<summary>Abstract</summary>
Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable insights into DRL algorithm design for long-horizon problems and provide a publicly available software framework for the CPP problem.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Driven-Neural-Polar-Codes-for-Unknown-Channels-With-and-Without-Memory"><a href="#Data-Driven-Neural-Polar-Codes-for-Unknown-Channels-With-and-Without-Memory" class="headerlink" title="Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory"></a>Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03148">http://arxiv.org/abs/2309.03148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziv Aharoni, Bashar Huleihel, Henry D. Pfister, Haim H. Permuter</li>
<li>for: 该论文的目的是提出一种基于数据驱动的极码设计方法，用于处理有和无内存的通道。</li>
<li>methods: 该方法利用成功级联（SC）解码器的结构，采用神经网络（NN）取代原始SC解码器的核心元素，包括检查节点、位节点和软决策。此外，还采用一个附加的NN来嵌入通道输出到SC解码器的输入空间。</li>
<li>results: 该方法可以提供理论保证和计算复杂度不随通道内存大小增长。在实验中，该方法在无内存通道和具有内存通道上表现出优于最佳极码解码器（SC和SCT解码器）。此外，该方法还适用于SC和SCT解码器不适用的情况。<details>
<summary>Abstract</summary>
In this work, a novel data-driven methodology for designing polar codes for channels with and without memory is proposed. The methodology is suitable for the case where the channel is given as a "black-box" and the designer has access to the channel for generating observations of its inputs and outputs, but does not have access to the explicit channel model. The proposed method leverages the structure of the successive cancellation (SC) decoder to devise a neural SC (NSC) decoder. The NSC decoder uses neural networks (NNs) to replace the core elements of the original SC decoder, the check-node, the bit-node and the soft decision. Along with the NSC, we devise additional NN that embeds the channel outputs into the input space of the SC decoder. The proposed method is supported by theoretical guarantees that include the consistency of the NSC. Also, the NSC has computational complexity that does not grow with the channel memory size. This sets its main advantage over successive cancellation trellis (SCT) decoder for finite state channels (FSCs) that has complexity of $O(|\mathcal{S}|^3 N\log N)$, where $|\mathcal{S}|$ denotes the number of channel states. We demonstrate the performance of the proposed algorithms on memoryless channels and on channels with memory. The empirical results are compared with the optimal polar decoder, given by the SC and SCT decoders. We further show that our algorithms are applicable for the case where there SC and SCT decoders are not applicable.
</details>
<details>
<summary>摘要</summary>
“这个研究中提出了一种基于数据的方法，用于设计具有和无记忆频道的� polar 码。这种方法适用于频道给出了“黑盒”，设计师可以通过频道生成输入和输出的观察，但无法取得频道的具体模型。我们的方法利用成功的继续取消（SC）解oder的结构，创建了一个内置 neural network（NSC）解oder。NSC解oder使用神经网络（NN）取代原始 SC 解oder的核心元素，包括检查点、位元点和软决定。此外，我们还创建了一个附加的 NN，将频道输出嵌入到 SC 解oder 的输入空间中。我们的方法具有理论保证，包括 NSC 的一致性。此外，NSC 的计算复杂度不随频道内存大小增长。这使得 NSC 在finite state channels（FSCs）中的计算复杂度与 SCT 解oder 不同，SCT 解oder 的计算复杂度为 $O(|\mathcal{S}|^3 N\log N)$，where $|\mathcal{S}|$ 表示频道状态的数量。我们在无记忆频道和记忆频道上进行实验，比较了我们的方法与最佳 polar 解oder（SC 和 SCT 解oder）的实验结果。我们还证明了我们的方法可以应用于 SC 和 SCT 解oder 不适用的情况。”
</details></li>
</ul>
<hr>
<h2 id="The-Best-Arm-Evades-Near-optimal-Multi-pass-Streaming-Lower-Bounds-for-Pure-Exploration-in-Multi-armed-Bandits"><a href="#The-Best-Arm-Evades-Near-optimal-Multi-pass-Streaming-Lower-Bounds-for-Pure-Exploration-in-Multi-armed-Bandits" class="headerlink" title="The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits"></a>The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03145">http://arxiv.org/abs/2309.03145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepehr Assadi, Chen Wang</li>
<li>for: 这个论文是用来解决多臂枪（Multi-armed Bandit，MAB）中的纯探索问题，即如何在有限资源下选择最佳臂来获得最大的回报。</li>
<li>methods: 这个论文使用了流式算法，并且使用了优化的抽样复杂度来解决这个问题。具体来说，这个论文使用了$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度，其中$n$是臂的数量，$\Delta$是最佳臂和第二最佳臂之间的差距。</li>
<li>results: 这个论文获得了一个near-optimal的抽象贸易优化，即使用流式算法可以在$O\left(\frac{\log(1&#x2F;\Delta)}{\log\log(1&#x2F;\Delta)}\right)$个过程中获得$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度。这个结果与 Jin et al. 的 $O(\log(\frac{1}{\Delta}))$-pass algorithm相同（即使用$O(1)$的内存和$O\left(\frac{n}{\Delta^2}\right)$的抽样复杂度），并解决了 Assadi 和 Wang 提出的一个开问题。<details>
<summary>Abstract</summary>
We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}{\log\log{(1/\Delta)})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
</details>
<details>
<summary>摘要</summary>
我们提供了一种近似优化的样本传递贸易，用于纯exploration在多把枪（MAB）中，通过多路流动算法：任何具有减 Linear 内存的流动算法都需要 $\Omega(\frac{\log{(1/\Delta)}{\log\log{(1/\Delta)})$  passes。在这里， $n$ 是把枪的数量， $\Delta$ 是最佳和第二最佳把枪之间的奖励差。我们的结果与 Jin et al. 的 $O(\log(\frac{1}{\Delta}))$-pass算法（up to lower order terms）相匹配，该算法只需 $O(1)$ 内存并解决了Assadi 和 Wang 提出的问题（STOC'20）。
</details></li>
</ul>
<hr>
<h2 id="Using-Multiple-Vector-Channels-Improves-E-n-Equivariant-Graph-Neural-Networks"><a href="#Using-Multiple-Vector-Channels-Improves-E-n-Equivariant-Graph-Neural-Networks" class="headerlink" title="Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks"></a>Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03139">http://arxiv.org/abs/2309.03139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Levy, Sékou-Oumar Kaba, Carmelo Gonzales, Santiago Miret, Siamak Ravanbakhsh</li>
<li>for: 用于物理科学领域的机器学习研究</li>
<li>methods: 使用多通道E(n)-对称图 neural network</li>
<li>results: 在多种物理系统 benchmark 任务上表现更好，但 RUNTIME 和参数数量几乎不变<details>
<summary>Abstract</summary>
We present a natural extension to E(n)-equivariant graph neural networks that uses multiple equivariant vectors per node. We formulate the extension and show that it improves performance across different physical systems benchmark tasks, with minimal differences in runtime or number of parameters. The proposed multichannel EGNN outperforms the standard singlechannel EGNN on N-body charged particle dynamics, molecular property predictions, and predicting the trajectories of solar system bodies. Given the additional benefits and minimal additional cost of multi-channel EGNN, we suggest that this extension may be of practical use to researchers working in machine learning for the physical sciences
</details>
<details>
<summary>摘要</summary>
我团队提出了一种自然扩展，用于E(n)-对称图 neural network，使用每个节点多个对称向量。我们阐述了这种扩展，并证明其在不同物理系统 benchmark 任务中提高性能，差异非常小，运行时间和参数数量也几乎不变。我们提出的多通道EGNN 超过了标准单通道EGNN 在N-体电荷 particel动力学、分子性质预测和太阳系天体轨道预测等任务中的性能。考虑到这种扩展的附加优点和运行时间和参数数量的几乎不变，我们建议这种扩展可能对物理科学领域的研究人员有实际意义。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Manufacturing-Defects-in-PCBs-via-Data-Centric-Machine-Learning-on-Solder-Paste-Inspection-Features"><a href="#Detecting-Manufacturing-Defects-in-PCBs-via-Data-Centric-Machine-Learning-on-Solder-Paste-Inspection-Features" class="headerlink" title="Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features"></a>Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03113">http://arxiv.org/abs/2309.03113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jubilee Prasad-Rao, Roohollah Heidary, Jesse Williams</li>
<li>For: 印刷电路板（PCB）生产中的缺陷检测，使用粘接粉检测（SPI）和自动光学检测（AOI）机器可以提高操作效率，大幅减少人工干预。* Methods: 使用SPI提取的特征来训练机器学习（ML）模型，在PCB生产过程中检测缺陷。使用6万个封装的特征，对2万个组件进行训练，共计15387个PCB。在数据预处理步骤中进行迭代，使用基础的极大均值抛物质（XGBoost）ML模型。* Results: 使用不同级别的训练实例，包括封装级、组件级和PCB级，使ML模型能够捕捉到pin级、组件级和PCB级之间的交互效果。结果表明，将不同级别的训练实例组合起来可以提高缺陷检测性能。<details>
<summary>Abstract</summary>
Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, and the detection results from the different models are combined to identify defective components.
</details>
<details>
<summary>摘要</summary>
自动检测印刷电路板（PCB）制造过程中的缺陷使用沉积镀 paste inspection（SPI）和自动光学检查（AOI）机器可以提高运行效率，并大幅减少人工干预。在这篇论文中，使用SPI提取的600万个封装件特征来训练机器学习（ML）模型，以检测PCB制造过程中的缺陷。这600万个PCB封装件相应于2000万个组件，这些组件属于15387个PCB。使用基础的极限梯度提升（XGBoost）ML模型，我们在数据预处理步骤中进行迭代优化。将封装件级SPI特征与组件和PCB ID相结合，我们开发了训练实例，同时在组件和PCB级别进行训练。这样允许ML模型捕捉到封装件级、组件级和PCB级之间的相互作用，从而提高检测性能。训练得到的不同模型的检测结果进行合并，以识别缺陷的组件。
</details></li>
</ul>
<hr>
<h2 id="Graph-Theory-Applications-in-Advanced-Geospatial-Research"><a href="#Graph-Theory-Applications-in-Advanced-Geospatial-Research" class="headerlink" title="Graph Theory Applications in Advanced Geospatial Research"></a>Graph Theory Applications in Advanced Geospatial Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03249">http://arxiv.org/abs/2309.03249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surajit Ghosh, Archita Mallick, Anuva Chowdhury, Kounik De Sarkar</li>
<li>for: 本研究探讨了图论算法在地理科学中的应用，包括网络分析、空间连接性、地理信息系统等多种空间问题的解决方案。</li>
<li>methods: 本文介绍了图论算法在地理科学中的应用，包括度量空间关系、分析空间结构、地理信息系统等方面的实现。</li>
<li>results: 本研究提供了各种实际应用的案例研究，描述了图论算法在地理科学中的实际 significanc，以及未来研究的可能性和挑战。<details>
<summary>Abstract</summary>
Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This technical report explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios. It provides a comprehensive idea about the key concepts and algorithms of graph theory that assist the modelling processes. The report provides insights into the practical significance of graph theory in addressing real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this field.
</details>
<details>
<summary>摘要</summary>
地理科学包括各种应用，从环境监测到交通规划，以及基础设施规划，同时还包括位置基于分析和服务。数学中的图论算法在这些领域中已成为不可或缺的工具，因为它可以有效地模拟和分析空间关系。本技术报告探讨了图论算法在地理科学中的应用，包括网络分析、空间连接性、地理信息系统等多种空间问题解决方案。报告还提供了关键概念和算法的全面了解，以及在实际世界中图论的实际应用和挑战。报告还列举了这一领域的广泛研究、创新技术和方法。
</details></li>
</ul>
<hr>
<h2 id="ContrastWSD-Enhancing-Metaphor-Detection-with-Word-Sense-Disambiguation-Following-the-Metaphor-Identification-Procedure"><a href="#ContrastWSD-Enhancing-Metaphor-Detection-with-Word-Sense-Disambiguation-Following-the-Metaphor-Identification-Procedure" class="headerlink" title="ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure"></a>ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03103">http://arxiv.org/abs/2309.03103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Elzohbi, Richard Zhao</li>
<li>for: 本研究旨在提高 метафора检测精度，提出了一种基于 RoBERTa 的 Metaphor Detection 模型，称为 ContrastWSD。</li>
<li>methods: 该模型结合 Metaphor Identification Procedure (MIP) 和 Word Sense Disambiguation (WSD)，通过对词语上下文含义和基本含义进行对比，判断 sentence 中是否存在 метафо拉用法。</li>
<li>results: 在多个 benchmark 数据集上进行测试，与强基线相比，ContrastWSD 表现出色， indicating the effectiveness in advancing metaphor detection。<details>
<summary>Abstract</summary>
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了ContrastWSD模型，基于RoBERTa语言模型，结合了Metaphor Identification Procedure（MIP）和Word Sense Disambiguation（WSD）两者，以提取和对比语言中单词的上下文含义和基本含义，以判断单词是否在句子中使用了象征性。通过利用WSD模型中的词义，我们的模型提高了象征检测的过程，并超越了仅仅基于上下文嵌入或者将基本定义和其他外部知识相结合的方法。我们在各种标准数据集上评估了我们的方法，并与强基线相比较，表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="ORL-AUDITOR-Dataset-Auditing-in-Offline-Deep-Reinforcement-Learning"><a href="#ORL-AUDITOR-Dataset-Auditing-in-Offline-Deep-Reinforcement-Learning" class="headerlink" title="ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning"></a>ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03081">http://arxiv.org/abs/2309.03081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/link-zju/orl-auditor">https://github.com/link-zju/orl-auditor</a></li>
<li>paper_authors: Linkang Du, Min Chen, Mingyang Sun, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang<br>for:* 这篇论文的目的是为了提供一个新的数据实验验证机制，以保护机器学习模型在安全重要领域中的数据库。methods:* 这篇论文使用了聚合奖励作为唯一识别符，以验证训练在特定数据库上的机器学习模型。results:* 这篇论文的实验结果显示，使用 ORL-AUDITOR 可以实现高准确率（95%）和低伪阳性率（2.88%）的数据实验验证，并且可以实现实际的实验设置和资料集验证。<details>
<summary>Abstract</summary>
Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据是人工智能中关键资产，高质量数据集可以显著提高机器学习模型的性能。在安全关键领域如自动驾驶车辆，线上深度优化学习（线上DRL）常常用于训练模型，而不是在真实环境中进行交互式训练。为支持这些模型的开发，许多机构将数据集公开发布，但这些数据集受到潜在的违用或侵犯的威胁。把水印注入到数据集可以保护数据的知识产权，但它无法处理已经发布的数据集，也无法在后续更改。现有的解决方案，如数据集推理和会员推理，在线上DRL场景中不太有效，因为模型的行为特点和环境约束。在这篇论文中，我们主张一种新的思路，利用总奖励可以作为特定数据集训练DRL模型的唯一标识符。为此，我们提出了ORL-AUDITOR，它是第一个路径级数据集审核机制。我们的实验表明，ORL-AUDITOR的审核精度高于95%，false positive率低于2.88%。我们还提供了实现ORL-AUDITOR的有价值参数研究，以及对open-source数据集的审核能力。ORL-AUDITOR已经开源在https://github.com/link-zju/ORL-Auditor。
</details></li>
</ul>
<hr>
<h2 id="Parameterizing-pressure-temperature-profiles-of-exoplanet-atmospheres-with-neural-networks"><a href="#Parameterizing-pressure-temperature-profiles-of-exoplanet-atmospheres-with-neural-networks" class="headerlink" title="Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks"></a>Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03075">http://arxiv.org/abs/2309.03075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timothygebhard/ml4ptp">https://github.com/timothygebhard/ml4ptp</a></li>
<li>paper_authors: Timothy D. Gebhard, Daniel Angerhausen, Björn S. Konrad, Eleonora Alei, Sascha P. Quanz, Bernhard Schölkopf</li>
<li>for: 这篇论文的目的是提出一个新的、数据驱动的压力-温度（PT）�profile parameterization scheme，以便改善行星大气层的探测和理解。</li>
<li>methods: 这篇论文使用一个基于神经网络的隐藏Variable模型，以学习一个分布 над PT profile的函数，并使用一个对应的解oder网络将 $P$ 转换为 $T$.</li>
<li>results: 在对两个公开可用的自我一致PT profile数据集进行训练和评估后，发现这个方法可以在比较少的 Parameters下，实现与基于多个 Parameters的基eline方法相同或更好的适摄性。在一个基于现有文献的AR中，使用这个方法（仅使用两个参数）可以实现一个更紧密、更准确的 posterior distribution over PT profile，并且还可以将AR的运算速度提高到三倍以上。<details>
<summary>Abstract</summary>
Atmospheric retrievals (AR) of exoplanets typically rely on a combination of a Bayesian inference technique and a forward simulator to estimate atmospheric properties from an observed spectrum. A key component in simulating spectra is the pressure-temperature (PT) profile, which describes the thermal structure of the atmosphere. Current AR pipelines commonly use ad hoc fitting functions here that limit the retrieved PT profiles to simple approximations, but still use a relatively large number of parameters. In this work, we introduce a conceptually new, data-driven parameterization scheme for physically consistent PT profiles that does not require explicit assumptions about the functional form of the PT profiles and uses fewer parameters than existing methods. Our approach consists of a latent variable model (based on a neural network) that learns a distribution over functions (PT profiles). Each profile is represented by a low-dimensional vector that can be used to condition a decoder network that maps $P$ to $T$. When training and evaluating our method on two publicly available datasets of self-consistent PT profiles, we find that our method achieves, on average, better fit quality than existing baseline methods, despite using fewer parameters. In an AR based on existing literature, our model (using two parameters) produces a tighter, more accurate posterior for the PT profile than the five-parameter polynomial baseline, while also speeding up the retrieval by more than a factor of three. By providing parametric access to physically consistent PT profiles, and by reducing the number of parameters required to describe a PT profile (thereby reducing computational cost or freeing resources for additional parameters of interest), our method can help improve AR and thus our understanding of exoplanet atmospheres and their habitability.
</details>
<details>
<summary>摘要</summary>
通常情况下，外星 planet 的大气 retrieval 都是通过一种 bayesian inference 技术和一个前向模拟器来估算大气属性从观测 спектrum 中。在模拟 спектrum 时，一个关键的组件是压力-温度（PT）规则，它描述了大气的热结构。现有的 AR 管道通常使用一些假设来限制获取的 PT profile 的形式，但仍然使用较多的参数。在这个工作中，我们提出了一种新的、数据驱动的参数化方法，不需要明确PT profile的函数形式假设，并且使用 fewer 参数 than existing methods。我们的方法包括一个 latent variable model（基于神经网络），它学习一个分布 sobre 函数（PT profile）。每个 profile 被表示为一个低维度的向量，可以用来 condition 一个 decoder network 将 $P$ 映射到 $T$。在训练和评估我们的方法时，我们发现在两个公共可用的数据集上，我们的方法可以在平均上实现更好的适应质量，尽管使用 fewer 参数。在基于现有文献的 AR 中，我们的模型（使用两个参数）可以生成一个更紧凑、更准确的 posterior  для PT profile，而且也可以提高计算速度，比基eline 方法多于三倍。通过提供 физиikal consistent PT profile 的参数化访问，并且降低计算 cost 或释放更多的参数来描述 PT profile，我们的方法可以帮助改进 AR，从而提高外星 planet 大气的理解和可居住性。
</details></li>
</ul>
<hr>
<h2 id="Character-Queries-A-Transformer-based-Approach-to-On-Line-Handwritten-Character-Segmentation"><a href="#Character-Queries-A-Transformer-based-Approach-to-On-Line-Handwritten-Character-Segmentation" class="headerlink" title="Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation"></a>Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03072">http://arxiv.org/abs/2309.03072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jungomi/character-queries">https://github.com/jungomi/character-queries</a></li>
<li>paper_authors: Michael Jungo, Beat Wolf, Andrii Maksai, Claudiu Musat, Andreas Fischer</li>
<li>for: 这个论文的目的是提出一种基于Transformer架构的在线手写文本分割方法，用于帮助手写识别模型进一步改进分割精度。</li>
<li>methods: 这种方法使用了一种基于$k$-means算法的 clustering 分配方法，将样本点与文本中的字符进行匹配，并在Transformer搅拌块中使用学习的字符查询来形成每个分支。</li>
<li>results: 在两个常用的在线手写数据集IAM-OnDB和HANDS-VNOnDB上， authors 创建了分割ground truth，并对多种方法进行评估，得出了总体最佳的结果。<details>
<summary>Abstract</summary>
On-line handwritten character segmentation is often associated with handwriting recognition and even though recognition models include mechanisms to locate relevant positions during the recognition process, it is typically insufficient to produce a precise segmentation. Decoupling the segmentation from the recognition unlocks the potential to further utilize the result of the recognition. We specifically focus on the scenario where the transcription is known beforehand, in which case the character segmentation becomes an assignment problem between sampling points of the stylus trajectory and characters in the text. Inspired by the $k$-means clustering algorithm, we view it from the perspective of cluster assignment and present a Transformer-based architecture where each cluster is formed based on a learned character query in the Transformer decoder block. In order to assess the quality of our approach, we create character segmentation ground truths for two popular on-line handwriting datasets, IAM-OnDB and HANDS-VNOnDB, and evaluate multiple methods on them, demonstrating that our approach achieves the overall best results.
</details>
<details>
<summary>摘要</summary>
在线手写字符分 segmentation 常常与手写识别相关，尽管识别模型包含了定位相关的机制，但通常不够准确地分 segmentation。解耦分 segmentation 和识别可以解放更多的潜在用途。我们专注于知道过程中的文本束缚，在这种情况下，字符分 segmentation 变成了将样本点掌握轨迹与字符在文本中的匹配问题。受 $k$-means 聚合算法启发，我们从cluster分配的角度出发，并在Transformer嵌入oder块中学习每个cluster的字符查询。为评估我们的方法质量，我们创建了两个常见在线手写数据集的字符分 segmentation真实值，并评估了多种方法，并示出我们的方法在总体上获得了最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Active-Subspaces-for-Effective-and-Scalable-Uncertainty-Quantification-in-Deep-Neural-Networks"><a href="#Learning-Active-Subspaces-for-Effective-and-Scalable-Uncertainty-Quantification-in-Deep-Neural-Networks" class="headerlink" title="Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks"></a>Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03061">http://arxiv.org/abs/2309.03061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Jantre, Nathan M. Urban, Xiaoning Qian, Byung-Jun Yoon</li>
<li>for: 提供了Well-calibrated predictions with quantified uncertainty and robustness for neural networks.</li>
<li>methods: 使用constructing a low-dimensional subspace of the neural network parameters to reduce computational complexity and enable effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods or variational inference.</li>
<li>results: 提供了可靠的预测和稳定的uncertainty estimates for various regression tasks.<details>
<summary>Abstract</summary>
Bayesian inference for neural networks, or Bayesian deep learning, has the potential to provide well-calibrated predictions with quantified uncertainty and robustness. However, the main hurdle for Bayesian deep learning is its computational complexity due to the high dimensionality of the parameter space. In this work, we propose a novel scheme that addresses this limitation by constructing a low-dimensional subspace of the neural network parameters-referred to as an active subspace-by identifying the parameter directions that have the most significant influence on the output of the neural network. We demonstrate that the significantly reduced active subspace enables effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods, otherwise computationally intractable, or variational inference. Empirically, our approach provides reliable predictions with robust uncertainty estimates for various regression tasks.
</details>
<details>
<summary>摘要</summary>
bayesian 推论 для神经网络，或 bayesian 深度学习，具有提供准确的预测和质量量化的不确定性的潜力。然而，bayesian 深度学习的主要障碍是其参数空间的维度太高，导致计算复杂度过高。在这种工作中，我们提出了一种新的方案，即在神经网络参数空间中构建一个低维的子空间（称为活跃子空间），并将神经网络输出的影响因素约束在这个子空间中。我们证明了这个减少后的活跃子空间可以实现有效和可扩展的bayesian推论，通过MC抽样方法或变量推断。我们的方法在各种回归任务上提供了可靠的预测和强度的不确定性估计。
</details></li>
</ul>
<hr>
<h2 id="CoLA-Exploiting-Compositional-Structure-for-Automatic-and-Efficient-Numerical-Linear-Algebra"><a href="#CoLA-Exploiting-Compositional-Structure-for-Automatic-and-Efficient-Numerical-Linear-Algebra" class="headerlink" title="CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra"></a>CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03060">http://arxiv.org/abs/2309.03060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wilson-labs/cola">https://github.com/wilson-labs/cola</a></li>
<li>paper_authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson</li>
<li>for:  Linear algebra problems in machine learning, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation.</li>
<li>methods:  Proposes a simple and general framework called CoLA (Compositional Linear Algebra) that combines a linear operator abstraction with compositional dispatch rules to construct memory and runtime efficient numerical algorithms.</li>
<li>results:  Accelerates many algebraic operations and makes it easy to prototype matrix structures and algorithms, with applications in partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.<details>
<summary>Abstract</summary>
Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named CoLA (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-in tool for virtually any computational effort that requires linear algebra. We showcase its efficacy across a broad range of applications, including partial differential equations, Gaussian processes, equivariant model construction, and unsupervised learning.
</details>
<details>
<summary>摘要</summary>
很多机器学习和科学领域都需要大规模的线性代数问题，如特征值分解、解linear系统、计算矩阵幂和跟踪估计。这些矩阵通常具有克罗内cker、卷积、块对角、和乘法结构。在这篇论文中，我们提出了一个简单 yet 通用的大规模线性代数问题解决方案，名为CoLA（Compositional Linear Algebra）。通过将线性运算抽象与compositional发送规则相结合，CoLA自动构建了高效的内存和运行时数值算法。此外，CoLA还提供了内存高效的自动微分、低精度计算和GPU加速，并在JAX和PyTorch中支持多个发送规则。CoLA可以加速许多线性运算，同时使得创建矩阵结构和算法变得容易，提供了许多应用领域的Drop-in工具。我们在各种应用中展示了CoLA的效果，包括偏微分方程、Gaussian процес序、对称型模型构建和无监督学习。
</details></li>
</ul>
<hr>
<h2 id="Automated-CVE-Analysis-for-Threat-Prioritization-and-Impact-Prediction"><a href="#Automated-CVE-Analysis-for-Threat-Prioritization-and-Impact-Prediction" class="headerlink" title="Automated CVE Analysis for Threat Prioritization and Impact Prediction"></a>Automated CVE Analysis for Threat Prioritization and Impact Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03040">http://arxiv.org/abs/2309.03040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Aghaei, Ehab Al-Shaer, Waseem Shadid, Xi Niu</li>
<li>For: The paper aims to address the challenge of accurately estimating the attack severity of publicly disclosed cybersecurity vulnerabilities and identifying potential countermeasures.* Methods: The paper proposes a novel predictive model and tool called CVEDrill, which uses machine learning techniques to estimate the CVSS vector and classify CVEs into the appropriate CWE hierarchy classes.* Results: The paper claims that CVEDrill can accurately estimate the CVSS vector and outperform state-of-the-art tools like ChaptGPT in terms of accuracy and timeliness. The paper also demonstrates the effectiveness of CVEDrill in identifying potential countermeasures for a large number of CVEs.<details>
<summary>Abstract</summary>
The Common Vulnerabilities and Exposures (CVE) are pivotal information for proactive cybersecurity measures, including service patching, security hardening, and more. However, CVEs typically offer low-level, product-oriented descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking the essential attack semantic information required for comprehensive weakness characterization and threat impact estimation. This critical insight is essential for CVE prioritization and the identification of potential countermeasures, particularly when dealing with a large number of CVEs. Current industry practices involve manual evaluation of CVEs to assess their attack severities using the Common Vulnerability Scoring System (CVSS) and mapping them to Common Weakness Enumeration (CWE) for potential mitigation identification. Unfortunately, this manual analysis presents a major bottleneck in the vulnerability analysis process, leading to slowdowns in proactive cybersecurity efforts and the potential for inaccuracies due to human errors. In this research, we introduce our novel predictive model and tool (called CVEDrill) which revolutionizes CVE analysis and threat prioritization. CVEDrill accurately estimates the CVSS vector for precise threat mitigation and priority ranking and seamlessly automates the classification of CVEs into the appropriate CWE hierarchy classes. By harnessing CVEDrill, organizations can now implement cybersecurity countermeasure mitigation with unparalleled accuracy and timeliness, surpassing in this domain the capabilities of state-of-the-art tools like ChaptGPT.
</details>
<details>
<summary>摘要</summary>
共享漏洞和曝露（CVE）是重要的cyber安全措施的基础信息，包括服务修补、安全强化等。然而，CVEs通常只提供低层级、产品特定的漏洞描述，缺乏关键的攻击semantic信息，这使得全面的漏洞特点和威胁影响的评估受到限制。这种缺失的信息是CVEDrill的出现所需的，CVEDrill是一种新的预测模型和工具，可以准确地估算CVSS向量，并准确地将CVE分类到适当的CWE层次结构中。通过CVEDrill，组织可以现在实现cyber安全防范措施的准确性和时效性，超越现有的工具 like ChaptGPT。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Polycystic-Kidney-Disease-Utilizing-Neural-Networks-for-Accurate-and-Early-Detection-through-Gene-Expression-Analysis"><a href="#Deep-Learning-for-Polycystic-Kidney-Disease-Utilizing-Neural-Networks-for-Accurate-and-Early-Detection-through-Gene-Expression-Analysis" class="headerlink" title="Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis"></a>Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03033">http://arxiv.org/abs/2309.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kapil Panda, Anirudh Mazumder</li>
<li>for: 早期诊断肾脏瘤病（PKD），以避免疾病的发展并实现有效的管理。</li>
<li>methods: 使用深度学习方法分析患者的基因表达，以实现准确和可靠的疾病检测。</li>
<li>results: 研究提出了一种可以准确预测患有PKD的患者的神经网络模型。Translation:</li>
<li>for: Early detection of polycystic kidney disease (PKD) to avoid the progression of the disease and achieve effective management.</li>
<li>methods: Using deep learning methods to analyze patient gene expressions to achieve accurate and robust predictions of possible PKD.</li>
<li>results: The study proposed a deep neural network model that can accurately predict PKD in patients based on their gene expressions.<details>
<summary>Abstract</summary>
With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
</details>
<details>
<summary>摘要</summary>
您的文本如下：肾脏癌病（PKD）可能会导致患者严重的合并症状，因为肾脏中形成肿瘤。因此，早期检测PKD非常重要，以便有效管理这种疾病。然而，各种患者特定的因素会影响诊断，使得临床医生面临着一个复杂的谜题。因此，在这项研究中，我们采用了深度学习基本的方法，以便在患者基因表达中准确预测PKD的可能性。
</details></li>
</ul>
<hr>
<h2 id="Universal-Preprocessing-Operators-for-Embedding-Knowledge-Graphs-with-Literals"><a href="#Universal-Preprocessing-Operators-for-Embedding-Knowledge-Graphs-with-Literals" class="headerlink" title="Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals"></a>Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03023">http://arxiv.org/abs/2309.03023</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/patryk.preisner/mkga">https://gitlab.com/patryk.preisner/mkga</a></li>
<li>paper_authors: Patryk Preisner, Heiko Paulheim</li>
<li>for: 该论文主要针对知识图（KG）中的实体进行压缩表示，以便更好地进行知识检索和推论。</li>
<li>methods: 该论文提出了一组通用预处理算子，可以将KG中的文字、数值、时间和图像信息转换为numerical embedding，以便与任何嵌入方法结合使用。</li>
<li>results: 在kgbench数据集上，使用三种不同的嵌入方法，对 transformed KGs 进行嵌入后，得到了promising的结果。<details>
<summary>Abstract</summary>
Knowledge graph embeddings are dense numerical representations of entities in a knowledge graph (KG). While the majority of approaches concentrate only on relational information, i.e., relations between entities, fewer approaches exist which also take information about literal values (e.g., textual descriptions or numerical information) into account. Those which exist are typically tailored towards a particular modality of literal and a particular embedding method. In this paper, we propose a set of universal preprocessing operators which can be used to transform KGs with literals for numerical, temporal, textual, and image information, so that the transformed KGs can be embedded with any method. The results on the kgbench dataset with three different embedding methods show promising results.
</details>
<details>
<summary>摘要</summary>
知识图embeddings是知识图中实体的 dense数字表示。大多数方法只关注关系信息，即实体之间的关系，而 fewer方法会考虑实体的文字描述或数字信息。这些方法通常针对特定的modalität和嵌入方法进行定制。在这篇论文中，我们提议一组通用预处理操作，可以将知识图中的文字、时间、数字和图像信息转换为可以使用任何嵌入方法的格式。对kgbench数据集的实验结果表明，这些预处理操作可以取得承诺的结果。
</details></li>
</ul>
<hr>
<h2 id="Amortised-Inference-in-Bayesian-Neural-Networks"><a href="#Amortised-Inference-in-Bayesian-Neural-Networks" class="headerlink" title="Amortised Inference in Bayesian Neural Networks"></a>Amortised Inference in Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03018">http://arxiv.org/abs/2309.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheev13/bnn_amort_inf">https://github.com/sheev13/bnn_amort_inf</a></li>
<li>paper_authors: Tommy Rochussen</li>
<li>for: 本研究目的是提出一种更加数据效率的概率元学习方法，以便在具有有限数据量的情况下进行预测。</li>
<li>methods: 我们提出了一种基于权重权值的概率元学习方法，即APOVI-BNN，它通过单个前向传播来实现权重权值的权值整合。</li>
<li>results: 我们的模型在有限数据量情况下表现出色，在一个一维回归问题和一个图像完成问题中，我们的模型在其他概率元学习模型中表现出色。<details>
<summary>Abstract</summary>
Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.   In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional variational inference, despite the fact that the amortised inference is performed in a single forward pass. We then discuss how the APOVI-BNN may be viewed as a new member of the neural process family, motivating the use of neural process training objectives for potentially better predictive performance on complex problems as a result. Finally, we assess the predictive performance of the APOVI-BNN against other probabilistic meta-models in both a one-dimensional regression problem and in a significantly more complex image completion setting. In both cases, when the amount of training data is limited, our model is the best in its class.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>机器学习框架中的元学习是通过多个数据集进行训练，以生成新数据集上的预测。在过去几年中，对于probabilistic meta-learning而言，研究者们对其进行了大量的关注，但是现有的许多probabilistic meta-模型具有一个共同的问题，即它们需要很多数据集来生成高质量的预测和准确度估计。在许多应用中，这些数据集的量可能不够多。在这个论文中，我们提出了一种更加数据效率的元学习方法，通过权重参数化的概率meta-学习，引入了 Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN)。我们首先表明，我们的approximate posterior obtained under our amortized scheme和traditional variational inference scheme的质量相似或更高，即使在单个前进推进行权重参数化的概率meta-学习。然后，我们讨论了APOVI-BNN如何被视为一种新的神经过程成员，并且motivate使用神经过程训练目标可能更好地适应复杂问题。最后，我们评估了APOVI-BNN的预测性能与其他probabilistic meta-模型在一个一维回归问题和一个更加复杂的图像完成问题中。在这两个问题中，当训练数据有限时，我们的模型成为其类型中的最佳。
</details></li>
</ul>
<hr>
<h2 id="SymED-Adaptive-and-Online-Symbolic-Representation-of-Data-on-the-Edge"><a href="#SymED-Adaptive-and-Online-Symbolic-Representation-of-Data-on-the-Edge" class="headerlink" title="SymED: Adaptive and Online Symbolic Representation of Data on the Edge"></a>SymED: Adaptive and Online Symbolic Representation of Data on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03014">http://arxiv.org/abs/2309.03014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Hofstätter, Shashikant Ilager, Ivan Lujic, Ivona Brandic</li>
<li>for: 该论文旨在处理互联网物联网（IoT）生成的数据，使用Edge computing模式将数据处理靠近数据源。</li>
<li>methods: 该论文提出了一种Symbolic Representation（SR）算法，用于将原始数据转换为符号，以便在Edge设备上进行数据分析（例如异常检测和趋势预测）。SR算法是一种可靠的数据压缩技术，但现有的SR算法都是中央化的设计，无法处理实时数据。该论文提出了一种在线、适应、分布式的Symbolic Edge Data表示方法（SymED）。</li>
<li>results: 该论文的实验结果表明，SymED可以（i）将原始数据压缩到9.5%的平均压缩率；(ii)在DTW空间保持低的重建误差（13.25）；(iii)同时提供在线适应性，以便处理实时流动的IoT数据，typical latency为42ms每个符号。<details>
<summary>Abstract</summary>
The edge computing paradigm helps handle the Internet of Things (IoT) generated data in proximity to its source. Challenges occur in transferring, storing, and processing this rapidly growing amount of data on resource-constrained edge devices. Symbolic Representation (SR) algorithms are promising solutions to reduce the data size by converting actual raw data into symbols. Also, they allow data analytics (e.g., anomaly detection and trend prediction) directly on symbols, benefiting large classes of edge applications. However, existing SR algorithms are centralized in design and work offline with batch data, which is infeasible for real-time cases. We propose SymED - Symbolic Edge Data representation method, i.e., an online, adaptive, and distributed approach for symbolic representation of data on edge. SymED is based on the Adaptive Brownian Bridge-based Aggregation (ABBA), where we assume low-powered IoT devices do initial data compression (senders) and the more robust edge devices do the symbolic conversion (receivers). We evaluate SymED by measuring compression performance, reconstruction accuracy through Dynamic Time Warping (DTW) distance, and computational latency. The results show that SymED is able to (i) reduce the raw data with an average compression rate of 9.5%; (ii) keep a low reconstruction error of 13.25 in the DTW space; (iii) simultaneously provide real-time adaptability for online streaming IoT data at typical latencies of 42ms per symbol, reducing the overall network traffic.
</details>
<details>
<summary>摘要</summary>
Edge compute 模式可以处理互联网对象（IoT）生成的数据在其原始位置附近。但是，将这样快速增长的数据转移、存储和处理在资源有限的边缘设备上存在挑战。symbolic representation（SR）算法是一种有 promise 的解决方案，可以将实际的Raw data 转换为符号，从而降低数据大小。此外，SR 还允许在符号上进行数据分析（例如，异常检测和趋势预测），对大多数边缘应用程序产生了好处。然而，现有的 SR 算法都是中央化的设计，并且在批处理数据的情况下进行了线上工作，这对实时情况不太可靠。我们提出了SymED - 符号Edge数据表示方法，即在线、适应性、分布式的Symbolic Representation方法。SymED基于Adaptive Brownian Bridge-based Aggregation（ABBA），假设低功率IoT设备进行初步数据压缩（发送器），而更 robust的边缘设备进行符号转换（接收器）。我们通过测量压缩性能、重建精度通过动态时间戳（DTW）距离和计算延迟来评估SymED。结果显示，SymED能够：(i) 将原始数据压缩到平均压缩率为9.5%；(ii) 在DTW空间保持低于13.25的重建错误；(iii) 同时提供在线适应性，对常见42ms每个符号的实时串流IoT数据进行实时适应。
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Explanation-of-Activation-Sparsity-through-Flat-Minima-and-Adversarial-Robustness"><a href="#Theoretical-Explanation-of-Activation-Sparsity-through-Flat-Minima-and-Adversarial-Robustness" class="headerlink" title="Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness"></a>Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03004">http://arxiv.org/abs/2309.03004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Peng, Lei Qi, Yinghuan Shi, Yang Gao<br>for:这篇论文旨在解释 activation sparsity 的出现，以及如何采用 gradient sparsity 来解释其 Emergence。methods:这篇论文使用了 theoretical explanation 和 random matrix theory (RMT) 来分析 activation sparsity 的出现。results:这篇论文提出了两个 plug-and-play module 和一个 radical modification，以提高训练和推理的效率。 Validational experiments 表明这些 modify 可以提高 sparsity，并且可以降低训练和推理的成本。<details>
<summary>Abstract</summary>
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources of flatness when arguing sparsities' necessity, we discover the phenomenon of spectral concentration, i.e., the ratio between the largest and the smallest non-zero singular values of weight matrices is small. We utilize random matrix theory (RMT) as a powerful theoretical tool to analyze stochastic gradient noises and discuss the emergence of spectral concentration. With these insights, we propose two plug-and-play modules for both training from scratch and sparsity finetuning, as well as one radical modification that only applies to from-scratch training. Another under-testing module for both sparsity and flatness is also immediate from our theories. Validational experiments are conducted to verify our explanation. Experiments for productivity demonstrate modifications' improvement in sparsity, indicating further theoretical cost reduction in both training and inference.
</details>
<details>
<summary>摘要</summary>
近期观察到多层感知（MLP）层中的活动稀畴可以带来很大的计算成本减少的机会。尽管一些研究归因于训练动力学，但理论上解释活动稀畴的出现仍然受到限制，只有在浅网络、小训练步骤和修改训练中才能解释。为了填补这些差距，我们提出了梯度稀畴的概念，作为活动稀畴的来源，并提出了基于这个概念的理论解释，它解释了梯度稀畴和活动稀畴是对抗攻击的必要步骤，这是因为隐藏特征和参数的平均值的折衔。这种理论适用于标准地训练的层 нор化纯MLP，并可以扩展到转换器或其他架构。为了消除其他源的平均值，我们发现了特征集中性现象，即权重矩阵的最大和最小非零特征值之间的比率很小。我们利用随机矩阵理论（RMT）作为一种强大的理论工具，分析随机梯度噪声，并讨论emergence of spectral concentration。基于这些发现，我们提出了两个插件和完善模块，以及一个 радикаль modification，只适用于从头开始训练。另外，我们还提出了一个尚未测试的模块，可以用于 both sparsity和平均性。验证性实验表明，我们的解释是正确的。产品性实验表明，这些修改可以提高减少训练和推理的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Natural-and-Robust-Walking-using-Reinforcement-Learning-without-Demonstrations-in-High-Dimensional-Musculoskeletal-Models"><a href="#Natural-and-Robust-Walking-using-Reinforcement-Learning-without-Demonstrations-in-High-Dimensional-Musculoskeletal-Models" class="headerlink" title="Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models"></a>Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02976">http://arxiv.org/abs/2309.02976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle</li>
<li>for: The paper aims to develop a reinforcement learning (RL) method for natural walking in complex natural environments, with the goal of achieving human-like walking without relying on extensive expert data sets.</li>
<li>methods: The paper uses a reinforcement learning approach to train a controller that can handle the multi-objective control problem of stability, robustness, and energy efficiency in bipedal walking. The controller is trained using a simulation environment and optimizes energy minimization as the objective function.</li>
<li>results: The paper achieves human-like walking with bipedal biomechanical models in 3D using reinforcement learning, without relying on extensive expert data sets. The resulting controllers are robust against perturbations and can adapt to new behaviors, demonstrating the potential of RL for studying human walking in complex natural environments.<details>
<summary>Abstract</summary>
Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive use of expert data sets. This strong reliance on demonstrations often results in brittle policies and limits the application to new behaviors, especially considering the potential variety of movements for high-dimensional musculoskeletal models in 3D. Achieving natural locomotion with RL without sacrificing its incredible robustness might pave the way for a novel approach to studying human walking in complex natural environments. Videos: https://sites.google.com/view/naturalwalkingrl
</details>
<details>
<summary>摘要</summary>
人类在复杂的自然环境中能够表现出稳定的双脚行走。每一步，他们能够有效地调整生物机械动力学和神经信号的交互，以使得行走具有对地面条件不确定性的鲁棒性。然而，nervous system如何解决musculoskeletal redundancy以解决多目标控制问题，包括稳定性、鲁棒性和能效性，仍未完全了解。在计算机 simulations中，能量最小化已经被证明是一个成功的优化目标，通过轨迹优化或刺激控制方法来复制自然的行走。然而，这些方法通常只关注特定的动作，而 resulting controllers 有限制性，无法赔偿干扰。在机器人学中，使用奖励学习（RL）方法已经实现了高稳定性（而且高效）的四足系统行走，但生成人类类似的行走需要大量的专家数据集。这种强依赖于示例的方式通常会导致 brittle policies 和限制应用于新的行为，特别是考虑到高维musculoskeletal模型在3D中的可能性的多样性。实现自然的行走通过RL而不 sacrificing its incredible robustness 可能会开启一种新的方法来研究人类在复杂自然环境中的行走。Video: <https://sites.google.com/view/naturalwalkingrl>
</details></li>
</ul>
<hr>
<h2 id="On-the-Impact-of-Feeding-Cost-Risk-in-Aquaculture-Valuation-and-Decision-Making"><a href="#On-the-Impact-of-Feeding-Cost-Risk-in-Aquaculture-Valuation-and-Decision-Making" class="headerlink" title="On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making"></a>On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02970">http://arxiv.org/abs/2309.02970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kevinkamm/aquaculturestochasticfeeding">https://github.com/kevinkamm/aquaculturestochasticfeeding</a></li>
<li>paper_authors: Christian Oliver Ewald, Kevin Kamm</li>
<li>for: 研究了动物商品中的随机 alimentation 成本对影响，特别是关注于养殖业。</li>
<li>methods: 使用了 soybean futures 来推断鲑鱼饲料的随机行为，假设饲料采用 Schwartz-2-factor 模型。 compare了使用决策规则，包括随机或决定性 alimentation 成本，以及不包括 alimentation 成本风险。</li>
<li>results: 发现在一些情况下，考虑到随机 alimentation 成本可以导致显著改善，而在其他情况下，决定性 alimentation 成本可以作为备用。新采用的规则都显示出了更好的性能，而计算成本几乎为零。此外，我们还使用了深度神经网络来推断决策边界，从而改进了以往的回归方法和拟合方法。<details>
<summary>Abstract</summary>
We study the effect of stochastic feeding costs on animal-based commodities with particular focus on aquaculture. More specifically, we use soybean futures to infer on the stochastic behaviour of salmon feed, which we assume to follow a Schwartz-2-factor model. We compare the decision of harvesting salmon using a decision rule assuming either deterministic or stochastic feeding costs, i.e. including feeding cost risk. We identify cases, where accounting for stochastic feeding costs leads to significant improvements as well as cases where deterministic feeding costs are a good enough proxy. Nevertheless, in all of these cases, the newly derived rules show superior performance, while the additional computational costs are negligible. From a methodological point of view, we demonstrate how to use Deep-Neural-Networks to infer on the decision boundary that determines harvesting or continuation, improving on more classical regression-based and curve-fitting methods. To achieve this we use a deep classifier, which not only improves on previous results but also scales well for higher dimensional problems, and in addition mitigates effects due to model uncertainty, which we identify in this article. effects due to model uncertainty, which we identify in this article.
</details>
<details>
<summary>摘要</summary>
我们研究生物动物商品的影响，特别是鱼养。我们使用сояbean futures来推测鲑鱼饲料的随机行为，假设采用斯威尔-2-因子模型。我们比较使用决策规则，假设饲料成本是确定的或随机的，即包括饲料成本风险。我们发现在某些情况下，考虑随机饲料成本会导致显著改善，而在其他情况下，确定饲料成本是可以作为备用的。然而，在所有情况下，我们新 derive的规则具有更高的性能，而计算成本也很低。从方法学的角度来看，我们示例了如何使用深度神经网络来推测决策边界，这有所提高过传统的回归分析和适应方法。为了实现这一点，我们使用深度分类器，不仅提高了前一代结果，还可以适应更高维度的问题，同时减轻模型不确定性的影响。
</details></li>
</ul>
<hr>
<h2 id="CR-VAE-Contrastive-Regularization-on-Variational-Autoencoders-for-Preventing-Posterior-Collapse"><a href="#CR-VAE-Contrastive-Regularization-on-Variational-Autoencoders-for-Preventing-Posterior-Collapse" class="headerlink" title="CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse"></a>CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02968">http://arxiv.org/abs/2309.02968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fotios Lygerakis, Elmar Rueckert</li>
<li>for: 解决Variational Autoencoder（VAE）的后降现象，即 latent representation 与输入 Independence。</li>
<li>methods: 提出了一种新的解决方案——强制对应regularization for Variational Autoencoders（CR-VAE），通过增加一个对应的对比目标函数，以最大化输入和其latent representation之间的信息流。</li>
<li>results: 在多个视觉数据集上测试，CR-VAE 比前一个方法更好地避免后降现象，并且表现更好。<details>
<summary>Abstract</summary>
The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
</details>
<details>
<summary>摘要</summary>
“Variational Autoencoder（VAE）经常会面临“ posterior collapse”现象，即生成的伪阶梯表现与输入无关。这导致输入的表现受损，归因于VAE的目标函数所带来的限制。在这个研究中，我们提出一个新的解决方案，即对VAE进行对比调整（CR-VAE）。我们的方法是通过增加原始VAE的对比目标，以 Maximize the mutual information between the representations of similar visual inputs。这策略可以确保输入和其伪阶梯表现之间的信息流汇流，彻底避免 posterior collapse。我们在一系列的视觉数据集上评估了我们的方法，并证明了CR-VAE可以更好地避免 posterior collapse。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="M3D-NCA-Robust-3D-Segmentation-with-Built-in-Quality-Control"><a href="#M3D-NCA-Robust-3D-Segmentation-with-Built-in-Quality-Control" class="headerlink" title="M3D-NCA: Robust 3D Segmentation with Built-in Quality Control"></a>M3D-NCA: Robust 3D Segmentation with Built-in Quality Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02954">http://arxiv.org/abs/2309.02954</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Kalkhof, Anirban Mukhopadhyay</li>
<li>for: 这篇论文的目的是提出一种具有效率和可靠性的医疗影像分类方法，以应对资源受限的环境和对影像领域的变化。</li>
<li>methods: 本文使用的方法是基于神经细胞自动机（NCA）的分类方法，并通过n-level patchification来实现3D医疗影像的分类。此外，本文还提出了一个基于M3D-NCA的质量指标，可自动检测NCAs中的错误。</li>
<li>results: 相比于UNet模型，M3D-NCA在脑径和膀胱分类中表现出2%的 Dies积分误差，并且可以在Raspberry Pi 4 Model B（2GB RAM）上运行。这表明M3D-NCA可能是一个有效和可靠的医疗影像分类方法，尤其在资源受限的环境中。<details>
<summary>Abstract</summary>
Medical image segmentation relies heavily on large-scale deep learning models, such as UNet-based architectures. However, the real-world utility of such models is limited by their high computational requirements, which makes them impractical for resource-constrained environments such as primary care facilities and conflict zones. Furthermore, shifts in the imaging domain can render these models ineffective and even compromise patient safety if such errors go undetected. To address these challenges, we propose M3D-NCA, a novel methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D medical images using n-level patchification. Moreover, we exploit the variance in M3D-NCA to develop a novel quality metric which can automatically detect errors in the segmentation process of NCAs. M3D-NCA outperforms the two magnitudes larger UNet models in hippocampus and prostate segmentation by 2% Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the potential of M3D-NCA as an effective and efficient alternative for medical image segmentation in resource-constrained environments.
</details>
<details>
<summary>摘要</summary>
医疗图像分割依赖于大规模深度学习模型，如UNet基于的建筑。然而，实际应用中这些模型的计算需求很高，使其在资源有限的环境中，如初级医疗机构和武装冲突区域，成为不切实际的。此外，图像领域的变化可以让这些模型失效，甚至威胁 patient safety 如果这些错误未经检测。为解决这些挑战，我们提出了 M3D-NCA，一种新的方法，利用神经细胞自动机（NCA）分割三维医疗图像，使用 n-level 补丁化。此外，我们利用 M3D-NCA 的变异来开发一种新的质量指标，可以自动检测 NCAs 分割过程中的错误。M3D-NCA 在 hippocampus 和肾脏分割中比两个 UNet 模型高出 2% Dice，并且可以在 Raspberry Pi 4 Model B（2GB RAM）上运行。这说明 M3D-NCA 可以作为医疗图像分割的有效和高效的替代方案。
</details></li>
</ul>
<hr>
<h2 id="EvoCLINICAL-Evolving-Cyber-Cyber-Digital-Twin-with-Active-Transfer-Learning-for-Automated-Cancer-Registry-System"><a href="#EvoCLINICAL-Evolving-Cyber-Cyber-Digital-Twin-with-Active-Transfer-Learning-for-Automated-Cancer-Registry-System" class="headerlink" title="EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System"></a>EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03246">http://arxiv.org/abs/2309.03246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simula-complex/evoclinical">https://github.com/simula-complex/evoclinical</a></li>
<li>paper_authors: Chengjie Lu, Qinghua Xu, Tao Yue, Shaukat Ali, Thomas Schwitalla, Jan F. Nygård</li>
<li>for: The paper is written for the Cancer Registry of Norway (CRN) and its stakeholders, providing a solution for the correct operation of the automated cancer registry system (GURI) and the synchronization of the cyber-cyber digital twin (CCDT) with the evolving GURI.</li>
<li>methods: The paper proposes EvoCLINICAL, a method that fine-tunes the pretrained CCDT with a dataset labelled by querying a new GURI version, using a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset.</li>
<li>results: The paper demonstrates the effectiveness of EvoCLINICAL through evaluation on three evolution processes, showing that the precision, recall, and F1 score are all greater than 91%. Additionally, the paper shows that employing active learning in EvoCLINICAL consistently improves its performance.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为抗癌 региistry of Norway（CRN）和其各方面利益者提供的解决方案，确保自动化抗癌注册系统（GURI）的正确运行，以及 cyber-cyber数字生物体（CCDT）与不断发展的 GURI 同步。</li>
<li>methods: 论文提出了 EvoCLINICAL，一种方法，通过使用遗传算法选择优化的肿瘤消息子集，将预先训练的 CCDT 与新版 GURI 标注的数据进行微调。</li>
<li>results: 论文通过对三次进化过程进行评估，显示 EvoCLINICAL 的效果是非常高，其精度、回归率和 F1 分数都大于 91%。此外，论文还表明，在 EvoCLINICAL 中使用活动学习可以逐次提高其表现。<details>
<summary>Abstract</summary>
The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, and hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.
</details>
<details>
<summary>摘要</summary>
挪威癌症注册系统（CRN）收集癌症患者信息，通过收到医疗机构（如医学实验室和医院）发送的癌症消息。这些消息被自动化癌症注册系统：GURI validate。GURI的正常运行是关键，因为它为癌症研究提供关键的癌症相关统计，并且是基础设施。为了促进GURI的研发和分析，我们提出了基于数字响应的癌症数字双（CCDT）的构建。然而，GURI不断演化，因为新的医学诊断和治疗、技术进步等。因此，CCDT也需要不断更新，以同步 avec GURI。这种同步困难在于，需要新版本的GURI标注的大量数据来更新CCDT。为解决这个问题，我们提出了EvoCLINICAL，它将以前版本的CCDT作为预训练模型，并使用新版本GURI标注的数据进行微调。EvoCLINICAL使用遗传算法选择候选 dataset中的最佳子集，并将其提交给GURI进行查询。我们对EvoCLINICAL进行了三次演化测试，结果显示，精度、准确率和F1分数都高于91%，这表明EvoCLINICAL的效果。此外，我们将EvoCLINICAL的活动学习部分替换为随机选择，以研究转移学习对EvoCLINICAL的总表现的贡献。结果表明，在EvoCLINICAL中使用活动学习可以持续提高其表现。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-quantum-classical-fusion-neural-network-to-improve-protein-ligand-binding-affinity-predictions-for-drug-discovery"><a href="#A-hybrid-quantum-classical-fusion-neural-network-to-improve-protein-ligand-binding-affinity-predictions-for-drug-discovery" class="headerlink" title="A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery"></a>A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03919">http://arxiv.org/abs/2309.03919</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Banerjee, S. He Yuxun, S. Konakanchi, L. Ogunfowora, S. Roy, S. Selvaras, L. Domingo, M. Chehimi, M. Djukic, C. Johnson</li>
<li>for: 预测药物和蛋白质之间的绑定Affinity，特别是在疾病发展中直接影响蛋白质的蛋白质。</li>
<li>methods: 提出了一种新的Hybrid量子-классический深度学习模型，结合3D和空间图像卷积神经网络，并在优化的量子架构中进行了 synergistic Integration。</li>
<li>results: 对比于现有的类型模型，提出的模型在预测绑定Affinity方面提高了6%的准确率，同时对比于前期类型方法，其 convergence性能显著更稳定。<details>
<summary>Abstract</summary>
The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. Simulation results demonstrate a 6% improvement in prediction accuracy relative to existing classical models, as well as a significantly more stable convergence performance compared to previous classical approaches.
</details>
<details>
<summary>摘要</summary>
领域的药物发现核心在于准确预测药物分子和目标蛋白之间的绑定亲和力，特别是当这些蛋白直接影响疾病进程时。然而，估计绑定亲和力需要较大的金融和计算资源。现有的方法使用了经典的机器学习（ML）技术，而新兴的量子机器学习（QML）模型则表现出了改善表现，具有内置的并行性和数据维度的加法性。然而，现有的模型受到了稳定性和预测精度的限制。这篇论文提出了一种新的量子-古典深度学习模型，用于预测药物绑定亲和力。特别是，该模型 synergistically  integrate了3D和空间图 convolutional neural networks within an optimized quantum architecture。实验结果表明，该模型相比现有的古典模型，提高了预测精度的6%，同时也比前一些古典方法更加稳定地跑出结果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-irregular-water-demands-with-physics-informed-machine-learning-to-inform-leakage-detection"><a href="#Estimating-irregular-water-demands-with-physics-informed-machine-learning-to-inform-leakage-detection" class="headerlink" title="Estimating irregular water demands with physics-informed machine learning to inform leakage detection"></a>Estimating irregular water demands with physics-informed machine learning to inform leakage detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02935">http://arxiv.org/abs/2309.02935</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swn-group-at-tu-berlin/lila-pinn">https://github.com/swn-group-at-tu-berlin/lila-pinn</a></li>
<li>paper_authors: Ivo Daniel, Andrea Cominola</li>
<li>for:  This paper aims to develop a physics-informed machine learning algorithm for timely identifying and accurately localizing leakages in drinking water distribution networks.</li>
<li>methods:  The algorithm utilizes pressure data and a fully connected neural network to estimate unknown irregular water demands, leveraging the Bernoulli equation and effectively linearizing the leakage detection problem.</li>
<li>results:  The algorithm was tested on data from the L-Town benchmark network and showed good capability for estimating most irregular demands, with R2 larger than 0.8. Identification results for leakages under the presence of irregular demands could be improved by a factor of 5.3 for abrupt leaks and a factor of 3.0 for incipient leaks when compared to results disregarding irregular demands.<details>
<summary>Abstract</summary>
Leakages in drinking water distribution networks pose significant challenges to water utilities, leading to infrastructure failure, operational disruptions, environmental hazards, property damage, and economic losses. The timely identification and accurate localisation of such leakages is paramount for utilities to mitigate these unwanted effects. However, implementation of algorithms for leakage detection is limited in practice by requirements of either hydraulic models or large amounts of training data. Physics-informed machine learning can utilise hydraulic information thereby circumventing both limitations. In this work, we present a physics-informed machine learning algorithm that analyses pressure data and therefrom estimates unknown irregular water demands via a fully connected neural network, ultimately leveraging the Bernoulli equation and effectively linearising the leakage detection problem. Our algorithm is tested on data from the L-Town benchmark network, and results indicate a good capability for estimating most irregular demands, with R2 larger than 0.8. Identification results for leakages under the presence of irregular demands could be improved by a factor of 5.3 for abrupt leaks and a factor of 3.0 for incipient leaks when compared the results disregarding irregular demands.
</details>
<details>
<summary>摘要</summary>
饮水供应网络中的泄漏问题对水公司带来了重大挑战，导致基础设施失效、运营干扰、环境危害、财务损失等。快速识别和准确定位泄漏是Utility公司控制这些不良影响的关键。然而，实施泄漏检测算法在实践中受到了水力模型的要求和大量的训练数据的限制。 физи学 Informed machine learning可以利用水力信息，这样就可以 circumvent这些限制。在这种工作中，我们提出了一种基于物理学习的泄漏检测算法，通过分析压力数据，以全连接神经网络的方式来估算不可知的异常水需求，最终利用白劳利方程，有效地线性化泄漏检测问题。我们的算法在L-Town标准网络数据集上进行测试，结果显示，我们的算法可以准确地估算大多数异常水需求，R2值大于0.8。在存在异常水需求的情况下，泄漏的标识结果可以提高了5.3倍（偏泄漏）和3.0倍（incipient leakage）。
</details></li>
</ul>
<hr>
<h2 id="GroupEnc-encoder-with-group-loss-for-global-structure-preservation"><a href="#GroupEnc-encoder-with-group-loss-for-global-structure-preservation" class="headerlink" title="GroupEnc: encoder with group loss for global structure preservation"></a>GroupEnc: encoder with group loss for global structure preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02917">http://arxiv.org/abs/2309.02917</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Novak, Sofie Van Gassen, Yvan Saeys</li>
<li>for: 本研究旨在提出一种基于Variational Autoencoder（VAE）和SQuadMDS算法的深度学习模型，用于降维高维数据并 preserve its结构。</li>
<li>methods: 该模型使用了一种叫做’group loss’函数，以避免VAE模型中的全局结构扭曲问题，同时保持模型参数化和架构灵活。</li>
<li>results: 通过使用公共可用的生物单细胞脱氧核酸数据集进行验证，本研究发现该模型可以更好地 preserve 数据的结构，并且在RNX曲线上表现较好。<details>
<summary>Abstract</summary>
Recent advances in dimensionality reduction have achieved more accurate lower-dimensional embeddings of high-dimensional data. In addition to visualisation purposes, these embeddings can be used for downstream processing, including batch effect normalisation, clustering, community detection or trajectory inference. We use the notion of structure preservation at both local and global levels to create a deep learning model, based on a variational autoencoder (VAE) and the stochastic quartet loss from the SQuadMDS algorithm. Our encoder model, called GroupEnc, uses a 'group loss' function to create embeddings with less global structure distortion than VAEs do, while keeping the model parametric and the architecture flexible. We validate our approach using publicly available biological single-cell transcriptomic datasets, employing RNX curves for evaluation.
</details>
<details>
<summary>摘要</summary>
近期的维度减少技术已经实现了更高精度的lower-dimensional嵌入。除了可视化目的外，这些嵌入还可以用于下游处理，包括批处理normalization、聚类、社区探测或轨迹推断。我们基于variational autoencoder（VAE）和stochastic quartet loss（SQuadMDS）算法提出了一种深度学习模型，称之为GroupEnc。我们的编码器模型使用了'group loss'函数来创建具有较少全局结构扭曲的嵌入，而保持模型参数化和架构灵活。我们使用公共可用的生物单细胞肽转录数据集进行验证，并使用RNX曲线进行评估。
</details></li>
</ul>
<hr>
<h2 id="Persona-aware-Generative-Model-for-Code-mixed-Language"><a href="#Persona-aware-Generative-Model-for-Code-mixed-Language" class="headerlink" title="Persona-aware Generative Model for Code-mixed Language"></a>Persona-aware Generative Model for Code-mixed Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02915">http://arxiv.org/abs/2309.02915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victor7246/paradox">https://github.com/victor7246/paradox</a></li>
<li>paper_authors: Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在开发一种基于人物的代码混淆生成模型，以生成更加真实的混淆文本。</li>
<li>methods: 该模型基于Transformer编码器-解码器，通过编码每句话根据用户人物来生成混淆文本，并提出了一个对齐模块以 garantizar生成的序列匹配实际混淆文本的格式。</li>
<li>results: 对比非人物基于模型，PARADOX在四个新指标中平均表现出1.6个BLEU分、47%更好的混淆性和32%更好的语义凝集性。<details>
<summary>Abstract</summary>
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better perplexity and 32% better semantic coherence than the non-persona-based counterparts.
</details>
<details>
<summary>摘要</summary>
《Code-mixing和script-mixing在在线社交媒体和多语言社会中很普遍。然而，用户对代码混合的偏好取决于用户的社oeconomicStatus、用户的人口结构和当地的文化环境，现有的生成模型大多忽略这些因素。在这项工作中，我们提出了一种人物意识感知的代码混合生成模型，名为PARADOX。PARADOX是一种基于Transformer的编码器-解码器模型，对于一句话来说，通过用户的人物来Conditional Encoding，生成代码混合文本，不需要单语言参考数据。我们还提出了一个对齐模块，用于重新调整生成的序列，使其更加接近实际的代码混合文本。PARADOX生成的代码混合文本具有更高的semantic coherence和linguistic validity。为评估PARADOX的人格化能力，我们提出了四种新的评价指标：CM BLEU、CM Rouge-1、CM Rouge-L和CM KS。在 average，PARADOX在这些指标中表现了1.6个BLEU点、47%的perplexity和32%的semantic coherence提高。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the Traditional Chinese version of the text, and some characters and phrases may be different in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-DNN-for-Age-of-Information-Minimization-in-UAV-assisted-Networks"><a href="#Ensemble-DNN-for-Age-of-Information-Minimization-in-UAV-assisted-Networks" class="headerlink" title="Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks"></a>Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02913">http://arxiv.org/abs/2309.02913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouhamed Naby Ndiaye, El Houcine Bergou, Hajar El Hammouti</li>
<li>For: 本研究旨在解决无人机支持网络中的年龄信息问题（Age-of-Information，AoI）。我们的目标是最小化各设备之间的预期AoI。* Methods: 我们首先 derive了AoI的闭式表达式，然后将问题转化为非束缚最小化问题，并使用Ensemble Deep Neural Network（EDNN）方法解决。具体来说，我们使用了Lagrangian函数来隐式地培养DNNs。* Results: 我们的实验表明，提议的EDNN方法可以减少预期AoI，实现了$29.5%$的减少。<details>
<summary>Abstract</summary>
This paper addresses the problem of Age-of-Information (AoI) in UAV-assisted networks. Our objective is to minimize the expected AoI across devices by optimizing UAVs' stopping locations and device selection probabilities. To tackle this problem, we first derive a closed-form expression of the expected AoI that involves the probabilities of selection of devices. Then, we formulate the problem as a non-convex minimization subject to quality of service constraints. Since the problem is challenging to solve, we propose an Ensemble Deep Neural Network (EDNN) based approach which takes advantage of the dual formulation of the studied problem. Specifically, the Deep Neural Networks (DNNs) in the ensemble are trained in an unsupervised manner using the Lagrangian function of the studied problem. Our experiments show that the proposed EDNN method outperforms traditional DNNs in reducing the expected AoI, achieving a remarkable reduction of $29.5\%$.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multimodal-Learning-Framework-for-Comprehensive-3D-Mineral-Prospectivity-Modeling-with-Jointly-Learned-Structure-Fluid-Relationships"><a href="#A-Multimodal-Learning-Framework-for-Comprehensive-3D-Mineral-Prospectivity-Modeling-with-Jointly-Learned-Structure-Fluid-Relationships" class="headerlink" title="A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships"></a>A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02911">http://arxiv.org/abs/2309.02911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zheng, Hao Deng, Ruisheng Wang, Jingjie Wu</li>
<li>for: 这个研究旨在开发一个新的多modal融合模型，用于三维矿物潜力地图（3D MPM），实现了深度网络架构的多modal融合。</li>
<li>methods: 这个模型使用了卷积神经网（CNN）和多层感知神经网（MLP），并使用了对映分析（CCA）来调整和融合多modal的特征。</li>
<li>results: 实验结果显示，这个模型在分辨矿物含有区域和预测矿物潜力方面表现出色，比其他模型更好，并且通过ablation研究确定了联合特征使用和CCA包含的好处。<details>
<summary>Abstract</summary>
This study presents a novel multimodal fusion model for three-dimensional mineral prospectivity mapping (3D MPM), effectively integrating structural and fluid information through a deep network architecture. Leveraging Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs canonical correlation analysis (CCA) to align and fuse multimodal features. Rigorous evaluation on the Jiaojia gold deposit dataset demonstrates the model's superior performance in distinguishing ore-bearing instances and predicting mineral prospectivity, outperforming other models in result analyses. Ablation studies further reveal the benefits of joint feature utilization and CCA incorporation. This research not only advances mineral prospectivity modeling but also highlights the pivotal role of data integration and feature alignment for enhanced exploration decision-making.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "three-dimensional mineral prospectivity mapping" (3D MPM) is translated as "三维矿产可能性映射" (3D MPM)* "Convolutional Neural Networks" (CNN) is translated as "卷积神经网络" (CNN)* "Multilayer Perceptrons" (MLP) is translated as "多层感知器" (MLP)* "canonical correlation analysis" (CCA) is translated as "同异分析" (CCA)* "ore-bearing instances" is translated as "矿石样本" (ore-bearing instances)* "mineral prospectivity" is translated as "矿产可能性" (mineral prospectivity)* "ablation studies" is translated as "缺除研究" (ablation studies)
</details></li>
</ul>
<hr>
<h2 id="DECODE-Data-driven-Energy-Consumption-Prediction-leveraging-Historical-Data-and-Environmental-Factors-in-Buildings"><a href="#DECODE-Data-driven-Energy-Consumption-Prediction-leveraging-Historical-Data-and-Environmental-Factors-in-Buildings" class="headerlink" title="DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings"></a>DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02908">http://arxiv.org/abs/2309.02908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Mishra, Haroon R. Lone, Aayush Mishra</li>
<li>for: 预测建筑物的能源消耗，以提高效率的能源管理。</li>
<li>methods: 使用历史能源数据、占用模式和天气情况来预测建筑物的能源消耗，并使用Long Short-Term Memory（LSTM）模型进行预测。</li>
<li>results: 比较多种预测方法，LSTM模型在短、中、长期预测方面具有较高的准确率和最佳的 mean absolute error（MAE）值，并且可以使用有限的数据进行高效的预测。<details>
<summary>Abstract</summary>
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consumption forecasts even when trained on a limited dataset. We address concerns about overfitting (variance) and underfitting (bias) through rigorous training and evaluation on real-world data. In summary, our research contributes to energy prediction by offering a robust LSTM model that outperforms alternative methods and operates with remarkable efficiency, generalizability, and reliability.
</details>
<details>
<summary>摘要</summary>
Compared to linear regression, decision trees, and random forest, the proposed LSTM model demonstrates exceptional prediction accuracy, with an R2 score of 0.97 and a mean absolute error (MAE) of 0.007. The model is also efficient in achieving accurate energy consumption forecasts even when trained on a limited dataset. To address concerns about overfitting and underfitting, the model is trained and evaluated on real-world data.In summary, this research contributes to energy prediction by offering a robust LSTM model that outperforms alternative methods and operates with remarkable efficiency, generalizability, and reliability.
</details></li>
</ul>
<hr>
<h2 id="Testing-properties-of-distributions-in-the-streaming-model"><a href="#Testing-properties-of-distributions-in-the-streaming-model" class="headerlink" title="Testing properties of distributions in the streaming model"></a>Testing properties of distributions in the streaming model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03245">http://arxiv.org/abs/2309.03245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Sampriti Roy, Yadu Vasudev</li>
<li>for: 测试分布的标准访问模型和条件访问模型中的内存限制下的分布性测试</li>
<li>methods: 使用优化的样本数量和内存占用来测试分布性，并提供了一个可靠的分布表示法</li>
<li>results: 可以efficiently学习一个峰值分布，并且可以将其扩展到更大的可分解分布集合上<details>
<summary>Abstract</summary>
We study distribution testing in the standard access model and the conditional access model when the memory available to the testing algorithm is bounded. In both scenarios, the samples appear in an online fashion and the goal is to test the properties of distribution using an optimal number of samples subject to a memory constraint on how many samples can be stored at a given time. First, we provide a trade-off between the sample complexity and the space complexity for testing identity when the samples are drawn according to the conditional access oracle. We then show that we can learn a succinct representation of a monotone distribution efficiently with a memory constraint on the number of samples that are stored that is almost optimal. We also show that the algorithm for monotone distributions can be extended to a larger class of decomposable distributions.
</details>
<details>
<summary>摘要</summary>
我们研究分布测试在标准访问模型和条件访问模型中，当内存可用于测试算法是有限的情况下。在两个场景下，样本会出现在在线的方式下，目标是使用最优的样本数量来测试分布的性质，受到内存限制如何保存样本。首先，我们提供了样本复杂性和空间复杂性之间的贸易OFF，当样本按照条件访问 oracle 采样时。然后，我们示出了可以高效地学习均衡分布的简短表示，并且内存限制是几乎最优的。最后，我们展示了该算法可以扩展到更大的分布类型。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-for-Discovering-Discrete-Symmetries"><a href="#A-Unified-Framework-for-Discovering-Discrete-Symmetries" class="headerlink" title="A Unified Framework for Discovering Discrete Symmetries"></a>A Unified Framework for Discovering Discrete Symmetries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02898">http://arxiv.org/abs/2309.02898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Karjol, Rohan Kashyap, Aditya Gopalan, Prathosh A. P</li>
<li>for: 学习一个对Symmetry的函数</li>
<li>methods: 使用多臂炮算法和梯度下降来有效地优化线性和张量函数，以实现Symmetry的探索</li>
<li>results: 在图像数字和多项式回归任务上，提出了一种有效的方法，可以快速和有效地探索Symmetry<details>
<summary>Abstract</summary>
We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
我们考虑了学习一个函数尊重一种对称性的问题，从一类对称性中选择函数。我们开发了一个统一的框架，可以在各种子群中找到对称的发现，包括地方对称、二元对称和循环对称子群。框架的核心是一种新的建筑，包括线性和张量函数，这些函数可以在对称性原理下表示对称的函数。这种结构使我们可以利用多臂投机算法和梯度下降来有效地优化线性函数和张量函数，并从中推断出 ultimately 学习的对称性。我们还讨论了张量函数的必要性。在图像数字和多项式回归任务中进行了实验，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Non-Clashing-Teaching-Maps-for-Balls-in-Graphs"><a href="#Non-Clashing-Teaching-Maps-for-Balls-in-Graphs" class="headerlink" title="Non-Clashing Teaching Maps for Balls in Graphs"></a>Non-Clashing Teaching Maps for Balls in Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02876">http://arxiv.org/abs/2309.02876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémie Chalopin, Victor Chepoi, Fionn Mc Inerney, Sébastien Ratel</li>
<li>for: This paper is focused on studying non-clashing teaching and its applications in machine learning, particularly in the context of concept learning.</li>
<li>methods: The authors use techniques from graph theory and combinatorics to derive non-clashing teaching maps (NCTMs) and non-clashing teaching dimension (NCTD) for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. They also use the theory of NP-completeness to prove lower bounds on the size of NCTMs and NCTD.</li>
<li>results: The authors show that the decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in split, co-bipartite, and bipartite graphs, and they provide matching upper bounds for trees, interval graphs, cycles, and trees of cycles. They also design an approximate NCTM$^+$ for Gromov-hyperbolic graphs of size 2.<details>
<summary>Abstract</summary>
Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\cal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \in \cal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a $T(C)$, $C \in \cal{C}$. The non-clashing teaching dimension NCTD$(\cal{C})$ of $\cal{C}$ is the minimum size of an NCTM for $\cal{C}$. NCTM$^+$ and NCTD$^+(\cal{C})$ are defined analogously, except the teacher may only use positive examples.   We study NCTMs and NCTM$^+$s for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in split, co-bipartite, and bipartite graphs. Surprisingly, we even prove that, unless the ETH fails, {\sc B-NCTD$^+$} does not admit an algorithm running in time $2^{2^{o(vc)}\cdot n^{O(1)}$, nor a kernelization algorithm outputting a kernel with $2^{o(vc)}$ vertices, where vc is the vertex cover number of $G$. These are extremely rare results: it is only the second (fourth, resp.) problem in NP to admit a double-exponential lower bound parameterized by vc (treewidth, resp.), and only one of very few problems to admit an ETH-based conditional lower bound on the number of vertices in a kernel. We complement these lower bounds with matching upper bounds. For trees, interval graphs, cycles, and trees of cycles, we derive NCTM$^+$s or NCTMs for $\mathcal{B}(G)$ of size proportional to its VC-dimension. For Gromov-hyperbolic graphs, we design an approximate NCTM$^+$ for $\mathcal{B}(G)$ of size 2.
</details>
<details>
<summary>摘要</summary>
最近， Kirkpatrick 等人（ALT 2019）和 Fallat 等人（JMLR 2023）提出了不冲突教学模型，并证明它是最有效的机器教学模型，满足 Golman 和 Mathias 的合法性标准。一个教学地图 $T$ 对于一个概念集合 $\cal{C}$ 将每个概念 $C \in \cal{C}$  assigns一个（教学）集 $T(C)$ 的示例。一个教学地图是不冲突的，如果没有任何两个概念的教学集合的union是一致的。教学地图的大小（NCTM）是最大的 $T(C)$ 的大小，其中 $C \in \cal{C}$。概念集合 $\cal{C}$ 的不冲突教学维度（NCTD）是最小的 NCTM 的大小。NCTM 和 NCTD 的定义类似，只是教师可以仅使用正例示例。我们研究了 NCTM 和 NCTM 加上的概念集合 $\mathcal{B}(G)$，其中 $G$ 是一个图。我们证明了关联的决策问题（B-NCTD +) 是 NP 完备的，并且在 split、co-bipartite 和 bipartite 图中是 NP 完备的。这些结果非常罕见：只有第二个（第四个，resp.）的问题在 NP 中具有 double-exponential 下界参数化的 vc（vertex cover number of G），并且只有一些问题具有 ETH 基于的 conditional lower bound 参数化的 vertices 的数量。我们补充了这些下界参数化的下界。对于树、间隔图、循环图和树的循环图，我们得到了 NCTM 加上或 NCTM 的大小与其 VC 维度相当。对于 Gromov-hyperbolic 图，我们设计了一个 Approximate NCTM 加上的大小为 2。
</details></li>
</ul>
<hr>
<h2 id="Learning-Hybrid-Dynamics-Models-With-Simulator-Informed-Latent-States"><a href="#Learning-Hybrid-Dynamics-Models-With-Simulator-Informed-Latent-States" class="headerlink" title="Learning Hybrid Dynamics Models With Simulator-Informed Latent States"></a>Learning Hybrid Dynamics Models With Simulator-Informed Latent States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02873">http://arxiv.org/abs/2309.02873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 这 paper 的目的是提出一种新的 Hybrid 模型，用于将学习模型和物理模型相结合，以提高预测结果的物理意义和精度。</li>
<li>methods: 这 paper 使用了一种基于观察器的方法，将学习模型的latent state informing via black-box simulator，以避免预测结果的积累错误。</li>
<li>results: 这 paper 的实验结果表明，这种 Hybrid 模型可以更好地控制预测结果，并且可以避免因学习模型的缺失而导致的预测错误。<details>
<summary>Abstract</summary>
Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's latent states is not available. We tackle the task by leveraging observers, a well-known concept from control theory, inferring unknown latent states from observations and dynamics over time. In our learning-based setting, we jointly learn the dynamics and an observer that infers the latent states via the simulator. Thus, the simulator constantly corrects the latent states, compensating for modeling mismatch caused by learning. To maintain flexibility, we train an RNN-based residuum for the latent states that cannot be informed by the simulator.
</details>
<details>
<summary>摘要</summary>
随机动力模型学习的任务是从测量数据中推断未知动力学和未来系统的行为预测。一般来说，用训练回归模型来解决这个问题。然而，这些模型的预测通常不是物理意义上的。另外，它们因时间的积累而受到衰减的影响。而Physics-based模拟器通常是物理意义上的，但模型简化通常会导致不准确。因此，Hybrid模型是一种趋势，它旨在结合这两种世界。在这篇论文中，我们提出了一种新的Hybrid模型方法，我们通过黑盒模拟器来控制learned模型的预测。这是因为，与前一种方法不同，我们不可以直接访问黑盒模拟器的latent状态。我们利用observer，一种控制理论中的概念，在观测和动力学上时间的推断未知latent状态。在我们的学习基于的设置中，我们同时学习动力学和一个observer，它可以通过黑盒模拟器来推断latent状态。因此，黑盒模拟器不断地更正latent状态，以补做模型匹配问题。为保持灵活性，我们在learned模型中训练一个RNN基的剩余，用于latent状态的推断。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Momentum-Knowledge-Distillation-in-Online-Continual-Learning"><a href="#Rethinking-Momentum-Knowledge-Distillation-in-Online-Continual-Learning" class="headerlink" title="Rethinking Momentum Knowledge Distillation in Online Continual Learning"></a>Rethinking Momentum Knowledge Distillation in Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02870">http://arxiv.org/abs/2309.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Maorong Wang, Ling Xiao, Toshihiko Yamasaki</li>
<li>for: 这篇论文主要针对在连续数据流中训练神经网络，并且数据只能看到一次。</li>
<li>methods: 论文提出了一种直接又有效的方法，即将动力知识传播（MKD）应用于多个标志性OCL方法中，以提高现有方法的准确率。</li>
<li>results: 论文表明，通过应用MKD，可以提高现有state-of-the-art的准确率超过10%点，并且解释了MKD在OCL训练中的内幕和影响。<details>
<summary>Abstract</summary>
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that similar to replay, MKD should be considered a central component of OCL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:在线连续学习（OCL） addresses the problem of training神经网络在连续数据流中进行多个分类任务的顺序出现。与离线连续学习不同，在OCL中数据只能看一次。在这个上下文中，回放基本策略取得了非常出色的结果，而现状的大部分方法都倚靠它们。而知识传授（KD）在离线连续学习中广泛使用，但在OCL中它尚未得到充分利用，尽管它在这个领域潜在的潜力很大。在这篇论文中，我们理论分析了在OCL中应用KD的挑战。我们介绍了一种直接又有效的方法，将摩托力知识传授（MKD）应用到许多标准OCL方法中，并证明它能够改进现有的状态的表现。除了在ImageNet100上提高现有状态的准确率 более10个百分点外，我们还照明了MKD在训练中的内部机制和影响。我们认为，与回放一样，MKD应该被视为OCL的中央组成部分。
</details></li>
</ul>
<hr>
<h2 id="On-Reducing-Undesirable-Behavior-in-Deep-Reinforcement-Learning-Models"><a href="#On-Reducing-Undesirable-Behavior-in-Deep-Reinforcement-Learning-Models" class="headerlink" title="On Reducing Undesirable Behavior in Deep Reinforcement Learning Models"></a>On Reducing Undesirable Behavior in Deep Reinforcement Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02869">http://arxiv.org/abs/2309.02869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ophir M. Carmel, Guy Katz</li>
<li>for: 提高深度强化学习（DRL）系统的可靠性和可理解性，降低DRL系统的不良行为。</li>
<li>methods: 提出了一种基于EXTRACTING decision tree classifiers的 Framework，将其 integrate into DRL 训练 loop，以便评价系统的错误行为。</li>
<li>results: 实现了一种可以 straightforwardly extend 现有框架，仅增加训练时间的负面影响，可以significantly reduce the frequency of undesirable behavior，并且在一些情况下，even improve performance。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the technique on three significant case studies. We find that our approach can extend existing frameworks in a straightforward manner, and incurs only a slight overhead in training time. Further, it incurs only a very slight hit to performance, or even in some cases - improves it, while significantly reducing the frequency of undesirable behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Event-Sequence-Modeling-with-Contrastive-Relational-Inference"><a href="#Enhancing-Event-Sequence-Modeling-with-Contrastive-Relational-Inference" class="headerlink" title="Enhancing Event Sequence Modeling with Contrastive Relational Inference"></a>Enhancing Event Sequence Modeling with Contrastive Relational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02868">http://arxiv.org/abs/2309.02868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Wang, Zhixuan Chu, Tao Zhou, Caigao Jiang, Hongyan Hao, Minjie Zhu, Xindong Cai, Qing Cui, Longfei Li, James Y Zhang, Siqiao Xue, Jun Zhou</li>
<li>for: 这个论文主要用于模型连续时间事件序列，尤其是捕捉事件之间的交互关系，以进行事件序列预测任务。</li>
<li>methods: 该论文提出了一种新的方法，即基于神经 relacional inference（NRI）的 Hawkes 过程，可以同时学习事件序列的动态模式和事件间关系。该方法使用了强迫推理的方法，通过搜索prototype路径来拟合关系约束。</li>
<li>results: 实验结果表明，该模型能够有效地捕捉事件序列中事件间的交互关系，并且在三个实际数据集上表现出色。<details>
<summary>Abstract</summary>
Neural temporal point processes(TPPs) have shown promise for modeling continuous-time event sequences. However, capturing the interactions between events is challenging yet critical for performing inference tasks like forecasting on event sequence data. Existing TPP models have focused on parameterizing the conditional distribution of future events but struggle to model event interactions. In this paper, we propose a novel approach that leverages Neural Relational Inference (NRI) to learn a relation graph that infers interactions while simultaneously learning the dynamics patterns from observational data. Our approach, the Contrastive Relational Inference-based Hawkes Process (CRIHP), reasons about event interactions under a variational inference framework. It utilizes intensity-based learning to search for prototype paths to contrast relationship constraints. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model in capturing event interactions for event sequence modeling tasks.
</details>
<details>
<summary>摘要</summary>
neural temporal point processes (TPPs) 有推荐力模型 continuous-time 事件序列。然而，捕捉事件之间的互动是挑战性强且重要的，以便在事件序列数据上进行预测任务。现有的 TPP 模型主要关注未来事件的 conditional 分布，但它们忽略了事件之间的互动。在这篇论文中，我们提出了一种新的方法，利用 Neural Relational Inference (NRI) 来学习一个关系图，从事件序列数据中推断事件之间的互动关系，同时也学习事件序列的动态模式。我们的方法，即 Contrastive Relational Inference-based Hawkes Process (CRIHP)，在 variational inference 框架下进行事件互动的推理。它利用了强度学习来搜索prototype path，以规定关系约束。我们在三个实际 datasets 上进行了广泛的实验， demonstrates 我们的模型能够准确地捕捉事件之间的互动，以便进行事件序列模型任务。
</details></li>
</ul>
<hr>
<h2 id="A-recommender-for-the-management-of-chronic-pain-in-patients-undergoing-spinal-cord-stimulation"><a href="#A-recommender-for-the-management-of-chronic-pain-in-patients-undergoing-spinal-cord-stimulation" class="headerlink" title="A recommender for the management of chronic pain in patients undergoing spinal cord stimulation"></a>A recommender for the management of chronic pain in patients undergoing spinal cord stimulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03918">http://arxiv.org/abs/2309.03918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tigran Tchrakian, Mykhaylo Zayats, Alessandra Pascale, Dat Huynh, Pritish Parida, Carla Agurto Rios, Sergiy Zhuk, Jeffrey L. Rogers, ENVISION Studies Physician Author Group, Boston Scientific Research Scientists Consortium</li>
<li>For: 这篇研究的目的是为了提供一个推荐系统，以帮助抗痛症病人在进行脊梗刺激治疗时，选择最佳的刺激参数。* Methods: 这篇研究使用了一种名为多臂环境推荐（CMAB）的方法，将推荐疗法传递给病人，以改善他们的病情。这个系统使用了一个数位健康生态系统，让病人可以在自己的家中使用，并且与病人监控系统相互作用，以关闭病人的疗法循环。* Results: 这篇研究发现，使用这个推荐系统可以帮助抗痛症病人获得更好的治疗效果。在一群脊梗刺激植入器的患者中，使用这个系统可以 statistically significant 地提高生活质量指标和患者状态（PS）的改善率。在 moderate PS 的患者中（N&#x3D;7），100% 的患者表现了 statistically significant 的改善，并且 5&#x2F;7 的患者有 improved PS dwell time。<details>
<summary>Abstract</summary>
Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a combination of quality of life metrics and Patient States (PS), a novel measure of holistic outcomes. SCS recommendations provided statistically significant improvement in clinical outcomes (pain and/or QoL) in 85\% of all subjects (N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations, 100\% showed statistically significant improvements and 5/7 had improved PS dwell time. This analysis suggests SCS patients may benefit from SCS recommendations, resulting in additional clinical improvement on top of benefits already received from SCS therapy.
</details>
<details>
<summary>摘要</summary>
脊梗刺激疗法（SCS）是一种治疗方法用于管理慢性疼痛。它通过在脊梗中植入设备，发送电rical impulses，可以阻塞或掩盖疼痛信号。在临床中选择最佳刺激参数通常由提供者进行，而在家用SCS优化则由患者自行管理。本文提出一种基于多重抓拍机（CMAB）的推荐系统，用于chronic pain患者在receiving SCS治疗时的疼痛管理。这些推荐，通过数字医疗生态系统直接发送给患者，与患者监测系统结合，实现了chronic pain患者的整体疾病管理过程。我们在ENVISION研究（Clinicaltrials.gov ID：NCT03240588）中使用了质量生活指标和患者状态（PS），一种新的总体结果度量，对这些推荐进行评估。SCS推荐给85%的所有 subjects（N=21）带来了 statistically significant的临床改善（疼痛和/或质量生活）。在moderate PS（N=7）前receiving推荐的 subjects中，100%显示了 statistically significant的改善，并有5/7 subjects的 PS dwell time进行了改善。这一分析表明SCS患者可能会从SCS推荐中受益，从而得到额外的临床改善。
</details></li>
</ul>
<hr>
<h2 id="Generalised-Mutual-Information-a-Framework-for-Discriminative-Clustering"><a href="#Generalised-Mutual-Information-a-Framework-for-Discriminative-Clustering" class="headerlink" title="Generalised Mutual Information: a Framework for Discriminative Clustering"></a>Generalised Mutual Information: a Framework for Discriminative Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02858">http://arxiv.org/abs/2309.02858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Ohl, Pierre-Alexandre Mattei, Charles Bouveyron, Warith Harchaoui, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso</li>
<li>for: 本研究旨在探讨深度归一化中使用的互信息（MI）是否适用于归一化训练 neural network 的 clustering 目标，并提出一种基于 distance 或 kernel 的扩展方法来改进 MI。</li>
<li>methods: 本研究使用了一种基于 MI 的归一化训练方法，并对其进行了改进，包括在训练过程中添加正则化项以提高归一化质量。</li>
<li>results: 研究发现，使用 GEMINI 可以在不需要正则化的情况下实现更好的归一化质量，并且可以自动选择合适的cluster数量，这些特性在深度归一化 clustering 中尚未得到了充分研究。<details>
<summary>Abstract</summary>
In the last decade, recent successes in deep clustering majorly involved the Mutual Information (MI) as an unsupervised objective for training neural networks with increasing regularisations. While the quality of the regularisations have been largely discussed for improvements, little attention has been dedicated to the relevance of MI as a clustering objective. In this paper, we first highlight how the maximisation of MI does not lead to satisfying clusters. We identified the Kullback-Leibler divergence as the main reason of this behaviour. Hence, we generalise the mutual information by changing its core distance, introducing the Generalised Mutual Information (GEMINI): a set of metrics for unsupervised neural network training. Unlike MI, some GEMINIs do not require regularisations when training as they are geometry-aware thanks to distances or kernels in the data space. Finally, we highlight that GEMINIs can automatically select a relevant number of clusters, a property that has been little studied in deep discriminative clustering context where the number of clusters is a priori unknown.
</details>
<details>
<summary>摘要</summary>
最近一个 décennie，深度划分大多数使用了互信息（MI）作为无监督目标函数来训练神经网络，并逐渐增加了正则化。然而，对于改进质量的正则化，相对较少的注意力被吸引到互信息作为划分目标的可重要性。在这篇论文中，我们首先指出了maximizing MI不能导致满意的划分。我们认为，库拉布-莱布尔异常（KL divergence）是主要的原因。因此，我们总结了互信息，并引入了通用的互信息（GEMINI）：一组无监督神经网络训练中的度量集合。与MI不同，一些GEMINIs不需要训练时添加正则化，因为它们在数据空间中具有geometry-awareness，可以通过距离或核函数来保证。最后，我们指出了GEMINIs可以自动选择相关的几个划分，这是在深度探测划分上，划分数量是先验不知的情况下，很少被研究的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Common-Log-Data-Sets-Used-for-Evaluation-of-Sequence-based-Anomaly-Detection-Techniques"><a href="#A-Critical-Review-of-Common-Log-Data-Sets-Used-for-Evaluation-of-Sequence-based-Anomaly-Detection-Techniques" class="headerlink" title="A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques"></a>A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02854">http://arxiv.org/abs/2309.02854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ait-aecid/anomaly-detection-log-datasets">https://github.com/ait-aecid/anomaly-detection-log-datasets</a></li>
<li>paper_authors: Max Landauer, Florian Skopik, Markus Wurzenberger</li>
<li>for: 本研究旨在分析六种公开available的日志数据集，以探讨异常现象的表现和简单的检测技术。</li>
<li>methods: 本研究使用了深度学习技术来检测日志数据中的异常现象，并分析了这些异常的表现。</li>
<li>results: 研究发现，大多数异常不直接与顺序表现相关，并且高度的检测技术不必要以获得高检测率。<details>
<summary>Abstract</summary>
Log data store event execution patterns that correspond to underlying workflows of systems or applications. While most logs are informative, log data also include artifacts that indicate failures or incidents. Accordingly, log data are often used to evaluate anomaly detection techniques that aim to automatically disclose unexpected or otherwise relevant system behavior patterns. Recently, detection approaches leveraging deep learning have increasingly focused on anomalies that manifest as changes of sequential patterns within otherwise normal event traces. Several publicly available data sets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become standards for evaluating these anomaly detection techniques, however, the appropriateness of these data sets has not been closely investigated in the past. In this paper we therefore analyze six publicly available log data sets with focus on the manifestations of anomalies and simple techniques for their detection. Our findings suggest that most anomalies are not directly related to sequential manifestations and that advanced detection techniques are not required to achieve high detection rates on these data sets.
</details>
<details>
<summary>摘要</summary>
log数据存储事件执行模式，与系统或应用下的工作流相对应。大多数日志数据都很有用，但日志数据还包含了 artefacts， indicating failures or incidents.因此，日志数据经常用于评估异常检测技术，以揭示不可预期的系统行为模式。现在，使用深度学习的检测方法在增长。这些方法通常focus on sequential pattern changes within otherwise normal event traces，而publicly available data sets such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop have become standards for evaluating these techniques. However, the appropriateness of these data sets has not been closely investigated in the past. In this paper, we therefore analyze six publicly available log data sets with a focus on the manifestations of anomalies and simple techniques for their detection. Our findings suggest that most anomalies are not directly related to sequential manifestations and that advanced detection techniques are not required to achieve high detection rates on these data sets.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-Layer-that-Lets-the-Student-Decide"><a href="#Knowledge-Distillation-Layer-that-Lets-the-Student-Decide" class="headerlink" title="Knowledge Distillation Layer that Lets the Student Decide"></a>Knowledge Distillation Layer that Lets the Student Decide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02843">http://arxiv.org/abs/2309.02843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adagorgun/letkd-framework">https://github.com/adagorgun/letkd-framework</a></li>
<li>paper_authors: Ada Gorgun, Yeti Z. Gurbuz, A. Aydin Alatan</li>
<li>for: The paper is written for improving the performance of knowledge distillation (KD) in deep neural networks, specifically in the intermediate layers.</li>
<li>methods: The paper proposes a learnable KD layer for the student model, which embeds the teacher’s knowledge in the feature transform and improves KD with two distinct abilities: i) learning how to leverage the teacher’s knowledge, and ii) feeding forward the transferred knowledge deeper.</li>
<li>results: The paper demonstrates the effectiveness of the proposed approach on three popular classification benchmarks through rigorous experimentation, showing that the student model enjoys the teacher’s knowledge during inference as well as training.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提高深度神经网络中知识储存（KD）的性能，特别是在中间层。</li>
<li>methods: 论文提出了一种可学习的KD层，将教师模型的知识嵌入学生模型的特征转换中，并通过两种能力提高KD：i) 学习如何利用教师模型的知识，ii) 将知识传递 deeper。</li>
<li>results: 论文通过严谨的实验证明了该方法的有效性，在三个Popular classification benchmark上显示学生模型在推理过程中也能够获得教师模型的知识。<details>
<summary>Abstract</summary>
Typical technique in knowledge distillation (KD) is regularizing the learning of a limited capacity model (student) by pushing its responses to match a powerful model's (teacher). Albeit useful especially in the penultimate layer and beyond, its action on student's feature transform is rather implicit, limiting its practice in the intermediate layers. To explicitly embed the teacher's knowledge in feature transform, we propose a learnable KD layer for the student which improves KD with two distinct abilities: i) learning how to leverage the teacher's knowledge, enabling to discard nuisance information, and ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys the teacher's knowledge during the inference besides training. Formally, we repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region according to the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learning in the intermediate layers, we propose a novel form of supervision based on the teacher's decisions. Through rigorous experimentation, we demonstrate the effectiveness of our approach on 3 popular classification benchmarks. Code is available at: https://github.com/adagorgun/letKD-framework
</details>
<details>
<summary>摘要</summary>
To implement this approach, we repurpose a 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region of the student, based on the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learning in the intermediate layers, we propose a novel form of supervision based on the teacher's decisions.We have conducted rigorous experiments on three popular classification benchmarks, and the results demonstrate the effectiveness of our approach. The code for our method is available at: https://github.com/adagorgun/letKD-framework.
</details></li>
</ul>
<hr>
<h2 id="Random-postprocessing-for-combinatorial-Bayesian-optimization"><a href="#Random-postprocessing-for-combinatorial-Bayesian-optimization" class="headerlink" title="Random postprocessing for combinatorial Bayesian optimization"></a>Random postprocessing for combinatorial Bayesian optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02842">http://arxiv.org/abs/2309.02842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Morita, Yoshihiko Nishikawa, Masayuki Ohzeki</li>
<li>for: 这个论文主要是关于 Bayesian 优化的Sequential 方法，包括 Bayesian 优化技术的各种应用。</li>
<li>methods: 这个论文使用的方法主要是 Bayesian 优化技术，以及一种减少重复采样的后处理方法。</li>
<li>results: 研究发现，使用后处理方法可以减少 Bayesian 优化的步骤数量，特别是当使用最大 posterior 估计时。这种简单 yet 通用的策略可以解决高维问题中 Bayesian 优化的慢速收敛问题。<details>
<summary>Abstract</summary>
Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
模型基于的顺序方法，包括 bayesian 优化技术，通常对于 Interest 函数多次访问同一个点，从而导致寻找全局最优点需要很多步骤。在这里，我们 numerically 研究了在 bayesian 优化中使用禁止重复样本的后处理方法的影响。我们发现该后处理方法可以大幅减少在找到全局最优点的sequential步骤数量，特别是当使用最大 posterior 估计的获取函数时。我们的结果提供了一种简单 yet 通用的策略来解决高维问题中 bayesian 优化的慢 converges。
</details></li>
</ul>
<hr>
<h2 id="EGIC-Enhanced-Low-Bit-Rate-Generative-Image-Compression-Guided-by-Semantic-Segmentation"><a href="#EGIC-Enhanced-Low-Bit-Rate-Generative-Image-Compression-Guided-by-Semantic-Segmentation" class="headerlink" title="EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation"></a>EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03244">http://arxiv.org/abs/2309.03244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikolai10/egic">https://github.com/nikolai10/egic</a></li>
<li>paper_authors: Nikolai Körber, Eduard Kromer, Andreas Siebert, Sascha Hauke, Daniel Mueller-Gritschneder</li>
<li>for: 这篇论文是为了提出一种新的生成图像压缩方法，以便高效地从单个模型中横跨干扰度-感知曲线。</li>
<li>methods: 该方法使用了一种隐式编码的变体图像 interpolating，预测MSE优化和GAN优化解码器输出之间的差异。在接收方side，用户可以控制GAN基于的重建中 residual的影响。</li>
<li>results: EGIC超过了多种感知 oriented和干扰度 oriented的基elines，包括HiFiC、MRIC和DIRAC，并且在干扰度端与VTM-20.0接近。EGIC简单实现、轻量级（例如0.18x HiFiC的模型参数），并且具有优秀的 interpolating 特性，使其成为实际应用中的优秀候选人。<details>
<summary>Abstract</summary>
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
</details>
<details>
<summary>摘要</summary>
我们介绍EGIC，一种新的生成式图像压缩方法，可以快速地从单一模型中进行扭曲视觉曲线的积极控制。具体来说，我们提出了一种隐式编码的图像 interpolate 方法，可以预测MSE优化和 GAN 优化对 Decoder 输出的差异。在接收端，用户可以控制这个差异对 GAN 基础的重建影响。与改进的 GAN 基础块相结合，EGIC 在视觉方面超过了广泛的感知方向和扭曲方向的基准，包括 HiFiC、MRIC 和 DIRAC，同时与 VTM-20.0 的扭曲端相差几乎没有差异。EGIC 简单实现、轻量级（例如，0.18x 模型参数相比 HiFiC），且具有优秀的 interpolate 特性，这使得它在实际应用中成为一个有前途的候选人。
</details></li>
</ul>
<hr>
<h2 id="BigVSAN-Enhancing-GAN-based-Neural-Vocoders-with-Slicing-Adversarial-Network"><a href="#BigVSAN-Enhancing-GAN-based-Neural-Vocoders-with-Slicing-Adversarial-Network" class="headerlink" title="BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network"></a>BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02836">http://arxiv.org/abs/2309.02836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/bigvsan_eval">https://github.com/sony/bigvsan_eval</a></li>
<li>paper_authors: Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji</li>
<li>for: 这 paper 的目的是研究 Generative Adversarial Network (GAN) 基于的 vocoder 是否可以通过改进 GAN 训练框架来提高音频 synthesis 的质量。</li>
<li>methods: 这 paper 使用了 Slicing Adversarial Network (SAN) 训练框架，通过修改 least-squares GAN 的损失函数来满足 SAN 的要求，以提高 GAN-based vocoder 的性能。</li>
<li>results: 经过实验，这 paper 发现 SAN 可以提高 GAN-based vocoder 的表现，包括 BigVGAN，并且需要小量修改。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/sony/bigvsan">https://github.com/sony/bigvsan</a> 中找到。<details>
<summary>Abstract</summary>
Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)生成对抗网络（GAN）基本的语音合成器已经得到了广泛的研究，因为它们可以在实时之前生成高质量的音频波形。然而，有研究表明，大多数GAN无法在特征空间找到优化的投影，以便将真实和假数据分开。在文献中，已经证明了剖面对抗网络（SAN）是一种改进的GAN训练框架，可以在特征空间找到优化的投影。在这篇论文中，我们 investigate SAN在语音合成任务中的效果。为了实现这一目标，我们提议修改最小二乘GAN的损失函数，使其满足SAN的要求。通过我们的实验，我们证明了SAN可以提高GAN基本的语音合成器，包括BigVGAN， WITH小 modification。我们的代码可以在https://github.com/sony/bigvsan上获取。
</details></li>
</ul>
<hr>
<h2 id="Roulette-A-Semantic-Privacy-Preserving-Device-Edge-Collaborative-Inference-Framework-for-Deep-Learning-Classification-Tasks"><a href="#Roulette-A-Semantic-Privacy-Preserving-Device-Edge-Collaborative-Inference-Framework-for-Deep-Learning-Classification-Tasks" class="headerlink" title="Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks"></a>Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02820">http://arxiv.org/abs/2309.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Li, Guocheng Liao, Lin Chen, Xu Chen</li>
<li>For: This paper proposes a framework for task-oriented semantic privacy-preserving collaborative inference for deep learning classifiers, with a focus on protecting the ground truth of the data as private information.* Methods: The proposed framework, called Roulette, uses a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Additionally, the paper provides a differential privacy guarantee and analyzes the hardness of ground truth inference attacks.* Results: The paper conducts extensive performance evaluations using realistic datasets and demonstrates that Roulette can effectively defend against various attacks and achieve good model accuracy, improving inference accuracy by 21% averaged over benchmarks in a situation with severe non-i.i.d. data distribution.<details>
<summary>Abstract</summary>
Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradigm of split learning where the back-end DNN is frozen and the front-end DNN is retrained to be both a feature extractor and an encryptor. Moreover, we provide a differential privacy guarantee and analyze the hardness of ground truth inference attacks. To validate the proposed Roulette, we conduct extensive performance evaluations using realistic datasets, which demonstrate that Roulette can effectively defend against various attacks and meanwhile achieve good model accuracy. In a situation where the non-i.i.d. is very severe, Roulette improves the inference accuracy by 21\% averaged over benchmarks, while making the accuracy of discrimination attacks almost equivalent to random guessing.
</details>
<details>
<summary>摘要</summary>
深度学习分类器在人工智能时代具有核心作用。设备边缘基于的合作推理已广泛采用为提高其应用在互联网器 Things（IoT）和5G/6G网络中。然而，它受到异步数据分布的准确性下降和隐私泄露问题。Direct使用传输学习和分裂学习可能会带来高成本和隐私问题。为隐私泄露， криптография基本方法会带来巨大的负担。其他轻量级方法假设了地面 truth 是非敏感信息，可以暴露。但在许多应用程序中，地面 truth 是用户的关键隐私信息。在本文中，我们提出了一个名为 Roulette 的任务 oriented semantic privacy-preserving 合作推理框架，其中我们将地面 truth 视为私人信息。我们开发了一种新的分裂学习方法，其中后端 DNN 冻结，前端 DNN 重新训练为特征提取器和加密器。此外，我们提供了一种差分隐私保证，并分析了地面 truth 推理攻击的困难程度。为验证提出的 Roulette，我们进行了广泛的性能评估，使用实际数据集，其结果表明，Roulette 可以有效防止多种攻击，同时保持良好的模型准确率。在异步数据分布情况下，Roulette 可以提高推理准确率21%，而地面 truth 推理攻击的准确率接近随机猜测。
</details></li>
</ul>
<hr>
<h2 id="Combining-Thermodynamics-based-Model-of-the-Centrifugal-Compressors-and-Active-Machine-Learning-for-Enhanced-Industrial-Design-Optimization"><a href="#Combining-Thermodynamics-based-Model-of-the-Centrifugal-Compressors-and-Active-Machine-Learning-for-Enhanced-Industrial-Design-Optimization" class="headerlink" title="Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization"></a>Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02818">http://arxiv.org/abs/2309.02818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadi Ghiasi, Guido Pazzi, Concettina Del Grosso, Giovanni De Magistris, Giacomo Veneri<br>for: 这个论文主要是为了提高中心旋转压缩机的设计过程中的优化过程，使用活动学习（Active Learning）和 Gaussian Process 基于 surrogate 模型来减少计算成本。methods: 这个论文使用了一个内部的 thermodynamics-based 压缩机模型和 Gaussian Process 基于 surrogate 模型，在一个可deployable的 Active Learning  Setting 中结合了这两个模型，并通过了一些实验和扩展来提高 surrogate 模型的性能。results: 这个论文的实验结果表明，使用 ActiveCompDesign 框架可以significantly 提高 surrogate 模型的性能，并且在生产环境中可以减少压缩机的设计优化计算时间，相比于仅仅使用内部 thermodynamics-based 模型，提高了46%。<details>
<summary>Abstract</summary>
The design process of centrifugal compressors requires applying an optimization process which is computationally expensive due to complex analytical equations underlying the compressor's dynamical equations. Although the regression surrogate models could drastically reduce the computational cost of such a process, the major challenge is the scarcity of data for training the surrogate model. Aiming to strategically exploit the labeled samples, we propose the Active-CompDesign framework in which we combine a thermodynamics-based compressor model (i.e., our internal software for compressor design) and Gaussian Process-based surrogate model within a deployable Active Learning (AL) setting. We first conduct experiments in an offline setting and further, extend it to an online AL framework where a real-time interaction with the thermodynamics-based compressor's model allows the deployment in production. ActiveCompDesign shows a significant performance improvement in surrogate modeling by leveraging on uncertainty-based query function of samples within the AL framework with respect to the random selection of data points. Moreover, our framework in production has reduced the total computational time of compressor's design optimization to around 46% faster than relying on the internal thermodynamics-based simulator, achieving the same performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到简化中文。<</SYS>>设计中心rifugal压缩机需要应用优化过程，这个过程 computationally expensive 因为压缩机的动力学方程在下面的复杂的分析方程中。虽然回归准确模型可以减少计算成本，但是主要挑战是缺乏训练数据。为了策略性利用标注样本，我们提出了 Active-CompDesign 框架，其 combining 内部的 thermodynamics-based 压缩机模型和 Gaussian Process 基于的准确模型在一个可deployable Active Learning （AL）设置下。我们首先在 offline 设置中进行实验，然后将其扩展到 online AL 框架，在生产环境中实现实时交互。ActiveCompDesign 在 surrogate 模型中表现出了显著的性能提高，通过在 AL 框架中使用样本uncertainty 基于的问题函数来选择样本。此外，我们的框架在生产环境中减少了压缩机的设计优化总计算时间约为 46%，并且实现了同等性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Bioinformatics-Analysis-via-AutoBA"><a href="#Automated-Bioinformatics-Analysis-via-AutoBA" class="headerlink" title="Automated Bioinformatics Analysis via AutoBA"></a>Automated Bioinformatics Analysis via AutoBA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03242">http://arxiv.org/abs/2309.03242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joshuachou2018/autoba">https://github.com/joshuachou2018/autoba</a></li>
<li>paper_authors: Juexiao Zhou, Bin Zhang, Xiuying Chen, Haoyang Li, Xiaopeng Xu, Siyuan Chen, Xin Gao</li>
<li>for: 这份研究是为了提供一个自动化的 bioinformatics 分析工具，供处理各种 omics 数据。</li>
<li>methods: 这个工具基于大型自然语言模型，可以自动生成各种 bioinformatics 任务的详细步骤计划。用户只需提供少量的输入，便可以完成复杂的数据分析。</li>
<li>results: 这个工具在对不同类型的 omics 数据进行分析时，有着高度的自动化和灵活性。它可以根据输入数据的变化，自动设计分析过程。相比 online bioinformatics 服务，这个工具可以地方式进行分析，保持数据隐私。<details>
<summary>Abstract</summary>
With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreover, different from the predefined pipeline, AutoBA has adaptability in sync with emerging bioinformatics tools. Overall, AutoBA represents a convenient tool, offering robustness and adaptability for complex omics data analysis.
</details>
<details>
<summary>摘要</summary>
随着快速增长和发展的Omics数据，对于流lined和适应性好的分析工具的需求不断增长。为回应这个需求，我们介绍了自动生物信息分析（AutoBA），这是基于大型自然语言模型的自主AI代理，专门为传统的Omics数据分析设计。AutoBA简化了分析过程，需 minimal用户输入，并提供了详细的步骤计划 для多种生物信息分析任务。经过专家生物信息学家的严格验证，AutoBA在多种Omics分析场景中 Display robustness and adaptability，包括全 genomic sequencing（WGS）、RNA sequencing（RNA-seq）、单个细胞RNA-seq、ChIP-seq和 spatial transcriptomics。AutoBA的独特的自动设计分析过程基于输入数据的变化，进一步强调其 versatility。与在线生物信息服务相比，AutoBA在本地部署分析，保护数据隐私。此外，与预定的管道不同，AutoBA具有适应新生物信息工具的能力。总之，AutoBA表示一种便捷的工具，提供了 robustness和适应性 для复杂的Omics数据分析。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Thermodynamics-Informed-Symbolic-Regression-–-A-Tool-for-Thermodynamic-Equations-of-State-Development"><a href="#Introducing-Thermodynamics-Informed-Symbolic-Regression-–-A-Tool-for-Thermodynamic-Equations-of-State-Development" class="headerlink" title="Introducing Thermodynamics-Informed Symbolic Regression – A Tool for Thermodynamic Equations of State Development"></a>Introducing Thermodynamics-Informed Symbolic Regression – A Tool for Thermodynamic Equations of State Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02805">http://arxiv.org/abs/2309.02805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scoop-group/tisr">https://github.com/scoop-group/tisr</a></li>
<li>paper_authors: Viktor Martinek, Ophelia Frotscher, Markus Richter, Roland Herzog</li>
<li>for: 这篇论文旨在描述一种新的符号回归工具，用于加速热动力学方程状态（EOS）的发展过程。</li>
<li>methods: 这种符号回归工具基于现有的符号回归工具，并添加了一些扩展来处理热动力学数据的散度和不同的剩余处理选项。</li>
<li>results: 这篇论文报道了这种符号回归工具的现状和进展，并讨论了未来的发展方向。<details>
<summary>Abstract</summary>
Thermodynamic equations of state (EOS) are essential for many industries as well as in academia. Even leaving aside the expensive and extensive measurement campaigns required for the data acquisition, the development of EOS is an intensely time-consuming process, which does often still heavily rely on expert knowledge and iterative fine-tuning. To improve upon and accelerate the EOS development process, we introduce thermodynamics-informed symbolic regression (TiSR), a symbolic regression (SR) tool aimed at thermodynamic EOS modeling. TiSR is already a capable SR tool, which was used in the research of https://doi.org/10.1007/s10765-023-03197-z. It aims to combine an SR base with the extensions required to work with often strongly scattered experimental data, different residual pre- and post-processing options, and additional features required to consider thermodynamic EOS development. Although TiSR is not ready for end users yet, this paper is intended to report on its current state, showcase the progress, and discuss (distant and not so distant) future directions. TiSR is available at https://github.com/scoop-group/TiSR and can be cited as https://doi.org/10.5281/zenodo.8317547.
</details>
<details>
<summary>摘要</summary>
thermodynamic equation of state (EOS) 是多个行业以及学术界的重要工具。即使不考虑数据收集的昂贵和广泛的测量活动，EOS的开发还是一个非常时间consuming的过程，它frequently仍然受到专家知识和迭代精细调整的限制。为了改进和加速EOS开发过程，我们介绍了thermodynamic-informed symbolic regression (TiSR)，一种符号 regression (SR) 工具，旨在用于 termodynamic EOS 模型化。TiSR 已经是一种可靠的 SR 工具，在 https://doi.org/10.1007/s10765-023-03197-z 中的研究中使用。它旨在将 SR 基础结合 thermodynamic EOS 开发所需的扩展，以及不同的剩下预处理和后处理选项，以及考虑 termodynamic EOS 开发中的其他特性。虽然 TiSR 还没有满足用户的需求，但这篇文章的目的是报告当前状况，展示进步，并讨论未来的方向。TiSR 可以在 https://github.com/scoop-group/TiSR 上获取，并可以引用为 https://doi.org/10.5281/zenodo.8317547。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Encoding-and-Decoding-of-Information-for-Split-Learning-in-Mobile-Edge-Computing-Leveraging-Information-Bottleneck-Theory"><a href="#Dynamic-Encoding-and-Decoding-of-Information-for-Split-Learning-in-Mobile-Edge-Computing-Leveraging-Information-Bottleneck-Theory" class="headerlink" title="Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory"></a>Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02787">http://arxiv.org/abs/2309.02787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Alhussein, Moshi Wei, Arashmid Akhavain</li>
<li>For: 本文提出了一种基于数据处理不等和信息瓶颈理论的分布式学习框架，用于在移动端计算中实现隐私保护的分布式学习。* Methods: 本文提出了一种基于encoder-decoder神经网络架构的新训练机制，可以在不同的实时网络条件和应用需求下进行调整，以提高预测性能。* Results: 作者通过应用该训练机制于一个基于 millimeter-wave 的吞吐量预测问题，实现了提高预测性能的目的。此外，文章还提供了一些关于循环神经网络的新视角和挑战。<details>
<summary>Abstract</summary>
Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate varying real-time network conditions and application requirements, potentially reducing operational expenditure and enhancing network agility. As a proof of concept, we apply the training mechanism to a millimeter-wave (mmWave)-enabled throughput prediction problem. We also offer new insights and highlight some challenges related to recurrent neural networks from the perspective of the IB theory. Interestingly, we find a compression phenomenon across the temporal domain of the sequential model, in addition to the compression phase that occurs with the number of training epochs.
</details>
<details>
<summary>摘要</summary>
分布式学习是一种隐私保护的分布式学习模式，在其中，一个机器学习模型（例如神经网络）被分解成两个部分（即编码器和解码器）。编码器将所谓的潜在表示分享给模型训练，而不是直接使用原始数据。在移动边缘计算中，网络功能（如交通预测）可以通过分布式学习进行训练，其中编码器位于用户设备（UE）中，而解码器位于边缘网络中。基于数据处理不等式和信息瓶颈（IB）理论，我们提出了一新的框架和训练机制，以实现在传输资源消耗和潜在表示的信息含量之间进行动态均衡，直接影响预测性能。我们的训练机制提供了一种具有多种复杂度-相关性质的编码器-解码器神经网络架构，可以实现调整性能。这种适应性可以满足不同的实时网络条件和应用要求，可能减少操作成本和提高网络灵活性。作为证明，我们将训练机制应用于使用 millimeter-wave（mmWave）技术实现的吞吐量预测问题。我们还提供了一些新的意见和挑战，从信息瓶颈理论的角度出发，关于循环神经网络的问题。有趣的是，我们发现在时间频谱中的压缩现象，以及与训练环节数的压缩相关的压缩阶段。
</details></li>
</ul>
<hr>
<h2 id="CVE-driven-Attack-Technique-Prediction-with-Semantic-Information-Extraction-and-a-Domain-specific-Language-Model"><a href="#CVE-driven-Attack-Technique-Prediction-with-Semantic-Information-Extraction-and-a-Domain-specific-Language-Model" class="headerlink" title="CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model"></a>CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02785">http://arxiv.org/abs/2309.02785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Aghaei, Ehab Al-Shaer</li>
<li>for: 本研究目的是bridging Common Vulnerabilities and Exposures (CVEs) 和 ATT&amp;CK 攻击技术之间的知识空白，以便更好地预测和防范网络攻击。</li>
<li>methods: 本研究使用了Semantic Role Labeling (SRL) 技术来从不结构化的网络安全报告中提取威胁动作，然后将其与MITRE的攻击功能类 correlate。这种自动化的相关性可以帮助创建标注数据，以便分类新的威胁动作到攻击技能类和 TTPs。</li>
<li>results: 实验结果表明，TTPpredictor 的准确率达到了 Approximately 98%，F1-score 在95% 到 98% 之间。与现状的语言模型工具如 ChatGPT 相比，TTPpredictor 表现更加出色。总的来说，本研究提供了一种可靠的解决方案，可以将 CVEs 与可能的攻击技术相连接，从而提高网络安全专业人员的预测和防范能力。<details>
<summary>Abstract</summary>
This paper addresses a critical challenge in cybersecurity: the gap between vulnerability information represented by Common Vulnerabilities and Exposures (CVEs) and the resulting cyberattack actions. CVEs provide insights into vulnerabilities, but often lack details on potential threat actions (tactics, techniques, and procedures, or TTPs) within the ATT&CK framework. This gap hinders accurate CVE categorization and proactive countermeasure initiation. The paper introduces the TTPpredictor tool, which uses innovative techniques to analyze CVE descriptions and infer plausible TTP attacks resulting from CVE exploitation. TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It initially extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques. These actions, along with their contextual attributes, are correlated with MITRE's attack functionality classes. This automated correlation facilitates the creation of labeled data, essential for categorizing novel threat actions into threat functionality classes and TTPs. The paper presents an empirical assessment, demonstrating TTPpredictor's effectiveness with accuracy rates of approximately 98% and F1-scores ranging from 95% to 98% in precise CVE classification to ATT&CK techniques. TTPpredictor outperforms state-of-the-art language model tools like ChatGPT. Overall, this paper offers a robust solution for linking CVEs to potential attack techniques, enhancing cybersecurity practitioners' ability to proactively identify and mitigate threats.
</details>
<details>
<summary>摘要</summary>
TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques, and correlates these actions with MITRE's attack functionality classes. This automated correlation facilitates the creation of labeled data, essential for categorizing novel threat actions into threat functionality classes and TTPs.The paper presents an empirical assessment, demonstrating TTPpredictor's effectiveness with accuracy rates of approximately 98% and F1-scores ranging from 95% to 98% in precise CVE classification to ATT&CK techniques. TTPpredictor outperforms state-of-the-art language model tools like ChatGPT. This paper offers a robust solution for linking CVEs to potential attack techniques, enhancing cybersecurity practitioners' ability to proactively identify and mitigate threats.
</details></li>
</ul>
<hr>
<h2 id="Norm-Tweaking-High-performance-Low-bit-Quantization-of-Large-Language-Models"><a href="#Norm-Tweaking-High-performance-Low-bit-Quantization-of-Large-Language-Models" class="headerlink" title="Norm Tweaking: High-performance Low-bit Quantization of Large Language Models"></a>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02784">http://arxiv.org/abs/2309.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Li, Qingyuan Li, Bo Zhang, Xiangxiang Chu</li>
<li>for: 这个论文目的是提出一种名为“norm tweaking”的技术，用于提高大型自然语言模型（LLM）的压缩精度，并且不需要牺牲准确性。</li>
<li>methods: 本文使用了一种名为“channel-wise distance constraint”的约化方法，将数据生成和调整过的数据用于更新normalization层的加重。</li>
<li>results: 本文的方法可以实现高精度的压缩，并且在多个测试数据集上表现出色，比较现有的PTQ方法更好。甚至在GLM-130B和OPT-66B上，本文的方法可以达到浮点数压缩的同等精度水平。<details>
<summary>Abstract</summary>
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our method demonstrates significant improvements in both weight-only quantization and joint quantization of weights and activations, surpassing existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the same level of accuracy at 2-bit quantization as their float ones. Our simple and effective approach makes it more practical for real-world applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着大型自然语言模型（LLMs）的大小不断增长，模型压缩而不失 precisión的挑战已成为其部署的关键问题。虽然一些量化方法，如GPTQ，已经在实现了可接受的4位加量量化方面做出了进展，但在更低的位数量化中往往会导致性能下降。在这篇论文中，我们介绍了一种技术called norm tweaking，可以作为当前PTQ方法的插件，以实现高精度而且cost-efficient。我们的方法是在观察到Rectifying the quantized activation distribution to match its float counterpart可以快速地恢复LLMs的精度的基础上提出的。为了实现这一点，我们 metros carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization。我们在多个开源的LLMs上进行了广泛的实验，我们的方法在weight-only quantization和joint quantization of weights and activations中都有显著的改进，超越了现有的PTQ方法。在GLM-130B和OPT-66B上，我们的方法甚至可以在2位量化下达到与浮点版本的同等精度水平。我们的简单而有效的方法使其更加实用于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Improving-diagnosis-and-prognosis-of-lung-cancer-using-vision-transformers-A-scoping-review"><a href="#Improving-diagnosis-and-prognosis-of-lung-cancer-using-vision-transformers-A-scoping-review" class="headerlink" title="Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review"></a>Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02783">http://arxiv.org/abs/2309.02783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazrat Ali, Farida Mohsen, Zubair Shah<br>for:This scoping review aims to identify recent developments in vision transformer-based AI methods for lung cancer imaging applications.methods:The review includes 34 studies published from 2020 to 2022 that use vision transformer-based AI methods for lung cancer diagnosis and prognosis. These methods include the combination of vision transformers with convolutional neural networks or UNet models.results:The studies have shown that vision transformer-based models are increasingly popular for developing AI methods for lung cancer applications, but their computational complexity and clinical relevance are important factors to be considered for future research. The review provides valuable insights for researchers in the field of AI and healthcare to advance the state-of-the-art in lung cancer diagnosis and prognosis.Here is the simplified Chinese text for the three information points:for:这份scoping review旨在Identify最近发展在呼吸器transformer基于AI方法的肺癌成像应用领域。methods:这份综述包括2020年至2022年发表的34篇研究，这些研究使用呼吸器transformer基于AI方法进行肺癌诊断和预后预测。这些方法包括呼吸器transformer与卷积神经网络或UNet模型的组合。results:这些研究表明，呼吸器transformer基于模型在肺癌应用领域的流行度在增加，但计算复杂性和临床 relevance 是未来研究的关键因素。这份综述为医学AI和健康领域的研究人员提供了有价值的洞察，以进一步提高肺癌诊断和预后预测的state-of-the-art。<details>
<summary>Abstract</summary>
Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer imaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI methods for lung cancer diagnosis and prognosis. This scoping review aims to identify the recent developments on vision transformer-based AI methods for lung cancer imaging applications. It provides key insights into how vision transformers complemented the performance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets that contributed to advancing the field. Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most commonly addressed task in these studies was the classification of lung cancer types, such as lung squamous cell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other applications included survival prediction of lung cancer patients and segmentation of lungs. The studies lacked clear strategies for clinical transformation. SWIN transformer was a popular choice of the researchers; however, many other architectures were also reported where vision transformer was combined with convolutional neural networks or UNet model. It can be concluded that vision transformer-based models are increasingly in popularity for developing AI methods for lung cancer applications. However, their computational complexity and clinical relevance are important factors to be considered for future research work. This review provides valuable insights for researchers in the field of AI and healthcare to advance the state-of-the-art in lung cancer diagnosis and prognosis. We provide an interactive dashboard on lung-cancer.onrender.com/.
</details>
<details>
<summary>摘要</summary>
医学人工智能和肺癌影像领域中，视Transformer基本方法在不断发展。近年来，许多研究人员开发了基于视Transformer的AI方法用于肺癌诊断和预后预测。本篇综述的目的是找到最近发展的视Transformer基本方法在肺癌影像应用中的进展。它提供了关于如何使用视Transformer complement AI和深度学习方法的关键洞察。此外，文章还标出了该领域的主要数据集。总共检索到314篇文献，本综述包含2020年至2022年发表的34篇文献。研究中最常 addressed task是分类肺癌类型，如肺平滑细胞癌和肺管细胞癌，以及识别正常与恶性肺脏瘤。其他应用包括肺癌患者存活预测和肺分 segmentation。研究缺乏明确的临床转化策略。SWIN transformer是研究人员的受欢迎选择，但其他架构也被报道，例如将视Transformer与卷积神经网络或UNet模型结合使用。可以确定，基于视Transformer的模型在肺癌应用领域的 популярность不断增长。然而，计算复杂度和临床 relevance 是未来研究的重要因素。本综述为医学人工智能和健康领域的研究人员提供了有价值的洞察，以推动肺癌诊断和预后预测的状态艺。我们提供了肺癌诊断和预后预测的互动dashboard，可以在lung-cancer.onrender.com/上查看。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effects-of-Heterogeneous-Errors-on-Multi-fidelity-Bayesian-Optimization"><a href="#On-the-Effects-of-Heterogeneous-Errors-on-Multi-fidelity-Bayesian-Optimization" class="headerlink" title="On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization"></a>On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02771">http://arxiv.org/abs/2309.02771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Zanjani Foumani, Amin Yousefpour, Mehdi Shishehbor, Ramin Bostanabad</li>
<li>for: 这个研究的目的是提出一种多元信息（Multi-fidelity）优化方法，以缓解在实际应用中高精度数据（High-fidelity）搜集所带来的成本问题。</li>
<li>methods: 这个方法使用了多元信息优化（Multi-fidelity）技术，并将低精度数据（Low-fidelity）与高精度数据（High-fidelity）融合以提高优化效率。</li>
<li>results: 这个研究显示了该方法可以将低精度数据与高精度数据融合，以提高优化效率，并且可以减少高精度数据的搜集成本。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a sequential optimization strategy that is increasingly employed in a wide range of areas including materials design. In real world applications, acquiring high-fidelity (HF) data through physical experiments or HF simulations is the major cost component of BO. To alleviate this bottleneck, multi-fidelity (MF) methods are used to forgo the sole reliance on the expensive HF data and reduce the sampling costs by querying inexpensive low-fidelity (LF) sources whose data are correlated with HF samples. However, existing multi-fidelity BO (MFBO) methods operate under the following two assumptions that rarely hold in practical applications: (1) LF sources provide data that are well correlated with the HF data on a global scale, and (2) a single random process can model the noise in the fused data. These assumptions dramatically reduce the performance of MFBO when LF sources are only locally correlated with the HF source or when the noise variance varies across the data sources. In this paper, we dispense with these incorrect assumptions by proposing an MF emulation method that (1) learns a noise model for each data source, and (2) enables MFBO to leverage highly biased LF sources which are only locally correlated with the HF source. We illustrate the performance of our method through analytical examples and engineering problems on materials design.
</details>
<details>
<summary>摘要</summary>
泛化优化（BO）是一种顺序优化策略，在许多领域都在广泛应用，包括材料设计。在实际应用中，通过物理实验或高精度计算获得高精度数据是BO的主要成本 componenet。为了缓解这个瓶颈，使用多质量（MF）方法，可以减少抽样成本，通过寻求便宜的低精度（LF）来源的数据，这些数据与高精度数据相关。然而，现有的MFBO方法在实际应用中存在两个不当的假设：（1）LF来源提供的数据与高精度数据在全局范围内很好地相关，和（2）数据源中的噪声可以用单个随机过程模型。这两个假设在实际应用中非常少有效，这会使MFBO表现下降。在这篇论文中，我们抛弃这两个错误假设，提出了一种MF优化方法，它可以（1）学习每个数据源的噪声模型，和（2）使MFBO可以利用高偏见的LF来源。我们通过分析例子和工程设计问题来证明我们的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Unifying-over-smoothing-and-over-squashing-in-graph-neural-networks-A-physics-informed-approach-and-beyond"><a href="#Unifying-over-smoothing-and-over-squashing-in-graph-neural-networks-A-physics-informed-approach-and-beyond" class="headerlink" title="Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond"></a>Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02769">http://arxiv.org/abs/2309.02769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Shao, Dai Shi, Andi Han, Yi Guo, Qibin Zhao, Junbin Gao</li>
<li>For: The paper aims to address critical computational challenges in graph neural networks (GNNs), such as over-smoothing, over-squashing, and limited expressive power, by introducing a novel approach inspired by the time-reversal principle.* Methods: The proposed method, called Multi-Scaled Heat Kernel based GNN (MHKG), leverages the time-reversal of the graph heat equation to enhance the sharpness of graph node features. The method further generalizes to a more flexible model called G-MHKG, which allows for more control over over-smoothing, over-squashing, and expressive power.* Results: The proposed models are shown to outperform several GNN baseline models in performance across various graph datasets characterized by both homophily and heterophily. Additionally, the paper demonstrates the ability of the proposed models to handle both over-smoothing and over-squashing under mild conditions, and uncovers a trade-off between these two issues.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all aforementioned issues can be characterized and analyzed via the properties of the filtering functions, and uncover a trade-off between over-smoothing and over-squashing: enhancing node feature sharpness will make model suffer more from over-squashing, and vice versa. Furthermore, we manipulate the time again to show how G-MHKG can handle both two issues under mild conditions. Our conclusive experiments highlight the effectiveness of proposed models. It surpasses several GNN baseline models in performance across graph datasets characterized by both homophily and heterophily.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已经成为机器学习图数据的一种主要方法。 despite their great success,  However,  there are still several computational challenges that affect the performance of GNNs, such as over-smoothing, over-squashing, and limited expressive power. In this study, we draw inspiration from the time-reversal principle commonly used in classical and quantum physics, and reverse the time direction of the graph heat equation. This process yields a class of high-pass filtering functions that enhance the sharpness of graph node features. Based on this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by combining the effects of diverse filtering functions on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model called G-MHKG and thoroughly demonstrate the roles of each element in controlling over-smoothing, over-squashing, and expressive power.We find that all of these issues can be characterized and analyzed through the properties of the filtering functions, and discover a trade-off between over-smoothing and over-squashing: enhancing node feature sharpness will make the model more susceptible to over-squashing, and vice versa. Furthermore, we manipulate the time to show how G-MHKG can handle both of these issues under mild conditions. Our experimental results demonstrate the effectiveness of our proposed models, surpassing several GNN baseline models in performance across graph datasets characterized by both homophily and heterophily.
</details></li>
</ul>
<hr>
<h2 id="Towards-Unsupervised-Graph-Completion-Learning-on-Graphs-with-Features-and-Structure-Missing"><a href="#Towards-Unsupervised-Graph-Completion-Learning-on-Graphs-with-Features-and-Structure-Missing" class="headerlink" title="Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing"></a>Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02762">http://arxiv.org/abs/2309.02762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sichao Fu, Qinmu Peng, Yang He, Baokun Du, Xinge You</li>
<li>for: 提高graph neural network（GNN）在图数据中的表现，特别是在图数据中存在特定的特征和结构缺失时。</li>
<li>methods: 提出了一种更通用的graph completion learning（GCL）框架，通过自我监督学习提高存在特定特征和结构缺失的GNN变种在图数据中的表现。</li>
<li>results: 通过对八个数据集、三种GNN变种和五种缺失率进行广泛的实验，证明了我们提出的方法的效iveness。<details>
<summary>Abstract</summary>
In recent years, graph neural networks (GNN) have achieved significant developments in a variety of graph analytical tasks. Nevertheless, GNN's superior performance will suffer from serious damage when the collected node features or structure relationships are partially missing owning to numerous unpredictable factors. Recently emerged graph completion learning (GCL) has received increasing attention, which aims to reconstruct the missing node features or structure relationships under the guidance of a specifically supervised task. Although these proposed GCL methods have made great success, they still exist the following problems: the reliance on labels, the bias of the reconstructed node features and structure relationships. Besides, the generalization ability of the existing GCL still faces a huge challenge when both collected node features and structure relationships are partially missing at the same time. To solve the above issues, we propose a more general GCL framework with the aid of self-supervised learning for improving the task performance of the existing GNN variants on graphs with features and structure missing, termed unsupervised GCL (UGCL). Specifically, to avoid the mismatch between missing node features and structure during the message-passing process of GNN, we separate the feature reconstruction and structure reconstruction and design its personalized model in turn. Then, a dual contrastive loss on the structure level and feature level is introduced to maximize the mutual information of node representations from feature reconstructing and structure reconstructing paths for providing more supervision signals. Finally, the reconstructed node features and structure can be applied to the downstream node classification task. Extensive experiments on eight datasets, three GNN variants and five missing rates demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
近年来，图 neural network (GNN) 在多种图分析任务上取得了显著的进步。然而，GNN 的性能会受到部分缺失的节点特征或结构关系的不可预测因素产生严重损害。随后出现的图完成学习 (GCL) 得到了更多的关注，它目的是在指定的超级任务下重建缺失的节点特征或结构关系。虽然这些提出的 GCL 方法已经取得了很大的成功，但它们还存在以下问题：依赖于标签、节点特征和结构关系的偏见。此外，现有的 GCL 总的适应能力仍然面临着同时缺失节点特征和结构关系的挑战。为解决上述问题，我们提出了一个更加通用的 GCL 框架，通过自动学习提高现有 GNN 变体在图中缺失特征和结构时的任务性能，称为无监督 GCL (UGCL)。具体来说，为了避免在 GNN 的消息传递过程中缺失节点特征和结构的匹配问题，我们将特征重建和结构重建分别设计为独立的个性化模型，然后引入结构和特征两级对比损失来最大化节点表示之间的互信息。最后，重建的节点特征和结构可以应用于下游节点分类任务。我们在八个数据集、三种 GNN 变体和五种缺失率进行了广泛的实验，结果表明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="GPT-Can-Solve-Mathematical-Problems-Without-a-Calculator"><a href="#GPT-Can-Solve-Mathematical-Problems-Without-a-Calculator" class="headerlink" title="GPT Can Solve Mathematical Problems Without a Calculator"></a>GPT Can Solve Mathematical Problems Without a Calculator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03241">http://arxiv.org/abs/2309.03241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/mathglm">https://github.com/thudm/mathglm</a></li>
<li>paper_authors: Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang</li>
<li>for: 挑战大语言模型无法准确进行多位数 arithmetic 操作的假设</li>
<li>methods: 使用充足的训练数据，一个200亿参数的语言模型可以准确地进行多位数 arithmetic 操作，无需数据泄露</li>
<li>results: 我们的 MathGLM 在一个5,000个样本的中文数学问题测试集上达到了和GPT-4相似的性能，而GPT-4 的多位数 multiplication 准确率只有4.3%<details>
<summary>Abstract</summary>
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public at https://github.com/THUDM/MathGLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SWAP-Exploiting-Second-Ranked-Logits-for-Adversarial-Attacks-on-Time-Series"><a href="#SWAP-Exploiting-Second-Ranked-Logits-for-Adversarial-Attacks-on-Time-Series" class="headerlink" title="SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series"></a>SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02752">http://arxiv.org/abs/2309.02752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang George Dong, Liangwei Nathan Zheng, Weitong Chen, Wei Emma Zhang, Lin Yue</li>
<li>for: 这篇研究旨在提高时间序列分类模型的攻击性能。</li>
<li>methods: 提出了一种新的攻击方法SWAP，通过对第二名的条件预测值进行强化，以提高攻击成功率。</li>
<li>results: 实验结果显示SWAP可以实现超过50%的攻击成功率，较现有方法提高18%。<details>
<summary>Abstract</summary>
Time series classification (TSC) has emerged as a critical task in various domains, and deep neural models have shown superior performance in TSC tasks. However, these models are vulnerable to adversarial attacks, where subtle perturbations can significantly impact the prediction results. Existing adversarial methods often suffer from over-parameterization or random logit perturbation, hindering their effectiveness. Additionally, increasing the attack success rate (ASR) typically involves generating more noise, making the attack more easily detectable. To address these limitations, we propose SWAP, a novel attacking method for TSC models. SWAP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits. This is achieved by minimizing the Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution. Experimental results demonstrate that SWAP achieves state-of-the-art performance, with an ASR exceeding 50% and an 18% increase compared to existing methods.
</details>
<details>
<summary>摘要</summary>
时间序列分类（TSC）任务在不同领域中变得越来越重要，深度神经网络在TSC任务中表现出色。然而，这些模型受到恶意攻击的威胁，其中微的杂乱可能导致预测结果受到重大影响。现有的恶意方法经常受到过参数化或随机Logit扰动的限制，这限制了其效iveness。尽管提高攻击成功率（ASR）通常需要生成更多的噪音，但这会使攻击更易被发现。为解决这些局限性，我们提出了SWAP，一种新的攻击方法 дляTSC模型。SWAP通过提高第二个排名的Logit的信任程度，同时最小化其他Logit的扰动。这是通过控制target Logit分布和预测Logit分布之间的Kullback-Leibler差异来实现的。实验结果表明，SWAP可以达到状态的损失表现，ASR超过50%，与现有方法相比增加18%。
</details></li>
</ul>
<hr>
<h2 id="Safe-Neural-Control-for-Non-Affine-Control-Systems-with-Differentiable-Control-Barrier-Functions"><a href="#Safe-Neural-Control-for-Non-Affine-Control-Systems-with-Differentiable-Control-Barrier-Functions" class="headerlink" title="Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions"></a>Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04492">http://arxiv.org/abs/2309.04492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiao, Ross Allen, Daniela Rus</li>
<li>for: 本文解决了非线性控制系统的安全控制问题。</li>
<li>methods: 该方法使用控制边界函数(CBF)来优化 quadratic costs，并通过 incorporating higher-order CBFs into neural ordinary differential equation-based learning models 来保证安全性。</li>
<li>results: 该方法可以学习复杂且优化的控制策略，并能够 addressed the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries。furthermore, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online。<details>
<summary>Abstract</summary>
This paper addresses the problem of safety-critical control for non-affine control systems. It has been shown that optimizing quadratic costs subject to state and control constraints can be sub-optimally reduced to a sequence of quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our recently proposed High Order CBFs (HOCBFs) can accommodate constraints of arbitrary relative degree. The main challenges in this approach are that it requires affine control dynamics and the solution of the CBF-based QP is sub-optimal since it is solved point-wise. To address these challenges, we incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, and thus, they can address the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries. Moreover, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online. We illustrate the effectiveness of the proposed framework on LiDAR-based autonomous driving and compare it with existing methods.
</details>
<details>
<summary>摘要</summary>
To address these challenges, the authors incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, allowing them to address the conservativeness of CBFs and ensure that the system state does not stay unnecessarily far away from safe set boundaries. Additionally, the imitation learning model is capable of learning complex and optimal control policies that are usually intractable online.The proposed framework is evaluated on LiDAR-based autonomous driving and compared with existing methods, demonstrating its effectiveness. The key contributions of the paper include the development of a safe and efficient control method for non-affine control systems using higher-order CBFs and neural networks, and the demonstration of its effectiveness in a real-world application.
</details></li>
</ul>
<hr>
<h2 id="Offensive-Hebrew-Corpus-and-Detection-using-BERT"><a href="#Offensive-Hebrew-Corpus-and-Detection-using-BERT" class="headerlink" title="Offensive Hebrew Corpus and Detection using BERT"></a>Offensive Hebrew Corpus and Detection using BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02724">http://arxiv.org/abs/2309.02724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinalab/offensivehebrew">https://github.com/sinalab/offensivehebrew</a></li>
<li>paper_authors: Nagham Hamad, Mustafa Jarrar, Mohammad Khalilia, Nadim Nashif<br>for: 本研究 targets  Hebrew offensive language detection, which has been understudied in low-resource languages.methods: 本研究使用了 Twitter 上的 15,881 个微博，并采用了 Five-class 分类法（不夹着、仇恨、暴力、色情、无偏见）。 Each tweet was labeled by Arabic-Hebrew bilingual speakers, and the annotation process was challenging due to the need for familiarity with Israeli culture, politics, and practices.results: 研究人员使用了自己的数据集和另一个已发表的数据集进行 fine-tuning HeBERT 和 AlephBERT 模型。结果显示，将 HeBERT 模型在我们的数据集上进行 fine-tuning，并测试在 D_OLaH 上得到了 2% 的提升。而使用我们的数据集和 D_OLaH 进行 fine-tuning，然后测试在我们的数据集上得到了 69% 的准确率。这表明我们的数据集具有通用性。<details>
<summary>Abstract</summary>
Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offers. Our dataset and fine-tuned models are available on GitHub and Huggingface.
</details>
<details>
<summary>摘要</summary>
偏Language检测在多种语言中已经得到了广泛的研究，但在low-resource语言中，如希伯来语，却落后于其他语言。在这篇论文中，我们提供了一个新的希伯来语偏Language corpus。从Twitter上检索了15,881封微博，并将每封微博分为五类（凶恶、仇恨、暴力、色情或无偏Language），由阿拉伯希伯来双语 speaker进行标注。标注过程是具有挑战性的，因为每个标注员需要熟悉以色列文化、政治和实践，以理解每封微博的上下文。我们使用我们提议的数据集和另一个已发表的数据集进行练化两个希伯来BERT模型（HeBERT和AlephBERT）。我们发现，将HeBERT练化于我们的数据集后，与D_OLaH进行组合，可以提高HeBERT的性能2%。对AlephBERT进行练化并在D_OLaH上测试，可以达到69%的准确率，而对HeBERT进行练化并在我们的数据集上测试，可以达到57%的准确率，这可能是我们数据的总体化能力的表现。我们的数据集和练化后的模型都可以在GitHub和Huggingface上获取。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-frontiers-of-deep-learning-innovations-shaping-diverse-domains"><a href="#Unveiling-the-frontiers-of-deep-learning-innovations-shaping-diverse-domains" class="headerlink" title="Unveiling the frontiers of deep learning: innovations shaping diverse domains"></a>Unveiling the frontiers of deep learning: innovations shaping diverse domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02712">http://arxiv.org/abs/2309.02712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shams Forruque Ahmed, Md. Sakib Bin Alam, Maliha Kabir, Shaila Afrin, Sabiha Jannat Rafa, Aanushka Mehjabin, Amir H. Gandomi</li>
<li>for: 本研究旨在探讨深度学习在各个领域的应用和挑战。</li>
<li>methods: 本研究使用了深度学习来处理大量数据，并利用了LSTM和GRU等闭合架构来解决数据挑战。</li>
<li>results: 研究发现，深度学习可以在各个领域中提供高度的预测和分析精度，但需要大量数据来支持。<details>
<summary>Abstract</summary>
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate itself and optimize, making it effective in processing data with no prior training. Given its independence from training data, deep learning necessitates massive amounts of data for effective analysis and processing, much like data volume. To handle the challenge of compiling huge amounts of medical, scientific, healthcare, and environmental data for use in deep learning, gated architectures like LSTMs and GRUs can be utilized. For multimodal learning, shared neurons in the neural network for all activities and specialized neurons for particular tasks are necessary.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）允许开发计算机模型，能够学习、视觉、优化、纠正和预测数据。在过去几年，DL在各种领域应用，包括音频视频数据处理、农业、交通预测、自然语言、生物医学、灾害管理、生物信息学、药物设计、 genomics、人脸识别和生态学。为了探索当前深度学习的状态，需要调查最新的发展和应用深度学习在这些领域。然而，文献缺乏探索深度学习在所有领域的应用。这篇论文因此进行了广泛的调查，探讨了深度学习在所有主要领域的应用，以及相关的利点和挑战。据文献显示，DL在预测和分析中准确，使其成为强大的计算工具，同时具有自适应和优化能力，使其在没有先进trainings的情况下处理数据非常有效。由于DL的独立性，它需要巨量数据进行有效的分析和处理，类似于大量数据。为了处理医疗、科学、健康和环境数据的巨量化，可以使用门控架构如LSTM和GRU。为实现多模态学习，共享神经网络中的所有活动的特征神经和特定任务的专业神经是必要的。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Imperfect-Symmetry-a-Novel-Symmetry-Learning-Actor-Critic-Extension"><a href="#Addressing-Imperfect-Symmetry-a-Novel-Symmetry-Learning-Actor-Critic-Extension" class="headerlink" title="Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension"></a>Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02711">http://arxiv.org/abs/2309.02711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-abr/Adaptive-Symmetry-Learning">https://github.com/m-abr/Adaptive-Symmetry-Learning</a></li>
<li>paper_authors: Miguel Abreu, Luis Paulo Reis, Nuno Lau</li>
<li>for: 本研究的目的是capturing human brain的symmetry能力through reinforcement learning，以便在具有不完全或不准确的对称描述时，能够自适应地学习和执行任务。</li>
<li>methods: 本研究使用了一种名为Adaptive Symmetry Learning（ASL）的模型减少actor-critic扩展，该模型可以在学习过程中适应不完全或不准确的对称描述，并且包括对称适应组件和模块化损失函数，以保证所有状态具有共同的对称关系。</li>
<li>results: 对于一个四脚蚂蚁模型，ASL可以在多个方向的移动任务中从大的偏差中恢复，并且可以在不同的隐藏对称状态下扩展知识。相比之下，相对方法在大多数情况下只能达到相似或更差的性能，因此ASL是一种值得采用的方法，可以充分利用模型对称性，同时补做固有的偏差。<details>
<summary>Abstract</summary>
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for multidirectional locomotion tasks. The results demonstrate that ASL is capable of recovering from large perturbations and generalizing knowledge to hidden symmetric states. It achieves comparable or better performance than alternative methods in most scenarios, making it a valuable approach for leveraging model symmetry while compensating for inherent perturbations.
</details>
<details>
<summary>摘要</summary>
“对照”是一个基本概念，帮助我们更好地理解我们的环境。然而，从数学角度来看，它往往过度简化现实。人类是一个好例子，他们在外表和认知上存在偏差（如有主导手）。然而，我们的大脑可以快速超越这些不完整性，并快速适应 симметrical 任务。这种能力是我们 capture 的目的，我们提出了 Adaptive Symmetry Learning（ASL）。ASL 是一种基于 actor-critic 扩展的模型减少法，可以在学习过程中适应不完整或不准确的对照描述。ASL 包括对照适应组件和一个模块化损失函数，使得在学习策略时对所有状态都 enforces 一致的对照关系。我们在一个四脚蚂蚁模型中进行了一个案例研究，以评估 ASL 在多向移动任务上的性能。结果表明，ASL 可以快速复原大幅偏差，并将知识扩展到隐藏的对照状态。它在大多数情况下与其他方法相当或更好的性能，使得它成为一种值得使用的方法，用于利用模型对照性而弥补内在偏差。
</details></li>
</ul>
<hr>
<h2 id="Improved-Outlier-Robust-Seeding-for-k-means"><a href="#Improved-Outlier-Robust-Seeding-for-k-means" class="headerlink" title="Improved Outlier Robust Seeding for k-means"></a>Improved Outlier Robust Seeding for k-means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02710">http://arxiv.org/abs/2309.02710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Deshpande, Rameshwar Pratap</li>
<li>For: The paper proposes a simple and efficient algorithm for robust $k$-means clustering, which is robust to outliers and has a provable $O(1)$ approximation guarantee.* Methods: The algorithm uses a modified $D^2$ sampling distribution to make it robust to outliers, and it runs in $O(ndk)$ time.* Results: The algorithm outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and has a provable $O(1)$ approximation guarantee. The algorithm is also modified to output exactly $k$ clusters while keeping its running time linear in $n$ and $d$. The proposed algorithm outperforms previous methods, including $k$-means++, uniform random seeding, greedy sampling for $k$ means, and robust $k$-means++, on standard real-world and synthetic data sets.<details>
<summary>Abstract</summary>
The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$ approximation guarantee \cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \textit{w.r.t.} $k$-means solution on inliers, does not hold.   Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.   Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and \textit{robust $k$-means++} \cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\cite{tkmeanspp}, and robust $k$-means++~\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \cite{feldman2007ptas,langberg2010universal,feldman2011unified}.
</details>
<details>
<summary>摘要</summary>
“$k$-means是一个受欢迎的聚类目标，但它本身是非坚固的和敏感于噪音。$k$-means++的内置seed或初始化使用$D^{2}$抽样，并且有一个可证的$O(\log k)$近似保证 \cite{AV2007}。但在噪音或噪音的存在下，$D^{2}$抽样更可能从距离噪音的外部选择中心，而不是内部的填充集，因此其近似保证与$k$-means解不同。 assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers。”“我们的算法在$O(ndk)$时间内运行，输出$O(k)$个聚类，抛弃一点点多于最佳噪音数量，并且有一个可证的$O(1)$近似保证。我们的算法也可以修改以输出精确$k$个聚类，而不是$O(k)$个聚类，保持时间线性增长。这是与前一 Results for 坚固 $k$-means based on LP relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and robust $k$-means++ \cite{DeshpandeKP20} 的优化。我们的实验结果显示我们的算法在标准的实验数据上的优势，比$k$-means++ \cite{AV2007}、单调随机seed \cite{tkmeanspp}和坚固 $k$-means++ \cite{DeshpandeKP20}。”“我们的提议是容易实现批量、更快的$k$-means++实现 \cite{Bahmani,BachemL017}，并且独立 interessant для核心sets constructions在噪音存在下 \cite{feldman2007ptas,langberg2010universal,feldman2011unified}。”
</details></li>
</ul>
<hr>
<h2 id="Certifying-LLM-Safety-against-Adversarial-Prompting"><a href="#Certifying-LLM-Safety-against-Adversarial-Prompting" class="headerlink" title="Certifying LLM Safety against Adversarial Prompting"></a>Certifying LLM Safety against Adversarial Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02705">http://arxiv.org/abs/2309.02705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju</li>
<li>for: 防止语言模型生成有害内容</li>
<li>methods: 使用擦除和检查机制，对输入提问进行安全检查，并使用开源语言模型Llama 2作为安全筛选器</li>
<li>results: 对于恶意提问，实现了强制保证安全性的证明，同时对安全提问保持良好的性能<details>
<summary>Abstract</summary>
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial sequence at the end of the prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Empirical results demonstrate that our technique obtains strong certified safety guarantees on harmful prompts while maintaining good performance on safe prompts. For example, against adversarial suffixes of length 20, it certifiably detects 93% of the harmful prompts and labels 94% of the safe prompts as safe using the open source language model Llama 2 as the safety filter.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在公共使用时采用了保险措施，以确保其输出不会 causese harm。一个Alignment的语言模型应该拒绝用户的请求生成危险内容。然而，这些安全措施可能会被恶意设计的提示Sequence circumvent the model's safety guards and cause it to produce harmful content。在这项工作中，我们引入了“erase-and-check”，第一个防御性提示的框架，具有可靠的安全保证。我们将 individually erase tokens and inspect the resulting subsequences using a safety filter。如果任何 subsequences或输入提示被安全筛选器认为危险，我们就会将输入提示标记为危险。这保证了任何 adversarial modification of a harmful prompt up to a certain size will also be labeled harmful。我们防御了三种攻击模式：i） adversarial suffix，ii） adversarial insertion，和 iii） adversarial infusion。我们的技术在危险提示上获得了强的可靠安全保证，而不会影响安全提示的性能。例如，对于 adversarial suffixes of length 20，我们可靠地检测了93%的危险提示，并将94%的安全提示标记为安全使用开源语言模型 Llama 2 作为安全筛选器。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-EDFs-Bi-equivariant-Denoising-Generative-Modeling-on-SE-3-for-Visual-Robotic-Manipulation"><a href="#Diffusion-EDFs-Bi-equivariant-Denoising-Generative-Modeling-on-SE-3-for-Visual-Robotic-Manipulation" class="headerlink" title="Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation"></a>Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02685">http://arxiv.org/abs/2309.02685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomato1mule/diffusion_edf">https://github.com/tomato1mule/diffusion_edf</a></li>
<li>paper_authors: Hyunwoo Ryu, Jiwoo Kim, Junwoo Chang, Hyun Seok Ahn, Joohwan Seo, Taehan Kim, Yubin Kim, Jongeun Choi, Roberto Horowitz</li>
<li>for: 本研究旨在提高机器人学习的数据效率、通用性和稳定性，并应用扩展的Diffusion-EDFs方法。</li>
<li>methods: 该方法 integrates SE(3)-equivariance into diffusion generative modeling，以提高数据效率和通用性。</li>
<li>results: 研究表明，该方法只需要5-10个任务示例就能够实现有效的端到端训练，并且在扩展性方面表现出色。<details>
<summary>Abstract</summary>
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spatio-Temporal-Contrastive-Self-Supervised-Learning-for-POI-level-Crowd-Flow-Inference"><a href="#Spatio-Temporal-Contrastive-Self-Supervised-Learning-for-POI-level-Crowd-Flow-Inference" class="headerlink" title="Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference"></a>Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03239">http://arxiv.org/abs/2309.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songyu Ke, Ting Li, Li Song, Yanping Sun, Qintian Sun, Junbo Zhang, Yu Zheng</li>
<li>for: 这个研究旨在提高实时人流监控和城市规划等领域中的准确性，因为现有的城市感知技术所提供的数据质量不够高，无法实时监控每个Points of Interest（POI）的人流。</li>
<li>methods: 这个研究使用了自我supervised attributed graph representation learning技术，将人流监控问题转换为一个自我supervised learning任务，并提出了一个新的对应推导框架（CSST）。</li>
<li>results: 研究表明，使用CSST预训练了大量的杂凑数据，并与高精度的人流数据进行了调整，可以实现更高的精度和可靠性在人流监控中。<details>
<summary>Abstract</summary>
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) The scarcity and rarity of labeled data, 2) The intricate spatio-temporal dependencies among POIs, and 3) The myriad correlations between precise crowd flow and GPS reports.   To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel Contrastive Self-learning framework for Spatio-Temporal data (CSST). Our approach initiates with the construction of a spatial adjacency graph founded on the POIs and their respective distances. We then employ a contrastive learning technique to exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped prediction approach to anticipate the representation of the target subgraph from similar instances. Following the pre-training phase, the model is fine-tuned with accurate crowd flow data. Our experiments, conducted on two real-world datasets, demonstrate that the CSST pre-trained on extensive noisy data consistently outperforms models trained from scratch.
</details>
<details>
<summary>摘要</summary>
为了有效管理交通和城市规划，精准获取人流量 Points of Interest (POIs) 非常重要。然而，由于城市感知技术的限制，大多数数据质量不够高，无法精准监测 POIs 上的人流量。这使得从低质量数据中推断精准人流量变得非常困难。这些挑战的主要原因包括：1）数据的罕见性和罕见性，2） POIs 之间的复杂空间时间关系，3）精准人流量和 GPS 报告之间的多重相关性。为解决这些挑战，我们将人流量推断问题转化为一种自动归类 attributed graph representation learning 任务，并提出了一种新的 Contrastive Self-learning 框架 для Spatio-Temporal 数据 (CSST)。我们的方法从 POIs 的空间邻接图开始，然后使用对比学习技术来利用大量的无标注 Spatio-Temporal 数据。我们采用了交换预测方法，以预测目标子图的表示。在预训练阶段后，我们将模型精度地 fine-tune 到准确的人流量数据上。我们在两个真实世界数据集上进行了实验，结果表明，CSST 在庞大的噪音数据上进行预训练后， invariably 在精度上超过从零开始训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Design-Choices-and-Their-Impact-on-Emotion-Recognition-Model-Development-and-Evaluation"><a href="#Implicit-Design-Choices-and-Their-Impact-on-Emotion-Recognition-Model-Development-and-Evaluation" class="headerlink" title="Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation"></a>Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03238">http://arxiv.org/abs/2309.03238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mimansa Jaiswal</li>
<li>for: 这项研究的目的是提高情绪识别的准确性和可靠性，并处理数据采集和标注中的主观性问题。</li>
<li>methods: 本研究使用多模态数据集，包括视频、audio和文本数据，并使用数据拓展技术和对注释方法进行分析，以解决数据采集和标注中的问题。此外，研究还使用了对抗网络来隔离主观性变量，并提出了一种优化的社会学评价指标来测试模型。</li>
<li>results: 本研究实现了提高情绪识别的准确性和可靠性，并解决了数据采集和标注中的主观性问题。研究还发现，使用对抗网络可以隔离主观性变量，并且提出了一种优化的社会学评价指标，可以用于Cost-effective的情绪识别模型测试。<details>
<summary>Abstract</summary>
Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production.   To handle the challenge of non-representative training data, this work collects the Multimodal Stressed Emotion dataset, which introduces controlled stressors during data collection to better represent real-world influences on emotion production. To address issues with label subjectivity, this research comprehensively analyzes how data augmentation techniques and annotation schemes impact emotion perception and annotator labels. It further handles natural confounding variables and variations by employing adversarial networks to isolate key factors like stress from learned emotion representations during model training. For tackling concerns about leakage of sensitive demographic variables, this work leverages adversarial learning to strip sensitive demographic information from multimodal encodings. Additionally, it proposes optimized sociological evaluation metrics aligned with cost-effective, real-world needs for model testing.   This research advances robust, practical emotion recognition through multifaceted studies of challenges in datasets, labels, modeling, demographic and membership variable encoding in representations, and evaluation. The groundwork has been laid for cost-effective, generalizable emotion recognition models that are less likely to encode sensitive demographic information.
</details>
<details>
<summary>摘要</summary>
《情感认知问题具有复杂性，主要由情感生产和感知的主观性所致。这种主观性对计算机模型的开发带来了很大挑战。本论文研究了情感认知的关键方面，从数据采集开始。为了解决训练数据不准确表示实际情况的问题，本研究采集了多模态压力情感数据集，并在数据采集过程中引入了控制的压力因素，以更好地表示实际情况下的情感生产。为了解决标签主观性的问题，本研究全面分析了数据扩展技术和注释方案的影响，并对情感识别器的训练方法进行了优化。此外，本研究还处理了自然的干扰变量和变化，使用了对抗网络来隔离学习过程中的主要因素，如压力，从多模态编码中隔离了敏感人群信息。此外，本研究还提出了优化的社会学评价指标，以更好地适应实际的成本效益需求。通过这些研究，本论文提出了一种可靠、实用的情感认知模型，可以减少敏感人群信息的泄露。
</details></li>
</ul>
<hr>
<h2 id="RLSynC-Offline-Online-Reinforcement-Learning-for-Synthon-Completion"><a href="#RLSynC-Offline-Online-Reinforcement-Learning-for-Synthon-Completion" class="headerlink" title="RLSynC: Offline-Online Reinforcement Learning for Synthon Completion"></a>RLSynC: Offline-Online Reinforcement Learning for Synthon Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02671">http://arxiv.org/abs/2309.02671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frazier N. Baker, Ziqi Chen, Xia Ning</li>
<li>for: 本研究旨在提高 semi-template-based retrosynthesis 方法的实用性和可读性，通过对完整的 synthons 进行分析和评估，从而帮助化学家快速和有效地规划合成过程。</li>
<li>methods: 本研究使用了一种基于强化学习的 offline-online 方法 RLSynC，将一个代理人分配到每个 synthon，这些代理人通过一步步进行的动作搜索，以完成 synthons。RLSynC 会从 both offline 训练集和线上互动中学习策略，并使用一个前向合成模型来评估预测的反应中的可能性，以帮助搜索动作。</li>
<li>results: 实验结果显示，RLSynC 可以与现有的 retrosynthesis 方法相比，提高 synthon 完成率高达 14.9%，并且在 retrosynthesis 方面提高了 14.0%。这些结果显示 RLSynC 具有优秀的实用性和可读性，并且可以帮助化学家更快速地规划合成过程。<details>
<summary>Abstract</summary>
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare RLSynC with the state-of-the-art retrosynthesis methods. Our experimental results demonstrate that RLSynC can outperform these methods with improvement as high as 14.9% on synthon completion, and 14.0% on retrosynthesis, highlighting its potential in synthesis planning.
</details>
<details>
<summary>摘要</summary>
逆synthesis是确定所需产品的前一系列化学反应的过程。半模板基于的逆synthesis方法，首先预测产品中的反应中心，然后完成结果的synthons回到原料。这些方法提供了必要的可读性和实际应用性，以便在合成规划中提供指导。我们开发了一种新的线上-线下强化学习方法RLSynC，用于在半模板基于的方法中进行synthon完成。RLSynC分配了一个代理人到每个synthon中，这些代理人逐步进行动作，以同步进行完成。RLSynC从线上训练集和在线互动中学习策略，这allow RLSynC可以探索新的反应空间。RLSynC使用前向合成模型来评估预测的原料是否可以合成产品，并因此引导动作搜索。我们与当前领域的状态势比较RLSynC，我们的实验结果表明，RLSynC可以与现有的逆synthesis方法相比，在synthon完成和逆synthesis方面提高14.9%和14.0%。这highlights RLSynC在合成规划中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Marketing-Budget-Allocation-with-Offline-Constrained-Deep-Reinforcement-Learning"><a href="#Marketing-Budget-Allocation-with-Offline-Constrained-Deep-Reinforcement-Learning" class="headerlink" title="Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning"></a>Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02669">http://arxiv.org/abs/2309.02669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchi Cai, Jiyan Jiang, Wenpeng Zhang, Shiji Zhou, Xierui Song, Li Yu, Lihong Gu, Xiaodong Zeng, Jinjie Gu, Guannan Zhang</li>
<li>for: 本研究旨在解决在线广告运营中利用先前收集的线上数据的预算分配问题。</li>
<li>methods: 我们提出了一种基于游戏理论的半策略权值学习方法，可以减少存储多少策略的需求，从而实现减少策略数量，使其在实际应用中更加实用和有利。</li>
<li>results: 我们的实验表明，提出的方法可以在一个大规模的广告运营中，与多种基eline方法相比，具有更高的效果和稳定性。此外，我们还证明了该方法可以确定优化预算分配策略，而前一代值基学习方法无法实现这一点。<details>
<summary>Abstract</summary>
We study the budget allocation problem in online marketing campaigns that utilize previously collected offline data. We first discuss the long-term effect of optimizing marketing budget allocation decisions in the offline setting. To overcome the challenge, we propose a novel game-theoretic offline value-based reinforcement learning method using mixed policies. The proposed method reduces the need to store infinitely many policies in previous methods to only constantly many policies, which achieves nearly optimal policy efficiency, making it practical and favorable for industrial usage. We further show that this method is guaranteed to converge to the optimal policy, which cannot be achieved by previous value-based reinforcement learning methods for marketing budget allocation. Our experiments on a large-scale marketing campaign with tens-of-millions users and more than one billion budget verify the theoretical results and show that the proposed method outperforms various baseline methods. The proposed method has been successfully deployed to serve all the traffic of this marketing campaign.
</details>
<details>
<summary>摘要</summary>
我们研究线上广告营运中的预算分配问题，利用先前收集的线上数据。我们首先讨论了长期影响优化广告预算分配决策的效果。为了解决这个挑战，我们提出了一种新的游戏理论基础的偏变策略混合方法。这个方法可以将传统上需要储存无限多个策略的问题降低到仅储存常量多个策略，实现了近乎最佳策略效率，使其在实际应用中成为可能和有利的。我们进一步显示了这个方法可以对广告预算分配问题进行传递均衡，而这是先前值基推广学中无法实现的。我们的实验显示，这个方法可以在大规模的广告营运中实现理论上的最佳性和实际上的优化性，并且已经成功部署到处理整个广告营运的应用中。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Over-Images-Vertical-Decompositions-and-Pre-Trained-Backbones-Are-Difficult-to-Beat"><a href="#Federated-Learning-Over-Images-Vertical-Decompositions-and-Pre-Trained-Backbones-Are-Difficult-to-Beat" class="headerlink" title="Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat"></a>Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03237">http://arxiv.org/abs/2309.03237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erdong Hu, Yuxin Tang, Anastasios Kyrillidis, Chris Jermaine</li>
<li>for: 研究了许多 Federated Learning 中的算法，并对各种图像分类任务进行了测试。</li>
<li>methods: 考虑了许多前不得以考虑的问题，如数据集中图像多样性的影响、使用预训练特征提取“backbone”、评估学习器表现等。</li>
<li>results: 在多种设置下，发现Vertically decomposing a neural network 最佳化结果，并超越了标准的重新结合方法。<details>
<summary>Abstract</summary>
We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
</details>
<details>
<summary>摘要</summary>
我们仔细评估了许多 federated learning 环境下的算法，并测试其用于多种图像分类任务。我们考虑了许多未得到足够考虑的问题：不同数据集之间的图像多样性对结果的影响；使用预训练的特征提取“脊梁”；评估学习器性能的方法（我们认为准确率不够）等。总之，在各种设置下，我们发现将神经网络垂直分解得到最佳结果，并超越了标准的重新调节方法。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-as-Kernel-Approximation"><a href="#Contrastive-Learning-as-Kernel-Approximation" class="headerlink" title="Contrastive Learning as Kernel Approximation"></a>Contrastive Learning as Kernel Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02651">http://arxiv.org/abs/2309.02651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Christopher Tsiolis</li>
<li>for:  This paper is written for those interested in contrastive learning and its applications in feature learning.</li>
<li>methods: The paper uses contrastive learning methods to train low-dimensional vector representations of high-dimensional inputs on large unlabelled datasets. These methods include contrastive loss functions, such as those that implicitly approximate a positive semidefinite (PSD) kernel.</li>
<li>results: The paper provides an overview of the current theoretical understanding of contrastive learning, specifically in relation to the minimizers of contrastive loss functions and their relationship to prior methods for learning features from unlabelled data. It highlights popular contrastive loss functions whose minimizers implicitly approximate a PSD kernel.<details>
<summary>Abstract</summary>
In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.   The goal of this thesis is to provide an overview of the current theoretical understanding of contrastive learning, specifically as it pertains to the minimizers of contrastive loss functions and their relationship to prior methods for learning features from unlabelled data. We highlight popular contrastive loss functions whose minimizers implicitly approximate a positive semidefinite (PSD) kernel. The latter is a well-studied object in functional analysis and learning theory that formalizes a notion of similarity between elements of a space. PSD kernels provide an implicit definition of features through the theory of reproducing kernel Hilbert spaces.
</details>
<details>
<summary>摘要</summary>
通常的超级vised机器学习中需要为每个输入提供标签。然而，在许多应用领域中，Raw Data可以轻松地从互联网上获取，但是手动标签这些数据是非常昂贵的。为了解决这个问题，对比学习方法生成了低维度的向量表示（也称为特征），使得高维度的输入可以在大量未标记数据上进行学习。这是通过训练一个对比损失函数来实现的，该函数要求相似的输入在特征空间中具有高内积，而不相似的输入具有低内积。相比于每个输入都需要手动标签，只需定义一种采样相似和不相似的输入对来。对比特征可以被feed到标记数据集上的超级vised学习系统中，以实现高精度的终点任务。本论文的目标是提供对现有的对比学习理论的概括，具体来说是关于对比损失函数的最小化器和其与之前未标记数据学习的方法之间的关系。我们强调了流行的对比损失函数，其最小化器隐式地 Approximate一个正semidefinite（PSD）kernel。后者是函数分析和学习理论中已有研究的一种概念，它 formalizes a notion of similarity between elements of a space。PSD kernel Provides an implicit definition of features through the theory of reproducing kernel Hilbert spaces。
</details></li>
</ul>
<hr>
<h2 id="TFBEST-Dual-Aspect-Transformer-with-Learnable-Positional-Encoding-for-Failure-Prediction"><a href="#TFBEST-Dual-Aspect-Transformer-with-Learnable-Positional-Encoding-for-Failure-Prediction" class="headerlink" title="TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction"></a>TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02641">http://arxiv.org/abs/2309.02641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Mohapatra, Saptarshi Sengupta<br>for:* 预测硬盘失效，帮助数据中心避免数据损失和信任问题。methods:* 使用Self-Monitoring, Analysis and Reporting Technology（S.M.A.R.T）logs进行预测，并提出一种基于Transformer架构的新方法——Temporal-fusion Bi-encoder Self-attention Transformer（TFBEST）。results:* 在使用Seagate硬盘数据进行测试，TFBEST方法与当前状态艺术方法进行比较，显示TFBEST方法在10年的Backblaze数据中显著超越了state-of-the-art RUL预测方法。<details>
<summary>Abstract</summary>
Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic data loss to a question of goodwill, stakeholders want to avoid it like the plague. An important tool in proactively monitoring against HDD failure is timely estimation of the Remaining Useful Life (RUL). To this end, the Self-Monitoring, Analysis and Reporting Technology employed within HDDs (S.M.A.R.T.) provide critical logs for long-term maintenance of the security and dependability of these essential data storage devices. Data-driven predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based architectures heavily. However, they have suffered significantly in providing a confidence interval around the predicted RUL values as well as in processing very long sequences of logs. In addition, some of these approaches, such as those based on LSTMs, are inherently slow to train and have tedious feature engineering overheads. To overcome these challenges, in this work we propose a novel transformer architecture - a Temporal-fusion Bi-encoder Self-attention Transformer (TFBEST) for predicting failures in hard-drives. It is an encoder-decoder based deep learning technique that enhances the context gained from understanding health statistics sequences and predicts a sequence of the number of days remaining before a disk potentially fails. In this paper, we also provide a novel confidence margin statistic that can help manufacturers replace a hard-drive within a time frame. Experiments on Seagate HDD data show that our method significantly outperforms the state-of-the-art RUL prediction methods during testing over the exhaustive 10-year data from Backblaze (2013-present). Although validated on HDD failure prediction, the TFBEST architecture is well-suited for other prognostics applications and may be adapted for allied regression problems.
</details>
<details>
<summary>摘要</summary>
硬盘驱动器（HDD）在数据中心失效的情况非常昂贵，从悬峰数据丢失到口碑问题，各方都想避免这种情况。为了早期发现HDD失效，有一种重要的工具是计算硬盘的剩余有用生命（RUL）的准确预测。为此，硬盘内部的自我监控、分析和报告技术（S.M.A.R.T）提供了关键的日志记录，以长期保持硬盘存储设备的安全和可靠性。在过去，数据驱动的预测模型使用了这些S.M.A.R.T.日志和卷积神经网络（CNN/RNN）结构，但它们受到了预测RUL值的置信度范围和处理非常长的日志序列的限制。此外，一些这些方法，如基于LSTM的方法，在训练时间过长和特征工程杂乱的问题。为了解决这些挑战，我们在这种工作中提出了一种新的变换器架构——时间融合双指纹自注意Transformer（TFBEST），用于预测硬盘失效。这是一种基于encoder-decoder的深度学习技术，它可以从健康统计序列中获得更多的上下文，并预测硬盘失效的天数剩余。在这篇论文中，我们还提出了一种新的置信度范围统计，可以帮助制造商在一定时间内替换硬盘。在对Seagate HDD数据进行测试时，我们发现我们的方法在对比当前状态艺术RUL预测方法时表现出色。虽然我们的方法已经验证在硬盘失效预测中，但TFBEST架构适用于其他预测应用程序，可能适用于相关的回归问题。
</details></li>
</ul>
<hr>
<h2 id="Epi-Curriculum-Episodic-Curriculum-Learning-for-Low-Resource-Domain-Adaptation-in-Neural-Machine-Translation"><a href="#Epi-Curriculum-Episodic-Curriculum-Learning-for-Low-Resource-Domain-Adaptation-in-Neural-Machine-Translation" class="headerlink" title="Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation"></a>Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02640">http://arxiv.org/abs/2309.02640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyu Chen, Di Zhuang, Mingchen Li, J. Morris Chang</li>
<li>for: 提高新领域少量数据下NMT模型的表现</li>
<li>methods: 提出了一种新的 episodic 训练框架，以及一种 denoised curriculum learning 技术</li>
<li>results: 实验表明，Epi-Curriculum 可以提高模型的鲁棒性和适应性，并且可以在seen和unseen领域中提高模型的表现<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) models have become successful, but their performance remains poor when translating on new domains with a limited number of data. In this paper, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Experiments on English-German and English-Romanian translation show that: (i) Epi-Curriculum improves both model's robustness and adaptability in seen and unseen domains; (ii) Our episodic training framework enhances the encoder and decoder's robustness to domain shift.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiclass-Alignment-of-Confidence-and-Certainty-for-Network-Calibration"><a href="#Multiclass-Alignment-of-Confidence-and-Certainty-for-Network-Calibration" class="headerlink" title="Multiclass Alignment of Confidence and Certainty for Network Calibration"></a>Multiclass Alignment of Confidence and Certainty for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02636">http://arxiv.org/abs/2309.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinith Kugathasan, Muhammad Haris Khan</li>
<li>for: 提高模型预测结果的可靠性和准确性，特别是在安全关键应用场景。</li>
<li>methods: 基于模型参数全部使用的训练时间calibration方法，以及一种简单的插件式auxiliary loss函数multi-class alignment of predictive mean confidence and predictive certainty (MACC)。</li>
<li>results: 在10个复杂的数据集上进行了广泛的实验，得到了state-of-the-art的calibration性能，包括在预测内部和预测外部的预测。我们将代码和模型公开发布。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have made great strides in pushing the state-of-the-art in several challenging domains. Recent studies reveal that they are prone to making overconfident predictions. This greatly reduces the overall trust in model predictions, especially in safety-critical applications. Early work in improving model calibration employs post-processing techniques which rely on limited parameters and require a hold-out set. Some recent train-time calibration methods, which involve all model parameters, can outperform the postprocessing methods. To this end, we propose a new train-time calibration method, which features a simple, plug-and-play auxiliary loss known as multi-class alignment of predictive mean confidence and predictive certainty (MACC). It is based on the observation that a model miscalibration is directly related to its predictive certainty, so a higher gap between the mean confidence and certainty amounts to a poor calibration both for in-distribution and out-of-distribution predictions. Armed with this insight, our proposed loss explicitly encourages a confident (or underconfident) model to also provide a low (or high) spread in the presoftmax distribution. Extensive experiments on ten challenging datasets, covering in-domain, out-domain, non-visual recognition and medical image classification scenarios, show that our method achieves state-of-the-art calibration performance for both in-domain and out-domain predictions. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在许多复杂领域取得了大幅进步，但最新的研究表明它们有很大的自信问题。这会大幅降低模型预测结果的总体信任度，特别是在安全关键应用中。早期的模型校准方法使用了post处理技术，这些技术具有有限的参数和需要保留集。一些最近的运行时校准方法，它们包含所有模型参数，可以超越post处理方法。为此，我们提出了一种新的运行时校准方法，它具有一个简单、插件化的多类准确率对照（MACC）损失函数。这种损失函数基于模型误差与预测确定性之间的直接关系，因此，一个高于平均信任度的模型也应该提供一个低于平均信任度的 spreadoption 分布。利用这一见解，我们的提议的损失函数显式地鼓励一个自信（或不自信）的模型也提供一个低（或高）的 spreadoption 分布。我们在十个复杂的数据集上进行了广泛的实验，包括预测领域、非预测领域、非视觉识别和医学图像分类场景，结果显示，我们的方法在预测领域和非预测领域的校准性表现为状态机器人的最佳。我们的代码和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-from-Hierarchical-Weak-Preference-Feedback"><a href="#Deep-Reinforcement-Learning-from-Hierarchical-Weak-Preference-Feedback" class="headerlink" title="Deep Reinforcement Learning from Hierarchical Weak Preference Feedback"></a>Deep Reinforcement Learning from Hierarchical Weak Preference Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02632">http://arxiv.org/abs/2309.02632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abukharin3/heron">https://github.com/abukharin3/heron</a></li>
<li>paper_authors: Alexander Bukharin, Yixiao Li, Pengcheng He, Weizhu Chen, Tuo Zhao</li>
<li>for: 本研究的目的是开发一种新的奖励学习框架，以便在困难任务上学习奖励函数，并且减少人类干预的成本。</li>
<li>methods: 本研究使用了 preference-based 奖励模型，通过比较执行轨迹的决策树来训练奖励函数，然后使用这个奖励函数进行策略学习。</li>
<li>results: 研究发现，使用 HERON 框架可以训练高性能的代理人，同时提供更好的样本效率和鲁棒性。<details>
<summary>Abstract</summary>
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; simply by ranking the importance of the reward factors. More specifically, we propose a new RL framework -- HERON, which compares trajectories using a hierarchical decision tree induced by the given ranking. These comparisons are used to train a preference-based reward model, which is then used for policy learning. We find that our framework can not only train high performing agents on a variety of difficult tasks, but also provide additional benefits such as improved sample efficiency and robustness. Our code is available at https://github.com/abukharin3/HERON.
</details>
<details>
<summary>摘要</summary>
奖励设计是实用渐进学习（RL）的基本 yet 挑战性方面。 для简单任务，研究人员通常手工设计奖励函数，例如使用一些奖励因素的线性组合。 however, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; simply by ranking the importance of the reward factors. More specifically, we propose a new RL framework -- HERON, which compares trajectories using a hierarchical decision tree induced by the given ranking. These comparisons are used to train a preference-based reward model, which is then used for policy learning. We find that our framework can not only train high performing agents on a variety of difficult tasks, but also provide additional benefits such as improved sample efficiency and robustness. Our code is available at https://github.com/abukharin3/HERON.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/cs.LG_2023_09_06/" data-id="clmjn91mx00830j885uqu8ftz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/06/eess.AS_2023_09_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-09-06
        
      </div>
    </a>
  
  
    <a href="/2023/09/06/eess.IV_2023_09_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
