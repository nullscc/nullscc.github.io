
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="RoDia: A New Dataset for Romanian Dialect Identification from Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03378 repo_url: None paper_authors: Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-06">
<meta property="og:url" content="https://nullscc.github.io/2023/09/06/cs.SD_2023_09_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="RoDia: A New Dataset for Romanian Dialect Identification from Speech paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.03378 repo_url: None paper_authors: Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-06T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:17.067Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/06/cs.SD_2023_09_06/" class="article-date">
  <time datetime="2023-09-06T15:00:00.000Z" itemprop="datePublished">2023-09-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RoDia-A-New-Dataset-for-Romanian-Dialect-Identification-from-Speech"><a href="#RoDia-A-New-Dataset-for-Romanian-Dialect-Identification-from-Speech" class="headerlink" title="RoDia: A New Dataset for Romanian Dialect Identification from Speech"></a>RoDia: A New Dataset for Romanian Dialect Identification from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03378">http://arxiv.org/abs/2309.03378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu</li>
<li>For: The paper is written for the task of Romanian dialect identification from speech, with the goal of providing a valuable resource for future research.* Methods: The paper introduces a new dataset called RoDia, which includes a varied compilation of speech samples from five distinct regions of Romania, as well as a set of competitive models to be used as baselines.* Results: The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了罗马尼亚语言方言识别从语音中进行的，以提供未来研究的价值资源。* Methods: 论文介绍了一个新的数据集 called RoDia，该数据集包括来自罗马尼亚五个不同地区的语音样本，以及一些竞争性的模型。* Results: 最高分模型的macro F1分数为59.83%，微 F1分数为62.08%，表示任务是具有挑战性。<details>
<summary>Abstract</summary>
Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource that will stimulate research aiming to address the challenges of Romanian dialect identification. We publicly release our dataset and code at https://github.com/codrut2/RoDia.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>语言识别是语音处理和语言技术中的关键任务，提高了许多应用程序，如语音识别、speaker verification 等。然而，大多数研究都集中在广泛使用的语言上进行了研究，对低资源语言，如罗马尼亚语，的研究几乎没有任何关注。为了解决这个研究差距，我们介绍了 RoDia，罗马尼亚语言识别从speech的第一个数据集。RoDia数据集包括来自罗马尼亚五个不同区域的speech样本，涵盖了城市和农村环境，总计2小时的手动标注的语音数据。同时，我们也引入了一些竞争力强的模型，用于未来研究的基线。最高分模型的macro F1分数为59.83%，微 F1分数为62.08%，这表明这个任务非常困难。因此，我们认为RoDia是一个有价值的资源，将激发研究者解决罗马尼亚语言识别的挑战。我们将公开发布我们的数据集和代码在https://github.com/codrut2/RoDia。
</details></li>
</ul>
<hr>
<h2 id="Highly-Controllable-Diffusion-based-Any-to-Any-Voice-Conversion-Model-with-Frame-level-Prosody-Feature"><a href="#Highly-Controllable-Diffusion-based-Any-to-Any-Voice-Conversion-Model-with-Frame-level-Prosody-Feature" class="headerlink" title="Highly Controllable Diffusion-based Any-to-Any Voice Conversion Model with Frame-level Prosody Feature"></a>Highly Controllable Diffusion-based Any-to-Any Voice Conversion Model with Frame-level Prosody Feature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03364">http://arxiv.org/abs/2309.03364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyungguen Byun, Sunkuk Moon, Erik Visser</li>
<li>for: 这个论文的目的是提出一种可控性高的语音修饰系统，可以同时实现任意到任意语音转换（VC）和语速修饰。</li>
<li>methods: 该模型使用了一个帧级 просодиFeature来有效地传递气声和能量特征。具体来说，投射和内容嵌入被 feed到一个扩散基于decoder中，生成一个转换后的语音mel-spectrogram。为了调整说话速度，系统还包括一个自动学习模型后处理步骤，以提高可控性。</li>
<li>results: 该模型与State-of-the-art方法相比，可以覆盖各种基本频率（F0）、能量和速度修饰的范围，同时保持转换后的语音质量。<details>
<summary>Abstract</summary>
We propose a highly controllable voice manipulation system that can perform any-to-any voice conversion (VC) and prosody modulation simultaneously. State-of-the-art VC systems can transfer sentence-level characteristics such as speaker, emotion, and speaking style. However, manipulating the frame-level prosody, such as pitch, energy and speaking rate, still remains challenging. Our proposed model utilizes a frame-level prosody feature to effectively transfer such properties. Specifically, pitch and energy trajectories are integrated in a prosody conditioning module and then fed alongside speaker and contents embeddings to a diffusion-based decoder generating a converted speech mel-spectrogram. To adjust the speaking rate, our system includes a self-supervised model based post-processing step which allows improved controllability. The proposed model showed comparable speech quality and improved intelligibility compared to a SOTA approach. It can cover a varying range of fundamental frequency (F0), energy and speed modulation while maintaining converted speech quality.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高度可控的语音修饰系统，可同时进行任何到任何语音转换（VC）和情感调整。现状顶尖的VC系统可以传递句子水平特征，如说话者、情感和说话风格。然而，控制帧级的语音特征，如抽象高度和能量谱，仍然是挑战。我们的提议的模型利用帧级语音特征来有效地传递这些属性。 Specifically, 抽象高度和能量谱在一个语音条件模块中被 интеGRATED，然后与说话者和内容嵌入式一起被 diffusion-based decoder 生成一个转换后的语音mel-spectrogram。为了调整说话速度，我们的系统包括一个基于自我超vised模型的后期处理步骤，以提高可控性。我们的模型在比较Speech quality和智能性方面与顶尖方法相当，可以覆盖各种基本频率（F0）、能量和速度调整的范围，而不 sacrificing converted speech quality。
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Audio-Captioning-With-Faithful-Guidance-Using-Audio-text-Shared-Latent-Representation"><a href="#Parameter-Efficient-Audio-Captioning-With-Faithful-Guidance-Using-Audio-text-Shared-Latent-Representation" class="headerlink" title="Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation"></a>Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03340">http://arxiv.org/abs/2309.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arvind Krishna Sridhar, Yinyi Guo, Erik Visser, Rehana Mahfuz</li>
<li>for: 本研究旨在 Addressing overparameterization and hallucination issues in pretrained transformer architectures for automated audio captioning tasks.</li>
<li>methods: 我们提出了一种数据增强技术和一种 Parameter-efficient inference time faithful decoding algorithm to improve the performance and efficiency of audio captioning models.</li>
<li>results: 我们的方法可以在 benchmark datasets 上达到与更大的模型相同的性能，并且可以减少模型的复杂性和内存占用。<details>
<summary>Abstract</summary>
There has been significant research on developing pretrained transformer architectures for multimodal-to-text generation tasks. Albeit performance improvements, such models are frequently overparameterized, hence suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. In this paper, we address both these issues for the application of automated audio captioning. First, we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio-text shared latent space is suitable for detecting hallucination. Then, we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. During the beam decoding step, the smaller model utilizes an audio-text shared latent representation to semantically align the generated text with corresponding input audio. Faithful guidance is introduced into the beam probability by incorporating the cosine similarity between latent representation projections of greedy rolled out intermediate beams and audio clip. We show the efficacy of our algorithm on benchmark datasets and evaluate the proposed scheme against baselines using conventional audio captioning and semantic similarity metrics while illustrating tradeoffs between performance and complexity.
</details>
<details>
<summary>摘要</summary>
有很多研究关于开发预训练转换器架构来实现多Modal-to-文本生成任务。虽然性能提高，但这些模型往往过参数，因此容易出现幻觉和大内存占用问题，使其在边缘设备上部署困难。在这篇论文中，我们解决了这些问题，并应用于自动化音频captioning。我们首先提出了一种数据增强技术，用于生成幻觉音频caption，并证明了基于音频-文本共同特征空间的相似性是适用于检测幻觉的。然后，我们提出了一种具有效率的执行时实际 faithful decoding算法，可以使得音频captioning模型更小，性能与更多数据训练的大型模型相当。在探索步骤中，小型模型使用音频-文本共同特征来semantic地将生成的文本与输入音频进行对应。我们引入了对于扩展步骤的 faithful guidance，使用 audio clip的cosine similarity来衡量latent representation projections的greedy rolled out intermediate beams的相似性。我们在标准 benchmark数据集上证明了我们的算法的效果，并对基于传统音频captioning和semantic相似度度量的基eline进行评估，同时 illustrate tradeoffs between performance和复杂性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Geometrical-Acoustic-Simulations-of-Spatial-Room-Impulse-Responses-for-Improved-Sound-Event-Detection-and-Localization"><a href="#Leveraging-Geometrical-Acoustic-Simulations-of-Spatial-Room-Impulse-Responses-for-Improved-Sound-Event-Detection-and-Localization" class="headerlink" title="Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization"></a>Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03337">http://arxiv.org/abs/2309.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Ick, Brian McFee</li>
<li>for: 实现 зву标注数据的增加，以满足深度神经网络模型的需求。</li>
<li>methods: 使用几何学Acoustic simulation generate novel SRIR dataset，以训练SELD模型。</li>
<li>results: 透过实验资料增强现有的benchmark，并提高SELD模型的性能。<details>
<summary>Abstract</summary>
As deeper and more complex models are developed for the task of sound event localization and detection (SELD), the demand for annotated spatial audio data continues to increase. Annotating field recordings with 360$^{\circ}$ video takes many hours from trained annotators, while recording events within motion-tracked laboratories are bounded by cost and expertise. Because of this, localization models rely on a relatively limited amount of spatial audio data in the form of spatial room impulse response (SRIR) datasets, which limits the progress of increasingly deep neural network based approaches. In this work, we demonstrate that simulated geometrical acoustics can provide an appealing solution to this problem. We use simulated geometrical acoustics to generate a novel SRIR dataset that can train a SELD model to provide similar performance to that of a real SRIR dataset. Furthermore, we demonstrate using simulated data to augment existing datasets, improving on benchmarks set by state of the art SELD models. We explore the potential and limitations of geometric acoustic simulation for localization and event detection. We also propose further studies to verify the limitations of this method, as well as further methods to generate synthetic data for SELD tasks without the need to record more data.
</details>
<details>
<summary>摘要</summary>
In this study, we propose using simulated geometrical acoustics as a solution to this problem. By generating a novel SRIR dataset using simulated geometrical acoustics, we demonstrate that our approach can provide similar performance to that of a real SRIR dataset in training a SELD model. Additionally, we show that using simulated data to augment existing datasets can improve the performance of state-of-the-art SELD models.We explore the potential and limitations of geometric acoustic simulation for localization and event detection, and propose further studies to verify the limitations of this method. We also suggest additional methods for generating synthetic data for SELD tasks without the need for additional recording.Translation notes:* "field recordings" 被翻译为 "野外录音"* "motion-tracked laboratories" 被翻译为 "运动跟踪实验室"* "SRIR datasets" 被翻译为 "声学室应答数据集"* "simulated geometrical acoustics" 被翻译为 "计算机生成的几何声学"* "SELD models" 被翻译为 "声事件定位和检测模型"
</details></li>
</ul>
<hr>
<h2 id="Presenting-the-SWTC-A-Symbolic-Corpus-of-Themes-from-John-Williams’-Star-Wars-Episodes-I-IX"><a href="#Presenting-the-SWTC-A-Symbolic-Corpus-of-Themes-from-John-Williams’-Star-Wars-Episodes-I-IX" class="headerlink" title="Presenting the SWTC: A Symbolic Corpus of Themes from John Williams’ Star Wars Episodes I-IX"></a>Presenting the SWTC: A Symbolic Corpus of Themes from John Williams’ Star Wars Episodes I-IX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03298">http://arxiv.org/abs/2309.03298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claire Arthur, Frank Lehman, John McNamara</li>
<li>for: 这份报告描述了一个新的符号化乐曲资料库，名为星球战争主题集（SWTC），该库包含了John Williams创作的完整星球战争三部曲（集I-IX）中的64个不同、重复和符号意义强的乐Theme和动机。</li>
<li>methods: 该报告介绍了一种新的humdrum标准 для非功能和声编码，基于Harte（2005、2010）的研究。该报告还描述了译写和编码过程。</li>
<li>results: 该报告提供了一些简要的摘要统计数据，并表明SWTC虽然规模较小，但代表了20世纪一位最具影响力的作曲家之一的作品，以及电影和多媒体音乐材料的未exploredsubset。<details>
<summary>Abstract</summary>
This paper presents a new symbolic corpus of musical themes from the complete Star Wars trilogies (Episodes I-IX) by John Williams. The corpus files are made available in multiple formats (.krn, .sib, and .musicxml) and include melodic, harmonic, and formal information. The Star Wars Thematic Corpus (SWTC) contains a total of 64 distinctive, recurring, and symbolically meaningful themes and motifs, commonly referred to as leitmotifs. Through this corpus we also introduce a new humdrum standard for non-functional harmony encodings, **harte, based on Harte (2005, 2010). This report details the motivation, describes the transcription and encoding processes, and provides some brief summary statistics. While relatively small in scale, the SWTC represents a unified collection from one of the most prolific and influential composers of the 20th century, and the under-studied subset of film and multimedia musical material in general. We hope the SWTC will provide insights into John Williams' compositional style, as well as prove useful in comparisons against other thematic corpora from film and beyond.
</details>
<details>
<summary>摘要</summary>
Note: Some minor changes were made to the original text to improve readability and clarity, but the overall meaning and content remain the same.
</details></li>
</ul>
<hr>
<h2 id="Real-time-auralization-for-performers-on-virtual-stages"><a href="#Real-time-auralization-for-performers-on-virtual-stages" class="headerlink" title="Real-time auralization for performers on virtual stages"></a>Real-time auralization for performers on virtual stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03149">http://arxiv.org/abs/2309.03149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Accolti, Lukas Aspöck, Manuj Yadav, Michael Vorländer</li>
<li>for: 这篇论文描述了一个互动系统，用于实验室中的舞台声学实验。</li>
<li>methods: 该系统使用了一组矩阵来补做听到自己乐器的声音的问题，以及考虑了听到其他乐器的声音的问题。</li>
<li>results: 试验表明，该系统可以准确地模拟声音，并且可以跟踪实际的听众声音。<details>
<summary>Abstract</summary>
This article presents an interactive system for stage acoustics experimentation including considerations for hearing one's own and others' instruments. The quality of real-time auralization systems for psychophysical experiments on music performance depends on the system's calibration and latency, among other factors (e.g. visuals, simulation methods, haptics, etc). The presented system focuses on the acoustic considerations for laboratory implementations. The calibration is implemented as a set of filters accounting for the microphone-instrument distances and the directivity factors, as well as the transducers' frequency responses. Moreover, sources of errors are characterized using both state-of-the-art information and derivations from the mathematical definition of the calibration filter. In order to compensate for hardware latency without cropping parts of the simulated impulse responses, the virtual direct sound of musicians hearing themselves is skipped from the simulation and addressed by letting the actual direct sound reach the listener through open headphones. The required latency compensation of the interactive part (i.e. hearing others) meets the minimum distance requirement between musicians, which is 2 m for the implemented system. Finally, a proof of concept is provided that includes objective and subjective experiments, which give support to the feasibility of the proposed setup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Efficient-Temporary-Deepfake-Location-Approach-Based-Embeddings-for-Partially-Spoofed-Audio-Detection"><a href="#An-Efficient-Temporary-Deepfake-Location-Approach-Based-Embeddings-for-Partially-Spoofed-Audio-Detection" class="headerlink" title="An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection"></a>An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03036">http://arxiv.org/abs/2309.03036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankun Xie, Haonan Cheng, Yutian Wang, Long Ye</li>
<li>for: 本研究旨在提出一种精细化的半假声音检测方法，以准确地检测声音中的真实性。</li>
<li>methods: 本方法包括两个新的部分：嵌入相似模块和时间卷积操作。嵌入相似模块用于生成一个可分离真实和假声音的 embedding 空间，而时间卷积操作则用于计算邻域帧之间的相似性，并动态选择有用的邻域进行卷积。</li>
<li>results: 对于 ASVspoof2019 Partial Spoof 数据集，我们的方法表现出了对基线模型的超越性，并在跨数据集场景中也达到了优秀的表现。代码已经在线发布。<details>
<summary>Abstract</summary>
Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and demonstrate superior performance even in the crossdataset scenario. The code is released online.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传入文本转化为简化中文。<</SYS>>假 Audio 检测是一项复杂的任务，因为它需要准确地确定听录中的真实性，具体来说是在帧级别上。为解决这个问题，我们提议一种细化的假 Audio 检测方法，即 Temporal Deepfake Location（TDL），它可以准确地捕捉听录中的特征和位置信息。具体来说，我们的方法包括两个新的部分：嵌入相似模块和时间核算操作。为了增强真实和假听录之间的识别，嵌入相似模块是为了生成一个可以分离真听录和假听录的嵌入空间。而时间核算操作则是为了计算每帧的相似性，并选择相邻帧中的有用 neighboor 进行 convolution。通过这两个部分，我们的方法可以更好地捕捉听录中的信息，并且在跨数据集场景下表现出优于基eline模型。我们的代码已经在线发布。
</details></li>
</ul>
<hr>
<h2 id="Indoor-Localization-Using-Radio-Vision-and-Audio-Sensors-Real-Life-Data-Validation-and-Discussion"><a href="#Indoor-Localization-Using-Radio-Vision-and-Audio-Sensors-Real-Life-Data-Validation-and-Discussion" class="headerlink" title="Indoor Localization Using Radio, Vision and Audio Sensors: Real-Life Data Validation and Discussion"></a>Indoor Localization Using Radio, Vision and Audio Sensors: Real-Life Data Validation and Discussion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02961">http://arxiv.org/abs/2309.02961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilayda Yaman, Guoda Tian, Erik Tegler, Patrik Persson, Nikhil Challa, Fredrik Tufvesson, Ove Edfors, Kalle Astrom, Steffen Malkowsky, Liang Liu</li>
<li>for: 本研究探讨了使用 радио、视觉和听音感知器进行indoor定位方法，并在同一环境中进行评估。</li>
<li>methods: 本研究使用了现有算法来评估 радио基本的机器学习算法、视觉基本的ORB-SLAM3算法和听音基本的SFS2算法。</li>
<li>results: 研究发现，使用不同感知器可以实现高精度和可靠的indoor定位，但是每种感知器都有其优缺点，需要根据具体情况进行选择和权衡。<details>
<summary>Abstract</summary>
This paper investigates indoor localization methods using radio, vision, and audio sensors, respectively, in the same environment. The evaluation is based on state-of-the-art algorithms and uses a real-life dataset. More specifically, we evaluate a machine learning algorithm for radio-based localization with massive MIMO technology, an ORB-SLAM3 algorithm for vision-based localization with an RGB-D camera, and an SFS2 algorithm for audio-based localization with microphone arrays. Aspects including localization accuracy, reliability, calibration requirements, and potential system complexity are discussed to analyze the advantages and limitations of using different sensors for indoor localization tasks. The results can serve as a guideline and basis for further development of robust and high-precision multi-sensory localization systems, e.g., through sensor fusion and context and environment-aware adaptation.
</details>
<details>
<summary>摘要</summary>
Note:* " Simplified Chinese" is also known as "简化字" or "简化字" in Chinese.* " radio" is translated as "无线电" (wú xiàn diàn)* " vision" is translated as "视觉" (zhì jìng)* " audio" is translated as "声音" (shēng yīn)* " sensor" is translated as "感测器" (gǎn cè qì)* " localization" is translated as "位置确定" (wèi zhì jiè dìng)* " accuracy" is translated as "准确性" (zhèng qiú xìng)* " reliability" is translated as "可靠性" (kě jiào xìng)* " calibration" is translated as "准备" (zhèng bèi)* " system complexity" is translated as "系统复杂度" (xiàng zhì zhòng le)Please note that the translation is based on the standard Simplified Chinese language and may have some variations depending on the region or context.
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Blind-Spots-in-Spoken-Language-Processing"><a href="#Addressing-the-Blind-Spots-in-Spoken-Language-Processing" class="headerlink" title="Addressing the Blind Spots in Spoken Language Processing"></a>Addressing the Blind Spots in Spoken Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06572">http://arxiv.org/abs/2309.06572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef</li>
<li>for: 本研究探讨了人类交流中的关键 yet often overlooked 非语言表达，包括协同语言动作和 facial expressions，以及它们对自然语言处理（NLP）的影响。</li>
<li>methods: 我们提出了一种基于 sign language processing 的 universal automatic gesture segmentation and transcription models，用于将非语言表达转换为文本形式。这种方法旨在bridge spoken language understanding 中的盲点，提高 NLP 模型的范围和可用性。</li>
<li>results: 我们通过示例推动了强调仅仅 rely solely on text-based models 的局限性。我们提出了一种可计算 эффектив和灵活的方法，可以轻松地与现有的 NLP 管道集成。我们 conclude by calling upon the research community to contribute to the development of universal transcription methods and to validate their effectiveness in capturing the complexities of real-world, multi-modal interactions.<details>
<summary>Abstract</summary>
This paper explores the critical but often overlooked role of non-verbal cues, including co-speech gestures and facial expressions, in human communication and their implications for Natural Language Processing (NLP). We argue that understanding human communication requires a more holistic approach that goes beyond textual or spoken words to include non-verbal elements. Borrowing from advances in sign language processing, we propose the development of universal automatic gesture segmentation and transcription models to transcribe these non-verbal cues into textual form. Such a methodology aims to bridge the blind spots in spoken language understanding, enhancing the scope and applicability of NLP models. Through motivating examples, we demonstrate the limitations of relying solely on text-based models. We propose a computationally efficient and flexible approach for incorporating non-verbal cues, which can seamlessly integrate with existing NLP pipelines. We conclude by calling upon the research community to contribute to the development of universal transcription methods and to validate their effectiveness in capturing the complexities of real-world, multi-modal interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-Supervised-Disentanglement-of-Harmonic-and-Rhythmic-Features-in-Music-Audio-Signals"><a href="#Self-Supervised-Disentanglement-of-Harmonic-and-Rhythmic-Features-in-Music-Audio-Signals" class="headerlink" title="Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals"></a>Self-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02796">http://arxiv.org/abs/2309.02796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WuYiming6526/HARD-DAFx2023">https://github.com/WuYiming6526/HARD-DAFx2023</a></li>
<li>paper_authors: Yiming Wu</li>
<li>for: 本研究旨在推断音乐Audio生成过程中隐藏的多个有用的尺度表示，以实现可控的数据生成。</li>
<li>methods: 本文提出了一种基于深度神经网络的自监学习方法，用于推断音乐Audio生成过程中的谱写和和声表示。在训练阶段，Variational Autoencoder被训练来重建输入的mel-spectrogram，并在每次前向计算中应用了一个向量旋转操作，以假设特征向量的维度与抽象音间的幂距离相关。因此，在训练后的Variational Autoencoder中，旋转的尺度特征表示了音频中的抽象音谱，而不旋转的尺度特征表示了不变的rhythmic内容。</li>
<li>results: 在predictor-based拟合度量上测试了学习的特征，并证明了该方法的有效性。此外，本文还应用了该方法到自动生成音乐混音的问题中，并达到了良好的结果。<details>
<summary>Abstract</summary>
The aim of latent variable disentanglement is to infer the multiple informative latent representations that lie behind a data generation process and is a key factor in controllable data generation. In this paper, we propose a deep neural network-based self-supervised learning method to infer the disentangled rhythmic and harmonic representations behind music audio generation. We train a variational autoencoder that generates an audio mel-spectrogram from two latent features representing the rhythmic and harmonic content. In the training phase, the variational autoencoder is trained to reconstruct the input mel-spectrogram given its pitch-shifted version. At each forward computation in the training phase, a vector rotation operation is applied to one of the latent features, assuming that the dimensions of the feature vectors are related to pitch intervals. Therefore, in the trained variational autoencoder, the rotated latent feature represents the pitch-related information of the mel-spectrogram, and the unrotated latent feature represents the pitch-invariant information, i.e., the rhythmic content. The proposed method was evaluated using a predictor-based disentanglement metric on the learned features. Furthermore, we demonstrate its application to the automatic generation of music remixes.
</details>
<details>
<summary>摘要</summary>
目标是抽取数据生成过程中隐藏的多个有用特征表示，这是可控数据生成的关键因素。在这篇论文中，我们提出了一种基于深度神经网络的自supervised learning方法，用于抽取音乐Audio生成过程中的拍和和和声特征。我们训练了一个变量自动编码器，该编码器从两个隐藏特征中生成了一个Audio mel-spectrogram，其中一个隐藏特征表示拍和内容，另一个隐藏特征表示和声内容。在训练阶段，变量自动编码器被训练以重建输入 mel-spectrogram，并在每次前向计算中应用了一个向量旋转操作，假设输入特征向量的维度与抽象Interval相关。因此，在训练过程中，旋转的隐藏特征表示输入 mel-spectrogram 的抽象信息，而不旋转的隐藏特征表示拍和内容。我们使用predictor-based拟合度量来评估学习的特征。此外，我们还 demonstarted its应用于自动生成音乐remix。
</details></li>
</ul>
<hr>
<h2 id="GRASS-Unified-Generation-Model-for-Speech-to-Semantic-Tasks"><a href="#GRASS-Unified-Generation-Model-for-Speech-to-Semantic-Tasks" class="headerlink" title="GRASS: Unified Generation Model for Speech-to-Semantic Tasks"></a>GRASS: Unified Generation Model for Speech-to-Semantic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02780">http://arxiv.org/abs/2309.02780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aobo Xia, Shuyu Lei, Yushu Yang, Xiang Guo, Hua Chai</li>
<li>for: 本文探讨了对speech-to-semantic任务的指令细化技术，提出了一个综合的端到端（E2E）框架，通过给定任务相关的提示，对语音数据生成目标文本。</li>
<li>methods: 我们在大量和多样的数据进行预训练，使用文本到语音（TTS）系统生成 instruciton-speech 对。</li>
<li>results: 我们的提posed模型在许多标准准点上达到了状态元（SOTA）的 результа们，包括speech命名实体识别、speech情感分类、speech问答等。此外，我们的模型在零shot和几shotenario中也达到了竞争性的性能。<details>
<summary>Abstract</summary>
This paper explores the instruction fine-tuning technique for speech-to-semantic tasks by introducing a unified end-to-end (E2E) framework that generates target text conditioned on a task-related prompt for audio data. We pre-train the model using large and diverse data, where instruction-speech pairs are constructed via a text-to-speech (TTS) system. Extensive experiments demonstrate that our proposed model achieves state-of-the-art (SOTA) results on many benchmarks covering speech named entity recognition, speech sentiment analysis, speech question answering, and more, after fine-tuning. Furthermore, the proposed model achieves competitive performance in zero-shot and few-shot scenarios. To facilitate future work on instruction fine-tuning for speech-to-semantic tasks, we release our instruction dataset and code.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了对话 semantics 任务的指令细化技术，通过提出一种端到端 (E2E) 框架，使得目标文本根据任务相关的提示生成 Conditioned 于听力数据。我们在大量和多样的数据上进行预训练，使用文本到声音 (TTS) 系统来构建指令演讲对。广泛的实验表明，我们提出的模型在许多标准底下取得了状态的最佳 (SOTA) 结果，包括对话名 entity 识别、对话情感分类、对话问答等，并且在零 shot 和几 shot 情况下表现竞争力。为便于未来对指令细化 для speech-to-semantic 任务的研究，我们发布了我们的指令集和代码。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-Measurement-of-Multiple-Acoustic-Attributes-Using-Structured-Periodic-Test-Signals-Including-Music-and-Other-Sound-Materials"><a href="#Simultaneous-Measurement-of-Multiple-Acoustic-Attributes-Using-Structured-Periodic-Test-Signals-Including-Music-and-Other-Sound-Materials" class="headerlink" title="Simultaneous Measurement of Multiple Acoustic Attributes Using Structured Periodic Test Signals Including Music and Other Sound Materials"></a>Simultaneous Measurement of Multiple Acoustic Attributes Using Structured Periodic Test Signals Including Music and Other Sound Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02767">http://arxiv.org/abs/2309.02767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideki Kawahara, Kohei Yatabe, Ken-Ichi Sakakibara, Mitsunori Mizumachi, Tatsuya Kitamura</li>
<li>for: 这项研究旨在 simultaneously 测量音频性能中的线性时间不变（LTI）回响、信号依赖时间不变（SDTI）组件、随机和时间变化（RTV）组件。</li>
<li>methods: 这个框架使用结构化 periodic test signals 来测量音频性能，并可以使用音乐作品和其他声音材料作为测试信号，通过添加微量杂音来”保护”它们。</li>
<li>results: 实现了交互式、实时测量工具，并将其开源。此外，这个框架还用于对抽取器的性能进行对比性评价。<details>
<summary>Abstract</summary>
We introduce a general framework for measuring acoustic properties such as liner time-invariant (LTI) response, signal-dependent time-invariant (SDTI) component, and random and time-varying (RTV) component simultaneously using structured periodic test signals. The framework also enables music pieces and other sound materials as test signals by "safeguarding" them by adding slight deterministic "noise." Measurement using swept-sin, MLS (Maxim Length Sequence), and their variants are special cases of the proposed framework. We implemented interactive and real-time measuring tools based on this framework and made them open-source. Furthermore, we applied this framework to assess pitch extractors objectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的措施来同时测量音响性特性，包括时间不变（LTI）响应、时间不变信号（SDTI）组件以及随机时变（RTV）组件。这个框架还允许使用音乐作品和其他声音材料作为测试信号，并通过添加微量权值“噪声”来“保护”它们。使用滚动磁盘、MLS（最长长序）和其他变体的测量方法是这个框架的特殊情况。我们实现了基于这个框架的互动式和实时测量工具，并将其开源。此外，我们使用了这个框架对抽取器进行对比性评估。
</details></li>
</ul>
<hr>
<h2 id="MuLanTTS-The-Microsoft-Speech-Synthesis-System-for-Blizzard-Challenge-2023"><a href="#MuLanTTS-The-Microsoft-Speech-Synthesis-System-for-Blizzard-Challenge-2023" class="headerlink" title="MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023"></a>MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02743">http://arxiv.org/abs/2309.02743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihang Xu, Shaofei Zhang, Xi Wang, Jiajun Zhang, Wenning Wei, Lei He, Sheng Zhao</li>
<li>For: 本研究开发了一个名为MuLanTTS的微软终端神经文本读取系统，应援Blizzard Challenge 2023。* Methods: 本研究基于DelightfulTTS，采用了上下文和情感编码器，以扩展 sentences 的长形属性和对话表达性。* Results: MuLanTTS 在两个任务中获得了4.3和4.5的平均评价分数，与自然语音相似，并保持了好的相似性。<details>
<summary>Abstract</summary>
In this paper, we present MuLanTTS, the Microsoft end-to-end neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023. About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness. Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora. For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker fine tuning. MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment. The excellent and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了MuLanTTS系统，这是微软公司的终端神经文本读取（TTS）系统，旨在参加2023年的Blizzard挑战。我们发布了50小时的法语TTSHub任务和2小时的说话人适应任务的音频资料，用于建立不同测试目的的合成声音，包括句子、段落、同义词、列表等。基于DelightfulTTS，我们采用了上下文和情感编码器，以适应音频资料，提高长形味道和对话表达性。在录音质量方面，我们还应用了降噪算法和长 audio处理。在主任务中，我们只使用单个说话人的50小时数据来建立TTS系统，而在 spoke任务中，我们使用多个说话人的源模型进行目标说话人细化。MuLanTTS在两个任务中得到了4.3和4.5的平均评价分数，与自然语音相似，同时保持了好的相似性。这年的新和紧密的统计评价结果表明了我们提posed系统在两个任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="Stylebook-Content-Dependent-Speaking-Style-Modeling-for-Any-to-Any-Voice-Conversion-using-Only-Speech-Data"><a href="#Stylebook-Content-Dependent-Speaking-Style-Modeling-for-Any-to-Any-Voice-Conversion-using-Only-Speech-Data" class="headerlink" title="Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data"></a>Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02730">http://arxiv.org/abs/2309.02730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyungseob Lim, Kyungguen Byun, Sunkuk Moon, Erik Visser</li>
<li>for: 本研究旨在提出一种可以准确传递target speaker的语言风格信息的any-to-anyvoice转换方法。</li>
<li>methods: 本方法利用一种自动学习模型（SSL）来采集target speaker的发言风格信息，并将其转化为一组名为“stylebook”的嵌入。然后，这个stylebook与源语音的phonetic content进行attend，以确定最终的target style。最后，通过一种扩散模型来生成转换后的语音mel-spectrogram。</li>
<li>results: 实验结果显示，combined with our proposed method and a diffusion-based generative model, any-to-any voice conversion tasks can achieve better speaker similarity than baseline models, while the increase in computational complexity with longer utterances is suppressed.<details>
<summary>Abstract</summary>
While many recent any-to-any voice conversion models succeed in transferring some target speech's style information to the converted speech, they still lack the ability to faithfully reproduce the speaking style of the target speaker. In this work, we propose a novel method to extract rich style information from target utterances and to efficiently transfer it to source speech content without requiring text transcriptions or speaker labeling. Our proposed approach introduces an attention mechanism utilizing a self-supervised learning (SSL) model to collect the speaking styles of a target speaker each corresponding to the different phonetic content. The styles are represented with a set of embeddings called stylebook. In the next step, the stylebook is attended with the source speech's phonetic content to determine the final target style for each source content. Finally, content information extracted from the source speech and content-dependent target style embeddings are fed into a diffusion-based decoder to generate the converted speech mel-spectrogram. Experiment results show that our proposed method combined with a diffusion-based generative model can achieve better speaker similarity in any-to-any voice conversion tasks when compared to baseline models, while the increase in computational complexity with longer utterances is suppressed.
</details>
<details>
<summary>摘要</summary>
许多最近的任意到任意语音转换模型成功地将目标语音的风格信息传递到转换后的语音中，但他们仍然缺乏将目标说话者的说话风格忠实地复制到转换后的语音中的能力。在这项工作中，我们提出了一种新的方法，可以从目标语音中提取丰富的风格信息，并将其效率地传递到源语音内容中，不需要文本转录或说话者标注。我们的提议的方法使用一种自动学习（SSL）模型来收集目标说话者每个不同的phonetic content的说话风格。这些风格被表示为一组叫做风格书的嵌入。在下一步，风格书与源语音的phonetic content进行了attend操作，以确定每个源内容的最终目标风格。最后，从源语音中提取的内容信息和 Content-dependent target style embeddings被 fed into一个扩散-based decoder，以生成转换后的语音mel-spectrogram。实验结果表明，我们的提议方法与扩散-based生成模型结合使用可以在任意到任意语音转换任务中实现更高的说话者相似性，而且与长语音的计算复杂度增长相对减少。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/06/cs.SD_2023_09_06/" data-id="clmjn91od00bv0j88cejp349u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/07/eess.IV_2023_09_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-07
        
      </div>
    </a>
  
  
    <a href="/2023/09/06/eess.AS_2023_09_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-09-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
