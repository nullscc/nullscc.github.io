
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-09-02 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01040 repo_url: None paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. d">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-09-02">
<meta property="og:url" content="https://nullscc.github.io/2023/09/02/cs.LG_2023_09_02/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.01040 repo_url: None paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. d">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-02T10:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:14.925Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/cs.LG_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T10:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-09-02
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling"><a href="#Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling" class="headerlink" title="Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling"></a>Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01040">http://arxiv.org/abs/2309.01040</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer</li>
<li>for: 本研究旨在提出一种经济高效的适应扩 beamforming 算法，基于高效的 covariance matrix 重建（CMR）和迭代空间功率 спектrum（ISPS）。</li>
<li>methods: 提出了一种使用 simplify 的最大 Entropy power spectral density 函数来重建 INC 矩阵，并使用 conjugate gradient 算法来更新扩 beamforming  веса。</li>
<li>results:  simulation 结果表明，提出的方法可以减少附近 SOI 方向的干扰，并且可以在不同的干扰水平下提供不同的扩 beamforming 策略。<details>
<summary>Abstract</summary>
This work presents a cost-effective technique for designing robust adaptive beamforming algorithms based on efficient covariance matrix reconstruction with iterative spatial power spectrum (CMR-ISPS). The proposed CMR-ISPS approach reconstructs the interference-plus-noise covariance (INC) matrix based on a simplified maximum entropy power spectral density function that can be used to shape the directional response of the beamformer. Firstly, we estimate the directions of arrival (DoAs) of the interfering sources with the available snapshots. We then develop an algorithm to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences which lie in a small angular sector. We also devise a cost-effective adaptive algorithm based on conjugate gradient techniques to update the beamforming weights and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum. The proposed CMR-ISPS beamformer can suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths. Simulation results are provided to confirm the validity of the proposed method and make a comparison to existing approaches
</details>
<details>
<summary>摘要</summary>
First, the directions of arrival (DoAs) of the interfering sources are estimated using the available snapshots. Then, an algorithm is developed to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences, which lie in a small angular sector.Furthermore, a cost-effective adaptive algorithm based on conjugate gradient techniques is proposed to update the beamforming weights, and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum is also presented. The proposed CMR-ISPS beamformer can effectively suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths.Simulation results are provided to validate the proposed method and compare it to existing approaches. The proposed technique is found to be effective in suppressing interference and improving the signal-to-noise ratio (SNR) of the desired signal.
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-Reinforcement-Learning-and-Planning-A-Survey"><a href="#Neurosymbolic-Reinforcement-Learning-and-Planning-A-Survey" class="headerlink" title="Neurosymbolic Reinforcement Learning and Planning: A Survey"></a>Neurosymbolic Reinforcement Learning and Planning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01038">http://arxiv.org/abs/2309.01038</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. Acharya, W. Raza, C. M. J. M. Dourado Jr, A. Velasquez, H. Song<br>for:This paper aims to contribute to the emerging field of Neurosymbolic Reinforcement Learning (Neurosymbolic RL) by conducting a literature survey.methods:The paper categorizes works based on the role played by the neural and symbolic parts in Reinforcement Learning (RL), into three taxonomies: Learning for Reasoning, Reasoning for Learning, and Learning-Reasoning.results:The paper analyzes the RL components of each research work, including the state space, action space, policy module, and RL algorithm. Additionally, the paper identifies research opportunities and challenges in various applications within the dynamic field of Neurosymbolic RL.Here is the same information in Simplified Chinese text:for:这篇论文目标是为新兴领域的神经符号人工智能（神经符号人工智能）的文献survey。methods:论文将工作分为三类：学习为理解、理解为学习和学习理解。results:论文分析每个研究工作的RL组件，包括状态空间、行动空间、策略模块和RL算法。此外，论文还提出了在不同应用领域中的研究机会和挑战。<details>
<summary>Abstract</summary>
The area of Neurosymbolic Artificial Intelligence (Neurosymbolic AI) is rapidly developing and has become a popular research topic, encompassing sub-fields such as Neurosymbolic Deep Learning (Neurosymbolic DL) and Neurosymbolic Reinforcement Learning (Neurosymbolic RL). Compared to traditional learning methods, Neurosymbolic AI offers significant advantages by simplifying complexity and providing transparency and explainability. Reinforcement Learning(RL), a long-standing Artificial Intelligence(AI) concept that mimics human behavior using rewards and punishment, is a fundamental component of Neurosymbolic RL, a recent integration of the two fields that has yielded promising results. The aim of this paper is to contribute to the emerging field of Neurosymbolic RL by conducting a literature survey. Our evaluation focuses on the three components that constitute Neurosymbolic RL: neural, symbolic, and RL. We categorize works based on the role played by the neural and symbolic parts in RL, into three taxonomies:Learning for Reasoning, Reasoning for Learning and Learning-Reasoning. These categories are further divided into sub-categories based on their applications. Furthermore, we analyze the RL components of each research work, including the state space, action space, policy module, and RL algorithm. Additionally, we identify research opportunities and challenges in various applications within this dynamic field.
</details>
<details>
<summary>摘要</summary>
neurosymbolic 人工智能（Neurosymbolic AI）领域在 rapid development 中，已经成为研究的热点话题，包括 neurosymbolic deep learning（Neurosymbolic DL）和 neurosymbolic reinforcement learning（Neurosymbolic RL）等子领域。与传统学习方法相比，Neurosymbolic AI 提供了很大的优势，包括简化复杂性和提供透明性和解释性。人工智能（AI）概念，模拟人类行为使用奖励和惩罚的 reinforcement learning（RL），是 neurosymbolic RL 的基础组件，是一种最近 integrate 了这两个领域的成果。本文的目标是为 neurosymbolic RL 领域的emerging 作出一篇文献survey。我们的评估将注重在 neurosymbolic RL 中的三个组成部分：神经、符号和RL。我们根据这些部分在 RL 中的角色，将工作分为三类：学习 для理解、理解 для学习和学习-理解。这些类别进一步分为应用的不同子类。此外，我们还分析了每个研究作品中的 RL 组件，包括状态空间、动作空间、策略模块和RL算法。此外，我们还标识了在不同应用场景中的研究机会和挑战。
</details></li>
</ul>
<hr>
<h2 id="Online-Adaptive-Mahalanobis-Distance-Estimation"><a href="#Online-Adaptive-Mahalanobis-Distance-Estimation" class="headerlink" title="Online Adaptive Mahalanobis Distance Estimation"></a>Online Adaptive Mahalanobis Distance Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01030">http://arxiv.org/abs/2309.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Aravind Reddy, Zhao Song</li>
<li>for: 本文是为了提出一种快速算法来计算马哈拉诺比斯距离的维度减少方法。</li>
<li>methods: 本文使用了随机 Monte Carlo 数据结构，然后通过修改这个数据结构，使其能够处理适应性查询和在数据点和马哈拉诺比斯距离矩阵上进行在线更新。</li>
<li>results: 本文提供了一种高效的数据结构来解决 Approximate Distance Estimation（ADE）问题，用于计算马哈拉诺比斯距离。这个数据结构可以处理适应性查询和在线更新，并且可以与先前的在线学习马哈拉诺比斯距离算法结合使用。<details>
<summary>Abstract</summary>
Mahalanobis metrics are widely used in machine learning in conjunction with methods like $k$-nearest neighbors, $k$-means clustering, and $k$-medians clustering. Despite their importance, there has not been any prior work on applying sketching techniques to speed up algorithms for Mahalanobis metrics. In this paper, we initiate the study of dimension reduction for Mahalanobis metrics. In particular, we provide efficient data structures for solving the Approximate Distance Estimation (ADE) problem for Mahalanobis distances. We first provide a randomized Monte Carlo data structure. Then, we show how we can adapt it to provide our main data structure which can handle sequences of \textit{adaptive} queries and also online updates to both the Mahalanobis metric matrix and the data points, making it amenable to be used in conjunction with prior algorithms for online learning of Mahalanobis metrics.
</details>
<details>
<summary>摘要</summary>
马哈拉诺比斯度量广泛用于机器学习，与方法如k-最近邻、k-Means clustering和k-medians clustering结合使用。尽管它们的重要性，但没有任何先前的工作把笔记技术应用到加速马哈拉诺比斯度量算法上。在这篇论文中，我们开始研究维度减少 для马哈拉诺比斯度量。特别是，我们提供高效的数据结构解决 Approximate Distance Estimation（ADE）问题的马哈拉诺比斯距离。我们首先提供随机 Monte Carlo 数据结构。然后，我们示出如何将其变换为我们的主要数据结构，可以处理 sequences of 适应 queries 和在 Mahalanobis 度量矩阵和数据点上进行在线更新，使其适用于与先前算法结合使用。
</details></li>
</ul>
<hr>
<h2 id="Explainability-for-Large-Language-Models-A-Survey"><a href="#Explainability-for-Large-Language-Models-A-Survey" class="headerlink" title="Explainability for Large Language Models: A Survey"></a>Explainability for Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01029">http://arxiv.org/abs/2309.01029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du</li>
<li>for: 本研究旨在描述和解释基于大语言模型（LLM）的自然语言处理能力，以便理解和提高模型的行为、局限性和社会影响。</li>
<li>methods: 本文提出了一个Explainability技术分类方法，并对基于传统 Fine-tuning 和提问两种训练 paradigm 的 Explainability 技术进行了结构化概述。每个 paradigm 都有自己的目标和主导方法，包括生成本地预测解释和全局模型知识解释。</li>
<li>results: 本研究提出了一些针对 LLM 的 Explainability 技术，包括生成本地预测解释和全局模型知识解释。此外，本研究还讨论了评估生成的解释的指标，以及如何使用解释来调试模型和提高性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）已经显示出了很强的能力，但它们的内部机制仍然不清楚，这会带来不需要的风险 для下游应用。因此，理解和解释这些模型是非常重要的，以便了解它们的行为、局限性和社会影响。在这篇论文中，我们提出了解释技术的分类和推荐方法，并对传统 fine-tuning-based 和 prompting-based 训练方法进行了分类。我们对每种方法进行了总结，并讲述了每种方法的目标和主要的地方解释方法，以及评估生成的解释的度量。最后，我们讨论了关键挑战和新出现的机遇，以及在 LLM 时代对解释技术的未来发展。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Recommendations-with-Pre-Trained-Large-Language-Models-for-Multimodal-Nudging"><a href="#Zero-Shot-Recommendations-with-Pre-Trained-Large-Language-Models-for-Multimodal-Nudging" class="headerlink" title="Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging"></a>Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01026">http://arxiv.org/abs/2309.01026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paxnea/wain23">https://github.com/paxnea/wain23</a></li>
<li>paper_authors: Rachel Harrison, Anton Dereventsov, Anton Bibin</li>
<li>for: 本研究旨在开发一种基于生成AI的零shot推荐Multimodal非站点内容的方法。</li>
<li>methods: 本方法使用了rendering输入不同Modalities为文本描述，并使用预训练LLMs计算它们的数字表示。然后，对所有内容项进行了统一的semantic embedding计算，以实现零shot推荐。</li>
<li>results: 在一个 sintetic Multimodal挫折环境中，我们示出了我们的方法可以准确地推荐Multimodal非站点内容，不需要额外学习。<details>
<summary>Abstract</summary>
We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种零shot推荐多Modal非站点内容的方法，利用最近的生成AI技术进步。我们提议将不同modal的输入描述为文本描述，并使用预训练的LLM来获取它们的数字表示。一旦所有内容项的统一表示获取完毕， THEN 推荐可以通过计算相应的相似度metric来进行，无需额外学习。我们在一个 sintetic multimodal 担当环境中进行了示例，输入包括表格、文本和视觉数据。
</details></li>
</ul>
<hr>
<h2 id="On-the-training-and-generalization-of-deep-operator-networks"><a href="#On-the-training-and-generalization-of-deep-operator-networks" class="headerlink" title="On the training and generalization of deep operator networks"></a>On the training and generalization of deep operator networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01020">http://arxiv.org/abs/2309.01020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sanghyun Lee, Yeonjong Shin</li>
<li>for: 这个论文是为了提出一种新的深度 опера作符网络（DeepONets）训练方法，以解决深度 neural network 模型中的运动难题。</li>
<li>methods: 这种训练方法包括先训练树网络，然后顺序训练分支网络。这种方法的核心思想是根据分解整个复杂训练任务的分子化方法，从而降低训练难度。在数学上，我们建立了一个通用的泛化误差估计，用于评估 DeepONets 的训练数据量、网络宽度和输入和输出传感器数量。</li>
<li>results: 数学示例和实验结果表明，这种两步训练方法可以有效地提高 DeepONets 的稳定性和泛化能力。在 Darcy 流动中，这种方法可以更好地捕捉流体的运动特征。<details>
<summary>Abstract</summary>
We present a novel training method for deep operator networks (DeepONets), one of the most popular neural network models for operators. DeepONets are constructed by two sub-networks, namely the branch and trunk networks. Typically, the two sub-networks are trained simultaneously, which amounts to solving a complex optimization problem in a high dimensional space. In addition, the nonconvex and nonlinear nature makes training very challenging. To tackle such a challenge, we propose a two-step training method that trains the trunk network first and then sequentially trains the branch network. The core mechanism is motivated by the divide-and-conquer paradigm and is the decomposition of the entire complex training task into two subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization process is introduced which significantly improves stability and generalization ability. On the theoretical side, we establish a generalization error estimate in terms of the number of training data, the width of DeepONets, and the number of input and output sensors. Numerical examples are presented to demonstrate the effectiveness of the two-step training method, including Darcy flow in heterogeneous porous media.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的训练方法 для深度运算网络（DeepONets），这是一种非常受欢迎的神经网络模型。 DeepONets 由两个子网络组成：分支网络和主网络。 通常情况下，这两个子网络同时进行训练，这相当于在高维空间中解决一个复杂的优化问题。 此外，由于非 conjugate 和非线性的特性，训练非常困难。 为了解决这个挑战，我们提议一种两步训练方法，先训练主网络，然后顺序训练分支网络。 核心机制是根据分治思想，将整个复杂的训练任务分解成两个子任务，每个子任务都有较低的复杂性。 在这个过程中，我们引入了 Gram-Schmidt 正交化过程，这有助于提高稳定性和泛化能力。 在理论上，我们建立了一个通用的泛化误差估计，该估计取决于训练数据的数量、深度网络的宽度、输入和输出传感器的数量。 数据示例表明了我们的两步训练方法的效iveness，包括达尔Cy流在多种不同的孔雀媒体中。
</details></li>
</ul>
<hr>
<h2 id="MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training"><a href="#MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training" class="headerlink" title="MPTopic: Improving topic modeling via Masked Permuted pre-training"></a>MPTopic: Improving topic modeling via Masked Permuted pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01015">http://arxiv.org/abs/2309.01015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinche Zhang, Evangelos milios</li>
<li>For: 本研究旨在提高顺序分词模型的性能，并提出一种新的词汇选择方法以提高分词结果的质量。* Methods: 本文使用的方法包括TF-RDF和MPTopic两种新的分词算法，TF-RDF可以评估文档中每个词语的相对重要性，而MPTopic则通过TF-RDF的启发来实现更加精准的分词结果。* Results: 对比BERTopic和Top2Vec两种传统的分词方法，MPTopic和TF-RDF的组合能够更好地提取主题关键词，并且在不同的文本 dataset 上均具有优秀的性能。<details>
<summary>Abstract</summary>
Topic modeling is pivotal in discerning hidden semantic structures within texts, thereby generating meaningful descriptive keywords. While innovative techniques like BERTopic and Top2Vec have recently emerged in the forefront, they manifest certain limitations. Our analysis indicates that these methods might not prioritize the refinement of their clustering mechanism, potentially compromising the quality of derived topic clusters. To illustrate, Top2Vec designates the centroids of clustering results to represent topics, whereas BERTopic harnesses C-TF-IDF for its topic extraction.In response to these challenges, we introduce "TF-RDF" (Term Frequency - Relative Document Frequency), a distinctive approach to assess the relevance of terms within a document. Building on the strengths of TF-RDF, we present MPTopic, a clustering algorithm intrinsically driven by the insights of TF-RDF. Through comprehensive evaluation, it is evident that the topic keywords identified with the synergy of MPTopic and TF-RDF outperform those extracted by both BERTopic and Top2Vec.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose "TF-RDF" (Term Frequency - Relative Document Frequency), a unique approach to assess the relevance of terms within a document. Building on the strengths of TF-RDF, we introduce MPTopic, a clustering algorithm driven by the insights of TF-RDF. Through comprehensive evaluation, it is evident that the topic keywords identified with the combination of MPTopic and TF-RDF outperform those extracted by both BERTopic and Top2Vec.
</details></li>
</ul>
<hr>
<h2 id="Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification"><a href="#Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification" class="headerlink" title="Streaming Active Learning for Regression Problems Using Regression via Classification"></a>Streaming Active Learning for Regression Problems Using Regression via Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01013">http://arxiv.org/abs/2309.01013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shota Horiguchi, Kota Dohi, Yohei Kawaguchi</li>
<li>for: 提高机器学习模型在不同环境下的性能。</li>
<li>methods: 使用流动式活动学习，通过在训练集中添加新的标注样本来重新训练模型，以保持模型的性能。</li>
<li>results: 实验结果表明，提议的方法可以在同等标注成本下实现更高的准确率。<details>
<summary>Abstract</summary>
One of the challenges in deploying a machine learning model is that the model's performance degrades as the operating environment changes. To maintain the performance, streaming active learning is used, in which the model is retrained by adding a newly annotated sample to the training dataset if the prediction of the sample is not certain enough. Although many streaming active learning methods have been proposed for classification, few efforts have been made for regression problems, which are often handled in the industrial field. In this paper, we propose to use the regression-via-classification framework for streaming active learning for regression. Regression-via-classification transforms regression problems into classification problems so that streaming active learning methods proposed for classification problems can be applied directly to regression problems. Experimental validation on four real data sets shows that the proposed method can perform regression with higher accuracy at the same annotation cost.
</details>
<details>
<summary>摘要</summary>
一个机器学习模型部署的挑战是模型在运行环境变化时性能下降。为保持性能，流动式活动学习被使用，其中模型通过新添加到训练集中的批注样本进行重新训练，直到预测样本的确定性不够高。虽然许多流动式活动学习方法已经为分类问题提出，但对于 regression 问题，industrial field 中的几乎没有努力。本文提出使用 regression-via-classification 框架来实现流动式活动学习 для regression。regression-via-classification 将 regression 问题转化为 classification 问题，以便直接应用到流动式活动学习方法。实验 validate 在四个真实数据集上表明，提出的方法可以在同样的批注成本下实现更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Deep-Learning-Architectures-for-Breast-Cancer-Diagnosis-Using-the-BreaKHis-Dataset"><a href="#Comparative-Analysis-of-Deep-Learning-Architectures-for-Breast-Cancer-Diagnosis-Using-the-BreaKHis-Dataset" class="headerlink" title="Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset"></a>Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01007">http://arxiv.org/abs/2309.01007</a></li>
<li>repo_url: None</li>
<li>paper_authors: İrem Sayın, Muhammed Ali Soydaş, Yunus Emre Mert, Arda Yarkataş, Berk Ergun, Selma Sözen Yeh, Hüseyin Üvet</li>
<li>for: 评估深度学习模型在诊断乳腺癌方面的性能</li>
<li>methods: 使用和比较五种知名的深度学习模型对乳腺癌进行分类：VGG、ResNet、Xception、Inception和InceptionResNet</li>
<li>results: Xception模型在F1分数方面取得了最高分（0.9），并且准确率达到了89%，而Inception和InceptionResNet模型都达到了87%的准确率，但Inception模型的F1分数为87，而InceptionResNet模型的F1分数为86。这些结果表明深度学习方法在诊断乳腺癌中的重要性，并且有助于提供更好的诊断服务给患者。<details>
<summary>Abstract</summary>
Cancer is an extremely difficult and dangerous health problem because it manifests in so many different ways and affects so many different organs and tissues. The primary goal of this research was to evaluate deep learning models' ability to correctly identify breast cancer cases using the BreakHis dataset. The BreakHis dataset covers a wide range of breast cancer subtypes through its huge collection of histopathological pictures. In this study, we use and compare the performance of five well-known deep learning models for cancer classification: VGG, ResNet, Xception, Inception, and InceptionResNet. The results placed the Xception model at the top, with an F1 score of 0.9 and an accuracy of 89%. At the same time, the Inception and InceptionResNet models both hit accuracy of 87% . However, the F1 score for the Inception model was 87, while that for the InceptionResNet model was 86. These results demonstrate the importance of deep learning methods in making correct breast cancer diagnoses. This highlights the potential to provide improved diagnostic services to patients. The findings of this study not only improve current methods of cancer diagnosis, but also make significant contributions to the creation of new and improved cancer treatment strategies. In a nutshell, the results of this study represent a major advancement in the direction of achieving these vital healthcare goals.
</details>
<details>
<summary>摘要</summary>
乳癌是一个非常困难和危险的健康问题，因为它可以出现在多种不同的形式和影响多种器官和组织。本研究的主要目标是评估深度学习模型在乳癌诊断中的表现。使用了5种常见的深度学习模型进行比较：VGG、ResNet、Xception、Inception和InceptionResNet。结果显示，Xception模型的F1分数为0.9，准确率为89%，而Inception和InceptionResNet模型都达到了87%的准确率。然而，Inception模型的F1分数为87，而InceptionResNet模型的F1分数为86。这些结果表明深度学习方法在乳癌诊断中的重要性，这也标识了可以提供更好的诊断服务给患者。本研究的结果不仅改进了当前的癌症诊断方法，还为创造新的癌症治疗策略做出了重要贡献。总之，本研究的结果代表了医疗健康目标的重大进步。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding"><a href="#Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding" class="headerlink" title="Bayesian sparsity and class sparsity priors for dictionary learning and coding"></a>Bayesian sparsity and class sparsity priors for dictionary learning and coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00999">http://arxiv.org/abs/2309.00999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bocchinfuso, Daniela Calvetti, Erkki Somersalo</li>
<li>for:  solves challenging inverse problems with dictionary learning methods</li>
<li>methods:  uses sparse coding techniques and dictionary compression to reduce computational complexity</li>
<li>results:  demonstrates effectiveness in glitch detection in LIGO experiment and hyperspectral remote sensing<details>
<summary>Abstract</summary>
Dictionary learning methods continue to gain popularity for the solution of challenging inverse problems. In the dictionary learning approach, the computational forward model is replaced by a large dictionary of possible outcomes, and the problem is to identify the dictionary entries that best match the data, akin to traditional query matching in search engines. Sparse coding techniques are used to guarantee that the dictionary matching identifies only few of the dictionary entries, and dictionary compression methods are used to reduce the complexity of the matching problem. In this article, we propose a work flow to facilitate the dictionary matching process. First, the full dictionary is divided into subdictionaries that are separately compressed. The error introduced by the dictionary compression is handled in the Bayesian framework as a modeling error. Furthermore, we propose a new Bayesian data-driven group sparsity coding method to help identify subdictionaries that are not relevant for the dictionary matching. After discarding irrelevant subdictionaries, the dictionary matching is addressed as a deflated problem using sparse coding. The compression and deflation steps can lead to substantial decreases of the computational complexity. The effectiveness of compensating for the dictionary compression error and using the novel group sparsity promotion to deflate the original dictionary are illustrated by applying the methodology to real world problems, the glitch detection in the LIGO experiment and hyperspectral remote sensing.
</details>
<details>
<summary>摘要</summary>
《字典学习方法继续受到解决复杂反问题的感兴趣。在字典学习方法中，计算前方模型被替换为大量可能的结果字典，问题是将字典条目与数据匹配，类似于传统的查询匹配在搜索引擎中。简码技术用于保证字典匹配仅 identific few 字典条目，而字典压缩方法用于减少匹配问题的复杂性。在这篇文章中，我们提出了一个工作流程来促进字典匹配过程。首先，全字典被分解成分字典，并每个分字典都被单独压缩。ictionary compression error是在 bayesian 框架中处理为模型误差。此外，我们提出了一种新的 bayesian 数据驱动集成隐藏码法，以帮助标识不重要的分字典。在排除不重要分字典后，字典匹配被视为一个压缩问题，并使用简码解决。压缩和压缩步骤可以导致计算复杂性的明显减少。我们通过应用方法到实际问题，如 LIGO 实验中的异常检测和遥感谱辐成像，来证明资料做出补偿和使用新的集成隐藏码法减少原始字典的效果。
</details></li>
</ul>
<hr>
<h2 id="Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems"><a href="#Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems" class="headerlink" title="Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems"></a>Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00997">http://arxiv.org/abs/2309.00997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chhavisharma123/c-dpssg-cdc2023">https://github.com/chhavisharma123/c-dpssg-cdc2023</a></li>
<li>paper_authors: Chhavi Sharma, Vishnu Narayanan, P. Balamurugan</li>
<li>for: 解决非流式强CONCAVE-强CONVEX笆点问题的协同执行方法。</li>
<li>methods: 使用不准确的顺序Primaldual Hybrid Gradient（inexact PDHG）算法，允许普通的梯度计算观察者更新顺序和权值。</li>
<li>results: 提出一种名为协同抽象换算法（C-DPSSG），可以在初始阶段使用抽象梯度计算观察者（GSG），然后在合适的时间点 switched to SVRG观察者，以便更快地到达笆点解。该算法可以在不同的精度和时间约束下，以便获得低&#x2F;中精度的解决方案。<details>
<summary>Abstract</summary>
We consider a class of non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server. To solve a consensus formulation of problems in this class, we develop an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables. We first investigate the performance of inexact PDHG with stochastic variance reduction gradient (SVRG) oracle. Our numerical study uncovers a significant phenomenon of initial conservative progress of iterates of IPDHG with SVRG oracle. To tackle this, we develop a simple and effective switching idea, where a generalized stochastic gradient (GSG) computation oracle is employed to hasten the iterates' progress to a saddle point solution during the initial phase of updates, followed by a switch to the SVRG oracle at an appropriate juncture. The proposed algorithm is named Decentralized Proximal Switching Stochastic Gradient method with Compression (C-DPSSG), and is proven to converge to an $\epsilon$-accurate saddle point solution with linear rate. Apart from delivering highly accurate solutions, our study reveals that utilizing the best convergence phases of GSG and SVRG oracles makes C-DPSSG well suited for obtaining solutions of low/medium accuracy faster, useful for certain applications. Numerical experiments on two benchmark machine learning applications show C-DPSSG's competitive performance which validate our theoretical findings. The codes used in the experiments can be found \href{https://github.com/chhavisharma123/C-DPSSG-CDC2023}{here}.
</details>
<details>
<summary>摘要</summary>
我们考虑一类非滑瑞强烈强紧组点问题在分布式设置中，无中央服务器。为解决这类问题的协议形式，我们开发了一个不精确的原理双层对偶算法（inexact PDHG），让普通的梯度计算观察者更新对称和对偶变数。我们首先研究了不精确PDHG与减少偏态梯度（SVRG）观察者的性能。我们的数据分析发现在追踪更新的初期阶段，inexact PDHG的追踪进展受到了初始保守的限制。为解决这问题，我们提出了一个简单而有效的转换想法，在初期更新阶段使用通用梯度计算观察者（GSG），以加速追踪进度，然后在适当的时候转换到SVRG观察者。我们给这个算法命名为分布式预掌终端算法（C-DPSSG），并证明其可以在线性速率下落幅到ε-精确的终端解。我们的数据分析还显示，通过利用GSG和SVRG观察者的最佳径 converge phase，C-DPSSG可以实现低/中精度的解 faster，这有助于某些应用。我们的实验结果显示C-DPSSG在两个机器学习应用中的竞争性表现，与我们的理论成果相符。实验代码可以在以下网址找到：https://github.com/chhavisharma123/C-DPSSG-CDC2023
</details></li>
</ul>
<hr>
<h2 id="A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters"><a href="#A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters" class="headerlink" title="A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters"></a>A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00993">http://arxiv.org/abs/2309.00993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debsundar Dey, Suchandan Das, Anik Pal, Santanu Dey, Chandan Kumar Raul, Arghya Chatterjee</li>
<li>for: This study aims to predict the phases and crystal structures of High-Entropy Alloys (HEAs) using machine learning (ML) techniques.</li>
<li>methods: The study employs five distinct boosting algorithms (XGBoost, LightGBM, Random Forest, Gradient Boosting, and CatBoost) to predict phases and crystal structures, and introduces a methodical framework using the Pearson correlation coefficient to select strongly co-related features for improved accuracy.</li>
<li>results: The study achieves an accuracy of 94.05% for phase prediction and 90.07% for crystal structure prediction using XGBoost and LightGBM algorithms, respectively. Additionally, the study quantifies the influence of parameters on the model’s accuracy and introduces a new approach to elucidate the contribution of individual parameters in the phase prediction and crystal structure prediction processes.<details>
<summary>Abstract</summary>
The reason behind the remarkable properties of High-Entropy Alloys (HEAs) is rooted in the diverse phases and the crystal structures they contain. In the realm of material informatics, employing machine learning (ML) techniques to classify phases and crystal structures of HEAs has gained considerable significance. In this study, we assembled a new collection of 1345 HEAs with varying compositions to predict phases. Within this collection, there were 705 sets of data that were utilized to predict the crystal structures with the help of thermodynamics and electronic configuration. Our study introduces a methodical framework i.e., the Pearson correlation coefficient that helps in selecting the strongly co-related features to increase the prediction accuracy. This study employed five distinct boosting algorithms to predict phases and crystal structures, offering an enhanced guideline for improving the accuracy of these predictions. Among all these algorithms, XGBoost gives the highest accuracy of prediction (94.05%) for phases and LightGBM gives the highest accuracy of prediction of crystal structure of the phases (90.07%). The quantification of the influence exerted by parameters on the model's accuracy was conducted and a new approach was made to elucidate the contribution of individual parameters in the process of phase prediction and crystal structure prediction.
</details>
<details>
<summary>摘要</summary>
高级异杂合金（HEA）的remarkable性能的原因在于它们包含多种阶段和晶体结构。在材料信息学领域，使用机器学习（ML）技术来分类HEA的阶段和晶体结构的预测已经得到了许多关注。本研究集成了1345个HEA的不同组合，以预测它们的阶段。这1345个数据集中有705个数据用于预测晶体结构，使用热力学和电子配置来帮助预测。本研究提出了一种系统化框架，即Pearson相关系数，用于选择与预测相关的特征，以提高预测精度。本研究使用五种不同的扩展 boosting 算法来预测阶段和晶体结构，提供了改进预测精度的指南。其中，XGBoost 提供了预测阶段的最高准确率（94.05%），而 LightGBM 提供了预测晶体结构的阶段的最高准确率（90.07%）。研究还评估了模型准确率中参数的影响，并开发了一种新的方法来描述阶段预测和晶体结构预测中参数的贡献。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Dexterity-Chaining-Dexterous-Policies-for-Long-Horizon-Manipulation"><a href="#Sequential-Dexterity-Chaining-Dexterous-Policies-for-Long-Horizon-Manipulation" class="headerlink" title="Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation"></a>Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00987">http://arxiv.org/abs/2309.00987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanpei Chen, Chen Wang, Li Fei-Fei, C. Karen Liu</li>
<li>for: 这篇论文主要关注的是如何实现多个复杂的手部任务，并且可以自动调整和转换不同的功能模式。</li>
<li>methods: 这篇论文使用了动态学习（RL）来建立多个手部策略，并且使用了一个称为“transition feasibility function”的函数来积极地调整并组合多个策略，以提高连接成功率。</li>
<li>results: 这篇论文的结果显示，这个系统能够实现多个长期任务，并且能够自动调整和转换不同的功能模式，而且能够在真实世界中运行。更多细节和视频结果可以在<a target="_blank" rel="noopener" href="https://sequential-dexterity.github.io找到./">https://sequential-dexterity.github.io找到。</a><details>
<summary>Abstract</summary>
Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, our system demonstrates generalization capability to novel object shapes and is able to zero-shot transfer to a real-world robot equipped with a dexterous hand. More details and video results could be found at https://sequential-dexterity.github.io
</details>
<details>
<summary>摘要</summary>
多种实际操作任务通常包括一系列不同的子任务，这些长期任务展示了人工手的可靠性和多样性，可以无需重新抓取或使用外部工具而快速地转换到不同的功能模式。然而，由于高维动作空间和复杂的作业动力学，这些任务也存在挑战。我们提出了一种基于再征学习（RL）的执行系统，称为Sequential Dexterity，它将多个灵活政策串联起来实现长期任务目标。系统的核心是一个过程可行性函数，通过不断精细调整子政策来提高串联成功率，同时也允许自主policy-switching以恢复失败和 circumvent redundant stages。尽管只在 simulations 中训练了几个任务物体，我们的系统仍能通过 Zero-shot transfer 将其应用到真实世界中的一个配备了灵活手的机器人中。更多细节和视频结果可以在 <https://sequential-dexterity.github.io> 找到。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems"><a href="#An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems" class="headerlink" title="An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems"></a>An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00983">http://arxiv.org/abs/2309.00983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Bao, Zezhong Zhang, Guannan Zhang</li>
<li>for: 解决高维非线性筛选问题，提高精度。</li>
<li>methods: 使用分子筛和集成 Kalman 筛，以及一种新的训练自由的得分函数估计方法。</li>
<li>results: 在高维 Lorenz 系统上实现了可靠地跟踪 extremely high-dimensional Lorenz 系统（达到 1,000,000 维），具有高非线性观测过程，而 existing 筛选方法很难以实现。<details>
<summary>Abstract</summary>
We propose an ensemble score filter (EnSF) for solving high-dimensional nonlinear filtering problems with superior accuracy. A major drawback of existing filtering methods, e.g., particle filters or ensemble Kalman filters, is the low accuracy in handling high-dimensional and highly nonlinear problems. EnSF attacks this challenge by exploiting the score-based diffusion model, defined in a pseudo-temporal domain, to characterizing the evolution of the filtering density. EnSF stores the information of the recursively updated filtering density function in the score function, in stead of storing the information in a set of finite Monte Carlo samples (used in particle filters and ensemble Kalman filters). Unlike existing diffusion models that train neural networks to approximate the score function, we develop a training-free score estimation that uses mini-batch-based Monte Carlo estimator to directly approximate the score function at any pseudo-spatial-temporal location, which provides sufficient accuracy in solving high-dimensional nonlinear problems as well as saves tremendous amount of time spent on training neural networks. Another essential aspect of EnSF is its analytical update step, gradually incorporating data information into the score function, which is crucial in mitigating the degeneracy issue faced when dealing with very high-dimensional nonlinear filtering problems. High-dimensional Lorenz systems are used to demonstrate the performance of our method. EnSF provides surprisingly impressive performance in reliably tracking extremely high-dimensional Lorenz systems (up to 1,000,000 dimension) with highly nonlinear observation processes, which is a well-known challenging problem for existing filtering methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种ensemble score filter（EnSF），用于解决高维非线性筛选问题，并提供了更高的准确性。现有的筛选方法，如 particulate filters 或 ensemble Kalman filters，在处理高维和非线性问题时存在低准确性的问题。EnSF 利用分数基 diffusion 模型，在 pseudo-时空域中定义了筛选演化的分数函数，以存储 recursively 更新的筛选演化函数的信息。与现有的扩散模型不同，我们开发了一种无需训练的分数估计，使用 mini-batch-based Monte Carlo 估计器直接估计分数函数的值，提供了 suficient 的准确性来解决高维非线性问题，并减少了对 neural network 的训练时间。另一个关键特点 OF EnSF 是其分析更新步骤，逐渐地包含数据信息到分数函数中，这是解决非线性问题时面临的减少问题的关键。高维 Lorenz 系统被用来证明我们的方法的性能。EnSF 在可以可靠地跟踪 extremely high-dimensional Lorenz 系统（达到 1,000,000 维度）的高非线性观测过程中表现出了很好的性能，这是现有筛选方法所不能做到的。
</details></li>
</ul>
<hr>
<h2 id="Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction"><a href="#Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction" class="headerlink" title="Pure Message Passing Can Estimate Common Neighbor for Link Prediction"></a>Pure Message Passing Can Estimate Common Neighbor for Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00976">http://arxiv.org/abs/2309.00976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Dong, Zhichun Guo, Nitesh V. Chawla</li>
<li>for: 本研究旨在解释Message Passing Neural Networks（MPNNs）在图表示学中的缺陷，以及如何通过消息传递来捕捉 JOIN 结构特征，从而提高链接预测性能。</li>
<li>methods: 本研究使用了MPNNs和其他基eline方法进行比较，并通过研究消息传递的特点来提高链接预测性能。</li>
<li>results: 研究结果表明，MPNNs在链接预测中常常下掌了简单的规则，但是通过消息传递来捕捉 JOIN 结构特征可以提高链接预测性能。基于这些结果，我们提出了一种新的链接预测模型，即Message Passing Link Predictor（MPLP），它可以更好地捕捉 JOIN 结构特征，从而提高链接预测性能。<details>
<summary>Abstract</summary>
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture structural features could offset MPNNs' expressiveness limitations at the expense of estimation variance. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods.
</details>
<details>
<summary>摘要</summary>
message passing neural networks (MPNNs) 已经成为图像学习中的标准方法，但在链接预测方面经常遇到困难，被简单的规则如共同邻居 (CN) 超越。这种差异源于 MPNNs 的基本局限性：它们在节点级别表示方面出色，但在编码共同结构特征方面受限。为了bridging这个差距，我们认为，通过利用输入向量的正交性，纯粹的消息传递实际可以捕捉共同结构特征。我们研究 MPNNs 是否可以正确地预测链接。根据我们的发现，我们提出了一种新的链接预测模型——消息传递链接预测器 (MPLP)。MPLP 利用 quasi-正交的输入向量来估算链接级别的结构特征，同时保持节点级别的复杂性。此外，我们的方法示出，通过利用消息传递来捕捉结构特征可以将 MPNNs 的表达能力限制作为估计误差的代价。我们在不同领域的基本数据集上进行了实验，并 consistently 超过了基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Framework-for-Optimal-Selection-of-Soil-Sampling-Sites"><a href="#Deep-Learning-Framework-for-Optimal-Selection-of-Soil-Sampling-Sites" class="headerlink" title="Deep-Learning Framework for Optimal Selection of Soil Sampling Sites"></a>Deep-Learning Framework for Optimal Selection of Soil Sampling Sites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00974">http://arxiv.org/abs/2309.00974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tan-Hanh Pham, Praneel Acharya, Sravanthi Bachina, Kristopher Osterloh, Kim-Doang Nguyen</li>
<li>for: 这项研究旨在利用深度学习技术自动选择耕地中的优质采样点，以提高耕地管理和农业生产的效率和质量。</li>
<li>methods: 该研究使用了两种方法：一是使用现有的state-of-the-art模型，即卷积神经网络（CNN）；二是开发一种基于transformer和自注意的深度学习设计。研究框架采用了encoder-decoder结构，自注意机制作为特征提取器，生成特征地图。</li>
<li>results: 实验结果表明，该模型在测试集上达到了99.52%的平均准确率，57.35%的平均交集率，和71.47%的平均 dice coefficient，而state-of-the-art CNN-based模型的性能指标分别为66.08%, 3.85%, 1.98%。这表明，我们提出的模型在耕地采样数据集上的表现优于CNN-based方法。<details>
<summary>Abstract</summary>
This work leverages the recent advancements of deep learning in image processing to find optimal locations that present the important characteristics of a field. The data for training are collected at different fields in local farms with five features: aspect, flow accumulation, slope, NDVI (normalized difference vegetation index), and yield. The soil sampling dataset is challenging because the ground truth is highly imbalanced binary images. Therefore, we approached the problem with two methods, the first approach involves utilizing a state-of-the-art model with the convolutional neural network (CNN) backbone, while the second is to innovate a deep-learning design grounded in the concepts of transformer and self-attention. Our framework is constructed with an encoder-decoder architecture with the self-attention mechanism as the backbone. In the encoder, the self-attention mechanism is the key feature extractor, which produces feature maps. In the decoder, we introduce atrous convolution networks to concatenate, fuse the extracted features, and then export the optimal locations for soil sampling. Currently, the model has achieved impressive results on the testing dataset, with a mean accuracy of 99.52%, a mean Intersection over Union (IoU) of 57.35%, and a mean Dice Coefficient of 71.47%, while the performance metrics of the state-of-the-art CNN-based model are 66.08%, 3.85%, and 1.98%, respectively. This indicates that our proposed model outperforms the CNN-based method on the soil-sampling dataset. To the best of our knowledge, our work is the first to provide a soil-sampling dataset with multiple attributes and leverage deep learning techniques to enable the automatic selection of soil-sampling sites. This work lays a foundation for novel applications of data science and machine-learning technologies to solve other emerging agricultural problems.
</details>
<details>
<summary>摘要</summary>
这个工作利用深度学习的最新进展在图像处理中找到优化的场地特征。训练数据来自当地农场不同场地的五个特征：方向、流量总和、 Slope、 NDVI（ норма化植物质量指数）和收获。 soil sampling 数据集是一个挑战，因为真实值是高度不平衡的二值图像。我们采用了两种方法：一是使用现有的 state-of-the-art 模型，二是创新基于 transformer 和自注意的深度学习设计。我们的框架采用了Encoder-Decoder 架构，自注意机制作为关键特征提取器，生成特征图。在 Decoder 中，我们引入了尺度扩展 convolutional networks，将提取的特征 concatenate 并融合，然后输出优化的场地样本选择。现在，模型在测试数据集上已经实现了很好的结果，具有 Mean Accuracy 为 99.52%， Mean Intersection over Union 为 57.35%， Mean Dice Coefficient 为 71.47%，而 state-of-the-art CNN 模型的性能指标分别为 66.08%、3.85% 和 1.98%。这表明我们的提案模型在 soil-sampling 数据集上表现出色，超过了 CNN 模型。根据我们所知，这是首次提供多属性的 soil-sampling 数据集，并利用深度学习技术自动选择场地样本。这项工作为数据科学和机器学习技术在农业问题上的应用开创了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Diffusion-Based-Continuous-Constraint-Solvers"><a href="#Compositional-Diffusion-Based-Continuous-Constraint-Solvers" class="headerlink" title="Compositional Diffusion-Based Continuous Constraint Solvers"></a>Compositional Diffusion-Based Continuous Constraint Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00966">http://arxiv.org/abs/2309.00966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diffusion-ccsp/diffusion-ccsp.github.io">https://github.com/diffusion-ccsp/diffusion-ccsp.github.io</a></li>
<li>paper_authors: Zhutian Yang, Jiayuan Mao, Yilun Du, Jiajun Wu, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling</li>
<li>for: 这篇论文targets continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning, with the goal of developing a generalizable approach for solving these problems.</li>
<li>methods: 该方法基于diffusion模型，将CCSP表示为factor graphs，并将各种约束类型的能量组合起来，以derive global解决方案。</li>
<li>results: 实验表明，该方法可以强大地泛化到novel combinations of known约束，并且可以与任务和运动规划结合，以便开发包含离散和连续参数的长期计划。<details>
<summary>Abstract</summary>
This paper introduces an approach for learning to solve continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning. Previous methods primarily rely on hand-engineering or learning generators for specific constraint types and then rejecting the value assignments when other constraints are violated. By contrast, our model, the compositional diffusion continuous constraint solver (Diffusion-CCSP) derives global solutions to CCSPs by representing them as factor graphs and combining the energies of diffusion models trained to sample for individual constraint types. Diffusion-CCSP exhibits strong generalization to novel combinations of known constraints, and it can be integrated into a task and motion planner to devise long-horizon plans that include actions with both discrete and continuous parameters. Project site: https://diffusion-ccsp.github.io/
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种用于解决连续约束满意问题（CCSP）的机器学习方法。先前的方法主要依靠手工设计或学习生成器来满足特定约束类型，然后在其他约束被违反时拒绝值分配。而我们的模型——复合扩散连续约束解决器（Diffusion-CCSP）则通过表示为因子图并将各种约束类型的扩散模型训练来 derivation global solution to CCSPs。Diffusion-CCSP具有强大的泛化能力，可以适应新的知道的约束组合。此外，它可以与任务和运动规划集成，以生成包含杂参数的长期计划。项目网站：<https://diffusion-ccsp.github.io/>
</details></li>
</ul>
<hr>
<h2 id="eDKM-An-Efficient-and-Accurate-Train-time-Weight-Clustering-for-Large-Language-Models"><a href="#eDKM-An-Efficient-and-Accurate-Train-time-Weight-Clustering-for-Large-Language-Models" class="headerlink" title="eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models"></a>eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00964">http://arxiv.org/abs/2309.00964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsik Cho, Keivan A. Vahid, Qichen Fu, Saurabh Adya, Carlo C Del Mundo, Mohammad Rastegari, Devang Naik, Peter Zatloukal</li>
<li>for: 这个研究旨在将大型语言模型（LLM）带到移动设备上，以提高响应速度和隐私保护。</li>
<li>methods: 这个研究使用了 weight-clustering 技术来将 LLM 压缩，并且提出了一个内存效率的 differentiable KMeans Clustering（DKM）实现方案，以减少对 LLM 的训练时间复杂度。</li>
<li>results: 这个研究可以将预训 LLaMA 7B 模型压缩为 2.5 GB (3bit&#x2F;weight)，并且在实验中获得了良好的准确性表现（例如，PIQA 77.7%、Winograde 66.1% 等），同时实现了训练时间复杂度的大幅减少（130倍）。<details>
<summary>Abstract</summary>
Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM by orders of magnitudes. For a given tensor to be saved on CPU for the backward pass of DKM, we compressed the tensor by applying uniquification and sharding after checking if there is no duplicated tensor previously copied to CPU. Our experimental results demonstrate that \prjname can fine-tune and compress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with the Alpaca dataset by reducing the train-time memory footprint of a decoder layer by 130$\times$, while delivering good accuracy on broader LLM benchmarks (i.e., 77.7\% for PIQA, 66.1\% for Winograde, and so on).
</details>
<details>
<summary>摘要</summary>
因为大语言模型（LLM）在许多复杂语言任务中表现出色，因此有很大的兴趣将这些LLM搬到移动设备上进行更快的响应和更好的隐私保护。然而，LLM的大小（即十亿个参数）需要非常高效的压缩才能适应存储有限的设备。许多压缩技术中，量化是一种非线性的压缩技术，是LLM压缩的一个领先候选者，并且现代智能手机支持。然而，它的训练负担是LLM精细调整的瓶颈。特别是用于LLM精细调整的散列聚合（DKM）在训练过程中的内存负担很大，使其几乎不可能应用于LLM训练期间的压缩。在这篇论文中，我们提出了一种内存高效的DKM实现，基于新的技术来降低DKM的内存占用量。对于要保存在CPU上的一个tensor来进行DKM的反向传播，我们将tensor进行压缩，首先检查是否已经有重复的tensor被复制到CPU上，如果有，则应用uniquification和分割。我们的实验结果表明，我们可以使用eDKM将预训练的LLaMA 7B模型从12.6GB压缩到2.5GB（3比特/参数），并在Alpaca数据集上达到77.7%的准确率，在更广泛的LLMbenchmark上达到66.1%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints"><a href="#Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints" class="headerlink" title="Network Topology Inference with Sparsity and Laplacian Constraints"></a>Network Topology Inference with Sparsity and Laplacian Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00960">http://arxiv.org/abs/2309.00960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Ying, Xi Han, Rui Zhou, Xiwen Wang, Hing Cheung So</li>
<li>for: 这个论文是关于网络拓扑推断问题的研究，使用卷积环境模型来估计精度矩阵，从而解决网络拓扑推断问题。</li>
<li>methods: 这个论文使用了卷积环境模型，并在这个模型中添加了$\ell_0$-norm约束，以解决$\ell_1$-norm的局限性。具体来说，这个方法使用了梯度投影算法来解决由稀疏性和卷积环境约束组成的优化问题。</li>
<li>results: 通过使用实验数据集，包括 sintetic 数据集和金融时间序列数据集，研究人员发现该方法可以有效地解决网络拓扑推断问题。<details>
<summary>Abstract</summary>
We tackle the network topology inference problem by utilizing Laplacian constrained Gaussian graphical models, which recast the task as estimating a precision matrix in the form of a graph Laplacian. Recent research \cite{ying2020nonconvex} has uncovered the limitations of the widely used $\ell_1$-norm in learning sparse graphs under this model: empirically, the number of nonzero entries in the solution grows with the regularization parameter of the $\ell_1$-norm; theoretically, a large regularization parameter leads to a fully connected (densest) graph. To overcome these challenges, we propose a graph Laplacian estimation method incorporating the $\ell_0$-norm constraint. An efficient gradient projection algorithm is developed to solve the resulting optimization problem, characterized by sparsity and Laplacian constraints. Through numerical experiments with synthetic and financial time-series datasets, we demonstrate the effectiveness of the proposed method in network topology inference.
</details>
<details>
<summary>摘要</summary>
我们面临网络拓扑推理问题，利用卷积约束 Gaussian 图模型，将任务转化为估计一个精度矩阵，形式上等于图laplacian。Recent research 发现了 $\ell_1$-norm 学习稀疏图的局限性：实际上，规定参数的REGULARIZATION 参数增加后，解的非零个数增加;理论上，大的REGULARIZATION 参数导致最密集（最稀疏）图。为了解决这些挑战，我们提议一种包含 $\ell_0$-norm 约束的图laplacian 估计方法。我们开发了一种高效的梯度投影算法，解决这个具有稀疏和卷积约束的优化问题。通过数据分析实验，我们在synthetic 和金融时间序列 dataset 上证明了我们提议的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Index-aware-learning-of-circuits"><a href="#Index-aware-learning-of-circuits" class="headerlink" title="Index-aware learning of circuits"></a>Index-aware learning of circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00958">http://arxiv.org/abs/2309.00958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idoia Cortes Garcia, Peter Förster, Lennart Jansen, Wil Schilders, Sebastian Schöps</li>
<li>for: 该论文旨在提出一种基于机器学习的电路设计方法，以优化电路设计过程中的参数优化。</li>
<li>methods: 该方法使用了修改后的节点分析（Modified Nodal Analysis）来描述电路，并使用分解思想（Dissection Concept）将系统分解成仅依赖于导数变量的偏 differential equations 和纯 algebra equations。</li>
<li>results: 该方法可以保证解的 algebraic constraints 准确地满足，从而提高电路设计的精度和可靠性。<details>
<summary>Abstract</summary>
Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea then is to only learn the differential variables and reconstruct the algebraic ones using the relations from the decoupling. This approach guarantees that the algebraic constraints are fulfilled up to the accuracy of the nonlinear system solver, which represents the main benefit highlighted in this article.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visual-Kinematics-Graph-Learning-for-Procedure-agnostic-Instrument-Tip-Segmentation-in-Robotic-Surgeries"><a href="#Visual-Kinematics-Graph-Learning-for-Procedure-agnostic-Instrument-Tip-Segmentation-in-Robotic-Surgeries" class="headerlink" title="Visual-Kinematics Graph Learning for Procedure-agnostic Instrument Tip Segmentation in Robotic Surgeries"></a>Visual-Kinematics Graph Learning for Procedure-agnostic Instrument Tip Segmentation in Robotic Surgeries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00957">http://arxiv.org/abs/2309.00957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Liu, Yonghao Long, Kai Chen, Cheuk Hei Leung, Zerui Wang, Qi Dou</li>
<li>for: 用于Robotic surgery中的器官细部检测，如技能评估、工具-组织相互作用和形变建模，以及手术自动化。</li>
<li>methods: 利用机器人系统的遥感数据来提供可靠的先知，并通过图学学习框架对图像和遥感数据进行融合，以提高器官细部检测的精度。</li>
<li>results: 在一个私有的对比数据集上，包括多种手术类型，如肾脏切除术、全肠肉切除术、胃食道切除术和肝脏切除术，实现了与当前图像基的状态作呈现的超过11.2%的提升。<details>
<summary>Abstract</summary>
Accurate segmentation of surgical instrument tip is an important task for enabling downstream applications in robotic surgery, such as surgical skill assessment, tool-tissue interaction and deformation modeling, as well as surgical autonomy. However, this task is very challenging due to the small sizes of surgical instrument tips, and significant variance of surgical scenes across different procedures. Although much effort has been made on visual-based methods, existing segmentation models still suffer from low robustness thus not usable in practice. Fortunately, kinematics data from the robotic system can provide reliable prior for instrument location, which is consistent regardless of different surgery types. To make use of such multi-modal information, we propose a novel visual-kinematics graph learning framework to accurately segment the instrument tip given various surgical procedures. Specifically, a graph learning framework is proposed to encode relational features of instrument parts from both image and kinematics. Next, a cross-modal contrastive loss is designed to incorporate robust geometric prior from kinematics to image for tip segmentation. We have conducted experiments on a private paired visual-kinematics dataset including multiple procedures, i.e., prostatectomy, total mesorectal excision, fundoplication and distal gastrectomy on cadaver, and distal gastrectomy on porcine. The leave-one-procedure-out cross validation demonstrated that our proposed multi-modal segmentation method significantly outperformed current image-based state-of-the-art approaches, exceeding averagely 11.2% on Dice.
</details>
<details>
<summary>摘要</summary>
importante任务是将手术工具的 tip 分割，以实现后续应用程序在机器人手术中，如手术技巧评估、工具-组织交互和变形建模，以及手术自主。然而，这个任务非常具有挑战性，因为手术工具的 tip 很小，而手术场景也存在很大的差异。虽然已经有很多Visual基于的方法，但现有的分割模型仍然具有低稳定性，因此不可靠。幸运的是，机器人系统的动态数据可以提供可靠的工具位置的先导，这些先导是不同手术类型下的一致的。为了利用这些多modal信息，我们提议一种新的视觉-遥感图学学习框架，准确地分割手术工具的 tip。具体来说，我们提出了一个图学学习框架，用于编码工具部件的关系特征从视觉和遥感两个Modal。然后，我们设计了一种交叉模态对比损失函数，以汇集robust的几何先导从遥感到图像进行分割。我们在私有的Visual-遥感对应 dataset 上进行了实验，包括多种手术类型，如肾脏摘除、肠Rectal摘除、胃部切除和肠部切除。我们使用了离散一个手术程序的交叉验证，并表明我们的多模态分割方法在Dice指标上超过了当前图像基于状态艺术的方法的平均值11.2%。
</details></li>
</ul>
<hr>
<h2 id="From-Specific-to-Generic-Learned-Sorted-Set-Dictionaries-A-Theoretically-Sound-Paradigm-Yelding-Competitive-Data-Structural-Boosters-in-Practice"><a href="#From-Specific-to-Generic-Learned-Sorted-Set-Dictionaries-A-Theoretically-Sound-Paradigm-Yelding-Competitive-Data-Structural-Boosters-in-Practice" class="headerlink" title="From Specific to Generic Learned Sorted Set Dictionaries: A Theoretically Sound Paradigm Yelding Competitive Data Structural Boosters in Practice"></a>From Specific to Generic Learned Sorted Set Dictionaries: A Theoretically Sound Paradigm Yelding Competitive Data Structural Boosters in Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00946">http://arxiv.org/abs/2309.00946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/globosco/An-implementation-of-Generic-Learned-Static-Sorted-Sets-Dictionaries">https://github.com/globosco/An-implementation-of-Generic-Learned-Static-Sorted-Sets-Dictionaries</a></li>
<li>paper_authors: Domenico Amato, Giosué Lo Bosco, Raffaele Giancarlo</li>
<li>for: 本研究探讨了机器学习和 классические数据结构之间的交叉领域，即学习数据结构。它具有方法学意义和实践效应。本文专注于学习排序数据结构，即学习排序集数据结构。现有提案主要是对已有排序数据结构进行加速，例如二分搜索。本文提出了一种新的思路，可以生成任何排序集数据结构的学习版本，例如平衡二分搜索树或其他排序layout的搜索。</li>
<li>methods: 本文使用了机器学习和排序数据结构的方法，包括学习排序集数据结构和搜索算法。</li>
<li>results: 本文获得了一些有趣的结果，包括首个学习最优二分搜索森林，其 mean access time  bounded by  Entropy of 访问频率分布。此外，本文还获得了首个学习排序集数据结构，在动态情况下和在准确分析设置下，与经典数据结构具有相同的时间 bound，这在广泛接受的宇宙大小假设下。实验部分表明，我们的总结可以生成有效和竞争力强的学习数据结构扩展器，即使与特定的参考模型相比。<details>
<summary>Abstract</summary>
This research concerns Learned Data Structures, a recent area that has emerged at the crossroad of Machine Learning and Classic Data Structures. It is methodologically important and with a high practical impact. We focus on Learned Indexes, i.e., Learned Sorted Set Dictionaries. The proposals available so far are specific in the sense that they can boost, indeed impressively, the time performance of Table Search Procedures with a sorted layout only, e.g., Binary Search. We propose a novel paradigm that, complementing known specialized ones, can produce Learned versions of any Sorted Set Dictionary, for instance, Balanced Binary Search Trees or Binary Search on layouts other that sorted, i.e., Eytzinger. Theoretically, based on it, we obtain several results of interest, such as (a) the first Learned Optimum Binary Search Forest, with mean access time bounded by the Entropy of the probability distribution of the accesses to the Dictionary; (b) the first Learned Sorted Set Dictionary that, in the Dynamic Case and in an amortized analysis setting, matches the same time bounds known for Classic Dictionaries. This latter under widely accepted assumptions regarding the size of the Universe. The experimental part, somewhat complex in terms of software development, clearly indicates the nonobvious finding that the generalization we propose can yield effective and competitive Learned Data Structural Booster, even with respect to specific benchmark models.
</details>
<details>
<summary>摘要</summary>
Theoretically, we obtain several interesting results, including:1. The first learned optimum binary search forest, with a mean access time bounded by the entropy of the probability distribution of the accesses to the dictionary.2. The first learned sorted set dictionary that, in the dynamic case and in an amortized analysis setting, matches the same time bounds as classic dictionaries, under widely accepted assumptions about the size of the universe.The experimental part of our research, which involved complex software development, revealed a non-obvious finding: our generalization can yield effective and competitive learned data structural boosters, even compared to specific benchmark models.
</details></li>
</ul>
<hr>
<h2 id="Pressmatch-Automated-journalist-recommendation-for-media-coverage-with-Nearest-Neighbor-search"><a href="#Pressmatch-Automated-journalist-recommendation-for-media-coverage-with-Nearest-Neighbor-search" class="headerlink" title="Pressmatch: Automated journalist recommendation for media coverage with Nearest Neighbor search"></a>Pressmatch: Automated journalist recommendation for media coverage with Nearest Neighbor search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00944">http://arxiv.org/abs/2309.00944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumya Parekh, Jay Patel</li>
<li>for: 这个研究的目的是提出一个模型，用于自动推荐适合的журналісти来推广发布的新闻稿。</li>
<li>methods: 这个研究使用了自然语言处理和机器学习技术，推荐журналісти基于新闻稿的内容和 журналіст的 beat 进行推广。</li>
<li>results: 研究发现，这个模型可以帮助传播人员更好地选择适合的журналісти，提高新闻稿的推广效果。<details>
<summary>Abstract</summary>
Slating a product for release often involves pitching journalists to run stories on your press release. Good media coverage often ensures greater product reach and drives audience engagement for those products. Hence, ensuring that those releases are pitched to the right journalists with relevant interests is crucial, since they receive several pitches daily. Keeping up with journalist beats and curating a media contacts list is often a huge and time-consuming task. This study proposes a model to automate and expedite the process by recommending suitable journalists to run media coverage on the press releases provided by the user.
</details>
<details>
<summary>摘要</summary>
通常，发布产品的过程中需要向记者推销新闻稿，以提高产品的曝光率和让宠物用户参与。因此，向正确的记者推销是非常重要的，因为他们每天收到许多推销。保持记者的 beat 和积累媒体联系人名单是一项巨大和耗时的任务。这项研究提出了一个模型，以自动和加速推销过程，推荐用户提供的新闻稿适合的记者。Note: "媒体联系人名单" (méidiā liánxīn rénmín) is a term used in Chinese to refer to a list of contacts in the media industry, such as journalists or bloggers.
</details></li>
</ul>
<hr>
<h2 id="Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models"><a href="#Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models" class="headerlink" title="Emergent Linear Representations in World Models of Self-Supervised Sequence Models"></a>Emergent Linear Representations in World Models of Self-Supervised Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00941">http://arxiv.org/abs/2309.00941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajyl/mech_int_othellogpt">https://github.com/ajyl/mech_int_othellogpt</a></li>
<li>paper_authors: Neel Nanda, Andrew Lee, Martin Wattenberg</li>
<li>for: This paper is written for understanding how sequence models represent their decision-making process, specifically in the context of the game Othello.</li>
<li>methods: The paper uses a probing approach to investigate the internal state of the model, and demonstrates that the model represents the board state in a linear rather than nonlinear way.</li>
<li>results: The paper shows that probing for “my colour” vs. “opponent’s colour” provides a simple yet powerful way to interpret the model’s internal state, and that this understanding allows for significant interpretability progress, including the ability to control the model’s behavior with simple vector arithmetic.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了理解序列模型如何做出决策的过程，具体来说是在游戏奥菲托尔中进行的。</li>
<li>methods: 这篇论文使用探测方法来研究模型的内部状态，并证明模型表示棋盘状态的方式是线性的，而不是非线性的。</li>
<li>results: 这篇论文显示，对”我的颜色”与”对手的颜色”进行探测提供了一个简单 yet 强大的方式来解释模型的内部状态，并且这种理解允许对模型的行为进行简单的 вектор数学操作。<details>
<summary>Abstract</summary>
How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed." into 中文（简体）Here's the translation:sequence models的决策过程是如何表示？前作者建议othello游戏神经网络学习了板状态的非线性模型（Li et al., 2023）。在这个工作中，我们提供了一种相关的直线表示，具体来说，我们发现了“我的颜色”vs.“对手的颜色”的探测可能是一种简单又强大的内部状态 интерпреタability的方法。这种精确的内部表示允许我们通过简单的向量数学控制模型的行为。直线表示带来了重要的可读性进步，我们通过进一步探索世界模型如何计算来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="GBE-MLZSL-A-Group-Bi-Enhancement-Framework-for-Multi-Label-Zero-Shot-Learning"><a href="#GBE-MLZSL-A-Group-Bi-Enhancement-Framework-for-Multi-Label-Zero-Shot-Learning" class="headerlink" title="GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning"></a>GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00923">http://arxiv.org/abs/2309.00923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Jingcai Guo, Xiaocheng Lu, Song Guo, Peiran Dong, Jiewei Zhang</li>
<li>for: 本研究 investigate 一个 zero-shot learning 在多labelenario（MLZSL）中的挑战问题，即模型需要根据见到的类别和auxiliary知识，识别 sample 中的多个未看到类别。</li>
<li>methods: 本研究提出了一个 novel 和有效的集群强化框架（GBE-MLZSL），以充分利用当中的属性，实现更加精确和响广的数字semantic投射。特别是，将特征地图分为多个特征组，每个特征组可以独立地训练 Local Information Distinguishing Module（LID）以确保唯一性。另外，Global Enhancement Module（GEM）用于保持主要方向。此外，还设计了一个静态图strucuture来建立本地特征之间的相关。</li>
<li>results: 实验结果显示，提出的 GBE-MLZSL 方法在大规模 MLZSL 评量数据集 NUS-WIDE 和 Open-Images-v4 上，与其他现有的State-of-the-art方法之间存在大幅的优势。<details>
<summary>Abstract</summary>
This paper investigates a challenging problem of zero-shot learning in the multi-label scenario (MLZSL), wherein, the model is trained to recognize multiple unseen classes within a sample (e.g., an image) based on seen classes and auxiliary knowledge, e.g., semantic information. Existing methods usually resort to analyzing the relationship of various seen classes residing in a sample from the dimension of spatial or semantic characteristics, and transfer the learned model to unseen ones. But they ignore the effective integration of local and global features. That is, in the process of inferring unseen classes, global features represent the principal direction of the image in the feature space, while local features should maintain uniqueness within a certain range. This integrated neglect will make the model lose its grasp of the main components of the image. Relying only on the local existence of seen classes during the inference stage introduces unavoidable bias. In this paper, we propose a novel and effective group bi-enhancement framework for MLZSL, dubbed GBE-MLZSL, to fully make use of such properties and enable a more accurate and robust visual-semantic projection. Specifically, we split the feature maps into several feature groups, of which each feature group can be trained independently with the Local Information Distinguishing Module (LID) to ensure uniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed to preserve the principal direction. Besides, a static graph structure is designed to construct the correlation of local features. Experiments on large-scale MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the proposed GBE-MLZSL outperforms other state-of-the-art methods with large margins.
</details>
<details>
<summary>摘要</summary>
Existing methods often analyze the relationship between various seen classes within a sample from the perspectives of spatial or semantic characteristics, and then transfer the learned model to unseen classes. However, these methods tend to ignore the effective integration of local and global features. In other words, they do not fully utilize the principal direction of the image in the feature space, nor do they maintain the uniqueness of local features within a certain range. This integration neglect can cause the model to lose its grasp of the main components of the image.Furthermore, relying solely on the local existence of seen classes during the inference stage can introduce unavoidable bias. To address these issues, this paper proposes a novel and effective group bi-enhancement framework for MLZSL, called GBE-MLZSL. This framework splits the feature maps into several feature groups, each of which can be trained independently with the Local Information Distinguishing Module (LID) to ensure uniqueness. Additionally, a Global Enhancement Module (GEM) is designed to preserve the principal direction. A static graph structure is also designed to construct the correlation of local features.Experiments on large-scale MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the proposed GBE-MLZSL outperforms other state-of-the-art methods with large margins.
</details></li>
</ul>
<hr>
<h2 id="A-novel-framework-employing-deep-multi-attention-channels-network-for-the-autonomous-detection-of-metastasizing-cells-through-fluorescence-microscopy"><a href="#A-novel-framework-employing-deep-multi-attention-channels-network-for-the-autonomous-detection-of-metastasizing-cells-through-fluorescence-microscopy" class="headerlink" title="A novel framework employing deep multi-attention channels network for the autonomous detection of metastasizing cells through fluorescence microscopy"></a>A novel framework employing deep multi-attention channels network for the autonomous detection of metastasizing cells through fluorescence microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00911">http://arxiv.org/abs/2309.00911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Mamalakis, Sarah C. Macfarlane, Scott V. Notley, Annica K. B Gad, George Panoutsos</li>
<li>for: 分辨normal和恶性细胞的大规模计算机影像框架</li>
<li>methods: 结合多个注意通道网络和全球可解释技术</li>
<li>results: 实现了分辨normal和恶性细胞的高精度分类，并提供了可解释的全球权重平均值和局部GradCam得分In simpler English:</li>
<li>for: Developing a computer vision framework to distinguish between normal and cancerous cells</li>
<li>methods: Combining multi-attention channels and global explainable techniques</li>
<li>results: Achieving high-precision classification and providing interpretable global weighted mean and local GradCam scoresNote: “GradCam” is short for “gradients of class activations,” which is a technique used to visualize the internal workings of a deep learning model.<details>
<summary>Abstract</summary>
We developed a transparent computational large-scale imaging-based framework that can distinguish between normal and metastasizing human cells. The method relies on fluorescence microscopy images showing the spatial organization of actin and vimentin filaments in normal and metastasizing single cells, using a combination of multi-attention channels network and global explainable techniques. We test a classification between normal cells (Bj primary fibroblast), and their isogenically matched, transformed and invasive counterpart (BjTertSV40TRasV12). Manual annotation is not trivial to automate due to the intricacy of the biologically relevant features. In this research, we utilized established deep learning networks and our new multi-attention channel architecture. To increase the interpretability of the network - crucial for this application area - we developed an interpretable global explainable approach correlating the weighted geometric mean of the total cell images and their local GradCam scores. The significant results from our analysis unprecedently allowed a more detailed, and biologically relevant understanding of the cytoskeletal changes that accompany oncogenic transformation of normal to invasive and metastasizing cells. We also paved the way for a possible spatial micrometre-level biomarker for future development of diagnostic tools against metastasis (spatial distribution of vimentin).
</details>
<details>
<summary>摘要</summary>
Translation notes:* "computational large-scale imaging-based framework" becomes "计算机大规模成像基础框架" (jìsuànjí móshì zhìyì)* "normal and metastasizing human cells" becomes "正常和肿瘤人类细胞" (zhèngcháng hé jìngzhì rénlèi xiǎoquǎi)* "fluorescence microscopy images" becomes "抗体染料显微镜像" (kǎngshì ránliàng jiǎngwēi yǐngxìng)* "spatial organization of actin and vimentin filaments" becomes "Actin和维门铁线粒的空间组织" (Actin hé wéimén tiě tiáo de kōngjiān zhìsuǒ)* "multi-attention channels network" becomes "多通道注意网络" (duō tōngcháng zhùyì wǎngluò)* "global explainable techniques" becomes "全球可解释技术" (quánqiú kějiejiě bìngjiè)* "Bj primary fibroblast" becomes "Bj主细胞" (Bj zhōng xiǎoquǎi)* "BjTertSV40TRasV12" becomes "BjTertSV40TRasV12" (BjTertSV40TRasV12)* "manual annotation" becomes "手动标注" (shǒudòng biāo zhù)* "interpretable global explainable approach" becomes "可解释全球方法" (kějiejiě quánqiú fāngzhì)* "weighted geometric mean" becomes "加权地理均值" (jiāwù dì lǐjìn zhì)* "local GradCam scores" becomes "本地GradCam分数" (běn dì GradCam fēnshù)
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading"><a href="#A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading" class="headerlink" title="A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading"></a>A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00907">http://arxiv.org/abs/2309.00907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihuai Liang, Bo Yang, Zhiwen Yu, Xuelin Cao, Derrick Wing Kwan Ng, Chau Yuen</li>
<li>For: 提高移动边缘计算（MEC）性能，设计优化的卸载策略，包括卸载决策和计算资源分配。* Methods: 使用混合整数非线性程序（MINLP）问题的解决方法，通过在训练了深度神经网络（DNN）模型后进行在线推理来获得有效解决方案。* Results: 提出了一种多头集成多任务学习（MEMTL）方法，可以减少训练开销并提高推理性能，使得在时变系统环境下可以有效地解决卸载决策和资源分配问题。实验结果表明，相比基准方法，提出的MEMTL方法在推理准确率和平均方差Error中均表现出色，不需要额外的训练数据。<details>
<summary>Abstract</summary>
Computation offloading has become a popular solution to support computationally intensive and latency-sensitive applications by transferring computing tasks to mobile edge servers (MESs) for execution, which is known as mobile/multi-access edge computing (MEC). To improve the MEC performance, it is required to design an optimal offloading strategy that includes offloading decision (i.e., whether offloading or not) and computational resource allocation of MEC. The design can be formulated as a mixed-integer nonlinear programming (MINLP) problem, which is generally NP-hard and its effective solution can be obtained by performing online inference through a well-trained deep neural network (DNN) model. However, when the system environments change dynamically, the DNN model may lose efficacy due to the drift of input parameters, thereby decreasing the generalization ability of the DNN model. To address this unique challenge, in this paper, we propose a multi-head ensemble multi-task learning (MEMTL) approach with a shared backbone and multiple prediction heads (PHs). Specifically, the shared backbone will be invariant during the PHs training and the inferred results will be ensembled, thereby significantly reducing the required training overhead and improving the inference performance. As a result, the joint optimization problem for offloading decision and resource allocation can be efficiently solved even in a time-varying wireless environment. Experimental results show that the proposed MEMTL outperforms benchmark methods in both the inference accuracy and mean square error without requiring additional training data.
</details>
<details>
<summary>摘要</summary>
computation offloading 已成为支持 computationally intensive 和延迟敏感应用的受欢迎解决方案，通过将计算任务传输到 mobil edge server (MES) 进行执行，这称为 mobil/多接入边计算 (MEC)。为了提高 MEC 性能，需要设计优化的 offloading 策略，包括 offloading 决策（是否 offloading）和 MEC 的计算资源分配。该设计可以表示为混合整数非线性计划 (MINLP) 问题，通常是NP困难的，其有效解决方式可以通过在训练好的深度神经网络 (DNN) 模型上进行在线推理来获得。然而，当系统环境变化 dynamically 时，DNN 模型可能会失去有效性，因为输入参数的漂移，从而降低 DNN 模型的通用能力。为解决这个特殊挑战，在这篇论文中，我们提出了一种多头集成多任务学习 (MEMTL) 方法，其中包括一个共享基础和多个预测头 (PH)。具体来说，共享基础将在 PH 训练期间保持不变，并将推理结果 ensemble，从而减少了训练过程的必要时间和提高推理性能。因此，可以有效地解决 joint optimization 问题，即 offloading 决策和资源分配，即使在时变无线电环境中。实验结果表明，我们提出的 MEMTL 方法在推理精度和时间平均误差方面具有明显的优势，而不需要额外的训练数据。
</details></li>
</ul>
<hr>
<h2 id="Regularly-Truncated-M-estimators-for-Learning-with-Noisy-Labels"><a href="#Regularly-Truncated-M-estimators-for-Learning-with-Noisy-Labels" class="headerlink" title="Regularly Truncated M-estimators for Learning with Noisy Labels"></a>Regularly Truncated M-estimators for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00894">http://arxiv.org/abs/2309.00894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoboxia/rtm_lnl">https://github.com/xiaoboxia/rtm_lnl</a></li>
<li>paper_authors: Xiaobo Xia, Pengqian Lu, Chen Gong, Bo Han, Jun Yu, Jun Yu, Tongliang Liu</li>
<li>For: The paper is written to address the issues of noisy labels in deep learning, specifically the problem of selecting small-loss examples that may be contaminated with noise.* Methods: The paper proposes a regularly truncated M-estimator (RTME) method that alternates between truncated M-estimators and original M-estimators to adaptively select small-loss examples and reduce the side-effects of noisy labels.* Results: The paper demonstrates the label-noise-tolerance of the proposed method through theoretical analysis and comprehensive experimental results, showing that it can outperform multiple baselines and is robust to broad noise types and levels.Here is the same information in Simplified Chinese text:* For: 本文是为了解决深度学习中的噪声标签问题，具体来说是选择小损失示例的问题。* Methods: 本文提出了一种常见跳转M-估计（RTME）方法，该方法可以逐渐地选择小损失示例，并避免噪声标签的影响。* Results: 本文通过理论分析和广泛的实验结果验证了方法的噪声标签忍受性，并证明了它可以超过多个基线，并且对噪声类型和水平进行了广泛的验证。<details>
<summary>Abstract</summary>
The sample selection approach is very popular in learning with noisy labels. As deep networks learn pattern first, prior methods built on sample selection share a similar training procedure: the small-loss examples can be regarded as clean examples and used for helping generalization, while the large-loss examples are treated as mislabeled ones and excluded from network parameter updates. However, such a procedure is arguably debatable from two folds: (a) it does not consider the bad influence of noisy labels in selected small-loss examples; (b) it does not make good use of the discarded large-loss examples, which may be clean or have meaningful information for generalization. In this paper, we propose regularly truncated M-estimators (RTME) to address the above two issues simultaneously. Specifically, RTME can alternately switch modes between truncated M-estimators and original M-estimators. The former can adaptively select small-losses examples without knowing the noise rate and reduce the side-effects of noisy labels in them. The latter makes the possibly clean examples but with large losses involved to help generalization. Theoretically, we demonstrate that our strategies are label-noise-tolerant. Empirically, comprehensive experimental results show that our method can outperform multiple baselines and is robust to broad noise types and levels.
</details>
<details>
<summary>摘要</summary>
“ sample selection approach 非常受欢迎在听过噪音标签的学习中。深度网络会在首先学习 Pattern，因此先前的方法在 sample selection 上共享类似的训练过程：小损例可以被视为干净的例子，并用于帮助泛化，而大损例则被排除在网络参数更新中。然而，这种过程存在两点问题：（a）它不考虑随选小损例中噪音标签的坏影响；（b）它不好用释放的大损例，它们可能是干净的或具有泛化信息。在这篇论文中，我们提出了常见落实 truncated M-estimators（RTME）来解决以上两个问题。具体来说，RTME可以 alternate switching  между truncated M-estimators 和原始 M-estimators。前者可以适应ively 选择小损例，无需知道噪音率，并减少随选小损例中噪音标签的副作用。后者使得可能是干净的，但有大损例参与到泛化中。我们理论上表明了我们的策略是噪音抗性的。实验结果表明，我们的方法可以超越多个基elines，并在各种噪音类型和水平上表现稳定。”
</details></li>
</ul>
<hr>
<h2 id="Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers"><a href="#Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers" class="headerlink" title="Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers"></a>Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00889">http://arxiv.org/abs/2309.00889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alper Ahmetoglu, Batuhan Celik, Erhan Oztop, Emre Ugur</li>
<li>for: 本研究提出了一种新的深度学习架构，用于在表格环境中发现对象和其关系的符号表示。这种架构可以自动处理不同数量的对象，并将对象之间的关系映射到符号领域。</li>
<li>methods: 本研究使用了一个自注意层，计算对象特征中的抽象注意力权重，并将这些注意力权重用于对象符号的汇集和动作效果预测。</li>
<li>results: 实验结果显示，提出的架构在预测动作效果的同时，自动形成了对象符号和关系符号，并与其他基eline方法相比，表现更好。此外，分析表明，学习的符号与对象之间的相对位置、对象类型和桌面上的水平Alignment有关。<details>
<summary>Abstract</summary>
In this paper, we propose and realize a new deep learning architecture for discovering symbolic representations for objects and their relations based on the self-supervised continuous interaction of a manipulator robot with multiple objects on a tabletop environment. The key feature of the model is that it can handle a changing number number of objects naturally and map the object-object relations into symbolic domain explicitly. In the model, we employ a self-attention layer that computes discrete attention weights from object features, which are treated as relational symbols between objects. These relational symbols are then used to aggregate the learned object symbols and predict the effects of executed actions on each object. The result is a pipeline that allows the formation of object symbols and relational symbols from a dataset of object features, actions, and effects in an end-to-end manner. We compare the performance of our proposed architecture with state-of-the-art symbol discovery methods in a simulated tabletop environment where the robot needs to discover symbols related to the relative positions of objects to predict the observed effect successfully. Our experiments show that the proposed architecture performs better than other baselines in effect prediction while forming not only object symbols but also relational symbols. Furthermore, we analyze the learned symbols and relational patterns between objects to learn about how the model interprets the environment. Our analysis shows that the learned symbols relate to the relative positions of objects, object types, and their horizontal alignment on the table, which reflect the regularities in the environment.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出并实现了一种新的深度学习架构，用于发现对象和它们之间关系的符号表示。这种模型可以自动处理变化的对象数量，并将对象之间关系显式地映射到符号领域。在模型中，我们使用了一层自注意层，计算对象特征中的不同注意力权重，这些注意力权重被视为对象之间关系的符号。这些符号然后用于聚合学习的对象符号和预测每个对象的影响。结果是一个可以从对象特征、动作和效果的数据集中形成对象符号和关系符号的端到端管道。我们与现状顶尖的符号发现方法进行比较，在模拟的桌面环境中， robot需要通过发现对象的相对位置来预测观察到的效果。我们的实验表明，我们提出的架构在效果预测中表现更好于其他基elines，同时不仅形成对象符号，还形成了关系符号。此外，我们分析了学习的符号和对象之间的关系律pattern，以了解模型如何理解环境。我们的分析显示，学习的符号与对象的相对位置、对象类型和桌面上的水平对齐有关，这些Regularities在环境中。
</details></li>
</ul>
<hr>
<h2 id="Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy"><a href="#Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy" class="headerlink" title="Tight Bounds for Machine Unlearning via Differential Privacy"></a>Tight Bounds for Machine Unlearning via Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00886">http://arxiv.org/abs/2309.00886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Huang, Clément L. Canonne</li>
<li>for: 这篇论文是关于机器学习”忘记”的研究，即要求训练过的模型在请求时能够”忘记”一些训练数据点，作为如果它们从来没有被包含在模型中一样。</li>
<li>methods: 作者使用了差异隐私（DP）算法来实现机器”忘记”。</li>
<li>results: 作者完全关闭了DP算法实现机器”忘记”的上下限，获得了DP算法实现的最佳 deletion capacity。<details>
<summary>Abstract</summary>
We consider the formulation of "machine unlearning" of Sekhari, Acharya, Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called "right to be forgotten" by requiring that a trained model, upon request, should be able to "unlearn" a number of points from the training data, as if they had never been included in the first place. Sekhari et al. established some positive and negative results about the number of data points that can be successfully unlearnt by a trained model without impacting the model's accuracy (the "deletion capacity"), showing that machine unlearning could be achieved by using differentially private (DP) algorithms. However, their results left open a gap between upper and lower bounds on the deletion capacity of these algorithms: our work fully closes this gap, obtaining tight bounds on the deletion capacity achievable by DP-based machine unlearning algorithms.
</details>
<details>
<summary>摘要</summary>
我团队考虑了谢卡里、阿查纳、卡马斯和苏瑞希（NeurIPS 2021）所提出的机器“忘记” formalization，即要求已经训练过的模型，在请求的情况下，能够“忘记”一些训练数据点，好像它们从来没有被包含在内。谢卡里等人提出了一些积分和负积分结果，证明了机器“忘记”可以通过使用具有隐私保护（DP）算法来实现。然而，他们的结果留下了对删除容量的Upper和Lower bound的 gap：我们的工作完全关闭了这个差距，得到了DP基于机器“忘记”算法的准确 delete capacity 的紧跟 bounds。
</details></li>
</ul>
<hr>
<h2 id="A-Generic-Fundus-Image-Enhancement-Network-Boosted-by-Frequency-Self-supervised-Representation-Learning"><a href="#A-Generic-Fundus-Image-Enhancement-Network-Boosted-by-Frequency-Self-supervised-Representation-Learning" class="headerlink" title="A Generic Fundus Image Enhancement Network Boosted by Frequency Self-supervised Representation Learning"></a>A Generic Fundus Image Enhancement Network Boosted by Frequency Self-supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00885">http://arxiv.org/abs/2309.00885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Li, Haofeng Liu, Huazhu Fu, Yanwu Xu, Hui Shu, Ke Niu, Yan Hu, Jiang Liu</li>
<li>for:  This paper aims to develop a generic fundus image enhancement network (GFE-Net) to correct degraded fundus images without supervised or extra data, which can be used for clinical examination and intelligent systems.</li>
<li>methods: The proposed GFE-Net leverages image frequency information and self-supervised representation learning to learn robust structure-aware representations from degraded images, and then couples representation learning and image enhancement to accurately correct fundus images while preserving retinal structures.</li>
<li>results: Compared with state-of-the-art algorithms, GFE-Net achieves superior performance in data dependency, enhancement performance, deployment efficiency, and scale generalizability, and its modules are also verified to be effective for image enhancement.<details>
<summary>Abstract</summary>
Fundus photography is prone to suffer from image quality degradation that impacts clinical examination performed by ophthalmologists or intelligent systems. Though enhancement algorithms have been developed to promote fundus observation on degraded images, high data demands and limited applicability hinder their clinical deployment. To circumvent this bottleneck, a generic fundus image enhancement network (GFE-Net) is developed in this study to robustly correct unknown fundus images without supervised or extra data. Levering image frequency information, self-supervised representation learning is conducted to learn robust structure-aware representations from degraded images. Then with a seamless architecture that couples representation learning and image enhancement, GFE-Net can accurately correct fundus images and meanwhile preserve retinal structures. Comprehensive experiments are implemented to demonstrate the effectiveness and advantages of GFE-Net. Compared with state-of-the-art algorithms, GFE-Net achieves superior performance in data dependency, enhancement performance, deployment efficiency, and scale generalizability. Follow-up fundus image analysis is also facilitated by GFE-Net, whose modules are respectively verified to be effective for image enhancement.
</details>
<details>
<summary>摘要</summary>
血管照片受到影像质量削弱的影响，对于医生或智能系统进行临床评估。虽然有增强算法来提高血管观察，但高数据需求和有限的应用限制了其临床部署。为绕过这个瓶颈，本研究提出了一种通用血管图像增强网络（GFE-Net），可以无监督地修复不知道的血管图像。利用图像频率信息，GFE-Net通过自我监督学习来学习血管图像中的结构信息。然后，GFE-Net通过将表征学习和图像增强结合在一起，可以准确地修复血管图像，同时保持视网膜结构。我们进行了广泛的实验，以证明GFE-Net的有效性和优势。相比之前的算法，GFE-Net在数据依赖度、增强性、部署效率和扩展可行性等方面表现出色。此外，GFE-Net的模块也在各种图像增强任务中进行了验证，并证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Certified-Probabilistic-Robustness-with-High-Accuracy"><a href="#Towards-Certified-Probabilistic-Robustness-with-High-Accuracy" class="headerlink" title="Towards Certified Probabilistic Robustness with High Accuracy"></a>Towards Certified Probabilistic Robustness with High Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00879">http://arxiv.org/abs/2309.00879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihan Zhang, Peixin Zhang, Jun Sun</li>
<li>for: 本研究旨在提出一种可以同时实现高精度和证明性的神经网络模型。</li>
<li>methods: 本方法包括两部分：一是一种 probabilistic robust training 方法，它的目标是在随机扰动下减少模型的偏差变化，二是一种runtime inference方法，它可以在运行时提供证明性的 garantía。</li>
<li>results: 对多种扰动进行测试，本方法可以显著超越现有方法， both in terms of certification rate and accuracy。<details>
<summary>Abstract</summary>
Adversarial examples pose a security threat to many critical systems built on neural networks (such as face recognition systems, and self-driving cars). While many methods have been proposed to build robust models, how to build certifiably robust yet accurate neural network models remains an open problem. For example, adversarial training improves empirical robustness, but they do not provide certification of the model's robustness. On the other hand, certified training provides certified robustness but at the cost of a significant accuracy drop. In this work, we propose a novel approach that aims to achieve both high accuracy and certified probabilistic robustness. Our method has two parts, i.e., a probabilistic robust training method with an additional goal of minimizing variance in terms of divergence and a runtime inference method for certified probabilistic robustness of the prediction. The latter enables efficient certification of the model's probabilistic robustness at runtime with statistical guarantees. This is supported by our training objective, which minimizes the variance of the model's predictions in a given vicinity, derived from a general definition of model robustness. Our approach works for a variety of perturbations and is reasonably efficient. Our experiments on multiple models trained on different datasets demonstrate that our approach significantly outperforms existing approaches in terms of both certification rate and accuracy.
</details>
<details>
<summary>摘要</summary>
“对于多种攻击性应用（如识别 face recognition 系统和自驾车），对于内置 ней拥有数据的系统而言，防御性攻击是一个开启的问题。许多方法已经被提出来建立坚固的模型，但是如何建立认证可靠的坚固模型仍然是一个开启的问题。例如，这些模型可以通过防御训练提高实际的防御性，但是它们不会提供模型的认证可靠性。另一方面，认证训练可以提供认证可靠性，但是它们将会导致模型的准确性下降。在这个工作中，我们提出了一个新的方法，旨在实现高准确性和认证可靠性。我们的方法有两个部分：一个是一种具有额外目标的机会均衡训练方法，另一个是一种在Runtime inference的方法，用于认证模型的 probabilistic 可靠性。这个方法可以实现在多种攻击下进行认证，并且相对高效。我们的实验结果显示，我们的方法在多个模型和不同的数据集上具有较高的认证率和准确性。”
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning"><a href="#Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning" class="headerlink" title="Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning"></a>Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00878">http://arxiv.org/abs/2309.00878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyass Moummad, Romain Serizel, Nicolas Farrugia</li>
<li>for: 这个研究是为了解决 bioacoustic 应用中的声事件探测和分类问题，因为这些任务通常具有寥寥无几的标签训练数据，导致超级学习不是最佳的方法。</li>
<li>methods: 这个研究使用了一个自然语言处理框架，将问题转换为 few-shot 学习问题，即将仅有五个标签的训练数据用于训练系统。</li>
<li>results: 这个研究获得了一个 F-score 的63.46% 和 42.7% 在验证和测试集中，在 DCASE 挑战中排名第二。研究还进行了一个删除研究的研究，以了解哪些数据增强技术和学习策略在训练集中具有最大的影响。<details>
<summary>Abstract</summary>
Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46\% on the validation set and 42.7\% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set.
</details>
<details>
<summary>摘要</summary>
深度学习在近期内广泛应用于声音事件检测和分类。其成功与具有足够大的数据集，可能有相应的注释时进行监督学习的可用性相关。在生物声学应用中，大多数任务都具有少量标注训练数据，因为标注长录音是时间consuming和成本高的。因此，监督学习不是解决生物声学任务的最佳方法。生物声学社区将声音事件检测问题划入了少量学习框架中，即在只有几个标注示例的情况下训练一个系统。DCASE挑战中的声音事件检测任务是在长 audio recording 上检测事件，只需要提供每个类型的五个标注示例。在这篇文章中，我们表明了可以通过利用数据扩充和监督对比学习框架来学习 Rich feature extractor 从头来。我们强调了这个框架在五个事件检测中的传输性能。我们在验证集上获得了63.46%的 F-score，在测试集上获得了42.7%的 F-score，在DCASE挑战中排名第二。我们对数据扩充技术的关键选择以及在训练集上应用的学习策略进行了ablation研究。
</details></li>
</ul>
<hr>
<h2 id="Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models"><a href="#Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models" class="headerlink" title="Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models"></a>Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00866">http://arxiv.org/abs/2309.00866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/esdalmaijer/cluster_power_tutorial">https://github.com/esdalmaijer/cluster_power_tutorial</a></li>
<li>paper_authors: Edwin S Dalmaijer</li>
<li>for: 这个论文的目的是为了帮助研究者在进行分组分析时确定样本大小和影响大小。</li>
<li>methods: 这个论文使用了一种新的方法来计算样本大小和影响大小，这种方法可以帮助研究者在分组分析中更好地计算统计力和影响大小。</li>
<li>results: 这个论文通过使用 simulations 提供了一个参考表，用于帮助研究者在不同的分组分析方法下确定样本大小和影响大小，以确保他们的研究具有足够的统计力。<details>
<summary>Abstract</summary>
Before embarking on data collection, researchers typically compute how many individual observations they should do. This is vital for doing studies with sufficient statistical power, and often a cornerstone in study pre-registrations and grant applications. For traditional statistical tests, one would typically determine an acceptable level of statistical power, (gu)estimate effect size, and then use both values to compute the required sample size. However, for analyses that identify subgroups, statistical power is harder to establish. Once sample size reaches a sufficient threshold, effect size is primarily determined by the number of measured features and the underlying subgroup separation. As a consequence, a priory computations of statistical power are notoriously complex. In this tutorial, I will provide a roadmap to determining sample size and effect size for analyses that identify subgroups. First, I introduce a procedure that allows researchers to formalise their expectations about effect sizes in their domain of choice, and use this to compute the minimally required number of measured variables. Next, I outline how to establish the minimum sample size in subgroup analyses. Finally, I use simulations to provide a reference table for the most popular subgroup analyses: k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modelling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) to achieve acceptable statistical power, and can be readily used in study design.
</details>
<details>
<summary>摘要</summary>
In this tutorial, I will provide a step-by-step guide to determining sample size and effect size for analyses that identify subgroups. First, I will introduce a procedure that allows researchers to formalize their expectations about effect sizes in their domain of choice, and use this to compute the minimum number of measured variables required. Next, I will outline how to establish the minimum sample size in subgroup analyses. Finally, I will use simulations to provide a reference table for the most popular subgroup analyses, including k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modeling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) needed to achieve acceptable statistical power, and can be readily used in study design.
</details></li>
</ul>
<hr>
<h2 id="Equitable-FL-Federated-Learning-with-Sparsity-for-Resource-Constrained-Environment"><a href="#Equitable-FL-Federated-Learning-with-Sparsity-for-Resource-Constrained-Environment" class="headerlink" title="Equitable-FL: Federated Learning with Sparsity for Resource-Constrained Environment"></a>Equitable-FL: Federated Learning with Sparsity for Resource-Constrained Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00864">http://arxiv.org/abs/2309.00864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indrajeet Kumar Sinha, Shekhar Verma, Krishna Pratap Singh</li>
<li>for: 该论文旨在提出一种在资源受限环境中进行联合学习的方法，以确保学习可行性，不 mater nodes的资源充足程度。</li>
<li>methods: 该方法基于 Lottery Ticket Hypothesis approach，通过逐步减少模型参数量来鼓励有资源匮乏的节点参与协同训练。</li>
<li>results: 实验结果表明，Equitable-FL 在 $MNIST$, $F-MNIST$, $CIFAR-10$  benchmark 数据集以及 $Brain-MRI$ 数据集和 $PlantVillage$ 数据集上具有良好的效果，可以在各种资源受限环境下进行快速、高效的联合学习。<details>
<summary>Abstract</summary>
In Federated Learning, model training is performed across multiple computing devices, where only parameters are shared with a common central server without exchanging their data instances. This strategy assumes abundance of resources on individual clients and utilizes these resources to build a richer model as user's models. However, when the assumption of the abundance of resources is violated, learning may not be possible as some nodes may not be able to participate in the process. In this paper, we propose a sparse form of federated learning that performs well in a Resource Constrained Environment. Our goal is to make learning possible, regardless of a node's space, computing, or bandwidth scarcity. The method is based on the observation that model size viz a viz available resources defines resource scarcity, which entails that reduction of the number of parameters without affecting accuracy is key to model training in a resource-constrained environment. In this work, the Lottery Ticket Hypothesis approach is utilized to progressively sparsify models to encourage nodes with resource scarcity to participate in collaborative training. We validate Equitable-FL on the $MNIST$, $F-MNIST$, and $CIFAR-10$ benchmark datasets, as well as the $Brain-MRI$ data and the $PlantVillage$ datasets. Further, we examine the effect of sparsity on performance, model size compaction, and speed-up for training. Results obtained from experiments performed for training convolutional neural networks validate the efficacy of Equitable-FL in heterogeneous resource-constrained learning environment.
</details>
<details>
<summary>摘要</summary>
在联合学习中，模型训练通常会在多个计算设备上进行，只是共享参数而不是数据实例。这种策略假设每个客户端都有充足的资源，并利用这些资源建立更加丰富的模型。然而，当资源的充足假设被违反时，学习可能无法进行，因为一些节点可能无法参与过程中。在这篇论文中，我们提出了一种稀缺形式的联合学习方法，可以在资源受限环境中进行学习。我们的目标是让学习不受节点的资源限制，而是通过减少参数的数量来实现模型训练。我们利用了抽奖假设方法，逐步减少模型的参数，以便鼓励具有资源缺乏的节点参与合作训练。我们在$MNIST$, $F-MNIST$, $CIFAR-10$数据集上进行了实验，以及$Brain-MRI$数据集和$PlantVillage$数据集。此外，我们还研究了稀缺度对性能、模型大小压缩和训练速度的影响。实验结果表明，Equitable-FL在不同资源环境中进行联合学习时具有有效性。
</details></li>
</ul>
<hr>
<h2 id="DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal"><a href="#DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal" class="headerlink" title="DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal"></a>DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00855">http://arxiv.org/abs/2309.00855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwweiwei/dora">https://github.com/wwweiwei/dora</a></li>
<li>paper_authors: Wei-Wei Du, Wei-Yao Wang, Wen-Chih Peng</li>
<li>for: 本研究旨在开发一种具有偏好抑制的自动化评估模型，以便在金融机构进行高成本财产评估任务中减少域外专家的主观干预。</li>
<li>methods: 本研究提出了一种域基自动学习框架（DoRA），通过在它的预测任务中使用地理 metadata 来帮助财产表示具有域知识。此外，研究还使用了间隔样本学习来使 representations 更加抗性和可靠。</li>
<li>results: 对于三种实际交易的财产类型，DoRA 在少量交易enario中Significantly 超越了基于标签数据的 SSL 基elines、图形基本方法和经验法。DoRA 在 MAPE、MAE 和 HR10 等指标上提高了至少 7.6%、11.59% 和 3.34%。<details>
<summary>Abstract</summary>
The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an intra-sample geographic prediction as the pretext task based on the metadata of the real estate for equipping the real estate representations with prior domain knowledge. Furthermore, inter-sample contrastive learning is employed to generalize the representations to be robust for limited transactions of downstream tasks. Our benchmark results on three property types of real-world transactions show that DoRA significantly outperforms the SSL baselines for tabular data, the graph-based methods, and the supervised approaches in the few-shot scenarios by at least 7.6% for MAPE, 11.59% for MAE, and 3.34% for HR10%. We expect DoRA to be useful to other financial practitioners with similar marketplace applications who need general models for properties that are newly built and have limited records. The source code is available at https://github.com/wwweiwei/DoRA.
</details>
<details>
<summary>摘要</summary>
市场系统连接需求和供应，以开发不受偏见的决策方法。房地产评估作为高成本房产评估任务，需要领域专家根据相应的知识和市场判断来评估估价。现有的自动评估模型可以减少领域专家的主观性，但它们需要大量的交易数据进行有效评估，这主要受到交易标注的努力和新发展区域的通用性的限制。为了学习不标注的房地产集合中的表示，现有的自动学习（SSL） для表格数据忽略了许多重要特征，并且无法包含领域知识。本文提出了DoRA，一个基于领域的自动学习框架，用于低资源房地产评估。DoRA通过Metadata中的房地产数据进行内样地图预测作为预texte任务，以具备房地产表示的先前领域知识。此外，DoRA还使用了间样对比学习来普适表示，以便在有限交易情况下具备抗衰偏见能力。我们对实际交易中的三种不同类型的财产进行了比较，结果显示DoRA在少量enario下至少比SSL基线、图形基本方法和批处方法高出7.6%、11.59%和3.34%。我们预期DoRA将对其他金融实践者有用，他们需要对新建和有限纪录的财产进行通用的模型。代码可以在https://github.com/wwweiwei/DoRA中获取。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning"><a href="#A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning" class="headerlink" title="A Unifying Variational Framework for Gaussian Process Motion Planning"></a>A Unifying Variational Framework for Gaussian Process Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00854">http://arxiv.org/abs/2309.00854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Cosier, Rares Iordan, Sicelukwanda Zwane, Giovanni Franzese, James T. Wilson, Marc Peter Deisenroth, Alexander Terenin, Yasemin Bekiroglu</li>
<li>for: 本研究的目的是提出一种基于Variational Gaussian Processes的机器人运动规划方法，以便在高维状态空间中计算路径，同时考虑机械约束、降噪、模型误差等因素，并提供可靠的运动规划结果。</li>
<li>methods: 本研究使用了Variational Gaussian Processes来解决机器人运动规划问题，并提出了一种可以同时满足等式-based、不等式-based和软运动规划约束的框架。这种框架可以在练习过程中直接实现，并提供了间隔-based和Monte Carlo-based不确定性估计。</li>
<li>results: 对各种环境和机器人进行了比较 экспериiments，并与基准方法进行了比较，结果显示，提出的方法可以很好地平衡成功率和路径质量。<details>
<summary>Abstract</summary>
To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian Processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides both interval-based and Monte-Carlo-based uncertainty estimates. We conduct experiments using different environments and robots, comparing against baseline approaches based on the feasibility of the planned paths, and obstacle avoidance quality. Results show that our proposed approach yields a good balance between success rates and path quality.
</details>
<details>
<summary>摘要</summary>
为控制机器人的运动，动态规划算法必须计算高维状态空间中的路径，同时考虑机器人的电动机和关节的物理约束，生成平滑和稳定的运动，避免障碍物和冲突。因此，一个有效的动态规划算法应该平衡竞合的需求，并应该包含不确定性来处理噪声、模型错误和复杂环境中的部署。为解决这些问题，我们介绍了基于Variational Gaussian Processes的机器人动态规划框架，这种框架将概率推理基本的动态规划算法集成了一体。我们的框架可以在练习中直接包含等式基于、不等式基于和软动态规划约束，并提供了间隔基于和Monte Carlo基于的不确定性估计。我们在不同的环境和机器人上进行了实验，与基线方法进行比较，包括规划路径的可行性和障碍物避免质量。结果表明，我们的提议方法可以平衡成功率和路径质量。
</details></li>
</ul>
<hr>
<h2 id="A-Post-Processing-Based-Bengali-Document-Layout-Analysis-with-YOLOV8"><a href="#A-Post-Processing-Based-Bengali-Document-Layout-Analysis-with-YOLOV8" class="headerlink" title="A Post-Processing Based Bengali Document Layout Analysis with YOLOV8"></a>A Post-Processing Based Bengali Document Layout Analysis with YOLOV8</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00848">http://arxiv.org/abs/2309.00848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazmus Sakib Ahmed, Saad Sakib Noor, Ashraful Islam Shanto Sikder, Abhijit Paul</li>
<li>for: 提高孟加拉文档格式分析（DLA），使用YOLOv8模型和创新后处理技术。</li>
<li>methods: 采用数据扩展以提高模型鲁棒性，并实施两阶段预测策略以提高元素分 segmentation的准确性。</li>
<li>results:  ensemble模型，含Post处理，在BaDLAD数据集上达到了更高的性能，解决了在BaDLAD数据集中存在的问题。<details>
<summary>Abstract</summary>
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文关注使用YOLOv8模型和创新的后处理技术进行强化本地文档分析（DLA）。我们利用数据增强以提高模型的可靠性，并在完整的数据集上精心调整我们的方法，以实现精准的元素分割。我们的集成模型，与后处理相结合，超越了个别基础架构，解决了在BaDLAD数据集中存在的问题。我们希望通过这种方法进一步提高孟加拉文档分析，以提高OCR和文档理解。BaDLAD作为这一领域的基础资源，将为未来的研究提供帮助。此外，我们的实验又提供了关键的指导，以便在已有的解决方案中增加新的策略。
</details></li>
</ul>
<hr>
<h2 id="pSTarC-Pseudo-Source-Guided-Target-Clustering-for-Fully-Test-Time-Adaptation"><a href="#pSTarC-Pseudo-Source-Guided-Target-Clustering-for-Fully-Test-Time-Adaptation" class="headerlink" title="pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation"></a>pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00846">http://arxiv.org/abs/2309.00846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manogna Sreenivas, Goirik Chakrabarty, Soma Biswas</li>
<li>for: 这个论文的目的是提出一种新的测试时适应（TTA）方法，以便在实际场景中，模型能够在测试数据分布不同于训练数据分布下表现良好。</li>
<li>methods: 该方法受到目标划分技术的启发，利用源分类器生成 pseudo-source 样本，将测试样本策略地与这些 pseudo-source 样本相对应，从而提高 TTA 性能。</li>
<li>results: 实验 validate 了该方法的有效性，在多个领域差分数据集（VisDA、Office-Home、DomainNet-126、CIFAR-100C）上达到了显著提高预测精度和计算效率的目的。此外，我们还示出了该方法在连续 TTA 框架中的广泛性。<details>
<summary>Abstract</summary>
Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo-source samples. The test samples are strategically aligned with these pseudo-source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）是机器学习中的一个重要概念，它允许模型在实际场景中表现良好，其测试数据分布与训练数据分布不同。在这项工作中，我们提出了一种新的方法called pseudo Source guided Target Clustering（pSTarC），用于解决实际场景下的TTA问题，该问题尚未得到充分的研究。该方法 Draws inspiration from target clustering技术，并利用源分类器来生成 Pseudo-source 样本。测试样本被策略性地与这些 Pseudo-source 样本相对应，从而促进其分 clustering，并因此提高 TTA 性能。pSTarC 在完全测试时适应协议下运行，无需实际的源数据。我们对多个领域shift datasets进行了实验 validate pSTarC 的有效性，包括 VisDA、Office-Home、DomainNet-126 和 CIFAR-100C。结果表明，pSTarC 可以提高预测精度，同时具有有效的计算需求。此外，我们还证明了 pSTarC 框架的通用性，通过在连续 TTA 框架中应用该方法，并在不同的领域下达到了良好的性能。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning"><a href="#Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning" class="headerlink" title="Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning"></a>Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00837">http://arxiv.org/abs/2309.00837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amritpal Singh, Wenqi Shi, May D Wang</li>
<li>for: 这个研究的目的是为了开发一个可以进行软件运动的手术机器人，并通过实验证明其可行性。</li>
<li>methods: 这个研究使用了ROS相容的物理模拟环境，并在这个环境中模拟了手术过程中的软件互动。另外，这个研究还使用了示例导向的学习算法，以评估其性能和可行性。</li>
<li>results: 这个研究获得了软件运动 retraction 的自动化处理的证明，并且显示了将学习算法应用到软件互动中的可行性。这个研究提供了未来开发和改进手术机器人的基础。<details>
<summary>Abstract</summary>
In the context of surgery, robots can provide substantial assistance by performing small, repetitive tasks such as suturing, needle exchange, and tissue retraction, thereby enabling surgeons to concentrate on more complex aspects of the procedure. However, existing surgical task learning mainly pertains to rigid body interactions, whereas the advancement towards more sophisticated surgical robots necessitates the manipulation of soft bodies. Previous work focused on tissue phantoms for soft tissue task learning, which can be expensive and can be an entry barrier to research. Simulation environments present a safe and efficient way to learn surgical tasks before their application to actual tissue. In this study, we create a Robot Operating System (ROS)-compatible physics simulation environment with support for both rigid and soft body interactions within surgical tasks. Furthermore, we investigate the soft tissue interactions facilitated by the patient-side manipulator of the DaVinci surgical robot. Leveraging the pybullet physics engine, we simulate kinematics and establish anchor points to guide the robotic arm when manipulating soft tissue. Using demonstration-guided reinforcement learning (RL) algorithms, we investigate their performance in comparison to traditional reinforcement learning algorithms. Our in silico trials demonstrate a proof-of-concept for autonomous surgical soft tissue retraction. The results corroborate the feasibility of learning soft body manipulation through the application of reinforcement learning agents. This work lays the foundation for future research into the development and refinement of surgical robots capable of managing both rigid and soft tissue interactions. Code is available at https://github.com/amritpal-001/tissue_retract.
</details>
<details>
<summary>摘要</summary>
在手术上，机器人可以提供重要的帮助，执行小、重复的任务，如缝针交换和组织吸引，从而让外科医生能够更专注于更复杂的过程。然而，现有的手术任务学习主要关注固体交互，而更高级别的手术机器人的发展需要处理软体。现有的研究主要集中在模拟肿体上进行软组织任务学习，这可能是成本高并且可能成为研究入门障碍。在本研究中，我们创建了基于ROS（Robot Operating System）的物理 simulations环境，支持固体和软体交互在手术任务中。此外，我们研究了达文西手术机器人的病人侧 manipulate器在软组织上的交互。通过pybullet物理引擎，我们模拟了机械学和确定了安全点来导引机器人臂在软组织上操作。使用示例导向学习（RL）算法，我们研究了它们在比传统RL算法的性能。我们的silico实验证明了软组织吸引的自主化可能性。这些成果为未来研究开发可以处理固体和软体交互的手术机器人奠定了基础。代码可以在https://github.com/amritpal-001/tissue_retract上找到。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d"><a href="#Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d" class="headerlink" title="Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$"></a>Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00834">http://arxiv.org/abs/2309.00834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Drexler, Annika Hennes, Abhiruk Lahiri, Melanie Schmidt, Julian Wargalla</li>
<li>for: 这个论文是为了研究 $k$-min-sum-radii 问题的公平划分问题。</li>
<li>methods: 这篇论文使用了PTAS 算法来解决公平 $k$-min-sum-radii 问题。</li>
<li>results: 这篇论文提出了一个 PTAS 算法来解决公平 $k$-min-sum-radii 问题，并且可以适应不同的公平性约束。<details>
<summary>Abstract</summary>
The $k$-center problem is a classical clustering problem in which one is asked to find a partitioning of a point set $P$ into $k$ clusters such that the maximum radius of any cluster is minimized. It is well-studied. But what if we add up the radii of the clusters instead of only considering the cluster with maximum radius? This natural variant is called the $k$-min-sum-radii problem. It has become the subject of more and more interest in recent years, inspiring the development of approximation algorithms for the $k$-min-sum-radii problem in its plain version as well as in constrained settings.   We study the problem for Euclidean spaces $\mathbb{R}^d$ of arbitrary dimension but assume the number $k$ of clusters to be constant. In this case, a PTAS for the problem is known (see Bandyapadhyay, Lochet and Saurabh, SoCG, 2023). Our aim is to extend the knowledge base for $k$-min-sum-radii to the domain of fair clustering. We study several group fairness constraints, such as the one introduced by Chierichetti et al. (NeurIPS, 2017). In this model, input points have an additional attribute (e.g., colors such as red and blue), and clusters have to preserve the ratio between different attribute values (e.g., have the same fraction of red and blue points as the ground set). Different variants of this general idea have been studied in the literature. To the best of our knowledge, no approximative results for the fair $k$-min-sum-radii problem are known, despite the immense amount of work on the related fair $k$-center problem.   We propose a PTAS for the fair $k$-min-sum-radii problem in Euclidean spaces of arbitrary dimension for the case of constant $k$. To the best of our knowledge, this is the first PTAS for the problem. It works for different notions of group fairness.
</details>
<details>
<summary>摘要</summary>
“$k$-中心问题”是一个经典的聚集问题，需要将一个点集合($P$) partitioned into $k$个群，以 minimize 每个群的最大半径。这个问题已经受到了很多研究，但如果我们总和所有群的半径而不是仅考虑最大的半径，这个问题就会变成“$k$-最小和”问题。这个问题在最近的年份已经受到了越来越多的关注，并且发展了一些近似算法。我们在这篇文章中将研究这个问题在内部维度$d$的欧几何空间中，并假设$k$是常数。在这种情况下，一个PTAS（几何近似算法）已经是知道的（见Bandyapadhyay等（SoCG, 2023））。我们的目标是将这个知识库扩展到公平聚集领域。我们研究了许多公平群组征（例如，由Chierichetti等（NeurIPS, 2017）引入的一个征），这些征定义了输入点的额外特征（例如颜色），并且要求群 preserve 输入点的特征比率（例如，每个群中的红色和蓝色点的比率和输入点集合中的比率相同）。在文章中，我们提出了一个PTAS для公平$k$-最小和问题，这是首个知道的PTAS。它适用于不同的公平群组征。
</details></li>
</ul>
<hr>
<h2 id="ObjectLab-Automated-Diagnosis-of-Mislabeled-Images-in-Object-Detection-Data"><a href="#ObjectLab-Automated-Diagnosis-of-Mislabeled-Images-in-Object-Detection-Data" class="headerlink" title="ObjectLab: Automated Diagnosis of Mislabeled Images in Object Detection Data"></a>ObjectLab: Automated Diagnosis of Mislabeled Images in Object Detection Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00832">http://arxiv.org/abs/2309.00832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cleanlab/cleanlab">https://github.com/cleanlab/cleanlab</a></li>
<li>paper_authors: Ulyana Tkachenko, Aditya Thyagarajan, Jonas Mueller</li>
<li>for: 提高对象检测模型的训练品质，减少对象检测数据集中的注释错误对模型的影响。</li>
<li>methods: 提出了一种简单的算法 ObjectLab，可以检测多种对象检测标签的错误，包括：过look bounding box、错误地标注 bounding box 和类别标签错误。ObjectLab 利用任何已经训练过的对象检测模型来评估图像标签质量，以便自动优先级化 incorrectly labeled 图像进行标签审查&#x2F;修正。</li>
<li>results: 在不同的对象检测数据集（包括 COCO）和不同的模型（包括 Detectron-X101 和 Faster-RCNN）上，ObjectLab 能够准确地检测注释错误，与其他标签质量分数相比，具有更高的准确率&#x2F;回归率。<details>
<summary>Abstract</summary>
Despite powering sensitive systems like autonomous vehicles, object detection remains fairly brittle in part due to annotation errors that plague most real-world training datasets. We propose ObjectLab, a straightforward algorithm to detect diverse errors in object detection labels, including: overlooked bounding boxes, badly located boxes, and incorrect class label assignments. ObjectLab utilizes any trained object detection model to score the label quality of each image, such that mislabeled images can be automatically prioritized for label review/correction. Properly handling erroneous data enables training a better version of the same object detection model, without any change in existing modeling code. Across different object detection datasets (including COCO) and different models (including Detectron-X101 and Faster-RCNN), ObjectLab consistently detects annotation errors with much better precision/recall compared to other label quality scores.
</details>
<details>
<summary>摘要</summary>
尽管它用于激活敏感系统，如自动驾驶车辆，但 object detection 仍然相对脆弱，一大部分这是因为实际训练集中的注释错误。我们提议 ObjectLab，一种简单的算法，用于检测多种对象检测标签的错误，包括：忽略的 bounding box、不当位置的 box 和错误的类别标签分配。ObjectLab 使用任何已经训练过 object detection 模型来评估每个图像的标签质量，以便自动优先级检查和修正错误标签。正确处理错误数据可以训练更好的同一个 object detection 模型，无需更改现有的模型代码。在不同的对象检测集（包括 COCO）和不同的模型（包括 Detectron-X101 和 Faster-RCNN）上，ObjectLab 可以准确地检测注释错误，与其他标签质量分数相比，具有更高的精度/准确率。
</details></li>
</ul>
<hr>
<h2 id="Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding"><a href="#Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding" class="headerlink" title="Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding"></a>Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00816">http://arxiv.org/abs/2309.00816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min-Jeong Kim, Yeon-Chang Lee, David Y. Kang, Sang-Wook Kim</li>
<li>for: 本研究旨在提出一种基于图 convolutional networks (GCN) 的签名网络嵌入方法（SNE），以便在真实世界中更好地表示签名网络中的节点。</li>
<li>methods: 提出的方法基于GCN，并利用签名网络中边缘的可信度来 corrections 嵌入传递。该方法包括三个模块：生成每个节点的扩展EGO网络、测量边缘上的可信度、以及可信度感知的嵌入传递。</li>
<li>results: 实验结果表明，TrustSGCN 在四个真实世界签名网络数据集上 consistently 超越了五种 state-of-the-art GCN-based SNE 方法。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/kmj0792/TrustSGCN">https://github.com/kmj0792/TrustSGCN</a> 上获取。<details>
<summary>Abstract</summary>
The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCN) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node's extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known societal theories, i.e., balance and status. The experiments on four real-world signed network datasets demonstrate that TrustSGCN consistently outperforms five state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN.
</details>
<details>
<summary>摘要</summary>
“signed network embedding（SNE）问题在最近几年内吸引了广泛关注。虽然基于图 convolutional networks（GCN）的多种SNE方法已经被提出来解决这个问题，但我们发现这些方法具有假设decades-old balance theory总是在实际中成立的假设。为了解决这个限制，我们提出了一种基于GCN的新型SNE方法，名为TrustSGCN，它通过利用边签上的信任程度来更正GCN中的嵌入传播错误。提案的方法包括三个模块：（M1）每个节点的扩展EGO网络生成；（M2）边签上的信任程度测量；以及（M3）基于信任程度的嵌入传播。此外，TrustSGCN通过利用社会理论中的平衡和社会Status两个理论来学习节点嵌入。实验表明，TrustSGCN在四个真实的签记网络dataset上与五种state-of-the-art GCN-based SNE方法相比，具有更高的性能。代码可以在https://github.com/kmj0792/TrustSGCN中下载。”
</details></li>
</ul>
<hr>
<h2 id="Bypassing-the-Simulator-Near-Optimal-Adversarial-Linear-Contextual-Bandits"><a href="#Bypassing-the-Simulator-Near-Optimal-Adversarial-Linear-Contextual-Bandits" class="headerlink" title="Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits"></a>Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00814">http://arxiv.org/abs/2309.00814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Chen-Yu Wei, Julian Zimmert</li>
<li>for:  solves the adversarial linear contextual bandit problem with fully adversarially selected loss vectors and a fixed distribution for the per-round action set.</li>
<li>methods:  uses a new algorithm that achieves a regret of $\widetilde{O}(\sqrt{T})$ without a simulator, while maintaining computational efficiency when the action set in each round is small.</li>
<li>results:  greatly improves the existing results, which either require access to a simulator or achieve a sub-optimal regret no better than $\widetilde{O}(T^{\frac{5}{6})$.<details>
<summary>Abstract</summary>
We consider the adversarial linear contextual bandit problem, where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. Existing methods for this problem either require access to a simulator to generate free i.i.d. contexts, achieve a sub-optimal regret no better than $\widetilde{O}(T^{\frac{5}{6})$, or are computationally inefficient. We greatly improve these results by achieving a regret of $\widetilde{O}(\sqrt{T})$ without a simulator, while maintaining computational efficiency when the action set in each round is small. In the special case of sleeping bandits with adversarial loss and stochastic arm availability, our result answers affirmatively the open question by Saha et al. [2020] on whether there exists a polynomial-time algorithm with $poly(d)\sqrt{T}$ regret. Our approach naturally handles the case where the loss is linear up to an additive misspecification error, and our regret shows near-optimal dependence on the magnitude of the error.
</details>
<details>
<summary>摘要</summary>
我们考虑了敌对的线性上下文强化问题，loss вектор会被完全敌对选择，并且每次action集（即context）是从固定分布中抽出的。现有的方法对这个问题可能需要访问一个模拟器来生成免费i.i.d.context，仅实现了较差的对抗 regret，不超过 $\widetilde{O}(T^{\frac{5}{6})$。我们大幅改进了这些结果，在不需要模拟器的情况下，实现了 regret的 $\widetilde{O}(\sqrt{T})$，并且维持了每轮action集的小型化。在睡眠问题中，我们的结果答案了 Saha et al. [2020] 的开问题，而且我们的方法自然地处理了loss是线性的情况，并且 regret的对抗性取决于错误的大小。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making"><a href="#Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making" class="headerlink" title="Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making"></a>Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00805">http://arxiv.org/abs/2309.00805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Rehill, Nicholas Biddle</li>
<li>for: 这篇论文主要是为了探讨 causal machine learning 方法如何生成不同类型的减震效应估计，以及这些方法如何在政策决策中应用。</li>
<li>methods: 论文使用了 standard AI Fairness 方法，但认为这些方法不适用于所有 causal machine learning 应用。在描述 indirect 和 direct 决策场景时，论文提出了一种定义 fairness，即一个模型需要为决级者提供准确的政策结果估计。</li>
<li>results: 论文认为，因为 causal machine learning 模型的复杂性，可能会难以实现 fairness。而不是使用传统的 AI Fairness 调整，论文提出了一种解决方案，即通过细心模型和决策偏见的认知来解决这些问题。<details>
<summary>Abstract</summary>
Causal machine learning methods which flexibly generate heterogeneous treatment effect estimates could be very useful tools for governments trying to make and implement policy. However, as the critical artificial intelligence literature has shown, governments must be very careful of unintended consequences when using machine learning models. One way to try and protect against unintended bad outcomes is with AI Fairness methods which seek to create machine learning models where sensitive variables like race or gender do not influence outcomes. In this paper we argue that standard AI Fairness approaches developed for predictive machine learning are not suitable for all causal machine learning applications because causal machine learning generally (at least so far) uses modelling to inform a human who is the ultimate decision-maker while AI Fairness approaches assume a model that is making decisions directly. We define these scenarios as indirect and direct decision-making respectively and suggest that policy-making is best seen as a joint decision where the causal machine learning model usually only has indirect power. We lay out a definition of fairness for this scenario - a model that provides the information a decision-maker needs to accurately make a value judgement about just policy outcomes - and argue that the complexity of causal machine learning models can make this difficult to achieve. The solution here is not traditional AI Fairness adjustments, but careful modelling and awareness of some of the decision-making biases that these methods might encourage which we describe.
</details>
<details>
<summary>摘要</summary>
政府可以使用可变性机器学习方法来生成不同类型的对待方法估计，这些工具可能很有用于政策制定和实施。然而，如critical artificial intelligence文献所示，政府应该非常小心不良后果，因为机器学习模型可能会导致不良后果。为了保护 against不良后果，可以使用AI Fairness方法，该方法旨在创建不会根据敏感变量（如种族或性别）影响结果的机器学习模型。在这篇论文中，我们 argue that标准的AI Fairness方法（developed for predictive machine learning）不适用于所有 causal machine learning应用程序，因为 causal machine learning通常（至少到目前为止）使用模型来辅助人类决策者做出决定。我们将这些情况分为直接决策和间接决策两种方式，并认为政策决策是间接决策的混合决策。我们提出了一定的公平定义，即一个模型需要提供决策者需要准确判断公平政策结果的信息。我们也 argue that causal machine learning模型的复杂性可能使得这件事件困难实现。因此，不是使用传统的AI Fairness调整，而是需要仔细的模型和决策者偏见的意识。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Inverse-Problems"><a href="#Deep-Learning-and-Inverse-Problems" class="headerlink" title="Deep Learning and Inverse Problems"></a>Deep Learning and Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00802">http://arxiv.org/abs/2309.00802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics">https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics</a></li>
<li>paper_authors: Ali Mohammad-Djafari, Ning Chu, Li Wang, Liang Yu</li>
<li>for: 这篇论文主要针对 inverse problems 问题，即在 indirect measurement 下获取准确的解决方案。</li>
<li>methods: 该论文使用 Machine Learning (ML) 和 Deep Learning (DL) 方法来解决 inverse problems，特别是 Convolutional Neural Network (CNN) 和 Deep Neural Network (DNN)。</li>
<li>results: 该论文提出了两种方法来解决 inverse problems：第一种是使用 known forward operator 作为物理约束，第二种是使用 data-driven DL 方法。<details>
<summary>Abstract</summary>
Machine Learning (ML) methods and tools have gained great success in many data, signal, image and video processing tasks, such as classification, clustering, object detection, semantic segmentation, language processing, Human-Machine interface, etc. In computer vision, image and video processing, these methods are mainly based on Neural Networks (NN) and in particular Convolutional NN (CNN), and more generally Deep NN. Inverse problems arise anywhere we have indirect measurement. As, in general, those inverse problems are ill-posed, to obtain satisfactory solutions for them needs prior information. Different regularization methods have been proposed, where the problem becomes the optimization of a criterion with a likelihood term and a regularization term. The main difficulty, however, in great dimensional real applications, remains the computational cost. Using NN, and in particular Deep Learning (DL) surrogate models and approximate computation, can become very helpful. In this work, we focus on NN and DL particularly adapted for inverse problems. We consider two cases: First the case where the forward operator is known and used as physics constraint, the second more general data driven DL methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="League-of-Legends-Real-Time-Result-Prediction"><a href="#League-of-Legends-Real-Time-Result-Prediction" class="headerlink" title="League of Legends: Real-Time Result Prediction"></a>League of Legends: Real-Time Result Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02449">http://arxiv.org/abs/2309.02449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jailson B. S. Junior, Claudio E. C. Campelo</li>
<li>for: 预测LoL电子游戏match结果 using机器学习技术</li>
<li>methods: 使用未发表数据，评估不同变量和比赛阶段的准确率</li>
<li>results: 使用LightGBM模型获得了81.62%的平均准确率，Logistic Regression和Gradient Boosting模型在早期比赛阶段表现出色<details>
<summary>Abstract</summary>
This paper presents a study on the prediction of outcomes in matches of the electronic game League of Legends (LoL) using machine learning techniques. With the aim of exploring the ability to predict real-time results, considering different variables and stages of the match, we highlight the use of unpublished data as a fundamental part of this process. With the increasing popularity of LoL and the emergence of tournaments, betting related to the game has also emerged, making the investigation in this area even more relevant. A variety of models were evaluated and the results were encouraging. A model based on LightGBM showed the best performance, achieving an average accuracy of 81.62\% in intermediate stages of the match when the percentage of elapsed time was between 60\% and 80\%. On the other hand, the Logistic Regression and Gradient Boosting models proved to be more effective in early stages of the game, with promising results. This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.
</details>
<details>
<summary>摘要</summary>
The study evaluates various machine learning models and achieves encouraging results. The LightGBM model performed best, with an average accuracy of 81.62% in intermediate stages of the match when the elapsed time was between 60% and 80%. On the other hand, Logistic Regression and Gradient Boosting models were more effective in early stages of the game, with promising results.This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction"><a href="#Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction" class="headerlink" title="Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction"></a>Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00783">http://arxiv.org/abs/2309.00783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu</li>
<li>for: 本研究提出了一种基于扩散模型的图像重建方法，用于多极MRI和量化MRI重建。</li>
<li>methods: 该方法利用频率和参数领域中的域Conditioned扩散模型，并使用原始MRI物理作为嵌入，以确保数据一致性和特征学习。</li>
<li>results: 该方法在高速因素下的图像重建中表现出了显著的承诺，并在不同的生物结构中维持了高精度和效率。此外，该方法还具有普适性，可以应用于不同领域的反向问题。<details>
<summary>Abstract</summary>
This study introduces a novel approach for image reconstruction based on a diffusion model conditioned on the native data domain. Our method is applied to multi-coil MRI and quantitative MRI reconstruction, leveraging the domain-conditioned diffusion model within the frequency and parameter domains. The prior MRI physics are used as embeddings in the diffusion model, enforcing data consistency to guide the training and sampling process, characterizing MRI k-space encoding in MRI reconstruction, and leveraging MR signal modeling for qMRI reconstruction. Furthermore, a gradient descent optimization is incorporated into the diffusion steps, enhancing feature learning and improving denoising. The proposed method demonstrates a significant promise, particularly for reconstructing images at high acceleration factors. Notably, it maintains great reconstruction accuracy and efficiency for static and quantitative MRI reconstruction across diverse anatomical structures. Beyond its immediate applications, this method provides potential generalization capability, making it adaptable to inverse problems across various domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction"><a href="#Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction" class="headerlink" title="Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction"></a>Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00781">http://arxiv.org/abs/2309.00781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Rodriguez Dominguez, Muhammad Shahzad, Xia Hong</li>
<li>For: The paper is written for addressing the challenge of multi-modal regression, which is important in forecasting nonstationary processes or with a complex mixture of distributions.* Methods: The paper proposes an ensemble of multiple hypotheses predictors, called a Structured Radial Basis Function Network, which can efficiently interpolate a tessellation of centroidal Voronoi tessellations and approximate the multiple hypotheses target distribution.* Results: The paper achieves superior generalization performance and computational efficiency using only two-layer neural networks as predictors, and introduces a gradient-descent approach that is loss-agnostic regarding the predictors. The experiments show outperformance with respect to the top competitors in the literature.<details>
<summary>Abstract</summary>
Multi-modal regression is important in forecasting nonstationary processes or with a complex mixture of distributions. It can be tackled with multiple hypotheses frameworks but with the difficulty of combining them efficiently in a learning model. A Structured Radial Basis Function Network is presented as an ensemble of multiple hypotheses predictors for regression problems. The predictors are regression models of any type that can form centroidal Voronoi tessellations which are a function of their losses during training. It is proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation formation with the losses of individual predictors. A closed-form solution with least-squares is presented, which to the authors knowledge, is the fastest solution in the literature for multiple hypotheses and structured predictions. Superior generalization performance and computational efficiency is achieved using only two-layer neural networks as predictors controlling diversity as a key component of success. A gradient-descent approach is introduced which is loss-agnostic regarding the predictors. The expected value for the loss of the structured model with Gaussian basis functions is computed, finding that correlation between predictors is not an appropriate tool for diversification. The experiments show outperformance with respect to the top competitors in the literature.
</details>
<details>
<summary>摘要</summary>
多Modal重要预测非站ARY进程或复杂的混合分布。可以使用多个假设框架，但是将其组合到学习模型中是困难的。一种结构化基于几何函数网络的集成多个假设预测器被提出，该模型可以准确地 interpolate这些几何函数网络，并且等于 interpolating 多个假设目标分布。这个模型有一个固定点迭代算法，该算法在预测器和基函数中心之间进行迭代。可控制学习的多样性参数地 truncate 几何函数网络的形成，以控制各个预测器的损失。提供一个快速的关于多个假设和结构预测的关闭形式解，该解使用只有两层神经网络作为预测器，并控制多样性为成功的关键Component。引入一种梯度下降方法，该方法不关于预测器的损失。计算结构模型在 Gaussian 基函数下的预期损失值，发现相互 correlate 不是适用于多样化的工具。实验表明，该模型在文献中的竞争对手之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Feature-Masking-Open-Vocabulary-Vision-Transformer"><a href="#Contrastive-Feature-Masking-Open-Vocabulary-Vision-Transformer" class="headerlink" title="Contrastive Feature Masking Open-Vocabulary Vision Transformer"></a>Contrastive Feature Masking Open-Vocabulary Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00775">http://arxiv.org/abs/2309.00775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dahun Kim, Anelia Angelova, Weicheng Kuo<br>for:CFM-ViT is written for open-vocabulary object detection (OVD) tasks, with the goal of simultaneously learning image- and region-level representation.methods:CFM-ViT combines the masked autoencoder (MAE) objective with contrastive learning, and performs reconstruction in the joint image-text embedding space. Additionally, Positional Embedding Dropout (PED) is introduced to address scale variation during pretraining.results:CFM-ViT achieves state-of-the-art performance on the LVIS open-vocabulary detection benchmark, with an AP$r$ of 33.9 and outperforms the best approach by 7.6 points. It also achieves strong image-level representation on zero-shot image-text retrieval benchmarks, outperforming the state of the art on 8 out of 12 metrics.Here is the simplified Chinese text for the three key points:for:CFM-ViT 是为开放词汇对象检测任务而写的，目标是同时学习图像和区域水平表示。methods:CFM-ViT 将MAE objective与冲突学习相结合，并在图像和文本嵌入空间进行重建。此外，我们还引入Positional Embedding Dropout (PED)来Address scale variation during pretraining。results:CFM-ViT 在 LVIS 开放词汇检测标准 benchmark 上实现了状态机器的表现，AP$r$ 为 33.9，超过最佳方法的 7.6 点。它还在零扫预测 benchmark 上实现了强大的图像水平表示，在 12 个 metric 中击败了状态机器的表现。<details>
<summary>Abstract</summary>
We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an image-text pretraining methodology that achieves simultaneous learning of image- and region-level representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Unlike standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space as is customary with the classical MAE method, which causes the model to better learn region-level semantics. Moreover, we introduce Positional Embedding Dropout (PED) to address scale variation between image-text pretraining and detection finetuning by randomly dropping out the positional embeddings during pretraining. PED improves detection performance and enables the use of a frozen ViT backbone as a region classifier, preventing the forgetting of open-vocabulary knowledge during detection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT achieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6 points and achieves better zero-shot detection transfer. Finally, CFM-ViT acquires strong image-level representation, outperforming the state of the art on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.
</details>
<details>
<summary>摘要</summary>
我们提出了异构特征压缩视Transformer（CFM-ViT），一种图像和文本预训练方法，实现同时学习图像和区域水平表示。我们的方法将掩码自动编码（MAE）目标 integrates 到对比学习目标中，以提高本地化任务的表示。与标准 MAE 不同，我们在图像-文本嵌入空间中进行重建，而不是在像素空间中，这使得模型更好地学习区域水平 semantics。此外，我们引入了Positional Embedding Dropout（PED），以Address scale variation between image-text pretraining and detection finetuning。PED 可以在预训练中随机drop out positional embeddings，从而提高检测性能并使用冻结的 ViT 底层作为区域分类器，避免在检测finetuning中忘记开放词汇知识。在 LVIS 开放词汇检测标准 benchmark 上，CFM-ViT 实现了33.9 AP$r$ 的状态机器，比最佳方法提高7.6个点，并在零容量检测转移中表现出色。最后，CFM-ViT 获得了强大的图像级别表示，在零容量图像-文本检索标准 benchmark 上超过了状态机器的8个 из 12个维度。
</details></li>
</ul>
<hr>
<h2 id="Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models"><a href="#Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models" class="headerlink" title="Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models"></a>Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00771">http://arxiv.org/abs/2309.00771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changyu Liu, Yuling Jiao, Junhui Wang, Jian Huang</li>
<li>for: 评估 robust 估计器的性能 based on adversarial losses under misspecified models.</li>
<li>methods: 使用 adversarial risk 作为评估指标，并在 certain smoothness conditions 下显示 adversarial risk 等价于 distributional adversarial attack 所引起的风险。</li>
<li>results: 提供了一种 general approach 来评估 robust 估计器的性能，并在不同的 loss functions 上Establish non-asymptotic upper bounds for the adversarial excess risk. 另外, 在 classification 和 regression 问题中应用了这些总结结果.<details>
<summary>Abstract</summary>
We propose a general approach to evaluating the performance of robust estimators based on adversarial losses under misspecified models. We first show that adversarial risk is equivalent to the risk induced by a distributional adversarial attack under certain smoothness conditions. This ensures that the adversarial training procedure is well-defined. To evaluate the generalization performance of the adversarial estimator, we study the adversarial excess risk. Our proposed analysis method includes investigations on both generalization error and approximation error. We then establish non-asymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions. In addition, we apply our general results to adversarial training for classification and regression problems. For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over those for a general loss.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的方法来评估robust预测器的性能基于反对抗攻击的损失函数。我们首先表明了反对抗攻击风险与一定的光滑性Conditions下相等。这使得反对抗训练程序得到定义。为评估反对抗预测器的普遍性性能，我们研究了反对抗剩余风险。我们的提出分析方法包括对普遍错误和近似错误进行研究。我们然后建立了非假设性上的上限 bound для反对抗剩余风险，其中包括 Lipschitz 损失函数。此外，我们应用了我们的一般结论到反对抗训练 для分类和回归问题。对于非 Parametric 回归中的quadratic损失函数，我们表明了对反对抗剩余风险 bound可以超过一般损失函数的bound。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Fairness-in-Large-Language-Models-A-Survey"><a href="#Bias-and-Fairness-in-Large-Language-Models-A-Survey" class="headerlink" title="Bias and Fairness in Large Language Models: A Survey"></a>Bias and Fairness in Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00770">http://arxiv.org/abs/2309.00770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i-gallegos/fair-llm-benchmark">https://github.com/i-gallegos/fair-llm-benchmark</a></li>
<li>paper_authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed</li>
<li>for: 本文旨在概述和分析大型自然语言模型（LLM）中存在的社会偏见，以及如何评估和 mitigate 这些偏见。</li>
<li>methods: 本文提出了一种三分类系统来评估和 Mitigate LLM 中的社会偏见，包括：在模型预处理阶段、在训练阶段、在处理阶段和后处理阶段进行干预。</li>
<li>results: 本文总结了现有研究的结果，并提供了一个清晰的指南，以帮助研究人员和实践者更好地理解和预防 LLM 中的社会偏见。<details>
<summary>Abstract</summary>
Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLMs）的快速进步使得处理、理解和生成人类语言文本的能力得到了显著提高，并在社会圈中的系统中得到了普遍应用。然而，这些模型可以学习、传播和增强社会偏见。在这篇论文中，我们提供了对偏见评估和mitigation技术的全面评论。我们首先将社会偏见和公平在自然语言处理中的概念进行了整合、正式化和扩展，并定义了不同类型的危害和公平的感知。然后，我们提出了三种直观的分类，即评估 metric分类、数据集分类和mitigation技术分类。我们的首个评估 metric分类将 metric 和评估数据集之间的关系清晰地分类，并将 metric 分为模型中不同水平的三种：嵌入、概率和生成文本。我们的第二个数据集分类将数据集分为对应不同社会团体的 counterfactual 输入或提示，并标识目标危害和社会群体。我们还发布了一个汇总的公共可用数据集，以便更好地访问。我们的第三个mitigation技术分类将方法分为在预处理、训练、内部处理和后处理中进行的不同类型的 intervención，并且在每个类型中分配了细分类，以阐明研究趋势。最后，我们 indentified 未解决的问题和挑战，以便未来的研究。通过汇总了广泛的最新研究，我们希望通过这篇文章，为研究人员和实践人员提供一份清晰的指南，以便更好地理解和预防 LLMs 中的偏见传播。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids"><a href="#Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids" class="headerlink" title="Physics-informed machine learning of the correlation functions in bulk fluids"></a>Physics-informed machine learning of the correlation functions in bulk fluids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00767">http://arxiv.org/abs/2309.00767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqian Chen, Peiyuan Gao, Panos Stinis</li>
<li>for: 本研究使用机器学习模型解决了奥尔谢因-泽尼克方程（OZ方程），以解决现代积分方程理论中的液体独立方程。</li>
<li>methods: 本研究使用了物理学习模型，包括物理学习神经网络和物理学习神经操作网络，解决OZ方程的前向和反向问题。</li>
<li>results: 研究发现，物理学习模型在解决OZ方程问题上具有高准确率和高效率，并且可以应用于热力学状态理论中。<details>
<summary>Abstract</summary>
The Ornstein-Zernike (OZ) equation is the fundamental equation for pair correlation function computations in the modern integral equation theory for liquids. In this work, machine learning models, notably physics-informed neural networks and physics-informed neural operator networks, are explored to solve the OZ equation. The physics-informed machine learning models demonstrate great accuracy and high efficiency in solving the forward and inverse OZ problems of various bulk fluids. The results highlight the significant potential of physics-informed machine learning for applications in thermodynamic state theory.
</details>
<details>
<summary>摘要</summary>
“欧兹方程”（Ornstein-Zernike equation）是现代液体积分方程论中Computations的基本方程。在这种工作中，我们使用机器学习模型，主要是物理学习核网络和物理学习运算器网络，解决欧兹方程的前向和反向问题。这些物理学习模型在解决欧兹方程的问题上显示了极高的准确性和高效率。结果表明物理学习可以在热力学状态论中应用。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/cs.LG_2023_09_02/" data-id="clmjn91mu007v0j889w6jge3k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/02/cs.SD_2023_09_02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-09-02
        
      </div>
    </a>
  
  
    <a href="/2023/09/02/eess.IV_2023_09_02/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-09-02</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
