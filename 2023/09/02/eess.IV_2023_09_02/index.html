
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-02 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00995 repo_url: https:&#x2F;&#x2F;github.com&#x2F;xfsun99&#x2F;CCycleGAN-TF2 p">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-02">
<meta property="og:url" content="https://nullscc.github.io/2023/09/02/eess.IV_2023_09_02/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00995 repo_url: https:&#x2F;&#x2F;github.com&#x2F;xfsun99&#x2F;CCycleGAN-TF2 p">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-02T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:14.931Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/eess.IV_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T09:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-02
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Constrained-CycleGAN-for-Effective-Generation-of-Ultrasound-Sector-Images-of-Improved-Spatial-Resolution"><a href="#Constrained-CycleGAN-for-Effective-Generation-of-Ultrasound-Sector-Images-of-Improved-Spatial-Resolution" class="headerlink" title="Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution"></a>Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00995">http://arxiv.org/abs/2309.00995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xfsun99/CCycleGAN-TF2">https://github.com/xfsun99/CCycleGAN-TF2</a></li>
<li>paper_authors: Xiaofei Sun, He Li, Wei-Ning Lee<br>for: 这种研究旨在将多个适配器探针生成的声学影像翻译成具有更高空间分辨率的影像。methods: 该研究提出了一种受限制的循环GAN（CCycleGAN），该模型直接使用不同探针生成的声学影像进行生成。 CCycleGAN 还引入了一个标准的反对抗损失和循环一致性损失，以及一个基于声学反射信号的相似度损失来约束结构一致性和反射信号特征的保持。results: 在室内phantom实验中，CCycleGAN 成功地生成了具有更高空间分辨率和更高PSNR和SSIM值的影像。此外，CCycleGAN-生成的人体心脏动力学运动估计也比 benchmarks-生成的更高质量，特别是在深部区域。<details>
<summary>Abstract</summary>
Objective. A phased or a curvilinear array produces ultrasound (US) images with a sector field of view (FOV), which inherently exhibits spatially-varying image resolution with inferior quality in the far zone and towards the two sides azimuthally. Sector US images with improved spatial resolutions are favorable for accurate quantitative analysis of large and dynamic organs, such as the heart. Therefore, this study aims to translate US images with spatially-varying resolution to ones with less spatially-varying resolution. CycleGAN has been a prominent choice for unpaired medical image translation; however, it neither guarantees structural consistency nor preserves backscattering patterns between input and generated images for unpaired US images. Approach. To circumvent this limitation, we propose a constrained CycleGAN (CCycleGAN), which directly performs US image generation with unpaired images acquired by different ultrasound array probes. In addition to conventional adversarial and cycle-consistency losses of CycleGAN, CCycleGAN introduces an identical loss and a correlation coefficient loss based on intrinsic US backscattered signal properties to constrain structural consistency and backscattering patterns, respectively. Instead of post-processed B-mode images, CCycleGAN uses envelope data directly obtained from beamformed radio-frequency signals without any other non-linear postprocessing. Main Results. In vitro phantom results demonstrate that CCycleGAN successfully generates images with improved spatial resolution as well as higher peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) compared with benchmarks. Significance. CCycleGAN-generated US images of the in vivo human beating heart further facilitate higher quality heart wall motion estimation than benchmarks-generated ones, particularly in deep regions.
</details>
<details>
<summary>摘要</summary>
目标：使用phaseless或弯曲阵列生成ultrasound（US）图像，图像具有扇形观察领域（FOV），但图像在远区和两侧方向上具有逐渐递减的分辨率，导致图像质量下降。为了提高US图像的分辨率，本研究目的是将US图像翻译成具有更高分辨率的图像。CyclGAN是一种常用的无对应医学图像翻译方法，但它并不保证结构一致性也不保持各自的反射特征。方法：我们提出一种受限的CyclGAN（CCycleGAN），它直接将不同探针阵列生成US图像。CCycleGAN还添加了与普通的对抗搅拌和环状一致性损失相同的损失，以保持结构一致性和反射特征。而不是使用后处理的B模式图像，CCycleGAN使用直接从干扰信号中获得的扩散数据，无需任何其他非线性后处理。主要结果：在室内照相器中，CCycleGAN成功地生成图像，具有更高的分辨率和PSNR（干扰信号强度比）以及SSIM（结构相似度）。意义：CCycleGAN-生成的US图像在人体内部活跃心脏的区域更好地估计心墙运动，特别是深部区域。
</details></li>
</ul>
<hr>
<h2 id="AdLER-Adversarial-Training-with-Label-Error-Rectification-for-One-Shot-Medical-Image-Segmentation"><a href="#AdLER-Adversarial-Training-with-Label-Error-Rectification-for-One-Shot-Medical-Image-Segmentation" class="headerlink" title="AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation"></a>AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00971">http://arxiv.org/abs/2309.00971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsiangyuzhao/AdLER">https://github.com/hsiangyuzhao/AdLER</a></li>
<li>paper_authors: Xiangyu Zhao, Sheng Wang, Zhiyun Song, Zhenrong Shen, Linlin Yao, Haolei Yuan, Qian Wang, Lichi Zhang</li>
<li>for: 这个研究的目的是提高医疗影像自动分类的精度和可靠性，特别是在临床 Setting中，where limited training data is available.</li>
<li>methods: 这个方法使用一击分类（OSSLT），包括不supervised deformable registration、data augmentation with learned registration、和从增强的数据进行分类。但是现有的一击分类方法受到有限的数据多样性和可能的标签错误的影响。</li>
<li>results: 这个研究提出了一个新的一击医疗影像分类方法，利用反对抗训练和标签修正（AdLER），以提高分类性能。实验结果显示，提案的AdLER在CANDI和ABIDE数据集上比前一个 state-of-the-art 方法高出0.7%、3.6%和4.9%的Dice分数，分别。<details>
<summary>Abstract</summary>
Accurate automatic segmentation of medical images typically requires large datasets with high-quality annotations, making it less applicable in clinical settings due to limited training data. One-shot segmentation based on learned transformations (OSSLT) has shown promise when labeled data is extremely limited, typically including unsupervised deformable registration, data augmentation with learned registration, and segmentation learned from augmented data. However, current one-shot segmentation methods are challenged by limited data diversity during augmentation, and potential label errors caused by imperfect registration. To address these issues, we propose a novel one-shot medical image segmentation method with adversarial training and label error rectification (AdLER), with the aim of improving the diversity of generated data and correcting label errors to enhance segmentation performance. Specifically, we implement a novel dual consistency constraint to ensure anatomy-aligned registration that lessens registration errors. Furthermore, we develop an adversarial training strategy to augment the atlas image, which ensures both generation diversity and segmentation robustness. We also propose to rectify potential label errors in the augmented atlas images by estimating segmentation uncertainty, which can compensate for the imperfect nature of deformable registration and improve segmentation authenticity. Experiments on the CANDI and ABIDE datasets demonstrate that the proposed AdLER outperforms previous state-of-the-art methods by 0.7% (CANDI), 3.6% (ABIDE "seen"), and 4.9% (ABIDE "unseen") in segmentation based on Dice scores, respectively. The source code will be available at https://github.com/hsiangyuzhao/AdLER.
</details>
<details>
<summary>摘要</summary>
医疗图像自动分割通常需要大量高质量标注数据，因此在临床设置下难以实施，因为有限的训练数据。基于学习变换的一招分割（OSSLT）已经在具有极少标注数据的情况下显示了抗应用性。然而，当前的一招分割方法受到有限数据多样性的限制，以及可能的标签错误，导致分割性能下降。为解决这些问题，我们提出了一种新的一招医疗图像分割方法，具有对抗训练和标签错误修复（AdLER），以提高分割性能。具体来说，我们实施了一种新的双重一致性约束，以降低注射错误。此外，我们开发了一种对抗训练策略，以增加生成的数据多样性和分割稳定性。此外，我们还提出了修复可能的标签错误的方法，通过估计分割不确定性，以补偿注射注入的不完整性，提高分割 authenticty。实验表明，提出的 AdLER 方法在 CANDI 和 ABIDE 数据集上比前状态之前的方法高出 0.7%（CANDI）、3.6%（ABIDE "seen") 和 4.9%（ABIDE "unseen") 的分割基于 dice 分数，分别。源代码将于 <https://github.com/hsiangyuzhao/AdLER> 上公开。
</details></li>
</ul>
<hr>
<h2 id="S-3-MonoDETR-Supervised-Shape-Scale-perceptive-Deformable-Transformer-for-Monocular-3D-Object-Detection"><a href="#S-3-MonoDETR-Supervised-Shape-Scale-perceptive-Deformable-Transformer-for-Monocular-3D-Object-Detection" class="headerlink" title="S$^3$-MonoDETR: Supervised Shape&amp;Scale-perceptive Deformable Transformer for Monocular 3D Object Detection"></a>S$^3$-MonoDETR: Supervised Shape&amp;Scale-perceptive Deformable Transformer for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00928">http://arxiv.org/abs/2309.00928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan He, Kailun Yang, Junwei Zheng, Jin Yuan, Luis M. Bergasa, Hui Zhang, Zhiyong Li</li>
<li>for: 提高单视图3D物体检测的准确率，特别是对多类目物体的检测。</li>
<li>methods: 提出了一种新的“帮助”（S$^3$-DA）模块，利用视觉和深度特征生成多种形态和比例的多样化本地特征，并同时预测匹配分布，从而赋予每个查询点有价值的形态&amp;比例觉察。</li>
<li>results: 对KITTI和Waymo开放 dataset进行了广泛的实验，显示S$^3$-DA能够显著提高检测精度，在单一训练过程中实现单类和多类3D物体检测的州际最佳性。<details>
<summary>Abstract</summary>
Recently, transformer-based methods have shown exceptional performance in monocular 3D object detection, which can predict 3D attributes from a single 2D image. These methods typically use visual and depth representations to generate query points on objects, whose quality plays a decisive role in the detection accuracy. However, current unsupervised attention mechanisms without any geometry appearance awareness in transformers are susceptible to producing noisy features for query points, which severely limits the network performance and also makes the model have a poor ability to detect multi-category objects in a single training process. To tackle this problem, this paper proposes a novel "Supervised Shape&Scale-perceptive Deformable Attention" (S$^3$-DA) module for monocular 3D object detection. Concretely, S$^3$-DA utilizes visual and depth features to generate diverse local features with various shapes and scales and predict the corresponding matching distribution simultaneously to impose valuable shape&scale perception for each query. Benefiting from this, S$^3$-DA effectively estimates receptive fields for query points belonging to any category, enabling them to generate robust query features. Besides, we propose a Multi-classification-based Shape$\&$Scale Matching (MSM) loss to supervise the above process. Extensive experiments on KITTI and Waymo Open datasets demonstrate that S$^3$-DA significantly improves the detection accuracy, yielding state-of-the-art performance of single-category and multi-category 3D object detection in a single training process compared to the existing approaches. The source code will be made publicly available at https://github.com/mikasa3lili/S3-MonoDETR.
</details>
<details>
<summary>摘要</summary>
近期，基于转换器的方法在单视图3D对象检测中表现出色，可以从单个2D图像中预测3D属性。这些方法通常使用视觉和深度表示来生成查询点对象，其质量决定了检测精度。然而，当前无监督的注意机制无法捕捉物体的几何形态特征，导致生成查询点的特征具有噪声，从而限制网络性能，同时使得模型在单个训练过程中检测多个类别对象的能力很差。为解决这个问题，这篇论文提出了一种新的“supervised shape&scale-perceptive deformable attention”（S$^3$-DA）模块。具体来说，S$^3$-DA使用视觉和深度特征来生成多种形态和比例的多种本地特征，并同时预测匹配分布，以便对每个查询点进行有价值的形态&比例感知。由此，S$^3$-DA可以fficiently估算查询点所属类别的接受范围，使其能生成 Robust 的查询特征。此外，我们提出了一种基于多类别匹配的Shape$\&$Scale Matching（MSM）损失函数来监督上述过程。经验表明，S$^3$-DA可以显著提高检测精度，在单个训练过程中实现单个类别和多个类别3D对象检测的状态机器人性能。代码将在https://github.com/mikasa3lili/S3-MonoDETR中公开。
</details></li>
</ul>
<hr>
<h2 id="Correlated-and-Multi-frequency-Diffusion-Modeling-for-Highly-Under-sampled-MRI-Reconstruction"><a href="#Correlated-and-Multi-frequency-Diffusion-Modeling-for-Highly-Under-sampled-MRI-Reconstruction" class="headerlink" title="Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction"></a>Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00853">http://arxiv.org/abs/2309.00853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/cm-dm">https://github.com/yqx7150/cm-dm</a></li>
<li>paper_authors: Yu Guan, Chuanming Yu, Shiyu Lu, Zhuoxu Cui, Dong Liang, Qiegen Liu<br>for:This paper aims to improve the accuracy of Magnetic Resonance Imaging (MRI) reconstruction by leveraging the properties of k-space data and the diffusion process to preserve fine texture details in the reconstructed image.methods:The proposed method uses a combination of multi-frequency prior and high-frequency prior extractors to mine the multi-frequency prior and preserve fine texture details in the reconstructed image. Additionally, the method uses a diffusion process to accelerate the sampling process.results:The proposed method successfully obtains more accurate reconstruction and outperforms state-of-the-art methods, as verified by experimental results.<details>
<summary>Abstract</summary>
Most existing MRI reconstruction methods perform tar-geted reconstruction of the entire MR image without tak-ing specific tissue regions into consideration. This may fail to emphasize the reconstruction accuracy on im-portant tissues for diagnosis. In this study, leveraging a combination of the properties of k-space data and the diffusion process, our novel scheme focuses on mining the multi-frequency prior with different strategies to pre-serve fine texture details in the reconstructed image. In addition, a diffusion process can converge more quickly if its target distribution closely resembles the noise distri-bution in the process. This can be accomplished through various high-frequency prior extractors. The finding further solidifies the effectiveness of the score-based gen-erative model. On top of all the advantages, our method improves the accuracy of MRI reconstruction and accel-erates sampling process. Experimental results verify that the proposed method successfully obtains more accurate reconstruction and outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的MRI重建方法大多都是针对整个MRI图像进行targeted重建，而不考虑特定组织区域的重建精度。这可能会导致重建精度不足。在本研究中，我们运用了k-space数据的特性和扩散过程的优化方法，专注于保留重建图像中的细节Texture。此外，扩散过程可以更快地落实，如果其目标分布类似于处理中的噪音分布。这可以通过多种高频率优化抽取器来实现。发现更加强化了Score-based生成模型的有效性。除了所有的优点，我们的方法可以提高MRI重建精度和加速抽取过程。实验结果显示，我们的方法成功地取得了更高精度的重建和超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Data-driven-and-Anatomically-Constrained-Deep-Learning-Image-Registration-for-Adult-and-Fetal-Echocardiography"><a href="#Multi-scale-Data-driven-and-Anatomically-Constrained-Deep-Learning-Image-Registration-for-Adult-and-Fetal-Echocardiography" class="headerlink" title="Multi-scale, Data-driven and Anatomically Constrained Deep Learning Image Registration for Adult and Fetal Echocardiography"></a>Multi-scale, Data-driven and Anatomically Constrained Deep Learning Image Registration for Adult and Fetal Echocardiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00831">http://arxiv.org/abs/2309.00831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kamruleee51/ddc-ac-dlir">https://github.com/kamruleee51/ddc-ac-dlir</a></li>
<li>paper_authors: Md. Kamrul Hasan, Haobo Zhu, Guang Yang, Choon Hwai Yap<br>for: 这个论文的目的是提高电子医学图像注册的精度和稳定性，以便更好地估计心脏运动、肌肉弹性评估和心脏膜量测量。methods: 这篇论文提出了一种基于深度学习的图像注册方法，包括使用适应性损失函数保持图像的生物学可能性和图像质量，以及在成人和胎儿电子医学图像上进行多尺度训练。results: 实验结果表明，这种方法可以在成人和胎儿电子医学图像上提供优秀的注册结果，并且比传统的非深度学习标准注册方法（如摩托流和弹性投影）更精度和稳定。这些结果表明，这种方法可以提高心脏运动量的估计精度，并且可能有翻译性。<details>
<summary>Abstract</summary>
Temporal echocardiography image registration is a basis for clinical quantifications such as cardiac motion estimation, myocardial strain assessments, and stroke volume quantifications. In past studies, deep learning image registration (DLIR) has shown promising results and is consistently accurate and precise, requiring less computational time. We propose that a greater focus on the warped moving image's anatomic plausibility and image quality can support robust DLIR performance. Further, past implementations have focused on adult echocardiography, and there is an absence of DLIR implementations for fetal echocardiography. We propose a framework that combines three strategies for DLIR in both fetal and adult echo: (1) an anatomic shape-encoded loss to preserve physiological myocardial and left ventricular anatomical topologies in warped images; (2) a data-driven loss that is trained adversarially to preserve good image texture features in warped images; and (3) a multi-scale training scheme of a data-driven and anatomically constrained algorithm to improve accuracy. Our tests show that good anatomical topology and image textures are strongly linked to shape-encoded and data-driven adversarial losses. They improve different aspects of registration performance in a non-overlapping way, justifying their combination. Despite fundamental distinctions between adult and fetal echo images, we show that these strategies can provide excellent registration results in both adult and fetal echocardiography using the publicly available CAMUS adult echo dataset and our private multi-demographic fetal echo dataset. Our approach outperforms traditional non-DL gold standard registration approaches, including Optical Flow and Elastix. Registration improvements could be translated to more accurate and precise clinical quantification of cardiac ejection fraction, demonstrating a potential for translation.
</details>
<details>
<summary>摘要</summary>
Temporal echo医学像registration是临床量化的基础，如心动量估计、肌动强度评估和心脏血量量化。在过去的研究中，深度学习图像registratin（DLIR）已经表现出了承诺的结果，需要更少的计算时间。我们建议更重视扭曲移动图像的解剖学可能性和图像质量，以支持Robust DLIR性能。此外，过去的实现都是成人 echo， absence of DLIR实现对妊娠 echo。我们提出了一个框架， combinesthree策略来实现DLIR：（1）适应解剖形状编码损失，以保持生理physiological myocardial和左心脏解剖特征在扭曲图像中;（2）基于数据驱动的损失，通过对扭曲图像进行对抗训练，以保持好的图像特征;（3）基于多尺度的数据驱动和解剖限制的算法来提高准确性。我们的测试表明，解剖学可能性和图像特征是紧密相关的，这些损失可以在不相互干扰的情况下提高不同方面的registratin性能。尽管成人和妊娠 echo图像存在fundamental的区别，我们的方法可以在两种不同的echo图像上提供优秀的registratin结果，使用公共可用的CAMUS成人echo数据集和我们私有的多demographic妊娠echo数据集。我们的方法超过了传统的非DL金标 registration方法，包括激光流和Elastix。registration改进可以翻译到更准确和精确的临床量化， demonstrating a potential for translation。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer"><a href="#Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer" class="headerlink" title="Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer"></a>Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00800">http://arxiv.org/abs/2309.00800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Chen, Xiao Chen, Yikang Liu, Eric Z. Chen, Terrence Chen, Shanhui Sun</li>
<li>for:  This paper aims to improve the accuracy of left ventricle (LV), right ventricle (RV), and LV myocardium (MYO) segmentation in Cardiac Magnetic Resonance imaging (CMR) images.</li>
<li>methods:  The proposed method uses a classifier-guided two-stage network with an all-slice fusion transformer to enhance CMR segmentation accuracy, particularly in basal and apical slices.</li>
<li>results:  The proposed method demonstrated better performance in terms of Dice score compared to previous CNN-based and transformer-based models, and produced visually appealing segmentation shapes resembling human annotations.<details>
<summary>Abstract</summary>
Cardiac Magnetic Resonance imaging (CMR) is the gold standard for assessing cardiac function. Segmenting the left ventricle (LV), right ventricle (RV), and LV myocardium (MYO) in CMR images is crucial but time-consuming. Deep learning-based segmentation methods have emerged as effective tools for automating this process. However, CMR images present additional challenges due to irregular and varying heart shapes, particularly in basal and apical slices. In this study, we propose a classifier-guided two-stage network with an all-slice fusion transformer to enhance CMR segmentation accuracy, particularly in basal and apical slices. Our method was evaluated on extensive clinical datasets and demonstrated better performance in terms of Dice score compared to previous CNN-based and transformer-based models. Moreover, our method produces visually appealing segmentation shapes resembling human annotations and avoids common issues like holes or fragments in other models' segmentations.
</details>
<details>
<summary>摘要</summary>
心脏磁共振成像（CMR）是评估心脏功能的标准。在CMR图像中，正确地分割左心室（LV）、右心室（RV）和心肺肉（MYO）是关键，但是时间消耗很长。深度学习基于的分割方法在 automatize 这个过程中表现出了有效性。然而，CMR图像受到心形状的变化和不规则性的影响，特别是在基层和腰层图像中。在这项研究中，我们提议一种基于分类器的两stage网络，使用所有slice fusions transformer来提高CMR分割精度，特别是在基层和腰层图像中。我们的方法在丰富的临床数据集上进行了评估，并表现出了与前一代CNN基于和transformer基于模型相比的更好的性能， measured by dice score。此外，我们的方法生成的分割形状更加可观，与人工标注更加相似，而不会出现其他模型中的孔洞或 Fragment 问题。
</details></li>
</ul>
<hr>
<h2 id="Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera"><a href="#Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera" class="headerlink" title="Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera"></a>Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00787">http://arxiv.org/abs/2309.00787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cheng, Siyang Cao</li>
<li>For: 提高自适应驾驶和自动机器人系统的准确性和可靠性，通过雷达和摄像头系统的协同报告来实现更好的系统性能。* Methods: 利用深度学习提取雷达数据和摄像头图像的共同特征，然后使用这些共同特征来匹配雷达和摄像头系统中的同一个目标。通过这种方法，可以实现无目标的在线协同准确性报告。* Results: 通过实验示范，提出的方法可以准确地匹配雷达和摄像头系统中的同一个目标，并且可以提高准确性和可靠性。<details>
<summary>Abstract</summary>
Sensor fusion is essential for autonomous driving and autonomous robots, and radar-camera fusion systems have gained popularity due to their complementary sensing capabilities. However, accurate calibration between these two sensors is crucial to ensure effective fusion and improve overall system performance. Calibration involves intrinsic and extrinsic calibration, with the latter being particularly important for achieving accurate sensor fusion. Unfortunately, many target-based calibration methods require complex operating procedures and well-designed experimental conditions, posing challenges for researchers attempting to reproduce the results. To address this issue, we introduce a novel approach that leverages deep learning to extract a common feature from raw radar data (i.e., Range-Doppler-Angle data) and camera images. Instead of explicitly representing these common features, our method implicitly utilizes these common features to match identical objects from both data sources. Specifically, the extracted common feature serves as an example to demonstrate an online targetless calibration method between the radar and camera systems. The estimation of the extrinsic transformation matrix is achieved through this feature-based approach. To enhance the accuracy and robustness of the calibration, we apply the RANSAC and Levenberg-Marquardt (LM) nonlinear optimization algorithm for deriving the matrix. Our experiments in the real world demonstrate the effectiveness and accuracy of our proposed method.
</details>
<details>
<summary>摘要</summary>
感知融合是自动驾驶和自动机器人的关键技术，而各种各样的探测器融合系统已经得到了广泛应用。然而，为了实现有效的探测融合，准确的均衡化是非常重要。均衡化包括内在均衡和外在均衡，其中外在均衡尤其重要，以确保探测器之间的准确匹配。然而，许多目标基本均衡方法需要复杂的操作程序和优化的实验条件，这会让研究人员很难复制结果。为解决这个问题，我们提出了一种新的方法，利用深度学习来提取各种探测器数据中的共同特征（即距离-Doppler-角度数据）和摄像头图像。而不是直接表示这些共同特征，我们的方法即使利用这些共同特征来匹配探测器数据和摄像头图像中的同一个目标。具体来说，提取的共同特征可以作为一个在线无目标均衡方法的示例，用于 estimating 探测器和摄像头系统之间的外在变换矩阵。为了提高准确性和稳定性，我们使用RANSAC和Levenberg-Marquardt（LM）非线性优化算法来 derivation 矩阵。我们在实际情况中进行了实验，并证明了我们提出的方法的有效性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Full-Reference-Video-Quality-Assessment-for-Machine-Learning-Based-Video-Codecs"><a href="#Full-Reference-Video-Quality-Assessment-for-Machine-Learning-Based-Video-Codecs" class="headerlink" title="Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs"></a>Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00769">http://arxiv.org/abs/2309.00769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abrar Majeedi, Babak Naderi, Yasaman Hosseinkashi, Juhee Cho, Ruben Alvarez Martinez, Ross Cutler</li>
<li>for: 这 paper 的目的是提出一种新的视频编码评估方法，以便更好地评估基于机器学习的视频编码器。</li>
<li>methods: 该 paper 使用了一种新的数据集和一种基于 FRVQA 模型来评估机器学习视频编码器的质量。</li>
<li>results: 该 paper 提出了一种新的评估方法，其 Pearson 相关系数和Spearman 排名相关系数都达到了 0.99，可以帮助更好地评估基于机器学习的视频编码器。<details>
<summary>Abstract</summary>
Machine learning-based video codecs have made significant progress in the past few years. A critical area in the development of ML-based video codecs is an accurate evaluation metric that does not require an expensive and slow subjective test. We show that existing evaluation metrics that were designed and trained on DSP-based video codecs are not highly correlated to subjective opinion when used with ML video codecs due to the video artifacts being quite different between ML and video codecs. We provide a new dataset of ML video codec videos that have been accurately labeled for quality. We also propose a new full reference video quality assessment (FRVQA) model that achieves a Pearson Correlation Coefficient (PCC) of 0.99 and a Spearman's Rank Correlation Coefficient (SRCC) of 0.99 at the model level. We make the dataset and FRVQA model open source to help accelerate research in ML video codecs, and so that others can further improve the FRVQA model.
</details>
<details>
<summary>摘要</summary>
machine learning基于的视频编码器在过去几年中做出了重大进步。一个关键的发展领域是一个准确的评估指标，不需要贵重和慢的主观测试。我们表明了现有的评估指标，由DSP基于的视频编码器而设计和训练的，与ML视频编码器不具有高相关性，因为视频artefacts在ML和视频编码器之间很不同。我们提供了一个新的ML视频编码器视频集，已经准确地标注了质量。我们还提议一种全参照视频质量评估（FRVQA）模型，实现了Pearson相关系数（PCC）0.99和Spearman排名相关系数（SRCC）0.99的模型水平。我们将数据集和FRVQA模型开源，以便加速ML视频编码器的研究，并让其他人可以进一步改进FRVQA模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/eess.IV_2023_09_02/" data-id="clmjn91qt00hf0j887ehe5ri8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/02/cs.LG_2023_09_02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-02
        
      </div>
    </a>
  
  
    <a href="/2023/09/01/cs.SD_2023_09_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-09-01</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
