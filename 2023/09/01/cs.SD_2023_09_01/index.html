
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00454 repo_url: https:&#x2F;&#x2F;github.com&#x2F;topel&#x2F;audioset-convnext-inf paper">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-01">
<meta property="og:url" content="https://nullscc.github.io/2023/09/01/cs.SD_2023_09_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00454 repo_url: https:&#x2F;&#x2F;github.com&#x2F;topel&#x2F;audioset-convnext-inf paper">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-01T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:14.425Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.SD_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T15:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding"><a href="#CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding" class="headerlink" title="CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding"></a>CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00454">http://arxiv.org/abs/2309.00454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topel/audioset-convnext-inf">https://github.com/topel/audioset-convnext-inf</a></li>
<li>paper_authors: Étienne Labbé, Thomas Pellegrini, Julien Pinquier</li>
<li>For: The paper focuses on developing an automated audio captioning (AAC) model using a ConvNeXt architecture as the audio encoder, which is adapted from the vision domain to audio classification. The model is designed to generate natural language descriptions of audio content using encoder-decoder architectures.* Methods: The proposed model, called CNext-trans, uses a ConvNeXt architecture as the audio encoder, which is combined with a Transformer decoder for caption generation. The model is trained on multiple AAC datasets, including AC, CL, MACS, and WavCaps, to improve cross-dataset performance. The authors also introduced a Task Embedding (TE) token to identify the source dataset for each input sample and improve the model’s performance.* Results: The proposed model, named CoNeTTE, achieved state-of-the-art scores on the AudioCaps (AC) dataset and performed competitively on Clotho (CL) while using four to forty times fewer parameters than existing models. The model also showed the ability to generate more accurate and informative captions by incorporating dataset-specific Task Embeddings. The SPIDEr scores of the proposed model were 44.1% and 30.5% on AC and CL, respectively.<details>
<summary>Abstract</summary>
Automated Audio Captioning (AAC) involves generating natural language descriptions of audio content, using encoder-decoder architectures. An audio encoder produces audio embeddings fed to a decoder, usually a Transformer decoder, for caption generation. In this work, we describe our model, which novelty, compared to existing models, lies in the use of a ConvNeXt architecture as audio encoder, adapted from the vision domain to audio classification. This model, called CNext-trans, achieved state-of-the-art scores on the AudioCaps (AC) dataset and performed competitively on Clotho (CL), while using four to forty times fewer parameters than existing models. We examine potential biases in the AC dataset due to its origin from AudioSet by investigating unbiased encoder's impact on performance. Using the well-known PANN's CNN14, for instance, as an unbiased encoder, we observed a 1.7% absolute reduction in SPIDEr score (where higher scores indicate better performance). To improve cross-dataset performance, we conducted experiments by combining multiple AAC datasets (AC, CL, MACS, WavCaps) for training. Although this strategy enhanced overall model performance across datasets, it still fell short compared to models trained specifically on a single target dataset, indicating the absence of a one-size-fits-all model. To mitigate performance gaps between datasets, we introduced a Task Embedding (TE) token, allowing the model to identify the source dataset for each input sample. We provide insights into the impact of these TEs on both the form (words) and content (sound event types) of the generated captions. The resulting model, named CoNeTTE, an unbiased CNext-trans model enriched with dataset-specific Task Embeddings, achieved SPIDEr scores of 44.1% and 30.5% on AC and CL, respectively. Code available: https://github.com/Labbeti/conette-audio-captioning.
</details>
<details>
<summary>摘要</summary>
自动化音频描述（AAC）技术涉及生成音频内容的自然语言描述，使用编码器-解码器架构。音频编码器生成音频嵌入，并将其传递给解码器进行描述生成。在这种工作中，我们描述了我们的模型，其特点在于使用ConvNeXt架构作为音频编码器，从视觉领域中适应音频分类。这个模型被称为CNext-trans，在AudioCaps（AC）数据集上实现了状态机器的分数，并在Clotho（CL）数据集上表现竞争力强，而使用四到四十个参数少于现有模型。我们调查了AC数据集中可能的偏见，并证明使用不偏见的编码器对性能有负面的影响。使用PANN的CNN14，例如，作为不偏见的编码器，我们观察到了相对于SPIDEr分数的1.7%绝对下降。为提高跨数据集性能，我们进行了多个AAC数据集（AC、CL、MACS、WavCaps）的训练。虽然这种策略提高了总模型性能，但仍然落后于特定目标数据集上训练的模型，表明没有一size-fits-all的模型。为 Mitigate performance gaps between datasets，我们引入了任务嵌入（TE）token，让模型可以确定输入样本的源数据集。我们对TE的影响进行了深入的分析，并发现TE对描述（words）和内容（声音事件类型）的生成caption均有正面的影响。最终，我们提出了CoNeTTE模型，即不偏见的CNext-trans模型，以及数据集特定的任务嵌入，实现了SPIDEr分数的44.1%和30.5%在AC和CL数据集上。代码可以在https://github.com/Labbeti/conette-audio-captioning中下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining"><a href="#Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining" class="headerlink" title="Learning Speech Representation From Contrastive Token-Acoustic Pretraining"></a>Learning Speech Representation From Contrastive Token-Acoustic Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00424">http://arxiv.org/abs/2309.00424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Yixin Tian, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这个研究是为了提出一种名为“对照掌握（CTAP）”的方法，用于将语音和字幕带到一个共同多modal空间中，以学习将语音和字幕连接在帧级上。</li>
<li>methods: 这个方法使用两个Encoder来将语音和字幕带到一个共同多modal空间中，并通过对帧级的连接学习，以提高下游任务的精确性。</li>
<li>results: 这个CTAP方法在210k语音和字幕文本对中训练，并在无监督的情况下进行语音转换、自动话语识别和文本读取等下游任务，获得了良好的成果。<details>
<summary>Abstract</summary>
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representations extracted from speech should serve as a "bridge" between text and acoustic information, containing information from both modalities. The semantic content is emphasized, while the paralinguistic information such as speaker identity and acoustic details should be de-emphasized. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Contrastive learning is a good method for modeling intermediate representations from two modalities. However, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named "Contrastive Token-Acoustic Pretraining (CTAP)", which uses two encoders to bring phoneme and speech into a joint multimodal space, learning how to connect phoneme and speech at the frame level. The CTAP model is trained on 210k speech and phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a promising solution for fine-grained generation and recognition downstream tasks in speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTFor fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representations extracted from speech should serve as a "bridge" between text and acoustic information, containing information from both modalities. The semantic content is emphasized, while the paralinguistic information such as speaker identity and acoustic details should be de-emphasized. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Contrastive learning is a good method for modeling intermediate representations from two modalities. However, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named "Contrastive Token-Acoustic Pretraining (CTAP)", which uses two encoders to bring phoneme and speech into a joint multimodal space, learning how to connect phoneme and speech at the frame level. The CTAP model is trained on 210k speech and phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a promising solution for fine-grained generation and recognition downstream tasks in speech processing.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Remixing-based-Unsupervised-Source-Separation-from-Scratch"><a href="#Remixing-based-Unsupervised-Source-Separation-from-Scratch" class="headerlink" title="Remixing-based Unsupervised Source Separation from Scratch"></a>Remixing-based Unsupervised Source Separation from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00376">http://arxiv.org/abs/2309.00376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Saijo, Tetsuji Ogawa</li>
<li>for: 本研究旨在提出一种无监督的方法，用于从零开始训练分离模型，使用 RecmixIT 和 Self-Remixing 等自动学习方法来精炼预训练模型。</li>
<li>methods: 本方法首先使用一个教师模型将混合物分离，然后创建 pseudo-混合物，将混合物的信号随机排序和重新混合。学生模型则使用教师模型的输出或初始混合物作为监督来分离 pseudo-混合物。为了改进教师模型的输出，教师模型的参数将被更新为学生模型的参数。</li>
<li>results: 实验结果表明，提议的方法可以超越现有的混合性训练方法，并且可以在无监督情况下训练一个声学分离模型。此外，我们还提出了一种简单的混合方法来稳定训练。<details>
<summary>Abstract</summary>
We propose an unsupervised approach for training separation models from scratch using RemixIT and Self-Remixing, which are recently proposed self-supervised learning methods for refining pre-trained models. They first separate mixtures with a teacher model and create pseudo-mixtures by shuffling and remixing the separated signals. A student model is then trained to separate the pseudo-mixtures using either the teacher's outputs or the initial mixtures as supervision. To refine the teacher's outputs, the teacher's weights are updated with the student's weights. While these methods originally assumed that the teacher is pre-trained, we show that they are capable of training models from scratch. We also introduce a simple remixing method to stabilize training. Experimental results demonstrate that the proposed approach outperforms mixture invariant training, which is currently the only available approach for training a monaural separation model from scratch.
</details>
<details>
<summary>摘要</summary>
我们提出一种无监督的方法，使用RecmixIT和Self-Remixing来训练从头开始的分离模型。这些方法最初将混合物分离成多个信号，然后通过乱序和重新混合这些信号来生成 pseudo-混合物。然后，一个学生模型将被训练来分离 pseudo-混合物，使用教师模型的输出或初始混合物作为监督。为了改善教师的输出，教师的权重将被更新为学生的权重。尽管这些方法原本假设了教师已经预训练，但我们证明它们可以训练模型从头开始。我们还提出了一种简单的混合方法来稳定训练。实验结果表明，我们的方法在训练离合模型从头开始时比混合 invariant 训练更高效。
</details></li>
</ul>
<hr>
<h2 id="Towards-Contrastive-Learning-in-Music-Video-Domain"><a href="#Towards-Contrastive-Learning-in-Music-Video-Domain" class="headerlink" title="Towards Contrastive Learning in Music Video Domain"></a>Towards Contrastive Learning in Music Video Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00347">http://arxiv.org/abs/2309.00347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karel Veldkamp, Mariya Hendriksen, Zoltán Szlávik, Alexander Keijser</li>
<li>for: 本研究是 investigate whether contrastive learning can be applied to the domain of music videos, and evaluate the quality of learned representations on downstream tasks.</li>
<li>methods: 我们使用 dual en-coder for audio and video modalities, and train it using a bidirectional contrastive loss.</li>
<li>results: 我们发现，没有对比学习练化后，pre-trained networks still outperform our contrastive learning approach on both music tagging and genre classification tasks.  Additionally, we perform a qualitative analysis of the learned representations and find that contrastive learning may have difficulties uniting embeddings from two modalities.<details>
<summary>Abstract</summary>
Contrastive learning is a powerful way of learning multimodal representations across various domains such as image-caption retrieval and audio-visual representation learning. In this work, we investigate if these findings generalize to the domain of music videos. Specifically, we create a dual en-coder for the audio and video modalities and train it using a bidirectional contrastive loss. For the experiments, we use an industry dataset containing 550 000 music videos as well as the public Million Song Dataset, and evaluate the quality of learned representations on the downstream tasks of music tagging and genre classification. Our results indicate that pre-trained networks without contrastive fine-tuning outperform our contrastive learning approach when evaluated on both tasks. To gain a better understanding of the reasons contrastive learning was not successful for music videos, we perform a qualitative analysis of the learned representations, revealing why contrastive learning might have difficulties uniting embeddings from two modalities. Based on these findings, we outline possible directions for future work. To facilitate the reproducibility of our results, we share our code and the pre-trained model.
</details>
<details>
<summary>摘要</summary>
“对比学习是一种强大的多modal表现学习方法，应用于不同领域，如图像描述和多频道表现学习。在这个工作中，我们探索是否这些发现可以应用到音乐类别中。我们建立了一个双向的en-coder，用于音频和视频模式，并使用对称对应损失来训练。我们使用了550,000首音乐录影带和公共的百万首歌曲Dataset，并评估学习的表现质量在下游任务中，包括音乐标签和流派分类。我们发现，无需对照精练，预训练的网络在两个任务中都能表现出色。为了更好地理解为什么对照学习无法成功地融合两个模式的嵌入，我们进行了质量分析，发现对照学习可能会面临两个模式之间的问题。基于这些发现，我们提出了未来的可能的方向。为了促进我们的结果的重复性，我们分享了我们的代码和预训练模型。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training"><a href="#Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training" class="headerlink" title="Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training"></a>Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00284">http://arxiv.org/abs/2309.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohuan Zhou, Xu Li, Zhiyong Wu, Ying Shan, Helen Meng</li>
<li>for: 提高单声道合唱voice synthesis（SVS）的音高范围和音质相似性</li>
<li>methods: 使用多个 singer 预训练方法，包括预测器来生成框架级别的音频信息和歌手编码器来模型不同歌手的声音变化</li>
<li>results: 提高单声道合唱voice synthesis（SVS）的音质和节奏自然性，并超过基线的评价<details>
<summary>Abstract</summary>
The single-speaker singing voice synthesis (SVS) usually underperforms at pitch values that are out of the singer's vocal range or associated with limited training samples. Based on our previous work, this work proposes a melody-unsupervised multi-speaker pre-training method conducted on a multi-singer dataset to enhance the vocal range of the single-speaker, while not degrading the timbre similarity. This pre-training method can be deployed to a large-scale multi-singer dataset, which only contains audio-and-lyrics pairs without phonemic timing information and pitch annotation. Specifically, in the pre-training step, we design a phoneme predictor to produce the frame-level phoneme probability vectors as the phonemic timing information and a speaker encoder to model the timbre variations of different singers, and directly estimate the frame-level f0 values from the audio to provide the pitch information. These pre-trained model parameters are delivered into the fine-tuning step as prior knowledge to enhance the single speaker's vocal range. Moreover, this work also contributes to improving the sound quality and rhythm naturalness of the synthesized singing voices. It is the first to introduce a differentiable duration regulator to improve the rhythm naturalness of the synthesized voice, and a bi-directional flow model to improve the sound quality. Experimental results verify that the proposed SVS system outperforms the baseline on both sound quality and naturalness.
</details>
<details>
<summary>摘要</summary>
通常情况下，单个说话者的声音合成（SVS）在声音值出现在说话者的 vocal range 之外或与有限的训练样本有关时会下降性能。基于我们的前一项工作，这项工作提出了一种不含调谱信息和普通时间信息的多说话者预训练方法，以提高单个说话者的声音范围，不会影响声音相似性。这种预训练方法可以应用于大规模多说话者数据集，只包含音频和歌词对。具体来说，在预训练阶段，我们设计了一个音频预测器，以生成帧级别的音频概率向量作为调谱信息，并使用说话者编码器来模型不同说话者的声音变化。这些预训练模型参数在精度阶段作为先知知识传递给单个说话者的声音合成，以提高其声音范围。此外，这项工作还贡献到了改善合成唱 voz 的音质和节奏自然性。它是首次引入了可导的时间控制器，以改善合成唱 voz 的节奏自然性，并使用双向流模型，以改善音质。实验结果表明，提案的 SVS 系统在音质和自然性两个指标上都超过了基线。
</details></li>
</ul>
<hr>
<h2 id="The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge"><a href="#The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge" class="headerlink" title="The FruitShell French synthesis system at the Blizzard 2023 Challenge"></a>The FruitShell French synthesis system at the Blizzard 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00223">http://arxiv.org/abs/2309.00223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Qi, Xiaopeng Wang, Zhiyong Wang, Wang Liu, Mingming Ding, Shuchen Shi</li>
<li>for: 这个论文是为了参加2023年的Blizzard Challenge提交的法语文本识别系统。</li>
<li>methods: 作者使用了一种基于VITS的声学模型和hifigan vocoder，并在模型中包含了发音人信息。</li>
<li>results: 论文的评估结果显示，Hub任务的质量MOS分数为3.6，Spoke任务的质量MOS分数为3.4，与所有参赛队伍的平均水平相当。<details>
<summary>Abstract</summary>
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data. Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model. The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details>
<details>
<summary>摘要</summary>
Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience.For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data.Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model.The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.SD_2023_09_01/" data-id="clmjn91o900bl0j8846zrep2x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/02/eess.IV_2023_09_02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-09-02
        
      </div>
    </a>
  
  
    <a href="/2023/09/01/cs.LG_2023_09_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-09-01</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
