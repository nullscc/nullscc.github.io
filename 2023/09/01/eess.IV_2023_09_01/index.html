
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-09-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00755 repo_url: None paper_authors: Ruiz">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-09-01">
<meta property="og:url" content="https://nullscc.github.io/2023/09/01/eess.IV_2023_09_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.00755 repo_url: None paper_authors: Ruiz">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-01T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-14T20:38:14.429Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/eess.IV_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T09:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-09-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction"><a href="#High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction" class="headerlink" title="High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction"></a>High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00755">http://arxiv.org/abs/2309.00755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhi Cao, Cheng Shen, Changhuei Yang<br>for: 这篇论文旨在提出一种新的计算成像方法，可以高效地生成高分辨率、大领域视图的扭差自由图像。methods: 该方法使用多个倾斜照射来实现高速成像，并使用新的分析式phas retrieval框架来重建复杂的场。results: 该方法可以高效地重建复杂的场，并且可以Analytically retrieve complex aberrations of an imaging system with no additional hardware。此外，该方法比FPM更具robust性，可以Address aberration whose maximal phase difference exceeds 3.8π。<details>
<summary>Abstract</summary>
Computational imaging methods empower modern microscopy with the ability of producing high-resolution, large field-of-view, aberration-free images. One of the dominant computational label-free imaging methods, Fourier ptychographic microscopy (FPM), effectively increases the spatial-bandwidth product of conventional microscopy by using multiple tilted illuminations to achieve high-throughput imaging. However, its iterative reconstruction method is prone to parameter selection, can be computationally expensive and tends to fail under excessive aberrations. Recently, spatial Kramers-Kronig methods show it is possible to analytically reconstruct complex field but lacks the ability of correcting aberrations or providing extended resolution enhancement. Here, we present a closed-form method, termed APIC, which weds the strengths of both methods. A new analytical phase retrieval framework is established in APIC, which demonstrates, for the first time, the feasibility of analytically reconstructing the complex field associated with darkfield measurements. In addition, APIC can analytically retrieve complex aberrations of an imaging system with no additional hardware. By avoiding iterative algorithms, APIC requires no human designed convergence metric and always obtains a closed-form complex field solution. The faithfulness and correctness of APIC's reconstruction are guaranteed due to its analytical nature. We experimentally demonstrate that APIC gives correct reconstruction result while FPM fails to do so when constrained to the same number of measurements. Meanwhile, APIC achieves 2.8 times faster computation using image tile size of 256 (length-wise). We also demonstrate APIC is unprecedentedly robust against aberrations compared to FPM - APIC is capable of addressing aberration whose maximal phase difference exceeds 3.8${\pi}$ when using a NA 0.25 objective in experiment.
</details>
<details>
<summary>摘要</summary>
现代微镜技术得到了计算影像方法的支持，使得可以生成高分辨率、广领域视图、不受扭曲影响的图像。其中一种主要的计算无标签影像方法是干扰ptychographic microscopy（FPM），通过多个倾斜照明来实现高速度成像。然而，它的迭代重构方法容易选择参数，计算成本高，并且容易在过度扭曲下失败。最近，空间卡尔斯-克里戈夫方法表示可以分析重构复杂场景，但缺乏修正扭曲或提供扩展的分辨率提升能力。在这里，我们提出一种封闭式方法，名为APIC，它将传统的干扰ptychographic microscopy和空间卡尔斯-克里戈夫方法的优点相结合。我们建立了一个新的分析阶段重构框架，可以分析性地重构复杂场景相关的黑场测量数据。此外，APIC可以分析性地回收测试系统的扭曲 aberrations，无需额外硬件。由于不需要迭代算法，APIC不需要人工设置迭代约束，并且总是获得一个封闭式复杂场景解决方案。由于APIC的分析性质，它的重构结果具有真实性和正确性。我们在实验中证明，APIC可以在相同数量的测量下提供正确的重构结果，而FPM则失败。此外，APIC在计算速度方面也具有优势，使用图像分割大小为256时，计算速度高于FPM2.8倍。最后，我们还证明APIC在扭曲度较大时具有前所未有的稳定性，可以修正扭曲率达到3.8$\pi$的最大值，使用NA 0.25 objective在实验中。
</details></li>
</ul>
<hr>
<h2 id="Indexing-Irises-by-Intrinsic-Dimension"><a href="#Indexing-Irises-by-Intrinsic-Dimension" class="headerlink" title="Indexing Irises by Intrinsic Dimension"></a>Indexing Irises by Intrinsic Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00705">http://arxiv.org/abs/2309.00705</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Michael Rozmus</li>
<li>for: 这个论文是为了提高人脸识别精度和速度而写的。</li>
<li>methods: 论文使用了主成分分析（PCA）将一小部分的正常化萝卜图像转换成四维内在空间，以便快速识别不知名人员。</li>
<li>results: 论文测试结果表明，使用这种方法可以快速准确地识别人脸，通常只需要比较少量的数据库。<details>
<summary>Abstract</summary>
28,000+ high-quality iris images of 1350 distinct eyes from 650+ different individuals from a relatively diverse university town population were collected. A small defined unobstructed portion of the normalized iris image is selected as a key portion for quickly identifying an unknown individual when submitting an iris image to be matched to a database of enrolled irises of the 1350 distinct eyes. The intrinsic dimension of a set of these key portions of the 1350 enrolled irises is measured to be about four (4). This set is mapped to a four-dimensional intrinsic space by principal components analysis (PCA). When an iris image is presented to the iris database for identification, the search begins in the neighborhood of the location of its key portion in the 4D intrinsic space, typically finding a correct identifying match after comparison to only a few percent of the database.
</details>
<details>
<summary>摘要</summary>
“我们收集了650+名不同个体的大学城区人口中的1350个不同眼睛的28,000多个高品质豢眼图像。我们从这些豢眼图像中选择了一小部分作为快速识别不知名人的关键部分，这个部分通常位于豢眼图像的一小部分。我们使用主成分分析（PCA）将这些关键部分映射到四维的内在空间中。当将豢眼图像提交至豢眼数据库进行识别时，搜寻从该豢眼图像的关键部分的 neighboorhood开始，通常可以在几%的豢眼数据库中找到正确的识别匹配。”
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm"><a href="#A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm" class="headerlink" title="A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm"></a>A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00514">http://arxiv.org/abs/2309.00514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanyi Wang, Pin Cao, Yihui Zhang, Haotian Hu, Yongying Yang</li>
<li>for: 这篇论文主要关注于大口径非球面光学元件表面缺陷探测中，精确地调整光轴与机械转子轴的对接。</li>
<li>methods: 本论文提出了一种机器视觉方法来缓解不对称错误。它首先强调了对于非球面光学元件的对称错误探测中，严重的折冲模糊问题，可能导致缓解失败。因此，本论文提出了一个改进的 Adaptive Enhancement Algorithm (AEA)，包括已有的 Guided Filter Dark Channel Dehazing Algorithm (GFA) 和提出的轻量级 Multi-scale Densely Connected Network (MDC-Net)。</li>
<li>results: 本论文的结果显示，使用 AEA 可以对于不对称错误进行缓解，并且可以在短时间内执行。具体来说，AEA 可以在十个 200 像素 200 像素 Region of Interest (ROI) 图像中，实现了调整光轴与机械转子轴的对接，并且可以将错误降至 10 微米以下。<details>
<summary>Abstract</summary>
In the procedure of surface defects detection for large-aperture aspherical optical elements, it is of vital significance to adjust the optical axis of the element to be coaxial with the mechanical spin axis accurately. Therefore, a machine vision method for eccentric error correction is proposed in this paper. Focusing on the severe defocus blur of reference crosshair image caused by the imaging characteristic of the aspherical optical element, which may lead to the failure of correction, an Adaptive Enhancement Algorithm (AEA) is proposed to strengthen the crosshair image. AEA is consisted of existed Guided Filter Dark Channel Dehazing Algorithm (GFA) and proposed lightweight Multi-scale Densely Connected Network (MDC-Net). The enhancement effect of GFA is excellent but time-consuming, and the enhancement effect of MDC-Net is slightly inferior but strongly real-time. As AEA will be executed dozens of times during each correction procedure, its real-time performance is very important. Therefore, by setting the empirical threshold of definition evaluation function SMD2, GFA and MDC-Net are respectively applied to highly and slightly blurred crosshair images so as to ensure the enhancement effect while saving as much time as possible. AEA has certain robustness in time-consuming performance, which takes an average time of 0.2721s and 0.0963s to execute GFA and MDC-Net separately on ten 200pixels 200pixels Region of Interest (ROI) images with different degrees of blur. And the eccentricity error can be reduced to within 10um by our method.
</details>
<details>
<summary>摘要</summary>
在大开口非球面光元件表面缺陷检测过程中，准确调整光轴和机械轴之间的协调是非常重要。因此，本文提出了一种机器视觉方法来修正轴心错。关注非球面光元件的极大投射模糊的参考十字图像引起的严重模糊，我们提出了适应增强算法（AEA）来加强十字图像。AEA由现有的导引灰度阈值滤波算法（GFA）和提议的轻量级多尺度紧密连接网络（MDC-Net）组成。GFA的增强效果非常出色，但是时间耗费较高，而MDC-Net的增强效果虽然不及GFA，但是快速响应非常重要。因此，我们根据SMD2定义评价函数的实际阈值，将GFA和MDC-Net分别应用于高度和轻度模糊的十字图像，以保证增强效果的同时尽量降低时间成本。AEA具有一定的时间性能稳定性，每个执行AEA的时间平均为0.2721秒和0.0963秒，分别在十个200像素x200像素区域内的不同程度模糊的ROI图像上执行GFA和MDC-Net。而我们的方法可以将轴心错降至10微米以下。
</details></li>
</ul>
<hr>
<h2 id="Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels"><a href="#Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels" class="headerlink" title="Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels"></a>Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00470">http://arxiv.org/abs/2309.00470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Wu, Yulin Shao, Chenghong Bian, Krystian Mikolajczyk, Deniz Gündüz</li>
<li>for: 这个论文旨在应用当代图像传输中的多入多出多元几何（MIMO）通道，并提出一个基于视觉对变换器（ViT）的深度联合源和通道编码（DeepJSCC）方案，并在这个方案中运用自我参考机制来实现对图像和通道条件的智能适应。</li>
<li>methods: 这个方案使用了ViT的自我参考机制来学习特有的图像特征和通道条件，并且运用了 DeepJSCC 架构来实现智能的电力分配和对应映射策略。</li>
<li>results: 实验结果显示，对于各种测试场景，DeepJSCC-MIMO 可以实现优化的图像传输质量，并且具有适应性和稳定性。另外，DeepJSCC-MIMO 还可以在不同的通道条件、通道估计错误和天线数量下显示出优异的性能。<details>
<summary>Abstract</summary>
This paper introduces a vision transformer (ViT)-based deep joint source and channel coding (DeepJSCC) scheme for wireless image transmission over multiple-input multiple-output (MIMO) channels, denoted as DeepJSCC-MIMO. We consider DeepJSCC-MIMO for adaptive image transmission in both open-loop and closed-loop MIMO systems. The novel DeepJSCC-MIMO architecture surpasses the classical separation-based benchmarks with robustness to channel estimation errors and showcases remarkable flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining. Specifically, by harnessing the self-attention mechanism of ViT, DeepJSCC-MIMO intelligently learns feature mapping and power allocation strategies tailored to the unique characteristics of the source image and prevailing channel conditions. Extensive numerical experiments validate the significant improvements in transmission quality achieved by DeepJSCC-MIMO for both open-loop and closed-loop MIMO systems across a wide range of scenarios. Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it an appealing solution for emerging semantic communication systems.
</details>
<details>
<summary>摘要</summary>
The novel DeepJSCC-MIMO architecture surpasses classical separation-based benchmarks and demonstrates robustness to channel estimation errors. It also showcases remarkable flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining.The DeepJSCC-MIMO scheme harnesses the self-attention mechanism of ViT to intelligently learn feature mapping and power allocation strategies tailored to the unique characteristics of the source image and prevailing channel conditions. Extensive numerical experiments confirm significant improvements in transmission quality achieved by DeepJSCC-MIMO for both open-loop and closed-loop MIMO systems across a wide range of scenarios.Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it a promising solution for emerging semantic communication systems.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation"><a href="#Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation" class="headerlink" title="Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation"></a>Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00453">http://arxiv.org/abs/2309.00453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Deniz Bezek, Maxim Haas, Richard Rau, Orcun Goksel<br>for:This paper aims to improve the accuracy of speed-of-sound (SoS) imaging by learning a forward imaging model based on data, rather than relying on hand-crafted models.methods:The proposed method uses a convolutional formulation of the pulse-echo SoS imaging problem, and learns a single unified kernel for the entire field-of-view. Least-squares estimation is used to learn the convolutional kernel, which can be constrained and regularized for numerical stability.results:The proposed method was tested on k-Wave simulations, phantom data, and in-vivo data. Compared to a conventional hand-crafted line-based wave-path model, the simulation-learned model improved the median contrast of SoS reconstructions by 63%, and the phantom-learned model quadrupled the SoS contrast. The in-vivo data results showed impressive 7 and 10 folds contrast improvements over the conventional model.<details>
<summary>Abstract</summary>
Speed-of-sound (SoS) is an emerging ultrasound contrast modality, where pulse-echo techniques using conventional transducers offer multiple benefits. For estimating tissue SoS distributions, spatial domain reconstruction from relative speckle shifts between different beamforming sequences is a promising approach. This operates based on a forward model that relates the sought local values of SoS to observed speckle shifts, for which the associated image reconstruction inverse problem is solved. The reconstruction accuracy thus highly depends on the hand-crafted forward imaging model. In this work, we propose to learn the SoS imaging model based on data. We introduce a convolutional formulation of the pulse-echo SoS imaging problem such that the entire field-of-view requires a single unified kernel, the learning of which is then tractable and robust. We present least-squares estimation of such convolutional kernel, which can further be constrained and regularized for numerical stability. In experiments, we show that a forward model learned from k-Wave simulations improves the median contrast of SoS reconstructions by 63%, compared to a conventional hand-crafted line-based wave-path model. This simulation-learned model generalizes successfully to acquired phantom data, nearly doubling the SoS contrast compared to the conventional hand-crafted alternative. We demonstrate equipment-specific and small-data regime feasibility by learning a forward model from a single phantom image, where our learned model quadruples the SoS contrast compared to the conventional hand-crafted model. On in-vivo data, the simulation- and phantom-learned models respectively exhibit impressive 7 and 10 folds contrast improvements over the conventional model.
</details>
<details>
<summary>摘要</summary>
射频速度（SoS）是一种emerging ultrasound contrast模式，使用普通的探频器提供多种优点。用于估计组织SoS分布的方法之一是在不同探频序列中使用统一的探频器进行缝合，从而从相对的射频扫描shift中推算出组织SoS分布。这借助了一个前向模型，该模型连接射频扫描shift和所需的本地SoS值。实际上，这个问题的解决方案高度取决于手工设计的前向射频模型。在这个研究中，我们提出了基于数据学习的SoS射频模型。我们将探频调频问题表述为一个数据学习问题，并且使用卷积方法来解决。我们提出了一种最小二乘估计方法来学习卷积核心，这个核心可以进一步紧缩和调整以确保数据稳定性。在实验中，我们发现了一个由k-Wave simulations学习的前向模型，与传统手工设计的线性射频模型相比，可以提高SoS重建的中位对比度63%。这个模型可以成功对于真实资料进行扩展，并且在小数据 regime中显示出Equipment-specific和�系统特定的可行性。在人体实验中，我们显示了模型的优秀表现，模型可以在不同的实验设备和资料上显示出7-10倍的SoS对比度提升。
</details></li>
</ul>
<hr>
<h2 id="On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models"><a href="#On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models" class="headerlink" title="On the Localization of Ultrasound Image Slices within Point Distribution Models"></a>On the Localization of Ultrasound Image Slices within Point Distribution Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00372">http://arxiv.org/abs/2309.00372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Bastian, Vincent Bürgin, Ha Young Kim, Alexander Baumann, Benjamin Busam, Mahdi Saleh, Nassir Navab</li>
<li>For: 本研究旨在提高ultrasound图像的自动地位置标定，以便更好地诊断甲状腺疾病。* Methods: 该研究使用了对ultrasound图像和三维形态模型之间的相似性学习，并通过cross-modality registration和Procrustes分析将ultrasound图像注册到个体特定的甲状腺三维形态模型中。* Results: 实验结果表明，该多Modal注册框架可以准确地将ultrasound图像注册到个体特定的甲状腺三维形态模型和统计学模型中，并且可以预测slice的位置在个体特定的甲状腺三维形态模型上的平均误差为1.2毫米，而在统计学模型上的平均误差为4.6毫米。<details>
<summary>Abstract</summary>
Thyroid disorders are most commonly diagnosed using high-resolution Ultrasound (US). Longitudinal nodule tracking is a pivotal diagnostic protocol for monitoring changes in pathological thyroid morphology. This task, however, imposes a substantial cognitive load on clinicians due to the inherent challenge of maintaining a mental 3D reconstruction of the organ. We thus present a framework for automated US image slice localization within a 3D shape representation to ease how such sonographic diagnoses are carried out. Our proposed method learns a common latent embedding space between US image patches and the 3D surface of an individual's thyroid shape, or a statistical aggregation in the form of a statistical shape model (SSM), via contrastive metric learning. Using cross-modality registration and Procrustes analysis, we leverage features from our model to register US slices to a 3D mesh representation of the thyroid shape. We demonstrate that our multi-modal registration framework can localize images on the 3D surface topology of a patient-specific organ and the mean shape of an SSM. Experimental results indicate slice positions can be predicted within an average of 1.2 mm of the ground-truth slice location on the patient-specific 3D anatomy and 4.6 mm on the SSM, exemplifying its usefulness for slice localization during sonographic acquisitions. Code is publically available: \href{https://github.com/vuenc/slice-to-shape}{https://github.com/vuenc/slice-to-shape}
</details>
<details>
<summary>摘要</summary>
thyroid disorders 通常通过高分辨率ultrasound (US) 诊断。 longitudinal nodule tracking 是诊断的关键协议，但是这会对临床医生带来很大的认知压力，因为需要维护一个MENTAL 3D 重建 thyroid 的组织结构。为了解决这个问题，我们提出了一种自动化 US 图像片段位置标定方法。我们的提议方法通过对 US 图像块和个体 thyroid 形状的3D 表示进行对比度学习来学习一个公共几何空间。通过cross-modality 注册和Procrustes分析，我们利用我们的模型特征来注册 US 块到个体 thyroid 形状的3D 网格表示。我们的多Modal 注册框架可以将 US 块位于个体 thyroid 形状的3D 表面 topology 和统计学上的Shape模型 (SSM) 中。我们的实验结果表明，我们可以在1.2 mm 的平均误差内将 US 块位于patient-specific 3D  анатомии和SSM 中的slice位置。代码可以在以下链接中下载：https://github.com/vuenc/slice-to-shape。
</details></li>
</ul>
<hr>
<h2 id="How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis"><a href="#How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis" class="headerlink" title="How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis"></a>How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00350">http://arxiv.org/abs/2309.00350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dewinda Julianensi Rumala</li>
<li>for: 这个研究探讨了医疗影像分析中深度学习模型的数据泄露问题，具体来说是使用3D卷积神经网络（CNNs）进行脑MRI分析。</li>
<li>methods: 这个研究使用了不同的数据分割策略来影响模型性能的探讨，并通过GradCAM可视化来揭示模型中的快捷路径问题。</li>
<li>results: 研究发现，不当的数据分割策略可能会导致模型性能下降，特别是在长itudinal的医疗影像数据中，模型可能会学习到主题以及诊断特征之间的相互关系。<details>
<summary>Abstract</summary>
Deep learning models have revolutionized the field of medical image analysis, offering significant promise for improved diagnostics and patient care. However, their performance can be misleadingly optimistic due to a hidden pitfall called 'data leakage'. In this study, we investigate data leakage in 3D medical imaging, specifically using 3D Convolutional Neural Networks (CNNs) for brain MRI analysis. While 3D CNNs appear less prone to leakage than 2D counterparts, improper data splitting during cross-validation (CV) can still pose issues, especially with longitudinal imaging data containing repeated scans from the same subject. We explore the impact of different data splitting strategies on model performance for longitudinal brain MRI analysis and identify potential data leakage concerns. GradCAM visualization helps reveal shortcuts in CNN models caused by identity confounding, where the model learns to identify subjects along with diagnostic features. Our findings, consistent with prior research, underscore the importance of subject-wise splitting and evaluating our model further on hold-out data from different subjects to ensure the integrity and reliability of deep learning models in medical image analysis.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:深度学习模型已经革命化医疗影像分析领域，提供了 significante 的推荐价值，以提高诊断和患者照顾。然而，它们的性能可能受到隐藏的坑害，即“数据泄露”。在这项研究中，我们调查了3D医疗影像中的数据泄露，specifically using 3D Convolutional Neural Networks (CNNs) for brain MRI analysis。虽然3D CNNs 似乎比2D counterparts 更抵触数据泄露，但是在跨 validate （CV）时不当的数据分割可以仍然存在问题，尤其是longitudinal imaging data 包含了同一个主体多次扫描的情况。我们研究不同数据分割策略对 longitudinal brain MRI 分析中的模型性能的影响，并确定了数据泄露的可能问题。GradCAM 可视化帮助揭示了 CNN 模型中由于identify confounding 而导致的短路，这些短路使模型学习主体以及诊断特征。我们的发现与先前研究一致，强调了subject-wise 分割和在不同主体的 hold-out 数据上进一步评估我们的模型，以确保深度学习模型在医疗影像分析中的完整性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review"><a href="#Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review" class="headerlink" title="Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review"></a>Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00265">http://arxiv.org/abs/2309.00265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatima Al Zegair, Nathasha Naranpanawa, Brigid Betz-Stablein, Monika Janda, H. Peter Soyer, Shekhar S. Chandra</li>
<li>for: 这种研究旨在提高皮肤癌诊断的准确率和 simplify 医疗决策，特别是在皮肤癌专业人员短缺的情况下。</li>
<li>methods: 这种研究使用了计算机支持诊断（CAD）技术，结合了机器学习（ML）和深度学习（DL）技术，以帮助检测皮肤癌和可疑的皮肤变化。</li>
<li>results: 这种研究表明，使用 ML 和 DL 技术可以提高皮肤癌检测的准确率，并且比专业 dermatologist 的性能更高。  Additionally, the study found that using ML and DL techniques can help identify “ugly duckling” naevi, which are naevi that stand out from others in the same individual and may indicate the presence of a cancerous melanoma.<details>
<summary>Abstract</summary>
Skin lesions known as naevi exhibit diverse characteristics such as size, shape, and colouration. The concept of an "Ugly Duckling Naevus" comes into play when monitoring for melanoma, referring to a lesion with distinctive features that sets it apart from other lesions in the vicinity. As lesions within the same individual typically share similarities and follow a predictable pattern, an ugly duckling naevus stands out as unusual and may indicate the presence of a cancerous melanoma. Computer-aided diagnosis (CAD) has become a significant player in the research and development field, as it combines machine learning techniques with a variety of patient analysis methods. Its aim is to increase accuracy and simplify decision-making, all while responding to the shortage of specialized professionals. These automated systems are especially important in skin cancer diagnosis where specialist availability is limited. As a result, their use could lead to life-saving benefits and cost reductions within healthcare. Given the drastic change in survival when comparing early stage to late-stage melanoma, early detection is vital for effective treatment and patient outcomes. Machine learning (ML) and deep learning (DL) techniques have gained popularity in skin cancer classification, effectively addressing challenges, and providing results equivalent to that of specialists. This article extensively covers modern Machine Learning and Deep Learning algorithms for detecting melanoma and suspicious naevi. It begins with general information on skin cancer and different types of naevi, then introduces AI, ML, DL, and CAD. The article then discusses the successful applications of various ML techniques like convolutional neural networks (CNN) for melanoma detection compared to dermatologists' performance. Lastly, it examines ML methods for UD naevus detection and identifying suspicious naevi.
</details>
<details>
<summary>摘要</summary>
皮肤损伤知为“脓肿”，具有不同大小、形状和颜色的特征。“ ugly duckling naevus”这个概念在监测恶性肿瘤方面发挥作用，指的是与周围其他损伤不同的损伤。由于同一个人的损伤通常具有相似的特征和预测的模式，“ ugly duckling naevus”会出现为不寻常的，可能指示恶性肿瘤存在。计算机支持诊断（CAD）在研发领域中扮演着重要角色，它将机器学习技术与多种患者分析方法结合，以提高准确性和简化决策。由于专业人员的匮乏，这些自动化系统在皮肤癌诊断中扮演着越来越重要的角色。因此，它们的使用可能导致生命的拯救和医疗成本的减少。由于晚期皮肤癌的存生差异很大，早期发现是致命的。机器学习（ML）和深度学习（DL）技术在皮肤癌分类中获得了广泛的应用，成功解决了许多挑战，并提供了与专业人员相当的结果。这篇文章从皮肤癌的基础知识和不同类型的脓肿开始，然后介绍了人工智能、机器学习、深度学习和CAD。文章 THEN 讲述了由不同的ML技术，如卷积神经网络（CNN），在恶性肿瘤检测中比专业人员的表现更好。最后，它检查了ML方法在UD脓肿检测和恶性肿瘤检测中的表现。
</details></li>
</ul>
<hr>
<h2 id="Gap-and-Overlap-Detection-in-Automated-Fiber-Placement"><a href="#Gap-and-Overlap-Detection-in-Automated-Fiber-Placement" class="headerlink" title="Gap and Overlap Detection in Automated Fiber Placement"></a>Gap and Overlap Detection in Automated Fiber Placement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00206">http://arxiv.org/abs/2309.00206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assef Ghamisi, Homayoun Najjaran</li>
<li>for: 提高复合材料部件质量，自动检测和修复制造缺陷</li>
<li>methods: 使用Optical Coherence Tomography（OCT）感知器和计算机视觉技术检测和定位复合部件中的异常</li>
<li>results: 实现高精度和高效的异常检测和定位，提高复合部件质量<details>
<summary>Abstract</summary>
The identification and correction of manufacturing defects, particularly gaps and overlaps, are crucial for ensuring high-quality composite parts produced through Automated Fiber Placement (AFP). These imperfections are the most commonly observed issues that can significantly impact the overall quality of the composite parts. Manual inspection is both time-consuming and labor-intensive, making it an inefficient approach. To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps that may exist between them. Any gaps or overlaps exceeding a predefined tolerance threshold are considered manufacturing defects. To evaluate the performance of our approach, we compare the detected defects with the ground truth annotated by experts. The results demonstrate a high level of accuracy and efficiency in gap and overlap segmentation.
</details>
<details>
<summary>摘要</summary>
“composite parts的制造瑕疵排查和修正，特别是空隙和重叠，是 Ensure High-quality Composite Parts produced through Automated Fiber Placement (AFP) 的关键。这些瑕疵是制造瑕疵中最常见的问题，可能对全面质量产生严重的影响。 manual inspection 是时间consuming 和劳动密集的，因此不是可行的方法。 To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps that may exist between them. Any gaps or overlaps exceeding a predefined tolerance threshold are considered manufacturing defects. To evaluate the performance of our approach, we compare the detected defects with the ground truth annotated by experts. The results demonstrate a high level of accuracy and efficiency in gap and overlap segmentation.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/eess.IV_2023_09_01/" data-id="clmjn91qs00hd0j88a8p2bl1j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/01/cs.LG_2023_09_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-09-01
        
      </div>
    </a>
  
  
    <a href="/2023/08/31/cs.SD_2023_08_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
