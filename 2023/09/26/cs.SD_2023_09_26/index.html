
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-09-26 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.15064 repo_url: None paper_authors: Harshvardhan Takawale, Nirupam Roy fo">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-09-26">
<meta property="og:url" content="https://nullscc.github.io/2023/09/26/cs.SD_2023_09_26/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.15064 repo_url: None paper_authors: Harshvardhan Takawale, Nirupam Roy fo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-26T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:20.291Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/cs.SD_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T15:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-09-26
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Simultaneously-Learning-Speaker’s-Direction-and-Head-Orientation-from-Binaural-Recordings"><a href="#Simultaneously-Learning-Speaker’s-Direction-and-Head-Orientation-from-Binaural-Recordings" class="headerlink" title="Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings"></a>Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15064">http://arxiv.org/abs/2309.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshvardhan Takawale, Nirupam Roy</li>
<li>for: 这篇论文的目的是在实现基于EARable设备的应用中，用声学记录来估算说话人和听众的头部方向和orientation。</li>
<li>methods: 该论文提出了一种基于人声直径和听众的头部相关函数（HRTF）的 convolutional neural network 模型，用于同时预测说话人和听众的头部方向。该模型利用听众耳上的微机顺序记录，根据声音的 direktivity 和高频响应特性，来估算说话人和听众的头部方向。</li>
<li>results: 研究人员通过实验证明，该模型可以准确地预测说话人和听众的头部方向，并且可以在不同的听众位置和环境中提供高精度的估算结果。<details>
<summary>Abstract</summary>
Estimation of a speaker's direction and head orientation with binaural recordings can be a critical piece of information in many real-world applications with emerging `earable' devices, including smart headphones and AR/VR headsets. However, it requires predicting the mutual head orientations of both the speaker and the listener, which is challenging in practice. This paper presents a system for jointly predicting speaker-listener head orientations by leveraging inherent human voice directivity and listener's head-related transfer function (HRTF) as perceived by the ear-mounted microphones on the listener. We propose a convolution neural network model that, given binaural speech recording, can predict the orientation of both speaker and listener with respect to the line joining the two. The system builds on the core observation that the recordings from the left and right ears are differentially affected by the voice directivity as well as the HRTF. We also incorporate the fact that voice is more directional at higher frequencies compared to lower frequencies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文：<</SYS>>使用扬声器录音的方式估算发言人的方向和头姿可以是许多实际应用中的关键信息，包括智能HEADSET和AR/VR头戴式设备。然而，这需要预测发言人和听众双方的相互头姿，这在实践中很困难。这篇论文提出了一种系统，通过利用人声直达性和听众耳部 Transfer Function (HRTF)，以 ear-mounted microphones 上的听众所感受到的方式来联合预测发言人和听众的头姿。我们提议一种卷积神经网络模型， givens binaural speech recording，可以预测发言人和听众的方向相对于两点线。该系统基于核心observation ，左耳和右耳的录音被声 directivity 以及 HRTF 所不同地影响。我们还 incorporate 声音在高频段的方向性比低频段更强。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-approach-for-identifying-schizophrenia-using-cross-modal-attention"><a href="#A-multi-modal-approach-for-identifying-schizophrenia-using-cross-modal-attention" class="headerlink" title="A multi-modal approach for identifying schizophrenia using cross-modal attention"></a>A multi-modal approach for identifying schizophrenia using cross-modal attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15136">http://arxiv.org/abs/2309.15136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gowtham Premananth, Yashish M. Siriwardena, Philip Resnik, Carol Espy-Wilson</li>
<li>for: 这项研究旨在用不同的人类通信模式分类健康个体和患有强烈精神病的subject，以便更好地诊断和治疗精神病。</li>
<li>methods: 该研究使用了多modal的人类通信数据，包括视频、音频和文本，并从这些数据提取了低级特征，如面部动作单元和 vocals tract 变量，以计算高级协调特征。然后，将多modal数据 fusion为一个Session-level classifier和文本模型，使用 hierarchical Attention Network (HAN) with cross-modal attention。</li>
<li>results: 根据Weighted average F1 score，该多Modal系统在与前一个状态的多Modal系统进行比较时，提高了8.53%。<details>
<summary>Abstract</summary>
This study focuses on how different modalities of human communication can be used to distinguish between healthy controls and subjects with schizophrenia who exhibit strong positive symptoms. We developed a multi-modal schizophrenia classification system using audio, video, and text. Facial action units and vocal tract variables were extracted as low-level features from video and audio respectively, which were then used to compute high-level coordination features that served as the inputs to the audio and video modalities. Context-independent text embeddings extracted from transcriptions of speech were used as the input for the text modality. The multi-modal system is developed by fusing a segment-to-session-level classifier for video and audio modalities with a text model based on a Hierarchical Attention Network (HAN) with cross-modal attention. The proposed multi-modal system outperforms the previous state-of-the-art multi-modal system by 8.53% in the weighted average F1 score.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Segment-Level-Vectorized-Beam-Search-Based-on-Partially-Autoregressive-Inference"><a href="#Segment-Level-Vectorized-Beam-Search-Based-on-Partially-Autoregressive-Inference" class="headerlink" title="Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference"></a>Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14922">http://arxiv.org/abs/2309.14922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masao Someki, Nicholas Eng, Yosuke Higuchi, Shinji Watanabe</li>
<li>for: 提高自动语音识别（ASR）模型的推理速度，尤其是针对AR嵌入式模型。</li>
<li>methods: 提出一种部分AR框架，通过 segment-level vectorized beam search 加速ASR模型的推理速度，而不会影响准确性。</li>
<li>results: 实验结果显示，我们的方法可以在 LibriSpeech 集合上提高推理速度 12-13 倍，而且保持高度准确。<details>
<summary>Abstract</summary>
Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.
</details>
<details>
<summary>摘要</summary>
注意型编码器-解码器模型具有自动推理（AR）解码功能，在自动语音识别（ASR）领域具有优秀表现。然而，它们经常受到慢速推理的困扰。这主要归结于逐个计算decoder的增量。本文提出了一种部分AR框架，使用段级 вектор化的搜索 beam search来提高ASR模型基于混合连接式时间分类（CTC）注意力基 architecture的推理速度。它首先使用批量CTC解码生成一个初始假设，并将低信度的token标识出来基于其输出概率。然后，我们使用decoder进行段级 вектор化的搜索，并在并行进行重新预测，只需要最少的decoder计算。实验结果显示，我们的方法在LibriSpeech corpus上的推理速度比AR解码快12-13倍，保持高精度。
</details></li>
</ul>
<hr>
<h2 id="Emphasized-Non-Target-Speaker-Knowledge-in-Knowledge-Distillation-for-Automatic-Speaker-Verification"><a href="#Emphasized-Non-Target-Speaker-Knowledge-in-Knowledge-Distillation-for-Automatic-Speaker-Verification" class="headerlink" title="Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification"></a>Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14838">http://arxiv.org/abs/2309.14838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ductuantruong/enskd">https://github.com/ductuantruong/enskd</a></li>
<li>paper_authors: Duc-Tuan Truong, Ruijie Tao, Jia Qi Yip, Kong Aik Lee, Eng Siong Chng</li>
<li>for: 提高自动人员识别性能</li>
<li>methods: 利用知识混合法强化教师网络和学生网络之间的一致性，并强调非目标说话人的分类概率</li>
<li>results: 在三种不同的学生模型架构上应用修改后的知识混合法，在VoxCeleb数据集上实现了13.67%的EER提高 compared to embedding-level和标准标签水平的知识混合法<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is used to enhance automatic speaker verification performance by ensuring consistency between large teacher networks and lightweight student networks at the embedding level or label level. However, the conventional label-level KD overlooks the significant knowledge from non-target speakers, particularly their classification probabilities, which can be crucial for automatic speaker verification. In this paper, we first demonstrate that leveraging a larger number of training non-target speakers improves the performance of automatic speaker verification models. Inspired by this finding about the importance of non-target speakers' knowledge, we modified the conventional label-level KD by disentangling and emphasizing the classification probabilities of non-target speakers during knowledge distillation. The proposed method is applied to three different student model architectures and achieves an average of 13.67% improvement in EER on the VoxCeleb dataset compared to embedding-level and conventional label-level KD methods.
</details>
<details>
<summary>摘要</summary>
知识缩减（KD）可以提高自动说话识别性能，确保大教师网络和轻量级学生网络在嵌入层或标签层之间具有一致性。然而，传统的标签级KD忽略了非目标说话者的知识，特别是他们的分类概率，这些知识对自动说话识别非常重要。在这篇论文中，我们首先证明了使用更多的训练非目标说话者可以提高自动说话识别模型的性能。 inspirited by这一发现，我们修改了传统的标签级KD，通过分解和强调非目标说话者的分类概率来进行知识缩减。我们对三种不同的学生模型架构进行应用，并在VoxCeleb数据集上实现了13.67%的EER提升，比 embedding-level和传统标签级KD方法更好。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Techniques-for-a-Physical-Model-of-Human-Vocalisation"><a href="#Optimization-Techniques-for-a-Physical-Model-of-Human-Vocalisation" class="headerlink" title="Optimization Techniques for a Physical Model of Human Vocalisation"></a>Optimization Techniques for a Physical Model of Human Vocalisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14761">http://arxiv.org/abs/2309.14761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Cámara, Zhiyuan Xu, Yisu Zong, José Luis Blanco, Joshua D. Reiss</li>
<li>for: 优化和评估非语音音频效果的生成模型。</li>
<li>methods: 使用粉色 trombone synthesizer 作为一种简化的 vocal tract 生产模型，针对非语音人工声音信号 —— yawnings。选择并优化控制参数，以减少实际和生成 Audio 之间的差异。</li>
<li>results: 比较了不同优化技术和声音表示方式的结果，发现 génétic 和 swarm 优化器在计算成本上较高，但可以更好地优化模型。specific combinations of optimizers and audio representations offer significantly different results。<details>
<summary>Abstract</summary>
We present a non-supervised approach to optimize and evaluate the synthesis of non-speech audio effects from a speech production model. We use the Pink Trombone synthesizer as a case study of a simplified production model of the vocal tract to target non-speech human audio signals --yawnings. We selected and optimized the control parameters of the synthesizer to minimize the difference between real and generated audio. We validated the most common optimization techniques reported in the literature and a specifically designed neural network. We evaluated several popular quality metrics as error functions. These include both objective quality metrics and subjective-equivalent metrics. We compared the results in terms of total error and computational demand. Results show that genetic and swarm optimizers outperform least squares algorithms at the cost of executing slower and that specific combinations of optimizers and audio representations offer significantly different results. The proposed methodology could be used in benchmarking other physical models and audio types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非监督式的方法来优化和评估非语音音效的生成模型。我们使用了淡红 trombone synthesizer 作为一个简化的 vocal tract 生产模型，targeting 非语音人类声音信号 -- yawnings。我们选择和优化控制参数，以减少实际和生成声音之间的差异。我们验证了文献中通常报道的优化技术和一种特制的神经网络。我们使用了多种广泛使用的质量指标，包括对象质量指标和主观相当的指标。我们比较了结果，包括总错误和计算负担。结果显示，遗传和群体优化器在计算 slower 的情况下，可以超过 least squares 算法；具体的组合优化器和声音表示可以得到明显不同的结果。我们的方法可以用于对其他物理模型和声音类型进行benchmarking。
</details></li>
</ul>
<hr>
<h2 id="Exploring-RWKV-for-Memory-Efficient-and-Low-Latency-Streaming-ASR"><a href="#Exploring-RWKV-for-Memory-Efficient-and-Low-Latency-Streaming-ASR" class="headerlink" title="Exploring RWKV for Memory Efficient and Low Latency Streaming ASR"></a>Exploring RWKV for Memory Efficient and Low Latency Streaming ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14758">http://arxiv.org/abs/2309.14758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyu An, Shiliang Zhang</li>
<li>for: 这个论文的目的是提出一种基于RWKV的流处理ASR模型，以提高流处理ASR的准确率和效率。</li>
<li>methods: 这个论文使用了RWKV变体的线性注意力变换器，combines the superior performance of transformers和RNNs的推理效率，适用于流处理ASR场景， где时间和内存预算有限。</li>
<li>results: 实验表明，RWKV-Transducer和RWKV-Boundary-Aware-Transducer在不同的规模（100h-10000h）上具有和chunk conformer transducer相当或更高的准确率，同时具有较少的延迟和推理内存成本。<details>
<summary>Abstract</summary>
Recently, self-attention-based transformers and conformers have been introduced as alternatives to RNNs for ASR acoustic modeling. Nevertheless, the full-sequence attention mechanism is non-streamable and computationally expensive, thus requiring modifications, such as chunking and caching, for efficient streaming ASR. In this paper, we propose to apply RWKV, a variant of linear attention transformer, to streaming ASR. RWKV combines the superior performance of transformers and the inference efficiency of RNNs, which is well-suited for streaming ASR scenarios where the budget for latency and memory is restricted. Experiments on varying scales (100h - 10000h) demonstrate that RWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve comparable to or even better accuracy compared with chunk conformer transducer, with minimal latency and inference memory cost.
</details>
<details>
<summary>摘要</summary>
近些时候，基于自注意力的transformer和conformer被提出为RNN的替代者 для语音识别器模型。然而，全序列注意机制是不可流动的并且计算成本高，因此需要修改，如分割和缓存，以实现高效的流动语音识别。在这篇论文中，我们提议使用RWKV，一种变体的线性注意力变换器，来应用流动语音识别。RWKV结合了transformer的性能和RNN的推理效率，适合流动语音识别场景，具有限制的时钟和内存成本。在不同规模（100小时-10000小时）的实验中，RWKV-Transducer和RWKV-Boundary-Aware-Transducer实现了与分割对应者扫描器相当或更高的准确率，同时具有最小的延迟和推理内存成本。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Session-Variability-Leveraging-Session-Embeddings-for-Session-Robustness-in-Speaker-Verification"><a href="#Rethinking-Session-Variability-Leveraging-Session-Embeddings-for-Session-Robustness-in-Speaker-Verification" class="headerlink" title="Rethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification"></a>Rethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14741">http://arxiv.org/abs/2309.14741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hee-Soo Heo, KiHyun Nam, Bong-Jin Lee, Youngki Kwon, Minjae Lee, You Jin Kim, Joon Son Chung</li>
<li>for: 减少Session或通道变化对Speaker验证的影响</li>
<li>methods: 使用额外的 embedding 表示Session信息，通过附加到Speaker embedding抽取器的 auxilary network 进行训练，从而获得两个相似性分数：一个为Speaker信息，另一个为Session信息</li>
<li>results: 无需重新训练 embedding extractor，Session信息可以得到有效的补偿<details>
<summary>Abstract</summary>
In the field of speaker verification, session or channel variability poses a significant challenge. While many contemporary methods aim to disentangle session information from speaker embeddings, we introduce a novel approach using an additional embedding to represent the session information. This is achieved by training an auxiliary network appended to the speaker embedding extractor which remains fixed in this training process. This results in two similarity scores: one for the speakers information and one for the session information. The latter score acts as a compensator for the former that might be skewed due to session variations. Our extensive experiments demonstrate that session information can be effectively compensated without retraining of the embedding extractor.
</details>
<details>
<summary>摘要</summary>
在说话识别领域，会话或渠道变化对 speaker 识别带来极大的挑战。虽然现代方法通常尝试将会话信息与说话人嵌入分离开来，但我们提出了一种新的方法，即通过附加一个额外的嵌入来表示会话信息。这种方法通过在 speaker 嵌入提取器的训练过程中附加一个辅助网络来实现。这将生成两个相似度分数：一个是说话人信息，另一个是会话信息。后者的分数将作为前者可能因会话变化而偏移的补偿。我们的广泛实验表明，不需要重新训练 embedding 提取器，就可以有效地补偿会话信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/cs.SD_2023_09_26/" data-id="closbroul00xc0g882u0416fw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/27/eess.SP_2023_09_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-09-27
        
      </div>
    </a>
  
  
    <a href="/2023/09/26/eess.AS_2023_09_26/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-09-26</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
