
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-10-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.03938 repo_url: None paper_authors: Tejes Srivastava, Jiato">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-10-05">
<meta property="og:url" content="https://nullscc.github.io/2023/10/05/cs.SD_2023_10_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.03938 repo_url: None paper_authors: Tejes Srivastava, Jiato">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-05T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:27.027Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_10_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/05/cs.SD_2023_10_05/" class="article-date">
  <time datetime="2023-10-05T15:00:00.000Z" itemprop="datePublished">2023-10-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-10-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="EFFUSE-Efficient-Self-Supervised-Feature-Fusion-for-E2E-ASR-in-Multilingual-and-Low-Resource-Scenarios"><a href="#EFFUSE-Efficient-Self-Supervised-Feature-Fusion-for-E2E-ASR-in-Multilingual-and-Low-Resource-Scenarios" class="headerlink" title="EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios"></a>EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03938">http://arxiv.org/abs/2310.03938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejes Srivastava, Jiatong Shi, William Chen, Shinji Watanabe</li>
<li>for: 提高多语言语音识别 task 的性能</li>
<li>methods: 使用 SSL 模型进行预测，并将多个 SSL 模型的特征进行预测</li>
<li>results: 提高了 ML-SUPERB  benchmarck 中的平均 SUPERB 分数，并且减少了模型参数大小和执行时间<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing SSL models could achieve superior performance compared to using one SSL model. However, fusion models have increased model parameter size, leading to longer inference times. In this paper, we propose a novel approach of predicting other SSL models' features from a single SSL model, resulting in a light-weight framework with competitive performance. Our experiments show that SSL feature prediction models outperform individual SSL models in multilingual speech recognition tasks. The leading prediction model achieves an average SUPERB score increase of 135.4 in ML-SUPERB benchmarks. Moreover, our proposed framework offers an efficient solution, as it reduces the resulting model parameter size and inference times compared to previous fusion models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Challenges-and-Insights-Exploring-3D-Spatial-Features-and-Complex-Networks-on-the-MISP-Dataset"><a href="#Challenges-and-Insights-Exploring-3D-Spatial-Features-and-Complex-Networks-on-the-MISP-Dataset" class="headerlink" title="Challenges and Insights: Exploring 3D Spatial Features and Complex Networks on the MISP Dataset"></a>Challenges and Insights: Exploring 3D Spatial Features and Complex Networks on the MISP Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03901">http://arxiv.org/abs/2310.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Shao</li>
<li>for: 本研究旨在探讨多通道多人说话识别问题中，如干扰声、延迟和 overlap 等问题，以及如何通过 Contextual cues 来分离目标说话人的speech。</li>
<li>methods: 本研究使用了3D spatial feature，具体来说是通过计算目标说话人的位势信息来提高识别率。</li>
<li>results: 研究发现，通过使用3D spatial feature，可以减少或完全消除中间处理步骤，从而提高识别率。此外，对 MISP 数据集的扩展和模型的验证也表明了该方法的可行性和有效性。<details>
<summary>Abstract</summary>
Multi-channel multi-talker speech recognition presents formidable challenges in the realm of speech processing, marked by issues such as background noise, reverberation, and overlapping speech. Overcoming these complexities requires leveraging contextual cues to separate target speech from a cacophonous mix, enabling accurate recognition. Among these cues, the 3D spatial feature has emerged as a cutting-edge solution, particularly when equipped with spatial information about the target speaker. Its exceptional ability to discern the target speaker within mixed audio, often rendering intermediate processing redundant, paves the way for the direct training of "All-in-one" ASR models. These models have demonstrated commendable performance on both simulated and real-world data. In this paper, we extend this approach to the MISP dataset to further validate its efficacy. We delve into the challenges encountered and insights gained when applying 3D spatial features to MISP, while also exploring preliminary experiments involving the replacement of these features with more complex input and models.
</details>
<details>
<summary>摘要</summary>
多通道多发言人语音识别面临多种复杂性，如背景噪音、反射和 overlap 的问题。为了解决这些复杂性，需要利用上下文ual cue 分离目标语音从杂乱的混音中，以实现准确的识别。在这些上下文ual cue 中，3D 空间特征已经成为一种前导的解决方案，特别当配备空间信息目标说话人时。它的出色能力在杂乱的音频中分离目标说话人，经常使得中间处理redundant，从而降低了ASR模型的训练复杂性。在这篇论文中，我们将这种方法应用到MISP数据集，以进一步验证其效果。我们将详细介绍在应用3D 空间特征时遇到的挑战和获得的洞察，同时也将展开一些将这些特征更换为更复杂的输入和模型的先期实验。
</details></li>
</ul>
<hr>
<h2 id="Audio-Event-Relational-Graph-Representation-Learning-for-Acoustic-Scene-Classification"><a href="#Audio-Event-Relational-Graph-Representation-Learning-for-Acoustic-Scene-Classification" class="headerlink" title="Audio Event-Relational Graph Representation Learning for Acoustic Scene Classification"></a>Audio Event-Relational Graph Representation Learning for Acoustic Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03889">http://arxiv.org/abs/2310.03889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Chuang Yu, Wenwu Wang, Dick Botteldooren</li>
<li>for: 本研究旨在揭示实际生活中各种听觉场景与语音事件关系图中的Semantic embedding的关系。</li>
<li>methods: 本研究提出了一种事件关系图表示学习（ERGL）框架，用于实现听觉场景分类，同时清晰地表明分类所用的cue。在事件关系图中，每个事件的嵌入被视为节点，而每对节点之间的关系cue被描述为多维边特征。</li>
<li>results: 在一个真实的听觉场景分类 dataset上，提出的ERGL方法实现了与限制数量的语音事件嵌入相关的高度竞争性表现。结果表明可以通过语音事件关系图来识别多样化的听觉场景。可见化的语音事件关系图 Representation可以在这里（<a target="_blank" rel="noopener" href="https://github.com/Yuanbo2020/ERGL%EF%BC%89">https://github.com/Yuanbo2020/ERGL）</a> obtener。<details>
<summary>Abstract</summary>
Most deep learning-based acoustic scene classification (ASC) approaches identify scenes based on acoustic features converted from audio clips containing mixed information entangled by polyphonic audio events (AEs). However, these approaches have difficulties in explaining what cues they use to identify scenes. This paper conducts the first study on disclosing the relationship between real-life acoustic scenes and semantic embeddings from the most relevant AEs. Specifically, we propose an event-relational graph representation learning (ERGL) framework for ASC to classify scenes, and simultaneously answer clearly and straightly which cues are used in classifying. In the event-relational graph, embeddings of each event are treated as nodes, while relationship cues derived from each pair of nodes are described by multi-dimensional edge features. Experiments on a real-life ASC dataset show that the proposed ERGL achieves competitive performance on ASC by learning embeddings of only a limited number of AEs. The results show the feasibility of recognizing diverse acoustic scenes based on the audio event-relational graph. Visualizations of graph representations learned by ERGL are available here (https://github.com/Yuanbo2020/ERGL).
</details>
<details>
<summary>摘要</summary>
大多数深度学习基于的声音场景分类（ASC）方法都是基于声音特征，将混合多种声音事件（AEs）转换为声音特征。然而，这些方法往往难以解释它们如何标识场景。本文提出了第一个研究声音场景与 Semantic Embeddings 之间的关系的研究，以及一种Event-Relational Graph Representation Learning（ERGL）框架，用于分类场景。在事件关系图中，每个事件的嵌入被视为节点，而每对节点之间的关系cue被描述为多维边feature。实验表明，提出的ERGL可以在一个有限数量的AEs上达到竞争力的ASC性能。结果表明，可以通过声音事件关系图来识别多样化的声音场景。可以在这里查看Visualization of ERGL学习的图表（https://github.com/Yuanbo2020/ERGL）。
</details></li>
</ul>
<hr>
<h2 id="Securing-Voice-Biometrics-One-Shot-Learning-Approach-for-Audio-Deepfake-Detection"><a href="#Securing-Voice-Biometrics-One-Shot-Learning-Approach-for-Audio-Deepfake-Detection" class="headerlink" title="Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection"></a>Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03856">http://arxiv.org/abs/2310.03856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awais Khan, Khalid Mahmood Malik</li>
<li>for: 防止冒饵攻击 voice biometrics 系统，尤其是运用 audio deepfakes 进行逻辑存取攻击。</li>
<li>methods: 使用 one-shot learning 和 Metric Learning 技术探测和识别不同统计分布的 Synthetic 攻击，并使用有效的 спектраль特征集抽出有价的时间嵌入。</li>
<li>results: 在 ASVspoof 2019 逻辑存取（LA）数据集上评估了 Quick-SpoofNet 的表现，并在不同的 deepfake 攻击下进行了测试。实验结果显示 Quick-SpoofNet 能够具有高度的攻击探测率和优化的一致性。<details>
<summary>Abstract</summary>
The Automatic Speaker Verification (ASV) system is vulnerable to fraudulent activities using audio deepfakes, also known as logical-access voice spoofing attacks. These deepfakes pose a concerning threat to voice biometrics due to recent advancements in generative AI and speech synthesis technologies. While several deep learning models for speech synthesis detection have been developed, most of them show poor generalizability, especially when the attacks have different statistical distributions from the ones seen. Therefore, this paper presents Quick-SpoofNet, an approach for detecting both seen and unseen synthetic attacks in the ASV system using one-shot learning and metric learning techniques. By using the effective spectral feature set, the proposed method extracts compact and representative temporal embeddings from the voice samples and utilizes metric learning and triplet loss to assess the similarity index and distinguish different embeddings. The system effectively clusters similar speech embeddings, classifying bona fide speeches as the target class and identifying other clusters as spoofing attacks. The proposed system is evaluated using the ASVspoof 2019 logical access (LA) dataset and tested against unseen deepfake attacks from the ASVspoof 2021 dataset. Additionally, its generalization ability towards unseen bona fide speech is assessed using speech data from the VSDC dataset.
</details>
<details>
<summary>摘要</summary>
“自动话语识别（ASV）系统面临伪造活动的威胁，包括语音深圳攻击（Deepfake）。这些深圳攻击对话语音识别器具有潜在的威胁，因为近年来的生成AI和语音合成技术得到了进步。虽然许多深度学习模型用于语音合成检测已经发展出来，但大多数它们在不同的统计分布下显示出差。因此，这篇文章提出了快速攻击网络（Quick-SpoofNet），用于检测ASV系统中见到和未见到的合成攻击。这个方法使用有效的спектраль特征集，将声音样本中的时间特征提取出来，并使用度量学习和三重损失来评估相似性指数。系统可以划分相似的声音嵌入，将真正的话语识别为目标类别，并识别其他嵌入为伪造攻击。这个系统在ASVspoof 2019逻辑存取（LA）数据集上进行评估，并对未见到的深圳攻击进行测试。此外，它的普遍能力也被评估使用VSDC数据集上的话语数据。”
</details></li>
</ul>
<hr>
<h2 id="Speaker-localization-using-direct-path-dominance-test-based-on-sound-field-directivity"><a href="#Speaker-localization-using-direct-path-dominance-test-based-on-sound-field-directivity" class="headerlink" title="Speaker localization using direct path dominance test based on sound field directivity"></a>Speaker localization using direct path dominance test based on sound field directivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03688">http://arxiv.org/abs/2310.03688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boaz Rafaely, Koby Alhaiany</li>
<li>for: 这项研究的目的是开发一种robust to reverberation的DOA估计方法。</li>
<li>methods: 该方法基于时域频域分布的直接路径占据性测试，但不需要频率缓和矩阵分解。</li>
<li>results: 对比之前的方法，提议的方法在噪声和泛音条件下保持了相似的Robustness，并且计算效率高于原方法四倍。<details>
<summary>Abstract</summary>
Estimation of the direction-of-arrival (DoA) of a speaker in a room is important in many audio signal processing applications. Environments with reverberation that masks the DoA information are particularly challenging. Recently, a DoA estimation method that is robust to reverberation has been developed. This method identifies time-frequency bins dominated by the contribution from the direct path, which carries the correct DoA information. However, its implementation is computationally demanding as it requires frequency smoothing to overcome the effect of coherent early reflections and matrix decomposition to apply the direct-path dominance (DPD) test. In this work, a novel computationally-efficient alternative to the DPD test is proposed, based on the directivity measure for sensor arrays, which requires neither frequency smoothing nor matrix decomposition, and which has been reformulated for sound field directivity with spherical microphone arrays. The paper presents the proposed method and a comparison to previous methods under a range of reverberation and noise conditions. Result demonstrate that the proposed method shows comparable performance to the original method in terms of robustness to reverberation and noise, and is about four times more computationally efficient for the given experiment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate( Estimation of the direction-of-arrival (DoA) of a speaker in a room is important in many audio signal processing applications. Environments with reverberation that masks the DoA information are particularly challenging. Recently, a DoA estimation method that is robust to reverberation has been developed. This method identifies time-frequency bins dominated by the contribution from the direct path, which carries the correct DoA information. However, its implementation is computationally demanding as it requires frequency smoothing to overcome the effect of coherent early reflections and matrix decomposition to apply the direct-path dominance (DPD) test. In this work, a novel computationally-efficient alternative to the DPD test is proposed, based on the directivity measure for sensor arrays, which requires neither frequency smoothing nor matrix decomposition, and which has been reformulated for sound field directivity with spherical microphone arrays. The paper presents the proposed method and a comparison to previous methods under a range of reverberation and noise conditions. Result demonstrate that the proposed method shows comparable performance to the original method in terms of robustness to reverberation and noise, and is about four times more computationally efficient for the given experiment. )中文简体版：<<SYS>>音频信号处理应用中，确定发声者的方向来（DoA）在房间中非常重要。尤其是在听到延迟响应的环境中，DoA信息会被遮盖。最近，一种可以在延迟响应的环境中具有高Robustness的DoA估算方法已经被开发出来。这种方法可以在时域频域中标识由直接路径提供的DoA信息的占据率。然而，它的实现具有计算挺大的问题，需要频率平滑以超越协同早期反射的效果，并且需要矩阵分解来应用直通性测试。在这项工作中，一种新的计算高效的代替方法被提出，基于探测阵列的直接性度，不需要频率平滑也不需要矩阵分解。这种方法的实现可以在圆形 Mikrofon 阵列上进行 reformulation。文章介绍了该方法，并对之前的方法进行比较，包括各种噪声和延迟的条件下的性能。结果表明，该方法在robustness和计算效率方面与原始方法相似，且计算效率高于原始方法四倍。
</details></li>
</ul>
<hr>
<h2 id="Performance-and-energy-balance-a-comprehensive-study-of-state-of-the-art-sound-event-detection-systems"><a href="#Performance-and-energy-balance-a-comprehensive-study-of-state-of-the-art-sound-event-detection-systems" class="headerlink" title="Performance and energy balance: a comprehensive study of state-of-the-art sound event detection systems"></a>Performance and energy balance: a comprehensive study of state-of-the-art sound event detection systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03455">http://arxiv.org/abs/2310.03455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ronfrancesca/sed_carbon_footprint">https://github.com/ronfrancesca/sed_carbon_footprint</a></li>
<li>paper_authors: Francesca Ronchini, Romain Serizel</li>
<li>for: 这篇研究旨在探讨深度学习系统中增加复杂性和能耗问题的趋势，以及这些系统对环境的影响。</li>
<li>methods: 本研究使用了过去两年的探测和分类响应挑战 зада务的提交作为基础，进行比较和详细分析。</li>
<li>results: 研究发现，过去两年中深度学习系统的复杂性和能耗问题有所增加，并且这些系统对环境的影响也逐渐增加。<details>
<summary>Abstract</summary>
In recent years, deep learning systems have shown a concerning trend toward increased complexity and higher energy consumption. As researchers in this domain and organizers of one of the Detection and Classification of Acoustic Scenes and Events challenges tasks, we recognize the importance of addressing the environmental impact of data-driven SED systems. In this paper, we propose an analysis focused on SED systems based on the challenge submissions. This includes a comparison across the past two years and a detailed analysis of this year's SED systems. Through this research, we aim to explore how the SED systems are evolving every year in relation to their energy efficiency implications.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习系统在复杂性和能耗方面表现出了担忧的趋势。作为这个领域的研究人员和挑战任务组织者之一，我们认为对数据驱动的SED系统环境影响的问题非常重要。在这篇论文中，我们提出了基于挑战提交的SED系统分析。包括过去两年的比较和本年SED系统的详细分析。通过这些研究，我们希望探讨每年SED系统在能效环境方面的发展趋势。
</details></li>
</ul>
<hr>
<h2 id="VaSAB-The-variable-size-adaptive-information-bottleneck-for-disentanglement-on-speech-and-singing-voice"><a href="#VaSAB-The-variable-size-adaptive-information-bottleneck-for-disentanglement-on-speech-and-singing-voice" class="headerlink" title="VaSAB: The variable size adaptive information bottleneck for disentanglement on speech and singing voice"></a>VaSAB: The variable size adaptive information bottleneck for disentanglement on speech and singing voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03444">http://arxiv.org/abs/2310.03444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Bous, Axel Roebel</li>
<li>for: voice transformation, disentanglement of F0 parameter</li>
<li>methods: dropout-based information bottleneck auto-encoder, adaptive bottleneck size</li>
<li>results: improved disentanglement of F0 parameter for both speech and singing voice, improved synthesis quality, universal voice model for both speech and singing voice<details>
<summary>Abstract</summary>
The information bottleneck auto-encoder is a tool for disentanglement commonly used for voice transformation. The successful disentanglement relies on the right choice of bottleneck size. Previous bottleneck auto-encoders created the bottleneck by the dimension of the latent space or through vector quantization and had no means to change the bottleneck size of a specific model. As the bottleneck removes information from the disentangled representation, the choice of bottleneck size is a trade-off between disentanglement and synthesis quality. We propose to build the information bottleneck using dropout which allows us to change the bottleneck through the dropout rate and investigate adapting the bottleneck size depending on the context. We experimentally explore into using the adaptive bottleneck for pitch transformation and demonstrate that the adaptive bottleneck leads to improved disentanglement of the F0 parameter for both, speech and singing voice leading to improved synthesis quality. Using the variable bottleneck size, we were able to achieve disentanglement for singing voice including extremely high pitches and create a universal voice model, that works on both speech and singing voice with improved synthesis quality.
</details>
<details>
<summary>摘要</summary>
信息瓶颈自适应Encoder是一种常用的分解工具，通常用于音频变换。成功的分解取决于瓶颈大小的选择。过去的瓶颈自适应Encoder通过缺省空间维度或VECTOR量化来创建瓶颈，而无法改变特定模型中的瓶颈大小。因为瓶颈从分解表示中移除信息，因此瓶颈大小的选择是一种负担很大的负担，即分解和生成质量之间的权衡。我们提议使用dropout来构建信息瓶颈，这allow us可以通过dropout率来改变瓶颈大小，并且在不同的上下文中进行调整。我们通过实验explore使用可变瓶颈大小来进行音高变换，并证明可变瓶颈可以提高F0参数的分解，并且对于语音和歌唱voice都可以提高生成质量。使用可变瓶颈大小，我们可以实现对歌唱voice的分解，包括极高的音高，并创建一个通用的语音模型，可以在语音和歌唱voice上进行改进的生成。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/05/cs.SD_2023_10_05/" data-id="clot2mhi100xxx7881l8u7rmb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/06/eess.SP_2023_10_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-10-06
        
      </div>
    </a>
  
  
    <a href="/2023/10/05/eess.AS_2023_10_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-10-05</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
