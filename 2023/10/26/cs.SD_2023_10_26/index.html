
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-10-26 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Single channel speech enhancement by colored spectrograms paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.17142 repo_url: None paper_authors: Sania Gul, Muhammad Salman Khan, Muhammad Fazeel for: 提高混吵 speech 质量和">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-10-26">
<meta property="og:url" content="https://nullscc.github.io/2023/10/26/cs.SD_2023_10_26/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Single channel speech enhancement by colored spectrograms paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.17142 repo_url: None paper_authors: Sania Gul, Muhammad Salman Khan, Muhammad Fazeel for: 提高混吵 speech 质量和">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-26T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:03.539Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/cs.SD_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T15:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-10-26
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Single-channel-speech-enhancement-by-colored-spectrograms"><a href="#Single-channel-speech-enhancement-by-colored-spectrograms" class="headerlink" title="Single channel speech enhancement by colored spectrograms"></a>Single channel speech enhancement by colored spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17142">http://arxiv.org/abs/2310.17142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sania Gul, Muhammad Salman Khan, Muhammad Fazeel</li>
<li>for: 提高混吵 speech 质量和理解度</li>
<li>methods: 使用深度神经网络 (DNN) 架构，采用 pix2pix 生成整数网络 (GAN) 训练 colored spectrograms 混吵 speech 去噪</li>
<li>results: 比对基eline方法，提高了 almost 0.84 点 PESQ 和 1% STOI，且 computational cost 大幅减少<details>
<summary>Abstract</summary>
Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.
</details>
<details>
<summary>摘要</summary>
音响提升关注于从目标语音中除去不想要的背景声音，以提高其质量和可理解性。在这篇论文中，我们提出了一种基于深度神经网络（DNN）的单通道语音提升方法，使用颜色spectrogram。我们采用了基于 pix2pix生成对抗网络（GAN）的DNN架构，并在颜色spectrogram上训练其来减噪。减噪后，颜色spectrogram的颜色被翻译为快时傅立声变换（STFT）的大小使用一个浅层神经网络进行预测。这些估算的STFT大小后与噪音相加，以获得提升的语音。结果表明，与不处理噪音数据相比，提升语音质量和可理解性的改进约为0.84分（PESQ）和1%（STOI）。与比较基线方法相比，提升的质量和可理解性减噪量约为90%，而计算成本减少了约10倍。提议的解决方案提供了相对于基线方法的PESQ分数，但计算成本减少了约10倍。此外，与另一个基线系统（CNN-GAN）相比，提升的STOI减噪量约为28倍，而计算成本减少了约28倍。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Neonatal-Chest-Sound-Separation-using-Deep-Learning"><a href="#Real-time-Neonatal-Chest-Sound-Separation-using-Deep-Learning" class="headerlink" title="Real-time Neonatal Chest Sound Separation using Deep Learning"></a>Real-time Neonatal Chest Sound Separation using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17116">http://arxiv.org/abs/2310.17116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yi Poh, Ethan Grooby, Kenneth Tan, Lindsay Zhou, Arrabella King, Ashwin Ramanathan, Atul Malhotra, Mehrtash Harandi, Faezeh Marzbanrad</li>
<li>for: 这篇论文是为了提供一种基于深度学习的胸部听觉分离方法，以提高胸部听觉诊断的质量。</li>
<li>methods: 该论文提出了一种基于Conv-TasNet模型的深度学习方法，包括编码器、解码器和面Generator。编码器使用1D卷积模型，解码器使用反卷积，面Generator使用堆叠的1D卷积和变换器。</li>
<li>results: 该论文在人工数据集上比前方法提高了2.01dB至5.06dB的对象扭曲度量，同时计算时间也提高了至少17倍。因此，该方法可以作为任何胸部听觉监测系统的预处理步骤。<details>
<summary>Abstract</summary>
Auscultation for neonates is a simple and non-invasive method of providing diagnosis for cardiovascular and respiratory disease. Such diagnosis often requires high-quality heart and lung sounds to be captured during auscultation. However, in most cases, obtaining such high-quality sounds is non-trivial due to the chest sounds containing a mixture of heart, lung, and noise sounds. As such, additional preprocessing is needed to separate the chest sounds into heart and lung sounds. This paper proposes a novel deep-learning approach to separate such chest sounds into heart and lung sounds. Inspired by the Conv-TasNet model, the proposed model has an encoder, decoder, and mask generator. The encoder consists of a 1D convolution model and the decoder consists of a transposed 1D convolution. The mask generator is constructed using stacked 1D convolutions and transformers. The proposed model outperforms previous methods in terms of objective distortion measures by 2.01 dB to 5.06 dB in the artificial dataset, as well as computation time, with at least a 17-time improvement. Therefore, our proposed model could be a suitable preprocessing step for any phonocardiogram-based health monitoring system.
</details>
<details>
<summary>摘要</summary>
来诊检测新生儿是一种简单且不侵入性的诊断方法，用于诊断循环和呼吸道疾病。然而，在大多数情况下，获取高质量心脏和肺声 зву乐是非常困难，因为胸部声音包含了心脏、肺声和噪音声音。为了解决这个问题，通常需要进行额外的处理，以分离胸部声音成为心脏声音和肺声音。这篇论文提出了一个新的深度学习方法，用于将胸部声音分类为心脏声音和肺声音。这个方法受到Conv-TasNet模型的激发，并包括Encoder、Decoder和面组生成器。Encoder由1D梯度核心组成，Decoder由转置1D梯度组成，而面组生成器则由堆叠1D梯度和对称器组成。这个方法在人工数据集上比前方法提高了2.01dB至5.06dB的对象歪斜度指数，以及计算时间，至少提高了17倍。因此，我们的提案方法可以作为任何phonocardiogram基于的医疗监控系统的适当预处理步骤。
</details></li>
</ul>
<hr>
<h2 id="Multi-Speaker-Expressive-Speech-Synthesis-via-Semi-supervised-Contrastive-Learning"><a href="#Multi-Speaker-Expressive-Speech-Synthesis-via-Semi-supervised-Contrastive-Learning" class="headerlink" title="Multi-Speaker Expressive Speech Synthesis via Semi-supervised Contrastive Learning"></a>Multi-Speaker Expressive Speech Synthesis via Semi-supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17101">http://arxiv.org/abs/2310.17101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinfa Zhu, Yuke Li, Yi Lei, Ning Jiang, Guoqing Zhao, Lei Xie</li>
<li>for: 建立一个可表达多种语音样式和情感的 TTS系统，以实现多 speaker 的语音合成。</li>
<li>methods: 提出一种基于对照学习的 TTS方法，通过将负例和正例构成组合，以更好地提取语音中的构造、情感和 speaker 特征。</li>
<li>results: 透过 semi-supervised 训练和多元数据，提高 VITS 模型的表现，使其能够实现多种语音样式和情感的语音合成。<details>
<summary>Abstract</summary>
This paper aims to build an expressive TTS system for multi-speakers, synthesizing a target speaker's speech with multiple styles and emotions. To this end, we propose a novel contrastive learning-based TTS approach to transfer style and emotion across speakers. Specifically, we construct positive-negative sample pairs at both utterance and category (such as emotion-happy or style-poet or speaker A) levels and leverage contrastive learning to better extract disentangled style, emotion, and speaker representations from speech. Furthermore, we introduce a semi-supervised training strategy to the proposed approach to effectively leverage multi-domain data, including style-labeled data, emotion-labeled data, and unlabeled data. We integrate the learned representations into an improved VITS model, enabling it to synthesize expressive speech with diverse styles and emotions for a target speaker. Experiments on multi-domain data demonstrate the good design of our model.
</details>
<details>
<summary>摘要</summary>
这篇论文目标建立一个表达力强的多话者Text-to-Speech（TTS）系统，使得目标说话者的speech中包含多种风格和情感。为此，我们提出了一种基于对比学习的TTS方法，用于传递风格和情感 across speakers。具体来说，我们构建了一个utterance和类别（例如情感-高兴或风格-诗人或说话者A）两级的正负样本对，并利用对比学习来更好地提取speech中的分离风格、情感和说话者表示。此外，我们提出了一种半监督训练策略，以更好地利用多个频道数据，包括风格标注数据、情感标注数据和无标注数据。我们将学习的表示 integrate into an improved VITS模型，使其能够合成具有多种风格和情感的表达性speech for a target speaker。实验结果表明我们的模型设计很好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/cs.SD_2023_10_26/" data-id="clpxp6c7o011uee8816q4a9ic" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/27/eess.SP_2023_10_27/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-10-27
        
      </div>
    </a>
  
  
    <a href="/2023/10/26/eess.AS_2023_10_26/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-10-26</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
