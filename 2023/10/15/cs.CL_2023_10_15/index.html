
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-10-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="UvA-MT’s Participation in the WMT23 General Translation Shared Task paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.09946 repo_url: None paper_authors: Di Wu, Shaomu Tan, David Stap, Ali Araabi, Christof Monz fo">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-10-15">
<meta property="og:url" content="https://nullscc.github.io/2023/10/15/cs.CL_2023_10_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="UvA-MT’s Participation in the WMT23 General Translation Shared Task paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.09946 repo_url: None paper_authors: Di Wu, Shaomu Tan, David Stap, Ali Araabi, Christof Monz fo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-15T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:55.473Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.CL_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T11:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-10-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task"><a href="#UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task" class="headerlink" title="UvA-MT’s Participation in the WMT23 General Translation Shared Task"></a>UvA-MT’s Participation in the WMT23 General Translation Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09946">http://arxiv.org/abs/2310.09946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wu, Shaomu Tan, David Stap, Ali Araabi, Christof Monz</li>
<li>for: 这个研究报告描述了阿姆斯特丹大学的自然语言处理实验室（UvA-MT）在2023年世界机器翻译大会（WMT）共享任务中的参加。他们在英文&lt;-&gt;希伯来两个方向的受限Track中参加竞赛，并显示了使用一个模型处理对向任务时，可以达到相似的结果，比较 Traditional的双语翻译。</li>
<li>methods: 这个研究使用了一些有效的策略，如回 перевод、重定义的嵌入表格和任务导向的精细调整，以提高自动评估中的最终结果。</li>
<li>results: 在自动评估中，他们在英文-&gt;希伯来和希伯来-&gt;英文两个方向中都获得了竞争性的结果。<details>
<summary>Abstract</summary>
This paper describes the UvA-MT's submission to the WMT 2023 shared task on general machine translation. We participate in the constrained track in two directions: English <-> Hebrew. In this competition, we show that by using one model to handle bidirectional tasks, as a minimal setting of Multilingual Machine Translation (MMT), it is possible to achieve comparable results with that of traditional bilingual translation for both directions. By including effective strategies, like back-translation, re-parameterized embedding table, and task-oriented fine-tuning, we obtained competitive final results in the automatic evaluation for both English -> Hebrew and Hebrew -> English directions.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:这篇论文描述了UvA-MT在WMT 2023共同任务中的提交，我们在Constrained Track中参加了英文 <-> 希伯来两个方向的翻译。在这次竞赛中，我们表明，通过使用一个模型处理双向任务，作为多语言翻译的最小设置（MMT），可以达到相同的结果。通过包括有效策略，如回译、重新参数表示表 и任务导向精度调整，我们在自动评估中获得了对 beiden方向的竞争性最终结果。
</details></li>
</ul>
<hr>
<h2 id="FiLM-Fill-in-Language-Models-for-Any-Order-Generation"><a href="#FiLM-Fill-in-Language-Models-for-Any-Order-Generation" class="headerlink" title="FiLM: Fill-in Language Models for Any-Order Generation"></a>FiLM: Fill-in Language Models for Any-Order Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09930">http://arxiv.org/abs/2310.09930</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shentianxiao/film">https://github.com/shentianxiao/film</a></li>
<li>paper_authors: Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, Yejin Choi</li>
<li>for: 填充语言模型 (Fill-in Language Model, FiLM) 的目的是提供一种可以在任意位置进行灵活生成的语言模型，以便在填充文本中使用双向文本上下文。</li>
<li>methods: FiLM 使用了一种新的语言模型方法，即采用 beta 分布中的变化掩码概率来提高 FiLM 的生成能力。在推理过程中，FiLM 可以顺利地插入缺失的句子、段落或整个文本，以确保输出的文本流畅、与周围上下文一致。</li>
<li>results: 在自动和人工评估中，FiLM 表现出色，超过了基于左到右语言模型的填充方法。FiLM 可以轻松地在不同的文本长度和难度水平上进行调整，并且可以在不同的语言模型大小上进行训练和 fine-tuning。<details>
<summary>Abstract</summary>
Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.
</details>
<details>
<summary>摘要</summary>
现代人工智能系统中，语言模型已成为背景模型。然而，这些主要左往右生成的语言模型限制了使用对向文本填充的 bidirectional 上下文，这是装备填充文本的任务中非常重要。我们提出了填充语言模型（FiLM），一种新的语言模型化方法，可以在任何位置进行 flexible 生成，不受特定生成顺序的限制。它的训练将推广遮盾语言模型的对话预设，透过对应排版的 beta 分布来增强FiLM的生成能力。在推断中，FiLM可以顺利地插入缺失的句子、句末或段落，以确保输出的流畅和与周围上下文一致。在自动和人工评估中，FiLM比靠左往右的语言模型训练在重新排序的文本段落上的填充方法表现出色，并且可以轻松地从头部训练或精革左往右语言模型。值得一提的是，当模型的大小增加时，FiLM的误差接近强左往右语言模型相似大小的误差，这表明FiLM在大型模型中的可扩展性和潜力。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition"><a href="#Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition" class="headerlink" title="Prompting Scientific Names for Zero-Shot Species Recognition"></a>Prompting Scientific Names for Zero-Shot Species Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09929">http://arxiv.org/abs/2310.09929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong</li>
<li>for: 本研究旨在使用CLIP进行零shot认知高级生物物种，包括鸟类、植物和动物的species recognition。</li>
<li>methods: 本研究使用CLIP进行零shot认知，并使用大语言模型（LLM）生成描述（例如物种颜色和形状）以提高性能。</li>
<li>results: 研究发现，使用common名称（例如mountain hare）而不是学名（例如Lepus Timidus）在prompt中可以提高CLIP的认知精度，并且可以达到2∼5倍的提升。<details>
<summary>Abstract</summary>
Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., "a photo of Lepus Timidus" (which is a scientific name in Latin). Because these names are usually not included in CLIP's training set. To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. We find that they bring only marginal gains. Differently, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2$\sim$5 times higher accuracy on benchmarking datasets of fine-grained species recognition.
</details>
<details>
<summary>摘要</summary>
<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>Here's the translation in Traditional Chinese as well:<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation"><a href="#Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation" class="headerlink" title="Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation"></a>Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09917">http://arxiv.org/abs/2310.09917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadezhda Chirkova, Sheng Liang, Vassilina Nikoulina</li>
<li>for: 这个论文旨在研究零shot cross-语言生成技术，即使finetuning多语言预训练语言模型（mPLM）在一种语言上的一个生成任务，然后用其来预测这个任务在其他语言上的结果。</li>
<li>methods: 这篇论文测试了一些替代的mPLM模型，包括mBART和NLLB，并考虑了全 Parameters 的 fine-tuning 和 parameter-efficient fine-tuning with adapters。</li>
<li>results: 研究发现，mBART with adapters 与 mT5 相似，NLLB 可以在一些情况下与 mT5 竞争。 此外，研究发现训练学习率对 fine-tuning 的调整可以减轻生成错误语言的问题。<details>
<summary>Abstract</summary>
Zero-shot cross-lingual generation assumes finetuning the multilingual pretrained language model (mPLM) on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work, we test alternative mPLMs, such as mBART and NLLB, considering full finetuning and parameter-efficient finetuning with adapters. We find that mBART with adapters performs similarly to mT5 of the same size, and NLLB can be competitive in some cases. We also underline the importance of tuning learning rate used for finetuning, which helps to alleviate the problem of generation in the wrong language.
</details>
<details>
<summary>摘要</summary>
zero-shot 跨语言生成假设通过质量化多语言预训练语言模型（mPLM）的 Fine-tuning 进行一种语言的生成任务，然后用其来预测这个任务的其他语言。  previous works 发现生成 incorrect language 的问题，并提出了解决方案，通常使用 mT5 作为基础模型。 在这个工作中，我们测试了不同的 mPLM，如 mBART 和 NLLB，包括全部 Fine-tuning 和参数有效的 Fine-tuning  WITH 适配器。我们发现 mBART  WITH 适配器 和 mT5 的同等大小下表现相似，而 NLLB 在一些情况下可以达到竞争水平。我们还强调了在 Fine-tuning 中调整学习率的重要性，可以减轻生成 incorrect language 的问题。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis"><a href="#Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis" class="headerlink" title="Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis"></a>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09909">http://arxiv.org/abs/2310.09909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaoyi-wu/gpt-4v_medical_evaluation">https://github.com/chaoyi-wu/gpt-4v_medical_evaluation</a></li>
<li>paper_authors: Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</li>
<li>for: This paper assesses the performance of OpenAI’s GPT-4V model in multimodal medical diagnosis, evaluating its ability to distinguish between medical image modalities and anatomy, as well as its ability to generate comprehensive reports.</li>
<li>methods: The evaluation uses 17 human body systems and 8 modalities of medical images, with or without patent history provided, to probe the GPT-4V’s ability on multiple clinical tasks such as imaging modality and anatomy recognition, disease diagnosis, and report generation.</li>
<li>results: The study finds that while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports, highlighting the limitations of large multimodal models in supporting real-world medical applications and clinical decision-making.Here are the three key points in Simplified Chinese:</li>
<li>for: 这项研究用于评估OpenAI的GPT-4V模型在多模态医学诊断中的表现，包括分辨医疗影像模式和解剖结构等能力。</li>
<li>methods: 这项评估使用17个人体系统和8种医疗影像模式，有或无患者历史提供，以探索GPT-4V在多种临床任务上的能力，包括影像模式和解剖结构识别、疾病诊断、报告生成等。</li>
<li>results: 研究发现，虽然GPT-4V在分辨医疗影像模式和解剖结构方面表现出色，但在疾病诊断和生成全面报告方面受到了重大挑战，表明大型多模态模型在实际医疗应用和临床决策中仍有很大的发展空间。<details>
<summary>Abstract</summary>
Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, disease diagnosis, report generation, disease localisation.   Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.   All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</details>
<details>
<summary>摘要</summary>
由大型基础模型驱动，人工智能的发展最近几年有了很大的进步，引起了公众的广泛关注。在这项研究中，我们想要评估OpenAI的最新模型GPT-4V（视觉）在多modal医学诊断方面的表现。我们的评估覆盖了17个人体系统，包括中枢神经系统、头颈部、心脏、胸部、血液系统、肝胆系统、肠道系统、尿道系统、妇科、儿科、骨骼系统、脊梁系统、血管系统、肿瘤系统、护理、外伤等，图像来自日常临床 Routine的8种模式，例如X射线、计算tomography（CT）、核磁共振成像（MRI）、 позитрон发射tomography（PET）、数字抽取ANGIOGRAPHY（DSA）、胸部X射线、计算tomography（CT）、ultrasound和pathology。我们 probing GPT-4V的能力在多种临床任务上，包括图像模式和解剖学识别、疾病诊断、报告生成、疾病Localization。我们的观察表明，GPT-4V能够Distinguish between different medical imaging modalities and anatomy, but it faces significant challenges in disease diagnosis and report generation. These findings highlight that while large multimodal models have made significant advancements in computer vision and natural language processing, they are still far from being used to effectively support real-world medical applications and clinical decision-making.所有图像使用在这项报告中可以在GitHub上找到：https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia"><a href="#Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia" class="headerlink" title="Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia"></a>Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09897">http://arxiv.org/abs/2310.09897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Gkoumas, Matthew Purver, Maria Liakata</li>
<li>for: 这个研究是为了 automatization 语言障碍模式，以便更好地识别和评估词语障碍。</li>
<li>methods: 这个研究使用了一个已经训练过的自然语言处理（NLP）模型，并对其进行了修改，以便在NLP任务中强制使用语言模式。然后，他们使用了这些任务的概率估计来构建数字语言标记，用于评估语言交流质量和语言障碍的严重程度。</li>
<li>results: 研究发现，提出的语言标记能够准确地识别患有 деменция 的人的语言障碍，并且与临床标记呈正相关。此外，这些语言标记还提供了词语障碍的可观察性和恰当性，可以用于评估词语障碍的进程。<details>
<summary>Abstract</summary>
Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability estimates from the best model to construct digital linguistic markers measuring the overall quality in communication and the intensity of a variety of language disorders. We investigate how the digital markers characterize dementia speech from a longitudinal perspective. We find that our proposed communication marker is able to robustly and reliably characterize the language of people with dementia, outperforming existing linguistic approaches; and shows external validity via significant correlation with clinical markers of behaviour. Finally, our proposed linguistic disorder markers provide useful insights into gradual language impairment associated with disease progression.
</details>
<details>
<summary>摘要</summary>
偏僻症与语言障碍有关，我们通过自动学习受控语言模型，训练其专注于修改后NLP任务和相关的语言模式。我们的实验显示，包含语言上下文信息并在语言模式中增强梯度信号的NLP任务可以提高表现。然后，我们使用最佳模型的概率估计来构建数字语言标记，评估整体沟通质量和语言障碍的严重程度。我们研究如何使用我们的提议的沟通标记来 caracterize dementia speech的长期趋势。我们发现，我们的提议的语言障碍标记能够坚定可靠地 caracterize人们患有偏僻症的语言，高于现有的语言方法；并与临床标记相关。最后，我们的语言障碍标记提供了有用的透视 gradual language impairment与疾病进程相关的语言障碍。
</details></li>
</ul>
<hr>
<h2 id="Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning"><a href="#Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning" class="headerlink" title="Bounding and Filling: A Fast and Flexible Framework for Image Captioning"></a>Bounding and Filling: A Fast and Flexible Framework for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09876">http://arxiv.org/abs/2310.09876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changxinwang/boficap">https://github.com/changxinwang/boficap</a></li>
<li>paper_authors: Zheng Ma, Changxin Wang, Bo Huang, Zixuan Zhu, Jianbing Zhang</li>
<li>for: 这篇论文目的是提出一种快速和灵活的图像描述模型，以解决现有的描述模型具有 significiant inference latency 问题。</li>
<li>methods: 该模型使用 bounding 和 filling 技术，将图像分割成多个区域，然后采用 two-generation 方式填充每个区域。</li>
<li>results: 该模型在 MS-COCO 测试集上取得了状态的最佳性能（CIDEr 125.6），并且比基eline模型快速 9.22 倍；在半循环的情况下，该模型达到了 128.4 的 CIDEr 性能，并且速度比基eline模型快速 3.69 倍。<details>
<summary>Abstract</summary>
Most image captioning models following an autoregressive manner suffer from significant inference latency. Several models adopted a non-autoregressive manner to speed up the process. However, the vanilla non-autoregressive manner results in subpar performance, since it generates all words simultaneously, which fails to capture the relationships between words in a description. The semi-autoregressive manner employs a partially parallel method to preserve performance, but it sacrifices inference speed. In this paper, we introduce a fast and flexible framework for image captioning called BoFiCap based on bounding and filling techniques. The BoFiCap model leverages the inherent characteristics of image captioning tasks to pre-define bounding boxes for image regions and their relationships. Subsequently, the BoFiCap model fills corresponding words in each box using two-generation manners. Leveraging the box hints, our filling process allows each word to better perceive other words. Additionally, our model offers flexible image description generation: 1) by employing different generation manners based on speed or performance requirements, 2) producing varied sentences based on user-specified boxes. Experimental evaluations on the MS-COCO benchmark dataset demonstrate that our framework in a non-autoregressive manner achieves the state-of-the-art on task-specific metric CIDEr (125.6) while speeding up 9.22x than the baseline model with an autoregressive manner; in a semi-autoregressive manner, our method reaches 128.4 on CIDEr while a 3.69x speedup. Our code and data is available at https://github.com/ChangxinWang/BoFiCap.
</details>
<details>
<summary>摘要</summary>
大多数图像描述模型采用回归方式，却受到显著的推理延迟。一些模型采用非回归方式以加速过程，但这会导致性能下降，因为它们同时生成所有 слова，无法捕捉图像描述中 слова之间的关系。半回归方式使用部分并行方法保持性能，但是它们牺牲推理速度。本文提出一种快速和灵活的图像描述模型called BoFiCap，基于缓存和填充技术。BoFiCap模型利用图像描述任务的特点，先定义图像区域的缓存框，然后使用两种生成方式填充对应的字。利用框提示，我们的填充过程让每个字etter perceive其他字。此外，我们的模型提供了自适应的图像描述生成：1）根据速度或性能要求使用不同的生成方式，2）生成基于用户指定的盒子的多种句子。在COCO数据集上的实验评估 demonstrate了我们的框架在非回归方式下达到了状态之arte（CIDEr=125.6），同时速度比基eline模型（具有回归方式）快9.22倍。在半回归方式下，我们的方法达到了128.4的CIDEr，速度比基eline模型快3.69倍。我们的代码和数据可以在https://github.com/ChangxinWang/BoFiCap上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Stance-Classification-with-Quantified-Moral-Foundations"><a href="#Enhancing-Stance-Classification-with-Quantified-Moral-Foundations" class="headerlink" title="Enhancing Stance Classification with Quantified Moral Foundations"></a>Enhancing Stance Classification with Quantified Moral Foundations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09848">http://arxiv.org/abs/2310.09848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhang, Prasanta Bhattacharya, Wei Gao, Liang Ze Wong, Brandon Siyuan Loh, Joseph J. P. Simons, Jisun An</li>
<li>for: 这 paper 的目的是增强社交媒体上的立场检测，通过 incorporating deeper psychological attributes，特别是个人的道德基础。</li>
<li>methods: 这 paper 使用的方法包括EXTRACTING moral foundation features from text, 以及 message semantic features，来 классифика stance 在 message- 和 user-levels 上。</li>
<li>results:  Preliminary results 表明， encoding moral foundations 可以提高 stance detection 任务的性能，并帮助描述特定道德基础和 online stance 之间的关系。  results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.<details>
<summary>Abstract</summary>
This study enhances stance detection on social media by incorporating deeper psychological attributes, specifically individuals' moral foundations. These theoretically-derived dimensions aim to provide a comprehensive profile of an individual's moral concerns which, in recent work, has been linked to behaviour in a range of domains, including society, politics, health, and the environment. In this paper, we investigate how moral foundation dimensions can contribute to predicting an individual's stance on a given target. Specifically we incorporate moral foundation features extracted from text, along with message semantic features, to classify stances at both message- and user-levels across a range of targets and models. Our preliminary results suggest that encoding moral foundations can enhance the performance of stance detection tasks and help illuminate the associations between specific moral foundations and online stances on target topics. The results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts"><a href="#Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts" class="headerlink" title="Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"></a>Merging Experts into One: Improving Computational Efficiency of Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09832">http://arxiv.org/abs/2310.09832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shwai-he/meo">https://github.com/shwai-he/meo</a></li>
<li>paper_authors: Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao</li>
<li>for: 提高语言模型的大小通常会导致NLPTasks的进步，但是会增加计算成本。零含量的混合专家（MoE）可以减少计算成本，但是如果增加激活专家的数量，计算成本会增加很快，限制实际应用。本文提出一种名为\textbf{\texttt{Merging Experts into One}（MEO）的计算效率的方法，可以保持增加专家的优点而不导致计算成本增加。</li>
<li>methods: 我们首先证明选择多个专家的优势，然后提出一种计算效率的方法，即\textbf{\texttt{Merging Experts into One}（MEO），可以将计算成本降低到单个专家的水平。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。</li>
<li>results: 我们进行了广泛的实验，显示MEO可以减少计算成本，例如FLOPS从72.0G下降到28.6G（MEO）。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。例如，在GLUE benchmark上，MEO的平均分数为83.3%，而vanilla MoE的平均分数为82.6%。<details>
<summary>Abstract</summary>
Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs. 82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: \url{https://github.com/Shwai-He/MEO}.
</details>
<details>
<summary>摘要</summary>
通常，将语言模型的大小扩展到可观的尺度会导致NLPTasks的显著进步。然而，这经常会带来计算成本的增加。虽然 sparse Mixture of Experts（MoE）可以降低计算成本，但是如果启用更多的专家，计算成本会快速增加，限制其实际应用。我们是否可以保留添加更多专家的优点而不导致计算成本增加很多？在这篇论文中，我们首先表明了多个专家的选择的优势，然后我们提出了一种 computation-efficient的方法called \textbf{\texttt{Merging Experts into One}（MEO），可以降低计算成本到单个专家的水平。我们进行了广泛的实验，发现 MEO 可以减少 FLOPS 的值，例如，从 vanilla MoE 的 72.0G 降低到 28.6G（MEO）。此外，我们还提出了一种循环预测块，可以进一步提高 MEO 的效率和性能，例如，在 GLUE 测试准则上，MEO 的平均分数为 83.3%，而 vanilla MoE 的平均分数为 82.6%。我们将代码发布在接受后。代码将发布在：\url{https://github.com/Shwai-He/MEO}.
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Reliability-of-Large-Language-Model-Knowledge"><a href="#Assessing-the-Reliability-of-Large-Language-Model-Knowledge" class="headerlink" title="Assessing the Reliability of Large Language Model Knowledge"></a>Assessing the Reliability of Large Language Model Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09820">http://arxiv.org/abs/2310.09820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng</li>
<li>for: 评估大语言模型（LLMs）的知识可靠性。</li>
<li>methods: 提出了一种名为 Model Knowledge Relibility Score (MONITOR) 的新度量方法，用于直接测试 LLMs 的事实可靠性。</li>
<li>results: 在一系列12种 LLMS 上进行了实验，并证明了 MONITOR 的效iveness 以及低计算成本。此外，还释放了一个名为 Factual Knowledge Test Corpus (FKTC) 的测试集，以便进一步研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been treated as knowledge bases due to their strong performance in knowledge probing tasks. LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability. How do we evaluate the capabilities of LLMs to consistently produce factually correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly measure LLMs' factual reliability. MONITOR computes the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness of MONITOR in evaluating the factual reliability of LLMs while maintaining a low computational overhead. In addition, we release the FKTC (Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total to foster research along this line (https://github.com/Vicky-Wil/MONITOR).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training"><a href="#RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training" class="headerlink" title="RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training"></a>RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09773">http://arxiv.org/abs/2310.09773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tommytyc/rsvp">https://github.com/tommytyc/rsvp</a></li>
<li>paper_authors: Yu-Chien Tang, Wei-Yao Wang, An-Zi Yen, Wen-Chih Peng</li>
<li>for: 提供用户task-oriented对话中的精准回答和24小时支持</li>
<li>methods: 利用神经网络模型检测客户意图 based on their utterances</li>
<li>results: 与状态空间的基eline比进行了比较，得到了4.95%的准确率提升，3.4%的MRR@3提升和2.75%的MRR@5提升的结果<details>
<summary>Abstract</summary>
The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority. In addition, they neglect the information within the conversational responses of the agents, which have a lower collection cost, but are significant to customer intent as agents must tailor their replies based on the customers' intent. In this paper, we propose RSVP, a self-supervised framework dedicated to task-oriented dialogues, which utilizes agent responses for pre-training in a two-stage manner. Specifically, we introduce two pre-training tasks to incorporate the relations of utterance-response pairs: 1) Response Retrieval by selecting a correct response from a batch of candidates, and 2) Response Generation by mimicking agents to generate the response to a given utterance. Our benchmark results for two real-world customer service datasets show that RSVP significantly outperforms the state-of-the-art baselines by 4.95% for accuracy, 3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are investigated to show the validity of incorporating agent responses into the pre-training stage.
</details>
<details>
<summary>摘要</summary>
Dialogue 系统在客户服务中已经采用神经网络模型，以提供用户精准的答案和24小时的支持，通过检测客户意图基于他们的谈话来进行任务化对话。现有的意图检测方法强调适应性地预训练语言模型，但这可能增加成本。此外，它们忽略了代理人回复的信息，尽管这些信息在客户意图方面具有重要性，因为代理人必须根据客户的意图修改他们的回复。在本文中，我们提出了 RSVP，一个自动预训练框架，专门用于任务化对话。我们在两个阶段中使用代理人回复进行预训练：1）回复选择，选择一个正确的回复从批处理中的候选者中，2）回复生成，模仿代理人生成一个回复来回应给一个谈话。我们对两个实际的客户服务数据集进行了比较，结果显示，RSVP在精度、MRR@3和MRR@5等指标上平均高于状态之前的基eline by 4.95%、3.4%和2.75%。我们还进行了广泛的案例研究，以证明代理人回复的包含在预训练阶段是有效的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation"><a href="#Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation" class="headerlink" title="Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation"></a>Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09772">http://arxiv.org/abs/2310.09772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhou, Wenyu Chen, Dingyi Zeng, Hong Qu, Daniel Hershcovich</li>
<li>for: 本研究旨在探讨图意表示（GMRs）在关系EXTRACTION任务中的精确影响。</li>
<li>methods: 本研究提出了一种简单和参数效率高的神经网络架构，用于分离上下文表示学习和结构信息传递。</li>
<li>results: 研究结果表明，GMRs在四个英文和两个中文 dataset 中有所提高表达关系的性能，特别是英文dataset更加精确。然而，在文学领域dataset中，GMRs的效果较低。这些发现可以为将来关系EXTRACTION任务中的GMRs和 parser设计提供更好的指导。<details>
<summary>Abstract</summary>
In the field of natural language understanding, the intersection of neural models and graph meaning representations (GMRs) remains a compelling area of research. Despite the growing interest, a critical gap persists in understanding the exact influence of GMRs, particularly concerning relation extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and parameter-efficient neural architecture designed to decouple contextual representation learning from structural information propagation. Coupled with various sequence encoders and GMRs, this architecture provides a foundation for systematic experimentation on two English and two Chinese datasets. Our empirical analysis utilizes four different graph formalisms and nine parsers. The results yield a nuanced understanding of GMRs, showing improvements in three out of the four datasets, particularly favoring English over Chinese due to highly accurate parsers. Interestingly, GMRs appear less effective in literary-domain datasets compared to general-domain datasets. These findings lay the groundwork for better-informed design of GMRs and parsers to improve relation classification, which is expected to tangibly impact the future trajectory of natural language understanding research.
</details>
<details>
<summary>摘要</summary>
在自然语言理解领域，神经网络和图意表示（GMR）的交叉研究仍然吸引着广泛的关注。尽管有增长的兴趣，但是关于GMR的具体影响仍然存在一个重要的知识 gap。为了解决这个问题，我们介绍了DAGNN-plus，一种简单而参数有效的神经网络架构，用于分离上下文表示学习和结构信息传递。与不同的序列编码器和GMR相结合，这个架构提供了对系统实验的基础，并在四种图形式和九个解析器的支持下进行了实验分析。我们的实验结果表明，GMR在英文和中文两个领域中的表现不同，特别是在文学领域比通用领域更具有优势。这些发现为将来改进GMR和解析器的设计，以提高关系类别的识别，这将对自然语言理解研究的未来轨迹产生直接的影响。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation"><a href="#Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation" class="headerlink" title="Large Language Model-Aware In-Context Learning for Code Generation"></a>Large Language Model-Aware In-Context Learning for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09748">http://arxiv.org/abs/2310.09748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Li, Ge Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, Zhi Jin</li>
<li>for: 这 paper 的目的是提出一种基于学习的选择方法，以提高 Code Generation 中 LLMS 的培养效果。</li>
<li>methods: 这 paper 使用了 LLMS 自身的生成概率来评估候选示例，然后通过对概率反馈来标注候选示例为正负。最后，通过带有对比学习目标的带有对比学习目标的导入，训练一个有效的检索器，以获得 LLMS 在 Code Generation 中的偏好。</li>
<li>results: 这 paper 的实验结果表明，LAIL 可以在 CodeGen 和 GPT-3.5 上提高 LLMS 的培养效果，相比之前的基eline 提高了11.58%、6.89%和5.07%，以及4.38%、2.85%和2.74%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive in-context learning (ICL) ability in code generation. LLMs take a prompt consisting of requirement-code examples and a new requirement as input, and output new programs. Existing studies have found that ICL is highly dominated by the examples and thus arises research on example selection. However, existing approaches randomly select examples or only consider the textual similarity of requirements to retrieve, leading to sub-optimal performance. In this paper, we propose a novel learning-based selection approach named LAIL (LLM-Aware In-context Learning) for code generation. Given a candidate example, we exploit LLMs themselves to estimate it by considering the generation probabilities of ground-truth programs given a requirement and the example. We then label candidate examples as positive or negative through the probability feedback. Based on the labeled data, we import a contrastive learning objective to train an effective retriever that acquires the preference of LLMs in code generation. We apply LAIL to three LLMs and evaluate it on three representative datasets (e.g., MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in terms of Pass@1, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在代码生成中表现出了吸引人的上下文学习（ICL）能力。LLM 接受一个包含需求代码示例和新需求的提示，并输出新的程序。现有的研究发现，ICL 受到示例的影响很大，因此引发了研究示例选择的研究。然而，现有的方法 Randomly 选择示例或者只考虑需求文本相似性来 retrieve，导致表现不佳。在这篇论文中，我们提出了一种新的学习基于选择方法 named LAIL（LLM-Aware In-context Learning）。给定一个候选示例，我们利用 LLM 自己来估算它，通过考虑需求和示例下的生成概率来Feedback probability。然后，我们将候选示例标记为正例或者负例，根据概率反馈。基于标记数据，我们导入了对比学习目标，以培养一个有效的检索器，使其获得 LLM 在代码生成中的偏好。我们在三个 LLM 上应用 LAIL，并对 MBJP、MBPP 和 MBCPP 三个表示性数据集进行评估。LATA 与当前基eline 相比，提高了代码生成的性能，具体是11.58%、6.89% 和 5.07% 的提升。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining"><a href="#Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining" class="headerlink" title="Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining"></a>Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12172">http://arxiv.org/abs/2310.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhexiong Liu, Mohamed Elaraby, Yang Zhong, Diane Litman</li>
<li>for: 这篇论文提供了 ImageArg 共同任务的概述，这是第一个 Multimodal Argument Mining 共同任务，它在 EMNLP 2023 年度会议上召开。</li>
<li>methods: 这篇论文描述了两个分类子任务：（1）Argument Stance Classification，即判断一个包含图片和文本的推文是否支持或反对一个热点话题（如枪支持和堕胎）；（2）Image Persuasiveness Classification，即判断图片是否使文本更加吸引人。</li>
<li>results: 这个共同任务收到了 31 个参赛作品，其中 21 个来自 9 个团队，来自 6 个国家。最佳提交在 Subtask-A 中获得了 F1 分数 0.8647，而在 Subtask-B 中获得了 F1 分数 0.5561。<details>
<summary>Abstract</summary>
This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了图像论据共同任务（ImageArg），这是在EMNLP 2023年工作坊上的第一个多Modal Argument Mining共同任务。该任务包括两个分类子任务：（1）子任务A：图像立场分类；（2）子任务B：图像宣传效果分类。前者确定一个推文中的图像和文本对于一个争议话题（例如，枪支控制和堕胎）的立场。后者确定图像是否使得推文文本更加吸引人。共同任务收到了31个提交 для子任务A和21个提交 для子任务B来自9个不同的团队在6个国家。最佳提交在子任务A中取得了F1分数0.8647，而最佳提交在子任务B中取得了F1分数0.5561。
</details></li>
</ul>
<hr>
<h2 id="KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models"><a href="#KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models" class="headerlink" title="KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models"></a>KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09725">http://arxiv.org/abs/2310.09725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopoldwhite/kgquiz">https://github.com/leopoldwhite/kgquiz</a></li>
<li>paper_authors: Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov</li>
<li>for: 这个论文旨在探讨大语言模型（LLM）在知识培养任务上的表现，以及如何系统地评估LLM的知识能力和其在不同知识领域和任务格式下的普适性。</li>
<li>methods: 这篇论文提出了一个名为KGQuiz的知识强度测试 benchmark，用于全面检验LLM的知识普适性和可行性。KGQuiz包括三个知识领域和五种任务 formats，从简单的真或假问题到复杂的开放知识生成。</li>
<li>results: 经过广泛的实验表明，LLM在简单的知识 QA 任务上表现出色，但是需要更复杂的推理或使用域pecific的知识时仍然存在很大挑战。这些结果表明KGQuiz可以用于分析LLM的知识能力和普适性在不同知识领域和任务格式下的变化。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks, while settings and contexts requiring more complex reasoning or employing domain-specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats, and ultimately to understand, evaluate, and improve LLMs' knowledge abilities across a wide spectrum of knowledge domains and tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在知识密集任务中表现出色，表明其模型参数中含有真实世界知识。然而，关于如何系统地评估 LLM 的知识能力和其知识能力是否可以普遍应用于多个知识领域和复杂任务格式，还不够了解。为此，我们提出了 KGQuiz，一个用于全面探索 LLM 的知识普适能力的benchmark。KGQuiz 基于 triplet 知识结构，覆盖了三个知识领域，包括五种任务 formats，从简单的true-or-false 和多选问答，到复杂的blank filling和factual editing，最后是开放式知识生成。为了更好地理解 LLM 的知识能力和其普适性，我们在 KGQuiz benchmark 上测试了 10 个开源和黑盒 LLM，并进行了广泛的实验。结果表明， LLM 在直观知识 QA 任务中表现出色，但是需要更复杂的解释或使用域pecific的事实时仍然存在很大的挑战。我们认为 KGQuiz 可以作为一个测试台来分析这些 nuanced 的表现差异，并 ultimately 理解、评估和提高 LLM 的知识能力在多个知识领域和任务格式中。
</details></li>
</ul>
<hr>
<h2 id="HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings"><a href="#HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings" class="headerlink" title="HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings"></a>HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09720">http://arxiv.org/abs/2310.09720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuofeng Wu, Chaowei Xiao, VG Vinod Vydiswaran</li>
<li>for: 提高序列表示学习的效率和有效性，通过地方归一化和全序列归一化的对比学习来学习地方和全序列之间的关系。</li>
<li>methods: 提出了一种层次对比学习框架（HiCL），将序列分成多个段，使用地方和全序列归一化对比学习来学习段级和序列级关系，并且通过首先编码短段并然后聚合以提高训练效率。</li>
<li>results: 对比于传统方法，HiCL能够提高7种广泛评估的STS任务的前一个表现，升师平均提高+0.2%（BERT-large）和+0.44%（RoBERTa-large）。<details>
<summary>Abstract</summary>
In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others, often neglecting local representation learning, leading to challenges in generalizing to shorter texts. Conversely, HiCL improves its effectiveness by dividing the sequence into several segments and employing both local and global contrastive learning to model segment-level and sequence-level relationships. Further, considering the quadratic time complexity of transformers over input tokens, HiCL boosts training efficiency by first encoding short segments and then aggregating them to obtain the sequence representation. Extensive experiments show that HiCL enhances the prior top-performing SNCSE model across seven extensively evaluated STS tasks, with an average increase of +0.2% observed on BERT-large and +0.44% on RoBERTa-large.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个层次对比学习框架，即HiCL，该框架考虑了本地分割段级和全序列级关系，以提高训练效率和有效性。传统方法通常将序列编码为整体对比他们，而忽略本地表示学习，这会导致对短文本掌握困难。相反，HiCL通过将序列分割成多个段，并使用本地和全序列对比学习来模型段级和序列级关系。此外，考虑到 transformer 对输入字符数的平方时间复杂度，HiCL 提高了训练效率，先对短段进行编码，然后将其聚合以获得序列表示。广泛的实验表明，HiCL 可以提高先前的最佳 SNCSE 模型在七个广泛评估的 STS 任务上，平均提高 +0.2% 在 BERT-large 上和 +0.44% 在 RoBERTa-large 上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.CL_2023_10_15/" data-id="clpztdnep00dges886magb80a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/15/cs.AI_2023_10_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-10-15
        
      </div>
    </a>
  
  
    <a href="/2023/10/15/cs.LG_2023_10_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-10-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
