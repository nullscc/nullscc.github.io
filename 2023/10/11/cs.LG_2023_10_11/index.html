
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-10-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07940 repo_url: None paper_authors: Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Pune">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-10-11">
<meta property="og:url" content="https://nullscc.github.io/2023/10/11/cs.LG_2023_10_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07940 repo_url: None paper_authors: Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Pune">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-11T10:00:00.000Z">
<meta property="article:modified_time" content="2023-10-16T04:38:04.015Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.LG_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T10:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-10-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines"><a href="#Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines" class="headerlink" title="Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines"></a>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07940">http://arxiv.org/abs/2310.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Puneet Gupta</li>
<li>for: This paper aims to enable widely-applicable smart devices by overcoming the storage and processing requirements of deep neural networks.</li>
<li>methods: The paper explores hardware&#x2F;software co-design, quantization, model scaling, and multi-modality to optimize system design and model deployment for cost-constrained platforms.</li>
<li>results: The paper demonstrates an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board, and provides guidelines for optimal system design and model deployment.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是使深度神经网络的存储和处理要求不再限制智能设备的普及。</li>
<li>methods: 这篇论文采用硬件&#x2F;软件共设方法，包括量化、模型缩放和多模态，以优化系统设计和模型部署，以满足成本和用户体验的要求。</li>
<li>results: 这篇论文通过使用 $20 ESP-EYE 板子实现了端到端、在设备上进行生物认证系统，并提供了优化系统设计和模型部署的指南。<details>
<summary>Abstract</summary>
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and develop a set of guidelines for optimal system design and model deployment for the most cost-constrained platforms. We demonstrate our approach using an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables"><a href="#Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables" class="headerlink" title="Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables"></a>Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07927">http://arxiv.org/abs/2310.07927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Zou, Pratyush Tiwary</li>
<li>for: 这种研究使用图 neural network 学习方法，以获取实验晶体结构中观察到的特征变量的低维度表示。</li>
<li>methods: 这种方法使用简单的卷积和聚合方法，并使用自适应器设置。</li>
<li>results: 研究人员通过对铁和глицин的多种相态和晶体结构的熔融态进行考察，发现图 latent variables 在宽温度热动力学中偏导转换 между状态，并实现了准确的自由能计算，两者都是样本采集的标志。<details>
<summary>Abstract</summary>
In this study, we present a graph neural network-based learning approach using an autoencoder setup to derive low-dimensional variables from features observed in experimental crystal structures. These variables are then biased in enhanced sampling to observe state-to-state transitions and reliable thermodynamic weights. Our approach uses simple convolution and pooling methods. To verify the effectiveness of our protocol, we examined the nucleation of various allotropes and polymorphs of iron and glycine from their molten states. Our graph latent variables when biased in well-tempered metadynamics consistently show transitions between states and achieve accurate free energy calculations in agreement with experiments, both of which are indicators of dependable sampling. This underscores the strength and promise of our graph neural net variables for improved sampling. The protocol shown here should be applicable for other systems and with other sampling methods.
</details>
<details>
<summary>摘要</summary>
在这种研究中，我们提出了基于图 neural network的学习方法，使用自适应Encoder设置来 derivate低维度变量从实验室中观察的晶体结构特征。这些变量然后在增强抽样中偏导，以观察状态之间的转移和可靠的热力学权重。我们的方法使用简单的卷积和聚合方法。为了验证我们的协议的有效性，我们对铁和glycine的多种晶体和多形的气相转换进行了研究。我们的图秘密变量，当偏导在well-tempered metadynamics中， consistently display state transitions and achieve accurate free energy calculations, both of which are indicators of reliable sampling.这些结果证明了我们的图 neural net变量的强大和承诺，并且这种协议应用于其他系统和其他抽样方法。
</details></li>
</ul>
<hr>
<h2 id="First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs"><a href="#First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs" class="headerlink" title="First-Order Dynamic Optimization for Streaming Convex Costs"></a>First-Order Dynamic Optimization for Streaming Convex Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07925">http://arxiv.org/abs/2310.07925</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Rostami, H. Moradian, S. S. Kia</li>
<li>for: 这个论文是为了解决一类具有时间变化的 convex 优化问题而提出的一些新的优化算法。</li>
<li>methods: 我们开发了一种方法来跟踪优化问题的最优解，并且只使用第一个Derivative 来实现。这使得我们的算法在优化时间变化的成本函数时更加 computationally efficient。</li>
<li>results: 我们比较了我们的算法与梯度下降算法，并证明了梯度下降算法在优化问题中不是有效的。我们还提供了一些例子，包括使用模型预测控制问题作为 convex 优化问题的解决方案。<details>
<summary>Abstract</summary>
This paper proposes a set of novel optimization algorithms for solving a class of convex optimization problems with time-varying streaming cost function. We develop an approach to track the optimal solution with a bounded error. Unlike the existing results, our algorithm is executed only by using the first-order derivatives of the cost function which makes it computationally efficient for optimization with time-varying cost function. We compare our algorithms to the gradient descent algorithm and show why gradient descent is not an effective solution for optimization problems with time-varying cost. Several examples including solving a model predictive control problem cast as a convex optimization problem with a streaming time-varying cost function demonstrate our results.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的优化算法集合，用于解决时间变化的流动成本函数 convex 优化问题。我们开发了一种方法来跟踪最优解，并保证错误在 bounded 的范围内。与现有结果不同，我们的算法仅使用函数成本的首个导数进行计算，这使得它在时间变化的成本函数上进行优化变得计算效率。我们与梯度下降算法进行比较，并解释了为什么梯度下降算法不适合时间变化的成本函数上进行优化。 several 个例子，包括解决一个模型预测控制问题，即asta convex 优化问题，演示了我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning"><a href="#Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning" class="headerlink" title="Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning"></a>Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07902">http://arxiv.org/abs/2310.07902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noémie Jaquier, Leonel Rozo, Tamim Asfour<br>for: This paper aims to address the “single tangent space fallacy” in the incorporation of Riemannian geometry into robot learning applications, and to provide a theoretical elucidation of various misconceptions surrounding this approach.methods: The paper offers experimental evidence of the shortcomings of this approach and provides valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.results: The paper presents a theoretically-founded critique of the “single tangent space fallacy” and demonstrates its limitations through experimental evidence. It provides valuable insights to promote best practices in the use of Riemannian geometry in robot learning applications.<details>
<summary>Abstract</summary>
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.
</details>
<details>
<summary>摘要</summary>
在机器人学领域，许多下游机器人任务利用机器学习方法处理、模型或生成数据。经常情况下，这些数据包含具有几何约束的变量，如rigid-body orientations的quaternions的单位 нормаcondition或操作灵活性 ellipsoids的正见定义性。有效地处理这些几何约束需要在机器学习方法的形ulation中包含泛函几何工具。在这个上下文中，里曼几何 manifold emerges as a powerful mathematical framework to handle such geometric constraints. However, its recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the "single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.
</details></li>
</ul>
<hr>
<h2 id="Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs"><a href="#Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs" class="headerlink" title="Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs"></a>Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07895">http://arxiv.org/abs/2310.07895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Werner, Christoph Gerum, Moritz Reiber, Jörg Nick, Oliver Bringmann</li>
<li>for: 这个论文是用于分类 Gastroenterologic 部分图像，来自 Video Capsule Endoscopy (VCE) 研究。</li>
<li>methods: 这个方法利用 Convolutional Neural Network (CNN) 进行分类，并利用时间序列分析特性来 correction errors in CNN 输出。</li>
<li>results: 这个方法在 Rhode Island (RI) Gastroenterology 数据集上达到了 $98.04%$ 的准确率，可以准确地定位 Gastrointestinal (GI) 轨道，仅需约 1M 参数，适用于低功耗设备。<details>
<summary>Abstract</summary>
This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control"><a href="#ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control" class="headerlink" title="ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control"></a>ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07892">http://arxiv.org/abs/2310.07892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalil Chavez-Galaviz, Jianwen Li, Ajinkya Chaudhary, Nina Mahmoudian</li>
<li>For: The paper is written for Autonomous Surface Vehicles (ASVs) to improve their station-keeping ability in confined spaces with wind disturbances.* Methods: The paper proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances.* Results: The proposed NNSEM-MPC approach performs better than other controllers in simulation and field experiments, reducing the mean position and heading error by at least 31% and 46%, respectively, and is at least 36% faster than other MPC controllers.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为Autonomous Surface Vehicles（ASV）提高其在狭窄空间中的定站能力，特别是在风干扰下。</li>
<li>methods: 这篇论文提出了一种基于Neural Network Simulation Error Minimization（NNSEM-MPC）的模型预测控制器，以准确预测ASV在风干扰下的动态。</li>
<li>results: 对于 simulate 和实验测试，提出的NNSEM-MPC方法与其他控制器相比，可以减少平均定位和方向误差至少31%和46%，并且在其他MPC控制器的执行速度上占优，至少36% faster。<details>
<summary>Abstract</summary>
Station keeping is an essential maneuver for Autonomous Surface Vehicles (ASVs), mainly when used in confined spaces, to carry out surveys that require the ASV to keep its position or in collaboration with other vehicles where the relative position has an impact over the mission. However, this maneuver can become challenging for classic feedback controllers due to the need for an accurate model of the ASV dynamics and the environmental disturbances. This work proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances. The performance of the proposed scheme under wind disturbances is tested and compared against other controllers in simulation, using the Robotics Operating System (ROS) and the multipurpose simulation environment Gazebo. A set of six tests were conducted by combining two wind speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and 180$^\circ$). The simulation results clearly show the advantage of the NNSEM-MPC over the following methods: backstepping controller, sliding mode controller, simplified dynamics MPC (SD-MPC), neural ordinary differential equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed NNSEM-MPC approach performs better than the rest in 4 out of the 6 test conditions, and it is the second best in the 2 remaining test cases, reducing the mean position and heading error by at least 31\% and 46\% respectively across all the test cases. In terms of execution speed, the proposed NNSEM-MPC is at least 36\% faster than the rest of the MPC controllers. The field experiments on two different ASV platforms showed that ASVs can effectively keep the station utilizing the proposed method, with a position error as low as $1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at least $150$s.
</details>
<details>
<summary>摘要</summary>
驱动Autonomous Surface Vehicles (ASVs)的基本推进方法是站 Keeping，特别在封闭空间中使用，以进行需要ASV保持其位置或与其他车辆相合作，其中相对位置对任务有着重要影响。然而，这种推进方法可能会对 класси型反馈控制器造成挑战，因为需要高度准确的ASV动态模型和环境干扰的模型。本文提出一种使用神经网络预测误差最小化（NNSEM-MPC）的模型预测控制器，以准确预测ASV在风干扰下的动态。在实验中，与其他控制器进行比较，使用Robotics Operating System（ROS）和 multipurpose simulation environment Gazebo进行测试，共进行六个测试，其中两个风速（3 m/s和6 m/s）和三个风向（0$^\circ$, 90$^\circ$,和180$^\circ）。测试结果表明，提议的NNSEM-MPC方法在六个测试条件中比其他方法 луч，其中四个测试条件中表现最佳，剩下两个测试条件中表现第二好，可以将预测误差和 heading error降低至少31%和46%。在执行速度方面，提议的NNSEM-MPC方法至少36%更快于其他MPC控制器。在两个不同的ASV平台上进行了田下实验，结果表明，ASV可以效果地保持站点，位置误差为1.68米， heading error为6.14°，在至少150秒的时间窗口内。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks"><a href="#A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks" class="headerlink" title="A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks"></a>A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07891">http://arxiv.org/abs/2310.07891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban</li>
<li>for: This paper is written to explore the feature learning process in deep neural networks, specifically the appearance of separated rank-one components in the spectrum of the feature matrix, and how this can lead to improved learning of non-linear components.</li>
<li>methods: The paper uses two-layer fully-connected neural networks with a constant gradient descent step size, and shows that with a learning rate that grows with the sample size, multiple rank-one components are introduced, each corresponding to a specific polynomial feature.</li>
<li>results: The paper demonstrates that these non-linear features can enhance learning, and provides precise analysis of the improvement in the loss. The limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes.<details>
<summary>Abstract</summary>
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Feature learning是深度神经网络的成功的重要原因之一。已经确立了在满足certain conditions下，两层受束神经网络中的一步梯度下降后，ridge回归可以导致特征学习;这被称为特征突起的存在，特征突起在特征矩阵的спектру中出现。然而，固定梯度下降步长，这个突起只会带来linear компонент的信息，因此学习非线性Component是不可能的。我们展示了，在样本大小和训练样本数量增加时，使用增长的学习率，实际上会引入多个rank-one组件，每个组件都对应于特定的多项式特征。我们进一步证明，在大量训练和测试样本下，更新后的神经网络的训练和测试错误都会完全受到这些突起的限制。通过精确分析失准的改进，我们证明了这些非线性特征可以提高学习。Note: Simplified Chinese is used here, as it is the standard form of Chinese used in mainland China and widely understood by Chinese speakers. Traditional Chinese is also widely used, but it may not be as familiar to all Chinese speakers.
</details></li>
</ul>
<hr>
<h2 id="Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression"><a href="#Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression" class="headerlink" title="Refined Mechanism Design for Approximately Structured Priors via Active Regression"></a>Refined Mechanism Design for Approximately Structured Priors via Active Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07874">http://arxiv.org/abs/2310.07874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Boutsikas, Petros Drineas, Marios Mertzanidis, Alexandros Psomas, Paritosh Verma</li>
<li>for: 本文主要针对一个带有大量商品的销售商面临多个竞拍客户的情况，并且客户的价值评估是从高维不知的先验分布中独立采样的。</li>
<li>methods: 本文使用了一种Recently introduced by Cai and Daskalakis的模型，即将投标客户的先验分布近似为主题模型。文章包括一个活动学部分，负责与投标客户进行互动，并输出低维的客户类型估计，以及一个机制设计部分，负责为低维模型中的类型进行机制设计。</li>
<li>results: 文章提出了一种新的机制设计方法，可以在不需要访问到原始分布的情况下，针对低维模型中的类型进行机制设计。此外，文章还 imports 了多个Randomized Linear Algebra (RLA) 的突破成果，并将其适应到该设定中。<details>
<summary>Abstract</summary>
We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast our problem in the framework of Randomized Linear Algebra (RLA) for regression problems, allowing us to import several breakthrough results from that line of research, and adapt them to our setting. On the mechanism design front, we remove many restrictive assumptions of prior work on the type of access needed to the underlying distributions and the associated mechanisms. To the best of our knowledge, our work is the first to formulate connections between mechanism design, and RLA for active learning of regression problems, opening the door for further applications of randomized linear algebra primitives to mechanism design.
</details>
<details>
<summary>摘要</summary>
我们考虑一个收益最大化的买家，有一大量的物品($m$)供$n$名策略性投标者购买，投标者的价值是由高维度、未知的对应分布所生成的。这个设定中的优化和近似优化机制是非常困难 Compute 和找到，而且当机制存在时，它们常常具有各种Counter-intuitive 的性质。在这篇文章中，我们遵循 Cai 和 Daskalakis （2022）的模型，假设投标者的对应分布可以被快速地近似为主题模型。我们设计了一个活动学习部分，负责与投标者进行互动，并将投标者的类型转换为低维度的近似值，以及一个机制设计部分，负责对低维度模型进行机制设计，以适应投标者的近似类型。在活动学习方面，我们将问题推到了Randomized Linear Algebra（RLA）的框架中，这让我们可以将RLA中的多个突破性结果import 到我们的设定中，并适应它们。在机制设计方面，我们从之前的研究中解除了访问到底下的分布和相关机制的严格限制，以便更好地适应实际的应用。根据我们所知，我们的工作是首个将机制设计与RLA для活动学习的回传问题连接起来，这开启了进一步应用randomized linear algebra primitives 的可能性。
</details></li>
</ul>
<hr>
<h2 id="QArchSearch-A-Scalable-Quantum-Architecture-Search-Package"><a href="#QArchSearch-A-Scalable-Quantum-Architecture-Search-Package" class="headerlink" title="QArchSearch: A Scalable Quantum Architecture Search Package"></a>QArchSearch: A Scalable Quantum Architecture Search Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07858">http://arxiv.org/abs/2310.07858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankit Kulshrestha, Danylo Lykov, Ilya Safro, Yuri Alexeev</li>
<li>for: 这篇论文的目的是提供一种基于人工智能的量子架构搜索包，用于实现适当的单位变换以处理输入量子态。</li>
<li>methods: 该论文使用了\texttt{QTensor}库作为后端，并采用了一种两级并行的搜索策略，以实现高效地搜索大型量子电路。</li>
<li>results: 该论文通过实验表明，\texttt{QArchSearch}能够高效地搜索大型量子电路，并且可以允许更多的复杂模型 для不同的量子应用。<details>
<summary>Abstract</summary>
The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \texttt{QArchSearch}, an AI based quantum architecture search package with the \texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polaris supercomputer.
</details>
<details>
<summary>摘要</summary>
当今量子计算时代已经浮现了许多算法，这些算法承诺可以提供高效的计算。然而，在实际应用中，很少有指导如何设计合适的量子电路，以实现对输入量子态的相应的单位变换。在这篇论文中，我们介绍了\texttt{QArchSearch}，一个基于人工智能的量子架构搜索包，它使用\texttt{QTensor}库作为后端，并提供了一种原则性的和自动化的方法来找到任务和输入量子态的最佳模型。我们示示了\texttt{QArchSearch}能够有效率地搜索大量的量子电路，并允许探索不同的量子应用程序中的更复杂的模型。\texttt{QArchSearch}在高性能计算系统上运行，使用了两级并行的分布式计算策略，在CPUs和GPUs上进行并行计算，这已经在Polaris超级计算机上得到证明。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism"><a href="#On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism" class="headerlink" title="On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism"></a>On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07852">http://arxiv.org/abs/2310.07852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saptarshi Roy, Ambuj Tewari</li>
<li>for: 这个论文主要针对高维ensional sparse linear regression模型下的隐私保护问题，具体来说是 differentially private best subset selection 问题。</li>
<li>methods: 论文采用了 Well-known exponential mechanism 选择最佳模型，并在某种margin condition下确立了它的强模型重建性。</li>
<li>results: 论文提出了一种 Metropolis-Hastings 算法来避免 exponential search space 的计算瓶颈，并证明了它的多项式混合时间减小到 Parameters $n,p$, 和 $s$ 上。此外，论文还证明了 final estimates 的 approximate differential privacy 性。<details>
<summary>Abstract</summary>
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results.
</details>
<details>
<summary>摘要</summary>
我们考虑了在高维稀疏线性回归模型下进行模型选择，并在拥有差异隐私框架下进行研究。特别是，我们研究了具有差异隐私最佳子集选择的问题，并对其Utility guarantee进行了研究。我们采用了广泛使用的指数机制来选择最佳模型，并在满足某种margin条件下，Establish its strong model recovery property。然而，指数搜索空间中的指数机制带来了严重的计算瓶颈。为了解决这个挑战，我们提议了 Metropolis-Hastings算法来实现抽样步骤，并证明其在参数$n$, $p$, 和 $s$中的几何时间到其stationary distribution的 mixing property。此外，我们还证明了对最终估计的Metropolis-Hastings随机步进行了approximate差异隐私。最后，我们还进行了一些与 теория结论相符的实验 simulations。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Feature-Sparsity-in-Language-Models"><a href="#Measuring-Feature-Sparsity-in-Language-Models" class="headerlink" title="Measuring Feature Sparsity in Language Models"></a>Measuring Feature Sparsity in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07837">http://arxiv.org/abs/2310.07837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Deng, Lucas Tao, Joe Benton</li>
<li>for: 这些研究旨在探索语言模型活动可以被模型为稀疏线性组合的哪些特征。</li>
<li>methods: 这些研究使用稀疏编码技术来重建特征方向。</li>
<li>results: 研究发现，语言模型活动可以准确地被模型为稀疏线性组合的特征，比控制数据集更加准确。此外，模型活动显示在第一层和最后一层比较稀疏。<details>
<summary>Abstract</summary>
Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters"><a href="#Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters" class="headerlink" title="Large Language Models Are Zero-Shot Time Series Forecasters"></a>Large Language Models Are Zero-Shot Time Series Forecasters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07820">http://arxiv.org/abs/2310.07820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngruver/llmtime">https://github.com/ngruver/llmtime</a></li>
<li>paper_authors: Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson</li>
<li>for: 这个论文旨在探讨语言模型如何用于时间序列预测，以及这种方法如何与专门为时间序列预测设计的模型相比。</li>
<li>methods: 作者使用了大语言模型（LLM） such as GPT-3和LLaMA-2，并提出了一些方法来减少数字化时间序列数据的复杂性，例如使用简单的数字符号来表示复杂的时间序列 distribuions。</li>
<li>results: 作者发现，使用LLM可以在无需特定训练的情况下，对时间序列进行预测，并且其性能与专门为时间序列预测设计的模型相当或甚至超过。此外，LLM还可以处理缺失数据、考虑文本副信息和回答预测问题。然而，作者发现，增加模型大小通常会提高时间序列预测的性能，但GPT-4可能会比GPT-3 worse perfomance due to its tokenization strategy and uncertainty calibration issues。<details>
<summary>Abstract</summary>
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore"><a href="#Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore" class="headerlink" title="Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore"></a>Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07811">http://arxiv.org/abs/2310.07811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gellért Weisz, András György, Csaba Szepesvári</li>
<li>for: 这个论文研究的目的是提出一种在线学习 Reinforcement Learning（RL）算法，可以在 episodic Markov decision processes（MDPs）中实现 polynomial-sample-complexity 的学习。</li>
<li>methods: 该算法使用了 linear $q^\pi$-realizability 假设，即所有策略的动作价值可以表示为线性函数的状态动作特征。该假设被认为是更通用的，比 linear MDPs 更加一般。</li>
<li>results: 作者们提出了一种新的学习算法，可以在 linearly $q^\pi$-realizable MDPs 中同时学习缺省策略和隐藏的 linear MDP 问题。该算法可以在 $\text{polylog}(H, d)&#x2F;\epsilon^2$ 交互后返回 $\epsilon$-优化策略，其中 $H$ 是时间范围和 $d$ 是特征向量的维度。此外，作者们还证明了这种算法在不准确的情况下的样本复杂性。<details>
<summary>Abstract</summary>
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.
</details>
<details>
<summary>摘要</summary>
我们考虑在线上强化学习（RL）中的集集合数 Markov问题（MDP）下，假设所有政策的动作值可以表示为特征vector的线性函数。这个分类被认为是线性 MDP 的更一般化情况，而不是单纯的线性 MDP，其中transition构造和优化函数都是特征 вектор的线性函数。作为我们的第一个贡献，我们显示了这两个分类之间的差异在于在线性 qπ 可能性下的状态存在。具体来说，任何政策在这些状态下都有相当的动作值，并且透过 seguing 到这些状态中的任何政策可以将问题转换为线性 MDP。基于这个观察，我们提出了一个新的学习算法，可以同时学习哪些状态应该被跳过以及将这些状态转换为线性 MDP 中的问题。这个方法可以在 $\text{polylog}(H, d)/\epsilon^2$ 互动后返回 $\epsilon$-优化的政策，其中 $H$ 是时间点数和 $d$ 是特征 вектор的维度。我们还证明了这个方法在不精确情况下的样本复杂度，其样本复杂度会随着错误水平的下降。
</details></li>
</ul>
<hr>
<h2 id="FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning"><a href="#FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning" class="headerlink" title="FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning"></a>FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07807">http://arxiv.org/abs/2310.07807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ensiye Kiyamousavi, Boris Kraychev, Ivan Koychev<br>for: This paper focuses on addressing the challenges of data heterogeneity and model aggregation effectiveness in federated learning (FL) by proposing a method that leverages entropy and symmetry to construct diverse and controllable data distributions.methods: The proposed method uses techniques such as data partitioning, entropy, and symmetry to create ‘the most challenging’ and controllable data distributions with gradual difficulty.results: The proposed method is shown to be superior to existing FL data partitioning approaches, with the ability to gradually challenge model aggregation algorithms and produce more distinct models. Experimental results demonstrate the effectiveness of the proposed method in improving the robustness and accuracy of FL models.<details>
<summary>Abstract</summary>
Federated learning (FL) is a decentralized machine learning approach where independent learners process data privately. Its goal is to create a robust and accurate model by aggregating and retraining local models over multiple rounds. However, FL faces challenges regarding data heterogeneity and model aggregation effectiveness. In order to simulate real-world data, researchers use methods for data partitioning that transform a dataset designated for centralized learning into a group of sub-datasets suitable for distributed machine learning with different data heterogeneity. In this paper, we study the currently popular data partitioning techniques and visualize their main disadvantages: the lack of precision in the data diversity, which leads to unreliable heterogeneity indexes, and the inability to incrementally challenge the FL algorithms. To resolve this problem, we propose a method that leverages entropy and symmetry to construct 'the most challenging' and controllable data distributions with gradual difficulty. We introduce a metric to measure data heterogeneity among the learning agents and a transformation technique that divides any dataset into splits with precise data diversity. Through a comparative study, we demonstrate the superiority of our method over existing FL data partitioning approaches, showcasing its potential to challenge model aggregation algorithms. Experimental results indicate that our approach gradually challenges the FL strategies, and the models trained on FedSym distributions are more distinct.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式机器学习方法，其目标是通过聚合和重新训练本地模型来创建一个坚固和准确的模型。然而，FL面临着数据多样性和模型聚合效果的挑战。为了模拟实际数据，研究人员使用数据 partitioning 技术将一个用于中央化学习的数据集转换为适于分布式机器学习的多个子集，每个子集都具有不同的数据多样性。在这篇论文中，我们研究了目前流行的数据 partitioning 技术，并将其主要缺点进行视觉化：缺乏精度的数据多样性，导致不可靠的多样性指标，以及无法逐步挑战 FL 算法。为解决这个问题，我们提出了一种方法，该方法利用熵和对称来构建 '最具挑战性' 和可控的数据分布。我们引入了一个度量来衡量学习代理之间的数据多样性，并提出了一种分割技术，可以将任何数据集分成多个分布式学习适用的Split。通过比较研究，我们证明了我们的方法在现有 FL 数据 partitioning 方法之上具有优势，并表明其可以逐步挑战 FL 算法。实验结果表明，我们的方法可以逐步挑战 FL 策略，并且模型在 FedSym 分布上训练得到更加独特。
</details></li>
</ul>
<hr>
<h2 id="Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data"><a href="#Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data" class="headerlink" title="Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data"></a>Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07787">http://arxiv.org/abs/2310.07787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Wong, Phue Thant, Pratiksha Yadav, Ruta Antaliya, Jongwook Woo</li>
<li>For: The paper aims to predict airline ticket fares for non-stop flights across the US using a large dataset of flight pricing data.* Methods: The paper uses four regression machine learning algorithms (Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines) and assesses their performance and generalization capability using Cross Validator and Training Validator functions.* Results: The paper seeks to determine the best models for predicting airline ticket fares in the real world, with a focus on good generalization capability and optimized processing times.<details>
<summary>Abstract</summary>
This paper discusses predictive performance and processes undertaken on flight pricing data utilizing r2(r-square) and RMSE that leverages a large dataset, originally from Expedia.com, consisting of approximately 20 million records or 4.68 gigabytes. The project aims to determine the best models usable in the real world to predict airline ticket fares for non-stop flights across the US. Therefore, good generalization capability and optimized processing times are important measures for the model.   We will discover key business insights utilizing feature importance and discuss the process and tools used for our analysis. Four regression machine learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines utilizing Cross Validator and Training Validator functions for assessing performance and generalization capability.
</details>
<details>
<summary>摘要</summary>
这篇论文讨论了预测性能和基于飞行价格数据进行的处理过程，使用r2（r平方）和RMSE指标，利用Expedia.com原始数据集，包含约2000万记录或4.68 gigabytes的数据。项目的目标是找到适用于实际世界的最佳预测模型，以预测美国之间的直达航班票价。因此，良好的总体化能力和优化的处理时间是重要的评价指标。我们将通过特征重要性的探索和分析工具的介绍，挖掘出预测模型的关键业务洞察。在本项目中，我们使用了四种回归机器学习算法：随机森林、梯度提升树、决策树和分解机器，并使用了交叉验证和训练验证函数来评估性能和总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling"><a href="#Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling" class="headerlink" title="Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling"></a>Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07786">http://arxiv.org/abs/2310.07786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheqing Zhu, Yueyang Liu, Xu Kuang, Benjamin Van Roy</li>
<li>for: 非站立性上下文强化学习问题的实际应用，如推荐系统等，经常受到季节性、偶然性和社会趋势的影响，导致非站立性上下文强化学习算法的开发成为紧迫需要。</li>
<li>methods: 我们提出了一种新的非站立性上下文强化学习算法，它结合了可扩展的深度神经网络结构和精心设计的探索机制，以优先级Collect informative data with the most lasting value in a non-stationary environment。</li>
<li>results: 我们通过对两个实际推荐数据集进行实验，发现我们的方法与状态艺术基eline Significantly outperform the baseline。<details>
<summary>Abstract</summary>
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
在实际应用中，Contextual Bandits的应用 frequently exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. Although a number of non-stationary Contextual Bandit learning algorithms have been proposed in the literature, they often explore excessively due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action sets, or both. In this paper, we introduce a novel non-stationary Contextual Bandit algorithm that addresses these concerns. It combines a scalable, deep neural network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches"><a href="#Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches" class="headerlink" title="Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches"></a>Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07780">http://arxiv.org/abs/2310.07780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng</li>
<li>for: 提高随机缓和的抗击性能，以提供可证明的鲁棒性保证。</li>
<li>methods: 提出了两种成本效果的方法，包括AdvMacer和EsbRS。AdvMacer combinines adversarial training和鲁棒性证明最大化，而EsbRS使用模型集成来提高鲁棒性证明。</li>
<li>results: 比较SOTA基线的实验结果表明，AdvMacer可以提高随机缓和的抗击性能，而EsbRS可以大幅提高模型集成的鲁棒性。<details>
<summary>Abstract</summary>
Randomized smoothing has recently attracted attentions in the field of adversarial robustness to provide provable robustness guarantees on smoothed neural network classifiers. However, existing works show that vanilla randomized smoothing usually does not provide good robustness performance and often requires (re)training techniques on the base classifier in order to boost the robustness of the resulting smoothed classifier. In this work, we propose two cost-effective approaches to boost the robustness of randomized smoothing while preserving its clean performance. The first approach introduces a new robust training method AdvMacerwhich combines adversarial training and robustness certification maximization for randomized smoothing. We show that AdvMacer can improve the robustness performance of randomized smoothing classifiers compared to SOTA baselines, while being 3x faster to train than MACER baseline. The second approach introduces a post-processing method EsbRS which greatly improves the robustness certificate based on building model ensembles. We explore different aspects of model ensembles that has not been studied by prior works and propose a novel design methodology to further improve robustness of the ensemble based on our theoretical analysis.
</details>
<details>
<summary>摘要</summary>
优化后的随机缓和方法在反击机器学习领域吸引了关注，以提供可证明的安全保证。然而，现有的工作表明，普通的随机缓和方法通常不提供好的安全性表现，经常需要（重）训练技术来提高随机缓和后的类ifier的安全性。在这项工作中，我们提出了两种可行的方法来提高随机缓和方法的安全性，同时保持其净度性。第一种方法是我们提出的 AdvMacer，它结合了对随机缓和类ifier的对抗训练和安全性证明最大化。我们表明，AdvMacer可以提高随机缓和类ifier的安全性表现，并且比MACER基线 faster to train。第二种方法是我们提出的 EsbRS，它可以大幅提高基于模型集的安全证明。我们探索了不同的模型集方面，并提出了一种新的设计方法来进一步提高模型集的安全性。我们的理论分析表明，EsbRS可以提高模型集的安全性表现。
</details></li>
</ul>
<hr>
<h2 id="Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights"><a href="#Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights" class="headerlink" title="Feature Learning and Generalization in Deep Networks with Orthogonal Weights"></a>Feature Learning and Generalization in Deep Networks with Orthogonal Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07765">http://arxiv.org/abs/2310.07765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Day, Yonatan Kahn, Daniel A. Roberts</li>
<li>for: 该文章研究了深度 neural network 的调参问题，尤其是在宽度和深度相对较长时，如何避免网络中信号的扩散和干扰。</li>
<li>methods: 作者使用了矩阵 ensemble 和 rectangular network 的方法，并提出了一种新的初始化方法，即使用 orthogonal matrices 初始化网络的权重。</li>
<li>results: 作者通过分析和实验表明，使用这种新的初始化方法可以避免网络中信号的扩散和干扰，并且可以提高网络的泛化和训练速度。<details>
<summary>Abstract</summary>
Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We speculate that this structure preserves finite-width feature learning while reducing overall noise, thus improving both generalization and training speed. We provide some experimental justification by relating empirical measurements of the NTK to the superior performance of deep nonlinear orthogonal networks trained under full-batch gradient descent on the MNIST and CIFAR-10 classification tasks.
</details>
<details>
<summary>摘要</summary>
完全连接深度神经网络的权重初始化为独立的高斯分布可以调整到极点，从而防止信号在网络中 exponential 增长或减少。然而，这些网络仍然会出现 linear 增长的振荡，与网络宽度相比，这可能会降低网络的训练效果。我们 analytically 表明，使用 rectangle 网络和 tanh 活化函数，初始化 weights 为 orthogonal matrices 的ensemble，then the pre-activation fluctuations are independent of depth, to leading order in inverse width. In addition, we numerically show that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We speculate that this structure preserves finite-width feature learning while reducing overall noise, thus improving both generalization and training speed. We provide some experimental justification by relating empirical measurements of the NTK to the superior performance of deep nonlinear orthogonal networks trained under full-batch gradient descent on the MNIST and CIFAR-10 classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Representation-Learning-From-Random-Data-Projectors"><a href="#Self-supervised-Representation-Learning-From-Random-Data-Projectors" class="headerlink" title="Self-supervised Representation Learning From Random Data Projectors"></a>Self-supervised Representation Learning From Random Data Projectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07756">http://arxiv.org/abs/2310.07756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/layer6ai-labs/lfr">https://github.com/layer6ai-labs/lfr</a></li>
<li>paper_authors: Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs</li>
<li>for: 本研究旨在开发一种可以应用于多种数据模式和网络架构的自然语言处理和计算机视觉领域的自我监督学习方法。</li>
<li>methods: 本研究使用随机数据投影来学习高质量的数据表示。</li>
<li>results: 对多种表示学习任务进行了广泛的评估，并与多个状态 искусственный难度基线进行比较，得到了优于基线的结果。<details>
<summary>Abstract</summary>
Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue that learning from randomness is a fruitful research direction worthy of attention and further study.
</details>
<details>
<summary>摘要</summary>
自适应 represencing 学习（SSRL）在人工设计的数据增强下已经取得了很大的进步，通过利用数据增强下的变换不变性假设。而这些增强基于的 SSRL 算法在计算机视觉和自然语言处理领域的性能边缘很高，但它们通常不直接适用于其他数据类型，并且可能与应用特定的数据增强约束 conflicting。这篇文章提出了一种不依赖于增强或masking的 SSRL 方法，可以应用于任何数据类型和网络架构。我们表明，可以通过重建随机数据投影来学习高质量的数据表示。我们对多种表示学习任务进行了广泛的评估，这些任务覆盖了多种Modalities和实际应用。我们发现，该方法可以超过多个状态对的 SSRL 基elines。由于其广泛适用性和强大的实际结果，我们认为学习Randomness 是一个有前途的研究方向，值得关注和进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates"><a href="#Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates" class="headerlink" title="Stabilizing Estimates of Shapley Values with Control Variates"></a>Stabilizing Estimates of Shapley Values with Control Variates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07672">http://arxiv.org/abs/2310.07672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Goldwasser, Giles Hooker</li>
<li>for: 用于稳定黑盒机器学习模型的预测解释</li>
<li>methods: 使用控制SHAP方法，基于Monte Carlo技术的控制变量</li>
<li>results: 在高维数据集上可以 produz dramatic reductions in the Monte Carlo variability of Shapley estimates<details>
<summary>Abstract</summary>
Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates." into 简化字 Simplified Chinese.Here's the translation:<<SYS>>预测黑obox机器学习模型的解释工具中，诺比利值是最受欢迎的工具之一。然而，它们的计算成本高，导致使用抽样近似，从而引入了较大的不确定性。为稳定这些模型解释，我们提议ControlSHAP，基于Monte Carlo技术的控制准确量。我们的方法适用于任何机器学习模型，需要virtually no extra computation或模型定制努力。在一些高维数据集上，我们发现它可以生成很大的 reductions in the Monte Carlo variability of Shapley estimates。Note that "virtually no extra computation" is a bit tricky to translate, as it is a bit long and has a specific meaning in English. Here's one possible way to translate it into Simplified Chinese:<<SYS>>我们的方法需要几乎没有额外计算或模型定制努力，几乎没有额外成本。I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="The-First-Pathloss-Radio-Map-Prediction-Challenge"><a href="#The-First-Pathloss-Radio-Map-Prediction-Challenge" class="headerlink" title="The First Pathloss Radio Map Prediction Challenge"></a>The First Pathloss Radio Map Prediction Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07658">http://arxiv.org/abs/2310.07658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Çağkan Yapar, Fabian Jaensch, Ron Levie, Gitta Kutyniok, Giuseppe Caire</li>
<li>for: 本研究是为了解决路径损失预测问题，以便促进研究和对最近提出的路径损失广播地图预测方法进行公平的比较。</li>
<li>methods: 本研究使用了提供的数据集和挑战任务，并采用了挑战评价方法来评估参与者的预测方法。</li>
<li>results: 本研究通过对挑战任务的解决来展示了不同预测方法的性能，并提供了一系列的结果和分析。<details>
<summary>Abstract</summary>
To foster research and facilitate fair comparisons among recently proposed pathloss radio map prediction methods, we have launched the ICASSP 2023 First Pathloss Radio Map Prediction Challenge. In this short overview paper, we briefly describe the pathloss prediction problem, the provided datasets, the challenge task and the challenge evaluation methodology. Finally, we present the results of the challenge.
</details>
<details>
<summary>摘要</summary>
为了推动研究和促进最近提出的路径损失 ради图预测方法的公平比较，我们在ICASSP 2023年第一届路径损失 ради图预测挑战中发起了这项挑战。在这篇简短的概述 paper 中，我们简要介绍了路径损失预测问题，提供的数据集，挑战任务以及评价方法。最后，我们展示了挑战的结果。Here's the word-for-word translation:为了推动研究和促进最近提出的路径损失 ради图预测方法的公平比较，我们在ICASSP 2023年第一届路径损失 ради图预测挑战中发起了这项挑战。在这篇简短的概述 paper 中，我们简要介绍了路径损失预测问题，提供的数据集，挑战任务以及评价方法。最后，我们展示了挑战的结果。
</details></li>
</ul>
<hr>
<h2 id="Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals"><a href="#Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals" class="headerlink" title="Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals"></a>Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07648">http://arxiv.org/abs/2310.07648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Eleonora Chiarantano, Eleonora Grassucci, Danilo Comminiello</li>
<li>For: 该论文主要目标是提出一种基于嵌入式复杂网络的多模态情绪认知方法，以提高情绪识别精度。* Methods: 该方法使用了一种新的融合模块，其包括 Parametric Hypercomplex Multiplications，可以更好地模型情绪表达的多模态特征。* Results: 该方法在使用 MAHNOB-HCI 数据集进行 классифика任务中，超越了现有的多模态状态的网络。Here’s the English version of the summary for reference:* For: The main goal of the paper is to propose a multimodal emotion recognition method based on an embedded hypercomplex network, to improve the accuracy of emotion recognition.* Methods: The method uses a novel fusion module that includes Parametric Hypercomplex Multiplications to better model the multimodal features of emotional expressions.* Results: The method outperforms existing multimodal state-of-the-art networks on the MAHNOB-HCI dataset.<details>
<summary>Abstract</summary>
Multimodal emotion recognition from physiological signals is receiving an increasing amount of attention due to the impossibility to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on extracted handcrafted features, not taking full advantage of the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. Indeed, by operating in a hypercomplex domain the operations follow algebraic rules which allow to model latent relations among learned feature dimensions for a more effective fusion step. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, employing the publicly available database MAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of our work is freely available at https://github.com/ispamm/MHyEEG.
</details>
<details>
<summary>摘要</summary>
“多模式情感识别从生物学信号方面 receiving increasing attention, due to the inability to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on manually extracted features, not fully utilizing the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. By operating in a hypercomplex domain, the operations follow algebraic rules, allowing for more effective fusion of learned feature dimensions. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, using the publicly available database MAHNOB-HCI and surpassing a multimodal state-of-the-art network. The code of our work is freely available at <https://github.com/ispamm/MHyEEG>.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey"><a href="#Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey" class="headerlink" title="Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"></a>Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07745">http://arxiv.org/abs/2310.07745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Palmer, Chris Parry, Daniel J. B. Harrold, Chris Willis</li>
<li>for: 本研究旨在应对现代网络攻击的自动化防御方法。</li>
<li>methods: 本研究使用深度强化学习（DRL）方法来 Mitigate cyber attacks。</li>
<li>results: 本研究总结了DRL在ACO中的挑战，并提出了未来研究方向。<details>
<summary>Abstract</summary>
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties that define the ACO problem; ii.) A comprehensive evaluation of the extent to which domains used for benchmarking DRL approaches are comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling DRL to domains that confront learners with the curse of dimensionality, and; iv.) A survey and critique of current methods for limiting the exploitability of agents within adversarial settings from the perspective of ACO. We conclude with open research questions that we hope will motivate future directions for researchers and practitioners working on ACO.
</details>
<details>
<summary>摘要</summary>
随着最近几年的网络攻击数量快速增加，需要有原则的方法来防御网络免受恶意攻击者。深度强化学习（DRL）已经出现为防御攻击的有力方法。然而，虽然DRL在网络防御方面具有巨大的潜力，但是在大规模自动化网络操作（ACO）中应用DRL还是一个开放的挑战。ACO环境面临着非常高维状态空间、大量多 discrete 动作空间以及对学习的敌对学习。 recent works 报告了解决这些问题的成功，而且也有卓越的工程努力以实现这些问题的解决。然而，将DRL应用到整个ACO问题仍然是一个开放的挑战。在这篇文章中，我们对DRL相关文献进行了抽象，并提出了一个理想化的ACO-DRL代理。我们提供了：1. ACO问题的域属性的总结，包括ACO问题的特点和挑战。2. DRL在不同环境中的比较分析，以确定ACO问题是否与DRL相关的环境相似。3. 对高维度状态空间和多 discrete 动作空间的扩展，以及对敌对学习的限制。4. 对现有的ACO-DRL方法的评价和批判，以及未来研究的开放问题。我们结束于，ACO-DRL是一个有挑战性的领域，但是通过综合分析和理解ACO问题的特点，以及探索新的技术和方法，我们可以寻找更多的机会和潜力。
</details></li>
</ul>
<hr>
<h2 id="Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates"><a href="#Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates" class="headerlink" title="Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates"></a>Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07631">http://arxiv.org/abs/2310.07631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimeng Shi, Vitalii Stebliankin, Zhaonan Wang, Shaowen Wang, Giri Narasimhan</li>
<li>for: 预测洪水，帮助进行好的洪水管理</li>
<li>methods: 使用图Transformer网络（FloodGTN），学习水位的空间时间相关性，并使用LSTM和Graph Neural Networks（GNNs）来模拟洪水的行为</li>
<li>results: 在南佛瑞达水资源管理区的数据上测试，FloodGTN比Physics-based模型（HEC-RAS）具有更高的准确率，提高70%，并在运行时间上减少至少500倍<details>
<summary>Abstract</summary>
Floods can be very destructive causing heavy damage to life, property, and livelihoods. Global climate change and the consequent sea-level rise have increased the occurrence of extreme weather events, resulting in elevated and frequent flood risk. Therefore, accurate and timely flood forecasting in coastal river systems is critical to facilitate good flood management. However, the computational tools currently used are either slow or inaccurate. In this paper, we propose a Flood prediction tool using Graph Transformer Network (FloodGTN) for river systems. More specifically, FloodGTN learns the spatio-temporal dependencies of water levels at different monitoring stations using Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to consider external covariates such as rainfall, tide, and the settings of hydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the river. We use a Transformer to learn the attention given to external covariates in computing water levels. We apply the FloodGTN tool to data from the South Florida Water Management District, which manages a coastal area prone to frequent storms and hurricanes. Experimental results show that FloodGTN outperforms the physics-based model (HEC-RAS) by achieving higher accuracy with 70% improvement while speeding up run times by at least 500x.
</details>
<details>
<summary>摘要</summary>
洪水可以非常破坏生命、财产和生活方式。全球气候变化和海平面上升导致极端天气事件的增加，使得洪水风险增加。因此，精准和时间Constraints accurate flood forecasting in coastal river systems is critical to facilitate good flood management. However, the current computational tools are either slow or inaccurate. In this paper, we propose a flood prediction tool using Graph Transformer Network (FloodGTN) for river systems. Specifically, FloodGTN learns the spatio-temporal dependencies of water levels at different monitoring stations using Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to consider external covariates such as rainfall, tide, and the settings of hydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the river. We use a Transformer to learn the attention given to external covariates in computing water levels. We apply the FloodGTN tool to data from the South Florida Water Management District, which manages a coastal area prone to frequent storms and hurricanes. Experimental results show that FloodGTN outperforms the physics-based model (HEC-RAS) by achieving higher accuracy with 70% improvement while speeding up run times by at least 500x.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification"><a href="#Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification" class="headerlink" title="Differentiable Euler Characteristic Transforms for Shape Classification"></a>Differentiable Euler Characteristic Transforms for Shape Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07630">http://arxiv.org/abs/2310.07630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aidos-lab/dect">https://github.com/aidos-lab/dect</a></li>
<li>paper_authors: Ernst Roell, Bastian Rieck</li>
<li>for: 这个论文主要用于解决ECT不能学习任务特定表示问题，开发了一种新的计算层，使ECT可以在端到端方式学习。</li>
<li>methods: 该方法使用了ECT，并开发了一种新的计算层，使ECT可以在端到端方式学习。</li>
<li>results: 该方法在图像和点云分类任务中表现了优秀的性能，与更复杂的模型相当，同时还保持了ECT的简洁特点。<details>
<summary>Abstract</summary>
The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method DECT is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly unexpressive statistic still provides the same topological expressivity as more complex topological deep learning layers provide.
</details>
<details>
<summary>摘要</summary>
ECT（欧勒CharacteristicTransform）是一种强大的表示方式，可以结合图形和图形的 geometrical和topological特征。然而，ECT previously unable to learn任务特定的表示。我们解决了这个问题，并开发了一种新的计算层，使ECT可以在端到端的方式进行学习。我们的方法DECT具有快速和计算效率的优点，并在图形和点云分类任务中展现了与更复杂的模型相当的性能。此外，我们还证明ECT still provides the same topological expressivity as more complex topological deep learning layers provide.Note: "ECT" is short for "Euler Characteristic Transform", and "DECT" is short for "Deep ECT".
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations"><a href="#Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations" class="headerlink" title="Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations"></a>Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07626">http://arxiv.org/abs/2310.07626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria</li>
<li>for: 这个论文是为了研究使用海上高程测量卫星数据来估算海洋表面流动的方法。</li>
<li>methods: 论文使用了深度学习网络，并使用海面温度（SST）信息来优化插值方法。</li>
<li>results: 论文发现，无需训练数据，可以使用SST信息来改善插值性能，并且可以减少41%的平均平方根误差。<details>
<summary>Abstract</summary>
Satellite-based remote sensing missions have revolutionized our understanding of the Ocean state and dynamics. Among them, spaceborne altimetry provides valuable measurements of Sea Surface Height (SSH), which is used to estimate surface geostrophic currents. However, due to the sensor technology employed, important gaps occur in SSH observations. Complete SSH maps are produced by the altimetry community using linear Optimal Interpolations (OI) such as the widely-used Data Unification and Altimeter Combination System (DUACS). However, OI is known for producing overly smooth fields and thus misses some mesostructures and eddies. On the other hand, Sea Surface Temperature (SST) products have much higher data coverage and SST is physically linked to geostrophic currents through advection. We design a realistic twin experiment to emulate the satellite observations of SSH and SST to evaluate interpolation methods. We introduce a deep learning network able to use SST information, and a trainable in two settings: one where we have no access to ground truth during training and one where it is accessible. Our investigation involves a comparative analysis of the aforementioned network when trained using either supervised or unsupervised loss functions. We assess the quality of SSH reconstructions and further evaluate the network's performance in terms of eddy detection and physical properties. We find that it is possible, even in an unsupervised setting to use SST to improve reconstruction performance compared to SST-agnostic interpolations. We compare our reconstructions to DUACS's and report a decrease of 41\% in terms of root mean squared error.
</details>
<details>
<summary>摘要</summary>
卫星远感任务已经革命化了我们对海洋状态和动力学的理解。其中，空间探测技术提供了海面高程（SSH）的重要测量，用于估计表面地OSTROPIC currents。然而，由于探测技术的限制，SSH观测存在重要的缺陷。complete SSH map由altimetry社区使用线性优化方法（OI）生成，如广泛使用的数据统一和探测系统（DUACS）。然而，OI经常生成过于平滑的场景，因此缺乏一些中规模的结构和涝流。在这种情况下，海面温度（SST）产品具有更高的数据覆盖率，SST与地OSTROPIC currents physically linked through advection。我们设计了一个现实的双子实验，用于模拟卫星观测的 SSH 和 SST。我们引入了一个深度学习网络，可以使用 SST 信息，并在两种设定下训练：一个没有训练数据的情况，一个可以访问训练数据。我们的调查包括对这种网络在不同的训练设定下进行比较分析，以及评估网络的性能。我们发现，即使在无supervision的情况下，也可以使用 SST 来改进重建性能，并且我们的重建与DUACS的重建相比，Root Mean Squared Error（RMSE）下降41%。
</details></li>
</ul>
<hr>
<h2 id="Prospective-Side-Information-for-Latent-MDPs"><a href="#Prospective-Side-Information-for-Latent-MDPs" class="headerlink" title="Prospective Side Information for Latent MDPs"></a>Prospective Side Information for Latent MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07596">http://arxiv.org/abs/2310.07596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Yonathan Efroni, Shie Mannor, Constantine Caramanis</li>
<li>for: 这个论文是关于在协作决策环境中固有不可见信息的研究。具体来说，在对话系统中，用户的偏好等信息可能不被提供。在这种环境下， latent information 会在每个话语中保持不变。这种环境可以模型为 Latent Markov Decision Process (LMDP)，是 Partially Observed Markov Decision Processes (POMDPs) 的特例。</li>
<li>methods: 这篇论文使用了 LMDP 模型，并研究了在这种模型下的近似优秀策略可以如何高效地学习。具体来说， authors 使用了“prospective side information”，即在每个话语开始时得到了一些较弱描述 latent context 的信息。</li>
<li>results: 研究发现，这种问题并不是现有的 partially observed 环境和算法设计的一部分。 authors then  establishment 了任何高效学习算法都会受到至少 $\Omega(K^{2&#x2F;3})$ 的 regret，并设计了一个匹配的上界。<details>
<summary>Abstract</summary>
In many interactive decision-making settings, there is latent and unobserved information that remains fixed. Consider, for example, a dialogue system, where complete information about a user, such as the user's preferences, is not given. In such an environment, the latent information remains fixed throughout each episode, since the identity of the user does not change during an interaction. This type of environment can be modeled as a Latent Markov Decision Process (LMDP), a special instance of Partially Observed Markov Decision Processes (POMDPs). Previous work established exponential lower bounds in the number of latent contexts for the LMDP class. This puts forward a question: under which natural assumptions a near-optimal policy of an LMDP can be efficiently learned? In this work, we study the class of LMDPs with {\em prospective side information}, when an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode. We show that, surprisingly, this problem is not captured by contemporary settings and algorithms designed for partially observed environments. We then establish that any sample efficient algorithm must suffer at least $\Omega(K^{2/3})$-regret, as opposed to standard $\Omega(\sqrt{K})$ lower bounds, and design an algorithm with a matching upper bound.
</details>
<details>
<summary>摘要</summary>
在许多互动决策Setting中，存在潜在的和不可见的信息，这些信息在Each episode中保持不变。例如，在对话系统中，用户的偏好完全不给出。在这种环境中，潜在信息在每个话语中保持不变，因为用户的身份不会在交互中改变。这种环境可以被模型为潜在Markov决策过程（LMDP），这是Partially Observed Markov Decision Processes（POMDPs）的特殊情况。之前的工作已经证明了LMDP类型的下界为数字latent context的指数。这提出了一个问题：在哪些自然假设下，一个LMDP的优化策略可以高效地学习？在这项工作中，我们研究了在LMDP中提供了前景信息（prospective side information），用户在每个话语开始时接收到额外、薄弱揭示的潜在信息。我们发现，这个问题并不是现代的部分观察环境和算法设计的一部分。我们然后证明任何高效样本算法都会产生至少 $\Omega(K^{2/3})$ 的倒退，而不是标准的 $\Omega(\sqrt{K})$ 下界，并设计了一个匹配的上界。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics"><a href="#Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics" class="headerlink" title="Transformers for Green Semantic Communication: Less Energy, More Semantics"></a>Transformers for Green Semantic Communication: Less Energy, More Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07592">http://arxiv.org/abs/2310.07592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhabrata Mukherjee, Cory Beard, Sejun Song</li>
<li>for: 本研究旨在提高semantic communication的能效性，实现更低的延迟、带宽使用和更高的吞吐量。</li>
<li>methods: 本研究提出了一种新的多目标损失函数名为“能源优化semantic损失”（EOSL），以解决semantic communication中的 universal metrics问题。</li>
<li>results: 经过对transformer模型的测试，包括CPU和GPU的能源消耗，研究发现EOSL可以在推理阶段 saves up to 90%的能源，同时提高semantic similarity性能44%。<details>
<summary>Abstract</summary>
Semantic communication aims to transmit meaningful and effective information rather than focusing on individual symbols or bits, resulting in benefits like reduced latency, bandwidth usage, and higher throughput compared to traditional communication. However, semantic communication poses significant challenges due to the need for universal metrics for benchmarking the joint effects of semantic information loss and practical energy consumption. This research presents a novel multi-objective loss function named "Energy-Optimized Semantic Loss" (EOSL), addressing the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90\% of energy while achieving a 44\% improvement in semantic similarity performance during inference in this experiment. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details>
<details>
<summary>摘要</summary>
Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election"><a href="#Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election" class="headerlink" title="Analyzing Trendy Twitter Hashtags in the 2022 French Election"></a>Analyzing Trendy Twitter Hashtags in the 2022 French Election</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07576">http://arxiv.org/abs/2310.07576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aamir Mandviwalla, Lake Yin, Boleslaw K. Szymanski</li>
<li>for: 预测社交媒体用户未来活动</li>
<li>methods: 使用semantic网络特征来提高机器学习模型的准确性</li>
<li>results: 使用semantic网络特征可以实现高于0.5的$R^2$值，验证了这种特征的有用性。<details>
<summary>Abstract</summary>
Regressions trained to predict the future activity of social media users need rich features for accurate predictions. Many advanced models exist to generate such features; however, the time complexities of their computations are often prohibitive when they run on enormous data-sets. Some studies have shown that simple semantic network features can be rich enough to use for regressions without requiring complex computations. We propose a method for using semantic networks as user-level features for machine learning tasks. We conducted an experiment using a semantic network of 1037 Twitter hashtags from a corpus of 3.7 million tweets related to the 2022 French presidential election. A bipartite graph is formed where hashtags are nodes and weighted edges connect the hashtags reflecting the number of Twitter users that interacted with both hashtags. The graph is then transformed into a maximum-spanning tree with the most popular hashtag as its root node to construct a hierarchy amongst the hashtags. We then provide a vector feature for each user based on this tree. To validate the usefulness of our semantic feature we performed a regression experiment to predict the response rate of each user with six emotions like anger, enjoyment, or disgust. Our semantic feature performs well with the regression with most emotions having $R^2$ above 0.5. These results suggest that our semantic feature could be considered for use in further experiments predicting social media response on big data-sets.
</details>
<details>
<summary>摘要</summary>
“预测社交媒体用户未来活动需要丰富的特征。许多高级模型可以生成这些特征，但是它们在巨大数据集上进行计算时间复杂度可能是禁止的。一些研究表明，使用 semantics 网络特征可以够简单，对于机器学习任务来说，这些特征可以提供高度的准确性。我们提出一种使用 semantics 网络为用户级别特征的方法。我们使用了一个Twitter Hashtag 网络，包含370万则发送时间关于2022年法国总统选举的推文。将这些 Hashtag 转换为一个两分支 гра图，其中 Hashtag 为顶点，并将它们之间的关系为权重边。然后将这个 гра图转换为最大 span 树，并将最受欢迎的 Hashtag 作为根点。我们将这个树转换为一个向量特征，并将其用于预测每个用户对六种情感（如愤怒、喜悦、厌恶等）的回应率。我们的semantic特征表现良好，大多数情感的 $R^2$ 高于0.5。这些结果表明，我们的semantic特征可以考虑用于预测大规模的社交媒体回应。”
</details></li>
</ul>
<hr>
<h2 id="Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning"><a href="#Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning" class="headerlink" title="Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning"></a>Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07558">http://arxiv.org/abs/2310.07558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeqi Ye, Hansheng Jiang</li>
<li>for: 这个研究是针对非Parametric 需求函数的动态价格问题，并专注于适应未知的 H&quot;older 平滑程度 $\beta$。</li>
<li>methods: 我们使用了自相似性条件来启动适应，并提出了一个可靠的动态价格算法，并证明了这个算法可以在不知道 $\beta$ 的情况下实现最佳的 regret。</li>
<li>results: 我们证明了这个问题的最佳 regret 是 $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$，并且显示了这个 regret 下界不受自相似性条件的影响。<details>
<summary>Abstract</summary>
We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $\beta$.
</details>
<details>
<summary>摘要</summary>
我们研究动态价格问题，其中需求函数是非 Parametric 和Holder平滑的。我们专注于适应未知的Holder平滑性parameter $\beta$。传统上最佳动态价格算法严重依赖 $\beta$ 的知识，以 Achieve 最佳优化 regret $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$。但我们强调了这问题的适应性挑战，并证明无法适应地 дости��arz regret  minus  $\beta$ 的知识。这问题的适应性挑战的原因在于，需求函数的Holder平滑性parameter $\beta$ 是未知的，导致算法无法适应地调整价格。我们提出了一个自相似性条件，可以帮助解决这问题。我们证明了，这个自相似性条件不会增加问题的复杂度，并且保留了 regret 下界 $\Omega(T^{\frac{\beta+1}{2\beta+1})$。接下来，我们开发了一个适应性价格算法，并证明了这个算法可以 Achieve 最佳优化 regret bound $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$  minus  $\beta$ 的知识。
</details></li>
</ul>
<hr>
<h2 id="Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation"><a href="#Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation" class="headerlink" title="Provable Advantage of Parameterized Quantum Circuit in Function Approximation"></a>Provable Advantage of Parameterized Quantum Circuit in Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07528">http://arxiv.org/abs/2310.07528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhan Yu, Qiuhao Chen, Yuling Jiao, Yinan Li, Xiliang Lu, Xin Wang, Jerry Zhijian Yang</li>
<li>for: 本研究旨在探讨量子机器学习中参数化量子电路（PQC）的表达能力，以及PQC在完成机器学习任务中的可能优势。</li>
<li>methods: 本文使用函数拟合的视角分析PQC的表达能力，并提出了一种量化的近似误差 bounds，以衡量PQC需要多大以上来近似目标函数。作者们使用量子信号处理和线性组合方法构建PQC，并使用Bernstein polynomials和本地Taylor展开来实现全球和本地拟合。</li>
<li>results: 作者们的研究表明，PQC的表达能力与其宽度、深度和可调参数数量有直接的关系。具体来说，作者们在高维smooth函数的拟合中，使用PQC和深度神经网络进行比较，发现PQC的模型大小与深度神经网络的模型大小之间存在指数关系。这种指数关系表明，PQC在高维机器学习任务中可能具有显著的优势。<details>
<summary>Abstract</summary>
Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Taylor expansion and analyze their performances in the quantum setting. We also compare our proposed PQCs to nearly optimal deep neural networks in approximating high-dimensional smooth functions, showing that the ratio between model sizes of PQC and deep neural networks is exponentially small with respect to the input dimension. This suggests a potentially novel avenue for showcasing quantum advantages in quantum machine learning.
</details>
<details>
<summary>摘要</summary>
理解Parameterized quantum circuits（PQCs）在机器学习任务中的力量是机器学习领域的一个最重要的问题。在这篇论文中，我们通过函数近似来分析PQCs的表达能力。先前的 universality approximation theorems for PQCs 是非构建的，导致我们对以下问题：PQCs 如何在误差阈值为给定的情况下，将目标函数近似到Target function? We present explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions, and establish quantitative approximation error bounds in terms of the width, the depth, and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Taylor expansion, and analyze their performances in the quantum setting. We also compare our proposed PQCs to nearly optimal deep neural networks in approximating high-dimensional smooth functions, showing that the ratio between model sizes of PQC and deep neural networks is exponentially small with respect to the input dimension. This suggests a potentially novel avenue for showcasing quantum advantages in quantum machine learning.Note: Simplified Chinese is a written language that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and is the official language of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning"><a href="#Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning" class="headerlink" title="Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning"></a>Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07518">http://arxiv.org/abs/2310.07518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, Giorgia Ramponi</li>
<li>for: 提高强化学习的样本效率，使用 posterior sampling 技术。</li>
<li>methods: 使用 causal graph 来表示环境变量之间的 causal 关系，并使用 hierarchical Bayesian 方法 simultaneously 学习 full causal graph 和 resulting factored dynamics。</li>
<li>results: 数值研究表明，C-PSRL 可以强化 posterior sampling 的效率，并且与 posterior sampling 使用 full causal graph 的效果相似。<details>
<summary>Abstract</summary>
Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.
</details>
<details>
<summary>摘要</summary>
后采样allowstheexploitationof prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a（partial）causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.Here's the breakdown of the translation:* 后采样 (hòu shēng sāng) - posterior sampling* allowstheexploitation (děi yǎn jì) - allows the exploitation* of prior knowledge (xīn jiàn yì) - of prior knowledge* to improve (gèng yǐn) - to improve* the sample efficiency (shuāng yǐn jì) - the sample efficiency* of reinforcement learning (qì yǎn jì) - of reinforcement learning* The prior (xīn) - the prior* is typically specified (dēi yǐn) - is typically specified* as a class (bǎn) - as a class* of parametric distributions (fāng xiào yǐn) - of parametric distributions* a task (gōng yì) - a task* that can be cumbersome (kě shì) - that can be cumbersome* in practice (shì yī jī) - in practice* often resulting (yǐn yì) - often resulting* in the choice (děi yǎn) - in the choice* of uninformative priors (qǐ yǐn xīn) - of uninformative priors* In this work (zhèng gōng) - In this work* we propose (wǒ zhèng) - we propose* a novel (xīn) - a novel* posterior sampling approach (hòu shēng sāng yì jì) - a posterior sampling approach* in which (dēi yǐn) - in which* the prior (xīn) - the prior* is given (gěi) - is given* as a (partial) causal graph (xīn fāng yǐn) - as a (partial) causal graph* over the environment's variables (yuè yì zhī) - over the environment's variables* The latter (dài) - the latter* is often more natural (fēng zhī) - is often more natural* to design (suǒ yì) - to design* such as (xīn yǐn) - such as* listing known causal dependencies (jiè yǐn kě yì) - listing known causal dependencies* between biometric features (yì xīng yǐn) - between biometric features* in a medical treatment study (yī jīng zhī yì) - in a medical treatment study* Specifically (xīn yǐn) - specifically* we propose (wǒ zhèng) - we propose* a hierarchical Bayesian procedure (hierarchical Bayesian procedure) - a hierarchical Bayesian procedure* called C-PSRL (called C-PSRL) - called C-PSRL* simultaneously learning (xīn xué yì) - simultaneously learning* the full causal graph (quán xīn fāng) - the full causal graph* at the higher level (shàng kāi) - at the higher level* and the parameters (fāng yǐn) - and the parameters* of the resulting factored dynamics (yǐn yuè yǐn) - of the resulting factored dynamics* For this procedure (zhèng gōng) - For this procedure* we provide (wǒ jiāng) - we provide* an analysis (xīn yì) - an analysis* of its Bayesian regret (Bayesian regret) - of its Bayesian regret* which explicitly connects (dēi yǐn) - which explicitly connects* the regret rate (hǎo yǐn) - the regret rate* with the degree of prior knowledge (xīn jì zhī) - with the degree of prior knowledge* Our numerical evaluation (shuāng yì yì) - Our numerical evaluation* conducted in illustrative domains (dài yì) - conducted in illustrative domains* confirms (jiān yì) - confirms* that C-PSRL (C-PSRL) - that C-PSRL* strongly improves (qiǎo yǐn) - strongly improves* the efficiency (yǐn jì) - the efficiency* of posterior sampling (hòu shēng sāng) - of posterior sampling* with an uninformative prior (qǐ yǐn xīn) - with an uninformative prior* while performing (yǐn yì) - while performing* close to (jì qǐ) - close to* posterior sampling with the full causal graph (hòu shēng sāng quán xīn fāng) - posterior sampling with the full causal graph
</details></li>
</ul>
<hr>
<h2 id="Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance"><a href="#Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance" class="headerlink" title="Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance"></a>Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07491">http://arxiv.org/abs/2310.07491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs</li>
<li>For: The paper aims to improve the predictive performance of personalized models for emotional behavior by utilizing group-based information through clustering.* Methods: The paper investigates two model-based clustering approaches using personalized models and optimizes the clustering based on performance.* Results: Clustering based on performance shows the best results in terms of all examined evaluation measures, and the group-models outperform three baseline scenarios.<details>
<summary>Abstract</summary>
Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describe all individuals belonging to one cluster. Among these, clustering based on performance shows the best results, in terms of all examined evaluation measures. As another level of evaluation, those group-models' performance is compared to three baseline scenarios, the personalized, the all-in-one group and the random group-based concept. According to this comparison, the superiority of clustering-based methods is again confirmed, indicating that the utilization of group-based information could be effectively enhance the overall performance of all individuals' data.
</details>
<details>
<summary>摘要</summary>
Two model-based clustering approaches are examined: the first uses model-extracted parameters of personalized models, while the second is optimized for model-based forecasting performance. Both methods are evaluated using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) and the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describing all individuals belonging to one cluster.Clustering based on performance shows the best results, as evaluated by all examined measures. To further evaluate the effectiveness of clustering-based methods, the performance of the group-models is compared to three baseline scenarios: personalized, all-in-one group, and random group-based concepts. The results confirm the superiority of clustering-based methods, indicating that utilizing group-based information can effectively enhance the overall performance of all individuals' data.
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes"><a href="#Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes" class="headerlink" title="Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes"></a>Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07485">http://arxiv.org/abs/2310.07485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Schwerdtner, Philipp Schulze, Jules Berman, Benjamin Peherstorfer</li>
<li>for: 该研究探讨了使用深度网络非线性参数化方法保存干扰量、质量和动量的问题。</li>
<li>methods: 该方法基于Neural Galerkin方法，使用Dirac–Frenkel变分原理来在时间上顺序训练非线性参数化。</li>
<li>results: 数值实验表明，该方法可以准确保存干扰量、质量和动量。<details>
<summary>Abstract</summary>
This work focuses on the conservation of quantities such as Hamiltonians, mass, and momentum when solution fields of partial differential equations are approximated with nonlinear parametrizations such as deep networks. The proposed approach builds on Neural Galerkin schemes that are based on the Dirac--Frenkel variational principle to train nonlinear parametrizations sequentially in time. We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes. Numerical experiments demonstrate that the proposed approach conserves quantities up to machine precision.
</details>
<details>
<summary>摘要</summary>
We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes.Numerical experiments demonstrate that the proposed approach conserves quantities up to machine precision.simplified Chinese translation:这个工作关注在使用深度网络作为参数化方程解的解场量保守性。提议的方法基于神经加尔基 schemes，这些方法基于Dirac--Frenkel变量原理来顺序地在时间上训练非线性参数化。我们首先表明，只是在继续时间添加保守量的约束可能是不充分的，因为非线性参数的依赖性使得解场量中的量变为非线性参数，这使得在时间绘制中很难处理。而我们提议的神经加尔基方法在每个时间步骤上计算非线性参数化解场量的Explicit embedding，以保证量的保守性。这些映射可以与标准的Explicit和隐式时间步骤综合使用。 numerics experiments表明，提议的方法可以保持量到机器精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma"><a href="#Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma" class="headerlink" title="Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma"></a>Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07464">http://arxiv.org/abs/2310.07464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Fang, Yihan Liu, Yifeng Wang, Xiangyang Zhang, Yang Chen, Changjing Cai, Yiyang Lin, Ying Han, Zhi Wang, Shan Zeng, Hong Shen, Jun Tan, Yongbing Zhang<br>for:多种低等级肿瘤（LGG）的诊断和治疗中，生物标记的检测是不可或缺的。然而，现有的LGG生物标记检测方法往往需要使用高成本和复杂的分子遗传测试，需要专业人员分析结果，并且经常报告了内部变化。methods:我们提出了一种可解释的深度学习管道，基于多例学习（MIL）框架，用于预测LGG五个生物标记的状态，只需使用染色和染色涂抹的整个扫描图像和扫描标签。Specifically，通过将一类分类 integrating 到 MIL 框架中，实现了准确的实例 Pseudo-labeling，这种 complement 扫描标签 greatly improves 生物标记预测性能。results:Multi-Beholder 在两个群组（n&#x3D;607）中表现出色，其预测性能和一致性（AUROC&#x3D;0.6469-0.9735）在多种不同的种族和扫描协议下都具有出色的一致性。此外，Multi-Beholder 的优秀可解释性允许发现潜在的量化和质量相关性 между生物标记状态和生物学特征。我们的管道不仅提供了一种新的生物标记预测方法，提高了LGG患者的分子治疗应用性，还可以探索新的分子功能和LGG发展机制。<details>
<summary>Abstract</summary>
Biomarker detection is an indispensable part in the diagnosis and treatment of low-grade glioma (LGG). However, current LGG biomarker detection methods rely on expensive and complex molecular genetic testing, for which professionals are required to analyze the results, and intra-rater variability is often reported. To overcome these challenges, we propose an interpretable deep learning pipeline, a Multi-Biomarker Histomorphology Discoverer (Multi-Beholder) model based on the multiple instance learning (MIL) framework, to predict the status of five biomarkers in LGG using only hematoxylin and eosin-stained whole slide images and slide-level biomarker status labels. Specifically, by incorporating the one-class classification into the MIL framework, accurate instance pseudo-labeling is realized for instance-level supervision, which greatly complements the slide-level labels and improves the biomarker prediction performance. Multi-Beholder demonstrates superior prediction performance and generalizability for five LGG biomarkers (AUROC=0.6469-0.9735) in two cohorts (n=607) with diverse races and scanning protocols. Moreover, the excellent interpretability of Multi-Beholder allows for discovering the quantitative and qualitative correlations between biomarker status and histomorphology characteristics. Our pipeline not only provides a novel approach for biomarker prediction, enhancing the applicability of molecular treatments for LGG patients but also facilitates the discovery of new mechanisms in molecular functionality and LGG progression.
</details>
<details>
<summary>摘要</summary>
低度 Glioma (LGG) 诊断和治疗中不可或缺的一部分是生物标志物的检测。然而，现有的LGG生物标志物检测方法仍然 rely on 昂贵和复杂的分子遗传学测试，需要专业人员分析结果，并且间谍者变化 frequently 被报告。为了解决这些挑战，我们提议一种可解释深度学习管道，即多个生物标志物形态发现器（Multi-Beholder）模型，基于多个实例学习（MIL）框架，以predict LGG 中五个生物标志物的状态，只需使用染色涂抹整幅图像和批量标签。specifically，通过将一类分类 incorporated 到 MIL 框架中，实现了准确的实例 pseudo-标签，这对实例级指导提供了很好的补做，从而提高了生物标志物预测性能。Multi-Beholder 在两个 cohort（n=607）中显示出优秀的预测性能和普遍性，其中包括不同的种族和扫描协议。此外，Multi-Beholder 的优秀可解释性使得可以发现生物标志物状态和形态特征之间的量化和质量相关性。我们的管道不仅提供了一种新的生物标志物预测方法，扩展了 LGG 患者可应用分子治疗的可能性，还可以促进分子功能和 LGG 进程中新的机制发现。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI"><a href="#Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI" class="headerlink" title="Uncovering ECG Changes during Healthy Aging using Explainable AI"></a>Uncovering ECG Changes during Healthy Aging using Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07463">http://arxiv.org/abs/2310.07463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4healthuol/ecg-aging">https://github.com/ai4healthuol/ecg-aging</a></li>
<li>paper_authors: Gabriel Ott, Yannik Schaubelt, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff</li>
<li>for: 这个论文的目的是为了更好地理解心脏年龄过程，以便诊断心血管健康问题。</li>
<li>methods: 这个论文使用了深度学习模型和树型分类器来分析健康人群的ECG数据，并使用可解释AI技术来确定ECG特征或Raw信号特征是最有力量地分类不同年龄组的。</li>
<li>results: 研究发现，随着年龄增长，推断呼吸速率下降，而SDANN值明显高的 elderly 个体呈现出 distinguish 特征，与年轻成年人相比。此外，深度学习模型表明，随着年龄增长，P-波的分布发生了变化，这可能对年龄预测产生了影响。这些发现超越了传统基于特征的方法，提供了新的年龄相关ECG变化的视角。<details>
<summary>Abstract</summary>
Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adults. Furthermore, the deep-learning model underscores the pivotal role of the P-wave in age predictions across all age groups, suggesting potential changes in the distribution of different P-wave types with age. These findings shed new light on age-related ECG changes, offering insights that transcend traditional feature-based approaches.
</details>
<details>
<summary>摘要</summary>
心血管疾病仍然是全球致死率最高的主要原因。这意味着我们需要深入了解心脏年龄过程，以诊断心血管健康水平的约束。传统上，大多数这些发现都是通过心电图特征变化来获得的，但这些特征可能会隐藏数据之间的关系。在这篇论文中，我们使用深度学习模型和树 структуры模型来分析健康个体心电图数据，并使用可解释AI技术来确定心电图特征或原始信号特征是否能够区分不同年龄组。我们的分析表明，年龄相关的呼吸率下降和高SDANN值是识别老年人的特征，与年轻人相比分化。此外，深度学习模型表明，随着年龄增长，P波的分布发生了变化，这对年龄预测具有重要作用。这些发现为心血管年龄变化提供了新的见解，超出了传统的特征基于方法。
</details></li>
</ul>
<hr>
<h2 id="ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting"><a href="#ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting" class="headerlink" title="ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting"></a>ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07446">http://arxiv.org/abs/2310.07446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Zhang, Xumeng Wen, Shun Zheng, Jia Li, Jiang Bian</li>
<li>for: 这篇论文旨在探讨时间序列预测的两个分支：一个是专门为时间序列设计特有的神经网络架构，另一个是利用深度生成模型进行 probabilistic 预测。</li>
<li>methods: 这篇论文使用了一个名为 ProbTS 的工具集，它可以融合和比较这两个分支的不同方法。</li>
<li>results: 通过使用 ProbTS，论文发现了这两个分支的不同特点、优劣点和需要进一步探索的领域。<details>
<summary>Abstract</summary>
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further exploration. Our analyses point to new avenues for research, aiming for more effective time-series forecasting.
</details>
<details>
<summary>摘要</summary>
时间序列预测作为许多应用领域的关键环节，其中有两个主要分支：一个是为时间序列设计特定的神经网络架构，另一个是利用高级深度生成模型进行 probabilistic 预测。尽管这两个分支都取得了重要进展，但它们在数据场景、方法重点和解码方案等方面存在差异，这些差异尚未得到深入研究。为了bridging这一知识差距，我们提出了ProbTS工具集，这是一个能够结合和比较这两个分支的先锋工具。ProbTS具有一个统一的数据模块、一个模块化的模型模块和一个全面的评价模块，这使得我们可以对领先的方法进行重新评估和比较。我们的分析表明，这两个分支具有不同的特点、优势和缺点，以及需要进一步探索的领域。我们的研究开发了新的方向，以更好地预测时间序列。
</details></li>
</ul>
<hr>
<h2 id="A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales"><a href="#A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales" class="headerlink" title="A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales"></a>A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07437">http://arxiv.org/abs/2310.07437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien Wang</li>
<li>for: 预测普罗旺斯的低视力事件或雾霾的发生</li>
<li>methods: 使用多元年度地域天气和水文变量作为输入特征，并使用视野观测数据作为目标进行训练</li>
<li>results: 通过两棵分支架构，提高网络的性能，在验证和盲预测评估中获得了2021和2022年数据不使用训练和验证数据的合理分数<details>
<summary>Abstract</summary>
A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
</details>
<details>
<summary>摘要</summary>
“一个深度学习平台已经开发来预测低可见度事件或雾化的发生。它是使用多decadal日间地域气象和水文变数作为输入特征，并使用地面可见度观测作为目标进行训练。为了更好地保留不同输入特征的特征空间信息，最近两个分支架构已经为 París 雾化情况开发出来。这两个新架构已经提高了网络的性能，在验证和隐藏预测评估中获得了合理的分数，使用2021和2022年的数据进行预测。”Note: "Paris hazes" in the text refers to haze events that occur in Paris, France.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data"><a href="#Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data" class="headerlink" title="Generalized Mixture Model for Extreme Events Forecasting in Time Series Data"></a>Generalized Mixture Model for Extreme Events Forecasting in Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07435">http://arxiv.org/abs/2310.07435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Wang, Yue Gao</li>
<li>for: 这个研究旨在提高时间序列预测中的极值识别和预测性能。</li>
<li>methods: 本研究提出了一个 novel 的 Deep Extreme Mixture Model with Autoencoder (DEMMA) 架构，包括两个主要模组：1）一个通用混合分布基于 Hurdle 模型和一个对应数据的 GP 分布，2）一个 Autoencoder-based LSTM 特征提取器和一个时间注意力机制。</li>
<li>results: 在多个实际的雨水时间序列数据上，本研究示出了 DEMMA 模型的优化性能。<details>
<summary>Abstract</summary>
Time Series Forecasting (TSF) is a widely researched topic with broad applications in weather forecasting, traffic control, and stock price prediction. Extreme values in time series often significantly impact human and natural systems, but predicting them is challenging due to their rare occurrence. Statistical methods based on Extreme Value Theory (EVT) provide a systematic approach to modeling the distribution of extremes, particularly the Generalized Pareto (GP) distribution for modeling the distribution of exceedances beyond a threshold. To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model comprises two main modules: 1) a generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution form independent of the extreme threshold, 2) an Autoencoder-based LSTM feature extractor and a quantile prediction module with a temporal attention mechanism. We demonstrate the effectiveness of our approach on multiple real-world rainfall datasets.
</details>
<details>
<summary>摘要</summary>
To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model consists of two main modules:1. A generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution that is independent of the extreme threshold.2. An Autoencoder-based Long Short-Term Memory (LSTM) feature extractor and a quantile prediction module with a temporal attention mechanism.We demonstrate the effectiveness of our approach on multiple real-world rainfall datasets.
</details></li>
</ul>
<hr>
<h2 id="Non-backtracking-Graph-Neural-Networks"><a href="#Non-backtracking-Graph-Neural-Networks" class="headerlink" title="Non-backtracking Graph Neural Networks"></a>Non-backtracking Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07430">http://arxiv.org/abs/2310.07430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, Sungsoo Ahn</li>
<li>for: 本文旨在提出一种解决图神经网络（GNN）的循环引用问题的方法，以提高GNN的表示能力和计算效率。</li>
<li>methods: 本文提出了一种非循环引用图神经网络（NBA-GNN），通过不包含先前访问的节点的消息来解决循环引用问题。</li>
<li>results: 实验表明，NBA-GNN可以有效地解决循环引用问题，并且在长距离图 bench 和推理节点分类问题上显示出了出色的表现。<details>
<summary>Abstract</summary>
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-GNN on long-range graph benchmark and transductive node classification problems.
</details>
<details>
<summary>摘要</summary>
“ celebrity message-passing updates for graph neural networks 让大规模图可以通过本地和计算可行的更新表示。然而，本地更新受到回tracking的影响，即消息流经同一个边两次并返回已经访问过的节点。由于消息流量随更新数量呈指数增长，本地更新中的重复导致图神经网络无法准确地识别特定消息流，这对下游任务造成了影响。在这种情况下，我们提议使用非回tracking图神经网络（NBA-GNN），该网络在更新消息时不会包含已经访问过的节点的消息。我们进一步研究了NBA-GNN如何解决GNNS中的过度压缩问题，并证明了NBA-GNN和非回tracking更新的非常有用性。我们通过对长距离图benchmark和推uctive节点分类问题进行实验来证明NBA-GNN的有效性。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions"><a href="#Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions" class="headerlink" title="Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions"></a>Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07427">http://arxiv.org/abs/2310.07427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengmeng Xu, Hai Lin</li>
<li>for: 这种研究旨在提高时间序列预测精度，使用量子计算技术和深度学习方法。</li>
<li>methods: 该方法使用量子圈抽象场(QGAF)，它将时间序列数据转换为适合 convolutional neural network(CNN) 训练的二维图像。与传统的gramian angular field(GAF)方法不同，QGAF不需要数据Normalization和 inverse cosine 计算，从而简化了数据转换过程。</li>
<li>results: 在中国A股市场、香港股市场和美国股市场的数据集上进行了实验，发现QGAF方法比传统GAF方法更高精度，预测误差下降了25%的mean absolute error(MAE)和48%的mean squared error(MSE)。这项研究证明了将量子计算技术与深度学习方法结合使用可以提高金融时间序列预测的精度。<details>
<summary>Abstract</summary>
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improved time series prediction accuracy, reducing prediction errors by an average of 25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This research confirms the potential and promising prospects of integrating quantum computing with deep learning techniques in financial time series forecasting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种时间序列预测方法，名为量子agramian angular field（QGAF）。这种方法结合了量子计算技术和深度学习，目的是提高时间序列分类和预测的精度。我们成功地将股票回报时间序列数据转化为适合深度神经网络训练的两维图像，通过设计专门的量子电路。与 классиical gramian angular field（GAF）方法不同的是，QGAF方法不需要数据Normalization和 inverse cosine 计算，从时间序列数据到二维图像的转换过程就变得更加简单。为验证这种方法的效果，我们在三个主要股票市场的数据集上进行了实验：中国A股市场、香港股市和美国股市。实验结果表明，相比于 классиical GAF 方法，QGAF 方法在时间序列预测精度上有显著提高，减少预测错误的平均绝对值（MAE）和平均平方误差（MSE）的值，减少了25%和48%。这项研究证明了将量子计算技术与深度学习技术结合在金融时间序列预测中的潜在优势和前景。
</details></li>
</ul>
<hr>
<h2 id="Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization"><a href="#Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization" class="headerlink" title="Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization"></a>Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07392">http://arxiv.org/abs/2310.07392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Raina, SH Chandrashekhara, Richard Voyles, Juan Wachs, Subir Kumar Saha</li>
<li>for: 提高Robotic Ultrasound（A-RUS）的图像质量和效率，减少医生的工作负担。</li>
<li>methods: 使用深度神经网络学习低维度的核函数，并使用两种图像质量估计器来提供实时反馈。</li>
<li>results: 实现了超过50%的样本效率提升，并且这种性能提升不依赖具体的训练数据集，表示了在不同病人身上的可靠性。<details>
<summary>Abstract</summary>
Ultrasound is a commonly used medical imaging modality that requires expert sonographers to manually maneuver the ultrasound probe based on the acquired image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to this manual procedure in order to reduce sonographers' workload. The key challenge to A-RUS is optimizing the ultrasound image quality for the region of interest across different patients. This requires knowledge of anatomy, recognition of error sources and precise probe position, orientation and pressure. Sample efficiency is important while optimizing these parameters associated with the robotized probe controller. Bayesian Optimization (BO), a sample-efficient optimization framework, has recently been applied to optimize the 2D motion of the probe. Nevertheless, further improvements are needed to improve the sample efficiency for high-dimensional control of the probe. We aim to overcome this problem by using a neural network to learn a low-dimensional kernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained using probe and image data acquired during the procedure. The two image quality estimators are proposed that use a deep convolution neural network and provide real-time feedback to the BO. We validated our framework using these two feedback functions on three urinary bladder phantoms. We obtained over 50% increase in sample efficiency for 6D control of the robotized probe. Furthermore, our results indicate that this performance enhancement in BO is independent of the specific training dataset, demonstrating inter-patient adaptability.
</details>
<details>
<summary>摘要</summary>
超声成为医疗图像获取的常用方法，需要专业的sonoographer手动操作超声探测器根据获取的图像。自动化超声探测器（A-RUS）是为了减轻sonoographer的工作负担，但是需要在不同患者中优化超声图像质量的区域。这需要了解解剖学、识别错误来源和精确的探测器位置、orientation和压力。样本效率是重要的而且需要优化这些相关的探测器控制参数。bayesian优化（BO），一种样本效率的优化框架，已经应用于优化2D探测器的运动。然而，需要进一步提高样本效率，以便在高维度控制中提高效率。我们想使用神经网络学习一个低维度的核心，称为深度核心（DK）。神经网络的DK在BO中训练，使用在过程中获取的探测器和图像数据。我们提出了两种图像质量估计器，使用深度卷积神经网络，并为BO提供实时反馈。我们验证了我们的框架，使用这两种反馈函数在三个尿液膜phantom上进行验证。我们获得了6D控制的机器人式探测器的50%以上的样本效率提高。此外，我们的结果表明，这种性能提高在BO中是不同训练数据集的独立的，表明了患者间适应性。
</details></li>
</ul>
<hr>
<h2 id="Experimental-quantum-natural-gradient-optimization-in-photonics"><a href="#Experimental-quantum-natural-gradient-optimization-in-photonics" class="headerlink" title="Experimental quantum natural gradient optimization in photonics"></a>Experimental quantum natural gradient optimization in photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07371">http://arxiv.org/abs/2310.07371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhi Wang, Shichuan Xue, Yaxuan Wang, Jiangfang Ding, Weixu Shi, Dongyang Wang, Yong Liu, Yingwen Liu, Xiang Fu, Guangyao Huang, Anqi Huang, Mingtang Deng, Junjie Wu</li>
<li>for: 实现实用near-term量子应用</li>
<li>methods: 使用量子自然导数（QNG）优化</li>
<li>results: 实验ally obtained the dissociation curve of He-H$^+$ cation with chemical accuracy, demonstrating the outperformance of QNG optimization on a photonic device.<details>
<summary>Abstract</summary>
Variational quantum algorithms (VQAs) combining the advantages of parameterized quantum circuits and classical optimizers, promise practical quantum applications in the Noisy Intermediate-Scale Quantum era. The performance of VQAs heavily depends on the optimization method. Compared with gradient-free and ordinary gradient descent methods, the quantum natural gradient (QNG), which mirrors the geometric structure of the parameter space, can achieve faster convergence and avoid local minima more easily, thereby reducing the cost of circuit executions. We utilized a fully programmable photonic chip to experimentally estimate the QNG in photonics for the first time. We obtained the dissociation curve of the He-H$^+$ cation and achieved chemical accuracy, verifying the outperformance of QNG optimization on a photonic device. Our work opens up a vista of utilizing QNG in photonics to implement practical near-term quantum applications.
</details>
<details>
<summary>摘要</summary>
“量子变量算法（VQA），结合参数化量子电路和类别优化器的优点，承诺实现量子应用程序在噪响中等量子时代。VQA的性能强度取决于优化方法。相比于梯度计算和普通梯度下降方法，量子自然梯度（QNG），它反映参数空间的几何结构，可以更快地趋向于稳定点，更容易避免地陷入地方 minimum，因此可以降低电路执行成本。我们通过全功能可编程光学芯片实验ally estimatin QNG在光学中，并实现了He-H$^+$阴离子的分离曲线，达到化学精度，证明了QNG优化在光学设备上的出色表现。我们的工作开启了使用QNG在光学中实现实用近期量子应用程序的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities"><a href="#Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities" class="headerlink" title="Orthogonal Random Features: Explicit Forms and Sharp Inequalities"></a>Orthogonal Random Features: Explicit Forms and Sharp Inequalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07370">http://arxiv.org/abs/2310.07370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nizar Demni, Hachem Kadri</li>
<li>for: 这个论文旨在探讨随机特征的扩展，以便使用随机化技术来提高内积方法。</li>
<li>methods: 论文使用了随机 Fourier 特征和正交随机特征来近似 Gaussian kernel。</li>
<li>results: 研究人员分析了随机特征近似的偏差和方差，并提供了正常 BELL 函数的准确表达和锐度上的约束。<details>
<summary>Abstract</summary>
Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
</details>
<details>
<summary>摘要</summary>
随机特性被引入扩大内积方法 via 随机技术。特别是随机傅里父特性和正交随机特性被用来近似各种各样的加aussian颗点。前者通过随机γ矩阵实现，并在均值后变为加aussian颗点。在这项工作中，我们分析内积方法的偏差和方差基于正交随机特性，使用正则化的贝塞尔函数获得明确的表达，并 deriv出锐尖指数 bound，支持我们的观点，即正交随机特性比Random Fourier Features更有用。
</details></li>
</ul>
<hr>
<h2 id="Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model"><a href="#Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model" class="headerlink" title="Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model"></a>Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07367">http://arxiv.org/abs/2310.07367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Zhu, Meng Ding, Vaneet Aggarwal, Jinhui Xu, Di Wang</li>
<li>for: 这个论文是为了解决在本地权限保护（LDP）模型下的稀疏线性回归问题。</li>
<li>methods: 这篇论文使用了非交互式LDP模型和分段交互式LDP模型，并提出了一种新的非交互式LDP算法。</li>
<li>results: 这篇论文得出了一个lower bound的下界，证明了对于具有$k$-稀疏参数的情况，非交互式LDP算法的性能是有限制的。同时，提出了一种高效的非交互式LDP算法，并且该算法还生成了一个高效的估计器。<details>
<summary>Abstract</summary>
In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an upper bound of $\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$ for the estimation error when the data is sub-Gaussian, which can be further improved by a factor of $O(\sqrt{d})$ if the server has additional public but unlabeled data. For the sequentially interactive LDP model, we show a similar lower bound of $\Omega({\frac{\sqrt{dk}{\sqrt{n}\epsilon})$. As for the upper bound, we rectify a previous method and show that it is possible to achieve a bound of $\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$. Our findings reveal fundamental differences between the non-private case, central DP model, and local DP model in the sparse linear regression problem.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新考虑了在本地隐私（LDP）模型下的稀疏线性回归问题。现有研究在非交互和顺序本地模型下都集中在了对于$1$-稀疏情况下的下界问题，并将其扩展到更一般的$k$-稀疏情况却是困难的。另外，是否存在高效的非交互LDP（NLDP）算法仍然是一个问题。为了解决这些问题，我们首先考虑了在$\epsilon$非交互LDP模型下的问题，并提供了$\ell_2$-范数估计误差的下界为$\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$，其中$n$是样本大小，$d$是空间维度。我们提出了一种创新的NLDP算法，这是这个问题的第一个解决方案。结果显示，这个算法还提供了一个高效的估计器作为副产品。我们的算法在SUB-Gaussian数据下的估计误差为$\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$，可以通过增加$O(\sqrt{d})$的因子进一步改进。如果服务器拥有额外的公共 но不带标签的数据，那么我们的算法可以在这些数据下进一步改进估计误差。在顺序交互LDP模型下，我们显示了一个相似的下界为$\Omega(\frac{\sqrt{dk}{\sqrt{n}\epsilon})$。在上界方面，我们修复了之前的方法，并显示了可以实现$\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$的上界。我们的发现表明了私人情况、中央DP模型和本地DP模型在稀疏线性回归问题中存在根本的不同。
</details></li>
</ul>
<hr>
<h2 id="GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning"><a href="#GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning" class="headerlink" title="GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning"></a>GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07365">http://arxiv.org/abs/2310.07365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 本研究旨在Addressing the “transferability-specificity dilemma” in graph domain transfer learning, 即如何在不同的图数据上训练预训练模型以获得更好的目标数据表现。</li>
<li>methods: 本研究提出了一种基于ControlNet的图控制模块（GraphControl），用于实现更好的图域传输学习。该模块通过使用通用结构预训练模型和ControlNet进行细化调整，使得输入空间的对应性提高，并 incorporate 目标数据的特有特征作为 conditional inputs。</li>
<li>results: 对于预训练模型的应用在目标 attributed 数据上，该方法可以获得1.4-3x的性能提升，同时与从scratch 训练方法相比，具有更好的表现和更快的收敛速率。<details>
<summary>Abstract</summary>
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.
</details>
<details>
<summary>摘要</summary>
GRaph-structured data 是全球各地模型复杂对象之间的关系，使得众多网络应用程序得以实现。日常网络上的未标注GRaph数据具有巨大的潜在可能性。GRaph自我supervised算法在丰富的未标注GRaph数据上取得了显著的成功，从而获得了一般知识。这些预训练模型可以应用于各种下游网络应用程序，从而节省训练时间并提高下游性能。然而，不同的GRaph，即使在看似相似的领域中，可能具有显著不同的属性 semantics，这会带来在传输预训练模型到下游任务的困难，甚至不可能。例如，下游任务中特定的任务特有节点信息通常会被故意忽略，以便利用预训练表示。这种困难被称为“传输性-特定性之争”在这种工作中。为Addressing this challenge, we introduce an innovative deployment module called GraphControl, inspired by ControlNet, to achieve better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.
</details></li>
</ul>
<hr>
<h2 id="Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction"><a href="#Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Atom-Motif Contrastive Transformer for Molecular Property Prediction"></a>Atom-Motif Contrastive Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07351">http://arxiv.org/abs/2310.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yu, Shuo Chen, Chen Gong, Gang Niu, Masashi Sugiyama</li>
<li>for: 这个论文的目的是提出一种基于Graph Transformer（GT）模型的新型分子属性预测（MPP）方法，以提高MPP的效果。</li>
<li>methods: 该方法不仅探索了基于对的对应关系（pairwise atoms）的基本互动，还考虑了分子中重要的功能组（e.g., 含有几个原子的函数组）的互动。</li>
<li>results: 对七个流行的数据集进行了广泛的评估，并与现有的状态艺术方法进行了比较，结果表明，该方法的效果明显高于现有的方法。<details>
<summary>Abstract</summary>
Recently, Graph Transformer (GT) models have been widely used in the task of Molecular Property Prediction (MPP) due to their high reliability in characterizing the latent relationship among graph nodes (i.e., the atoms in a molecule). However, most existing GT-based methods usually explore the basic interactions between pairwise atoms, and thus they fail to consider the important interactions among critical motifs (e.g., functional groups consisted of several atoms) of molecules. As motifs in a molecule are significant patterns that are of great importance for determining molecular properties (e.g., toxicity and solubility), overlooking motif interactions inevitably hinders the effectiveness of MPP. To address this issue, we propose a novel Atom-Motif Contrastive Transformer (AMCT), which not only explores the atom-level interactions but also considers the motif-level interactions. Since the representations of atoms and motifs for a given molecule are actually two different views of the same instance, they are naturally aligned to generate the self-supervisory signals for model training. Meanwhile, the same motif can exist in different molecules, and hence we also employ the contrastive loss to maximize the representation agreement of identical motifs across different molecules. Finally, in order to clearly identify the motifs that are critical in deciding the properties of each molecule, we further construct a property-aware attention mechanism into our learning framework. Our proposed AMCT is extensively evaluated on seven popular benchmark datasets, and both quantitative and qualitative results firmly demonstrate its effectiveness when compared with the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
近期，图Transformer（GT）模型在分子性质预测（MPP）任务中广泛使用，因为它们可以准确地描述分子图节点之间的隐藏关系。然而，大多数现有的GT基本方法通常只研究对于每对原子的基本互动，因此它们忽略了分子中重要的功能块互动（例如，由几个原子组成的功能块）。在分子中，功能块是重要的特征征，它们对分子性质具有决定性的影响（例如，毒性和溶解度）。因此，忽略功能块互动将导致MPP的效果受限。为解决这个问题，我们提出了一种新的Atom-Motif Contrastive Transformer（AMCT）模型。AMCT不仅探索原子间互动，还考虑功能块互动。由于分子中的原子和功能块表示是同一个实例的两种视角，因此它们自然启合生成自我超级vis的信号。此外，同一个功能块可以出现在不同的分子中，因此我们还使用了对比损失来最大化功能块之间的表示协调。最后，为了明确每个分子中决定性的功能块，我们进一步构建了一个property-aware的注意机制到我们的学习框架中。我们的提出的AMCT模型在七个流行的benchmark数据集上进行了广泛的评估，结果表明，对于现有的state-of-the-art方法，AMCT模型在分子性质预测任务中表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundation-Models-for-Learning-on-Tabular-Data"><a href="#Towards-Foundation-Models-for-Learning-on-Tabular-Data" class="headerlink" title="Towards Foundation Models for Learning on Tabular Data"></a>Towards Foundation Models for Learning on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07338">http://arxiv.org/abs/2310.07338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, Jiang Bian<br>for: TabFMs are designed to overcome the limitations of current transferable tabular models, which lack support for direct instruction following in new tasks and neglect acquiring foundational knowledge and capabilities from diverse tabular datasets.methods: TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets.results: TabFMs significantly excel in instruction-following tasks like zero-shot and in-context inference, and achieve remarkable efficiency and competitive performance with scarce data. Additionally, TabFMs approach or even transcend the performance of renowned closed-source LLMs like GPT-4.<details>
<summary>Abstract</summary>
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference, but it also showcases performance that approaches, and in instances, even transcends, the renowned yet mysterious closed-source LLMs like GPT-4. Furthermore, when fine-tuning with scarce data, our model achieves remarkable efficiency and maintains competitive performance with abundant training data. Finally, while our results are promising, we also delve into TabFM's limitations and potential opportunities, aiming to stimulate and expedite future research on developing more potent TabFMs.
</details>
<details>
<summary>摘要</summary>
学习标准数据下的应用非常广泛。尽管有很大努力开发有效的学习模型 для标准数据，但目前可传递的标准模型仍然处于初始阶段，受到新任务 direct instruction 的支持或从多种标准数据集中获得基本知识和技能的忽略。在这篇论文中，我们提出了标准基本模型（TabFM），以解决这些局限性。TabFM 利用了生成标准学习的潜力，使用预训练的大型自然语言模型（LLM）作为基本模型，并通过特定目标进行精度调整，以涵盖广泛的标准数据集。这种方法赋予 TabFM 对标准数据的深刻理解和 universally 的能力。我们的评估表明，TabFM 非但在 zero-shot 和 context 推理任务中表现出色，而且在一些情况下，其表现甚至超越了知名但谜一族的关闭源 LLM 如 GPT-4。此外，当 fine-tuning  WITH 罕见数据时，我们的模型实现了很好的效率，并在有够数据进行 fine-tuning 时保持竞争性。最后，虽然我们的结果很有前途，但我们也探讨了 TabFM 的局限性和未来研究的可能性，以便激发和加速未来的 TabFM 研究。
</details></li>
</ul>
<hr>
<h2 id="Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer"><a href="#Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer" class="headerlink" title="Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer"></a>Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07323">http://arxiv.org/abs/2310.07323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zheng, Guogang Zhang, Chenchen Zhao, Qianqian Zhu</li>
<li>for: 这篇论文主要针对电力变压器诊断，即使现有的诊断方法主要基于粒子溶解分析，忽略了多通道连续数据的深度特征提取。</li>
<li>methods: 该文提出了多通道连续数据交叉提取结构（MCDC），用于全面利用变压器的内在特征，并且提出了一维 convolutional neural network注意力机制（1DCNN-attention），以更好地适应变压器诊断场景。</li>
<li>results: 实验表明，MCDC和1DCNN-attention比其他算法更有效，并且1DCNN-attention具有更好的稳定性。<details>
<summary>Abstract</summary>
Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are validated in experiments conducted on a dataset collected from real operation cases of power transformer. Additionally, the better stability of 1DCNN-attention has also been certified.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Power transformer 在网络基础设施中扮演着关键的角色，其诊断对稳定运行至关重要。然而，现有的变压器诊断方法主要基于离散气体分析，忽略了深入EXTRACT多个通道 consecutive data中的特征。这些未利用的序列数据包含了变压器状况的重要时间信息。为此，本文提出了多通道 consecutive data cross-EXTRACT (MCDC) 结构，以全面利用变压器的内在特征并评估其状况。此外，为更好地适应变压器诊断场景，本文还引入了一dimensional convolution neural network attention (1DCNN-attention) 机制，提供了更高效的解决方案，采用简化的空间复杂度。最后，实验 validate MCDC 和 1DCNN-attention 的效果，并证明它们在其他算法相比具有更高的泛化能力和稳定性。</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Resilient-Decentralized-Multi-Armed-Bandits"><a href="#Byzantine-Resilient-Decentralized-Multi-Armed-Bandits" class="headerlink" title="Byzantine-Resilient Decentralized Multi-Armed Bandits"></a>Byzantine-Resilient Decentralized Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07320">http://arxiv.org/abs/2310.07320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxuan Zhu, Alec Koppel, Alvaro Velasquez, Ji Liu</li>
<li>for: 这篇论文旨在研究在分散式合作多臂枪手（MAB）中，每个代理 observer 获得自己的奖励流，并寻求与其他代理交换信息，以选择一系列的臂，以最小化其后悔。</li>
<li>methods: 这篇论文使用了一种完全分散式抗衰伪（Byzantine）的Upper-Confidence Bound（UCB）算法，融合了代理之间的信息混合步骤，以及调整对奖励mean-estimates或信心集的错误值。</li>
<li>results: 这篇论文的主要贡献是开发了一个名为“完全分散式抗衰伪UCB”的算法，可以在分散式合作MAB中，防止潜在的Byzantine代理对奖励流的干扰，并且保证每个正常的代理的表现不会比过往的单一代理UCB1算法差。此外，研究显示，当每个代理有至少3f+1名邻的情况下，正常代理的总后悔会比非合作情况更好，其中f是最大可能的Byzantine代理数。<details>
<summary>Abstract</summary>
In decentralized cooperative multi-armed bandits (MAB), each agent observes a distinct stream of rewards, and seeks to exchange information with others to select a sequence of arms so as to minimize its regret. Agents in the cooperative setting can outperform a single agent running a MAB method such as Upper-Confidence Bound (UCB) independently. In this work, we study how to recover such salient behavior when an unknown fraction of the agents can be Byzantine, that is, communicate arbitrarily wrong information in the form of reward mean-estimates or confidence sets. This framework can be used to model attackers in computer networks, instigators of offensive content into recommender systems, or manipulators of financial markets. Our key contribution is the development of a fully decentralized resilient upper confidence bound (UCB) algorithm that fuses an information mixing step among agents with a truncation of inconsistent and extreme values. This truncation step enables us to establish that the performance of each normal agent is no worse than the classic single-agent UCB1 algorithm in terms of regret, and more importantly, the cumulative regret of all normal agents is strictly better than the non-cooperative case, provided that each agent has at least 3f+1 neighbors where f is the maximum possible Byzantine agents in each agent's neighborhood. Extensions to time-varying neighbor graphs, and minimax lower bounds are further established on the achievable regret. Experiments corroborate the merits of this framework in practice.
</details>
<details>
<summary>摘要</summary>
在分布式合作多臂抓拍机 (MAB) 中，每个代理都观察自己独特的奖励流，并尝试与其他代理交换信息，以选择一系列的臂，以最小化它的 regret。在合作环境中，代理可以超越单独运行 MAB 方法，如Upper-Confidence Bound (UCB)，独立运行。在这个工作中，我们研究如何在未知的代理中有 Byzantine 特性，即通过奖励价值或信任集的不正确通信，以获得类似的行为。这个框架可以用来模型计算机网络中的攻击者，导入推荐系统中的不良内容推广者，或财务市场中的操纵者。我们的主要贡献是开发一个完全分布式抗错误上界 (UCB) 算法，它结合代理之间的信息混合步骤与跳过矛盾和极端值的调整步骤。这个调整步骤使我们能够证明每个正常的代理的性能不 inferior 于过去的单一代理 UCB1 算法，并且更重要的是，所有正常的代理的总 regret 比非合作情况更好，只要每个代理至少有 3f+1 个邻居，其中 f 是最大可能的 Byzantine 代理数量。我们还将进一步推广到时间 varying 邻国图，以及最坏情况下的下界。实验证明了这个框架在实践中的优点。
</details></li>
</ul>
<hr>
<h2 id="Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction"><a href="#Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction" class="headerlink" title="Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction"></a>Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07313">http://arxiv.org/abs/2310.07313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Michał Sadowski, Piotr Kozakowski, Ruard van Workum, Stanisław Jastrzębski</li>
<li>for: 这个论文是为了解决有机化学中复杂分子的合成问题，使用机器学习算法预测可能的反应substrate。</li>
<li>methods: 这个论文使用的方法是基于最小模板（minimal templates），它们是简化反应模式，只包含关键分子变化。这种方法可以减少计算开销，并达到了标准测试数据集的状态前的结果。</li>
<li>results: 论文得到了最新的结果，在标准测试数据集上达到了最高的预测精度。<details>
<summary>Abstract</summary>
Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chineseRetrosynthesis是指将复杂分子synthesize成更简单的前体物质。在有机化学中，这种问题提出了挑战，特别是预测目标分子可能的反应substrate。这些解决方案主要分为模板基的和模板自由的两类。前者是高效的，但需要大量的预定反应模式，而后者具有更多的灵活性，但计算 overhead 较高，并且更难于解释。为解决这些问题，我们介绍了METRO（分子修改模板 для RetrOsynthesis），一种机器学习模型，通过使用最小的模板预测反应，减少计算开销，并在标准的benchmark上实现了状态的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Score-Regularized-Policy-Optimization-through-Diffusion-Behavior"><a href="#Score-Regularized-Policy-Optimization-through-Diffusion-Behavior" class="headerlink" title="Score Regularized Policy Optimization through Diffusion Behavior"></a>Score Regularized Policy Optimization through Diffusion Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07297">http://arxiv.org/abs/2310.07297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/srpo">https://github.com/thu-ml/srpo</a></li>
<li>paper_authors: Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, Jun Zhu</li>
<li>for: 增强offline reinforcement学习中的行为策略表示能力，通过抽象模型的准确描述来提高行为策略的生成能力。</li>
<li>methods: 提议从批判模型中提取高效的推理策略，并将其与预训练的扩散行为模型结合使用，以直接在优化过程中使用行为分布的分数函数来规范策略梯度。</li>
<li>results: 对D4RL任务进行了广泛的测试，结果显示，我们的方法可以在涉及到行动的任务中提高行动采样速度，并且仍然保持状态革命的性能水平，与其他主流的扩散基于方法相比，可以提高行动采样速度超过25倍。<details>
<summary>Abstract</summary>
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD"><a href="#Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD" class="headerlink" title="Why Does Sharpness-Aware Minimization Generalize Better Than SGD?"></a>Why Does Sharpness-Aware Minimization Generalize Better Than SGD?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07269">http://arxiv.org/abs/2310.07269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, Quanquan Gu</li>
<li>for: 该论文旨在解释Sharpness-Aware Minimization（SAM）如何提高大型神经网络的泛化能力，特别是在非线性神经网络和分类任务中。</li>
<li>methods: 论文使用了Stochastic Gradient Descent（SGD）和SAM两种训练方法进行比较，并通过分析损失函数的特性来解释SAM的泛化优势。</li>
<li>results: 实验结果表明，SAM在Synthetic和实际数据上都能够更好地适应不稳定的损失函数，从而提高神经网络的泛化能力。<details>
<summary>Abstract</summary>
The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby facilitating more effective learning of features. Experiments on both synthetic and real data corroborate our theory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大脑网络训练中的拟合问题，即模型记忆训练数据而无法泛化测试数据，已成为训练大脑网络的挑战。为解决这个挑战，锐度感知训练方法（SAM）已经出现了，它可以提高 neural networks 的泛化性，即使存在标签噪声。然而，SAM 在非线性 neural networks 和分类任务中的工作机制仍然不够了解。这篇文章填补这个空白，并证明 SAM 在某种数据模型和两层卷积 ReLU 网络中能够更好地泛化。我们的研究问题的损失场景是非拟合的，因此现有基于 Hessian 信息的解释不够。我们的结果解释了 SAM 的优势，特别是它在早期阶段防止噪声学习，从而促进更有效的特征学习。实验表明，Synthetic 和实际数据都支持我们的理论。
</details></li>
</ul>
<hr>
<h2 id="RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence"><a href="#RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence" class="headerlink" title="RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence"></a>RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07268">http://arxiv.org/abs/2310.07268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changan Yang, Yaxing Chen, Yao Zhang, Helei Cui, Zhiwen Yu, Bin Guo, Zheng Yan, Zijiang Yang</li>
<li>for: 本研究旨在解决智能交通系统中数据隐私协议问题。</li>
<li>methods: 该研究提出了一种基于raft协议的联邦学习框架，以保护数据隐私。</li>
<li>results: 实验结果显示，该框架比基eline对比更好地减少通信开销、提高模型准确率和加速模型融合。<details>
<summary>Abstract</summary>
Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated by state-of-the-art vehicular ad-hoc networks and artificial intelligence, various VCI applications come to place, e.g., collaborative sensing, positioning, and mapping. The collaborative property of VCI applications generally requires data to be shared among participants, thus forming network-wide intelligence. How to fulfill this process without compromising data privacy remains a challenging issue. Although federated learning (FL) is a promising tool to solve the problem, adapting conventional FL frameworks to VCI is nontrivial. First, the centralized model aggregation is unreliable in VCI because of the existence of stragglers with unfavorable channel conditions. Second, existing FL schemes are vulnerable to Non-IID data, which is intensified by the data heterogeneity in VCI. This paper proposes a novel federated learning framework called RaftFed to facilitate privacy-preserving VCI. The experimental results show that RaftFed performs better than baselines regarding communication overhead, model accuracy, and model convergence.
</details>
<details>
<summary>摘要</summary>
Vehicular crowd intelligence (VCI) 是一个emerging研究领域。受到现代交通自适应网络和人工智能的支持，VCI应用广泛，例如合作探测、定位和地图生成。VCI应用的协作性通常需要参与者共享数据，从而形成网络范围内的智能。然而，保持数据隐私性的问题仍然是一个挑战。虽然联邦学习（FL）是一种有前途的工具来解决这个问题，但将传统的FL框架应用于VCI是非常困难。首先，中央模型聚合是VCI中不可靠的，因为存在具有不利通信条件的废寸者。其次，现有的FL方案容易受到非同kind的数据影响，这在VCI中加剧了数据多样性。本文提出了一种新的联邦学习框架called RaftFed，以保护隐私的VCI。实验结果表明，RaftFed在通信开销、模型准确率和模型融合方面比基elinebetter。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review"><a href="#Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review" class="headerlink" title="Classification of Dysarthria based on the Levels of Severity. A Systematic Review"></a>Classification of Dysarthria based on the Levels of Severity. A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07264">http://arxiv.org/abs/2310.07264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Al-Ali, Somaya Al-Maadeed, Moutaz Saleh, Rani Chinnappa Naidu, Zachariah C Alex, Prakash Ramachandran, Rajeev Khoodeeram, Rajesh Kumar M</li>
<li>For: 这项研究的目的是对慢性喉突症的自动分类方法进行系统性的回顾和分析，以提高诊断的准确性和可靠性。* Methods: 该研究使用了数据库和灰Literature进行搜索，并根据研究问题的相关性进行选择。抽取数据包括使用了哪些方法、哪些特征来进行分类，以及使用了哪些人工智能技术。* Results: 该研究发现了一些有效的特征和人工智能技术，可用于自动分类慢性喉突症的严重程度。这些发现有助于提高诊断的准确性和可靠性，并可能提高患者的治疗效果。<details>
<summary>Abstract</summary>
Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.
</details>
<details>
<summary>摘要</summary>
某�ayer�症是一种神经学 speech 障碍，可能对患者的沟通能力和全面生活质量产生重要影响。精准和客观地分类某�ayer�症和其严重程度是诊断 intervención 的关键。现有的传统评估方法由speech-language pathologists (SLPs) 进行，但这些评估方法frequently 主观、时间consuming 和 между practitioners 可能存在差异。新兴的机器学习基本模型已经显示出可以提供更客观的某�ayer�症诊断，提高诊断准确性和可靠性。本系统性文献综述 aimsto comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.
</details></li>
</ul>
<hr>
<h2 id="Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation"><a href="#Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation" class="headerlink" title="Deep ReLU networks and high-order finite element methods II: Chebyshev emulation"></a>Deep ReLU networks and high-order finite element methods II: Chebyshev emulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07261">http://arxiv.org/abs/2310.07261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joost A. A. Opschoor, Christoph Schwab</li>
<li>for: This paper focuses on addressing expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) for continuous, piecewise polynomial functions on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$.</li>
<li>methods: The paper develops novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients, which can be computed easily from the values of the function in the Clenshaw–Curtis points using the inverse fast Fourier transform.</li>
<li>results: The paper obtains bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials, and provides ReLU NN emulation error estimates for various classes of functions and norms, including analytic functions with point singularities.<details>
<summary>Abstract</summary>
Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for various classes of functions and norms, commonly encountered in numerical analysis. In particular, we show exponential ReLU emulation rate bounds for analytic functions with point singularities and develop an interface between Chebfun approximations and constructive ReLU NN emulations.
</details>
<details>
<summary>摘要</summary>
文本中的问题是关于深度ReLU神经网络（NN）中 Sobolev norm 的表达率和稳定性，以及NN的参数数量对于连续、分割函数在固定区间（a, b）上的性能。我们提出了一种基于Chebyshev多项式扩展的ReLU NN伪函数代码的新构造方法，该方法可以从Clenshaw-Curtis点中计算Chebyshev约数。我们获得了基于于ReLU NN模拟精度和参数数量的上下文下的表达率和稳定性的较好的上限。我们还提供了各种函数和 нор 的ReLU NN模拟误差估计，常见于数值分析中。特别是，我们证明了对分布函数的点缺陷函数的 exponentially fast ReLU 模拟率上限。此外，我们还建立了Chebfun近似和构造性ReLU NN模拟的接口。
</details></li>
</ul>
<hr>
<h2 id="CacheGen-Fast-Context-Loading-for-Language-Model-Applications"><a href="#CacheGen-Fast-Context-Loading-for-Language-Model-Applications" class="headerlink" title="CacheGen: Fast Context Loading for Language Model Applications"></a>CacheGen: Fast Context Loading for Language Model Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07240">http://arxiv.org/abs/2310.07240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen Jiang</li>
<li>for: 提高 LLM 系统的响应速度和可扩展性，使其能够更好地处理长 Context  Task。</li>
<li>methods:  CacheGen 使用了一种新的编码器，将 Key-Value 特征压缩成更加Compact 的 Bitstream 表示形式，以降低带宽使用情况。此外，CacheGen 还使用了一个控制器，根据 Context 的长度和 LLM 的大小，选择合适的压缩级别和加载 Context 的方式，以最小化总的延迟和带宽使用情况。</li>
<li>results: CacheGen 比现有的长 Context 处理方法减少了带宽使用情况 by 3.7-4.3x，并将总的延迟和处理 Context 的时间减少到 2.7-3x，同时保持 LLM 在不同任务上的性能相似。<details>
<summary>Abstract</summary>
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).   This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a tailored arithmetic coder, taking advantage of the KV features' distributional properties, such as locality across tokens. Furthermore, CacheGen minimizes the total delay in fetching and processing a context by using a controller that determines when to load the context as compressed KV features or raw text and picks the appropriate compression level if loaded as KV features. We test CacheGen on three models of various sizes and three datasets of different context lengths. Compared to recent methods that handle long contexts, CacheGen reduces bandwidth usage by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x while maintaining similar LLM performance on various tasks as loading the text contexts.
</details>
<details>
<summary>摘要</summary>
LLMs 在进行更复杂的任务时，其输入会包含更长的上下文，以回答需要领域知识或用户特定的对话历史的问题。然而，使用长上下文会对响应 LLM 系统造成挑战，因为没有可以生成任何内容直到所有上下文都被 fetched 并处理于 LLM 中。现有系统通常仅仅优化计算延迟在上下文处理中（例如，通过缓存中间键值特征），但这经常会导致网络延迟更长。本文提出了 CacheGen，用于最小化 LLM 上下文的抓取和处理延迟。CacheGen 减少了在传输长上下文中的键值特征的带宽，通过一种新的编码器，将键值特征转换为更 компакт的比特流表示。该编码器结合了自适应量化和特制的加法编码器，利用键值特征的分布特性，如token之间的本地性。此外，CacheGen 还减少了抓取和处理上下文的总延迟，通过一个控制器，确定在加载上下文时是作为压缩键值特征还是原始文本，并选择适当的压缩级别。我们在不同的模型和数据集上测试了 CacheGen，相比之前的长上下文处理方法，CacheGen 可以减少带宽使用率为 3.7-4.3倍，并减少抓取和处理上下文的总延迟为 2.7-3倍，而保持 LLM 在不同任务上的性能相似。
</details></li>
</ul>
<hr>
<h2 id="Are-GATs-Out-of-Balance"><a href="#Are-GATs-Out-of-Balance" class="headerlink" title="Are GATs Out of Balance?"></a>Are GATs Out of Balance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07235">http://arxiv.org/abs/2310.07235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz</li>
<li>for: 这个论文的目的是研究图解析网络（Graph Neural Network，GNN）的优化和学习动态。</li>
<li>methods: 这篇论文使用了Graph Attention Network（GAT） Architecture，并对其进行了分析和研究。</li>
<li>results: 论文发现GAT中的权重系数化的注意力归一化会导致许多参数在训练中难以更改，这会导致深度的GAT表现更差。 authors提出了一种 Initialization scheme，可以更好地传递梯度，并使得更深的网络可以更好地训练。<details>
<summary>Abstract</summary>
While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.
</details>
<details>
<summary>摘要</summary>
而且表达能力和计算能力 OF 图 neural networks (GNNs) 已经被理论研究，但是它们的优化和学习动力，总的来说，还很少研究。我们的研究探讨了 Graph Attention Network (GAT)，一种流行的 GNN 架构，其中每个节点的邻居汇集被参数化的注意系数权重。我们得出了 GAT 梯度流动动力学保守法则，解释了为什么大多数 GAT 参数在训练中难以更改。这个效应在更深的 GAT 中更加明显，它们在训练和结束时间比其浅层对手更差。为了解决这个问题，我们提出了一种Initialization scheme，它可以更好地传递梯度，从而使得深度网络更容易训练，并且在训练和结束时间比标准Initialization更快。我们的主要定理作为 изучение positive homogeneous models with attention mechanism 的学习动力的开门篇。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality"><a href="#Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality" class="headerlink" title="Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality"></a>Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07234">http://arxiv.org/abs/2310.07234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/hide-prompt">https://github.com/thu-ml/hide-prompt</a></li>
<li>paper_authors: Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu</li>
<li>for: 这 paper 的目的是提出一种基于 prompt-based  kontinual learning 的方法，以优化 continual learning 的性能。</li>
<li>methods: 这 paper 使用了 hierarchical decomposition 的思想，将 continual learning 目标划分为三个组成部分：内 task 预测、任务标识推理和任务适应预测。并使用了一个 ensemble 的任务特定 prompt 和 Both 的统计来优化这些组成部分。</li>
<li>results: 这 paper 的实验结果表明，使用 HiDe-Prompt 方法可以在 continual learning 中提高性能，并且对不同的 pre-training 方法具有较高的灵活性和可repeatability。在 Split CIFAR-100 和 Split ImageNet-R 上，HiDe-Prompt 方法可以达到15.01% 和9.61% 的提升。<details>
<summary>Abstract</summary>
Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \url{https://github.com/thu-ml/HiDe-Prompt}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment"><a href="#Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment" class="headerlink" title="Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment"></a>Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07229">http://arxiv.org/abs/2310.07229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, Weiying Ma, Zhiming Ma, Yanyan Lan</li>
<li>for: 这个研究是为了提出一种新的蛋白质口袋预训练方法，以便更好地模型蛋白质和药物之间的互动。</li>
<li>methods: 这个方法使用了高解析力的蛋白质结构知识，以及已经预训的小分子表示，将蛋白质结构切割为药物相似的片段和其相应的口袋，以获得可靠的药物-蛋白质互动模型。</li>
<li>results: 这个方法名为ProFSA，可以在不同的任务上 дости得到最佳性能，包括口袋可用性预测、口袋匹配和药物缩紧价预测等。另外，这个方法可以轻松地处理大量的蛋白质结构数据，并且可以提高药物设计的效率。<details>
<summary>Abstract</summary>
Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.
</details>
<details>
<summary>摘要</summary>
腋部表示在各种生物医学应用中发挥重要作用，如药物可用性预测、药物粘性预测和初始药物设计。现有的几何特征和预训 repre sentation 已经达到了一定的成果，但是它们通常忽略了药物和腋部之间的基本交互。然而， Protein Data Bank 数据库中的药物-腋部复合结构数据量非常有限（ menos than 100 thousand non-redundant pairs），这限制了大规模预训的可行性。为了解决这一问题，我们提出了一种新的腋部预训方法，利用高级别的原子保健结构知识，并且利用已经预训的小分子表示。我们将蛋白结构分解成可以模拟药物-腋部交互的药物类型和相应的腋部，从而生成了超过500万个复合体。然后，我们将腋部编码器在对 pseudo-ligand 的表示进行对比的情况下进行准确的训练。我们的方法，名为 ProFSA，在不同的任务中均达到了领先的性能，包括腋部可用性预测、腋部匹配和药物粘性预测。此外，我们的工作打开了一个新的途径，通过利用高质量和多样化的蛋白结构数据库，来缓解蛋白-药物复合数据的稀缺。
</details></li>
</ul>
<hr>
<h2 id="COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL"><a href="#COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL" class="headerlink" title="COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL"></a>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07220">http://arxiv.org/abs/2310.07220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, Furong Huang</li>
<li>for: 提高模型基于学习方法的效率和终点性能</li>
<li>methods: 使用保守的模型执行和乐观环境探索，并通过不确定性感知来帮助模型预测错误的修正</li>
<li>results: 对一系列 proprioceptive 和视觉 kontinuous control 任务进行了实验，并证明了 $\texttt{COPlanner}$ 可以提高模型基于方法的样本效率和终点性能<details>
<summary>Abstract</summary>
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. $\texttt{COPlanner}$ is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with $\texttt{COPlanner}$.
</details>
<details>
<summary>摘要</summary>
模型基于的强化学习方法包括两个阶段：模型执行以生成策略学习的样本，以及使用当前策略进行真实环境中的动态模型学习。然而，由于真实世界环境的复杂性，是不可避免学习不准确的动态模型，这会导致策略学习错误，从而导致优化解决方案。在这篇论文中，我们提出了 $\texttt{COPlanner}$，一种基于规划的框架，用于解决不准确学习动态模型问题。 $\texttt{COPlanner}$ 利用了一种不确定性意识权益导向的模型预测控制（UP-MPC）组件，以计划多步不确定性估计。这个估计的不确定性然后用于模型执行中的罚款，以及在真实环境中的探索中的奖励。因此， $\texttt{COPlanner}$ 可以避免模型不确定区域，从而减轻模型预测错误的影响。同时，它可以活动地探索高奖励模型不确定区域，以减少模型预测错误。 $\texttt{COPlanner}$ 是一个可插入的框架，可以与任何强化学习方法结合使用。实验结果表明，在一系列的 proprioceptive 和视觉连续控制任务上， $\texttt{COPlanner}$ 可以显著提高强化学习方法的样本效率和最终性能。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices"><a href="#Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices" class="headerlink" title="Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices"></a>Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07217">http://arxiv.org/abs/2310.07217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessio Burrello, Matteo Risso, Beatrice Alessandra Motetti, Enrico Macii, Luca Benini, Daniele Jahier Pagliari</li>
<li>for: 这篇论文的目的是提出一种能够实现内置式神经网络搜寻（Differentiable NAS）来实现内置式神经网络的最佳化，并且能够考虑多个硬件限制，以提高内置式神经网络的精度和效率。</li>
<li>methods: 这篇论文使用了一种称为“Differentiable NAS”的搜寻方法，可以同时考虑多个硬件限制，例如内存和延迟等限制，以实现最佳化。</li>
<li>results: 这篇论文的实验结果显示，使用Differentiable NAS可以在实现内置式神经网络的同时，实现87.4%的内存和54.2%的延迟的减少，并且保持与现有的手动调整深度神经网络相同的精度。<details>
<summary>Abstract</summary>
The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency in a time comparable to a single standard training. The proposed approach is evaluated on five IoT-relevant benchmarks, including the MLPerf Tiny suite and Tiny ImageNet, demonstrating that, with a single search, it is possible to reduce memory and latency by 87.4% and 54.2%, respectively (as defined by our targets), while ensuring non-inferior accuracy on state-of-the-art hand-tuned deep neural networks for TinyML.
</details>
<details>
<summary>摘要</summary>
“由于互联网物联网（IoT）设备的快速扩散，需要有效率和准确的深度学习（DL）模型，但传统的DL模型通常过于复杂和计算投入。为解决这个挑战，深度架构搜索（NAS）已成为一种受欢迎的设计自动化技术，可以同时优化模型的准确率和复杂度。然而，现有的NAS技术通常需要许多迭代才能生成遵循硬件限制的网络，例如硬件中最大内存容量或target应用中允许的延迟。在这个工作中，我们提出了一种新的方法，可以将多个约束 integrating into so-called Differentiable NAS optimization methods，允许在一次射击中生成遵循用户定义的内存和延迟约束的模型，并且与现有的手动调整深度神经网络相比，可以在相同的时间内减少内存和延迟量，同时保证模型的准确率不下降。我们的方法在五个 IoT 相关的 bencmarks 上进行了评估，包括 MLPerf Tiny suite 和 Tiny ImageNet，结果表明，通过单一的搜索，可以在内存和延迟方面减少 87.4% 和 54.2%，respectively（根据我们的目标），而且与现有的手动调整深度神经网络相比，模型的准确率不下降。”
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes"><a href="#Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes" class="headerlink" title="Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes"></a>Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07216">http://arxiv.org/abs/2310.07216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehyeong Jo, Sung Ju Hwang</li>
<li>for: 模型数据的分布在里曼尼安 manifolds 中，是许多科学领域中必需的，但现有的生成模型在 manifolds 上受到假设热体kernel的限制，只能应用于简单的几何结构。本文提出了里曼尼安扩散混合（Riemannian Diffusion Mixture），一种理想的生成过程模型，基于endpoint-conditioned扩散过程，而不是前一些扩散模型的减量方法。</li>
<li>methods: 我们提出了一种简单 yet efficient的训练目标，可以轻松应用于一般 manifolds。我们的方法在多种 manifold 上进行了比较，并且可以在高维度下进行扩散，需要少量的在训练过程中的 simulations 步骤。</li>
<li>results: 我们的方法在多种 manifold 上进行了比较，并且在高维度下进行扩散，需要少量的在训练过程中的 simulations 步骤。我们的方法可以在不同的 manifold 上模型数据的分布，并且可以在高维度下进行扩散。<details>
<summary>Abstract</summary>
Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outperforms previous generative models on various manifolds while scaling to high dimensions and requires a dramatically reduced number of in-training simulation steps for general manifolds.
</details>
<details>
<summary>摘要</summary>
“理解数据在里曼尼安 manifold 上的分布是模型非欧几何空间数据的关键，这是许多科学领域的应用所需。然而，现有的生成模型在 manifold 上受到严重的分布计算成本或基于热征函数的 aproximation 的限制，这限制了它们的可靠性和扩展性。在这项工作中，我们介绍了里曼尼安混合模型，一种基于endpoint-conditioned diffusion process的主导性框架 для生成过程。我们还提出了一种简单 yet efficient的训练目标，可以方便地应用于通用 manifold。我们的方法在多种 manifold 上比previous生成模型表现出色，可以扩展到高维度，并且需要减少很多在训练过程中的 simulations 步骤。”Note: Simplified Chinese is a writing system used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other countries. The translation is written in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration"><a href="#Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration" class="headerlink" title="Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration"></a>Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07211">http://arxiv.org/abs/2310.07211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Yunan Wang, Guojian Zhan, Jie Li, Shengbo Eben Li</li>
<li>for: 这 paper 证明了正则化策略迭代法（Regularized Policy Iteration, RPI）与标准的新颖-拉普逊方法（Newton-Raphson method）在减少 Bellman 方程的情况下是等价的。</li>
<li>methods: 这 paper 使用了正则化策略迭代法（RPI）和减少 Bellman 方程的方法来证明 RPI 的全局线性减少率为 $\gamma$（折扣因子），并且在本地区域内 converge 到最优值 quadratic 速率。</li>
<li>results: 这 paper 证明了 RPI 在 global 和本地区域内都有 linear 减少率，其中 global 减少率为 $\gamma$，而在本地区域内 converge 到最优值 quadratic 速率。<details>
<summary>Abstract</summary>
Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite-step policy evaluation, is equivalent to inexact Newton method where the Newton iteration formula is solved with truncated iterations. We prove that the associated algorithm achieves an asymptotic linear convergence rate of $\gamma^M$ in which $M$ denotes the number of steps carried out in policy evaluation. Our results take a solid step towards a better understanding of the convergence properties of regularized policy iteration algorithms.
</details>
<details>
<summary>摘要</summary>
“常规化”是强化学习算法中最重要的技术之一。软 actor-批评算法是常规化 policy 迭代的特殊情况，其正则化项选择为 entropy 函数。虽然软 actor-批评算法在实际上获得了一些成功，但其理论基础仍然不清楚。这篇论文证明了软 actor-批评算法与标准的 Newton-Raphson 方法在 Bellman 方程缓和强 convex 函数时是等价的。这种等价关系为软 actor-批评算法的全面分析奠定了基础。我们证明了软 actor-批评算法在 discount 因子 $\gamma$ 下有全面线性减少率 $\gamma$，并且当它进入一个地方邻域时，其减少率为 $\gamma^2$。我们还证明了一种修改后的软 actor-批评算法，即 finite-step policy evaluation，与不准确的 Newton 方法相等。我们证明了这种算法在 $\gamma^M$ 下具有极限线性减少率，其中 $M$ 是在 policy evaluation 中执行的步数。我们的结果为软 actor-批评算法的减少性质做出了一个坚实的步骤。
</details></li>
</ul>
<hr>
<h2 id="Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances"><a href="#Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances" class="headerlink" title="Robust Safe Reinforcement Learning under Adversarial Disturbances"></a>Robust Safe Reinforcement Learning under Adversarial Disturbances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07207">http://arxiv.org/abs/2310.07207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang</li>
<li>for: 本研究的目的是提出一种可靠的强化学习框架，以处理实际控制任务中的外部干扰。</li>
<li>methods: 本文提出了一种基于强化学习的两个玩家零 SUM 游戏，用于解决最差情况下的干扰问题。首先，本文解决了一个政策迭代算法，用于解决最差情况下的干扰问题。其次，本文将此算法 integrate 到一种受限制的强化学习算法中，以同时Synthesize 最差情况下的Robust invariants set和用于受限制的政策优化。</li>
<li>results: 实验表明，提出的方法可以在classic control tasks中实现零约束违反，同时也可以与其他基线算法相比，在缺省情况下达到相似的性能。<details>
<summary>Abstract</summary>
Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set. Second, this paper integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization. This algorithm tackles both optimality and safety, i.e., learning a policy that attains high rewards while maintaining safety under worst-case disturbances. Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Our proposed method also attains comparable performance as the baselines even in the absence of the adversary.
</details>
<details>
<summary>摘要</summary>
安全是控制任务中应用强化学习的基本问题，尤其在外部干扰存在时。然而，现有的安全强化学习算法很少考虑外部干扰，这限制了它们在实际应用中的可靠性和灵活性。为解决这个挑战，这篇论文提出了一种可靠安全强化学习框架，可以面对最坏情况的干扰。首先，这篇论文提出了一种策略迭代算法，用于解决可靠集的问题。在这种算法中，我们首先将控制输入和外部干扰转化为两个 игро之间的两进制零sum游戏，其中控制输入（ protagonist）的目标是保持安全，而外部干扰（ adversary）的目标是打砸安全。我们证明了该策略迭代算法 monotonic converge 到最大可靠 invariant set。其次，这篇论文将提出的策略迭代算法与 constrained reinforcement learning 算法结合，以同时Synthesize 可靠 invariant set 和用其进行约束化策略优化。这个算法同时解决了最优和安全的问题，即学习一个策略，可以在最坏情况下维护安全，同时尝试高的奖励。经典控制任务的实验表明，我们的提出方法可以避免安全约束的违反，而其他基线算法却有显著的违反。此外，我们的方法还可以与基线算法相比，在没有敌人情况下达到相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance"><a href="#Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance" class="headerlink" title="Boosting Learning for LDPC Codes to Improve the Error-Floor Performance"></a>Boosting Learning for LDPC Codes to Improve the Error-Floor Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07194">http://arxiv.org/abs/2310.07194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hee-Youl Kwak, Dae-Young Yun, Yongjune Kim, Sang-Hyo Kim, Jong-Seon No</li>
<li>for: 这个研究旨在提高低密度条件检查（LDPC）码的错误处理能力，以及实现LDPC码在需要极高可靠性的应用中的应用。</li>
<li>methods: 本研究使用了boosting学习技术，将捉猿网络分成两个网络，并将后网络特化为处理未被首网络处理的错误码字。此外，本研究还使用了区块化训练时间表，以地方地训练网络单元，并将不满意验证节点赋予不同的重量，以降低错误地板。</li>
<li>results: 本研究显示，通过这些训练方法，可以将LDPC码的错误地板降低到最低水平，并且不需要新增硬件成本。此外，与其他解码方法相比，本研究的NMS解码器在错误地板方面表现最佳。<details>
<summary>Abstract</summary>
Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction ability and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, poses challenges in achieving extremely low error rates and the application of LDPC codes in scenarios demanding ultra high reliability. In this work, we propose training methods to optimize neural min-sum (NMS) decoders that are robust to the error-floor. Firstly, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two networks and train the post network to be specialized for uncorrected codewords that failed in the first network. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning different weights to unsatisfied check nodes effectively lowers the error-floor with a minimal number of weights. By applying these training methods to standard LDPC codes, we achieve the best error-floor performance compared to other decoding methods. The proposed NMS decoder, optimized solely through novel training methods without additional modules, can be implemented into current LDPC decoders without incurring extra hardware costs. The source code is available at https://github.com/ghy1228/LDPC_Error_Floor.
</details>
<details>
<summary>摘要</summary>
低密度差异检查（LDPC）编码已经成功商业化在通信系统中，因为它们具有强大的错误检测能力和简单的解码过程。然而，LDPC编码中的错误地面现象，导致错误率在某个水平上停止下降，对于实现极高可靠性的应用场景带来挑战。在这项工作中，我们提出了优化神经网络最小和（NMS）解码器的训练方法，以便对错误地面进行鲁棒性处理。首先，我们利用神经网络集成技术的扩展学习方法，将解码网络分成两个网络，并在第二个网络中训练后网络来处理未被首个网络正确解码的代码字。其次，为了解决训练过程中的消失梯度问题，我们引入了块 wise 训练时间表，以地方性地训练每个块的 weights，而不是全网络。最后，我们发现，对不满足的检查节点赋予不同的权重，可以有效降低错误地面，并且只需要少量的权重。通过这些训练方法，我们对标准 LDPC 编码进行优化，实现了最佳的错误地面性能。我们的 NMS 解码器，通过solely 利用新的训练方法而无需添加额外模块，可以在现有 LDPC 解码器中实现，无需新增硬件成本。源代码可以在 GitHub 上找到：https://github.com/ghy1228/LDPC_Error_Floor。
</details></li>
</ul>
<hr>
<h2 id="Neural-networks-deep-shallow-or-in-between"><a href="#Neural-networks-deep-shallow-or-in-between" class="headerlink" title="Neural networks: deep, shallow, or in between?"></a>Neural networks: deep, shallow, or in between?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07190">http://arxiv.org/abs/2310.07190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guergana Petrova, Przemyslaw Wojtaszczyk</li>
<li>for: 这 paper 是为了研究Feed-forward neural network的输出错误率的估计。</li>
<li>methods: 这 paper 使用了 Lipschitz 活化函数和宽度 W、深度 l 的 feed-forward neural network。</li>
<li>results: 这 paper 发现，当 depth l 增长到无穷大时，Feed-forward neural network 的输出错误率可以达到更好的水平，且 fixing width W 并让 depth l 增长不会带来任何改善。<details>
<summary>Abstract</summary>
We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
</details>
<details>
<summary>摘要</summary>
我们提供以下估计对封闭子集的扩张错误的渐近近似，使用内置构造神经网络的输出。我们显示，对于内置深度趋向无限大的神经网络，可能达到比据数据类型的 entropy 数据更好的近似率，并且随着宽度 W 变大，获得的近似率不会提高。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival"><a href="#Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival" class="headerlink" title="Kernel Cox partially linear regression: building predictive models for cancer patients’ survival"></a>Kernel Cox partially linear regression: building predictive models for cancer patients’ survival</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07187">http://arxiv.org/abs/2310.07187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rongyaohua/reggkm">https://github.com/rongyaohua/reggkm</a></li>
<li>paper_authors: Yaohua Rong, Sihai Dave Zhao, Xia Zheng, Yi Li</li>
<li>for: 预测肿瘤患者的临床结果，准确预测肿瘤患者的生存时间。</li>
<li>methods: 使用kernel Cox分数模型和regularized garrotized kernel machine（RegGKM）方法，同时自动 remov irrelevant parametric和非 Parametric预测器。</li>
<li>results: 在模拟试验中，提出的方法 sempre有更高的预测精度，并在多发性骨癌数据集中预测患者的死亡负担基于基因表达。<details>
<summary>Abstract</summary>
Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method always has better predictive accuracy. We apply this method to analyze a multiple myeloma dataset and predict patients' death burden based on their gene expressions. Our results can help classify patients into groups with different death risks, facilitating treatment for better clinical outcomes.
</details>
<details>
<summary>摘要</summary>
广泛的多样性存在于癌症患者的存活时间中，从几个月到数十年不等。要准确预测临床结果，需要建立一个准确预测模型，将患者的分子 profilest 与患者的存活时间相关联。由于存活时间和高维分子预测器之间存在复杂的关系，同时需要进行非 Parametric 模型化和无关预测器的去除，这使得模型建立变得具有挑战性。在本文中，我们建立了一个 kernel Cox 协方差 semi-parametric 模型，并提出了一种新的 RegGKM 方法来适应模型。我们使用 kernel machine 方法来描述存活时间和预测器之间的复杂关系，同时通过 Lasso 罚因自动去除无关 Parametric 和非 Parametric 预测器。我们开发了一种高维度的高效算法来实现该方法。与其他竞争方法在 simulated 中进行比较表明，我们的方法总是有更高的预测精度。我们应用该方法分析了一个多发性骨髓癌数据集，并预测患者的死亡负担基于他们的基因表达。我们的结果可以帮助分类患者为不同的死亡风险群，以便进行更好的临床结果。
</details></li>
</ul>
<hr>
<h2 id="SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation"><a href="#SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation" class="headerlink" title="SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation"></a>SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07183">http://arxiv.org/abs/2310.07183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shellredia/sam-octa">https://github.com/shellredia/sam-octa</a></li>
<li>paper_authors: Xinrun Chen, Chengliang Wang, Haojian Ning, Shiying Li<br>for:* 这个论文主要是为了提出一种基于low-rank适应技术的自适应模型，用于进行多种Optical coherence tomography angiography（OCTA）图像分割任务。methods:* 该方法使用了基于low-rank适应技术的基础模型，并提出了相应的提示点生成策略，以处理不同的OCTA图像分割任务。results:* 该方法在公共可用的OCTA-500和ROSE数据集上进行了实验，并达到或超越了当前状态的分割性能指标。* 该方法可以实现当地血管分割和有效的血管-血管分割，这些任务在前一些工作中尚未得到好的解决。<details>
<summary>Abstract</summary>
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The code is available at https://github.com/ShellRedia/SAM-OCTA.
</details>
<details>
<summary>摘要</summary>
《Optical coherence tomography angiography（OCTA）图像分析中的目标分割问题》的研究中，需要进行特定目标的分割。现有的方法通常是在有限的样本（约一百个）上进行指导学习，这可能会导致过拟合。为解决这个问题，我们采用了低级别适应技术，并提出了相应的提示点生成策略，以处理多种OCTA图像分割任务。我们命名这种方法为SAM-OCTA，并在公共可用的OCTA-500和ROSE数据集上进行了实验。SAM-OCTA方法实现了或接近了状态之 arts的分割性能指标。我们还详细讨论了提示点的效果和应用性，特别是在Retinal vessel、foveal avascular zone、capillary、artery和vein segmentation任务中。此外，SAM-OCTA实现了本地血管分割和有效的artery-vein分割，这在前一个works中尚未得到解决。代码可以在https://github.com/ShellRedia/SAM-OCTA上下载。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions"><a href="#Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions" class="headerlink" title="Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions"></a>Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07174">http://arxiv.org/abs/2310.07174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungtaek Kim, Jeongbeen Yoon, Minsu Cho</li>
<li>for: 这篇论文的主要目标是解决排序问题，并对传统排序算法的问题定义进行扩展和改进。</li>
<li>methods: 该论文使用神经网络来解决排序问题，并采用了一种名为“排序网络”的方法。该方法使用了多头注意力 Mechanism来捕捉输入之间的依赖关系，并通过一种名为“满足非减少和导数条件”的满足函数来保证排序网络的导数性。</li>
<li>results: 实验结果显示，该方法在多种排序 benchmark 上表现比基eline方法更好，或者与基eline方法相当。<details>
<summary>Abstract</summary>
Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
</details>
<details>
<summary>摘要</summary>
Sorting 是计算机系统中的基本操作之一，长期作为研究主题。我们在传统排序算法的问题定义之外，考虑排序问题的更广泛和表达力强的输入，例如多个数字图像和图像断片，通过神经网络进行排序。为了学习高维输入到排序变量的映射，排序网络的导数性需要保证。在这篇论文中，我们定义了一种软化错误函数，并开发了一种不减少和导数性的排序网络。此外，我们采用了 permutation-equivariant Transformer 网络， capture 输入的依赖关系，并利用其自注意力来提高模型的表达能力。实验表明，我们的方法在多个排序 benchmark 上表现较好或与基eline 方法相当。
</details></li>
</ul>
<hr>
<h2 id="Federated-Generalization-via-Information-Theoretic-Distribution-Diversification"><a href="#Federated-Generalization-via-Information-Theoretic-Distribution-Diversification" class="headerlink" title="Federated Generalization via Information-Theoretic Distribution Diversification"></a>Federated Generalization via Information-Theoretic Distribution Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07171">http://arxiv.org/abs/2310.07171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheshun Wu, Zenglin Xu, Dun Zeng, Qifan Wang</li>
<li>for: 这篇论文旨在解决联合学习中的非独立同分布（non-IID）挑战，协助提高联合学习模型的通用化能力。</li>
<li>methods: 本论文提出了一个信息理论基础的通用化框架，并引入了一种量子聚合方法和两种客户端选择策略，以增强联合学习的通用化能力。</li>
<li>results: 经过实验评估，本论文的提案方法能够增强联合学习模型的通用化能力，并且与理论建构之间存在良好的一致性。<details>
<summary>Abstract</summary>
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic generalization framework for FL. Specifically, it quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. Inspired by our deduced generalization bounds, we introduce a weighted aggregation approach and a duo of client selection strategies. These innovations aim to bolster FL's generalization prowess by encompassing a more varied set of client data distributions. Our extensive empirical evaluations reaffirm the potency of our proposed methods, aligning seamlessly with our theoretical construct.
</details>
<details>
<summary>摘要</summary>
受到协同学习（Federated Learning，FL）的推动，FL 在无需直接数据共享的情况下实现了模型训练的合作。然而，客户端数据分布的差异（non-Independent Identically Distributed，non-IID）问题对 FL 的泛化能力产生了重要的障碍。在一些客户端不参与训练过程的情况下，这种情况变得更加复杂，使得训练模型的泛化能力评估变得更加困难。Recent studies have focused on the generalization gap for unseen data from participating clients with diverse distributions, but the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked.为了应对这些挑战，我们的论文揭示了一种信息理论基础的泛化框架 для FL。具体来说，它衡量了本地分布的信息熵，并识别这些分布之间的差异。我们根据这些泛化误差的 bound 引入了一种权重聚合方法和两种客户端选择策略。这些创新目的是为了增强 FL 的泛化能力，使其包括更多的客户端数据分布。我们的广泛的实验证明了我们的提议的效果，与我们的理论结构契合。
</details></li>
</ul>
<hr>
<h2 id="LLark-A-Multimodal-Foundation-Model-for-Music"><a href="#LLark-A-Multimodal-Foundation-Model-for-Music" class="headerlink" title="LLark: A Multimodal Foundation Model for Music"></a>LLark: A Multimodal Foundation Model for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07160">http://arxiv.org/abs/2310.07160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner</li>
<li>for: 这篇论文是为了探讨音乐理解的问题，即使是专业人士和现有的AI系统都难以理解音乐的复杂结构。</li>
<li>methods: 作者提出了一种基于多Modal模型的LLark，该模型通过对多种开源音乐数据进行数据创建和一种统一的指令调整格式进行调整。</li>
<li>results: 作者在三种任务（音乐理解、描述和理解）中表现出色，其中与现有基线比较，在零shot泛化中对音乐理解任务表现卓越，而人类在描述和理解任务中与模型的回答有高度一致。<details>
<summary>Abstract</summary>
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .
</details>
<details>
<summary>摘要</summary>
音乐具有独特和复杂的结构，对于专家人类和现有的AI系统来说都是挑战，与其他形式的音频不同。我们介绍LLark，一种基于指令调整的多modal模型，用于音乐理解。我们详细介绍了我们的数据创建过程，包括对多种开源音乐数据集的扩充和转换为一致的指令调整格式。我们提议一种多modal的LLark模型，将音乐生成模型和语言模型集成。在三种任务（音乐理解、描述和理解）的评估中，我们显示了我们的模型在零shot泛化中与现有基eline匹配或超越，人类在描述和理解任务中与模型的回答 Display a high degree of agreement。LLark通过 entirely open-source music data和模型进行训练，我们在发表这篇论文时将我们的训练代码和数据公开发布。更多结果和音频示例可以通过https://bit.ly/llark查看，代码可以在https://github.com/spotify-research/llark上获取。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-from-Purified-Demonstration"><a href="#Imitation-Learning-from-Purified-Demonstration" class="headerlink" title="Imitation Learning from Purified Demonstration"></a>Imitation Learning from Purified Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07143">http://arxiv.org/abs/2310.07143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunke Wang, Minjing Dong, Bo Du, Chang Xu</li>
<li>for: addressing sequential decision-making problems with imperfect expert demonstrations</li>
<li>methods: purify potential perturbations in imperfect demonstrations via a two-step diffusion process, then conduct imitation learning from purified demonstrations</li>
<li>results: effective improvement in imitation learning performance, as demonstrated through theoretical evidence and evaluation results on MuJoCo.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在Addressing sequential decision-making problems with imperfect expert demonstrations</li>
<li>methods: 通过两步扩散过程纯化 potential perturbations in imperfect demonstrations,然后进行 imitation learning from purified demonstrations</li>
<li>results: 通过 teoretic evidence 和 MuJoCo 上的评估结果，证明了我们的方法可以有效地提高 imitation learning 性能。<details>
<summary>Abstract</summary>
Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is utilized to recover the optimal expert demonstrations from the diffused ones. We provide theoretical evidence supporting our approach, demonstrating that total variance distance between the purified and optimal demonstration distributions can be upper-bounded. The evaluation results on MuJoCo demonstrate the effectiveness of our method from different aspects.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>受欢迎的模仿学习方法在执行顺序决策问题上展示出了承诺，假设专家示范是最佳的。然而，在实际场景中，专家示范经常不完美，这会导致模仿学习的应用困难。现有的研究通常是通过优化不完美示范来解决这些问题，但是这通常需要一定的完美示范来保证性能。为了解决这些问题，我们提议纯化不完美示范中的潜在干扰，然后通过纯化后的示范进行模仿学习。针对成功的扩散模型，我们引入了两步纯化过程。在第一步，我们通过引入额外噪声来使用前向扩散过程，有效地平滑化不完美示范中的潜在干扰。接着，我们利用反生成过程来恢复优质专家示范从扩散过的示范中。我们提供了对我们方法的理论证明，证明了纯化后示范和优质示范之间的总差距的上界。我们的评估结果在MuJoCo上表明我们的方法在不同的方面具有效果。
</details></li>
</ul>
<hr>
<h2 id="Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models"><a href="#Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models" class="headerlink" title="Risk Assessment and Statistical Significance in the Age of Foundation Models"></a>Risk Assessment and Statistical Significance in the Age of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07132">http://arxiv.org/abs/2310.07132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</li>
<li>for: 评估基础模型中的社会技术风险，并提供量化统计 significativity 的分布式框架。</li>
<li>methods: 基于首选和第二选择风险 Statistics 的新统计测试，以及与经济数学金融中常用的均值-风险模型相关的第二颗统计学。</li>
<li>results: 通过这种框架，可以形式地开发一种基于指标集和风险权衡的模型选择策略，并通过bootstrap强度估计来证明统计 significativity。 使用这种框架，对不同的大语言模型进行风险评估，包括逸误和攻击性输出等。<details>
<summary>Abstract</summary>
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.
</details>
<details>
<summary>摘要</summary>
我们提出了一种分布式框架，用于评估基础模型中的社会技术风险，并使用量化的统计学 significado measure 评估这些风险。我们的方法基于新的第一和第二级杂度测试，这些测试可以测试实际随机变量之间的统计学上的占据关系。我们表明了这些第二级统计学与经济数学金融中常用的均值-风险模型之间的联系。使用这个框架，我们正式开发了一种具有风险意识的基础模型选择方法，给出了 guardrails 的量化指标。我们受到股票选择理论和股票估值理论的启发，定义了每个模型的 " metric 股票"，用于对多个指标进行汇总，并根据这些股票的杂度来选择模型。我们的统计学 significado measure 的有效性是基于中心假设的 asymptotic analysis 的实际应用，并通过 bootstrap variance estimate 来评估。我们使用我们的框架对不同的大语言模型进行评估，关于不遵循指令和输出恶意内容的风险。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs"><a href="#Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs" class="headerlink" title="Machine Learning Methods for Background Potential Estimation in 2DEGs"></a>Machine Learning Methods for Background Potential Estimation in 2DEGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07089">http://arxiv.org/abs/2310.07089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo da Cunha, Nobuyuki Aoki, David Ferry, Kevin Vora, Yu Zhang</li>
<li>for: 提高二dimensional电子液体（2DEG）中的瑕然和缺陷的识别和评估</li>
<li>methods: 使用扫描门微scopia（SGM）数据来估算2DEG背景电势，并应用三种不同的机器学习技术：生成对抗学习 neural network、细胞神经网络和进化搜索</li>
<li>results:  DES中的进化搜索算法能够在数据约束下提供有效的瑕然分析，这成果不仅推动了2DEG的理解，还探讨了机器学习在探测量子材料方面的潜在应用。<details>
<summary>Abstract</summary>
In the realm of quantum-effect devices and materials, two-dimensional electron gases (2DEGs) stand as fundamental structures that promise transformative technologies. However, the presence of impurities and defects in 2DEGs poses substantial challenges, impacting carrier mobility, conductivity, and quantum coherence time. To address this, we harness the power of scanning gate microscopy (SGM) and employ three distinct machine learning techniques to estimate the background potential of 2DEGs from SGM data: image-to-image translation using generative adversarial neural networks, cellular neural network, and evolutionary search. Our findings, despite data constraints, highlight the effectiveness of an evolutionary search algorithm in this context, offering a novel approach for defect analysis. This work not only advances our understanding of 2DEGs but also underscores the potential of machine learning in probing quantum materials, with implications for quantum computing and nanoelectronics.
</details>
<details>
<summary>摘要</summary>
在量子效应设备和材料的王国中，二维电子扩散（2DEG）作为基本结构，承诺引领技术的变革。然而，2DEG中的杂质和瑕疵对运载体的移动、导电性和量子准确时间产生了重大影响。为解决这一问题，我们利用扫描门镜学（SGM）的力量，并采用三种不同的机器学习技术来估计2DEG背景电气potential：图像到图像翻译使用生成对抗神经网络、细胞神经网络和进化搜索。我们的发现，尽管数据约束较为严格，但是进化搜索算法在这种情况下表现出了杰出的效果，提供了一种新的瑕疵分析方法。这项工作不仅提高了我们对2DEG的理解，也强调了机器学习在探索量子材料方面的潜在作用，对量子计算和纳米电子学产生了重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.LG_2023_10_11/" data-id="clnsn0viz00jfgf88eys8aonh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/11/cs.CL_2023_10_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-10-11
        
      </div>
    </a>
  
  
    <a href="/2023/10/11/eess.IV_2023_10_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-10-11</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">22</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
