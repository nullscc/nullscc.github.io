
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-10-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Dynamic Appearance Particle Neural Radiance Field paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07916 repo_url: None paper_authors: Ancheng Lin, Jun Li for: 模elling 3D 动态场景的 Dynamic NeRFs 扩展了这种模型，通常使用变形场景来捕捉时间">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-10-11">
<meta property="og:url" content="https://nullscc.github.io/2023/10/11/cs.CV_2023_10_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Dynamic Appearance Particle Neural Radiance Field paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07916 repo_url: None paper_authors: Ancheng Lin, Jun Li for: 模elling 3D 动态场景的 Dynamic NeRFs 扩展了这种模型，通常使用变形场景来捕捉时间">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-11T13:00:00.000Z">
<meta property="article:modified_time" content="2023-10-16T04:38:03.996Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.CV_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T13:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-10-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Appearance-Particle-Neural-Radiance-Field"><a href="#Dynamic-Appearance-Particle-Neural-Radiance-Field" class="headerlink" title="Dynamic Appearance Particle Neural Radiance Field"></a>Dynamic Appearance Particle Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07916">http://arxiv.org/abs/2310.07916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ancheng Lin, Jun Li</li>
<li>for: 模elling 3D 动态场景的 Dynamic NeRFs 扩展了这种模型，通常使用变形场景来捕捉时间变化的元素。</li>
<li>methods: DAP-NeRF 引入了粒子基的表示方法，模型了动态场景中视觉元素的运动。它由静态场景和动态场景的积和而组成，其中动态场景是由一系列的“visual particles”组成，每个粒子携带了小元素的视觉信息和运动模型。所有组件都是从单摄视频中学习而来，不需要任何先知的场景知识。</li>
<li>results: DAP-NeRF 能够有效地捕捉动态场景中的 appearances 和物理意义的运动。我们构建了一个新的评估数据集，并开发了高效的计算框架。实验结果表明，DAP-NeRF 是一种有效的技术来捕捉3D 动态场景中的视觉信息和物理运动。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of superposition of a static field and a dynamic field. The dynamic field is quantised as a collection of {\em appearance particles}, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and motion models of the particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modelling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 有大量的潜力来模型3D场景。动态NeRFs进一步发展了这种模型，通常使用变形场来捕捉时间变化的元素。现有的动态NeRFs使用类似的尤里安表示法来表示光辐Radiance和变形场。这会导致外观和运动之间的强烈相互关联，缺乏物理解释。在这项工作中，我们提出了动态外观粒子神经辐Radiance场（DAP-NeRF），它通过粒子基本表示来模拟动态3D场景中的视觉元素的运动。DAP-NeRF包括一个静止场和一个动态场的积addition。动态场被量化为一个集合的“外观粒子”，这些粒子携带了小动态场景中的视觉信息，并装备了运动模型。所有组件，包括静止场、视觉特征和运动模型，都是从单光视频中学习而来，不需要任何先前的场景知识。我们开发了一个高效的计算框架，并构建了一个新的数据集来评估运动模elling。实验结果表明，DAP-NeRF是一种有效的技术来捕捉3D动态场景中的外观和物理意义的运动。
</details></li>
</ul>
<hr>
<h2 id="NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration"><a href="#NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration" class="headerlink" title="NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration"></a>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07896">http://arxiv.org/abs/2310.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine<br>for:This paper aims to provide a unified diffusion policy for both task-oriented navigation and task-agnostic exploration in unfamiliar environments.methods:The proposed method uses a large-scale Transformer-based policy and a diffusion model decoder to handle both goal-conditioned and goal-agnostic navigation.results:The proposed method shows better overall performance in navigating to visually indicated goals in novel environments, with significant improvements in performance and lower collision rates compared to five alternative methods.<details>
<summary>Abstract</summary>
Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/
</details>
<details>
<summary>摘要</summary>
机器人学习 для未知环境导航需要提供任务域导航策略和任务无关探索策略。通常，这些角色是通过不同的模型来实现，例如使用互助提案、规划或分开的导航策略。在这篇论文中，我们描述了如何使用单一的扩散策略来处理目标域导航和任务无关探索，其中前者可以帮助机器人达到用户指定的目标，而后者可以在新环境中搜索目标。我们表明，这种单一策略会比使用生成模型的互助提案或先前的秘密变量模型方法更好地表现，并且在未知环境中导航到视觉指定的目标时，我们的实验结果表明，这种策略比其他五种方法更好。我们在一个真实世界的移动机器人 платформа上实现了我们的方法，并在多个机器人的数据上训练了一个大规模的Transformer基于策略。我们的实验结果显示，我们的方法可以在未seen环境中有效地导航，并且与其他五种方法相比，具有更高的性能和较低的碰撞率，即使使用较小的模型。更多视频、代码和预训练模型检查点，请参考<https://general-navigation-models.github.io/nomad/>。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder"><a href="#Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder" class="headerlink" title="Unsupervised Structured Noise Removal with Variational Lossy Autoencoder"></a>Unsupervised Structured Noise Removal with Variational Lossy Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07887">http://arxiv.org/abs/2310.07887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krulllab/DVLAE">https://github.com/krulllab/DVLAE</a></li>
<li>paper_authors: Benjamin Salmon, Alexander Krull</li>
<li>for: 这篇论文旨在开发一个不需要清洁图像或噪声模型的无监督深度学习基础设计于类型的噪声去除方法。</li>
<li>methods: 这篇论文使用了一种名为Variational Autoencoder（VAE）的深度学习模型，并将其拓展为一个具有自适应的推导器，可以模型类型的噪声 Component，但无法独立地模型清洁信号 Component。</li>
<li>results: 实验结果显示，这种方法可以比以往的自我监督和无监督图像噪声去除方法表现更好，并且具有较大的推导器适应范围。<details>
<summary>Abstract</summary>
Most unsupervised denoising methods are based on the assumption that imaging noise is either pixel-independent, i.e., spatially uncorrelated, or signal-independent, i.e., purely additive. However, in practice many imaging setups, especially in microscopy, suffer from a combination of signal-dependent noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe shaped scanning or readout artifacts). In this paper, we present the first unsupervised deep learning-based denoiser that can remove this type of noise without access to any clean images or a noise model. Unlike self-supervised techniques, our method does not rely on removing pixels by masking or subsampling so can utilize all available information. We implement a Variational Autoencoder (VAE) with a specially designed autoregressive decoder capable of modelling the noise component of an image but incapable of independently modelling the underlying clean signal component. As a consequence, our VAE's encoder learns to encode only underlying clean signal content and to discard imaging noise. We also propose an additional decoder for mapping the encoder's latent variables back into image space, thereby sampling denoised images. Experimental results demonstrate that our approach surpasses existing methods for self- and unsupervised image denoising while being robust with respect to the size of the autoregressive receptive field. Code for this project can be found at https://github.com/krulllab/DVLAE.
</details>
<details>
<summary>摘要</summary>
大多数无监督降噪方法假设图像噪声是像素独立的，即空间无相关性，或者信号独立的，即纯加性的。然而，在实际的摄影设置中，尤其是在 Mikroskop 中，往往会出现混合的信号依赖噪声（例如，Poisson 抽样噪声）和水平对齐的噪声（例如，扫描或者读取器artefacts）。在这篇论文中，我们提出了第一个无监督深度学习基于的降噪器，可以 removing 这种类型的噪声，不需要任何净图像或噪声模型。不同于自动学习技术，我们的方法不需要通过屏蔽或者抽取来移除像素，因此可以使用所有可用的信息。我们实现了一个Variational Autoencoder（VAE），其中的特别设计的推理树可以模型图像中的噪声组成部分，而不可以独立地模型图像中的净信号组成部分。因此，我们的VAE的编码器将只编码净信号内容，并且抛弃摄影噪声。我们还提出了一个用于将编码器的秘密变量映射回图像空间的额外decoder，从而生成降噪后的图像。实验结果表明，我们的方法超越了现有的自动和无监督图像降噪方法，并且对探测器感知场的大小具有Robust性。代码可以在 https://github.com/krulllab/DVLAE 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection"><a href="#A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection" class="headerlink" title="A Survey of Feature Types and Their Contributions for Camera Tampering Detection"></a>A Survey of Feature Types and Their Contributions for Camera Tampering Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07886">http://arxiv.org/abs/2310.07886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Mantini, Shishir K. Shah</li>
<li>for: 本研究旨在探讨视频监控摄像头中的干扰检测，以确定未经授权或不意图的 Alterations。</li>
<li>methods: 本研究使用时间序列分析方法进行干扰检测，并对不同特征类型进行评估。</li>
<li>results: 研究发现了不同特征类型的predictability和干扰检测能力，并对不同时间序列模型的选择进行了评估。<details>
<summary>Abstract</summary>
Camera tamper detection is the ability to detect unauthorized and unintentional alterations in surveillance cameras by analyzing the video. Camera tampering can occur due to natural events or it can be caused intentionally to disrupt surveillance. We cast tampering detection as a change detection problem, and perform a review of the existing literature with emphasis on feature types. We formulate tampering detection as a time series analysis problem, and design experiments to study the robustness and capability of various feature types. We compute ten features on real-world surveillance video and apply time series analysis to ascertain their predictability, and their capability to detect tampering. Finally, we quantify the performance of various time series models using each feature type to detect tampering.
</details>
<details>
<summary>摘要</summary>
Surveillance 相机损害检测是指通过视频分析来检测侦测器中的未授权或不计划的修改。相机修改可能由自然事件引起，也可能由意外的方式引起，以阻断侦测。我们将修改检测定义为变化检测问题，并对现有文献进行了审查，特别是关于特征类型。我们将修改检测定义为时间序列分析问题，并设计了实验来评估不同特征类型的稳定性和检测能力。我们在实际的侦测视频中计算了10个特征，并应用时间序列分析来确定它们的预测性和修改检测能力。最后，我们使用不同的时间序列模型来评估各种特征类型的修改检测性能。
</details></li>
</ul>
<hr>
<h2 id="BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI"><a href="#BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI" class="headerlink" title="BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI"></a>BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08608">http://arxiv.org/abs/2310.08608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Singh, Dr. Mrunal Bewoor, Ammar Ranapurwala, Satyam Rai, Sheetal Patil</li>
<li>for: 这个研究旨在使用深度学习框架将三维ultrasound图像转换为三维MRI图像。</li>
<li>methods: 该方法使用Pix2Pix GAN模型，输入3Dultrasound图像，通过UNET生成器和patch检测器，生成相应的3DMRI图像。</li>
<li>results: 实验结果表明，生成的MRI图像与预期结果具有一定的相似性。 despite some challenges, the method successfully generated MRI volume with a satisfactory similarity score, demonstrating the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion.<details>
<summary>Abstract</summary>
The study presents a deep learning framework aimed at synthesizing 3D MRI volumes from three-dimensional ultrasound images of the brain utilizing the Pix2Pix GAN model. The process involves inputting a 3D volume of ultrasounds into a UNET generator and patch discriminator, generating a corresponding 3D volume of MRI. Model performance was evaluated using losses on the discriminator and generator applied to a dataset of 3D ultrasound and MRI images. The results indicate that the synthesized MRI images exhibit some similarity to the expected outcomes. Despite challenges related to dataset size, computational resources, and technical complexities, the method successfully generated MRI volume with a satisfactory similarity score meant to serve as a baseline for further research. It underscores the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion, showcasing their viability for medical applications. Further refinement and exploration are warranted for enhanced clinical relevance.
</details>
<details>
<summary>摘要</summary>
这项研究提出了一种基于深度学习的3D MRI卷积图像合成框架，用于将三维超声图像转换为3D MRI卷积图像。该过程中，将3D超声图像输入到UNET生成器和质量检测器中，生成匹配的3D MRI卷积图像。模型性能通过对3D超声和MRI图像集进行损失评估来评估。结果表明，生成的MRI卷积图像具有一定的相似性。虽面临 dataset 大小、计算资源和技术复杂性等挑战，但方法仍然成功地生成了一个满足要求的MRI卷积图像，并且达到了一定的相似性分数。这表明了深度学习基于超声到MRI卷积图像转换的技术在医疗应用中的潜在可能性。进一步的优化和探索是需要的，以提高临床实用性。
</details></li>
</ul>
<hr>
<h2 id="CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping"><a href="#CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping" class="headerlink" title="CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping"></a>CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07855">http://arxiv.org/abs/2310.07855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars</li>
<li>for: 提高 dense visual representation learning 的精度和效果，特别是在 scene-centric 图像 dataset 上。</li>
<li>methods: 使用 object-level nearest neighbor bootstrapping 方法，通过在训练时对每个对象进行 nearest neighbor 选择，提高 visual representation 的精度和准确性。</li>
<li>results: CrIBo 在 in-context learning 任务上表现出色，与标准 downstream segmentation 任务具有高度竞争力。<details>
<summary>Abstract</summary>
Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream segmentation tasks. Our code and pretrained models will be publicly available upon acceptance.
</details>
<details>
<summary>摘要</summary>
利用最近邻居检索 для自主学习视觉表示学习已经在对象中心图像上得到了好的效果。然而，这种方法在场景中心数据集上遇到限制，因为图像中的多个对象只是间接地被捕捉在全局表示中。这可能导致对象表示的杂糅。此外，即使是对象中心数据集也可以从更细化的杂糅方法中受益。为了解决这些挑战，我们介绍了一种新的隐藏图像对象级Bootstrap方法（CrIBo），用于增强精细的视觉表示学习。在训练期间，CrIBo使用对象级最近邻居检索，以提高密集视觉表示学习。CrIBo在测试时使用最近邻居检索，并达到了状态机器人的性能，同时在标准下游分类任务中也具有高竞争力。我们将代码和预训练模型在接受后公开。
</details></li>
</ul>
<hr>
<h2 id="Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models"><a href="#Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models" class="headerlink" title="Explorable Mesh Deformation Subspaces from Unstructured Generative Models"></a>Explorable Mesh Deformation Subspaces from Unstructured Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07814">http://arxiv.org/abs/2310.07814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman Maesumi, Paul Guerrero, Vladimir G. Kim, Matthew Fisher, Siddhartha Chaudhuri, Noam Aigerman, Daniel Ritchie</li>
<li>for:  explore variations of 3D shapes</li>
<li>methods:  construct a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model</li>
<li>results:  produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories<details>
<summary>Abstract</summary>
Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth variations between them. We then show how to turn the variations in this subspace into deformation fields, to transfer those variations to high-quality meshes for the landmark shapes. Our results show that our method can produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories, especially as compared to prior work on learning deformation spaces for 3D shapes.
</details>
<details>
<summary>摘要</summary>
输入文本翻译成简化中文：传统3D模型创建工具中调查3D形状的变化是时间consuming的过程。深度生成模型的3D形状通常具有连续的秘密空间，可以从输入形状开始探索可能的变化。在实践中，这可能会是一些问题：秘密空间具有高维度和难以视见，包含不相关的形状，以及线性路径通常会导致形状转化不优化。此外，我们希望能够探索输入高质量网格用于训练生成模型的变化，而不是生成模型的输出几何体。在这篇论文中，我们提出了一种方法，通过从易于探索的2D探索空间到 pré-训练的生成模型的子空间中映射，来探索给定的标志形状中的变化。我们首先描述了如何找到映射，使得它覆盖输入标志形状并且在它们之间展现平滑的变化。然后，我们示出了如何将这些变化转化为几何场，以将这些变化应用到高质量网格上。我们的结果显示，我们的方法可以生成可见和易于探索的2D探索空间，特别是与先前学习3D形状几何场的方法相比。</SYS>Here's the translation in Simplified Chinese:传统3D模型创建工具中调查3D形状的变化是时间consuming的过程。深度生成模型的3D形状通常具有连续的秘密空间，可以从输入形状开始探索可能的变化。在实践中，这可能会是一些问题：秘密空间具有高维度和难以视见，包含不相关的形状，以及线性路径通常会导致形状转化不优化。此外，我们希望能够探索输入高质量网格用于训练生成模型的变化，而不是生成模型的输出几何体。在这篇论文中，我们提出了一种方法，通过从易于探索的2D探索空间到 pré-训练的生成模型的子空间中映射，来探索给定的标志形状中的变化。我们首先描述了如何找到映射，使得它覆盖输入标志形状并且在它们之间展现平滑的变化。然后，我们示出了如何将这些变化转化为几何场，以将这些变化应用到高质量网格上。我们的结果显示，我们的方法可以生成可见和易于探索的2D探索空间，特别是与先前学习3D形状几何场的方法相比。
</details></li>
</ul>
<hr>
<h2 id="CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving"><a href="#CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving" class="headerlink" title="CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving"></a>CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07794">http://arxiv.org/abs/2310.07794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli</li>
<li>for: 本文旨在提供一个新的评估自动驾驶路径预测模型的 benchmarking 方法，以提高现有的评估方法的精度和完整性。</li>
<li>methods: 本文提出了一种基于道路结构、模型性能和数据特性的驱动enario分类方法，以及一组新的偏好free metrics，用于衡量路径多样性和可接受性。</li>
<li>results: 经过对大规模的 Argoverse 数据集进行实验，本文示出了该benchmark可以更好地评估路径预测模型的性能，并提供了一种Characterizing模型行为的方法。此外，本文还进行了减少研究，以阐明不同元素对计算所提出的贡献。<details>
<summary>Abstract</summary>
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.   In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction models; 2) A set of new bias-free metrics for measuring diversity, by incorporating the characteristics of a given scenario, and admissibility, by considering the structure of roads and kinematic compliancy, motivated by real-world driving constraints. 3) Using the proposed benchmark, we conduct extensive experimentation on a representative set of the prediction models using the large scale Argoverse dataset. We show that the proposed benchmark can produce a more accurate ranking of the models and serve as a means of characterizing their behavior. We further present ablation studies to highlight contributions of different elements that are used to compute the proposed metrics.
</details>
<details>
<summary>摘要</summary>
benchmarking是自适应驾驶模型评估的常见方法。现有的benchmark都依赖于dataset，这些dataset偏向于更常见的enario，如驶行，并使用距离基于的metric来计算。这种办法只提供一些Scene的Properties，不能够准确地评估模型在不同的enario下的性能。 существуют一些补偿metric，用于测量trajectory的多样性和合法性，但它们受到length of trajectory的偏见。在这篇论文中，我们提出了一种新的benchmarking方法，用于评估trajectory prediction Approaches（CRITERIA）。具体来说，我们提出了以下方法：1. 提取驾驶enario的不同水平Specificity，根据道路结构、模型性能和数据特性进行细化的排名预测模型。2. 使用新的偏好度为零的多样性度量，通过包含场景特点来测量trajectory的多样性。3. 使用道路结构和动力学兼容性来测量trajectory的合法性，以驱动模型在实际驾驶中的行为。使用我们提出的benchmark，我们在Argoverse dataset上进行了广泛的实验，并证明了我们的benchmark可以更准确地排名模型，并且可以用来描述模型的行为。我们还进行了剥离研究，以阐明不同元素的贡献。
</details></li>
</ul>
<hr>
<h2 id="An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions"><a href="#An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions" class="headerlink" title="An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions"></a>An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07782">http://arxiv.org/abs/2310.07782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PurdueCAM2Project/focused-convolutions">https://github.com/PurdueCAM2Project/focused-convolutions</a></li>
<li>paper_authors: Caleb Tung, Nicholas Eliopoulos, Purvish Jajal, Gowri Ramshankar, Chen-Yun Yang, Nicholas Synovic, Xuecen Zhang, Vipin Chaudhary, George K. Thiruvathukal, Yung-Hsiang Lu</li>
<li>for: 提高卷积神经网络的能效性，不需要重新训练。</li>
<li>methods: 插入一个阈值层，对前一些层的活化进行过滤，以实现快速的搜寻和精炼操作，从而提高卷积神经网络的执行效率和能效性。</li>
<li>results: 对多种流行的预训练卷积神经网络进行修改，可以降低推理延迟（最多25%）和能耗成本（最多22%），而减少准确性的损失。<details>
<summary>Abstract</summary>
Computer vision often uses highly accurate Convolutional Neural Networks (CNNs), but these deep learning models are associated with ever-increasing energy and computation requirements. Producing more energy-efficient CNNs often requires model training which can be cost-prohibitive. We propose a novel, automated method to make a pretrained CNN more energy-efficient without re-training. Given a pretrained CNN, we insert a threshold layer that filters activations from the preceding layers to identify regions of the image that are irrelevant, i.e. can be ignored by the following layers while maintaining accuracy. Our modified focused convolution operation saves inference latency (by up to 25%) and energy costs (by up to 22%) on various popular pretrained CNNs, with little to no loss in accuracy.
</details>
<details>
<summary>摘要</summary>
计算机视觉经常使用高度精准的卷积神经网络（CNNs），但这些深度学习模型与计算和能源需求不断增长。制作更能效的CNNs通常需要模型训练，但这可能是成本禁止的。我们提出了一种新的自动化方法，可以使给定的预训练CNN更加能效，不需要重新训练。我们在预训练CNN中插入了一层阈值层，可以从前一些层的活动中过滤无关的图像区域，保持精度不变。我们的修改后的减少推理延迟（最多下降25%）和能耗成本（最多下降22%），在多种流行的预训练CNN上都有显著的效果，而且几乎没有损失精度。
</details></li>
</ul>
<hr>
<h2 id="3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers"><a href="#3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers" class="headerlink" title="3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers"></a>3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07781">http://arxiv.org/abs/2310.07781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Beckschen/3D-TransUNet">https://github.com/Beckschen/3D-TransUNet</a></li>
<li>paper_authors: Jieneng Chen, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou</li>
<li>for: 这篇论文的目的是将Transformer应用到医疗影像分类 tasks中，以提高分类效果。</li>
<li>methods: 这篇论文使用了Transformer的自我注意力机制，并与U-Net结合，以实现更好的长距离依赖关系modeling。</li>
<li>results: 实验结果显示，TransUNet在不同的医疗应用中具有优秀的表现，并且在多组织分类和肿瘤分类 task中表现更好。<details>
<summary>Abstract</summary>
Medical image segmentation plays a crucial role in advancing healthcare systems for disease diagnosis and treatment planning. The u-shaped architecture, popularly known as U-Net, has proven highly successful for various medical image segmentation tasks. However, U-Net's convolution-based operations inherently limit its ability to model long-range dependencies effectively. To address these limitations, researchers have turned to Transformers, renowned for their global self-attention mechanisms, as alternative architectures. One popular network is our previous TransUNet, which leverages Transformers' self-attention to complement U-Net's localized information with the global context. In this paper, we extend the 2D TransUNet architecture to a 3D network by building upon the state-of-the-art nnU-Net architecture, and fully exploring Transformers' potential in both the encoder and decoder design. We introduce two key components: 1) A Transformer encoder that tokenizes image patches from a convolution neural network (CNN) feature map, enabling the extraction of global contexts, and 2) A Transformer decoder that adaptively refines candidate regions by utilizing cross-attention between candidate proposals and U-Net features. Our investigations reveal that different medical tasks benefit from distinct architectural designs. The Transformer encoder excels in multi-organ segmentation, where the relationship among organs is crucial. On the other hand, the Transformer decoder proves more beneficial for dealing with small and challenging segmented targets such as tumor segmentation. Extensive experiments showcase the significant potential of integrating a Transformer-based encoder and decoder into the u-shaped medical image segmentation architecture. TransUNet outperforms competitors in various medical applications.
</details>
<details>
<summary>摘要</summary>
医疗图像分割在提高医疗系统的疾病诊断和治疗规划方面扮演着关键角色。U-Net建筑物，广泛地知道为各种医疗图像分割任务中的成功之路。然而，U-Net的卷积操作自然地限制了其能够有效地模型长距离依赖关系的能力。为了解决这些限制，研究人员转向了转换器，因为它们的全局自注意机制，成为了可能的替代体系。我们之前的TransUNet网络，利用转换器的全局自注意机制，补充U-Net的本地信息，并获得全球上下文。在这篇论文中，我们将2D TransUNet架构扩展到3D网络，基于当前领域的nnU-Net架构，并充分发挥转换器的潜力。我们引入了两个关键组件：1）使用转换器Encoder将图像块 Tokenize成CNN特征图，以EXTRACT全局上下文，2）使用转换器Decoderadaptively进行候选区域精细调整，通过候选提案和U-Net特征之间的交叉注意力。我们的调查发现，不同的医疗任务会有不同的建筑设计。转换器Encoder在多器官分割任务中表现出色，而转换器Decoder在小型和复杂的分割目标，如肿瘤分割任务中表现更加出色。我们的延伸实验表明，将转换器基于Encoder和Decoderintegretch到U-shaped医疗图像分割架构中，可以提高TransUNet在各种医疗应用中的表现。TransUNet在各种医疗应用中表现出色，超过了竞争对手。
</details></li>
</ul>
<hr>
<h2 id="OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation"><a href="#OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation" class="headerlink" title="OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"></a>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07749">http://arxiv.org/abs/2310.07749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo</li>
<li>for: 这个研究探讨了一个开放领域的图文生成任务，即根据输入查询生成杂合的图文内容。</li>
<li>methods: 我们提出了一个基于大型自然语言模型（LLM）和预训练的文本到图像（T2I）模型的新的杂合生成框架，称之为OpenLEAF。在OpenLEAF中，LLM生成文本描述，协调T2I模型，创建视觉提示 для生成图像，并将全局上下文 integrate到T2I模型中。</li>
<li>results: 根据我们构建的评估集，使用大型多Modal模型（LMM）评估实体和风格一致性，我们的提posed杂合生成框架可以生成高质量的图文内容，适用于多个领域和应用，如问答、故事、图文重新编写、幕布生成等等。此外，我们验证了我们提出的LMM评估技术的有效性通过人工评估。<details>
<summary>Abstract</summary>
This work investigates a challenging task named open-domain interleaved image-text generation, which generates interleaved texts and images following an input query. We propose a new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions, coordinates T2I models, creates visual prompts for generating images, and incorporates global contexts into the T2I models. This global context improves the entity and style consistencies of images in the interleaved generation. For model assessment, we first propose to use large multi-modal models (LMMs) to evaluate the entity and style consistencies of open-domain interleaved image-text sequences. According to the LMM evaluation on our constructed evaluation set, the proposed interleaved generation framework can generate high-quality image-text content for various domains and applications, such as how-to question answering, storytelling, graphical story rewriting, and webpage/poster generation tasks. Moreover, we validate the effectiveness of the proposed LMM evaluation technique with human assessment. We hope our proposed framework, benchmark, and LMM evaluation could help establish the intriguing interleaved image-text generation task.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了一个复杂的任务 named 开放领域交叠图文生成，该任务通过输入查询生成交叠的图文内容。我们提出了一个新的交叠生成框架，基于大型自然语言模型（LLM）和预训练的文本到图像（T2I）模型，称之为 OpenLEAF。在 OpenLEAF 中，LLM 生成文本描述、协调 T2I 模型、创建视觉提示生成图像，并将全局上下文 integrate 到 T2I 模型中。这个全局上下文提高了图像在交叠生成中的实体和风格一致性。为了评估模型效果，我们首先提出了使用大型多Modal模型（LMM）来评估交叠生成中的实体和风格一致性。根据 LMM 的评估，我们构建的评估集上，提案的交叠生成框架可以生成高质量的图文内容，适用于多个领域和应用，如如何问答、故事告诉、图文重新编写、网页/海报生成等任务。此外，我们还验证了我们提议的 LMM 评估技术的有效性，通过人工评估。我们希望我们的提议框架、标准和 LMM 评估能够为开放领域交叠图文生成任务做出贡献。
</details></li>
</ul>
<hr>
<h2 id="ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models"><a href="#ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models" class="headerlink" title="ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models"></a>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07702">http://arxiv.org/abs/2310.07702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingqinghe/scalecrafter">https://github.com/yingqinghe/scalecrafter</a></li>
<li>paper_authors: Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan</li>
<li>for: 本研究探索了使用预训练的扩散模型在更高的分辨率上生成图像，并且图像的比例可以自由调整。</li>
<li>methods: 我们提出了一种简单 yet effective的重定义，以及散布 convolution 和噪声抑制的自由导航，以实现ultra-high-resolution图像生成（例如4096 x 4096）。</li>
<li>results: 我们的方法可以减少对象重复问题，并且在高分辨率图像生成中达到状态艺术的性能，特别是在文本细节方面。我们的方法不需要任何训练或优化。<details>
<summary>Abstract</summary>
In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了使用预训练的扩散模型生成图像的能力，以高于训练图像分辨率为目标。此外，生成图像的形态应该是任意的。在直接使用1024x1024分辨率的扩散模型进行图像生成时，我们发现了固定的对象重复和不合理的对象结构问题。现有的高分辨率生成方法，如注意力基本的和共同扩散方法，无法好解这些问题。我们从新的视角出发，检查扩散模型中的结构组件，并确定了卷积核心的有限见距问题是关键原因。基于这一关键观察，我们提出了一种简单 yet有效的重定向法，可以在推理过程中动态调整卷积核心的见距。我们还提出了分散卷积和无噪抑制器-自由导向，这些方法可以实现ultra-高分辨率图像生成（例如4096x4096）。值得注意的是，我们的方法不需要任何训练或优化。广泛的实验表明，我们的方法可以很好地解决对象重复问题，并在高分辨率图像生成中达到领先性表现，特别是在文本细节方面。我们的工作还表明，一个预训练的扩散模型可以直接用于高分辨率视觉生成，无需进一步调整，这可能为未来的超高分辨率图像和视频生成做出了意见。
</details></li>
</ul>
<hr>
<h2 id="ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation"><a href="#ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation" class="headerlink" title="ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation"></a>ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07697">http://arxiv.org/abs/2310.07697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao</li>
<li>for: 本研究的目的是提出一种没有需要训练的文本到视频生成方法，通过利用现有的文本到图像生成方法（如稳定扩散）来生成基于提供的条件、视频和输入文本的真实动态视频。</li>
<li>methods: 本研究使用了一种名为 ConditionVideo 的方法，该方法利用了 off-the-shelf 文本到图像生成方法（如稳定扩散），并将动态运动表示分解为 condition-guided 和景象运动组成部分。为了提高时间准确性，我们引入了罕见的双向空间时间注意力（sBiST-Attn）。在 Temporal 领域中，我们还提出了一种三维控制网络（3D control network），以强化 conditional 生成的准确性。</li>
<li>results: 对于 frame consistency、clip score 和 conditional accuracy 等评价指标，ConditionVideo 方法表现出色，超越了其他比较的方法。<details>
<summary>Abstract</summary>
Recent works have successfully extended large-scale text-to-image models to the video domain, producing promising results but at a high computational cost and requiring a large amount of video data. In this work, we introduce ConditionVideo, a training-free approach to text-to-video generation based on the provided condition, video, and input text, by leveraging the power of off-the-shelf text-to-image generation methods (e.g., Stable Diffusion). ConditionVideo generates realistic dynamic videos from random noise or given scene videos. Our method explicitly disentangles the motion representation into condition-guided and scenery motion components. To this end, the ConditionVideo model is designed with a UNet branch and a control branch. To improve temporal coherence, we introduce sparse bi-directional spatial-temporal attention (sBiST-Attn). The 3D control network extends the conventional 2D controlnet model, aiming to strengthen conditional generation accuracy by additionally leveraging the bi-directional frames in the temporal domain. Our method exhibits superior performance in terms of frame consistency, clip score, and conditional accuracy, outperforming other compared methods.
</details>
<details>
<summary>摘要</summary>
最近的研究已经成功地扩展了大规模文本到图像模型到视频领域，但是 computational cost 高和需要大量的视频数据。在这种工作中，我们介绍 ConditionVideo，一种不需要训练的文本到视频生成方法，通过利用现有的文本到图像生成方法（例如 Stable Diffusion），从随机噪声或给定场景视频中生成真实的动态视频。我们的方法明确分解了运动表示为受条件导向和景观运动组成部分。为此，ConditionVideo 模型采用 UNet 支持和控制支持。为了改善时间准确性，我们引入稀疏双向空间时间注意力（sBiST-Attn）。3D 控制网络延伸了传统的 2D 控制网络模型，以更加强化 conditional 生成精度，通过同时利用时间域的双向帧。我们的方法在 Frame Consistency、Clip Score 和 conditional accuracy 等指标上表现出色，超越其他比较的方法。
</details></li>
</ul>
<hr>
<h2 id="Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole"><a href="#Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole" class="headerlink" title="Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole"></a>Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07687">http://arxiv.org/abs/2310.07687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviad Levis, Andrew A. Chael, Katherine L. Bouman, Maciek Wielgus, Pratul P. Srinivasan</li>
<li>for: 这个研究的目的是解释黑洞吸收过程中高能射线、红外和广播信号的产生机制。</li>
<li>methods: 这个研究使用了一种新的人工智能方法（ neural 3D representation）和一种 gravitational model for black holes 来解决高度不定的Tomography问题，并通过对数据进行physically motivated choices来确定结果的稳定性。</li>
<li>results: 研究发现了一个位于黑洞事 horizon 处的6倍距离的彩虹短脊，并发现这些短脊在顺时针方向进行旋转，这与之前的EHT和GRAVITY合作的研究结果一致。<details>
<summary>Abstract</summary>
The interaction between the supermassive black hole at the center of the Milky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces high energy flares seen in X-ray, infrared and radio. One mechanism for observed flares is the formation of compact bright regions that appear within the accretion disk and close to the event horizon. Understanding these flares can provide a window into black hole accretion processes. Although sophisticated simulations predict the formation of these flares, their structure has yet to be recovered by observations. Here we show the first three-dimensional (3D) reconstruction of an emission flare in orbit recovered from ALMA light curves observed on April 11, 2017. Our recovery results show compact bright regions at a distance of roughly 6 times the event horizon. Moreover, our recovery suggests a clockwise rotation in a low-inclination orbital plane, a result consistent with prior studies by EHT and GRAVITY collaborations. To recover this emission structure we solve a highly ill-posed tomography problem by integrating a neural 3D representation (an emergent artificial intelligence approach for 3D reconstruction) with a gravitational model for black holes. Although the recovered 3D structure is subject, and sometimes sensitive, to the model assumptions, under physically motivated choices we find that our results are stable and our approach is successful on simulated data. We anticipate that in the future, this approach could be used to analyze a richer collection of time-series data that could shed light on the mechanisms governing black hole and plasma dynamics.
</details>
<details>
<summary>摘要</summary>
黑洞中心的聚合物质黑洞，如ours sagittarius A*，与其吸收盘发生交互作用， occasional produce high energy flares visible in X-ray, infrared and radio。一种机制是在吸收盘中形成compact bright regions，靠近事件 horizons。理解这些闪光可以提供黑洞吸收过程的窗口。 although sophisticated simulations predict the formation of these flares, their structure has yet to be recovered by observations. here we show the first three-dimensional (3D) reconstruction of an emission flare in orbit recovered from ALMA light curves observed on April 11, 2017. our recovery results show compact bright regions at a distance of roughly 6 times the event horizon. Moreover, our recovery suggests a clockwise rotation in a low-inclination orbital plane, a result consistent with prior studies by EHT and GRAVITY collaborations. to recover this emission structure, we solve a highly ill-posed tomography problem by integrating a neural 3D representation (an emergent artificial intelligence approach for 3D reconstruction) with a gravitational model for black holes. although the recovered 3D structure is subject, and sometimes sensitive, to the model assumptions, under physically motivated choices we find that our results are stable and our approach is successful on simulated data. we anticipate that in the future, this approach could be used to analyze a richer collection of time-series data that could shed light on the mechanisms governing black hole and plasma dynamics.
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images"><a href="#Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images" class="headerlink" title="Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images"></a>Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07682">http://arxiv.org/abs/2310.07682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe</li>
<li>for:  This paper aims to develop a pre-screening algorithm to predict MET overexpression in non-small cell lung cancer (NSCLC) using routinely available digitized hematoxylin and eosin (H&amp;E)-stained slides.</li>
<li>methods: The authors leveraged a large database of matched H&amp;E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&amp;E images.</li>
<li>results: The model demonstrated an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set, suggesting that H&amp;E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这项研究目的是开发一种预选择算法，以便在非小细胞肺癌（NSCLC）患者中预测MET蛋白过表达，使用常见的数字化颜色染色（H&amp;E）染色片。</li>
<li>methods: 作者们利用了大量匹配的H&amp;E染色片和RNA表达数据库，以训练一种弱监督模型，直接从H&amp;E图像中预测MET蛋白过表达。</li>
<li>results: 模型在独立的检查集上达到了ROC-AUC值为0.70（95%Interval：0.66-0.74），性能特征稳定地适应不同的患者临床变量，并在测试集上具有鲁棒性， suggesting that H&amp;E-based predictive models可能有用于优先检测MET蛋白或MET基因表达状态的检查。<details>
<summary>Abstract</summary>
MET protein overexpression is a targetable event in non-small cell lung cancer (NSCLC) and is the subject of active drug development. Challenges in identifying patients for these therapies include lack of access to validated testing, such as standardized immunohistochemistry (IHC) assessment, and consumption of valuable tissue for a single gene/protein assay. Development of pre-screening algorithms using routinely available digitized hematoxylin and eosin (H&E)-stained slides to predict MET overexpression could promote testing for those who will benefit most. While assessment of MET expression using IHC is currently not routinely performed in NSCLC, next-generation sequencing is common and in some cases includes RNA expression panel testing. In this work, we leveraged a large database of matched H&E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&E images. This model was evaluated on an independent holdout test set of 300 over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set. These results suggest that H&E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.
</details>
<details>
<summary>摘要</summary>
MET蛋白过表达是非小细胞肺癌（NSCLC）中可Targetable的事件，现在有多种药物开发在进行。问题在于确定需要这些治疗的患者，包括无法获得有效验证的测试，如标准化的免疫染色（IHC）评估，以及用于单个蛋白质测试的资源浪费。为了解决这些问题，我们开发了一种使用 Routinely 可用的数字化HE染色（H&E）干ovat的预creening算法，以预测MET蛋白过表达。这种算法可以用来预测MET蛋白过表达，不需要直接测试IHC。我们使用了一个大量的匹配HE染色和RNA表达数据库来训练一种弱Supervised模型，以直接从H&E图像中预测MET蛋白过表达。这个模型在一个独立的卷积测试集上进行了评估，其ROC-AUC为0.70（95%信息Interval：0.66-0.74），性能特征稳定，不同患者临床变量下的性能也很稳定，并且对于synthetic noise的测试集表现强势。这些结果表明HE染色基于的预测模型可能可以有用地优先检测MET蛋白过表达或MET基因表达状况。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns"><a href="#Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns" class="headerlink" title="Accelerating Vision Transformers Based on Heterogeneous Attention Patterns"></a>Accelerating Vision Transformers Based on Heterogeneous Attention Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07664">http://arxiv.org/abs/2310.07664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deli Yu, Teng Xi, Jianwei Li, Baopu Li, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang</li>
<li>for: 本文旨在提高视觉变换器（ViT）的运行速度，以便在计算机视觉领域应用。</li>
<li>methods: 该文提出了一个整合压缩管道，包括动态指导静态自注意（DGSSA）和全球聚合峰（GLAD）两部分。DGSSA方法通过继承自减少动态自注意矩阵的信息来提高ViT的特征表示能力。GLAD方法通过全局聚合来减少 later layer的token数量，从而提高运行速度。</li>
<li>results: 实验结果表明，整合压缩管道可以提高ViT的运行速度，相比DeiT，它的运行速度可以提高121%。这也超过了所有State-of-the-art（SOTA）方法。<details>
<summary>Abstract</summary>
Recently, Vision Transformers (ViTs) have attracted a lot of attention in the field of computer vision. Generally, the powerful representative capacity of ViTs mainly benefits from the self-attention mechanism, which has a high computation complexity. To accelerate ViTs, we propose an integrated compression pipeline based on observed heterogeneous attention patterns across layers. On one hand, different images share more similar attention patterns in early layers than later layers, indicating that the dynamic query-by-key self-attention matrix may be replaced with a static self-attention matrix in early layers. Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs. On the other hand, the attention maps have more low-rank patterns, which reflect token redundancy, in later layers than early layers. In a view of linear dimension reduction, we further propose a method of global aggregation pyramid (GLAD) to reduce the number of tokens in later layers of ViTs, such as Deit. Experimentally, the integrated compression pipeline of DGSSA and GLAD can accelerate up to 121% run-time throughput compared with DeiT, which surpasses all SOTA approaches.
</details>
<details>
<summary>摘要</summary>
最近，计算机视觉领域内的视觉变换器（ViT）吸引了很多关注。通常情况下，ViT的强大代表性能归功于自注意机制，但自注意机制的计算复杂度较高。为了加速ViT，我们提议一种集成压缩管道，基于层次级别的观察到的不同类型的注意模式。在一个手中，不同的图像在早期层次比较更加相似的注意模式，这表明可以将动态Query-by-key自注意矩阵替换为静态自注意矩阵。然后，我们提议一种动态引导静态自注意（DGSSA）方法，其中矩阵继承自动置换的自注意信息，以提高ViT的特征表示能力。在另一个手中，注意图在后期层次比较多的低级别模式，这表明可以通过线性维度减少法（GLAD）减少ViT的后期层次的TOKEN数量，如Deit。实验结果表明，我们的集成压缩管道可以提高ViT的运行时间吞吐量达121%，比SOTA方法更高。
</details></li>
</ul>
<hr>
<h2 id="Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision"><a href="#Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision" class="headerlink" title="Deep Video Inpainting Guided by Audio-Visual Self-Supervision"></a>Deep Video Inpainting Guided by Audio-Visual Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07663">http://arxiv.org/abs/2310.07663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting">https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting</a></li>
<li>paper_authors: Kyuyeon Kim, Junsik Jung, Woo Jae Kim, Sung-Eui Yoon</li>
<li>for: 提高视频填充质量</li>
<li>methods: 使用深度学习模型模仿人类的听视感知，并通过两个新的损失函数传递听视相关知识到视频填充网络</li>
<li>results: 实验结果表明，我们的提议方法可以恢复更广泛的视频场景，并在听视对象部分遮盖的情况下特别有效<details>
<summary>Abstract</summary>
Humans can easily imagine a scene from auditory information based on their prior knowledge of audio-visual events. In this paper, we mimic this innate human ability in deep learning models to improve the quality of video inpainting. To implement the prior knowledge, we first train the audio-visual network, which learns the correspondence between auditory and visual information. Then, the audio-visual network is employed as a guider that conveys the prior knowledge of audio-visual correspondence to the video inpainting network. This prior knowledge is transferred through our proposed two novel losses: audio-visual attention loss and audio-visual pseudo-class consistency loss. These two losses further improve the performance of the video inpainting by encouraging the inpainting result to have a high correspondence to its synchronized audio. Experimental results demonstrate that our proposed method can restore a wider domain of video scenes and is particularly effective when the sounding object in the scene is partially blinded.
</details>
<details>
<summary>摘要</summary>
人类可以轻松地从听音信息中想象出场景，在这篇论文中，我们模拟了人类的这种 Innate 能力，用于改善视频填充的质量。首先，我们训练了一个听视网络，该网络学习听音和视觉信息之间的相关性。然后，我们使用这个听视网络作为引导器，将听视网络传递给视频填充网络，以便将优先知识传递给视频填充。我们提出了两个新的损失函数：听视注意力损失和听视假类一致损失。这两个损失函数可以进一步改善视频填充的性能，使得填充结果与同步的声音更加一致。实验结果表明，我们的提议方法可以恢复更广泛的视频场景，特别是在声音对象在场景中部分遮盲的情况下更加有效。
</details></li>
</ul>
<hr>
<h2 id="Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images"><a href="#Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images" class="headerlink" title="Context-Enhanced Detector For Building Detection From Remote Sensing Images"></a>Context-Enhanced Detector For Building Detection From Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07638">http://arxiv.org/abs/2310.07638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Huang, Mingming Zhang, Qingjie Liu, Wei Wang, Zhe Dong, Yunhong Wang</li>
<li>for: 本研究旨在提高遥感图像中建筑物检测精度，因为建筑物的多样性和场景复杂度导致检测困难。</li>
<li>methods: 我们提出了一种Context-Enhanced Detector（CEDet）方法，它使用三 stage cascade结构增强Contextual information的提取，以提高建筑物检测精度。特别是，我们引入了两个模块：Semantic Guided Contextual Mining（SGCM）模块和Instance Context Mining Module（ICMM）。SGCM模块通过多 scales的 Context aggregation和注意力机制来捕捉长距离相互作用，而ICMM模块通过构建空间关系图和实例特征归一化来捕捉实例水平关系。此外，我们引入了基于pseudo-mask的semantic segmentation loss来导引Contextual information提取。</li>
<li>results: 我们的方法在三个建筑物检测标准benchmark上达到了领先的性能：CNBuilding-9P、CNBuilding-23P和SpaceNet。<details>
<summary>Abstract</summary>
The field of building detection from remote sensing images has made significant progress, but faces challenges in achieving high-accuracy detection due to the diversity in building appearances and the complexity of vast scenes. To address these challenges, we propose a novel approach called Context-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade structure to enhance the extraction of contextual information and improve building detection accuracy. Specifically, we introduce two modules: the Semantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale contexts and incorporates an attention mechanism to capture long-range interactions, and the Instance Context Mining Module (ICMM), which captures instance-level relationship context by constructing a spatial relationship graph and aggregating instance features. Additionally, we introduce a semantic segmentation loss based on pseudo-masks to guide contextual information extraction. Our method achieves state-of-the-art performance on three building detection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.
</details>
<details>
<summary>摘要</summary>
场景检测从远程感知图像领域已经做出了重要进步，但仍面临高精度检测的挑战，主要是因为建筑物的多样性和场景的复杂性。为解决这些挑战，我们提出了一种新的方法 called Context-Enhanced Detector (CEDet)。我们的方法采用三个阶段卷积结构，以提高Contextual information的提取和建筑物检测精度。具体来说，我们引入了两个模块：卷积模块，用于聚合多尺度上下文，并使用注意机制来捕捉长距离相互作用；实例上下文挖掘模块，用于捕捉实例级别的关系上下文，通过构建空间关系图并聚合实例特征来完成。此外，我们还引入了基于pseudo-mask的semantic segmentation损失，以指导Contextual information的提取。我们的方法在三个建筑物检测标准测试 benchmark上达到了领先的性能水平，包括CNBuilding-9P、CNBuilding-23P和SpaceNet。
</details></li>
</ul>
<hr>
<h2 id="Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification"><a href="#Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification" class="headerlink" title="Attention-Map Augmentation for Hypercomplex Breast Cancer Classification"></a>Attention-Map Augmentation for Hypercomplex Breast Cancer Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07633">http://arxiv.org/abs/2310.07633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Filippo Betello, Federico Carmignani, Eleonora Grassucci, Danilo Comminiello</li>
<li>for: 这篇论文旨在提高乳腺癌早期诊断性能，使用深度学习技术。</li>
<li>methods: 本文提出一个框架，叫做参数化复杂注意地图（PHAM），以解决乳腺癌与良性肿瘤的区别问题。这个框架包括了一个增强步骤，使用注意地图来computing attention maps，然后将这些注意地图用于分类步骤，通过将原始乳腺癌图像和对应的注意地图融合为一个多维输入。在这个步骤中，使用参数化复杂神经网络（PHNN）进行乳腺癌分类。</li>
<li>results: 本文的方法比过往的注意力基于的网络和实值版本的方法表现更好，在乳腺癌和良性肿瘤的分类任务中均有着优秀的表现。<details>
<summary>Abstract</summary>
Breast cancer is the most widespread neoplasm among women and early detection of this disease is critical. Deep learning techniques have become of great interest to improve diagnostic performance. Nonetheless, discriminating between malignant and benign masses from whole mammograms remains challenging due to them being almost identical to an untrained eye and the region of interest (ROI) occupying a minuscule portion of the entire image. In this paper, we propose a framework, parameterized hypercomplex attention maps (PHAM), to overcome these problems. Specifically, we deploy an augmentation step based on computing attention maps. Then, the attention maps are used to condition the classification step by constructing a multi-dimensional input comprised of the original breast cancer image and the corresponding attention map. In this step, a parameterized hypercomplex neural network (PHNN) is employed to perform breast cancer classification. The framework offers two main advantages. First, attention maps provide critical information regarding the ROI and allow the neural model to concentrate on it. Second, the hypercomplex architecture has the ability to model local relations between input dimensions thanks to hypercomplex algebra rules, thus properly exploiting the information provided by the attention map. We demonstrate the efficacy of the proposed framework on both mammography images as well as histopathological ones, surpassing attention-based state-of-the-art networks and the real-valued counterpart of our method. The code of our work is available at https://github.com/elelo22/AttentionBCS.
</details>
<details>
<summary>摘要</summary>
乳癌是女性最常见的肿瘤，早期发现这种疾病非常重要。深度学习技术在提高诊断性能方面表现出了很大的兴趣。然而，从整个照片中分别识别癌变和正常组织仍然是一项极其困难的任务，因为它们在无经验的眼光下看起来几乎一样，而且诊断区域占整个照片的非常小。在这篇论文中，我们提出了一个框架，即参数化复杂注意地图（PHAM）。我们在这个框架中首先使用计算注意地图的步骤进行增强。然后，我们使用注意地图来 condition 分类步骤，通过构建一个多维输入，其包括原始乳癌图像和相应的注意地图。在这个步骤中，我们使用参数化复杂神经网络（PHNN）进行乳癌分类。这个框架具有两个主要优势。第一，注意地图提供了关键的诊断区域信息，使神经网络可以专注于它。第二，复杂架构可以通过复杂代数规则来模型输入维度之间的本地关系，因此能够正确地利用注意地图提供的信息。我们在照片和组织病学图像上进行了实验，超过了注意力基 estado del arte 网络和我们实际值对应的方法。我们的代码可以在 <https://github.com/elelo22/AttentionBCS> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Backdoors-in-Visual-Prompt-Learning"><a href="#Prompt-Backdoors-in-Visual-Prompt-Learning" class="headerlink" title="Prompt Backdoors in Visual Prompt Learning"></a>Prompt Backdoors in Visual Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07632">http://arxiv.org/abs/2310.07632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: 论文旨在探讨大规模预训练计算机视觉模型的精细调整是resource-limited用户不可能进行的。因此，视觉提示学习（VPL）已经成为一种有效和灵活的代替方案，通过Visual Prompt as a Service（VPPTaaS）提供者优化视觉提示，以便用户使用大规模预训练模型进行预测。然而，这种新的学习模式可能也会带来安全风险，当VPPTaaS提供者而不是提供正确的视觉提示时。本文通过探讨这种风险，提出了BadVisualPrompt，一种简单又有效的后门攻击。</li>
<li>methods: 我们提出了BadVisualPrompt，一种后门攻击，通过恶意修改CIFAR10训练数据来控制预测结果。我们发现，只需杀死5%的训练数据，可以达到99%的攻击成功率，同时只减少模型的准确率1.5%。此外，我们还发现了一种新的技术挑战，即后门触发器和视觉提示之间的交互，这种挑战不存在于传统的模型级别后门攻击中。</li>
<li>results: 我们发现，现有的七种后门防御机制都无法有效地防止BadVisualPrompt。这些防御机制包括模型级别、提示级别和输入级别的防御。在总的来说，这些防御机制都是无效的或实际上不可行。这表明，VPL受到了严重的安全攻击。<details>
<summary>Abstract</summary>
Fine-tuning large pre-trained computer vision models is infeasible for resource-limited users. Visual prompt learning (VPL) has thus emerged to provide an efficient and flexible alternative to model fine-tuning through Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider optimizes a visual prompt given downstream data, and downstream users can use this prompt together with the large pre-trained model for prediction. However, this new learning paradigm may also pose security risks when the VPPTaaS provider instead provides a malicious visual prompt. In this paper, we take the first step to explore such risks through the lens of backdoor attacks. Specifically, we propose BadVisualPrompt, a simple yet effective backdoor attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to above $99\%$ attack success rates with only negligible model accuracy drop by $1.5\%$. In particular, we identify and then address a new technical challenge related to interactions between the backdoor trigger and visual prompt, which does not exist in conventional, model-level backdoors. Moreover, we provide in-depth analyses of seven backdoor defenses from model, prompt, and input levels. Overall, all these defenses are either ineffective or impractical to mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
</details>
<details>
<summary>摘要</summary>
大型预训计算机视觉模型的细调是资源有限的用户无法进行。因此，视觉提示学习（VPL）已经出现了，作为一种高效和灵活的代替方案。具体来说，VPL提供者将可见提示给下游数据进行优化，然后下游用户可以使用这个提示和大型预训模型进行预测。然而，这个新的学习模式也可能存在安全风险，当VPL提供者而不是提供有利可图的可见提示。在这篇论文中，我们开始探讨这些风险，通过视觉提示的透传攻击来强调。具体来说，我们提出了BadVisualPrompt，一种简单 yet有效的可见提示攻击。例如，对CIFAR10训练数据进行毒素攻击，可以达到99%的攻击成功率，而且只带来模型准确率下降1.5%。在特定情况下，我们发现和解决了可见提示攻击和后门触发的新技术挑战，这不同于传统的模型级别后门攻击。此外，我们还对七种后门防御方法进行了深入分析，包括模型、提示和输入级别的防御方法。总之，这些防御方法都是无效或不实际的，这表明VPL具有极高的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autononous-Driving"><a href="#Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autononous-Driving" class="headerlink" title="Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous Driving"></a>Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07602">http://arxiv.org/abs/2310.07602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adept-thu/dual-radar">https://github.com/adept-thu/dual-radar</a></li>
<li>paper_authors: Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, Jun Li</li>
<li>for: 本研究旨在提供一个大规模多模式数据集，用于研究基于4D radar的自动驾驶环境感知。</li>
<li>methods: 本研究使用了两种不同的4D radar，并对其进行了大规模的同步采集和精心标注。</li>
<li>results: 研究发现，使用不同的4D radar可以提高自动驾驶系统的环境感知能力，但是不同的雷达类型在同一场景中的性能有很大差异。<details>
<summary>Abstract</summary>
Radar has stronger adaptability in adverse scenarios for autonomous driving environmental perception compared to widely adopted cameras and LiDARs. Compared with commonly used 3D radars, latest 4D radars have precise vertical resolution and higher point cloud density, making it a highly promising sensor for autonomous driving in complex environmental perception. However, due to the much higher noise than LiDAR, manufacturers choose different filtering strategies, resulting in an inverse ratio between noise level and point cloud density. There is still a lack of comparative analysis on which method is beneficial for deep learning-based perception algorithms in autonomous driving. One of the main reasons is that current datasets only adopt one type of 4D radar, making it difficult to compare different 4D radars in the same scene. Therefore, in this paper, we introduce a novel large-scale multi-modal dataset featuring, for the first time, two types of 4D radars captured simultaneously. This dataset enables further research into effective 4D radar perception algorithms.Our dataset consists of 151 consecutive series, most of which last 20 seconds and contain 10,007 meticulously synchronized and annotated frames. Moreover, our dataset captures a variety of challenging driving scenarios, including many road conditions, weather conditions, nighttime and daytime with different lighting intensities and periods. Our dataset annotates consecutive frames, which can be applied to 3D object detection and tracking, and also supports the study of multi-modal tasks. We experimentally validate our dataset, providing valuable results for studying different types of 4D radars. This dataset is released on https://github.com/adept-thu/Dual-Radar.
</details>
<details>
<summary>摘要</summary>
雷达在自动驾驶环境感知中具有更强的适应性，比广泛使用的相机和激光雷达更为有利。相比通常使用的3D雷达，最新的4D雷达具有高分辨率和更高的点云密度，使其成为自动驾驶复杂环境感知中非常有前途的感知器。然而，由于雷达噪声比激光雷达更高，制造商们采用不同的过滤策略，导致点云密度与噪声水平存在反比关系。到目前为止，没有对不同类型4D雷达的比较分析，对深度学习基于感知算法的影响进行了系统的研究。主要原因是目前的数据集只采用一种类型的4D雷达，使其不能在同一场景中比较不同的4D雷达。因此，在本文中，我们提出了一个新的大规模多模式数据集，该数据集首次同时采集了两种类型的4D雷达。这个数据集启用了进一步研究4D雷达感知算法。我们的数据集包括151个连续系列，大多数系列持续20秒钟，共包含10,007个精心同步和注释的帧。此外，我们的数据集捕捉了各种挑战性的驾驶场景，包括多种路面条件、天气条件、夜晚和白天的不同照明强度和时间段。我们的数据集连续注释帧，可以应用于3D物体检测和跟踪，也支持多模式任务的研究。我们实验 validate了我们的数据集，提供了价值的研究4D雷达的结果。这个数据集在https://github.com/adept-thu/Dual-Radar上发布。
</details></li>
</ul>
<hr>
<h2 id="PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks"><a href="#PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks" class="headerlink" title="PeP: a Point enhanced Painting method for unified point cloud tasks"></a>PeP: a Point enhanced Painting method for unified point cloud tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07591">http://arxiv.org/abs/2310.07591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Dong, Hang Ji, Xufeng Huang, Weikun Zhang, Xin Zhan, Junbo Chen</li>
<li>for: The paper is written for improving the performance of point cloud recognition by proposing a novel feature encoding module called PeP.</li>
<li>methods: The paper uses a refined point painting method and a language model (LM)-based point encoder to enhance the feature encoding mechanism.</li>
<li>results: The proposed PeP module achieves superior performance on both semantic segmentation and object detection tasks in both lidar and multi-modal settings, with advantages over existing methods.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本文是为提高点云识别性能，提出了一种新的特征编码模块 called PeP。</li>
<li>methods: 本文使用了改进的点云涂抹方法和语言模型（LM）基于的点云编码器，以增强特征编码机制。</li>
<li>results: 提出的 PeP 模块在点云和多modal设置下的 semantic segmentation 和物体检测任务中具有优异性能，与现有方法相比有所提升。<details>
<summary>Abstract</summary>
Point encoder is of vital importance for point cloud recognition. As the very beginning step of whole model pipeline, adding features from diverse sources and providing stronger feature encoding mechanism would provide better input for downstream modules. In our work, we proposed a novel PeP module to tackle above issue. PeP contains two main parts, a refined point painting method and a LM-based point encoder. Experiments results on the nuScenes and KITTI datasets validate the superior performance of our PeP. The advantages leads to strong performance on both semantic segmentation and object detection, in both lidar and multi-modal settings. Notably, our PeP module is model agnostic and plug-and-play. Our code will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>点编码器对点云识别是非常重要的。作为整个模型管道的起始步骤，从多种来源添加特征并提供更强的特征编码机制，可以提供更好的输入 dla下游模块。在我们的工作中，我们提出了一种新的PeP模块来解决上述问题。PeP包括两个主要部分：一种精度加工的点涂抹方法和一种LM基于的点编码器。在nuScenes和KITTI数据集上进行实验，我们证明了我们的PeP模块在 semantic segmentation和物体检测方面具有优秀的表现，并且在 lidar 和多Modal 环境中也有出色的表现。值得注意的是，我们的PeP模块是模型不依赖的和插件化的。我们的代码将很快地公开。
</details></li>
</ul>
<hr>
<h2 id="A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection"><a href="#A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection" class="headerlink" title="A Discrepancy Aware Framework for Robust Anomaly Detection"></a>A Discrepancy Aware Framework for Robust Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07585">http://arxiv.org/abs/2310.07585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caiyuxuan1120/daf">https://github.com/caiyuxuan1120/daf</a></li>
<li>paper_authors: Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, Xiang Bai<br>for:This paper focuses on the robustness of self-supervised learning models for defect detection in artificial intelligence.methods:The paper presents a Discrepancy Aware Framework (DAF) that leverages an appearance-agnostic cue to guide the decoder in identifying defects, alleviating the reliance on synthetic appearance. The method employs a teacher-student network trained based on synthesized outliers to compute the discrepancy map.results:The paper shows that the proposed method outperforms existing methods by a large margin under simple synthesis strategies, and achieves state-of-the-art localization performance. Extensive experiments on two challenging datasets prove the robustness of the method.<details>
<summary>Abstract</summary>
Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this paper, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a Discrepancy Aware Framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. Code is available at: https://github.com/caiyuxuan1120/DAF.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>检测缺陷是人工智能研究领域中的一个关键问题。最近，基于 sintetic 数据的自我指导学习已经展示了很大的潜力在这项任务上。虽然有很多复杂的生成策略存在，但是对于不同的策略的研究还很少。在这篇论文中，我们将关注这个问题，并发现现有方法具有高度敏感性。为了解决这个问题，我们提出了一种缺陷意识框架（DAF），它在不同的缺陷检测benchmark上显示了稳定的表现，并且可以采用简单和便宜的策略。我们认为现有的自我指导方法在使用 sintetic 数据时存在高度的敏感性，因为它们在解码时强调 sintetic 数据的视觉特征。与此相反，我们的方法利用一种视觉无关的cue来导引解码器，以避免对 sintetic 数据的依赖。为此，我们采用了一种教师生成器，该生成器基于生成的异常数据来计算缺陷地图。我们在两个复杂的dataset上进行了广泛的实验，并证明了我们的方法的稳定性和高效性。在简单的生成策略下，它超过了现有方法，并且也实现了当前的最佳地方化性能。代码可以在 GitHub 上找到：https://github.com/caiyuxuan1120/DAF。
</details></li>
</ul>
<hr>
<h2 id="Centrality-of-the-Fingerprint-Core-Location"><a href="#Centrality-of-the-Fingerprint-Core-Location" class="headerlink" title="Centrality of the Fingerprint Core Location"></a>Centrality of the Fingerprint Core Location</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07584">http://arxiv.org/abs/2310.07584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurenz Ruzicka, Bernhard Strobl, Bernhard Kohn, Clemens Heitzinger</li>
<li>for: 这个研究旨在investigating the empirical distribution of the fingerprint core over a large, combined dataset of rolled and plain fingerprint recordings.</li>
<li>methods: 该研究使用了一种多步骤的方法来描述rolled fingerprint recordings的分布，包括Anderson-Darling正态性测试、Bayesian Information Criterion和Generalized Monte Carlogoodness-of-fit过程。</li>
<li>results: 研究发现，rolled fingerprint recordings的core deviates from the fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the finger. Additionally, the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Finally, the non-central Fischer distribution best describes the cores’ horizontal positions.<details>
<summary>Abstract</summary>
Fingerprints have long been recognized as a unique and reliable means of personal identification. Central to the analysis and enhancement of fingerprints is the concept of the fingerprint core. Although the location of the core is used in many applications, to the best of our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled, as well as plain fingerprint recordings. We identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. After correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the finger. Additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. The process consists of an Anderson-Darling normality test, the Bayesian Information Criterion to reduce the number of possible candidate distributions and finally a Generalized Monte Carlo goodness-of-fit procedure to find the best fitting distribution. We find the non-central Fischer distribution best describes the cores' horizontal positions. Finally, we investigate the correlation between mean core position offset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details>
<details>
<summary>摘要</summary>
fingerprints 已经被认为是个人身份识别的唯一和可靠方法。 fingerprint 核心是分析和提高 fingerprint 的中心概念。 although the location of the core is used in many applications, to the best of our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled, as well as plain fingerprint recordings. we identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. after correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% ± 5.2% to 7.6% ± 6.9%, depending on the finger. additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. the process consists of an anderson-darling normality test, the bayesian information criterion to reduce the number of possible candidate distributions and finally a generalized monte carlo goodness-of-fit procedure to find the best fitting distribution. we find the non-central fischer distribution best describes the cores' horizontal positions. finally, we investigate the correlation between mean core position offset and the nfiq 2 score and find that the nfiq 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details></li>
</ul>
<hr>
<h2 id="Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation"><a href="#Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation" class="headerlink" title="Relational Prior Knowledge Graphs for Detection and Instance Segmentation"></a>Relational Prior Knowledge Graphs for Detection and Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07573">http://arxiv.org/abs/2310.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald</li>
<li>for: 本研究探讨了使用物体之间关系来进行物体检测和实例分割的可效性。</li>
<li>methods: 提议一种基于关系优先的特征增强模型（RP-FEM），利用场景图获取初始提案特征，并同时学习关系上下文模型以提高物体检测和实例分割性能。</li>
<li>results: 实验结果表明，在COCO dataset上，通过使用场景图和关系优先来增强对象提案特征，可以提高物体检测和实例分割性能，并且可以降低不可能的类别预测和重复预测，与基础模型相比具有改进。<details>
<summary>Abstract</summary>
Humans have a remarkable ability to perceive and reason about the world around them by understanding the relationships between objects. In this paper, we investigate the effectiveness of using such relationships for object detection and instance segmentation. To this end, we propose a Relational Prior-based Feature Enhancement Model (RP-FEM), a graph transformer that enhances object proposal features using relational priors. The proposed architecture operates on top of scene graphs obtained from initial proposals and aims to concurrently learn relational context modeling for object detection and instance segmentation. Experimental evaluations on COCO show that the utilization of scene graphs, augmented with relational priors, offer benefits for object detection and instance segmentation. RP-FEM demonstrates its capacity to suppress improbable class predictions within the image while also preventing the model from generating duplicate predictions, leading to improvements over the baseline model on which it is built.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的能力，能够理解和掌握周围环境中的物体关系。在这篇论文中，我们研究了使用这些关系来进行物体检测和实例分割。为此，我们提出了一种基于关系优先的特征增强模型（RP-FEM），这是一种图变换器，它在从初始提案中获得的场景图上进行增强物体提案特征。我们的建议的架构同时学习了场景图中的关系上下文模型，以便同时进行物体检测和实例分割。在COCO数据集上进行实验评估表明，通过使用场景图和关系优先，可以提高物体检测和实例分割的性能。RP-FEM可以降低图像中不可能的类别预测，同时避免模型生成重复预测，从而超越基础模型。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery"><a href="#Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery" class="headerlink" title="Impact of Label Types on Training SWIN Models with Overhead Imagery"></a>Impact of Label Types on Training SWIN Models with Overhead Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07572">http://arxiv.org/abs/2310.07572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Ford, Kenneth Hutchison, Nicholas Felts, Benjamin Cheng, Jesse Lew, Kyle Jackson</li>
<li>for: 这个研究探讨了对于模型训练和性能的数据集设计影响，以减少调取遥测和顶部标签数据的成本。</li>
<li>methods: 这个研究使用了对焦盒和分类标签进行训练shifted window transformers，其中分类标签更加昂贵。</li>
<li>results: 研究发现，将模型训练只使用目标像素（通过分类标签提取）不会提高分类任务的性能，似乎是把背景像素 mistakenly included in the evaluation set with target pixels。对object detection模型进行训练，使用任一标签类型都会得到相等的性能。 bounding boxes 足够 для不需要更多复杂的标签的任务。<details>
<summary>Abstract</summary>
Understanding the impact of data set design on model training and performance can help alleviate the costs associated with generating remote sensing and overhead labeled data. This work examined the impact of training shifted window transformers using bounding boxes and segmentation labels, where the latter are more expensive to produce. We examined classification tasks by comparing models trained with both target and backgrounds against models trained with only target pixels, extracted by segmentation labels. For object detection models, we compared performance using either label type when training. We found that the models trained on only target pixels do not show performance improvement for classification tasks, appearing to conflate background pixels in the evaluation set with target pixels. For object detection, we found that models trained with either label type showed equivalent performance across testing. We found that bounding boxes appeared to be sufficient for tasks that did not require more complex labels, such as object segmentation. Continuing work to determine consistency of this result across data types and model architectures could potentially result in substantial savings in generating remote sensing data sets for deep learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape"><a href="#Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape" class="headerlink" title="Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape"></a>Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07555">http://arxiv.org/abs/2310.07555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wen, Tianqin Li, Tai Sing Lee</li>
<li>for: 本研究旨在评估深度学习模型对形状的敏感性，并提出一种新的测试工具箱（Distorted Shape Testbench，DiST）来评估模型对全局形状的敏感性。</li>
<li>methods: 本研究使用了一组包含2400张ImageNet-1K原始图像的图像集，每张图像都有两个拟合原始图像的全局形状的图像，用于评估模型对全局形状的敏感性。</li>
<li>results: 研究发现，传统的形状偏好评估方法不能准确反映模型的表现，而新的DiST测试工具箱可以准确评估模型对全局形状的敏感性，并且训练使用DiST图像可以bridge人类和现有SOTA模型之间的性能差距，同时保持模型在标准图像分类任务中的准确率。<details>
<summary>Abstract</summary>
Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape for object recognition. The current benchmark for evaluating a model's shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of shape sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local shapes. We provide a Distorted Shape Testbench (DiST) as an alternative measurement of global shape sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image distorted while preserving its texture via the texture synthesis program. We found that (1) models that performed well on the previous shape bias evaluation do not fare well in the proposed DiST; (2) the widely adopted ViT models do not show significant advantages over Convolutional Neural Networks (CNNs) on this benchmark despite that ViTs rank higher on the previous shape bias tests. (3) training with DiST images bridges the significant gap between human and existing SOTA models' performance while preserving the models' accuracy on standard image classification tasks; training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance both the global and local shape sensitivity of the network. Our code will be host at: https://github.com/leelabcnbc/DiST
</details>
<details>
<summary>摘要</summary>
深度学习模型通常会表现出强烈的文化偏见，而人类则更加重视全局形态的认知。现有的标准测试方法是使用style transfer技术来评估模型的形态偏见，假设模型对style transfer攻击的抵抗度与其全局形态敏感度之间存在相关性。在这项工作中，我们发现了一点：使用style transfer图像进行训练后，模型会忽略样式，但是其形态偏见主要来自本地形态。为了衡量全局形态敏感度，我们提出了一种Distorted Shape Testbench（DiST）。我们的测试包括2400个ImageNet-1K原始图像，每个图像都有两个global shape的扭曲图像，通过Texture Synthesis程序保持图像的文化。我们发现以下结论：1. 在我们提出的DiST测试中，高于前一个形态偏见测试的模型表现不佳。2. 广泛采用的ViT模型与传统Convolutional Neural Networks（CNNs）在这一benchmark上并没有显著的优势，尽管ViTs在前一个形态偏见测试中排名更高。3. 使用DiST图像进行训练可以bridges模型与人类的表现差距，同时保持模型在标准图像分类任务上的准确率。使用DiST图像和style transfer图像进行训练是补偿的，可以同时增强模型的全局形态敏感度和本地形态敏感度。我们的代码将被托管在GitHub上：https://github.com/leelabcnbc/DiST。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning"><a href="#Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning" class="headerlink" title="Attribute Localization and Revision Network for Zero-Shot Learning"></a>Attribute Localization and Revision Network for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07548">http://arxiv.org/abs/2310.07548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhe Xu, Suling Duan, Chenwei Tang, Zhenan He, Jiancheng Lv</li>
<li>for: 本研究旨在提出一种能够在无需训练数据的情况下进行预测的模型，该模型可以利用auxiliary semantic信息来识别未经训练的类别。</li>
<li>methods: 本研究使用了Attribute Localization Module (ALM)和Attribute Revision Module (ARM)两种模块来解决当前的zero-shot learning问题。ALM模块可以 capture both local and global features from image regions，而ARM模块可以 revise the ground-truth value of each attribute to compensate for performance degradation caused by ignoring intra-class variation。</li>
<li>results: 根据三个广泛使用的benchmark测试，本研究的模型在zero-shot prediction任务中表现出色，可以准确地预测未经训练的类别。<details>
<summary>Abstract</summary>
Zero-shot learning enables the model to recognize unseen categories with the aid of auxiliary semantic information such as attributes. Current works proposed to detect attributes from local image regions and align extracted features with class-level semantics. In this paper, we find that the choice between local and global features is not a zero-sum game, global features can also contribute to the understanding of attributes. In addition, aligning attribute features with class-level semantics ignores potential intra-class attribute variation. To mitigate these disadvantages, we present Attribute Localization and Revision Network in this paper. First, we design Attribute Localization Module (ALM) to capture both local and global features from image regions, a novel module called Scale Control Unit is incorporated to fuse global and local representations. Second, we propose Attribute Revision Module (ARM), which generates image-level semantics by revising the ground-truth value of each attribute, compensating for performance degradation caused by ignoring intra-class variation. Finally, the output of ALM will be aligned with revised semantics produced by ARM to achieve the training process. Comprehensive experimental results on three widely used benchmarks demonstrate the effectiveness of our model in the zero-shot prediction task.
</details>
<details>
<summary>摘要</summary>
zero-shot learning 允许模型识别未经见过的类别，通过auxiliary semantic information如特征。现有工作提议从本地图像区域检测特征并将提取的特征与类别水平 semantics 对应。在这篇论文中，我们发现选择本地和全局特征不是一个零和游戏，全局特征也可以帮助理解特征。此外，对于类别水平 semantics 的对应忽略了可能存在的内类特征变化。为了缓解这些缺点，我们在本文中提出了特征地方化和修订网络（Attribute Localization and Revision Network，简称ALRN）。首先，我们设计了特征地方化模块（Attribute Localization Module，简称ALM），用于从图像区域中捕捉本地和全局特征。其次，我们提出了特征修订模块（Attribute Revision Module，简称ARM），通过修订真实值来补偿因为忽略内类特征变化而导致的性能下降。最后，ALM 的输出与 ARM 生成的修订 semantics 进行对齐，以实现训练过程。我们在三个常用的 benchmark 上进行了广泛的实验，结果表明我们的模型在零shot 预测任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields"><a href="#S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields" class="headerlink" title="S4C: Self-Supervised Semantic Scene Completion with Neural Fields"></a>S4C: Self-Supervised Semantic Scene Completion with Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07522">http://arxiv.org/abs/2310.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers</li>
<li>for: 本研究旨在解决计算机视觉中的3DsemanticScene理解挑战，即从稀疏观察数据中jointly estimating dense geometry和semantic信息，以便自主规划和导航任意环境。</li>
<li>methods: 我们的方法基于自我监督学习，不需要3D真实数据，可以从单张图像和视频数据中学习Scene Semantic Consistency（SSC）。我们使用pseudo segmentation truth生成于市面上的图像分割网络，并使用rendering-based自我监督损失来训练我们的模型。</li>
<li>results: 我们的方法可以在不同的视角下Synthesize高精度的分割图像，并且在不同的环境下进行强大的泛化。与现有方法相比，我们的方法可以减少 annotated 数据的需求，并且可以在不同的环境下进行更好的一致性。<details>
<summary>Abstract</summary>
3D semantic scene understanding is a fundamental challenge in computer vision. It enables mobile agents to autonomously plan and navigate arbitrary environments. SSC formalizes this challenge as jointly estimating dense geometry and semantic information from sparse observations of a scene. Current methods for SSC are generally trained on 3D ground truth based on aggregated LiDAR scans. This process relies on special sensors and annotation by hand which are costly and do not scale well. To overcome this issue, our work presents the first self-supervised approach to SSC called S4C that does not rely on 3D ground truth data. Our proposed method can reconstruct a scene from a single image and only relies on videos and pseudo segmentation ground truth generated from off-the-shelf image segmentation network during training. Unlike existing methods, which use discrete voxel grids, we represent scenes as implicit semantic fields. This formulation allows querying any point within the camera frustum for occupancy and semantic class. Our architecture is trained through rendering-based self-supervised losses. Nonetheless, our method achieves performance close to fully supervised state-of-the-art methods. Additionally, our method demonstrates strong generalization capabilities and can synthesize accurate segmentation maps for far away viewpoints.
</details>
<details>
<summary>摘要</summary>
三维semantic场景理解是计算机视觉的基本挑战。它使移动代理能够自主规划和探索不确定环境。SSC formalizes this challenge as同时估算环境的厚度和Semantic信息从笔记scans中的稀疏观察数据。现有的方法通常通过3D实际数据进行训练，这个过程依赖特殊的感器和手动标注，这些成本高并不扩展好。为了解决这个问题，我们提出了首个不需要3D实际数据的自动学习方法called S4C。我们的提议的方法可以从单个图像中重建场景，只需要视频和 pseudo segmentation标注来进行训练。与现有方法不同，我们使用 implicit semantic fields来表示场景。这种表示方式允许在摄像头封闭中任意点进行存储和Semantic类别的查询。我们的架构通过渲染基于自我超级vised损失进行训练。尽管如此，我们的方法可以与完全超级vised方法的性能相似。此外，我们的方法还能够强大地泛化，可以生成正确的分割图像 для远距离视角。
</details></li>
</ul>
<hr>
<h2 id="CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing"><a href="#CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing" class="headerlink" title="CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing"></a>CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07517">http://arxiv.org/abs/2310.07517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaru Chen, Ruohao Guo, Xubo Liu, Peipei Wu, Guangyao Li, Zhenbo Li, Wenwu Wang</li>
<li>for: 这paper是为了提高视频分割 tasks的性能，特别是在使用弱标签时。</li>
<li>methods: 该paper提出了一种名为CM-PIE的新方法，它利用段级注意力模块来学习细化特征，同时通过交叉模态协同归一化块来强化间modal交互。</li>
<li>results: 实验结果显示，CM-PIE方法在Look, Listen, and Parse数据集上的分割性能比其他方法更高。<details>
<summary>Abstract</summary>
Audio-visual video parsing is the task of categorizing a video at the segment level with weak labels, and predicting them as audible or visible events. Recent methods for this task leverage the attention mechanism to capture the semantic correlations among the whole video across the audio-visual modalities. However, these approaches have overlooked the importance of individual segments within a video and the relationship among them, and tend to rely on a single modality when learning features. In this paper, we propose a novel interactive-enhanced cross-modal perception method~(CM-PIE), which can learn fine-grained features by applying a segment-based attention module. Furthermore, a cross-modal aggregation block is introduced to jointly optimize the semantic representation of audio and visual signals by enhancing inter-modal interactions. The experimental results show that our model offers improved parsing performance on the Look, Listen, and Parse dataset compared to other methods.
</details>
<details>
<summary>摘要</summary>
audio-visual视频分解任务是将视频分割成不同类别的segmentlevel，并使用弱标签预测它们是 audible 还是 visible 事件。现有的方法对此任务借鉴了注意机制，以捕捉全视频的各个模式之间的含义相关性。然而，这些方法往往忽略了视频中每个段落的重要性和相互关系，而且往往仅仅在学习特征时依赖单一的感知模式。在本文中，我们提出了一种新的互动增强交叉模态识别方法（CM-PIE），它可以通过应用段基 attention模块来学习细腻的特征。此外，我们还引入了交叉模态汇聚块，以联合优化音频和视频信号的含义表示。实验结果表明，我们的模型在Look, Listen, and Parse数据集上的分解性能比其他方法更高。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning"><a href="#A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning" class="headerlink" title="A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning"></a>A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07511">http://arxiv.org/abs/2310.07511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingtao-li-cver/uniadrs">https://github.com/jingtao-li-cver/uniadrs</a></li>
<li>paper_authors: Jingtao Li, Xinyu Wang, Hengwei Zhao, Liangpei Zhang, Yanfei Zhong</li>
<li>for: 检测地球表面上的异常现象，包括多种模式和场景下的异常。</li>
<li>methods: 基于异常偏离本地Context的特征，提出一种通用的异常检测器，可以在多种感知器和场景下检测异常。</li>
<li>results: 采用 conditional probability 模型，在五种模式（包括 Hyperspectral、可见光、Synthetic Aperture Radar（SAR）、Infrared和low light）下，实现了通用的异常检测能力。<details>
<summary>Abstract</summary>
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation of the reformulation in the data, architecture, and optimization aspects. Simulated spectral and spatial anomalies drive the instantiated architecture. The model is optimized directly for the conditional probability ranking. The proposed model was validated in five modalities including the hyperspectral, visible light, synthetic aperture radar (SAR), infrared and low light to show its unified detection ability.
</details>
<details>
<summary>摘要</summary>
<<SYS>> remote 感知异常检测器可以找到背景中异常的对象作为潜在目标。由于地球异常类型的多样性，一个跨modalities和场景的统一异常检测器应该是成本效益和灵活的。然而，现有的异常检测器都是单一modalities和单一场景的，因为它们想要学习不同的背景分布。受到地球异常的通用异常偏差特征启发，我们利用这个特征来建立一个统一的异常检测器。首先，我们将异常检测任务转换为一个无向双层图，基于偏差关系，其中异常分数是模型的条件概率， giventhe 背景和正常对象的模式。然后，我们设计了实现的数据、architecture和优化方面。在实验中，我们使用了模拟的spectral和空间异常驱动了实现的建筑。模型直接优化为条件概率排名的问题。我们 Validated 该模型在五个modalities中，包括光谱、可见光、Synthetic Aperture Radar（SAR）、红外和低光照，以显示其统一的检测能力。Note: "<<SYS>>" is used to indicate the beginning of the translation, and ">>" is used to indicate the end of the translation.
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning"><a href="#Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning" class="headerlink" title="Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning"></a>Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07510">http://arxiv.org/abs/2310.07510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiming Qian</li>
<li>for: The paper aims to develop a novel pre-training framework for vision representation learning, which can facilitate the efficiency of common-sense recognition by leveraging both self-supervised and supervised visual pre-text tasks in a multi-task manner.</li>
<li>methods: The proposed pre-training framework consists of both multi-label classification and self-supervised learning tasks, including Masked Image Modeling (MIM) and contrastive learning. The framework takes an image as input and uses a heuristic approach to consider its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding.</li>
<li>results: The proposed pre-trained models achieve results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks, including 85.3% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. The performance demonstrates the ability of the vision foundation model to serve general purpose vision tasks.Here are the three information points in Simplified Chinese text:</li>
<li>for: 本研究旨在开发一种新的视觉表示学习预训练框架，以便提高常识认识的效率，通过结合自我指导学习和指导学习的视觉预测任务。</li>
<li>methods: 提议的预训练框架包括多类别分类和自我指导学习任务，包括掩码图像模型（MIM）和对比学习。该框架将图像作为输入，采取一种启发式的方法，考虑图像的内在风格特征、内部对象的位置和相关性，以及图像在3D空间中的基本视觉理解。</li>
<li>results: 提议的预训练模型可以在多个视觉任务中达到或超过当前最佳（SOTA）结果，包括ImageNet-1K分类任务的85.3%顶部一准确率、COCO对象检测任务的47.9个框损失率和ADE-20Ksemantic segmentation任务的50.6个mIoU。这些结果表明提议的视觉基础模型能够满足通用视觉任务的需求。<details>
<summary>Abstract</summary>
To mimic human vision with the way of recognizing the diverse and open world, foundation vision models are much critical. While recent techniques of self-supervised learning show the promising potentiality of this mission, we argue that signals from labelled data are also important for common-sense recognition, and properly chosen pre-text tasks can facilitate the efficiency of vision representation learning. To this end, we propose a novel pre-training framework by adopting both self-supervised and supervised visual pre-text tasks in a multi-task manner. Specifically, given an image, we take a heuristic way by considering its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding. However, large-scale object bounding boxes and correlations are usually hard to achieve. Alternatively, we develop a hybrid method by leveraging both multi-label classification and self-supervised learning. On the one hand, under the multi-label supervision, the pre-trained model can explore the detailed information of an image, e.g., image types, objects, and part of semantic relations. On the other hand, self-supervised learning tasks, with respect to Masked Image Modeling (MIM) and contrastive learning, can help the model learn pixel details and patch correlations. Results show that our pre-trained models can deliver results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3\% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. The performance shows the ability of our vision foundation model to serve general purpose vision tasks.
</details>
<details>
<summary>摘要</summary>
Given an image, we take a heuristic approach by considering its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding. However, large-scale object bounding boxes and correlations are usually difficult to achieve.Alternatively, we develop a hybrid method that leverages both multi-label classification and self-supervised learning. Under multi-label supervision, the pre-trained model can explore detailed image information, such as image types, objects, and part of semantic relations. Self-supervised learning tasks, including Masked Image Modeling (MIM) and contrastive learning, help the model learn pixel details and patch correlations.Our pre-trained models achieve results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. These results demonstrate the ability of our vision foundation model to serve general-purpose vision tasks.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation"><a href="#Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation" class="headerlink" title="Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation"></a>Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07506">http://arxiv.org/abs/2310.07506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haizhong Zheng, Jiachen Sun, Shutong Wu, Bhavya Kailkhura, Zhuoqing Mao, Chaowei Xiao, Atul Prakash</li>
<li>for: 这篇论文旨在提出一种基于对数测量的数据缩寸方法，以提高模型训练的性能。</li>
<li>methods: 这篇论文提出了一种叫做层次记忆网络（Hierarchical Memory Network，HMN）的新数据参数化架构，将数据缩寸到三层结构中，表示集合、类别和实例等层级的特征。</li>
<li>results: 在四个公开 dataset（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上评估了HMN和八个基于数据缩寸的基eline，结果显示HMN在训练时使用较少的GPU内存仍然能够实现更高的性能。<details>
<summary>Abstract</summary>
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results show that our proposed method outperforms all baselines, even when trained with a batch-based loss consuming less GPU memory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个实际世界数据集，数据压缩（DC）目标是将数据集中的知识压缩到较小的数据集上，以高性能进行模型训练。 现有的工作提议通过数据参数化来增强DC，将数据压缩到参数化数据容器中而不是像素空间。数据参数化的INTUITION是将图像中共享的特征编码，以避免额外存储成本。在这篇论文中，我们认为图像在层次结构上共享特征，而当前的数据参数化方法忽略了这种层次结构。为了更好地将DC与这种层次结构相匹配，我们提议一种新的数据参数化架构，即层次记忆网络（HMN）。HMN将压缩数据存储在三层结构中，表示数据集级、类别级和实例级特征。另外，层次架构的帮助特性是HMN可以自然地保证图像之间的独立性，同时实现图像之间的信息共享。这使得HMN可以进行实例级别的减少冗余信息，从而进一步减少冗余和提高性能。我们在四个公共数据集（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上评估HMN，并与八个DC基准方法进行比较。评估结果表明，我们提议的方法在所有基准方法中表现出色，即使在使用较少的GPU内存的批处理损失下进行训练。
</details></li>
</ul>
<hr>
<h2 id="PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction"><a href="#PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction" class="headerlink" title="PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction"></a>PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07504">http://arxiv.org/abs/2310.07504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Gan, Qiuchen Zhai, Michael Thompson McCann, Cristina Garcia Cardona, Ulugbek S. Kamilov, Brendt Wohlberg</li>
<li>For: ptychographic image reconstruction* Methods: deep model-based network (Vision Transformer + deep unrolling network)* Results: outperforms existing deep learning methods, reduces computational cost compared to iterative methods, maintains competitive performance.Here’s the text in Simplified Chinese:</li>
<li>for: ptychographic 图像重建</li>
<li>methods: 深度模型基于网络 (视觉变换器 + 深度卷积网络)</li>
<li>results: 超过现有深度学习方法表现, 比较iterative方法计算成本下降, 维持竞争性表现.<details>
<summary>Abstract</summary>
Ptychography is an imaging technique that captures multiple overlapping snapshots of a sample, illuminated coherently by a moving localized probe. The image recovery from ptychographic data is generally achieved via an iterative algorithm that solves a nonlinear phase-field problem derived from measured diffraction patterns. However, these approaches have high computational cost. In this paper, we introduce PtychoDV, a novel deep model-based network designed for efficient, high-quality ptychographic image reconstruction. PtychoDV comprises a vision transformer that generates an initial image from the set of raw measurements, taking into consideration their mutual correlations. This is followed by a deep unrolling network that refines the initial image using learnable convolutional priors and the ptychography measurement model. Experimental results on simulated data demonstrate that PtychoDV is capable of outperforming existing deep learning methods for this problem, and significantly reduces computational cost compared to iterative methodologies, while maintaining competitive performance.
</details>
<details>
<summary>摘要</summary>
ptychography 是一种图像技术，通过多个重叠的报告来捕捉样本，由移动的本地化探针启发干涉光。图像从ptychographic数据中的恢复通常通过迭代算法解决非线性phaserange-field问题来实现，但这些方法具有高计算成本。在这篇文章中，我们介绍了ptychodv，一种新的深度学习模型基网络，用于高效、高品质ptychographic图像恢复。ptychodv包括一个视Transformer，该生成 Raw Measurements 集合中的初始图像，考虑到这些测量之间的相互关系。然后是一个深度折叠网络，该使用学习的卷积约束和ptychography测量模型来细化初始图像。实验结果表明，ptychodv可以在 simulated data 上超过现有的深度学习方法，并significantly reduce computational cost compared to iterative methodologies，同时维持竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation"><a href="#FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation" class="headerlink" title="FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation"></a>FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07473">http://arxiv.org/abs/2310.07473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Sun, Peihao Chen, Jugang Fan, Thomas H. Li, Jian Chen, Mingkui Tan</li>
<li>for: 这种研究旨在解决自主系统在图像目标导航中困难的问题，即从拍摄图像中理解目标位置。</li>
<li>methods: 我们采用了细化目标提示（FGPrompt）方法，利用高分辨率特征图作为提示，以便在目标图像中保留细节信息并使观察Encoder关注目标相关区域。</li>
<li>results: 相比现有方法，我们在3个图像目标导航数据集（Gibson、MP3D和HM3D）上显示出了显著的性能改进，尤其是在Gibson数据集上，我们的方法超过了状态时的成功率，但使用的模型规模只是1&#x2F;50。项目页面：<a target="_blank" rel="noopener" href="https://xinyusun.github.io/fgprompt-pages">https://xinyusun.github.io/fgprompt-pages</a><details>
<summary>Abstract</summary>
Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems. The agent is required to reason the goal location from where a picture is shot. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (FGPrompt) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8% with only 1/50 model size. Project page: https://xinyusun.github.io/fgprompt-pages
</details>
<details>
<summary>摘要</summary>
学习寻找图像指定目标是自主系统的重要 yet 挑战性任务。 agent 需要从拍摄图像中理解目标位置。现有方法通过学习导航策略，以独立地捕捉图像目标和观察图像的semantic特征，并最后进行导航动作预测。然而，这些方法受到两大限制。1）它们可能会损失图像目标中的细节信息，因此无法理解目标位置。2）更重要的是，它们难以将观察图像中的关键区域关注于目标，因为它们没有将目标作为条件来理解观察。在本文中，我们希望超越这些限制，通过设计高精度目标提示（FGPrompt）方法，以提高图像目标导航性能。具体来说，我们利用图像目标中的高精度和高分辨率特征地图作为提示，以进行条件嵌入，保留图像目标中的细节信息，并使观察编码器更加注重目标相关区域。与现有方法相比，我们在3个图像目标导航数据集（i.e., Gibson, MP3D, HM3D）上表现出了显著的性能提升。特别是在Gibson上，我们超过了状态的杰出成功率，只使用1/50个模型大小。项目页面：https://xinyusun.github.io/fgprompt-pages
</details></li>
</ul>
<hr>
<h2 id="PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction"><a href="#PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction" class="headerlink" title="PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction"></a>PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07449">http://arxiv.org/abs/2310.07449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr</li>
<li>for: 提高 neural surface reconstruction 的稳定性和准确性，特别是在真实世界中采集的摄像头pose 中存在噪声的情况下。</li>
<li>methods: 我们引入了一个新的偏置字段(\textbf{PoRF}), 使用多层感知（MLP）进行姿态更新的推断，从而更加稳定地优化姿态参数。此外，我们还提出了一种眼点几何学损失，以增强超级视觉检测的指导，无需额外计算过程。</li>
<li>results: 我们在 DTU 数据集上减少了 COLMAP 姿态错误的旋转角度错误78%，从而降低了重建 Chamfer 距离从 3.48mm 降至 0.85mm。在 MobileBrick 数据集上，我们改进了 ARKit 姿态和重建 F1 分数，从 69.18 提高到 75.67，比 dataset 提供的基准姿态（75.14）还要好。这些成果表明了我们的方法在真实世界中采集的摄像头pose 中提高姿态精度和重建稳定性。<details>
<summary>Abstract</summary>
Neural surface reconstruction is sensitive to the camera pose noise, even if state-of-the-art pose estimators like COLMAP or ARKit are used. More importantly, existing Pose-NeRF joint optimisation methods have struggled to improve pose accuracy in challenging real-world scenarios. To overcome the challenges, we introduce the pose residual field (\textbf{PoRF}), a novel implicit representation that uses an MLP for regressing pose updates. This is more robust than the conventional pose parameter optimisation due to parameter sharing that leverages global information over the entire sequence. Furthermore, we propose an epipolar geometry loss to enhance the supervision that leverages the correspondences exported from COLMAP results without the extra computational overhead. Our method yields promising results. On the DTU dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the MobileBrick dataset that contains casually captured unbounded 360-degree videos, our method refines ARKit poses and improves the reconstruction F1 score from 69.18 to 75.67, outperforming that with the dataset provided ground-truth pose (75.14). These achievements demonstrate the efficacy of our approach in refining camera poses and improving the accuracy of neural surface reconstruction in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
neural surface reconstruction 敏感于摄像头pose随机变化，即使使用最新的pose估计器 like COLMAP或ARKit。更重要的是，现有的pose-NeRF联合优化方法在实际世界场景中表现不佳。为了解决这些挑战，我们引入 pose residual field（PoRF），一种新的隐式表示方法，使用多层感知（MLP）来回归 pose 更新。这比传统的 pose 参数优化更加稳定，因为参数共享利用全序列的全局信息。此外，我们提议使用 Epipolar geometry loss来增强监督，这是基于 COLMAP 结果出口的对准不带额外计算过程。我们的方法在 DTU 数据集上减少了 COLMAP 姿态错误的旋转误差，从而降低了折射距离从 3.48mm 降至 0.85mm。在包含抓拍 capture 的 unbounded 360度视频 MobileBrick 数据集上，我们的方法改进了 ARKit 姿态，提高了 reconstruction F1 分数从 69.18 提高到 75.67，超过了提供的数据集真实pose（75.14）。这些成果表明我们的方法在实际世界场景中提高摄像头姿态和神经表面重建的准确性。
</details></li>
</ul>
<hr>
<h2 id="Distance-based-Weighted-Transformer-Network-for-Image-Completion"><a href="#Distance-based-Weighted-Transformer-Network-for-Image-Completion" class="headerlink" title="Distance-based Weighted Transformer Network for Image Completion"></a>Distance-based Weighted Transformer Network for Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07440">http://arxiv.org/abs/2310.07440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Xuelong Li, Yue Lu</li>
<li>for: This paper proposes a new architecture for image completion tasks, which leverages the strengths of both Convolutional Neural Networks (CNNs) and Distance-based Weighted Transformer (DWT) to enhance the image completion process.</li>
<li>methods: The proposed model uses a combination of CNNs and DWT blocks to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Additionally, the model introduces Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder’s skip features with the coarse features provided by the generator.</li>
<li>results: Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of the proposed model compared to existing approaches.<details>
<summary>Abstract</summary>
The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image's components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder's skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabiliser. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
描述文本：图像生成挑战已被模型为结构优先或变换问题。然而，现有模型在理解全局输入图像结构方面表现不佳，主要因为特定的内置特征（例如，本地推导优先）。现代研究表明，自我注意是图像完成问题的有效模型化技术。在这篇论文中，我们提出一种新的架构，利用距离基于权重Transformer（DWT）来更好地理解图像组件之间的关系。我们在模型中利用了Convolutional Neural Networks（CNNs）和DWT块的优点，以提高图像完成过程。具体来说，CNNs用于增强粗略先验的本地 тексту强度信息，而DWT块用于恢复一些粗略的文本和一致视觉结构。与现有方法一样，我们使用CNNs创建特征地图，但不同的是，我们使用DWT来编码全局依赖关系，计算距离基于权重特征图，这样可以减少视觉混乱的问题。此外，我们还提出了一种简单 yet有效的技术，使得非零值的卷积权重至正，并通过网络层的精度补做来提供高效的训练稳定器。广泛的量化和质量测试表明，我们提出的模型在三个复杂的数据集上表现出色，与现有方法相比，具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction"><a href="#DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction" class="headerlink" title="DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction"></a>DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07438">http://arxiv.org/abs/2310.07438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli</li>
<li>for: 预测路用户的路径准确性在多代理设定下是一项挑战，因为代理的特性和意图都是未知的。</li>
<li>methods: 我们提出了Dynamic goal quErieS with temporal Transductive alIgNmEnt（DESTINE）方法，与过去的方法不同，我们的方法1）动态预测代理的目标，不受特定道路结构的限制，以便生成更准确的目的预测; 2）通过生成未来轨迹的粗细化过程，使预测 trajectory 与地图兼容; 3）通过掩码注意力模块，使预测 trajectory 在时间上进行协调。</li>
<li>results: 使用 Argoverse 测试集，我们的方法在各种指标上达到了状态的末点性能，并通过了全面的减少研究，以证明提出的模块的贡献。<details>
<summary>Abstract</summary>
Predicting temporally consistent road users' trajectories in a multi-agent setting is a challenging task due to unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity. To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike past arts, our approach 1) dynamically predicts agents' goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via masked attention. Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到Simplified Chinese。<</SYS>>预测行为 temporal consistent road users的轨迹在多智能体设定中是一项具有挑战性的任务，因为智能体的特性和意图都是未知的。除了使用 semantic map 信息和交互模型外，还需要建立一种能够理解不同粒度的行为的机制。为此，我们提议动态目标 quErieS with temporal Transductive alIgNmEnt（DESTINE）方法。与过去的艺术不同，我们的方法具有以下特点：1. 动态预测智能体的目标，不受特定的公路结构，如车道，允许方法生成更加准确的目的地预测;2. 实现地图兼容预测，通过在下一帧执行的粗略预测作为中间目标，并在上一帧执行的精细预测作为精度预测;3. 使用带有干扰模块的注意力机制，以在预测轨迹的时间上进行对齐。使用常用的 Argoverse  benchmark 数据集，我们展示了我们的方法在不同维度上的出色表现，并进行了全面的折衔研究，以便更好地了解提议的模块的贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos"><a href="#A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos" class="headerlink" title="A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos"></a>A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07416">http://arxiv.org/abs/2310.07416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pedestriandynamics/vcnn4pude">https://github.com/pedestriandynamics/vcnn4pude</a></li>
<li>paper_authors: Ahmed Alia, Mohammed Maree, Mohcine Chraibi, Armin Seyfried</li>
<li>for: 了解人群压力行为的微观动态特征，以便更好地理解人群流动模式和互动方式。</li>
<li>methods: 提出了一种新的自动检测人群压力行为的框架，包括Feature提取和视频标注两个主要组成部分。</li>
<li>results: 对六个实验实际数据进行训练和测试，结果表明提出的框架在比较方法中表现出色，能够更好地检测人群压力行为。<details>
<summary>Abstract</summary>
Analyzing the microscopic dynamics of pushing behavior within crowds can offer valuable insights into crowd patterns and interactions. By identifying instances of pushing in crowd videos, a deeper understanding of when, where, and why such behavior occurs can be achieved. This knowledge is crucial to creating more effective crowd management strategies, optimizing crowd flow, and enhancing overall crowd experiences. However, manually identifying pushing behavior at the microscopic level is challenging, and the existing automatic approaches cannot detect such microscopic behavior. Thus, this article introduces a novel automatic framework for identifying pushing in videos of crowds on a microscopic level. The framework comprises two main components: i) Feature extraction and ii) Video labeling. In the feature extraction component, a new Voronoi-based method is developed for determining the local regions associated with each person in the input video. Subsequently, these regions are fed into EfficientNetV1B0 Convolutional Neural Network to extract the deep features of each person over time. In the second component, a combination of a fully connected layer with a Sigmoid activation function is employed to analyze these deep features and annotate the individuals involved in pushing within the video. The framework is trained and evaluated on a new dataset created using six real-world experiments, including their corresponding ground truths. The experimental findings indicate that the suggested framework outperforms seven baseline methods that are employed for comparative analysis purposes.
</details>
<details>
<summary>摘要</summary>
可以通过分析人群中微型动态的推担行为来获得价值的启示，以提高人群模式和互动的理解。通过在人群视频中识别推担行为的实例，可以更深入地了解推担行为发生的时间、地点和原因。这些知识是创建更有效的人群管理策略、优化人群流动和提高总体人群体验的关键。然而，手动识别人群中微型推担行为是困难的，现有的自动方法无法检测这种微型行为。因此，本文提出了一种新的自动框架，用于在人群视频中识别推担行为。该框架包括两个主要组成部分：一是特征提取部分，二是视频标注部分。在特征提取部分，我们开发了一种基于Voronoi区域的新方法，用于确定输入视频中每个人的本地区域。然后，这些区域将被传递给EfficientNetV1B0卷积神经网络进行深度特征提取。在视频标注部分，我们使用了一个全连接层和sigmoid活化函数，以分析这些深度特征并将视频中涉及到推担行为的个体进行标注。该框架被训练并评估使用了六个实际实验的新数据集，包括其相应的真实标注。实验结果表明，提议的框架在比较分析中超过了七个基线方法。
</details></li>
</ul>
<hr>
<h2 id="CLIP-for-Lightweight-Semantic-Segmentation"><a href="#CLIP-for-Lightweight-Semantic-Segmentation" class="headerlink" title="CLIP for Lightweight Semantic Segmentation"></a>CLIP for Lightweight Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07394">http://arxiv.org/abs/2310.07394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Jin, Wankou Yang</li>
<li>for: 这 paper 旨在提出一种新的语言引导 semantic segmentation 方法，以便在轻量级网络上应用。</li>
<li>methods: 这 paper 使用了一种新的 feature fusion module，该模块包括一个 CNN 和一个 transformer，并通过两个方向的桥连接来实现语言引导的功能。</li>
<li>results: 对于多个 experiment，这 paper 的方法可以达到更高的性能，比如 DenseCLIP 等之前的 SOTA 工作。<details>
<summary>Abstract</summary>
The large-scale pretrained model CLIP, trained on 400 million image-text pairs, offers a promising paradigm for tackling vision tasks, albeit at the image level. Later works, such as DenseCLIP and LSeg, extend this paradigm to dense prediction, including semantic segmentation, and have achieved excellent results. However, the above methods either rely on CLIP-pretrained visual backbones or use none-pretrained but heavy backbones such as Swin, while falling ineffective when applied to lightweight backbones. The reason for this is that the lightweitht networks, feature extraction ability of which are relatively limited, meet difficulty embedding the image feature aligned with text embeddings perfectly. In this work, we present a new feature fusion module which tackles this problem and enables language-guided paradigm to be applied to lightweight networks. Specifically, the module is a parallel design of CNN and transformer with a two-way bridge in between, where CNN extracts spatial information and visual context of the feature map from the image encoder, and the transformer propagates text embeddings from the text encoder forward. The core of the module is the bidirectional fusion of visual and text feature across the bridge which prompts their proximity and alignment in embedding space. The module is model-agnostic, which can not only make language-guided lightweight semantic segmentation practical, but also fully exploit the pretrained knowledge of language priors and achieve better performance than previous SOTA work, such as DenseCLIP, whatever the vision backbone is. Extensive experiments have been conducted to demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
大规模预训练模型CLIP，在400万张图像文本对应对得到了许多应用场景，尤其是图像水平上的任务。后续的工作，如 denseclip 和 LSeg，在 dense prediction 领域进一步扩展了这种方法，并取得了出色的成绩。然而，这些方法都是通过使用CLIP预训练的视觉后ION或者使用不预训练的 pero 重量级网络，如 Swin，来实现。这是因为轻量级网络的特点是其特征提取能力相对较弱，难以与文本嵌入完美地匹配。在这种情况下，我们提出了一种新的特征融合模块，可以解决这个问题，并使得语言指导方法可以应用于轻量级网络。具体来说，该模块是一种并行的 CNN 和 transformer 的设计，其中 CNN 提取图像Encoder中的空间信息和视觉上下文，而 transformer 将文本Encoder中的文本嵌入传递前进。模块的核心是在桥接之间进行对文本和图像特征的双向融合，以便它们在嵌入空间中的距离和对齐。该模块是模型无关的，可以不仅使得语言指导的轻量级semantic segmentation成为现实，还可以充分利用预训练的语言优先知识，并超过先前的 SOTA 工作，如 denseclip，无论视觉后ION是什么。我们已经进行了广泛的实验来证明我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters"><a href="#Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters" class="headerlink" title="Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters"></a>Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07361">http://arxiv.org/abs/2310.07361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Michalkiewicz, Masoud Faraki, Xiang Yu, Manmohan Chandraker, Mahsa Baktashmotlagh</li>
<li>for: 减少深度神经网络中过拟合的问题</li>
<li>methods: 基于梯度信号噪声比（GSNR）选择Dropout掩码，并使用元学习法寻找最佳Dropout比例</li>
<li>results: 在标准领域总结化Benchmark上实现了竞争力的分类和人脸反假检测结果<details>
<summary>Abstract</summary>
Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To compensate for the over-parameterized models, numerous regularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of domain shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sampled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of network's parameters. Specifically, at each training step, parameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the optimal dropout ratio by leveraging a meta-learning approach. We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classification and face anti-spoofing problems.
</details>
<details>
<summary>摘要</summary>
常见的问题是在深度神经网络中使用梯度下降式训练时发生过拟合源领域问题。为了弥补这个问题，许多调整技术已经被提出，如基于dropout的方法。这些方法在经典的benchmark上achieve significant improvement，但是它们在测试集中的类型转换时表现下降。在这篇文章中，我们从 классичногоapproach中逃避bernoulli抽出dropout mask的建构方法，而是基于网络参数的梯度噪声比率(GSNR)来选择parameter。具体来说，在每次训练步骤中，具有高GSNR的参数将被弃用。此外，我们透过元学习方法来缓解手动搜寻最佳dropout比率的负担。我们在标准的领域扩展benchmark上评估了我们的方法，并取得了竞争的结果在分类和面部防诈问题上。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method"><a href="#Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method" class="headerlink" title="Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method"></a>Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07359">http://arxiv.org/abs/2310.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masood Hamed Saghayan, Mohammad Hossein Zolfagharnasab, Ali Khadem, Farzam Matinfar, Hassan Rashidi</li>
<li>for: 诊断帕金逊病（BD），使用三维结构MRI图像（sMRI），而不是常见的函数MRI（fMRI）、电enzephalography（EEG）和行为 симптом。</li>
<li>methods: 提议使用混合GAN-CNN模型诊断BD，并测试了不同的扩充比例。</li>
<li>results: 取得75.8%的准确率，60.3%的敏感度和82.5%的特异度，比前一工作高3-5%，使用样本数少于6%。此外，还 demonstarted了GAN生成器可以有效地复制复杂的3D脑样本，而无需手动图像处理。<details>
<summary>Abstract</summary>
Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive cycles of hypomania and depression. Since diagnosing BD relies on subjective behavioral assessments over a long period, a solid diagnosis based on objective criteria is not straightforward. The current study responded to the described obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI samples rather than conventional datasets such as functional MRI (fMRI), electroencephalography (EEG), and behavioral symptoms while removing the data insufficiency usually encountered when dealing with sMRI samples. The impact of various augmentation ratios is also tested using 5-fold cross-validation. Based on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of 60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while utilizing less than 6% sample counts. Next, it is demonstrated that a 2- D layer-based GAN generator can effectively reproduce complex 3D brain samples, a more straightforward technique than manual image processing. Lastly, the optimum augmentation threshold for the current study using 172 sMRI samples is 50%, showing the applicability of the described method for larger sMRI datasets. In conclusion, it is established that data augmentation using GAN improves the accuracy of the CNN classifier using sMRI samples, thus developing more reliable decision support systems to assist practitioners in identifying BD patients more reliably and in a shorter period
</details>
<details>
<summary>摘要</summary>
精神疾病（BD）是一种心理疾病，通过重复的强迫症状和抑郁来诊断。由于诊断BD需要长期的主观行为评估，因此不可靠的诊断方法不是很 straightforward。现study中提出了一种hybrid GAN-CNN模型，用于通过三维结构MRI图像（sMRI）诊断BD。这种研究的创新在于利用sMRI样本进行诊断，而不是常见的fMRI、EEG和行为症状数据。此外，该研究还测试了不同的扩展率，并使用5-fold cross-validation。根据结果，该研究获得了75.8%的准确率，60.3%的敏感性和82.5%的特异性，这些值高于前一次研究3-5%，同时使用的样本数量少于6%。然后，研究表明了一个2D层基于GAN生成器可以有效地重produce复杂的3D脑样本，这种方法比手动图像处理更为简单。最后，该研究发现了使用172个sMRI样本的最佳扩展阈值为50%。总之，这种方法可以在更大的sMRI样本上进行数据扩展，从而提高CNN分类器的准确率，并为BD诊断提供更可靠的决策支持系统，以更加准确地诊断BD患者并更快地进行诊断。
</details></li>
</ul>
<hr>
<h2 id="IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training"><a href="#IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training" class="headerlink" title="IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training"></a>IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07355">http://arxiv.org/abs/2310.07355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 本研究旨在提高医疗影像语言预训练（VLP）中对医疗报告中的结构信息的利用，以提高视语同步性。</li>
<li>methods: 本研究提出了一种新的临床指导VLP框架，称之为IMITATE，该框架利用医疗报告的层次结构，并对相应的描述性和诊断性文本进行视语同步。此外，该研究还引入了一种新的临床知识 Informed Contrastive Loss，以考虑临床优先知识在对比学习中的样本相关性。</li>
<li>results: 对六个不同的数据集进行比较，IMITATE模型在五种医疗影像下沉淀任务中都超过了基eline VLP方法。实验结果表明，将医疗报告中的结构信息integrated into VLP可以提高视语同步性。<details>
<summary>Abstract</summary>
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furthermore, a new clinical-informed contrastive loss is introduced for cross-modal learning, which accounts for clinical prior knowledge in formulating sample correlations in contrastive learning. The proposed model, IMITATE, outperforms baseline VLP methods across six different datasets, spanning five medical imaging downstream tasks. Comprehensive experimental results highlight the advantages of integrating the hierarchical structure of medical reports for vision-language alignment.
</details>
<details>
<summary>摘要</summary>
在医学视语预训（VLP）领域，有很大的努力投入到从临床报告和相关医疗图像中提取文本和图像特征。然而，大多数现有方法可能忽略了利用临床报告的自然层次结构，这些报告通常被拆分成描述性内容的“发现”和结论的“印象”。相反，现有的医学VLP方法通常将报告简化为单一实体或分解成多个token。在这项工作中，我们提出了一种名为IMITATE的新的临床导向VLP框架，以学习医学报告的层次结构信息，并对报告中的描述性和结论部分进行视语同步。此外，我们还引入了一种基于临床知识的对比损失函数，用于跨模态学习，该函数考虑了临床知识在对比学习中的样本相关性。提出的模型IMITATE，在六个不同的数据集上，比基eline VLP方法表现出色，并且对五种医疗下游任务进行了评估。全面的实验结果表明，将临床报告的层次结构integrated into VLP框架可以提高视语同步的性能。
</details></li>
</ul>
<hr>
<h2 id="PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation"><a href="#PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation" class="headerlink" title="PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation"></a>PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07743">http://arxiv.org/abs/2310.07743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haibo-qiu/PointHR">https://github.com/haibo-qiu/PointHR</a></li>
<li>paper_authors: Haibo Qiu, Baosheng Yu, Yixin Chen, Dacheng Tao</li>
<li>for: 本研究旨在提高3D点云分割的高分辨率架构，以提高3D点云分析的精度。</li>
<li>methods: 提出了一个通用架构PointHR，包括KNN顺序操作器和差分扩散操作器，以及预计算序列和扩散操作器的索引。</li>
<li>results: 对S3DIS和ScanNetV2 dataset进行了广泛的实验，并取得了与现有状态艺术方法相当或更高的性能，无需额外增加细节。<details>
<summary>Abstract</summary>
Significant progress has been made recently in point cloud segmentation utilizing an encoder-decoder framework, which initially encodes point clouds into low-resolution representations and subsequently decodes high-resolution predictions. Inspired by the success of high-resolution architectures in image dense prediction, which always maintains a high-resolution representation throughout the entire learning process, we consider it also highly important for 3D dense point cloud analysis. Therefore, in this paper, we explore high-resolution architectures for 3D point cloud segmentation. Specifically, we generalize high-resolution architectures using a unified pipeline named PointHR, which includes a knn-based sequence operator for feature extraction and a differential resampling operator to efficiently communicate different resolutions. Additionally, we propose to avoid numerous on-the-fly computations of high-resolution architectures by pre-computing the indices for both sequence and resampling operators. By doing so, we deliver highly competitive high-resolution architectures while capitalizing on the benefits of well-designed point cloud blocks without additional effort. To evaluate these architectures for dense point cloud analysis, we conduct thorough experiments using S3DIS and ScanNetV2 datasets, where the proposed PointHR outperforms recent state-of-the-art methods without any bells and whistles. The source code is available at \url{https://github.com/haibo-qiu/PointHR}.
</details>
<details>
<summary>摘要</summary>
“近期，在点云分割方面有所进步，使用编码-解码框架，首先将点云编码成低分辨率表示，然后解码高分辨率预测。受图像高分辨率建筑的成功启发，我们认为3D点云分析中也非常重要。因此，在这篇论文中，我们探索了3D点云分割中高分辨率建筑。具体来说，我们将高分辨率建筑总称为PointHR，它包括基于KNN的序列运算器和差分扩散运算器，以及高效地传递不同分辨率的索引。此外，我们还提出了避免高分辨率建筑的许多在飞行计算中的即时计算，通过预计算序列和扩散运算器的索引。通过这种方式，我们实现了高度竞争力的高分辨率建筑，而无需额外努力。为评估这些建筑，我们对S3DIS和ScanNetV2 datasets进行了严格的实验，并发现提议的PointHR在无任何额外装饰的情况下高度竞争力。源代码可以在GitHub上找到：https://github.com/haibo-qiu/PointHR。”
</details></li>
</ul>
<hr>
<h2 id="Guided-Attention-for-Interpretable-Motion-Captioning"><a href="#Guided-Attention-for-Interpretable-Motion-Captioning" class="headerlink" title="Guided Attention for Interpretable Motion Captioning"></a>Guided Attention for Interpretable Motion Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07324">http://arxiv.org/abs/2310.07324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rd20karim/m2t-interpretable">https://github.com/rd20karim/m2t-interpretable</a></li>
<li>paper_authors: Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</li>
<li>For: This paper focuses on generating text from motion, specifically exploring the combination of movement encoders with spatio-temporal attention models to improve the interpretability of the architectures and highlight perceptually pertinent areas of the skeleton in time.* Methods: The proposed approach uses movement encoders and spatio-temporal attention models to generate text from motion. The authors propose strategies to guide the attention during training, such as adding guided attention with adaptive gate, to improve the interpretability of the models and highlight perceptually pertinent areas of the skeleton in time.* Results: The authors achieved state-of-the-art (SOTA) results on two motion capture datasets, KIT MLD and HumanML3D, with significant improvements in interpretability compared to higher parameter-count non-interpretable systems. Specifically, they obtained a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%) on KIT MLD, and a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA +6.1%), a CIDEr of 61.6 (SOTA -10.9%), and a Bertscore of 40.3% (SOTA +2.5%) on HumanML3D.<details>
<summary>Abstract</summary>
While much effort has been invested in generating human motion from text, relatively few studies have been dedicated to the reverse direction, that is, generating text from motion. Much of the research focuses on maximizing generation quality without any regard for the interpretability of the architectures, particularly regarding the influence of particular body parts in the generation and the temporal synchronization of words with specific movements and actions. This study explores the combination of movement encoders with spatio-temporal attention models and proposes strategies to guide the attention during training to highlight perceptually pertinent areas of the skeleton in time. We show that adding guided attention with adaptive gate leads to interpretable captioning while improving performance compared to higher parameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we obtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D, we obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA +6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our code implementation and reproduction details will be soon available at https://github.com/rd20karim/M2T-Interpretable/tree/main.
</details>
<details>
<summary>摘要</summary>
“尽管有很多研究投入到人体动作从文本生成中，但相对少数研究专门关注文本生成到人体动作的反向方向。大多数研究都是强调生成质量的最大化，而忽略了生成过程中特定部位的影响和时间同步。本研究探讨将运动编码器与空间时间注意力模型结合使用，并提出了引导注意力的策略，以便在训练时高亮有意义的骨骼区域。我们展示了在 KIT MLD 数据集上，通过添加引导注意力和适应门限来实现可读性的captioning，并提高了相比高参数计数的非可读性 SOTA 系统的性能。在 HumanML3D 数据集上，我们获得了 BLEU@4 的 25.00% (SOTA +2.7%),ROUGE-L 分数的 55.4% (SOTA +6.1%),CIDEr 分数的 61.6 (SOTA -10.9%),Bertscore 分数的 40.3% (SOTA +2.5%).我们的代码实现和重现细节将很快地在 GitHub 上公开。”
</details></li>
</ul>
<hr>
<h2 id="A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation"><a href="#A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation" class="headerlink" title="A webcam-based machine learning approach for three-dimensional range of motion evaluation"></a>A webcam-based machine learning approach for three-dimensional range of motion evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07322">http://arxiv.org/abs/2310.07322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoye Michael Wang, Derek T. Smith, Qin Zhu</li>
<li>for: 用于评估Physical therapy中的肢体范围运动</li>
<li>methods: 使用机器学习算法和网络摄像头进行评估</li>
<li>results: 实现了高度的重复性和对比性，可用于评估肢体范围运动的临床实践和远程物理治疗<details>
<summary>Abstract</summary>
Background. Joint range of motion (ROM) is an important quantitative measure for physical therapy. Commonly relying on a goniometer, accurate and reliable ROM measurement requires extensive training and practice. This, in turn, imposes a significant barrier for those who have limited in-person access to healthcare.   Objective. The current study presents and evaluates an alternative machine learning-based ROM evaluation method that could be remotely accessed via a webcam.   Methods. To evaluate its reliability, the ROM measurements for a diverse set of joints (neck, spine, and upper and lower extremities) derived using this method were compared to those obtained from a marker-based optical motion capture system.   Results. Data collected from 25 healthy adults demonstrated that the webcam solution exhibited high test-retest reliability, with substantial to almost perfect intraclass correlation coefficients for most joints. Compared with the marker-based system, the webcam-based system demonstrated substantial to almost perfect inter-rater reliability for some joints, and lower inter-rater reliability for other joints (e.g., shoulder flexion and elbow flexion), which could be attributed to the reduced sensitivity to joint locations at the apex of the movement.   Conclusions. The proposed webcam-based method exhibited high test-retest and inter-rater reliability, making it a versatile alternative for existing ROM evaluation methods in clinical practice and the tele-implementation of physical therapy and rehabilitation.
</details>
<details>
<summary>摘要</summary>
背景：关节范围运动（ROM）是物理治疗中的重要量化指标，通常使用尺 mesurement device，需要准确和可靠的测量，但是需要具备广泛的训练和实践经验。这种限制了医疗访问的情况，对于没有充分的面对面医疗访问的人来说，是一个重要的障碍。目标：本研究提出了一种基于机器学习的ROM评估方法，可以通过网络摄像头进行远程访问。方法：为评估这种方法的可靠性，对一组多种关节（ neck、脊梁、上肢和下肢）的ROM测量结果，与使用标记器基于光学运动追踪系统获取的测量结果进行比较。结果：对于25名健康成人的数据显示，网络摄像头解决方案具有高的测试-重复和 между人重要性，大多数关节的 intraclass correlation coefficient 在0.7到0.9之间，与标记器基于系统的测量结果相比，网络摄像头解决方案在一些关节（如肩屈和肘屈）表现出了高度的重要性，而在其他关节（如肩屈和肘屈）表现出了较低的重要性，这可能是因为网络摄像头解决方案在运动的峰值位置感知的reduced sensitivity。结论：提出的网络摄像头解决方案具有高的测试-重复和 между人重要性，可以作为现有的ROM评估方法的可靠的替代方案，在临床实践和远程物理治疗中应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy"><a href="#Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy" class="headerlink" title="Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy"></a>Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07310">http://arxiv.org/abs/2310.07310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei C. Aioanei, Regine Hunziker-Rodewald, Konstantin Klein, Dominik L. Michels</li>
<li>for: 提高古代铭文的解读精度，使用现代人工智能技术如机器学习（ML）。</li>
<li>methods: 开发了一种创新的方法，通过生成自然语言模拟器（GAN）生成了具有照明、损害和变换的实验性阿拉伯字母数据集。</li>
<li>results: 使用这些数据集训练了一个差异函数网络（ResNet）模型，并在8世纪前 Beside Hadad雕塑作品中的真实图像中达到了高准确率。<details>
<summary>Abstract</summary>
Epigraphy increasingly turns to modern artificial intelligence (AI) technologies such as machine learning (ML) for extracting insights from ancient inscriptions. However, scarce labeled data for training ML algorithms severely limits current techniques, especially for ancient scripts like Old Aramaic. Our research pioneers an innovative methodology for generating synthetic training data tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic Aramaic letter datasets, incorporating textural features, lighting, damage, and augmentations to mimic real-world inscription diversity. Despite minimal real examples, we engineer a dataset of 250,000 training and 25,000 validation images covering the 22 letter classes in the Aramaic alphabet. This comprehensive corpus provides a robust volume of data for training a residual neural network (ResNet) to classify highly degraded Aramaic letters. The ResNet model demonstrates high accuracy in classifying real images from the 8th century BCE Hadad statue inscription. Additional experiments validate performance on varying materials and styles, proving effective generalization. Our results validate the model's capabilities in handling diverse real-world scenarios, proving the viability of our synthetic data approach and avoiding the dependence on scarce training data that has constrained epigraphic analysis. Our innovative framework elevates interpretation accuracy on damaged inscriptions, thus enhancing knowledge extraction from these historical resources.
</details>
<details>
<summary>摘要</summary>
隐写技术逐渐使用现代人工智能（AI）技术，如机器学习（ML）来提取古代铭文中的信息。然而，古代文字如老阿拉伯语言的有限的标注数据，对当前技术的应用带来了严重的限制。我们的研究开拓了一种创新的方法，用于生成适用于老阿拉伯字母的合成训练数据。我们的管道synthesizes photo-realistic Aramaic letter datasets，包括文字的特征、照明、损害和扩展，以模拟实际铭文的多样性。尽管有限的实际示例，我们引入了250,000个训练图像和25,000个验证图像，覆盖了阿拉伯字母的22个字母类。这个全面的数据集提供了训练一个剩余神经网络（ResNet）来分类高度受损的阿拉伯字母的稳定的数据源。ResNet模型在8世纪前后期埃及神话雕塑铭文中的真实图像中显示了高精度的分类能力。进一步的实验证明了模型在不同材质和风格下的有效总体化。我们的结果证明了我们的合成数据方法的可行性，并且避免了对古代铭文的分析中的稀缺的标注数据的依赖。我们的创新框架提高了对损害铭文的解读精度，从而扩大了对历史资源的知识提取。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation"><a href="#Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation" class="headerlink" title="Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation"></a>Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07265">http://arxiv.org/abs/2310.07265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Zheng, Yunhao Luo, Pengyuan Zhou, Lin Wang</li>
<li>For: 本研究目标是将预训练的笨重 yet 表现良好的 Convolutional Neural Network (CNN) 模型转移到学习 Compact Vision Transformer (ViT) 模型，保持学习能力。* Methods: 我们提出了一种新的 Knowledge Distillation (KD) 框架，名为 C2VKD，以将 teacher 模型的知识传授到 student 模型。我们首先提出了一种 visual-linguistic feature distillation (VLFD) 模块，以便在对齐的视觉和语言相关表示之间进行有效的 KD。此外，由于 teacher 模型和 student 模型之间的容量差距和不可避免的预测错误，我们 THEN 提出了一种像素级分离分配 (PDD) 模块，以便在 combinational 标签和教师的预测值上监督学生。* Results: 我们在三个 semantic segmentation  benchmark  dataset上进行了实验，结果显示，我们的方法可以提高 mIoU 的提升量超过 200% 的 SoTA KD 方法。<details>
<summary>Abstract</summary>
In this paper, we tackle a new problem: how to transfer knowledge from the pre-trained cumbersome yet well-performed CNN-based model to learn a compact Vision Transformer (ViT)-based model while maintaining its learning capacity? Due to the completely different characteristics of ViT and CNN and the long-existing capacity gap between teacher and student models in Knowledge Distillation (KD), directly transferring the cross-model knowledge is non-trivial. To this end, we subtly leverage the visual and linguistic-compatible feature character of ViT (i.e., student), and its capacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD framework, dubbed C2VKD. Importantly, as the teacher's features are heterogeneous to those of the student, we first propose a novel visual-linguistic feature distillation (VLFD) module that explores efficient KD among the aligned visual and linguistic-compatible representations. Moreover, due to the large capacity gap between the teacher and student and the inevitable prediction errors of the teacher, we then propose a pixel-wise decoupled distillation (PDD) module to supervise the student under the combination of labels and teacher's predictions from the decoupled target and non-target classes. Experiments on three semantic segmentation benchmark datasets consistently show that the increment of mIoU of our method is over 200% of the SoTA KD methods
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了一个新的问题：如何从预训练的庞大又高效的 CNN 模型传承知识到学习具有较小体积的 Vision Transformer（ViT）模型，而保持学习能力？由于 ViT 和 CNN 模型之间的完全不同特征和长期存在的容量差异，直接传承交叉模型知识非常困难。为此，我们细腻地利用 ViT 模型（即学生）的视觉和语言相容特征，以及它们与 CNN 模型（即教师）之间的容量差，并提出了一种新的 CNN-to-ViT KD 框架，称为 C2VKD。重要的是，由于教师的特征与学生的特征是不同的，我们首先提出了一种新的视觉语言相容特征采样（VLFD）模块，以实现有效的 KD 。此外，由于教师和学生之间的容量差较大，以及不可避免的预测错误，我们则提出了一种像素级分离采样（PDD）模块，以supervise 学生在拥有标签和教师预测的decoupled 目标和非目标类下进行学习。在三个semantic segmentation benchmark数据集上，我们的方法的提升率在 SoTA KD 方法上超过 200%。
</details></li>
</ul>
<hr>
<h2 id="ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion"><a href="#ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion" class="headerlink" title="ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion"></a>ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07255">http://arxiv.org/abs/2310.07255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fangfang11-plog/adasr">https://github.com/fangfang11-plog/adasr</a></li>
<li>paper_authors: Jinghui Qin, Lihuang Fang, Ruitao Lu, Liang Lin, Yukai Shi</li>
<li>for: 这个论文旨在提高深度学习基于多spectral图像（HSI）的超分辨率（HR-HSI），使用深度神经网络（DNN）来将HSI和多spectral图像（MSI）进行融合。</li>
<li>methods: 我们提出了一种新的挑战性自动数据增强框架ADASR，该框架可以自动优化和扩充HSI-MSI样本对的数据多样性，以便更好地进行HSI-MSI融合。我们的框架是 Sample-aware，并通过对增强网络和两个下采样网络的 JOINT  adversarial学习来帮助我们学习更加Robust的下采样网络，以便在训练上映射网络时更好地进行训练。</li>
<li>results: 我们的ADASR在两个公共的古典多spectral数据集上进行了广泛的实验，与当前状态的方法进行比较，并达到了更高的性能。<details>
<summary>Abstract</summary>
Deep learning-based hyperspectral image (HSI) super-resolution, which aims to generate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image (HSI) and multispectral image (MSI) with deep neural networks (DNNs), has attracted lots of attention. However, neural networks require large amounts of training data, hindering their application in real-world scenarios. In this letter, we propose a novel adversarial automatic data augmentation framework ADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich data diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes an augmentor network and two downsampling networks jointly by adversarial learning so that we can learn more robust downsampling networks for training the upsampling network. Extensive experiments on two public classical hyperspectral datasets demonstrate the effectiveness of our ADASR compared to the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language Simplified Chinese;Deep learning-based 干扰特征图像（HSI）超分辨率，旨在通过将干扰特征图像（HSI）和多spectral图像（MSI）与深度神经网络（DNNs）结合，生成高空间分辨率干扰特征图像（HR-HSI）。然而，神经网络需要大量的训练数据，使其在实际场景中应用受限。在这封信中，我们提出了一种novel的对抗自动数据增强框架ADASR，可以自动优化和扩充HSI-MSI样本对的多样性，以便用于HSI-MSI融合。我们的框架是样本意识的，并且通过对抗学习来优化一个增强网络和两个下采样网络。广泛的实验表明，我们的ADASR比 estado-of-the-art 方法更有效。Note: "干扰特征图像" (HSI) is short for "hyperspectral image", and "多spectral图像" (MSI) is short for "multispectral image".
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation"><a href="#A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation" class="headerlink" title="A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation"></a>A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07252">http://arxiv.org/abs/2310.07252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashid Khan, Bingding Huang, Haseeb Hassan, Asim Zaman, Zhongfu Ye</li>
<li>for: 这篇论文主要针对图像描述 зада务，旨在使用计算机视觉和自然语言处理技术生成图像描述文本。</li>
<li>methods: 该方法使用了多个预训练的卷积神经网络作为Encoder来提取图像特征，并使用GRU基于的语言模型作为Decoder来生成描述句。另外，我们还采用了Bahdanau注意力机制与GRU嵌入式语言模型进行学习关注特定图像部分。</li>
<li>results: 我们在MSCOCO和Flickr30k datasets上进行评估，并显示了与当前方法相当的分数。我们的提议的框架可以bridge计算机视觉和自然语言之间的差距，并可以扩展到特定领域。<details>
<summary>Abstract</summary>
Image captioning is a challenging task involving generating a textual description for an image using computer vision and natural language processing techniques. This paper proposes a deep neural framework for image caption generation using a GRU-based attention mechanism. Our approach employs multiple pre-trained convolutional neural networks as the encoder to extract features from the image and a GRU-based language model as the decoder to generate descriptive sentences. To improve performance, we integrate the Bahdanau attention model with the GRU decoder to enable learning to focus on specific image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets and show that it achieves competitive scores compared to state-of-the-art methods. Our proposed framework can bridge the gap between computer vision and natural language and can be extended to specific domains.
</details>
<details>
<summary>摘要</summary>
Image 描述是一个复杂的任务，它使用计算机视觉和自然语言处理技术来生成一个图像的文本描述。这篇论文提出了一种深度神经网络框架，用于图像描述生成。我们的方法使用多个预训练的卷积神经网络作为编码器，从图像中提取特征，并使用 GRU 语言模型作为解码器，生成详细的句子。为了提高性能，我们将 Bahdanau 注意力模型与 GRU 解码器结合使用，允许学习关注特定的图像部分。我们使用 MSCOCO 和 Flickr30k 数据集进行评估，并显示了与州际方法相当的分数。我们的提议的框架可以跨越计算机视觉和自然语言之间的差距，并可以扩展到特定领域。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset"><a href="#Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset" class="headerlink" title="Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset"></a>Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07250">http://arxiv.org/abs/2310.07250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Ethem Hamamci</li>
<li>for: 这个研究旨在使用生成 adversarial network (GAN) Synthesize missing glioblastoma MRI sequence, 以便临床医生能够更好地诊断和治疗 brain tumor.</li>
<li>methods: 我们使用了 open-source GAN 方法，让我们可以将任何三个 MRI 序列作为输入，生成缺失的第四个构造序列。</li>
<li>results: 我们的实验结果显示，我们的方法可以生成高质量和实际的 MRI 序列，帮助临床医生提高诊断能力，并支持 AI 方法应用于 brain tumor MRI 量化。<details>
<summary>Abstract</summary>
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing high-quality and realistic MRI sequences, enabling clinicians to improve their diagnostic capabilities and support the application of AI methods to brain tumor MRI quantification.
</details>
<details>
<summary>摘要</summary>
高级肿瘤性肿瘤（Glioblastoma）是脑肿瘤的一种高度致命的形式。核磁共振成像（MRI）在诊断、治疗规划和监测高级肿瘤患者中扮演着非常重要的角色，因为它无需侵入性和辐射。国际脑肿瘤分 segmentation（BraTS）挑战对于精准和高效地分 segment glioblastoma 下层组织使用四种结构 MRI 扫描（T1、T1Gd、T2、T2-FLAIR）提供了许多人工智能算法。然而，这四种 MRI 扫描可能不总是可用。为解决这个问题，生成敌对网络（GANs）可以用来生成缺失的 MRI 扫描。在这篇论文中，我们实现了一种开源 GAN 方法，该方法使用任何三种 MRI 扫描作为输入，以生成缺失的第四种结构 MRI 扫描。我们的提议方法被添加到了社区驱动的通用 nuanced deep learning 框架（GaNDLF）中，并在生成高质量和真实的 MRI 扫描方面达到了有 promise 的结果，使临床医生可以提高诊断能力，并支持脑肿瘤 MRI 量化的应用。
</details></li>
</ul>
<hr>
<h2 id="IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors"><a href="#IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors" class="headerlink" title="IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors"></a>IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07248">http://arxiv.org/abs/2310.07248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Wang, Qiang Hu, Hongkuan Shi, Li He, Man He, Wenxuan Dai, Ting Li, Yitong Zhang, Dun Li, Mei Liu, Qiang Li</li>
<li>for: 这篇论文的目的是提出一种基于盒子指导的肿瘤分 segmentation方法，以提高肿瘤分 segmentation的精度和效率。</li>
<li>methods: 这篇论文使用了两种新的学习方法：Improved Box-dice（IBox）和Contrastive Latent-Anchors（CLA），并将这两种方法组合使用来训练一个 Robust 的盒子指导分 segmentation模型。IBox 方法将 segmentation 图像转换成一个代理图像，然后使用形态分离和异常区域交换来隔离形态和位置&#x2F;大小信息。CLA 方法生成了两种类型的隐藏栅栏，并使用振荡和分割肿瘤来更新隐藏栅栏，从而使模型更好地捕捉肿瘤和背景特征。</li>
<li>results: 这篇论文对五个公共的肿瘤分 datasets 进行了比较，结果表明 IBoxCLA 与最近的完全监督肿瘤分 segmentation方法相比，有着竞争性的性能，并且与其他盒子指导 state-of-the-arts 相比，具有至少6.5%和7.5%的全部 mDice 和 mIoU 的提升。<details>
<summary>Abstract</summary>
Box-supervised polyp segmentation attracts increasing attention for its cost-effective potential. Existing solutions often rely on learning-free methods or pretrained models to laboriously generate pseudo masks, triggering Dice constraint subsequently. In this paper, we found that a model guided by the simplest box-filled masks can accurately predict polyp locations/sizes, but suffers from shape collapsing. In response, we propose two innovative learning fashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and combine them to train a robust box-supervised model IBoxCLA. The core idea behind IBoxCLA is to decouple the learning of location/size and shape, allowing for focused constraints on each of them. Specifically, IBox transforms the segmentation map into a proxy map using shape decoupling and confusion-region swapping sequentially. Within the proxy map, shapes are disentangled, while locations/sizes are encoded as box-like responses. By constraining the proxy map instead of the raw prediction, the box-filled mask can well supervise IBoxCLA without misleading its shape learning. Furthermore, CLA contributes to shape learning by generating two types of latent anchors, which are learned and updated using momentum and segmented polyps to steadily represent polyp and background features. The latent anchors facilitate IBoxCLA to capture discriminative features within and outside boxes in a contrastive manner, yielding clearer boundaries. We benchmark IBoxCLA on five public polyp datasets. The experimental results demonstrate the competitive performance of IBoxCLA compared to recent fully-supervised polyp segmentation methods, and its superiority over other box-supervised state-of-the-arts with a relative increase of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.
</details>
<details>
<summary>摘要</summary>
《 Box-supervised 肿瘤分割吸引了越来越多的注意，因为它的成本效果很高。现有的解决方案 often 采用学习无关的方法或预训练模型，以生成 pseudo masks，从而触发 dice 约束。在这篇论文中，我们发现一个由 simplest box-filled masks 导向的模型可以准确预测肿瘤的位置/大小，但是受到形态塌陷的影响。为了解决这个问题，我们提出了两种创新的学习方式，Improved Box-dice（IBox）和 Contrastive Latent-Anchors（CLA），并将它们结合使用来训练一个 Robust box-supervised 模型 IBoxCLA。IBoxCLA 的核心思想是将学习位置/大小和形态的学习分解，以便对它们进行专门的约束。具体来说，IBox 将 segmentation 图像转换成一个代理图像，然后在代理图像中进行形态塌陷和混淆区域的交替处理。在代理图像中，形态和位置/大小都被解耦，而 box-like 响应被编码。通过约束代理图像而不是直接约束 raw prediction，可以使 box-filled mask 良好地指导 IBoxCLA 不会误导它的形态学习。此外，CLA 对 shape learning 做出了贡献，通过生成两种类型的潜在锚点，使 IBoxCLA 可以在对比的方式中捕捉肿瘤和背景特征。这些潜在锚点通过旋转和划分肿瘤和背景来学习和更新。我们对 IBoxCLA 在五个公共肿瘤数据集上进行了Benchmark。实验结果表明 IBoxCLA 与最近的完全监督肿瘤分割方法相比，表现竞争力强，与其他 box-supervised 状态的至少6.5%和7.5%的全局 mDice 和 mIoU 相比，表现出了明显的提升。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving"><a href="#Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving" class="headerlink" title="Optimizing the Placement of Roadside LiDARs for Autonomous Driving"></a>Optimizing the Placement of Roadside LiDARs for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07247">http://arxiv.org/abs/2310.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu</li>
<li>for: 这篇论文目的是优化路边LiDAR的布置，以提高自动驾驶车辆的感知性能。</li>
<li>methods: 该论文提出了一种基于贪婪算法的方法，通过遍历场景中的不同位置，选择最佳的LiDAR布置，以提高感知性能。该方法基于感知增强，定义感知增强为在新LiDAR添加后，感知能力的提高。</li>
<li>results: 该论文使用了一个名为Roadside-Opt的数据集，通过使用单个点云帧来评估LiDAR布置的影响，并提出了一种基于PointNet的感知预测器，以估计LiDAR布置的影响。<details>
<summary>Abstract</summary>
Multi-agent cooperative perception is an increasingly popular topic in the field of autonomous driving, where roadside LiDARs play an essential role. However, how to optimize the placement of roadside LiDARs is a crucial but often overlooked problem. This paper proposes an approach to optimize the placement of roadside LiDARs by selecting optimized positions within the scene for better perception performance. To efficiently obtain the best combination of locations, a greedy algorithm based on perceptual gain is proposed, which selects the location that can maximize the perceptual gain sequentially. We define perceptual gain as the increased perceptual capability when a new LiDAR is placed. To obtain the perception capability, we propose a perception predictor that learns to evaluate LiDAR placement using only a single point cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator to facilitate research on the roadside LiDAR placement problem.
</details>
<details>
<summary>摘要</summary>
多智能机器人合作感知是自驾车领域中日益受欢迎的话题，路边LiDAR在这一领域扮演着关键性的角色。然而，如何优化路边LiDAR的布局是一个重要但经常被忽略的问题。本文提出了一种方法来优化路边LiDAR的布局，通过选择场景中最佳位置来提高感知性能。为了效率地获得最佳组合，我们提出了一种基于探索性赢得的排序算法，该算法可以顺序选择最大化探索性的位置。我们定义了探索性为在新加LiDAR处理后提高的感知能力。为了获得感知能力，我们提出了一种感知预测器，该预测器可以通过单个点云帧来评估LiDAR布局。为了促进路边LiDAR布局问题的研究，我们创建了名为Roadside-Opt的数据集，该数据集使用CARLA simulate器生成。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs"><a href="#Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs" class="headerlink" title="Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs"></a>Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07245">http://arxiv.org/abs/2310.07245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</li>
<li>for: 这 paper 的目的是提高人群计数的准确性和可靠性，特别在具有雾、尘埃和低光照的环境下。</li>
<li>methods: 这 paper 使用的方法是使用 Pix2Pix 生成器对人群图像进行预处理，以提高计数模型的性能。</li>
<li>results: 测试结果表明，使用 Pix2Pix 生成器可以提高人群计数模型在具有雾、尘埃和低光照的环境下的性能，并且可以提供高度可靠和准确的人群计数结果。<details>
<summary>Abstract</summary>
Visual crowd counting estimates the density of the crowd using deep learning models such as convolution neural networks (CNNs). The performance of the model heavily relies on the quality of the training data that constitutes crowd images. In harsh weather such as fog, dust, and low light conditions, the inference performance may severely degrade on the noisy and blur images. In this paper, we propose the use of Pix2Pix generative adversarial network (GAN) to first denoise the crowd images prior to passing them to the counting model. A Pix2Pix network is trained using synthetic noisy images generated from original crowd images and then the pretrained generator is then used in the inference engine to estimate the crowd density in unseen, noisy crowd images. The performance is tested on JHU-Crowd dataset to validate the significance of the proposed method particularly when high reliability and accuracy are required.
</details>
<details>
<summary>摘要</summary>
“视觉人群计数”使用深度学习模型，如卷积神经网络（CNN）来估算人群密度。模型的性能受训练数据的质量的影响很大，而训练数据通常是来自人群图像的。在恶劣天气条件下，如雾、尘埃和低光照下，推断性能可能受到图像噪声和模糊的影响，导致推断结果减少。在这篇论文中，我们提议使用 Pix2Pix 生成整型网络（GAN）来首先减少人群图像中的噪声，然后将减少后的图像传递给计数模型进行计数。 Pix2Pix 网络在训练过程中使用自动生成的噪声图像，然后在推断引擎中使用预训练的生成器来估算人群密度。我们在JHU-Crowd数据集上测试了该方法，以验证该方法在需要高可靠性和准确性时的效果是否显著。
</details></li>
</ul>
<hr>
<h2 id="SAGE-ICP-Semantic-Information-Assisted-ICP"><a href="#SAGE-ICP-Semantic-Information-Assisted-ICP" class="headerlink" title="SAGE-ICP: Semantic Information-Assisted ICP"></a>SAGE-ICP: Semantic Information-Assisted ICP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07237">http://arxiv.org/abs/2310.07237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Cui, Jiming Chen, Liang Li</li>
<li>for: 提高雷达器在未知环境中的稳定和准确pose进行预测</li>
<li>methods: 利用有效semantic信息，提出了一种名为SAGE-ICP的新的semantic-assisted ICP方法，其中 semantics在odaometry中得到了抽象</li>
<li>results: 对KITTI和KITTI-360进行了实验评估，与基准方法相比，方法可以在大规模场景中提高localization精度，并且可以在实时性要求下保持高速运行<details>
<summary>Abstract</summary>
Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate.
</details>
<details>
<summary>摘要</summary>
Robust和准确的姿态估算在未知环境中是机器人应用中的关键部分。我们关注LiDAR基于点对点ICP的方法，并利用有效的semantic信息。这篇论文提出了一种基于semantic信息的ICP方法，称为SAGE-ICP，它在odometry中利用semantic信息。整个扫描的semantic信息在时间上是有效的和高效地提取，并且这些点 wise标签在注册过程中深度参与到每一个部分中，包括semantic精度下采样、数据关联、自适应本地地图和动态车辆除去。与前一些semantic援助方法不同，我们的方法可以在大规模场景中提高姿态准确性，即使semantic信息有一定的错误。实验评估在KITTI和KITTI-360上表明，我们的方法在准确性和实时性之间取得了平衡，即 faster than sensor frame rate。
</details></li>
</ul>
<hr>
<h2 id="AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation"><a href="#AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation"></a>AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07236">http://arxiv.org/abs/2310.07236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</li>
<li>for: 该 paper 的目的是提出一种基于 speech-driven 3D 面部动画的方法，以生成同时与驱动语音同步的面部运动，并具有个性化的 talking style。</li>
<li>methods: 该 paper 使用的方法包括 mixture-of-low-rank adaptation (MoLoRA) 和 discrete pose prior，以有效地捕捉面部表达和姿势样式。</li>
<li>results: 对比其他方法，该 paper 的方法能够更好地保持 Referenced 视频中的 talking style，并生成更为生动的面部动画。<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available at https://adamesh.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将语音驱动的3D面部动画翻译成中文简体版本。</SYS>>现有研究主要忽略了个人特有的说话样式在生成中，包括表情和头部姿态样式。一些研究尝试通过细化模块来捕捉个人风格。然而，受限于培训数据的限制，生成的表情和头部姿态具有假性。在这项工作中，我们提出了AdaMesh，一种新的适应语音驱动面部动画方法，可以从约10秒的参考视频中学习个人化说话风格，并生成生动的表情和头部姿态。具体来说，我们提出了 Mixture-of-low-rank adaptation（MoLoRA）来细化表情适应器，以高效地捕捉表情风格。而为个人化姿态风格，我们提出了姿态适应器，通过建立离散姿态先验和使用具有语义意识的姿态风格矩阵而不需要细化来获取相应的风格嵌入。广泛的实验结果表明，我们的方法在比较 estado-of-the-art 方法的基础上具有优势，保留参考视频中的说话风格，并生成了生动的面部动画。补充视频和代码将在 <https://adamesh.github.io> 上公开。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data"><a href="#Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data" class="headerlink" title="Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data"></a>Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07223">http://arxiv.org/abs/2310.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrodriguezortega/msmtu">https://github.com/jrodriguezortega/msmtu</a></li>
<li>paper_authors: José Rodríguez-Ortega, Rohaifa Khaldi, Domingo Alcaraz-Segura, Siham Tabik</li>
<li>for: 这个论文的目的是提出一种基于深度学习模型的多спектраль时间序列数据无结构光谱分解方法，用于提取杂合 pixel 中的不同Land Use和Land Cover类型和相应的含量。</li>
<li>methods: 该方法使用了深度学习模型，包括Long-Short Term Memory（LSTM）模型，并在模型中添加了地ографи�技术和气象 ancillary 信息，以提高LULC类别的含量估计。</li>
<li>results: 实验表明，将spectral-temporal输入数据与地ографи�技术和气象 ancillary 信息结合在一起，可以substantially improve LULC类别的含量估计在杂合 pixel 中。<details>
<summary>Abstract</summary>
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data together with geo-topographic and climatic information substantially improves the abundance estimation of LULC classes in mixed pixels. To carry out this study, we built a new labeled dataset of the region of Andalusia (Spain) with monthly multispectral time series of pixels for the year 2013 from MODIS at 460m resolution, for two hierarchical levels of LULC classes, named Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides, at the pixel level, a multispectral time series plus ancillary information annotated with the abundance of each LULC class inside each pixel. The dataset and code are available to the public.
</details>
<details>
<summary>摘要</summary>
remote 感知数据受到杂合用陆地和陆地覆盖类型（LULC）的影响。spectral 无杂解决方案可以提取杂合像素中的各种LULC类型和相应的含量。传统上，解决这个任务需要 Either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data together with geo-topographic and climatic information substantially improves the abundance estimation of LULC classes in mixed pixels. To carry out this study, we built a new labeled dataset of the region of Andalusia (Spain) with monthly multispectral time series of pixels for the year 2013 from MODIS at 460m resolution, for two hierarchical levels of LULC classes, named Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides, at the pixel level, a multispectral time series plus ancillary information annotated with the abundance of each LULC class inside each pixel. The dataset and code are available to the public.
</details></li>
</ul>
<hr>
<h2 id="Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model"><a href="#Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model" class="headerlink" title="Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model"></a>Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07222">http://arxiv.org/abs/2310.07222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysy31415/unipaint">https://github.com/ysy31415/unipaint</a></li>
<li>paper_authors: Shiyuan Yang, Xiaodong Chen, Jing Liao</li>
<li>for: 用于实现多modal的图像填充和修饰，提供不同类型的指导方式，包括无条件、文本驱动、触感驱动和示例驱动的填充方法，以及这些模式的组合。</li>
<li>methods: 基于静态扩散的稳定扩散模型（Stable Diffusion），不需要任务特定的训练，可以通过少量的示例来实现多modal的敏捷适应。</li>
<li>results: 通过对多种图像填充和修饰任务进行质量和量化评估，显示了我们的方法可以与单 modal 方法匹配的效果，同时提供了多 modal 填充和修饰的可能性。<details>
<summary>Abstract</summary>
Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have demonstrated impressive image generation capabilities and have also been successfully applied to image inpainting. However, in practice, users often require more control over the inpainting process beyond textual guidance, especially when they want to composite objects with customized appearance, color, shape, and layout. Unfortunately, existing diffusion-based inpainting methods are limited to single-modal guidance and require task-specific training, hindering their cross-modal scalability. To address these limitations, we propose Uni-paint, a unified framework for multimodal inpainting that offers various modes of guidance, including unconditional, text-driven, stroke-driven, exemplar-driven inpainting, as well as a combination of these modes. Furthermore, our Uni-paint is based on pretrained Stable Diffusion and does not require task-specific training on specific datasets, enabling few-shot generalizability to customized images. We have conducted extensive qualitative and quantitative evaluations that show our approach achieves comparable results to existing single-modal methods while offering multimodal inpainting capabilities not available in other methods. Code will be available at https://github.com/ysy31415/unipaint.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:最近，文本到图像的干净抽象概率模型（DDPM）已经展示了印象万能的图像生成能力，并且也成功应用于图像填充。然而，在实践中，用户经常需要更多的控制来指导填充过程，特别是当他们想要根据自己的需求自定义对象的外观、颜色、形状和布局时。 Unfortunately，现有的扩散基于的填充方法受到单一模式的导航限制，需要任务特定的训练，这限制了它们的跨模式可扩展性。为了解决这些限制，我们提议Uni-paint，一个通用的多模式填充框架，它提供了不同的导航模式，包括无条件、文本驱动、roke驱动、示例驱动填充，以及这些模式的组合。此外，我们的Uni-paint基于预训练的稳定扩散，不需要任务特定的训练，可以在特定数据集上进行几步扩展，实现个性化图像填充。我们进行了详细的质量和量化评估，表明我们的方法可以与现有的单模式方法相比，同时提供多模式填充能力不可用于其他方法。代码将在https://github.com/ysy31415/unipaint中提供。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-Explainable-Skin-Lesion-Classification"><a href="#Multi-task-Explainable-Skin-Lesion-Classification" class="headerlink" title="Multi-task Explainable Skin Lesion Classification"></a>Multi-task Explainable Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07209">http://arxiv.org/abs/2310.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahapara Khurshid, Mayank Vatsa, Richa Singh</li>
<li>for: 针对皮肤癌症的早期诊断，提高诊断精度和速度。</li>
<li>methods: 提出了一种多任务几个样本基于方法，结合分割网络作为注意模块和分类网络，并将分割和分类损失相乘Weighted。</li>
<li>results: 对三个皮肤癌症数据集进行了全面评估，实验结果表明该方法有效。<details>
<summary>Abstract</summary>
Skin cancer is one of the deadliest diseases and has a high mortality rate if left untreated. The diagnosis generally starts with visual screening and is followed by a biopsy or histopathological examination. Early detection can aid in lowering mortality rates. Visual screening can be limited by the experience of the doctor. Due to the long tail distribution of dermatological datasets and significant intra-variability between classes, automatic classification utilizing computer-aided methods becomes challenging. In this work, we propose a multitask few-shot-based approach for skin lesions that generalizes well with few labelled data to address the small sample space challenge. The proposed approach comprises a fusion of a segmentation network that acts as an attention module and classification network. The output of the segmentation network helps to focus on the most discriminatory features while making a decision by the classification network. To further enhance the classification performance, we have combined segmentation and classification loss in a weighted manner. We have also included the visualization results that explain the decisions made by the algorithm. Three dermatological datasets are used to evaluate the proposed method thoroughly. We also conducted cross-database experiments to ensure that the proposed approach is generalizable across similar datasets. Experimental results demonstrate the efficacy of the proposed work.
</details>
<details>
<summary>摘要</summary>
皮肤癌是一种非常危险的疾病，如果不得到治疗，死亡率会非常高。诊断通常从视觉检查开始，然后是比opsy或 histopathological examination。早期发现可以降低死亡率。 however， visual screening 可能受医生的经验限制。由于皮肤病学数据集的长尾分布和类别之间的显著差异，自动分类使用计算机辅助方法变得困难。在这种情况下，我们提出了一种多任务少量数据基于的方法，可以在皮肤病学数据集中进行有效的分类。我们的方法包括一个 segmentation 网络作为注意力模块，以及一个分类网络。 segmentation 网络的输出帮助分类网络做出决定，同时也帮助分类网络专注于最有特征的特征。为了进一步提高分类性能，我们将 segmentation 和分类损失相乘，并在权重的情况下进行融合。我们还提供了算法做出决定的视觉结果，以便更好地了解算法的决策逻辑。我们使用了三个皮肤病学数据集来评估我们的提案，并进行了跨数据集的测试，以确保我们的方法可以在类似数据集上普遍适用。实验结果表明，我们的方法具有较高的效果。
</details></li>
</ul>
<hr>
<h2 id="DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation"><a href="#DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation" class="headerlink" title="DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation"></a>DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07206">http://arxiv.org/abs/2310.07206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rong Wang, Wei Mao, Hongdong Li</li>
<li>for: 该 paper  targets the task of 3D pose estimation for a hand interacting with an object, using a single image observation.</li>
<li>methods: 该 paper 使用了 DeepSimHO，一种新的深度学习管道， combinign forward physics simulation and backward gradient approximation with a neural network.</li>
<li>results:  compared with previous works, DeepSimHO  noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization.Here are the three points in English for reference:</li>
<li>for: The paper targets the task of 3D pose estimation for a hand interacting with an object from a single image observation.</li>
<li>methods: The paper uses DeepSimHO, a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network.</li>
<li>results: Compared with previous works, DeepSimHO noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization.<details>
<summary>Abstract</summary>
This paper addresses the task of 3D pose estimation for a hand interacting with an object from a single image observation. When modeling hand-object interaction, previous works mainly exploit proximity cues, while overlooking the dynamical nature that the hand must stably grasp the object to counteract gravity and thus preventing the object from slipping or falling. These works fail to leverage dynamical constraints in the estimation and consequently often produce unstable results. Meanwhile, refining unstable configurations with physics-based reasoning remains challenging, both by the complexity of contact dynamics and by the lack of effective and efficient physics inference in the data-driven learning framework. To address both issues, we present DeepSimHO: a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network. Specifically, for an initial hand-object pose estimated by a base network, we forward it to a physics simulator to evaluate its stability. However, due to non-smooth contact geometry and penetration, existing differentiable simulators can not provide reliable state gradient. To remedy this, we further introduce a deep network to learn the stability evaluation process from the simulator, while smoothly approximating its gradient and thus enabling effective back-propagation. Extensive experiments show that our method noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization. The code is available at https://github.com/rongakowang/DeepSimHO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition"><a href="#SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition" class="headerlink" title="SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition"></a>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07189">http://arxiv.org/abs/2310.07189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwei Ren, Yue Zhou, Yulong Huang, Haotian Fu, Xiaopeng Lin, Jie Song, Bojun Cheng</li>
<li>for: Event-based action recognition tasks, specifically achieving ultra-low power consumption and high accuracy with sparse event data.</li>
<li>methods: Proposes a novel end-to-end point-based SNN architecture called SpikePoint, which effectively extracts global and local features through a singular-stage structure, and leverages surrogate training to achieve high accuracy with few parameters.</li>
<li>results: Achieves state-of-the-art performance on four event-based action recognition datasets using only 16 timesteps, with approximately 0.3% of the parameters and 0.5% of power consumption employed by artificial neural networks (ANNs).<details>
<summary>Abstract</summary>
Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on four event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3\% of the parameters and 0.5\% of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications.
</details>
<details>
<summary>摘要</summary>
事件摄像机是生物发现的感知器，响应当地辐射强度的变化，具有低延迟、高能效率和高 dinamic 范围。同时，神经元逻辑网络（SNN）已引起广泛关注，因其非常高效和抗错能力。通过将事件摄像机和 SNN 的能效特性相互融合，可以实现超低功耗应用场景，如行为识别任务。然而，现有方法通常需要将异步事件转换为传统的帧，从而导致额外的数据映射努力和数据损失，与 SNN 和事件摄像机的设计概念相 contradiction。为解决这个挑战，我们提出了 SpikePoint，一种新的终端点基 SNN 架构。SpikePoint 能够高效处理稀疏事件云数据，通过单阶段结构提取全局和局部特征。通过使用代理训练方法，SpikePoint 在多个数据集上实现高精度，只使用少量参数，并保持低功耗。具体来说，SpikePoint 在四个事件基据集上实现了状态当前（SOTA）性能，使用仅 16 个时间步骤，超过了其他 SNN 方法。此外，它还在三个数据集上实现了 SOTA 性能，使用约 0.3% 的参数和 0.5% 的功耗，与人工神经网络（ANN）相比。这些结果证明 Point Cloud 的重要性，并开阔了许多超低功耗事件基据处理应用场景的先河。
</details></li>
</ul>
<hr>
<h2 id="NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations"><a href="#NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations" class="headerlink" title="NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations"></a>NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07184">http://arxiv.org/abs/2310.07184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeongjoonju/neuroinspect">https://github.com/yeongjoonju/neuroinspect</a></li>
<li>paper_authors: Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee</li>
<li>for:  This paper aims to provide an interpretable neuron-based debugging framework for deep learning (DL) models to help practitioners interpret the decision-making process within the networks and identify the responsible neurons for mistakes.</li>
<li>methods:  The proposed debugging framework consists of three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. It uses a novel feature visualization method called CLIP-Illusion to generate images representing features conditioned on classes, which helps provide more human-interpretable explanations for model errors without altering the trained network or requiring additional data.</li>
<li>results:  The proposed framework is validated through evaluation in real-world settings, demonstrating its effectiveness in addressing false correlations and improving inferences for classes with the worst performance. Additionally, the paper shows that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding.<details>
<summary>Abstract</summary>
Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. We alleviate convoluted explanations of the conventional visualization approach by employing class information, thereby isolating mixed properties. This process offers more human-interpretable explanations for model errors without altering the trained network or requiring additional data. Furthermore, our framework mitigates false correlations learned from a dataset under a stochastic perspective, modifying decisions for the neurons considered as the main causes. We validate the effectiveness of our framework by addressing false correlations and improving inferences for classes with the worst performance in real-world settings. Moreover, we demonstrate that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding. The code is openly available at https://github.com/yeongjoonJu/NeuroInspect.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）已经取得了各种领域的显著进步，但DL模型仍然容易出错。这个问题需要有效的调试工具，以便DL实践者可以解释网络做出决策的过程。然而，现有的调试方法通常需要额外的数据或者网络决策的修改，限制了其应用性。为解决这个问题，我们提出了NeuroInspect，一个可解释的神经元基于的调试框架。我们的调试框架首先在网络中标识出负责错误的神经元，然后使用CLIP-Illusion novel feature visualization方法来生成可被人类理解的图像，以便检查神经元和决策层之间的连接。我们采用类信息来缓解传统视觉方法中的混乱解释，从而提供更加人类可理解的错误解释。此外，我们的框架还可以避免由数据下的杂合所学习的假相关性，从而修改神经元被视为主要原因的决策。我们验证了NeuroInspect的有效性，通过在真实情况下修复类型错误和提高各类型的推理。此外，我们还证明了NeuroInspect可以帮助调试DL模型的错误。代码可以在https://github.com/yeongjoonJu/NeuroInspect上下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models"><a href="#Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models" class="headerlink" title="Improving mitosis detection on histopathology images using large vision-language models"></a>Improving mitosis detection on histopathology images using large vision-language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07176">http://arxiv.org/abs/2310.07176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiwen Ding, James Hall, Neil Tenenholtz, Kristen Severson</li>
<li>for: 这个研究旨在提高肿瘤检测的准确性，使用了大规模的视觉语言模型，同时利用视觉特征和自然语言来减少人工评分的主观性。</li>
<li>methods: 该研究使用了卷积神经网络（CNN）来检测肿瘤，并利用了 metadata such as 肿瘤和扫描器类型作为上下文。</li>
<li>results: 研究结果表明，使用大规模的视觉语言模型可以提高肿瘤检测的准确性，并且比拥有基准模型的性能更高。<details>
<summary>Abstract</summary>
In certain types of cancerous tissue, mitotic count has been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, convolutional neural networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
</details>
<details>
<summary>摘要</summary>
某些types of cancerous tissue中， Mitotic count有 been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, Convolutional Neural Networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
</details></li>
</ul>
<hr>
<h2 id="Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent"><a href="#Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent" class="headerlink" title="Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent"></a>Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07166">http://arxiv.org/abs/2310.07166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyuan Ou, Siwei Wang, Pei Zhang, Sihang Zhou, En Zhu</li>
<li>for: 多视角嵌入 clustering 的研究在 recent 年来受到了越来越多的关注，因为它可以融合不同来源的信息并且在公共事务中具有承诺的前途。</li>
<li>methods: 我们提出了一种基于 Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) 的方法，通过一种统一的采样策略在相似空间中进行嵌入 clustering，从而降低了计算复杂性至线性时间成本。</li>
<li>results: 我们的提出的模型在多个公共 benchmark 数据集上进行了广泛的实验，并 consistently 超越了当前状态的技术。<details>
<summary>Abstract</summary>
Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time complexity w.r.t. the number of the objects. However, we propose Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to further reduce the computing complexity to linear time cost through a unified sampling strategy in resemblance space( STAGE 2), followed by subspace clustering to learn the representation collectively( STAGE 3). Extensive experimental results on public benchmark datasets demonstrate that our proposed model consistently outperforms the state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
多视角划分已经吸引了越来越多的关注，因为它可以聚合不同来源的信息并且在公共事务中具有替代的前景。到目前为止，许多先进的方法已经在最新的文献中提出。然而，在不同视角之间匹配特征的问题仍然很大。我们通过层次特征递减来解决这个问题，从而创造了一个共同的潜在空间（Stage 1）。这个潜在空间被称为“相似空间”，因为它暴露了不同视角之间的相似性和依赖关系。此外，由于大多数现有的多视角划分算法来自于k-means划分和спектраль划分，这会导致立方体时间复杂度与对象数成正比。然而，我们提出了 anchor-based multi-view subspace clustering with hierarchical feature descent（MVSC-HFD），以减少计算复杂度到线性时间成本，通过一种统一采样策略在相似空间（Stage 2），然后通过subspace clustering来学习共同表示（Stage 3）。我们的提出的模型在公共标准数据集上进行了广泛的实验，并 consistently 超越了当前技术的状态。
</details></li>
</ul>
<hr>
<h2 id="Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation"><a href="#Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation" class="headerlink" title="Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation"></a>Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07149">http://arxiv.org/abs/2310.07149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hye-Seong Hong, Abhishek Kumar, Dong-Gyu Lee</li>
<li>for: 本研究旨在提高无监督领域适应的性能，尤其是准确地分割 объекts boundaries。</li>
<li>methods: 该研究提出了一种新的领域适应方法，利用内部和外部信息的共同作用，增强预测器网络的准确性。具体来说，该方法通过在推知器网络中添加 Edge 的概率值，以提高分割结果的清晰度。此外，该方法还提出了一种概率分享网络，用于更有效地进行分割。</li>
<li>results: 实验结果表明，该方法在不同的无监督领域适应场景中具有优于STATE-OF-THE-ART 方法的性能。这表明该方法具有较好的一致性和灵活性。<details>
<summary>Abstract</summary>
The generalization capability of unsupervised domain adaptation can mitigate the need for extensive pixel-level annotations to train semantic segmentation networks by training models on synthetic data as a source with computer-generated annotations. Entropy-based adversarial networks are proposed to improve source domain prediction; however, they disregard significant external information, such as edges, which have the potential to identify and distinguish various objects within an image accurately. To address this issue, we introduce a novel approach to domain adaptation, leveraging the synergy of internal and external information within entropy-based adversarial networks. In this approach, we enrich the discriminator network with edge-predicted probability values within this innovative framework to enhance the clarity of class boundaries. Furthermore, we devised a probability-sharing network that integrates diverse information for more effective segmentation. Incorporating object edges addresses a pivotal aspect of unsupervised domain adaptation that has frequently been neglected in the past -- the precise delineation of object boundaries. Conventional unsupervised domain adaptation methods usually center around aligning feature distributions and may not explicitly model object boundaries. Our approach effectively bridges this gap by offering clear guidance on object boundaries, thereby elevating the quality of domain adaptation. Our approach undergoes rigorous evaluation on the established unsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Mapillary. Experimental results show that the proposed model attains better performance than state-of-the-art methods. The superior performance across different unsupervised domain adaptation scenarios highlights the versatility and robustness of the proposed method.
</details>
<details>
<summary>摘要</summary>
通用化能力可以减少需要大量像素级别的标注来训练Semantic segmentation网络，通过训练模型使用计算机生成的数据作为源，并使用计算机生成的标注。但是，基于 entropy 的对抗网络可能会忽略一些重要的外部信息，如对象边缘，这些信息可以准确地识别和分辨不同的对象。为解决这个问题，我们提出了一种新的领域适应方法，利用内部和外部信息的协同在 entropy 基于对抗网络中增强类boundary的明确性。此外，我们还设计了一种概率分享网络，将多种信息集成到更有效的分类中。在这种方法中，涉及对象边缘的信息可以准确地识别对象的边界，从而提高领域适应的质量。传统的领域适应方法通常是将特征分布 align，可能不会显式地模型对象边界。我们的方法可以准确地 bridge 这一鸿 gap，并提供明确的对象边界指导，从而提高领域适应的质量。我们的方法在已知的领域适应benchmark上进行了严格的评估，包括 SYNTHIA 到 Cityscapes 和 SYNTHIA 到 Mapillary。实验结果表明，我们的方法可以在不同的领域适应场景中表现出比州际的性能。这种灵活性和可靠性表明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model"><a href="#Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model" class="headerlink" title="Echocardiography video synthesis from end diastolic semantic map via diffusion model"></a>Echocardiography video synthesis from end diastolic semantic map via diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07131">http://arxiv.org/abs/2310.07131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phi Nguyen Van, Duc Tran Minh, Hieu Pham Huy, Long Tran Quoc<br>for:本研究旨在使用Diffusion Probabilistic Models（DDPMs）生成基于semantic anatomical信息的echocardiography视频，以解决现有数据集的约束，lacking comprehensive frame-wise annotations for every cardiac cycle。methods:本研究扩展了现有的视频Diffusion模型，使其能够使用semantic maps of the initial frame during the cardiac cycle，通常称为end diastole。我们还 integrate spatial adaptive normalization into multiscale feature maps，以包含semantic guidance during synthesis，提高生成的视频序列的realism和coherence。results:我们在CAMUS数据集上进行了实验，并证明了我们的模型在多个指标（包括FID、FVD和SSMI）上表现比标准扩散技术更好。<details>
<summary>Abstract</summary>
Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated significant achievements in various image and video generation tasks, including the domain of medical imaging. However, generating echocardiography videos based on semantic anatomical information remains an unexplored area of research. This is mostly due to the constraints imposed by the currently available datasets, which lack sufficient scale and comprehensive frame-wise annotations for every cardiac cycle. This paper aims to tackle the aforementioned challenges by expanding upon existing video diffusion models for the purpose of cardiac video synthesis. More specifically, our focus lies in generating video using semantic maps of the initial frame during the cardiac cycle, commonly referred to as end diastole. To further improve the synthesis process, we integrate spatial adaptive normalization into multiscale feature maps. This enables the inclusion of semantic guidance during synthesis, resulting in enhanced realism and coherence of the resultant video sequences. Experiments are conducted on the CAMUS dataset, which is a highly used dataset in the field of echocardiography. Our model exhibits better performance compared to the standard diffusion technique in terms of multiple metrics, including FID, FVD, and SSMI.
</details>
<details>
<summary>摘要</summary>
德函数泛化模型（DDPM）已经在不同的图像和视频生成任务中显示出了很大的成果，包括医疗影像领域。然而，基于 semantics  анатомиче信息生成echocardiography 视频仍然是一个未经探索的领域，主要是因为目前可用的数据集缺乏大规模的整个律动周期的框架级别注释。这篇论文目的是通过扩展现有的视频泛化模型来解决以上挑战。我们更具体地是在cardiac cycle 的起始帧semantic maps中进行视频生成。为了进一步改进生成过程，我们将 spatial adaptive normalization integrated into multiscale feature maps，以实现在生成过程中的semantic guidance，从而提高生成的视频序列的真实感和一致性。我们的模型在CAMUS数据集上进行了实验，并与标准泛化技术进行了比较。我们的模型在多个指标上（包括FID、FVD和SSMI）表现出了更好的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.CV_2023_10_11/" data-id="clnsn0vh700djgf88b97q3gut" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/11/eess.AS_2023_10_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-10-11
        
      </div>
    </a>
  
  
    <a href="/2023/10/11/cs.AI_2023_10_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-10-11</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">22</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
