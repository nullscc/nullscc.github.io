
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-10-10 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.06554 repo_url: None paper_authors: Mattes Ohlenbusch, Christ">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-10-10">
<meta property="og:url" content="https://nullscc.github.io/2023/10/10/cs.SD_2023_10_10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.06554 repo_url: None paper_authors: Mattes Ohlenbusch, Christ">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-10T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:30.413Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.SD_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T15:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-10-10
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Modeling-of-Speech-dependent-Own-Voice-Transfer-Characteristics-for-Hearables-with-In-ear-Microphones"><a href="#Modeling-of-Speech-dependent-Own-Voice-Transfer-Characteristics-for-Hearables-with-In-ear-Microphones" class="headerlink" title="Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones"></a>Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06554">http://arxiv.org/abs/2310.06554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattes Ohlenbusch, Christian Rollwage, Simon Doclo</li>
<li>for: 这篇论文是为了研究听力器中的自己声音传递特性而写的。</li>
<li>methods: 该论文使用了语音认知技术来建立一个语音依赖的系统标定模型，以估计听力器中自己声音的传递特性。</li>
<li>results: 研究发现，使用提议的语音依赖模型可以更好地模拟听力器中的自己声音传递特性，并且比适应 filtering-based 模型更好地适应新的语音。此外，研究还发现，对于不同的说话者，使用 talked-averaged 模型可以更好地泛化到不同的说话者。<details>
<summary>Abstract</summary>
Hearables often contain an in-ear microphone, which may be used to capture the own voice of its user. However, due to ear canal occlusion the in-ear microphone mostly records body-conducted speech, which suffers from band-limitation effects and is subject to amplification of low frequency content. These transfer characteristics are assumed to vary both based on speech content and between individual talkers. It is desirable to have an accurate model of the own voice transfer characteristics between hearable microphones. Such a model can be used, e.g., to simulate a large amount of in-ear recordings to train supervised learning-based algorithms aiming at compensating own voice transfer characteristics. In this paper we propose a speech-dependent system identification model based on phoneme recognition. Using recordings from a prototype hearable, the modeling accuracy is evaluated in terms of technical measures. We investigate robustness of transfer characteristic models to utterance or talker mismatch. Simulation results show that using the proposed speech-dependent model is preferable for simulating in-ear recordings compared to a speech-independent model. The proposed model is able to generalize better to new utterances than an adaptive filtering-based model. Additionally, we find that talker-averaged models generalize better to different talkers than individual models.
</details>
<details>
<summary>摘要</summary>
听ables 经常包含在耳朵中的一个内耳麦克风，可以用来捕捉其用户的自己声音。然而，由于耳朵封闭，内耳麦克风主要记录的是身体传导的语音，这种语音受到频率限制的影响，同时也受到低频强调效果的增强。这些传输特性的变化受到语音内容和个体演说者的影响。因此，有一个准确的自己声音传输特性模型可以用于训练基于supervised learning的算法，以资acia减少自己声音传输特性的影响。在这篇论文中，我们提出了基于phoneme认识的语音依赖系统模型。使用一种原型听ables 的录音，我们评估了模型的准确性，并进行了技术性的评估。我们也研究了语音或演说者之间的模型的稳定性。结果表明，使用我们提出的语音依赖模型在模拟耳朵录音时比使用语音独立模型更好。此外，我们发现了 talker-averaged 模型在不同的演说者之间更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="Topological-data-analysis-of-human-vowels-Persistent-homologies-across-representation-spaces"><a href="#Topological-data-analysis-of-human-vowels-Persistent-homologies-across-representation-spaces" class="headerlink" title="Topological data analysis of human vowels: Persistent homologies across representation spaces"></a>Topological data analysis of human vowels: Persistent homologies across representation spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06508">http://arxiv.org/abs/2310.06508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem Bonafos, Jean-Marc Freyermuth, Pierre Pudlo, Samuel Tronçon, Arnaud Rey</li>
<li>for: 这篇论文是用于研究数据分析方法的，具体来说是研究如何从各种数据表示空间中提取有用的特征，以便进行预测和分类。</li>
<li>methods: 这篇论文使用的方法包括 persistent homology 理论和 topologic 数据分析 (TDA) 技术，以及一些 Machine Learning 算法，如 random forest。</li>
<li>results: 这篇论文的结果表明，使用不同的数据表示空间可以提取到不同的特征，而这些特征之间存在一定的相互补做作用。此外，使用 topologic 数据分析可以提高预测和分类的准确率。<details>
<summary>Abstract</summary>
Topological Data Analysis (TDA) has been successfully used for various tasks in signal/image processing, from visualization to supervised/unsupervised classification. Often, topological characteristics are obtained from persistent homology theory. The standard TDA pipeline starts from the raw signal data or a representation of it. Then, it consists in building a multiscale topological structure on the top of the data using a pre-specified filtration, and finally to compute the topological signature to be further exploited. The commonly used topological signature is a persistent diagram (or transformations of it). Current research discusses the consequences of the many ways to exploit topological signatures, much less often the choice of the filtration, but to the best of our knowledge, the choice of the representation of a signal has not been the subject of any study yet. This paper attempts to provide some answers on the latter problem. To this end, we collected real audio data and built a comparative study to assess the quality of the discriminant information of the topological signatures extracted from three different representation spaces. Each audio signal is represented as i) an embedding of observed data in a higher dimensional space using Taken's representation, ii) a spectrogram viewed as a surface in a 3D ambient space, iii) the set of spectrogram's zeroes. From vowel audio recordings, we use topological signature for three prediction problems: speaker gender, vowel type, and individual. We show that topologically-augmented random forest improves the Out-of-Bag Error (OOB) over solely based Mel-Frequency Cepstral Coefficients (MFCC) for the last two problems. Our results also suggest that the topological information extracted from different signal representations is complementary, and that spectrogram's zeros offers the best improvement for gender prediction.
</details>
<details>
<summary>摘要</summary>
topological数据分析（TDA）已经成功地应用于各种信号/图像处理任务，从视觉化到指导/无指导分类。经常地， topological特征来自 persistente homology理论。TDA管道从原始信号数据或信号表示开始，然后在基于预先指定的筛选器上建立多级 topological结构，最后计算 topological签名以进一步利用。通常使用的 topological签名是持续 diagram（或其变形）。当前研究的问题是 exploit topological签名的多种方法，而不是筛选器的选择，而且尚未考虑信号表示的选择。这篇论文尝试提供一些答案，并通过对实际的音频数据进行比较性研究来评估不同表示空间中的 topological签名的质量。我们使用了三种不同的表示空间来表示每个音频信号：1. 使用 Takens 表示法将数据embedding到高维空间中。2. 视为三维 ambient空间中的表面，使用 spectrogram。3. spectrogram中的 zeros 集。对于女性语音录制，我们使用 topological签名进行三个预测问题：speaker gender、vowel type和个人。我们发现，使用 topologically-augmented random forest 可以在 Out-of-Bag Error（OOB）中提高 Mel-Frequency Cepstral Coefficients（MFCC）的性能。我们的结果还表明，不同的表示空间中的 topological信息是夹带的，而spectrogram中的 zeros 提供了最好的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Cognitive-Consensus-guided-Audio-Visual-Segmentation"><a href="#Cross-modal-Cognitive-Consensus-guided-Audio-Visual-Segmentation" class="headerlink" title="Cross-modal Cognitive Consensus guided Audio-Visual Segmentation"></a>Cross-modal Cognitive Consensus guided Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06259">http://arxiv.org/abs/2310.06259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaofeng Shi, Qingbo Wu, Hongliang Li, Fanman Meng, Linfeng Xu<br>for:* 这篇论文的目的是提出一种 Audio-Visual Segmentation (AVS) 方法，用于从视频帧中提取听到的对象。methods:* 该方法使用 dense feature-level audio-visual interaction，忽略不同模式之间的维度差异。* 使用 Cross-modal Cognitive Consensus guided Network (C3N) align audio-visual semantics 从全Dimension 维度和地进行进一步的注意力机制。results:* 经验表明，该方法可以在 Single Sound Source Segmentation (S4) 和 Multiple Sound Source Segmentation (MS3) 任务上达到状态之最好性能。<details>
<summary>Abstract</summary>
Audio-Visual Segmentation (AVS) aims to extract the sounding object from a video frame, which is represented by a pixel-wise segmentation mask. The pioneering work conducts this task through dense feature-level audio-visual interaction, which ignores the dimension gap between different modalities. More specifically, the audio clip could only provide a \textit{Global} semantic label in each sequence, but the video frame covers multiple semantic objects across different \textit{Local} regions. In this paper, we propose a Cross-modal Cognitive Consensus guided Network (C3N) to align the audio-visual semantics from the global dimension and progressively inject them into the local regions via an attention mechanism. Firstly, a Cross-modal Cognitive Consensus Inference Module (C3IM) is developed to extract a unified-modal label by integrating audio/visual classification confidence and similarities of modality-specific label embeddings. Then, we feed the unified-modal label back to the visual backbone as the explicit semantic-level guidance via a Cognitive Consensus guided Attention Module (CCAM), which highlights the local features corresponding to the interested object. Extensive experiments on the Single Sound Source Segmentation (S4) setting and Multiple Sound Source Segmentation (MS3) setting of the AVSBench dataset demonstrate the effectiveness of the proposed method, which achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
音视频分割（AVS）目标是从视频帧中提取听到的对象，它通过像素级别的音视频交互来实现。在这种情况下，音频片断只能提供一个全局Semantic标签，而视频帧则包含多个不同地方的Semantic对象。在这篇论文中，我们提议一种协调音视频 semantics的网络（C3N），以将全局维度上的音视频 semantics 与本地区域相协调。首先，我们开发了一种协调音视频 Semantic Inference模块（C3IM），以抽取音视频分类信任度和模式特征之间的相似性。然后，我们将这个协调模式标签返回给视频底层，并通过一种协调注意力模块（CCAM）来高亮对应的本地特征。我们对AVSBench数据集的Single Sound Source Segmentation（S4）和Multiple Sound Source Segmentation（MS3）两个设置进行了广泛的实验，并证明了我们的方法的有效性，达到了当前最佳性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.SD_2023_10_10/" data-id="clombedxp00wns08884uy1axn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/11/eess.SP_2023_10_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-10-11
        
      </div>
    </a>
  
  
    <a href="/2023/10/10/eess.AS_2023_10_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-10-10</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">113</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">63</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
