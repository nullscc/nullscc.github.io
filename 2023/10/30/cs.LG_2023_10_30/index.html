
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-10-30 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Subgraph GNNs by Learning Effective Selection Policies paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20082 repo_url: None paper_authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-10-30">
<meta property="og:url" content="https://nullscc.github.io/2023/10/30/cs.LG_2023_10_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Subgraph GNNs by Learning Effective Selection Policies paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20082 repo_url: None paper_authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-30T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.359Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.LG_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T10:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-10-30
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies"><a href="#Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies" class="headerlink" title="Efficient Subgraph GNNs by Learning Effective Selection Policies"></a>Efficient Subgraph GNNs by Learning Effective Selection Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20082">http://arxiv.org/abs/2310.20082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, Haggai Maron</li>
<li>for: 本研究旨在学习从多个子图中学习图表示，但是存在许多可能的子图，选择这些子图是一项 computationally expensive 的任务。</li>
<li>methods: 我们提出了一种新的方法，called Policy-Learn，它通过Iterative manner来学习选择子图。</li>
<li>results: 我们的实验结果表明，Policy-Learn 可以在各种 dataset 上超过现有的基准值。<details>
<summary>Abstract</summary>
Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing baselines across a wide range of datasets.
</details>
<details>
<summary>摘要</summary>
<font face="Times New Roman"><font size="5"><font color="#333333"> SUBGRAPH GNNs 是可证明表示力强的神经网络架构，可以从多个子图中学习图表示。然而，它们的应用受到多个子图计算消息传递的复杂性所限制。在这篇论文中，我们考虑了选择一小集合可能的大量子图的问题。我们首先提出了这个问题的问题，并证明了存在一些 families of WL-indistinguishable graphs 中存在高效的子图选择策略：小 subsets of subgraphs 可以已经识别整个家族中的所有图。然后，我们提出了一种新的方法，叫做 Policy-Learn，可以在循环的方式中学习选择子图。我们证明了，与很多Random policies 和现有的相关工作不同，我们的架构可以学习上述高效的策略。我们的实验结果表明，Policy-Learn 在各种数据集上都超过了现有的基准值。</font></font></font>
</details></li>
</ul>
<hr>
<h2 id="Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors"><a href="#Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors" class="headerlink" title="Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors"></a>Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20079">http://arxiv.org/abs/2310.20079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen M. Wang, Darren T. Garnier, Cristina Rea</li>
<li>for: 这个论文的目的是提高核聚变炉的控制和稳定性，以便使其成为可靠的能源来源。</li>
<li>methods: 这篇论文使用神经网络Ordinary Differential Equations（ODE）框架来预测核聚变炉中的电流和内 inductance 动态。</li>
<li>results: 研究发现，将物理基于的方程和神经网络模型结合在一起，能够更好地预测核聚变炉中的动态，并且比现有的物理驱动的ODE模型和纯神经网络模型性能更高。<details>
<summary>Abstract</summary>
While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model that combines physics-based equations with a neural ODE performs better than both existing physics-motivated ODEs and a pure neural ODE model.
</details>
<details>
<summary>摘要</summary>
tokamak核聚变 реактор显示出可能成为一种可靠的能源来源，但是进一步的材料控制和失控事件处理技术的发展仍然需要进一步。目前的控制算法应用受到更好的材料仿真的限制，现有的物理基于的仿真方法和数据驱动的方法都有局限性。为解决这个问题，本研究使用神经网络Ordinary Differential Equations（ODE）框架来预测材料动态的一部分，即材料电流和内部 inductance 动态。由于神经网络ODE框架具有自然地包含物理基于的 inductive bias，我们在 Alcator C-Mod 核聚变反应堆数据上训练了物理基于的 ODE 模型和神经网络模型，并发现一个结合物理基于的 ODE 和神经网络模型的模型在物理基于的 ODE 模型和纯神经网络模型之间具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery"><a href="#Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery" class="headerlink" title="Meek Separators and Their Applications in Targeted Causal Discovery"></a>Meek Separators and Their Applications in Targeted Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20075">http://arxiv.org/abs/2310.20075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uhlerlab/meek_sep">https://github.com/uhlerlab/meek_sep</a></li>
<li>paper_authors: Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</li>
<li>for: 学习干预结构FROM� intervened数据</li>
<li>methods: 引入Meek separator，一种可以将尚未oriented edges分解为小型连接组件的subset of vertices</li>
<li>results: 提出了两种随机算法，可以在 subset search 和 causal matching 问题上实现 logarithmic approximation 的近似解决方案，并且提供了首次known的平均情况证明 garantuee。<details>
<summary>Abstract</summary>
Learning causal structures from interventional data is a fundamental problem with broad applications across various fields. While many previous works have focused on recovering the entire causal graph, in practice, there are scenarios where learning only part of the causal graph suffices. This is called $targeted$ causal discovery. In our work, we focus on two such well-motivated problems: subset search and causal matching. We aim to minimize the number of interventions in both cases.   Towards this, we introduce the $Meek~separator$, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.
</details>
<details>
<summary>摘要</summary>
Towards this, we introduce the Meek separator, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches.In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.
</details></li>
</ul>
<hr>
<h2 id="Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation"><a href="#Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation" class="headerlink" title="Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation"></a>Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20062">http://arxiv.org/abs/2310.20062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Ramesh, Rui Zhao, Naman Goel<br>for:The paper is written for the purpose of exploring the use of synthetic data in machine learning, with a focus on privacy and trustworthiness.methods:The paper proposes a novel system that utilizes three building blocks: Solid (Social Linked Data), MPC (Secure Multi-Party Computation), and Trusted Execution Environments (TEEs) to generate differentially private synthetic data in a decentralized and scalable manner.results:The paper presents empirical results on simulated and real datasets, demonstrating the effectiveness of the proposed system in addressing various challenges in responsible and trustworthy synthetic data generation, including contributor autonomy, decentralization, privacy, and scalability.<details>
<summary>Abstract</summary>
Synthetic data is emerging as a promising way to harness the value of data, while reducing privacy risks. The potential of synthetic data is not limited to privacy-friendly data release, but also includes complementing real data in use-cases such as training machine learning algorithms that are more fair and robust to distribution shifts etc. There is a lot of interest in algorithmic advances in synthetic data generation for providing better privacy and statistical guarantees and for its better utilisation in machine learning pipelines. However, for responsible and trustworthy synthetic data generation, it is not sufficient to focus only on these algorithmic aspects and instead, a holistic view of the synthetic data generation pipeline must be considered. We build a novel system that allows the contributors of real data to autonomously participate in differentially private synthetic data generation without relying on a trusted centre. Our modular, general and scalable solution is based on three building blocks namely: Solid (Social Linked Data), MPC (Secure Multi-Party Computation) and Trusted Execution Environments (TEEs). Solid is a specification that lets people store their data securely in decentralised data stores called Pods and control access to their data. MPC refers to the set of cryptographic methods for different parties to jointly compute a function over their inputs while keeping those inputs private. TEEs such as Intel SGX rely on hardware based features for confidentiality and integrity of code and data. We show how these three technologies can be effectively used to address various challenges in responsible and trustworthy synthetic data generation by ensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4) scalability. We support our claims with rigorous empirical results on simulated and real datasets and different synthetic data generation algorithms.
</details>
<details>
<summary>摘要</summary>
人工数据正在成为一种有前途的数据利用方式，同时减少隐私风险。人工数据的潜力不仅限于隐私友好数据发布，还包括补充真实数据在使用场景中，如训练不同分布下的机器学习算法等。有很多关注于人工数据生成算法的进步，以提供更好的隐私和统计保证，并更好地在机器学习管道中使用。但是，为了负责任地和可信地生成人工数据，不能仅仅专注于算法方面，而是需要考虑整个人工数据生成管道的各个方面。我们建立了一个新的系统，允许真实数据的contributors自主参与在匿名 differentially private 人工数据生成中，不需要依赖于信任中心。我们的模块化、通用和可扩展解决方案基于以下三个基础 componenets：Solid（社交链接数据）、MPC（安全多方计算）和TEEs（信任执行环境）。Solid是一种规范，允许人们安全地存储他们的数据在分布式数据存储 called Pods 中，控制对他们的数据的访问。MPC是一组 криптографических方法，用于不同党在他们的输入上进行共同计算，而保持这些输入private。TEEs，如 intel SGX，基于硬件特性，提供代码和数据的confidentiality和完整性。我们示出了这三种技术可以有效地解决负责任和可信的人工数据生成中的各种挑战，包括：1）参与者自主权，2）分布化，3）隐私和4）可扩展性。我们支持我们的主张通过对 simulated 和实际数据集和不同的人工数据生成算法进行严格的实验结果来证明。
</details></li>
</ul>
<hr>
<h2 id="AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces"><a href="#AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces" class="headerlink" title="AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces"></a>AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20060">http://arxiv.org/abs/2310.20060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jvictormata/adasub">https://github.com/jvictormata/adasub</a></li>
<li>paper_authors: João Victor Galvão da Mata, Martin S. Andersen</li>
<li>for: 本研究旨在提出一种基于第二阶信息的搜索方向算法，以优化计算成本和算法效率。</li>
<li>methods: 该算法使用随机优化方法，并在低维度子空间中基于当前和历史信息进行适应性定义。</li>
<li>results: 数据结果表明，AdaSub 超过了流行的随机优化器在时间和迭代次数方面达到给定精度所需的成本和效率。<details>
<summary>Abstract</summary>
We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍AdaSub，一种随机优化算法，它基于现有的第二项信息在低维度的子空间中计算搜寻方向。相比于第一项方法，第二项方法在数据数据库中具有更好的征具特性，但是计算Hessian矩阵在每个迭代中的需求导致过度的计算成本，使得它们不实际。为了解决这个问题，我们的方法可以选择搜寻子空间的维度，以管理计算成本和算法效率。我们的代码可以在GitHub上免费下载，我们的初步数据显示AdaSub比流行的随机优化器在时间和迭代数量上优化。
</details></li>
</ul>
<hr>
<h2 id="Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo"><a href="#Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo" class="headerlink" title="Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo"></a>Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20053">http://arxiv.org/abs/2310.20053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Szilvia Ujváry, Gergely Flamich, Vincent Fortuin, José Miguel Hernández Lobato</li>
<li>for: 本文探讨了在PAC-Bayes理论中，如何在 posterior family 中强制 Gaussian 分布时，失去的紧张程度。</li>
<li>methods: 本文使用了优化的 Gibbs  posterior，使用 Hamiltonian Monte Carlo 采样，并使用 thermodynamic integration 来估算 KL 差。</li>
<li>results: 实验结果表明，使用优化的 posterior 可以减少紧张程度，在一些情况下可以达到 5-6% 的差异。<details>
<summary>Abstract</summary>
An important yet underexplored question in the PAC-Bayes literature is how much tightness we lose by restricting the posterior family to factorized Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this issue by estimating data-independent PAC-Bayes bounds using the optimal posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1) sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2) estimate its KL divergence from the prior with thermodynamic integration, and (3) propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset reveal significant tightness gaps, as much as 5-6\% in some cases.
</details>
<details>
<summary>摘要</summary>
“一个重要但未得到充分探讨的问题在PAC-Bayes文献中是：当我们将 posterior 家族限制为分布式 Gaussian 分布时，我们会失去多少紧密性？我们调查这个问题，使用最佳 Gibbs  posterior，与 MFVI  Comparing  them, we propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset show significant tightness gaps, as much as 5-6% in some cases.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Expressibility-of-Polynomial-based-Attention-Scheme"><a href="#The-Expressibility-of-Polynomial-based-Attention-Scheme" class="headerlink" title="The Expressibility of Polynomial based Attention Scheme"></a>The Expressibility of Polynomial based Attention Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20051">http://arxiv.org/abs/2310.20051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Guangyi Xu, Junze Yin</li>
<li>for: 本研究旨在理解幂数注意 Mechanism的表达能力。</li>
<li>methods: 我们使用两个特意设计的数据集，分别是 $\mathcal{D}_0$ 和 $\mathcal{D}_1$，并通过单层幂数注意网络分类这两个数据集来进行分析。</li>
<li>results: 我们的分析发现，当幂数degree高 enough时，单层幂数注意网络可以有效地分类 $\mathcal{D}_0$ 和 $\mathcal{D}_1$。但是，当幂数degree低时，网络无法有效地分类这两个数据集。这种分析表明高度幂数可以更好地强调大值，并提供了在幂数注意 Mechanism中包含更高级幂数的理由。<details>
<summary>Abstract</summary>
Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.   In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability of high-degree and low-degree polynomial attention. Specifically, we construct two carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where $\mathcal{D}_1$ includes a feature with a significantly larger value compared to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree $\beta$, a single-layer polynomial attention network can distinguish between $\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the network cannot effectively separate the two datasets. This analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets. Our analysis offers insight into the representational capacity of polynomial attention and provides a rationale for incorporating higher-degree polynomials in attention mechanisms to capture intricate linguistic correlations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经对我们日常生活中的多个领域产生了重要影响，包括医疗、教育和决策等。这些模型不仅提高了生产力和决策过程，而且也提高了存取性。因此，它们已经影响了和改变了人们的生活方式。然而， трансформа器架构中的对话复杂度问题对于处理长文本背景时仍然是一个挑战。这个问题使得训练非常大的模型或在处理长文本时使用它们不可避免地成为问题。在一篇最近发表的研究《[KMZ23]》中，提出了一种替代使用多项函数和概率卷积来加速对话机制的技术。然而，这新的方法的理论基础仍然未被彻底了解。在这篇论文中，我们提供了对于多项函数的表达能力的理论分析。我们的研究显示，高度的多项函数能够更好地增强大值，并且可以让单层多项函数网络分辨不同的数据集。具体来说，我们创建了两个特别设计的数据集，分别为$\mathcal{D}_0$和$\mathcal{D}_1$。 $\mathcal{D}_1$ 包含了一个具有许多更大的特征值，相比于 $\mathcal{D}_0$。我们展示了，透过调整$\beta$的高度，单层多项函数网络可以对 $\mathcal{D}_0$ 和 $\mathcal{D}_1$ 进行分辨。然而，在低度$\beta$下，这个网络无法有效地分辨这两个数据集。这一分析显示出高度的多项函数在增强大值和分辨不同数据集方面的更好表现。我们的分析给出了对于多项函数的表达能力的理论基础，并且提供了将更高度的多项函数包含在对话机制中以捕捉复杂的语言相关性的理论基础。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Riemannian-Diffusion-Models"><a href="#Scaling-Riemannian-Diffusion-Models" class="headerlink" title="Scaling Riemannian Diffusion Models"></a>Scaling Riemannian Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20030">http://arxiv.org/abs/2310.20030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louaaron/Scaling-Riemannian-Diffusion">https://github.com/louaaron/Scaling-Riemannian-Diffusion</a></li>
<li>paper_authors: Aaron Lou, Minkai Xu, Stefano Ermon</li>
<li>for: 学习扩展到普通的欧几丁素空间之外的扩展分布（Riemannian diffusion models），以便在普通多槽中进行分布学习。</li>
<li>methods: 提出了一些实用的改进方法，包括使用各种 Ansatz 来快速计算相关量，以及利用Symmetric spaces的特点来提高计算精度。</li>
<li>results: 在低维数据集上，提出的改进方法能够提供明显的改进，使扩散方法在对抗其他方法的比赛中表现出色，并且在高维任务上也能够成功应用。<details>
<summary>Abstract</summary>
Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ans\"{a}tze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on $SU(n)$ lattices and contrastively learned embeddings on high dimensional hyperspheres.
</details>
<details>
<summary>摘要</summary>
里曼尼扩散模型 Draws inspiration from标准Euclidean空间扩散模型 To learn distribution on general manifolds. Unfortunately, the additional geometric complexity makes the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds areSymmetric spaces, which are much more amenable to computation. By leveraging and combining various ansätze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on $SU(n)$ lattices and contrastively learned embeddings on high dimensional hyperspheres.Note: Simplified Chinese is a romanization of Chinese, and the translation may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices"><a href="#PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices" class="headerlink" title="PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices"></a>PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19991">http://arxiv.org/abs/2310.19991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Yan, Hongyi Wang, Shivaram Venkataraman</li>
<li>for: 这篇论文旨在调查使用硬件组件的配置方式对神经网络（NN）的推理过程中的能源消耗。</li>
<li>methods: 该论文使用了受限的权重优化算法来优化硬件组件的配置，以保持能源消耗的最佳平衡。</li>
<li>results: 实验结果显示，PolyThrottle可以将 популяр的模型的能源消耗减少至36%，并且可以快速地 converges到近似优化的设置，满足应用程序的约束。<details>
<summary>Abstract</summary>
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 在多个领域部署时，其能量需求也随之增长。虽然先前的研究主要关注在训练阶段减少能量消耗，但是 ML 搭建在运行阶段的能量使用也是非常重要的。这篇论文研究了在设备硬件元素配置，如 GPU、内存和 CPU 频率等，在 NN 推理过程中对能量消耗的影响。我们提出了 PolyThrottle，一种解决方案，通过受限的 Bayesian 优化来优化硬件元素的配置，以节省能量。我们的实验证明，PolyThrottle 可以保持最佳配置，同时满足应用程序的限制。我们的实验结果还表明，PolyThrottle 可以在减少训练时间的情况下，将能量消耗减少到 36% 左右。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations"><a href="#Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations" class="headerlink" title="Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations"></a>Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19978">http://arxiv.org/abs/2310.19978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Raff, Amol Khanna, Fred Lu</li>
<li>for: 这个论文是为了提出一种可以在单簇输入数据上训练具有数据隐私的回归模型。</li>
<li>methods: 这个论文使用了Frank-Wolfe算法，并将其修改以适应单簇输入数据，并且使用了这些单簇输入来优化算法的训练时间。</li>
<li>results: 这个论文的结果显示，使用这种方法可以将训练时间从 $\mathcal{O}( T D S + T N S)$ 降至 $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$，具体取决于隐私参数 $\epsilon$ 和资料集的绝对簇率 $S$。这个结果显示了这种方法可以将训练时间降低到最多 $2,200\times$。<details>
<summary>Abstract</summary>
To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
</details>
<details>
<summary>摘要</summary>
“根据我们所知，现在没有训练具有零入力数据的差异化公式 regression 模型的方法。为了解决这个问题，我们将 Frank-Wolfe 算法 для $L_1$ 责任 linear regression 修改来考虑 sparse input，并将其使用有效地。这将training时间从 $\mathcal{O}(TD+TNS)$ 降至 $\mathcal{O}(NS+T\sqrt{D}\log{D}+TS^2)$，其中 $T$ 是迭代次数，$S$ 是资料集中的简洁率，$N$ 是资料集中的行数，$D$ 是资料集中的特征数。我们的结果显示，这个程序可以将runtime降低到最多 $2,200\times$，具体取决于隐私参数 $\epsilon$ 和资料集的简洁率。”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy"><a href="#Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy" class="headerlink" title="Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy"></a>Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19973">http://arxiv.org/abs/2310.19973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, Weijie J. Su</li>
<li>for: This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP.</li>
<li>methods: The paper derives a closed-form expression of the trade-off function for shuffling models and investigates the effects of random initialization on the privacy of one-iteration DP-GD.</li>
<li>results: The paper shows that random initialization can enhance the privacy of DP-GD and derives a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文关注于改进洗牌模型和一轮权值私有化梯度下降（DP-GD）的随机初始化影响私有性保证。</li>
<li>methods: 论文 derive了洗牌模型的减少函数的封闭式表述，并 investigate了随机初始化对DP-GD的私有性影响。</li>
<li>results: 论文显示随机初始化可以增强DP-GD的私有性，并 derive了洗牌模型的减少函数封闭式表述，超过当前最佳的($\epsilon$,$\delta$)-DP结果。<details>
<summary>Abstract</summary>
Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an inequality for trade-off functions introduced in this paper. This inequality implies the joint convexity of $F$-divergences. Finally, we study an $f$-DP analog of the advanced joint convexity of the hockey-stick divergence related to $(\epsilon,\delta)$-DP and apply it to analyze the privacy of mixture mechanisms.
</details>
<details>
<summary>摘要</summary>
Diffitionally private（DP）机器学习算法中存在多种Randomness，如随机初始化，随机批处理抽样和排序。然而，这些随机性很难在证明批处理保密约束时进行考虑，因为它们会导致算法输出的混合分布，难以分析。这篇论文关注于提高洗混模型和一轮批处理批处理批处理批处理批处理的隐私约束，使用$f$-DP。我们得到了洗混模型的封闭形式的贸易函数表达，超过目前最佳的($\epsilon$, $\delta$)-DP结果。此外，我们还研究了随机初始化对DP-GD的隐私的影响。我们的数值计算表明，随机初始化可以增强DP-GD的隐私。我们的分析表明，$f$-DP保证对这些混合机制的隐私性具有较好的性质。 finally，我们研究了$f$-DP中的进阶共轭性，并应用其分析洗混机制的隐私性。Note: Simplified Chinese is also known as "Mandarin" Chinese, and it is the official language of the People's Republic of China. It is written using the same characters as Traditional Chinese, but with simpler stroke order and fewer characters.
</details></li>
</ul>
<hr>
<h2 id="Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records"><a href="#Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records" class="headerlink" title="Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records"></a>Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19967">http://arxiv.org/abs/2310.19967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Wang, Weizi Li, Anthony Bradlow, Antoni T. Y. Chan, Eghosa Bazuaye</li>
<li>for: 早期识别急性关节炎 (IA) 是预测病人能够在有限的医疗资源下得到及时治疗和避免疾病诊断的关键。</li>
<li>methods: 我们使用多模态数据 fusion 和 ensemble learning 技术来支持早期识别IA。</li>
<li>results: 我们的研究表明，使用多模态数据可以提高早期识别IA的精度和效果，并且可以帮助医生更快地做出诊断。<details>
<summary>Abstract</summary>
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and ensemble learning-based methods using multimodal data to assist decision-making in the early detection of IA. To the best of our knowledge, our study is the first attempt to utilize multimodal data to support the early detection of IA from GP referrals.
</details>
<details>
<summary>摘要</summary>
早期检测Inflammatory Arthritis（IA）是关键，以实现有效和准确的医疗机构推荐，以及防止IA疾病趋势加剧，特别是在医疗资源有限的情况下。现在，手动评估是在实践中最常见的方法，但它具有很大的劳动成本和不fficient。每次从普通医生（GP） referring 到医院，需要评估大量的临床信息。机器学习表明了很大的潜力，可以自动进行重复的评估任务，并为IA早期检测提供决策支持。然而，大多数机器学习基于IA检测方法都依赖于血液测试结果。但在实践中，血液测试数据不总是在提交病人时可以获得，因此我们需要使用多modal数据来支持IA早期检测。在这项研究中，我们提出了 fusione和ensemble学习方法，使用多modal数据来帮助决策。到我们所知，这是第一次使用多modal数据来支持IA早期检测。
</details></li>
</ul>
<hr>
<h2 id="Topological-Learning-for-Motion-Data-via-Mixed-Coordinates"><a href="#Topological-Learning-for-Motion-Data-via-Mixed-Coordinates" class="headerlink" title="Topological Learning for Motion Data via Mixed Coordinates"></a>Topological Learning for Motion Data via Mixed Coordinates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19960">http://arxiv.org/abs/2310.19960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrluo/topologicalmotionseries">https://github.com/hrluo/topologicalmotionseries</a></li>
<li>paper_authors: Hengrui Luo, Jisu Kim, Alice Patania, Mikael Vejdemo-Johansson</li>
<li>for: 本研究旨在吸收批处理数据中的结构信息，并在转移学习中使用多输出 Gaussian process 模型。</li>
<li>methods: 作者提出了一种基于混合类别坐标的新框架，以处理时间序列中的直线趋势。同时，他们还使用 topological induced clustering 构建一个基于群集的函数kernel，以便更好地学习多个时间序列。</li>
<li>results: 研究人员通过实验表明，使用这种方法可以更好地捕捉时间序列的结构信息，并且在转移学习中提高模型的性能。<details>
<summary>Abstract</summary>
Topology can extract the structural information in a dataset efficiently. In this paper, we attempt to incorporate topological information into a multiple output Gaussian process model for transfer learning purposes. To achieve this goal, we extend the framework of circular coordinates into a novel framework of mixed valued coordinates to take linear trends in the time series into consideration.   One of the major challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose to use topologically induced clustering to construct a cluster based kernel in a multiple output Gaussian process model. This kernel not only incorporates the topological structural information, but also allows us to put forward a unified framework using topological information in time and motion series.
</details>
<details>
<summary>摘要</summary>
One of the challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose using topologically induced clustering to construct a cluster-based kernel in the model. This kernel not only incorporates topological structural information but also allows us to present a unified framework using topological information in time and motion series.
</details></li>
</ul>
<hr>
<h2 id="PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning"><a href="#PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning" class="headerlink" title="PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning"></a>PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19958">http://arxiv.org/abs/2310.19958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou</li>
<li>for: 这个论文旨在 investigate  federated learning (FL) 中的模型缩减技术对隐私保护的影响。</li>
<li>methods: 本论文使用 information-theoretic upper bounds 和 comprehensive experiments 来研究模型缩减对隐私保护的影响。</li>
<li>results: 研究结果表明，模型缩减可以提供一定的隐私保护，但是不同的缩减策略和参数可能会影响这种保护的效果。在这基础之上， authors 提出了一种名为 PriPrune 的隐私意识 algorithm，可以在 client 端应用于任何缩减 FL 算法，并在尝试隐私攻击时提供了更好的隐私保护。<details>
<summary>Abstract</summary>
Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.   In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with comprehensive experiments that involve state-of-the-art privacy attacks, on several state-of-the-art FL pruning schemes, using benchmark datasets. This evaluation provides valuable insights into the choices and parameters that can affect the privacy protection provided by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware algorithm for local model pruning, which uses a personalized per-client defense mask and adapts the defense pruning rate so as to jointly optimize privacy and model performance. PriPrune is universal in that can be applied after any pruned FL scheme on the client, without modification, and protects against any inversion attack by the server. Our empirical evaluation demonstrates that PriPrune significantly improves the privacy-accuracy tradeoff compared to state-of-the-art pruned FL schemes that do not take privacy into account.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种 paradigm，allowing several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However, this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks. In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings with comprehensive experiments that involve state-of-the-art privacy attacks, on several state-of-the-art FL pruning schemes, using benchmark datasets. This evaluation provides valuable insights into the choices and parameters that can affect the privacy protection provided by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware algorithm for local model pruning, which uses a personalized per-client defense mask and adapts the defense pruning rate so as to jointly optimize privacy and model performance. PriPrune is universal in that it can be applied after any pruned FL scheme on the client, without modification, and protects against any inversion attack by the server. Our empirical evaluation demonstrates that PriPrune significantly improves the privacy-accuracy tradeoff compared to state-of-the-art pruned FL schemes that do not take privacy into account.
</details></li>
</ul>
<hr>
<h2 id="The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks"><a href="#The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks" class="headerlink" title="The Acquisition of Physical Knowledge in Generative Neural Networks"></a>The Acquisition of Physical Knowledge in Generative Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19943">http://arxiv.org/abs/2310.19943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross32768/PlaNet_PyTorch">https://github.com/cross32768/PlaNet_PyTorch</a></li>
<li>paper_authors: Luca M. Schulze Buschoff, Eric Schulz, Marcel Binz</li>
<li>for:  investigate how the learning trajectories of deep generative neural networks compare to children’s developmental trajectories using physical understanding as a testbed</li>
<li>methods:  using physical understanding as a testbed, examine two distinct hypotheses of human development - stochastic optimization and complexity increase</li>
<li>results:  find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.<details>
<summary>Abstract</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
</details>
<details>
<summary>摘要</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.Here's the translation in Traditional Chinese: As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
</details></li>
</ul>
<hr>
<h2 id="Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller"><a href="#Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller" class="headerlink" title="Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller"></a>Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19938">http://arxiv.org/abs/2310.19938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saiedeh Akbari, Emily J. Griffis, Omkar Sudhir Patil, Warren E. Dixon</li>
<li>for: 论文目的是提出一种基于深度神经网络（DNN）的适应控制器，用于资料处理系统中的不结构化不确定性补做。</li>
<li>methods: 论文使用了Dropout正则化技术，在训练过程中随机禁用节点，以避免过拟合和共适应问题。同时，文章提出了基于Lyapunov函数的实时权重更新方法，以进行在线不监督学习。</li>
<li>results:  simulations 表明，与基线控制器相比，提出的Dropout DNN-based适应控制器可以提高跟踪错误率38.32%，函数适应错误率53.67%，控制努力50.44%。<details>
<summary>Abstract</summary>
Deep neural network (DNN)-based adaptive controllers can be used to compensate for unstructured uncertainties in nonlinear dynamic systems. However, DNNs are also very susceptible to overfitting and co-adaptation. Dropout regularization is an approach where nodes are randomly dropped during training to alleviate issues such as overfitting and co-adaptation. In this paper, a dropout DNN-based adaptive controller is developed. The developed dropout technique allows the deactivation of weights that are stochastically selected for each individual layer within the DNN. Simultaneously, a Lyapunov-based real-time weight adaptation law is introduced to update the weights of all layers of the DNN for online unsupervised learning. A non-smooth Lyapunov-based stability analysis is performed to ensure asymptotic convergence of the tracking error. Simulation results of the developed dropout DNN-based adaptive controller indicate a 38.32% improvement in the tracking error, a 53.67% improvement in the function approximation error, and 50.44% lower control effort when compared to a baseline adaptive DNN-based controller without dropout regularization.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)深度神经网络（DNN）基于的自适应控制器可以补偿非结构性不确定性在非线性动态系统中。然而，DNN也很易于过拟合和共适应。Dropout常量化是一种approach，其中在训练过程中随机dropout节点以解决过拟合和共适应问题。在这篇论文中，我们开发了dropout DNN基于的自适应控制器。我们的dropout技术允许每个层内的权重随机停用。同时，我们引入了基于Lyapunov的实时重量更新法，以更新DNN所有层的重量进行在线无监督学习。我们通过非稀 Ligado-based稳定分析，确保追踪错误的极限收敛。实验结果表明，与基线 adaptive DNN基于控制器无dropout常量化相比，我们的dropout DNN基于自适应控制器可以提高追踪错误38.32%，功能近似错误53.67%，控制努力50.44%。
</details></li>
</ul>
<hr>
<h2 id="Sim2Real-for-Environmental-Neural-Processes"><a href="#Sim2Real-for-Environmental-Neural-Processes" class="headerlink" title="Sim2Real for Environmental Neural Processes"></a>Sim2Real for Environmental Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19932">http://arxiv.org/abs/2310.19932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonas-scholz123/sim2real-downscaling">https://github.com/jonas-scholz123/sim2real-downscaling</a></li>
<li>paper_authors: Jonas Scholz, Tom R. Andersson, Anna Vaughan, James Requeima, Richard E. Turner</li>
<li>for: 本研究旨在提高天气预测和气候监测的准确性，通过使用机器学习（ML）模型来利用观测数据。</li>
<li>methods: 本研究使用了一种名为ConvCNP的 convolutional conditional neural process（ConvCNP）模型，该模型可以conditioning on both gridded和off-the-grid context data来做uncertainty-aware预测。</li>
<li>results: 研究发现，通过使用Sim2Real方法（pre-training on reanalysis和 fine-tuning on observational data），可以substantially improve Surface air temperature interpolation performance over Germany， compared to only using reanalysis data or only using station data。<details>
<summary>Abstract</summary>
Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest in training ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations. However, the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is 'Sim2Real': pre-training on reanalysis and fine-tuning on observational data. We analyse Sim2Real with a ConvCNP trained to interpolate surface air temperature over Germany, using varying numbers of weather stations for fine-tuning. On held-out weather stations, Sim2Real training substantially outperforms the same model architecture trained only with reanalysis data or only with station data, showing that reanalysis data can serve as a stepping stone for learning from real observations. Sim2Real could thus enable more accurate models for weather prediction and climate monitoring.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）基于天气模型最近几年发展迅速。这些模型通常通过数值数据吸收系统获得格子化再分析数据训练。然而，再分析数据存在一些限制，如物理法律假设和低空时间分辨率。这导致了使用直接训练ML模型于天气站数据的增加兴趣。模型散布和罕见环境观测数据需要扩展和灵活的ML体系结构，其中之一是卷积条件神经过程（ConvCNP）。ConvCNP可以通过对格子和非格子上下文数据进行条件学习来预测目标位置的不确定性。然而，实际观测数据的稀缺性对深度学习模型如ConvCNP进行挑战。一个可能的解决方案是“Sim2Real”：先在再分析数据上进行预训练，然后在观测数据上进行微调。我们对德国的表面温度 interpolating 使用了一个ConvCNP，并使用不同数量的天气站进行微调。在保留的天气站上，Sim2Real 训练显著超过了同样的模型结构只使用再分析数据或只使用站数据进行训练，这表明了再分析数据可以作为学习实际观测数据的步骤。Sim2Real 可能可以实现更准确的天气预测和气候监测。
</details></li>
</ul>
<hr>
<h2 id="Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning"><a href="#Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning" class="headerlink" title="Solving a Class of Cut-Generating Linear Programs via Machine Learning"></a>Solving a Class of Cut-Generating Linear Programs via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19920">http://arxiv.org/abs/2310.19920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Rajabalizadeh, Danial Davarnia</li>
<li>for: 提高杂 integer  программирования的解法</li>
<li>methods: 使用机器学习 Approximate CGLP 来选择加入分支和约束树的优秀节点</li>
<li>results: 使用该方法可以提高解法的效率和精度，并且可以应用到大量的节点中<details>
<summary>Abstract</summary>
Cut-generating linear programs (CGLPs) play a key role as a separation oracle to produce valid inequalities for the feasible region of mixed-integer programs. When incorporated inside branch-and-bound, the cutting planes obtained from CGLPs help to tighten relaxations and improve dual bounds. However, running the CGLPs at the nodes of the branch-and-bound tree is computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes. As a result, CGLPs are often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds. In this paper, we propose a novel framework based on machine learning to approximate the optimal value of a CGLP class that determines whether a cutting plane can be generated at a node of the branch-and-bound tree. Translating the CGLP as an indicator function of the objective function vector, we show that it can be approximated through conventional data classification techniques. We provide a systematic procedure to efficiently generate training data sets for the corresponding classification problem based on the CGLP structure. We conduct computational experiments on benchmark instances using classification methods such as logistic regression. These results suggest that the approximate CGLP obtained from classification can improve the solution time compared to that of conventional cutting plane methods. Our proposed framework can be efficiently applied to a large number of nodes in the branch-and-bound tree to identify the best candidates for adding a cut.
</details>
<details>
<summary>摘要</summary>
刻生成线性程序（CGLP）在杂integer程序的分解 oracle 中发挥关键作用，生成有效的不等式来缩小relaxation 和提高对偶下界。然而，在branch-and-bound 树中运行 CGLP  computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes。因此，CGLP  often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds。在这篇论文中，我们提出了一种基于机器学习的新框架，用于approximate CGLP 的优化值。我们将 CGLP 表示为目标函数 вектор 的指示函数，并示出可以通过 convential data classification techniques 来approximate it。我们还提供了一种系统的进程来生成相关的训练数据集，以便应用 classification 方法来解决相关的分类问题。我们在 benchmark instances 上进行了计算实验，使用 logistic regression 等分类方法，结果表明，使用我们的框架可以提高解决时间。我们的提posed framework可以高效地应用到 branch-and-bound 树中的大量节点，以确定最佳的加法点。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks"><a href="#Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks" class="headerlink" title="Meta-Learning Strategies through Value Maximization in Neural Networks"></a>Meta-Learning Strategies through Value Maximization in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19919">http://arxiv.org/abs/2310.19919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Carrasco-Davis, Javier Masís, Andrew M. Saxe</li>
<li>for: 这个论文的目的是 investigate optimal strategies for learning control in deep networks, and provide a tractable theoretical test bed to study normative benefits of interventions in various learning systems.</li>
<li>methods: 这个论文使用了 average dynamical equations for gradient descent, 以及一种 learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning.</li>
<li>results: 研究发现，在不同的学习设置下，控制努力在早期学习 easier aspects of a task 是最有利的，然后坚持努力 harder aspects of the task. 此外，研究还发现了一些关于 optimal curricula 和 neuronal resource allocation 的结论。<details>
<summary>Abstract</summary>
Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting. We apply this framework to investigate the effect of approximations in common meta-learning algorithms; infer aspects of optimal curricula; and compute optimal neuronal resource allocation in a continual learning setting. Across settings, we find that control effort is most beneficial when applied to easier aspects of a task early in learning; followed by sustained effort on harder aspects. Overall, the learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories posited by established theories in cognitive neuroscience.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate optimal strategies in a tractable setting. We develop a learning effort framework that can efficiently optimize control signals based on a fully normative objective: discounted cumulative performance throughout learning. We use average dynamical equations for gradient descent, which are available for simple neural network architectures, to achieve computational tractability. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting.We apply our framework to investigate the effect of approximations in common meta-learning algorithms, infer aspects of optimal curricula, and compute optimal neuronal resource allocation in a continual learning setting. Our results show that control effort is most beneficial when applied to easier aspects of a task early in learning, followed by sustained effort on harder aspects.Overall, our learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories as posited by established theories in cognitive neuroscience.
</details></li>
</ul>
<hr>
<h2 id="GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models"><a href="#GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models" class="headerlink" title="GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models"></a>GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19915">http://arxiv.org/abs/2310.19915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwon Kim, Parisa Mollaei, Akshay Antony, Rishikesh Magar, Amir Barati Farimani</li>
<li>for: 本研究旨在开发一种基于Transformers和大语言模型（LLMs）的G Protein-Coupled Receptors（GPCRs）模型，以便更好地理解GPCRs的Sequential设计。</li>
<li>methods: 本研究使用了 pré-trained protein模型（Prot-Bert），并通过对变化的掩码任务进行精度调整，以提高预测隐藏的残基的精度。此外，还利用了模型的注意力权重和隐藏状态，以EXTRACT隐藏的残基的贡献。</li>
<li>results: 研究发现，通过对模型的预测结果进行分析，可以了解GPCRs的靶点残基与掩码的关系，以及一些保守的残基组合（如NPxxY、CWxP、E&#x2F;DRY）的作用。此外，还可以利用3D结构分析来描述GPCRs的高级相互作用。<details>
<summary>Abstract</summary>
With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved motifs. To achieve this, we took advantage of attention weights, and hidden states of the model that are interpreted to extract the extent of contributions of amino acids in dictating the type of masked ones. The fine-tuned models demonstrated high accuracy in predicting hidden residues within the motifs. In addition, the analysis of embedding was performed over 3D structures to elucidate the higher-order interactions within the conformations of the receptors.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:随着Transformers和Large Language Models（LLMs）在化学和生物领域的普及，新的 Avenues for the design and understanding of therapeutics 已经开放给科学社区。蛋白序列可以被视为语言，可以利用最近的LLMs的进步，特别是我们对蛋白序列数据的丰富访问。在这篇论文中，我们开发了GPCR-BERT模型，用于理解G蛋白coupled receptors（GPCRs）的序列设计。GPCRs 是FDA批准药品的目标之一，但是还没有充分了解蛋白序列、 Ligand selectivity 和 conformational motifs（如NPxxY、CWxP、E/DRY）之间的关系。我们利用预训练的蛋白模型（Prot-Bert）和预测任务的变化来 fins-tune GPCRs 的序列设计。通过利用模型的注意力权重和隐藏状态，我们可以提取蛋白结构中各个残基的贡献程度，以及它们如何决定遮盖的残基。 fine-tuned 模型在预测隐藏在抑制motifs中的残基上达到了高精度。此外，我们还使用了三维结构的分析，以描述GPCRs 的高级相互作用。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions"><a href="#Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions" class="headerlink" title="Bayesian Simulation-based Inference for Cosmological Initial Conditions"></a>Bayesian Simulation-based Inference for Cosmological Initial Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19910">http://arxiv.org/abs/2310.19910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian List, Noemi Anau Montel, Christoph Weniger</li>
<li>for: 用于重construct astrophysical和cosmological场的观测数据</li>
<li>methods: 使用 Bayesian场重建算法，基于模拟基于推理和自适应模型</li>
<li>results: 在一个证明性应用中，成功地 recovered cosmological initial conditions from late-time density fields<details>
<summary>Abstract</summary>
Reconstructing astrophysical and cosmological fields from observations is challenging. It requires accounting for non-linear transformations, mixing of spatial structure, and noise. In contrast, forward simulators that map fields to observations are readily available for many applications. We present a versatile Bayesian field reconstruction algorithm rooted in simulation-based inference and enhanced by autoregressive modeling. The proposed technique is applicable to generic (non-differentiable) forward simulators and allows sampling from the posterior for the underlying field. We show first promising results on a proof-of-concept application: the recovery of cosmological initial conditions from late-time density fields.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将astro物理和cosmology领域中的场景重建为观测数据是一项复杂的任务。这需要考虑非线性变换、空间结构的混合以及噪声。相比之下，向前的模拟器可以快速地将场景映射到观测数据上。我们介绍了一种可靠的bayesian场景重建算法，基于 simulations-based inference 和自动关联模型。该算法适用于通用（非�ifferentiable）向前模拟器，并允许采样 posterior 中的下面场景。我们在一个证明性应用中表明了这种方法的首次成果： cosmological initial condition 的恢复从 late-time density fields。Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic. Please let me know if you have any further questions or requests.
</details></li>
</ul>
<hr>
<h2 id="BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours"><a href="#BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours" class="headerlink" title="BTRec: BERT-Based Trajectory Recommendation for Personalized Tours"></a>BTRec: BERT-Based Trajectory Recommendation for Personalized Tours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19886">http://arxiv.org/abs/2310.19886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nxh912/BTRec_RecSys23">https://github.com/nxh912/BTRec_RecSys23</a></li>
<li>paper_authors: Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim</li>
<li>For: 提供个性化旅游路线建议，使旅游者在不熟悉的城市中享有愉悦的旅行体验。* Methods: 使用BERT框架，结合用户个人信息和历史 POI 访问记录，提出个性化 POI 访问预测算法 BTREC。* Results: 对八座不同规模的城市的实验结果表明，提posed算法稳定性高，与多种序列预测算法相比， measured by recall, precision, and F1-scores 表现出色。<details>
<summary>Abstract</summary>
An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferences for categories of POIs and time availability. Our recommendation algorithm is largely inspired by the problem of sentence completion in natural language processing (NLP). Using a dataset of eight cities of different sizes, our experimental results demonstrate that our proposed algorithm is stable and outperforms many other sequence prediction algorithms, measured by recall, precision, and F1-scores.
</details>
<details>
<summary>摘要</summary>
旅游者有一项非常重要的任务，即制定一份合理的行程安排，尤其当访问不熟悉的城市时。许多旅游推荐工具只考虑有限数量的因素，如受欢迎的景点（POI）和路径约束。因此，它们提供的解决方案可能并不一定与个人用户匹配。我们在本文中提出了一种迭代算法，即BTREC（基于BERT的 trajectory推荐算法），它从POIBERT嵌入算法中扩展，为用户提供个性化的行程安排。我们的BTREC算法将用户的人口信息和过去访问的 POI 纳入修改后的 BERT 语言模型，以提供基于源和目的 POI 的个性化 POI 行程预测。我们的推荐系统可以创建一个 maximizes POIs 访问的旅游路线，同时也考虑用户对类型 POIs 和时间可用性的偏好。我们的推荐算法受到自然语言处理（NLP）中句子完成问题的启发，使用了八个不同规模的城市的 Dataset。我们的实验结果表明，我们的提议算法稳定和许多其他序列预测算法之上， measured by recall, precision 和 F1-scores。
</details></li>
</ul>
<hr>
<h2 id="Learning-quantum-states-and-unitaries-of-bounded-gate-complexity"><a href="#Learning-quantum-states-and-unitaries-of-bounded-gate-complexity" class="headerlink" title="Learning quantum states and unitaries of bounded gate complexity"></a>Learning quantum states and unitaries of bounded gate complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19882">http://arxiv.org/abs/2310.19882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimeng Zhao, Laura Lewis, Ishaan Kannan, Yihui Quek, Hsin-Yuan Huang, Matthias C. Caro</li>
<li>for: 本文研究了量子状态批量学习的复杂性，特别是考虑到实际应用场景下的量子状态和单位里程计数的复杂性。</li>
<li>methods: 作者使用了数据采样和查询复杂度来研究量子状态和单位里程计数的学习复杂性。</li>
<li>results: 作者证明了在学习量子状态和单位里程计数时，样本复杂度必须线性增长，而查询复杂度可以达到小平均误差。此外，作者还证明了在某些理想的 крипτографических假设下，计算复杂性必须线性增长。这些结果解释了量子机器学习模型的表达能力和创建量子状态和单位里程计数的复杂性之间的关系。<details>
<summary>Abstract</summary>
While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lunch theorems in unitary learning. Together, our results answer how the complexity of learning quantum states and unitaries relate to the complexity of creating these states and unitaries.
</details>
<details>
<summary>摘要</summary>
“量子状态批量检测是非常困难的，大多数状态对实用检测师来说无法吸引兴趣。因为自然界中的状态和单位里是有限门数的，那么我们可以问：是否存在高效的学习方法？在这项工作中，我们证明了为了通过量子电路中的$G$个二量子门来学习一个状态，需要一个样本复杂度 linearly 增长于 $G$。我们还证明了在平均错误下学习一个由 $G$ 个门组成的单位ри数学需要一个平均Query complexity linearly 增长于 $G$。虽然可以实现高效的学习，但我们表明了在合理的 криптографических假设下，计算复杂性 для学习状态和单位里的计算复杂性必须 exponentially 增长于 $G$。我们表明了这些结果是量子机器学习模型的基本限制，并提供了新的视角来解释无免答定理在单位学习中。这些结果回答了学习量子状态和单位里的复杂性与创造这些状态和单位里的复杂性之间的关系。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Metric-Flows-with-Neural-Networks"><a href="#Metric-Flows-with-Neural-Networks" class="headerlink" title="Metric Flows with Neural Networks"></a>Metric Flows with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19870">http://arxiv.org/abs/2310.19870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teluashish/traffic-flow-volume-prediction">https://github.com/teluashish/traffic-flow-volume-prediction</a></li>
<li>paper_authors: James Halverson, Fabian Ruehle</li>
<li>for: 该论文旨在研究各种流体在里曼度量空间中的流动，以及这些流动如何与神经网络的梯度下降相关。</li>
<li>methods: 论文使用神经网络梯度下降引起的里曼度量空间中的流动理论，并 derive了相应的流动方程，其中包括一个复杂的、非本地的 metric neural tangent kernel。</li>
<li>results: 论文发现了一些架构在无限宽限下，该流动简化，并且可以通过特定的假设来引入本地性，使得流动可以实现 péri Mérel’s Ricci flow 表述，解决了三维凯茨玻射悖论。 另外，论文还应用了这些想法到数字Calabi-Yau度量中，包括一个关于特征学习的讨论。<details>
<summary>Abstract</summary>
We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
</details>
<details>
<summary>摘要</summary>
我们开发了一种在里曼度量空间中的流理论，它是由神经网络梯度下降引起的。这是在近期用神经网络approximate Calabi-Yau度量的进步和理解流在神经网络空间中的进步的基础之上。我们 derivate了相应的流方程，它们是由一个metric neural tangent kernel控制的，这是一个复杂的、非本地的对象，它在时间演化。然而，许多架构允许无穷宽限制，在这种情况下，核心变得固定，动态简化。额外的假设可以使得流动具有本地性，使得可以实现Perelman的 Ricci流形式化，这是用于解决3D Poincaré conjecture的。我们应用这些想法来数字Calabi-Yau度量，包括一个关于特征学习的讨论。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation"><a href="#Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation" class="headerlink" title="Posterior Sampling for Competitive RL: Function Approximation and Partial Observation"></a>Posterior Sampling for Competitive RL: Function Approximation and Partial Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19861">http://arxiv.org/abs/2310.19861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, Tong Zhang</li>
<li>for: 这个论文研究了激励学习（RL）中的后验抽样算法，特别是在通用函数近似中。</li>
<li>methods: 论文提出了自我玩家和反对敌人学习下的两种情况下的扩展通用杰伦评价因子（GEC）作为函数近似的复杂性度量，并基于自我玩家 GEC 提出了一种基于模型的自我玩家 posterior 抽样方法来控制两个玩家学习 Nash 平衡。</li>
<li>results: 论文提出了一种可以成功处理部分可见状态的模型基于 posterior 抽样方法，并在部分可见状态下学习反对敌人的模型。此外，论文还提出了一种基于反对敌人 GEC 的模型基于 posterior 抽样方法，可以在部分可见状态下学习反对敌人的模型。论文还提供了对 proposed 算法的低征退 bound，可以在 proposed GEC 和 episodes 数量 $T$ 上下降幂函数。<details>
<summary>Abstract</summary>
This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.
</details>
<details>
<summary>摘要</summary>
First, we propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, which capture the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle partial observability of states.Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability.We provide low regret bounds for our proposed algorithms, which can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.
</details></li>
</ul>
<hr>
<h2 id="Robust-Causal-Bandits-for-Linear-Models"><a href="#Robust-Causal-Bandits-for-Linear-Models" class="headerlink" title="Robust Causal Bandits for Linear Models"></a>Robust Causal Bandits for Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19794">http://arxiv.org/abs/2310.19794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Yan, Arpan Mukherjee, Burak Varıcı, Ali Tajer</li>
<li>for: 这个论文是为了研究Sequential Design of Experiments（SDE）在 causal systems 中的优化问题。</li>
<li>methods: 这个论文使用了 causal bandits（CBs）模型，并研究了这些模型在 temporal model fluctuations 中的稳定性。</li>
<li>results: 论文表明，existin 的方法在 temporal model fluctuations 中不稳定，并且可能会导致 linear regret。而 proposed 算法可以在这些 fluctuations 中实现 nearly optimal 的 regret。<details>
<summary>Abstract</summary>
Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with even a few instances of model deviation. Specifically, when the number of instances with model deviation is as few as $T^\frac{1}{2L}$, where $T$ is the time horizon and $L$ is the longest causal path in the graph, the existing algorithms will have linear regret in $T$. Next, a robust CB algorithm is designed, and its regret is analyzed, where upper and information-theoretic lower bounds on the regret are established. Specifically, in a graph with $N$ nodes and maximum degree $d$, under a general measure of model deviation $C$, the cumulative regret is upper bounded by $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$ and lower bounded by $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$. Comparing these bounds establishes that the proposed algorithm achieves nearly optimal $\tilde{\mathcal{O}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and maintains sub-linear regret for a broader range of $C$.
</details>
<details>
<summary>摘要</summary>
sequential设计实验可以有效地模型 causal系统中的奖励函数。在现有文献中，一个关键假设是 causal模型在时间上是相对定常的。然而，这个假设并不一定成立在复杂系统中，这些系统在时间上不断发生模型波动。这篇论文考虑了 causal系统中 linear structural equation models (SEMs) 的Robustness。我们的目标是设计一个序列 intervención，使其最小化对 oracle 所知道的整个 causal模型和其波动的 regret。首先，我们证明了现有的方法在模型偏移情况下无法保持 regret 线性。specifically，当模型偏移的数量为 $T^\frac{1}{2L}$，where $T$ 是时间框架和 $L$ 是最长 causal 路径，现有的算法将在 $T$ 时间框架内具有线性 regret。接着，我们设计了一种 Robust CB 算法，并分析了其 regret。我们设定了 upper 和 information-theoretic lower bounds on regret，其中，在一个 $N$ 节点、最大度为 $d$ 的图中，在一般测度 $C$ 下，累积 regret 上限为 $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$，下限为 $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$. Comparing these bounds shows that the proposed algorithm achieves nearly optimal $\tilde{\mathcal{O}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and maintains sub-linear regret for a broader range of $C$.
</details></li>
</ul>
<hr>
<h2 id="On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow"><a href="#On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow" class="headerlink" title="On Learning Gaussian Multi-index Models with Gradient Flow"></a>On Learning Gaussian Multi-index Models with Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19793">http://arxiv.org/abs/2310.19793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bietti, Joan Bruna, Loucas Pillaud-Vivien</li>
<li>for: 这个论文是关于高维 Gaussian 数据的多指量回归问题中的梯度流算法研究。</li>
<li>methods: 这种算法使用了两个时间尺度的方法，其中低维链函数使用非Parametric模型，而且Parametrizing低维投影的子空间 correlation matrices 的matrix semigroup structure进行了有效利用。</li>
<li>results: 研究结果表明，这种梯度流算法在Grassmannian population gradient flow dynamics中具有全球收敛性，并且可以对链函数进行有效的描述。另外，这种算法在 Plant 问题中的优化景观存在摇摆性，梯度流动可能会在高概率下被困。<details>
<summary>Abstract</summary>
We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.   We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these positive results, we also show that the related \emph{planted} problem, where the link function is known and fixed, in fact has a rough optimization landscape, in which gradient flow dynamics might get trapped with high probability.
</details>
<details>
<summary>摘要</summary>
我们研究梯度流在多指标回归问题上，特别是高维 Gaussian 数据上。多指标函数可以看作是一个低维线性投影和一个低维链函数的复杂组合，因此它们成为了神经网络中的自然特征学习模板。我们考虑了两个时间步长的算法，其中低维链函数使用非参数化模型在无穷多个步长上学习，而低维线性投影则由参数化的子空间协同矩阵的matrix semigroup结构来学习。我们证明了这种涌流动力学的全球收敛性，并给出了相应的`鞍点-鞍点'动力学的量化描述。与此同时，我们还显示了相关的植入问题（即链函数已知和固定）实际上有一个恶劣优化景观，在这种情况下，梯度流动可能会陷入高概率下。
</details></li>
</ul>
<hr>
<h2 id="Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget"><a href="#Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget" class="headerlink" title="Locally Optimal Best Arm Identification with a Fixed Budget"></a>Locally Optimal Best Arm Identification with a Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19788">http://arxiv.org/abs/2310.19788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato</li>
<li>for: 这种研究探讨了如何确定最佳治疗臂，即具有最高预期效果的治疗臂。</li>
<li>methods: 我们使用了各种方法，包括最佳臂标识（BAI）和ORDINAL优化。</li>
<li>results: 我们发现，在小差度 Régime下，我们可以通过设计一个称为通用-EBA策略（Generalized-Neyman-Allocation-Empirical-Best-Arm）来减少错误的可能性。这种策略是 asymptotically 优化的，即其错误率与下界相对应。<details>
<summary>Abstract</summary>
This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. Then, assuming that the variances are known, we design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA strategy, we show that the strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details>
<details>
<summary>摘要</summary>
To achieve this objective, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. We then design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011).We show that the GNA-EBA strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas"><a href="#Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas" class="headerlink" title="Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas"></a>Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19767">http://arxiv.org/abs/2310.19767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Stylianopoulos, Murat Bayraktar, Nuria González Prelcic, George C. Alexandropoulos</li>
<li>for: 这篇论文旨在探讨非线性多paths环境下的用户追踪，并提出了一个基于机器学习的二阶方法。</li>
<li>methods: 这篇论文使用了一个具有注意力的神经网络，首先将杂凑的通道响应映射到潜在的用户位置，然后使用一个学习式autoregressive模型来利用时间相关的通道信息获取最终的位置预测。</li>
<li>results: numerical evaluation over an outdoor ray-tracing scenario shows that this methodology is capable of achieving high position accuracy across various multipath settings, even in the presence of LoS blockage.<details>
<summary>Abstract</summary>
User localization and tracking in the upcoming generation of wireless networks have the potential to be revolutionized by technologies such as the Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or require pilot transmission sequences whose length is comparable to the number of DMA elements, thus, leading to limited effectiveness and considerable measurement overheads in blocked LoS and dynamic multipath environments. In this paper, we present a two-stage machine-learning-based approach for user tracking, specifically designed for non-LoS multipath settings. A newly proposed attention-based Neural Network (NN) is first trained to map noisy channel responses to potential user positions, regardless of user mobility patterns. This architecture constitutes a modification of the prominent vision transformer, specifically modified for extracting information from high-dimensional frequency response signals. As a second stage, the NN's predictions for the past user positions are passed through a learnable autoregressive model to exploit the time-correlated channel information and obtain the final position predictions. The channel estimation procedure leverages a DMA receive architecture with partially-connected radio frequency chains, which results to reduced numbers of pilots. The numerical evaluation over an outdoor ray-tracing scenario illustrates that despite LoS blockage, this methodology is capable of achieving high position accuracy across various multipath settings.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:User localization and tracking in the upcoming generation of wireless networks have the potential to be revolutionized by technologies such as the Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or require pilot transmission sequences whose length is comparable to the number of DMA elements, thus, leading to limited effectiveness and considerable measurement overheads in blocked LoS and dynamic multipath environments. In this paper, we present a two-stage machine-learning-based approach for user tracking, specifically designed for non-LoS multipath settings. A newly proposed attention-based Neural Network (NN) is first trained to map noisy channel responses to potential user positions, regardless of user mobility patterns. This architecture constitutes a modification of the prominent vision transformer, specifically modified for extracting information from high-dimensional frequency response signals. As a second stage, the NN's predictions for the past user positions are passed through a learnable autoregressive model to exploit the time-correlated channel information and obtain the final position predictions. The channel estimation procedure leverages a DMA receive architecture with partially-connected radio frequency chains, which results to reduced numbers of pilots. The numerical evaluation over an outdoor ray-tracing scenario illustrates that despite LoS blockage, this methodology is capable of achieving high position accuracy across various multipath settings.Translate the text into Simplified Chinese:</SYS>Here's the translation:用户本地化和跟踪在未来的无线网络中具有很大的潜力，尤其是通过动态元件天线（DMA）等技术。常见的算法方法假设有相对主要的直线视图（LoS）路径，或者需要与DMA元件数量相同的导航传输序列，从而导致效果有限和测量开销很大在阻塞的LoS和动态吸收环境中。在这篇论文中，我们提出了一种基于机器学习的两 stage方法，特别是设计用于非LoS吸收环境。我们首先使用一种新的注意力基本的神经网络（NN）来映射噪声通道响应到潜在的用户位置，无论用户移动模式。这种架构改进了知名的视Transformer，特意为提取高维频响应信号中的信息。作为第二stage，NN的预测结果以前的用户位置通过一个可学习的自 regression模型来利用时相关的通道信息，以获得最终的位置预测。通道估计过程利用了DMA接收架构中的半连接 radio frequency链，从而减少了数量的导航传输。数值评估在一个外部照明场景中表明，尽管LoS堵塞，这种方法可以在不同的 multipath 设置下实现高精度的位置预测。
</details></li>
</ul>
<hr>
<h2 id="Epidemic-outbreak-prediction-using-machine-learning-models"><a href="#Epidemic-outbreak-prediction-using-machine-learning-models" class="headerlink" title="Epidemic outbreak prediction using machine learning models"></a>Epidemic outbreak prediction using machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19760">http://arxiv.org/abs/2310.19760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshara Pramod, JS Abhishek, Dr. Suganthi K</li>
<li>for: 预测疫情爆发（Influenza、肝炎和马拉疫苗）在纽约州、美国</li>
<li>methods: 使用机器学习和深度学习算法，以及一个portal来预测疫情爆发，并且利用历史数据预测5周内可能的病例数</li>
<li>results: 预测疫情爆发的可能性，并且利用非клиниче因素（Google搜索趋势、社交媒体数据和天气数据）预测疫情爆发的概率<details>
<summary>Abstract</summary>
In today's world,the risk of emerging and re-emerging epidemics have increased.The recent advancement in healthcare technology has made it possible to predict an epidemic outbreak in a region.Early prediction of an epidemic outbreak greatly helps the authorities to be prepared with the necessary medications and logistics required to keep things in control. In this article, we try to predict the epidemic outbreak (influenza, hepatitis and malaria) for the state of New York, USA using machine and deep learning algorithms, and a portal has been created for the same which can alert the authorities and health care organizations of the region in case of an outbreak. The algorithm takes historical data to predict the possible number of cases for 5 weeks into the future. Non-clinical factors like google search trends,social media data and weather data have also been used to predict the probability of an outbreak.
</details>
<details>
<summary>摘要</summary>
今天的世界中，突发和复发疫情的风险增加了。最近的医疗科技进步使得可以预测一个地区的疫情爆发。在这篇文章中，我们使用机器学习和深度学习算法预测新 York 州的 influenza、hepatitis 和 malaria 疫情爆发，并创建了一个 portal，可以警示当地权力机构和医疗组织在疫情爆发时。算法使用历史数据预测下一个5个星期内可能出现的病例数。此外，我们还使用google搜索趋势、社交媒体数据和天气数据预测疫情爆发的可能性。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Reward-Estimation-with-Preference-Feedback"><a href="#Differentially-Private-Reward-Estimation-with-Preference-Feedback" class="headerlink" title="Differentially Private Reward Estimation with Preference Feedback"></a>Differentially Private Reward Estimation with Preference Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19733">http://arxiv.org/abs/2310.19733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Ray Chowdhury, Xingyu Zhou, Nagarajan Natarajan</li>
<li>for: 这个论文的目的是如何使用偏好反馈来调整生成模型，以便更好地满足人类的兴趣。</li>
<li>methods: 这个论文使用了反馈学习 WITH human feedback (RLHF) 方法，首先从人类标签器获取反馈，然后计算出奖励模型，最后使用奖励模型来制定策略。</li>
<li>results: 这个论文通过引入标签幂 differential privacy (DP) 的概念，解决了基于偏好反馈的奖励估计中人类标签器隐私泄露的问题。 Specifically, 论文考虑了 Bradley-Terry-Luce (BTL) 模型，并在标准的最小最大估计框架下提供了紧Binding的Upper和Lower bounds on 奖励估计错误。<details>
<summary>Abstract</summary>
Learning from preference-based feedback has recently gained considerable traction as a promising approach to align generative models with human interests. Instead of relying on numerical rewards, the generative models are trained using reinforcement learning with human feedback (RLHF). These approaches first solicit feedback from human labelers typically in the form of pairwise comparisons between two possible actions, then estimate a reward model using these comparisons, and finally employ a policy based on the estimated reward model. An adversarial attack in any step of the above pipeline might reveal private and sensitive information of human labelers. In this work, we adopt the notion of label differential privacy (DP) and focus on the problem of reward estimation from preference-based feedback while protecting privacy of each individual labelers. Specifically, we consider the parametric Bradley-Terry-Luce (BTL) model for such pairwise comparison feedback involving a latent reward parameter $\theta^* \in \mathbb{R}^d$. Within a standard minimax estimation framework, we provide tight upper and lower bounds on the error in estimating $\theta^*$ under both local and central models of DP. We show, for a given privacy budget $\epsilon$ and number of samples $n$, that the additional cost to ensure label-DP under local model is $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$, while it is $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$ under the weaker central model. We perform simulations on synthetic data that corroborate these theoretical results.
</details>
<details>
<summary>摘要</summary>
学习从偏好反馈中获得了 considerable 的满意度，这种方法可以让生成模型与人类的兴趣相匹配。而不是依靠数字奖励，这些生成模型通过人类反馈（RLHF）进行训练。这些方法首先从人类标签器获得反馈，通常是两个可能的动作之间的对比，然后计算出一个奖励模型，最后使用这个奖励模型来采取策略。在上述执行管道中，一个反对攻击可能泄露人类标签器的私人和敏感信息。在这个工作中，我们采用标签权限隐私（DP）的想法，并关注在基于偏好反馈中保护每名标签器的隐私问题。我们考虑 Bradley-Terry-Luce（BTL）模型，这是一种对比式反馈模型，其中包含一个隐藏奖励参数 $\theta^* \in \mathbb{R}^d$。在标准的最小最大估计框架中，我们提供了紧跟的上下限错误估计 $\theta^*$ 的误差，并且分析了在本地和中央模型下的DP保护成本。我们发现，对于给定的隐私预算 $\epsilon$ 和样本数 $n$，在本地模型下添加额外的DP保护成本是 $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$，而在更弱的中央模型下是 $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$。我们在 sintetic 数据上进行了仪表实验，并证明了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="Support-matrix-machine-A-review"><a href="#Support-matrix-machine-A-review" class="headerlink" title="Support matrix machine: A review"></a>Support matrix machine: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19717">http://arxiv.org/abs/2310.19717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Anuradha Kumari, Mushir Akhtar, Rupal Shah, M. Tanveer</li>
<li>for: 这个研究是为了解决支持向量机器学习（SVM）在处理矩阵输入数据时所遇到的问题。</li>
<li>methods: 这个研究使用支持矩阵机器学习（SMM）方法，它是一种处理矩阵输入数据的新型机器学习方法。SMM方法利用特征矩阵的特性，保留矩阵资料中的结构资讯，并且可以处理高维度输入数据。</li>
<li>results: 这个研究提出了多种SMM模型的变形，包括预期抗噪、簇短、分类不均等模型。这些模型都可以实现高度的准确率和高效率。此外，这个研究还简述了SMM模型的应用和未来可能的研究方向。<details>
<summary>Abstract</summary>
Support vector machine (SVM) is one of the most studied paradigms in the realm of machine learning for classification and regression problems. It relies on vectorized input data. However, a significant portion of the real-world data exists in matrix format, which is given as input to SVM by reshaping the matrices into vectors. The process of reshaping disrupts the spatial correlations inherent in the matrix data. Also, converting matrices into vectors results in input data with a high dimensionality, which introduces significant computational complexity. To overcome these issues in classifying matrix input data, support matrix machine (SMM) is proposed. It represents one of the emerging methodologies tailored for handling matrix input data. The SMM method preserves the structural information of the matrix data by using the spectral elastic net property which is a combination of the nuclear norm and Frobenius norm. This article provides the first in-depth analysis of the development of the SMM model, which can be used as a thorough summary by both novices and experts. We discuss numerous SMM variants, such as robust, sparse, class imbalance, and multi-class classification models. We also analyze the applications of the SMM model and conclude the article by outlining potential future research avenues and possibilities that may motivate academics to advance the SMM algorithm.
</details>
<details>
<summary>摘要</summary>
支持向量机 (SVM) 是机器学习领域中最受研究的一种 paradigma，用于分类和回归问题。它基于 вектор化输入数据。然而，实际世界中大量数据存在矩阵形式，需要通过将矩阵转换为向量来输入 SVM。这个过程会破坏矩阵数据中的空间相关性，并且将输入数据的维度增加，导致计算复杂性增加。为了解决这些问题，支持矩阵机 (SMM) 被提出。它是一种处理矩阵输入数据的新趋势。SMM 方法利用矩阵数据的特征信息，使用 spectral elastic net 性质，这是一种组合核心值和 Frobenius 值的方法。本文提供了 SMM 模型的首个深入分析，可以作为新手和专家的参考。我们讨论了多种 SMM 变体，如 robust、稀热、类偏振和多类分类模型。我们还分析了 SMM 模型的应用，并在文章结尾列出了可能的未来研究方向和动机，以便学术人士继续推进 SMM 算法。
</details></li>
</ul>
<hr>
<h2 id="Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model"><a href="#Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model" class="headerlink" title="Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model"></a>Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19854">http://arxiv.org/abs/2310.19854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilien Dreveton, Felipe S. Fernandes, Daniel R. Figueiredo</li>
<li>for: 本文旨在解决节点网络中的社区分割问题，但是节点也有属性信息，这些信息和网络信息可以并行利用以设计高性能的分 clustering算法。</li>
<li>methods: 本文提出了一个信息理论基础的优化算法，可以准确地还原社区标签，并研究了这种算法在不同的网络和属性模型下的性能。</li>
<li>results: 对比 классических算法和现有的算法，提出的算法在synthetic数据上表现出优于其他算法，并且可以处理不同的网络和属性模型，以及稀疏网络。<details>
<summary>Abstract</summary>
Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attributes belong to exponential families. This covers a broad range of possible interactions (e.g., edges with weights) and attributes (e.g., non-Gaussian models), as well as sparse networks, while also exploring the connection between exponential families and Bregman divergences. Extensive numerical experiments using synthetic data indicate that the proposed algorithm outperforms classic algorithms that leverage only network or only attribute information as well as state-of-the-art algorithms that also leverage both sources of information. The contributions of this work provide insights into the fundamental limits and practical techniques for inferring community labels on node-attributed networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling"><a href="#Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling" class="headerlink" title="Convolutional State Space Models for Long-Range Spatiotemporal Modeling"></a>Convolutional State Space Models for Long-Range Spatiotemporal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19694">http://arxiv.org/abs/2310.19694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimmy T. H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon</li>
<li>for: 这篇论文的目的是对长时间空间序列进行有效地模型，以满足需要同时模型复杂的空间相关性和长时间依赖性。</li>
<li>methods: 这篇论文提出了一种将tensor值状态更新为回归神经网络的方法，并且使用了平行扫描来实现快速的自动生成。另外，这篇论文还提出了一种基于state space方法的长时间模型，将数值状态转换为字串，以便平行处理整个时空序列。</li>
<li>results: 这篇论文的结果显示，ConvS5在一个长时间Moving-MNIST实验中与Transformers和ConvLSTM相比，训练3倍 faster，并在生成数据时间方面比Transformers快400倍。此外，ConvS5在DMLab、Minecraft和Habitat预测测试中与状态点方法匹配或超越，并开启了新的长时间空间序列模型方向。<details>
<summary>Abstract</summary>
Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.
</details>
<details>
<summary>摘要</summary>
长时间空间序列模型化是一项具有挑战性的任务，因为需要同时模型复杂的空间相关性和长距离时间相关性。ConvLSTM通过更新tensor值状态使用Recurrent Neural Networks来解决这个问题，但它们的顺序计算使得它们在训练时slow。与之相比，Transformers可以在同时处理整个空间时间序列，压缩成token，但它们的注意力成本随序列长度平方增加，限制它们对更长的序列的可扩展性。在这里，我们解决先前方法的挑战，并引入了 convolutional state space models (ConvSSM)，它们将ConvLSTM中的tensor模型 идеи与状态方法 such as S4和S5的长时间序列模型approaches相结合。首先，我们展示了如何通过并行扫描来实现subquadratic parallelization，并在 autoregressive generation 中实现快速生成。然后，我们证明了 ConvSSM 的动力学和 SSM 的动力学之间的等价性，这种等价性驱动了参数化和初始化策略，以便模型长距离相关性。最终，我们提出了 ConvS5，一种高效的 ConvSSM 变体，用于长距离空间时间模型化。ConvS5在一个长期 Moving-MNIST 实验中表现出色，同时训练 3X  faster than ConvLSTM 和生成样本 400X faster than Transformers。此外，ConvS5 与当前状态艺术方法相当或更好的性能在 DMLab、Minecraft 和 Habitat 预测 benchmark 上，并启动了新的长时间空间序列模型化方向。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds"><a href="#Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds" class="headerlink" title="Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds"></a>Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19690">http://arxiv.org/abs/2310.19690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Gong, Ben Usman, Han Zhao, David I. Inouye</li>
<li>for: 学习不变表示，应用于公平和鲁棒性。</li>
<li>methods: 非对抗性概率基的方法，不需要模型反转性和假设潜在分布。</li>
<li>results: 可以取代对抗损失，在标准不变表示学习管道中使用，无需修改原始架构。In English, this means:</li>
<li>for: Learning invariant representations for fairness and robustness.</li>
<li>methods: Non-adversarial likelihood-based approaches that do not require model invertibility and can be applied to any model pipeline.</li>
<li>results: Our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures, significantly broadening the applicability of non-adversarial alignment methods.<details>
<summary>Abstract</summary>
Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly broadening the applicability of non-adversarial alignment methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Distribution alignment可以用来学习不变表示，并有应用于公平和Robustness。大多数先前的工作采用了对抗启动方法，但 resulting minimax问题是不稳定并难以优化。非对抗的可能性基于方法 Either require model invertibility, 强制 latent prior 的限制，或lack 一个通用的对齐框架。为了突破这些限制，我们提议一种非对抗 VAE-based 对齐方法，可以应用于任何模型管道。我们开发了一系列对齐上限（包括噪声 bound），它们具有 VAE-like 目标，但从不同的视角来看。我们在理论和实验方面仔细比较了我们的方法和先前 VAE-based 对齐方法。最后，我们示出了我们的新对齐损失可以在标准不变表示学习管道中代替对抗损失，无需修改原始架构 --  thereby significantly broadening the applicability of non-adversarial alignment methods。
</details></li>
</ul>
<hr>
<h2 id="DGFN-Double-Generative-Flow-Networks"><a href="#DGFN-Double-Generative-Flow-Networks" class="headerlink" title="DGFN: Double Generative Flow Networks"></a>DGFN: Double Generative Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19685">http://arxiv.org/abs/2310.19685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaine Lau, Nikhil Vemgal, Doina Precup, Emmanuel Bengio</li>
<li>for: 这个研究用于探索药物设计中的问题，特别是在缺乏对答问题和高维度状态空间中。</li>
<li>methods: 这个研究使用了两个流程网络（GFlowNets&#x2F;GFNs），并将其与强化学习和双层深度Q学习结合，以增强探索。</li>
<li>results: 实验结果显示，这种方法可以有效地增强探索在缺乏对答问题和高维度状态空间中，并且在药物设计中进行创新设计。<details>
<summary>Abstract</summary>
Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
</details>
<details>
<summary>摘要</summary>
深度学习在药物发现中展示出了有效性，具有预测和生成模型的潜在应用。生成流网络（GFlowNets/GFNs）是最近引入的方法，被认可为能够生成多样化候选者，尤其在小分子生成任务中。在这项工作中，我们引入双生成流网络（DGFNs）。启发自强化学习和双层深度Q学习，我们引入目标网络用于采样轨迹，同时更新主网络使用这些采样轨迹。实验结果表明，DGFNs有效地增强了探索性在稀资 reward 领域和高维状态空间中，这些领域都是药物发现中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning"><a href="#Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning" class="headerlink" title="Density Estimation for Entry Guidance Problems using Deep Learning"></a>Density Estimation for Entry Guidance Problems using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19684">http://arxiv.org/abs/2310.19684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens A. Rataczak, Davide Amato, Jay W. McMahon</li>
<li>for: 这个研究旨在使用深度学习方法来估算入 atmospheric density profiles，用于 planetary entry guidance 问题。</li>
<li>methods: 研究使用了 long short-term memory (LSTM) 神经网络，学习从飞行器上可available的测量数据和density profile之间的映射。测量数据包括圆柱体状态表示、 Cartesian 感知加速度分量和surface-pressure measurement。</li>
<li>results: 研究发现，使用 LSTM 网络可以不仅预测飞行器将飞行的density profile，还可以重建飞行器已经飞行过的density profile。与其他两种估算技术相比，使用 LSTM 网络得到了更高的终端准确性。<details>
<summary>Abstract</summary>
This work presents a deep-learning approach to estimate atmospheric density profiles for use in planetary entry guidance problems. A long short-term memory (LSTM) neural network is trained to learn the mapping between measurements available onboard an entry vehicle and the density profile through which it is flying. Measurements include the spherical state representation, Cartesian sensed acceleration components, and a surface-pressure measurement. Training data for the network is initially generated by performing a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm that utilizes an exponential density model, while the truth density profiles are sampled from MarsGRAM. A curriculum learning procedure is developed to refine the LSTM network's predictions for integration within the FNPEG algorithm. The trained LSTM is capable of both predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. Results demonstrate that using the LSTM model results in superior terminal accuracy compared to the other two techniques when considering both noisy and noiseless measurements.
</details>
<details>
<summary>摘要</summary>
The training data is generated through a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm with an exponential density model. The truth density profiles are sampled from MarsGRAM. To refine the LSTM network's predictions, a curriculum learning procedure is developed.The trained LSTM is capable of predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. The results show that the LSTM model achieves superior terminal accuracy compared to the other two techniques, both with noisy and noiseless measurements.
</details></li>
</ul>
<hr>
<h2 id="An-Online-Bootstrap-for-Time-Series"><a href="#An-Online-Bootstrap-for-Time-Series" class="headerlink" title="An Online Bootstrap for Time Series"></a>An Online Bootstrap for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19683">http://arxiv.org/abs/2310.19683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Nicolai Palm, Thomas Nagler</li>
<li>for: This paper is written for researchers and practitioners who need to perform uncertainty quantification on large streams of dependent data, such as time series or spatially correlated observations.</li>
<li>methods: The paper proposes a novel bootstrap method that accounts for data dependencies and can be executed online, making it suitable for real-time applications. The method is based on an autoregressive sequence of increasingly dependent resampling weights.</li>
<li>results: The proposed bootstrap scheme is proven to be theoretically valid under general conditions, and is demonstrated to provide reliable uncertainty quantification even in the presence of complex data dependencies through extensive simulations.<details>
<summary>Abstract</summary>
Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and practitioners in dynamic, data-rich environments.
</details>
<details>
<summary>摘要</summary>
traditional bootstrap方法在受到大量相互相关数据时，其可行性有限。在这篇论文中，我们提出了一种新的bootstrap方法，可以考虑数据相互关系，并可以在线执行，因此特别适用于实时应用。这种方法基于一个自增式相关的排重采样序列。我们证明了这种bootstrap方案的理论有效性，并通过广泛的仿真实验证了其可靠性。我们的工作将经典采样技术与现代数据分析的需求相连接，为研究人员和实践者在动态数据丰富环境中提供了一种有价值的工具。
</details></li>
</ul>
<hr>
<h2 id="HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding"><a href="#HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding" class="headerlink" title="HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding"></a>HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19676">http://arxiv.org/abs/2310.19676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Angelotti</li>
<li>for: 提高Transformer架构中的注意力机制的可 permutation-invariance性，以便在输入序列中推断 tokens 的顺序。</li>
<li>methods: 引入Hyperbolic Positional Encoding (HyPE)，一种使用高维函数性质来编码 tokens 的相对位置的新方法，不需要存储 $O(L^2)$ 值的 маask，并且可以通过预liminary concatenation operations和矩阵乘法来实现编码。</li>
<li>results: HyPE 可以准确地 aproximate ALiBi 的注意力偏好，并且可以在不同的长度上进行推断，这表示 HyPE 具有良好的泛化能力。<details>
<summary>Abstract</summary>
In Transformer-based architectures, the attention mechanism is inherently permutation-invariant with respect to the input sequence's tokens. To impose sequential order, token positions are typically encoded using a scheme with either fixed or learnable parameters. We introduce Hyperbolic Positional Encoding (HyPE), a novel method that utilizes hyperbolic functions' properties to encode tokens' relative positions. This approach biases the attention mechanism without the necessity of storing the $O(L^2)$ values of the mask, with $L$ being the length of the input sequence. HyPE leverages preliminary concatenation operations and matrix multiplications, facilitating the encoding of relative distances indirectly incorporating biases into the softmax computation. This design ensures compatibility with FlashAttention-2 and supports the gradient backpropagation for any potential learnable parameters within the encoding. We analytically demonstrate that, by careful hyperparameter selection, HyPE can approximate the attention bias of ALiBi, thereby offering promising generalization capabilities for contexts extending beyond the lengths encountered during pretraining. The experimental evaluation of HyPE is proposed as a direction for future research.
</details>
<details>
<summary>摘要</summary>
在基于变换器的架构中，注意机制自然地具有输入序列中元素的排序不敏感性。为了强制实现顺序，通常使用固定或学习参数的编码方案来编码token的位置。我们介绍了一种新的方法——折射函数编码（HyPE），它利用折射函数的性质来编码token的相对位置。这种方法不需要存储$O(L^2)$的掩码值，其中$L$是输入序列的长度。HyPE通过预处理的 concatenation 操作和矩阵乘法，实现编码相对距离，并通过直接在softmax计算中涂抹偏好来实现。这种设计允许HyPE与FlashAttention-2兼容，并且支持任何可能的学习参数在编码中的梯度反射。我们分析表明，通过精心选择参数，HyPE可以近似ALiBi的注意偏好，从而提供了扩展训练集之外的扩展能力。HyPE的实验性评估被提议为未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes"><a href="#Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes" class="headerlink" title="Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes"></a>Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19666">http://arxiv.org/abs/2310.19666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes">https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes</a></li>
<li>paper_authors: Zheng Wang, Shikai Fang, Shibo Li, Shandian Zhe</li>
<li>for: 这个论文的目的是提出一种基于神经Diffusion-Reaction过程的动态tensor分解方法，以更好地捕捉多方数据分析中的时间结构信息。</li>
<li>methods: 该方法使用神经网络构建多部分图来编码缺失的tensor模式中的关系，然后通过协同演化图像演化轨迹和个体特征来捕捉个体的共同特征和个体特点。</li>
<li>results: 作者通过实验研究和实际应用示例显示了该方法的优势，并提供了一个可用的代码库。<details>
<summary>Abstract</summary>
Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation study and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.
</details>
<details>
<summary>摘要</summary>
tensor 分解是多方数据分析中的重要工具。在实践中，数据通常是稀疏的，但具有丰富的时间信息。现有方法往往忽略时间信息，并忽略tensor中稀疏观察到的结构知识。为了超越这些局限性，我们提出了动态嵌入（DEMOTE）。我们采用神经扩散-反应过程来估计动态嵌入 для每个tensor模式中的实体。具体来说，根据观察到的tensor入口，我们建立了多部分图来编码实体之间的相关性。我们构建了图扩散过程来同步嵌入轨迹的演化，并使用神经网络构建每个个体的反应过程。这样，我们的模型可以捕捉到不同实体的共同特征和个性特征在演化过程中的变化。然后，我们使用神经网络来模型每个入口的值为非线性函数。为估计模型，我们结合ODE解引入了随机批处理算法。我们提出了一种层次随机抽样法，以平衡处理每个批处理的成本，从而提高整体效率。我们在实验研究和实际应用中显示了我们的方法的优势。代码可以在https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes中找到。
</details></li>
</ul>
<hr>
<h2 id="Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model"><a href="#Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model" class="headerlink" title="Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model"></a>Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19849">http://arxiv.org/abs/2310.19849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurekazhu/diffaffinity">https://github.com/eurekazhu/diffaffinity</a></li>
<li>paper_authors: Shiwei Liu, Tian Zhu, Milong Ren, Chungong Yu, Dongbo Bu, Haicang Zhang</li>
<li>for: 预测蛋白质-蛋白质交互的作用对蛋白质工程和药物发现是关键。</li>
<li>methods: 本文提出了SidechainDiff，一种基于学习表征的方法，利用无标注实验数据来预测蛋白质-蛋白质交互的作用。SidechainDiff使用瑞氏幂函数模型来学习生成残氨链的过程，同时还可以提供蛋白质-蛋白质界面上的结构上下文表示。</li>
<li>results: 利用SidechainDiff进行预测，对蛋白质-蛋白质交互的作用具有最佳性能。此外，SidechainDiff是首先使用扩散模型生成残氨链的方法，与之前的主要关注蛋白质脊梁结构生成相区别。<details>
<summary>Abstract</summary>
Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distinguishing it from prior efforts that have predominantly focused on generating protein backbone structures.
</details>
<details>
<summary>摘要</summary>
多种生物过程都依赖于蛋白质-蛋白质交互网络。预测蛋白质-蛋白质绑定的效果是蛋白工程和药物发现中的关键。然而，实验数据缺乏绑定能量的标注，对于开发计算方法，特别是深度学习方法，呈现了 significante挑战。在这项工作中，我们提出了SidechainDiff方法，它是基于学习扩散过程的 representation learning 方法，可以利用无标注实验蛋白结构来学习蛋白的侧链姿态。 SidechainDiff 还可以给蛋白质-蛋白质界面上的变化提供结构上下文表示。利用学习的表示，我们实现了对蛋白质-蛋白质绑定效果的预测，并且 SidechainDiff 是首个 для侧链的扩散型生成模型，与之前的主要关注在蛋白质脊梁结构生成方面。
</details></li>
</ul>
<hr>
<h2 id="Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity"><a href="#Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity" class="headerlink" title="Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity"></a>Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19614">http://arxiv.org/abs/2310.19614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmi-basel/disinhibitory-control">https://github.com/fmi-basel/disinhibitory-control</a></li>
<li>paper_authors: Julian Rossbroich, Friedemann Zenke</li>
<li>for: 这个研究的目的是解释如何 neuronal circuits 实现 credit assignment，即在系统神经科学中一个中心未解决的问题。</li>
<li>methods: 这个研究使用了一种可能的 microcircuit model 和 Hebbian learning rule，这些模型来自 adaptive control theory 框架。</li>
<li>results: 研究发现，当假设错误是通过 top-down 抑制性 synaptic afferents 编码时，error-modulated learning 会自然地出现在 circuit level，并且可以解释实验室中观察到的材料塑性。此外，这种学习规则还可以与 back-propagation of error 相比肩，在非线性分离的benchmark上表现相似。<details>
<summary>Abstract</summary>
How neuronal circuits achieve credit assignment remains a central unsolved question in systems neuroscience. Various studies have suggested plausible solutions for back-propagating error signals through multi-layer networks. These purely functionally motivated models assume distinct neuronal compartments to represent local error signals that determine the sign of synaptic plasticity. However, this explicit error modulation is inconsistent with phenomenological plasticity models in which the sign depends primarily on postsynaptic activity. Here we show how a plausible microcircuit model and Hebbian learning rule derived within an adaptive control theory framework can resolve this discrepancy. Assuming errors are encoded in top-down dis-inhibitory synaptic afferents, we show that error-modulated learning emerges naturally at the circuit level when recurrent inhibition explicitly influences Hebbian plasticity. The same learning rule accounts for experimentally observed plasticity in the absence of inhibition and performs comparably to back-propagation of error (BP) on several non-linearly separable benchmarks. Our findings bridge the gap between functional and experimentally observed plasticity rules and make concrete predictions on inhibitory modulation of excitatory plasticity.
</details>
<details>
<summary>摘要</summary>
neronal 网络中实现信用分配的问题仍然是系统神经科学的中心问题。多个研究表明可能的解决方案是通过多层网络传递错误讯号的后填宽频率调控。这些仅功能上验证的模型假设了不同的 neuronal 分 compartment 来表示本地错误讯号，这些讯号决定了synaptic 静电� Monday 的方向。然而，这些Explicit error 调控是与现象学静电� Monday 模型不一致，这些模型中错误的方向主要取决于跟随频率的postsynaptic 活动。在这篇文章中，我们显示了一个可能的微网络模型和Hebbian 学习规则，这个规则是在数据控制理论框架下 derivation 的。我们假设错误是通过上方的抑制性 synaptic 联系传递的，我们显示出错误调控的学习现象将 naturally 出现在网络水平上，当recurrent 抑制性explicitly influencing Hebbian 学习。同时，我们显示了这个学习规则可以解释实验观察到的plasticity 现象，并且与back-propagation of error (BP) 相比，这个学习规则在多个非线性分类 benchmark 上表现出来的相似。我们的发现 bridge 了功能和实验观察到的静电� Monday 规则之间的差距，并且对抑制性的影响 på excitatory 静电� Monday 的学习提出了具体预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning"><a href="#Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning" class="headerlink" title="Efficient Exploration in Continuous-time Model-based Reinforcement Learning"></a>Efficient Exploration in Continuous-time Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19848">http://arxiv.org/abs/2310.19848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lenart Treven, Jonas Hübotter, Bhavya Sukhija, Florian Dörfler, Andreas Krause</li>
<li>for: 本研究旨在提出一种基于模型的强化学习算法，用于处理连续时间动态系统。</li>
<li>methods: 本paper使用非线性常微分方程（ODE）来表示连续时间动态系统，并使用受限概率模型来捕捉知识uncertainty。用户选择策略（MSS）来控制探索和观察。</li>
<li>results: 我们的 regret bound表明，使用GP动力学模型和合适的MSS可以实现下线性的征 regret。我们还提出了一种自适应、数据依赖的实用MSS，可以在几何上减少样本数量，并且在多个应用中显示出优势。<details>
<summary>Abstract</summary>
Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of continuous-time modeling over its discrete-time counterpart, as well as our proposed adaptive MSS over standard baselines, on several applications.
</details>
<details>
<summary>摘要</summary>
Note: The text is translated into Simplified Chinese, which is the standard writing system used in mainland China.Please note that the translation is done by a machine and may not be perfect, especially for idiomatic expressions and cultural references.
</details></li>
</ul>
<hr>
<h2 id="On-Feynman–Kac-training-of-partial-Bayesian-neural-networks"><a href="#On-Feynman–Kac-training-of-partial-Bayesian-neural-networks" class="headerlink" title="On Feynman–Kac training of partial Bayesian neural networks"></a>On Feynman–Kac training of partial Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19608">http://arxiv.org/abs/2310.19608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Zhao, Sebastian Mair, Thomas B. Schön, Jens Sjölund</li>
<li>for: 这个论文是为了提出一种高效的训练方法，以便在具有限制的参数 circumstance 下使用具有概率性的神经网络。</li>
<li>methods: 这种训练方法基于 Feynman–Kac 模型，并使用了继承 Monte Carlo 样本的变种来同时估计参数和秘密 posterior distribution。</li>
<li>results: 在各种 sintetic 和实际数据集上，这种训练方法比现有方法有更高的预测性能。<details>
<summary>Abstract</summary>
Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Kalman-Filters-Can-Filter"><a href="#Deep-Kalman-Filters-Can-Filter" class="headerlink" title="Deep Kalman Filters Can Filter"></a>Deep Kalman Filters Can Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19603">http://arxiv.org/abs/2310.19603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rishabhpahuja/Apple-Tracking">https://github.com/rishabhpahuja/Apple-Tracking</a></li>
<li>paper_authors: Blanka Hovart, Anastasis Kratsios, Yannick Limmer, Xuwei Yang</li>
<li>for: 这篇论文旨在为数字 kalman 筛（DKF）提供数学基础，以便DKF可以更加广泛地应用于数学金融中的模型调整和股票价格预测等领域。</li>
<li>methods: 这篇论文使用了一种名为 continuous-time DKF 的 neural network模型，该模型可以在基于数据的顺序序列中生成高度分布的概率测度。</li>
<li>results: 论文的结果表明，这种 continuous-time DKF 可以在一定的条件下，即 measurement noise 是可控的，以高度的准确性来实现模型的调整和股票价格预测。此外，论文还提供了一种用于评估DKF的数学基础的方法，该方法可以在具有不同概率分布的数据上进行预测。<details>
<summary>Abstract</summary>
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
</details>
<details>
<summary>摘要</summary>
深度卡尔曼筛 (DKF) 是一类基于神经网络的模型，用于生成来自顺序数据的高 probabilistic 度量。虽然 DKF 受 Kalman 筛的影响，但它们与传统的模型基于筛法的应用领域之间没有直接的理论联系，因此其应用范围受限于传统的模型基于筛法的应用领域，例如股票和期货价格的数学金融中的模型准确性。我们在数学深度学习的基础上解决这个问题，展示了一类可以高度度量的非马歇尔分布 Signal 过程的 conditional 法则，给出了噪声 kontinuierliche 测量结果的approximation 结果。我们的approximation 结果在sufficiently 紧密的Compact 集上uniform 地保持，错误量由worst-case 2-Wasserstein 距离来量化，uniform 地计算在给定 Compact 集上的所有路径上。
</details></li>
</ul>
<hr>
<h2 id="Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions"><a href="#Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions" class="headerlink" title="Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions"></a>Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19590">http://arxiv.org/abs/2310.19590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lin, Zhiping Mao, Zhicheng Wang, George Em Karniadakis</li>
<li>for: 解决具有锐解的 partiall differential equations (PDEs) 的问题</li>
<li>methods:  combinig Physics-informed Neural Networks (PINNs) 和 Deep Operator Network (DeepONet) 方法</li>
<li>results: 提出了一种新的 Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN) 方法，能够高效地解决具有锐解的 PDEs 问题，并且可以更好地处理不同类型的 boundary conditions 问题。<details>
<summary>Abstract</summary>
Physics-informed Neural Networks (PINNs) have been shown as a promising approach for solving both forward and inverse problems of partial differential equations (PDEs). Meanwhile, the neural operator approach, including methods such as Deep Operator Network (DeepONet) and Fourier neural operator (FNO), has been introduced and extensively employed in approximating solution of PDEs. Nevertheless, to solve problems consisting of sharp solutions poses a significant challenge when employing these two approaches. To address this issue, we propose in this work a novel framework termed Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize DeepONet to learn the solution operator for a set of smooth problems relevant to the PDEs characterized by sharp solutions. Subsequently, we integrate the pre-trained DeepONet with PINN to resolve the target sharp solution problem. We showcase the efficacy of OL-PINN by successfully addressing various problems, such as the nonlinear diffusion-reaction equation, the Burgers equation and the incompressible Navier-Stokes equation at high Reynolds number. Compared with the vanilla PINN, the proposed method requires only a small number of residual points to achieve a strong generalization capability. Moreover, it substantially enhances accuracy, while also ensuring a robust training process. Furthermore, OL-PINN inherits the advantage of PINN for solving inverse problems. To this end, we apply the OL-PINN approach for solving problems with only partial boundary conditions, which usually cannot be solved by the classical numerical methods, showing its capacity in solving ill-posed problems and consequently more complex inverse problems.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经被证明为解决部分梯度方程（PDEs）的前向和反向问题的有力方法。同时，神经网络方法，包括深度运算网络（DeepONet）和傅里叶神经网络（FNO），已经被引入并广泛应用于PDEs的解的 aproximation。然而，解决具有锐度解的问题却存在一定的挑战，当使用这两种方法时。为Addressing this issue, we propose in this work a novel framework termed Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize DeepONet to learn the solution operator for a set of smooth problems relevant to the PDEs characterized by sharp solutions. Subsequently, we integrate the pre-trained DeepONet with PINN to resolve the target sharp solution problem. We showcase the efficacy of OL-PINN by successfully addressing various problems, such as the nonlinear diffusion-reaction equation, the Burgers equation, and the incompressible Navier-Stokes equation at high Reynolds number. Compared with the vanilla PINN, the proposed method requires only a small number of residual points to achieve a strong generalization capability. Moreover, it substantially enhances accuracy, while also ensuring a robust training process. Furthermore, OL-PINN inherits the advantage of PINN for solving inverse problems. To this end, we apply the OL-PINN approach for solving problems with only partial boundary conditions, which usually cannot be solved by the classical numerical methods, showing its capacity in solving ill-posed problems and consequently more complex inverse problems.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing"><a href="#Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing" class="headerlink" title="Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing"></a>Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19589">http://arxiv.org/abs/2310.19589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jypark0/hermes">https://github.com/jypark0/hermes</a></li>
<li>paper_authors: Jung Yeon Park, Lawson L. S. Wong, Robin Walters</li>
<li>for: 解决 computer graphics 和生物物理系统中的数据上非欧氏空间的问题，这些问题常常是 surface mesh 的离散化表示。</li>
<li>methods: 我们使用 gauge equivariant 杂交网络，这种网络可以充分利用 surface mesh 的下面结构，并且可以处理复杂非线性动力学。</li>
<li>results: 我们的新方法可以在具有高度复杂和非线性动力学的 Domain 中达到更高的性能，但是设计决策还是会偏向于 convolutional、attentional 或 message passing 网络，具体取决于任务的需求。<details>
<summary>Abstract</summary>
Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit."中文翻译：数据在非欧几何空间上，通常为表面网格化的数据，在计算机图形和生物物理系统中自然出现。特别是，权值方程（PDE）在 manifold上的解决取决于下面的几何结构。虽然图 neural network 已经成功应用于 PDE，但它们不考虑表面几何和地方 gauge symmetry。相反，当前的 gauge 等变 convolutional 和注意力架构在网格上使用了下面的几何结构，但在模型表面 PDE 的复杂非线性动力学时表现不佳。为解决这些问题，我们引入了一种新的 gauge 等变架构，使用非线性消息传递。我们的新架构在具有高度复杂和非线性动力学的领域中表现更高效，而不同的任务的设计评估权衡会倾向于 convolutional、注意力或消息传递网络。我们在哪些情况下 investigate 我们的消息传递方法提供最大的优势。
</details></li>
</ul>
<hr>
<h2 id="Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees"><a href="#Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees" class="headerlink" title="Model Uncertainty based Active Learning on Tabular Data using Boosted Trees"></a>Model Uncertainty based Active Learning on Tabular Data using Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19573">http://arxiv.org/abs/2310.19573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath M Shankaranarayana</li>
<li>for: 这个论文主要针对的是活动学习在表格数据上的应用，具体来说是使用树 boosting 模型在活动学习中选择最有价值的数据实例进行模型训练，并且仅对这些实例进行人类标注。</li>
<li>methods: 本文使用的方法包括 uncertainty based sampling 和 cost-effective active learning，其中 uncertainty based sampling 是根据模型预测结果的最大不确定性选择数据实例进行人类标注，而 cost-effective active learning 则是一种基于模型预测结果的成本效果进行活动学习。</li>
<li>results: 本文的实验结果表明，使用 boosted trees 模型 uncertainty 方法在活动学习中可以提高模型的准确率，而且可以避免人类标注的成本。此外，本文还提出了一种新的成本效果的活动学习方法和一种改进的成本效果的活动学习方法，可以在 regression 和 classification 任务中具有更高的效果。<details>
<summary>Abstract</summary>
Supervised machine learning relies on the availability of good labelled data for model training. Labelled data is acquired by human annotation, which is a cumbersome and costly process, often requiring subject matter experts. Active learning is a sub-field of machine learning which helps in obtaining the labelled data efficiently by selecting the most valuable data instances for model training and querying the labels only for those instances from the human annotator. Recently, a lot of research has been done in the field of active learning, especially for deep neural network based models. Although deep learning shines when dealing with image\textual\multimodal data, gradient boosting methods still tend to achieve much better results on tabular data. In this work, we explore active learning for tabular data using boosted trees. Uncertainty based sampling in active learning is the most commonly used querying strategy, wherein the labels of those instances are sequentially queried for which the current model prediction is maximally uncertain. Entropy is often the choice for measuring uncertainty. However, entropy is not exactly a measure of model uncertainty. Although there has been a lot of work in deep learning for measuring model uncertainty and employing it in active learning, it is yet to be explored for non-neural network models. To this end, we explore the effectiveness of boosted trees based model uncertainty methods in active learning. Leveraging this model uncertainty, we propose an uncertainty based sampling in active learning for regression tasks on tabular data. Additionally, we also propose a novel cost-effective active learning method for regression tasks along with an improved cost-effective active learning method for classification tasks.
</details>
<details>
<summary>摘要</summary>
超vised机器学习需要有高质量的标签数据进行模型训练。标签数据通常通过人工标注获得，这是一个费时费力的过程，经常需要专家的帮助。活动学习是机器学习的一个子领域，它帮助在获取标签数据的过程中选择最有价值的数据实例进行模型训练，并且只需要对这些实例的标签进行人工标注。在深度学习领域，最近很多研究是关于活动学习，特别是基于深度神经网络的模型。虽然深度学习在处理图像\textual\多Modal数据方面表现出色，但是梯度提升方法在标量数据上仍然具有优异的表现。在这个工作中，我们探索了基于树的活动学习方法，并使用树的模型不确定性来选择需要人工标注的数据实例。在活动学习中，通常使用不确定性来评估模型的不确定性。然而，不确定性并不是模型不确定性的直接度量。虽然在深度学习中有很多研究是关于模型不确定性的测量和应用，但是它们尚未被应用于非神经网络模型。为了解决这个问题，我们研究了基于树的模型不确定性方法在活动学习中的效iveness。此外，我们还提出了一种新的cost-effective的活动学习方法和一种改进的cost-effective的活动学习方法 для类别任务。
</details></li>
</ul>
<hr>
<h2 id="DataZoo-Streamlining-Traffic-Classification-Experiments"><a href="#DataZoo-Streamlining-Traffic-Classification-Experiments" class="headerlink" title="DataZoo: Streamlining Traffic Classification Experiments"></a>DataZoo: Streamlining Traffic Classification Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19568">http://arxiv.org/abs/2310.19568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Luxemburk, Karel Hynek</li>
<li>for: This paper is written for researchers and practitioners working in the field of network traffic classification, particularly those who are interested in using machine learning techniques for this task.</li>
<li>methods: The paper introduces a toolset called DataZoo, which provides a standardized API for accessing three extensive datasets and includes methods for feature scaling and realistic dataset partitioning.</li>
<li>results: The paper aims to address the lack of standard benchmark datasets and supportive software in the field of network traffic classification, and to provide a toolset that simplifies the creation of realistic evaluation scenarios and makes it easier to cross-compare classification methods and reproduce results.<details>
<summary>Abstract</summary>
The machine learning communities, such as those around computer vision or natural language processing, have developed numerous supportive tools and benchmark datasets to accelerate the development. In contrast, the network traffic classification field lacks standard benchmark datasets for most tasks, and the available supportive software is rather limited in scope. This paper aims to address the gap and introduces DataZoo, a toolset designed to streamline dataset management in network traffic classification and to reduce the space for potential mistakes in the evaluation setup. DataZoo provides a standardized API for accessing three extensive datasets -- CESNET-QUIC22, CESNET-TLS22, and CESNET-TLS-Year22. Moreover, it includes methods for feature scaling and realistic dataset partitioning, taking into consideration temporal and service-related factors. The DataZoo toolset simplifies the creation of realistic evaluation scenarios, making it easier to cross-compare classification methods and reproduce results.
</details>
<details>
<summary>摘要</summary>
machine learning社区，如计算机视觉或自然语言处理等，已经开发出了许多支持工具和标准 benchmark datasets，以加速开发。然而，网络流量分类领域缺乏大多数任务的标准 benchmark datasets，可用的支持软件范围很限定。这篇论文旨在填补这个差距，并引入 DataZoo，一套设计用于加速网络流量分类 dataset 管理的工具集。DataZoo 提供了访问 CESNET-QUIC22、CESNET-TLS22 和 CESNET-TLS-Year22 三个广泛的 dataset 的标准 API。此外，它还包括了考虑时间和服务相关因素的Feature scaling 和实际dataset partitioning 方法。DataZoo 工具集使得创建实际的评估enario 变得更加简单，使得cross- compare 分类方法和重复结果变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-regression-for-robot-learning-on-manifolds"><a href="#Non-parametric-regression-for-robot-learning-on-manifolds" class="headerlink" title="Non-parametric regression for robot learning on manifolds"></a>Non-parametric regression for robot learning on manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19561">http://arxiv.org/abs/2310.19561</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. C. Lopez-Custodio, K. Bharath, A. Kucukyilmaz, S. P. Preston</li>
<li>for: 本研究旨在提出一种在拓扑空间上直接进行回归的”内在”方法，以提高 robot learning 中对 manifold-valued data 的处理精度。</li>
<li>methods: 该方法基于一种可靠的 probability distribution 在拓扑空间上，其参数函数是一个 predictor 变量，例如时间。通过一种 “local likelihood” 方法，使用 kernel 来估计该函数。</li>
<li>results: 实验结果表明，该方法在三种常见的拓扑空间上进行回归时，能够达到更高的预测精度，比 projection-based 算法更好。<details>
<summary>Abstract</summary>
Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood estimation. The approach is conceptually simple, and generally applicable to different manifolds. We implement it with three different types of manifold-valued data that commonly appear in robotics applications. The results of these experiments show better predictive accuracy than projection-based algorithms.
</details>
<details>
<summary>摘要</summary>
许多 robot 学习工具都是为欧几何数据设计的，但是许多 robotics 应用中的数据是 manifold-valued 数据。一个常见的例子是 orientation，它可以表示为 3x3 转换矩阵或 quarternion，这些空间都是非欧几何 manifold。在 robot 学习中， manifold-valued 数据通常是通过将 manifold 映射到适当的欧几何空间来处理，或者通过将数据投影到一个或多个 tangent space。这些方法可能会导致预测精度不高和算法复杂。在这篇论文中，我们提出了一种 "内在" 的方法来进行回归，即在 manifold 上直接使用适当的概率分布，让参数是一个时间变量的函数，然后使用 "本地概率" 方法来估算该函数，这种方法可以 incorporate 一个核函数。我们称之为 kernelised likelihood estimation。该方法概念简单，通用于不同的 manifold。我们在三种常见的 manifold-valued 数据中实现了这种方法，其中包括 rotation matrix、quaternion 和 pose 数据。实验结果显示，我们的方法的预测精度比投影基于的算法更高。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification"><a href="#Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification" class="headerlink" title="Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification"></a>Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19558">http://arxiv.org/abs/2310.19558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, Tony Q. S. Quek</li>
<li>for: 该论文探讨了 Federated Learning (FL) 领域中的一类非 convex 和非光滑损失函数问题，这些问题在 FL 应用中很普遍，但是它们的复杂非 convexity 和非光滑性 nature 以及保护客户端数据隐私的矛盾要求使其成为一个挑战。</li>
<li>methods: 该论文提出了一种基于 primal-dual 算法的 Federated Learning 算法，该算法通过对模型进行 bidirectional 模型缩放来适应非 convex 和非光滑 FL 问题，同时应用了差分隐私来保证强大的隐私保证。</li>
<li>results: 该论文的实验结果表明，提出的 Federated Learning 算法在实际世界数据上具有非常出色的效果，与一些状态当前的 FL 算法相比，具有明显的优势，同时 validate 了所有的分析结果和性质。<details>
<summary>Abstract</summary>
Federated learning (FL) has been recognized as a rapidly growing research area, where the model is trained over massively distributed clients under the orchestration of a parameter server (PS) without sharing clients' data. This paper delves into a class of federated problems characterized by non-convex and non-smooth loss functions, that are prevalent in FL applications but challenging to handle due to their intricate non-convexity and non-smoothness nature and the conflicting requirements on communication efficiency and privacy protection. In this paper, we propose a novel federated primal-dual algorithm with bidirectional model sparsification tailored for non-convex and non-smooth FL problems, and differential privacy is applied for strong privacy guarantee. Its unique insightful properties and some privacy and convergence analyses are also presented for the FL algorithm design guidelines. Extensive experiments on real-world data are conducted to demonstrate the effectiveness of the proposed algorithm and much superior performance than some state-of-the-art FL algorithms, together with the validation of all the analytical results and properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space"><a href="#Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space" class="headerlink" title="Approximation Theory, Computing, and Deep Learning on the Wasserstein Space"></a>Approximation Theory, Computing, and Deep Learning on the Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19548">http://arxiv.org/abs/2310.19548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimo Fornasier, Pascal Heid, Giacomo Enrico Sodini</li>
<li>for: 本研究 aimed at solving the challenging problem of approximating Sobolev-smooth functions defined on probability spaces, with a focus on the Wasserstein distance function.</li>
<li>methods: 本研究 employs three machine learning-based approaches: 1) solving a finite number of optimal transport problems and computing the corresponding Wasserstein potentials; 2) using empirical risk minimization with Tikhonov regularization in Wasserstein Sobolev spaces; 3) addressing the problem through the saddle point formulation of the Tikhonov functional’s Euler-Lagrange equation.</li>
<li>results: 本研究 provides explicit and quantitative bounds on generalization errors for each of these solutions, and the numerical implementation uses appropriately designed neural networks as basis functions, which can be rapidly evaluated after training, significantly enhancing the evaluation speed and surpassing state-of-the-art methods by several orders of magnitude.<details>
<summary>Abstract</summary>
The challenge of approximating functions in infinite-dimensional spaces from finite samples is widely regarded as formidable. In this study, we delve into the challenging problem of the numerical approximation of Sobolev-smooth functions defined on probability spaces. Our particular focus centers on the Wasserstein distance function, which serves as a relevant example. In contrast to the existing body of literature focused on approximating efficiently pointwise evaluations, we chart a new course to define functional approximants by adopting three machine learning-based approaches: 1. Solving a finite number of optimal transport problems and computing the corresponding Wasserstein potentials. 2. Employing empirical risk minimization with Tikhonov regularization in Wasserstein Sobolev spaces. 3. Addressing the problem through the saddle point formulation that characterizes the weak form of the Tikhonov functional's Euler-Lagrange equation. As a theoretical contribution, we furnish explicit and quantitative bounds on generalization errors for each of these solutions. In the proofs, we leverage the theory of metric Sobolev spaces and we combine it with techniques of optimal transport, variational calculus, and large deviation bounds. In our numerical implementation, we harness appropriately designed neural networks to serve as basis functions. These networks undergo training using diverse methodologies. This approach allows us to obtain approximating functions that can be rapidly evaluated after training. Consequently, our constructive solutions significantly enhance at equal accuracy the evaluation speed, surpassing that of state-of-the-art methods by several orders of magnitude.
</details>
<details>
<summary>摘要</summary>
《 Sobolev 函数的数值近似在无穷数据时的挑战》在这项研究中，我们围绕 Sobolev 平滑函数定义在概率空间上的数值近似问题进行了深入的研究。我们的特点是通过三种机器学习基于方法来定义函数近似：1. 解决有限多个交通问题，并计算相应的 Wasserstein 潜在函数。2. 使用 Wasserstein Sobolev 空间中的empirical risk minimization，并添加 Tikhonov 正则化。3. 通过极点形式来表示 Tikhonov 函数的弱形Euler-Lagrange方程。我们在证明中利用了度量 Sobolev 空间的理论，并结合了优化交通、变量 calculus 和大偏度上界。我们的数值实现中使用了适当设计的神经网络作为基函数。这些神经网络在训练过程中进行了改进，以便在训练后快速计算近似函数。因此，我们的构建解决方案可以快速提高评估速度，超过当前方法的几个数量级。
</details></li>
</ul>
<hr>
<h2 id="On-consequences-of-finetuning-on-data-with-highly-discriminative-features"><a href="#On-consequences-of-finetuning-on-data-with-highly-discriminative-features" class="headerlink" title="On consequences of finetuning on data with highly discriminative features"></a>On consequences of finetuning on data with highly discriminative features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19537">http://arxiv.org/abs/2310.19537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Masarczyk, Tomasz Trzciński, Mateusz Ostaszewski</li>
<li>for: 这篇论文旨在探讨在转移学习时，神经网络是如何保留先前学习的知识，以便用于新任务。</li>
<li>methods: 这篇论文使用了转移学习的方法，并分析了其对神经网络性能和内部表示的影响。</li>
<li>results: 研究发现，在使用转移学习时，神经网络可能会忽略有价值的先前学习特征，导致性能下降。这种现象被称为“特征衰变”。<details>
<summary>Abstract</summary>
In the era of transfer learning, training neural networks from scratch is becoming obsolete. Transfer learning leverages prior knowledge for new tasks, conserving computational resources. While its advantages are well-documented, we uncover a notable drawback: networks tend to prioritize basic data patterns, forsaking valuable pre-learned features. We term this behavior "feature erosion" and analyze its impact on network performance and internal representations.
</details>
<details>
<summary>摘要</summary>
在转移学习时代，从头开始训练神经网络已经变得过时。转移学习利用了先前学习的知识来解决新任务，并节省计算资源。虽然它的优点已经得到了广泛的文献记录，但我们发现了一个重要的缺点：神经网络往往会强调基本数据模式，抛弃价值的先前学习特征。我们称这种行为为“特征衰变”，并分析它对网络性能和内部表示的影响。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation"><a href="#Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation" class="headerlink" title="Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation"></a>Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19536">http://arxiv.org/abs/2310.19536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Zeyu He, Xiangyu Zhao, Jun Li</li>
<li>for: 本研究旨在解决学习奖励（LTR）问题，它是基于奖励学习的核心问题。在过去的方法中， Either introduce additional procedures for learning to reward, thereby increasing the complexity of optimization, or assume that user-agent interactions provide perfect demonstrations, which is not feasible in practice.</li>
<li>methods: 我们提议一种批量反式学习 paradigma，可以同时优化奖励和策略。我们使用折扣站点分布 corrections来结合LTR和推荐客户端评估。为了满足compositional requirement，我们引入了对照保守的概念，并通过bellman transform和KL regularization来约束 consecutive policy updates。</li>
<li>results: 我们在两个实际数据集上进行了empirical studies，结果表明，提议的方法可以相对提高效果（2.3%）和效率（11.53%）。<details>
<summary>Abstract</summary>
Rewards serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. In this research, we focus on the problem of learning to reward (LTR), which is fundamental to reinforcement learning. Previous approaches either introduce additional procedures for learning to reward, thereby increasing the complexity of optimization, or assume that user-agent interactions provide perfect demonstrations, which is not feasible in practice. Ideally, we aim to employ a unified approach that optimizes both the reward and policy using compositional demonstrations. However, this requirement presents a challenge since rewards inherently quantify user feedback on-policy, while recommender agents approximate off-policy future cumulative valuation. To tackle this challenge, we propose a novel batch inverse reinforcement learning paradigm that achieves the desired properties. Our method utilizes discounted stationary distribution correction to combine LTR and recommender agent evaluation. To fulfill the compositional requirement, we incorporate the concept of pessimism through conservation. Specifically, we modify the vanilla correction using Bellman transformation and enforce KL regularization to constrain consecutive policy updates. We use two real-world datasets which represent two compositional coverage to conduct empirical studies, the results also show that the proposed method relatively improves both effectiveness (2.3\%) and efficiency (11.53\%)
</details>
<details>
<summary>摘要</summary>
奖励 serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. 在这些研究中，我们关注了学习奖励（LTR）问题，这是基本的再奖学习问题。先前的方法可能会添加额外的过程来学习奖励，从而增加优化复杂度，或者假设用户-代理交互提供完美的示范，这并不是实际情况。理想情况是使用一种统一的方法，同时优化奖励和策略使用 Compositional demonstrations。然而，这种需求呈现出一个挑战，因为奖励自然地衡量用户反馈on-policy，而推荐代理 approximates off-policy future cumulative valuation。为解决这个挑战，我们提出了一种新的批量反向学习 paradigm。我们的方法使用折扣站立分布 corrections  combinese LTR和推荐代理评估。为了满足compositional requirement，我们在 vanilla correction 中添加了 Bellman transformation 和 KL regularization，以限制 consecutive policy updates。我们使用两个实际数据集，代表两种compositional coverage，进行empirical studies，结果还显示了我们提posed方法相对提高了效iveness（2.3%）和效率（11.53%）。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-Actor-Critic"><a href="#Decoupled-Actor-Critic" class="headerlink" title="Decoupled Actor-Critic"></a>Decoupled Actor-Critic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19527">http://arxiv.org/abs/2310.19527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Nauman, Marek Cygan</li>
<li>for: 本研究旨在解决actor-critic方法的两个矛盾问题，即评价器倾向于过度估计，需要从保守策略优化的下界Q值中采样 temporal-difference 目标；同时，已知的结果表明在不确定性面前，乐观的策略具有更低的 regret 水平。</li>
<li>methods: 我们提出了一种叫做Decoupled Actor-Critic（DAC）的离散actor-critic方法，该方法通过梯度反propagation来学习两种不同的演员：一个保守的演员用于 temporal-difference 学习，另一个乐观的演员用于探索。</li>
<li>results: 我们在DeepMind Control任务中进行了low和high replay ratio regime的测试，并对多个设计选择进行了简洁。 despite minimal computational overhead, DAC在locomotion任务上实现了state-of-the-art perfomance和样本效率。<details>
<summary>Abstract</summary>
Actor-Critic methods are in a stalemate of two seemingly irreconcilable problems. Firstly, critic proneness towards overestimation requires sampling temporal-difference targets from a conservative policy optimized using lower-bound Q-values. Secondly, well-known results show that policies that are optimistic in the face of uncertainty yield lower regret levels. To remedy this dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy algorithm that learns two distinct actors by gradient backpropagation: a conservative actor used for temporal-difference learning and an optimistic actor used for exploration. We test DAC on DeepMind Control tasks in low and high replay ratio regimes and ablate multiple design choices. Despite minimal computational overhead, DAC achieves state-of-the-art performance and sample efficiency on locomotion tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise"><a href="#Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise" class="headerlink" title="Generator Identification for Linear SDEs with Additive and Multiplicative Noise"></a>Generator Identification for Linear SDEs with Additive and Multiplicative Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19491">http://arxiv.org/abs/2310.19491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wang, Xi Geng, Wei Huang, Biwei Huang, Mingming Gong</li>
<li>for: 本研究探讨了如何从解决方程的分布来确定生成器的线性随机振荡方程（SDE）的生成器。</li>
<li>methods: 本研究使用了线性SDE的解决方程的分布来确定生成器。</li>
<li>results: 本研究提出了线性SDE的生成器确定性条件，包括加法噪声和乘法噪声两种类型的条件。这些条件是必要和足够的，可以用来在 causal inference 中识别生成器。此外，研究还提供了这些条件的 geometric 解释，以便更好地理解。<details>
<summary>Abstract</summary>
In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了线性随机杂分方程（SDE）生成器的标识条件，基于给定的固定初始状态的解过程的分布。这些标识条件是 causal inference 中 linear SDE 的关键因素，它们允许将 post-intervention 分布从 observational 分布中标识出来。我们得到了线性 SDE 加法噪声的必要和 suficient 条件，以及线性 SDE 乘法噪声的 suficient 条件。我们发现这些条件对于两种类型的 SDE 都是通用的。此外，我们还提供了这些标识条件的几何 интерпретаción，以增强它们的理解。为验证我们的理论结果，我们进行了一系列的仿真实验，支持和证明了我们的发现。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems"><a href="#Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems" class="headerlink" title="Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems"></a>Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19489">http://arxiv.org/abs/2310.19489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Trommer, Halil Yigit Oksuz</li>
<li>for: 用于非线性系统状态估计</li>
<li>methods: 使用机器学习的meta-学习方法和非线性变换Map来设计一个可靠的状态估计器</li>
<li>results: 通过实验结果，显示了高精度、泛化能力和噪声鲁棒性，并且可以在不同的系统条件和属性下进行在线适应<details>
<summary>Abstract</summary>
The theory of Kazantzis-Kravaris/Luenberger (KKL) observer design introduces a methodology that uses a nonlinear transformation map and its left inverse to estimate the state of a nonlinear system through the introduction of a linear observer state space. Data-driven approaches using artificial neural networks have demonstrated the ability to accurately approximate these transformation maps. This paper presents a novel approach to observer design for nonlinear dynamical systems through meta-learning, a concept in machine learning that aims to optimize learning models for fast adaptation to a distribution of tasks through an improved focus on the intrinsic properties of the underlying learning problem. We introduce a framework that leverages information from measurements of the system output to design a learning-based KKL observer capable of online adaptation to a variety of system conditions and attributes. To validate the effectiveness of our approach, we present comprehensive experimental results for the estimation of nonlinear system states with varying initial conditions and internal parameters, demonstrating high accuracy, generalization capability, and robustness against noise.
</details>
<details>
<summary>摘要</summary>
基于Kazantzis-Kravaris/Luenberger（KKL）观察器设计理论，本文提出了一种基于机器学习的观察器设计方法，通过非线性变换Map和其左逆函数来估算非线性系统的状态。使用人工神经网络进行数据驱动的方法已经证明了高度准确地表示这些变换 Map。本文提出了一种基于meta-学习的观察器设计方法，通过更好地理解下面学习问题的内在特性来优化学习模型，以便快速适应多种任务的分布。我们提出了一种基于测量系统输出信息的框架，用于设计一种可在线适应系统条件和特性的学习基于KKL观察器。为验证我们的方法的有效性，我们在不同的初始条件和内部参数下进行了广泛的实验，并达到了高精度、泛化能力和鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking"><a href="#Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking" class="headerlink" title="Grokking Tickets: Lottery Tickets Accelerate Grokking"></a>Grokking Tickets: Lottery Tickets Accelerate Grokking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19470">http://arxiv.org/abs/2310.19470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gouki510/grokking-tickets">https://github.com/gouki510/grokking-tickets</a></li>
<li>paper_authors: Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo</li>
<li>for: 这篇论文探讨了神经网络通过’’感知’’（Grokking）过程中的急剧改善，即一个网络从初始的记忆化解决方案转化为完美的通用解决方案。</li>
<li>methods: 作者使用了’’抽奖票’’（Grokking tickets）来描述这个过程的关键，即在精炼网络中找到’’好’’的子网络（good sparse subnetworks），以加速感知过程。</li>
<li>results: 作者通过多种配置（MLP和Transformer，以及数学和图像分类任务）和比较’’抽奖票’’和 dense 网络来证明 ‘’抽奖票’’ 的重要性，并发现在适当的剪除率下，感知可以在不使用 weight decay 的情况下完成。<details>
<summary>Abstract</summary>
Grokking is one of the most surprising puzzles in neural network generalization: a network first reaches a memorization solution with perfect training accuracy and poor generalization, but with further training, it reaches a perfectly generalized solution. We aim to analyze the mechanism of grokking from the lottery ticket hypothesis, identifying the process to find the lottery tickets (good sparse subnetworks) as the key to describing the transitional phase between memorization and generalization. We refer to these subnetworks as ''Grokking tickets'', which is identified via magnitude pruning after perfect generalization. First, using ''Grokking tickets'', we show that the lottery tickets drastically accelerate grokking compared to the dense networks on various configurations (MLP and Transformer, and an arithmetic and image classification tasks). Additionally, to verify that ''Grokking ticket'' are a more critical factor than weight norms, we compared the ''good'' subnetworks with a dense network having the same L1 and L2 norms. Results show that the subnetworks generalize faster than the controlled dense model. In further investigations, we discovered that at an appropriate pruning rate, grokking can be achieved even without weight decay. We also show that speedup does not happen when using tickets identified at the memorization solution or transition between memorization and generalization or when pruning networks at the initialization (Random pruning, Grasp, SNIP, and Synflow). The results indicate that the weight norm of network parameters is not enough to explain the process of grokking, but the importance of finding good subnetworks to describe the transition from memorization to generalization. The implementation code can be accessed via this link: \url{https://github.com/gouki510/Grokking-Tickets}.
</details>
<details>
<summary>摘要</summary>
干脆是神经网络泛化的一个最有趣的谜题：一个网络在完美地训练时达到了记忆解决方案，但是在进一步训练时达到了完美的泛化解决方案。我们想要分析干脆的机制，从抽奖签分 hypothesis开始，identify the process of finding good sparse subnetworks（即''Grokking tickets''）作为描述 transition phase between memorization and generalization的关键。我们将这些子网络称为''干脆签''，通过 magnitude pruning 后的完美泛化来标识。我们发现，使用 ''干脆签'' 可以在不同的配置（MLP和Transformer，以及数学和图像分类任务）上加速干脆，并且比 dense network 更快。此外，我们还发现，在适当的剪枝率下，可以通过剪枝来实现干脆，而不需要weight decay。此外，我们发现，在使用 ''干脆签'' 时，速度不会增加，而是在初始化（Random pruning、Grasp、SNIP和Synflow）时剪枝时才会增加速度。这些结果表明，网络参数的重量 нор 不能完全解释干脆的过程，而是找到好的 sparse subnetworks 来描述 transition phase between memorization and generalization 的重要性。实现代码可以通过以下链接获取：https://github.com/gouki510/Grokking-Tickets。
</details></li>
</ul>
<hr>
<h2 id="Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems"><a href="#Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems" class="headerlink" title="Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems"></a>Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19468">http://arxiv.org/abs/2310.19468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Yi</li>
<li>For: 这个论文的目的是研究多智能合作学习（MACL）系统在Sequential Decision Making（SDM）问题上的设计和分析。* Methods: 本论文使用了多智能合作学习算法来解决SDM问题，并研究了不同的sequential decision making问题，包括多智能多投机问题、全信息或投机反馈问题等。* Results: 本论文提出了一系列的 regret lower bounds，用于衡量多智能合作学习算法在SDM问题上的性能。这些 regret lower bounds 适用于不同的communication network和communication delay情况，可以帮助设计MACL系统的communication协议。<details>
<summary>Abstract</summary>
A Multi-Agent Cooperative Learning (MACL) system is an artificial intelligence (AI) system where multiple learning agents work together to complete a common task. Recent empirical success of MACL systems in various domains (e.g. traffic control, cloud computing, robotics) has sparked active research into the design and analysis of MACL systems for sequential decision making problems. One important metric of the learning algorithm for decision making problems is its regret, i.e. the difference between the highest achievable reward and the actual reward that the algorithm gains. The design and development of a MACL system with low-regret learning algorithms can create huge economic values. In this thesis, I analyze MACL systems for different sequential decision making problems. Concretely, the Chapter 3 and 4 investigate the cooperative multi-agent multi-armed bandit problems, with full-information or bandit feedback, in which multiple learning agents can exchange their information through a communication network and the agents can only observe the rewards of the actions they choose. Chapter 5 considers the communication-regret trade-off for online convex optimization in the distributed setting. Chapter 6 discusses how to form high-productive teams for agents based on their unknown but fixed types using adaptive incremental matchings. For the above problems, I present the regret lower bounds for feasible learning algorithms and provide the efficient algorithms to achieve this bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the regret depends on the connectivity of the communication network and the communication delay, thus giving useful guidance on design of the communication protocol in MACL systems
</details>
<details>
<summary>摘要</summary>
一个多智能合作学习（MACL）系统是一个人工智能（AI）系统，其中多个学习代理共同完成一项共同任务。在不同领域（如交通控制、云计算、机器人等）的实践成功已经引起了关于MACL系统的设计和分析的活跃研究。一个重要的学习算法度量 для决策问题是它的 regret，即实际获得的奖励与最高可能获得的奖励之差。设计和开发一个MACL系统 WITH low-regret 学习算法可以创造巨大的经济价值。在这个论文中，我分析了MACL系统在不同的决策问题上。特别是第3章和第4章研究了多智能合作多臂投机问题，包括全信息或投机反馈，在其中多个学习代理可以通过通信网络进行信息交换，代理只能观察它们选择的动作的奖励。第5章考虑了在分布式环境中的通信 regret 与满意度之间的贸易。第6章讨论了如何基于代理的未知但固定类型形成高产力团队，使用适应增量匹配。对于这些问题，我提供了可行的学习算法的 regret 下界，并提供了有效的算法来实现这个下界。我在第3、4、5章中提供的 regret 下界表明了通信网络的连接性和通信延迟对 regret 的影响，因此为设计通信协议的设计提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation"><a href="#MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation" class="headerlink" title="MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation"></a>MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19454">http://arxiv.org/abs/2310.19454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrani Kumari, Rahul Siddharthan<br>for:This paper proposes new algorithms for two tasks related to heterogeneous tabular datasets: clustering and synthetic data generation.methods:The proposed algorithms use an EM-based clustering method called MMM (Madras Mixture Model) and a synthetic data generation method called MMMsynth, which pre-clusters the input data and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns.results:The proposed algorithms outperform standard algorithms in determining clusters in synthetic heterogeneous data and recover structure in real data. The synthetic tabular data generation algorithm approaches the performance of training purely with real data and outperforms other literature tabular-data generators.<details>
<summary>Abstract</summary>
We provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. Tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. Moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.   We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture Model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. Based on this, we demonstrate a synthetic tabular data generation algorithm, MMMsynth, that pre-clusters the input data, and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns. We benchmark this algorithm by testing the performance of standard ML algorithms when they are trained on synthetic data and tested on real published datasets. Our synthetic data generation algorithm outperforms other literature tabular-data generators, and approaches the performance of training purely with real data.
</details>
<details>
<summary>摘要</summary>
我们提供了新的算法对异构表格数据进行划分和生成 sintetico 数据。表格数据通常包含异构数据类型（数值、排序、分类）的列，但也可能具有隐藏的划分结构在行中：例如，它们可能来自不同的地理、社会经济、方法ологических 源泉，因此结果变量（例如疾病存在）可能会受到其他变量以及划分上下文的影响。此外，生物医学数据分享受到患者隐私法规的限制，现在有兴趣于使用深度学习生成 sintetico 表格数据。我们描述了一种新的EM基于的划分算法，MMM（Madras Mixture Model），它在异构数据上比标准算法更高效地划分划分结构，并在实际数据上回归结构。基于这种算法，我们提出了一种生成 sintetico 表格数据的算法，MMMsynth，它先划分输入数据，然后为每个划分群分别生成假数据，假设每列的数据分布为划分群pecific的数据分布。我们对这种算法进行了 benchmark，测试了使用生成的 sintetico 数据训练标准机器学习算法，并测试在实际发表的数据上。我们发现，我们的 sintetico 数据生成算法在与标准Literatura 表格数据生成算法进行比较时表现更好，并且在训练只使用实际数据时的性能接近于使用实际数据进行训练。
</details></li>
</ul>
<hr>
<h2 id="Hodge-Compositional-Edge-Gaussian-Processes"><a href="#Hodge-Compositional-Edge-Gaussian-Processes" class="headerlink" title="Hodge-Compositional Edge Gaussian Processes"></a>Hodge-Compositional Edge Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19450">http://arxiv.org/abs/2310.19450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maosheng Yang, Viacheslav Borovitskiy, Elvin Isufi</li>
<li>for: 本文提出了基于 Gaussian processes（GPs）的方法，用于模型 simplicial 2-complex 上的函数，这种结构类似于图，但允许边界形成三角形面。这种方法适用于学习网络上的流数据，其边界流可以表示为离散异常和旋流的积分。</li>
<li>methods: 本文首先开发了 divergence-free 和 curl-free 边GPs 类，适用于不同应用场景。然后， authors 将这些类合并成 \emph{Hodge-compositional edge GPs}，可以表示任何边函数。这些 GPs 允许直接和独立地学习不同 Hodge  ком成分的边函数，以便在 hyperparameter 优化中捕捉它们的相关性。</li>
<li>results:  authors 在 currency exchange, ocean flows 和 water supply 网络中应用了这些 GPs，与其他模型进行比较。结果表明，Hodge-compositional edge GPs 能够更好地捕捉流数据的特点，并且在 hyperparameter 优化中表现更好。<details>
<summary>Abstract</summary>
We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
</details>
<details>
<summary>摘要</summary>
我们提出原理式加aussian processes（GP）来模型 simplicial 2-complex 上的函数，这种结构类似于图，在其边界上可能会形成三角形面。这种方法适用于学习网络上流体数据，其边界流可以通过离散异变和旋转来描述。基于哈代分解，我们首先开发了不同的漏斗和旋转自由边GP，适用于各种应用。然后，我们将它们组合成\emph{Hodge-compositional edge GP}，可以表示任何边函数。这些GP 使得边函数的不同哈代组件可以独立地学习，以便捕捉其相关性 durante 超参数优化。为了强调其实用性，我们在货币交易、海洋流和水利网络中应用了它们，并与其他模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="A-Federated-Learning-Framework-for-Stenosis-Detection"><a href="#A-Federated-Learning-Framework-for-Stenosis-Detection" class="headerlink" title="A Federated Learning Framework for Stenosis Detection"></a>A Federated Learning Framework for Stenosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19445">http://arxiv.org/abs/2310.19445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariachiara Di Cosmo, Giovanna Migliorelli, Matteo Francioni, Andi Mucaj, Alessandro Maolo, Alessandro Aprile, Emanuele Frontoni, Maria Chiara Fiorentino, Sara Moccia</li>
<li>for: 这项研究探讨了基于联合学习（Federated Learning，FL）的 coronary angiography 图像（CA）中的狭窄血管检测。</li>
<li>methods: 我们使用了一种基于 Faster R-CNN 模型的狭窄血管检测方法，并在两个不同机构的数据上进行了训练。在我们的 FL 框架中，只有模型背部的参数被共享，使用了联合平均（FedAvg）来聚合参数。</li>
<li>results: 我们的结果表明，FL 框架对客户端 2 的性能没有显著影响， client 2 的本地训练已经达到了良好的性能。而对 client 1 来说，FL 框架提高了性能，具体来说，与本地模型相比，FL 框架提高了 +3.76%、+17.21% 和 +10.80% 的精度、回归率和 F1 分别。最终，我们达到了 P rec &#x3D; 73.56，Rec &#x3D; 67.01 和 F1 &#x3D; 70.13。这些结果表明，FL 可以帮助多中心研究自动 CA 图像中的狭窄血管检测，同时保持患者隐私。<details>
<summary>Abstract</summary>
This study explores the use of Federated Learning (FL) for stenosis detection in coronary angiography images (CA). Two heterogeneous datasets from two institutions were considered: Dataset 1 includes 1219 images from 200 patients, which we acquired at the Ospedale Riuniti of Ancona (Italy); Dataset 2 includes 7492 sequential images from 90 patients from a previous study available in the literature. Stenosis detection was performed by using a Faster R-CNN model. In our FL framework, only the weights of the model backbone were shared among the two client institutions, using Federated Averaging (FedAvg) for weight aggregation. We assessed the performance of stenosis detection using Precision (P rec), Recall (Rec), and F1 score (F1). Our results showed that the FL framework does not substantially affects clients 2 performance, which already achieved good performance with local training; for client 1, instead, FL framework increases the performance with respect to local model of +3.76%, +17.21% and +10.80%, respectively, reaching P rec = 73.56, Rec = 67.01 and F1 = 70.13. With such results, we showed that FL may enable multicentric studies relevant to automatic stenosis detection in CA by addressing data heterogeneity from various institutions, while preserving patient privacy.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Federated Learning" (FL) is translated as "合作学习" (hezuo xuexí)* "coronary angiography images" (CA) is translated as "心血管影像" (xin xue kan yingxiang)* "stenosis detection" is translated as "stenosis检测" (stenosis jiandete)* "Faster R-CNN" is translated as "更快的R-CNN" (gengkuai de R-CNN)* "Precision" (Precision), "Recall" (Recall), and "F1 score" (F1) are translated as "准确率" (zhèngzhèng lǐ), "回归率" (huíguī lǐ), and "F1分数" (F1 fēnshū) respectively.* "client institutions" is translated as "客户机构" (kehu jigou)* "local training" is translated as "本地训练" (ben di xùntraining)* "FL framework" is translated as "FL框架" (FL kuàikāng)* "weight aggregation" is translated as "weight合并" (weight hebing)
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications"><a href="#Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications" class="headerlink" title="Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications"></a>Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19439">http://arxiv.org/abs/2310.19439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintian Ren, Jun Wu, Hansong Xu, Qianqian Pan<br>for:The paper aims to address the security vulnerabilities of semantic communication systems by proposing a secure semantic communication system called DiffuSeC.methods:The proposed system utilizes a diffusion model and deep reinforcement learning (DRL) to mitigate semantic perturbations, including data source attacks and channel attacks. Additionally, a DRL-based channel-adaptive diffusion step selection scheme is developed to improve robustness under unstable channel conditions.results:The proposed DiffuSeC system demonstrates higher robust accuracy than previous works under a wide range of channel conditions, and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.<details>
<summary>Abstract</summary>
Semantic communication has emerged as a new deep learning-based communication paradigm that drives the research of end-to-end data transmission in tasks like image classification, and image reconstruction. However, the security problem caused by semantic attacks has not been well explored, resulting in vulnerabilities within semantic communication systems exposed to potential semantic perturbations. In this paper, we propose a secure semantic communication system, DiffuSeC, which leverages the diffusion model and deep reinforcement learning (DRL) to address this issue. With the diffusing module in the sender end and the asymmetric denoising module in the receiver end, the DiffuSeC mitigates the perturbations added by semantic attacks, including data source attacks and channel attacks. To further improve the robustness under unstable channel conditions caused by semantic attacks, we developed a DRL-based channel-adaptive diffusion step selection scheme to achieve stable performance under fluctuating environments. A timestep synchronization scheme is designed for diffusion timestep coordination between the two ends. Simulation results demonstrate that the proposed DiffuSeC shows higher robust accuracy than previous works under a wide range of channel conditions, and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.
</details>
<details>
<summary>摘要</summary>
《含义通信：一种新的深度学习基于的通信模式》（DiffuSeC）是一种新的安全含义通信系统，它利用扩散模型和深度强化学习（DRL）来解决含义攻击的安全问题。在发送端有扩散模块，而接收端有非对称减噪模块，DiffuSeC可以减轻由含义攻击引起的扰动。为了进一步提高不稳定的通信环境下的稳定性，我们开发了基于DRL的通道适应扩散步选择策略，以实现在不稳定的环境下稳定的性能。同时，我们还设计了一种时间步同步方案，以确保扩散步的协调。实验结果表明，提案的DiffuSeC在各种通信环境下 exhibits higher robust accuracy than previous works, and can quickly adjust model state according to signal-to-noise ratios (SNRs) in unstable environments.
</details></li>
</ul>
<hr>
<h2 id="LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation"><a href="#LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation" class="headerlink" title="LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation"></a>LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19394">http://arxiv.org/abs/2310.19394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dang Minh Nguyen, Chenfei Wang, Yan Shen, Yifan Zeng</li>
<li>for: 这个研究旨在应用图形神经网络（GNN）解决大规模电子商务推荐中的项目搜寻问题。</li>
<li>methods: 我们使用了一种简单 yet novel和具有影响力的图构成、模型和资料偏好处理技术。具体来说，我们结合强信号用户行为和高精度共同推荐（CF）算法来建立高质量的项目图。然后，我们开发了一个名为LightSAGE的新的GNN架构，以生成高品质的项目嵌入 Vector 搜寻。</li>
<li>results: 我们的模型在线上评估、A&#x2F;B 测试和生产环境中获得改善，并且在Shopee 的推荐广告系统中部署。<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) is the trending solution for item retrieval in recommendation problems. Most recent reports, however, focus heavily on new model architectures. This may bring some gaps when applying GNN in the industrial setup, where, besides the model, constructing the graph and handling data sparsity also play critical roles in the overall success of the project. In this work, we report how GNN is applied for large-scale e-commerce item retrieval at Shopee. We introduce our simple yet novel and impactful techniques in graph construction, modeling, and handling data skewness. Specifically, we construct high-quality item graphs by combining strong-signal user behaviors with high-precision collaborative filtering (CF) algorithm. We then develop a new GNN architecture named LightSAGE to produce high-quality items' embeddings for vector search. Finally, we design multiple strategies to handle cold-start and long-tail items, which are critical in an advertisement (ads) system. Our models bring improvement in offline evaluations, online A/B tests, and are deployed to the main traffic of Shopee's Recommendation Advertisement system.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是当前推荐问题的流行解决方案。然而，最新的报告强调新的模型架构，这可能会导致在实际应用中，除了模型外，构建图和处理数据稀缺性也发挥关键作用。在这项工作中，我们报道了在大规模电商Item Retrieval中使用GNN的应用。我们介绍了我们的简单 yet novel和重要的图建构、模型化和数据偏好技术。具体来说，我们组合强signal用户行为和高精度的collaborative filtering（CF）算法来构建高质量的item图。然后，我们开发了一种新的GNN架构名为LightSAGE，以生成高质量的项 embeddings  дляvector搜索。最后，我们设计了多种方法来处理冷启动和长尾项，这些方法在广告（ads）系统中是关键的。我们的模型在线评估、在线A/B测试中提高了性能，并被部署到Shopee推荐广告系统的主要流量中。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness"><a href="#Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness" class="headerlink" title="Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness"></a>Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19391">http://arxiv.org/abs/2310.19391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ehyaei/Causal-Fair-Metric-Learning">https://github.com/Ehyaei/Causal-Fair-Metric-Learning</a></li>
<li>paper_authors: Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi</li>
<li>for: 本研究旨在提出一种基于 causal 结构的 fair 度量，以便在机器学习模型中实现公平待遇。</li>
<li>methods: 本研究使用了 causal 推理来定义一种新的 fair 度量，并提出了一种基于 protected causal perturbation 的 robustness 分析方法。</li>
<li>results: 本研究的结果表明，新引入的 fair 度量可以准确地捕捉机器学习模型中的不公平现象，并且可以在不同的应用场景中进行metric learning和投入。<details>
<summary>Abstract</summary>
Adversarial perturbation is used to expose vulnerabilities in machine learning models, while the concept of individual fairness aims to ensure equitable treatment regardless of sensitive attributes. Despite their initial differences, both concepts rely on metrics to generate similar input data instances. These metrics should be designed to align with the data's characteristics, especially when it is derived from causal structure and should reflect counterfactuals proximity. Previous attempts to define such metrics often lack general assumptions about data or structural causal models. In this research, we introduce a causal fair metric formulated based on causal structures that encompass sensitive attributes. For robustness analysis, the concept of protected causal perturbation is presented. Additionally, we delve into metric learning, proposing a method for metric estimation and deployment in real-world problems. The introduced metric has applications in the fields adversarial training, fair learning, algorithmic recourse, and causal reinforcement learning.
</details>
<details>
<summary>摘要</summary>
<INST>这里使用敌对扰乱来曝光机器学习模型的漏洞，而个人公平性的概念则希望在敏感特征上进行公平对待。这两个概念都需要基于度量来生成相似的输入数据实例。这些度量应该和数据的特点相匹配，特别是当它们来自 causal 结构时。先前的尝试定义这些度量通常缺乏一般对数据或结构 causal 模型的假设。在这个研究中，我们引入了基于 causal 结构的公平度量，并提出了保护 causal 扰乱的概念。此外，我们还进行了度量学习，提出了度量估计和部署在实际问题上的方法。引入的度量具有敌对训练、公平学习、算法公平和 causal 强化学习等应用。</INST>Note that Simplified Chinese is used here, as it is more widely used in mainland China and other parts of the world. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Implicit-Manifold-Gaussian-Process-Regression"><a href="#Implicit-Manifold-Gaussian-Process-Regression" class="headerlink" title="Implicit Manifold Gaussian Process Regression"></a>Implicit Manifold Gaussian Process Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19390">http://arxiv.org/abs/2310.19390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernardo Fichera, Viacheslav Borovitskiy, Andreas Krause, Aude Billard</li>
<li>for: 这 paper 是为了提高 Gaussian process regression 在高维数据上的预测性能和准确性而写的。</li>
<li>methods: 这 paper 使用了一种新的方法，可以直接从数据中推导出隐藏的 manifold 结构，而不需要显式提供 manifold 结构。这种方法 是基于 fully differentiable 的 Gaussian process regression 技术。</li>
<li>results: 这 paper 的结果表明，使用这种新方法可以提高 Gaussian process regression 在高维数据上的预测性能和准确性，并且可以处理大量数据点（上千个数据点）。<details>
<summary>Abstract</summary>
Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of the standard Gaussian process regression in high-dimensional~settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators"><a href="#Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators" class="headerlink" title="Gradient-free online learning of subgrid-scale dynamics with neural emulators"></a>Gradient-free online learning of subgrid-scale dynamics with neural emulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19385">http://arxiv.org/abs/2310.19385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugo Frezat, Guillaume Balarac, Julien Le Sommer, Ronan Fablet</li>
<li>for: 本研究提出了一种通用算法，用于在线训练基于机器学习的子网络参数化，即使 numerical solvers 无法导数。</li>
<li>methods: 提议的方法利用神经emuulator来训练减少状态空间解决方案的近似，然后使用这个近似来允许时间推进中的梯度传播。</li>
<li>results: 实验表明，通过分别训练神经emuulator和参数化组件，可以减少一些近似偏差的传播。<details>
<summary>Abstract</summary>
In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种通用的算法，用于在线训练基于机器学习的子网络参数化，即使使用不可导的数值方法。我们的方法利用神经 emulator 来训练一个简化的状态空间解决方案的approximation，然后使用这个approximation来允许时间整合步骤中的梯度传播。我们的算法可以在大多数情况下重新获得在线策略中的优点，而不需要计算原始解决方案的梯度。我们还证明了在训练神经 emulator 和参数化组件时分别使用相应的损失函数是必要的，以避免某些近似误差的卷积。
</details></li>
</ul>
<hr>
<h2 id="Deep-anytime-valid-hypothesis-testing"><a href="#Deep-anytime-valid-hypothesis-testing" class="headerlink" title="Deep anytime-valid hypothesis testing"></a>Deep anytime-valid hypothesis testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19384">http://arxiv.org/abs/2310.19384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teodora Pandeva, Patrick Forré, Aaditya Ramdas, Shubhanshu Shekhar</li>
<li>for: 这 paper 的目的是提出一种通用的框架，用于构建强大的序列假设测试方法，以解决一类非参数测试问题。</li>
<li>methods: 这 paper 使用了两个已知操作符来定义 null 假设，并通过 continuous monitoring 和有效地聚合证据来快速验证 null 假设。</li>
<li>results: 实验结果表明，使用这 paper 提出的方法可以与专门的基eline测试方法竞争，并且可以适应不知道的问题难度。<details>
<summary>Abstract</summary>
We propose a general framework for constructing powerful, sequential hypothesis tests for a large class of nonparametric testing problems. The null hypothesis for these problems is defined in an abstract form using the action of two known operators on the data distribution. This abstraction allows for a unified treatment of several classical tasks, such as two-sample testing, independence testing, and conditional-independence testing, as well as modern problems, such as testing for adversarial robustness of machine learning (ML) models. Our proposed framework has the following advantages over classical batch tests: 1) it continuously monitors online data streams and efficiently aggregates evidence against the null, 2) it provides tight control over the type I error without the need for multiple testing correction, 3) it adapts the sample size requirement to the unknown hardness of the problem. We develop a principled approach of leveraging the representation capability of ML models within the testing-by-betting framework, a game-theoretic approach for designing sequential tests. Empirical results on synthetic and real-world datasets demonstrate that tests instantiated using our general framework are competitive against specialized baselines on several tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了一个通用框架，用于构建强大、顺序的假设测试，用于覆盖一类非参数测试问题。 null假设使用两个已知运算符来定义数据分布。这种抽象允许我们对多种传统任务，如两个样本测试、独立测试和条件独立测试，以及现代问题，如机器学习模型对抗性测试，进行统一处理。我们的提议的优点包括：1) Continuously monitoring online数据流并有效地聚合对null的证据，2) 不需要多测试修正，可以保持控制型I错误的紧张，3) 可以根据未知问题的难度自适应样本大小。我们开发了一种基于测试-by-betting框架的原则，一种基于游戏理论的方法，用于设计顺序测试。实验结果表明，使用我们的通用框架实例化的测试与特殊基准相比，在多个任务上具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Musical-Form-Generation"><a href="#Musical-Form-Generation" class="headerlink" title="Musical Form Generation"></a>Musical Form Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19842">http://arxiv.org/abs/2310.19842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1">https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1</a></li>
<li>paper_authors: Lilac Atassi</li>
<li>for: 这篇论文旨在生成结构化、可持续演奏的乐曲。</li>
<li>methods: 该方法使用条件生成模型创建乐曲段落，并在这些段落之间进行转折。而高级 Compositional 提示的生成则由大型自然语言模型完成。</li>
<li>results: 该方法可生成结构化、可持续演奏的乐曲，并且可以提供多种不同的 musical form。<details>
<summary>Abstract</summary>
While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
</details>
<details>
<summary>摘要</summary>
近期的生成模型可以生成有趣的音乐，但它们的用途受限。音乐的变化通常由偶合机会决定，导致作品缺乏结构。extend beyond a minute的作品可能会变得无法理解或重复。这篇论文提出了一种生成结构化、无限长的音乐作品的方法。中心思想是通过决定 Musical segments的conditional生成模型，并在这些段落之间进行过渡。Prompt的生成，用于决定高级作曲形式，与生成细节的详细进行分离。然后，一个大型语言模型被用来建议音乐的形式。
</details></li>
</ul>
<hr>
<h2 id="An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions"><a href="#An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions" class="headerlink" title="An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions"></a>An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19841">http://arxiv.org/abs/2310.19841</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nus-dbe/truck-driver-safety-climate">https://github.com/nus-dbe/truck-driver-safety-climate</a></li>
<li>paper_authors: Kailai Sun, Tianxiang Lan, Yang Miang Goh, Sufiana Safiena, Yueng-Hsiang Huang, Bailey Lytle, Yimin He</li>
<li>for: 本研究旨在探讨大客车驾驶员的安全氛围因素和其影响组织内部的安全性。</li>
<li>methods: 本研究使用了5种不同的聚类算法来分析大客车驾驶员对安全氛围的感受，并引入了可解释的机器学习度量来解释聚类结果。</li>
<li>results: 研究发现，对大客车驾驶员的安全氛围感受进行聚类分析可以分出不同的驾驶群体，并发现监督者关心帮助分开不同驾驶群体。<details>
<summary>Abstract</summary>
The transportation industry, particularly the trucking sector, is prone to workplace accidents and fatalities. Accidents involving large trucks accounted for a considerable percentage of overall traffic fatalities. Recognizing the crucial role of safety climate in accident prevention, researchers have sought to understand its factors and measure its impact within organizations. While existing data-driven safety climate studies have made remarkable progress, clustering employees based on their safety climate perception is innovative and has not been extensively utilized in research. Identifying clusters of drivers based on their safety climate perception allows the organization to profile its workforce and devise more impactful interventions. The lack of utilizing the clustering approach could be due to difficulties interpreting or explaining the factors influencing employees' cluster membership. Moreover, existing safety-related studies did not compare multiple clustering algorithms, resulting in potential bias. To address these issues, this study introduces an interpretable clustering approach for safety climate analysis. This study compares 5 algorithms for clustering truck drivers based on their safety climate perceptions. It proposes a novel method for quantitatively evaluating partial dependence plots (QPDP). To better interpret the clustering results, this study introduces different interpretable machine learning measures (SHAP, PFI, and QPDP). Drawing on data collected from more than 7,000 American truck drivers, this study significantly contributes to the scientific literature. It highlights the critical role of supervisory care promotion in distinguishing various driver groups. The Python code is available at https://github.com/NUS-DBE/truck-driver-safety-climate.
</details>
<details>
<summary>摘要</summary>
交通业界，特别是卡车运输业，具有高度的工作意外和死亡率。大型卡车相关的意外占了交通意外总死亡人数的许多。为了预防意外，研究人员对安全气候的因素进行了广泛的研究，并且尝试了度量其影响。现有的数据驱动的安全气候研究已经做出了很大的进步，但是使用受众分 clustering 方法仍然是一个新的探索。通过分组 drivers 根据他们的安全气候观点，企业可以对员工进行资料分析和更有效的干预措施。然而，使用 clustering 方法可能会受到几个因素的限制，例如：解释和解释 clustering 结果的困难，以及现有的安全相关研究未能比较多种 clustering 算法，这可能会导致偏见。这个研究采用了一个可解释的 clustering 方法，并且比较了5种 clustering 算法，以及一个新的量化评估方法（QPDP）。这个研究还引入了不同的可解释机器学习度量（SHAP、PFI和QPDP），以便更好地解释 clustering 结果。基于超过7,000名美国卡车司机的数据，这个研究具有很大的科学文献意义。它显示出监管者关爱宣传的重要性，以区分不同的司机群体。Python 代码可以在 GitHub 上获取：https://github.com/NUS-DBE/truck-driver-safety-climate。
</details></li>
</ul>
<hr>
<h2 id="ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting"><a href="#ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting"></a>ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19322">http://arxiv.org/abs/2310.19322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>for: 这 paper 是为了解决多时间序列预测问题，提出了一种基于深度学习的 ProNet 模型，可以同时利用 AR 和 NAR 策略。</li>
<li>methods: ProNet 模型使用了 segmentation 技术，将预测时间轴分成多个段落，并在每个段落中采用非autoregressive 策略预测最重要的时间步骤，而其余时间步骤则通过 autoregressive 策略预测。 segmentation 过程基于 latent variables，可以有效地捕捉每个时间步骤的重要性。</li>
<li>results: ProNet 模型在四个大规模数据集上进行了广泛的评估，并对 ProNet 模型进行了ablation study。结果显示，ProNet 模型在精度和预测速度两个方面表现出色，与 AR 和 NAR 预测模型相比，具有更高的准确率和更快的预测速度。<details>
<summary>Abstract</summary>
In this paper, we introduce ProNet, an novel deep learning approach designed for multi-horizon time series forecasting, adaptively blending autoregressive (AR) and non-autoregressive (NAR) strategies. Our method involves dividing the forecasting horizon into segments, predicting the most crucial steps in each segment non-autoregressively, and the remaining steps autoregressively. The segmentation process relies on latent variables, which effectively capture the significance of individual time steps through variational inference. In comparison to AR models, ProNet showcases remarkable advantages, requiring fewer AR iterations, resulting in faster prediction speed, and mitigating error accumulation. On the other hand, when compared to NAR models, ProNet takes into account the interdependency of predictions in the output space, leading to improved forecasting accuracy. Our comprehensive evaluation, encompassing four large datasets, and an ablation study, demonstrate the effectiveness of ProNet, highlighting its superior performance in terms of accuracy and prediction speed, outperforming state-of-the-art AR and NAR forecasting models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的深度学习方法，称为ProNet，用于多个时间序列预测。这种方法可以适应ively分割预测时间序列，在每个分割中采用非autoregressive（NAR）策略预测最重要的步骤，而剩下的步骤使用autoregressive（AR）策略预测。这个分割过程基于隐藏变量，可以有效地捕捉各个时间步骤的重要性。相比AR模型，ProNet具有明显的优势，需要 fewer AR迭代，导致更快的预测速度，并可以减轻预测错误的积累。相比NAR模型，ProNet考虑了预测输出空间中的相互关系，从而提高预测精度。我们的全面评估，包括四个大型数据集，以及一个减少研究，表明ProNet的效果，其中包括精度和预测速度方面的表现，超越了当前AR和NAR预测模型的表现。
</details></li>
</ul>
<hr>
<h2 id="Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration"><a href="#Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration" class="headerlink" title="Dual-Directed Algorithm Design for Efficient Pure Exploration"></a>Dual-Directed Algorithm Design for Efficient Pure Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19319">http://arxiv.org/abs/2310.19319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Qin, Wei You</li>
<li>for: 这篇论文主要研究的是在随机顺序式适应试验中的探索问题，具体来说是在一组可选项中精确地回答一个查询问题，以高度确idence并且尽量减少测量努力。</li>
<li>methods: 这篇论文使用了 dual variables 来描述优化条件，并使用了适应试验的概率论基础来解决问题。</li>
<li>results: 这篇论文提出了一种新的算法方法，可以在随机顺序式适应试验中实现高效的探索问题解决方案，并且可以解决一些已有的开放问题。 numerics 表明该算法的效率比现有算法更高。<details>
<summary>Abstract</summary>
We consider pure-exploration problems in the context of stochastic sequential adaptive experiments with a finite set of alternative options. The goal of the decision-maker is to accurately answer a query question regarding the alternatives with high confidence with minimal measurement efforts. A typical query question is to identify the alternative with the best performance, leading to ranking and selection problems, or best-arm identification in the machine learning literature. We focus on the fixed-precision setting and derive a sufficient condition for optimality in terms of a notion of strong convergence to the optimal allocation of samples. Using dual variables, we characterize the necessary and sufficient conditions for an allocation to be optimal. The use of dual variables allow us to bypass the combinatorial structure of the optimality conditions that relies solely on primal variables. Remarkably, these optimality conditions enable an extension of top-two algorithm design principle, initially proposed for best-arm identification. Furthermore, our optimality conditions give rise to a straightforward yet efficient selection rule, termed information-directed selection, which adaptively picks from a candidate set based on information gain of the candidates. We outline the broad contexts where our algorithmic approach can be implemented. We establish that, paired with information-directed selection, top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm identification, solving a glaring open problem in the pure exploration literature. Our algorithm is optimal for $\epsilon$-best-arm identification and thresholding bandit problems. Our analysis also leads to a general principle to guide adaptations of Thompson sampling for pure-exploration problems. Numerical experiments highlight the exceptional efficiency of our proposed algorithms relative to existing ones.
</details>
<details>
<summary>摘要</summary>
我们考虑了纯粹的探索问题在随机顺序适应实验中，具有finite集合的选择选项。决策者的目标是通过高度信任度和最小测试努力来准确回答关于选项的查询问题。一般的查询问题是确定最佳选项，导致排名和选择问题，或最佳臂标识在机器学习文献中。我们关注固定精度设定下的情况，并 deriv a sufficient condition for optimality in terms of a strong convergence notion to the optimal allocation of samples.使用对偶变量，我们Characterize the necessary and sufficient conditions for an allocation to be optimal.这些优化条件使我们可以绕过 primal variable的 combinatorial结构，减少优化条件的复杂性。另外，这些优化条件允许我们扩展top-two algorithm design principle，最初是为best-arm identification提出的。此外，我们的优化条件会导致一种简单又高效的选择规则，称为信息导向选择，这种选择规则可以动态地从候选集中选择基于候选者的信息增益。我们详细介绍了我们的算法approach可以应用的广泛场景。我们证明，在信息导向选择的情况下，top-two Thompson sampling是（几何）最佳的，解决了纯探索领域中的一个开问题。我们的算法是最佳的 дляeps-best-arm identification和阈值bandit问题。我们的分析还导致了一个通用的指南，用于导向Thompson sampling的修改。 numerics experiments highlight the exceptional efficiency of our proposed algorithms relative to existing ones.
</details></li>
</ul>
<hr>
<h2 id="A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields"><a href="#A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields" class="headerlink" title="A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields"></a>A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19306">http://arxiv.org/abs/2310.19306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengjie Shi, Zhiping Xu</li>
<li>for: 本研究旨在开发一种基于神经网络的破坏力场模型（NN-F$^3），以便更好地预测高度机械处理过程中的破坏行为。</li>
<li>methods: 研究人员采用了分子模拟技术，通过对破坏过程中的微structural变化进行解决，以探索破坏过程中的机械能量消耗、破坏路径选择和动态不稳定性（如弯曲、分支）。</li>
<li>results: 研究人员通过对各种材料（如h-BN和扭曲双层graphene）的破坏过程进行模拟，证明了NN-F$^3$模型的能力，并与实验结果相符。此外，研究还表明了需要在预测EXTREME mechanical processes中包含电子结构的知识，以确保模型的准确性。<details>
<summary>Abstract</summary>
Extreme mechanical processes such as strong lattice distortion and bond breakage during fracture are ubiquitous in nature and engineering, which often lead to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenged by their multiscale characteristics spanning from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations offer an important tool to resolve the progressive microstructural changes at crack fronts and are widely used to explore processes therein, such as mechanical energy dissipation, crack path selection, and dynamic instabilities (e.g., kinking, branching). Empirical force fields developed based on local descriptors based on atomic positions and the bond orders do not yield satisfying predictions of fracture, even for the nonlinear, anisotropic stress-strain relations and the energy densities of edges. High-fidelity force fields thus should include the tensorial nature of strain and the energetics of rare events during fracture, which, unfortunately, have not been taken into account in both the state-of-the-art empirical and machine-learning force fields. Based on data generated by first-principles calculations, we develop a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. The capability of NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer graphene as model problems. The simulation results confirm recent experimental findings and highlight the necessity to include the knowledge of electronic structures from first-principles calculations in predicting extreme mechanical processes.
</details>
<details>
<summary>摘要</summary>
EXTREME mechanical processes such as strong lattice distortion and bond breakage during fracture are ubiquitous in nature and engineering, which often lead to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenged by their multiscale characteristics spanning from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations offer an important tool to resolve the progressive microstructural changes at crack fronts and are widely used to explore processes therein, such as mechanical energy dissipation, crack path selection, and dynamic instabilities (e.g., kinking, branching). Empirical force fields developed based on local descriptors based on atomic positions and the bond orders do not yield satisfying predictions of fracture, even for the nonlinear, anisotropic stress-strain relations and the energy densities of edges. High-fidelity force fields thus should include the tensorial nature of strain and the energetics of rare events during fracture, which, unfortunately, have not been taken into account in both the state-of-the-art empirical and machine-learning force fields. Based on data generated by first-principles calculations, we develop a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. The capability of NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer graphene as model problems. The simulation results confirm recent experimental findings and highlight the necessity to include the knowledge of electronic structures from first-principles calculations in predicting extreme mechanical processes.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection"><a href="#Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection" class="headerlink" title="Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection"></a>Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19304">http://arxiv.org/abs/2310.19304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swanand Ravindra Kadhe, Heiko Ludwig, Nathalie Baracaldo, Alan King, Yi Zhou, Keith Houck, Ambrish Rawat, Mark Purcell, Naoise Holohan, Mikio Takeuchi, Ryo Kawahara, Nir Drucker, Hayim Shaul, Eyal Kushnir, Omri Soceanu</li>
<li>for: 本研究旨在提高金融异常探测的效率，并且解决多家金融机构之间的信任问题。</li>
<li>methods: 本研究使用了联合学习（Federated Learning）、完全同质加密（Fully Homomorphic Encryption）、安全多方计算（Secure Multi-Party Computation）、减少隐私（Differential Privacy）和随机化技术来保证隐私和准确性。</li>
<li>results: 本研究的解决方案可以实现高度的隐私和准确性，并且可以实现高效的金融异常探测。 Specifically, 本研究显示在不信任的假设下，银行不会学习任何敏感特征，而 payment network system 则不会学习任何关于银行的数据。<details>
<summary>Abstract</summary>
The effective detection of evidence of financial anomalies requires collaboration among multiple entities who own a diverse set of data, such as a payment network system (PNS) and its partner banks. Trust among these financial institutions is limited by regulation and competition. Federated learning (FL) enables entities to collaboratively train a model when data is either vertically or horizontally partitioned across the entities. However, in real-world financial anomaly detection scenarios, the data is partitioned both vertically and horizontally and hence it is not possible to use existing FL approaches in a plug-and-play manner.   Our novel solution, PV4FAD, combines fully homomorphic encryption (HE), secure multi-party computation (SMPC), differential privacy (DP), and randomization techniques to balance privacy and accuracy during training and to prevent inference threats at model deployment time. Our solution provides input privacy through HE and SMPC, and output privacy against inference time attacks through DP. Specifically, we show that, in the honest-but-curious threat model, banks do not learn any sensitive features about PNS transactions, and the PNS does not learn any information about the banks' dataset but only learns prediction labels. We also develop and analyze a DP mechanism to protect output privacy during inference. Our solution generates high-utility models by significantly reducing the per-bank noise level while satisfying distributed DP. To ensure high accuracy, our approach produces an ensemble model, in particular, a random forest. This enables us to take advantage of the well-known properties of ensembles to reduce variance and increase accuracy. Our solution won second prize in the first phase of the U.S. Privacy Enhancing Technologies (PETs) Prize Challenge.
</details>
<details>
<summary>摘要</summary>
要有效探测金融异常，需要多个金融机构之间的合作，其中包括支付网络系统（PNS）和其合作银行。但是，这些金融机构之间的信任受到了法规和竞争的限制。基于联合学习（FL）的解决方案可以在数据分布在不同机构之间时进行模型训练，但是在实际的金融异常探测场景中，数据通常会分布在 Vertical 和 Horizontal 两个方向上，因此不能直接使用现有的 FL 方法。我们的新解决方案，PV4FAD，结合了完全同质加密（HE）、安全多方计算（SMPC）、不同隐私（DP）和随机化技术，以平衡隐私和准确性 durante 训练，并在模型部署时防止推理攻击。我们的解决方案提供了输入隐私通过 HE 和 SMPC，并在推理时对输出进行隐私保护。我们还开发了一种 DP 机制来保护输出隐私。我们的解决方案可以减少每家银行的噪音水平，同时满足分布式隐私的要求。为了保证高准确性，我们采用了随机森林 ensemble 模型。这使我们可以利用随机森林的 bekannt properties 来减少差异和提高准确性。我们的解决方案在美国隐私提升技术（PETs）奖励挑战第一阶段中获得了第二名。
</details></li>
</ul>
<hr>
<h2 id="Stage-Aware-Learning-for-Dynamic-Treatments"><a href="#Stage-Aware-Learning-for-Dynamic-Treatments" class="headerlink" title="Stage-Aware Learning for Dynamic Treatments"></a>Stage-Aware Learning for Dynamic Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19300">http://arxiv.org/abs/2310.19300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanwen Ye, Wenzhuo Zhou, Ruoqing Zhu, Annie Qu</li>
<li>for: 这篇论文旨在提出一种新的个人化学习方法，以提高适应性评估和决策过程中的标准化和稳定性。</li>
<li>methods: 本论文提出了一种新的个人化学习方法，包括估算DTR和考虑几个决策阶段的重要性。</li>
<li>results:  empirical results show that the proposed method can significantly improve the sample efficiency and stability of inverse probability weighted based methods, and provide more accurate and personalized treatment recommendations.<details>
<summary>Abstract</summary>
Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals' specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framework as a special case of ours. Moreover, we introduce the notion of stage importance scores along with an attention mechanism to explicitly account for heterogeneity among decision stages. We establish the theoretical properties of the proposed approach, including the Fisher consistency and finite-sample performance bound. Empirically, we evaluate the proposed method in extensive simulated environments and a real case study for COVID-19 pandemic.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting"><a href="#AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting"></a>AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19289">http://arxiv.org/abs/2310.19289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>For: 多Horizon时间系列预测，需要高准确和快速。而AutoRegressive（AR）模型在短期预测方面表现出色，但是随着预测时间跨度增加，它们的速度和误差问题日益突出。Non-AutoRegressive（NAR）模型更适合长期预测，但是它们在不同时间序列之间的互相关系不强，导致预测结果不真实。* Methods: 我们提出了AMLNet，一种创新的NAR模型，通过在线知识传授（KD）方法实现了可靠的预测。AMLNet combinesthe strengths of AR和NAR模型，通过训练一个深度AR解码器和一个深度NAR解码器，并将它们作为ensemble teachers，向一个浅度NAR解码器传授知识。这种知识传授是通过两种关键机制：1）结果驱动KD， dynamically weights the contribution of KD losses from the teacher models，使得浅度NAR解码器能够 incorporate ensemble的多样性；2）提示驱动KD，使用对抗训练提取模型的隐藏状态中的有价值信息进行传授。* Results: 广泛的实验表明AMLNet在传统AR和NAR模型的基础上提供了更高的准确率和更快的计算速度。因此，AMLNet提供了一个可靠的多Horizon时间系列预测方法，为不同预测任务提供了一个有 promise的 Avenues。<details>
<summary>Abstract</summary>
Multi-horizon time series forecasting, crucial across diverse domains, demands high accuracy and speed. While AutoRegressive (AR) models excel in short-term predictions, they suffer speed and error issues as the horizon extends. Non-AutoRegressive (NAR) models suit long-term predictions but struggle with interdependence, yielding unrealistic results. We introduce AMLNet, an innovative NAR model that achieves realistic forecasts through an online Knowledge Distillation (KD) approach. AMLNet harnesses the strengths of both AR and NAR models by training a deep AR decoder and a deep NAR decoder in a collaborative manner, serving as ensemble teachers that impart knowledge to a shallower NAR decoder. This knowledge transfer is facilitated through two key mechanisms: 1) outcome-driven KD, which dynamically weights the contribution of KD losses from the teacher models, enabling the shallow NAR decoder to incorporate the ensemble's diversity; and 2) hint-driven KD, which employs adversarial training to extract valuable insights from the model's hidden states for distillation. Extensive experimentation showcases AMLNet's superiority over conventional AR and NAR models, thereby presenting a promising avenue for multi-horizon time series forecasting that enhances accuracy and expedites computation.
</details>
<details>
<summary>摘要</summary>
多Horizon时间序列预测，在多个领域都是关键，需要高精度和速度。而AutoRegressive（AR）模型在短期预测方面 excel，但是作为预测 horizon 增长，其速度和错误问题日益突出。Non-AutoRegressive（NAR）模型更适合长期预测，但是它们在相互依赖关系下预测结果不实际。我们介绍AMLNet，一种创新的NAR模型，通过在线知识传授（KD）方法实现实实际的预测。AMLNet 利用了 AR 和 NAR 模型的优点，通过在深度AR decoder和深度NAR decoder之间的协作，使得 ensemble 教师模型对一个较浅的 NAR decoder进行知识传授。这种知识传授是通过两个关键机制进行：1）结果驱动KD，通过动态权重 teacher 模型中 KD 损失的贡献，使得较浅的 NAR decoder能够包含 ensemble 的多样性；2）提示驱动KD，通过对模型的隐藏状态进行对抗训练，提取价值的信息进行传授。广泛的实验表明AMLNet 在传统 AR 和 NAR 模型的基础上提高了精度和计算速度，因此提供了一个可靠的多Horizon时间序列预测方法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality"><a href="#Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality" class="headerlink" title="Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality"></a>Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19287">http://arxiv.org/abs/2310.19287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Amir Jaberzadeh, Jason Geng</li>
<li>for: 这篇论文旨在解决分布式联合学习中的可扩展性和可靠性问题，通过结合区块链技术。</li>
<li>methods: 论文提出了一种新的方法，即 Semi-Decentralized Federated Learning with Blockchain (SDFL-B)，通过增强参与节点的可信度而实现公平、安全和透明的合作机器学习环境，而无需妥协数据隐私。</li>
<li>results: 研究人员通过实验和讨论，证明了SDFL-B系统的优势，包括可扩展性、可靠性和数据隐私保护等方面。<details>
<summary>Abstract</summary>
The paper presents an innovative approach to address the challenges of scalability and reliability in Distributed Federated Learning by leveraging the integration of blockchain technology. The paper focuses on enhancing the trustworthiness of participating nodes through a trust penalization mechanism while also enabling asynchronous functionality for efficient and robust model updates. By combining Semi-Decentralized Federated Learning with Blockchain (SDFL-B), the proposed system aims to create a fair, secure and transparent environment for collaborative machine learning without compromising data privacy. The research presents a comprehensive system architecture, methodologies, experimental results, and discussions that demonstrate the advantages of this novel approach in fostering scalable and reliable SDFL-B systems.
</details>
<details>
<summary>摘要</summary>
文章提出了一种创新的方法，用于解决分布式联合学习中的可扩展性和可靠性问题，通过启用区块链技术的整合。文章通过加入信任惩罚机制来增强参与节点的可信worthiness，同时允许 asynchronous 功能，以实现高效的模型更新。通过结合 Semi-Decentralized Federated Learning with Blockchain (SDFL-B)，提出的系统旨在创造一个公正、安全、透明的合作机器学习环境，无需妥协数据隐私。文章介绍了完整的系统架构、方法、实验结果和讨论，以示该新方法在推动可扩展和可靠 SDFL-B 系统的优势。
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes"><a href="#Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes" class="headerlink" title="Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes"></a>Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19285">http://arxiv.org/abs/2310.19285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouc20/hodgerandomwalk">https://github.com/zhouc20/hodgerandomwalk</a></li>
<li>paper_authors: Cai Zhou, Xiyuan Wang, Muhan Zhang</li>
<li>for: 这篇论文旨在系统分析随机漫步在不同级别 simplicial complexes 中对图神经网络的提升作用，并设计基于随机漫步的表达能力更高的位置编码方法。</li>
<li>methods: 论文使用随机漫步在不同级别 simplicial complexes 中进行分析，并提出基于随机漫步和霍德 Laplacians 的表达能力更高的位置编码方法，包括节点级别的 PE 和 Hodge1Lap，以及边级别的 EdgeRWSE。</li>
<li>results: 实验表明，这些随机漫步基于的位置编码方法可以提高图神经网络的表达能力，并且可以在不同的 simplicial complexes 上进行扩展。<details>
<summary>Abstract</summary>
Node-level random walk has been widely used to improve Graph Neural Networks. However, there is limited attention to random walk on edge and, more generally, on $k$-simplices. This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity. First, on $0$-simplices or node level, we establish a connection between existing positional encoding (PE) and structure encoding (SE) methods through the bridge of random walk. Second, on $1$-simplices or edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and design corresponding edge PE respectively. In the spatial domain, we directly make use of edge level random walk to construct EdgeRWSE. Based on the spectral analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding. Third, we generalize our theory to random walk on higher-order simplices and propose the general principle to design PE on simplices based on random walk and Hodge Laplacians. Inter-level random walk is also introduced to unify a wide range of simplicial networks. Extensive experiments verify the effectiveness of our random walk-based methods.
</details>
<details>
<summary>摘要</summary>
< sys>Node-level random walk 已经广泛应用于提高图神经网络。然而，对于Random Walk在边和更广泛地在k-简单形上的注意力却有限。这篇论文系统地分析了Random Walk在不同阶 simplicial complexes (SC) 上的facilitates GNNs的理论表达能力。首先，在0-简单形或节点水平，我们建立了 pozitional encoding (PE) 和 structure encoding (SE) 方法之间的连接，通过Random Walk的桥接。其次，在1-简单形或边水平，我们将edge-level random walk 和Hodge 1-Laplacians相连，并设计相应的边 pozitional encoding (EdgeRWSE)。在空间领域中，我们直接使用边水平 random walk 来构建EdgeRWSE。基于Hodge 1-Laplacians的 спектраль分析，我们提出了一种可 permutation equivariant 和表达力强的边级 pozitional encoding，即Hodge1Lap。第三，我们推广了我们的理论到高阶 simplicial complexes 上，并提出了一个通用的方法来在 simplicial complexes 上设计 pozitional encoding 基于Random Walk和Hodge Laplacians。在多级 simplicial networks 中，我们还引入了Inter-level random walk 来统一广泛的 simplicial networks。广泛的实验证明了我们的Random Walk-based方法的效果。</sys>Note: Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition"><a href="#rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition" class="headerlink" title="rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition"></a>rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19283">http://arxiv.org/abs/2310.19283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Enokibori</li>
<li>for: 这个论文提出了一种基于IMU的人体活动识别（HAR）的新型深度学习模型（DNN），即rTsfNet。</li>
<li>methods: 该模型使用多头3D旋转和时间序列特征提取（TSF）来自动选择3D基架，然后使用多层感知网络（MLP）进行人体活动识别。</li>
<li>results: 该模型在管理良好的benchmark条件下和多个数据集（UCI HAR、PAMAP2、Daphnet、OPPORTUNITY）中达到了最高准确率，超过了现有模型的性能。<details>
<summary>Abstract</summary>
This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
</details>
<details>
<summary>摘要</summary>
Here is the Simplified Chinese translation of the text:这篇论文提出了一种新的深度神经网络（DNN）模型，即rTsfNet，用于基于各种传感器的人体活动识别（HAR）。该模型具有多头3D旋转和时间序列特征提取功能，自动从3D基础中选择特征。模型在管理的标准环境下和多个数据集（UCI HAR、PAMAP2、Daphnet和OPPORTUNITY）下达到了最高精度，这些数据集涵盖了不同的活动。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds"><a href="#Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds" class="headerlink" title="Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds"></a>Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19276">http://arxiv.org/abs/2310.19276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Choi, Rak-Kyeong Seong</li>
<li>for: 这 paper 是 investigate Sasaki-Einstein 5-manifolds 的最小体积问题的。</li>
<li>methods: 这 paper 使用了机器学习规范技术来解决这个问题，并提出了基于 geometric invariants 的准确的解。</li>
<li>results: 这 paper 的结果表明，使用机器学习规范技术可以准确地计算 Sasaki-Einstein 5-manifolds 的最小体积，并且可以提供可读写的解释性式。<details>
<summary>Abstract</summary>
We present a collection of explicit formulas for the minimum volume of Sasaki-Einstein 5-manifolds. The cone over these 5-manifolds is a toric Calabi-Yau 3-fold. These toric Calabi-Yau 3-folds are associated with an infinite class of 4d N=1 supersymmetric gauge theories, which are realized as worldvolume theories of D3-branes probing the toric Calabi-Yau 3-folds. Under the AdS/CFT correspondence, the minimum volume of the Sasaki-Einstein base is inversely proportional to the central charge of the corresponding 4d N=1 superconformal field theories. The presented formulas for the minimum volume are in terms of geometric invariants of the toric Calabi-Yau 3-folds. These explicit results are derived by implementing machine learning regularization techniques that advance beyond previous applications of machine learning for determining the minimum volume. Moreover, the use of machine learning regularization allows us to present interpretable and explainable formulas for the minimum volume. Our work confirms that, even for extensive sets of toric Calabi-Yau 3-folds, the proposed formulas approximate the minimum volume with remarkable accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一系列Explicit的方程式，用于找到Sasaki-Einstein 5-次元空间中的最小体积。这些5-次元空间的对偶是一组toric Calabi-Yau 3-次元多普遍，这些多普遍与一个无穷的4d N=1 supersymmetric gauge theory相关，它们是D3- Branes在这些toric Calabi-Yau 3-次元多普遍的世界体积理论。根据AdS/CFT对偶，Sasaki-Einstein 5-次元空间的最小体积与4d N=1 superconformal field theory中的中心荷电荷有逆比例关系。我们提出的方程式使用机器学习调整技术，可以在许多toric Calabi-Yau 3-次元多普遍中精确地找到最小体积。此外，这些方程式具有可读性和解释性，可以帮助我们更好地理解Sasaki-Einstein 5-次元空间的特性。我们的研究确认，即使是广泛的toric Calabi-Yau 3-次元多普遍，我们提出的方程式可以对其最小体积进行高精度的预测。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks"><a href="#Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks" class="headerlink" title="Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks"></a>Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19274">http://arxiv.org/abs/2310.19274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehong Chung, Rasool Ahmad, WaiChing Sun, Wei Cai, Tapan Mukerji</li>
<li>for: 预测岩石弹性模量</li>
<li>methods: 使用图 neuronal networks (GNNs) 方法，将数字 CT 扫描图像转换为图Dataset，并通过训练获得预测弹性模量的能力。</li>
<li>results: GNN 模型在不同尺度的子 кубы上表现出了良好的预测能力，并且可以预测未经测试的岩石和未知的子 кубы尺度。 Comparative analysis 表明，GNNs 在预测未经测试的岩石性质方面表现出了superiority。<details>
<summary>Abstract</summary>
This study presents a Graph Neural Networks (GNNs)-based approach for predicting the effective elastic moduli of rocks from their digital CT-scan images. We use the Mapper algorithm to transform 3D digital rock images into graph datasets, encapsulating essential geometrical information. These graphs, after training, prove effective in predicting elastic moduli. Our GNN model shows robust predictive capabilities across various graph sizes derived from various subcube dimensions. Not only does it perform well on the test dataset, but it also maintains high prediction accuracy for unseen rocks and unexplored subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs) reveals the superior performance of GNNs in predicting unseen rock properties. Moreover, the graph representation of microstructures significantly reduces GPU memory requirements (compared to the grid representation for CNNs), enabling greater flexibility in the batch size selection. This work demonstrates the potential of GNN models in enhancing the prediction accuracy of rock properties and boosting the efficiency of digital rock analysis.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这项研究提出了基于图 neural network (GNN) 的方法，用于预测岩石的有效弹性模量从其数字 CT-扫描图像中。我们使用 Mapper 算法将三维数字岩石图像转换为图Dataset，捕捉到重要的几何信息。这些图， после训练，证明可以有效预测弹性模量。我们的 GNN 模型在不同的图大小和维度下表现出了良好的预测能力。不仅在测试数据集上表现出色，而且在未看过的岩石和未探索的子 куби数上也保持高的预测精度。与 Convolutional Neural Networks (CNNs) 进行比较分析表明，GNNs 在预测未看过的岩石特性方面表现出了superior的性能。此外，图表示 Microstructure 的 Representation 可以减少 GPU 内存需求（与网格 Representation 相比，用于 CNNs），使得批处理大小的选择更加灵活。这项研究示出 GNN 模型在提高岩石特性预测精度和数字岩石分析效率方面的潜力。
</details></li>
</ul>
<hr>
<h2 id="Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach"><a href="#Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach" class="headerlink" title="Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach"></a>Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19270">http://arxiv.org/abs/2310.19270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathael Da Costa, Cyrus Mostajeran, Juan-Pablo Ortega, Salem Said</li>
<li>for: 证明 классический Gaussian kernel 在非欧几何空间上 nunca 是 positively definite。</li>
<li>methods: 发展了新的几何和分析方法，提供了几何空间上 Gaussian kernel 的完善Characterization，但是只有一些低维度的情况需要数值计算。主要结果包括 L$^{!\scriptscriptstyle p}$-Godement 定理（$p &#x3D; 1,2），它们提供了几何空间上 Gaussian kernel 的必要和 suficient 条件，但是这些条件是可靠的。</li>
<li>results: 获得了几何空间上 Gaussian kernel 的完善Characterization，并提供了许多未来应用的特定几何分析工具。<details>
<summary>Abstract</summary>
This work aims to prove that the classical Gaussian kernel, when defined on a non-Euclidean symmetric space, is never positive-definite for any choice of parameter. To achieve this goal, the paper develops new geometric and analytical arguments. These provide a rigorous characterization of the positive-definiteness of the Gaussian kernel, which is complete but for a limited number of scenarios in low dimensions that are treated by numerical computations. Chief among these results are the L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement theorems (where $p = 1,2$), which provide verifiable necessary and sufficient conditions for a kernel defined on a symmetric space of non-compact type to be positive-definite. A celebrated theorem, sometimes called the Bochner-Godement theorem, already gives such conditions and is far more general in its scope, but is especially hard to apply. Beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the study of invariant kernels on symmetric spaces, bringing forth specific harmonic analysis tools that suggest many future applications.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这个工作目的是证明经典的加aussian kernel，当定义在非欧几何同胚空间时，从任何参数选择来说是无法确定正定的。为达到这个目标，这篇论文发展了新的几何和分析方法。这些方法提供了非欧几何同胚空间上加aussian kernel的正定性的彻底Characterization，但是只有在低维度情况下，通过数值计算来处理一些特殊情况。主要结果包括L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement定理（其中$p = 1,2），这些定理提供了非欧几何同胚空间上kernel的正定性的必要和 suficient Conditions，并且这些条件是可靠的。此外，这些结果还提供了对各种几何同胚空间上的抽象几何和分析工具的深入了解，这些工具在未来应用中具有广泛的前途。
</details></li>
</ul>
<hr>
<h2 id="A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks"><a href="#A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks" class="headerlink" title="A Metadata-Driven Approach to Understand Graph Neural Networks"></a>A Metadata-Driven Approach to Understand Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19263">http://arxiv.org/abs/2310.19263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting Wei Li, Qiaozhu Mei, Jiaqi Ma</li>
<li>for: 本文旨在分析 graph neural networks (GNNs) 在不同图数据上表现的敏感性，并提出一种 $\textit{metadata-driven}$ 方法来分析 GNNs 的敏感性。</li>
<li>methods: 本文使用了多ivariate sparse regression 分析 metadata，从而得到了一些关键的数据特性。然后，通过理论分析和控制实验，证明了该数据特性对 GNNs 表现的影响。</li>
<li>results: 研究发现，在图数据上，具有更平衡的度分布的dataset表现更好，这是因为这些dataset中的节点表示更好地 Linear separability，从而导致更好的 GNN 表现。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved remarkable success in various applications, but their performance can be sensitive to specific data properties of the graph datasets they operate on. Current literature on understanding the limitations of GNNs has primarily employed a $\textit{model-driven}$ approach that leverage heuristics and domain knowledge from network science or graph theory to model the GNN behaviors, which is time-consuming and highly subjective. In this work, we propose a $\textit{metadata-driven}$ approach to analyze the sensitivity of GNNs to graph data properties, motivated by the increasing availability of graph learning benchmarks. We perform a multivariate sparse regression analysis on the metadata derived from benchmarking GNN performance across diverse datasets, yielding a set of salient data properties. To validate the effectiveness of our data-driven approach, we focus on one identified data property, the degree distribution, and investigate how this property influences GNN performance through theoretical analysis and controlled experiments. Our theoretical findings reveal that datasets with more balanced degree distribution exhibit better linear separability of node representations, thus leading to better GNN performance. We also conduct controlled experiments using synthetic datasets with varying degree distributions, and the results align well with our theoretical findings. Collectively, both the theoretical analysis and controlled experiments verify that the proposed metadata-driven approach is effective in identifying critical data properties for GNNs.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经取得了各种应用的显著成功，但它们在不同的图 dataset 上表现的性能可能会受到特定的数据特性的影响。现有文献中理解 GNN 的限制的方法主要是使用 $\textit{model-driven}$ 方法，利用网络科学或图论中的习惯和专业知识来模型 GNN 的行为，这是时间consuming 和高度主观的。在这项工作中，我们提出了一种 $\textit{metadata-driven}$ 方法来分析 GNN 对图 dataset 的敏感性，由于图学学习 benchmark 的 increasing 可用性而 Motivated。我们通过对 benchmark 中 GNN 性能的多ivariate 稀疏回归分析得到了一组突出的数据特性。为了验证我们的数据驱动方法的有效性，我们选择了一个被归类为重要的数据特性，即度分布，并通过理论分析和控制实验来调查这个特性对 GNN 性能的影响。我们的理论发现表明，具有更平衡的度分布的图 dataset 会具有更好的线性分离性，从而导致更好的 GNN 性能。我们还通过使用 Synthetic 数据集来实验 validate 我们的理论发现，结果与理论发现一致。总之，两者的结果证明了我们的数据驱动方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement"><a href="#Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement" class="headerlink" title="Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement"></a>Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19261">http://arxiv.org/abs/2310.19261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daesol Cho, Seungjae Lee, H. Jin Kim</li>
<li>for: 解决RL在无知搜索问题中的挑战，提出一种新的课程RL方法called Diversify for Disagreement &amp; Conquer (D2C)。</li>
<li>methods: D2C方法需要只几个欲达到结果的示例，无论环境的几何结构或欲达到结果的分布，可以在任何环境中工作。该方法首先实现了目标状态分类器的多样化，以确定与访问的状态相似的点，并确保在未知区域中的状态分布不同，从而可以量化未探索区域和设计一个简单直观的目标做出策略奖励信号。然后，该方法使用两分配匹配定义一个课程学习目标，以生成一系列适应度较高的中间目标，使得agent自动探索和征服未探索区域。</li>
<li>results: 实验结果表明，D2C方法在量和质上都高于先前的课程RL方法，即使欲达到结果的示例随机分布。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) often faces the challenges of uninformed search problems where the agent should explore without access to the domain knowledge such as characteristics of the environment or external rewards. To tackle these challenges, this work proposes a new approach for curriculum RL called Diversify for Disagreement & Conquer (D2C). Unlike previous curriculum learning methods, D2C requires only a few examples of desired outcomes and works in any environment, regardless of its geometry or the distribution of the desired outcome examples. The proposed method performs diversification of the goal-conditional classifiers to identify similarities between visited and desired outcome states and ensures that the classifiers disagree on states from out-of-distribution, which enables quantifying the unexplored region and designing an arbitrary goal-conditioned intrinsic reward signal in a simple and intuitive way. The proposed method then employs bipartite matching to define a curriculum learning objective that produces a sequence of well-adjusted intermediate goals, which enable the agent to automatically explore and conquer the unexplored region. We present experimental results demonstrating that D2C outperforms prior curriculum RL methods in both quantitative and qualitative aspects, even with the arbitrarily distributed desired outcome examples.
</details>
<details>
<summary>摘要</summary>
常见的强化学习（RL）问题中，智能机器会面临无知搜索问题，其中智能机器需要在没有环境特征或外部奖励的情况下探索。为解决这些问题，这项工作提出了一种新的目标学习方法，称为多样化为分裂和征服（D2C）。与过去的学习目标方法不同，D2C只需很少的感兴趣结果示例，并且可以在任何环境中工作，无论环境的geometry或感兴趣结果示例的分布。该方法首先将目标 conditioned 分类器多样化，以确定visited和感兴趣结果状态之间的相似性，并确保分类器对非标准分布的状态表示不同意见，从而使得可以量化未探索区域和设计一个简单直观的目标conditioned内在奖励信号。然后，该方法使用两分图匹配来定义学习目标，生成一系列适应度高的中间目标，使得智能机器自动探索和征服未探索区域。我们对D2C进行了实验，并证明它在量化和质量上都高于过去的目标学习方法，即使感兴趣结果示例随机分布。
</details></li>
</ul>
<hr>
<h2 id="Flow-based-Distributionally-Robust-Optimization"><a href="#Flow-based-Distributionally-Robust-Optimization" class="headerlink" title="Flow-based Distributionally Robust Optimization"></a>Flow-based Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19253">http://arxiv.org/abs/2310.19253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie</li>
<li>for: 解决流基于分布robust优化（DRO）问题，其中需要最差分布（也称为最有利分布，LFD）是连续的，以便可以扩展到更大的样本大小和实现更好的泛化能力。</li>
<li>methods: 使用流基模型，连续时间可逆运输映射来解决计算挑战的无穷维度优化问题，并开发了 Wasserstein 距离 proximal 流体系类型的算法。在实践中，我们使用一系列神经网络来参数化运输映射，通过梯度下降进行块式训练。</li>
<li>results: 在对真实高维数据进行测试中，提出了一种基于数据驱动分布扰动隐私的新机制，并在分布robust假设测试和数据驱动分布扰动隐私方面实现了强有力的实验性表现。<details>
<summary>Abstract</summary>
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various applications. We demonstrate its usage in adversarial learning, distributionally robust hypothesis testing, and a new mechanism for data-driven distribution perturbation differential privacy, where the proposed method gives strong empirical performance on real high-dimensional data.
</details>
<details>
<summary>摘要</summary>
我们提出一种 computationally efficient 框架，called \texttt{FlowDRO}，用于解决流量基于分布ally robust优化（DRO）问题，具有 Wasserstein 不确定集，并要求最差情况分布（也称为最有利分布，LFD）是连续的，以便可以适用于更大的样本大小和更好的泛化能力。为了解决计算复杂的无穷维度优化问题，我们利用流量模型，连续时间可逆运输Map zwischen 数据分布和目标分布，并开发了 Wasserstein 靠近流类型的梯度流动算法。在实践中，我们归一化运输Map 使用一个序列化的神经网络，逐步在块内使用梯度下降进行训练。我们的计算框架是通用的，可以处理高维数据和大样本大小，并可以用于各种应用。我们在抗 adversarial 学习、分布ally robust假设测试和数据驱动分布泛化隐私中示出了提出的方法的强有效性。
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data"><a href="#Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data" class="headerlink" title="Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data"></a>Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19250">http://arxiv.org/abs/2310.19250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayana Pereira, Meghana Kshirsagar, Sumit Mukherjee, Rahul Dodhia, Juan Lavista Ferres, Rafael de Sousa<br>for: 这个论文主要是为了研究使用异步private的人工数据集来保护个人数据提供者的隐私，并对这种方法在医疗和人道主义领域中的应用进行分析。methods: 这个论文使用了两种主要的异步private数据生成算法：marginal-based和GAN-based。同时，该论文还提出了一种不要求实际数据存在的训练和评估框架。results: 研究结果表明，使用marginal-based数据生成算法可以达到模型训练的同等Utility水平，而且该算法还可以让模型同时实现Utility和公平性的特点。此外，该论文还对异步private数据生成算法的多种定义公平性进行了广泛的分析。<details>
<summary>Abstract</summary>
Differentially private (DP) synthetic data sets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic data set generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generator MWEM PGM can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.
</details>
<details>
<summary>摘要</summary>
diferencialmente privado (DP) 的 sintética datos sets 是一种解决分享数据而保护个人数据提供者隐私的解决方案。 理解使用 DP sintética datos in end-to-end machine learning pipelines 的影响，特别是在医疗和人道主义领域， где data 稀缺并受到严格隐私法规限制。在这项工作中，我们 investigate 如何使用 sintética datos replace 实际的 tabular data 在 machine learning pipelines 中，并 identify 最有效的 sintética datos generation techniques  для训练和评估机器学习模型。我们 investigate 使用异 differentially private sintética datos 对下游分类任务的影响，包括实用性和公平性。我们的分析是全面的，包括两种主要的 sintética datos generation algorithms：marginal-based 和 GAN-based。到目前为止，我们的工作是第一个：1. 提出一种不假设实际数据可用于测试机器学习模型在 sintética datos 上的训练和公平性的训练和评估框架。2. 对 sintética datos set generation algorithms 进行了最广泛的分析，包括实用性和公平性在内。3. 涵盖了多种定义的公平性。我们的发现表明，marginal-based sintética datos generators 在机器学习模型训练中的实用性比 GAN-based 更高。我们显示，使用 marginal-based 生成的数据可以训练模型，其Utility 与实际数据训练模型相似。我们的分析还显示，使用 MWEM PGM 生成的 sintética datos 可以训练同时实现实用性和公平性的模型，与实际数据训练模型几乎相当。
</details></li>
</ul>
<hr>
<h2 id="A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications"><a href="#A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications" class="headerlink" title="A spectral regularisation framework for latent variable models designed for single channel applications"></a>A spectral regularisation framework for latent variable models designed for single channel applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19246">http://arxiv.org/abs/2310.19246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Balshaw, P. Stephan Heyns, Daniel N. Wilke, Stephan Schmidt</li>
<li>for: 这篇论文的目的是提出一个 Python 包，用于解决单 канал Latent Variable Models (LVMs) 中的数据重复问题，并通过添加一个新的spectral regularization term来解决这个问题。</li>
<li>methods: 该论文使用的方法是在 LVM 优化过程中添加spectral regularization term，以提高 LVM 的拟合精度和稳定性。</li>
<li>results: 该论文提出的 Python 包可以在单 канал时间序列应用中提供一个一致的线性 LVM 优化框架，并通过spectral regularization来提高 LVM 的拟合精度。<details>
<summary>Abstract</summary>
Latent variable models (LVMs) are commonly used to capture the underlying dependencies, patterns, and hidden structure in observed data. Source duplication is a by-product of the data hankelisation pre-processing step common to single channel LVM applications, which hinders practical LVM utilisation. In this article, a Python package titled spectrally-regularised-LVMs is presented. The proposed package addresses the source duplication issue via the addition of a novel spectral regularisation term. This package provides a framework for spectral regularisation in single channel LVM applications, thereby making it easier to investigate and utilise LVMs with spectral regularisation. This is achieved via the use of symbolic or explicit representations of potential LVM objective functions which are incorporated into a framework that uses spectral regularisation during the LVM parameter estimation process. The objective of this package is to provide a consistent linear LVM optimisation framework which incorporates spectral regularisation and caters to single channel time-series applications.
</details>
<details>
<summary>摘要</summary>
The proposed package provides a framework for spectral regularization in single channel LVM applications, making it easier to investigate and utilize LVMs with spectral regularization. The package uses symbolic or explicit representations of potential LVM objective functions and incorporates spectral regularization during the LVM parameter estimation process. The objective of this package is to provide a consistent linear LVM optimization framework that incorporates spectral regularization and caters to single channel time-series applications.
</details></li>
</ul>
<hr>
<h2 id="Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning"><a href="#Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning" class="headerlink" title="Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning"></a>Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19222">http://arxiv.org/abs/2310.19222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wfwf10/mkor">https://github.com/wfwf10/mkor</a></li>
<li>paper_authors: Feng Wang, Senem Velipasalar, M. Cenk Gursoy</li>
<li>for: 防止客户端资料遭到泄露，保持隐私。</li>
<li>methods: 使用秘密修改参数，从客户端的梯度更新中恢复输入数据。</li>
<li>results: 在MNIST、CIFAR-100和ImageNet dataset上表现出色，比现有方法高品质。<details>
<summary>Abstract</summary>
Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 目的是保持客户端数据本地，以保持隐私。而不是收集客户端数据本身，服务器只收集客户端发送的聚合梯度更新。随着 Federated learning 的普及，有很多研究揭露了 Federated learning 方法的漏洞，可以从梯度更新中重建输入数据。然而，大多数现有工作假设了 Federated learning 的batch size很小，并且在大batch size时的图像质量很差。其他工作会修改神经网络的架构或参数，使其变得异常，因此可以被客户端探测。此外，大多数方法只能从大批量中重建一个样本输入。为解决这些限制，我们提出了一种新的、完全分析的方法， referred to as maximum knowledge orthogonality reconstruction (MKOR)，可以从客户端发送的梯度更新中重建客户端的输入数据。我们的提议方法可以从大批量中重建高质量的数学确定的图像。MKOR只需服务器在客户端所需的情况下隐藏地将参数发送给客户端，并可以高效地、不受注意的重建输入图像。我们对 MKOR 的性能进行了 MNIST、CIFAR-100 和 ImageNet  dataset 的测试，并与现有方法进行了比较。结果表明，MKOR 在输入图像质量和梯度更新精度等方面都有较好的表现，引起了隐私保护方面的进一步研究，以开发全面的防御策略。
</details></li>
</ul>
<hr>
<h2 id="From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals"><a href="#From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals" class="headerlink" title="From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals"></a>From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19220">http://arxiv.org/abs/2310.19220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titing Cui, Su Jia, Thomas Lavastida<br>for:The paper is written to address the dynamic pricing problem in a stream of customers, where high-valuation customers tend to make purchases earlier and leave the market, leading to a shift in the valuation distribution.methods:The paper uses a model where a pool of non-strategic unit-demand customers interact repeatedly with the seller, and each customer monitors the price intermittently according to an independent Poisson process. The paper presents a minimax optimal algorithm that computes a non-adaptive policy which guarantees a $1&#x2F;k$ fraction of the optimal revenue, given any set of $k$ prices. Additionally, the paper presents an adaptive learn-then-earn policy based on a novel debiasing approach.results:The paper achieves an $\tilde O(kn^{3&#x2F;4})$ regret bound for the adaptive policy, and further improves the bound to $\tilde O(k^{3&#x2F;4} n^{3&#x2F;4})$ using martingale concentration inequalities.<details>
<summary>Abstract</summary>
The dynamic pricing problem has been extensively studied under the \textbf{stream} model: A stream of customers arrives sequentially, each with an independently and identically distributed valuation. However, this formulation is not entirely reflective of the real world. In many scenarios, high-valuation customers tend to make purchases earlier and leave the market, leading to a \emph{shift} in the valuation distribution. Thus motivated, we consider a model where a \textbf{pool} of $n$ non-strategic unit-demand customers interact repeatedly with the seller. Each customer monitors the price intermittently according to an independent Poisson process and makes a purchase if the observed price is lower than her \emph{private} valuation, whereupon she leaves the market permanently. We present a minimax \emph{optimal} algorithm that efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of the optimal revenue, given any set of $k$ prices. Moreover, we present an adaptive \emph{learn-then-earn} policy based on a novel \emph{debiasing} approach, and prove an $\tilde O(kn^{3/4})$ regret bound. We further improve the bound to $\tilde O(k^{3/4} n^{3/4})$ using martingale concentration inequalities.
</details>
<details>
<summary>摘要</summary>
“流动价格问题已经广泛研究过，使用流动模型：一条流动的客户来sequentially，每个客户都有独立并同分布的评估。但这种形式并不完全反映现实世界。在许多场景下，高评估客户往往在早期购买并离开市场，导致评估分布的变化。因此，我们考虑了一个池塘模型：$n$个不策略性单元需求客户与卖家互动着重。每个客户按照独立的波利逊过程监测价格，如果观察到的价格低于她的私有评估，就会购买并永久离开市场。我们提出了一种最优化算法，可以快速计算一个不适应的策略，保证收入的$1/k$部分是最优的。此外，我们还提出了一种学习然后获得的策略，基于一种新的减偏方法，并证明了$\tilde O(kn^{3/4})$的违和 bound。最后，我们使用 martingale concetration不等式提高 bound to $\tilde O(k^{3/4} n^{3/4})$。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions"><a href="#A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions" class="headerlink" title="A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions"></a>A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19218">http://arxiv.org/abs/2310.19218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Yang, Yang Zhao</li>
<li>for: This paper provides a comprehensive survey of Federated Unlearning (FU), which is an emerging area that aims to selectively unlearn specific information in a decentralized and privacy-preserving manner.</li>
<li>methods: The paper discusses various algorithms, objectives, and evaluation metrics for FU, and identifies some challenges in this area.</li>
<li>results: The paper summarizes existing studies on FU into a taxonomy, including schemes, potential applications, and future directions.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提供了 Federated Unlearning（FU）的全面检讨，FU 是一个emerging领域，旨在在分布式和隐私保护的情况下选择性地忘记特定的信息。</li>
<li>methods: 论文讨论了 FU 中不同的算法、目标和评价指标，并指出了一些挑战。</li>
<li>results: 论文将现有的 FU 研究总结为一个分类表，包括方案、应用 potential 和未来方向。<details>
<summary>Abstract</summary>
With the development of trustworthy Federated Learning (FL), the requirement of implementing right to be forgotten gives rise to the area of Federated Unlearning (FU). Comparing to machine unlearning, a major challenge of FU lies in the decentralized and privacy-preserving nature of FL, in which clients jointly train a global model without sharing their raw data, making it substantially more intricate to selectively unlearn specific information. In that regard, many efforts have been made to tackle the challenges of FU and have achieved significant progress. In this paper, we present a comprehensive survey of FU. Specially, we provide the existing algorithms, objectives, evaluation metrics, and identify some challenges of FU. By reviewing and comparing some studies, we summarize them into a taxonomy for various schemes, potential applications and future directions.
</details>
<details>
<summary>摘要</summary>
随着可靠的联合学习（FL）的发展，实施“忘记权”的需求带来了联合学习（FU）的领域。与机器学习中的机器忘记相比，联合学习中的主要挑战在于在分布式和隐私保护的情况下，客户端共同训练全球模型，而不是分享raw数据，使其变得显著更加复杂，以选择性地忘记特定信息。为此，许多努力已经被作出，并取得了显著进步。在这篇论文中，我们提供了联合学习的完整报告。特别是，我们提供了现有的算法、目标、评价指标，并 indentified一些联合学习的挑战。通过对一些研究的复习和比较，我们将其总结为一种分类表，包括不同的方案、应用领域和未来方向。
</details></li>
</ul>
<hr>
<h2 id="On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization"><a href="#On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization" class="headerlink" title="On the accuracy and efficiency of group-wise clipping in differentially private optimization"></a>On the accuracy and efficiency of group-wise clipping in differentially private optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19215">http://arxiv.org/abs/2310.19215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Bu, Ruixuan Liu, Yu-Xiang Wang, Sheng Zha, George Karypis</li>
<li>for: 这个研究旨在探讨差异private（DP）深度学习中的渐进式对策，特别是在大规模的 Computer Vision 和自然语言处理模型中。</li>
<li>methods: 这个研究使用了渐进式对策的各种不同式抑制方法，包括层别抑制和各层抑制，并进行了实验和分析。</li>
<li>results: 研究结果显示，不同的抑制式对策有相同的时间复杂度，但是它们实现了精确度-储存空间负面的贸易：对于大型模型，层别抑制可以实现更高的精确度和更低的峰值储存空间。<details>
<summary>Abstract</summary>
Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices"><a href="#Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices" class="headerlink" title="Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices"></a>Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19214">http://arxiv.org/abs/2310.19214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvxgrp/mlr_fitting">https://github.com/cvxgrp/mlr_fitting</a></li>
<li>paper_authors: Tetiana Parshakova, Trevor Hastie, Eric Darve, Stephen Boyd</li>
<li>for: 这个论文关注了multilevel low rank（MLR）矩阵的问题，MLR矩阵是一种扩展了低级矩阵的矩阵，它们共享许多特性，如积分器产生的存储量和积分器-向量乘法的复杂性。</li>
<li>methods: 该论文提出了三个问题的解决方案，即因子适应、级别分配和层次分区。它们使用了一种基于因子的方法来适应MLR矩阵，并且提供了一个开源包来实现这些方法。</li>
<li>results: 该论文的结果表明，使用该方法可以高效地适应MLR矩阵，并且可以控制积分器的存储量和复杂性。它们还提供了一些对MLR矩阵的分析和评估。<details>
<summary>Abstract</summary>
We consider multilevel low rank (MLR) matrices, defined as a row and column permutation of a sum of matrices, each one a block diagonal refinement of the previous one, with all blocks low rank given in factored form. MLR matrices extend low rank matrices but share many of their properties, such as the total storage required and complexity of matrix-vector multiplication. We address three problems that arise in fitting a given matrix by an MLR matrix in the Frobenius norm. The first problem is factor fitting, where we adjust the factors of the MLR matrix. The second is rank allocation, where we choose the ranks of the blocks in each level, subject to the total rank having a given value, which preserves the total storage needed for the MLR matrix. The final problem is to choose the hierarchical partition of rows and columns, along with the ranks and factors. This paper is accompanied by an open source package that implements the proposed methods.
</details>
<details>
<summary>摘要</summary>
我们考虑多层低级矩阵（MLR矩阵），定义为一个行列排序的卷积矩阵的和，其中每一个矩阵都是前一个矩阵的块分解，所有块都是低级矩阵的块分解， givens in factored form。 MLR矩阵扩展了低级矩阵，但与其有许多共同性，如绝对存储量和矩阵-向量乘法的复杂度。我们解决了三个在使用给定矩阵适应MLR矩阵的 Frobenius 范数中出现的问题。第一个问题是调整MLR矩阵的因子。第二个问题是分配级别，即在每个层中选择块的级别，保持总级别的值，这 preserved 绝对存储量需要的MLR矩阵。最后一个问题是选择行列层次结构，以及级别和因子。这篇文章附有一个开源包，实现我们提议的方法。
</details></li>
</ul>
<hr>
<h2 id="Investigative-Pattern-Detection-Framework-for-Counterterrorism"><a href="#Investigative-Pattern-Detection-Framework-for-Counterterrorism" class="headerlink" title="Investigative Pattern Detection Framework for Counterterrorism"></a>Investigative Pattern Detection Framework for Counterterrorism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19211">http://arxiv.org/abs/2310.19211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashika R. Muramudalige, Benjamin W. K. Hung, Rosanne Libretti, Jytte Klausen, Anura P. Jayasumana</li>
<li>for: 预防暴力激进分子发动袭击，保障公众安全。</li>
<li>methods: 使用自动化工具提取信息，回答分析员的问题，不断扫描新信息，与过去事件集成，提醒出现风险。</li>
<li>results: 开发了一套名为INSPECT的调查模式检测框架，可以自动执行大规模的详细审讯评估，形成知识网络，并提供行为指标和激进路径的查询功能。<details>
<summary>Abstract</summary>
Law-enforcement investigations aimed at preventing attacks by violent extremists have become increasingly important for public safety. The problem is exacerbated by the massive data volumes that need to be scanned to identify complex behaviors of extremists and groups. Automated tools are required to extract information to respond queries from analysts, continually scan new information, integrate them with past events, and then alert about emerging threats. We address challenges in investigative pattern detection and develop an Investigative Pattern Detection Framework for Counterterrorism (INSPECT). The framework integrates numerous computing tools that include machine learning techniques to identify behavioral indicators and graph pattern matching techniques to detect risk profiles/groups. INSPECT also automates multiple tasks for large-scale mining of detailed forensic biographies, forming knowledge networks, and querying for behavioral indicators and radicalization trajectories. INSPECT targets human-in-the-loop mode of investigative search and has been validated and evaluated using an evolving dataset on domestic jihadism.
</details>
<details>
<summary>摘要</summary>
法警调查用于预防暴力激进分子的袭击已成为公共安全的重要问题。这问题受到巨量数据的检索和分析的困难，以检测激进分子和组织的复杂行为。我们面临的挑战是 automatizat 调查模式的检测和发现潜在威胁。为解决这些挑战，我们提出了一个调查模式检测框架（INSPECT）。该框架 integrates 多种计算工具，包括机器学习技术和图pattern匹配技术，以检测行为指标和风险个人/组织。INSPECT 还自动化了大规模的审批细节传记、组织知识网络和查询行为指标和激进化轨迹。INSPECT 采用人类在循环搜索模式，并已经验证和评估使用了随时间变化的数据集，以适应家庭激进主义的调查需求。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.LG_2023_10_30/" data-id="clogxf3p600rk5xrahuz94psf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/30/cs.CL_2023_10_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-10-30
        
      </div>
    </a>
  
  
    <a href="/2023/10/30/eess.SP_2023_10_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-10-30</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
