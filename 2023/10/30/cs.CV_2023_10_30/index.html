
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-10-30 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20083 repo_url: None paper_authors: Shuvam Keshari, Ta">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-10-30">
<meta property="og:url" content="https://nullscc.github.io/2023/10/30/cs.CV_2023_10_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20083 repo_url: None paper_authors: Shuvam Keshari, Ta">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-30T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.310Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CV_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T13:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-10-30
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview"><a href="#Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview" class="headerlink" title="Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview"></a>Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20083">http://arxiv.org/abs/2310.20083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuvam Keshari, Tanusree Dutta, Raju Mullick, Ashish Rathor, Priyadarshi Patnaik</li>
<li>for: 这个研究旨在帮助采访者更好地评估面试者的人格特征，减少采访者的认知压力。</li>
<li>methods: 这个研究使用了 behaviometry 技术，具体来说是通过分析面试者的表情、嗓音、肤色反应、近距离行为、身体语言等不同方面的行为特征，以获取不受社会可能性和假装影响的 объектив信息。</li>
<li>results: 研究发现，通过使用 behaviometry 技术，可以对面试者的行为进行对比分析，并且可以在不同的视频样本中获取高度相似的结果。在实验中，使用 open-source 计算机视觉算法和库（python-opencv 和 dlib），对三个 YouTube 视频样本进行了帧对帆分析，并发现 SSID 分数在 75% 以上的情况下表现出行为一致性。<details>
<summary>Abstract</summary>
Choosing the right person for the right job makes the personnel interview process a cognitively demanding task. Psychometric tests, followed by an interview, have often been used to aid the process although such mechanisms have their limitations. While psychometric tests suffer from faking or social desirability of responses, the interview process depends on the way the responses are analyzed by the interviewers. We propose the use of behaviometry as an assistive tool to facilitate an objective assessment of the interviewee without increasing the cognitive load of the interviewer. Behaviometry is a relatively little explored field of study in the selection process, that utilizes inimitable behavioral characteristics like facial expressions, vocalization patterns, pupillary reactions, proximal behavior, body language, etc. The method analyzes thin slices of behavior and provides unbiased information about the interviewee. The current study proposes the methodology behind this tool to capture facial expressions, in terms of facial asymmetry and micro-expressions. Hemi-facial composites using a structural similarity index was used to develop a progressive time graph of facial asymmetry, as a test case. A frame-by-frame analysis was performed on three YouTube video samples, where Structural similarity index (SSID) scores of 75% and more showed behavioral congruence. The research utilizes open-source computer vision algorithms and libraries (python-opencv and dlib) to formulate the procedure for analysis of the facial asymmetry.
</details>
<details>
<summary>摘要</summary>
选择合适的人 für 合适的工作是人员面试的认知需求很高。常用的方法包括心理测试 followed by 面试，但这些机制有其局限性。心理测试受到假答案和社会受欢迎的响应的困扰，而面试过程则取决于面试官如何分析回答。我们提议使用 behaviometry 作为助手，以帮助对面试者进行 объектив的评估，不增加面试官的认知负担。 behaviometry 是一个还未受到充分探索的选拔过程中的研究领域，它利用不同人的唯一行为特征，如面部表情、嗓音模式、眼动反应、躯体语言等，来对面试者进行评估。该方法分析精细的行为迹象，提供不受偏见的信息。本研究提出了对于捕捉面部表情的方法，包括面部不均匀性和微表情。通过 Hemiface composite 技术，建立了面部不均匀性的进度图。对 YouTube 视频三个样本进行了帧对帆分析，SSID 分数达 75% 以上显示行为一致。研究使用了开源计算机视觉算法和库（python-opencv 和 dlib）来定义面部不均匀性的分析过程。
</details></li>
</ul>
<hr>
<h2 id="LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart"><a href="#LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart" class="headerlink" title="LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart"></a>LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20065">http://arxiv.org/abs/2310.20065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Fanwei Kong, Shawn Shadden</li>
<li>for: automatically generate computer models of the human heart from patient imaging data</li>
<li>methods: deforming a template mesh to fit the cardiac structures, using a two-stage diffeomorphic deformation process and a novel loss function to minimize mesh self-penetration</li>
<li>results: meshes free of self-intersections, comparable accuracy with state-of-the-art methods, and readily usable in physics-based simulation without the need for post-processing and cleanup.Here’s the full text in Simplified Chinese:</li>
<li>for:  automatische genereren van computer modellen van het menselijke hart uit patiënt-imaging-gegevens</li>
<li>methods: deformeren van een template-mesh om de hartstructuren te passen, gebruik maken van een twee-fase diffeomorfe deformatieve proces en een nieuwe verliesfunctie om mes-zelf-doorboringen te minimaliseren</li>
<li>results: mesh-vrij van zelf-doorboringen, vergelijkbare accuracy met staat-van-het-kunst-methoden, en direct gebruikbaar in fysisch-gebaseerde simulatie zonder post-behandeling en opschoning nodig hebben.<details>
<summary>Abstract</summary>
We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
</details>
<details>
<summary>摘要</summary>
我们提出了一种深度学习模型，用于自动生成人体心脏模型从患者影像数据中，强调其能够生成薄壁心脏结构。我们的方法是将模板网格变形以适应给定图像中的心脏结构。与先前的深度学习方法不同，我们的框架是为避免网格自交叠，通常在变形 superfic 上存在小距离之间的自交叠。我们实现了这一目标通过两stage diffeomorphic deformation 过程和一种新的损失函数，该函数基于运动 dinamics 中的几何学特征，以惩罚表面接触和交叠。我们的模型与当前状态的方法具有相似的准确性，同时还可以生成自交叠的网格。所得到的网格可以directly用于基于物理学的仿真，减少后期处理和整理的需求。
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal"><a href="#A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal" class="headerlink" title="A Scalable Training Strategy for Blind Multi-Distribution Noise Removal"></a>A Scalable Training Strategy for Blind Multi-Distribution Noise Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20064">http://arxiv.org/abs/2310.20064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Zhang, Sakshum Kulshrestha, Christopher Metzler</li>
<li>for: 提出一种基于adaptive sampling&#x2F;active learning的 universal denoising networks训练方法，以提高特定频率训练的效果。</li>
<li>methods: 使用一种基于多项式近似的特性损失地形图，以减少训练时间，并使用适应样本选择策略来吸引训练数据的优化。</li>
<li>results: 通过测试在模拟的联合波音-高斯-斑雾噪声中，提出的训练策略可以使一个普通的权重相同的单个缺陷网络达到特定频率训练的特化网络的峰峰信号强率 bound，并且在一个小型图像集上，对比uniform训练的基准，提出的适应样本选择策略可以获得更好的性能。<details>
<summary>Abstract</summary>
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.   In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed universal denoiser training strategy by extending these results to higher dimensions and by incorporating a polynomial approximation of the true specification-loss landscape. This approximation allows us to reduce training times by almost two orders of magnitude. We test our method on simulated joint Poisson-Gaussian-Speckle noise and demonstrate that with our proposed training strategy, a single blind, generalist denoiser network can achieve peak signal-to-noise ratios within a uniform bound of specialized denoiser networks across a large range of operating conditions. We also capture a small dataset of images with varying amounts of joint Poisson-Gaussian-Speckle noise and demonstrate that a universal denoiser trained using our adaptive-sampling strategy outperforms uniformly trained baselines.
</details>
<details>
<summary>摘要</summary>
尽管最近有所进步，总结束universal锈排除和遗传物理误差网络仍然是一个大多数开放的问题：固定网络参数时，一定程度上特化于一个任务（例如，除掉Poisson锈）的性能会降低另一个任务（例如，除掉锦锈锈）的性能。此外，训练这种网络也是困难的，因为维度的诅咒：随着特征空间的维度的增加（即需要描述噪声分布的参数数），需要训练的唯一特征的数量会 exponential增加。随机抽取这个空间会导致一个网络可以在非常困难的任务特征上做出非常好的表现，但是在容易的任务特征上做出非常差的表现，即使大的错误也只会对总的平方误差产生非常小的影响。在这个工作中，我们提出了使用adaptive-sampling/active-learning策略来训练锈排除网络。我们的工作超越了最近提出的universal锈排除训练策略，并在更高的维度上进行扩展，并通过使用幂等式approximation来减少训练时间。我们在 simulated joint Poisson-Gaussian-Speckle 噪声下测试了我们的方法，并证明了一个替身、通用的锈排除网络可以在一个广泛的操作条件下达到特殊化锈排除网络的峰值信号强度。我们还捕捉了一个小型的图像数据集，该数据集包含不同量的 joint Poisson-Gaussian-Speckle 噪声，并证明了一个通用的锈排除网络，使用我们的adaptive-sampling策略训练后，可以超过uniform训练的基eline。
</details></li>
</ul>
<hr>
<h2 id="SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling"><a href="#SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling" class="headerlink" title="SolarFormer: Multi-scale Transformer for Solar PV Profiling"></a>SolarFormer: Multi-scale Transformer for Solar PV Profiling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20057">http://arxiv.org/abs/2310.20057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Luis, Minh Tran, Taisei Hanyu, Anh Tran, Liao Haitao, Roy McCann, Alan Mantooth, Ying Huang, Ngan Le</li>
<li>for: 该论文旨在提供高精度的太阳能板 Installation 地图，以便更好地理解太阳能板的采用和扩展。</li>
<li>methods: 该论文提出了 SolarFormer 模型，通过多尺度 Transformer 编码器和带mask的 Transformer 解码器，解决了太阳能板识别的复杂问题，并利用低级特征和实例查询机制进一步提高了地图的精度。</li>
<li>results: 经过广泛的实验表明，SolarFormer 模型能够与或超过当前状态的模型，提供高精度的太阳能板分 segmentation，为全球可再生能源发展提供了有价值的参考。<details>
<summary>Abstract</summary>
As climate change intensifies, the global imperative to shift towards sustainable energy sources becomes more pronounced. Photovoltaic (PV) energy is a favored choice due to its reliability and ease of installation. Accurate mapping of PV installations is crucial for understanding their adoption and informing energy policy. To meet this need, we introduce the SolarFormer, designed to segment solar panels from aerial imagery, offering insights into their location and size. However, solar panel identification in Computer Vision is intricate due to various factors like weather conditions, roof conditions, and Ground Sampling Distance (GSD) variations. To tackle these complexities, we present the SolarFormer, featuring a multi-scale Transformer encoder and a masked-attention Transformer decoder. Our model leverages low-level features and incorporates an instance query mechanism to enhance the localization of solar PV installations. We rigorously evaluated our SolarFormer using diverse datasets, including GGE (France), IGN (France), and USGS (California, USA), across different GSDs. Our extensive experiments consistently demonstrate that our model either matches or surpasses state-of-the-art models, promising enhanced solar panel segmentation for global sustainable energy initiatives.
</details>
<details>
<summary>摘要</summary>
However, identifying solar panels using computer vision is a complex task due to factors such as weather conditions, roof conditions, and variations in ground sampling distance (GSD). To tackle these challenges, the SolarFormer employs a multi-scale transformer encoder and a masked-attention transformer decoder, which leverages low-level features and incorporates an instance query mechanism to enhance the localization of solar PV installations.We evaluated the SolarFormer using diverse datasets from France, California, and other regions, and our extensive experiments consistently showed that our model either matches or outperforms existing models in terms of solar panel segmentation. This promises to enhance sustainable energy initiatives globally.
</details></li>
</ul>
<hr>
<h2 id="Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation"><a href="#Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation" class="headerlink" title="Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation"></a>Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20039">http://arxiv.org/abs/2310.20039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoichi Watanabe, Rukhsora Akramova<br>for: 这项研究旨在评估验血医生和自动分割工具的分割精度，并评估Dice相似度系数（DSC）是否能够准确评估分割精度。methods: 本研究使用PyRadiomics提取 радиомькс特征，并根据Concordance Correlation Coefficient（CCC）选择可重复的特征来评估分割精度。results: 研究发现206个 радиомькс特征的CCC值大于0.93，表明这些特征具有Robust重复性。此外，7个特征表现出低Intraclass Correlation Coefficients（ICC），这意味着它们更敏感于分割差异。特别是，原始形态特征，包括球形性、延展性和平坦性，ICC值在0.1177-0.995之间。与此相反，所有DSC值都大于0.778。这项研究表明， радиомькс特征，特别是形态和能量相关的特征，可以捕捉分割精度中的细微差异，而DSC则无法做到这一点。因此，Radiomics特征与ICC证明为评估医生分割能力和自动分割工具性能的优秀指标。这些新指标可以用来评估新的自动分割方法，并提高医疗分割训练，从而提高放疗医疗实践。<details>
<summary>Abstract</summary>
In high-quality radiotherapy delivery, precise segmentation of targets and healthy structures is essential. This study proposes Radiomics features as a superior measure for assessing the segmentation ability of physicians and auto-segmentation tools, in comparison to the widely used Dice Similarity Coefficient (DSC). The research involves selecting reproducible radiomics features for evaluating segmentation accuracy by analyzing radiomics data from 2 CT scans of 10 lung tumors, available in the RIDER Data Library. Radiomics features were extracted using PyRadiomics, with selection based on the Concordance Correlation Coefficient (CCC). Subsequently, CT images from 10 patients, each segmented by different physicians or auto-segmentation tools, were used to assess segmentation performance. The study reveals 206 radiomics features with a CCC greater than 0.93 between the two CT images, indicating robust reproducibility. Among these features, seven exhibit low Intraclass Correlation Coefficients (ICC), signifying increased sensitivity to segmentation differences. Notably, ICCs of original shape features, including sphericity, elongation, and flatness, ranged from 0.1177 to 0.995. In contrast, all DSC values exceeded 0.778. This research demonstrates that radiomics features, particularly those related to shape and energy, can capture subtle variations in tumor segmentation characteristics, unlike DSC. As a result, Radiomics features with ICC prove superior for evaluating a physician's tumor segmentation ability and the performance of auto-segmentation tools. The findings suggest that these new metrics can be employed to assess novel auto-segmentation methods and enhance the training of individuals in medical segmentation, thus contributing to improved radiotherapy practices.
</details>
<details>
<summary>摘要</summary>
高品质放射治疗需要精准的目标和健康结构分割。本研究提出，基于放射学特征（Radiomics）的评价方法比普遍使用的相似度系数（Dice Similarity Coefficient，DSC）更为可靠。研究选择了可重复的放射学特征，通过分析20个肺癌CT扫描图，从RIDER数据库中获得的放射学数据，以确定评价准确性的可靠特征。然后，从10名患者的CT图像中，每名患者由不同的医生或自动分割工具进行分割，以评估分割性能。研究发现，206个放射学特征之间的 concordance correlation coefficient（CCC）大于0.93，表明可靠的重复性。其中，7个特征具有低的间类相关系数（ICC），表明它们更敏感于分割差异。特别是，原始形状特征，包括圆形性、长宽比和平整度，其ICC值分别为0.1177-0.995。与此相比，所有DSC值都大于0.778。这种研究表明，基于形状和能量的放射学特征可以更好地捕捉小差异在肿瘤分割特征中，与DSC不同。因此，基于ICC的放射学特征评价方法比DSC更为可靠，可以用于评估新的自动分割方法和医生分割能力的训练。这些发现可能对放射治疗实践产生积极影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning"><a href="#Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning" class="headerlink" title="Adaptive Anchor Label Propagation for Transductive Few-Shot Learning"></a>Adaptive Anchor Label Propagation for Transductive Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19996">http://arxiv.org/abs/2310.19996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michalislazarou/a2lp">https://github.com/michalislazarou/a2lp</a></li>
<li>paper_authors: Michalis Lazarou, Yannis Avrithis, Guangyu Ren, Tania Stathaki</li>
<li>for: 该研究是为了解决基于有限标注数据的图像分类问题。</li>
<li>methods: 该研究使用了推理扩充法，特别是标签传播法，以利用大量无标注数据来提高少量学习的性能。</li>
<li>results: 该研究提出了一种新的算法，称为自适应固定标签传播（Adaptive Anchor Label Propagation），该算法可以在扩充法中更好地调整标签，从而提高少量学习的性能。对四个常用的少量学习 benchmark 数据集和两种常用的背部骨架进行了实验，并证明了该算法的优势。<details>
<summary>Abstract</summary>
Few-shot learning addresses the issue of classifying images using limited labeled data. Exploiting unlabeled data through the use of transductive inference methods such as label propagation has been shown to improve the performance of few-shot learning significantly. Label propagation infers pseudo-labels for unlabeled data by utilizing a constructed graph that exploits the underlying manifold structure of the data. However, a limitation of the existing label propagation approaches is that the positions of all data points are fixed and might be sub-optimal so that the algorithm is not as effective as possible. In this work, we propose a novel algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function optimizing their positions in the manifold in the process. Our novel algorithm, Adaptive Anchor Label Propagation}, outperforms the standard label propagation algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings respectively. We provide experimental results highlighting the merits of our algorithm on four widely used few-shot benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and WideResNet-28-10. The source code can be found at https://github.com/MichalisLazarou/A2LP.
</details>
<details>
<summary>摘要</summary>
几个拟合学习地址了使用有限的标注数据来分类图像的问题。通过使用无标注数据的推理方法，如标签传播，可以在几个拟合学习中显著提高性能。标签传播将无标注数据的 pseudo-标签推断出来，利用构建的图像结构下的数据图 Mann 的方法。然而，现有的标签传播方法的局限性是所有数据点的位置都是固定的，这可能会导致算法不太有效。在这种情况下，我们提出了一种新的算法，可以适应标注数据的特征表示进行最优化。我们称之为 Adaptive Anchor Label Propagation（A2LP）。在1-shot和5-shot设置中，A2LP算法可以与标准标签传播算法相比，提高性能达7%和2%。我们通过在四个广泛使用的几个拟合学习测试集上进行实验，分别是miniImageNet、tieredImageNet、CUB和CIFAR-FS，以及两种常用的后向核心，ResNet12和WideResNet-28-10，来证明我们的算法的优势。代码可以在https://github.com/MichalisLazarou/A2LP 找到。
</details></li>
</ul>
<hr>
<h2 id="Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning"><a href="#Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning" class="headerlink" title="Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning"></a>Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19995">http://arxiv.org/abs/2310.19995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Etesam, Ozge Nilay Yalcin, Chuxuan Zhang, Angelica Lim</li>
<li>for: 本研究的目的是评估 latest large vision language models (CLIP, LLaVA) 和 large language models (GPT-3.5) 中嵌入的情感常识，并evaluate a purely text-based language model on images。</li>
<li>methods: 研究者使用 “narrative captions” 来描述情感，并使用 872 个物理社会信号描述和 224 个情感相关的环境 Labels 来构建 image-to-language-to-emotion 任务。</li>
<li>results: 研究发现，将 “fast” 和 “slow” 两种理解结合使用可以提高情感识别系统的性能，但 zero-shot 情感理论心问题仍然存在，与之前在 EMOTIC 数据集上进行的研究相比，还有一定的差距。<details>
<summary>Abstract</summary>
The emotional theory of mind problem in images is an emotion recognition task, specifically asking "How does the person in the bounding box feel?" Facial expressions, body pose, contextual information and implicit commonsense knowledge all contribute to the difficulty of the task, making this task currently one of the hardest problems in affective computing. The goal of this work is to evaluate the emotional commonsense knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely text-based language model on images, we construct "narrative captions" relevant to emotion perception, using a set of 872 physical social signal descriptions related to 26 emotional categories, along with 224 labels for emotionally salient environmental contexts, sourced from writer's guides for character expressions and settings. We evaluate the use of the resulting captions in an image-to-language-to-emotion task. Experiments using zero-shot vision-language models on EMOTIC show that combining "fast" and "slow" reasoning is a promising way forward to improve emotion recognition systems. Nevertheless, a gap remains in the zero-shot emotional theory of mind task compared to prior work trained on the EMOTIC dataset.
</details>
<details>
<summary>摘要</summary>
“情感理论心智问题在图像中是一个情感识别任务，具体的问题是“人在矩形盒中如何感到？”Facial expressions、身体姿势、contextual information和隐藏的常识知识都会增加这个任务的difficulty，使其成为现在情感 computing中最难的问题。本工作的目的是评估 latest large vision language models (CLIP, LLaVA) 和 large language models (GPT-3.5) 中嵌入的情感常识，以及在 EMOTIC 数据集上的表现。为了评估仅基于文本的语言模型在图像上，我们建立了“narative captions”，这些caption relevante 到情感识别，使用872个物理社交信号描述和26个情感分类，以及224个情感突出的环境 Labels，它们来自作家的表达和设置指南。我们使用这些 captions 进行图像-语言-情感任务的评估。这些 zero-shot vision-language models 在 EMOTIC 数据集上的实验表明，结合“快”和“慢”的思考是一种有前途的方法来提高情感识别系统。然而，在 zero-shot 情感理论心智任务中，与先前工作相比，仍有一个差距。”
</details></li>
</ul>
<hr>
<h2 id="Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models"><a href="#Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models" class="headerlink" title="Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models"></a>Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19986">http://arxiv.org/abs/2310.19986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preetam Prabhu Srikar Dammu, Yunhe Feng, Chirag Shah</li>
<li>for: 提高机器学习模型对弱化群体的表现</li>
<li>methods: 利用网络搜索和生成模型提高机器学习模型的鲁棒性和减少偏见</li>
<li>results: 在图像分类问题上使用ImageNet的人类树subset中的例子，并达到了减少性别差距的目的，同时提高模型的总性能。<details>
<summary>Abstract</summary>
Machine learning (ML) technologies are known to be riddled with ethical and operational problems, however, we are witnessing an increasing thrust by businesses to deploy them in sensitive applications. One major issue among many is that ML models do not perform equally well for underrepresented groups. This puts vulnerable populations in an even disadvantaged and unfavorable position. We propose an approach that leverages the power of web search and generative models to alleviate some of the shortcomings of discriminative models. We demonstrate our method on an image classification problem using ImageNet's People Subtree subset, and show that it is effective in enhancing robustness and mitigating bias in certain classes that represent vulnerable populations (e.g., female doctor of color). Our new method is able to (1) identify weak decision boundaries for such classes; (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion; and (3) show how these newly captured training samples could alleviate population bias issue. While still improving the model's overall performance considerably, we achieve a significant reduction (77.30\%) in the model's gender accuracy disparity. In addition to these improvements, we observed a notable enhancement in the classifier's decision boundary, as it is characterized by fewer weakspots and an increased separation between classes. Although we showcase our method on vulnerable populations in this study, the proposed technique is extendable to a wide range of problems and domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion"><a href="#‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion" class="headerlink" title="‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion"></a>‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19981">http://arxiv.org/abs/2310.19981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sourojit Ghosh, Aylin Caliskan</li>
<li>For: The paper examines the stereotypes embedded in the text-to-image generator Stable Diffusion, specifically looking at gender and nationality&#x2F;continental identity.* Methods: The authors use vision-language model CLIP’s cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 and manually examine the results. They use prompts with front-facing images of persons from different continents, nationalities, and genders.* Results: The authors find that Stable Diffusion outputs of “a person” without any additional gender&#x2F;nationality information correspond closest to images of men and least with persons of nonbinary gender. They also observe continental stereotypes and resultant harms, such as the erasure of Indigenous Oceanic peoples. Additionally, they unexpectedly observe a pattern of oversexualization of women, specifically Latin American, Mexican, Indian, and Egyptian women relative to other nationalities.<details>
<summary>Abstract</summary>
We study stereotypes embedded within one of the most popular text-to-image generators: Stable Diffusion. We examine what stereotypes of gender and nationality/continental identity does Stable Diffusion display in the absence of such information i.e. what gender and nationality/continental identity is assigned to `a person', or to `a person from Asia'. Using vision-language model CLIP's cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 verified by manual examination, we chronicle results from 136 prompts (50 results/prompt) of front-facing images of persons from 6 different continents, 27 nationalities and 3 genders. We observe how Stable Diffusion outputs of `a person' without any additional gender/nationality information correspond closest to images of men and least with persons of nonbinary gender, and to persons from Europe/North America over Africa/Asia, pointing towards Stable Diffusion having a concerning representation of personhood to be a European/North American man. We also show continental stereotypes and resultant harms e.g. a person from Oceania is deemed to be Australian/New Zealander over Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who form a majority over descendants of colonizers both in Papua New Guinea and in Oceania overall. Finally, we unexpectedly observe a pattern of oversexualization of women, specifically Latin American, Mexican, Indian and Egyptian women relative to other nationalities, measured through an NSFW detector. This demonstrates how Stable Diffusion perpetuates Western fetishization of women of color through objectification in media, which if left unchecked will amplify this stereotypical representation. Image datasets are made publicly available.
</details>
<details>
<summary>摘要</summary>
我们研究了Stable Diffusion中嵌入的 gender和国家/地域标准的偏见。我们发现在没有任何信息时，Stable Diffusion将人物映射到男性和欧美人类中最为常见的情况下。使用CLIP的cosine similarity来比较由CLIP基于的Stable Diffusion v2.1生成的图像，我们对6个大陆、27个国家和3个 gender的136个提示（每个提示50个结果）进行了chronicle。我们发现Stable Diffusion无gender/国家信息时输出的图像最接近男性图像，并且与非binary gender人物最少相似，而且倾向于欧美人类而不是非洲/亚洲人类。这表明Stable Diffusion存在一种担忧的人类标准，即欧美男性。我们还发现了 continent预设和其相关的危害，例如，在 Oceanian 中，人物被认为是澳大利亚/新西兰人而不是 Papua New Guinean，这表明了殖民者的后代和原住民之间的人类观念的抹杀。最后，我们意外地发现了一种对女性进行性化的偏见，具体来说是对拉丁美洲、墨西哥、印度和埃及女性的性化，与其他国家的女性相比，这种偏见会通过媒体对女性的对象化来增强。我们将图像集公开发布。
</details></li>
</ul>
<hr>
<h2 id="Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks"><a href="#Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks" class="headerlink" title="Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks"></a>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19909">http://arxiv.org/abs/2310.19909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsouri/battle-of-the-backbones">https://github.com/hsouri/battle-of-the-backbones</a></li>
<li>paper_authors: Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein</li>
<li>For: The paper aims to benchmark a diverse suite of pretrained models for computer vision tasks, including vision-language models, self-supervised learning models, and the Stable Diffusion backbone, and to provide insights for the research community to advance computer vision.* Methods: The paper uses a diverse set of computer vision tasks, including classification, object detection, and out-of-distribution (OOD) generalization, and conducts more than 1500 training runs to evaluate the performance of various pretrained models.* Results: The paper finds that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks, and that self-supervised learning (SSL) backbones are highly competitive and indicate potential for future advancements with advanced architectures and larger pretraining datasets.<details>
<summary>Abstract</summary>
Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones
</details>
<details>
<summary>摘要</summary>
神经网络基于的计算机视觉系统通常建立在脊梁上，脊梁可以是预训练或随机初始化的特征提取器。过去几年，默认选择是ImageNet快速学习卷积神经网络。然而，最近几年，各种算法和数据集驱动的许多脊梁逐渐出现。这种多样性使得各种系统表现得更好，但是为实践者做出 Informed 决策变得更加困难。“Backbone Battle”（BoB）使得这种选择变得更加容易，它对一系列预训练模型进行了多种计算机视觉任务的测试，包括分类、物体检测和OOD泛化等。此外，BoB还为研究者提供了推进计算机视觉的可能性的指导，通过对超过1500次训练运行的全面分析，揭示了现有方法的优缺点。虽然视transformer（ViTs）和自动学习（SSL）在增长，但我们发现，通过大规模预训练集进行预训练的卷积神经网络仍然在大多数任务上表现最好。此外，在同一种架构和相同大小的预训练集上进行了Apples-to-Apples比较，我们发现SSL脊梁在相同的架构和预训练集上表现很竞争力，这意味着未来的工作应该使用更高级的架构和更大的预训练集进行SSL预训练。我们将实验结果和相关代码发布到GitHub上：<https://github.com/hsouri/Battle-of-the-Backbones>
</details></li>
</ul>
<hr>
<h2 id="MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder"><a href="#MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder" class="headerlink" title="MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder"></a>MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19898">http://arxiv.org/abs/2310.19898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahman-motiur/mist">https://github.com/rahman-motiur/mist</a></li>
<li>paper_authors: Md Motiur Rahman, Shiva Shokouhmand, Smriti Bhatt, Miad Faezipour</li>
<li>for: 这个研究是为了提高医疗影像分类的深度学习方法，特别是使用 transformer 来捕捉医疗影像中的长距离相依性。</li>
<li>methods: 我们提出了一个名为 Medical Image Segmentation Transformer (MIST) 的新方法，它包括一个预训 multi-axis vision transformer (MaxViT) 作为 Encoder，以及一个具有 Convolutional Attention Mixing (CAM) 解码器来于所有空间维度中捕捉长距离相依性。</li>
<li>results: 我们的 MIST transformer 在 ACDC 和 Synapse 数据集上与顶尖模型相比，实现了医疗影像分类的最佳性能。另外，我们还证明了在不同的维度上加入 CAM 解码器可以将分类性能提高到一个更高的水平。<details>
<summary>Abstract</summary>
One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details>
<details>
<summary>摘要</summary>
一种常见且有前途的深度学习方法是使用变换器进行医学图像分割，因为它可以通过自注意力来捕捉图像像素之间的长距离依赖关系。Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details></li>
</ul>
<hr>
<h2 id="DiffEnc-Variational-Diffusion-with-a-Learned-Encoder"><a href="#DiffEnc-Variational-Diffusion-with-a-Learned-Encoder" class="headerlink" title="DiffEnc: Variational Diffusion with a Learned Encoder"></a>DiffEnc: Variational Diffusion with a Learned Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19789">http://arxiv.org/abs/2310.19789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther</li>
<li>for: 这篇论文主要研究了Diffusion模型，它可以视为一种层次变换自动机（VAEs），具有两个改进：参数共享在生成过程中的条件分布，以及独立计算生成过程中的损失。</li>
<li>methods: 论文提出了两种改进 diffusion 模型，包括引入数据和深度相关的均值函数，以及让反编码过程中噪声方差的比率为自由参数。</li>
<li>results: 论文实现了 CIFAR-10 的状态之论文，并提供了一些理论探讨，如使用权重 diffusion 损失来优化噪声计划，以及在无穷深度层次中使用 ELBO 作为目标函数。<details>
<summary>Abstract</summary>
Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite-depth hierarchy, on the other hand, the weight parameter has to be 1 to have a well-defined ELBO.
</details>
<details>
<summary>摘要</summary>
Diffusion models可以看作为层次variational autoencoder (VAEs)，其中有两个改进：在生成过程中共享参数 для conditional distribution，以及独立计算损失的独立项。我们考虑了对 diffusion model 进行两种改变，以增加模型的灵活性。首先，我们在 diffusion process 中引入了数据和深度相依的均值函数，这导致了一个修改后的扩散损失。我们的提议框架，DiffEnc，在 CIFAR-10 上达到了状态泰�值的可能性。其次，我们允许反推进程中噪声变量的比率被视为一个自由参数，而不是固定为 1。这导致了一些理论上的洞察：在有限深度层次结构下，可以使用损失函数作为一个权重 diffusion loss 的目标，并且可以优化噪声 schedules 特别是 для推理。而在无限深度层次结构下，权重必须为 1，以便有一个准确的 ELBO。
</details></li>
</ul>
<hr>
<h2 id="MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision"><a href="#MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision" class="headerlink" title="MM-VID: Advancing Video Understanding with GPT-4V(ision)"></a>MM-VID: Advancing Video Understanding with GPT-4V(ision)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19773">http://arxiv.org/abs/2310.19773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang</li>
<li>for: 本文提出了一个整合GPT-4V的视频理解系统，用于解决长形视频和复杂任务，如多集集成的故事和多小时内容理解。</li>
<li>methods: 本文使用视频转脚本生成器，将多modal元素转化为长文本脚本，以便大语言模型实现视频理解。</li>
<li>results: 实验结果表明，MM-VID可以处理不同类型的视频，并在交互环境中表现出色，如视频游戏和图形用户界面。<details>
<summary>Abstract</summary>
We present MM-VID, an integrated system that harnesses the capabilities of GPT-4V, combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.
</details>
<details>
<summary>摘要</summary>
我们介绍MM-VID，一个集成了GPT-4V的系统，结合视觉、声音和语音特化工具，以实现高级视频理解。MM-VID是为了解决长形视频和复杂任务，如在多集 episodes 中理解故事情节和在多小时内进行推理。MM-VID 使用视频到脚本生成，使用GPT-4V将多Modal元素转化为长文本脚本。生成的脚本包括人物移动、动作、表情和对话，使得大自然语言模型（LLM）可以实现视频理解。这使得可以实现高级功能，包括音频描述、人物识别和多Modal高级理解。实验结果表明MM-VID可以处理不同的视频类型和视频长度。此外，我们还展示了它在交互环境中的潜在应用，如视频游戏和图形用户界面。
</details></li>
</ul>
<hr>
<h2 id="Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP"><a href="#Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP" class="headerlink" title="Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP"></a>Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19752">http://arxiv.org/abs/2310.19752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idstcv/inmap">https://github.com/idstcv/inmap</a></li>
<li>paper_authors: Qi Qian, Yuanhong Xu, Juhua Hu</li>
<li>for: 这篇论文主要是为了提出一种新的方法，即通过文本proxy直接学习视觉代理，以提高零基础任务的性能。</li>
<li>methods: 该方法使用CLIP的vision-language预训练方法，并提出了一种新的抽象方法，即在视觉空间中学习视觉代理。</li>
<li>results: 实验结果表明，该方法可以在短时间内在单个GPU上学习视觉代理，并提高零基础任务的性能。特别是，在ImageNet上，使用ViT-L&#x2F;14@336预训练后的CLIP，可以通过InMaP提高零基础任务的性能从77.02%提高到80.21%.<details>
<summary>Abstract</summary>
Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from $77.02\%$ to $80.21\%$ on ImageNet with ViT-L/14@336 pre-trained by CLIP. Code is available at \url{https://github.com/idstcv/InMaP}.
</details>
<details>
<summary>摘要</summary>
“文本语言预训练方法，如CLIP，显示了无需训练数据的视觉分类表现是非常出佩的。然而，视觉和文本空间之间的模态差异可能会导致表现下降。我们 theoretically 表明，这个差异无法通过CLIP中的对偶损失来减小到足够的程度。因此，在没有标注目标视觉数据的情况下，我们提议通过文本代理来直接学习视觉代理。此外，根据我们的理论分析，我们开发了一些策略来进一步优化 pseudo 标签得到的 InMaP 中的视觉代理。实验表明，InMaP 可以在单个 GPU 上在一分钟内 obtian 视觉代理，并将预训练后 CLIP 的 ViT-L/14@336 的零shot 准确率提高到 $77.02\%$ 到 $80.21\%$。代码可以在 \url{https://github.com/idstcv/InMaP} 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization"><a href="#Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization" class="headerlink" title="Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization"></a>Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19743">http://arxiv.org/abs/2310.19743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Wysoczanska, Moran Beladev, Karen Lastmann Assaraf, Fengjun Wang, Ofri Kleinfeld, Gil Amsalem, Hadas Harush Boker</li>
<li>for: 提高Booking.com上Properties的视觉摘要的用户体验，使其更加符合用户的意图和偏好。</li>
<li>methods: 通过分析用户的评论，提取用户最重要的问题，并将其 integrate into 图像摘要中，以提高用户的满意度。</li>
<li>results: 通过人类感知研究，证明我们的跨Modal方法（CrossSummarizer）在无需昂贵的标注的情况下，提高了图像摘要的质量，比基eline和图像 clustering 方法高。<details>
<summary>Abstract</summary>
Image collection summarization techniques aim to present a compact representation of an image gallery through a carefully selected subset of images that captures its semantic content. When it comes to web content, however, the ideal selection can vary based on the user's specific intentions and preferences. This is particularly relevant at Booking.com, where presenting properties and their visual summaries that align with users' expectations is crucial. To address this challenge, we consider user intentions in the summarization of property visuals by analyzing property reviews and extracting the most significant aspects mentioned by users. By incorporating the insights from reviews in our visual summaries, we enhance the summaries by presenting the relevant content to a user. Moreover, we achieve it without the need for costly annotations. Our experiments, including human perceptual studies, demonstrate the superiority of our cross-modal approach, which we coin as CrossSummarizer over the no-personalization and image-based clustering baselines.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:图像集合概要技术的目的是为图像集合提供一个精选的子集，以捕捉它的semantic content。但是在网络内容上，用户的具体目标和喜好会影响最佳选择。这 particualry relevant at Booking.com，因为在这里，为用户提供Properties和它们的视觉概要，与用户的期望保持一致，是关键。为解决这个挑战，我们在property visual概要中考虑用户的意图，通过分析Property reviews和提取用户提到的最重要方面。通过在概要中包含这些概要，我们可以提高概要，并为用户提供相关的内容。此外，我们可以在不需要昂贵的标注的情况下实现这一点。我们的实验，包括人类感知研究，证明我们的 CrossSummarizer 方法在无个性化和图像基于归类基eline上方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models"><a href="#Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models" class="headerlink" title="Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models"></a>Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19721">http://arxiv.org/abs/2310.19721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Jiacheng Wang, Ipek Oguz</li>
<li>for: 这个论文主要针对医疗影像分类领域的诸问题，例如数据收集和标签可用性问题，提出了将自然图像领域的学习结果转移到医疗影像领域的方法。</li>
<li>methods: 这个论文提出了一个名为ProMISe的启发驱动的3D医疗影像分类模型，使用仅一个点启发来利用已经预训的2D图像基础模型中的知识。具体来说，这个模型使用预训的vision transformer从Segment Anything Model（SAM），并添加轻量级的适应器来提取深度相关（3D）空间上下文，不需要更新预训的重量。另外，这个模型还使用了混合网络，以提高结果的稳定性。</li>
<li>results: 这个论文的实验结果显示，Compared to现有的分类方法，包括对应的启发工程，ProMISe模型在两个公共数据集上的colon和肝脏癌瘤分类任务中表现出色，得到了superior的性能。<details>
<summary>Abstract</summary>
To address prevalent issues in medical imaging, such as data acquisition challenges and label availability, transfer learning from natural to medical image domains serves as a viable strategy to produce reliable segmentation results. However, several existing barriers between domains need to be broken down, including addressing contrast discrepancies, managing anatomical variability, and adapting 2D pretrained models for 3D segmentation tasks. In this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation model using only a single point prompt to leverage knowledge from a pretrained 2D image foundation model. In particular, we use the pretrained vision transformer from the Segment Anything Model (SAM) and integrate lightweight adapters to extract depth-related (3D) spatial context without updating the pretrained weights. For robust results, a hybrid network with complementary encoders is designed, and a boundary-aware loss is proposed to achieve precise boundaries. We evaluate our model on two public datasets for colon and pancreas tumor segmentations, respectively. Compared to the state-of-the-art segmentation methods with and without prompt engineering, our proposed method achieves superior performance. The code is publicly available at https://github.com/MedICL-VU/ProMISe.
</details>
<details>
<summary>摘要</summary>
要解决医学成像中的常见问题，如数据获取困难和标签可用性问题，从自然图像领域传输学习可以作为一个可靠的策略来生成可靠的分割结果。然而，需要破坏一些存在于域之间的抗 correlate，包括对比度差、管理生物学变化和适应2D预训练模型 для 3D分割任务。在本文中，我们提出了ProMISe，一种基于单点提示的3D医学成像分割模型，使用Segment Anything Model（SAM）预训练的视图变换器，并将lightweight adapter integrate到抽取深度相关（3D）空间上下文中，无需更新预训练的 веса。为了实现稳定的结果，我们设计了混合网络，并提出了边界意识损失来实现精确的边界。我们对两个公共数据集进行了colon和肠癌肿分割任务的评估，并与无提示工程和已有的状态艺法分割方法进行了比较。与之比较，我们的提posed方法实现了更高的性能。代码可以在https://github.com/MedICL-VU/ProMISe上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions"><a href="#Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions" class="headerlink" title="Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions"></a>Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19695">http://arxiv.org/abs/2310.19695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saúl Alonso-Monsalve, Davide Sgalaberna, Xingyu Zhao, Adrien Molines, Clark McGrew, André Rubbia</li>
<li>for: 这篇论文的目的是解析高能物理中的核反应域中的中微子互动，特别是在实验室中观察中微子互动的发生点，对于低动量的粒子进行精确的识别和量测，以提高中微子事件的能量解析。</li>
<li>methods: 这篇论文使用深度学习方法来精确地EXTRACT单一物体FROM多维 overlap sparse 图像中，并且与完全可微分的生成模型结合，进一步提高图像解析。</li>
<li>results: 这篇论文获得了前所未有的结果，通过深度学习方法进行图像解析，精确地识别并量测低动量粒子的kinematic 参数，进一步提高中微子事件的能量解析。<details>
<summary>Abstract</summary>
Image decomposition plays a crucial role in various computer vision tasks, enabling the analysis and manipulation of visual content at a fundamental level. Overlapping images, which occur when multiple objects or scenes partially occlude each other, pose unique challenges for decomposition algorithms. The task intensifies when working with sparse images, where the scarcity of meaningful information complicates the precise extraction of components. This paper presents a solution that leverages the power of deep learning to accurately extract individual objects within multi-dimensional overlapping-sparse images, with a direct application in high-energy physics with decomposition of overlaid elementary particles obtained from imaging detectors. In particular, the proposed approach tackles a highly complex yet unsolved problem: identifying and measuring independent particles at the vertex of neutrino interactions, where one expects to observe detector images with multiple indiscernible overlapping charged particles. By decomposing the image of the detector activity at the vertex through deep learning, it is possible to infer the kinematic parameters of the identified low-momentum particles - which otherwise would remain neglected - and enhance the reconstructed energy resolution of the neutrino event. We also present an additional step - that can be tuned directly on detector data - combining the above method with a fully-differentiable generative model to improve the image decomposition further and, consequently, the resolution of the measured parameters, achieving unprecedented results. This improvement is crucial for precisely measuring the parameters that govern neutrino flavour oscillations and searching for asymmetries between matter and antimatter.
</details>
<details>
<summary>摘要</summary>
In particular, the proposed approach tackles a highly complex yet unsolved problem: identifying and measuring independent particles at the vertex of neutrino interactions, where one expects to observe detector images with multiple indiscernible overlapping charged particles. By decomposing the image of the detector activity at the vertex through deep learning, it is possible to infer the kinematic parameters of the identified low-momentum particles - which otherwise would remain neglected - and enhance the reconstructed energy resolution of the neutrino event.Additionally, we present an additional step that combines the above method with a fully-differentiable generative model to improve the image decomposition further and, consequently, the resolution of the measured parameters. This improvement is crucial for precisely measuring the parameters that govern neutrino flavor oscillations and searching for asymmetries between matter and antimatter.The proposed approach can be applied to various fields such as high-energy physics, where the decomposition of overlaid elementary particles obtained from imaging detectors is a crucial task. By accurately identifying and measuring individual particles, the proposed approach can enhance the reconstructed energy resolution of the neutrino event and provide valuable insights into the properties of neutrinos and their role in the universe.
</details></li>
</ul>
<hr>
<h2 id="A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification"><a href="#A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification" class="headerlink" title="A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification"></a>A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19675">http://arxiv.org/abs/2310.19675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Achintha Wijesinghe, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这个论文主要针对低成本感知器应用于远程图像分类问题，涉及到Edge服务器和云端分类器之间的物理通道。</li>
<li>methods: 该论文提出了一种三步共同学习策略，以便在限制通道带宽的情况下，使encoder学习出高精度分类结果。</li>
<li>results: 测试结果显示，该方法可以在CIFAR-10和CIFAR-100上提高分类精度，相比传统朴素迪克逊学习方法。<details>
<summary>Abstract</summary>
Among applications of deep learning (DL) involving low cost sensors, remote image classification involves a physical channel that separates edge sensors and cloud classifiers. Traditional DL models must be divided between an encoder for the sensor and the decoder + classifier at the edge server. An important challenge is to effectively train such distributed models when the connecting channels have limited rate/capacity. Our goal is to optimize DL models such that the encoder latent requires low channel bandwidth while still delivers feature information for high classification accuracy. This work proposes a three-step joint learning strategy to guide encoders to extract features that are compact, discriminative, and amenable to common augmentations/transformations. We optimize latent dimension through an initial screening phase before end-to-end (E2E) training. To obtain an adjustable bit rate via a single pre-deployed encoder, we apply entropy-based quantization and/or manual truncation on the latent representations. Tests show that our proposed method achieves accuracy improvement of up to 1.5% on CIFAR-10 and 3% on CIFAR-100 over conventional E2E cross-entropy training.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）应用中使用低成本传感器时，远程图像分类具有物理通信频道，将边缘计算器和云端分类器分离开来。传统的DL模型需要将编码器与云端分类器分开，这会导致训练分布式模型时，通信频道带宽有限制。我们的目标是优化DL模型，使编码器生成的缓冲器具有低带宽，但仍能提供高精度分类。我们提出了一种三步结合学习策略，以导引编码器提取高度准确、可变性强的特征。在扫描阶段之前，我们对缓冲器维度进行了初步屏选。然后，我们通过权重学习来训练编码器，以便在缓冲器中提取高度准确的特征。最后，我们通过对缓冲器进行 entropy 基于压缩和/或手动跳转来控制缓冲器的带宽。我们的方法在 CIFAR-10 和 CIFAR-100 上测试，测试结果显示，我们的方法可以提高精度为 1.5% 和 3%。
</details></li>
</ul>
<hr>
<h2 id="DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization"><a href="#DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization" class="headerlink" title="DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization"></a>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19668">http://arxiv.org/abs/2310.19668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu<br>for:这篇论文的目的是解决现有的视觉学习控制任务中存在的各种问题，包括样本效率、极限性能和种子选择的不稳定性。methods:这篇论文使用了三种核心机制来引导代理人的冒险决策，包括活动减少率（dormant ratio）来衡量代理人网络中的不活跃程度。results:实验表明，DrM在三个连续控制 benchmark环境中（包括DeepMind Control Suite、MetaWorld和Adroit） achievement significant improvements in sample efficiency和极限性能，而无 broken seeds（76个种子）。此外，DrM成为了第一个没有示范的模型自由算法，在DeepMind Control Suite中解决了狗和手 manipulate tasks，以及Adroit中的三个灵活手 manipulate tasks。<details>
<summary>Abstract</summary>
Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.
</details>
<details>
<summary>摘要</summary>
视觉强化学习（RL）在连续控制任务中表现出了承诺。尽管它们已经取得了进步，但现有的算法仍然在许多方面表现不满足，包括样本效率、极限性能和随机种子的稳定性。在这篇论文中，我们发现了现有的视觉RL方法中的一个重要缺陷：代理人经常在训练的早期展现持续的不活跃，从而限制了它们的探索能力。基于这一重要观察，我们还发现了代理人倾向于不活跃探索的倾向和策略网络中的神经活动的相关性。为量化这种不活跃，我们采用了休眠率作为RL代理人网络中的活动度量表。实验证明，DrM可以有效地减少休眠率，从而提高样本效率和极限性能，无需破坏种子（76个种子）。此外，DrM还是首个不需要示例的模型自由算法，在DeepMind Control Suite中的狗和机器人控制任务中解决了任务，以及Adroit中的三个灵活手 manipulate任务。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines"><a href="#Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines" class="headerlink" title="Domain Generalization in Computational Pathology: Survey and Guidelines"></a>Domain Generalization in Computational Pathology: Survey and Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19656">http://arxiv.org/abs/2310.19656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Jahanifar, Manahil Raza, Kesi Xu, Trinh Vuong, Rob Jewsbury, Adam Shephard, Neda Zamanitajeddin, Jin Tae Kwak, Shan E Ahmed Raza, Fayyaz Minhas, Nasir Rajpoot</li>
<li>for: 本研究旨在探讨域特化（Domain Generalization，DS）的问题在计算 PATHOLOGY（CPath）中，以提高模型的泛化能力和可靠性。</li>
<li>methods: 本文系统性地介绍了各种DS类型，并对CPath领域中exist的多种域特化方法进行了概述和分类。同时， authors 还提供了一系列benchmarking实验来评估这些方法的有效性。</li>
<li>results: 研究发现，在CPath领域中，通过精心设计实验和使用特征增强技术，可以很好地解决域特化问题。然而，无一SIZE-fits-all的解决方案，因此 authors 提出了明确的指南和方法来检测和管理域特化问题。<details>
<summary>Abstract</summary>
Deep learning models have exhibited exceptional effectiveness in Computational Pathology (CPath) by tackling intricate tasks across an array of histology image analysis applications. Nevertheless, the presence of out-of-distribution data (stemming from a multitude of sources such as disparate imaging devices and diverse tissue preparation methods) can cause \emph{domain shift} (DS). DS decreases the generalization of trained models to unseen datasets with slightly different data distributions, prompting the need for innovative \emph{domain generalization} (DG) solutions. Recognizing the potential of DG methods to significantly influence diagnostic and prognostic models in cancer studies and clinical practice, we present this survey along with guidelines on achieving DG in CPath. We rigorously define various DS types, systematically review and categorize existing DG approaches and resources in CPath, and provide insights into their advantages, limitations, and applicability. We also conduct thorough benchmarking experiments with 28 cutting-edge DG algorithms to address a complex DG problem. Our findings suggest that careful experiment design and CPath-specific Stain Augmentation technique can be very effective. However, there is no one-size-fits-all solution for DG in CPath. Therefore, we establish clear guidelines for detecting and managing DS depending on different scenarios. While most of the concepts, guidelines, and recommendations are given for applications in CPath, we believe that they are applicable to most medical image analysis tasks as well.
</details>
<details>
<summary>摘要</summary>
深度学习模型在计算 PATH（CPath）中表现出了惊人的有效性，可以解决各种复杂的 Histology 图像分析应用。然而，由于不同来源的数据（如不同的扫描设备和不同的组织准备方法）而导致的域shift（DS）问题，使得训练过的模型在未看过的数据集上的泛化性受到了影响。为了解决这个问题，我们需要开发创新的域泛化（DG）解决方案。我们认为，DG 方法可以在癌症研究和临床实践中对诊断和预后模型产生重要影响，因此我们在这篇文章中提供了关于 DG 在 CPath 中的报告和指南。我们仔细定义了不同类型的 DS，系统地查询和分类了 CPath 领域中的现有 DG 方法和资源，并提供了这些方法的优缺点和适用范围。我们还进行了28种先进 DG 算法的精心的 benchmarking 实验，我们的发现表明，在 CPath 中使用 Careful 实验设计和特有的染料增强技术可以非常有效。然而，没有一个适用于所有情况的 DG 解决方案。因此，我们建立了明确的指南，以便在不同的场景下探测和处理 DS。大多数概念、指南和建议都适用于医学图像分析任务，因此我们认为这些概念、指南和建议是可靠的。
</details></li>
</ul>
<hr>
<h2 id="Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models"><a href="#Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models" class="headerlink" title="Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models"></a>Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19653">http://arxiv.org/abs/2310.19653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Z. Xiao, Johannes Zenn, Robert Bamler</li>
<li>for: 本研究旨在 Mitigating overfitting in variational autoencoders (VAEs) by training on samples from a pre-trained diffusion model.</li>
<li>methods: 该方法使用 diffusion models 的 encoder 固定，从而简化训练并实现精确地近似 true data distribution $p_{\text{data}(\mathbf{x})$。</li>
<li>results: 研究发现，通过在 VAEs 中训练使用 pre-trained diffusion model 的样本，可以有效地 mitigate overfitting。此外，该方法还可以提高 VAEs 的总体性能、挥uren gap 和 robustness，并且只需要一小量的 diffusion model 的样本即可获得这些优化。<details>
<summary>Abstract</summary>
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our proposed method on three different data sets. We find improvements in all metrics compared to both normal training and conventional data augmentation methods, and we show that a modest amount of samples from the diffusion model suffices to obtain these gains.
</details>
<details>
<summary>摘要</summary>
Variational autoencoders (VAEs) 是一种常用的表示学习模型，但它们的编码器容易过拟合 (Cremer et al., 2018)。这是因为它们在训练集而不是真实数据分布 $p_{\text{data}(\mathbf{x})$ 上训练。Diffusion models 则避免了这个问题，因为它们的编码器是固定的。这使得它们的表示变得更难理解，但是它们的训练变得更加简单，可以提供精准的连续近似 $p_{\text{data}(\mathbf{x})$。在这篇论文中，我们表明了使用预训练的Diffusion模型训练 VAE 的编码器可以有效地 Mitigate overfitting。这些结果有些意外，因为最近的发现 (Alemohammad et al., 2023; Shumailov et al., 2023)  Observation 生成模型训练数据上的模型表现下降。我们分析了 VAE 在不同数据集上的总体表现、积累差和稳定性，并发现它们在所有指标中均有改善。我们还发现，只需要一小量的Diffusion模型样本即可获得这些改善。
</details></li>
</ul>
<hr>
<h2 id="DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking"><a href="#DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking" class="headerlink" title="DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking"></a>DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19641">http://arxiv.org/abs/2310.19641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Ollion, Martin Maliet, Caroline Giuglaris, Elise Vacher, Maxime Deforet</li>
<li>for: 这个论文是为了提高二维细胞分 segmentation和跟踪的精度，特别是在复杂的细胞数据集上。</li>
<li>methods: 该论文提出了一种新的深度神经网络（DNN）架构，名为DistNet2D，它利用了中期和长期的时间上下文来减少错误率。DistNet2D使用了七帧输入，并使用了一种后处理过程来利用整个电影中的信息来修正分 segmentation 错误。</li>
<li>results: DistNet2D在两个实验数据集上表现出优于两种最近的方法，其中一个包含紧密排列的细菌细胞，另一个包含核细胞。此外，DistNet2D还可以在大量数据上对细胞大小和形态与其运输性能进行相关分析，并在细菌和核细胞上都达到了优秀的表现。<details>
<summary>Abstract</summary>
Extracting long tracks and lineages from videomicroscopy requires an extremely low error rate, which is challenging on complex datasets of dense or deforming cells. Leveraging temporal context is key to overcome this challenge. We propose DistNet2D, a new deep neural network (DNN) architecture for 2D cell segmentation and tracking that leverages both mid- and long-term temporal context. DistNet2D considers seven frames at the input and uses a post-processing procedure that exploits information from the entire movie to correct segmentation errors. DistNet2D outperforms two recent methods on two experimental datasets, one containing densely packed bacterial cells and the other containing eukaryotic cells. It has been integrated into an ImageJ-based graphical user interface for 2D data visualization, curation, and training. Finally, we demonstrate the performance of DistNet2D on correlating the size and shape of cells with their transport properties over large statistics, for both bacterial and eukaryotic cells.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译文本到简化中文。<</SYS>>从视频微型ECT需要非常低的错误率，特别是在复杂的 datasets 上，这会导致细胞的压缩或变形。我们提出了 DistNet2D，一种新的深度神经网络（DNN）架构，用于2D细胞分割和跟踪，利用了中期和长期的时间上下文。DistNet2D 使用输入7帧，并使用一种利用整个电影中的信息进行修正 segmentation 错误的后处理过程。DistNet2D 在两个实验数据集上表现出色，一个是 densely packed 细菌细胞，另一个是 eukaryotic 细胞。它已经被 integrate 到 ImageJ 基于的图形用户界面中，用于2D 数据可视化、审核和训练。最后，我们示出了 DistNet2D 在细胞大小和形状与其运输性能之间的相关性，包括细菌和 eukaryotic 细胞。
</details></li>
</ul>
<hr>
<h2 id="Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition"><a href="#Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition" class="headerlink" title="Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition"></a>Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19636">http://arxiv.org/abs/2310.19636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge">https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge</a></li>
<li>paper_authors: Yuhang Zhang, Yaqi Li, Lixiong Qin, Xuannan Liu, Weihong Deng</li>
<li>for: 本研究旨在解决面部表达识别（FER）问题中的数据不均衡问题，即大多数收集到的数据显示了快乐或中性表情，而恐惧或厌恶表情则少得多。</li>
<li>methods: 本研究提出了一种新的方法，即利用重新平衡的注意力地图来补做模型，使其从所有训练样本中提取转换不变的信息，以提高对少数类的识别能力。此外，我们还引入了重新平衡的平滑标签，以规则模型的权重，使其更加注意少数类。</li>
<li>results: 实验表明，两个模块共同工作，可以补做模型，并在不均衡FER任务中实现了状态畅的性能。<details>
<summary>Abstract</summary>
Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.
</details>
<details>
<summary>摘要</summary>
Facial expression data exhibits a significant imbalance, with most data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our goal is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by this, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models"><a href="#Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models" class="headerlink" title="Bidirectional Captioning for Clinically Accurate and Interpretable Models"></a>Bidirectional Captioning for Clinically Accurate and Interpretable Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19635">http://arxiv.org/abs/2310.19635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keegan Quigley, Miriam Cha, Josh Barua, Geeticka Chauhan, Seth Berkowitz, Steven Horng, Polina Golland</li>
<li>for: 这个论文主要是为了探讨视语预训练在医学影像分析中的应用和优劣点。</li>
<li>methods: 这篇论文使用了对医学影像报告的 bidirectional captioning 作为预训练方法，并对RadTex 架构进行优化。</li>
<li>results: 研究发现，不仅 captioning 预训练可以提供与对比学习方法相当的视觉编码器，而且 transformer 解码器可以生成丰富的医学信息和相关的交互输出。<details>
<summary>Abstract</summary>
Vision-language pretraining has been shown to produce high-quality visual encoders which transfer efficiently to downstream computer vision tasks. While generative language models have gained widespread attention, image captioning has thus far been mostly overlooked as a form of cross-modal pretraining in favor of contrastive learning, especially in medical image analysis. In this paper, we experiment with bidirectional captioning of radiology reports as a form of pretraining and compare the quality and utility of learned embeddings with those from contrastive pretraining methods. We optimize a CNN encoder, transformer decoder architecture named RadTex for the radiology domain. Results show that not only does captioning pretraining yield visual encoders that are competitive with contrastive pretraining (CheXpert competition multi-label AUC of 89.4%), but also that our transformer decoder is capable of generating clinically relevant reports (captioning macro-F1 score of 0.349 using CheXpert labeler) and responding to prompts with targeted, interactive outputs.
</details>
<details>
<summary>摘要</summary>
视觉语言预训程已经证明可以生成高质量的视觉编码器，这些编码器可以有效地传输到下游计算机视觉任务中。而生成语言模型已经受到广泛关注，但是医学影像分析中的图像描述还未得到了较多的关注。在这篇论文中，我们通过对医学报告的双向描述进行预训程，并与对比学习方法进行比较。我们优化了医学领域的CNNEncoder和TransformerDecoder architecture，并命名为RadTex。结果表明，不仅captioning预训程可以生成与对比学习方法相当的视觉编码器（CheXpert竞赛多标签AUC为89.4%），而且我们的TransformerDecoder还能生成有价值的医学报告（描述macro-F1分数为0.349，使用CheXpert标签器），并能够在提示中生成目标化、互动的输出。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles"><a href="#Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles" class="headerlink" title="Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles"></a>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19630">http://arxiv.org/abs/2310.19630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Rukundo, Andrea Behanova, Riccardo De Feo, Seppo Ronkko, Joni Oja, Jussi Tohka</li>
<li>for: The paper is written for manufacturers of virus-based gene therapy vector products and intermediates to help them monitor the primary particles and purity profiles of their products during development and manufacturing processes.</li>
<li>methods: The paper uses transmission electron microscopy (TEM) imaging to detect and characterize intact adenoviruses in the presence of debris, broken adenoviruses, and artefact particles. The authors developed two software tools for semi-automatic annotation and segmentation of adenoviruses, and automatic segmentation and detection of intact adenoviruses.</li>
<li>results: The authors evaluated the performance of their software tools using quantitative and qualitative methods and found outstanding true positive detection rates compared to false positive and negative rates. The tools were able to detect intact adenoviruses without mistaking them for real debris, broken adenoviruses, and&#x2F;or staining artefacts.<details>
<summary>Abstract</summary>
Regular monitoring of the primary particles and purity profiles of a drug product during development and manufacturing processes is essential for manufacturers to avoid product variability and contamination. Transmission electron microscopy (TEM) imaging helps manufacturers predict how changes affect particle characteristics and purity for virus-based gene therapy vector products and intermediates. Since intact particles can characterize efficacious products, it is beneficial to automate the detection of intact adenovirus against a non-intact-viral background mixed with debris, broken, and artefact particles. In the presence of such particles, detecting intact adenoviruses becomes more challenging. To overcome the challenge, due to such a presence, we developed a software tool for semi-automatic annotation and segmentation of adenoviruses and a software tool for automatic segmentation and detection of intact adenoviruses in TEM imaging systems. The developed semi-automatic tool exploited conventional image analysis techniques while the automatic tool was built based on convolutional neural networks and image analysis techniques. Our quantitative and qualitative evaluations showed outstanding true positive detection rates compared to false positive and negative rates where adenoviruses were nicely detected without mistaking them for real debris, broken adenoviruses, and/or staining artefacts.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Regular monitoring" is translated as "常规监测" (chángyè jiānnéng), which means regular or constant monitoring.* "Primary particles" is translated as "基本粒子" (jīběn zhízi), which refers to the basic particles that make up a drug product.* "Purity profiles" is translated as "纯度轨迹" (zhēngróng xiàoxi), which refers to the purity of the drug product over time.* "Transmission electron microscopy" is translated as "电子显微镜" (diàn xiǎnwēi jìng), which is a technique used to image samples at the nanoscale.* "Adenoviruses" is translated as "adenovirus" (adenovirus), which is a type of virus that is commonly used as a vector for gene therapy.* "Intact particles" is translated as "完整粒子" (quánzhì zhízi), which refers to particles that are not broken or damaged.* "Non-intact-viral background" is translated as "非完整病毒背景" (fēi quánzhì bìngdài xiàoxi), which refers to the presence of non-intact viral particles in the background of the image.* "Debris" is translated as "垃圾" (shīwù), which refers to any debris or contaminants present in the image.* "Broken adenoviruses" is translated as "损坏adenovirus" (shènghuà adenovirus), which refers to adenoviruses that have been damaged or broken.* "Staining artifacts" is translated as "染色artefacts" (rǎn sé artefacts), which refers to any artifacts or staining present in the image that are not related to the adenoviruses.* "Semi-automatic annotation and segmentation" is translated as "半自动注释和分割" (bàn zìdòng běndào xiàngxiàng yǔ fēnpiè), which refers to the process of annotating and segmenting the image manually, but with the assistance of software.* "Automatic segmentation and detection" is translated as "自动分割和检测" (zìdòng fēnpiè yǔ jiànnéng), which refers to the process of automatically segmenting and detecting the adenoviruses in the image.* "Convolutional neural networks" is translated as "卷积神经网络" (juéshì nervous networks), which is a type of neural network that is commonly used for image processing tasks.
</details></li>
</ul>
<hr>
<h2 id="GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo"><a href="#GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo" class="headerlink" title="GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo"></a>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19583">http://arxiv.org/abs/2310.19583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vkvats/GC-MVSNet">https://github.com/vkvats/GC-MVSNet</a></li>
<li>paper_authors: Vibhas K. Vats, Sripad Joshi, David J. Crandall, Md. Alimoor Reza, Soon-heung Jung</li>
<li>for: 本研究は多视角ステレオ（MVS）方法の改善を目的としています。</li>
<li>methods: 本研究では、多个源视角の参考ビュー深度地図の几何学的一致性を学习の中で直接推し量る新しいアプローチを提案します。</li>
<li>results: 我们の実験结果によると、我们のアプローチはDTUおよびBlendedMVSデータセットで新たな状态のアートであり、Tanks and Templesベンチマークでも竞争的な结果を得ることができます。<details>
<summary>Abstract</summary>
Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning.
</details>
<details>
<summary>摘要</summary>
传统的多视图顺序（MVS）方法强调光学和几何一致性约束，而 newer的机器学习基于MVS方法只在多个源视图之间的几何一致性检查为后处理步骤。在这篇论文中，我们提出了一种新的方法，该方法在多个源视图中规范化参考视图深度地图的几何一致性，并在学习过程中直接强制执行这种几何一致性约束。我们发现，添加这种几何一致性损失可以明显加速学习，因为它直接处罚不一致的像素，从而减少了学习迭代次数，相比其他MVS方法的两倍。我们的广泛实验表明，我们的方法在DTU和BlendedMVS数据集上达到了新的状态态枢纽，并在Tanks和Temples benchmark上获得了竞争性的结果。我们认为，GC-MVSNet是首次在多视图、多级几何一致性检查中进行学习的尝试。
</details></li>
</ul>
<hr>
<h2 id="Human-interpretable-and-deep-features-for-image-privacy-classification"><a href="#Human-interpretable-and-deep-features-for-image-privacy-classification" class="headerlink" title="Human-interpretable and deep features for image privacy classification"></a>Human-interpretable and deep features for image privacy classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19582">http://arxiv.org/abs/2310.19582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darya Baranouskaya, Andrea Cavallaro</li>
<li>for: 本研究旨在探讨隐私分类 datasets 和 controversial images 的特性，以及不同评估者对这些图像的隐私标签的差异。</li>
<li>methods: 本研究提出了 eight 隐私特定和人类可读的特征，以提高深度学习模型的性能和图像隐私分类表示。</li>
<li>results: 研究发现，这八种特征可以增加深度学习模型的表达能力，同时也可以提高图像隐私分类的精度。<details>
<summary>Abstract</summary>
Privacy is a complex, subjective and contextual concept that is difficult to define. Therefore, the annotation of images to train privacy classifiers is a challenging task. In this paper, we analyse privacy classification datasets and the properties of controversial images that are annotated with contrasting privacy labels by different assessors. We discuss suitable features for image privacy classification and propose eight privacy-specific and human-interpretable features. These features increase the performance of deep learning models and, on their own, improve the image representation for privacy classification compared with much higher dimensional deep features.
</details>
<details>
<summary>摘要</summary>
文本翻译到简化中文：隐私是一个复杂、主观和Contextual的概念，难以定义。因此，对隐私分类器的图像注释是一项具有挑战性的任务。在这篇论文中，我们分析了隐私分类数据集和不同评估员对涉及隐私的图像的分类标签的冲突。我们讨论了适合图像隐私分类的特征，并提出了八种适合人类理解的隐私特有的特征。这些特征可以提高深度学习模型的性能，同时也可以提高图像隐私分类的表示力。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The Traditional Chinese writing system is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model"><a href="#Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model" class="headerlink" title="Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model"></a>Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19581">http://arxiv.org/abs/2310.19581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyeon Lee, Chaeyoung Jung, Youngjoon Jang, Jaehun Kim, Joon Son Chung</li>
<li>for: 提高对话者的声音分离精度，使其能够更好地保持自然性。</li>
<li>methods: 基于扩散机制，提出了一种 audio-visual 语音分离模型，并提出了一种基于巧合注意力的特征融合机制，以便有效地融合两种模式。</li>
<li>results: 在 VoxCeleb2 和 LRS3 两个测试集上，提出的方法实现了状态之最的结果，并且生成的语音具有更好的自然性。<details>
<summary>Abstract</summary>
The objective of this work is to extract target speaker's voice from a mixture of voices using visual cues. Existing works on audio-visual speech separation have demonstrated their performance with promising intelligibility, but maintaining naturalness remains a challenge. To address this issue, we propose AVDiffuSS, an audio-visual speech separation model based on a diffusion mechanism known for its capability in generating natural samples. For an effective fusion of the two modalities for diffusion, we also propose a cross-attention-based feature fusion mechanism. This mechanism is specifically tailored for the speech domain to integrate the phonetic information from audio-visual correspondence in speech generation. In this way, the fusion process maintains the high temporal resolution of the features, without excessive computational requirements. We demonstrate that the proposed framework achieves state-of-the-art results on two benchmarks, including VoxCeleb2 and LRS3, producing speech with notably better naturalness.
</details>
<details>
<summary>摘要</summary>
“我们的目标是从多个声音混合中提取目标说话者的声音，使用视觉上的指示。现有的对话音频分类方法已经展示了有前途的可能性，但维持自然性仍然是一大挑战。为解决这个问题，我们提出了AVDiffuSS，一个基于扩散机制的对话音频分类模型。为实现两 modalities 的有效融合，我们也提出了一个基于对话的跨注意力特征融合机制。这个机制特别针对说话领域，以整合对话中的音频特征和视觉特征，以维持高时间分辨率的特征融合，无过度的计算需求。我们显示了我们的框架可以在VoxCeleb2和LRS3两个标准资料集上实现州立顶对话说话的成果，并且说话质量明显更高。”
</details></li>
</ul>
<hr>
<h2 id="A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction"><a href="#A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction" class="headerlink" title="A Perceptual Shape Loss for Monocular 3D Face Reconstruction"></a>A Perceptual Shape Loss for Monocular 3D Face Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19580">http://arxiv.org/abs/2310.19580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Otto, Prashanth Chandran, Gaspard Zoss, Markus Gross, Paulo Gotardo, Derek Bradley</li>
<li>for: 提出了一种新的损失函数，用于评估照明影响的3D人脸重建质量。</li>
<li>methods: 利用人类视觉系统中对3D形状的识别指标——光照效果，设计了一种新的’人体感知’形状损失函数，通过 critics网络来评估给定图像和geometry estimate的匹配程度。</li>
<li>results: 结果表明，将这种新的损失函数与传统的能量函数相结合，可以提高现有状态的最佳结果。<details>
<summary>Abstract</summary>
Monocular 3D face reconstruction is a wide-spread topic, and existing approaches tackle the problem either through fast neural network inference or offline iterative reconstruction of face geometry. In either case carefully-designed energy functions are minimized, commonly including loss terms like a photometric loss, a landmark reprojection loss, and others. In this work we propose a new loss function for monocular face capture, inspired by how humans would perceive the quality of a 3D face reconstruction given a particular image. It is widely known that shading provides a strong indicator for 3D shape in the human visual system. As such, our new 'perceptual' shape loss aims to judge the quality of a 3D face estimate using only shading cues. Our loss is implemented as a discriminator-style neural network that takes an input face image and a shaded render of the geometry estimate, and then predicts a score that perceptually evaluates how well the shaded render matches the given image. This 'critic' network operates on the RGB image and geometry render alone, without requiring an estimate of the albedo or illumination in the scene. Furthermore, our loss operates entirely in image space and is thus agnostic to mesh topology. We show how our new perceptual shape loss can be combined with traditional energy terms for monocular 3D face optimization and deep neural network regression, improving upon current state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
单眼3D脸重建是一个广泛的研究主题，现有的方法可以通过快速的神经网络推断或者离线迭代重建脸部几何。在任一情况下，当面对着精心设计的能量函数时，通常包括图像损失、点击复写损失和其他损失函数。在这个工作中，我们提出了一个新的损失函数 для单眼3D脸捕捉，灵感来自于人类对3D形状的视觉评价。我们发现，阴影提供了3D形状中强大的视觉指标。因此，我们的新的“感知”形状损失将评估3D脸估计是否具有优秀的视觉质感，使用对照绘制的阴影。我们的损失函数是一个批评网络，它将对一个脸形状估计和一个阴影绘制进行评估，并且预测这两个元素之间的视觉匹配程度。这个批评网络仅仅使用RGB图像和几何测量，没有需要场景照明估计。此外，我们的损失函数具有对 mesh 顶点数据的无知性，即使是在不同的 mesh 构造下。我们显示了我们的新的感知形状损失可以与传统的能量函数和神经网络回推进行结合，以提高目前的州OF-THE-ART结果。
</details></li>
</ul>
<hr>
<h2 id="Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms"><a href="#Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms" class="headerlink" title="Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms"></a>Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19574">http://arxiv.org/abs/2310.19574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debvrat Varshney, Masoud Yari, Oluwanisola Ibikunle, Jilu Li, John Paden, Maryam Rahnemoonfar</li>
<li>for: 这项研究的目的是提高空气雷达探测器上的冰层层次检测精度，以便计算冰川沉积量和海平面升高的贡献。</li>
<li>methods: 本研究使用了wavelet基于的多尺度深度学习架构来处理空气雷达探测器上的冰层信号，以提高冰层检测的精度。</li>
<li>results: 我们的提议的Skip-WaveNet架构可以在不同的数据集上实现高度的泛化能力，并且可以准确地估计冰层的深度和层次结构。这种网络可以帮助科学家跟踪冰层，计算年度的雪堆积量，估计冰川的表面质量减少，并帮助预测全球海平面升高。<details>
<summary>Abstract</summary>
Echograms created from airborne radar sensors capture the profile of firn layers present on top of an ice sheet. Accurate tracking of these layers is essential to calculate the snow accumulation rates, which are required to investigate the contribution of polar ice cap melt to sea level rise. However, automatically processing the radar echograms to detect the underlying firn layers is a challenging problem. In our work, we develop wavelet-based multi-scale deep learning architectures for these radar echograms to improve firn layer detection. We show that wavelet based architectures improve the optimal dataset scale (ODS) and optimal image scale (OIS) F-scores by 3.99% and 3.7%, respectively, over the non-wavelet architecture. Further, our proposed Skip-WaveNet architecture generates new wavelets in each iteration, achieves higher generalizability as compared to state-of-the-art firn layer detection networks, and estimates layer depths with a mean absolute error of 3.31 pixels and 94.3% average precision. Such a network can be used by scientists to trace firn layers, calculate the annual snow accumulation rates, estimate the resulting surface mass balance of the ice sheet, and help project global sea level rise.
</details>
<details>
<summary>摘要</summary>
雷达探测机器上的echogram可以捕捉到冰层的Profile，以便计算雪聚积率，这些积积率是研究北极冰川融化对海平面升高的重要因素。然而，自动从雷达echogram中检测下面的冰层是一项复杂的问题。在我们的工作中，我们开发了基于波лет的多尺度深度学习架构，以提高firn层的检测精度。我们发现，基于波лет的架构可以提高优化数据集大小（ODS）和优化图像大小（OIS）的F1分数 by 3.99%和3.7%，respectively，相比非波лет架构。此外，我们提出的Skip-WaveNet架构在每次迭代中生成新的波лет，实现了更高的普适性，并且测算层的深度平均绝对误差为3.31像素和94.3%的平均准确率。这种网络可以被科学家用来跟踪firn层，计算年度雪聚积率，估算冰川的表面质量平衡，并帮助计算全球海平面升高。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning"><a href="#Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning" class="headerlink" title="Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning"></a>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19559">http://arxiv.org/abs/2310.19559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL">https://github.com/Andy20178/DCL</a></li>
<li>paper_authors: Changsheng Lv, Shuai Zhang, Yapeng Tian, Mengshi Qi, Huadong Ma</li>
<li>for: 本研究提出了一种基于counterfactual学习的分离 Commonsense Physical Reasoning~(DCL)方法，用于物理 audiovisual 常识逻辑推理。</li>
<li>methods: 我们的提posed方法使用分离sequential encoder来解决视频数据中不同特征的问题，并通过对比损失函数来增强模型之间的相互信息。此外，我们还引入了对抗学习模块，以增强模型的逻辑能力。</li>
<li>results: 我们的实验结果表明， compared to基eline方法，我们的提posed方法可以提高模型的性能，并达到领先的状态。我们的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL">https://github.com/Andy20178/DCL</a> 中下载。<details>
<summary>Abstract</summary>
In this paper, we propose a Disentangled Counterfactual Learning~(DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. Our source code is available at https://github.com/Andy20178/DCL.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种分离性counterfactual学习（DCL）方法，用于物理audiovisual常识逻辑。任务的目标是根据视频和音频输入推理出物体的物理常识，主要挑战是如何模仿人类的思维能力。现有方法多数不能充分利用多modal数据的不同特征，同时模型中缺乏 causal 逻辑能力，这些问题限制了隐藏物理知识的推理进展。为解决这些问题，我们提出的DCL方法在幂值空间中分离视频为静态（时间不变）和动态（时间变化）因素，使用分离sequential编码器，该编码器采用了variational autoencoder（VAE）来最大化与对比损失函数的mutual information。此外，我们引入了对因果学习模块，以增强模型的逻辑能力，通过模型物理知识之间的相互关系的模拟，在对因果干预下进行推理。我们提出的方法可以与任何基础模型集成，并在实验中超越基础方法，达到了状态的最佳性能。我们的源代码可以在https://github.com/Andy20178/DCL中获取。
</details></li>
</ul>
<hr>
<h2 id="Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining"><a href="#Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining" class="headerlink" title="Harvest Video Foundation Models via Efficient Post-Pretraining"></a>Harvest Video Foundation Models via Efficient Post-Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19554">http://arxiv.org/abs/2310.19554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/internvideo">https://github.com/opengvlab/internvideo</a></li>
<li>paper_authors: Yizhuo Li, Kunchang Li, Yinan He, Yi Wang, Yali Wang, Limin Wang, Yu Qiao, Ping Luo</li>
<li>For: 提出了一种高效的基于图像的视频基础模型抽象方法，以便更好地利用大量的图像数据来快速建立高质量的视频基础模型。* Methods: 提出了一种INTUITIVE和简单的方法，包括随机drop输入视频patches和mask输入文本，以促进跨模态融合学习。* Results: 在多种视频语言下沉淀任务上展现出了优秀的表现，与一些努力预训练的视频基础模型的表现相当。该方法可以在8个GPU上训练completed within one day，只需要WebVid-10M作为预训练数据。<details>
<summary>Abstract</summary>
Building video-language foundation models is costly and difficult due to the redundant nature of video data and the lack of high-quality video-language datasets. In this paper, we propose an efficient framework to harvest video foundation models from image ones. Our method is intuitively simple by randomly dropping input video patches and masking out input text during the post-pretraining procedure. The patch dropping boosts the training efficiency significantly and text masking enforces the learning of cross-modal fusion. We conduct extensive experiments to validate the effectiveness of our method on a wide range of video-language downstream tasks including various zero-shot tasks, video question answering, and video-text retrieval. Despite its simplicity, our method achieves state-of-the-art performances, which are comparable to some heavily pretrained video foundation models. Our method is extremely efficient and can be trained in less than one day on 8 GPUs, requiring only WebVid-10M as pretraining data. We hope our method can serve as a simple yet strong counterpart for prevalent video foundation models, provide useful insights when building them, and make large pretrained models more accessible and sustainable. This is part of the InternVideo project \url{https://github.com/OpenGVLab/InternVideo}.
</details>
<details>
<summary>摘要</summary>
（建立视频语言基础模型是成本和困难的，因为视频数据具有重复性和缺乏高质量视频语言数据。在这篇论文中，我们提出一种高效的框架，可以从图像基础模型中抽取视频基础模型。我们的方法是通过随机删除输入视频补丁和隐藏输入文本来进行后期预训练。补丁删除提高了训练效率，而文本隐藏强制学习 crossing 模式融合。我们进行了广泛的实验，以验证我们的方法在多种视频语言下沉淀任务中的效果，包括多种零 shot 任务、视频问答和视频文本检索。尽管我们的方法简单，但它可以达到状态 искусственного智能性的表现，与一些努力预训练的视频基础模型相当。我们的方法非常高效，可以在8个GPU上训练完成 less than one day，只需要 WebVid-10M 作为预训练数据。我们希望我们的方法可以作为一种简单 yet strong 对等，为建立视频基础模型提供有用的指导，使大型预训练模型更加可持续和可 accessible。这是InternVideo项目的一部分，请参考 \url{https://github.com/OpenGVLab/InternVideo}.）
</details></li>
</ul>
<hr>
<h2 id="MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection"><a href="#MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection" class="headerlink" title="MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection"></a>MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19545">http://arxiv.org/abs/2310.19545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colton R. Crum, Adam Czajka</li>
<li>for: 提高困难任务中 CNN 的表现，如生物认证表现攻击检测。</li>
<li>methods: 利用人类注意力图生成 CNN 的训练数据，并通过两轮训练来快速地集成人类注意力到模型中。</li>
<li>results: 提高生物认证表现攻击检测性能，生成无穷数量的人类类似注意力图，提高模型训练效率。<details>
<summary>Abstract</summary>
Incorporating human salience into the training of CNNs has boosted performance in difficult tasks such as biometric presentation attack detection. However, collecting human annotations is a laborious task, not to mention the questions of how and where (in the model architecture) to efficiently incorporate this information into model's training once annotations are obtained. In this paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr iris pResentation attack detection), which addresses both of these issues through two unique rounds of training. First, we train an autoencoder to learn human saliency maps given an input iris image (both real and fake examples). Once this representation is learned, we utilize the trained autoencoder in two different ways: (a) as a pre-trained backbone for an iris presentation attack detector, and (b) as a human-inspired annotator of salient features on unknown data. We show that MENTOR's benefits are threefold: (a) significant boost in iris PAD performance when using the human perception-trained encoder's weights compared to general-purpose weights (e.g. ImageNet-sourced, or random), (b) capability of generating infinite number of human-like saliency maps for unseen iris PAD samples to be used in any human saliency-guided training paradigm, and (c) increase in efficiency of iris PAD model training. Sources codes and weights are offered along with the paper.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用人类焦点进行 convolutional neural network（CNN）训练，可以提高难度任务，如生物ometric presentation attack detection（PAD）的性能。然而，收集人类注解是一项劳动ioso的任务，而且模型建立的问题，即如何有效地将这些信息 integrate into 模型的训练中。本文介绍了MENTOR（huMan pErceptioN-guided preTraining fOr iris pResentation attack detection），它解决了这两个问题。我们首先在输入图像（真实和假示例）上使用自动encoder学习人类焦点地图。然后，我们利用这已经学习的地图在两种不同的方式：（a）作为预训练的backbone for iris PAD detector，和（b）作为人类静观注解的人类化特征标注工具。我们表明了MENTOR的优势是：（a）使用人类识别训练的encoder的 weights比如ImageNet或随机的 weights，可以获得显著提高iris PAD性能；（b）可以生成无数量的人类化焦点地图，用于未经见过的iris PAD样本训练；（c）提高iris PAD模型训练效率。我们提供了代码和 weights。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking"><a href="#Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking" class="headerlink" title="Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking"></a>Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19542">http://arxiv.org/abs/2310.19542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/AViTMP">https://github.com/Tchuanm/AViTMP</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer, Jianlin Zhang, Yongmei Huang</li>
<li>for: 提高单支持神经网络的视觉跟踪性能，尤其是在长时间跟踪和鲁棒性方面。</li>
<li>methods: 提出了一种适应 ViT 模型预测跟踪器（AViTMP），通过在 ViT 编码器中引入适应模块和联合目标状态嵌入来强化密度插值方式。然后，将 AViT-Enc 与密集混合解码器和抑制性目标模型结合，以便准确预测位置。最后，提出了一种新的推理管线called CycleTrack，可以在干扰物存在下提高跟踪 robustness。</li>
<li>results: 在十个跟踪 benchmark 上进行了全面评估，并获得了state-of-the-art表现，特别是在长时间跟踪和鲁棒性方面。<details>
<summary>Abstract</summary>
Despite achieving state-of-the-art performance in visual tracking, recent single-branch trackers tend to overlook the weak prior assumptions associated with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the effectiveness of discriminative trackers remains constrained due to the adoption of the dual-branch pipeline. To tackle the inferior effectiveness of the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP) to bridge the gap between single-branch network and discriminative models. Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module and joint target state embedding to enrich the dense embedding paradigm based on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a discriminative target model to predict accurate location. Further, to mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. Lastly, we propose a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. In the experiments, we evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that AViTMP attains state-of-the-art performance, especially on long-time tracking and robustness.
</details>
<details>
<summary>摘要</summary>
尽管最新的单支追踪器在视觉追踪性能方面培达了国际级水平，但是这些单支追踪器往往忽视了视觉转换器（ViT）encoder和推理管道中的弱优先级假设。此外，使用双支管道管道的追踪器效果受到了限制。为了解决普通的ViT追踪器的不足，我们提出了适应性ViT模型预测追踪器（AViTMP），以填补单支网络和推理模型之间的差距。具体来说，在我们提出的AViT-Encoder中，我们引入了适应模块和联合目标状态嵌入，以把拥有 dense embedding 的ViT Encoder更加强化。然后，我们将AViT-Encoder与 dense-fusion 解码器和一个推理模型组合起来，以准确预测位置。此外，为了缓解传统的推理实践中的局限性，我们提出了一种新的推理管道 called CycleTrack，它通过双向循环跟踪验证来增强追踪的Robustness。最后，我们提出了一种双帧更新推理策略，以适应长期enario中的挑战。在实验中，我们对十种追踪标准 benchmark 进行了广泛的评估，包括LaSOT、LaSOTExtSub、AVisT等。实验结果明确表明，AViTMP在长期追踪和Robustness方面具有国际级水平的表现。
</details></li>
</ul>
<hr>
<h2 id="IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models"><a href="#IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models" class="headerlink" title="IterInv: Iterative Inversion for Pixel-Level T2I Models"></a>IterInv: Iterative Inversion for Pixel-Level T2I Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19540">http://arxiv.org/abs/2310.19540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/IterInv">https://github.com/Tchuanm/IterInv</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer</li>
<li>for: 这个研究的目的是提供一个可控的文本至图像传播模型，让用户可以通过修改文本提示来控制生成的图像。</li>
<li>methods: 这个研究使用了内存散射模型（LDM）和深度对映（DeepFloyd-IF）等技术，并提出了一个迭代复位（IterInv）技术来解决对于DDIM倒数的问题。</li>
<li>results: 根据实验结果，IterInv技术可以将生成的图像修改为原始图像，并且与具有实际应用前景的图像修改方法结合，证明了IterInv的应用前景。<details>
<summary>Abstract</summary>
Large-scale text-to-image diffusion models have been a ground-breaking development in generating convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are relying on DDIM inversion as a common practice based on the Latent Diffusion Models (LDM). However, the large pretrained T2I models working on the latent space as LDM suffer from losing details due to the first compression stage with an autoencoder mechanism. Instead, another mainstream T2I pipeline working on the pixel level, such as Imagen and DeepFloyd-IF, avoids this problem. They are commonly composed of several stages, normally with a text-to-image stage followed by several super-resolution stages. In this case, the DDIM inversion is unable to find the initial noise to generate the original image given that the super-resolution diffusion models are not compatible with the DDIM technique. According to our experimental findings, iteratively concatenating the noisy image as the condition is the root of this problem. Based on this observation, we develop an iterative inversion (IterInv) technique for this stream of T2I models and verify IterInv with the open-source DeepFloyd-IF model. By combining our method IterInv with a popular image editing method, we prove the application prospects of IterInv. The code will be released at \url{https://github.com/Tchuanm/IterInv.git}.
</details>
<details>
<summary>摘要</summary>
大规模文本到图像扩散模型已经成为生成真实图像的重要发展之一，目标是通过修改文本提示来给用户控制生成图像。当前的图像编辑技术仍然是基于Diffusion-based Image Synthesis（DDIM）的倒推。然而，大规模预训练的T2I模型在干扰空间中作为LDM时会产生loss of details问题，这是因为第一个压缩阶段使用自适应机制。相比之下，另一个主流的T2I管道，如Imagen和DeepFloyd-IF，通常由文本到图像阶段和多个超分辨阶段组成，这些阶段可以减少loss of details问题。然而，使用DDIM倒推时，无法找到初始噪声，因为超分辨扩散模型与DDIM技术不兼容。根据我们的实验结果，iteratively concatenating the noisy image as the condition是这个问题的根本原因。基于这一观察，我们开发了一种iterative inversion（IterInv）技术，并在open-source DeepFloyd-IF模型上验证了IterInv。通过将IterInv与一种流行的图像编辑方法结合使用，我们证明了IterInv的应用前景。代码将在 \url{https://github.com/Tchuanm/IterInv.git} 上发布。
</details></li>
</ul>
<hr>
<h2 id="Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation"><a href="#Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation" class="headerlink" title="Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation"></a>Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19535">http://arxiv.org/abs/2310.19535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaowei Gao, Mingyang Song, Christopher Schroers, Yang Zhang</li>
<li>for: 这篇论文是为了提出一种基于深度学习的视频去扫描方法，以提高传统视频内容的显示质量。</li>
<li>methods: 该方法使用了深度学习技术，包括bidirectional空间时间信息传递、多尺度特征精度提升等，以提高去扫描效果。</li>
<li>results: 实验结果显示，该方法的性能比现有方法更高，可能在实时处理中得到应用。<details>
<summary>Abstract</summary>
Due to old CRT display technology and limited transmission bandwidth, early film and TV broadcasts commonly used interlaced scanning. This meant each field contained only half of the information. Since modern displays require full frames, this has spurred research into deinterlacing, i.e. restoring the missing information in legacy video content. In this paper, we present a deep-learning-based method for deinterlacing animated and live-action content. Our proposed method supports bidirectional spatio-temporal information propagation across multiple scales to leverage information in both space and time. More specifically, we design a Flow-guided Refinement Block (FRB) which performs feature refinement including alignment, fusion, and rectification. Additionally, our method can process multiple fields simultaneously, reducing per-frame processing time, and potentially enabling real-time processing. Our experimental results demonstrate that our proposed method achieves superior performance compared to existing methods.
</details>
<details>
<summary>摘要</summary>
More specifically, we design a Flow-guided Refinement Block (FRB) which performs feature refinement, including alignment, fusion, and rectification. Additionally, our method can process multiple fields simultaneously, reducing per-frame processing time and potentially enabling real-time processing. Our experimental results demonstrate that our proposed method achieves superior performance compared to existing methods.
</details></li>
</ul>
<hr>
<h2 id="Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification"><a href="#Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification" class="headerlink" title="Are Natural Domain Foundation Models Useful for Medical Image Classification?"></a>Are Natural Domain Foundation Models Useful for Medical Image Classification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19522">http://arxiv.org/abs/2310.19522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joana Palés Huix, Adithya Raju Ganeshan, Johan Fredin Haslum, Magnus Söderberg, Christos Matsoukas, Kevin Smith</li>
<li>for: 本研究旨在 investigate the transferability of various state-of-the-art foundation models to medical image classification tasks.</li>
<li>methods: 本研究使用了 five foundation models，namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP，并在 four well-established medical imaging datasets 进行了评估。不同的训练设置也被探讨，以便充分发挥这些模型的潜力。</li>
<li>results: DINOv2 在特定的训练设置下表现出色，并一直超过了标准的 ImageNet 预训练方法。然而，其他基础模型却无法一致地超过这个已知基准， indicating limitations in their transferability to medical image classification tasks.<details>
<summary>Abstract</summary>
The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 in particular, consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.
</details>
<details>
<summary>摘要</summary>
深度学习领域正在向通用基础模型的使用倾斜，以便轻松地适应多种任务。在自然语言处理领域，这种思维方式已经成为常见的实践，但在计算机视觉领域进步 slower。在这篇论文中，我们试图解决这个问题，通过调查不同状态的基础模型在医疗影像分类任务中的可转移性。我们评估了五种基础模型，namely SAM、SEEM、DINOv2、BLIP和OpenCLIP，在四个成熟的医疗影像数据集上的表现。我们探索了不同的训练设置，以充分利用这些模型的潜力。我们的研究显示了混合的结果。DINOv2在特别情况下一直表现出色，而其他基础模型则在医疗影像分类任务中的可转移性有限。
</details></li>
</ul>
<hr>
<h2 id="Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes"><a href="#Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes" class="headerlink" title="Generating Context-Aware Natural Answers for Questions in 3D Scenes"></a>Generating Context-Aware Natural Answers for Questions in 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19516">http://arxiv.org/abs/2310.19516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Munzer Dwedari, Matthias Niessner, Dave Zhenyu Chen</li>
<li>for: 这 paper 是为了解决 3D 视力语言领域中的问题回答问题。</li>
<li>methods: 该 paper 使用了将问题回答任务转化为序列生成任务，以生成自然的回答。具体来说，它们使用了语言奖励来优化模型，并采用了 Pragmatic 语言理解奖来进一步提高句子质量。</li>
<li>results: 该 paper 在 ScanQA 测试集上 achieved 新的 SOTA 成绩（CIDEr 分数为 72.22&#x2F;66.57）。<details>
<summary>Abstract</summary>
3D question answering is a young field in 3D vision-language that is yet to be explored. Previous methods are limited to a pre-defined answer space and cannot generate answers naturally. In this work, we pivot the question answering task to a sequence generation task to generate free-form natural answers for questions in 3D scenes (Gen3DQA). To this end, we optimize our model directly on the language rewards to secure the global sentence semantics. Here, we also adapt a pragmatic language understanding reward to further improve the sentence quality. Our method sets a new SOTA on the ScanQA benchmark (CIDEr score 72.22/66.57 on the test sets).
</details>
<details>
<summary>摘要</summary>
三维问答是一个年轻的领域，尚未得到充分探索。先前的方法受限于固定的答案空间，无法自然生成答案。在这种工作中，我们将问答任务转换为序列生成任务，以生成3D场景中自然的答案（Gen3DQA）。为此，我们直接优化我们的模型以获取语言奖励，以保证全句 semantics。此外，我们还适应了 Pragmatic language understanding奖励，以进一步提高句子质量。我们的方法在ScanQAbenchmark上达到了新的最高纪录（CIDEr分数72.22/66.57）。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather"><a href="#Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather" class="headerlink" title="Transformer-based nowcasting of radar composites from satellite images for severe weather"></a>Transformer-based nowcasting of radar composites from satellite images for severe weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19515">http://arxiv.org/abs/2310.19515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caglarkucuk/earthformer-satellite-to-radar">https://github.com/caglarkucuk/earthformer-satellite-to-radar</a></li>
<li>paper_authors: Çağlar Küçük, Apostolos Giannakos, Stefan Schneider, Alexander Jann</li>
<li>for: 这个研究旨在开发一个基于Transformer架构的nowcasting模型，用于处理由卫星数据驱动的陆基雷达图像序列。</li>
<li>methods: 这个模型使用了卫星数据，并使用Transformer架构进行学习，以预测不同天气现象下的雷达场。</li>
<li>results: 模型在不同天气现象下预测雷达场的能力具有高准确性，并且能够在短时间内预测降水现象。<details>
<summary>Abstract</summary>
Weather radar data are critical for nowcasting and an integral component of numerical weather prediction models. While weather radar data provide valuable information at high resolution, their ground-based nature limits their availability, which impedes large-scale applications. In contrast, meteorological satellites cover larger domains but with coarser resolution.   However, with the rapid advancements in data-driven methodologies and modern sensors aboard geostationary satellites, new opportunities are emerging to bridge the gap between ground- and space-based observations, ultimately leading to more skillful weather prediction with high accuracy.   Here, we present a Transformer-based model for nowcasting ground-based radar image sequences using satellite data up to two hours lead time. Trained on a dataset reflecting severe weather conditions, the model predicts radar fields occurring under different weather phenomena and shows robustness against rapidly growing/decaying fields and complex field structures.   Model interpretation reveals that the infrared channel centered at 10.3 $\mu m$ (C13) contains skillful information for all weather conditions, while lightning data have the highest relative feature importance in severe weather conditions, particularly in shorter lead times.   The model can support precipitation nowcasting across large domains without an explicit need for radar towers, enhance numerical weather prediction and hydrological models, and provide radar proxy for data-scarce regions. Moreover, the open-source framework facilitates progress towards operational data-driven nowcasting.
</details>
<details>
<summary>摘要</summary>
天气雷达数据是现场预报中非常重要的一种数据来源，它们提供高分辨率的信息，但是由于地面设备的限制，其可用性受到限制，这使得大规模应用变得困难。相比之下，气象卫星可以覆盖更大的区域，但是其分辨率相对较低。然而，随着数据驱动方法和现代探测器技术的快速发展，新的机会正在出现，可以bridge the gap between ground-和空中数据，从而实现更准确的天气预报。在这篇文章中，我们提出了一种基于Transformer模型的nowcasting模型，用于预测基于雷达图像序列的天气情况，使用卫星数据作为输入，可以在两个小时前的预测。这个模型在不同的天气情况下预测雷达场景，并且对于快速增长/衰减的场景和复杂的场景表现出了Robustness。模型解释表明，在10.3微米（C13）温差频谱中，infrared通道含有有用的信息，而且在严重天气情况下，闪电数据的相对特征重要性最高。这个模型可以支持大规模的降水预报，不需要显式的雷达塔，可以增强数值天气预报和水文模型，并且可以提供数据缺乏地区的雷达代理。此外，我们的开源框架可以促进操作数据驱动的nowcasting的进步。
</details></li>
</ul>
<hr>
<h2 id="VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation"><a href="#VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation" class="headerlink" title="VideoCrafter1: Open Diffusion Models for High-Quality Video Generation"></a>VideoCrafter1: Open Diffusion Models for High-Quality Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19512">http://arxiv.org/abs/2310.19512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/videocrafter">https://github.com/ailab-cvc/videocrafter</a></li>
<li>paper_authors: Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, Ying Shan</li>
<li>for: 这两种扩散模型是为了提供高质量的视频生成模型，以便研究人员和工程师可以进行更多的研究和应用。</li>
<li>methods: 这两种模型分别使用文本输入和图像输入来生成视频，其中T2V模型可以生成高质量、电影级的视频，而I2V模型可以保持给定图像的内容、结构和风格，并将其转换成视频形式。</li>
<li>results: 这两种模型都可以生成高质量的视频，并且T2V模型可以在质量上超过其他开源T2V模型。<details>
<summary>Abstract</summary>
Video generation has increasingly gained interest in both academia and industry. Although commercial tools can generate plausible videos, there is a limited number of open-source models available for researchers and engineers. In this work, we introduce two diffusion models for high-quality video generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V models synthesize a video based on a given text input, while I2V models incorporate an additional image input. Our proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style. This model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints. We believe that these open-source video generation models will contribute significantly to the technological advancements within the community.
</details>
<details>
<summary>摘要</summary>
视频生成在学术和industry中都受到越来越多的关注。虽然商业工具可以生成看起来很可信的视频，但是学术和工程师可以使用的开源模型却有限。在这个工作中，我们介绍了两种扩散模型，即文本到视频（T2V）和图像到视频（I2V）模型。T2V模型将给定的文本输入Synthesize一个视频，而I2V模型具有一个额外的图像输入。我们提出的T2V模型可以生成高质量、电影级的视频，解决了其他开源T2V模型的质量不够问题。I2V模型是一个首个开源基础模型，可以将给定的图像转换成视频clip，保持内容、结构和风格的一致性。我们认为这些开源视频生成模型将对社区技术进步产生重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Visual-Navigation-of-Underwater-Robots"><a href="#Deep-Learning-for-Visual-Navigation-of-Underwater-Robots" class="headerlink" title="Deep Learning for Visual Navigation of Underwater Robots"></a>Deep Learning for Visual Navigation of Underwater Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19495">http://arxiv.org/abs/2310.19495</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Sunbeam</li>
<li>for: 本研究旨在简报深度学习方法用于水下 робо得视觉导航。</li>
<li>methods: 本文涵盖了水下 робо得视觉导航中使用深度学习方法的视觉感知、可用的视频水下数据集、模仿学习和奖励学习方法等方面。</li>
<li>results: 本文对现有的深度学习方法用于水下导航进行了简要的概述，并将相关的研究分为imitazione学习或深度学习 paradigm下的训练方法。<details>
<summary>Abstract</summary>
This paper aims to briefly survey deep learning methods for visual navigation of underwater robotics. The scope of this paper includes the visual perception of underwater robotics with deep learning methods, the available visual underwater datasets, imitation learning, and reinforcement learning methods for navigation. Additionally, relevant works will be categorized under the imitation learning or deep learning paradigm for underwater robots for clarity of the training methodologies in the current landscape. Literature that uses deep learning algorithms to process non-visual data for underwater navigation will not be considered, except as contrasting examples.
</details>
<details>
<summary>摘要</summary>
这篇论文目的是简要报告深度学习方法用于水下机器人视觉导航。论文的范围包括水下机器人视觉处理深度学习方法、可用的水下视像数据集、模仿学习和奖励学习方法用于导航。此外，相关的工作会根据训练方法类别为水下机器人下的imitating learning或深度学习 парадиг进行分类。文献使用深度学习算法处理非视觉数据用于水下导航不会被考虑，除非作为对比例。
</details></li>
</ul>
<hr>
<h2 id="VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation"><a href="#VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation" class="headerlink" title="VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation"></a>VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19477">http://arxiv.org/abs/2310.19477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Wu, Zhiyan Du, Zhi Li, Feng-Lei Fan, Tieyong Zeng</li>
<li>for:  recovering clear images from blurry ones with an unknown blur kernel</li>
<li>methods:  using deep image prior (DIP) with variational deep image prior (VDIP) and total generalized variational (TGV) regularization, and solving the model using alternating direction method of multipliers (ADMM)</li>
<li>results:  surpassing various state-of-the-art models quantitatively and qualitatively in recovering image edges and details while reducing oil painting artifacts.Here’s the simplified Chinese text:</li>
<li>for:  recuperar imágenes claras a partir de imágenes borrosas con una cámara de desconocida blur</li>
<li>methods:  utilizar prior de imagen profunda (DIP) con prior de imagen variacional (VDIP) y regularización de generalización total (TGV), y resolver el modelo utilizando el método de multiplicadores en sentido alternante (ADMM)</li>
<li>results:  superar a diferentes modelos estado-de-la-arte cuantitativamente y qualitativamente en la recuperación de límites y detalles de imágenes mientras reducen los artefactos de pintura al óleo.<details>
<summary>Abstract</summary>
Recovering clear images from blurry ones with an unknown blur kernel is a challenging problem. Deep image prior (DIP) proposes to use the deep network as a regularizer for a single image rather than as a supervised model, which achieves encouraging results in the nonblind deblurring problem. However, since the relationship between images and the network architectures is unclear, it is hard to find a suitable architecture to provide sufficient constraints on the estimated blur kernels and clean images. Also, DIP uses the sparse maximum a posteriori (MAP), which is insufficient to enforce the selection of the recovery image. Recently, variational deep image prior (VDIP) was proposed to impose constraints on both blur kernels and recovery images and take the standard deviation of the image into account during the optimization process by the variational principle. However, we empirically find that VDIP struggles with processing image details and tends to generate suboptimal results when the blur kernel is large. Therefore, we combine total generalized variational (TGV) regularization with VDIP in this paper to overcome these shortcomings of VDIP. TGV is a flexible regularization that utilizes the characteristics of partial derivatives of varying orders to regularize images at different scales, reducing oil painting artifacts while maintaining sharp edges. The proposed VDIP-TGV effectively recovers image edges and details by supplementing extra gradient information through TGV. Additionally, this model is solved by the alternating direction method of multipliers (ADMM), which effectively combines traditional algorithms and deep learning methods. Experiments show that our proposed VDIP-TGV surpasses various state-of-the-art models quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
recuperar imagens claras de imagens borrosas com um kernel de blur desconhecido é um problema desafiador. A prioridade de imagem profunda (DIP) propõe usar a rede profunda como um regularizador para uma imagem individual em vez de um modelo de aprendizado supervisionado, o que alcança resultados encorajadores no problema de desblurring não cego. No entanto, desde que a relação entre as imagens e as arquiteturas de rede é incerta, é difícil encontrar uma arquitetura adequada para fornecer restrições suficientes sobre os kernel de blur e as imagens limpas. Além disso, DIP usa a máxima a posteriori sparse (MAP), o que é insuficiente para impor a seleção da imagem de recovery.Recentemente, o prior de imagem profunda variável (VDIP) foi proposto para impor restrições sobre os kernel de blur e as imagens de recovery e considerar a variância da imagem durante o processo de otimização pelo princípio variacional. No entanto, encontramos empreiticamente que VDIP tem dificuldade em processar detalhes de imagem e tende a gerar resultados subótimos quando o kernel de blur é grande. Portanto, combinamos a regularização total geral variável (TGV) com VDIP neste artigo para superar as deficiências de VDIP. TGV é uma regularização flexível que utiliza as características das derivações parciais de órders variáveis para regularizar as imagens em escalas diferentes, reduzindo artefatos de óleo pintura enquanto manteve as bordos afiados. A nossa propriedade VDIP-TGV efetivamente recupera as bordos e detalhes das imagens by suplementando informações de gradiente adicionais através de TGV. Além disso, este modelo é resolvido pelo método de direções alternadas de multiplicadores (ADMM), que eficazmente combina métodos tradicionais e de aprendizado profundo. Os resultados experimentais mostram que nossa propriedade VDIP-TGV ultrapassa modelos estado-da-arte quantitativamente e qualitativamente.
</details></li>
</ul>
<hr>
<h2 id="Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions"><a href="#Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions" class="headerlink" title="Generative Neural Fields by Mixtures of Neural Implicit Functions"></a>Generative Neural Fields by Mixtures of Neural Implicit Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19464">http://arxiv.org/abs/2310.19464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tackgeun/mNIF">https://github.com/tackgeun/mNIF</a></li>
<li>paper_authors: Tackgeun You, Mijeong Kim, Jungtaek Kim, Bohyung Han</li>
<li>for: 学习生成神经场的线性组合卷积网络表示</li>
<li>methods: 采用meta-学习或自动解码方法学习卷积网络的含义表示和其系数在隐藏空间</li>
<li>results: 提出一种可以增加生成神经场容量的方法，同时保持推理网络的小型化，并通过权重平均来减少推理时间和内存占用。此外，通过适应推理任务来采样latent mixture coefficient，使得最终模型能够生成未见数据。实验表明，该方法在多种图像、体量数据和NeRF场景中达到了竞争性的生成性能。<details>
<summary>Abstract</summary>
We propose a novel approach to learning the generative neural fields represented by linear combinations of implicit basis networks. Our algorithm learns basis networks in the form of implicit neural representations and their coefficients in a latent space by either conducting meta-learning or adopting auto-decoding paradigms. The proposed method easily enlarges the capacity of generative neural fields by increasing the number of basis networks while maintaining the size of a network for inference to be small through their weighted model averaging. Consequently, sampling instances using the model is efficient in terms of latency and memory footprint. Moreover, we customize denoising diffusion probabilistic model for a target task to sample latent mixture coefficients, which allows our final model to generate unseen data effectively. Experiments show that our approach achieves competitive generation performance on diverse benchmarks for images, voxel data, and NeRF scenes without sophisticated designs for specific modalities and domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来学习生成神经场，即利用线性组合的隐式基准网络表示。我们的算法在隐藏空间学习基准网络和其相对权重，可以通过meta-学习或自动解码方式进行。这种方法可以轻松扩大生成神经场的容量，只需增加基准网络的数量，而不需增加推理网络的大小。因此，使用该模型进行采样实例会具有较低的延迟和内存占用。此外，我们可以根据目标任务自定义销毁扩散概率模型，从而使我们的最终模型能够生成未看过的数据。实验表明，我们的方法在多个图像、VOXEL数据和NeRF场景上实现了竞争力强的生成性能，无需特殊的设计 для具体的Modalities和Domains。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers"><a href="#Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers" class="headerlink" title="Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers"></a>Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19447">http://arxiv.org/abs/2310.19447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsong Zhang, Lingfeng Gu, Yu-Kun Lai, Xueyang Wang, Kun Li</li>
<li>for: 本研究旨在提高大规模场景中的群体检测精度，特别是在多个人体重叠的情况下。</li>
<li>methods: 我们提出了一个端到端框架，即GroupTransformer，用于大规模场景中的群体检测。我们设计了一个干扰编码器，用于检测和抑制严重干扰人体的裁剪。同时，我们还提出了空间时间变换器，用于同时提取人体轨迹信息和 fusion inter-person特征在层次结构中。</li>
<li>results: 我们的方法在大规模场景和小规模场景上都实现了比州前方法更好的性能，具体来说，在大规模场景上，我们的方法可以提高精度和F1分数的表现，提高了10%以上。在小规模场景上，我们的方法仍然提高了F1分数的表现，提高了5%以上。<details>
<summary>Abstract</summary>
Group detection, especially for large-scale scenes, has many potential applications for public safety and smart cities. Existing methods fail to cope with frequent occlusions in large-scale scenes with multiple people, and are difficult to effectively utilize spatio-temporal information. In this paper, we propose an end-to-end framework,GroupTransformer, for group detection in large-scale scenes. To deal with the frequent occlusions caused by multiple people, we design an occlusion encoder to detect and suppress severely occluded person crops. To explore the potential spatio-temporal relationship, we propose spatio-temporal transformers to simultaneously extract trajectory information and fuse inter-person features in a hierarchical manner. Experimental results on both large-scale and small-scale scenes demonstrate that our method achieves better performance compared with state-of-the-art methods. On large-scale scenes, our method significantly boosts the performance in terms of precision and F1 score by more than 10%. On small-scale scenes, our method still improves the performance of F1 score by more than 5%. The project page with code can be found at http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans.
</details>
<details>
<summary>摘要</summary>
团体探测，特别是在大规模场景中，有很多应用前景，如公共安全和智能城市。现有方法在大规模场景中频繁受到多人 occlusion 的影响，而且很难有效地利用空间时间信息。在这篇论文中，我们提出了一个端到端框架，GroupTransformer，用于大规模场景中的团体探测。为了处理由多人引起的严重遮挡，我们设计了遮挡编码器来检测并抑制严重遮挡人裁剪。为了探索团体之间的空间时间关系，我们提出了空间时间变换器，以同时提取 trajectory 信息并在层次结构中融合人群特征。实验结果表明，我们的方法在大规模场景和小规模场景上都有更好的表现，与现有方法相比，在大规模场景上提高了精度和 F1 分数的表现，提高了10%以上；在小规模场景上提高了 F1 分数的表现，提高了5%以上。项目页面和代码可以在 http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans 找到。
</details></li>
</ul>
<hr>
<h2 id="One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation"><a href="#One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation" class="headerlink" title="One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation"></a>One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19444">http://arxiv.org/abs/2310.19444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao840/ofakd">https://github.com/hao840/ofakd</a></li>
<li>paper_authors: Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, Chang Xu<br>for:This paper focuses on the challenge of knowledge distillation (KD) between heterogeneous models, specifically teacher and student models with different architectures.methods:The proposed method, OFA-KD, utilizes centered kernel alignment (CKA) to compare learned features between the teacher and student models, and projects intermediate features into an aligned latent space such as the logits space to discard architecture-specific information. Additionally, an adaptive target enhancement scheme is introduced to prevent the student from being disturbed by irrelevant information.results:The proposed OFA-KD framework significantly improves the distillation performance between heterogeneous architectures, with notable performance improvements for the student models, achieving a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset.<details>
<summary>Abstract</summary>
Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）已经证明是一种非常有效的方法，可以通过教师模型和学生模型之间的培训策略来提高模型性能。然而，大多数现有的塑化方法都是基于教师和学生模型属于同一个模型家族的假设，特别是Hint-based方法。我们使用中心kernel对比（CKA）来比较教师和学生模型学习的特征之间的差异，我们发现了显著的特征差异。这种差异表明了前一些Hint-based方法在跨建制塑化中的效果不佳。为了解决跨建制塑化的挑战，我们提出了一个简单 yet有效的一对所有KD框架，称为OFA-KD。具体来说，我们将中间特征投影到一个Alignment的特征空间，例如logits空间，这里抛弃了建制特定的信息。此外，我们引入了一种适应的目标增强方案，以避免学生被无关信息所干扰。我们对不同建制的模型，包括CNN、Transformer和MLP，进行了广泛的实验，结果表明了我们的OFA-KD框架在跨建制塑化中的优越性。具体来说，当我们的OFA-KD框架与学生模型结合使用时，学生模型的性能得到了明显的提高，最大提高为CIFAR-100数据集上的8.0%和ImageNet-1K数据集上的0.7%。PyTorch代码和检查点可以在https://github.com/Hao840/OFAKD中找到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements"><a href="#Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements" class="headerlink" title="Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements"></a>Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19441">http://arxiv.org/abs/2310.19441</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. James Cotton, Colleen Peyton</li>
<li>for: 这个研究是为了发展一种可靠的三维运动分析方法，以帮助诊断和治疗儿童中的神经发展障碍和其他疾病。</li>
<li>methods: 这个研究使用了动态 Gaussian splatting 技术，将其应用于儿童身上的罕见 markerless motion capture 资料。这种方法利用 semantic segmentation 图像来专注在婴儿身上，以改善Scene的初始化。</li>
<li>results: 研究结果显示了这种方法的潜在，可以实现新的景象和婴儿运动的追踪。这些成果开辟了一条新的道路，以帮助对多种临床人口进行进一步的运动分析，特别是在婴儿的早期诊断中。<details>
<summary>Abstract</summary>
Easy access to precise 3D tracking of movement could benefit many aspects of rehabilitation. A challenge to achieving this goal is that while there are many datasets and pretrained algorithms for able-bodied adults, algorithms trained on these datasets often fail to generalize to clinical populations including people with disabilities, infants, and neonates. Reliable movement analysis of infants and neonates is important as spontaneous movement behavior is an important indicator of neurological function and neurodevelopmental disability, which can help guide early interventions. We explored the application of dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our approach leverages semantic segmentation masks to focus on the infant, significantly improving the initialization of the scene. Our results demonstrate the potential of this method in rendering novel views of scenes and tracking infant movements. This work paves the way for advanced movement analysis tools that can be applied to diverse clinical populations, with a particular emphasis on early detection in infants.
</details>
<details>
<summary>摘要</summary>
便捷的3D运动跟踪可以有助于很多方面的康复。一个挑战是，虽然有很多数据集和预训练算法可以用于健康成人，但这些算法经常无法泛化到临床人口，包括残疾人、婴儿和新生儿。准确的婴儿和新生儿运动分析非常重要，因为自发运动行为是脑功能和发育障碍的重要指标，可以帮助早期 intervene。我们探讨了使用动态Gaussian泵浦法处理缺失标记的运动捕捉数据。我们的方法利用 semantic segmentation 面积来关注婴儿，可以大幅提高场景的初始化。我们的结果表明这种方法有potential用于生成新视图和跟踪婴儿运动。这项工作为临床各种人口提供了先进的运动分析工具，尤其是早期检测。
</details></li>
</ul>
<hr>
<h2 id="GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning"><a href="#GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning" class="headerlink" title="GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning"></a>GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19418">http://arxiv.org/abs/2310.19418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cosmaadrian/gaitformer">https://github.com/cosmaadrian/gaitformer</a></li>
<li>paper_authors: Adrian Cosma, Emilian Radoi</li>
<li>for: 本研究旨在提出一种基于人体运动模式的人识别方法，同时还能够提取多种外观特征。</li>
<li>methods: 该方法使用了 dense-trajectory 技术自动从视频流中提取人体运动模式，并在多任务学习模式下预训练一个 transformer 模型（GaitFormer）。</li>
<li>results: 该方法可以在 CASIA-B 和 FVG 数据集上达到 92.5% 和 85.33% 的准确率，比类似方法提高 +14.2% 和 +9.67%。此外，该方法还能够准确地识别性别信息和多种外观特征。<details>
<summary>Abstract</summary>
Gait analysis is proven to be a reliable way to perform person identification without relying on subject cooperation. Walking is a biometric that does not significantly change in short periods of time and can be regarded as unique to each person. So far, the study of gait analysis focused mostly on identification and demographics estimation, without considering many of the pedestrian attributes that appearance-based methods rely on. In this work, alongside gait-based person identification, we explore pedestrian attribute identification solely from movement patterns. We propose DenseGait, the largest dataset for pretraining gait analysis systems containing 217K anonymized tracklets, annotated automatically with 42 appearance attributes. DenseGait is constructed by automatically processing video streams and offers the full array of gait covariates present in the real world. We make the dataset available to the research community. Additionally, we propose GaitFormer, a transformer-based model that after pretraining in a multi-task fashion on DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without utilizing any manually annotated data. This corresponds to a +14.2% and +9.67% accuracy increase compared to similar methods. Moreover, GaitFormer is able to accurately identify gender information and a multitude of appearance attributes utilizing only movement patterns. The code to reproduce the experiments is made publicly.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文。<</SYS>>研究人员通过径行分析确认了径行分析是可靠的人体特征标识方法，不需要参与者的合作。径行是一种不会在短时间内发生 significiant 变化的生物метри，可以作为每个人独特的标识。在这项研究中，我们不仅进行了人体标识，还 explore了通过运动模式来确定 pedestrian 的属性。我们提出了 DenseGait 数据集，包含 217 万个匿名的跟踪样本，自动获得了 42 个外观特征的标注。DenseGait 数据集通过自动处理视频流程建立，包含了实际世界中的所有径行 covariates。我们将数据集提供给研究人员。此外，我们提出了 GaitFormer 模型，通过在多任务方式进行预训练后，在 CASIA-B 和 FVG 上达到了 92.5% 和 85.33% 的准确率，不使用任何手动标注数据。这相当于与类似方法相比增加了 +14.2% 和 +9.67% 的准确率。此外，GaitFormer 能够通过运动模式来准确地确定 gender 信息和多种外观特征。我们将实验代码公开。
</details></li>
</ul>
<hr>
<h2 id="CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance"><a href="#CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance" class="headerlink" title="CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance"></a>CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19413">http://arxiv.org/abs/2310.19413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani</li>
<li>for: 本研究旨在提供个性化人识别模块，以便在拥挤的环境中Robot与合适的个体合作，并且能够适应视觉出入和干扰。</li>
<li>methods: 本研究使用了不断改进的视觉适应技术，以确保Robot与人类之间的无间隔合作。</li>
<li>results: 实验结果显示，对于各个视频，CARPE-ID可以准确地跟踪每个选择的目标，而s-o-t-a MOT方法则有4个跟踪错误。<details>
<summary>Abstract</summary>
In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency exists to assume that the robot shall cooperate with the closest individual or that the scene involves merely a singular human actor. However, in realistic scenarios, such as shop floor operations, such an assumption may not hold and personalized target recognition by the robot in crowded environments is required. To fulfil this requirement, in this work, we propose a person re-identification module based on continual visual adaptation techniques that ensure the robot's seamless cooperation with the appropriate individual even subject to varying visual appearances or partial or complete occlusions. We test the framework singularly using recorded videos in a laboratory environment and an HRI scenario, i.e., a person-following task by a mobile robot. The targets are asked to change their appearance during tracking and to disappear from the camera field of view to test the challenging cases of occlusion and outfit variations. We compare our framework with one of the state-of-the-art Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can accurately track each selected target throughout the experiments in all the cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of 4 tracking errors for each video.
</details>
<details>
<summary>摘要</summary>
今天的人机交互（HRI）场景中，一种普遍的偏好是假设机器人与最近的个体或场景中只有一个人actor进行交互。然而，在实际场景中，如生产工程中，这种假设可能不成立，机器人需要在拥挤的环境中进行个体识别。为了实现这一目标，在这个工作中，我们提出了基于不断视觉适应技术的人重识别模块，以确保机器人在变化的视觉表现和部分或完全遮挡的情况下仍能够无缝地合作与正确的个体。我们使用实验室环境中录制的视频进行单个测试，以及人机交互场景，即移动机器人进行跟踪任务。目标人员在跟踪中改变外表和隐藏于相机视野中进行测试具有挑战性的 occlusion 和衣服变化情况。我们与一种state-of-the-art多对目标跟踪（MOT）方法进行比较，结果显示，CARPE-ID 能够在所有情况下（除了两个限制情况）accurately track每个选择的目标，而 s-o-t-a MOT 的平均跟踪错误为每个视频4。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images"><a href="#Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images" class="headerlink" title="Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images"></a>Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19411">http://arxiv.org/abs/2310.19411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Yaqub, Feng Jinchao</li>
<li>For: The paper is written for early detection of breast cancer (BC) using mammography images.* Methods: The proposed method uses a deep learning approach that includes data collection from established benchmark sources, image segmentation employing an Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet (ACA-AMDN) model. The hyperparameters within the models are optimized using the Modified Mussel Length-based Eurasian Oystercatcher Optimization (MML-EOO) algorithm.* Results: The proposed BC detection framework attains superior precision rates in early disease detection, demonstrating its potential to enhance mammography-based screening methodologies.Here are the three key information points in Simplified Chinese text:* For: 这篇论文是为早期检测乳腺癌（BC）使用乳腺图像而写的。* Methods: 提议的方法使用深度学习方法，包括数据收集自确认标准源，使用具有扩展核心的Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet（ACA-ATRUNet）建筑，以及使用Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet（ACA-AMDN）模型进行BC标识。模型中的超参数使用改进的贝叶绒长度基于欧洲乌苏鸟优化算法进行优化。* Results: 提议的BC检测框架在早期疾病检测中具有更高的准确率，表明其可能性用于提高乳腺图像基本creening方法。<details>
<summary>Abstract</summary>
Breast cancer (BC) significantly contributes to cancer-related mortality in women, underscoring the criticality of early detection for optimal patient outcomes. A mammography is a key tool for identifying and diagnosing breast abnormalities; however, accurately distinguishing malignant mass lesions remains challenging. To address this issue, we propose a novel deep learning approach for BC screening utilizing mammography images. Our proposed model comprises three distinct stages: data collection from established benchmark sources, image segmentation employing an Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet (ACA-AMDN) model. The hyperparameters within the ACA-ATRUNet and ACA-AMDN models are optimised using the Modified Mussel Length-based Eurasian Oystercatcher Optimization (MML-EOO) algorithm. Performance evaluation, leveraging multiple metrics, is conducted, and a comparative analysis against conventional methods is presented. Our experimental findings reveal that the proposed BC detection framework attains superior precision rates in early disease detection, demonstrating its potential to enhance mammography-based screening methodologies.
</details>
<details>
<summary>摘要</summary>
乳癌（BC）对女性患有癌症的致死率具有重要性，因此早期发现是至关重要的。而现在，诊断乳腺疾病的主要工具是胸部X射线图像，但是准确地识别肿瘤困难。为解决这个问题，我们提出了一种基于深度学习的新型乳癌检测方法，使用胸部X射线图像。我们的提议的模型包括三个阶段：数据收集自确定的参照源，使用基于Atrous Convolution的Attentive和Adaptive Trans-Res-UNet（ACA-ATRUNet）架构进行图像分割，以及使用基于Atrous Convolution的Attentive和Adaptive Multi-scale DenseNet（ACA-AMDN）模型进行乳癌识别。模型中的hyperparameter被MML-EOO算法优化。我们进行了性能评估，并对传统方法进行比较分析。我们的实验结果表明，我们的乳癌检测框架可以在早期疾病检测中实现更高的精度率，这显示了它在胸部X射线图像基本的检测方法中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models"><a href="#Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models" class="headerlink" title="Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models"></a>Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19410">http://arxiv.org/abs/2310.19410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minxingzhang/miagm">https://github.com/minxingzhang/miagm</a></li>
<li>paper_authors: Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, Yang Zhang</li>
<li>for: 这篇论文的目的是测试生成模型的隐私漏洞，以及对它们的攻击性能。</li>
<li>methods: 本研究使用了一种通用的会员推论攻击（MIA），可以对多种生成模型进行攻击，包括生成对抗网络、自适应器、隐藏函数和传播模型。这种攻击只需要对目标生成器生成的分布进行访问，并且不需要shadow模型或白盒式存取。</li>
<li>results: 实验结果显示了所有的生成模型都受到了本研究的攻击，例如在DDPM、DDIM和FastDPM预测器上，攻击得分高于0.99。对VQGAN、LDM（文本参数生成）和LIIF进行攻击也得到了AUC高于0.90。因此，本研究强调了生成模型的隐私漏洞，并呼吁设计和发布生成模型时需要考虑这一点。<details>
<summary>Abstract</summary>
Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary non-member datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC $>0.99$ against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC $>0.90.$ As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese生成模型在各种视觉创作任务中表现出了革命性的成功，但在同时，它们也面临了泄露私人训练数据信息的风险。多种会员推断攻击（MIA）已经提出，以示生成模型的隐私漏洞，但这些攻击受到了重大限制，例如需要附加模型和白盒访问，并且忽略或只关注涉及到扩散模型的唯一特性，这限制了它们的普适性。相比之下，我们提出了首次总体会员推断攻击，可以 against 多种生成模型，如生成对抗网络、自适应网络、隐藏函数和扩散模型。我们只需要使用目标生成器生成的分布，并且不需要访问target generator的 Architecture或应用场景，因此可以视为target generator为黑盒。实验证明了所有生成模型都是易受我们攻击的。例如，我们的工作在 DDPM、DDIM 和 FastDPM 上对 CIFAR-10 和 CelebA 进行了攻击，并获得了 AUC > 0.99。对 VQGAN、LDM（ для文本 conditional generation）和 LIIF 进行了攻击，并获得了 AUC > 0.90。因此，我们呼吁我们的社区在设计和发布生成模型时注意隐私泄露风险。
</details></li>
</ul>
<hr>
<h2 id="Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks"><a href="#Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks" class="headerlink" title="Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks"></a>Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19405">http://arxiv.org/abs/2310.19405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzeen Munir, Shoaib Azam, Tomasz Kucner, Ville Kyrki, Moongu Jeon</li>
<li>for: 提高抗风险的自动驾驶系统准确检测环境中的对象。</li>
<li>methods:  combining radar和lidar数据，使用添加性注意力进行 integrate，并使用一种新的平行分支结构（PFS）来管理尺度变化。</li>
<li>results: 在Radiate数据集上使用COCO指标测试，与状态艺法比较，提高了1.89%和2.61%在有利和不利天气条件下。<details>
<summary>Abstract</summary>
Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due to the data's sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by $1.89\%$ and $2.61\%$ in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.
</details>
<details>
<summary>摘要</summary>
Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due to the data's sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by 1.89% and 2.61% in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.Here's the word-for-word translation of the text into Simplified Chinese:对各种感知系统来说，对象检测是核心组件，为ego车辆提供环境信息，确保安全路径规划。尽管摄像头和激光探测技术有所进步，但在不利天气条件下，它们的性能可能有限。相比之下，毫米波技术可以使radar在这些条件下 функциональ effectively。然而，仅仅通过radar来构建感知系统并不能完全捕捉环境，因为数据的稀疏性。为此，感知融合策略被引入。我们提议一种双支分支框架，用于融合radar和激光数据以获得提高的对象检测。主支分支专注提取radar特征，而辅助支分支提取激光特征。这些特征然后被combine用添加性注意。然后，混合特征被传递 через一种新型的并行分支结构（PFS）来管理尺度变化。一个区域提议头然后用于对象检测。我们使用Radiate数据集对我们提议的方法进行评估，使用COCO指标。结果显示，它在有利和不利天气条件下比 estado-of-the-art 方法提高 $1.89\%$ 和 $2.61\%$。这表明，毫米波-激光融合在恶势夹攻击天气条件下能够实现精准的对象检测和定位。
</details></li>
</ul>
<hr>
<h2 id="A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma"><a href="#A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma" class="headerlink" title="A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma"></a>A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19392">http://arxiv.org/abs/2310.19392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navodini Wijethilake, Steve Connor, Anna Oviedova, Rebecca Burger, Tom Vercauteren, Jonathan Shapey<br>for:This paper aims to automate and improve the process of clinical decision making for patients with Vestibular Schwannoma by using deep learning-based segmentation to extract relevant clinical features.methods:The authors use deep learning-based segmentation to extract measurements from T1 and T2 weighted MRI scans, and propose a novel algorithm to choose and extract the most appropriate maximum linear measurement from the segmented regions based on the size of the extrameatal portion of the tumor.results:The authors achieved Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, and 0.8222 +- 0.2108 and 0.9049 +- 0.0646 for T1 weighted MRI. The measurements extracted manually by an expert neuroradiologist showed a significant correlation with the automated measurements (p &lt; 0.0001).<details>
<summary>Abstract</summary>
Vestibular Schwannoma is a benign brain tumour that grows from one of the balance nerves. Patients may be treated by surgery, radiosurgery or with a conservative "wait-and-scan" strategy. Clinicians typically use manually extracted linear measurements to aid clinical decision making. This work aims to automate and improve this process by using deep learning based segmentation to extract relevant clinical features through computational algorithms. To the best of our knowledge, our study is the first to propose an automated approach to replicate local clinical guidelines. Our deep learning based segmentation provided Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, whereas 0.8222 +- 0.2108 and 0.9049 +- 0.0646 were obtained for T1 weighted MRI. We propose a novel algorithm to choose and extract the most appropriate maximum linear measurement from the segmented regions based on the size of the extrameatal portion of the tumour. Using this tool, clinicians will be provided with a visual guide and related metrics relating to tumour progression that will function as a clinical decision aid. In this study, we utilize 187 scans obtained from 50 patients referred to a tertiary specialist neurosurgical service in the United Kingdom. The measurements extracted manually by an expert neuroradiologist indicated a significant correlation with the automated measurements (p < 0.0001).
</details>
<details>
<summary>摘要</summary>
vestibular schwannoma 是一种无害的脑肿，来自平衡神经的增生。患者可能会通过手术、放射学手术或保守的 "等待和扫描" 策略进行治疗。临床医生通常使用手动提取的线性测量以帮助临床决策。这项工作想要自动化和改进这个过程，使用深度学习基于分割来提取相关的临床特征。据我们所知，我们的研究是第一个提出自动化方法来复制当地临床指南。我们使用深度学习基于分割提取的 dice 分数为 0.8124 ± 0.2343 和 0.8969 ± 0.0521  для T2 重度 MRI，而为 T1 重度 MRI 分别为 0.8222 ± 0.2108 和 0.9049 ± 0.0646。我们提出了一种新的算法，可以从分割区域中选择和提取最有用的最大线性测量，基于脑外部分的大小。使用这个工具，临床医生将获得一个可见导航和相关的各种指标，用于诊断和治疗。在本研究中，我们使用 187 个扫描数据，来自 50 名患者，被提交到英国特等专业神经外科服务。手动由专家神经 radiologist 提取的测量显示与自动测量存在显著相关性（p < 0.0001）。
</details></li>
</ul>
<hr>
<h2 id="TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition"><a href="#TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition" class="headerlink" title="TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition"></a>TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19380">http://arxiv.org/abs/2310.19380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmmmeng/transxnet">https://github.com/lmmmeng/transxnet</a></li>
<li>paper_authors: Meng Lou, Hong-Yu Zhou, Sibei Yang, Yizhou Yu</li>
<li>for: This paper aims to improve the performance of vision backbone networks by integrating convolution and self-attention mechanisms.</li>
<li>methods: The proposed Dual Dynamic Token Mixer (D-Mixer) aggregates global information and local details in an input-dependent way, using an efficient global attention module and input-dependent depthwise convolution.</li>
<li>results: The proposed TransXNet vision backbone network delivers compelling performance on ImageNet-1K image classification and other dense prediction tasks, outperforming other state-of-the-art networks while having lower computational costs.<details>
<summary>Abstract</summary>
Recent studies have integrated convolution into transformers to introduce inductive bias and improve generalization performance. However, the static nature of conventional convolution prevents it from dynamically adapting to input variations, resulting in a representation discrepancy between convolution and self-attention as self-attention calculates attention matrices dynamically. Furthermore, when stacking token mixers that consist of convolution and self-attention to form a deep network, the static nature of convolution hinders the fusion of features previously generated by self-attention into convolution kernels. These two limitations result in a sub-optimal representation capacity of the constructed networks. To find a solution, we propose a lightweight Dual Dynamic Token Mixer (D-Mixer) that aggregates global information and local details in an input-dependent way. D-Mixer works by applying an efficient global attention module and an input-dependent depthwise convolution separately on evenly split feature segments, endowing the network with strong inductive bias and an enlarged effective receptive field. We use D-Mixer as the basic building block to design TransXNet, a novel hybrid CNN-Transformer vision backbone network that delivers compelling performance. In the ImageNet-1K image classification task, TransXNet-T surpasses Swin-T by 0.3\% in top-1 accuracy while requiring less than half of the computational cost. Furthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability, achieving top-1 accuracy of 83.8\% and 84.6\% respectively, with reasonable computational costs. Additionally, our proposed network architecture demonstrates strong generalization capabilities in various dense prediction tasks, outperforming other state-of-the-art networks while having lower computational costs.
</details>
<details>
<summary>摘要</summary>
最近的研究已经结合卷积 INTO transformers 以引入导引偏好并提高通用性表现。然而，传统的卷积方法因为静态的特性而无法灵活地适应输入变化，导致卷积和自我注意的表现差异。此外，当堆叠token mixer  consisting of convolution and self-attention to form a deep network 时，静态的卷积方法会阻碍对先前由自我注意生成的特征进行融合。这两个限制导致建立的网络表现下限。为了解决这个问题，我们提出了一个轻量级的双动态token mixer (D-Mixer)，它可以在输入相依的方式下对全局信息和本地细节进行弹性聚合。D-Mixer 通过单簇全球注意模组和输入相依的深度卷积分别在批量分割后的特征段进行对应。这使得网络具有强大的导引偏好和扩大的有效访问范围。我们使用 D-Mixer 为基础建立 TransXNet，一个新的混合 CNN-Transformer 视觉后续网络，它在 ImageNet-1K 图像识别任务中，轻松过越 Swin-T 的顶部一个精度，并且需要较少的计算成本。此外，TransXNet-S 和 TransXNet-B 在不同的计算成本下表现出色，分别获得了83.8% 和 84.6% 的顶部精度。此外，我们提出的网络架构显示了强大的通用能力，在不同的紧密预测任务中表现出色，而且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="Color-Equivariant-Convolutional-Networks"><a href="#Color-Equivariant-Convolutional-Networks" class="headerlink" title="Color Equivariant Convolutional Networks"></a>Color Equivariant Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19368">http://arxiv.org/abs/2310.19368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/attila94/ceconv">https://github.com/attila94/ceconv</a></li>
<li>paper_authors: Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert</li>
<li>for:  addresses the issue of color-based domain shifts in Convolutional Neural Networks (CNNs)</li>
<li>methods:  proposes a novel deep learning building block called Color Equivariant Convolutions (CEConvs), which enables shape feature sharing across the color spectrum while retaining important color information</li>
<li>results:  demonstrates the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts.<details>
<summary>Abstract</summary>
Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs.
</details>
<details>
<summary>摘要</summary>
颜色是计算机视觉中的一个关键视觉提示符，它被卷积神经网络（CNN）广泛利用于物体识别。然而，如果数据中存在颜色变化的偏度，CNN会受到影响，而色彩不变则会消除所有颜色信息，导致识别力下降。在这篇论文中，我们提出了颜色共轭卷积（CEConvs），一种新的深度学习构建块，它可以在颜色谱中共享形状特征，同时保留重要的颜色信息。我们扩展了卷积神经网络中的对称性概念，从 геомétric 变换中扩展到 photométriques 变换，通过在 neural network 中共享参数来实现色彩映射。我们示出了 CEConvs 在不同任务上的下游性能和颜色变化的Robustness，以及可以轻松地整合到现有的架构中，如 ResNets。这种方法可以解决计算机视觉中的颜色域shift问题，并提供了一个可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection"><a href="#Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection" class="headerlink" title="Semi- and Weakly-Supervised Domain Generalization for Object Detection"></a>Semi- and Weakly-Supervised Domain Generalization for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19351">http://arxiv.org/abs/2310.19351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Furuta, Yoichi Sato</li>
<li>For: 这篇论文是为了解决域别训练和测试数据之间的域差问题，提出了两个新的问题设定：半监督域通用物类检测（SS-DGOD）和弱监督域通用物类检测（WS-DGOD）。* Methods: 这篇论文使用了学生-教师学习框架，其中一个学生网络是由另一个教师网络的pseudo标签output trains on unlabeled or weakly labeled data。* Results: 实验结果显示，这篇论文所提出的问题设定可以将物类检测器训练到同等或更好的性能，而不需要依赖目标域数据的训练。<details>
<summary>Abstract</summary>
Object detectors do not work well when domains largely differ between training and testing data. To solve this problem, domain generalization approaches, which require training data with ground-truth labels from multiple domains, have been proposed. However, it is time-consuming and labor-intensive to collect those data for object detection because not only class labels but also bounding boxes must be annotated. To overcome the problem of domain gap in object detection without requiring expensive annotations, we propose to consider two new problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. We show that object detectors can be effectively trained on the proposed settings with the same student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. The experimental results demonstrate that the object detectors trained on the proposed settings significantly outperform baseline detectors trained on one labeled domain data and perform comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, while ours do not use target domain data for training in contrast to UDA.
</details>
<details>
<summary>摘要</summary>
Object detectors 不工作好当域和测试数据之间差异较大。为解决这个问题，域合一扩展方法（Domain Generalization，DG），需要训练数据包含多个域的真实标签，有 been proposed. However, collecting such data for object detection is time-consuming and labor-intensive, as not only class labels but also bounding boxes must be annotated. To overcome the problem of domain gap in object detection without requiring expensive annotations, we propose two new problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). Unlike conventional DG, which requires labeled data from multiple domains, SS-DGOD and WS-DGOD only require labeled data from one domain and unlabeled or weakly-labeled data from multiple domains for training. We show that object detectors can be effectively trained on the proposed settings using a student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. Experimental results demonstrate that object detectors trained on the proposed settings significantly outperform baseline detectors trained on one labeled domain data and perform comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, without using target domain data for training.
</details></li>
</ul>
<hr>
<h2 id="Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer"><a href="#Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer" class="headerlink" title="Label-Only Model Inversion Attacks via Knowledge Transfer"></a>Label-Only Model Inversion Attacks via Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19342">http://arxiv.org/abs/2310.19342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>For: This paper is focused on addressing the privacy threat of model inversion (MI) attacks, specifically in the label-only setup where the adversary only has access to the model’s predicted labels.* Methods: The proposed approach, called LOKT, uses transfer learning from an opaque target model to surrogate models, and leverages generative modeling to effectively transfer knowledge from the target model. The approach involves creating a new model called Target model-assisted ACGAN (T-ACGAN) to facilitate knowledge transfer.* Results: The proposed method significantly outperforms existing state-of-the-art (SOTA) label-only MI attacks by more than 15% across all MI benchmarks, and compares favorably in terms of query budget. The study highlights the rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed.Here’s the simplified Chinese text for the three key points:* For: 这篇论文关注了机器学习模型反向攻击（MI）的隐私问题，特别是在标签只setup中， adversary只有模型预测的标签（硬标签）的情况下。* Methods: 提议的方法是基于目标模型的知识传递，使用转移学习来训练助手模型，并使用生成模型来有效地传递知识。* Results: 提议的方法在所有MI benchmark上比现有SOTA标签只MI攻击提高了更多于15%，并与查询预算相比具有优势。这种研究指出ML模型even when minimal information (i.e., hard labels) is exposed的隐私问题在增长。<details>
<summary>Abstract</summary>
In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information.   In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）模型倒逼（MI）攻击中，敌方利用对ML模型的访问来推断和重建私有训练数据。在白盒和黑盒设置中，敌方有访问完整的模型或模型的软输出。然而，有很少的研究在最复杂但实际上最重要的设置中：标签只MI攻击，敌方只有访问模型预测的标签（硬标签）而没有信任分数也没有其他模型信息。在这种情况下，我们提出了一种新的方法：LOKT。我们的思路是通过目标模型的知识传递到临时模型中。然后，我们可以通过这些临时模型使用高级白盒攻击。我们提出了基于生成模型的知识传递，并引入了一种新的模型：Target model-assisted ACGAN（T-ACGAN）。我们的方法将复杂的标签只MI转化为更易于处理的白盒设置。我们提供分析支持，表明我们的方法可以使用临时模型来实现有效的MI攻击。我们的实验表明，我们的方法在所有MI benchmark中比现有最佳状态的标签只MI攻击方法高效性超过15%。此外，我们的方法与查询预算相比也很有优势。我们的研究显示，即使只暴露硬标签，ML模型仍然面临着严重的隐私威胁。我们的研究也表明，随着ML模型的普及，隐私威胁的发展将变得更加严重。我们的代码、 demo、模型和重建数据可以在我们的项目页面上获得：https://ngoc-nguyen-0.github.io/lokt/Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="On-Measuring-Fairness-in-Generative-Models"><a href="#On-Measuring-Fairness-in-Generative-Models" class="headerlink" title="On Measuring Fairness in Generative Models"></a>On Measuring Fairness in Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19297">http://arxiv.org/abs/2310.19297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher T. H. Teo, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>for: 本研究旨在提供一种准确的公平度评估方法，以便评估公平的生成模型。</li>
<li>methods: 本研究提出了一种新的公平度评估框架，称为CLassifier Error-Aware Measurement（CLEAM），它使用统计模型来补做敏感特征分类器的不准确性，从而减少了评估公平度时的错误。</li>
<li>results: 本研究表明，使用CLEAM评估StyleGAN2模型的gender公平度时，错误率从4.98%降低到0.62%，而且这种改进只需要少量的额外开销。此外，本研究还发现了一些重要的文本生成模型和GANs中的偏见问题，这些问题需要进一步的研究和解决。<details>
<summary>Abstract</summary>
Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on fairness measurement, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., 4.98% $\rightarrow$ 0.62% for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. Code and more resources: https://sutd-visual-computing-group.github.io/CLEAM/.
</details>
<details>
<summary>摘要</summary>
最近，关于公平生成模型的兴趣增长。在这项工作中，我们进行了第一次深入研究公平度量测试，这是评估公平生成模型进步的关键组件。我们做出了三个贡献：首先，我们进行了一项研究，发现现有的公平度量测试框架具有较大的测量误差，即使使用高度准确的敏感特征（SA）分类器也是如此。这些发现质量上的公平改进的成果into question。第二，为解决这个问题，我们提议了一种新的公平度量测试框架：CLassifier Error-Aware Measurement（CLEAM）。CLEAM使用统计模型来考虑SA分类器的不准确性，从而减少测量误差。我们的提议的CLEAM可以减少测量误差，例如，StyleGAN2中的性别从4.98%降至0.62%。此外，CLEAM可以实现这一目标，而无需增加额外的负担。第三，我们使用CLEAM测量了文本到图像生成器和GANs中的公平性，发现这些模型存在较大的偏见，这引发了对其应用的担忧。代码和更多资源可以在以下链接中找到：https://sutd-visual-computing-group.github.io/CLEAM/.
</details></li>
</ul>
<hr>
<h2 id="FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound"><a href="#FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound" class="headerlink" title="FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound"></a>FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19293">http://arxiv.org/abs/2310.19293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyu Chen, Xin Yang, Yuhao Huang, Wenlong Shi, Yan Cao, Mingyuan Luo, Xindi Hu, Lei Zhue, Lequan Yu, Kejuan Yue, Yuanji Zhang, Yi Xiong, Dong Ni, Weijun Huang</li>
<li>For: 提供全面的胎儿信息，包括胎儿的各个解剖结构的连接。* Methods: 提出了一种新的3D胎儿姿态估计框架（叫做FetusMapV2），以解决胎儿US图像中的多种挑战，如图像质量不佳、限制GPU内存等。* Results: 对比其他强大竞争者，该方法在一大量的胎儿US数据集上表现出优异的表达效果。<details>
<summary>Abstract</summary>
Fetal pose estimation in 3D ultrasound (US) involves identifying a set of associated fetal anatomical landmarks. Its primary objective is to provide comprehensive information about the fetus through landmark connections, thus benefiting various critical applications, such as biometric measurements, plane localization, and fetal movement monitoring. However, accurately estimating the 3D fetal pose in US volume has several challenges, including poor image quality, limited GPU memory for tackling high dimensional data, symmetrical or ambiguous anatomical structures, and considerable variations in fetal poses. In this study, we propose a novel 3D fetal pose estimation framework (called FetusMapV2) to overcome the above challenges. Our contribution is three-fold. First, we propose a heuristic scheme that explores the complementary network structure-unconstrained and activation-unreserved GPU memory management approaches, which can enlarge the input image resolution for better results under limited GPU memory. Second, we design a novel Pair Loss to mitigate confusion caused by symmetrical and similar anatomical structures. It separates the hidden classification task from the landmark localization task and thus progressively eases model learning. Last, we propose a shape priors-based self-supervised learning by selecting the relatively stable landmarks to refine the pose online. Extensive experiments and diverse applications on a large-scale fetal US dataset including 1000 volumes with 22 landmarks per volume demonstrate that our method outperforms other strong competitors.
</details>
<details>
<summary>摘要</summary>
《胎儿pose估计在3D超声成像中存在许多挑战，包括图像质量不佳、限制性的GPU内存、相似的解剖结构和胎儿姿态的巨大变化。本研究提出了一种新的3D胎儿pose估计框架（称为FetusMapV2），以解决以上挑战。我们的贡献包括以下三个方面：一、我们提出了一种启发性的网络结构方法，包括不受限制的GPU内存管理方法和活动不受限制的网络结构方法，可以在有限的GPU内存条件下提高输入图像的分辨率，从而获得更好的结果。二、我们设计了一种新的对应损失函数，用于减少由相似的解剖结构和同一类型的损失函数所引起的混淆。该损失函数将隐藏的分类任务和准确的地标任务分离开来，从而逐渐促进模型的学习。三、我们提出了一种基于形态约束的自动学习方法，通过选择稳定的地标来进行在线的姿态约束。广泛的实验和多种应用在1000个3D胎儿超声数据集上，表明我们的方法在其他强大竞争对手之上表现出色。》
</details></li>
</ul>
<hr>
<h2 id="EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution"><a href="#EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution" class="headerlink" title="EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution"></a>EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19288">http://arxiv.org/abs/2310.19288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xy-boy/ediffsr">https://github.com/xy-boy/ediffsr</a></li>
<li>paper_authors: Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, Xianyu Jin, Liangpei Zhang</li>
<li>for: 这篇论文的目的是提出一种高效的遥感图像超分辨（SR）方法，以提高遥感图像的视觉质量。</li>
<li>methods: 该方法使用了Diffusion Probabilistic Model（DPM）和Efficient Activation Network（EANet）来实现高效的噪声预测和图像重建。DPM可以生成视觉愉悦的图像，而EANet可以提高噪声预测性能。此外，该方法还提出了一种Conditional Prior Enhancement Module（CPEM），可以帮助提取更多的有用信息，以提高SR的准确性。</li>
<li>results: 对四个遥感图像 dataset 进行了广泛的实验，结果表明，EDiffSR可以在模拟和真实遥感图像上，Restore visual-pleasant images， both quantitatively and qualitatively。<details>
<summary>Abstract</summary>
Recently, convolutional networks have achieved remarkable development in remote sensing image Super-Resoltuion (SR) by minimizing the regression objectives, e.g., MSE loss. However, despite achieving impressive performance, these methods often suffer from poor visual quality with over-smooth issues. Generative adversarial networks have the potential to infer intricate details, but they are easy to collapse, resulting in undesirable artifacts. To mitigate these issues, in this paper, we first introduce Diffusion Probabilistic Model (DPM) for efficient remote sensing image SR, dubbed EDiffSR. EDiffSR is easy to train and maintains the merits of DPM in generating perceptual-pleasant images. Specifically, different from previous works using heavy UNet for noise prediction, we develop an Efficient Activation Network (EANet) to achieve favorable noise prediction performance by simplified channel attention and simple gate operation, which dramatically reduces the computational budget. Moreover, to introduce more valuable prior knowledge into the proposed EDiffSR, a practical Conditional Prior Enhancement Module (CPEM) is developed to help extract an enriched condition. Unlike most DPM-based SR models that directly generate conditions by amplifying LR images, the proposed CPEM helps to retain more informative cues for accurate SR. Extensive experiments on four remote sensing datasets demonstrate that EDiffSR can restore visual-pleasant images on simulated and real-world remote sensing images, both quantitatively and qualitatively. The code of EDiffSR will be available at https://github.com/XY-boy/EDiffSR
</details>
<details>
<summary>摘要</summary>
最近，卷积网络在遥感图像超分辨 (SR) 领域取得了显著的发展，通过最小化差分目标函数，如 Mean Squared Error (MSE) 损失函数。然而，这些方法经常受到过度平滑问题的困扰，导致图像质量不佳。生成对抗网络可以推测细节，但它们容易塌陷，导致不良的artefacts。为了缓解这些问题，本文提出了Diffusion Probabilistic Model (DPM)  для高效的遥感图像 SR，称为 EDiffSR。EDiffSR 易于训练和维护，并保留 DPM 生成的感知性图像。与前一些使用重量积网络进行噪声预测的方法不同，我们开发了一个高效的活化网络 (EANet)，通过简单的通道注意力和简单的闸机操作，可以获得优秀的噪声预测性能。此外，为了将更多的有价值前景知识引入到提议的 EDiffSR 中，我们开发了一个实用的 Conditional Prior Enhancement Module (CPEM)，以帮助提取更丰富的condition。与大多数DPM-based SR模型直接将LR图像扩充为condition，不同的是，我们的 CPEM 可以帮助保留更多的信息 clue  для准确的 SR。我们在四个遥感数据集上进行了广泛的实验，显示EDiffSR可以在模拟和实际遥感图像上还原高质量的视觉 pleasant 图像， both quantitatively and qualitatively。代码将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition"><a href="#Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition" class="headerlink" title="Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition"></a>Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19258">http://arxiv.org/abs/2310.19258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Shi, Yanyuan Qiao, Qi Wu, Lingqiao Liu, Feras Dayoub</li>
<li>for: 这个研究是为了提高移动机器人中的实时物体检测，并且在无法控制的环境中进行适应。</li>
<li>methods: 这篇研究使用了线上无标本领域适应（O-SFDA），并且提出了一种新的方法来优化这种适应。这个方法会将最有价的无标本数据包含在线上训练过程中。</li>
<li>results: 实验结果显示，这种方法可以超越现有的州际顶尖O-SFDA技术，并且在实际应用中表现出色。<details>
<summary>Abstract</summary>
Effective object detection in mobile robots is challenged by deployment in diverse and unfamiliar environments. Online Source-Free Domain Adaptation (O-SFDA) offers real-time model adaptation using a stream of unlabeled data from a target domain. However, not all captured frames in mobile robotics contain information that is beneficial for adaptation, particularly when there is a strong domain shift. This paper introduces a novel approach to enhance O-SFDA for adaptive object detection in mobile robots via unsupervised data acquisition. Our methodology prioritizes the most informative unlabeled samples for inclusion in the online training process. Empirical evaluation on a real-world dataset reveals that our method outperforms existing state-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised data acquisition for improving adaptive object detection in mobile robots.
</details>
<details>
<summary>摘要</summary>
“智能物体检测在移动机器人中是面临挑战的，特别是在不熟悉的环境中部署。在线源自自适应（O-SFDA）可以在目标领域上进行实时模型适应，但不所有捕捉到的帧都含有有用的信息，特别是当域名shift强大时。这篇论文介绍了一种新的方法，通过不supervised数据收集来增强O-SFDA的适应性。我们的方法会优先选择目标领域中最有用的无标签样本进行在线训练。实验表明，我们的方法在实际 datasets 上表现出了与现有状态的先进技术相比的优势，这说明了不supervised数据收集的可行性以及适应性。”Note that Simplified Chinese is a common writing system used in mainland China, and it may differ from Traditional Chinese, which is used in other regions such as Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture"><a href="#A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture" class="headerlink" title="A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture"></a>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19257">http://arxiv.org/abs/2310.19257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Shen, Yunhan Zhao, Nahyun Kwon, Jeeeun Kim, Yanan Li, Shu Kong</li>
<li>for:  This paper aims to address the long-standing problem of instance detection (InsDet) in robotics and computer vision, which involves detecting object instances (predefined by visual examples) in a cluttered scene.</li>
<li>methods:  The paper introduces a new InsDet dataset and protocol, which includes a realistic setup for training and testing, as well as the release of a real-world database with multi-view capture of 100 object instances and high-resolution testing images.</li>
<li>results:  The paper extensively studies baseline methods for InsDet on the new dataset and finds that using an off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving &gt;10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).<details>
<summary>Abstract</summary>
Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).
</details>
<details>
<summary>摘要</summary>
Instance detection (InsDet) 是一个长期存在的问题在机器人和计算机视觉领域，旨在在拥挤的场景中检测预先定义的对象实例。尽管其实际意义很大，但它的发展受到了对象检测的遮挡，这是因为当前的 InsDet 数据集都很小。例如，流行的 InsDet 数据集 GMU (发布于 2016 年) 只有 23 个实例，相比 COCO (80 个类)，一个很好的对象检测数据集，发布于 2014 年。我们被激励提出一个新的 InsDet 数据集和协议。首先，我们定义了实际的 InsDet 设置：训练数据包括多视图实例捕捉，以及包含多种场景图像，以便通过贴上实例图像并自由地标注盒子来生成训练图像。第二，我们发布了一个真实世界数据库，包含 100 个对象实例的多视图捕捉，以及高分辨率 (6k x 8k) 测试图像。第三，我们进行了广泛的基线方法研究，分析了它们的性能，并提出了未来工作。有些奇异的是，使用开源的无类型分割模型 (Segment Anything Model, SAM) 和自然学习的特征表示 DINOv2 可以达到 >10 AP 更高的性能，比抽象的 InsDet 模型（例如 FasterRCNN 和 RetinaNet）的结束到练项训练的模型。
</details></li>
</ul>
<hr>
<h2 id="There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation"><a href="#There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation" class="headerlink" title="There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation"></a>There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19231">http://arxiv.org/abs/2310.19231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Schmitt, Seyed Ali Ahmadi, Yonghao Xu, Gulsen Taskin, Ujjwal Verma, Francescopaolo Sica, Ronny Hansch</li>
<li>for: 本文旨在强调EARTH OBSERVATION（EO）领域中机器学习 datasets的重要性，并将这些数据集作为AI应用的基础进行探讨。</li>
<li>methods: 本文根据历史发展和现有资源，对EARTH OBSERVATION领域的机器学习数据集进行了概述，并提出了未来发展的视角。</li>
<li>results: 本文希望通过强调EARTH OBSERVATION领域数据集的特点，了解这些数据集的特殊性，并认为这些特点是我们领域的核心技能之一。<details>
<summary>Abstract</summary>
Carefully curated and annotated datasets are the foundation of machine learning, with particularly data-hungry deep neural networks forming the core of what is often called Artificial Intelligence (AI). Due to the massive success of deep learning applied to Earth Observation (EO) problems, the focus of the community has been largely on the development of ever-more sophisticated deep neural network architectures and training strategies largely ignoring the overall importance of datasets. For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for Earth observation. With this article, we want to change the perspective and put machine learning datasets dedicated to Earth observation data and applications into the spotlight. Based on a review of the historical developments, currently available resources are described and a perspective for future developments is formed. We hope to contribute to an understanding that the nature of our data is what distinguishes the Earth observation community from many other communities that apply deep learning techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline.
</details>
<details>
<summary>摘要</summary>
《深入探讨Machine Learning数据集的重要性》Introduction: Machine learning（机器学习）是现代计算机科学中的一个重要分支，它的应用领域包括地球观测（Earth Observation，EO）等。在深度学习（deep learning）的应用中，特别是在地球观测领域，数据集的精心挑选和注释已成为机器学习的基础。然而，由于社区的焦点主要集中在深度学习模型的设计和训练策略上，数据集的重要性被忽略了。本文旨在改变这种观点，将地球观测领域的机器学习数据集推到前线，并对历史发展、当前资源和未来发展提出了一种视角。我们希望通过这篇文章，让读者理解地球观测领域的数据特点，并认识到我们的数据是与其他图像数据领域不同的。Historical Background: Earth observation（EO）是指通过卫星、飞机、气象雷达等设备对地球的观测和记录。EO数据的应用领域包括气候变化、自然灾害、资源管理、农业等。随着深度学习技术的发展，EO领域的应用也逐渐增多。在过去的几年中，许多专门的任务数据集被创建，但这些数据集却被许多关于AI的评论文章忽略。Current Resources: 目前，EO领域的机器学习数据集已经具有了很好的资源。以下是一些常用的数据集：1. 地球观测数据集（EO-1）：由中国国家地面观测中心提供，包括卫星图像、附近观测数据等。2. 地球观测数据集（EO-2）：由美国地球观测署提供，包括卫星图像、附近观测数据等。3. 气候变化数据集（Climate Change）：由世界 Meteorological Organization（WMO）提供，包括卫星图像、气象数据等。Future Perspective: 未来，EO领域的机器学习数据集将继续增长和改进。随着深度学习技术的发展，EO领域的应用将变得更加广泛和复杂。在这种情况下，数据集的质量和可靠性将成为非常重要的瓶颈。我们需要继续探讨和研究EO领域的数据特点，以提高机器学习模型的性能和可靠性。同时，我们还需要与其他领域的专家合作，以推动EO领域的发展。Conclusion: 在这篇文章中，我们提出了一种将地球观测领域的机器学习数据集推到前线的视角。我们希望通过这篇文章，让读者理解地球观测领域的数据特点，并认识到我们的数据是与其他图像数据领域不同的。未来，我们需要继续探讨和研究EO领域的数据特点，以提高机器学习模型的性能和可靠性。同时，我们还需要与其他领域的专家合作，以推动EO领域的发展。
</details></li>
</ul>
<hr>
<h2 id="CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging"><a href="#CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging" class="headerlink" title="CHAMMI: A benchmark for channel-adaptive models in microscopy imaging"></a>CHAMMI: A benchmark for channel-adaptive models in microscopy imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19224">http://arxiv.org/abs/2310.19224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/channel_adaptive_models">https://github.com/chaudatascience/channel_adaptive_models</a></li>
<li>paper_authors: Zitong Chen, Chau Pham, Siqi Wang, Michael Doron, Nikita Moshkov, Bryan A. Plummer, Juan C. Caicedo</li>
<li>for: This paper is written for researchers and developers working on microscopy imaging and neural networks. The paper aims to address the challenge of varying channel numbers in microscopy images and the lack of reusable models for different settings.</li>
<li>methods: The paper proposes a benchmark for investigating channel-adaptive models in microscopy imaging, which includes a dataset of varied-channel single-cell images and a biologically relevant evaluation framework. The authors also adapt several existing techniques to create channel-adaptive models and compare their performance to fixed-channel, baseline models.</li>
<li>results: The paper finds that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. The authors contribute a curated dataset and an evaluation API to facilitate objective comparisons in future research and applications.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为微scopic imaging和神经网络研究人员写的，目的是解决微scopic图像中通道数量不固定的问题，并提供可重用的模型 для不同的设置。</li>
<li>methods: 论文提出了一个用于研究可适应通道数量的微scopic imaging模型的 benchmark，包括一个包含不同通道数量单元细胞图像的数据集和一个生物学 relevance 的评估框架。作者还适应了一些现有的技术，创建了可适应通道数量的模型，并与基线模型进行比较。</li>
<li>results: 论文发现，可适应通道数量的模型可以更好地适应不同的任务和数据集，并且可以减少计算成本。作者提供了一个可搜索的数据集和评估 API，以便未来的研究和应用中进行对比。<details>
<summary>Abstract</summary>
Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications.
</details>
<details>
<summary>摘要</summary>
大多数神经网络假设输入图像具有固定数量的通道（三个 für RGB 图像）。然而，在微scopic 中有很多情况下，通道的数量可能会变化，例如微scopic 图像中的通道数量可能会根据设备和实验目标而变化。然而，到目前为止没有一个系统性的尝试创建和评估可以快速适应通道数量和类型的神经网络。因此，训练的模型通常只能在特定的研究中使用，而不能在其他微scopic 设置中 reuse。在这篇论文中，我们提供了一个用于调查可适应通道的模型在微scopic 成像中的 benchmark，包括：1）一个包含不同通道单元细胞图像的 dataset，和2）一个生物学上相关的评估框架。此外，我们还采用了一些现有的技术，以创建可适应通道的模型，并将其比较基线模型的性能。我们发现可适应通道模型可以更好地对应用于尖端任务，并且可以减少计算成本。我们提供了一个已经检索的 dataset（https://doi.org/10.5281/zenodo.7988357）和一个评估 API（https://github.com/broadinstitute/MorphEm.git），以便在未来的研究和应用中进行 объектив的比较。
</details></li>
</ul>
<hr>
<h2 id="Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images"><a href="#Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images" class="headerlink" title="Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images"></a>Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19223">http://arxiv.org/abs/2310.19223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong Li</li>
<li>for: 本研究旨在使用低成本的RGB图像检测抓取姿势，而传统方法则依赖深度感知器。</li>
<li>methods: 本研究提出了一种模块化学习网络，结合抓取检测和语义分割，针对具有平行板抓取器的机器人。网络不仅可以识别可抓取物体，还可以结合先前的抓取分析和语义分割，进一步提高抓取检测精度。</li>
<li>results: 实验和评估表明，我们提出的方法可以准确地检测抓取姿势，并且可以适应受扰和模糊的视觉。我们的设计具有可重复性和灵活性，能够应用于实际场景中。<details>
<summary>Abstract</summary>
While traditional methods relies on depth sensors, the current trend leans towards utilizing cost-effective RGB images, despite their absence of depth cues. This paper introduces an interesting approach to detect grasping pose from a single RGB image. To this end, we propose a modular learning network augmented with grasp detection and semantic segmentation, tailored for robots equipped with parallel-plate grippers. Our network not only identifies graspable objects but also fuses prior grasp analyses with semantic segmentation, thereby boosting grasp detection precision. Significantly, our design exhibits resilience, adeptly handling blurred and noisy visuals. Key contributions encompass a trainable network for grasp detection from RGB images, a modular design facilitating feasible grasp implementation, and an architecture robust against common image distortions. We demonstrate the feasibility and accuracy of our proposed approach through practical experiments and evaluations.
</details>
<details>
<summary>摘要</summary>
而传统方法倚靠深度感知器，当前趋势却是利用便宜的RGB图像，尽管它们缺乏深度提示。这篇论文介绍了一种有趣的方法，用于从单个RGB图像中检测抓取姿势。为此，我们提议一种具有抓取检测和语义分割的模块学习网络，适用于配备平行板抓取机器人。我们的网络不仅可以识别可抓取物体，还可以结合先前的抓取分析和语义分割，因此提高了抓取检测精度。此外，我们的设计还能够处理模糊和噪声的视觉，从而提高了系统的稳定性和可靠性。我们的关键贡献包括一种可学习的RGB图像抓取检测网络、一种可 Module 化的设计，以及一种对常见图像扭曲的抗衰减设计。我们通过实验和评估证明了我们提出的方法的可行性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Category-Discovery-with-Clustering-Assignment-Consistency"><a href="#Generalized-Category-Discovery-with-Clustering-Assignment-Consistency" class="headerlink" title="Generalized Category Discovery with Clustering Assignment Consistency"></a>Generalized Category Discovery with Clustering Assignment Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19210">http://arxiv.org/abs/2310.19210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangli Yang, Xinglin Pan, Irwin King, Zenglin Xu</li>
<li>for: 这篇论文的目的是提出一种基于openset任务的通用类发现方法，使得无标签样本集中的无标签样本可以通过从标签样本集中获取的信息进行自动分类。</li>
<li>methods: 该方法使用了协助学习和强化数据生成的方法，首先将样本经压缩变换后生成两个不同的视图，然后通过协同学习假设来学习一个具有一致性的表示学习策略，最后使用这些协同学习的准确性来构建一个原始稀有网络，并使用社区探测方法来获取分类结果和类数同时。</li>
<li>results: 实验表明，该方法在三个通用 benchmark 上达到了状态的前一个基eline，并在三个细化的视觉识别任务上达到了优秀的性能。尤其是在 ImageNet-100 数据集上，该方法在 \texttt{Novel} 和 \texttt{All} 类上超过了最佳基eline的15.5%和7.0%。<details>
<summary>Abstract</summary>
Generalized category discovery (GCD) is a recently proposed open-world task. Given a set of images consisting of labeled and unlabeled instances, the goal of GCD is to automatically cluster the unlabeled samples using information transferred from the labeled dataset. The unlabeled dataset comprises both known and novel classes. The main challenge is that unlabeled novel class samples and unlabeled known class samples are mixed together in the unlabeled dataset. To address the GCD without knowing the class number of unlabeled dataset, we propose a co-training-based framework that encourages clustering consistency. Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. Extensive experiments show that our method achieves state-of-the-art performance on three generic benchmarks and three fine-grained visual recognition datasets. Especially in the ImageNet-100 data set, our method significantly exceeds the best baseline by 15.5\% and 7.0\% on the \texttt{Novel} and \texttt{All} classes, respectively.
</details>
<details>
<summary>摘要</summary>
通用类发现（GCD）是一个最近提出的开放世界任务。给定一个包含标注和无标注实例的图像集合，GCD的目标是自动将无标注样本分组使用从标注集合中获取的信息。无标注集合包含已知和新类。主要挑战是无标注新类样本和已知类样本在无标注集合中混合在一起。为解决GCD而不知道无标注集合的类数，我们提出了基于协同学习的框架。 Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. 经验表明，我们的方法在三个通用权重折衔测试集上达到了状态之最好性能。特别是在ImageNet-100数据集上，我们的方法在\texttt{Novel}和\texttt{All}类上明显超过了最佳基准值，相差15.5%和7.0%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CV_2023_10_30/" data-id="clogxf3nn00jy5xra864a1yh2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/30/eess.AS_2023_10_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-10-30
        
      </div>
    </a>
  
  
    <a href="/2023/10/30/cs.AI_2023_10_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-10-30</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
