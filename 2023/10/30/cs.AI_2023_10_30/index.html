
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-10-30 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20081 repo_url: None paper_authors: Chris Richardson, Yao Zhang, Kel">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-10-30">
<meta property="og:url" content="https://nullscc.github.io/2023/10/30/cs.AI_2023_10_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20081 repo_url: None paper_authors: Chris Richardson, Yao Zhang, Kel">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-30T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.227Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.AI_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T12:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-10-30
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models"><a href="#Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models" class="headerlink" title="Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models"></a>Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20081">http://arxiv.org/abs/2310.20081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, Abhinav Sethy</li>
<li>for: 提高自然语言处理（NLP）系统的用户体验，特别是通过大语言模型（LLM）来更好地个性化用户体验。</li>
<li>methods: 使用语言模型提取过去用户数据，并将其作为下游任务的提示进行个性化。</li>
<li>results:  experiments show 我们的方法可以在实际环境下，即使有时间和成本限制，也能够具有与抽取方法相当或更好的表现，并且可以减少75%的用户数据 Retrieval。<details>
<summary>Abstract</summary>
Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. The summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. Experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. We demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details>
<details>
<summary>摘要</summary>
personalization, tailoring a system to individual users, is a crucial aspect of user experience with natural language processing (NLP) systems. with the emergence of large language models (LLMs), a key question is how to leverage these models to better personalize user experiences. to personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. however, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. to overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. the summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. we demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details></li>
</ul>
<hr>
<h2 id="FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space"><a href="#FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space" class="headerlink" title="FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space"></a>FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20071">http://arxiv.org/abs/2310.20071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, Tarek Abdelzaher</li>
<li>for: 提出了一种新的对比学习框架FOCAL，用于从多Modal时间序列感知信号中提取全面特征，通过无监督训练。</li>
<li>methods: FOCAL使用多Modal时间序列中的特征编码，并使用modal匹配目标和变换不变目标来提取共同特征和专用特征。同时，它还引入了时间结构约束，以保证模式特征之间的距离关系。</li>
<li>results: FOCAL在四个多Modal感知数据集上进行了广泛的评估，并与现有的基eline进行了比较。结果显示，FOCAL在下游任务中具有明显的优势，具有较高的准确率和较低的损失值。<details>
<summary>Abstract</summary>
This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details>
<details>
<summary>摘要</summary>
First, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective, while the private space extracts modality-exclusive information through a transformation-invariant objective.Second, it introduces a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. This ensures that the model learns to capture the temporal relationships between samples.The proposed framework is evaluated on four multimodal sensing datasets with two backbone encoders and two classifiers. The results show that FOCAL consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details></li>
</ul>
<hr>
<h2 id="Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks"><a href="#Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks" class="headerlink" title="Vignat: Vulnerability identification by learning code semantics via graph attention networks"></a>Vignat: Vulnerability identification by learning code semantics via graph attention networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20067">http://arxiv.org/abs/2310.20067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Liu, Gail Kaiser</li>
<li>for: 本研究旨在提高软件安全性，通过自适应学习图级别Semantic Representation来发现漏洞。</li>
<li>methods: 我们使用Code Property Graphs (CPGs)来表示代码，并使用Graph Attention Networks (GATs)进行漏洞检测。</li>
<li>results: 我们在可靠的 datasets 上实现了 $57.38%$ 的准确率，并且可以获得漏洞模式的可读性。<details>
<summary>Abstract</summary>
Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transliteration: Vulnerability zhìyè shì yòu zhìyè zhòngjì xìtiě de yìqie zhòngjì zhìyè shì yòu xìtiě zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì y
</details></li>
</ul>
<hr>
<h2 id="Concept-Alignment-as-a-Prerequisite-for-Value-Alignment"><a href="#Concept-Alignment-as-a-Prerequisite-for-Value-Alignment" class="headerlink" title="Concept Alignment as a Prerequisite for Value Alignment"></a>Concept Alignment as a Prerequisite for Value Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20059">http://arxiv.org/abs/2310.20059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunayana Rane, Mark Ho, Ilia Sucholutsky, Thomas L. Griffiths</li>
<li>for: 本研究旨在建立AI系统，以安全可靠的方式与人类交互。</li>
<li>methods: 本研究使用 inverse reinforcement learning Setting 进行形式化分析，并证明了概念Alignment 是值Alignment 的必要前提。</li>
<li>results: 人类参与者在实验中证明了，当agent acts intentionally时，人类会根据agent使用的概念来进行合理的思维。<details>
<summary>Abstract</summary>
Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>值Alignment是AI系统与人类之间安全、可靠交互的关键。然而，人类的价值观与可能理解和评估世界的概念相关。因此，概念Alignment是价值Alignment的前提——代理需要与人类的情况表示相对应才能成功地Alignment其价值。我们在 inverse reinforcement learning  Setting formally analyze the concept alignment problem, show that neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. In addition, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.中文简体版
</details></li>
</ul>
<hr>
<h2 id="Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning"><a href="#Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning" class="headerlink" title="Constrained Hierarchical Monte Carlo Belief-State Planning"></a>Constrained Hierarchical Monte Carlo Belief-State Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20054">http://arxiv.org/abs/2310.20054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arec Jamgochian, Hugo Buurmeijer, Kyle H. Wray, Anthony Corso, Mykel J. Kochenderfer</li>
<li>for: 这篇论文目的是为了解决受限制的部分可观察Markov问题（CPOMDP）中的最佳规划问题，并在不同的状态和转换 uncertainty 下保持安全的规划。</li>
<li>methods: 这篇论文使用的方法是将问题分解为较低层的控制问题，使用高层的动作原则（options）来进行搜寻。</li>
<li>results: 这篇论文的结果显示，如果将基本的选项控制器定义为满足指派的紧张预算，那么COBeTS就可以确保满足紧张预算任何时候。否则，COBeTS将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现runtime safety。<details>
<summary>Abstract</summary>
Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot.
</details>
<details>
<summary>摘要</summary>
最佳计划在受限 partially observable Markov decision process (CPOMDP) 中最大化 reward 目标，同时满足硬件成本限制，广泛应用于安全观察下的规划。然而，在大型或连续问题领域中的线上 CPOMDP 规划具有极高的问题难度。在许多大型机器人领域中，层次分解可以简化规划，使用工具 для low-level control 给 high-level action primitives（选项）。我们介绍 Constrained Options Belief Tree Search (COBeTS)，以利用这个层次，将线上搜寻基于 CPOMDP 规划 scales 到大型机器人问题领域。我们证明，如果单元选项控制器是将任务分配到硬件成本预算，COBeTS 就一定会满足限制。否则，COBeTS 将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现 runtime 安全。我们在 Several safety-critical, constrained partially observable robotic domains 中评估 COBeTS，结果显示它可以在连续 CPOMDP 中成功规划，而非层次基于的基底不能。
</details></li>
</ul>
<hr>
<h2 id="Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning"><a href="#Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning" class="headerlink" title="Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning"></a>Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20052">http://arxiv.org/abs/2310.20052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachyonicclock/surprisenet-cikm-23">https://github.com/tachyonicclock/surprisenet-cikm-23</a></li>
<li>paper_authors: Anton Lee, Yaqian Zhang, Heitor Murilo Gomes, Albert Bifet, Bernhard Pfahringer</li>
<li>for: 这种研究旨在解决人工智能网络在不断学习中遇到的悬峰性干扰和分类境界知识的问题。</li>
<li>methods: 这种方法使用参数隔离方法和基于异常检测的自适应器来解决悬峰性干扰，并且不依赖于图像特定的逻辑假设。</li>
<li>results: 实验表明，SurpriseNet在传统视觉不断学习标准准则上表现出色，以及在结构化数据集上。源代码可以在<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.8247906%E5%92%8Chttps://github.com/tachyonicClock/SurpriseNet-CIKM-23%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://doi.org/10.5281/zenodo.8247906和https://github.com/tachyonicClock/SurpriseNet-CIKM-23中下载。</a><details>
<summary>Abstract</summary>
Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning "cross-task knowledge," where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is "replay," where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. Source code made available at https://doi.org/10.5281/zenodo.8247906 and https://github.com/tachyonicClock/SurpriseNet-CIKM-23
</details>
<details>
<summary>摘要</summary>
In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. The source code is available at <https://doi.org/10.5281/zenodo.8247906> and <https://github.com/tachyonicClock/SurpriseNet-CIKM-23>.
</details></li>
</ul>
<hr>
<h2 id="SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics"><a href="#SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics" class="headerlink" title="SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics"></a>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20049">http://arxiv.org/abs/2310.20049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-kuenzli/surf-fluidsimulation">https://github.com/s-kuenzli/surf-fluidsimulation</a></li>
<li>paper_authors: Stefan Künzli, Florain Grötschla, Joël Mathys, Roger Wattenhofer</li>
<li>for: 这个论文是为了测试学习基于图的流体动力学模型的通用性而写的。</li>
<li>methods: 这篇论文使用了学习模型来模拟流体动力学，并使用了特定的数据集来测试和比较不同模型的通用性。</li>
<li>results: 研究发现，当模型需要适应不同的结构、分辨率或热动力学范围时，学习基于图的模型的通用性会受到影响。<details>
<summary>Abstract</summary>
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the-art graph-based models, yielding new insights into their generalization.
</details>
<details>
<summary>摘要</summary>
模拟流体动力学是设计和开发过程中的关键环节，从简单的阀门到复杂的液压机。准确解决下面的物理方程是计算昂贵的。因此，学习型解决方案，即模型在网格上的交互，在计算速度方面表现出了扎根。然而，这些模型是否真正理解下面的物理原理，并能泛化而不仅是 interpolate？泛化是一个关键的要求，以便建立一个通用的流体 simulator，可以适应不同的topology、分辨率或热动力范围。我们提出了 SURF，一个用于测试学习型流体 simulator 的泛化能力的benchmark。SURF包括各个数据集，并提供了特定的性能和泛化指标，用于评估和比较不同的模型。我们进行了大量的实验，证明了 SURF 的可靠性和有用性，并且对两种当前最佳的图像基本模型进行了深入的探索，从而获得了新的理解。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization"><a href="#Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization" class="headerlink" title="Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization"></a>Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20033">http://arxiv.org/abs/2310.20033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prakamya Mishra, Zonghai Yao, Shuwei Chen, Beining Wang, Rohan Mittal, Hong Yu<br>for: This paper aims to improve the factual consistency of clinical note summarization using ChatGPT to generate high-quality feedback data.methods: The authors use ChatGPT to generate edit feedback for improving the factual consistency of clinical note summarization.results: The authors evaluate the effectiveness of using GPT edits in human alignment, showing promising results in improving factual consistency.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT和LLaMA家族在捕捉和简化关键上下文信息方面表现出了异常的能力，并达到了当前最佳性能的概要任务。然而，社区对这些模型的幻觉问题仍然增长。LLM sometimes generates factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details></li>
</ul>
<hr>
<h2 id="GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models"><a href="#GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models" class="headerlink" title="GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models"></a>GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20025">http://arxiv.org/abs/2310.20025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mianchu Wang, Rui Yang, Xi Chen, Meng Fang</li>
<li>for: 学习通用策略从多种和多任务的离线数据集中。</li>
<li>methods: 使用两stage模型基于框架，包括预训练一个可以捕捉多种动作分布的先前策略，然后使用划算法与规划来生成假数据 для练化策略。</li>
<li>results: 在多种离线多目标摆动任务上达到了状态之 arts 性能，并且能够处理小数据预算和不同目标的扩展。<details>
<summary>Abstract</summary>
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by planning with learned models for both intra-trajectory and inter-trajectory goals. Through experimental evaluations, we demonstrate that GOPlan achieves state-of-the-art performance on various offline multi-goal manipulation tasks. Moreover, our results highlight the superior ability of GOPlan to handle small data budgets and generalize to OOD goals.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>Offline目标条件RL（GCRL）提供了一个可行的 парадиг，从多样化和多任务的离线数据集中学习通用策略。尽管最近有所进步，主要的离线GCRL方法受限于无模型的方法，这限制了它们对有限数据预算和未看过目标的泛化能力。在这项工作中，我们提出了一种新的两阶段模型基于框架，即目标条件的计划（GOPlan），包括（1）预训练一个能够捕捉多模态动作分布在多目标数据集中的先前策略；（2）使用计划方法和规划来生成假 trajectory 进行迭代优化策略。具体来说，先前策略基于带有优化的 Conditioned Generative Adversarial Networks（CGANs），可以快速分离出异常行为（OOD）的问题。为了进一步优化策略，计划方法生成了高质量的假数据，通过规划learned模型来实现内 trajectory和间 trajectory的目标。经过实验评估，我们表明 GOPlan 可以在多个离线多目标机械处理任务中 дости得状态之Art的表现。此外，我们的结果还 highlight了 GOPlan 对小数据预算和 OOD 目标的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach"><a href="#Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach" class="headerlink" title="Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach"></a>Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20024">http://arxiv.org/abs/2310.20024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Macktoobian, Zhan Shu, Qing Zhao</li>
<li>for: 这个论文主要是为了预测无架 robot 网络中发生故障后网络的重建可能性。</li>
<li>methods: 本文使用了 Bayesian Gaussian mixture models 的二条通路数据驱动模型，透过两条不同的预测路径，预测网络中发生故障后网络的重建可能性。</li>
<li>results: 本文的结果显示，与文献中现有最佳策略相比，二条通路数据驱动模型能够成功地解决网络（不）可回复预测问题。<details>
<summary>Abstract</summary>
Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
</details>
<details>
<summary>摘要</summary>
FAULTS 发生在随机机器人网络中可能导致网络结构的不稳定，从而导致一些网络的分支断开。优化网络结构是在大规模随机机器人网络中实时进行的资源投入和时间consuming的任务。我们只应在缺陷发生后的概率超过了不可回复的概率时进行网络结构重新计算。我们将这个问题形式化为 binary 分类问题。然后，我们开发了基于极 bayesian Gaussian mixture 模型的两路数据驱动模型，该模型通过两个不同的预缺陷和后缺陷预测路径来预测一个典型问题的解决方案。结果显示，通过将这两个路径的预测结果融合，我们的模型在解决网络（不）可回复预测问题上具有显著的成功，比文献中最佳策略更高。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Feature-Attribution-for-Outliers"><a href="#Multiscale-Feature-Attribution-for-Outliers" class="headerlink" title="Multiscale Feature Attribution for Outliers"></a>Multiscale Feature Attribution for Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20012">http://arxiv.org/abs/2310.20012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeff Shen, Peter Melchior</li>
<li>for: 这个论文是为了解决自动异常点检测问题，即使数据量很大，也能够更快、更可重复地检测到异常点。</li>
<li>methods: 这篇论文提出了一种新的特征归因方法，即反多尺度遮盖方法，这种方法专门针对异常点，因为在异常点上我们知道的特征很少，模型性能可能也不太好。</li>
<li>results: 论文的实验结果表明，这种特征归因方法在检测银河谱pectra中的异常点时比较有 interpretable 性，比如alternative归因方法更好。<details>
<summary>Abstract</summary>
Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
</details>
<details>
<summary>摘要</summary>
使用机器学习技术可以自动找出大量数据中的异常数据点，比人工检查更快速和可重复。但是发现这些异常数据点后，我们就会问：哪些特征使这个输入异常？我们提出了一种新的特征归因方法，反向多Scale遮盲，特意为异常数据点设计，我们对这些异常数据点知之甚少，而且预期模型性能很差，因为异常测试数据可能超出了训练数据的范围。我们在银河谱spectra中检测到的异常数据点上应用了这种方法，并发现其结果比替代归因方法更易于理解。
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game"><a href="#Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game" class="headerlink" title="Evolutionary Tabletop Game Design: A Case Study in the Risk Game"></a>Evolutionary Tabletop Game Design: A Case Study in the Risk Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20008">http://arxiv.org/abs/2310.20008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lana Bertoldo Rossato, Leonardo Boaventura Bombardelli, Anderson Rocha Tavares</li>
<li>for: 这个论文旨在创造和评估桌面游戏，以提高现有游戏的创新和变化。</li>
<li>methods: 这篇论文使用了进化算法和自动游戏测试来创造和评估桌面游戏。</li>
<li>results: 这篇论文的结果表明，通过使用遗传算法和规则引入的智能游戏测试，可以创造出新的桌面游戏变体，比如卡牌游戏和地图游戏。这些变体的游戏时间 shorter，并且比原始游戏更具 equilibrio。但是，这种方法还有一些局限性，例如，在许多情况下，目标函数 Correctly pursued，但生成的游戏几乎是平庸的。<details>
<summary>Abstract</summary>
Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, maintaining the usual drama. We also identified limitations in the process, where, in many cases, where the objective function was correctly pursued, but the generated games were nearly trivial. This work paves the way towards promising research regarding the use of evolutionary game design beyond classic board games.
</details>
<details>
<summary>摘要</summary>
创造和评估游戏手动是一项艰难和劳动密集的任务。生成式内容创造可以帮助，但通常不能创造整个游戏。进化游戏设计，将进化算法与自动游戏测试结合，已经用于创造了一些简单的桌面游戏，但原始方法并不包括复杂的桌面游戏，如骰子、牌和地图。本工作提出了对桌面游戏的扩展，通过生成 variants of Risk，一款军事策略游戏，要求玩家征服地图区域以赢得。我们使用了遗传算法进化选择的参数，以及规则基于的智能客户端来测试游戏，以及多种质量标准来评估新生成的变化。我们的结果显示了创造了原版游戏的新变体，地图较小，比赛更短。此外，新变体具有更平衡的比赛，保持了正常的戏剧。我们还发现了过程中的限制，在许多情况下， objective function 正确追求，但生成的游戏几乎是无聊的。这项工作开启了对进化游戏设计在古典桌面游戏之外的探索。
</details></li>
</ul>
<hr>
<h2 id="Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning"><a href="#Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning" class="headerlink" title="Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning"></a>Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20007">http://arxiv.org/abs/2310.20007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmadreza Moradipari, Mohammad Pedramfar, Modjtaba Shokrian Zini, Vaneet Aggarwal</li>
<li>for: 这个论文是为了证明 Thompson Sampling 在强化学习中的首个 Bayesian  regret bound。</li>
<li>methods: 这个论文使用了一种简化学习问题的离散集合环境，并通过 posterior consistency 进行了精细的信息率分析。</li>
<li>results: 这个论文得到了时间不同束的强化学习问题中的上界，其上界为 $\widetilde{O}(H\sqrt{d_{l_1}T})$，其中 $H$ 是 episode length，$d_{l_1}$ 是环境空间的 Kolmogorov $l_1-$ 度量。<details>
<summary>Abstract</summary>
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明了决策者抽取（Thompson Sampling）在强化学习中的第一个悔弃 regret bounds。我们通过简化学习问题，使用离散的代理环境集，并对信息倍数进行细化分析，从而得到时间不同权重学习问题中的上界为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$是话语长度，$d_{l_1}$是环境空间的科尔莫戈罗夫-$l_1-$度量。然后，我们在各种设置下获得具体的$d_{l_1}$bounds，包括表格、线性和finite mixtures，并讨论了我们的结果如何超越现有的最佳成果。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek"><a href="#Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek" class="headerlink" title="Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?"></a>Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19990">http://arxiv.org/abs/2310.19990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankur Nath, Alan Kuhnle</li>
<li>for: 这个论文旨在探讨 combine neural networks with local search heuristics 在 combinatorial optimization 领域的实践中的问题。</li>
<li>methods: 这个论文使用的方法包括 Tabu Search 和 deep learning 等多种方法。</li>
<li>results: 研究发现，一个简单的学习基于 Tabu Search 的规则可以超越当前最佳学习规则，并且具有更高的性能和普适性。<details>
<summary>Abstract</summary>
In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.
</details>
<details>
<summary>摘要</summary>
Recently, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has shown promising results with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.Translation in Simplified Chinese:近些年来，将神经网络与本地搜索规则结合在一起已成为 combinatorial optimization 领域的 популяр趋势。尽管它们的计算需求相对较高，但这种方法在 minimal 的人工工程下已经展现出了扎心的成果。然而，我们在实证评估中发现了三个关键的限制。首先，有中等复杂度和弱基线的实例会增加评估学习基于方法的准确性问题。其次，缺乏抽象研究使得准确地归因改进到深度学习架构很困难。最后，学习基于分布的搜索规则的总体化仍未得到充分的探索。在这个研究中，我们通过对这些已知的限制进行全面的调查和分析，并 surprisingly 发现一种简单的学习基于 Tabu Search 的规则，在性能和总体化方面超越了当前的学习基于方法。我们的发现推翻了先前的假设，开 up 了未来研究和创新的潜在空间。
</details></li>
</ul>
<hr>
<h2 id="BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing"><a href="#BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing" class="headerlink" title="BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"></a>BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19975">http://arxiv.org/abs/2310.19975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu</li>
<li>for: 本研究旨在提高生物医学自然语言处理（BioNLP）领域中大语言模型（LLMs）的性能，通过特定任务的指令特有数据集（BioInstruct）进行调整。</li>
<li>methods: 本研究使用GPT-4语言模型生成了超过25,000个例子的自然语言指令数据集（BioInstruct），并通过这些指令进行LLMs的微调。</li>
<li>results: 通过BioInstruct数据集的微调，我们可以提高LLMs在BioNLP应用中的性能，包括信息抽取、问答和文本生成等。此外，我们还发现了多任务学习原则如何帮助指令的贡献。<details>
<summary>Abstract</summary>
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principles.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在许多自然语言处理（NLP）任务中获得了很大的成功。这是通过预训练LLMs在庞大数据量上并在特定领域进行调整而实现的。然而，只有很少的指令在生物医学领域发布。为解决这个问题，我们介绍了 BioInstruct，一个自定义任务特定的指令集合，包含超过25,000个示例。这个数据集是通过提示一个GPT-4语言模型三个种子样本的80个人类批准的指令来生成的。通过使用BioInstruct数据集进行训练LLMs，我们 hoping to 优化LLMs在生物医学自然语言处理（BioNLP）中的表现。我们在LLaMA LLMs（1&2, 7B&13B）上进行了指令调整，并对其进行了BioNLP应用程序的评估，包括信息提取、问答和文本生成。我们还评估了指令对模型性能的贡献，使用多任务学习原理。
</details></li>
</ul>
<hr>
<h2 id="ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design"><a href="#ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design" class="headerlink" title="ExPT: Synthetic Pretraining for Few-Shot Experimental Design"></a>ExPT: Synthetic Pretraining for Few-Shot Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19961">http://arxiv.org/abs/2310.19961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Sudhanshu Agrawal, Aditya Grover</li>
<li>for: 本研究目的是解决实验设计中的样本效率问题，因为现实中的实验评估需要耗费时间、金钱和安全成本。</li>
<li>methods: 本文使用一种名为Experiment Pretrained Transformers（ExPT）的基本模型，这是一种基于环境学习的减少样本数据集合的方法。</li>
<li>results: 研究表明，ExPT在减少样本数据集合的情况下可以达到更高的性能和普适性，并且在各种复杂的实验设计任务中表现出色。<details>
<summary>Abstract</summary>
Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.
</details>
<details>
<summary>摘要</summary>
实验设计是许多科学和工程领域的基本问题。在这个问题中，sample efficiency是非常重要，因为实验评估的时间、钱和安全成本都是非常高昂的。现有的方法都是靠活的数据收集或者有大量的过去实验标签数据来进行，这些方法在实际 scenarios 中是不实际的。在这个工作中，我们处理更加问题的设计问题，其中只有几个标签的输入设计和其对应的值是可用的。我们这个问题作为一个 conditional generation 任务，我们的模型将根据几个标签的例子和目标值来生成最佳的输入设计。为了实现这个目标，我们引入 Experiment Pretrained Transformers（ExPT），一个基于 transformer 神经网络的基础模型，它使用了一种新的组合方法，将 synthetic pretraining 与 in-context learning 相结合。在 ExPT 中，我们仅仅假设知道输入领域中的一个finite collection 的无标例子，并将 transformer 神经网络预训来优化这个领域上的多个无相关函数。无标预训 позволяет ExPT 在测试时以内容的方式适应任务，通过根据几个标签的目标值和目标值来获得候选的最佳设计。我们评估 ExPT 在几 shot 实验设计中的一般性和表现，并证明它在挑战性的领域中比现有的方法表现更好。请见 https://github.com/tung-nd/ExPT.git 的源代码。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges"><a href="#Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges" class="headerlink" title="Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges"></a>Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19957">http://arxiv.org/abs/2310.19957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Jiang</li>
<li>for: 本研究旨在探讨深度学习技术在 espacial 大数据领域中的应用，以及这些技术在处理不同类型的 espacial 大数据时的挑战和未来研究需求。</li>
<li>methods: 本研究使用了 Earth 图像大数据等多种 espacial 大数据，并应用了深度学习技术来解决各种陆地覆盖和陆地使用模型化任务。</li>
<li>results: 本研究描述了 espacial 大数据的特点和深度学习技术在这些数据上的应用，并提出了未来研究中需要解决的一些挑战。<details>
<summary>Abstract</summary>
With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.
</details>
<details>
<summary>摘要</summary>
With the advancements in GPS, remote sensing, and computational simulation, an enormous amount of spatiotemporal data is being collected at an increasing speed from various application domains, including Earth sciences, agriculture, smart cities, and public safety. This emerging geospatial and spatiotemporal big data, combined with recent advances in deep learning technologies, has opened up new opportunities to solve problems that were previously unsolvable. For example, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the unique characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.Here's the translation of the text in Traditional Chinese:随着GPS、远程感知和计算 simulated的进步，各个应用领域产生了巨量的空间时间数据，包括地球科学、农业、智能城市和公共安全。这些emerging geospatial和空间时间大数据，与最近的深度学习技术的进步，带来了新的机会，例如：将地球图像大数据用于多种土地覆盖和土地使用模型任务的对称基模型训练。海岸模型师可以将AI参数器训练为加速numerical simulations。然而， espaciotemporal big data的特有特征对深度学习技术提出了新的挑战。本 vision paper introduces various types of espaciotemporal big data, discusses new research opportunities in the realm of deep learning applied to espaciotemporal big data, lists the unique challenges, and identifies several future research needs.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Unscented-Autoencoders-for-Trajectory-Prediction"><a href="#Conditional-Unscented-Autoencoders-for-Trajectory-Prediction" class="headerlink" title="Conditional Unscented Autoencoders for Trajectory Prediction"></a>Conditional Unscented Autoencoders for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19944">http://arxiv.org/abs/2310.19944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschresearch/cuae-prediction">https://github.com/boschresearch/cuae-prediction</a></li>
<li>paper_authors: Faris Janjoš, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov, Andreas Zell, J. Marius Zöllner</li>
<li>for: 这篇论文的目的是挑战 \ac{CVAE} 中的一些关键 комponent，并提出改进方案以提高预测性能。</li>
<li>methods: 本论文使用了最近的 VAE 技术，包括不可应用 sampling 和更Structured mixture latent space，以及一种新的、可能更表达ive的推论方法。</li>
<li>results: 试验结果显示，我们的模型在 INTERACTION 预测 dataset 上表现出色，超过了现有的州检验标准，并在 CelebA  dataset 上进行图像模型化任务上也超过了基本的 vanilla CVAE。<details>
<summary>Abstract</summary>
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well as at the task of image modeling on the CelebA dataset, outperforming the baseline vanilla CVAE. Code is available at https://github.com/boschresearch/cuae-prediction.
</details>
<details>
<summary>摘要</summary>
《CVAE》是自驾报道预测领域中最广泛使用的模型之一。它捕捉了驾驶Context和其真实未来的关系，并将其转化为一个 probabilistic 的latent space，以生成预测。在这篇论文中，我们挑战了CVAE的关键组件。我们利用了最近的 VAE 的进步，CVAE 的基础，发现一种简单的改变抽样方法可以大幅提高性能。我们发现， deterministic 抽样（unscented sampling）可以更好地适应 trajectory prediction than potentially dangerous random sampling。我们还提供了其他改进，包括一种更结构化的混合幂space，以及一种可能更具表达力的CVAE inference方法。我们通过评估 INTERACTION prediction 数据集和 CelebA 图像模型任务，证明了我们的模型在各个领域中的广泛适用性，并超越了状态前的最佳性能。代码可以在 https://github.com/boschresearch/cuae-prediction 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient"><a href="#Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient" class="headerlink" title="Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?"></a>Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19936">http://arxiv.org/abs/2310.19936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cea-list/mt-detr">https://github.com/cea-list/mt-detr</a></li>
<li>paper_authors: Quentin Bouniot, Angélique Loesch, Romaric Audigier, Amaury Habrard</li>
<li>for: 这个研究是为了提出一种适用于现有最佳物体检测器 Deformable DETR 的几shot 和半supervised 学习设置的 semi-supervised 方法。</li>
<li>methods: 本研究使用了一个学生-教师架构，避免依赖学生模型中的敏感后处理 Pseudo-labels。</li>
<li>results: 在 COCO 和 Pascal VOC 的半supervised 物体检测 benchmark 上评估了我们的方法，与之前的方法比较，特别是当标签少时表现更好。我们认为我们的贡献开启了新的可能性，以适应类似的物体检测方法。<details>
<summary>Abstract</summary>
For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well.
</details>
<details>
<summary>摘要</summary>
特别是在特殊和紧张的下游任务，如物体检测，标注数据需要专家知识和成本很高，使得几步和半supervised模型变得非常吸引人。在几步设置下，我们发现基于转换器的物体检测器比基于 convolution的两stage模型在同等参数量下表现更好。然而，在 semi-supervised 设置下，它们不那么有效。在这篇论文中，我们提出了一种针对当前领先的物体检测器Deformable DETR在几步学习设置中使用学生-教师架构的半supervised方法。我们避免了依赖于敏感的后处理 pseudo-labels 生成于教师模型。我们在 COCO 和 Pascal VOC  semi-supervised 物体检测数据集上评估了我们的方法，并与之前的方法相比，它在标注稀缺的情况下表现更好。我们认为，我们的贡献打开了新的可能性，使得类似的物体检测方法在这种设置中也可以适应。
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms"><a href="#Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms" class="headerlink" title="Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms"></a>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19927">http://arxiv.org/abs/2310.19927</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agentification/rp_pgm">https://github.com/agentification/rp_pgm</a></li>
<li>paper_authors: Shenao Zhang, Boyi Liu, Zhaoran Wang, Tuo Zhao</li>
<li>for: 该 paper 是为了研究 model-based ReParameterization Policy Gradient Methods (RP PGMs) 在长期 reinforcement learning 问题中的应用和优化问题。</li>
<li>methods: 该 paper 使用了 theoretical 和 experimental 方法来研究 RP PGMs 的整体性和优化问题，并提出了spectral normalization 方法来缓解长模型拓展导致的潜在梯度变iance问题。</li>
<li>results: 实验结果表明，采用 spectral normalization 方法可以有效缓解梯度变iance问题，并且提高了 RP PGMs 的性能，与其他梯度估计器（如 likelihood Ratio 梯度估计器）相当或更高。<details>
<summary>Abstract</summary>
ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable("ReParameterization（RP）Policy Gradient Methods（PGMs）have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio（LR）gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.）Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents"><a href="#Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents" class="headerlink" title="Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents"></a>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19923">http://arxiv.org/abs/2310.19923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao</li>
<li>for: The paper is written for researchers and practitioners working on text embedding models, particularly those interested in developing models that can handle long documents.</li>
<li>methods: The paper introduces Jina Embeddings 2, an open-source text embedding model that can accommodate up to 8192 tokens, which is much longer than the conventional 512-token limit. The model uses a novel combination of techniques to achieve state-of-the-art performance on a range of embedding-related tasks.</li>
<li>results: The paper reports that Jina Embeddings 2 achieves performance on par with OpenAI’s proprietary ada-002 model on the MTEB benchmark, and that an extended context can enhance performance in tasks such as NarrativeQA.<details>
<summary>Abstract</summary>
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.   To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI's proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.
</details>
<details>
<summary>摘要</summary>
文本嵌入模型已经成为强大工具，可以将句子转换成固定大小的特征向量，捕捉 semantic 信息。而这些模型在信息检索、semantic 聚合和文本重新排序等任务中是必备的，但现有的大多数开源模型，特别是基于 BERT 的模型，在处理长文档时很难表现，通常会导致 truncation。为了解决这个挑战，我们介绍 Jina Embeddings 2，一个可以处理 Up to 8192 个字的开源文本嵌入模型。这个模型不仅在 MTEB 竞赛中表现出优于 Convention 512 个字的限制，还能够高效地处理长文档。 Jina Embeddings 2 不仅达到了一系列嵌入相关任务的状态 искусственный智能表现，还与 OpenAI 的专有 ada-002 模型匹配。此外，我们的实验表明，扩展上下文可以提高 NarrativeQA 等任务的表现。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records"><a href="#Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records" class="headerlink" title="Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records"></a>Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19917">http://arxiv.org/abs/2310.19917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou</li>
<li>for: 这项研究的目的是系统性地查询利用电子医疗记录（EHR）数据的人工智能（AI）应用中的偏见问题。</li>
<li>methods: 这项研究采用了遵循PRISMA指南的系统性回顾方法，从PubMed、Web of Science和IEEE检索到252篇文章，并对其中的20篇文章进行了最终审查。</li>
<li>results: 这项研究发现，在20篇文章中，5种主要的偏见问题得到了覆盖，即8篇文章分析了选择偏见问题，6篇文章分析了隐式偏见问题，5篇文章分析了干扰偏见问题，4篇文章分析了计量偏见问题，2篇文章分析了算法偏见问题。在偏见处理方法方面，10篇文章在模型开发阶段发现了偏见问题，而17篇文章提出了避免偏见问题的方法。<details>
<summary>Abstract</summary>
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten studies identified bias during model development, while seventeen presented methods to mitigate the bias. Discussion: Bias may infiltrate the AI application development process at various stages. Although this review discusses methods for addressing bias at different development stages, there is room for implementing additional effective approaches. Conclusion: Despite growing attention to bias in healthcare AI, research using EHR data on this topic is still limited. Detecting and mitigating AI bias with EHR data continues to pose challenges. Further research is needed to raise a standardized method that is generalizable and interpretable to detect, mitigate and evaluate bias in medical AI.
</details>
<details>
<summary>摘要</summary>
目的：人工智能（AI）应用使用电子健康纪录（EHR）得到了广泛的应用，但也会产生不同类型的偏见。本研究目的是系统性地对EHR数据使用AI研究中的偏见进行评估。方法：按照Preferred Reporting Items for Systematic Reviews and Meta-analyses（PRISMA）指南进行系统性综述。我们从2010年1月1日到2022年10月31日 retrievePubMed、Web of Science和Institute of Electrical and Electronics Engineers上的文章。我们定义了六种主要的偏见类型，并summarized现有的偏见处理方法。结果：从252篇文章中，20篇符合包含期刊的要求，进行了最终审查。八种偏见中，八种是选择偏见；六种是隐式偏见；五种是混合偏见；四种是测量偏见；两种是算法偏见。对偏见处理方法，十篇文章在模型开发阶段检测了偏见，而十七篇文章提出了避免偏见的方法。讨论：偏见可能在AI应用开发过程中各个阶段偏见。虽然这篇文章讨论了在不同阶段检测和避免偏见的方法，但还需要实施更多有效的方法。结论：尽管健康AI中的偏见问题已经得到了越来越多的关注，但使用EHR数据进行的研究还是有限的。检测和避免EHR数据上的AI偏见还需要继续进行更多的研究。为了提高AI医疗应用的标准化方法，需要采用一种通用、可读性高的方法来检测、避免和评估偏见。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Prototype-based-Graph-Information-Bottleneck"><a href="#Interpretable-Prototype-based-Graph-Information-Bottleneck" class="headerlink" title="Interpretable Prototype-based Graph Information Bottleneck"></a>Interpretable Prototype-based Graph Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19906">http://arxiv.org/abs/2310.19906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sang-woo-seo/pgib">https://github.com/sang-woo-seo/pgib</a></li>
<li>paper_authors: Sangwoo Seo, Sungwon Kim, Chanyoung Park</li>
<li>for: 这个论文的目的是提出一种可解释的图 нейрон网络（PGIB）框架，用于提高图 нейрон网络的解释性和性能。</li>
<li>methods: 这个论文使用了prototype学习和信息瓶颈框架，从输入图中提取关键子图，并通过这些关键子图来提供可解释的预测结果。</li>
<li>results: 对比其他状态速法，PGIB在预测性能和解释性两个方面均表现出色，并且通过质量分析得到了证明。<details>
<summary>Abstract</summary>
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details>
<details>
<summary>摘要</summary>
graph TD成功的图 neuronal networks (GNNs) 带来了理解它们的决策过程和提供对其预测的解释，这 hath led to explainable AI (XAI) 提供了透明的解释 для黑色 Box 模型。 在最近，使用 prototype 已成功地提高了模型的解释性，通过学习 prototype 来Imply training graphs that affect the prediction。然而，这些approaches 往往提供 prototype 中过度的信息，从整个图中获取信息，导致遗漏关键子结构或包含无关信息，这可能会限制模型在下游任务中的解释性和性能。在这项工作中，我们提出了一种新的解释 GNN 框架，called interpretable Prototype-based Graph Information Bottleneck (PGIB)。PGIB 在信息瓶颈框架中 incorporates prototype learning 来提供 key subgraph 从输入图中对模型预测的重要性。这是首次 incorporates prototype learning 到 identify 预测性能的关键子图的过程中。extensive experiments, including qualitative analysis, show that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details></li>
</ul>
<hr>
<h2 id="Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer"><a href="#Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer" class="headerlink" title="Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer"></a>Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19902">http://arxiv.org/abs/2310.19902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这篇论文的目的是解决现有的LLM模型在实际应用中的访问和扩展问题，以及对于这些模型的性能评估。</li>
<li>methods: 这篇论文使用了开源模型库和智能路由器来组织和选择合适的LLM模型，以提高其性能和可靠性。</li>
<li>results: 论文表明，一个由多种开源模型组成的“牧场”可以与商业模型匹配或超越其性能，而且这些模型的大小比商业模型要小得多。此外，当GPT无法回答查询时，“牧场”可以识别一个能够回答查询的模型，超过40%的时间。<details>
<summary>Abstract</summary>
Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5x smaller. We show that in cases where GPT is not able to answer the query, Herd is able to identify a model that can, at least 40% of the time.
</details>
<details>
<summary>摘要</summary>
Here, we show that a herd of open-source models can match or exceed the performance of proprietary models via an intelligent router. Specifically, we show that a herd of open-source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5 times smaller. Additionally, we show that in cases where GPT is not able to answer a query, the herd is able to identify a model that can, at least 40% of the time.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometry-of-Blind-Spots-in-Vision-Models"><a href="#Exploring-Geometry-of-Blind-Spots-in-Vision-Models" class="headerlink" title="Exploring Geometry of Blind Spots in Vision Models"></a>Exploring Geometry of Blind Spots in Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19889">http://arxiv.org/abs/2310.19889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-geometry">https://github.com/SriramB-98/blindspots-geometry</a></li>
<li>paper_authors: Sriram Balasubramanian, Gaurang Sriramanan, Vinu Sankar Sadasivan, Soheil Feizi</li>
<li>for: 这种研究旨在探讨深度神经网络在视觉任务中的不敏感性问题，即大 magnitude 的输入变换不会导致网络活动变化。</li>
<li>methods: 该研究使用了Level Set Traversal算法，通过探索输入空间中的高确idence区域，以找到与源图像相似但属于其他类别的输入图像。</li>
<li>results: 研究发现，深度神经网络的等Confidence水平集在输入空间中存在星型结构，而且可以使用高确idence路径连接这些等Confidence水平集。此外，研究还评估了这些连接的高维空间范围。code可以在<a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-neurips-sub%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SriramB-98/blindspots-neurips-sub上获取。</a><details>
<summary>Abstract</summary>
Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence. The code for this project is publicly available at https://github.com/SriramB-98/blindspots-neurips-sub
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在各种设置中表现出了惊人的成功，但是一些研究表明，这些神经网络对于微不足的干扰（ adversarial attacks）具有极高的敏感性。然而，也有一些研究发现，深度网络可能具有不够敏感的问题，即在输入空间中的大规模干扰不会导致神经网络的活动变化。在这种情况下，我们对视力模型，如CNNs和Transformers，进行了详细的研究，并提出了一些技术来研究这些神经网络的几何结构和扩展。我们提出了一种Level Set Traversal算法，该算法可以在输入空间中循环探索高信任级别的区域，并使用本地梯度的正交分量来探索这些区域。给定一个源图像，我们使用这种算法来找到与源图像在输入空间中的同一个等信任水平集的输入图像，并观察到这些输入图像与源图像之间存在一条直线连接，揭示了深度网络的等信任水平集具有星型结构。此外，我们尝试了为这些相关的更高维度区域的扩展，以便更好地了解深度网络在它们中的行为。相关代码可以在https://github.com/SriramB-98/blindspots-neurips-sub上获取。
</details></li>
</ul>
<hr>
<h2 id="DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies"><a href="#DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies" class="headerlink" title="DEFT: Dexterous Fine-Tuning for Real-World Hand Policies"></a>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19797">http://arxiv.org/abs/2310.19797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityak77/deft-data">https://github.com/adityak77/deft-data</a></li>
<li>paper_authors: Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna Mannam, Deepak Pathak</li>
<li>for: 本研究旨在探讨人类手部 manipulate 软、可变形物体以及复杂、长期任务中的挑战，以提高机器人 manipulate 的能力。</li>
<li>methods: 本研究提出了一种新的方法，即 DEFT（dexterous fine-tuning for hand policies），它利用人类驱动的假设，通过在实际世界中直接执行来改进这些假设。该方法还包括一种高效的在线优化过程。</li>
<li>results: DEFT 在多个任务中显示出成功，以及一种数据效率的普适的软件 manipulate 路径，用于掌握复杂的 manipulate 任务。您可以通过访问我们的网站 <a target="_blank" rel="noopener" href="https://dexterous-finetuning.github.io/">https://dexterous-finetuning.github.io</a> 查看视频结果。<details>
<summary>Abstract</summary>
Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. However, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation. Please see our website at https://dexterous-finetuning.github.io for video results.
</details>
<details>
<summary>摘要</summary>
dexterity 常被看作复杂的操作的基石。人们可以通过手部执行许多技能，从制备食物到操作工具。在这篇论文中，我们调查这些挑战，尤其是在软、可变形物体以及复杂、较长时间任务上。然而，从头来学习这些行为可以是数据不fficient。为了缓解这个问题，我们提出了一种新的方法，即 DEFT（手部精细调整 для手指策略），它利用人类驱动的先验知识，直接在实际世界中执行。为了提高这些先验知识，DEFT包括一种高效的在线优化过程。通过人类学习和在线细化，以及软体机械手部，DEFT在多种任务中成功，建立了一条可靠、数据fficient的通路 toward 普遍的手部精细操作。请参考我们网站 <https://dexterous-finetuning.github.io> 查看视频结果。
</details></li>
</ul>
<hr>
<h2 id="Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus"><a href="#Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus" class="headerlink" title="Re-evaluating Retrosynthesis Algorithms with Syntheseus"></a>Re-evaluating Retrosynthesis Algorithms with Syntheseus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19796">http://arxiv.org/abs/2310.19796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Maziarz, Austin Tripp, Guoqing Liu, Megan Stanley, Shufang Xie, Piotr Gaiński, Philipp Seidl, Marwin Segler</li>
<li>for: 本研究主要目标是提高化学synthesis的计划和评估方法。</li>
<li>methods: 本研究使用了一个名为syntheseus的 benchmarking 库，该库鼓励了best practice的使用，以便对单步和多步 retrosynthesis 算法进行一致的评估。</li>
<li>results: 通过使用syntheseus库进行重新评估，发现了一些之前的 retrosynthesis 算法的排名发生了变化。<details>
<summary>Abstract</summary>
The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
</details>
<details>
<summary>摘要</summary>
“retrosynthesis的规划”在过去几年内，机器学习和化学社区的关注越来越高。尽管表面上看来有稳定的进步，我们认为现有的评价标准和比较方法存在系统性的缺陷。为了解决这个问题，我们提出了一个名为 syntheseus的评价库，它默认采用了最佳实践，使得单步和多步retrosynthesis算法的meaningful评价成为可能。我们使用 syntheseus 重新评估了一些先前的retrosynthesis算法，并发现当仔细评估时，现状的模型的排名发生变化。最后，我们提出了未来这个领域的指导方针。
</details></li>
</ul>
<hr>
<h2 id="Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone"><a href="#Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone" class="headerlink" title="Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone"></a>Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19859">http://arxiv.org/abs/2310.19859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou</li>
<li>for: 本研究旨在提出一种新的参数高效调整方法，以便将大规模基模型传递到下游应用中。</li>
<li>methods: 该方法基于不同的调整策略，通过意图解耦调整器与基模型的关系，使得调整器的设计和学习不再依赖基模型。</li>
<li>results: 对于权重调整和泛化调整等多种调整策略，该方法能够提供更高效的参数调整，并且可以轻松地搭配多种调整策略。经验表明，该方法在推理和生成任务上具有较高的效果和效率。<details>
<summary>Abstract</summary>
Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$.
</details>
<details>
<summary>摘要</summary>
大规模基础模型转移到下游应用的Parameter-efficient tuning已成为当前趋势。现有方法通常将轻量级调参器 embedding到后向，其设计和学习均高度依赖于基模型。这项工作提出了一新调参方式，名为Res-Tuning，它意图将调参器解除与后向的绑定。我们通过理论和实验证明，流行的调参方法均有其对应的等价对手在我们的解绑形式下，因此可以轻松地 интеGRATE到我们的框架中。由于结构分离，我们可以在调参器的设计上免除网络架构的限制，实现灵活的调参策略组合。此外，我们还提出了内存效率改进的Res-Tuning变体，其中分支（formed by a sequence of tuners）被有效地分离于主支，使得梯度只回传给调参器而不回传到后向。这种分离还允许一次主支前进 для多任务推理。广泛的实验表明，我们的方法在效果和效率两个角度上都超越了现有的方法。项目页面： $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$。
</details></li>
</ul>
<hr>
<h2 id="SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization"><a href="#SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization" class="headerlink" title="SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization"></a>SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19795">http://arxiv.org/abs/2310.19795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/donghao51/simmmdg">https://github.com/donghao51/simmmdg</a></li>
<li>paper_authors: Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, Olga Fink<br>for: 这个研究旨在解决多modal Distribution Generalization (DG) 中的挑战，当模型需要在不同的modalities上缩减到未知的target Distribution。methods: 我们提出了一个简单 yet effective的多modal DG框架，叫做SimMMDG。我们认为将不同modalities的特征映射到同一个嵌入空间会降低模型的通用性。因此，我们提出了在每个modalities中分解特征为modalitiespecific和modalitieshared部分。我们运用了监督式对应学习 modalitieshared特征，以保持它们具有共同性，并对modalitiespecific特征强制距离。此外，我们引入了跨modalities翻译模块，以调整学习的特征。results: 我们的框架理论上得到支持，并在EPIC-Kitchens dataset和我们在本文中介绍的新的Human-Animal-Cartoon (HAC) dataset上实现了强大的多modal DG性能。我们的原始代码和HAC dataset可以在<a target="_blank" rel="noopener" href="https://github.com/donghao51/SimMMDG%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/donghao51/SimMMDG上获得。</a><details>
<summary>Abstract</summary>
In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，实现领域泛化（DG）存在重大挑战，因为模型需要泛化到未知目标分布。在多modal的场景中，泛化到未见多Modal的分布呈现更大的挑战，因为不同modalities具有不同的特性。为了解决多modal的领域泛化问题，我们提出了SimMMDG框架，这是一种简单 yet有效的多Modal DG框架。我们认为将不同modalities的特征映射到同一个嵌入空间内会阻碍模型泛化。为此，我们提议将每个modalities的特征分为具有共同特性的特征和具有特定特性的特征。我们使用supervised contrastive学习来确保共同特性，并对特定特性进行距离约束，以促进多Modal特征的多样性。此外，我们还引入了跨Modal翻译模块，以规范学习的特征。我们的框架理论上有良好支持，并在EPIC-Kitchens数据集和我们在本文中介绍的新的人类动物卡通（HAC）数据集上实现了强大的性能。我们的源代码和HAC数据集可以在https://github.com/donghao51/SimMMDG上获取。
</details></li>
</ul>
<hr>
<h2 id="LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code"><a href="#LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code" class="headerlink" title="LILO: Learning Interpretable Libraries by Compressing and Documenting Code"></a>LILO: Learning Interpretable Libraries by Compressing and Documenting Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19791">http://arxiv.org/abs/2310.19791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabegrand/lilo">https://github.com/gabegrand/lilo</a></li>
<li>paper_authors: Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas</li>
<li>for: 本研究旨在开发一个基于神经符号学术的代码生成框架，以帮助开发人员快速生成可读可写的代码库。</li>
<li>methods: 本研究使用了大语言模型（LLM）引导的程序生成技术，以及Stitch符号压缩系统来高效地压缩代码。此外，研究还引入了自动文档（AutoDoc）程序，以帮助理解和应用学习抽象。</li>
<li>results: 对三个 inductive 程序生成benchmark进行了评测，并与现有的神经和符号方法进行了比较。研究发现，LILO可以解决更复杂的任务，并学习更加深入的语言知识。<details>
<summary>Abstract</summary>
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在在代码生成方面表现出色，但是软件开发中一个关键方面是 refactoring：将代码集成到可重用和易读的库中。在这篇论文中，我们介绍了 LILO，一个神经符号学框架，可以逐步生成、压缩和文档代码，以建立适应特定问题领域的库。LILO将神经网络引导的程序生成与Stitch的符号压缩系统相结合，以实现高效的lambda抽象。为了让这些抽象更易理解，我们引入了自动文档（AutoDoc）过程，可以根据Contextual例子来生成自然语言名称和docstrings。除了提高人类可读性外，我们发现AutoDoc会提高LILO的生成器使用学习的性能。我们对LILO进行了三个induced程序生成benchmark测试，包括字符串编辑、Scene reasoning和图形组合。相比 existed神经和符号方法，包括DreamCoder库学习算法，LILO可以解决更复杂的任务，并学习更加深入的语言知识。
</details></li>
</ul>
<hr>
<h2 id="From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces"><a href="#From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces" class="headerlink" title="From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces"></a>From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19786">http://arxiv.org/abs/2310.19786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, Noah Golowich</li>
<li>for: 这个论文目的是为了提出一种新的减少方法，即将换 regret 转换为外部 regret，以优化 classical  reductions 中的一些不稳定性。</li>
<li>methods: 这个论文使用了一种新的减少方法，即将换 regret 转换为外部 regret，并通过对这种减少方法的分析，得出了一些新的结论。</li>
<li>results: 这个论文的结果表明，当存在一个无外部 regret 算法时，必然也存在一个无换 regret 算法，并且这种无换 regret 算法的性能比 classical  reductions 更好。此外，这个论文还提供了一个新的下界，其表明在某些游戏中，换 regret 的数量必然是 $\tilde\Omega(N&#x2F;\epsilon^2)$ 或者是 exponential in $1&#x2F;\epsilon$。<details>
<summary>Abstract</summary>
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can employ distributions over experts, showing that the number of rounds must be $\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.   Our reduction implies that, if no-regret learning is possible in some game, then this game must have approximate correlated equilibria, of arbitrarily good approximation. This strengthens the folklore implication of no-regret learning that approximate coarse correlated equilibria exist. Importantly, it provides a sufficient condition for the existence of correlated equilibrium which vastly extends the requirement that the action set is finite, thus answering a question left open by [DG22; Ass+23]. Moreover, it answers several outstanding questions about equilibrium computation and/or learning in games.
</details>
<details>
<summary>摘要</summary>
我们提供了一种新的减少方法，将交换 regret 转化为外部 regret，从而超越了布姆-曼索尔（BM07）和斯托尔-卢戈西（SL05）的经典减少方法，因为它不需要动作空间的Finite。我们证明了，当存在一个无外部 regret 算法时，也一定存在一个无交换 regret 算法。在学习专家建议中，我们的结果表明，可以保证在 $\log(N)^{O(1/\epsilon)}$ 轮后，交换 regret 不超过 $\epsilon$，并且每轮复杂度为 $O(N)$，where $N$ 是专家数量。而布姆-曼索尔和斯托尔-卢戈西的经典减少方法需要 $O(N/\epsilon^2)$ 轮和至少 $\Omega(N^2)$ 每轮复杂度。我们的结果还有一个相关的下界，这下界在对快速反应者和 $\ell_1$ 约束学习者来说，并且可以使用分布来选择专家，显示了轮数必须是 $\tilde\Omega(N/\epsilon^2)$ 或者 exponential in $1/\epsilon$。我们的减少方法表明，如果有一个不 regret 学习是可能的游戏，那么这个游戏一定有approximate correlated equilibria，并且这些 equilibria 的准确程度可以是arbitrarily good。这个结论超越了布姆-曼索尔的结论，因为它不需要动作空间的Finite。此外，我们的结论还回答了一些关于平衡计算和学习的问题。
</details></li>
</ul>
<hr>
<h2 id="CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a>CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19784">http://arxiv.org/abs/2310.19784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, Ying Shan</li>
<li>for: 文章目的是提出一种基于 CustomNet 的对象自定义方法，以实现在文本到图像生成中实现对象的自定义。</li>
<li>methods: 该方法使用了三个重要的技术：1) 3D 新视角合成；2) 对象自定义；3) 文本描述或特定用户定义的图像来控制位置和背景。</li>
<li>results: 该方法可以在不需要测试时间优化的情况下，实现零 instances 的对象自定义，同时具有较好的个体保持和多样性。<details>
<summary>Abstract</summary>
Incorporating a customized object into image generation presents an attractive feature in text-to-image generation. However, existing optimization-based and encoder-based methods are hindered by drawbacks such as time-consuming optimization, insufficient identity preservation, and a prevalent copy-pasting effect. To overcome these limitations, we introduce CustomNet, a novel object customization approach that explicitly incorporates 3D novel view synthesis capabilities into the object customization process. This integration facilitates the adjustment of spatial position relationships and viewpoints, yielding diverse outputs while effectively preserving object identity. Moreover, we introduce delicate designs to enable location control and flexible background control through textual descriptions or specific user-defined images, overcoming the limitations of existing 3D novel view synthesis methods. We further leverage a dataset construction pipeline that can better handle real-world objects and complex backgrounds. Equipped with these designs, our method facilitates zero-shot object customization without test-time optimization, offering simultaneous control over the viewpoints, location, and background. As a result, our CustomNet ensures enhanced identity preservation and generates diverse, harmonious outputs.
</details>
<details>
<summary>摘要</summary>
通过包含自定义对象在图像生成中，文本到图像生成具有吸引人的特点。然而，现有的优化方法和编码器方法受到了一些缺点，如时间消耗优化、保持对象标识不足和广泛的复制效应。为了解决这些局限性，我们介绍了CustomNet，一种新的对象自定义方法，其中Explicitly incorporates 3D新视野合成能力到对象自定义过程中。这种整合使得可以调整空间位置关系和视点，从而生成多样的输出，同时有效地保持对象标识。此外，我们还引入了细腻的设计，使得通过文本描述或特定用户定义的图像来控制位置和背景，超越现有的3D新视野合成方法的限制。我们还利用了更好的数据构建管道，可以更好地处理真实世界中的对象和复杂背景。准备这些设计，我们的方法可以在零时优化下实现无需测试时优化的自定义对象，同时控制视点、位置和背景。因此，我们的CustomNet可以保持对象标识并生成多样、和谐的输出。
</details></li>
</ul>
<hr>
<h2 id="Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review"><a href="#Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review" class="headerlink" title="Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review"></a>Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19778">http://arxiv.org/abs/2310.19778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catalina Gomez, Sue Min Cho, Shichang Ke, Chien-Ming Huang, Mathias Unberath</li>
<li>for: 提高人工智能在决策支持系统中的用户体验，增强人工智能与人类的交互。</li>
<li>methods: 系统atic review of AI-assisted decision making literature，分析105篇论文，提出了一种交互模式分类法，用于描述不同的人工智能交互方式。</li>
<li>results: 现有交互主要是简单的合作模式，报告了相对少的交互功能支持。 taxonomy 能够帮助理解现有决策支持系统中人工智能交互的现状，并促进交互设计的审慎选择。<details>
<summary>Abstract</summary>
Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, analyzing 105 selected articles, which grounds the introduction of a taxonomy of interaction patterns that delineate various modes of human-AI interactivity. We find that current interactions are dominated by simplistic collaboration paradigms and report comparatively little support for truly interactive functionality. Our taxonomy serves as a valuable tool to understand how interactivity with AI is currently supported in decision-making contexts and foster deliberate choices of interaction designs.
</details>
<details>
<summary>摘要</summary>
对于人工智能（AI）在决策支持系统中的应用，各方面的努力都集中在技术进步上，而忽略了算法的人类预期之间的协调。为了解决这个问题，可观察AI的发展方式更加人类中心。决定AI为人类提供什么样的信息是重要的，但是如何呈现这些信息，例如推荐的顺序和寻求解释的方式，也是非常重要的。这为人类AI互动的研究提供了动机，并且发现了多种应用领域中的人类AI互动协议。但是，目前还没有一个通用的语言来描述人类AI互动协议。为了解决这个问题，我们描述了105篇选择的文献的系统性评审结果，并从这些文献中提取了人类AI互动协议的分类。我们发现现有的互动都偏向简单的合作模式，并报告了相对较少的支持真正互动性能。我们的分类可以作为了解人类AI互动在决策context中的支持方式，并且激发人们对互动设计的意识。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery"><a href="#Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery" class="headerlink" title="Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery"></a>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19776">http://arxiv.org/abs/2310.19776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sarahrastegar/infosieve">https://github.com/sarahrastegar/infosieve</a></li>
<li>paper_authors: Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek</li>
<li>for: 提出了一种能够在测试时发现未知类别的新方法</li>
<li>methods: 基于优化的思路，对数据实例分配最短类别编码，从而控制类别细分程度</li>
<li>results: 经过实验证明，该方法可以在测试时成功地处理未知类别，并且与现有标准模型进行比较In English, this translates to:</li>
<li>for: Proposed a new method for discovering unknown categories at test time</li>
<li>methods: Based on optimization, assign minimum length category codes to individual data instances to control category granularity</li>
<li>results: Experimental results demonstrate the effectiveness of the method in handling unknown categories at test time, with comparisons to state-of-the-art benchmarks<details>
<summary>Abstract</summary>
In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a \textit{category}? In this paper, we conceptualize a \textit{category} through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our model to handle fine-grained categories adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark comparisons, testify to the efficacy of our solution in managing unknown categories at test time. Furthermore, we fortify our proposition with a theoretical foundation, providing proof of its optimality. Our code is available at: \url{https://github.com/SarahRastegar/InfoSieve}.
</details>
<details>
<summary>摘要</summary>
“在试用时探索新的分类ategories，我们面临传统超级vised recognition模型的内在限制。这些模型仅仅受到预先定义的分类category set的限制，而我们则寻求在试用时自动发现新的分类categories。在这篇论文中，我们将分类category视为一个优化问题的最佳解决方案。我们利用这个独特的概念，提出一种新的、效率高且自动化的方法，可以在试用时发现未知的分类categories。我们的方法将实现分类category code的最小长度对个别数据实例的对应，这为我们提供了更好的分类精确度控制，因此我们的模型可以更好地处理细部分类。我们的实验结果，以及与现有的基eline比较，证明了我们的解决方案在处理未知分类ategories的能力。此外，我们还提供了理论基础，证明了我们的方法是最佳的。我们的代码可以在以下连结中找到：https://github.com/SarahRastegar/InfoSieve。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions"><a href="#Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions" class="headerlink" title="Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions"></a>Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19775">http://arxiv.org/abs/2310.19775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Longo, Mario Brcic, Federico Cabitza, Jaesik Choi, Roberto Confalonieri, Javier Del Ser, Riccardo Guidotti, Yoichi Hayashi, Francisco Herrera, Andreas Holzinger, Richard Jiang, Hassan Khosravi, Freddy Lecue, Gianclaudio Malgieri, Andrés Páez, Wojciech Samek, Johannes Schneider, Timo Speith, Simone Stumpf</li>
<li>for: 本研究旨在探讨透明AI（XAI）的发展和应用，以及相关领域的实际挑战。</li>
<li>methods: 本文使用 manifold learning 和 feature importance 等方法来解释AI模型的决策过程。</li>
<li>results: 本研究提出了27个开问，并分类为9个类别，以便各个领域的研究人员可以共同努力解决XAI领域的挑战。<details>
<summary>Abstract</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details>
<details>
<summary>摘要</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.Translated text in Simplified Chinese:随着透明化人工智能（AI）系统在多个实际应用场景中的普及，理解这些黑盒模型已经成为非常重要。为此，解释AI（XAI）已经成为一种研究领域，具有实用和伦理上的利益。本文不仅探讨了XAI的发展和应用，还Addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Renaissance-in-Neural-PDE-Solvers"><a href="#Autoregressive-Renaissance-in-Neural-PDE-Solvers" class="headerlink" title="Autoregressive Renaissance in Neural PDE Solvers"></a>Autoregressive Renaissance in Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19763">http://arxiv.org/abs/2310.19763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 本研究旨在提出一种基于graph neural network的 partial differential equation（PDE）解决方法，以替代传统的束缚方法和Fourier Neural Operator。</li>
<li>methods: 该方法使用了一种名为message passing graph neural network的新型网络架构，通过消息传递机制来实现PDE的解决。</li>
<li>results: 研究表明，该方法可以与或超越传统的PDE解决方法和Fourier Neural Operator在泛化能力和性能上，并且可以更好地处理一些复杂的PDE问题。<details>
<summary>Abstract</summary>
Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper "Message Passing Neural PDE Solver" by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.
</details>
<details>
<summary>摘要</summary>
近期在神经partial differential equation（PDE）解决方法领域，强调神经操作符的发展。然而，布兰德塞特特等在ICLR 2022年发表的论文《消息传递神经PDE解决方法》，重新评估了自动律型模型，并设计了一个可与或超越现有的快扩散 Neil 算法和传统类型PDE解决方法的消息传递图 neural network 架构。本博客文章将探讨这项工作的关键贡献，探讨了自动律型模型中常见的不稳定性问题的解决方案，以及消息传递图 neural network 架构的设计选择。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats"><a href="#Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats" class="headerlink" title="Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"></a>Adversarial Attacks and Defenses in Large Language Models: Old and New Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19737">http://arxiv.org/abs/2310.19737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schwinnl/llm_embedding_attack">https://github.com/schwinnl/llm_embedding_attack</a></li>
<li>paper_authors: Leo Schwinn, David Dobre, Stephan Günnemann, Gauthier Gidel</li>
<li>for: 本研究旨在解决 neural network 的Robustness问题，尤其是在 natural language processing 领域中，以防止 adversarial attack。</li>
<li>methods: 本研究使用了一些新的方法来评估 robustness，包括 embedding space attacks 和 LLM-specific best practices。</li>
<li>results: 研究发现，without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach。此外， embedding space attacks 也成为了一种可行的威胁模型。<details>
<summary>Abstract</summary>
Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-A-Comprehensive-Survey"><a href="#Evaluating-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Evaluating Large Language Models: A Comprehensive Survey"></a>Evaluating Large Language Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19736">http://arxiv.org/abs/2310.19736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/awesome-llms-evaluation-papers">https://github.com/tjunlp-lab/awesome-llms-evaluation-papers</a></li>
<li>paper_authors: Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong</li>
<li>For: 评估大语言模型（LLMs）的能力和安全性。* Methods: 分为三类：知识和能力评估、对Alignment评估和安全评估。* Results: 提供了一个全面的评估方法和标准套件，以及一些特定领域的评估研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.   This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.   We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种任务上表现出了惊人的能力，引起了广泛的关注和应用。然而，与一个双刃剑相似，LLM也存在潜在的风险。它们可能会导致私人数据泄露或生成不当、伤害或误导的内容。此外，LLM的快速进步也引起了关于可能出现无适应安全措施的超智系统的担忧。为了有效利用LLM的能力并确保其安全和有益的发展，对LLM的评估是非常重要。本调查尝试提供LLM评估的全面视图。我们将LLM评估分为三个主要类别：知识和能力评估、对齐评估和安全评估。此外，我们还提供了对这三个方面评估方法和标准的全面评论，并收录了关于LLM在特定领域的表现评估，以及建立了涵盖LLM评估能力、对齐性、安全性和可用性的完整评估平台。我们希望这份全面的概述能够激发更多关于LLM评估的研究兴趣，以实现评估成为LLM发展的重要指南，以最大化社会 benefit  while minimizing potential risks。相关论文的汇总可以在 <https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers> 中找到。
</details></li>
</ul>
<hr>
<h2 id="ViR-Vision-Retention-Networks"><a href="#ViR-Vision-Retention-Networks" class="headerlink" title="ViR: Vision Retention Networks"></a>ViR: Vision Retention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19731">http://arxiv.org/abs/2310.19731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hatamizadeh, Michael Ranzinger, Jan Kautz</li>
<li>for: 该 paper 的目的是提出一种新的计算机视网络模型，以实现高速的推理和平行的训练。</li>
<li>methods: 该 paper 使用了新的拓展 Transformer 模型，并提出了一种新的并行和循环表示方法，以实现高效的推理和训练。</li>
<li>results: 该 paper 通过了多种 dataset 和不同的图像分辨率进行了广泛的实验，并 achieved 竞争性的性能。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts have proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. The ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. We have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. Our code and pretrained models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
视transformer（ViT）在最近几年内引起了很多关注，因为它们在模型长距离空间相互关联的能力和批处理大规模训练中表现出色。 although self-attention机制的训练并行性起到了重要的作用，但它的quadratic复杂性使得ViT在许多场景中不适用，特别是需要快速推理的应用场景。在自然语言处理（NLP）领域，一些新的努力提出了并行化模型的概念，使得在生成应用中实现快速推理变得可能。 drawing inspiration from this trend, we propose a new class of computer vision models, called Vision Retention Networks (ViR), which strike an optimal balance between fast inference and parallel training with competitive performance. in particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. we have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. our code and pretrained models will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="Generating-Medical-Instructions-with-Conditional-Transformer"><a href="#Generating-Medical-Instructions-with-Conditional-Transformer" class="headerlink" title="Generating Medical Instructions with Conditional Transformer"></a>Generating Medical Instructions with Conditional Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19727">http://arxiv.org/abs/2310.19727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Warren Del-Pinto, Goran Nenadic<br>for:The paper is written to introduce a novel task-specific model architecture, Label-To-Text-Transformer (LT3), which generates synthetic medical instructions based on provided labels.methods:The LT3 model is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, and it uses a task-specific transformer architecture to generate synthetic medical instructions.results:The paper evaluates the performance of LT3 by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, and shows that LT3 can generate high-quality and diverse synthetic medical instructions. The generated synthetic data is used to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset, and the results show that the model trained on synthetic data can achieve a 96-98% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form.<details>
<summary>Abstract</summary>
Access to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
</details>
<details>
<summary>摘要</summary>
accessed to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="A-Path-to-Simpler-Models-Starts-With-Noise"><a href="#A-Path-to-Simpler-Models-Starts-With-Noise" class="headerlink" title="A Path to Simpler Models Starts With Noise"></a>A Path to Simpler Models Starts With Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19726">http://arxiv.org/abs/2310.19726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lesia Semenova, Harry Chen, Ronald Parr, Cynthia Rudin</li>
<li>For: 这篇论文探讨了决策森林模型在各种领域中的性能问题，特别是在含有噪声的数据集上。* Methods: 该论文提出了一种机制，即数据生成过程和分析者在学习过程中的选择，对决策森林模型的性能产生影响。同时，该论文引入了一个名为“模式多样性”的指标，用于衡量决策森林模型在不同分类模式下的差异。* Results: 该论文发现，噪声程度高的数据集会导致决策森林模型的性能更高，并且模式多样性指标与噪声度之间存在正相关关系。这些结果解释了为什么简单的模型在复杂的数据集上可以达到黑盒模型的同等精度水平。<details>
<summary>Abstract</summary>
The Rashomon set is the set of models that perform approximately equally well on a given dataset, and the Rashomon ratio is the fraction of all models in a given hypothesis space that are in the Rashomon set. Rashomon ratios are often large for tabular datasets in criminal justice, healthcare, lending, education, and in other areas, which has practical implications about whether simpler models can attain the same level of accuracy as more complex models. An open question is why Rashomon ratios often tend to be large. In this work, we propose and study a mechanism of the data generation process, coupled with choices usually made by the analyst during the learning process, that determines the size of the Rashomon ratio. Specifically, we demonstrate that noisier datasets lead to larger Rashomon ratios through the way that practitioners train models. Additionally, we introduce a measure called pattern diversity, which captures the average difference in predictions between distinct classification patterns in the Rashomon set, and motivate why it tends to increase with label noise. Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets.
</details>
<details>
<summary>摘要</summary>
“Rashomon集”是指一组模型在给定数据集上表现相似的情况下，“Rashomon比”则是指所有模型空间中的模型数量在Rashomon集中的比例。在刑事、医疗、贷款、教育等领域的表格数据中，Rashomon比 часто很大，这有实际意义，例如是否可以使用简单的模型来达到与更复杂的模型相同的准确率水平。工作中，我们提出了数据生成过程中的一种机制，以及分析者在学习过程中通常会选择的决策，这会决定Rashomon比的大小。我们发现，含有噪声的数据会导致Rashomon比更大，这与模型训练过程中的噪声有关。我们还引入了一个名为“模式多样性”的指标，用于捕捉Rashomon集中不同分类模式之间的预测差异，并解释了这种差异在噪声增加时会增加。我们的结果解释了为什么简单的模型在复杂的噪声数据上常常能够表现出类似的水平。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Knowledge-Editing-of-Neural-Networks"><a href="#A-Survey-on-Knowledge-Editing-of-Neural-Networks" class="headerlink" title="A Survey on Knowledge Editing of Neural Networks"></a>A Survey on Knowledge Editing of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19704">http://arxiv.org/abs/2310.19704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vittorio Mazzia, Alessandro Pedrani, Andrea Caciolai, Kay Rottmann, Davide Bernardi</li>
<li>for: 本研究旨在解决人工智能中的神经网络编辑问题，即如何通过不影响神经网络已经学习的任务来更新神经网络模型，以适应数据的变化。</li>
<li>methods: 本研究使用了多种方法来解决神经网络编辑问题，包括常规化技术、元学习、直接模型编辑和建筑策略等。</li>
<li>results: 本研究提供了一个简洁的概述神经网络编辑领域的最新研究成果，并将相关的方法和数据集分类为四个家族：常规化技术、元学习、直接模型编辑和建筑策略等。<details>
<summary>Abstract</summary>
Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance on a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model re-training to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pre-training, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pre-trained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant knowledge editing approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.
</details>
<details>
<summary>摘要</summary>
深度神经网络在学术和实践中日益普及，与人类表现相当或超越人类在各种领域和相关任务上。然而，就如人类一样，即使是最大的人工神经网络也会出错，并且已经正确的预测可能会变成过时的信息随着时间的推移。因此，在实际应用中，常常将数据集添加了包含错误或最新信息的样本以作为 workaround。然而，神经网络参数中的隐式记忆知识具有快速填充和损害的问题，常需要完整的模型重新训练来实现愿望的行为。这是昂贵、不可预测和不可靠的，与当前大规模自我超级vised学习的趋势不兼容，因此需要找到更有效率的方法来适应神经网络模型变化的数据。为此，知识编辑在人工智能领域出现了，旨在允许可靠、数据效率、快速地修改预先训练的目标模型，不影响模型在之前学习任务上的行为。在这篇评论中，我们首先介绍了修改神经网络的问题，将其形式化为通用框架，并与更知名的连续学习分支相区分。然后，我们提供了最新的知识编辑方法和数据集的综述，将工作分为四个家族：规范技术、元学习、直接模型编辑和建筑策略。最后，我们介绍了与其他领域的交叉和未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness"><a href="#Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness" class="headerlink" title="Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness"></a>Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19691">http://arxiv.org/abs/2310.19691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacyanthis/causal-context">https://github.com/jacyanthis/causal-context</a></li>
<li>paper_authors: Jacy Reese Anthis, Victor Veitch</li>
<li>for:  This paper focuses on the problem of fairness in machine learning, specifically addressing the concept of counterfactual fairness and its relationship to other fairness metrics.</li>
<li>methods:  The authors use a causal context to bridge the gap between counterfactual fairness, robust prediction, and group fairness. They develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness.</li>
<li>results:  The authors show that in three common fairness contexts (measurement error, selection on label, and selection on predictors), counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Additionally, they demonstrate that counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.<details>
<summary>Abstract</summary>
Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.
</details>
<details>
<summary>摘要</summary>
counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients"><a href="#Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients" class="headerlink" title="Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients"></a>Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19686">http://arxiv.org/abs/2310.19686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Margerie Huet-Dastarac, Dan Nguyen, Steve Jiang, John Lee, Ana Barragan Montero</li>
<li>for: 这篇论文旨在提供一种可靠和高效的深度学习模型 uncertainty 估计方法，并且能够检测出资料集外的数据（Out-of-distribution，OOD）。</li>
<li>methods: 这篇论文提出了一种直接使用构造汇流（bottleneck）来估计模型 uncertainty的方法，具体来说是将构造汇流中的一支分支用来重建输入数据。</li>
<li>results: 在这篇论文中，这种方法在预报癌症肿瘤疗法剂量预测 tasks 中与 MCDO 和 DE 相比，得到了更高的 Pearson 相関系数（0.620），并且能够轻松地检测出 OOD 数据（Z-score 34.05）。<details>
<summary>Abstract</summary>
Estimating the uncertainty of deep learning models in a reliable and efficient way has remained an open problem, where many different solutions have been proposed in the literature. Most common methods are based on Bayesian approximations, like Monte Carlo dropout (MCDO) or Deep ensembling (DE), but they have a high inference time (i.e. require multiple inference passes) and might not work for out-of-distribution detection (OOD) data (i.e. similar uncertainty for in-distribution (ID) and OOD). In safety critical environments, like medical applications, accurate and fast uncertainty estimation methods, able to detect OOD data, are crucial, since wrong predictions can jeopardize patients safety. In this study, we present an alternative direct uncertainty estimation method and apply it for a regression U-Net architecture. The method consists in the addition of a branch from the bottleneck which reconstructs the input. The input reconstruction error can be used as a surrogate of the model uncertainty. For the proof-of-concept, our method is applied to proton therapy dose prediction in head and neck cancer patients. Accuracy, time-gain, and OOD detection are analyzed for our method in this particular application and compared with the popular MCDO and DE. The input reconstruction method showed a higher Pearson correlation coefficient with the prediction error (0.620) than DE and MCDO (between 0.447 and 0.612). Moreover, our method allows an easier identification of OOD (Z-score of 34.05). It estimates the uncertainty simultaneously to the regression task, therefore requires less time or computational resources.
</details>
<details>
<summary>摘要</summary>
深度学习模型的不确定性估计问题已成为一个开放的问题，文献中有许多不同的解决方案。大多数常用的方法基于 bayesian 近似，如 Monte Carlo dropout (MCDO) 或 Deep ensembling (DE)，但它们的推理时间较长（需要多次推理），而且可能无法处理非标本数据（OOD）。在安全关键环境，如医疗应用，准确和快速的不确定性估计方法，能够检测 OOD 数据，是非常重要的，因为错误预测可能会威胁病人的安全。在这种情况下，我们提出了一种直接的不确定性估计方法，并应用于回归 U-Net 架构。该方法是通过添加从瓶颈来的一个分支，来重construct 输入。输入重建错误可以作为模型不确定性的Surrogate。为证明，我们对抗癌病头颈患者的辐射剂量预测进行了应用。我们分析了我们方法的准确率、时间提升和 OOD 检测，并与 MCDO 和 DE 进行了比较。我们的方法显示了更高的归一化相关系数（0.620），与 DE 和 MCDO（ между 0.447 和 0.612）相比。此外，我们的方法可以更容易地检测 OOD（Z-score 34.05）。它同时估计不确定性和回归任务，因此需要更少的时间或计算资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation"><a href="#Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation" class="headerlink" title="Integrating Pre-trained Language Model into Neural Machine Translation"></a>Integrating Pre-trained Language Model into Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19680">http://arxiv.org/abs/2310.19680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soon-Jae Hwang, Chang-Sung Jeong</li>
<li>for: 提高Neural Machine Translation（NMT）性能，解决高质量双语对应语料不足问题。</li>
<li>methods: 使用预训练语言模型（PLM）提供上下文信息，并提出PLM集成NMT（PiNMT）模型，包括PLM多层转换器、嵌入合并和夹角匹配等三个关键组件。</li>
<li>results: 通过提出的PiNMT模型和训练策略（分离学习率和双步训练），在IWSLT’14 En$\leftrightarrow$De数据集上实现了状态级表现。<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies are exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes a PLM-integrated NMT (PiNMT) model to overcome the identified problems. The PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieved state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset. This study's outcomes are noteworthy as they demonstrate a novel approach for efficiently integrating PLM with NMT to overcome incompatibility and enhance performance.
</details>
<details>
<summary>摘要</summary>
PiNMT 模型包括三个关键组件：PLM 多层转换器、扩展嵌入和偏心对Alignment。每一个组件都在提供 PLM 信息给 NMT 中扮演着重要的角色。此外，本研究还提出了两种训练策略：分离学习速率和双步训练。通过实施提议的 PiNMT 模型和训练策略，我们在 IWSLT'14 En$\leftrightarrow$De 数据集上实现了状态略的性能。这些结果具有意义，因为它们证明了一种有效的 PLM 与 NMT 集成方法，以解决不兼容问题并提高性能。
</details></li>
</ul>
<hr>
<h2 id="AI-Alignment-A-Comprehensive-Survey"><a href="#AI-Alignment-A-Comprehensive-Survey" class="headerlink" title="AI Alignment: A Comprehensive Survey"></a>AI Alignment: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19852">http://arxiv.org/abs/2310.19852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/AlignmentSurvey">https://github.com/PKU-Alignment/AlignmentSurvey</a></li>
<li>paper_authors: Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao<br>for: This paper aims to provide a systematic survey of AI alignment research, including the key objectives, methodology, and practice of alignment research.methods: The paper uses a four-principle framework (Robustness, Interpretability, Controllability, and Ethicality) to identify the key objectives of AI alignment, and decomposes current alignment research into two components: forward alignment and backward alignment.results: The paper discusses various techniques for forward alignment, including learning from feedback and overcoming distribution shift, and verification techniques for backward alignment to improve the assurance of forward alignment outcomes. Additionally, the paper provides a constantly updated website featuring tutorials, collections of papers, blogs, and other learning resources.<details>
<summary>Abstract</summary>
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss how to conduct learning from various types of feedback (a.k.a., outer alignment) and how to overcome the distribution shift to avoid goal misgeneralization (a.k.a., inner alignment). On backward alignment, we discuss verification techniques that can tell the degree of value alignment for various AI systems deployed, which can further improve the assurance of forward alignment outcomes.   Based on this, we also release a constantly updated website featuring tutorials, collections of papers, blogs, and other learning resources at https://www.alignmentsurvey.com.
</details>
<details>
<summary>摘要</summary>
人工智能启 align 目标是建立与人类意愿和价值观念相一致的 AI 系统。随着 AI 系统拥有超人类能力的出现，大规模的风险相关于不一致系统的出现也显得潜在危险。众多 AI 专家和公众人物表达了对 AI 风险的担忧， argued that mitigating the risk of AI extinction should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. 因为现有的系统性的调查不够新，在这篇论文中，我们进行了对Alignment研究的核心概念、方法和实践的探讨。首先，我们确定了四个目标为AI启 align的关键原则：Robustness、Interpretability、Controllability和Ethicality（RICE）。我们还将当前的启 align研究景观分解为两个关键组成部分：前向启 align和后向启 align。前者希望通过启 align训练来使 AI 系统相互适应，而后者则希望通过获得证明 AI 系统的启 align度以避免加剧不一致风险。在前向启 align方面，我们讨论了从不同类型的反馈（即外部启 align）学习以及如何避免目标泛化风险（即内部启 align）。在后向启 align方面，我们讨论了如何证明不同 AI 系统的启 align度，以便进一步提高前向启 align 的结果。此外，我们还发布了一个不断更新的网站，包括教程、论文集、博客和其他学习资源，请参考 <https://www.alignmentsurvey.com>。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding"><a href="#Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding" class="headerlink" title="Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding"></a>Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19671">http://arxiv.org/abs/2310.19671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Tom Kouwenhoven, Marco R. Spruit, Max J. van Duijn</li>
<li>for: 这篇论文主要是为了评估大型自然语言处理器（LLM）的能力，并且探讨关于 LLM 的评价和含义。</li>
<li>methods: 本文使用了理论和实证方法来评估 LLM 的能力，包括对三个常见批评点进行了严谨的分析。</li>
<li>results: 本文的结果表明，对 LLM 的评价需要更加细化，并且提出了一种 Pragmatic 视角来理解 LLM 的含义和意图。<details>
<summary>Abstract</summary>
Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.
</details>
<details>
<summary>摘要</summary>
当前的大语言模型（LLM）在生成 grammatically 正确，流畅文本方面表现出了无与伦比的能力。 LLM 的出现速度非常快，但是关于 LLM 的能力的讨论却落后于其出现。因此，在这份位点纸中，我们首先 zoom 到了评估 LLM 能力的辩论中的三个点：一、LLM 只是遵循训练数据中的统计模式；二、LLM 掌握了形式语言能力，而不是真正的语言能力；三、语言学习在 LLM 中不能 Inform 人类语言学习。通过实证和理论上的论据，我们表明这些点需要更加细腻。二、我们阐述了在 LLM 中理解和意愿的问题上的 Pragmatic 视角。理解和意愿是指我们归功于他人的心理状态，因为它们具有 Pragmatic 的价值：它们允许我们忽略复杂的下面机制，预测行为效果。我们反思在哪些情况下，人类可能会对 LLM 归功于心理状态，从而描述了一种 Pragmatic 哲学上的 LLM 技术在社会中的增长。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection"><a href="#Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection" class="headerlink" title="Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection"></a>Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19658">http://arxiv.org/abs/2310.19658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Ziems, Gang Liu, John Flanagan, Meng Jiang</li>
<li>for: 本研究旨在提高网络入侵检测（NID）系统的决策树模型，以便更好地检测恶意网络流量。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）来提供解释和背景知识，以帮助用户更好地理解决策树的决策。</li>
<li>results: 研究发现，LLM生成的决策树解释与人类评价的可读性、质量和背景知识之间呈高度相关，同时能够提供更好的决策边界的理解。<details>
<summary>Abstract</summary>
Network intrusion detection (NID) systems which leverage machine learning have been shown to have strong performance in practice when used to detect malicious network traffic. Decision trees in particular offer a strong balance between performance and simplicity, but require users of NID systems to have background knowledge in machine learning to interpret. In addition, they are unable to provide additional outside information as to why certain features may be important for classification.   In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree NID systems. Further, we introduce a new human evaluation framework for decision tree explanations, which leverages automatically generated quiz questions that measure human evaluators' understanding of decision tree inference. Finally, we show LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge while simultaneously providing better understanding of decision boundaries.
</details>
<details>
<summary>摘要</summary>
网络侵入检测（NID）系统利用机器学习显示在探测恶意网络流量方面具有优秀表现。决策树特别是在性能和简单性之间做出了良好的折衔，但需要NID系统用户具备机器学习背景知识来解释。此外，它们无法提供外部信息，以解释特定特征的分类重要性。在这项工作中，我们探讨使用大语言模型（LLM）提供解释和背景知识来增强决策树NID系统。此外，我们提出了一种新的人类评估框架 для决策树解释，该框架利用自动生成的测验题来评估人类评估者对决策树推理的理解程度。最后，我们发现LLM生成的决策树解释与人类评分的可读性、质量和背景知识相关性高，同时提供更好的决策边界理解。
</details></li>
</ul>
<hr>
<h2 id="MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval"><a href="#MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval" class="headerlink" title="MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval"></a>MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19654">http://arxiv.org/abs/2310.19654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie, Haonan Lu</li>
<li>for: 降低大规模图文预训练模型的模型大小和加速其终端设备部署。</li>
<li>methods: 提出了一种多教师跨Modalities对齐填充（MCAD）技术，将单流模型的优点和双流模型的优点结合起来。</li>
<li>results: 通过在双流模型中插入单流模型的混合特征，并进行Logit和特征填充，提高了学生双流模型的检索性能，同时不增加搜索复杂度。<details>
<summary>Abstract</summary>
With the success of large-scale visual-language pretraining models and the wide application of image-text retrieval in industry areas, reducing the model size and streamlining their terminal-device deployment have become urgently necessary. The mainstream model structures for image-text retrieval are single-stream and dual-stream, both aiming to close the semantic gap between visual and textual modalities. Dual-stream models excel at offline indexing and fast inference, while single-stream models achieve more accurate cross-model alignment by employing adequate feature fusion. We propose a multi-teacher cross-modality alignment distillation (MCAD) technique to integrate the advantages of single-stream and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher features and logits. Then, we conduct both logit and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a mobile CLIP model on Snapdragon clips with only 93M running memory and 30ms search latency, without apparent performance degradation of the original large CLIP.
</details>
<details>
<summary>摘要</summary>
随着大规模视语言预训模型的成功和图像文本关联在行业领域的广泛应用，减少模型大小和加速终端设备部署已成为非常重要的需求。主流的图像文本关联模型结构有单流和双流两种，都想要填补视语言模态之间的 semantic gap。双流模型在离线索引和快速推理方面表现出色，而单流模型通过适当的特征融合来实现更高精度的跨模型对齐。我们提出了一种多教师跨模态对齐截取（MCAD）技术，通过将单流特征融合到图像和文本特征中来形成新的修改教师特征和 logits。然后，我们进行了 both logit 和特征截取来提高学生双流模型的能力，实现高度的 retrieve 性能无需增加推理复杂度。广泛的实验表明 MCAD 在图像文本关联任务中表现出了非常出色的性能和高效性。此外，我们在 Snapdragon clips 上实现了基于 CLIP 的移动模型，只需93M的运行内存和30ms的搜索延迟，而无 Apparent 性能下降。
</details></li>
</ul>
<hr>
<h2 id="Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria"><a href="#Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria" class="headerlink" title="Fast swap regret minimization and applications to approximate correlated equilibria"></a>Fast swap regret minimization and applications to approximate correlated equilibria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19647">http://arxiv.org/abs/2310.19647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghui Peng, Aviad Rubinstein</li>
<li>For: The paper is written to resolve the main open problem of [Blum and Mansour 2007] and to provide a new, matching lower bound.* Methods: The paper uses a simple and computationally efficient algorithm that obtains $\varepsilon T$-swap regret within only $T &#x3D; \mathsf{polylog}(n)$ rounds.* Results: The paper achieves an exponential improvement compared to the super-linear number of rounds required by the state-of-the-art algorithm, and resolves the main open problem of [Blum and Mansour 2007]. The algorithm also implies faster convergence to $\varepsilon$-Correlated Equilibrium in several regimes, including normal form two-player games and extensive-form games.<details>
<summary>Abstract</summary>
We give a simple and computationally efficient algorithm that, for any constant $\varepsilon>0$, obtains $\varepsilon T$-swap regret within only $T = \mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the super-linear number of rounds required by the state-of-the-art algorithm, and resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an exponential dependence on $\varepsilon$, but we prove a new, matching lower bound.   Our algorithm for swap regret implies faster convergence to $\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For normal form two-player games with $n$ actions, it implies the first uncoupled dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE in two-player games (resolving an open problem mentioned by [Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]; and an $\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem of [Babichenko'2020] and obtaining the first separation between $\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity model).   For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$ $\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept often conjectured to be computationally intractable (e.g. [Stengel-Forges'08, Fujii'23]).
</details>
<details>
<summary>摘要</summary>
我们提供一个简单而计算效率高的算法，可以在任何常数 $\varepsilon>0$ 下获得 $\varepsilon T$-交换误差，只需要 $T = \mathsf{polylog}(n)$ 轮次，这比现状态艺术算法的超线性轮次数快很多，解决了 [Blum 和 Mansour 2007] 中的主要开问。我们的算法具有对 $\varepsilon$ 的幂次依赖，但我们证明了一个新的匹配下界。我们的交换误差算法意味着在几个场景中更快地 converges to $\varepsilon$-相关平衡（$\varepsilon$-CE）：1. 正常形两个玩家游戏中，我们的算法可以在 $\polylog(n)$ 轮次内 converges to $\varepsilon$-CE;2. 我们可以实现一个 $\mathsf{polylog}(n)$-位信息协议来实现 $\varepsilon$-CE在两个玩家游戏中;3. 我们可以实现一个 $\tilde{O}(n)$-询问算法来实现 $\varepsilon$-CE。此外，我们的算法还解决了对 $\varepsilon$-CE 和 $\varepsilon$- Nash 平衡之间的分离问题，这是一个在询问复杂度模型中的开问。在扩展形游戏中，我们的算法意味着一个 PTAS  для $\mathit{normal}$ $\mathit{form}$ $\mathit{相关}$ $\mathit{平衡}$，这是一个 computationally conjectured 的问题（例如 [Stengel-Forges'08, Fujii'23]）。
</details></li>
</ul>
<hr>
<h2 id="RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency"><a href="#RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency" class="headerlink" title="RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency"></a>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19629">http://arxiv.org/abs/2310.19629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vlar-group/raydf">https://github.com/vlar-group/raydf</a></li>
<li>paper_authors: Zhuoman Liu, Bo Yang</li>
<li>for: 本研究targets the problem of continuous 3D shape representations, aiming to improve the efficiency and accuracy of 3D shape reconstruction.</li>
<li>methods: 我们提出了一种新的框架 called RayDF, which consists of three major components: 1) simple ray-surface distance field, 2) novel dual-ray visibility classifier, and 3) multi-view consistency optimization module.</li>
<li>results: 我们进行了广泛的 evaluate our method on three public datasets, and the results show that our method clearly surpasses existing coordinate-based and ray-based baselines in 3D surface point reconstruction, with a 1000x faster speed than coordinate-based methods to render an 800x800 depth image.<details>
<summary>Abstract</summary>
In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called RayDF. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a 1000x faster speed than coordinate-based methods to render an 800x800 depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at https://github.com/vLAR-group/RayDF
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了连续3D形状表示的问题。现有大多数成功方法是坐标基的偏折神经表示。然而，它们在渲染新视图或者恢复明确的表面点时效率低下。一些工作开始将3D形状表示为射线基的神经函数，但学习结构因为缺乏多视图几何一致性而受到限制。为了解决这些挑战，我们提出了一个新框架called RayDF。它包括三个主要组成部分：1）简单的射线表面距离场，2）新的双射线可见分类器，3）多视图一致优化模块，使得学习的射线表面距离具有多视图几何一致性。我们对三个公共数据集进行了广泛的评估，并证明了我们的方法在 sintetic和实际世界3D场景中的3D表面点重建表现出色，明显超过了坐标基和射线基的基准。其中，我们的方法可以在渲染800x800深度图像时达到1000倍的速度，显示了我们的方法在3D形状表示方面的优势。我们的代码和数据可以在https://github.com/vLAR-group/RayDF上获取。
</details></li>
</ul>
<hr>
<h2 id="Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities"><a href="#Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities" class="headerlink" title="Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities"></a>Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19626">http://arxiv.org/abs/2310.19626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengliang Liu, Yiwei Li, Qian Cao, Junwen Chen, Tianze Yang, Zihao Wu, John Hale, John Gibbs, Khaled Rasheed, Ninghao Liu, Gengchen Mai, Tianming Liu</li>
<li>For: The paper provides a comprehensive analysis of the applications and implications of artificial general intelligence (AGI) in the arts and humanities, with a focus on its potential to promote creativity, knowledge, and cultural values while avoiding negative consequences such as factuality, toxicity, biases, and public safety concerns.* Methods: The paper surveys cutting-edge AGI systems and their usage in various areas such as poetry, history, marketing, film, and classical art, and outlines mitigation strategies to address concerns related to the technology’s responsible deployment.* Results: The paper argues for multi-stakeholder collaboration to ensure AGI’s technological capacities are aligned with enduring social goods, and highlights promising directions for further research to achieve this goal.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文探讨了人工通用智能（AGI）在艺术和人文领域的应用和影响，以及如何通过多方合作确保AGI促进创造力、知识和文化价值而不让技术发展带来负面影响。</li>
<li>methods: 论文对 cutting-edge AGI 系统和其在不同领域的应用进行了概括，并提出了Addressing Concerns related to factuality, toxicity, biases, and public safety in AGI systems的缓解策略。</li>
<li>results: 论文强调了多方合作的重要性，以确保AGI的技术 capacities 与持续社会价值相align，并 highlighted 了未来研究的可能性，以实现这一目标。<details>
<summary>Abstract</summary>
Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
</details>
<details>
<summary>摘要</summary>
This paper provides a comprehensive analysis of the applications and implications of AGI in the fields of text, graphics, audio, and video related to arts and the humanities. We survey cutting-edge systems and their uses in various areas, such as poetry, history, marketing, film, and communication, as well as classical art. We also highlight significant concerns regarding factuality, toxicity, biases, and public safety in AGI systems and propose mitigation strategies.The paper argues for the importance of multi-stakeholder collaboration to ensure that AGI promotes creativity, knowledge, and cultural values without compromising truth or human dignity. Our timely contribution provides a comprehensive overview of a rapidly developing field, highlighting promising directions while advocating for responsible progress that prioritizes human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Post-Training-Quantization-of-Protein-Language-Models"><a href="#Exploring-Post-Training-Quantization-of-Protein-Language-Models" class="headerlink" title="Exploring Post-Training Quantization of Protein Language Models"></a>Exploring Post-Training Quantization of Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19624">http://arxiv.org/abs/2310.19624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Peng, Fei Yang, Ning Sun, Sheng Chen, Yanfeng Jiang, Aimin Pan</li>
<li>for: 这个研究是为了解决 protein language model (ProteinLM) 的高计算需求、大量存储需求和延迟问题，以提高 ProteinLM 的可用性和效率。</li>
<li>methods: 这个研究使用了 post-training quantization (PTQ) 技术来实现 ProteinLM 的数字化。具体来说，我们对 ESMFold 进行了全量对应的数字化，并进行了广泛的数字化实验，以探索适当的数字化方法。</li>
<li>results: 我们的研究结果显示，使用 Typical uniform quantization method 对 ESMFold 的数字化会导致严重的损失TM-Score，尤其是在使用 8 位数字化时。我们还发现 ESMFold 具有特殊的问题，例如具有高度不对称的活动范围，使得使用低位数字化格式进行表示具有困难。为解决这些问题，我们提出了一个新的 PTQ 方法，使用 Piecewise linear quantization 来精确地数字化具有不对称活动值的 ESMFold。我们的研究结果显示，这个方法可以实现精确地数字化 ESMFold，并且可以应用到聚合点预测任务中。<details>
<summary>Abstract</summary>
Recent advancements in unsupervised protein language models (ProteinLMs), like ESM-1b and ESM-2, have shown promise in different protein prediction tasks. However, these models face challenges due to their high computational demands, significant memory needs, and latency, restricting their usage on devices with limited resources. To tackle this, we explore post-training quantization (PTQ) for ProteinLMs, focusing on ESMFold, a simplified version of AlphaFold based on ESM-2 ProteinLM. Our study is the first attempt to quantize all weights and activations of ProteinLMs. We observed that the typical uniform quantization method performs poorly on ESMFold, causing a significant drop in TM-Score when using 8-bit quantization. We conducted extensive quantization experiments, uncovering unique challenges associated with ESMFold, particularly highly asymmetric activation ranges before Layer Normalization, making representation difficult using low-bit fixed-point formats. To address these challenges, we propose a new PTQ method for ProteinLMs, utilizing piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, demonstrating that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, showcasing its versatility. In summary, our study introduces an innovative PTQ method for ProteinLMs, addressing specific quantization challenges and potentially leading to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details>
<details>
<summary>摘要</summary>
We observed that typical uniform quantization methods perform poorly on ESMFold, resulting in a significant drop in TM-Score when using 8-bit quantization. We conducted extensive quantization experiments and discovered unique challenges associated with ESMFold, particularly highly asymmetric activation ranges before Layer Normalization, which make representation difficult using low-bit fixed-point formats.To address these challenges, we proposed a new PTQ method for ProteinLMs, utilizing piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, showing that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, demonstrating its versatility.In summary, our study introduces an innovative PTQ method for ProteinLMs, addressing specific quantization challenges and potentially leading to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details></li>
</ul>
<hr>
<h2 id="Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners"><a href="#Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners" class="headerlink" title="Large Trajectory Models are Scalable Motion Predictors and Planners"></a>Large Trajectory Models are Scalable Motion Predictors and Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19620">http://arxiv.org/abs/2310.19620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-mars-lab/statetransformer">https://github.com/tsinghua-mars-lab/statetransformer</a></li>
<li>paper_authors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo, Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao</li>
<li>for: 本研究的目的是提出一种可扩展的轨迹模型（State Transformer，STR），用于自动驾驶中的运动预测和规划问题。</li>
<li>methods: 本研究使用了一种简单的模型设计，将观察、状态和动作拼接成一个统一的序列模型任务，以解决自动驾驶中的复杂问题。</li>
<li>results: 实验结果显示，大轨迹模型（LTM），如STR，在运动预测和规划问题中具有出色的适应性和学习效率。 Qualitative results还表明，LTM可以在训练数据分布不符的场景下做出可靠的预测，并且学习长期规划 ohnexpress的高级审核或高成本的标注。<details>
<summary>Abstract</summary>
Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. With a simple model design, STR consistently outperforms baseline approaches in both problems. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting outstanding adaptability and learning efficiency. Qualitative results further demonstrate that LTMs are capable of making plausible predictions in scenarios that diverge significantly from the training data distribution. LTMs also learn to make complex reasonings for long-term planning, without explicit loss designs or costly high-level annotations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate="zh-CN"<</SYS>>行为预测和规划是自动驾驶中的关键任务，最近的努力都集中在机器学习基于的方法上。挑战包括理解多样化的道路地貌、跟踪交通动态过程长时间 horizon、解释不同行为类型、并在大规模连续状态空间中生成策略。启发于大语言模型在面临类似复杂性时的成功，我们介绍了一种可扩展的轨迹模型，称为State Transformer（STR）。STR将轨迹预测和轨迹规划问题重新排序为一个统一的序列模型任务。 STR的简单设计使其在基eline方法上表现出色，并且在实验结果中显示出了卓越的适应性和学习效率。另外，实验结果还表明，大轨迹模型（LTM），如STR，遵循了扩展法律，并且在不同于训练数据分布的场景中做出了可靠的预测。LTM也能够无需显式损失函数或高级标注来学习复杂的长期规划。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models"><a href="#Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models" class="headerlink" title="Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models"></a>Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19619">http://arxiv.org/abs/2310.19619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mars-tin/awesome-theory-of-mind">https://github.com/mars-tin/awesome-theory-of-mind</a></li>
<li>paper_authors: Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai</li>
<li>for: This paper aims to provide a comprehensive evaluation of machine ToM and develop a more effective evaluation protocol for assessing ToM in large language models (LLMs).</li>
<li>methods: The authors use a taxonomic approach to categorize machine ToM into 7 mental state categories, and they propose a holistic and situated evaluation protocol that considers the physical and social context of LLMs.</li>
<li>results: The authors present a pilot study in a grid world setup as a proof of concept, demonstrating the potential of their proposed evaluation protocol to provide a more comprehensive assessment of mental states in LLMs.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提供大语言模型（LLM）机器理解思维的全面评估，以及为了评估思维的更有效的评价协议。</li>
<li>methods: 作者使用分类方法将机器思维分为7种情感类别，并提议在物理和社会上将LLM视为 Agent 进行全面和围绕的评估。</li>
<li>results: 作者在格子世界设置下进行了一个证明性研究，展示了其提议的评价协议的潜在。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind
</details>
<details>
<summary>摘要</summary>
In this position paper, we aim to answer two key questions:1. How can we categorize the various aspects of machine ToM?2. What is a more effective evaluation protocol for machine ToM?Drawing on psychological studies, we categorize machine ToM into 7 mental state categories and examine existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM, where LLMs are treated as agents who are physically and socially situated in environments and interactions with humans. This approach provides a more comprehensive assessment of mental states and may mitigate the risk of shortcuts and data leakage.We present a pilot study in a grid world setup as a proof of concept. Our hope is that this position paper will facilitate future research in integrating ToM with LLMs and offer a straightforward means for researchers to better position their work in the landscape of ToM.More information can be found on our project page: <https://github.com/Mars-tin/awesome-theory-of-mind>
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation"><a href="#Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation" class="headerlink" title="Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation"></a>Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19607">http://arxiv.org/abs/2310.19607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GPPassos/learning-relevance-aacbr-technical-report">https://github.com/GPPassos/learning-relevance-aacbr-technical-report</a></li>
<li>paper_authors: Guilherme Paulino-Passos, Francesca Toni</li>
<li>for: 这 paper 是关于 case-based reasoning 在法律 Setting 中的一种新方法，使用抽象的 argumentation 支持，Arguments 代表 case，Attack  между arguments 是由结果不同和 case 和 relevance 之间的 disagreement 引起的。</li>
<li>methods: 这 paper 使用了 decision trees 来自动学习 relevance，并结合 abstract argumentation 和 case-based reasoning 进行预测。</li>
<li>results: 对两个法律 Dataset 的验证表明，AA-CBR 和 decision tree-based learning of case relevance 在 comparison 中表现竞争力强，并且 AA-CBR 可以获得更加 Compact 的 Representation，可能对于获得 cognitively tractable explanations 是有利的。<details>
<summary>Abstract</summary>
Case-based reasoning is known to play an important role in several legal settings. In this paper we focus on a recent approach to case-based reasoning, supported by an instantiation of abstract argumentation whereby arguments represent cases and attack between arguments results from outcome disagreement between cases and a notion of relevance. In this context, relevance is connected to a form of specificity among cases. We explore how relevance can be learnt automatically in practice with the help of decision trees, and explore the combination of case-based reasoning with abstract argumentation (AA-CBR) and learning of case relevance for prediction in legal settings. Specifically, we show that, for two legal datasets, AA-CBR and decision-tree-based learning of case relevance perform competitively in comparison with decision trees. We also show that AA-CBR with decision-tree-based learning of case relevance results in a more compact representation than their decision tree counterparts, which could be beneficial for obtaining cognitively tractable explanations.
</details>
<details>
<summary>摘要</summary>
Case-based reasoning 知名于多个法律场景中扮演重要角色。在这篇论文中，我们关注了一种最近的case-based reasoning方法，基于抽象论证的实现，其中Arguments表示案例，而冲突 междуArguments来自结果不一致 между案例和一种 relevance 的概念。在这个上下文中， relevance 与案例特定性相关。我们研究如何在实践中自动学习case relevance，使用决策树，并将case-based reasoning与抽象论证（AA-CBR）和学习案例相关性结合以进行预测。我们发现，对于两个法律数据集，AA-CBR和决策树学习案例相关性可以与决策树相比，并且AA-CBR与决策树学习案例相关性可以获得更加紧凑的表示，这可能有助于获得更易于理解的解释。
</details></li>
</ul>
<hr>
<h2 id="LLMaAA-Making-Large-Language-Models-as-Active-Annotators"><a href="#LLMaAA-Making-Large-Language-Models-as-Active-Annotators" class="headerlink" title="LLMaAA: Making Large Language Models as Active Annotators"></a>LLMaAA: Making Large Language Models as Active Annotators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19596">http://arxiv.org/abs/2310.19596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ridiculouz/LLMaAA">https://github.com/ridiculouz/LLMaAA</a></li>
<li>paper_authors: Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou</li>
<li>for: 这篇论文旨在探讨如何充分利用大型自然语言处理（NLP）模型，以减少需要大量高质量标注数据的训练成本。</li>
<li>methods: 论文提出了一种称为LLMaAA的方法，将大型NLP模型（LLMs）训练为标注员，并将其置入一个活动学习循环中，以有效地决定需要标注的内容。</li>
<li>results: 实验结果显示， LLMaAA 可以比较有效率地将大量未标注数据训练为NLP任务，并且可以对任务特定的模型进行训练，以获得更高的性能。<details>
<summary>Abstract</summary>
Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.
</details>
<details>
<summary>摘要</summary>
通用的监督学习方法在自然语言处理（NLP）领域广泛使用，但这些方法需要大量高质量标注数据。在实践中，获取这些数据是一件昂贵的困难任务。在最近，大语言模型（LLM）的优异几个shot性能促进了数据生成的发展，其中数据 solely 由 LLM 生成。然而，这种方法通常受到低质量问题的困扰，需要数量级更多的标注数据以达到满意性。为了充分利用 LLM 的潜力并使用巨量的未标注数据，我们提出了 LLMaAA，它将 LLM 作为标注者，并将其放入活动学习循环来确定如何效率地标注。为了学习Robustly 使用 pseudo 标签，我们优化了标注和训练过程：（1）我们从小示例池中随机选择 k-NN 示例，并将其作为上下文示例使用，（2）我们采用示例权重技术，将训练样本分配learnable 权重。与之前的方法相比，LLMaAA 具有高效和可靠的特点。我们在名实Recognition 和关系抽取两个 класси型 NLP 任务上进行了实验和分析。使用 LLMaAA，由 LLM 生成的标签上训练的任务特定模型可以在只有百度之前达到教师水平，这是其他基eline 相比的费用更高的。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Locally-Stationary-Data-Using-Expert-Advice"><a href="#Prediction-of-Locally-Stationary-Data-Using-Expert-Advice" class="headerlink" title="Prediction of Locally Stationary Data Using Expert Advice"></a>Prediction of Locally Stationary Data Using Expert Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19591">http://arxiv.org/abs/2310.19591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir V’yugin, Vladimir Trunov</li>
<li>for: 本研究探讨了连续机器学习的问题。</li>
<li>methods: 本文使用游戏理论方法，不假设数据源的随机性，可以是抽象、算法或概率性，数据源的参数可以随机变化。提出了一种在线预测算法。</li>
<li>results: 提出的算法可以有效地预测本地站点时间序列。获得了效率估计。<details>
<summary>Abstract</summary>
The problem of continuous machine learning is studied. Within the framework of the game-theoretic approach, when for calculating the next forecast, no assumptions about the stochastic nature of the source that generates the data flow are used -- the source can be analog, algorithmic or probabilistic, its parameters can change at random times, when building a prognostic model, only structural assumptions are used about the nature of data generation. An online forecasting algorithm for a locally stationary time series is presented. An estimate of the efficiency of the proposed algorithm is obtained.
</details>
<details>
<summary>摘要</summary>
“ continuous machine learning 问题被研究。使用游戏理论方法时，不假设数据流源的随机性，数据流源可以是杂音、算法或概率的，它的参数可以随机时间变化，建立预测模型时只假设数据生成的结构。一种在本地站点时间序列上线预测算法被提出。对提出的算法的效率估计得到。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles"><a href="#CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles" class="headerlink" title="CreoleVal: Multilingual Multitask Benchmarks for Creoles"></a>CreoleVal: Multilingual Multitask Benchmarks for Creoles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19567">http://arxiv.org/abs/2310.19567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hclent/creoleval">https://github.com/hclent/creoleval</a></li>
<li>paper_authors: Heather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen, Marcell Fekete, Esther Ploeger, Li Zhou, Hans Erik Heje, Diptesh Kanojia, Paul Belony, Marcel Bollmann, Loïc Grobol, Miryam de Lhoneux, Daniel Hershcovich, Michel DeGraff, Anders Søgaard, Johannes Bjerva</li>
<li>for: 这个论文的目的是为了推动计算语言学和自然语言处理领域中关于克里奥语言的研究。</li>
<li>methods: 这篇论文使用了多种方法，包括创建了8种NLP任务的benchmark集合，以及对这些任务的零例设定实验。</li>
<li>results: 这篇论文的结果表明，通过利用克里奥语言之间的共同根，可以在zero-shot设定下实现transfer learning，并且可以为克里奥语言提供更多的 annotated data。<details>
<summary>Abstract</summary>
Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, the goal of CreoleVal is to empower research on Creoles in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for Creole language users around the globe.
</details>
<details>
<summary>摘要</summary>
创ollages represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between 创ollages and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to the lack of annotated data. In this work, we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 创ollages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for 创ollages, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for 创ollages. Ultimately, the goal of CreoleVal is to empower research on 创ollages in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for 创ollages language users around the globe.
</details></li>
</ul>
<hr>
<h2 id="A-General-Neural-Causal-Model-for-Interactive-Recommendation"><a href="#A-General-Neural-Causal-Model-for-Interactive-Recommendation" class="headerlink" title="A General Neural Causal Model for Interactive Recommendation"></a>A General Neural Causal Model for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19519">http://arxiv.org/abs/2310.19519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Peng Zhou, Xiangyu Zhao, Jun Li</li>
<li>for: This paper aims to mitigate survivor bias in observational data to optimize recommender systems towards global optima.</li>
<li>methods: The paper proposes a neural causal model to achieve counterfactual inference, using a learnable structural causal model, Gumbel-max function, and reinforcement optimizations.</li>
<li>results: The paper demonstrates the effectiveness of the proposed solution through both theoretical and empirical studies.<details>
<summary>Abstract</summary>
Survivor bias in observational data leads the optimization of recommender systems towards local optima. Currently most solutions re-mines existing human-system collaboration patterns to maximize longer-term satisfaction by reinforcement learning. However, from the causal perspective, mitigating survivor effects requires answering a counterfactual problem, which is generally unidentifiable and inestimable. In this work, we propose a neural causal model to achieve counterfactual inference. Specifically, we first build a learnable structural causal model based on its available graphical representations which qualitatively characterizes the preference transitions. Mitigation of the survivor bias is achieved though counterfactual consistency. To identify the consistency, we use the Gumbel-max function as structural constrains. To estimate the consistency, we apply reinforcement optimizations, and use Gumbel-Softmax as a trade-off to get a differentiable function. Both theoretical and empirical studies demonstrate the effectiveness of our solution.
</details>
<details>
<summary>摘要</summary>
specifically，我们首先构建了一个可学习的结构 causal model，基于可用的图形表示来质量地描述人们的喜好转移。在 mitigating  survivor bias 时，我们使用 counterfactual consistency。为了识别一致性，我们使用 Gumbel-max 函数作为结构约束。为了估计一致性，我们应用了强化优化，并使用 Gumbel-Softmax 作为一个可导的函数。 Both theoretical 和实验研究表明我们的解决方案的效果。
</details></li>
</ul>
<hr>
<h2 id="Inverse-folding-for-antibody-sequence-design-using-deep-learning"><a href="#Inverse-folding-for-antibody-sequence-design-using-deep-learning" class="headerlink" title="Inverse folding for antibody sequence design using deep learning"></a>Inverse folding for antibody sequence design using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19513">http://arxiv.org/abs/2310.19513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric A. Dreyer, Daniel Cutting, Constantin Schneider, Henry Kenlay, Charlotte M. Deane</li>
<li>for: 本研究是针对抗体序列设计，使用三维结构信息。</li>
<li>methods: 我们提出了一种特化于抗体结构的反向折叠模型，并对抗体序列恢复和结构稳定性进行了优化。我们还研究了补充定量区域的编码方法，并对这些方法进行了评估。</li>
<li>results: 我们的模型在抗体序列设计方面显示了 notable improvement，特别是在Hypervariable CDR-H3 loop中。我们还发现了一些physics-based方法可以用于评估提议的序列质量。<details>
<summary>Abstract</summary>
We consider the problem of antibody sequence design given 3D structural information. Building on previous work, we propose a fine-tuned inverse folding model that is specifically optimised for antibody structures and outperforms generic protein models on sequence recovery and structure robustness when applied on antibodies, with notable improvement on the hypervariable CDR-H3 loop. We study the canonical conformations of complementarity-determining regions and find improved encoding of these loops into known clusters. Finally, we consider the applications of our model to drug discovery and binder design and evaluate the quality of proposed sequences using physics-based methods.
</details>
<details>
<summary>摘要</summary>
我团队考虑了抗体序列设计问题，使用三维结构信息。基于先前的研究，我们提出了特制 inverse folding 模型，特化于抗体结构，并在抗体序列恢复和结构稳定性方面超越了通用蛋白质模型，具有显著改善的质量loop CDR-H3。我们研究了完整性决定区域的 canonical  conformations，并发现了这些循环的更好的编码到已知群体。最后，我们考虑了我们模型的应用于药物发现和绑定设计，并使用物理学方法评估提议的序列质量。Note that Simplified Chinese is a standardized form of Chinese that uses shorter words and phrases, and is commonly used in mainland China. Traditional Chinese is another form of Chinese that is commonly used in Hong Kong, Macau, and Taiwan, and has a more complex grammar and character set. If you prefer Traditional Chinese, I can provide the translation as well.
</details></li>
</ul>
<hr>
<h2 id="SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity"><a href="#SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity" class="headerlink" title="SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity"></a>SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19509">http://arxiv.org/abs/2310.19509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lswzjuer/SparseByteNN">https://github.com/lswzjuer/SparseByteNN</a></li>
<li>paper_authors: Haitao Xu, Songwei Liu, Yuyang Xu, Shuai Wang, Jiashi Li, Chenqian Yan, Liangqiang Li, Lean Fu, Xin Pan, Fangmin Chen</li>
<li>for: 本研究旨在解决网络大小增加的挑战，通过网络剔除来实现高速并高精度的移动推理。</li>
<li>methods: 本研究提出了一种新的移动推理加速框架——SparseByteNN，它利用细化的kernel缺省来实现实时执行并高精度。SparseByteNN包括两部分：（a）细化kernel缺省schema，其中设计了多种缺省模式 для不同的运算符。与我们提出的整个网络重新排序策略相结合，这个schema可以实现高压缩率和高精度同时。（b）基于缺省模式的推理引擎。</li>
<li>results: 实验结果表明，SparseByteNN在Qualcomm 855上对30% sparse MobileNet-v1实现1.27倍的速度提升和1.29倍的效率提升，与dense版本和State-of-the-art sparse推理引擎MNN相比，只有0.224%的精度下降。<details>
<summary>Abstract</summary>
To address the challenge of increasing network size, researchers have developed sparse models through network pruning. However, maintaining model accuracy while achieving significant speedups on general computing devices remains an open problem. In this paper, we present a novel mobile inference acceleration framework SparseByteNN, which leverages fine-grained kernel sparsity to achieve real-time execution as well as high accuracy. Our framework consists of two parts: (a) A fine-grained kernel sparsity schema with a sparsity granularity between structured pruning and unstructured pruning. It designs multiple sparse patterns for different operators. Combined with our proposed whole network rearrangement strategy, the schema achieves a high compression rate and high precision at the same time. (b) Inference engine co-optimized with the sparse pattern. The conventional wisdom is that this reduction in theoretical FLOPs does not translate into real-world efficiency gains. We aim to correct this misconception by introducing a family of efficient sparse kernels for ARM and WebAssembly. Equipped with our efficient implementation of sparse primitives, we show that sparse versions of MobileNet-v1 outperform strong dense baselines on the efficiency-accuracy curve. Experimental results on Qualcomm 855 show that for 30% sparse MobileNet-v1, SparseByteNN achieves 1.27x speedup over the dense version and 1.29x speedup over the state-of-the-art sparse inference engine MNN with a slight accuracy drop of 0.224%. The source code of SparseByteNN will be available at https://github.com/lswzjuer/SparseByteNN
</details>
<details>
<summary>摘要</summary>
为了解决网络规模的挑战，研究人员已经通过网络剔除来开发了稀疏模型。然而，在普通计算设备上实现显著的速度提升 while 保持模型准确性仍然是一个开放的问题。在这篇论文中，我们提出了一种新的移动设备推理加速框架SparseByteNN，该框架利用细化的kernel疏松来实现实时执行和高精度。我们的框架包括两部分：(a) 细化kernel疏松Schema，该Schema设计了多种不同操作符的疏松模式。通过我们的提议的整个网络重新排序策略，Schema可以同时实现高压缩率和高精度。(b) 推理引擎与疏松模式相似的优化。传统的思路是，这种理论的FLOPs减少不会在实际中带来性能提升。我们希望通过引入高效的疏松元素来改变这种误区，并证明稀疏版的MobileNet-v1可以在效率-准确度曲线上超越强 dense 基准。我们的实验结果表明，在Qualcomm 855上，为30%稀疏的MobileNet-v1，SparseByteNN可以相比 dense 版本提供1.27倍的速度提升和1.29倍的速度提升，与MNN相比有一定的精度下降（0.224%）。SparseByteNN的源代码将在https://github.com/lswzjuer/SparseByteNN上公开。
</details></li>
</ul>
<hr>
<h2 id="Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination"><a href="#Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination" class="headerlink" title="Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination"></a>Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19503">http://arxiv.org/abs/2310.19503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis-Daniel Ibáñez, John Domingue, Sabrina Kirrane, Oshani Seneviratne, Aisling Third, Maria-Esther Vidal</li>
<li>for: 本研究旨在支持基于知识 graphs（KGs）的人工智能（AI），以确保公民自主权的保障。</li>
<li>methods: 本研究使用了基于 neuronal learning（e.g., Large Language Models（LLMs））的 neuro-symbolic AI，以探讨如何使AI系统的输出可信、数据驱动和内部工作方式可见，以及如何使AI系统负责决策的问题。</li>
<li>results: 本研究提出了一个研究计划，以支持KG-based AI的发展，并在实际场景中analyzed了公民自主权的挑战和机遇。<details>
<summary>Abstract</summary>
Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions. The regulation sets what technologists need to do, leading to questions concerning: How can the output of AI systems be trusted? What is needed to ensure that the data fuelling and the inner workings of these artefacts are transparent? How can AI be made accountable for its decision-making? This paper conceptualises the foundational topics and research pillars to support KG-based AI for self-determination. Drawing upon this conceptual framework, challenges and opportunities for citizen self-determination are illustrated and analysed in a real-world scenario. As a result, we propose a research agenda aimed at accomplishing the recommended objectives.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 已经成为智能决策的基础 плаform，并在许多大公司，如 Google、Walmart 和 Airbnb 中使用。KGs 补充机器学习 (ML) 算法，提供数据 контекст和 semantics，因此可以实现更多的推理和问答能力。目前，KGs 与 neuronal learning（例如大语言模型）的结合，被称为 neuro-symbolic AI，是活跃的研究领域。然而，随着 KG-based AI 的广泛应用，公民的自主权可能会遭受威胁。我们越依赖这些中央化的技术，我们就越无法决定自己的命运。为了解决这个问题，AI 的规制，如欧盟 (EU) AI 法规，在某些地区被提出。这些法规规定了技术人员需要做什么，导致问题：如何确保 AI 系统输出的可信？如何确保数据驱动和这些 artifacts 的内部工作是透明的？如何让 AI 受到决策的责任？本文概括了基础主题和研究柱，以支持 KG-based AI 的自主权。基于这个概念框架，我们 illustrated 和分析了在实际场景中的挑战和机遇，并提出了研究计划，以实现这些目标。
</details></li>
</ul>
<hr>
<h2 id="Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal"><a href="#Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal" class="headerlink" title="Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal"></a>Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19463">http://arxiv.org/abs/2310.19463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aicenter/optimize-planning-heuristics-to-rank">https://github.com/aicenter/optimize-planning-heuristics-to-rank</a></li>
<li>paper_authors: Leah Chrestien, Tomás Pevný, Stefan Edelkamp, Antonín Komenda</li>
<li>for: 本研究旨在优化搜索算法中的启发函数参数，以提高搜索效率。</li>
<li>methods: 本研究使用了解决过的问题实例来优化启发函数参数，并提出了一家基于排名的损失函数家族。</li>
<li>results: 实验比较表明，基于排名的损失函数能够有效地优化搜索算法的性能。<details>
<summary>Abstract</summary>
In imitation learning for planning, parameters of heuristic functions are optimized against a set of solved problem instances. This work revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A* and greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory.
</details>
<details>
<summary>摘要</summary>
在模仿学习方面的规划中，规则函数的参数被优化对于一组解决过的问题实例。这项工作检查了前向搜索算法，主要是A*和偏好最佳搜索，它们只在返回优质路径上展开状态。然后，它提出了一个基于排名的家族函数，特制为给定的前向搜索算法。此外，从学习理论的角度来看，它讨论了优化成本到目标的成本是无法很容易的。实验比较在多种问题上，不可遗憾地支持 derive 的理论。Here's the breakdown of the translation:* "imitation learning" is translated as "模仿学习" (mó shì xué xí)* "parameters of heuristic functions" is translated as "规则函数的参数" (guī zhèng fún xiàng)* "optimized against a set of solved problem instances" is translated as "被优化对于一组解决过的问题实例" (bèi yǐng gāo jì zhèng yì jī)* "strictly optimally efficient heuristics" is translated as "前向搜索算法" (qian yù sōu xiǎng suān)* "which expand only states on the returned optimal path" is translated as "它们只在返回优质路径上展开状态" (tā men zhǐ shàng zhèng jì lù pí thàn)* "loss functions based on ranking" is translated as "基于排名的家族函数" (jī bù pinyīn de jiā zú fún)* "from a learning theory point of view" is translated as "从学习理论的角度来看" (cong xué xí lǐ yì zhì)* "optimizing cost-to-goal \hstar\ is unnecessarily difficult" is translated as "优化成本到目标的成本是无法很容易的" (yòu gǎn jī zhèng yì jī zhèng yì)* "The experimental comparison on a diverse set of problems unequivocally supports the derived theory" is translated as "实验比较在多种问题上，不可遗憾地支持 derive 的理论" (shí yè bǐ jiǎo zài yī yī wèn tí zhèng yì)
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI"><a href="#Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI" class="headerlink" title="Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI"></a>Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19460">http://arxiv.org/abs/2310.19460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 这个论文旨在提出一种实用的无线通信系统，以便在具有硬件缺陷的接收器下实现数据生成。</li>
<li>methods: 该论文使用的方法是基于扩散模型（DDPM），它将数据生成过程分解成多个步骤，以减少数据生成过程中的噪声。</li>
<li>results: 该论文的结果表明，使用该方法可以在低SNR条件下提供网络可靠性，并且在不同的硬件缺陷水平和量化误差下实现几乎不变的重建性表现。同时，该方法还可以在非泊松噪声下实现稳定的重建性表现。<details>
<summary>Abstract</summary>
Thanks to the outstanding achievements from state-of-the-art generative models like ChatGPT and diffusion models, generative AI has gained substantial attention across various industrial and academic domains. In this paper, denoising diffusion probabilistic models (DDPMs) are proposed for a practical finite-precision wireless communication system with hardware-impaired transceivers. The intuition behind DDPM is to decompose the data generation process over the so-called "denoising" steps. Inspired by this, a DDPM-based receiver is proposed for a practical wireless communication scheme that faces realistic non-idealities, including hardware impairments (HWI), channel distortions, and quantization errors. It is shown that our approach provides network resilience under low-SNR regimes, near-invariant reconstruction performance with respect to different HWI levels and quantization errors, and robust out-of-distribution performance against non-Gaussian noise. Moreover, the reconstruction performance of our scheme is evaluated in terms of cosine similarity and mean-squared error (MSE), highlighting more than 25 dB improvement compared to the conventional deep neural network (DNN)-based receivers.
</details>
<details>
<summary>摘要</summary>
thanks to the outstanding achievements of state-of-the-art generative models like ChatGPT and diffusion models, generative AI has gained substantial attention across various industrial and academic domains. In this paper, denoising diffusion probabilistic models (DDPMs) are proposed for a practical finite-precision wireless communication system with hardware-impaired transceivers. The intuition behind DDPM is to decompose the data generation process over the so-called "denoising" steps. Inspired by this, a DDPM-based receiver is proposed for a practical wireless communication scheme that faces realistic non-idealities, including hardware impairments (HWI), channel distortions, and quantization errors. It is shown that our approach provides network resilience under low-SNR regimes, near-invariant reconstruction performance with respect to different HWI levels and quantization errors, and robust out-of-distribution performance against non-Gaussian noise. Moreover, the reconstruction performance of our scheme is evaluated in terms of cosine similarity and mean-squared error (MSE), highlighting more than 25 dB improvement compared to the conventional deep neural network (DNN)-based receivers.Here's the translation in Traditional Chinese:感谢现代 générative AI 的杰出成就，如 ChatGPT 和扩散模型，它们在不同的工业和学术领域获得了广泛的注意。在这篇论文中，我们提出了一种实用的 finite-precision 无线通信系统，使用受到硬件问题的接收器。DDPM 的假设是将数据生成过程分解为“混杂”步骤。受到这个想法的激发，我们提出了一种基于 DDPM 的接收器，用于实际的无线通信问题，面临实际的非理想情况，包括硬件问题（HWI）、频道扭曲和采样误差。我们展示了我们的方法在低 SNR 情况下提供了网络可靠性，并且在不同的 HWI 水平和采样误差水平下提供了近似对称重建性和Robust OOD 性。此外，我们评估了我们的方法的重建性，使用 cosine similarity 和 mean-squared error (MSE)，显示了较于 25 dB 的改善，相比于传统的深度神经网络（DNN）基于接收器。
</details></li>
</ul>
<hr>
<h2 id="ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction"><a href="#ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction" class="headerlink" title="ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction"></a>ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19453">http://arxiv.org/abs/2310.19453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Yong Yu</li>
<li>for: 这 paper 的目的是提出一种新的 CTR 预测方法，以提高个性化在线服务中 CTR 预测的准确率。</li>
<li>methods: 这 paper 使用了两种不同的模型，一种是传统的 CTR 模型，它使用一个简单的一阶FeatueInteraction模型来捕捉协作信号。另一种是使用 pretrained language models (PLMs) 来提取语义知识，并将其与 CTR 模型结合使用。</li>
<li>results: 该 paper 的实验结果表明，使用 ALT 方法可以大幅提高 CTR 预测的准确率，并且可以与不同的语言和 CTR 模型结合使用。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction plays as a core function module in various personalized online services. According to the data modality and input format, the models for CTR prediction can be mainly classified into two categories. The first one is the traditional CTR models that take as inputs the one-hot encoded ID features of tabular modality, which aims to capture the collaborative signals via feature interaction modeling. The second category takes as inputs the sentences of textual modality obtained by hard prompt templates, where pretrained language models (PLMs) are adopted to extract the semantic knowledge. These two lines of research generally focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. Therefore, in this paper, we propose to conduct fine-grained feature-level Alignment between Language and CTR models (ALT) for CTR prediction. Apart from the common CLIP-like instance-level contrastive learning, we further design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose three different finetuning strategies with the option to train the aligned language and CTR models separately or jointly for downstream CTR prediction tasks, thus accommodating the varying efficacy and efficiency requirements for industrial applications. Extensive experiments on three real-world datasets demonstrate that ALT outperforms SOTA baselines, and is highly compatible for various language and CTR models.
</details>
<details>
<summary>摘要</summary>
Click-through rate（CTR）预测作为个人化在线服务的核心功能模块，可以根据数据模式和输入格式分为两类模型。第一类是传统的 CTR 模型，通过一个简单的一Hot编码ID特征来捕捉协作信号，而第二类则是使用硬模板生成的文本数据，通过预训练语言模型（PLM）提取semantic知识。这两种研究方向通常都专注于输入数据（即文本和表格modalities）的不同特征，形成了一种明确的补做关系。因此，在这篇论文中，我们提议通过细致的特征水平匹配（ALT）来进行 CTR 预测。除了常见的 CLIP-like 类型的INSTANCE-level对比学习外，我们还设计了一种新的联合重建预训练任务，以便在语言和表格模型之间建立特征水平的交互和匹配。此外，我们还提出了三种不同的Finetuning策略，可以根据下游应用的效率和可行性要求来训练一起或分别训练语言和 CTR 模型。广泛的实验表明，ALT 可以超过当前的基eline，并且可以高效地与不同的语言和 CTR 模型结合使用。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency"><a href="#Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency" class="headerlink" title="Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency"></a>Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19449">http://arxiv.org/abs/2310.19449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ralf Graafe, Qutub Syed Sha, Florian Geissler, Michael Paulitsch<br>for:This paper aims to address the issue of silent data errors (SDE) in Neural Networks (NN) caused by hardware faults, and to develop a novel fault injection framework called PyTorchALFI to study the effects of hardware faults on software and NN models.methods:The paper uses a novel fault injection framework called PyTorchALFI, which is based on PyTorchFI, to inject faults into PyTorch models and study their effects. The framework provides an efficient way to define randomly generated and reusable sets of faults, enhances data sets, and generates test KPIs.results:The paper provides details about the definition of test scenarios, software architecture, and several examples of how to use the new framework to apply iterative changes in fault location and number, compare different model modifications, and analyze test results. The results show that PyTorchALFI can effectively study the effects of hardware faults on NN models and provide insights into the robustness of these models.Here is the simplified Chinese translation of the three points:for:这篇论文目标是解决神经网络（NN）中的静默数据错误（SDE），由硬件错误引起的问题，并开发了一个新的硬件投入框架called PyTorchALFI。方法:论文使用基于PyTorchFI的PyTorchALFI框架，将硬件错误投入到PyTorch模型中，以研究其影响。该框架提供了一种高效的 randomly生成和重用的硬件错误集，增强数据集，并生成测试KPI。结果:论文提供了测试场景的定义，软件架构，以及使用新框架进行迭代更改硬件错误的位置和数量，比较不同的模型修改，并分析测试结果的方法。结果表明，PyTorchALFI可以有效地研究硬件错误对NN模型的影响，并为NN模型的可靠性提供了新的视角。<details>
<summary>Abstract</summary>
Transient or permanent faults in hardware can render the output of Neural Networks (NN) incorrect without user-specific traces of the error, i.e. silent data errors (SDE). On the other hand, modern NNs also possess an inherent redundancy that can tolerate specific faults. To establish a safety case, it is necessary to distinguish and quantify both types of corruptions. To study the effects of hardware (HW) faults on software (SW) in general and NN models in particular, several fault injection (FI) methods have been established in recent years. Current FI methods focus on the methodology of injecting faults but often fall short of accounting for large-scale FI tests, where many fault locations based on a particular fault model need to be analyzed in a short time. Results need to be concise, repeatable, and comparable. To address these requirements and enable fault injection as the default component in a machine learning development cycle, we introduce a novel fault injection framework called PyTorchALFI (Application Level Fault Injection for PyTorch) based on PyTorchFI. PyTorchALFI provides an efficient way to define randomly generated and reusable sets of faults to inject into PyTorch models, defines complex test scenarios, enhances data sets, and generates test KPIs while tightly coupling fault-free, faulty, and modified NN. In this paper, we provide details about the definition of test scenarios, software architecture, and several examples of how to use the new framework to apply iterative changes in fault location and number, compare different model modifications, and analyze test results.
</details>
<details>
<summary>摘要</summary>
非暂时或永久的硬件故障可以让神经网络（NN）输出错误无法诊断到用户特定的错误迹象，即静默数据错误（SDE）。然而，现代NN也拥有内置的重复性，可以承受特定的故障。为建立安全案例，需要分化和评估两种类型的损害。为了研究硬件（HW）故障对软件（SW）的影响，以及NN模型的影响，数年来有多种硬件故障插入（FI）方法得到了创新。当前FI方法通常专注于插入故障的方法，而很少考虑大规模FI测试，其中需要对特定故障模型中的多个故障位置进行分析，并在短时间内进行测试。结果需要是简洁、重复、比较。为了解决这些需求并使硬件故障插入成为机器学习开发周期中的默认组件，我们介绍了一个新的硬件故障插入框架 called PyTorchALFI（基于PyTorch的应用层硬件故障插入）。PyTorchALFI提供了一种效率的方式来定义随机生成的和可重用的硬件故障，定义复杂的测试场景，增强数据集，生成测试KPI，同时紧密地集成 fault-free、FAULTY 和修改后的NN。在这篇文章中，我们提供了定义测试场景、软件架构以及多个例子，用于使用新框架进行应用iterative改变故障位置和数量，比较不同的模型修改，以及分析测试结果。
</details></li>
</ul>
<hr>
<h2 id="Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations"><a href="#Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations" class="headerlink" title="Explaining the Decisions of Deep Policy Networks for Robotic Manipulations"></a>Explaining the Decisions of Deep Policy Networks for Robotic Manipulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19432">http://arxiv.org/abs/2310.19432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongun Kim, Jaesik Choi</li>
<li>for: This paper aims to enhance the transparency of deep policy networks in robotic manipulation by explaining robot behaviors through input attribution methods.</li>
<li>methods: The paper presents two methods for applying input attribution methods to robot policy networks: (1) measuring the importance factor of each joint torque and (2) modifying a relevance propagation method to handle negative inputs and outputs.</li>
<li>results: The paper reports on the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation, to the best of the authors’ knowledge.Here’s the simplified Chinese text format you requested:</li>
<li>for: 这篇论文目标是通过输入归因方法解释深度策略网络在机器人控制中的行为。</li>
<li>methods: 论文提出了两种应用输入归因方法到机器人策略网络的方法：（1）测量每个 JOINT 扭矩的重要性因子，以反映机器人的 END-EFFECTOR 运动受 JOINT 扭矩的影响；（2）修改 relevance propagation 方法，以正确处理深度策略网络中的负输入和输出。</li>
<li>results: 论文报告了深度策略网络在机器人控制中的输入归因变化，特别是在多Modal 感知输入中。<details>
<summary>Abstract</summary>
Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to reflect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.
</details>
<details>
<summary>摘要</summary>
深度策略网络可以让机器人学习解决各种复杂实际任务的终端方式。然而，它缺乏透明度，不能提供行为的原因。因此，这种黑盒模型在实践中常导致低可靠性和干扰行为。为了增强其透明度，需要解释机器人行为，考虑每个输入特征对决策的影响程度。在这篇论文中，我们通过输入贡献方法来解释深度策略模型中每个输入特征对机器人决策的影响。我们提出了两种应用输入贡献方法来解释机器人策略网络：（1）测量每个 JOINT 扭矩的重要性因子，以反映电动机扭矩对终端运动的影响，（2）修改 relevance propagation 方法，以处理深度策略网络中的负输入和输出。我们认为，这是首次在深度策略网络中在线识别多Modal 感知输入的动态变化。
</details></li>
</ul>
<hr>
<h2 id="Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans"><a href="#Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans" class="headerlink" title="Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans"></a>Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19427">http://arxiv.org/abs/2310.19427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leekwoon/rgg">https://github.com/leekwoon/rgg</a></li>
<li>paper_authors: Kyowoon Lee, Seongun Kim, Jaesik Choi</li>
<li>for: 这 paper 的目的是提出一种新的方法来修正由扩散模型生成的不可靠的计划，以便在安全关键应用中使用扩散模型。</li>
<li>methods: 该 paper 使用的方法包括提出了一种新的 metric  named restoration gap，以评估扩散模型生成的计划质量，并提出了一种 attribution map regularizer 来防止由 sub-optimal gap predictor 生成的 adversarial refining guidance。</li>
<li>results: 该 paper 在三个不同的 benchmark 上进行了评估，并证明了其效果。同时， paper 还提供了一种 explainability 的机制，通过显示 gap predictor 的归因地图，以便更深入地理解生成的计划。<details>
<summary>Abstract</summary>
Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.
</details>
<details>
<summary>摘要</summary>
这文本将被翻译为简化字Simplified Chinese。Diffusion-based planning在长时间、罕见的奖励任务中显示出了有前途的结果，通过训练涡散模型和使用辅助指南函数来条件sampled trajectories。然而，由于它们是生成型模型，所以diffusion models不能保证生成可行的计划，导致执行失败和禁止planners在安全敏感应用中使用。在这个工作中，我们提出了一种新的方法，用于精焕不可靠的计划，包括提供修复指南来修正错误的计划。为此，我们建议了一个新的度量名为修复差，用于评估单一计划的质量。修复差是由一个差分预测器生成的修复差指南，用于修正不可靠的计划。我们还提出了一个属性图调整仪来防止由低质量的差分预测器生成的对抗式修复指南，允许进一步修正不可靠的计划。我们在三个不同的benchmark上证明了我们的方法的有效性，并显示了我们的方法具有解释性，通过显示差分预测器的属性图和显示错误的转换，允许更深入的理解生成的计划。
</details></li>
</ul>
<hr>
<h2 id="Artificial-intelligence-and-the-limits-of-the-humanities"><a href="#Artificial-intelligence-and-the-limits-of-the-humanities" class="headerlink" title="Artificial intelligence and the limits of the humanities"></a>Artificial intelligence and the limits of the humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19425">http://arxiv.org/abs/2310.19425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Włodzisław Duch</li>
<li>for: 本研究旨在探讨现代世界中文化复杂性的问题，以及人类认知的限制和人工智能的发展对人文学科的影响。</li>
<li>methods: 本研究采用了跨学科的方法，包括认知科学、人工智能和数据分析等，以探讨人文学科在数字时代中的发展和变革。</li>
<li>results: 研究发现，人工智能的发展将导致人文学科的重要性减退，而新的、跨学科的人文学科将出现，可以为现代社会提供更多的解决方案。此外，人工智能将对人文学科产生深远的影响，从艺术到政治科学和哲学，使这些领域变得更加吸引人和有前途。<details>
<summary>Abstract</summary>
The complexity of cultures in the modern world is now beyond human comprehension. Cognitive sciences cast doubts on the traditional explanations based on mental models. The core subjects in humanities may lose their importance. Humanities have to adapt to the digital age. New, interdisciplinary branches of humanities emerge. Instant access to information will be replaced by instant access to knowledge. Understanding the cognitive limitations of humans and the opportunities opened by the development of artificial intelligence and interdisciplinary research necessary to address global challenges is the key to the revitalization of humanities. Artificial intelligence will radically change humanities, from art to political sciences and philosophy, making these disciplines attractive to students and enabling them to go beyond current limitations.
</details>
<details>
<summary>摘要</summary>
现代世界的文化复杂性已经超出了人类理解的限度。认知科学怀疑传统基于心理模型的解释。核心人文科目可能会失去重要性。人文科学需要适应数字时代。新的跨学科人文科学出现。快速获取信息将被快速获取知识所替代。理解人类认知限制和人工智能和跨学科研究的发展机遇是人文科学的重要复兴之路。人工智能将对人文科学产生杰然的变革，从艺术到政治科学和哲学，使这些学科变得吸引人并让其走过当前的限制。
</details></li>
</ul>
<hr>
<h2 id="Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills"><a href="#Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills" class="headerlink" title="Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills"></a>Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19424">http://arxiv.org/abs/2310.19424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seongun-kim/vcrl">https://github.com/seongun-kim/vcrl</a></li>
<li>paper_authors: Seongun Kim, Kyowoon Lee, Jaesik Choi</li>
<li>for: 本研究旨在提出一种基于相互信息的强化学习框架，以便自动地无需任务奖励函数而学习复杂技能。</li>
<li>methods: 本研究使用了变量强化学习和目标条件强化学习的概念，并将其重新推出为课程学习。同时，我们还提出了一种基于信息理论的无监督技能发现方法，称为Value Uncertainty Variational Curriculum（VUVC）。</li>
<li>results: 我们的方法在复杂的导航和机器人控制任务中显示了更高的样本效率和状态覆盖速度，并且在实际世界机器人导航任务中成功地应用了已经学习的技能。同时，将这些技能与全球规划器结合使得任务性能得到进一步提高。<details>
<summary>Abstract</summary>
Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details>
<details>
<summary>摘要</summary>
互信息基 reinforcement learning (RL) 已被提议为自动学习复杂技能的有望框架，无需任务特定奖励函数，通过互信息 (MI) 最大化或变量赋能。然而，学习复杂技能仍然具有挑战，因为训练技能的顺序可以大大影响样本效率。 inspirited by this， we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details></li>
</ul>
<hr>
<h2 id="Text-to-3D-with-Classifier-Score-Distillation"><a href="#Text-to-3D-with-Classifier-Score-Distillation" class="headerlink" title="Text-to-3D with Classifier Score Distillation"></a>Text-to-3D with Classifier Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19415">http://arxiv.org/abs/2310.19415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi</li>
<li>for: 这个论文主要是为了研究文本到3D形态转换的技术，特别是基于Score Distillation Sampling（SDS）的方法。</li>
<li>methods: 这篇论文使用了一种新的方法，即Classifier Score Distillation（CSD），它利用了预训练的2D扩散模型，并发现了一种意外的发现：导航独立于分类器可以带来有效的文本到3D形态转换任务。</li>
<li>results: 论文在不同的文本到3D任务上，包括形状生成、 текстур合成和形状编辑等，实现了与当前最佳方法相当的效果。 project page: <a target="_blank" rel="noopener" href="https://xinyu-andy.github.io/Classifier-Score-Distillation">https://xinyu-andy.github.io/Classifier-Score-Distillation</a><details>
<summary>Abstract</summary>
Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project page is https://xinyu-andy.github.io/Classifier-Score-Distillation
</details>
<details>
<summary>摘要</summary>
文本到3D生成技术在近期内Has made remarkable progress, especially with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. Although the use of classifier-free guidance is widely recognized as crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is sufficient for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights into existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks, including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project page is <https://xinyu-andy.github.io/Classifier-Score-Distillation>.
</details></li>
</ul>
<hr>
<h2 id="Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting"><a href="#Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting" class="headerlink" title="Resource Constrained Semantic Segmentation for Waste Sorting"></a>Resource Constrained Semantic Segmentation for Waste Sorting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19407">http://arxiv.org/abs/2310.19407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting">https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting</a></li>
<li>paper_authors: Elisa Cascina, Andrea Pellegrino, Lorenzo Tozzi</li>
<li>for: 这个研究旨在发展一种高效的垃圾分类策略，以减少垃圾的环境影响。</li>
<li>methods: 本研究提出了一种具有10MB内存限制的资源受限 semantic segmentation模型，适用于对工业设置进行垃圾分类。研究将ICNet、BiSeNet（Xception39底层）和ENet三种网络作为实验用例。</li>
<li>results: 经过实验后，研究发现可以通过优化和剪裁技术，实现模型的优化，并且可以对垃圾分类 Task 取得更好的性能。此外，研究还提出了一种焦点和lovász损失函数的结合，以解决垃圾分类 зада难以掌握的隐藏类别问题。<details>
<summary>Abstract</summary>
This work addresses the need for efficient waste sorting strategies in Materials Recovery Facilities to minimize the environmental impact of rising waste. We propose resource-constrained semantic segmentation models for segmenting recyclable waste in industrial settings. Our goal is to develop models that fit within a 10MB memory constraint, suitable for edge applications with limited processing capacity. We perform the experiments on three networks: ICNet, BiSeNet (Xception39 backbone), and ENet. Given the aforementioned limitation, we implement quantization and pruning techniques on the broader nets, achieving positive results while marginally impacting the Mean IoU metric. Furthermore, we propose a combination of Focal and Lov\'asz loss that addresses the implicit class imbalance resulting in better performance compared with the Cross-entropy loss function.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Othello-is-Solved"><a href="#Othello-is-Solved" class="headerlink" title="Othello is Solved"></a>Othello is Solved</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19387">http://arxiv.org/abs/2310.19387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwarot1/othello-ai">https://github.com/mwarot1/othello-ai</a></li>
<li>paper_authors: Hiroki Takizawa</li>
<li>for: 这篇论文是解决扑克游戏“奥特洛”的 computationally solving 问题的。</li>
<li>methods: 该论文使用了计算机科学中的搜索技术来解决游戏。</li>
<li>results: 该论文提出了一个重要突破，即确认了扑克游戏的完美游戏结果，即无误的游戏结果。<details>
<summary>Abstract</summary>
The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game position. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved, computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides the solution which enables software to play the game perfectly.
</details>
<details>
<summary>摘要</summary>
投筹游戏“奥赛”是全球最复杂且受欢迎的游戏之一，尚未被计算解决。奥赛拥有约10个十进制废弃数（10到58次方）可能的游戏记录和10个十进制废弃数（10到28次方）可能的游戏位置。解决奥赛的挑战，即确定没有任何错误的游戏记录，长期被计算机科学视为一项大挑战。本文宣布了一项重要突破：奥赛已经被计算解决，确定了完美游戏记录，导致游戏双方都不会赢。强大的奥赛软件已经使用了经验设计的搜索技术。解决游戏提供了解决方案，使软件可以完美地游戏。
</details></li>
</ul>
<hr>
<h2 id="Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts"><a href="#Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts" class="headerlink" title="Protecting Publicly Available Data With Machine Learning Shortcuts"></a>Protecting Publicly Available Data With Machine Learning Shortcuts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19381">http://arxiv.org/abs/2310.19381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Maximilian Burgert, Pascal Debus, Jennifer Williams, Philip Sperl, Konstantin Böttinger</li>
<li>for: 保护在线数据库免受非法抓取（protect online databases from unauthorized crawling）</li>
<li>methods: 利用机器学习快捷（ML shortcuts）的潜在假 correlate（potential spurious correlations），并通过采用异常值填充（outlier filling）技术来防止其泄露（leakage）。</li>
<li>results: 通过实验结果表明，提出的方法可以在三个实际应用场景中减少非法抓取的风险，同时也减少了人类的察觉阈值（threshold of human perception）。<details>
<summary>Abstract</summary>
Machine-learning (ML) shortcuts or spurious correlations are artifacts in datasets that lead to very good training and test performance but severely limit the model's generalization capability. Such shortcuts are insidious because they go unnoticed due to good in-domain test performance. In this paper, we explore the influence of different shortcuts and show that even simple shortcuts are difficult to detect by explainable AI methods. We then exploit this fact and design an approach to defend online databases against crawlers: providers such as dating platforms, clothing manufacturers, or used car dealers have to deal with a professionalized crawling industry that grabs and resells data points on a large scale. We show that a deterrent can be created by deliberately adding ML shortcuts. Such augmented datasets are then unusable for ML use cases, which deters crawlers and the unauthorized use of data from the internet. Using real-world data from three use cases, we show that the proposed approach renders such collected data unusable, while the shortcut is at the same time difficult to notice in human perception. Thus, our proposed approach can serve as a proactive protection against illegitimate data crawling.
</details>
<details>
<summary>摘要</summary>
（简化中文）机器学习（ML）快捷或假相关性是数据集中的假象，它们会导致模型在训练和测试阶段表现非常好，但是很难在泛化中使用。这些快捷是隐藏的，因为它们在域内测试中表现非常好。在这篇论文中，我们研究了不同的快捷的影响，并证明了简单的快捷也很难被解释AI方法发现。然后，我们利用这一点，设计了一种防止网络数据库被抓取的方法：提供者如约会平台、时尚制造商或二手车销售商需要面对专业化的抓取行业，这些行业会大规模地抓取并重新销售数据点。我们显示了可以通过故意添加ML快捷来创建一种防止抓取的办法。这些扩展的数据集是不可用于ML用例，这种方法可以防止抓取和未经授权使用数据。使用实际数据，我们显示了我们的方法可以干扰收集的数据，同时快捷难以被人类发现。因此，我们的方法可以作为一种积极的数据保护 measure。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators"><a href="#Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators" class="headerlink" title="Few-shot Hybrid Domain Adaptation of Image Generators"></a>Few-shot Hybrid Domain Adaptation of Image Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19378">http://arxiv.org/abs/2310.19378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/echopluto/fhda">https://github.com/echopluto/fhda</a></li>
<li>paper_authors: Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Tu Zheng, Zheng Yang, Wenxiao Wang, Xiaohui Zhong, Xiaobo Ren, Xiaofei He</li>
<li>for: 能否适应多个目标领域的混合领域？</li>
<li>methods: 我们提出了一种无损器框架，通过直接编码不同领域的图像到可分离的子空间来解决这问题。我们还提出了一种新的方向loss，该损失函数包括距离损失和方向损失。</li>
<li>results: 我们的方法可以在单个适应器中获得多个目标领域的各种特征，比基eline方法在 semantic similarity、图像准确率和 cross-domain consistency 方面表现出色。<details>
<summary>Abstract</summary>
Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a source generator and several target domains, HDA aims to acquire an adapted generator that preserves the integrated attributes of all target domains, without overriding the source domain's characteristics. Compared with Domain Adaptation (DA), HDA offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, HDA also presents more challenges than DA as we have access only to images from individual target domains and lack authentic images from the hybrid domain. To address this issue, we introduce a discriminator-free framework that directly encodes different domains' images into well-separable subspaces. To achieve HDA, we propose a novel directional subspace loss comprised of a distance loss and a direction loss. Concretely, the distance loss blends the attributes of all target domains by reducing the distances from generated images to all target subspaces. The direction loss preserves the characteristics from the source domain by guiding the adaptation along the perpendicular to subspaces. Experiments show that our method can obtain numerous domain-specific attributes in a single adapted generator, which surpasses the baseline methods in semantic similarity, image fidelity, and cross-domain consistency.
</details>
<details>
<summary>摘要</summary>
可以使用预训练的生成器适应多个目标Domain的混合吗？在这项工作中，我们介绍了一个新任务——几shot Hybrid Domain Adaptation（HDA）。给定一个源生成器和多个目标Domain，HDA的目标是获得一个适应了所有目标Domain的特征的生成器，而不是覆盖源Domain的特征。相比于Domain Adaptation（DA），HDA具有更多的灵活性和可能性，以适应更复杂和广泛的Domain。然而，HDA也面临着更多的挑战，因为我们只有各个目标Domain的图像，缺乏真实的混合Domain的图像。为解决这个问题，我们提出了一个无需权重的框架，直接将不同的Domain的图像编码成分离的子空间。为实现HDA，我们提议一种新的方向性子空间损失，包括距离损失和方向损失。具体来说，距离损失将所有目标Domain的特征混合到生成图像中，而方向损失保持源Domain的特征，使适应过程垂直于子空间。实验表明，我们的方法可以在单个适应器中获得多个Domain的特征，超越基eline方法的semantic similarity、图像准确性和交叉Domain一致性。
</details></li>
</ul>
<hr>
<h2 id="RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules"><a href="#RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules" class="headerlink" title="RGB-X Object Detection via Scene-Specific Fusion Modules"></a>RGB-X Object Detection via Scene-Specific Fusion Modules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19372">http://arxiv.org/abs/2310.19372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsriaditya999/rgbxfusion">https://github.com/dsriaditya999/rgbxfusion</a></li>
<li>paper_authors: Sri Aditya Deevi, Connor Lee, Lu Gan, Sushruth Nagesh, Gaurav Pandey, Soon-Jo Chung</li>
<li>for: 提高自动驾驶车辆在各种天气条件下视觉理解环境的能力</li>
<li>methods: 利用小量、协调的多模式数据进行预训练、Scene-specific fusión模块来拼接单模式模型</li>
<li>results: 与现有方法相比，实现更高的融合性能，只需要小量额外参数Translation:</li>
<li>for: 提高自动驾驶车辆在各种天气条件下视觉理解环境的能力</li>
<li>methods: 利用小量、协调的多模式数据进行预训练、Scene-specific fusión模块来拼接单模式模型</li>
<li>results: 与现有方法相比，实现更高的融合性能，只需要小量额外参数<details>
<summary>Abstract</summary>
Multimodal deep sensor fusion has the potential to enable autonomous vehicles to visually understand their surrounding environments in all weather conditions. However, existing deep sensor fusion methods usually employ convoluted architectures with intermingled multimodal features, requiring large coregistered multimodal datasets for training. In this work, we present an efficient and modular RGB-X fusion network that can leverage and fuse pretrained single-modal models via scene-specific fusion modules, thereby enabling joint input-adaptive network architectures to be created using small, coregistered multimodal datasets. Our experiments demonstrate the superiority of our method compared to existing works on RGB-thermal and RGB-gated datasets, performing fusion using only a small amount of additional parameters. Our code is available at https://github.com/dsriaditya999/RGBXFusion.
</details>
<details>
<summary>摘要</summary>
多模式深度感知融合有能力使自动驾驶车辆在所有天气条件下视觉理解周围环境。然而，现有的深度感知融合方法通常采用复杂的建筑，混合多种模式特征，需要大量相关的多模式数据进行训练。在这个工作中，我们提出了一种高效和可Module的 RGB-X 融合网络，可以通过场景特定的融合模块来利用和融合预训练的单模式模型，从而实现 joint 输入适应网络架构，使用只需小量相关的多模式数据进行融合。我们的实验表明我们的方法比现有的RGB-thermal和RGB-gated数据集上的方法更高效，只需要小量额外参数。我们的代码可以在https://github.com/dsriaditya999/RGBXFusion上下载。
</details></li>
</ul>
<hr>
<h2 id="Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective"><a href="#Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective" class="headerlink" title="Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective"></a>Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19360">http://arxiv.org/abs/2310.19360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/rebat">https://github.com/pku-ml/rebat</a></li>
<li>paper_authors: Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, Yisen Wang</li>
<li>for: 本文研究了 adversarial training (AT) Algorithm 的一个问题：robust overfitting。</li>
<li>methods: 作者通过视 adversarial training 为动态的最小化游戏来解释这个问题，并分析了 learning rate (LR)  decay 如何使得模型师更强大的记忆能力，从而导致 robust overfitting。</li>
<li>results: 作者通过了大量实验 validate 这个理解，并提供了一个整体的视图，以及一种解决 robust overfitting 的方法：ReBalanced Adversarial Training (ReBAT)。实验表明，ReBAT 可以实现好的 robustness，并不会出现 robust overfitting 问题，即使是非常长的训练时间。<details>
<summary>Abstract</summary>
Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can attain good robustness and does not suffer from robust overfitting even after very long training. Code is available at https://github.com/PKU-ML/ReBAT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction"><a href="#Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction" class="headerlink" title="Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction"></a>Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19845">http://arxiv.org/abs/2310.19845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>for: 本研究旨在提出一种修改后的遗传算法，用于同时维度减少和超参优化，以提高Twitter社交媒体上的诈骗预测模型的性能。</li>
<li>methods: 该方法首先初始化了一个eXtreme Gradient Boosting类фика器，然后使用修改后的遗传算法将特征空间缩减至 tweets 数据集中，以生成一个诈骗预测模型。模型采用10次10个分割的十fold混合验证，并通过非 Parametric Statistical Tests 进行分析。</li>
<li>results: 实验结果表明，修改后的遗传算法在平均地达到了82.32%和92.67%的几何均值和准确率，并且使用了 less than 10% 的总特征空间。此外，eXtreme Gradient Boosting 超越了许多机器学习算法，包括 BERT 深度学习模型，在诈骗预测方面。此外，该方法还应用于短信诈骗预测模型，并与相关研究进行比较。<details>
<summary>Abstract</summary>
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy respectively, utilizing less than 10\% of the total feature space. The empirical results show that the modified genetic algorithm outperforms $Chi^2$ and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting outperforms many machine learning algorithms, including BERT-based deep learning model, in spam prediction. Furthermore, the proposed approach is applied to SMS spam modeling and compared to related works.
</details>
<details>
<summary>摘要</summary>
近些时间，社交媒体上的垃圾信息引起了研究和业务界的关注。推特成为了垃圾信息的主要传播媒体。许多研究努力以解决社交媒体上的垃圾信息问题。推特带来了额外的挑战，包括特征空间大小和数据分布不均。通常，相关研究工作会专注于这些主要挑战中的一部分，或生成黑盒模型。在这篇论文中，我们提出了一种修改后的遗传算法，用于同时维度减少和超参优化不均衡数据集。该算法首先初始化了一个极限梯度拟合分类器，然后减少了推特数据集的特征空间，以生成一个垃圾预测模型。该模型通过10次重复的10个分割验证，并通过非 Parametric 统计测试进行验证。结果表明，修改后的遗传算法在平均地达到82.32%和92.67%的垃圾预测率和准确率，使用了 menos than 10%的总特征空间。实验结果表明，修改后的遗传算法超过了$Chi^2$和$PCA$特征选择方法。此外，极限梯度拟合也超过了许多机器学习算法，包括基于BERT的深度学习模型，在垃圾预测方面。此外，我们的方法还应用于短信垃圾预测，并与相关研究相比较。
</details></li>
</ul>
<hr>
<h2 id="Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images"><a href="#Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images" class="headerlink" title="Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images"></a>Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19359">http://arxiv.org/abs/2310.19359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Morales-Álvarez, Arne Schmidt, José Miguel Hernández-Lobato, Rafael Molina</li>
<li>for: 这个论文是为了解决计算生物学中的多例学习问题，具体是用于肠癌检测。</li>
<li>methods: 该论文基于 Gaussian Processes（GP）的多例学习方法，并在这些方法中加入了一个新的牵引项，这个牵引项是基于统计物理学的 Ising 模型。</li>
<li>results: 该论文在两个实际问题中进行了评估，并表明了其在肠癌检测中的更好的性能，比如其他现有的可能性MAP 方法。  Additionally, the paper provides different visualizations and analysis to gain insights into the influence of the novel Ising term, which can be applied to other research areas.<details>
<summary>Abstract</summary>
In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.
</details>
<details>
<summary>摘要</summary>
最近几年，弱地监督多个实例学习（MIL）的思想在各个领域得到了广泛的应用。一个典型的例子是计算生物学，因为整个扫描图像缺乏小块级别标签，无法应用有监督的模型。基于 Gaussian Processes（GP）的概率MIL方法在这些领域已经取得了一定的成果，因为它们可以提供出色的不确定性估计能力。然而，这些是通用MIL方法，不考虑一个重要的事实：在生物学图像中，邻近的小块标签是相关的。在这种情况下，我们将一种状态艺术GP-based MIL方法，称为VGPMIL-PR，扩展以利用这种相关性。我们开发了一个独特的联系项， inspirited by the statistical physics Ising model。我们使用变量推断来估计所有模型参数。有趣的是，VGPMIL-PR的形式在weight that regulates the strength of the Ising term为零时被恢复。我们在两个实际问题中评估了我们的模型，即肾癌检测。我们发现我们的模型比其他状态艺术概率MIL方法更好。我们还提供了不同的视觉化和分析，以获得相关联系项的影响的视觉。这些视觉预期能够促进模型在其他领域的应用。
</details></li>
</ul>
<hr>
<h2 id="Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach"><a href="#Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach" class="headerlink" title="Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach"></a>Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19843">http://arxiv.org/abs/2310.19843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>For: This research aims to leverage telemarketing data to model the willingness of clients to make a term deposit and to find the most significant characteristics of the clients.* Methods: The research uses a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously, and builds an explainable prediction model.* Results: The models significantly outperform related works in terms of class of interest accuracy, with an average of 89.07% and a type I error of 0.059. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.Here are the three points in Simplified Chinese text:* For: 这些研究目的是利用电话营销数据来模型客户是否愿意投资贷款，以及客户最重要的特征。* Methods: 这些研究使用一种新的遗传算法基于分类器来选择最佳特征和调整分类器参数同时。* Results: 这些模型与相关研究相比，在类别 интерес精度方面表现出色，具有89.07%的平均值和0.059的类型 I 错误。这些模型预计可以最小化成本， simultaneously maximize potential profit margin and provide more insights to support marketing decision-making。<details>
<summary>Abstract</summary>
Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation and the selected features have been analyzed. The models significantly outperform the related works in terms of class of interest accuracy, they attained an average of 89.07\% and 0.059 in terms of geometric mean and type I error respectively. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.
</details>
<details>
<summary>摘要</summary>
（currently, almost all direct marketing activities are taking place virtually instead of in person, which is weakening interpersonal skills at an alarming rate. in addition, businesses have been trying to sense and foster their clients' willingness to accept marketing offers. the digital transformation and the increased virtual presence have forced companies to seek out new marketing research methods. this research aims to use telemarketing data to model clients' willingness to make a term deposit and to identify the most important characteristics of clients. real-world data from a portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. this research makes two key contributions. first, it proposes a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. second, it builds an explainable prediction model. the best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation, and the selected features have been analyzed. the models significantly outperform the related works in terms of class of interest accuracy, with an average of 89.07% and a type i error of 0.059. the model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.）
</details></li>
</ul>
<hr>
<h2 id="Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs"><a href="#Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs" class="headerlink" title="Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs"></a>Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19347">http://arxiv.org/abs/2310.19347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma</li>
<li>for: 提高基于大语言模型（LLM）的文本概要的可靠性</li>
<li>methods: 提出了一种逆反解耦方法，通过分离把握和膨胀能力来避免LLM生成的幻觉</li>
<li>results: 实验结果表明，使用DECENT方法可以显著提高基于LLM的文本概要的可靠性<details>
<summary>Abstract</summary>
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accurately and have enhanced abilities to distinguish hallucinations. Experimental results show that DECENT significantly improves the reliability of text summarization based on LLMs.
</details>
<details>
<summary>摘要</summary>
尽管最近大语言模型（LLMs）在文本摘要方面做出了一些进步，但它们经常生成的摘要与原文不匹配，这被称为“幻觉”（hallucinations）在文本生成中。与过去的小型模型（如BART、T5）相比，当前的LLMs更多地生成了更复杂的幻觉，如强制 causal relationship、添加假信息和过度总结等。这些幻觉通过传统方法很难以检测，这对改进文本摘要的准确性带来了巨大的挑战。在这篇论文中，我们提出了一种对抗分解方法，以分离LLMs的理解和赋予能力（DECENT）。此外，我们采用了一种参数高效的探测技术，以补做LLMs在训练过程中对真假敏感的缺失。这样，LLMs会更加精准地执行 instrucciones，并且具有更强的能力来分辨幻觉。实验结果表明，DECENT可以很有效地改进基于LLMs的文本摘要的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Skywork-A-More-Open-Bilingual-Foundation-Model"><a href="#Skywork-A-More-Open-Bilingual-Foundation-Model" class="headerlink" title="Skywork: A More Open Bilingual Foundation Model"></a>Skywork: A More Open Bilingual Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19341">http://arxiv.org/abs/2310.19341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyworkai/skywork">https://github.com/skyworkai/skywork</a></li>
<li>paper_authors: Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou</li>
<li>for: 这个技术报告介绍了 Skywork-13B，一个基于大语言模型（LLM）的家族，通过对英文和中文文本集的3.2万亿个字进行训练而得到。</li>
<li>methods: 这个模型使用了两个阶段的训练方法，首先是通用训练，然后是针对特定领域进行增强训练。</li>
<li>results: 这个模型不仅在各种标准测试中表现出色，还在不同领域的中文语言模型中实现了state of the art的性能。此外，作者还提出了一种泄露检测方法，表明测试数据泄露是LLM社区所需要进一步研究的问题。<details>
<summary>Abstract</summary>
In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.
</details>
<details>
<summary>摘要</summary>
在这份技术报告中，我们介绍了 Skywork-13B，一家大型自然语言模型（LLMs），在包含英语和中文文本的词库上进行了3.2亿个字的训练。这是当前最广泛训练和公开发布的相对规模相似的LMMs。我们采用了两个阶段的训练方法，首先是通用训练，然后是领域特定增强训练。我们发现，我们的模型不仅在各种标准测试 benchmark 上表现出色，而且在多个领域的中文语言模型中也达到了state of the art的性能。此外，我们还提出了一种新的泄露检测方法，表明测试数据污染是一个需要进一步调查的问题。为促进未来的研究，我们将 Skywork-13B 及其在 intermediate 阶段的检查点发布，以及一部分我们的 SkyPile 词库，这是当前最大的高质量开放中文预训练词库。我们希望 Skywork-13B 和我们的开放词库能为高质量 LLMS 的民主化提供一个价值的开源资源。
</details></li>
</ul>
<hr>
<h2 id="TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery"><a href="#TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery" class="headerlink" title="TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery"></a>TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19324">http://arxiv.org/abs/2310.19324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/tempme">https://github.com/graph-and-geometric-learning/tempme</a></li>
<li>paper_authors: Jialin Chen, Rex Ying</li>
<li>for: 本研究旨在提高现有的时间图 neural network (TGNN) 的解释性和可靠性，通过找出导致模型预测的时间模式。</li>
<li>methods: 我们提出了一种新的方法，即 Temporal Motifs Explainer (TempME)，它抽象出 TGNN 预测过程中最重要的时间模式，以提高解释性和可靠性。 TempME 基于信息瓶颈理论，旨在抽象出最重要的互动关系模式，以降低解释中包含的信息量，保持解释的简洁和精炼。</li>
<li>results: 我们的实验结果表明， TempME 可以提高 TGNN 的解释精度和预测精度，相比现有的方法。  especifically，我们在六个实际 datasets 上进行了广泛的实验，并 obtainted 8.21% 的提升在解释精度方面，以及22.96% 的提升在预测精度方面。<details>
<summary>Abstract</summary>
Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.
</details>
<details>
<summary>摘要</summary>
现代系统中的动态系统通常使用时间变化的交互来模型。在实际场景中，这些系统的下一个交互的生成机制通常是由动态系统中的时间模式所控制的。尽管现有的时间图 neural network (TGNN) 已经取得了成功和普遍性，但是仍然未知哪些时间模式是 TGNN 的准确预测Trigger 的重要指示器，这是当前 TGNN 的解释性和可靠性的主要挑战。为解决这个挑战，我们提出了一种新的方法，即 Temporal Motifs Explainer (TempME)，它可以揭示 TGNN 的预测中最重要的时间模式。基于信息瓶颈理论，TempME 提取了最关键的交互相关模式，同时尽量减少包含的信息量，以保持解释的简洁和精炼性。事件在 TempME 生成的解释中被证明更加 espacio-temporal 相关，提供更加理解的启示。广泛的实验证明 TempME 的优越性，在六个实际 dataset 上提高了解释准确率的最高达 8.21%，并在提高当前 TGNN 预测 Average Precision 的最高达 22.96%。
</details></li>
</ul>
<hr>
<h2 id="D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion"><a href="#D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion" class="headerlink" title="D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion"></a>D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19321">http://arxiv.org/abs/2310.19321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/d4explainer">https://github.com/graph-and-geometric-learning/d4explainer</a></li>
<li>paper_authors: Jialin Chen, Shirley Wu, Abhijit Gupta, Rex Ying</li>
<li>for: 提高 Graph Neural Networks (GNNs) 的解释性，以便更好地理解模型的预测结果，并提高模型的可信度。</li>
<li>methods: 提出了一种新的方法 D4Explainer，该方法通过涵盖生成图分布学习的目标函数，生成了可靠的在分布内的 GNN 解释，包括对应的counterfactual和模型级别解释。</li>
<li>results: 实验结果表明，D4Explainer 可以具有高度的解释准确性、实际性、多样性和稳定性，并且是首个结合 counterfactual 和模型级别解释的总体方法。<details>
<summary>Abstract</summary>
The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 的广泛部署引起了对其解释性的极大兴趣，这对于模型审核和建立信任worthy graph learning是非常重要。GNN解释的目标是找到影响模型预测的基本图结构，以确保生成的解释准确可靠。然而，现有的解释方法通常会限制生成的解释只是原始图结构，这会忽略在分布中的性质，导致解释不准确。为解决这些挑战，我们提出了D4Explainer，一种新的方法，可以为给定实例生成符合分布性的GNN解释，并提供模型水平的解释。D4Explainer通过将生成图分布学习纳入优化目标来实现两个目标：1）生成一个符合分布性的对应实例的多种可能性图，2）标识影响特定预测的最重要的图模式，从而提供模型水平的解释。需要注意的是，D4Explainer是首个结合对应和模型水平解释的统一框架。在synthetic和实际世界数据上进行的实验证明了D4Explainer在解释准确性、忠实度、多样性和Robustness等方面的前所未有的表现。
</details></li>
</ul>
<hr>
<h2 id="L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network"><a href="#L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network" class="headerlink" title="L2T-DLN: Learning to Teach with Dynamic Loss Network"></a>L2T-DLN: Learning to Teach with Dynamic Loss Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19313">http://arxiv.org/abs/2310.19313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoyang Hai, Liyuan Pan, Xiabi Liu, Zhengzheng Liu, Mirna Yunita</li>
<li>for: 这篇论文的目的是提出一种基于Dynamic loss函数的教师模型，用于指导学生模型的训练。</li>
<li>methods: 这篇论文使用了一种带有记忆单元的教师模型，以实现学生模型的学习受教导。此外，它还使用了动态损失网络，以使用损失函数的状态来帮助教师学习。</li>
<li>results: 实验表明，这种方法可以提高学生模型的学习效果，并在多种深度模型中提高实际任务的性能，包括分类、目标检测和 semantic segmentation 等。<details>
<summary>Abstract</summary>
With the concept of teaching being introduced to the machine learning community, a teacher model start using dynamic loss functions to teach the training of a student model. The dynamic intends to set adaptive loss functions to different phases of student model learning. In existing works, the teacher model 1) merely determines the loss function based on the present states of the student model, i.e., disregards the experience of the teacher; 2) only utilizes the states of the student model, e.g., training iteration number and loss/accuracy from training/validation sets, while ignoring the states of the loss function. In this paper, we first formulate the loss adjustment as a temporal task by designing a teacher model with memory units, and, therefore, enables the student learning to be guided by the experience of the teacher model. Then, with a dynamic loss network, we can additionally use the states of the loss to assist the teacher learning in enhancing the interactions between the teacher and the student model. Extensive experiments demonstrate our approach can enhance student learning and improve the performance of various deep models on real-world tasks, including classification, objective detection, and semantic segmentation scenarios.
</details>
<details>
<summary>摘要</summary>
随着机器学习社区中的教学概念的引入，一个教师模型开始使用动态损失函数来教育学生模型的训练。动态损失函数的目的是在不同阶段的学生模型学习过程中设置适应的损失函数。现有的工作中，教师模型1）仅根据学生模型当前状态来确定损失函数，即忽略教师模型的经验; 2）仅利用学生模型的状态，如训练迭代数和训练/验证集的损失/准确率，而忽略损失函数的状态。在这篇论文中，我们首先将损失调整视为一个时间任务，通过设计教师模型的记忆单元，以此使学生学习被教师模型的经验导航。然后，通过动态损失网络，我们可以再利用损失的状态来帮助教师学习，以便提高教师和学生模型之间的互动。广泛的实验证明了我们的方法可以提高学生学习并提高各种深度模型在真实任务上的性能，包括分类、目标检测和 semantic segmentation 等场景。
</details></li>
</ul>
<hr>
<h2 id="Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning"><a href="#Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning" class="headerlink" title="Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning"></a>Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19308">http://arxiv.org/abs/2310.19308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, Simon Shaolei Du</li>
<li>for:  solves sequential decision-making problems with off-policy dynamic programming techniques, but may not converge due to function approximation challenges.</li>
<li>methods:  uses return-conditioned supervised learning (RCSL) to circumvent the challenges of Bellman completeness, with a proven convergence guarantee under more relaxed assumptions.</li>
<li>results:  outperforms state-of-the-art model-free and model-based offline RL algorithms in several simulated robotics problems, and demonstrates the ability to learn from sub-optimal datasets using the proposed MBRCSL framework.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究用off-policy动态Programming技术解决了序列决策问题，但是在函数近似时可能无法保证收敛。</li>
<li>methods: 使用返回受控学习（RCSL）circumvent了 Bellman完整性挑战，并提供了一个证明收敛保证的更松解 assumptions。</li>
<li>results: 在数据预测问题中表现出色，并在多个模拟的 роботех挑战中超越了现有的模型自由和模型基于的offlineRL算法。此外，MBRCSL框架可以学习从不优化的数据中启示出来的动力学模型，并使用前向抽象来实现路径封锁。<details>
<summary>Abstract</summary>
Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be an important technique for solving sequential decision-making problems. However, in the presence of function approximation such algorithms are not guaranteed to converge, often diverging due to the absence of Bellman-completeness in the function classes considered, a crucial condition for the success of DP-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning (RCSL) are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow linearly with the state space size to satisfy Bellman-completeness while a constant layer width is enough for RCSL. These findings take a step towards explaining the superior empirical performance of RCSL methods compared to DP-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories. MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness that plagues all dynamic programming algorithms. We propose both theoretical analysis and experimental evaluation to back these claims, outperforming state-of-the-art model-free and model-based offline RL algorithms across several simulated robotics problems.
</details>
<details>
<summary>摘要</summary>
偏见的动态计划（DP）技术，如Q学习，在sequential decision-making问题中表现出非常重要。然而，在函数 aproximation 的情况下，这些算法并不一定会 converges，经常因为函数类型中缺少 Bellman 完善性，这是DP基于方法的成功所必需的条件。在这篇论文中，我们表明了基于返回conditioned supervised learning（RCSL）的偏见学习技术可以绕过DP中的 Bellman 完善性挑战，并在更松松的假设下 converges。我们证明了在使用两层多层感知机（MLP）作为函数估计器时，要使得 Bellman 完善性满足，状态空间大小 linear 增长是必需的，而Constant 宽度是 enough for RCSL。这些发现可以解释 RCSL 方法在实际中的超越DP方法的较好性能。此外，为了学习从不优化数据集中，我们提出了一个简单的框架called MBRCSL，允许 RCSL 方法通过动态计划来缝合分割的轨迹。MBRCSL 利用学习的动力模型和前向抽象来实现轨迹缝合，而不需要 Bellman 完善性，这使得 RCSL 方法可以避免所有的动态计划算法中的 Bellman 完善性问题。我们提出了理论分析和实验评估，在多个模拟的机器人问题上超越了当前的模型自由和模型基于的离线RL算法。
</details></li>
</ul>
<hr>
<h2 id="ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense"><a href="#ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense" class="headerlink" title="ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense"></a>ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19301">http://arxiv.org/abs/2310.19301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k-square-00/rome">https://github.com/k-square-00/rome</a></li>
<li>paper_authors: Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, Jing Jiang</li>
<li>for: 评估当前最先进的预训练视觉语言模型是否具备正确解释非常规内容的能力。</li>
<li>methods: 使用新的探索数据集ROME（理解非常规知识）来评估当前最先进的预训练视觉语言模型是否能够正确解释不符常规知识的图像。</li>
<li>results: 实验结果表明，大多数当前最先进的预训练视觉语言模型仍然具备不够的解释非常规内容的能力。<details>
<summary>Abstract</summary>
Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a vision-language model, whose reasoning could gravitate towards the common scenario that the fish is inside the bowl, despite the visual input. In this paper, we introduce a novel probing dataset named ROME (reasoning beyond commonsense knowledge) to evaluate whether the state-of-the-art pre-trained vision-language models have the reasoning capability to correctly interpret counter-intuitive content. ROME contains images that defy commonsense knowledge with regards to color, shape, material, size and positional relation. Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios. We hope that ROME will spur further investigations on reasoning beyond commonsense knowledge in vision-language research.
</details>
<details>
<summary>摘要</summary>
人类具有强大的理解超出常识能力。例如，给出一张不同寻常的图片，如一只鱼在桌子上 alongside an empty fishbowl，人类会很容易理解鱼不在鱼缸里。然而，这可能不同的情况下，一个视力语言模型的理解可能会受到常识的影响，即鱼在鱼缸里。在这篇论文中，我们提出了一个新的探索数据集名为ROME（理解超出常识知识），以评估当前最先进的预训练视力语言模型是否具有正确地解释不同寻常情况的能力。ROME数据集包含图片，这些图片与常识知识有冲突，包括颜色、形状、材质、大小和位置关系。实验表明，大多数当前最先进的预训练视力语言模型仍然无法正确地解释不同寻常情况。我们希望ROME会激发更多的研究在视力语言领域的理解超出常识知识。
</details></li>
</ul>
<hr>
<h2 id="ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout"><a href="#ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout" class="headerlink" title="ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout"></a>ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19295">http://arxiv.org/abs/2310.19295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyao Shu, Ang Wang, Ziji Shi, Hanyu Zhao, Yong Li, Lu Lu</li>
<li>for: 这篇研究旨在提高深度学习模型训练的内存使用效率，并且减少高级技术所带来的开销。</li>
<li>methods: 该研究提出了一种名为ROAM的策略，它在计算图层次上运行，以 derivation 最佳的内存减少执行计划，包括优化的算子执行顺序和维度缓存布局。</li>
<li>results: 实验显示，ROAM可以减少内存使用量35.7%、13.3%和27.2%，并且提供了非常快的53.7倍速化。此外，ROAM在大型GPT2-XL模型上进行了扩展性评估，并得到了有力的验证。<details>
<summary>Abstract</summary>
As deep learning models continue to increase in size, the memory requirements for training have surged. While high-level techniques like offloading, recomputation, and compression can alleviate memory pressure, they also introduce overheads. However, a memory-efficient execution plan that includes a reasonable operator execution order and tensor memory layout can significantly increase the models' memory efficiency and reduce overheads from high-level techniques. In this paper, we propose ROAM which operates on computation graph level to derive memory-efficient execution plan with optimized operator order and tensor memory layout for models. We first propose sophisticated theories that carefully consider model structure and training memory load to support optimization for large complex graphs that have not been well supported in the past. An efficient tree-based algorithm is further proposed to search task divisions automatically, along with delivering high performance and effectiveness to solve the problem. Experiments show that ROAM achieves a substantial memory reduction of 35.7%, 13.3%, and 27.2% compared to Pytorch and two state-of-the-art methods and offers a remarkable 53.7x speedup. The evaluation conducted on the expansive GPT2-XL further validates ROAM's scalability.
</details>
<details>
<summary>摘要</summary>
深度学习模型的大小继续增长，训练过程中的内存需求也在不断增加。高级技术如卸载、重计算和压缩可以减轻内存压力，但它们也会带来过程成本。然而，一个高效的内存执行计划，包括合理的操作符执行顺序和张量内存布局，可以大幅提高模型的内存利用率，并降低高级技术的过程成本。在这篇论文中，我们提出了ROAM，它在计算图 уров层上运行，以 derivation 高效的内存执行计划，包括优化的操作符执行顺序和张量内存布局，以适应大型复杂的图structured模型。我们首先提出了一些复杂的理论，考虑到模型结构和训练内存负担，以支持大型复杂图structured模型的优化。此外，我们还提出了一种高效的树状算法，自动搜索任务分解，并实现高性能和效果。实验表明，ROAM可以减少内存使用量35.7%、13.3%和27.2%，相比PYTORCH和两种状态对方法，并提供了Remarkable 53.7倍的速度提升。进一步的评估表明，ROAM在大型GPT2-XL模型上也具有良好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data"><a href="#The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data" class="headerlink" title="The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data"></a>The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19273">http://arxiv.org/abs/2310.19273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff, Mohammad Emtiyaz Khan</li>
<li>for: 本研究旨在提高模型训练数据的敏感性理解，以便更好地解决训练过程中的问题。</li>
<li>methods: 本研究使用抽象方法，根据束义理论来解释模型对训练数据的敏感性。</li>
<li>results: 研究发现，在训练过程中获得的敏感性估计可以准确预测模型在未经见测试数据上的泛化性。<details>
<summary>Abstract</summary>
Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>理解模型在训练数据上的敏感性是非常重要，但也可能困难和成本高昂，特别是在训练过程中。为了简化这些问题，我们提出了记忆干扰方程（MPE），它将模型在训练数据上的敏感性相关联到干扰的变化。基于泊利亚理论，MPE总结了许多敏感度指标，泛化到各种模型和算法，并揭示了敏感度的有用性质。我们的实验结果表明，训练期间获得的敏感度估计可以准确预测未经见过的测试数据上的泛化性能。我们预期，提出的方程将对未来关于Robust和Adaptive学习的研究具有帮助。
</details></li>
</ul>
<hr>
<h2 id="NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning"><a href="#NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning" class="headerlink" title="NPCL: Neural Processes for Uncertainty-Aware Continual Learning"></a>NPCL: Neural Processes for Uncertainty-Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19272">http://arxiv.org/abs/2310.19272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srvcodes/npcl">https://github.com/srvcodes/npcl</a></li>
<li>paper_authors: Saurav Jha, Dong Gong, He Zhao, Lina Yao</li>
<li>for: 本研究旨在提高深度神经网络在流动数据上的高效训练，同时限制新任务干扰已有任务的忘记。</li>
<li>methods: 本研究使用神经过程（NP），一种元学习器，对 CL 任务进行处理。NP 可以将不同任务编码成概率分布中的函数，同时提供可靠的不确定度估计。</li>
<li>results: 我们的实验表明，NPCL 比前一代 CL 方法表现更好。我们还验证了 NPCL 中的不确定度估计能够正确地 indentify 新数据和评估实例级模型信心。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/srvCodes/NPCL%7D">https://github.com/srvCodes/NPCL}</a> 上获取。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to train deep neural networks efficiently on streaming data while limiting the forgetting caused by new tasks. However, learning transferable knowledge with less interference between tasks is difficult, and real-world deployment of CL models is limited by their inability to measure predictive uncertainties. To address these issues, we propose handling CL tasks with neural processes (NPs), a class of meta-learners that encode different tasks into probabilistic distributions over functions all while providing reliable uncertainty estimates. Specifically, we propose an NP-based CL approach (NPCL) with task-specific modules arranged in a hierarchical latent variable model. We tailor regularizers on the learned latent distributions to alleviate forgetting. The uncertainty estimation capabilities of the NPCL can also be used to handle the task head/module inference challenge in CL. Our experiments show that the NPCL outperforms previous CL approaches. We validate the effectiveness of uncertainty estimation in the NPCL for identifying novel data and evaluating instance-level model confidence. Code is available at \url{https://github.com/srvCodes/NPCL}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union"><a href="#Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union" class="headerlink" title="Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union"></a>Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19252">http://arxiv.org/abs/2310.19252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/jdtlosses">https://github.com/zifuwanggg/jdtlosses</a></li>
<li>paper_authors: Zifu Wang, Maxim Berman, Amal Rannen-Triki, Philip H. S. Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko</li>
<li>for: 提高semantic segmentation模型评估的准确性和公正性，尤其是在面临类域不均衡和大小不均衡的情况下。</li>
<li>methods: 提出使用细化的mIoU指标，并与相关的最差情况指标进行对比，以提供更全面的评估方法。</li>
<li>results: 通过对15种现代神经网络模型在12种自然和飞行 segmentation 数据集上进行训练和评估，发现使用细化的mIoU指标可以减少对大物体的偏袋，并提供更多的统计信息和有价值的模型和数据集评估信息。<details>
<summary>Abstract</summary>
Semantic segmentation datasets often exhibit two types of imbalance: \textit{class imbalance}, where some classes appear more frequently than others and \textit{size imbalance}, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards \textit{majority classes} (e.g. overall pixel-wise accuracy) and \textit{large objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation数据集经常表现出两种类型的偏度：类偏度和大小偏度。前者表示一些类别在其他类别之上出现得更多，而后者表示一些对象占据了更多的像素。这会使传统的评估指标受到主要类别（例如总像素准确率）和大对象（例如平均像素准确率和数据集平均交集覆盖率）的偏袋。为了解决这些缺陷，我们提议使用细化的mIoU以及相应的最差情况指标，从而对segmentation技术进行更全面的评估。这些细化指标具有较低的偏袋度，更丰富的统计信息，并且对模型和数据集进行严格的审核。此外，我们进行了广泛的 benchmark 研究，在12种天然和航空Segmentation数据集上训练和评估了15种现代神经网络。我们的 benchmark 研究表明，不要基于单一指标进行评估，而是应该使用细化的mIoU来减少对大对象的偏袋。此外，我们还发现了 Architecture 设计和损失函数的重要作用，这导致了优化细化指标的最佳实践。代码可以在 \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective"><a href="#Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective" class="headerlink" title="Pre-trained Recommender Systems: A Causal Debiasing Perspective"></a>Pre-trained Recommender Systems: A Causal Debiasing Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19251">http://arxiv.org/abs/2310.19251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myhakureimu/prerec">https://github.com/myhakureimu/prerec</a></li>
<li>paper_authors: Ziqian Lin, Hao Ding, Nghia Hoang, Branislav Kveton, Anoop Deoras, Hao Wang</li>
<li>for: 本研究旨在 investigate the possibilities and challenges of adapting pre-trained vision&#x2F;language models to the context of recommender systems, which is less investigated from the perspective of pre-trained model.</li>
<li>methods: 我们提出了一种 generic recommender 的方法，通过在不同领域中提取用户项交互数据，捕捉一般交互模式，然后快速适应改进零下或几下学习性能。此外，我们还引入了一种 causal debiasing 视角，通过一种层次权重学习模型 named PreRec，以解决数据中带有不同文化偏见的问题。</li>
<li>results: 我们的实验结果表明，提出的模型在零下或几下学习情况下可以显著提高推荐性能，并在跨市场和跨平台场景中表现出色。<details>
<summary>Abstract</summary>
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).   However, unlike vision/language data which share strong conformity in the semantic space, universal patterns underlying recommendation data collected across different domains (e.g., different countries or different E-commerce platforms) are often occluded by both in-domain and cross-domain biases implicitly imposed by the cultural differences in their user and item bases, as well as their uses of different e-commerce platforms. As shown in our experiments, such heterogeneous biases in the data tend to hinder the effectiveness of the pre-trained model. To address this challenge, we further introduce and formalize a causal debiasing perspective, which is substantiated via a hierarchical Bayesian deep learning model, named PreRec. Our empirical studies on real-world data show that the proposed model could significantly improve the recommendation performance in zero- and few-shot learning settings under both cross-market and cross-platform scenarios.
</details>
<details>
<summary>摘要</summary>
近期研究表明，预训练的视力语言模型可以在人工智能中提供一种新的有前途的解决方案，即在具有广泛数据的基础上预训练模型，然后适应到各种下游任务中，即使训练数据scarce（如零或几个例子学习场景）。受到这些进步的 inspirited，我们在这篇论文中调查了适应这种解决方案到推荐系统的可能性和挑战。我们提议开发一种通用的推荐器，通过在不同领域中提取通用用户ITEM交互数据来捕捉通用交互模式，然后通过快速适应来提高几个例子学习中的表现。然而，与视语言数据不同，推荐数据中的通用模式受到各种文化差异的影响，导致用户和ITEM基础中的偏见和跨领域偏见隐藏了通用交互模式。我们的实验表明，这些多样性偏见在数据中存在，使得预训练模型的效果受到阻碍。为此，我们进一步引入和正式化一种 causal debiasing 的视角，通过一种层次 bayesian deep learning 模型，名为 PreRec，来解决这个问题。我们对实际数据进行了实验，并证明了我们的模型可以在零例子和几个例子学习场景下提高推荐性能。
</details></li>
</ul>
<hr>
<h2 id="IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI"><a href="#IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI" class="headerlink" title="IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI"></a>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19248">http://arxiv.org/abs/2310.19248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaaaaasuka/impress">https://github.com/aaaaaasuka/impress</a></li>
<li>paper_authors: Bochuan Cao, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li, Jinghui Chen</li>
<li>For: 保护原始图像免于未经授权数据使用，如颜料艺术或假信息制作。* Methods: 使用不可见干扰，引导扩散模型生成图像，并评估干扰对图像的影响，以开发新的优化策略来纯化图像。* Results: 提出了一个名为IMPRESS的干扰纯化平台，可以评估当今保护方法的效iveness，并用于未来保护方法的评估。<details>
<summary>Abstract</summary>
Diffusion-based image generation models, such as Stable Diffusion or DALL-E 2, are able to learn from given images and generate high-quality samples following the guidance from prompts. For instance, they can be used to create artistic images that mimic the style of an artist based on his/her original artworks or to maliciously edit the original images for fake content. However, such ability also brings serious ethical issues without proper authorization from the owner of the original images. In response, several attempts have been made to protect the original images from such unauthorized data usage by adding imperceptible perturbations, which are designed to mislead the diffusion model and make it unable to properly generate new samples. In this work, we introduce a perturbation purification platform, named IMPRESS, to evaluate the effectiveness of imperceptible perturbations as a protective measure. IMPRESS is based on the key observation that imperceptible perturbations could lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image, which can be used to devise a new optimization strategy for purifying the image, which may weaken the protection of the original image from unauthorized data usage (e.g., style mimicking, malicious editing). The proposed IMPRESS platform offers a comprehensive evaluation of several contemporary protection methods, and can be used as an evaluation platform for future protection methods.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图像生成模型，如稳定扩散或DALL-E 2，可以从给定的图像中学习并生成高质量的样本，以帮助提供指导的提示。例如，它们可以用于创建艺术性图像，模仿艺术家的原始作品风格，或者用于诡异修改原始图像，生成假内容。然而，这种能力也带来了严重的道德问题，需要对原始图像的所有权Owner进行适当的授权。为此，一些尝试已经被дела，以保护原始图像免受不当数据使用的潜在威胁。在这种情况下，我们提出了一种抗扰干扰平台，名为IMPRESS，用于评估抗扰干扰的有效性。IMPRESS基于关键观察，imperceptible扰动可能会导致原始图像和扩散重建图像之间的感知不一致，从而可以提出一种新的优化策略，用于纯化图像，可能减弱原始图像免受不当数据使用的保护。我们提出的IMPRESS平台可以对当今的保护方法进行全面的评估，并可以用作未来保护方法的评估平台。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection"><a href="#Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection" class="headerlink" title="Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection"></a>Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19247">http://arxiv.org/abs/2310.19247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ringbdstack/ucl_sed">https://github.com/ringbdstack/ucl_sed</a></li>
<li>paper_authors: Jiaqian Ren, Hao Peng, Lei Jiang, Zhiwei Liu, Jia Wu, Zhengtao Yu, Philip S. Yu</li>
<li>for: 这个研究旨在提高社交事件探测 task 的模型性能，特别是在面临严重的类别不均衡情况下。</li>
<li>methods: 本研究提出了一个 uncertainty-guided class imbalance learning 框架（UCL$<em>{SED}$）和其变体（UCL-EC$</em>{SED}$），以提高模型对不确定类别的普遍化和类别学习。</li>
<li>results: 实验结果显示，这两个模型在三个严重类别不均衡社交事件数据集（Events2012_100、Events2018_100、CrisisLexT_7）上均能够提高社交事件表现和类别任务的性能，特别是在不确定类别上。<details>
<summary>Abstract</summary>
Real-world social events typically exhibit a severe class-imbalance distribution, which makes the trained detection model encounter a serious generalization challenge. Most studies solve this problem from the frequency perspective and emphasize the representation or classifier learning for tail classes. While in our observation, compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance. To this end, we propose a novel uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim to improve the overall model performance by enhancing model generalization to those uncertain classes. Considering performance degradation usually comes from misclassifying samples as their confusing neighboring classes, we focus on boundary learning in latent space and classifier learning with high-quality uncertainty estimation. First, we design a novel uncertainty-guided contrastive learning loss, namely UCL and its variant - UCL-EC, to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space. Second, to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method. We conduct experiments on three severely imbalanced social event datasets including Events2012\_100, Events2018\_100, and CrisisLexT\_7. Our model significantly improves social event representation and classification tasks in almost all classes, especially those uncertain ones.
</details>
<details>
<summary>摘要</summary>
real-world社会活动通常具有严重的类别不均衡分布，这会使得训练的检测模型遇到严重的泛化挑战。大多数研究从频率角度出发，强调表示或分类学习的方法来解决这个问题。而我们所观察到的是，相比罕见的类别，通过尝试性深度学习网络得到的准确性估计更好地反映模型性能。为了解决这个问题，我们提出了一种基于不确定性的类别不均衡学习框架 - UCL$_{SED}$, 以及其变体 - UCL-EC$_{SED}$, 用于社会事件检测任务中的不均衡数据。我们的目标是通过提高模型对不确定类别的泛化来提高总体模型性能。由于性能下降通常来自于误分类为相似类型的样本，我们集中精力于边缘学习在特征空间和分类器学习中的高质量不确定性估计。首先，我们设计了一种基于不确定性的对比学习损失函数 - UCL和UCL-EC，以便在异常分布的数据上适应不同类别的表现。在训练中，它们让所有类别，特别是不确定的类别，在特征空间中适应自适应的清晰分割边缘。其次，为了获得更加稳定和准确的类别不确定性，我们将多视图证明类ifier的结果结合使用，并通过德мп斯特-沙佛理论进行监督和加权。我们在三个严重不均衡的社会事件 dataset上进行了实验，包括 Events2012\_100、Events2018\_100和 CrisisLexT\_7。我们的模型在大多数类别中显著提高了社会事件表示和分类任务的性能，特别是不确定的类别。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Configuration-Machines-FPGA-Implementation"><a href="#Stochastic-Configuration-Machines-FPGA-Implementation" class="headerlink" title="Stochastic Configuration Machines: FPGA Implementation"></a>Stochastic Configuration Machines: FPGA Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19225">http://arxiv.org/abs/2310.19225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plubplub1/bountyfarm">https://github.com/plubplub1/bountyfarm</a></li>
<li>paper_authors: Matthew J. Felicetti, Dianhui Wang</li>
<li>for: 本研究旨在实现SCM模型在Field Programmable Gate Array（FPGA）上，并将binary-coded输入添加到算法中。</li>
<li>methods: 该研究使用SCM模型，限制随机权重为二进制值，并使用机制模型提高学习性和结果可解性。</li>
<li>results: 对两个benchmark和两个工业数据集进行测试，SCM模型在单层和深度架构下都有良好的表现。<details>
<summary>Abstract</summary>
Neural networks for industrial applications generally have additional constraints such as response speed, memory size and power usage. Randomized learners can address some of these issues. However, hardware solutions can provide better resource reduction whilst maintaining the model's performance. Stochastic configuration networks (SCNs) are a prime choice in industrial applications due to their merits and feasibility for data modelling. Stochastic Configuration Machines (SCMs) extend this to focus on reducing the memory constraints by limiting the randomized weights to a binary value with a scalar for each node and using a mechanism model to improve the learning performance and result interpretability. This paper aims to implement SCM models on a field programmable gate array (FPGA) and introduce binary-coded inputs to the algorithm. Results are reported for two benchmark and two industrial datasets, including SCM with single-layer and deep architectures.
</details>
<details>
<summary>摘要</summary>
neural networks for industrial applications 通常有额外的约束，如响应速度、存储大小和功耗使用。随机学习者可以解决一些这些问题。然而，硬件解决方案可以提供更好的资源减少，同时保持模型的性能。随机配置网络（SCNs）在工业应用中是首选，因为它们在数据模型方面具有优点和可行性。随机配置机器（SCMs）进一步限制了随机权重，将随机权重限定为二进制值，并使用一种机制模型来提高学习性能和结果 интерпреtabILITY。这篇论文的目标是在场程可编程阵列（FPGA）上实现 SCM 模型，并将二进制编码输入到算法中。结果对两个标准准比例数据集和两个工业准比例数据集进行报告，包括 SCM 单层和深度架构。
</details></li>
</ul>
<hr>
<h2 id="EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions"><a href="#EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions" class="headerlink" title="EHRTutor: Enhancing Patient Understanding of Discharge Instructions"></a>EHRTutor: Enhancing Patient Understanding of Discharge Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19212">http://arxiv.org/abs/2310.19212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhang, Zonghai Yao, Huixue Zhou, Feiyun ouyang, Hong Yu</li>
<li>for: 这篇论文的目的是提出一种基于大语言模型的患者教育框架，帮助患者更好地理解他们的诊断和治疗计划。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）来实现对患者的教育，通过对话式问答来检测和评估患者的知识水平。</li>
<li>results: 论文的评估结果表明，使用EHRTutor可以有效地帮助患者更好地理解他们的诊断和治疗计划，并且可以提高患者的参与度和遵从率。<details>
<summary>Abstract</summary>
Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.
</details>
<details>
<summary>摘要</summary>
大型语言模型在教育领域中已经显示出优异成绩，尤其是当作教学导师。在医疗领域，教育病人关于他们的诊疗纪录和治疗计划有着重要的作用，可以帮助病人更好地遵循处方。本文介绍了EHRTutor，一个创新的多 ком成分框架，利用大型语言模型（LLM）来对病人进行教育，通过对话式的问答。EHRTutor首先将诊疗纪录中的问题形成，然后通过对话进行教育，最后将结果summarize为一个摘要。评估结果显示，使用LLM和专业人员的评价都倾向于优遇EHRTutor，而且EHRTutor还提供了一个生成Synthetic patient education dialogues的框架，可以用于未来的系统培训。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior"><a href="#Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior" class="headerlink" title="Leveraging generative artificial intelligence to simulate student learning behavior"></a>Leveraging generative artificial intelligence to simulate student learning behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19206">http://arxiv.org/abs/2310.19206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlin Xu, Xinyu Zhang</li>
<li>for: 增强学习效果、进步教育研究、制定有效教学方法</li>
<li>methods: 使用大语言模型（LLMs）实现学生学习行为模拟</li>
<li>results: 确认大语言模型可以实现学生学习行为模拟，并且可以捕捉学生学习行为与多种人口特征之间的复杂相关关系，包括学习成绩、课程材料、理解水平和参与度。<details>
<summary>Abstract</summary>
Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement. Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students. We validate this hypothesis through three experiments. The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors. The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling. The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels. Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness.
</details>
<details>
<summary>摘要</summary>
学生模拟提供了一种转型的方法来提高学习成果、进步教育研究和shape未来有效教学方法。我们explore LLMS的可行性，用于模拟学生学习行为。与传统的机器学习预测不同，我们利用LLMS实例化虚拟学生，并揭示了学习经验、课程材料、理解水平和参与度之间的细腻相关性。我们的目标不仅预测学习成果，而是复制真实学生的学习行为和模式。我们验证了这一假设通过三个实验。第一个实验，基于N = 145的数据集，模拟了学生学习成果的变化，发现与实际学生的各种民族因素相似。第二个实验（N = 4524），通过增加评估历史，使虚拟学生的行为变得越来越真实。第三个实验（N = 27），结合先前知识和课程互动，显示了虚拟学生学习行为与测验问题、课程材料、参与度和理解水平之间的强相关性。总之，这些发现深入了我们对LLMS的理解，并证明了其可行性，为更适应的课程设计增强包容性和教育效果。
</details></li>
</ul>
<hr>
<h2 id="Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing"><a href="#Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing" class="headerlink" title="Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing"></a>Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19204">http://arxiv.org/abs/2310.19204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Hung Luu, Huai Liu, Tsong Yueh Chen</li>
<li>for: 本研究用 ChatGPT 探索它在软件测试领域的潜在潜力，特别是在使用 metamorphic testing（MT）这种先进的软件测试技术时。</li>
<li>methods: 本研究使用 ChatGPT 生成 metamorphic relations（MR）的候选者，这些 MR 是软件系统的基本性质，需要人类智能来确定。这些 MR 候选者然后被专家评估 Correctness。</li>
<li>results: 研究表明，ChatGPT 可以生成新的正确 MR，用于测试多个软件系统。但是，大多数 MR 候选者都是不充分定义或者错误的，尤其是对于没有过 MT 测试的系统。 ChatGPT 可以用来提高软件测试智能，但是人类智能仍然必须参与以确定其正确性。<details>
<summary>Abstract</summary>
While ChatGPT is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing. We examine the capability of ChatGPT in advancing the intelligence of software testing through a case study on metamorphic testing (MT), a state-of-the-art software testing technique. We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify. These MR candidates are then evaluated in terms of correctness by domain experts. We show that ChatGPT can be used to generate new correct MRs to test several software systems. Having said that, the majority of MR candidates are either defined vaguely or incorrect, especially for systems that have never been tested with MT. ChatGPT can be used to advance software testing intelligence by proposing MR candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.AI_2023_10_30/" data-id="cloh3squ9006jh68866bo6jqt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/30/cs.CV_2023_10_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-10-30
        
      </div>
    </a>
  
  
    <a href="/2023/10/30/cs.CL_2023_10_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-10-30</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
