
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-10-30 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Partial Tensorized Transformers for Natural Language Processing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20077 repo_url: None paper_authors: Subhadra Vadlamannati, Ryan Solgi for: 提高BERT和ViT模型的精度和压缩 metho">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-10-30">
<meta property="og:url" content="https://nullscc.github.io/2023/10/30/cs.CL_2023_10_30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Partial Tensorized Transformers for Natural Language Processing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20077 repo_url: None paper_authors: Subhadra Vadlamannati, Ryan Solgi for: 提高BERT和ViT模型的精度和压缩 metho">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-30T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.262Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CL_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T11:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-10-30
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Partial-Tensorized-Transformers-for-Natural-Language-Processing"><a href="#Partial-Tensorized-Transformers-for-Natural-Language-Processing" class="headerlink" title="Partial Tensorized Transformers for Natural Language Processing"></a>Partial Tensorized Transformers for Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20077">http://arxiv.org/abs/2310.20077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhadra Vadlamannati, Ryan Solgi</li>
<li>for: 提高BERT和ViT模型的精度和压缩</li>
<li>methods: 使用tensor-train分解方法进行压缩和部分tensor化</li>
<li>results: 提高模型的精度，无需后处理调整，在tensor decomposition领域取得新的突破<details>
<summary>Abstract</summary>
The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
</details>
<details>
<summary>摘要</summary>
transformer 架构在自然语言处理（NLP）和其他机器学习任务中引起了革命，因为它们的前无古人略度。然而，它们的延展内存和参数需求经常限制它们的实际应用。在这项工作中，我们研究了tensor-train decompositions来提高BERT和ViT模型的准确率和压缩，包括嵌入层压缩和部分tensorization of neural networks（PTNN）。我们的新PTNN方法可以在不需要后处理调整的情况下提高现有模型的准确率，创造了新的tensor decompositions领域。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning"><a href="#Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning" class="headerlink" title="Automatic Evaluation of Generative Models with Instruction Tuning"></a>Automatic Evaluation of Generative Models with Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20072">http://arxiv.org/abs/2310.20072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuhaibm/heap">https://github.com/shuhaibm/heap</a></li>
<li>paper_authors: Shuhaib Mehri, Vered Shwartz</li>
<li>for: 本研究旨在自动评估自然语言生成（NLP）中的语言模型。</li>
<li>methods: 研究使用先修标准语言模型进行微调，以模拟人类评估标准。</li>
<li>results: 研究发现，通过对 HEAP 数据集进行 instrucion 微调，可以在多种评估任务和评价标准上达到良好的表现，但有些标准需要更多的微调。同时，将多个任务进行共同训练可以提高表现，这有助于未来具有少量或无人标注数据的任务。<details>
<summary>Abstract</summary>
Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
</details>
<details>
<summary>摘要</summary>
自然语言生成自动评估已经是NLP领域的一个长期目标。一种新的方法是根据人类评估标准来练习已经预训练的语言模型，以便模拟人类的评估标准。我们受到了通用化指令模型的总体能力的启发，并提出了一种基于 instrucion 调整的学习度量。为测试我们的方法，我们收集了HEAP数据集，这是多种NLG任务和评价标准的人类评估。我们的发现表明，通过HEAP上的 instrucion 调整语言模型可以在许多评价任务上显示出良好的性能，然而一些评价标准可能更难学习。此外，同时训练多个任务可以带来额外的性能提升，这可以对未来具有少量或无人标注数据的任务具有帮助。
</details></li>
</ul>
<hr>
<h2 id="Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection"><a href="#Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection" class="headerlink" title="Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection"></a>Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20046">http://arxiv.org/abs/2310.20046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/adaptive-in-context-learning">https://github.com/amazon-science/adaptive-in-context-learning</a></li>
<li>paper_authors: Costas Mavromatis, Balasubramaniam Srinivasan, Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala, Christos Faloutsos, George Karypis</li>
<li>for: 这篇论文是关于语言模型（LLM）在新任务上适应的研究。</li>
<li>methods: 这篇论文使用了活动学习方法，通过在语言模型中提供少量的注释来使其适应新任务。</li>
<li>results: 这篇论文的实验结果表明，使用模型不确定性 sampling 和 semantic diversity-based sampling 可以提高语言模型的性能，同时可以减少注释的数量。这种方法比现有的标准方法提高了7.7%的性能，并且可以在3x fewer ICL examples 下达到相同的性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）可以通过上下文学习（ICL）适应新任务。 ICL 高效，因为它不需要任何参数更新已经训练的 LLM，只需要一些标注的示例作为 LLM 的输入。在这项工作中，我们调查了一种活动学习方法 для ICL，其中有有限的预算用于标注示例。我们提议了一种自适应优化自由算法，称为 AdaICL，它可以在 LLM 中确定不确定的示例，并进行语义多样性基于的示例选择。多样性基本样本选择提高了总效果，而不确定样本选择提高了预算效率，并帮助 LLM 学习新信息。此外，AdaICL 将其抽样策略设置为一个最大覆盖问题，动态适应基于模型反馈，可以使用贪婪算法解决。广泛的实验表明，AdaICL 可以提高精度指标4.4%（相对提高7.7%），与 SOTA 比起来更高效，并且可以在7.7%的预算下达到 SOTA 的性能。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023"><a href="#Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023" class="headerlink" title="Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023"></a>Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20003">http://arxiv.org/abs/2310.20003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Marcelo Errecalde</li>
<li>for: 这个研究旨在早期探测西班牙语言中的心理健康问题。</li>
<li>methods: 这个研究使用了基于Transformer的模型，并且运用了一个决策政策根据早期探测框架定义的参数。</li>
<li>results: 在Task 1和Task 2中，我们的方法获得了第二名的成绩，并且在分类和延迟时间方面获得了良好的成绩，显示了我们的方法在西班牙语言中的应用有效性和一致性。<details>
<summary>Abstract</summary>
MentalRiskES is a novel challenge that proposes to solve problems related to early risk detection for the Spanish language. The objective is to detect, as soon as possible, Telegram users who show signs of mental disorders considering different tasks. Task 1 involved the users' detection of eating disorders, Task 2 focused on depression detection, and Task 3 aimed at detecting an unknown disorder. These tasks were divided into subtasks, each one defining a resolution approach. Our research group participated in subtask A for Tasks 1 and 2: a binary classification problem that evaluated whether the users were positive or negative. To solve these tasks, we proposed models based on Transformers followed by a decision policy according to criteria defined by an early detection framework. One of the models presented an extended vocabulary with important words for each task to be solved. In addition, we applied a decision policy based on the history of predictions that the model performs during user evaluation. For Tasks 1 and 2, we obtained the second-best performance according to rankings based on classification and latency, demonstrating the effectiveness and consistency of our approaches for solving early detection problems in the Spanish language.
</details>
<details>
<summary>摘要</summary>
MENTALRISKES是一个新的挑战，旨在解决西班牙语早期风险检测中的问题。该挑战的目标是，在可能最快速的速度下，检测telegram用户是否显示精神障碍的兆候，通过不同的任务。任务1涉及用户识别饮食障碍,任务2关注压力障碍的检测,任务3旨在检测未知障碍。这些任务被分解为不同的子任务，每个子任务都定义了一种解决方案。我们在子任务A中参与了Tasks 1和2的解决方案：一个binary分类问题，用于评估用户是否为正或负。为解决这些任务，我们提出了基于Transformers的模型，并采用根据早期检测框架定义的决策策略。我们的模型还增加了每个任务的重要词汇表，以便更好地解决这些任务。此外，我们还应用了根据模型在用户评估过程中的历史预测记录进行决策的策略。对于Tasks 1和2，我们在基于分类和延迟时间的排名中获得第二名，这说明了我们的方法在西班牙语早期风险检测中的有效性和一致性。
</details></li>
</ul>
<hr>
<h2 id="Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design"><a href="#Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design" class="headerlink" title="Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design"></a>Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19998">http://arxiv.org/abs/2310.19998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus J. Buehler</li>
<li>for: 这篇论文旨在探讨大语言模型（LLMs）在材料分析和设计中的应用，以及它们如何在人工语言、符号、代码和数字数据之间协同工作。</li>
<li>methods: 本论文使用了一个精度调整的模型——MechGPT，基于机械物理领域的训练数据进行调整。然后，通过对模型进行训练和检查，以确保它们在机械领域中具有合理的理解能力。</li>
<li>results: 研究发现，通过使用 Ontological Knowledge Graph 策略，可以让模型更好地理解不同领域之间的关系，并提供可读的图structures，其中包括节点、边和子图等信息。此外，通过非线性抽取和代理模型的应用，可以解决模型在不同领域之间的问题回答和代码生成等问题。<details>
<summary>Abstract</summary>
Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with reasonable understanding of domain knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty to recall correct information. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies that discern how the model understands what concepts are important and how they are related. Illustrated for a use case of relating distinct areas of knowledge - here, music and proteins - such strategies can also provide an interpretable graph structure with rich information at the node, edge and subgraph level. We discuss nonlinear sampling strategies and agent-based modeling applied to complex question answering, code generation and execution in the context of automated force field development from actively learned Density Functional Theory (DFT) modeling, and data analysis.
</details>
<details>
<summary>摘要</summary>
启发式神经网络表现了承诺的能力，尤其在材料分析、设计和生产方面，包括它们可以有效地处理人类语言、符号、代码和数字数据等多种数据类型。我们在这里探索使用大语言模型（LLM）作为工程分析材料的工具，包括检索关键信息、开发研究假设、在不同领域知识之间发现机制关系以及基于物理真实情况编写和执行互动知识生成代码。当作为具有特定特征、能力和指令的集合时，LLM可以提供有力的问题解决策略。我们的实验集中使用了微调的模型——MechGPT，基于材料机理领域的训练数据进行微调。我们首先证明了微调endoows LLMs with reasonable understanding of domain knowledge。然而，当被问到外部学习的范围之外时，LLMs可能具有困难回忆正确信息的问题。我们展示了如何通过 ontological Knowledge Graph 策略来解决这个问题，该策略可以掌握模型对概念的理解和如何将其相关联。例如，我们在 music 和蛋白质之间的不同领域知识之间进行了相关的示例，这种策略还可以提供可读性的图像结构，具有节点、边和子图等级别的丰富信息。我们讨论了非线性抽样策略和基于自适应学习的代理人模型，以及在自动化 DFT 模型化、代码生成和执行中的复杂问题回答。
</details></li>
</ul>
<hr>
<h2 id="Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023"><a href="#Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023" class="headerlink" title="Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023"></a>Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19970">http://arxiv.org/abs/2310.19970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Leticia Cagnina, Marcelo Errecalde</li>
<li>for: 本研究探讨了在互联网上发现不同风险的解决方案，包括寻找抑郁症的 симптом、早期检测Pathological gambling 风险以及评估吃吃症的严重程度。</li>
<li>methods: 本研究提出了基于Transformers的多种方法，包括基于contextualized embedding vectors的相似性方法、基于提示的现有机器学习技术，以及三个精心调整的模型。</li>
<li>results: 本研究在第一个任务中取得了良好的表现，包括基于决策 metric、排名 metric 和运行时间 metric。<details>
<summary>Abstract</summary>
The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three fine-tuned models followed by decision policy according to criteria defined by an early detection framework. One model presented extended vocabulary with important words to the addressed domain. In the last task, we obtained good performances considering the decision-based metrics, ranking-based metrics, and runtime. In this work, we explore different ways to deploy the predictive potential of Transformers in eRisk tasks.
</details>
<details>
<summary>摘要</summary>
CLEF eRisk实验室探索互联网上不同任务的风险检测解决方案。2023年版本中，任务1是搜寻受到抑郁症状的用户文章，目标是根据BDI问卷症状EXTRACT用户文章的相关性。任务2是早期检测Pathological gambling风险，参与者需要尽快检测用户是否存在风险。最后一任务是估计吃苹果症状的严重程度。我们的研究组参加了前两个任务，提出了基于Transformers的解决方案。对于任务1，我们应用了不同的方法，包括contextualized embedding vector的相似性和Prompting技术。对于任务2，我们提出了三个精度调整后的模型，并按照早期检测框架中定义的标准来做决策。其中一个模型增加了重要领域中的词汇表。在最后一任务中，我们获得了良好的性能，包括决策基 metrics、排名基 metrics和运行时间。在这项工作中，我们探索了不同的方式将Transformers在eRisk任务中的预测潜力部署出来。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization"><a href="#The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization" class="headerlink" title="The Impact of Depth and Width on Transformer Language Model Generalization"></a>The Impact of Depth and Width on Transformer Language Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19956">http://arxiv.org/abs/2310.19956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, Tal Linzen</li>
<li>for: 该研究旨在探讨语言模型（LM）在处理新句子时如何实现 Compositional Generalization，即将熟悉的元素组合在新的方式下。</li>
<li>methods: 研究使用 transformer 模型，并测试假设，即深度可以促进 Compositional Generalization。为了避免深度和大小之间的干扰，研究者构建了三种模型，其中每种模型有相同的总参数数量（41M、134M 和 374M）。</li>
<li>results: 研究发现，在 fine-tuning 后，深度较大的模型在 OUT-OF-DISTRIBUTION 上的泛化性能比较好，但随着添加更多层数，模型的性能提升速度逐渐减少。此外，在每个家族中，深度较大的模型在语言模型性能方面表现更好，但返回也逐渐减少。最后，研究发现，深度的作用于 Compositional Generalization 不能完全归结于语言模型性能或在 Distribution 上的数据表现。<details>
<summary>Abstract</summary>
To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling or on in-distribution data.
</details>
<details>
<summary>摘要</summary>
为处理新句子，语言模型（LM）必须通过compositional generalization来推断——将熟悉的元素组合在新的方式下。哪些方面的模型结构会促进compositional generalization？我们通过强调transformer，我们测试假设，基于最近的理论和实验研究，transformer在深度更大时会更好地推断compositional。因为单纯地添加层会增加总的参数数量，从而与深度混淆，我们构建了三类模型，其中每个模型都拥有相同的参数数量（41M、134M和374M参数）。我们在所有模型上进行预训练，然后在测试compositional generalization的任务上进行精化。我们发现以下三点：（1）在训练后，深度更大的模型在非标准数据上的推断性能更好，但附加层的效果很快消失；（2）在每个家族中，深度更大的模型在语言模型性能上表现更好，但返回的提升都很快消失；（3）深度的作用于compositional generalization不可能完全归因于语言模型性能或标准数据上的表现。
</details></li>
</ul>
<hr>
<h2 id="Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications"><a href="#Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications" class="headerlink" title="Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications"></a>Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19942">http://arxiv.org/abs/2310.19942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Arora, Youngja Park</li>
<li>For: 本研究强调NER问题，将其拆分为两个逻辑子任务： span detection 和 span classification。* Methods: 我们将这两个子任务转化为问答问题，并生成两个简单的模型，可以独立优化每个子任务。* Results: 我们在四个跨Domestic数据集上进行实验，发现这种两步方法非但有效，还能够减少训练时间。我们的系统 SplitNER 在 Ontotes5.0、WNUT17 和一个 cybersecurity 数据集上表现优于基eline，并在 BioNLP13CG 上具有相当的表现。<details>
<summary>Abstract</summary>
In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决NER问题的方法是将其分解为两个逻辑子任务：（1）Span检测，它简单地提取实体提及 span，不论实体类型；（2）Span分类，它将 span 分类为其实体类型。我们将两个子任务都转换为问答（QA）问题，并生成两个简单的模型，可以独立地优化每个子任务。我们在四个跨领域数据集上进行了实验，并证明这种两步方法是有效和时间高效的。我们的系统 SplitNER 在 Ontonotes5.0、WNUT17 和一个cybersecurity数据集上超过基eline，并在 BioNLP13CG 上达到了类似的性能。在所有情况下，它实现了对基eline的显著减少的训练时间。我们的系统的效果来自于对BERT模型进行了两次精度调整，分别为 span 检测和分类。源代码可以在https://github.com/c3sr/split-ner中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics"><a href="#The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics" class="headerlink" title="The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics"></a>The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19792">http://arxiv.org/abs/2310.19792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger</li>
<li>for: 这个论文的目的是探索提示和评分提取在自然语言处理中的应用，具体是在机器翻译和概要写作评估中。</li>
<li>methods: 这篇论文使用了一种新的竞赛设定，选择允许的大语言模型（LLMs）并禁止微调，以强调提示的效果。参与者们采用了不同的方法，包括提示选择和权重调整等。</li>
<li>results:  despite the task’s restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL。 In addition, a small-scale human evaluation of the plausibility of explanations given by the LLMs was also performed as a separate track.<details>
<summary>Abstract</summary>
With an increasing number of parameters and pre-training data, generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore prompting and score extraction for machine translation (MT) and summarization evaluation. Specifically, we propose a novel competition setting in which we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We present an overview of participants' approaches and evaluate them on a new reference-free test set spanning three language pairs for MT and a summarization dataset. Notably, despite the task's restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human evaluation of the plausibility of explanations given by the LLMs.
</details>
<details>
<summary>摘要</summary>
随着参数和预训练数据的增加，生成大型自然语言模型（LLM）在解决无或少任务相关示例的情况下表现出色。特别是，LLM在文本生成任务中作为评价指标得到了广泛应用。在这个 контексте，我们介绍了2023年的Eval4NLP共同任务，询问参与者探索提示和分析抽取在机器翻译（MT）和摘要评价中的应用。我们提出了一种新的竞赛设定，在allowed LLM列表中禁止细化，以确保关注提示。我们 предостави了参与者的方法概述和一个新的无参考测试集，覆盖三种语言对的机器翻译和摘要数据集。尤其是，尽管任务受限，最佳系统的表现与或超过了使用更大模型开发的最近无参考度量metric，包括GEMB和Comet-Kiwi-XXL。最后，作为一个小规模的人工评估Track，我们对LLM给出的解释的可能性进行了小规模的人工评估。
</details></li>
</ul>
<hr>
<h2 id="What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning"><a href="#What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning" class="headerlink" title="What’s “up” with vision-language models? Investigating their struggle with spatial reasoning"></a>What’s “up” with vision-language models? Investigating their struggle with spatial reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19785">http://arxiv.org/abs/2310.19785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amitakamath/whatsup_vlms">https://github.com/amitakamath/whatsup_vlms</a></li>
<li>paper_authors: Amita Kamath, Jack Hessel, Kai-Wei Chang</li>
<li>for: 本研究旨在评估当代视力语言模型（VL）是否可靠地 distinguishing “right” 和 “left”。</li>
<li>methods: 研究人员创建了三个新的测试集来量化模型对这些基本的空间关系的理解。这些测试集更加精准地测试模型的空间理解能力，比如果存在的 VQAv2 测试集。</li>
<li>results: 研究人员发现，18 种 VL 模型中的所有模型表现不佳，比如 BLIP 在 VQAv2 上进行 fine-tuning 的模型，只能达到 56% 的准确率，而人类准确率为 99%。研究人员还发现，popular 的视力语言预训 Corpora 如 LAION-2B 中含有少量可靠的数据来学习空间关系，而基本的模型设计改进如 up-weighting 预hoffmann 实例或 fine-tuning 在我们的 Corpora 中并不能解决这些测试集的挑战。<details>
<summary>Abstract</summary>
Recent vision-language (VL) models are powerful, but can they reliably distinguish "right" from "left"? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.
</details>
<details>
<summary>摘要</summary>
现代视力语言（VL）模型强大，但它们能够准确地 distinguishes "right" 和 "left" 吗？我们创建了三个新的 corpora 来量化模型对这些基本的空间关系的理解。这些测试更精准地测试模型的空间理解能力，比如现有的 VQAv2 dataset 更加精准 (参见 Figure 1：模型不仅需要理解一只狗在桌子下，还需要理解同一只狗在同一张桌子上)。我们评估了 18 个 VL 模型，发现所有都表现不佳，例如 BLIP 在 VQAv2 上精通的 fine-tuning 只能达到 56% 的准确率，而人类则达到 99%。我们 conclude 了这些 surprising 的行为的原因，发现：1）流行的视力语言预训练 corpora like LAION-2B 中含有少量可靠的数据用于学习空间关系; 2）基本的模型 interven 如 up-weighting 包含 preposition 的实例或 fine-tuning 在我们的 corpora 中并不足以解决我们的 benchmark 所提出的挑战。我们希望这些 corpora 能够促进进一步的研究，我们将数据和代码发布在 GitHub 上，请参考 <https://github.com/amitakamath/whatsup_vlms>。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media"><a href="#Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media" class="headerlink" title="Chain-of-Thought Embeddings for Stance Detection on Social Media"></a>Chain-of-Thought Embeddings for Stance Detection on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19750">http://arxiv.org/abs/2310.19750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Gatto, Omar Sharif, Sarah Masud Preum</li>
<li>for: 本研究旨在提高基于大语言模型（LLM）的社交媒体立场检测性能，因为在线对话中的新词汇和口语语言经常含有深层次的立场标签。</li>
<li>methods: 本研究使用链条思维（COT）提示来提高立场检测性能，但COT提示仍然困难于捕捉深层次的立场标签。为解决这个问题，我们提出了COT嵌入，将COT理由 integrates into 传统的RoBERTa基于的立场检测管道。</li>
<li>results: 我们的分析表明，1）文本编码器可以利用COT理由，即使有少量错误或幻觉，而不会扭曲COT输出标签。2）文本编码器可以忽略社交媒体特定的领域特征，导致样本预测受到干扰。我们的模型在多个社交媒体收集的多个立场检测数据集上达到了顶峰性能。<details>
<summary>Abstract</summary>
Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks -- alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.
</details>
<details>
<summary>摘要</summary>
<<SYS>>社交媒体上的立场检测对大语言模型（LLM）是一项挑战，因为在线上对话中出现的新的俚语和口语语言经常含有深刻的立场标签。链条思维（COT）提示已经被证明可以改善立场检测任务的表现——减少一些这些问题。然而，COT提示仍然努力于寻找不直观的立场标签。这个问题的原因是许多样本在模型通过训练数据获得了俚语和不同主题的知识后才能够理解，而这些知识需要通过训练数据来获得。在这种情况下，我们解决这个问题 by introducing COT Embeddings，它们可以改进COT的表现在立场检测任务中。我们的分析表明，1）文本编码器可以通过COT的理由来获得有少量错误或幻想的COT输出标签。2）文本编码器可以忽略围绕特定领域模式的误导性COT理由。我们的模型在多个社交媒体上的立场检测数据集上达到了最高的表现。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation"><a href="#Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation" class="headerlink" title="Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation"></a>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19740">http://arxiv.org/abs/2310.19740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qtli/coeval">https://github.com/qtli/coeval</a></li>
<li>paper_authors: Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi</li>
<li>for: 这个论文主要针对开放式自然语言生成任务（NLG）的评估问题，尤其是这类任务需要创ativity和多样化的评估标准。</li>
<li>methods: 这个论文提出了一种名为CoEval的合作评估管道，其中利用大语言模型（LLM）生成初步的想法，然后由人类进行审核和修正。</li>
<li>results: 研究发现，通过利用LLM，CoEval可以有效地评估长文本，提高评估效率和可靠性，但是人类审核仍然 игра着重要的角色，对LLM评估结果进行修正，以确保最终的可靠性。<details>
<summary>Abstract</summary>
Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.
</details>
<details>
<summary>摘要</summary>
人类在开放式自然语言生成任务（NLG）的评估中广泛参与，因为自动度量器经常表现出软相关性与人类评价。大型语言模型（LLM）最近出现了可扩展且成本效果的代替方案。然而，人类和LLM都有限制，即内在主观性和不可靠的评价，特别是面临开放任务需要适应性的度量。为了探索人类和LLM-基于评估器的共同作用并解决现有不一致的评价标准在开放NLG任务中，我们提出了一个协作评估管道CoEval，包括设计任务特定的检查列表和详细的文本评估。在CoEval中，LLM生成初步的想法，然后人类进行审核。我们进行了一系列实验，研究了LLM和人类在CoEval中的互动效果。结果表明，通过利用LLM，CoEval可以有效评估长篇文本， saves significant time和 reduces human evaluation outliers。然而，人类审核仍然扮演着重要的修订角色，修改了20%的LLM评估分。
</details></li>
</ul>
<hr>
<h2 id="Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach"><a href="#Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach" class="headerlink" title="Combining Language Models For Specialized Domains: A Colorful Approach"></a>Combining Language Models For Specialized Domains: A Colorful Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19708">http://arxiv.org/abs/2310.19708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Eitan, Menachem Pirchi, Neta Glazer, Shai Meital, Gil Ayach, Aviv Shamsian, Aviv Navon, Gil Hetz, Joseph Keshet</li>
<li>for: 本研究旨在应对通用语言模型（LM）在专业领域中遇到专业术语和术语的问题。</li>
<li>methods: 本研究使用了对专业LM的标注（或“颜色”），以便在通用LM中进行自动捷径识别和处理。</li>
<li>results: 本研究的结果显示，这种方法可以将专业术语和通用语言融合在一起，并且可以有效地降低专业领域中的错误率。<details>
<summary>Abstract</summary>
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-specific words without compromising performance in the general domain.
</details>
<details>
<summary>摘要</summary>
通用语言模型（LM）在处理域专词汇和专业术语时遇到困难，这些词汇 frequently 在医学或工业领域中使用。此外，它们经常难以理解混合语言，其中混合了通用语言和域专词汇。这对自动语音识别系统在这些特定领域中操作带来挑战。在这种情况下，我们提出了一种新的方法，即将域专语言模型（LM） integrate 到通用语言模型（LM）中。这种策略通过对每个词语标注或“颜色”来指示它们与通用语言模型或域专语言模型相关。我们开发了一种优化的算法，以便有效地处理含有颜色词语的推理。我们的评估结果表明，这种方法可以很好地将专业词汇 интеGRATE 到语言任务中。尤其是，我们的方法可以大幅降低域专词汇的错误率，而不会 compromise 通用语言模型的性能。
</details></li>
</ul>
<hr>
<h2 id="When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations"><a href="#When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations" class="headerlink" title="When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations"></a>When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19698">http://arxiv.org/abs/2310.19698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleksandarpetrov/prefix-tuning-theory">https://github.com/aleksandarpetrov/prefix-tuning-theory</a></li>
<li>paper_authors: Aleksandar Petrov, Philip H. S. Torr, Adel Bibi</li>
<li>for: 本文研究 Context-based fine-tuning 方法，包括提示、在Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在<details>
<summary>Abstract</summary>
Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.
</details>
<details>
<summary>摘要</summary>
Context-based 细化方法，包括提示、在 контексте学习、软提示（也称为提示调整）、前缀调整，在 empirical successes 上备受欢迎，但是理论上的理解却很少。我们表明，虽然连续 embedding 空间比 discrete token 空间更加表达力，但是软提示和前缀调整比全 Parameters 的 fine-tuning 更加有限制，即使它们具有相同的 learnable 参数数量。具体来说，context-based 细化无法改变内容和对应关系的相对担注模式，只能偏移 attention 层的输出方向。这表明，虽然提示、在 kontext 学习、软提示和 prefix 调整可以让 pretrained 模型中的技能得到发挥，但它们无法学习新的任务，需要新的担注模式。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews"><a href="#Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews" class="headerlink" title="Sentiment Analysis in Digital Spaces: An Overview of Reviews"></a>Sentiment Analysis in Digital Spaces: An Overview of Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19687">http://arxiv.org/abs/2310.19687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura E. M. Ayravainen, Joanne Hinds, Brittany I. Davidson</li>
<li>for: 本研究提供了一份系统性审review的概述，涵盖了2,275个原始研究。</li>
<li>methods: 该研究使用了一套自定义的质量评估框架，以评估系统性审review的方法质量和报告标准。</li>
<li>results: 研究发现了各种应用和方法，报告质量有限，以及时间的挑战。<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is commonly applied to digital textual data, revealing insight into opinions and feelings. Many systematic reviews have summarized existing work, but often overlook discussions of validity and scientific practices. Here, we present an overview of reviews, synthesizing 38 systematic reviews, containing 2,275 primary studies. We devise a bespoke quality assessment framework designed to assess the rigor and quality of systematic review methodologies and reporting standards. Our findings show diverse applications and methods, limited reporting rigor, and challenges over time. We discuss how future research and practitioners can address these issues and highlight their importance across numerous applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks"><a href="#MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks" class="headerlink" title="MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks"></a>MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19677">http://arxiv.org/abs/2310.19677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cicl-stanford/moca">https://github.com/cicl-stanford/moca</a></li>
<li>paper_authors: Allen Nie, Yuhui Zhang, Atharva Amdekar, Chris Piech, Tatsunori Hashimoto, Tobias Gerstenberg</li>
<li>for: 这些研究旨在探讨人类常识的物理和社会世界理解是如何组织的，以及这些理解如何支持 causal 和 moral 判断。</li>
<li>methods: 研究人员收集了 24 篇 cognitive science 论文中的故事集，并开发了一个系统来注释每个故事中调查的因素。然后，他们使用这个数据集测试大型自然语言处理模型（LLMs）是否对文本场景中的 causal 和 moral 判断与人类参与者相似。</li>
<li>results: 研究发现，较新的 LLMs 在总体水平上的听起来比较好，但是使用统计分析发现，LLMs 对不同因素进行了不同的重视，与人类参与者的偏好不符。这些结果表明，通过精心编辑、挑战数据集和认知科学的洞察，我们可以超越仅基于总体指标的比较，探索 LLMs 的隐性偏好，并评估这些偏好与人类常识之间的一致程度。<details>
<summary>Abstract</summary>
Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.
</details>
<details>
<summary>摘要</summary>
人类常识理解physical和social世界是通过直觉理论来组织的。这些理论支持我们作出 causal和道德判断。当omething bad happens，我们就会自然地问：who did what, and why? cognitive science中有一个丰富的文献研究了人们的 causal和道德直觉。这些研究发现了许多影响人们的判断的因素，如违反 norms和是否可避免或不可避免。我们收集了24篇 cognitive science paper的故事集并开发了一系统来标注每个故事中investigated的因素。使用这个数据集，我们测试了 whether large language models (LLMs) 对 text-based scenarios 的judgment是否与人类参与者相符。在聚合水平上，与更新的 LLMs 相比，alignment 有所提高。然而，通过统计分析，我们发现 LLMs 对不同因素进行了不同的重要性评估，与人类参与者不同。这些结果表明可以通过制备、挑战数据集和 cognitive science 的洞察，我们可以超越仅根据聚合指标进行比较：掌握 LLMs 的隐性倾向，并证明这些与人类直觉相符。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck"><a href="#Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck" class="headerlink" title="Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck"></a>Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19660">http://arxiv.org/abs/2310.19660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris Callison-Burch</li>
<li>for: 提高文本分类任务的可解释性，尤其是在高风险领域应用。</li>
<li>methods: 提出文本瓶颈模型（TBMs），一种内在可解释的文本分类框架，可以提供全局和局部解释。TBMs 不直接预测输出标签，而是预测一个稀疏的概念分布，并使用这些概念值进行最终预测。这些概念可以通过大语言模型（LLM）自动发现和评估，无需人工干预。</li>
<li>results: 在 12 种多样化的数据集上，使用 GPT-4 进行概念生成和评估，TBMs 能够与已知黑盒基eline模型相比，如 GPT-4 fewshot 和 DeBERTa 的Finetune 版本，具有类似的性能水平，而与 Finetune 版本 GPT-3.5 相比，TBMs 略显落后。总的来说，我们的发现表明，TBMs 是一种有前途的新框架，可以提高可解释性，而无需性能交换。<details>
<summary>Abstract</summary>
Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.
</details>
<details>
<summary>摘要</summary>
深度神经网络在文本分类任务上表现出色，但它们在高度竞争的领域应用受限因其解释性不足。为解决这问题，我们提出了文本瓶颈模型（TBM），一种内在可解释的文本分类框架，它可以提供全局和局部解释。TBM不直接预测输出标签，而是预测一个稀疏的概念集中的 categorical 值，然后使用这些概念值上的直线层来生成最终预测。这些概念可以通过大型自然语言模型（LLM）自动发现和测量，不需要人类审核。在12种多样化的数据集上，使用 GPT-4 进行概念生成和测量，我们发现TBM可以与已知黑盒基线相比，例如 GPT-4 几个步骤和 DeBERTa 的训练版本，在总体性能方面占据中间地位，落后于训练版本 GPT-3.5。总的来说，我们的发现表明TBM 是一种有前途的新框架，它可以增强解释性，并且与性能交易得来。特别是在通用领域的文本分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace"><a href="#Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace" class="headerlink" title="Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"></a>Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19651">http://arxiv.org/abs/2310.19651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, Yue Zhang</li>
<li>for: 本研究的目的是解释数据建构指南，以便更好地理解如何使Large Language Models（LLMs）实现通用智能。</li>
<li>methods: 本研究使用了深度研究每个基本能力的发展，如创意写作、代码生成和逻辑推理，并系统地研究数据量、参数大小和数据建构方法对每个能力的发展的影响。</li>
<li>results: 研究发现：（i）尽管数据量和参数Scale直接影响模型的总性能，但一些能力更sensitive于其增加，可以使用有限数据进行有效训练，而其他些能力却很难被改进。（ii）人工纠正的数据可以在效率和数据量方面高于GPT-4的 sintetic数据，并可以不断提高模型性能，而且可以在out-of-domain数据上取得反应。（iii）制定数据可以带来强大的跨能力普适性，并且可以在out-of-domain数据上取得相同的结果。<details>
<summary>Abstract</summary>
Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quality and distribution across existing datasets. Experimental conclusions drawn from these datasets are also inconsistent, with some studies emphasizing the importance of scaling instruction numbers, while others argue that a limited number of samples suffice. To better understand data construction guidelines, we deepen our focus from the overall model performance to the growth of each underlying ability, such as creative writing, code generation, and logical reasoning. We systematically investigate the effects of data volume, parameter size, and data construction methods on the development of various abilities, using hundreds of model checkpoints (7b to 33b) fully instruction-tuned on a new collection of over 40k human-curated instruction data. This proposed dataset is stringently quality-controlled and categorized into ten distinct LLM abilities. Our study reveals three primary findings: (i) Despite data volume and parameter scale directly impacting models' overall performance, some abilities are more responsive to their increases and can be effectively trained using limited data, while some are highly resistant to these changes. (ii) Human-curated data strongly outperforms synthetic data from GPT-4 in efficiency and can constantly enhance model performance with volume increases, but is unachievable with synthetic data. (iii) Instruction data brings powerful cross-ability generalization, with evaluation results on out-of-domain data mirroring the first two observations. Furthermore, we demonstrate how these findings can guide more efficient data constructions, leading to practical performance improvements on public benchmarks.
</details>
<details>
<summary>摘要</summary>
大量的 instruction tuning 是一种快速发展的方法来提高大型语言模型（LLM）的通用智能。然而，创建 instruction data 仍然受到较大的变量和不确定性影响，现有数据集的质量和分布仍然存在很大的差异。各个研究对这些数据集的结论也存在很大的不一致，一些研究认为需要扩大 instruction 数量，而其他研究则认为只需要一个有限的样本数量。为了更好地理解数据建构指南，我们将我们的关注深化到每个基础能力的发展，如创作写作、代码生成和逻辑推理。我们系统地 investigate 数据量、参数大小和数据建构方法对每个能力的发展的影响，使用 hundreds of model checkpoints (7b to 33b) 完全 instruction-tuned 在一个新收集的超过 40k 个人精心编辑的 instruction data 上。这个提posed dataset 是 strict quality-controlled 并分为十种不同的 LLM 能力。我们的研究发现了以下三个主要结论：1. 虽然数据量和参数缩放直接影响模型的总性能，但一些能力更敏感于这些变化，可以使用有限的数据进行有效地训练，而其他能力则具有很高的抗变化能力。2. 人类精心编辑的数据可以在效率和数据量方面大大超越 Synthetic data from GPT-4，并且可以随着数据量的增加不断提高模型性能。然而，synthetic data 无法 достичь这一点。3. instruction data 具有强大的跨能力总体化，评估结果表明，模型在不同的领域数据上的表现与前两个结论类似。此外，我们还证明了这些发现可以导向更有效的数据建构，从而实现实际性能的提高在公共benchmark上。
</details></li>
</ul>
<hr>
<h2 id="KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering"><a href="#KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering" class="headerlink" title="KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering"></a>KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19650">http://arxiv.org/abs/2310.19650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iftitahu Ni’mah, Samaneh Khoshrou, Vlado Menkovski, Mykola Pechenizkiy</li>
<li>for: 本研究旨在减少依赖于标签指导，通过Sequence-to-Sequence（Seq2Seq）文本生成器来学习文档嵌入。</li>
<li>methods: 我们将关键短语生成任务转换为多标签关键词生成在社区基于问答（cQA）中。</li>
<li>results: 我们的实验结果显示，KeyGen2Vec在整体上比多标签关键词分类器更高，具有14.7%的提升 based on Purity、Normalized Mutual Information（NMI）和F1-Score度量。尝试ingly，尽管在评估数据集上，获得标签指导的学习嵌入的绝对优势在Yahoo! cQA中较大，KeyGen2Vec仍然与这些标签指导的分类器竞争。<details>
<summary>Abstract</summary>
Representing documents into high dimensional embedding space while preserving the structural similarity between document sources has been an ultimate goal for many works on text representation learning. Current embedding models, however, mainly rely on the availability of label supervision to increase the expressiveness of the resulting embeddings. In contrast, unsupervised embeddings are cheap, but they often cannot capture implicit structure in target corpus, particularly for samples that come from different distribution with the pretraining source.   Our study aims to loosen up the dependency on label supervision by learning document embeddings via Sequence-to-Sequence (Seq2Seq) text generator. Specifically, we reformulate keyphrase generation task into multi-label keyword generation in community-based Question Answering (cQA). Our empirical results show that KeyGen2Vec in general is superior than multi-label keyword classifier by up to 14.7% based on Purity, Normalized Mutual Information (NMI), and F1-Score metrics. Interestingly, although in general the absolute advantage of learning embeddings through label supervision is highly positive across evaluation datasets, KeyGen2Vec is shown to be competitive with classifier that exploits topic label supervision in Yahoo! cQA with larger number of latent topic labels.
</details>
<details>
<summary>摘要</summary>
文档表示在高维空间内表示学习，保持文档来源之间的结构相似性是许多文本表示学习工作的最终目标。现有的嵌入模型大多数依赖于标签超级视图增加嵌入的表达力。然而，无监督嵌入是便宜的，但它们通常无法捕捉目标句子分布中的隐藏结构，特别是来自不同分布的样本。我们的研究旨在减少依赖于标签超级视图的限制，通过序列到序列（Seq2Seq）文本生成器学习文档嵌入。 Specifically，我们将关键短语生成任务转换为多标签关键词生成在社区基于问答（cQA）中。我们的实验结果表明，KeyGen2Vec在总体来说比多标签关键词分类器高出14.7%的纯度、 норма化共同信息（NMI）和F1-Score指标。有趣的是，虽然在评估数据集上，通常有监督嵌入学习的绝对优势，KeyGen2Vec在Yahoo! cQA中的更多 latent topic label 的情况下与使用主题标签监督的 классифика器竞争。
</details></li>
</ul>
<hr>
<h2 id="DPATD-Dual-Phase-Audio-Transformer-for-Denoising"><a href="#DPATD-Dual-Phase-Audio-Transformer-for-Denoising" class="headerlink" title="DPATD: Dual-Phase Audio Transformer for Denoising"></a>DPATD: Dual-Phase Audio Transformer for Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19588">http://arxiv.org/abs/2310.19588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Li, Pu Wang, Jialu Li, Xinzhe Wang, Youshan Zhang</li>
<li>for: 提高 speech 干扰除模型的性能，使其能够更好地处理长时间序列 audio 信号。</li>
<li>methods: 使用 smaller audio chunks 作为输入，并提出了一种新的 transformer 层结构，以便更好地学习干扰除 clean audio sequences。</li>
<li>results: 对比state-of-the-art方法，我们的模型表现更好，并且可以快速 converges。<details>
<summary>Abstract</summary>
Recent high-performance transformer-based speech enhancement models demonstrate that time domain methods could achieve similar performance as time-frequency domain methods. However, time-domain speech enhancement systems typically receive input audio sequences consisting of a large number of time steps, making it challenging to model extremely long sequences and train models to perform adequately. In this paper, we utilize smaller audio chunks as input to achieve efficient utilization of audio information to address the above challenges. We propose a dual-phase audio transformer for denoising (DPATD), a novel model to organize transformer layers in a deep structure to learn clean audio sequences for denoising. DPATD splits the audio input into smaller chunks, where the input length can be proportional to the square root of the original sequence length. Our memory-compressed explainable attention is efficient and converges faster compared to the frequently used self-attention module. Extensive experiments demonstrate that our model outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:近期高性能的转换器基于Speech Enhancement模型表明，时域方法可以达到相同的性能水平，与时域频谱方法相比。然而，时域语音净化系统通常处理大量的音频数据，这会使模型学习和训练变得困难。在这篇论文中，我们使用更小的音频块作为输入，以便有效地利用音频信息。我们提出了一种双相Audio Transformer for Denoising（DPATD），一种新的模型，用于在深度结构中学习干净的音频序列。DPATD将音频输入拆分成更小的块，块的长度与原始序列长度的平方根成正比。我们的内存压缩可解释注意力更加高效，并且在训练过程中更快 converges。广泛的实验表明，我们的模型超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning"><a href="#Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning" class="headerlink" title="Improving Input-label Mapping with Demonstration Replay for In-context Learning"></a>Improving Input-label Mapping with Demonstration Replay for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19572">http://arxiv.org/abs/2310.19572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan</li>
<li>for: 这篇论文旨在探讨一种新的卷积语言模型（ICL），它可以通过在输入和标签之间附加一些示例来提高模型对下游自然语言处理任务的理解，而无需直接调整模型参数。</li>
<li>methods: 该论文提出了一种新的ICL方法，即重复示例与滚动 causal attention（RdSca）。这种方法通过重复后期示例并将其添加到输入的开头，使模型可以在 causal 限制下„ observation“ later 信息。此外，该方法还引入了定制化 causal attention，以避免信息泄露。</li>
<li>results: 实验结果表明，该方法可以显著改善 ICL 示例中的输入-标签映射。此外，该论文还进行了对定制化 causal attention 的深入分析，这是在前一些研究中未经探讨的领域。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.
</details>
<details>
<summary>摘要</summary>
增强语言模型（ICL）是一种新兴能力，使用几个输入标签示例来增强模型对下游自然语言处理任务的理解，而不直接修改模型参数。ICL的效果可以归结于大型语言模型（LLM）的强大语言模型能力，它们可以通过示例学习映射输入和标签之间的关系。although ICL has achieved promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance.在这篇论文中，我们提出了一种新的ICL方法，即重复示例with Sliding Causal Attention（RdSca）。特别是，我们将后续的示例重复并 concatenate 到前面，使模型可以在 causal 约束下“观察”后续信息。此外，我们引入了滑动 causal attention，以适应 causal 约束，以避免信息泄露。实验结果表明，我们的方法可以显著提高ICL示例中的输入-标签映射。我们还进行了对自然语言处理任务的深入分析，以了解如何自然地调整 causal attention 而无需训练。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time"><a href="#A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time" class="headerlink" title="A Novel Representation to Improve Team Problem Solving in Real-Time"></a>A Novel Representation to Improve Team Problem Solving in Real-Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19539">http://arxiv.org/abs/2310.19539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Doboli</li>
<li>for: 这篇论文旨在提供一种新的表示方式，以帮助在实时问题解决中Team的行为理解和改进。</li>
<li>methods: 本论文使用了一种新的表示方式，捕捉了解决过程中Team的不同想像力的发展、增强和利用。</li>
<li>results: 本论文通过一个案例研究表明，该表示方式可以帮助理解和改进Team的问题解决行为。<details>
<summary>Abstract</summary>
This paper proposes a novel representation to support computing metrics that help understanding and improving in real-time a team's behavior during problem solving in real-life. Even though teams are important in modern activities, there is little computing aid to improve their activity. The representation captures the different mental images developed, enhanced, and utilized during solving. A case study illustrates the representation.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的表示方式，用于支持实时计算团队的行为，以便更好地理解和改进团队在实际问题解决过程中的行为。尽管团队在现代活动中占据重要地位，但是计算机支持团队活动的工具却很少。该表示方式捕捉了解决过程中发展、增强和使用的不同心理图像。一个案例研究 illustrate了该表示方式。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models"><a href="#InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models" class="headerlink" title="InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models"></a>InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19531">http://arxiv.org/abs/2310.19531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu</li>
<li>for: The paper aims to address the imbalance between frequent and infrequent tokens in training large generative language models, which can lead to the models neglecting the difficult-to-learn tokens.</li>
<li>methods: The proposed method is an Information Entropy Loss (InfoEntropy Loss) function, which dynamically assesses the learning difficulty of a to-be-learned token based on the information entropy of the predicted probability distribution over the vocabulary. The loss function scales the training loss adaptively to lead the model to focus more on the difficult-to-learn tokens.</li>
<li>results: The authors train generative language models at different scales (436M, 1.1B, and 6.7B parameters) on the Pile dataset and show that incorporating the proposed InfoEntropy Loss can consistently improve the performance of the models on downstream benchmarks.Here’s the Chinese version of the three key information points:</li>
<li>for:  paper 旨在解决大型生成语言模型在训练中频繁和少见token的不均衡问题，以便模型不会忽略困难学习的token。</li>
<li>methods: 提出的方法是一种信息熵损失函数（InfoEntropy Loss），它在预测概率分布中计算token的学习困难程度，然后根据学习困难程度进行自适应调整training损失。</li>
<li>results: 作者在不同规模（436M, 1.1B, 6.7B参数）的模型上在Pile数据集上进行了训练，并证明了 incorporating 提出的 InfoEntropy Loss 可以一直提高模型在下游任务上的表现。<details>
<summary>Abstract</summary>
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.
</details>
<details>
<summary>摘要</summary>
将给定的文本翻译成简化中文。大多数生成语言模型通常在大量文本资料上进行预训练，通过预测下一个token（即子字/词/短语）来预测下一个token。在最近的研究中，大型生成语言模型在下游任务上表现出了惊人的表现。然而，现有的生成语言模型通常忽略了文本资料中的自然挑战，即频率分布不均。这可能导致语言模型受到常见和易于学习的token的束缚，而忽略不常见和Difficult-to-learn的token。为了解决这个问题，我们提议一种信息熵损失函数（InfoEntropy Loss）。在训练中，它可以动态评估要学习的token的学习难度，根据对词汇表中预测的概率分布的信息熵。然后它可以对训练损失进行自适应缩放，以便让模型更注重困难的token。在Pile数据集上，我们在不同的参数大小436M、1.1B和6.7B中训练生成语言模型。实验表明，包含我们提议的InfoEntropy Loss的模型在下游 bencmarks 上具有透明的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Constituency-Parsing-using-LLMs"><a href="#Constituency-Parsing-using-LLMs" class="headerlink" title="Constituency Parsing using LLMs"></a>Constituency Parsing using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19462">http://arxiv.org/abs/2310.19462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, Yue Zhang</li>
<li>for: 本研究旨在探讨最新的大语言模型（LLMs）是否可以解决基本 yet 未解决的自然语言处理任务——构成分析。</li>
<li>methods: 我们采用三种线性化策略将输出树转换成符号序列，使LLMs可以通过生成线性化树来解决构成分析。</li>
<li>results: 我们在一个多样化的LLMs中进行实验，包括ChatGPT、GPT-4、OPT、LLaMA和Alpaca，并与现有的状态对抗构成分析器进行比较。我们的实验包括零shot、几shot和全程学习的学习设置，并在一个本地测试集和五个外部测试集上评估模型的性能。我们的发现提供了LLMs在构成分析中的表现、通用能力和挑战的深入了解。<details>
<summary>Abstract</summary>
Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>��ollar constituency parsing 是一个基本 yet 未解决的自然语言处理任务。在这篇论文中，我们探索了最近的大型语言模型（LLMs）在不同领域和任务上表现出色的潜力，以解决这个任务。我们采用三种线性化策略，将输出树转换成符号序列，以便 LLMS 可以通过生成线性化树来解决 constituency parsing。我们在一系列不同的 LLMs 上进行实验，包括 ChatGPT、GPT-4、OPT、LLaMA 和 Alpaca，与现有的状态 искусственный智能 constituency parser 进行比较。我们的实验包括零shot、几 shot 和全training 学习设定，并在一个 domain 内和五个 out-of-domain 测试集上评估模型的性能。我们的发现揭示了 LLMs 的性能、泛化能力和 constituency parsing 中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings"><a href="#Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings" class="headerlink" title="Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings"></a>Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19420">http://arxiv.org/abs/2310.19420</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Samuel</li>
<li>for: 这 paper 探讨了使用 latent bootstrapping，一种刚才自我监督技术，用于预训语言模型。</li>
<li>methods: 这 paper 使用 contextualized embeddings 作为更丰富的监督信号，而不是使用典型的自我监督方法，即使 discrete subwords。</li>
<li>results: 我们的实验表明，使用 latent bootstrapping 可以更好地从有限资源中获得语言知识。 Specifically, 我们的实验基于 BabyLM 共享任务，包括预训两个小型 curaated corpus 并在四个语言标准准则上进行评估。<details>
<summary>Abstract</summary>
This paper explores the use of latent bootstrapping, an alternative self-supervision technique, for pretraining language models. Unlike the typical practice of using self-supervision on discrete subwords, latent bootstrapping leverages contextualized embeddings for a richer supervision signal. We conduct experiments to assess how effective this approach is for acquiring linguistic knowledge from limited resources. Specifically, our experiments are based on the BabyLM shared task, which includes pretraining on two small curated corpora and an evaluation on four linguistic benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English"><a href="#A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English" class="headerlink" title="A Lightweight Method to Generate Unanswerable Questions in English"></a>A Lightweight Method to Generate Unanswerable Questions in English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19403">http://arxiv.org/abs/2310.19403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uds-lsv/unanswerable-question-generation">https://github.com/uds-lsv/unanswerable-question-generation</a></li>
<li>paper_authors: Vagrant Gautam, Miaoran Zhang, Dietrich Klakow</li>
<li>for: 这篇论文主要写于如何建立更好的问答系统，具体来说是通过对可answer的问题进行对称和实体交换来提高问答模型的性能。</li>
<li>methods: 这篇论文使用了对称和实体交换来生成无法答题的问题，以提高问答模型的性能。</li>
<li>results: 对比之前的状态艺术，使用这种训练自由和轻量级的方法可以提高问答模型的性能，并且人类评价相关性和可读性也提高了。<details>
<summary>Abstract</summary>
If a question cannot be answered with the available information, robust systems for question answering (QA) should know _not_ to answer. One way to build QA models that do this is with additional training data comprised of unanswerable questions, created either by employing annotators or through automated methods for unanswerable question generation. To show that the model complexity of existing automated approaches is not justified, we examine a simpler data augmentation method for unanswerable question generation in English: performing antonym and entity swaps on answerable questions. Compared to the prior state-of-the-art, data generated with our training-free and lightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data with BERT-large), and has higher human-judged relatedness and readability. We quantify the raw benefits of our approach compared to no augmentation across multiple encoder models, using different amounts of generated data, and also on TydiQA-MinSpan data (+9.3 F1 points with BERT-large). Our results establish swaps as a simple but strong baseline for future work.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:如果问题无法被 answered  WITH 可用信息，then robust question answering (QA) 系统应该知道不要回答。一种方法来建立 QA 模型是通过额外的训练数据，包括不可回答的问题，使用注释员或自动生成不可回答问题的方法。为了证明现有的自动化方法的模型复杂性不合理，我们研究了一种更简单的数据扩充方法：在可回答问题中进行反义和实体换换。与先前的状态 искусственный智能相比，我们的训练自由和轻量级方法在 SQuAD 2.0 数据上得到了更好的模型 (+1.6 F1 点 WITH BERT-large)，并且人类判断的相关性和可读性更高。我们通过不同的encoder模型和不同的生成数据量进行评估，并在 TydiQA-MinSpan 数据上 (+9.3 F1 点 WITH BERT-large) 获得了更高的Raw Benefits。我们的结果证明了交换是一种简单 yet strong 的基线 для未来的工作。
</details></li>
</ul>
<hr>
<h2 id="Japanese-SimCSE-Technical-Report"><a href="#Japanese-SimCSE-Technical-Report" class="headerlink" title="Japanese SimCSE Technical Report"></a>Japanese SimCSE Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19349">http://arxiv.org/abs/2310.19349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpprc/simple-simcse-ja">https://github.com/hpprc/simple-simcse-ja</a></li>
<li>paper_authors: Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda</li>
<li>for: 本研究是为了开发日本语言实体表示（SimCSE）模型，用于提高日本语言实体表示的研究。</li>
<li>methods: 本研究使用了24种预训练的日本语言或多语言模型，并在5个监督数据集和4个无监督数据集上进行了广泛的实验。</li>
<li>results: 本研究通过详细的训练设置和评估结果，为日本语言实体表示领域提供了一个可靠的基线。<details>
<summary>Abstract</summary>
We report the development of Japanese SimCSE, Japanese sentence embedding models fine-tuned with SimCSE. Since there is a lack of sentence embedding models for Japanese that can be used as a baseline in sentence embedding research, we conducted extensive experiments on Japanese sentence embeddings involving 24 pre-trained Japanese or multilingual language models, five supervised datasets, and four unsupervised datasets. In this report, we provide the detailed training setup for Japanese SimCSE and their evaluation results.
</details>
<details>
<summary>摘要</summary>
我们报道了日本SimCSE的开发，日本句子嵌入模型通过SimCSE进行了精细调教。由于日本的句子嵌入模型缺乏可以作为基准的研究句子嵌入模型，我们在日本句子嵌入方面进行了广泛的实验，使用了24种预训练的日本或多语言模型，5个指定数据集和4个无指定数据集。在这份报告中，我们提供了日本SimCSE的详细训练设置和评估结果。
</details></li>
</ul>
<hr>
<h2 id="Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES"><a href="#Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES" class="headerlink" title="Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES"></a>Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19345">http://arxiv.org/abs/2310.19345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Beatrice Savoldi, Marco Gaido, Matteo Negri, Luisa Bentivogli</li>
<li>for: 本研究使用两个新创建的测试集（MuST-SHE-WMT23和INES）来评估翻译系统对女性和男性语言形式的翻译能力以及生成包容性的翻译。</li>
<li>methods: 本研究使用en-de和de-en语言对的翻译系统进行评估，并采用新创建的测试集来评估系统对女性和男性语言形式的翻译能力以及生成包容性的翻译。</li>
<li>results: 研究结果显示，翻译系统在正常的自然性 gender 现象中 correctly 翻译女性和男性语言形式的能力相对较高，但生成包容性的翻译仍然是一个挑战，所有评估的MT模型都表现出不够的能力。<details>
<summary>Abstract</summary>
As part of the WMT-2023 "Test suites" shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems' ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic.
</details>
<details>
<summary>摘要</summary>
为了WMT-2023"测试集"共同任务，在这篇论文中，我们summarize了两个测试集评估结果：MuST-SHE-WMT23和INES。我们通过专注于英-德和德-英语对的语言对，使用这些新创建的测试集来调查系统在翻译女性和♂性形式和生成包容性翻译方面的能力。此外，我们还讨论了我们的测试集中关联的度量和通过人工评估验证了其合理性。我们的结果表明，评估自然语言 gender 现象时，系统的表现是相对合理的和相似的，但生成包容性翻译仍然是所有评估MT模型的挑战，这表明需要未来的改进和研究。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering"><a href="#Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering" class="headerlink" title="Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering"></a>Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19292">http://arxiv.org/abs/2310.19292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Su, Phillip Howard, Nagib Hakim, Steven Bethard</li>
<li>for: 本研究旨在探讨 whether large language models can perform temporal reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems.</li>
<li>methods: 本研究使用现有的时间信息抽取系统构建时间图，并 investigate different approaches for fusing these graphs into Transformer models.</li>
<li>results: 实验结果显示，我们提议的方法可以substantially enhance the temporal reasoning capabilities of Transformer models with or without fine-tuning。此外，我们的方法也比graph convolution-based approaches的性能更高，并在 SituatedQA 和 TimeQA 中设置三个分区的新州OF-the-art perfomance。<details>
<summary>Abstract</summary>
Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models with or without fine-tuning. Additionally, our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA.
</details>
<details>
<summary>摘要</summary>
回答时间敏感问题从长文档中需要时间逻辑。现有一个重要的开放问题是大语言模型可以通过提供的文档来完成这种逻辑，或者它们可以从其他系统中提取的其他时间信息中受益。我们解决这个研究问题 by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. 实验结果表明，我们提议的方法可以具有提高Transformer模型的时间逻辑能力，无需 Fine-tuning或者使用graph convolution-based approaches。此外，我们的方法还超过了多种graph convolution-based approaches，并在SituatedQA和TimeQA中达到了新的状态对。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task"><a href="#Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task" class="headerlink" title="Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task"></a>Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19271">http://arxiv.org/abs/2310.19271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaeljohnilagan/aestrollhunting">https://github.com/michaeljohnilagan/aestrollhunting</a></li>
<li>paper_authors: Michael John Ilagan</li>
<li>for: 本研究旨在提高聊天机器人的性能，通过减少具有错误标签的用户反馈数据的影响。</li>
<li>methods: 本研究使用多用户评分和隐藏类分析（LCA）来推断正确的标签，以避免具有错误标签的用户反馈数据的影响。</li>
<li>results: 实验结果表明，当具有错误标签的用户反馈数据占多数时，使用隐藏类分析（LCA）可以准确地推断正确的标签，即使这些用户是一致的。<details>
<summary>Abstract</summary>
Chatbots have the risk of generating offensive utterances, which must be avoided. Post-deployment, one way for a chatbot to continuously improve is to source utterance/label pairs from feedback by live users. However, among users are trolls, who provide training examples with incorrect labels. To de-troll training data, previous work removed training examples that have high user-aggregated cross-validation (CV) error. However, CV is expensive; and in a coordinated attack, CV may be overwhelmed by trolls in number and in consistency among themselves. In the present work, I address both limitations by proposing a solution inspired by methodology in automated essay scoring (AES): have multiple users rate each utterance, then perform latent class analysis (LCA) to infer correct labels. As it does not require GPU computations, LCA is inexpensive. In experiments, I found that the AES-like solution can infer training labels with high accuracy when trolls are consistent, even when trolls are the majority.
</details>
<details>
<summary>摘要</summary>
chatbots 有危险性，需要避免生成冒犯性的词汇。部署后，一种方法是通过用户反馈获取词汇/标签对的Source，以便不断改进。然而，用户中有些人是啰呛用户（troll），他们提供了错误的标签的示例。为了除啰呛示例，previous work 使用了用户共同验证（CV）错误值来除啰呛示例。然而，CV 是expensive，而且在协调攻击下，CV 可能会被啰呛用户淹没。在当前的工作中，我解决了这两个限制，提出了基于 automated essay scoring（AES）的方法：在每个词汇上有多个用户评分，然后使用潜在类分析（LCA）来推断正确的标签。由于LCA不需要GPU计算，因此它是便宜的。在实验中，我发现，AES-like 方法可以在啰呛用户是一致的情况下，高精度地推断训练标签。
</details></li>
</ul>
<hr>
<h2 id="Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals"><a href="#Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals" class="headerlink" title="Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals"></a>Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19268">http://arxiv.org/abs/2310.19268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijie Xi, Munindar P. Singh</li>
<li>for: 研究在社交媒体上的实际道德情境下的人类道德判断。</li>
<li>methods: 使用计算机技术 investigate moral judgment 的因素，包括事件触发社会常识和语言标志。</li>
<li>results: 发现事件相关的负性人格特质（如immature和无礼）吸引注意力，使得它们被负责任，表明道德异常之处和负责任之间存在互相关系。此外，Language 的形式和语意对commenters的认知过程产生影响，使得事件和人物的描述更有可能被读者视为道德异常之处。<details>
<summary>Abstract</summary>
Given the increasing realism of social interactions online, social media offers an unprecedented avenue to evaluate real-life moral scenarios. We examine posts from Reddit, where authors and commenters share their moral judgments on who is blameworthy. We employ computational techniques to investigate factors influencing moral judgments, including (1) events activating social commonsense and (2) linguistic signals. To this end, we focus on excerpt-which we term moral sparks-from original posts that commenters include to indicate what motivates their moral judgments. By examining over 24,672 posts and 175,988 comments, we find that event-related negative personal traits (e.g., immature and rude) attract attention and stimulate blame, implying a dependent relationship between moral sparks and blameworthiness. Moreover, language that impacts commenters' cognitive processes to depict events and characters enhances the probability of an excerpt become a moral spark, while factual and concrete descriptions tend to inhibit this effect.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans"><a href="#Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans" class="headerlink" title="Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans"></a>Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19267">http://arxiv.org/abs/2310.19267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: The paper aims to address the problem of fake news and false information on online social media platforms by developing an automatic claim detection system.</li>
<li>methods: The proposed system, called CLAIMSCAN, uses natural language processing techniques to identify and extract claims from social media posts, and then verifies their credibility.</li>
<li>results: The paper reports that CLAIMSCAN achieved high accuracy in both tasks, with an F1-score of 0.85 for Task A and an F1-score of 0.75 for Task B. Additionally, the system was able to identify claims in a variety of formats and contexts, demonstrating its versatility and effectiveness.<details>
<summary>Abstract</summary>
A significant increase in content creation and information exchange has been made possible by the quick development of online social media platforms, which has been very advantageous. However, these platforms have also become a haven for those who disseminate false information, propaganda, and fake news. Claims are essential in forming our perceptions of the world, but sadly, they are frequently used to trick people by those who spread false information. To address this problem, social media giants employ content moderators to filter out fake news from the actual world. However, the sheer volume of information makes it difficult to identify fake news effectively. Therefore, it has become crucial to automatically identify social media posts that make such claims, check their veracity, and differentiate between credible and false claims. In response, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval Evaluation (FIRE'2023). The primary objectives centered on two crucial tasks: Task A, determining whether a social media post constitutes a claim, and Task B, precisely identifying the words or phrases within the post that form the claim. Task A received 40 registrations, demonstrating a strong interest and engagement in this timely challenge. Meanwhile, Task B attracted participation from 28 teams, highlighting its significance in the digital era of misinformation.
</details>
<details>
<summary>摘要</summary>
在线社交媒体平台的快速发展已经提供了大量内容创作和信息交换的机会，这对我们来说非常有利。然而，这些平台也成为了散布 FALSE 信息、宣传和假新闻的天堂。我们的看法是基于声明的，但耻势在散布 FALSE 信息上。为了解决这个问题，社交媒体巨头雇用了内容筛选人员来从实际世界中排除假新闻。然而，巨大量的信息使得检测 FALSE 信息变得非常困难。因此，自动地找到社交媒体上的这些声明，评估其真实性，并将准确和 FALSE 声明分开变得非常重要。为此，我们在2023年 Forum for Information Retrieval Evaluation（FIRE'2023）上发表了 CLAIMSCAN。主要目标是解决两个关键任务：任务 A，判断社交媒体帖子是否表达了声明，以及任务 B，在帖子中 precisely Identify 声明的单词或短语。任务 A 得到了 40 个注册，表明这是一个有关时间的挑战。任务 B 吸引了 28 个团队的参与，强调其在数字时代中的重要性。
</details></li>
</ul>
<hr>
<h2 id="M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models"><a href="#M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models" class="headerlink" title="M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"></a>M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19240">http://arxiv.org/abs/2310.19240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kwanwaichung/m4le">https://github.com/kwanwaichung/m4le</a></li>
<li>paper_authors: Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, Kam-Fai Wong</li>
<li>for: 本研究旨在系统地评估大语言模型（LLM）在长序处理方面的能力。</li>
<li>methods: 本研究提出了一个多能力、多范围、多任务、多领域的benchmark，即M4LE（Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation），以evaluate LLMs在长序上的表现。</li>
<li>results: 研究发现，当务务要求多个 span 注意时，当前的 LLMs 很难理解长序上的上下文； semantic retrieval 任务对于高水平的 LLMs 更加困难；模型通过长文本的 fine-tuning 和NTK意识扩展方法来提高表现相当。<details>
<summary>Abstract</summary>
Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly distributed from 1k to 8k input length. We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的长序处理已成为一项重要和必要的功能。然而，评估 LLM 的长序能力仍然是一个开放的问题。一个原因是，现有的标准和广泛使用的benchmark主要由短序组成。在这篇论文中，我们提出了M4LE，一个多能力、多范围、多任务、多领域的benchmark，用于长Context评估。M4LE基于36个NLP任务、11种任务类型和12个领域的多样化任务池。为了解决短序任务的缺乏和多能力评估的问题，我们提出了一种自动化（但具有较少的人工注解）的方法，将短序任务转换成一个统一的长序场景，要求LLMs在长上下文中identify单个或多个相关的span，基于显式或 semantics 的提示。具体来说，场景包括5种能力：（1）显式单 span;（2）semantic single span;（3）显式多 span;（4）semantic multiple span;和（5）全文理解。M4LE的样本均匀分布于1k至8k输入长度。我们对11个已确立的LLM进行了系统性的评估。我们的结果表明：1）当前 LLMs 在长上下文中理解缺乏，特别是需要多个span的注意。2）semantic retrieval任务更难 для能力LLMs。3）通过长文本进行Position interpolating fine-tuning可以达到与没有 fine-tuning 的NTK 意识的比较类似的性能。我们将benchmark公开提供，以便未来的研究在这个挑战领域。
</details></li>
</ul>
<hr>
<h2 id="Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective"><a href="#Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective" class="headerlink" title="Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective"></a>Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19233">http://arxiv.org/abs/2310.19233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</li>
<li>for: 这个论文目的是研究如何使用大型自然语言模型（LLM）建立实际应用中的会议概要系统。</li>
<li>methods: 作者通过对多种关闭源和开源LLM进行广泛的评估和比较，包括GPT-4、GPT-3.5、PaLM-2和LLaMA-2等。</li>
<li>results: 研究发现大多数关闭源LLM在性能方面表现较好，但是较小的开源模型Like LLaMA-2（7B和13B）在零基础enario下仍可以达到相当于大关闭源模型的性能。考虑到关闭源模型的隐私问题和仅通过API访问的高成本，开源模型更有利可图在实际应用中使用。 LLama-2-7B模型在性能和成本之间做出了更好的平衡，因此更适合工业应用。<details>
<summary>Abstract</summary>
This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adapter-Pruning-using-Tropical-Characterization"><a href="#Adapter-Pruning-using-Tropical-Characterization" class="headerlink" title="Adapter Pruning using Tropical Characterization"></a>Adapter Pruning using Tropical Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19232">http://arxiv.org/abs/2310.19232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria</li>
<li>for: 本研究旨在找出适合下游应用的adaptor层参数数量，以提高模型性能。</li>
<li>methods: 本文提出了一种基于тропиcal геометри的adapter层杜台法，通过优化问题来减少无用参数。</li>
<li>results: 实验结果显示，基于 тропиcal геометри的方法可以更好地标识需要减少的参数，而combinedapproach在多个任务上表现最佳。<details>
<summary>Abstract</summary>
Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-language: zh-CN</SYS>Adapter 是自然语言处理领域非常流行的参数效率转移学习方法之一，它通过在预训练语言模型层之间插入可训练模块来实现。然而， apart from several heuristics， there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.Here's the translation in Traditional Chinese:<SYS> translate-language: zh-TW</SYS>Adapter 是自然语言处理领域非常流行的参数效率传授学习方法之一，它通过在预训练语言模型层之间插入可训练模块来实现。然而， apart from several heuristics， there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.
</details></li>
</ul>
<hr>
<h2 id="LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths"><a href="#LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths" class="headerlink" title="LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths"></a>LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19208">http://arxiv.org/abs/2310.19208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/launchnlp/litcab">https://github.com/launchnlp/litcab</a></li>
<li>paper_authors: Xin Liu, Muhammad Khalifa, Lu Wang</li>
<li>for: 本研究旨在提高语言模型（LM）的准确率，即使用于短文本生成和长文本生成任务上。</li>
<li>methods: 本研究提出了一种名为 LitCab 的轻量级准确机制，该机制通过对输入文本表示进行 Linear 层的处理，以改善 LM 的准确率。</li>
<li>results: 在使用 LitCab 进行准确化后，LM 的准确率得到了改善，具体来说，在 CaT  benchmark 上，LitCab 可以降低 ECE 平均值 by 20%。此外，研究还发现，大型模型在短文本生成任务上表现出较好的准确率，而 GPT 家族模型在各种任务上表现出较好的准确率。<details>
<summary>Abstract</summary>
A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LMs, as well as building more trustworthy models. Yet, popular neural model calibration techniques are not well-suited for LMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods like temperature scaling are often unable to reorder the candidate generations. Moreover, training-based methods require finetuning the entire model, which is impractical due to the increasing sizes of modern LMs. In this paper, we present LitCab, a lightweight calibration mechanism consisting of a single linear layer taking the input text representation and manipulateing the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of 7 text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, by reducing the average ECE score by 20%. We further conduct a comprehensive evaluation with 7 popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (1) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (2) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs.
</details>
<details>
<summary>摘要</summary>
一个模型被视为良好准确报告当其报告与实际输出正确性匹配。为语言模型（LM）进行准确性调整是非常重要，因为它可以掌握hallucination问题，并建立更可靠的模型。然而，流行的神经网络模型调整技术不适合LM，因为它们缺乏对正确答案的分辨率，并且计算成本较高。例如，后处理方法like温度scaling经常无法重新排序候选生成。此外，基于训练的方法需要进一步训练整个模型，这是现代LM的增大模型大小的不可避免问题。在这篇论文中，我们提出了LitCab，一种轻量级准确机制，由单个线性层对输入文本表示进行修改。LitCab提高了模型准确性，仅添加了<2%的原始模型参数。为了评估，我们构建了CaT，一个包含7个文本生成任务的benchmark，覆盖各种响应范围。我们在Llama2-7B上测试LitCab，其在所有任务上提高了准确性，减少了平均ECE分数20%。此外，我们进行了7种流行的开源LM的全面评估，得到以下关键发现：（1）同家大型模型在短文生成任务上表现更好，但并不一定是在更长的任务上。（2）GPT家族模型在准确性方面表现更好，尽管它们具有许多 fewer参数。（3）对于有限目标（如对话）的预训练模型（如LLaMA）进行训练可能会导致准确性下降，highlighting the importance of finetuning setups for calibrating LMs。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CL_2023_10_30/" data-id="clogy1z3200duffra841m6e8a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/30/cs.AI_2023_10_30/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-10-30
        
      </div>
    </a>
  
  
    <a href="/2023/10/30/cs.LG_2023_10_30/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-10-30</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
