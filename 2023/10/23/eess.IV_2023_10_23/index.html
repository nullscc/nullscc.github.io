
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-10-23 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Bitrate Ladder Prediction Methods for Adaptive Video Streaming: A Review and Benchmark paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.15163 repo_url: None paper_authors: Ahmed Telili, Wassim Hamidouche, Hadi Am">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-10-23">
<meta property="og:url" content="https://nullscc.github.io/2023/10/23/eess.IV_2023_10_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Bitrate Ladder Prediction Methods for Adaptive Video Streaming: A Review and Benchmark paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.15163 repo_url: None paper_authors: Ahmed Telili, Wassim Hamidouche, Hadi Am">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-23T09:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:01.369Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_10_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/23/eess.IV_2023_10_23/" class="article-date">
  <time datetime="2023-10-23T09:00:00.000Z" itemprop="datePublished">2023-10-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-10-23
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bitrate-Ladder-Prediction-Methods-for-Adaptive-Video-Streaming-A-Review-and-Benchmark"><a href="#Bitrate-Ladder-Prediction-Methods-for-Adaptive-Video-Streaming-A-Review-and-Benchmark" class="headerlink" title="Bitrate Ladder Prediction Methods for Adaptive Video Streaming: A Review and Benchmark"></a>Bitrate Ladder Prediction Methods for Adaptive Video Streaming: A Review and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15163">http://arxiv.org/abs/2310.15163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Telili, Wassim Hamidouche, Hadi Amirpour, Sid Ahmed Fezza, Luce Morin, Christian Timmerer</li>
<li>for: 这篇论文旨在概述不同的方法来预测内容优化的比特率梯度，以提高OTT视频流服务的流畅性。</li>
<li>methods: 这篇论文评估了多种方法，包括传统的和学习基于的方法，以预测内容优化的比特率梯度。</li>
<li>results: 这篇论文在使用大规模数据集进行了 benchmark 研究，并评估了多种学习基于的方法，以预测内容优化的比特率梯度。<details>
<summary>Abstract</summary>
HTTP adaptive streaming (HAS) has emerged as a widely adopted approach for over-the-top (OTT) video streaming services, due to its ability to deliver a seamless streaming experience. A key component of HAS is the bitrate ladder, which provides the encoding parameters (e.g., bitrate-resolution pairs) to encode the source video. The representations in the bitrate ladder allow the client's player to dynamically adjust the quality of the video stream based on network conditions by selecting the most appropriate representation from the bitrate ladder. The most straightforward and lowest complexity approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly exhaustive search encoding. This paper provides a comprehensive review of various methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study focusing exclusively on various learning-based approaches for predicting content-optimized bitrate ladders across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art encoders, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides baseline methods and insights, which will be valuable for future research in the field of bitrate ladder prediction. The source code of the proposed benchmark and the dataset will be made publicly available upon acceptance of the paper.
</details>
<details>
<summary>摘要</summary>
最简单且最低复杂度的方法是使用固定的比特率组，这些组合包括预先决定的比特率和分辨率的对。 然而，这些方法可能无法提供最佳的比特率组，因为每个影片都有不同的内容和质量需求。 因此，许多技术已经被提出供预测内容基于的比特率组，而不需要进行成本高昂的探索性编码。本文提供了各种方法的全面评论，包括传统和学习基于的方法。 此外，我们进行了专注于不同学习基于的方法的benchmark研究，以评估这些方法在多种codec设置下的性能。我们的分析提供了基线方法和对照，这些将是未来在这个领域的研究中的价值。我们将在接下来发布的proposed benchmark和dataset中公开source code。
</details></li>
</ul>
<hr>
<h2 id="DeepOrientation-convolutional-neural-network-for-fringe-pattern-orientation-map-estimation"><a href="#DeepOrientation-convolutional-neural-network-for-fringe-pattern-orientation-map-estimation" class="headerlink" title="DeepOrientation: convolutional neural network for fringe pattern orientation map estimation"></a>DeepOrientation: convolutional neural network for fringe pattern orientation map estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15209">http://arxiv.org/abs/2310.15209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mariasi1/deeporientationnetmodel">https://github.com/mariasi1/deeporientationnetmodel</a></li>
<li>paper_authors: Maria Cywinska, Mikolaj Rogalski, Filip Brzeski, Krzysztof Patorski, Maciej Trusiak</li>
<li>for: 本研究旨在提出一种基于卷积神经网络和深度学习的本地弯曲方向图像分割方法，以便在全场光学测量中准确地估计本地弯曲方向图像。</li>
<li>methods: 本研究使用了卷积神经网络和深度学习来实现本地弯曲方向图像分割。</li>
<li>results: 实验和数值仿真结果表明，提出的 DeepOrientation 方法可以准确地估计本地弯曲方向图像，并且比传统的方法（合并平面适应&#x2F;梯度法）更加稳定和自动化。<details>
<summary>Abstract</summary>
Fringe pattern based measurement techniques are the state-of-the-art in full-field optical metrology. They are crucial both in macroscale, e.g., fringe projection profilometry, and microscale, e.g., label-free quantitative phase microscopy. Accurate estimation of the local fringe orientation map can significantly facilitate the measurement process on various ways, e.g., fringe filtering (denoising), fringe pattern boundary padding, fringe skeletoning (contouring/following/tracking), local fringe spatial frequency (fringe period) estimation and fringe pattern phase demodulation. Considering all of that the accurate, robust and preferably automatic estimation of local fringe orientation map is of high importance. In this paper we propose novel numerical solution for local fringe orientation map estimation based on convolutional neural network and deep learning called DeepOrientation. Numerical simulations and experimental results corroborate the effectiveness of the proposed DeepOrientation comparing it with the representative of the classical approach to orientation estimation called combined plane fitting/gradient method. The example proving the effectiveness of DeepOrientation in fringe pattern analysis, which we present in this paper is the application of DeepOrientation for guiding the phase demodulation process in Hilbert spiral transform. In particular, living HeLa cells quantitative phase imaging outcomes verify the method as an important asset in label-free microscopy.
</details>
<details>
<summary>摘要</summary>
异常模式基于测量技术是现代全场光学测量领域的州际标准。它们在 макро尺度上，如异常投影 Profilometry，以及微尺度上，如无标签量phasemicroscopy。 precisely estimating the local fringe orientation map can significantly facilitate the measurement process in various ways, such as fringe filtering (denoising), fringe pattern boundary padding, fringe skeletoning (contouring/following/tracking), local fringe spatial frequency (fringe period) estimation, and fringe pattern phase demodulation. Therefore, accurately, robustly, and preferably automatically estimating the local fringe orientation map is of great importance. In this paper, we propose a novel numerical solution for local fringe orientation map estimation based on convolutional neural networks and deep learning called DeepOrientation. Numerical simulations and experimental results confirm the effectiveness of the proposed DeepOrientation compared to the classical approach to orientation estimation called combined plane fitting/gradient method. The example demonstrating the effectiveness of DeepOrientation in fringe pattern analysis, which we present in this paper, is the application of DeepOrientation for guiding the phase demodulation process in Hilbert spiral transform. In particular, the quantitative phase imaging outcomes of living HeLa cells verify the method as an important asset in label-free microscopy.
</details></li>
</ul>
<hr>
<h2 id="The-AIMI-Initiative-AI-Generated-Annotations-for-Imaging-Data-Commons-Collections"><a href="#The-AIMI-Initiative-AI-Generated-Annotations-for-Imaging-Data-Commons-Collections" class="headerlink" title="The AIMI Initiative: AI-Generated Annotations for Imaging Data Commons Collections"></a>The AIMI Initiative: AI-Generated Annotations for Imaging Data Commons Collections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14897">http://arxiv.org/abs/2310.14897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gowtham Krishnan Murugesan, Diana McCrumb, Mariam Aboian, Tej Verma, Rahul Soni, Fatima Memon, Jeff Van Oss</li>
<li>for: This paper aims to contribute to the research and development of advanced imaging tools and algorithms by providing AI-generated annotations for 11 medical imaging collections from the Image Data Commons (IDC).</li>
<li>methods: The authors used both publicly available and novel AI algorithms, along with expert annotations, to create the AI-generated annotations. They also reviewed and corrected a portion of the AI annotations with a radiologist to assess the AI models’ performances.</li>
<li>results: The study provided AI-generated annotations for 11 medical imaging collections from the IDC, covering modalities such as CT, MRI, and PET, and focusing on the chest, breast, kidneys, prostate, and liver. The study demonstrated the potential of expansive publicly accessible datasets and AI for increasing accessibility and reliability in cancer imaging research and development.<details>
<summary>Abstract</summary>
The Image Data Commons (IDC) contains publicly available cancer radiology datasets that could be pertinent to the research and development of advanced imaging tools and algorithms. However, the full extent of its research capabilities is limited by the fact that these datasets have few, if any, annotations associated with them. Through this study with the AI in Medical Imaging (AIMI) initiative a significant contribution, in the form of AI-generated annotations, was made to provide 11 distinct medical imaging collections from the IDC with annotations. These collections included computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) imaging modalities. The main focus of these annotations were in the chest, breast, kidneys, prostate, and liver. Both publicly available and novel AI algorithms were adopted and further developed using open-sourced data coupled with expert annotations to create the AI-generated annotations. A portion of the AI annotations were reviewed and corrected by a radiologist to assess the AI models' performances. Both the AI's and the radiologist's annotations conformed to DICOM standards for seamless integration into the IDC collections as third-party analyses. This study further cements the well-documented notion that expansive publicly accessible datasets, in the field of cancer imaging, coupled with AI will aid in increased accessibility as well as reliability for further research and development.
</details>
<details>
<summary>摘要</summary>
Image Data Commons (IDC) 包含公共可用的癌症医学像素数据集，这些数据集可能对预测和开发高级医学像素处理算法进行研究和开发提供有益。然而，IDC的全面研究能力受限因为这些数据集几乎没有注释。通过本研究与医学计算机视觉（AIMI）计划的合作，对IDC中的11个医学像素集进行了AI生成的注释。这些集合包括计算机扫描（CT）、核磁共振成像（MRI）和 позитроном辐射Tomography（PET）成像方式。主要注释焦点在胸部、乳腺、肾脏、膀胱和肝脏。使用公共可用的AI算法和专家注释开发了AI生成的注释。一部分AI注释由医生审核并修正以评估AI模型的性能。AI和医生的注释都遵循DICOM标准，以便轻松地集成到IDC集合中作为第三方分析。这项研究进一步证明了许多公共可用的癌症医学像素数据集，结合AI，将会提高研究和开发的可 accessible性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Non-Linear-MRI-Inversion-with-Diffusion-Priors"><a href="#Joint-Non-Linear-MRI-Inversion-with-Diffusion-Priors" class="headerlink" title="Joint Non-Linear MRI Inversion with Diffusion Priors"></a>Joint Non-Linear MRI Inversion with Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14842">http://arxiv.org/abs/2310.14842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Erlacher, Martin Zach</li>
<li>for: 加速MRI扫描过程，提高图像质量</li>
<li>methods: 使用数据驱动重构方法，并jointly estimate the sensitivity maps with the image</li>
<li>results: 实现了高效、高精度的MRI扫描，并且计算了高精度的扫描图像<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a potent diagnostic tool, but suffers from long examination times. To accelerate the process, modern MRI machines typically utilize multiple coils that acquire sub-sampled data in parallel. Data-driven reconstruction approaches, in particular diffusion models, recently achieved remarkable success in reconstructing these data, but typically rely on estimating the coil sensitivities in an off-line step. This suffers from potential movement and misalignment artifacts and limits the application to Cartesian sampling trajectories. To obviate the need for off-line sensitivity estimation, we propose to jointly estimate the sensitivity maps with the image. In particular, we utilize a diffusion model -- trained on magnitude images only -- to generate high-fidelity images while imposing spatial smoothness of the sensitivity maps in the reverse diffusion. The proposed approach demonstrates consistent qualitative and quantitative performance across different sub-sampling patterns. In addition, experiments indicate a good fit of the estimated coil sensitivities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="First-realization-of-macroscopic-Fourier-ptychography-for-hundred-meter-distance-sub-diffraction-imaging"><a href="#First-realization-of-macroscopic-Fourier-ptychography-for-hundred-meter-distance-sub-diffraction-imaging" class="headerlink" title="First realization of macroscopic Fourier ptychography for hundred-meter distance sub-diffraction imaging"></a>First realization of macroscopic Fourier ptychography for hundred-meter distance sub-diffraction imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14515">http://arxiv.org/abs/2310.14515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Zhang, Yuran Lu, Yinghui Guo, Yingjie Shang, Mingbo Pu, Yulong Fan, Rui Zhou, Xiaoyin Li, Fei Zhang, Mingfeng Xu, Xiangang Luo</li>
<li>for: 提高尺度为10米的远程超分色限 imaging</li>
<li>methods: 使用弯光函数优化和目标图像联合优化方法，从而实现补做镜像和同时估计折射函数</li>
<li>results: 实验中使用这种方法可以提高最大投影距离至12米、90米和170米，同时提高最大合成孔径至200毫米，相比之前的研究result有一个数量级的提高，并且解决了FOV limitation问题，从而开启了macroscopic FP的新阶段发展。<details>
<summary>Abstract</summary>
Fourier ptychography (FP) imaging, drawing on the idea of synthetic aperture, has been demonstrated as a potential approach for remote sub-diffraction-limited imaging. Nevertheless, the farthest imaging distance is still limited around 10 m even though there has been a significant improvement in macroscopic FP. The most severely issue in increasing the imaging distance is FoV limitation caused by far-field condition for diffraction. Here, we propose to modify the Fourier far-field condition for rough reflective objects, aiming to overcome the small FoV limitation by using a divergent beam to illuminate objects. A joint optimization of pupil function and target image is utilized to attain the aberration-free image while estimating the pupil function simultaneously. Benefiting from the optimized reconstruction algorithm which effectively expands the camera's effective aperture, we experimentally implement several FP systems suited for imaging distance of 12 m, 90 m, and 170 m with the maximum synthetic aperture of 200 mm. The maximum imaging distance and synthetic aperture are thus improved by more than one order of magnitude of the state-of-the-art works with a fourfold improvement in the resolution. Our findings demonstrate significant potential for advancing the field of macroscopic FP, propelling it into a new stage of development.
</details>
<details>
<summary>摘要</summary>
福尔勒普图графи（FP）成像，基于合成孔径的想法，已经被证明可以作为远程下限捕集成像的潜在方法。然而，最远捕集距离仍然受到10米的限制，即使有 significanth improvement in macroscopic FP。最严重的问题在于提高捕集距离的 FoV 限制，是由于远场干扰的折射所致。我们提议修改 Fourier 远场干扰的条件，以超越小 FoV 的限制。我们使用折射光束照射对象，并对目标图像和 pupil function 进行联合优化，以获得无扰图像，同时也可以估计 pupil function。由于改进的重建算法，我们实际实现了一些适合12米、90米和170米的FP系统，其中最大合成孔径达200毫米。因此，我们实现了一个至少一个数量级的提高，比之前的状态ola工作。我们的发现表明FP在 macroscopic 领域可能会进入一个新的发展阶段。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/23/eess.IV_2023_10_23/" data-id="cloh7tqq6016m7b883xy6en08" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/23/cs.LG_2023_10_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-10-23
        
      </div>
    </a>
  
  
    <a href="/2023/10/23/eess.SP_2023_10_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-10-23</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
