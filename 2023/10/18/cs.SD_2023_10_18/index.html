
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-10-18 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.12378 repo_url: None paper_authors: Tae Jin Park, He Huang, Ante Jukic, Kunal">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-10-18">
<meta property="og:url" content="https://nullscc.github.io/2023/10/18/cs.SD_2023_10_18/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.12378 repo_url: None paper_authors: Tae Jin Park, He Huang, Ante Jukic, Kunal">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-18T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:57.676Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.SD_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T15:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-10-18
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-CHiME-7-Challenge-System-Description-and-Performance-of-NeMo-Team’s-DASR-System"><a href="#The-CHiME-7-Challenge-System-Description-and-Performance-of-NeMo-Team’s-DASR-System" class="headerlink" title="The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System"></a>The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12378">http://arxiv.org/abs/2310.12378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C. Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 这份论文是为了开发一个多通道、多个话者的语音识别系统，特别是用于处理分散式麦克风和麦克风数组录音。</li>
<li>methods: 这个系统主要由三个基本模块组成：语音分类模块、多通道声音前端处理模块和语音识别模块。这些模块共同组成一个协同处理的系统，仔细处理多通道和多个话者的音频输入。</li>
<li>results: 这篇论文提出了一种基于NeMo工具包的多通道、多个话者语音识别系统，并通过了7个CHiME挑战任务的评测。系统的性能得到了 significiant进步，表明该系统在实际应用中具有极高的可靠性和精度。<details>
<summary>Abstract</summary>
We present the NVIDIA NeMo team's multi-channel speech recognition system for the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task, focusing on the development of a multi-channel, multi-speaker speech recognition system tailored to transcribe speech from distributed microphones and microphone arrays. The system predominantly comprises of the following integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End Processing Module, and the ASR Module. These components collectively establish a cascading system, meticulously processing multi-channel and multi-speaker audio input. Moreover, this paper highlights the comprehensive optimization process that significantly enhanced our system's performance. Our team's submission is largely based on NeMo toolkits and will be publicly available.
</details>
<details>
<summary>摘要</summary>
我们现在介绍NVIDIA NeMo团队的多通道语音识别系统，用于7个CHiME挑战远程自动语音识别（DASR）任务。我们专注于通过分布式麦克风和麦克风数组记录语音的多通道多发言人语音识别系统的开发。这个系统主要由以下几个基本模块组成：说话人分类模块、多通道音频前端处理模块和ASR模块。这些组件结合起来形成了一个减法系统，精心处理多通道和多发言人的音频输入。此外，这篇论文还描述了我们对系统性能的全面优化过程，这有效地提高了我们的系统性能。我们的提交基于NeMo工具包，将在公共可用。
</details></li>
</ul>
<hr>
<h2 id="Property-Aware-Multi-Speaker-Data-Simulation-A-Probabilistic-Modelling-Technique-for-Synthetic-Data-Generation"><a href="#Property-Aware-Multi-Speaker-Data-Simulation-A-Probabilistic-Modelling-Technique-for-Synthetic-Data-Generation" class="headerlink" title="Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation"></a>Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12371">http://arxiv.org/abs/2310.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, He Huang, Coleman Hooper, Nithin Koluguri, Kunal Dhawan, Ante Jukic, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 本文提供了一个复杂的多个说话人语音数据模拟器，用于生成多个说话人语音录音。这个模拟器可以根据参数来调整语音中的沟壑和重叠分布，以提供一个适应性强的训练环境，用于开发适用于 speaker diarization 和 voice activity detection 的神经网络模型。</li>
<li>methods: 本文使用了一种复杂的多个说话人语音数据模拟器，可以根据参数来调整语音中的沟壑和重叠分布。</li>
<li>results: 本文 demonstates 了该模拟器可以生成具有实际统计特性的大规模语音混合数据集，并且可以用于训练 speaker diarization 和 voice activity detection 模型，以实现高效的识别和分离。<details>
<summary>Abstract</summary>
We introduce a sophisticated multi-speaker speech data simulator, specifically engineered to generate multi-speaker speech recordings. A notable feature of this simulator is its capacity to modulate the distribution of silence and overlap via the adjustment of statistical parameters. This capability offers a tailored training environment for developing neural models suited for speaker diarization and voice activity detection. The acquisition of substantial datasets for speaker diarization often presents a significant challenge, particularly in multi-speaker scenarios. Furthermore, the precise time stamp annotation of speech data is a critical factor for training both speaker diarization and voice activity detection. Our proposed multi-speaker simulator tackles these problems by generating large-scale audio mixtures that maintain statistical properties closely aligned with the input parameters. We demonstrate that the proposed multi-speaker simulator generates audio mixtures with statistical properties that closely align with the input parameters derived from real-world statistics. Additionally, we present the effectiveness of speaker diarization and voice activity detection models, which have been trained exclusively on the generated simulated datasets.
</details>
<details>
<summary>摘要</summary>
我团队介绍了一种高级多话者语音数据 simulate器，特性是生成多话者语音录音。这个 simulate器的一个特点是通过调整统计参数来模拟 silence和 overlap 的分布。这种能力提供了一个适应性高的训练环境，用于开发适合 speaker diarization 和 voice activity detection 的神经网络模型。在多话者场景下获得大量的 speaker diarization 数据经常是一项大的挑战，而且 precisetimestamp 注释的语音数据是神经网络模型的训练必要因素。我们的提议的多话者 simulate器解决了这些问题，生成了具有统计性质相近于输入参数的大规模音频混合。我们还展示了通过 exclusively 在生成的模拟数据上训练的 speaker diarization 和 voice activity detection 模型的效果。
</details></li>
</ul>
<hr>
<h2 id="BUT-CHiME-7-system-description"><a href="#BUT-CHiME-7-system-description" class="headerlink" title="BUT CHiME-7 system description"></a>BUT CHiME-7 system description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11921">http://arxiv.org/abs/2310.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Karafiát, Karel Veselý, Igor Szöke, Ladislav Mošner, Karel Beneš, Marcin Witkowski, Germán Barchi, Leonardo Pepino</li>
<li>for: 这项研究是为了开发自动语音识别系统，特别是在CHiME-7挑战中的远场自动语音识别（DASR）领域。</li>
<li>methods: 这项研究使用了许多端到端模型，以及多个工具包。它们 heavily 依赖了指导源分离（GSS）将多通道音频转化为单通道。另外，ASR 使用了自我超vised学习模型生成的语音表示，并进行了多个 ASR 系统的融合。</li>
<li>results: 研究中的系统使用了 oracle 分 segmentation，并在远场自动语音识别（DASR）领域实现了良好的成绩。<details>
<summary>Abstract</summary>
This paper describes the joint effort of Brno University of Technology (BUT), AGH University of Krakow and University of Buenos Aires on the development of Automatic Speech Recognition systems for the CHiME-7 Challenge. We train and evaluate various end-to-end models with several toolkits. We heavily relied on Guided Source Separation (GSS) to convert multi-channel audio to single channel. The ASR is leveraging speech representations from models pre-trained by self-supervised learning, and we do a fusion of several ASR systems. In addition, we modified external data from the LibriSpeech corpus to become a close domain and added it to the training. Our efforts were focused on the far-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech Recognition (DASR), our systems use oracle segmentation.
</details>
<details>
<summary>摘要</summary>
这份报告描述布雷诺技术大学（BUT）、阿格大学（AGH）和布宜诺斯艾利斯大学（UBA）在CHiME-7挑战中开发自动语音识别系统的共同努力。我们使用了准则分离（GSS）将多通道音频转化为单通道，并使用自我supervised学习预训练的语音表示模型。我们还对外部数据集进行了修改，使其成为近频域数据集，并将其添加到训练中。我们的努力主要集中在Task 1 - 远程自动语音识别（DASR）的远场静音环境下，我们使用了oracle分割。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Neural-Network-for-Acoustic-Resonance-Analysis"><a href="#Physics-informed-Neural-Network-for-Acoustic-Resonance-Analysis" class="headerlink" title="Physics-informed Neural Network for Acoustic Resonance Analysis"></a>Physics-informed Neural Network for Acoustic Resonance Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11804">http://arxiv.org/abs/2310.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuya Yokota, Takahiko Kurahashi, Masajiro Abe</li>
<li>for: 这个研究使用物理学生网络（PINN）框架来解决声波方程的共振分析问题。</li>
<li>methods: 这种方法称为ResoNet，它除了使用传统PINN损失函数外，还尝试使用神经网络函数近似能力来有效地解决共振分析问题。此外，它也可以轻松应用于反向问题。</li>
<li>results: 在一个一维声波管中进行了共振分析，并通过对前向和反向分析进行比较，证明了提案的方法的有效性。<details>
<summary>Abstract</summary>
This study proposes the physics-informed neural network (PINN) framework to solve the wave equation for acoustic resonance analysis. ResoNet, the analytical model proposed in this study, minimizes the loss function for periodic solutions, in addition to conventional PINN loss functions, thereby effectively using the function approximation capability of neural networks, while performing resonance analysis. Additionally, it can be easily applied to inverse problems. Herein, the resonance in a one-dimensional acoustic tube was analyzed. The effectiveness of the proposed method was validated through the forward and inverse analyses of the wave equation with energy-loss terms. In the forward analysis, the applicability of PINN to the resonance problem was evaluated by comparison with the finite-difference method. The inverse analysis, which included the identification of the energy loss term in the wave equation and design optimization of the acoustic tube, was performed with good accuracy.
</details>
<details>
<summary>摘要</summary>
这项研究提出了物理学 Informed Neural Network（PINN）框架，以解决音频振荡分析中的波方程。ResoNet，这个研究所提出的分析模型，不仅会遵循普通的PINN损失函数，还会将 périodic solutions 作为损失函数的最小化，从而有效地利用神经网络的函数近似能力，进行振荡分析。此外，它可以轻松应用于反向问题。在这种情况下，我们对一维音频管进行了振荡分析。我们采用了PINN方法和finite-difference方法进行前向和反向分析，并证明了PINN方法的可靠性和精度。Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Blind-estimation-of-audio-effects-using-an-auto-encoder-approach-and-differentiable-signal-processing"><a href="#Blind-estimation-of-audio-effects-using-an-auto-encoder-approach-and-differentiable-signal-processing" class="headerlink" title="Blind estimation of audio effects using an auto-encoder approach and differentiable signal processing"></a>Blind estimation of audio effects using an auto-encoder approach and differentiable signal processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11781">http://arxiv.org/abs/2310.11781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peladeaucome/ICASSP-2024-BEAFX-using-DDSP">https://github.com/peladeaucome/ICASSP-2024-BEAFX-using-DDSP</a></li>
<li>paper_authors: Côme Peladeau, Geoffroy Peeters</li>
<li>for: 估算音频效果（BE-AFX）旨在估算一个原始、未处理的音频样本中应用的音频效果（AFX），不需要知道AFX的具体实现。</li>
<li>methods: 我们提出了一种替代方案，使用自适应oder来估算音频质量指标。我们对常用的Mastering AFX进行了不同的实现，包括极差信号处理或神经网络近似。</li>
<li>results: 我们的自适应oder方法可以在不知道AFX的具体实现情况下，对音频质量产生更好的估算，比传统的参数基于方法更好。<details>
<summary>Abstract</summary>
Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio Effects (AFXs) applied to an original, unprocessed audio sample solely based on the processed audio sample. To train such a system traditional approaches optimize a loss between ground truth and estimated AFX parameters. This involves knowing the exact implementation of the AFXs used for the process. In this work, we propose an alternative solution that eliminates the requirement for knowing this implementation. Instead, we introduce an auto-encoder approach, which optimizes an audio quality metric. We explore, suggest, and compare various implementations of commonly used mastering AFXs, using differential signal processing or neural approximations. Our findings demonstrate that our auto-encoder approach yields superior estimates of the audio quality produced by a chain of AFXs, compared to the traditional parameter-based approach, even if the latter provides a more accurate parameter estimation.
</details>
<details>
<summary>摘要</summary>
《盲目估计音效（BE-AFX）》的目标是基于原始、未处理的音频样本估计音效（AFX）。传统方法通过优化损失函数来学习AFX参数。这需要了解AFX的具体实现。在这个工作中，我们提出了一种不同的解决方案，即使用自适应网络方法，优化音质指标。我们研究、建议和比较了各种通用音压缩AFX的实现方式，使用演变信号处理或神经网络近似。我们的发现表明，我们的自适应网络方法可以在不知道AFX实现细节的情况下提供更高质量的音频估计结果，与传统参数基本方法相比，即使后者可以更准确地估计参数。
</details></li>
</ul>
<hr>
<h2 id="EchoScan-Scanning-Complex-Indoor-Geometries-via-Acoustic-Echoes"><a href="#EchoScan-Scanning-Complex-Indoor-Geometries-via-Acoustic-Echoes" class="headerlink" title="EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes"></a>EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11728">http://arxiv.org/abs/2310.11728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Iljoo Jeong, Seungchul Lee, Jung-Woo Choi</li>
<li>for: 该研究旨在实现准确的室内空间几何计算，以便建立准确的数字双子，其应用包括不熟悉环境导航和高效逃生规划，尤其是在低光照条件下。</li>
<li>methods: 该研究提出了一种深度神经网络模型，利用声学听觉采集到的听觉响应来进行室内空间几何计算。传统的声学技术仅仅估算室内几何参数，而不能直接推断室内几何图像，因此有限了可推断的室内几何类型。相比之下，EchoScan可以直接推断室内几何图像，并且可以处理具有拐角的室内空间。EchoScan的关键创新在于通过多个聚合模块分析室内响应的低频和高频听觉之间的复杂关系。</li>
<li>results: 与视觉方法相比，EchoScan在具有不同形状的室内空间中表现出色，其可以准确地计算室内几何图像。<details>
<summary>Abstract</summary>
Accurate estimation of indoor space geometries is vital for constructing precise digital twins, whose broad industrial applications include navigation in unfamiliar environments and efficient evacuation planning, particularly in low-light conditions. This study introduces EchoScan, a deep neural network model that utilizes acoustic echoes to perform room geometry inference. Conventional sound-based techniques rely on estimating geometry-related room parameters such as wall position and room size, thereby limiting the diversity of inferable room geometries. Contrarily, EchoScan overcomes this limitation by directly inferring room floorplans and heights, thereby enabling it to handle rooms with arbitrary shapes, including curved walls. The key innovation of EchoScan is its ability to analyze the complex relationship between low- and high-order reflections in room impulse responses (RIRs) using a multi-aggregation module. The analysis of high-order reflections also enables it to infer complex room shapes when echoes are unobservable from the position of an audio device. Herein, EchoScan was trained and evaluated using RIRs synthesized from complex environments, including the Manhattan and Atlanta layouts, employing a practical audio device configuration compatible with commercial, off-the-shelf devices. Compared with vision-based methods, EchoScan demonstrated outstanding geometry estimation performance in rooms with various shapes.
</details>
<details>
<summary>摘要</summary>
准确地估算室内空间几何结构是建立精准数字 duplicates 的关键，其广泛的工业应用包括在不熟悉环境中导航和有效逃生规划，特别是在低照明条件下。本研究介绍EchoScan，一种深度神经网络模型，利用声学折射来进行室内空间几何推测。传统的声音基本技术是估算室内几何参数，例如墙position和室内大小，因此限制了可以推测的室内几何类型。相比之下，EchoScan可以直接推测室内平面图和高度，因此可以处理具有任意形状的室内空间，包括拱形墙。EchoScan的关键创新在于使用多维度聚合模块来分析室内响应函数（RIRs）中的低频和高频响应之复杂关系。通过分析高频响应，EchoScan可以推测复杂的室内形状，即使声音在室内设备的位置不可见。在本研究中，EchoScan通过使用来自复杂环境的RIRs进行训练和评估，并使用实际的音频设备配置，与商业市场上可以购买的设备相符。与视觉基本方法相比，EchoScan在具有不同形状的室内空间中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Results-of-Underwater-Sound-Speed-Profile-Inversion-by-Few-shot-Multi-task-Learning"><a href="#Experimental-Results-of-Underwater-Sound-Speed-Profile-Inversion-by-Few-shot-Multi-task-Learning" class="headerlink" title="Experimental Results of Underwater Sound Speed Profile Inversion by Few-shot Multi-task Learning"></a>Experimental Results of Underwater Sound Speed Profile Inversion by Few-shot Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11708">http://arxiv.org/abs/2310.11708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Fan Gao, Junting Wang, Hao Zhang</li>
<li>for: 快速和准确地估算海域声速 Profile (SSP) 对于海域声信号的传播模式具有很大的影响，因此快速和准确地估算 SSP 非常重要。</li>
<li>methods: state-of-the-art SSP inversion methods include frameworks of matched field processing (MFP), compressive sensing (CS), and feedforeward neural networks (FNN), among which the FNN shows better real-time performance while maintaining the same level of accuracy.</li>
<li>results: MTL outperforms the state-of-the-art methods in terms of accuracy for SSP inversion, while inheriting the real-time advantage of FNN during the inversion stage.<details>
<summary>Abstract</summary>
Underwater Sound Speed Profile (SSP) distribution has great influence on the propagation mode of acoustic signal, thus the fast and accurate estimation of SSP is of great importance in building underwater observation systems. The state-of-the-art SSP inversion methods include frameworks of matched field processing (MFP), compressive sensing (CS), and feedforeward neural networks (FNN), among which the FNN shows better real-time performance while maintain the same level of accuracy. However, the training of FNN needs quite a lot historical SSP samples, which is diffcult to be satisfied in many ocean areas. This situation is called few-shot learning. To tackle this issue, we propose a multi-task learning (MTL) model with partial parameter sharing among different traning tasks. By MTL, common features could be extracted, thus accelerating the learning process on given tasks, and reducing the demand for reference samples, so as to enhance the generalization ability in few-shot learning. To verify the feasibility and effectiveness of MTL, a deep-ocean experiment was held in April 2023 at the South China Sea. Results shows that MTL outperforms the state-of-the-art methods in terms of accuracy for SSP inversion, while inherits the real-time advantage of FNN during the inversion stage.
</details>
<details>
<summary>摘要</summary>
水下声速谱（SSP）分布对声音信号的传播模式有着很大的影响，因此快速和准确地估算SSP是建设水下观测系统的关键。现状的SSP拟合方法包括匹配场处理（MFP）、压缩感知（CS）和前向神经网络（FNN）等，其中FNN在实时性方面表现更好，同时保持同等的准确性。然而，FNN的训练需要很多历史SSP样本，这在许多海洋区域是困难的满足。这种情况被称为“少shot learning”。为解决这个问题，我们提出了多任务学习（MTL）模型，其中参数之间有部分共享。通过MTL，可以提取共同特征，因此加速学习过程，降低参考样本的需求，从而提高总体化能力在少shot learning中。为验证MTL的可行性和效果，在2023年4月在南海进行了深海实验。结果显示，MTL比现状的方法在SSP拟合精度方面表现更好，同时继承FNN在拟合阶段的实时优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.SD_2023_10_18/" data-id="closbrov000yi0g88ht5ub0ym" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/19/eess.SP_2023_10_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-10-19
        
      </div>
    </a>
  
  
    <a href="/2023/10/18/eess.AS_2023_10_18/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-10-18</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
