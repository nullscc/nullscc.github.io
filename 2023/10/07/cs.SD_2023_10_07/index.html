
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-10-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04863 repo_url: None paper_authors: Yangze Li, Fan Yu, Yuhao Liang, Pengcheng Guo, Mohan Shi, Z">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-10-07">
<meta property="og:url" content="https://nullscc.github.io/2023/10/07/cs.SD_2023_10_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04863 repo_url: None paper_authors: Yangze Li, Fan Yu, Yuhao Liang, Pengcheng Guo, Mohan Shi, Z">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-07T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:28:28.375Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/cs.SD_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T15:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-10-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SA-Paraformer-Non-autoregressive-End-to-End-Speaker-Attributed-ASR"><a href="#SA-Paraformer-Non-autoregressive-End-to-End-Speaker-Attributed-ASR" class="headerlink" title="SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR"></a>SA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04863">http://arxiv.org/abs/2310.04863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangze Li, Fan Yu, Yuhao Liang, Pengcheng Guo, Mohan Shi, Zhihao Du, Shiliang Zhang, Lei Xie</li>
<li>for: 这个研究是为了提高自动话语识别（ASR）和Speaker Diagnosis（SD）的融合模型，以提高会议记录的准确率。</li>
<li>methods: 该研究使用了一种新的非自然语言模型（Paraformer），并提出了一种 speaker-filling 策略和一种 inter-CTC 策略来提高模型的性能。</li>
<li>results: 实验结果表明，我们的模型在 AliMeeting 数据集上比 cascaded SA-ASR 模型减少了6.1%的相对Speaker-dependent Character Error Rate（SD-CER），并且与 SOTA 联合 AR SA-ASR 模型的 SD-CER 相似（34.8%），但具有1&#x2F;10 RTF。<details>
<summary>Abstract</summary>
Joint modeling of multi-speaker ASR and speaker diarization has recently shown promising results in speaker-attributed automatic speech recognition (SA-ASR).Although being able to obtain state-of-the-art (SOTA) performance, most of the studies are based on an autoregressive (AR) decoder which generates tokens one-by-one and results in a large real-time factor (RTF). To speed up inference, we introduce a recently proposed non-autoregressive model Paraformer as an acoustic model in the SA-ASR model.Paraformer uses a single-step decoder to enable parallel generation, obtaining comparable performance to the SOTA AR transformer models. Besides, we propose a speaker-filling strategy to reduce speaker identification errors and adopt an inter-CTC strategy to enhance the encoder's ability in acoustic modeling. Experiments on the AliMeeting corpus show that our model outperforms the cascaded SA-ASR model by a 6.1% relative speaker-dependent character error rate (SD-CER) reduction on the test set. Moreover, our model achieves a comparable SD-CER of 34.8% with only 1/10 RTF compared with the SOTA joint AR SA-ASR model.
</details>
<details>
<summary>摘要</summary>
joint模型化多speaker ASR和speaker分类有最近显示出色的结果在speaker所有自动语音识别(SA-ASR)中。尽管能够获得状态之arte(SOTA)性能，大多数研究都基于一个autoregressive(AR) decoder，这会导致大量的实时因子(RTF)。为加速推理，我们引入了一个非autoregressive模型Paraformer作为SA-ASR模型中的声学模型。Paraformer使用单步decoder，以便并行生成，与SOTA AR transformer模型相当。此外，我们提出了一种speaker填充策略，以减少speaker认定错误，并采用一种inter-CTC策略，以提高encoder在声学模型中的能力。在AliMeeting corpus上的实验表明，我们的模型比杂合SA-ASR模型的测试集比例6.1%的相对speaker依赖字符错误率(SD-CER)下降。此外，我们的模型与SOTA联合AR SA-ASR模型相当的SD-CER值为34.8%，只需1/10 RTF。
</details></li>
</ul>
<hr>
<h2 id="FM-Tone-Transfer-with-Envelope-Learning"><a href="#FM-Tone-Transfer-with-Envelope-Learning" class="headerlink" title="FM Tone Transfer with Envelope Learning"></a>FM Tone Transfer with Envelope Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04811">http://arxiv.org/abs/2310.04811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fcaspe/fmtransfer">https://github.com/fcaspe/fmtransfer</a></li>
<li>paper_authors: Franco Caspe, Andrew McPherson, Mark Sandler</li>
<li>for: 本研究旨在控制生成的音频使用 musical instruments，提高表达性和短语表达能力。</li>
<li>methods: 本文提出了 Envelope Learning，一种新的 tone transfer 建模方法，通过在生成参数层次使用教学目标来映射音乐事件。</li>
<li>results: 本研究实现了在实时演奏场景中提高音频表达性、短语表达能力和多样性，并实现了精准地捕捉音频事件的开始和结束。<details>
<summary>Abstract</summary>
Tone Transfer is a novel deep-learning technique for interfacing a sound source with a synthesizer, transforming the timbre of audio excerpts while keeping their musical form content. Due to its good audio quality results and continuous controllability, it has been recently applied in several audio processing tools. Nevertheless, it still presents several shortcomings related to poor sound diversity, and limited transient and dynamic rendering, which we believe hinder its possibilities of articulation and phrasing in a real-time performance context.   In this work, we present a discussion on current Tone Transfer architectures for the task of controlling synthetic audio with musical instruments and discuss their challenges in allowing expressive performances. Next, we introduce Envelope Learning, a novel method for designing Tone Transfer architectures that map musical events using a training objective at the synthesis parameter level. Our technique can render note beginnings and endings accurately and for a variety of sounds; these are essential steps for improving musical articulation, phrasing, and sound diversity with Tone Transfer. Finally, we implement a VST plugin for real-time live use and discuss possibilities for improvement.
</details>
<details>
<summary>摘要</summary>
<<SYS>>采用深度学习技术，音源与 sintizer之间的 Tone Transfer 技术可以改变音频片断的时域特征，同时保持音乐形式内容。由于其音质效果良好和连续可控，因此在多个音频处理工具中应用。然而，它仍然存在许多缺点，如音色缺乏多样性和过渡和动态渲染的限制，这些缺点阻碍了 Tone Transfer 的表达和phrase演奏的可能性。在这项工作中，我们介绍了目前 Tone Transfer 架构的问题，以及控制 synthetic audio 的乐器的挑战。然后，我们介绍了 Envelope Learning，一种新的方法，可以在 synthesis 参数层次上使用 musical event 来训练 Tone Transfer 架构。我们的技术可以准确地渲染音乐事件的开始和结束，并且可以为不同的音频种类实现多样化。这些步骤对于提高 Tone Transfer 的表达、phrase 和音色多样性是关键。最后，我们实现了一个 VST 插件，用于实时现场使用，并讨论了进一步改进的可能性。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Progressive-Clustering-for-Semi-supervised-Domain-Adaptation-in-Speaker-Verification"><a href="#Multi-objective-Progressive-Clustering-for-Semi-supervised-Domain-Adaptation-in-Speaker-Verification" class="headerlink" title="Multi-objective Progressive Clustering for Semi-supervised Domain Adaptation in Speaker Verification"></a>Multi-objective Progressive Clustering for Semi-supervised Domain Adaptation in Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04760">http://arxiv.org/abs/2310.04760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Li, Yuke Lin, Ning Jiang, Xiaoyi Qin, Guoqing Zhao, Haiying Wu, Ming Li</li>
<li>for: 这个论文旨在提出一种新的半监督领域对应方法，专门针对语音识别 задачі。</li>
<li>methods: 本文使用限定目标领域的标签数据来 derivate 对应领域的领域特有描述子，并运用InfoMap算法进行嵌入聚类，以更正target领域的伪标签。此外，我们还引入subcenter-purification和进度融合策略来进一步改善伪标签的质量。</li>
<li>results: 本文的提案方法（Multi-objective Progressive Clustering，MoPC）在VoxSRC 2023 track 3的评估集上获得4.95% EER，排名第一名。此外，我们还进行了额外的实验在FFSVC dataset上，取得了良好的结果。<details>
<summary>Abstract</summary>
Utilizing the pseudo-labeling algorithm with large-scale unlabeled data becomes crucial for semi-supervised domain adaptation in speaker verification tasks. In this paper, we propose a novel pseudo-labeling method named Multi-objective Progressive Clustering (MoPC), specifically designed for semi-supervised domain adaptation. Firstly, we utilize limited labeled data from the target domain to derive domain-specific descriptors based on multiple distinct objectives, namely within-graph denoising, intra-class denoising and inter-class denoising. Then, the Infomap algorithm is adopted for embedding clustering, and the descriptors are leveraged to further refine the target domain's pseudo-labels. Moreover, to further improve the quality of pseudo labels, we introduce the subcenter-purification and progressive-merging strategy for label denoising. Our proposed MoPC method achieves 4.95% EER and ranked the 1$^{st}$ place on the evaluation set of VoxSRC 2023 track 3. We also conduct additional experiments on the FFSVC dataset and yield promising results.
</details>
<details>
<summary>摘要</summary>
使用大规模无标签数据的 Pseudo-labeling 算法在语音识别任务中成为了重要的 semi-supervised 领域适应技术。在这篇论文中，我们提出了一种新的 Pseudo-labeling 方法，即 Multi-objective Progressive Clustering (MoPC)，特意设计用于 semi-supervised 领域适应。首先，我们利用目标频道的有限标注数据来 derive 频道特有的描述符，基于多个不同的目标函数，即在 Graph 中的内部干扰、类内干扰和类间干扰。然后，我们采用 Infomap 算法进行嵌入聚类，并使用描述符进一步改进目标频道的 Pseudo-标签。此外，为了进一步提高 Pseudo-标签 的质量，我们引入 subcenter-purification 和 progressive-merging 策略来进行标签干扰。我们的提出的 MoPC 方法在 VoxSRC 2023 评测集上取得了 4.95% EER，并在评测集上排名第一。我们还进行了额外的 FFSVC 数据集的实验，并取得了有优的结果。
</details></li>
</ul>
<hr>
<h2 id="An-Exploration-of-Task-decoupling-on-Two-stage-Neural-Post-Filter-for-Real-time-Personalized-Acoustic-Echo-Cancellation"><a href="#An-Exploration-of-Task-decoupling-on-Two-stage-Neural-Post-Filter-for-Real-time-Personalized-Acoustic-Echo-Cancellation" class="headerlink" title="An Exploration of Task-decoupling on Two-stage Neural Post Filter for Real-time Personalized Acoustic Echo Cancellation"></a>An Exploration of Task-decoupling on Two-stage Neural Post Filter for Real-time Personalized Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04715">http://arxiv.org/abs/2310.04715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Jiayao Sun, Xianjun Xia, Ziqian Wang, Xiaopeng Yan, Yijian Xiao, Lei Xie</li>
<li>for: 这个论文旨在探讨personalized acoustic echo cancellation (PAEC)中的任务解耦策略，以及如何使用多尺度本地-全球speaker表示来提高speaker抽象。</li>
<li>methods: 这个论文提出了一种基于两阶段任务解耦post-filter (TDPF)的PAEC方法，并应用了多尺度本地-全球speaker表示来提高speaker抽象。</li>
<li>results: 实验结果表明，任务解耦模型可以比单一联合网络提供更好的性能，而且在任务解耦序列中，优化训练策略可以further提高模型的性能。<details>
<summary>Abstract</summary>
Deep learning based techniques have been popularly adopted in acoustic echo cancellation (AEC). Utilization of speaker representation has extended the frontier of AEC, thus attracting many researchers' interest in personalized acoustic echo cancellation (PAEC). Meanwhile, task-decoupling strategies are widely adopted in speech enhancement. To further explore the task-decoupling approach, we propose to use a two-stage task-decoupling post-filter (TDPF) in PAEC. Furthermore, a multi-scale local-global speaker representation is applied to improve speaker extraction in PAEC. Experimental results indicate that the task-decoupling model can yield better performance than a single joint network. The optimal approach is to decouple the echo cancellation from noise and interference speech suppression. Based on the task-decoupling sequence, optimal training strategies for the two-stage model are explored afterwards.
</details>
<details>
<summary>摘要</summary>
深度学习基于技术已经广泛应用于声学噪声抑制（AEC）领域。通过使用说话者表示方法，AEC的前iers延伸了，因此吸引了许多研究人员的关注。同时，任务解耦策略广泛应用于speech enhancement。为了进一步探索任务解耦方法，我们提议使用两个阶段任务解耦后filter（TDPF）在PAEC中。此外，我们采用多尺度本地-全球说话者表示方法来提高PAEC中的说话者抽取。实验结果表明，任务解耦模型可以在PAEC中提供更好的性能。最佳方法是在任务解耦序列中分离噪声和干扰音频抑制。基于任务解耦序列，我们后续探索了两个阶段模型的优化训练策略。
</details></li>
</ul>
<hr>
<h2 id="Spike-Triggered-Contextual-Biasing-for-End-to-End-Mandarin-Speech-Recognition"><a href="#Spike-Triggered-Contextual-Biasing-for-End-to-End-Mandarin-Speech-Recognition" class="headerlink" title="Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech Recognition"></a>Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04657">http://arxiv.org/abs/2310.04657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaixun Huang, Ao Zhang, Binbin Zhang, Tianyi Xu, Xingchen Song, Lei Xie</li>
<li>for: 提高自动语音识别（ASR）系统对 Contextual Phrases 的识别性能</li>
<li>methods: 使用 Attention-based Deep Contextual Biasing 方法，同时支持显式和隐式偏袋</li>
<li>results: 实现了对 Contextual Phrases 的显著改进（32.0% 相对 CER 减少），并可以与浅混合方法相互协作以获得更好的结果<details>
<summary>Abstract</summary>
The attention-based deep contextual biasing method has been demonstrated to effectively improve the recognition performance of end-to-end automatic speech recognition (ASR) systems on given contextual phrases. However, unlike shallow fusion methods that directly bias the posterior of the ASR model, deep biasing methods implicitly integrate contextual information, making it challenging to control the degree of bias. In this study, we introduce a spike-triggered deep biasing method that simultaneously supports both explicit and implicit bias. Moreover, both bias approaches exhibit significant improvements and can be cascaded with shallow fusion methods for better results. Furthermore, we propose a context sampling enhancement strategy and improve the contextual phrase filtering algorithm. Experiments on the public WenetSpeech Mandarin biased-word dataset show a 32.0% relative CER reduction compared to the baseline model, with an impressively 68.6% relative CER reduction on contextual phrases.
</details>
<details>
<summary>摘要</summary>
针对给定的上下文表达，基于注意力的深度上下文偏好方法已经证明可以有效提高端到端自动语音识别（ASR）系统的识别性能。与浅层融合方法不同的是，深度偏好方法不直接偏袋ASR模型的 posterior，而是通过隐式地整合上下文信息，使其控制上下文偏好的难度增加。在本研究中，我们提出了触发器驱动的深度偏好方法，可以同时支持显式和隐式偏好。此外，两种偏好方法都展现出了显著的改善，可以与浅层融合方法相互协同使用。此外，我们还提出了上下文采样优化策略和上下文表达过滤算法改进。实验结果表明，在公共的WenetSpeech普通话biased-word数据集上，与基eline模型相比，我们的模型可以 дости到32.0%的相对报告错误率（CER）减少，其中上下文表达减少了68.6%的相对CER。
</details></li>
</ul>
<hr>
<h2 id="Neural2Speech-A-Transfer-Learning-Framework-for-Neural-Driven-Speech-Reconstruction"><a href="#Neural2Speech-A-Transfer-Learning-Framework-for-Neural-Driven-Speech-Reconstruction" class="headerlink" title="Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech Reconstruction"></a>Neural2Speech: A Transfer Learning Framework for Neural-Driven Speech Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04644">http://arxiv.org/abs/2310.04644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cctn-bci/neural2speech">https://github.com/cctn-bci/neural2speech</a></li>
<li>paper_authors: Jiawei Li, Chunxu Guo, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li</li>
<li>for: 实现直接通过脑电传输机制进行脑语言交流，这是脑机器人界的一个重要目标。</li>
<li>methods: 我们提出了一个名为Neural2Speech的专业转移学习框架，这个框架包括两个不同的训练阶段。首先，我们使用可以在各种语音数据库中找到的语音自动化器进行预训练，以将语音波形从对应的语音表现中解析出来。其次，我们使用小型脑电资料进行适应器的训练，以将脑活动和语音表现相互调整。</li>
<li>results: 我们的提案Neural2Speech可以实现从脑电资料中重建语音，即使只有20分钟的脑电资料。与之比较，我们的方法在语音质量和准确性方面表现出色，较之前的基eline方法更高。<details>
<summary>Abstract</summary>
Reconstructing natural speech from neural activity is vital for enabling direct communication via brain-computer interfaces. Previous efforts have explored the conversion of neural recordings into speech using complex deep neural network (DNN) models trained on extensive neural recording data, which is resource-intensive under regular clinical constraints. However, achieving satisfactory performance in reconstructing speech from limited-scale neural recordings has been challenging, mainly due to the complexity of speech representations and the neural data constraints. To overcome these challenges, we propose a novel transfer learning framework for neural-driven speech reconstruction, called Neural2Speech, which consists of two distinct training phases. First, a speech autoencoder is pre-trained on readily available speech corpora to decode speech waveforms from the encoded speech representations. Second, a lightweight adaptor is trained on the small-scale neural recordings to align the neural activity and the speech representation for decoding. Remarkably, our proposed Neural2Speech demonstrates the feasibility of neural-driven speech reconstruction even with only 20 minutes of intracranial data, which significantly outperforms existing baseline methods in terms of speech fidelity and intelligibility.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>直接通过脑机器 interfaces 的沟通是重构自然语音的重要任务。先前的努力已经探索了使用复杂的深度神经网络（DNN）模型，通过大量神经记录数据进行训练，以实现语音重构。然而，在有限规模神经记录数据的情况下，实现满意的语音重构性能是困难的，主要是因为语音表示的复杂性和神经数据约束。为了解决这些挑战，我们提出了一种基于传输学习的神经驱动 speech reconstruction 框架，称为 Neural2Speech，该框架包括两个不同的训练阶段。首先，一个语音自适应器在可用的语音资料上进行预训练，以解码语音波形从编码的语音表示中。其次，一个轻量级的适配器在小规模神经记录数据上进行训练，以对神经活动和语音表示进行对应。值得注意的是，我们提出的 Neural2Speech 已经在只有 20 分钟的脑内数据上达到了比较出色的语音质量和可读性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/cs.SD_2023_10_07/" data-id="clpxp045a010rfm88a2pj99ky" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/08/eess.SP_2023_10_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.SP - 2023-10-08
        
      </div>
    </a>
  
  
    <a href="/2023/10/07/cs.CV_2023_10_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-10-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
